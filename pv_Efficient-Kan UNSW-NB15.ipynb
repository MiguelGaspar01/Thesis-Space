{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbdef64-5462-455b-96fa-863d4b36bdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2c096b-e87b-4597-a60e-d432d11ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = {\n",
    "    0: 20000,  # Class 0 should have 56000 samples\n",
    "    1: 20000,  # Class 1 should have 56000 samples\n",
    "    2: 30000,  # Class 2 should have 56000 samples\n",
    "    3: 34000,  # Class 3 should have 56000 samples\n",
    "    4: 30000,  # Class 4 should have 56000 samples\n",
    "    5: 41000,  # Class 5 should have 56000 samples\n",
    "    6: 56000,  # Class 6 should have 56000 samples\n",
    "    7: 30000,  # Class 7 should have 56000 samples\n",
    "    8: 20000,  # Class 8 should have 56000 samples\n",
    "    9: 10000,  # Class 9 should have 56000 samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45addaf4-c17e-4cb2-94b3-a5c6629a8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 6\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96863f09-6fdd-47a1-aa2a-0c75a380e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 5\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011c575f-d547-4b41-84c0-1af4ba290d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"majority\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af5f4e3-48a5-4a60-b951-1cb02855acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102d84fa-c8e2-425c-84f8-25b8a6272498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy = \"minority\", random_state=42, n_neighbors= 3)\n",
    "#train_data_X,train_data_y = adasyn.fit_resample(train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "#smote_enn = SMOTEENN(sampling_strategy=\"not majority\", enn = enn, random_state=42,n_neighbors = 5)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "#train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_data_y)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "#print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAANECAYAAADfROz+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsvQmYVNW1vr8FEQJqgiBDFDCICCKzyuAAKqSJQKKRQSSAGBABwQQxyCANhjCKijIoIsikjE4xMoMSrswgiMwoEBRBUHNVBOHa/+ddv2fX/1R1dXd1dTc9fe/z1E33Gffep9p7PtZa37ogKSkpyQkhhBBCCCGEOO8UOP+3FEIIIYQQQggBEmRCCCGEEEIIkU1IkAkhhBBCCCFENiFBJoQQQgghhBDZhASZEEIIIYQQQmQTEmRCCCGEEEIIkU1IkAkhhBBCCCFENiFBJoQQQgghhBDZhASZEEIIIYQQQmQTEmRCCCGylVdffdVdcMEF7uDBg9k9FJEF8GyHDBmS3cPIMej7nn/ge8+zjocHHnjAXXXVVZk+JpEzkSATQohseiGL9nniiSey5J4ffvihvRx8++23WXL9/MypU6dsbd9///3sHooIgOAJ/m0VKFDAXXbZZe53v/udW7t2bXYPL8euU/BTv359lxN57bXX3HPPPRfz8Qgb5tOkSZOo+19++eXQnDdt2pSJIxUiNi6M8TghhBCZzFNPPeV+85vfhG27/vrrs0yQDR061P7V9Ve/+pXLSXTo0MHdd999rnDhwi63CjLWFho3bpzdw8lx/Pjjj+7CC7PvdaNdu3burrvucv/3f//n9u7d6yZOnOhuv/12t3HjRle9evVsG1dOw69TkMsvv9zlVEG2Y8cO95e//CXmc4oUKeJWrVrlvvzyS1emTJmwfbNnz7b9p0+fzoLRCpE2EmRCCJFN8C/1N9xwg8vN/PDDD65YsWIZukbBggXtk9v4+eef3U8//ZTdw8jx8KKbndSpU8f96U9/Cv1+66232t/epEmTTJyJ6OuUWSByLrroIotQZic333yzifC5c+e6Rx99NLT9yJEj7t///re755573MKFC7N1jCL/opRFIYTIoSxatMheHhE8l1xyiWvevLn75JNPwo7Zvn27Rb0qVqxoL778y++DDz7oTp48GTqGdLrHH3/cfiYi51NzSFXy6UqkUaZV++PrIXbu3Onuv/9+V7x4cXfLLbeE9s+aNcvVrVvX/eIXv7DUMKJe//nPf+KqqSHFqEWLFpYGiGjlmkQzfFrgG2+8Yb8zZ+65devWsGuyJhdffLH79NNPXUJCgq3hr3/9a4tKJiUlJROVjz32mCtXrpxF6a699lr39NNPJzuOMT7yyCP2r+nVqlWzY1988cVQFIEomV9bv26xPJ/g2u7fvz8UxfzlL3/pOnfubBG4SFjrm266yRUtWtSew2233eaWLl2a7u9Peupeoj0n0rtY35IlS9oz4vvF/GL5HsUyV6JrvXv3tuszh9///vfu888/z1BdGmsCBw4cCNs+bdo0d8cdd7hSpUrZs73uuutMtEXiv5tr1qyxZ8Bz5fnOmDEj2bGsN9dkba688ko3bNgwE/LRQBz67xXf1Z49eyZLMSYCSxSd71WjRo3s+VeqVMktWLDA9n/wwQeuXr16dj++x8uXL3eZBX9LrVu3tr9t7ks647/+9a+wY/j75NnMmTPHDRo0yF1xxRV27P/+7//a/vXr17tmzZrZ82Y7c/if//mfsGt89913FvlinVkLnkfTpk3dli1bQmvAfQ8dOhT6e4ul1orn9Mc//tGia0Fef/11+xviexyNlStXhv6O+K7+4Q9/cLt27Up2HN+HG2+80e5z9dVXu5deeinFscT730qRd1GETAghson//ve/7sSJE2HbePGEmTNnuk6dOtlLwqhRo+xFlZdDBBDiw7+ALFu2zF6UeJnlZZ8XwMmTJ9v/rlu3zl5WeAkhVYsXj2effTZ0D4TEV199le5x81J2zTXXuOHDh4dEyz/+8Q/35JNPujZt2rguXbrYdV944QUTCow3njRJXtgRft26dbN/uUcktWzZ0kTQgAEDXI8ePey4ESNG2H337NkT9q/wpKjx8seL4+jRo93ixYtdYmKiO3funAkzYPy85JPK9Oc//9nVqlXLLVmyxAQsL/6sV+TL2bx580yYsY41a9a059K9e3f7F3bWGmrUqBHz8wnCPBA1zIkX0ClTptgLKd8BD8IPMdKwYUObB9EHXnQZ229/+9t0fX8ywvHjx+1+fI+ofeQZI9YQy7EQy1wRbKw3aa08RwQHwjIjeEHJS3gQ1gdBxPeBFMt//vOf9h1DQCGOIr+brVq1su8M6zx16lQbKy/ZXANIjSM1ku8b68MLPc+el/BIeJ48V2qc+C7xXWY8RHQQLIUKFQod+80335gg5CWev0WO42f+oQAh8/DDD9vfzZgxY2yMvOgjZtOC70jkf48QTtz72LFj9n3jGARyiRIl3PTp022tEIN894P8/e9/t+9l37593ZkzZ+xnvp9EJlkj/g75W/UimAgV4hYYP9fkbwxRzD9eIHYQQUTxBg4caP/tJLLl/z75x5dYYF34ziLGEU2AQGOdgmvsQdAyZgQ3z4h/IOC/a0Tb+M76v6OPP/449LfAcTxz5li6dOlk18yK/1aKPECSEEKI88q0adNQMVE/8N133yX96le/SuratWvYeV9++WXSL3/5y7Dtp06dSnb9119/3a61evXq0LYxY8bYts8++yzsWH5nO2OKhO2JiYmh3/mZbe3atQs77uDBg0kFCxZM+sc//hG2/eOPP0668MILk21PaT2CY6tQoYJt+/DDD0PblixZYtt+8YtfJB06dCi0/aWXXrLtq1atCm3r1KmTbevVq1do288//5zUvHnzpIsuuijpq6++sm1vvfWWHTds2LCwMbVq1SrpggsuSNq/f3/YehQoUCDpk08+CTuWa0WuVXqfj1/bBx98MOzYe+65J6lEiRKh3/ft22djYPv//d//hR3L/NL7/YmGH0taz+nNN9+03zdu3Jjq9VL6HqU1182bN9txf/nLX8KOe+CBB1Jc72jf7aFDh9ozYv7//ve/k2688UbbPn/+/DSfVUJCQlLFihXDtvnvZvD5HT9+PKlw4cJJjz32WGgb4+a49evXhx3HMwiuI9v4Tv72t78Ne6bjx4+346ZOnRra1qhRI9v22muvhbbt3r079N1ct25dsr+XaH/b0dYp2sf/Tfm5sH4evme/+c1vkq666qrQuDme41iz4Hry3bzmmmtsPf331K8512jatGloG+vTs2fPVMfM3zHPIVY4lnPOnTuXVKZMmaS///3vtn3nzp023g8++CD0/Q5+n2vVqpVUqlSppJMnT4a2bdu2zda6Y8eOoW133313UpEiRcL+u8S1+e9i8G8pPf+t5L9h6ZmjyN0oZVEIIbKJCRMmWAQl+AH+l1Qliuz5F2v/oc6KdCSiOZ7gv7ZTq8Fx3hnNp/hkNvwLdhAiIkQR+Bff4HiJCBFJC443PfCv4w0aNAj9ztyBf1EvX758su1EoiLhX9kjUw6p+/KpXO+9956tK//qH4QURrQEaX9BSLFiXLGS3ucTubakShEh8Clfb731lq314MGDk9Xk+Ghber4/GcH/S/67777rzp49m+7z05orEU3wkVBPr1690nUfIhVELvg+cg8iLWPHjrWoSErPykeved58r/g9CN8Bn/oIXJ8UweB3kO8Wz9pHfvxx7du3D7sW30W+k0S3gs+0a9eu7tJLL02WFkg0iIiYh/vyLKpWrRr6W0jr7yIaDz30ULL/HhEB9nNhHsEUZcbBOUQcSWMOQtQwuJ4fffSR27dvn0WoeMb+O0m68J133ulWr14dSuVkLkR8v/jiC5fZ8DfAf6fIFgCiiqQqB5+l5+jRozZuIp+kFXqIfpNCyZr4SDxR9bvvvjvsv0s8j8g0yKz6b6XI/ShlUQghsglecKKZevDi4oVHNHhJ83z99deW6kTNBilkQSJfIjOLSGdIxot44YUiGtFSgWIh+HLj06eAF6ho20nlCsLLLalGQSpXrhyWtkYdCvU6kSldvEz5/anNPS3S+3wi5+zT6pgbz51UK+aVmihMz/cnIyBW7r33XpsfqWPU9vBSykt3LI6Zac2VtWeukWtOzVR6QDSQ2ocgJm3u+eeft5foSEgNRLxhiR9Zy8az8t+zaGP34w9+Bxl/UCAFBVQQ/x2L3E6aH9/fyO8gtWiRqa6MLda/i5Tg7zclW/iU5hL8Owk6xEb7b4QXainBGrOGpBdzHPMhvRHnx44dOyb7W44Xvp98B7Zt22bpiojbaDWTKT0XP29EGIKSmjdSGaP9949zvXDLyv9WityPBJkQQuQw/L8UUwcUac8MQQtx/qUVS3tqnqh/4l+tOZ/aqZTMA4Kk1LQ02gurJ7IGhvtwHaJJ0dwSY63viCQl58WUtkeacGQF0ep/UiO9zycz5pae709GvhMcR60PtXDUW/GCiqEH0Se2pfXcz9dzDAoNaq+4LzVd1Hf5fxBB6BKpqVKlinvmmWdMDCCIeJlGbEY+q+z8DubEv4tY/hsB1LXxdxAN/33hb4aI1ZtvvmlGNZxDXSHRJeq5MgrCkvoxIpKfffaZCbTzRVb9t1LkfiTIhBAih+GLzTE4SOlfrP2/fK9YscIiFKSwRf5rdCwv2T4qEenmFvmv8mmNl5c+/lXcR6ByArz8kK4VHBPmJuCL8StUqGApY/wrdzBKtnv37tD+tEhpbdPzfNKz1syLFLGUXmxj/f6kRPA7ETQYSOk7QVoeH8wKiDiQkkdEEMOCjMDaM1demoMRBQw1MgKmEDQCxgXQp0UiKDGfeOedd8KiXxlJIWP80Z41hh2Rx/ntwSgQaYzMPZ5nmNkwxshxp+fvxH8niXzGMp+yZctaqiofIsuYefD98oIspb+5WCGdF8dLIl0p/R0Fn0u0eWPqg1ELrooI0FiedU79b6XIflRDJoQQOQzqDnhxwcUwWm2Od0b0/8Ia+S/gzz33XLJzfK+wSOHFfXixoIYjSHr6M+EsyFgQHpFj4fdIi/fzyfjx48PGwu+kBRENAd8wOHgcEBXhpS+Wf5HHvjva2qbn+cQKKYGk8eGuGBm18feJ9fuT1stz8DtBahauepGCM3Ju/uUWcZNRfP1N5HcRR7qMgMjEuZOIHjVCKT0rUuhwAYwXvltECjds2BC29tQtBUGgEI0jjS54/1deecXGkFFXycyAuTAP0jmD3wlcI/nHjbTqKkk95HuFU+r333+f4neSv8XIVF7+YYG04uB3iv+eZSQlm38sID2VaG5qopDvM9/74N82DamJ3Pkm2nx3+K5S33n48OHQcdQq8h3LLf+tFNmLImRCCJHD4GUaK2usvvmXYWocMAPg/9lT4I/lMgKC47BKpuaCF296/vCiwL+qR3sh8tEBrocowUKeFxteTkaOHGn/SwoXL+I+khQLvGjxr839+/e32ixEA9EmxkHaETU82F+fb/iXayIg1KOQpkSaEOuHZb7vHcYakLrGujB2TAxYw7fffttSmrw4SQ3+dZwXUhrO8q/eGABQT8Mn1ucTK9RPMVZsxUnr4gWPei3s0XlpxUI+1u9PSmDfTZQIS3dSLXmBxNbdX8PDiypiCctz1okoI5En7u9fVjMC31lq1BCwvKh623v/3cxIlITGwFyX7z3RPOaMKOL7gFhDNDAXxADmDvHwt7/9zdJGSU/lft72nsgLfcQ8rCt/O7ykcyxW8kRWWFv6WmVFs+b0QoonRhj8AwUGOHzHef58l2mmnFbTZ/bT1oDzaQtAGwj+HmgtQRSS7wxRSr5D1MhhuMLfIil8RLD5fgfFE98N/t769Olja8RxPLtY4RnE0seOdEnGjLkQfw/e9p76vOD5PDv+W8PfJFE9bO85jrkGn3VO/W+lyAFkt82jEELkN6LZK0cDC2lsorGBxlL56quvNsvvTZs2hY45cuSI2YVjc85xrVu3Tvriiy+i2oJj9XzFFVeYZXPQdhvr6T//+c92/iWXXJLUpk0bs+JOya7cW8ZHsnDhwqRbbrklqVixYvapUqWK2Vfv2bMnLtt7bKoj4bhIS2xv2421f9AymjEcOHDA7MSLFi2aVLp0aZtDpF089t1//etfk379618nFSpUyOy5uVbQnjule3uw569bt67ZlwfXLdbnk9LaRlsbwAq9du3aZrVevHhxs0NftmxZur8/KYHlfL169Ww+5cuXT3rmmWeSjWXLli3WAoH9jAN78BYtWiS7fkbm+sMPP9iaX3bZZUkXX3yx2YvzfeK4kSNHpjqHaN+LIKwFFuS+tcE777yTVKNGDVsrrNxHjRpl6xzrd5NnwCfI9u3bbRvX5G+Pv8FXXnkl6jPF5p6/Gb6DfFe7d++e9M033yS7R7Vq1ZLdOz1/L+ldJw9/S7SD4LvMfG666aakd999N+wYb3sf2VLAs3Xr1qQ//vGP1t6A7wzj5r83K1assP1nzpxJevzxx5Nq1qxp/y3ib5ifJ06cGHad77//Pun++++3sXC/tOzhU1qfWP67vHz58qSbb77Z2m1ceumlSS1btjRL+0iwzvf/DcD2/8UXX0yxhUQs/62U7X3+4gL+T3aLQiGEECIzwaoaw4lo6VEid0OaYe3atd2sWbOSWcgLIURuRDVkQgghhMiRkCIWCamGpMCRDiqEEHkB1ZAJIYQQIkdC/d3mzZutzg+7fuoA+VBrE9l3SwghcisSZEIIIYTIkTRs2NAtW7bMTExIP8VsBDMFjE2EECKvoBoyIYQQQgghhMgmVEMmhBBCCCGEENmEBJkQQgghhBBCZBOqIRMil/Hzzz+7L774wppJZqQxqhBCCCGEyBqoCqPZ+a9//es0m6dLkAmRy0CMyV1MCCGEECLn85///MddeeWVqR4jQSZELoPImP8Dv/TSS7N7OEIIIYQQIoL//d//tX9A9+9tqSFBJs47DzzwgPv222/dW2+9laHrkK735ptvurvvvtvlZho3buxq1aplzU5jwacpIsYkyIQQQgghci6xlJfI1EOcd8aNG+deffVVl5dBZP3lL38J2/b+++/bHyViVAghhBBCCFCELJ/y008/uYsuuihb7v3LX/7S5fc1yAyuT1ziChQumt3DEEIIIYTI8Rwc2dzlVBQhyycQsXnkkUcsalOyZEmXkJDgduzY4X73u9+5iy++2JUuXdp16NDBnThxIuycXr162TnFixe3Y15++WX3ww8/uM6dO1tObKVKldyiRYtC5/zf//2f+/Of/+x+85vfuF/84hfu2muvtYhYZMpiMM2Q+/Tu3dv97W9/c5dddpkrU6aMGzJkSNg5+/btc7fddpsrUqSIu+6669yyZcuSzZGaqjZt2rhf/epXdp0//OEP7uDBg8nu+49//MMcbxhbRpg4caK75pprbEysTatWrUL3+eCDD2zeRMT4MI7bb7/d9rOWbOM4IYQQQgiRv5Egy0dMnz7dIkL/8z//40aOHOnuuOMOV7t2bbdp0ya3ePFid+zYMRM0kecg4DZs2GDirHv37q5169auYcOGbsuWLe63v/2tCblTp06FLNlxkpk/f77buXOnGzx4sBswYICbN29emmMrVqyYW79+vRs9erR76qmnQqKLa/7xj3+0sbP/xRdfdP369Qs7/+zZsyYyEYn//ve/bY4IzWbNmlkkzLNixQq3Z88eu/a7774b91qyZohIxsn1WD8EIyDEGjRo4Lp27eqOHj1qH4o6Fy5caPs5nm2RQjUlzpw5Y4WhwY8QQgghhMgbKGUxH0E0B7EDw4YNMzE2fPjw0P6pU6eacNi7d6+rXLmybatZs6YbNGiQ/dy/f38Tcgg0xAYguCZNmuS2b9/u6tev7woVKuSGDh0auiaRsrVr15ogixR7QWrUqOESExND4xw/fryJp6ZNm7rly5e73bt3uyVLllhkCxg30T3P3LlzTbhNmTIlVDw5bdo0i5ZRu4VwBEQfx2Q0VfHw4cN2rRYtWpgIrFChgq2nT8nk+kWLFrVon4eoHZQqVcrGFSsjRowIW1MhhBBCCJF3UIQsH1G3bt3Qz9u2bXOrVq2yKJL/VKlSxfYdOHAgTCh5ChYs6EqUKOGqV68e2kaqHhw/fjy0bcKECXavyy+/3K47efJkEzCpEbwPlC1bNnTNXbt2mVD0YgyIQAVhPvv37zdx5OeDADp9+nTYfBh7ZtSNIRQRYRUrVrQI4ezZs0NRwswGIfzf//439CE1UwghhBBC5A0UIctHENHxfP/9965ly5Zu1KhRyY5DDHmIeAUh+hTc5qNRRKdgzpw5rm/fvm7s2LEmmhBIY8aMsVTD1Ih2H3/NWGA+iECEUSQIw2hrkBGYFymbRN+WLl1qkULq3jZu3Jiu6FcsFC5c2D5CCCGEECLvIUGWT6lTp47VNF111VXuwgsz72tA7Rb1ZT169AhtC0ao4qFq1aoWFaLuyovFdevWJZsPaYukA56v3lysW5MmTexDuiVCbOXKlaF6NwxOgvjIXOR2IYQQQgiRf5Egy6f07NnTHBPbtWsXcjck5Y8IFzVWpCfGA/VfM2bMsHov6sdmzpxpUSN+jhcEDzVtnTp1smgbphYDBw4MO6Z9+/a2D2dFjDYwFjl06JB74403bH78nplgCPLpp5+akQeuie+9955F9LxzI0KXqCDuij59khRHIn+ce9ddd5kLJfviZcfQBDWGFkIIIYTI5aiGLJ9CPRbRLKI1GF5QW4W9PVGeAgXi/1p069bNIkRt27Z19erVcydPngyLlsUD43nzzTfdjz/+6G666SbXpUsXs64PgoHG6tWrXfny5e3+RNWw36eGLCtEC+uE2MOpknvh/Pj666+7atWq2X7SNhG1WPSTMkkN3RVXXGHmHE888YTV3tGGQAghhBBC5G8uSEpKSsruQQghYocIIU6OGHwoQiaEEEIIkbvf1xQhE0IIIYQQQohsQjVkIt9CA+lgL7MgpEdS45Waq6MQQgghhBAZRYJMJOOBBx5w3377rXvrrbcydB0MLKj9uvvuu11Oo3Hjxu766693H330UVyCTAghhBBCiMxAgkwkY9y4cS4/lBZiW1+pUqVsF67xcn3iElegcNFsubcQQgiR2zg4snl2D0GIqEiQ5VB++umnUN+q8w0FiPl9DYQQQgghhDgfyNQjB6XQYYOO9XzJkiVdQkKC27Fjh9U40asKm/QOHTq4EydOhJ3Tq1cvO4deWBxDb7EffvjBde7c2V1yySUWAVq0aFHoHGzusYOnLxgpefTNIiIWGfkJphlyn969e4f6lZUpU8YNGTIk7Jx9+/ZZT64iRYqY1fuyZcuSzZHmzm3atDHLeK5DzzD6dEXeF0t7bPl9T694mThxovVFY0ysTatWrcL2nzt3ztYcAcqaP/nkk6HIIL3MSGmMpFatWnYc858+fbp7++23LTWTz/vvvx/TPDkO+/5ixYrZMTfffLP1TBNCCCGEEPkPCbIcBC/4RIToDzZy5EjrcVW7dm23adMmt3jxYnfs2DF70Y88BzGxYcMGE2fdu3d3rVu3dg0bNnRbtmyxHmMIuVOnTtnxNC+mSfL8+fPdzp073eDBg92AAQPcvHnz0hwbAoJmx6NHjzbB4kUX16T3F2NnPz25+vXrF3b+2bNnTWQiEjHTYI4IzWbNmlkkzLNixQq3Z88euzYNlOOFNUNEMk6ux/ohGCPnRNoia4cofeaZZ6wpNjz44INu165d1tTas3XrVrd9+3YTu/QZ41kw/qNHj9qHNU9rnohARGejRo3sWmvXrnUPPfSQCbqUOHPmjFmnBj9CCCGEECJvoD5kOQSiULxoI6Jg2LBh9kK/ZMmS0DFHjhxx5cqVM4FRuXJlO4eIF8cBPxPtQRzNmDHDtn355ZeubNmy9uJfv379qPcmSsRxCxYsiFobFXkfIMKDYEQ4Ll261DVv3tyiPES2AAFEdM+besyaNcvmhMjx4gOBQoSI+yAcuS/n0UQ5o6mKNG1GOLFmiKNo6338+HH3ySefhMZDw+Z33nnHhCrcdddd7qqrrrJIGyDwPv74Y7dq1aqo6wRpzfOGG25wJUqUsCgZoiwWiMbRUDqScn+ZpxoyIYQQIkZUQybOJ+pDlkupW7du6Odt27bZiz/RFf+pUqWK7Ttw4EDouBo1aoR+LliwoL3sV69ePbSNVD1AfHgmTJhg97r88svtupMnTzYRlBrB+wAiz18T8YFQ9GIMGjRoEHY889m/f7+JIz8f0vlOnz4dNh/Gnhl1Y02bNnUVKlRwFStWtAjh7NmzQ1FCDwI1GJlizKReIj6ha9eu7vXXX7cxIqpee+01i5ylRlrz5GeEHFG0li1bWmSO6Fpq9O/f3/6Y/YeUSCGEEEIIkTeQqUcOgpTAYJ8rXthHjRqV7DjEkKdQoUJh+xAYwW1ecJBWCHPmzLF0u7Fjx5oAQTiMGTPGUg1TI9p9/DVjgfkgAhFGkSAMo61BRmBeRBuJRBHBIzWTSBMpiESrYoH1L1y4sEX5EImkI0bWocUzz2nTplm0jWjg3Llz3aBBgyxFM6UIJmPgI4QQQggh8h4SZDmUOnXquIULF1rKHHVOmQU1TdQ69ejRI7QtGKGKh6pVq1rUhkiPF4vr1q1LNh/ER6lSpdIM22YWrFuTJk3sk5iYaEJs5cqVltIJkSKUMWMCQqTRn9+pUycTUAiy++67L6w3Gdt8NC2986Q2kA/RL4Qx0beUBFlK7BiacN7WUgghhBBCZA1KWcyh9OzZ03399deuXbt2FtVBNFFPRl1UpAhIDwgODC+41t69e80xMGhcEQ8IHmraEC+k7FFrNnDgwLBj2rdvb+YjOA6y/7PPPrPoFZEi6rwyGwxBnn/+eWv8TG0bNXVE9ILOjaRp9unTx2rySE184YUX3KOPPhp2nS5dupiII5oVma6IWMaYg/NxvySCltY8+R0RRk0f4yJ6R5okolYIIYQQQuQ/JMhyKNRjEc1CfGF4QW0V9vZEeQoUiP+xdevWzSJEbdu2dfXq1XMnT54Mi5bFA+Mhre/HH380sw9EDNb1QYoWLepWr17typcvb/dHgGC/T21VVkR5WCeMPTAe4V44PyK6qlWrFjqmY8eOoTEjgBFjOB5GClgiitTvsV5BqDFD4GHUQToizyutebJ/9+7d7t577zURy/24N89FCCGEEELkP+SyKEQq8OeBKEO0Ek3Lba49QgghhBAiZ7+vqYZMiBT46quvzASFlgCkigohhBBCCJHZSJDlQqL1v4oHnBJ9n7CcCDVY9DKLBqmGQYONaG6HGQVjDurBaAtQvHjx8/pshBBCCCFE/kCCLBdC76q8nmmKEcbtt9/uNm/eHDXMm5Ygywyyeo0l3oQQQgghhARZnNAoODMaGMcD+aj5ZQ1o7Bxr37Dc8vwyi+sTl7gChYtm9zCEEELkQQ6ObJ7dQxAi3yCXxRhp3Lixe+SRR8zpkDS2hIQEt2PHDkupu/jii13p0qVdhw4dzP48eE6vXr3sHFLeOObll192P/zwg9Uk0by4UqVKbtGiRaFzcFXEle83v/mNRYBw8SMiFhlZCaYZch9s1f/2t7+5yy67zJUpU8aaIAfBWv22225zRYoUcdddd501Io6EXmJt2rQxAcR1sG4/ePBgsvvioIgLZNBCPh7OnDnj+vXr58qVK2eNj1mLV155xe5JdAxYN1IruXdaLFiwwNwoWbcSJUqYHT9rndLYBwwYkMw5EWrWrOmeeuqpNO/Hs8Log/Xifqx/ZFQtpTHxfKZPn+7efvttmx8fooJCCCGEECJ/IUGWDniBJqqCvfnIkSPNUp3mvvT1ok/VsWPHTNBEnoOA27Bhg4mz7t27u9atW5uV+pYtW8zSHiF36tQpO55eWVdeeaWbP3++27lzpxs8eLAJh3nz5qU5tmLFilmz49GjR5ug8KKLa2LBztjZjwU8QigIPbQQmYhEareYI0KzWbNmFk3yrFixwvpucW16fWUEbOexoqdf2K5du9xLL71k90Sg0RQbuBcNpyNFaSQcQ882eoVxLcQNcw4KpMix0zOM5xJsjP3JJ59Yb7H7778/zfGPHTvWvfrqq27q1KluzZo11jeOmrxYxtS3b1/7rrC+HMeH74QQQgghhMhfKGUxHWB/jtiBYcOGmRgbPnx4aD8v5ogJGi7TY8pHWwYNGmQ/0xAYIYdAo4cVILgmTZpkIqB+/fquUKFCbujQoaFrEimjiTCCLFLsBalRo4ZLTEwMjXP8+PEmQJo2beqWL19uva9oBk10CBh30DBj7ty5JtymTJli0RqYNm2aRX8QEghHQPRxTEbT/Vgj5oQ4Imrk0xM9ROi8sUYsKYsImnPnzpngqVChgm0jMhUk2th5Pq+99po1yIbZs2db1IxoXVo899xz9ky5JyB0WeNYx0TUjCghEc3U4Bg+QRtVIYQQQgiRN1CELB3UrVs39PO2bdvcqlWrLKLjPzQPhmDEBaHkKViwoKWtBV/KSWOE48ePh7ZNmDDB7kWzYa6Ly9/hw4dTHVvwPlC2bNnQNYnOIBS9GIMGDRqEHc989u/fbxEyPx9EEQ2Ng/Nh7JlRe/XRRx/ZejRq1MhlBgirO++808ZHBJLU0G+++SbsmGhjJ0qGIAMiV0Ts2JYW9JRAcAVTHi+88EJrEp2eMcXCiBEjrG7Qf3iWQgghhBAibyBBlg6IsARt1Vu2bGnCIvjxtVoeIl5BiD4Ft/loFNEpoO8V6WzUkS1dutSuSb1ZMG0wGtHu468ZC8wHERg5HyJZwfS94BpkhMx2SETcEW2jHo8auRdeeMHqxD777LNUx05KIWmMpI9++OGHVkfXtm3b8zamWCAKhwD0H8YohBBCCCHyBhJkcVKnTh2rN7rqqqssvS34yYhooXaLWqIePXpYSiTXC0ao4qFq1ar2Ek9Ex7Nu3bpk80FMkiIYOZ+scHUkaoRg/OCDD6Lu95EsjDNiBRF68803W8rn1q1b7RrBmq5oUK9HlI5URT6keLIGacGaEIWkJs9DeiI2/bGOiZ9jmR+GJ1j/Bz9CCCGEECJvoBqyOOnZs6eloBFh8e6GpPwR4aJOiehIPFD/NWPGDKtFon5s5syZbuPGjfZzvFCjRU1bp06d3JgxY6wGaeDAgWHHkKbHPpwVMQRBqBw6dMi98cYbNj9+z0wQsowHwwtMPUjv436kWVIrR80VYgbzjbvuussiaqRRpgTCiJo5at0QVPz+1VdfmRhNC+ZO/R1RyGeffTbmOTz66KNWE8gzI131mWeesb5isY6JNeA5E6EjlRWRFxnpTI0dQxMkzoQQQgghcjmKkMUJ9VhEs4hw8MJNxAd7ewwoChSIf1m7detmJhCkzVGfdPLkSYuWZQTGQ1SGZso33XST69Kli9m/BylatKhbvXq1K1++vN0f0UDaJDVkWfXSj5lJq1atbH4IGoxOvE39FVdcYVGlJ554wursaDmQGoyR8SPeEJ8YqeCCGDQuSQnGwDrjdBlsJ5AWjz32mDlkIiypyaP+7p577ol5TMyXFEbqzqgX5PskhBBCCCHyFxckRTZOEkLkaIhwEk2jnkwRMiGEEEKI3P2+pgiZEEIIIYQQQmQTqiETcUMD6ZRSAkmPTM1JEVfH9IDtP06FKUETbdItM5PUatZwTrz11lsz9X5CCCGEECL/IUEmwnjggQfMmOKtt95K81hqn7DGjwZGF5iE4FqYWTV7Kd3L789sUrsfNW5CCCGEEEJkFAkyEca4ceOsQXIsEAHDFj8lsIVPbX96oOlyZl0rVs73/YQQQgghRP5DgiwHgv2678N1vsmKnmO5bQ1yC9cnLnEFChfN7mEIIcR55eDI5tk9BCGEyFRk6pEDaNy4sdm6Y5tfsmRJl5CQ4Hbs2GH1WdQxYfuOvfqJEyfCzunVq5edU7x4cTuGvmjYxnfu3Nks2InwUOvkwaIfK3t6mhHdwnKdiFhkymLQ+p379O7dO9RrrUyZMm7IkCFh59BQ+rbbbnNFihSxOq9ly5YlmyONqekvRlsArkO/s4MHDya7L3b8pB8ytowwceJES5tkTKwN1vYZWTtI65ksXrzY3XLLLTZH+oq1aNEirKk386W3Gr3dbr/9dms1QP+1tWvXZmiuQgghhBAi9yJBlkOYPn26RYToRUWz4TvuuMPVrl3bbdq0yV70jx07ZoIm8hwE3IYNG0xgdO/e3bVu3do1bNjQbdmyxfqjIRrorwU///yzNXieP3++mWAMHjzYDRgwwM2bNy/NsRUrVswaG48ePdoaR3vRxTXpW8bY2f/iiy+6fv36hZ1/9uxZE5kIHYxAmCOiplmzZhYJ89BEmSbJXJuG0PHCmiEiGSfXY/0QjBlZO+rq0nomCLo+ffrYfuZC/zf6krFGQWjK3bdvX6tRoz8ZzcXPnTuX4nzOnDlj1qnBjxBCCCGEyBuoD1kOgIgNL9kIARg2bJgJlyVLloSOOXLkiCtXrpwJDF7iOYeIF8cBP5NuiDiaMWOGbfvyyy+tjosITP369aPem8gcxy1YsCCqqUfkfYDm0ogThOPSpUtd8+bN3aFDh0LGGogVIkk0oybqNWvWLJvTrl27LEIECDEiSdwH8cN9OQ83xYymKhKBItLFmiECo613etculmcSCdEzGj5//PHH7vrrr7cIGdHJKVOmWKQSEMbVqlWztaE5djSISNIkO5Jyf5mnlEUhRL5DKYtCiNyA+pDlQurWrRv6edu2bW7VqlUWRfIf/7IeTIGrUaNG6OeCBQtamlz16tVD20irg+PHj4e2TZgwwe6FUOC6kydPNhGUGsH7AELFXxMhgSgJuhw2aNAg7Hjms3//fhNHfj6kLZ4+fTpsPow9M+rGcHasUKGCq1ixokW5Zs+eHYp0xbt2sTwTUjeJdnFf/vCuuuoq2x65vsF7s5bB+0Sjf//+9sfsP6R/CiGEEEKIvIFMPXIIpAQGe3S1bNnSjRo1Ktlx/gUeChUqFLaP6FNwm49G+ZS5OXPmWKrc2LFjTTQhkLCmJ9UwNaLdJzINLzWYDyIQYRQJwjDaGmQE5kW08f3337cIHqmZRJk2btxoUbl41i6WZ8J+hCD1aAhUziUyFkzLjLx35H2iUbhwYfsIIYQQQoi8hwRZDqROnTpu4cKFFmHB7j2zoHaLGqkePXqEtgUjVPFQtWpVi9gcPXo0JEzWrVuXbD5z5851pUqVSjNkm1mwbk2aNLFPYmKiCbGVK1daWmJWPJOTJ09a6iJizDeMXrNmTYbnIYQQQggh8jYSZDmQnj172os96W/e3ZCUPyJc1B+RYhcPuA5SI0UdFLVMM2fOtKgRP8cLgof6qU6dOlm0jXxZTCuCtG/f3vbhrIjRBsYi1JxR68X8+D0zwRDk008/NSMPXBTfe+89i0BlxLkxrWfCfUh7JAUUYUqa4hNPPOGykh1DE86bwBVCCCGEEFmDashyIKS7Ec3CbALDC2qbsGgnyoNzX7x069bNIkRt27Z19erVs6hOMFoWD4wH844ff/zRzD66dOli1vVBsHdfvXq1K1++vN2fqBqmFtSQZYWgYJ0QexiPcC+cH19//XUzz8iqZ8IHcbZ582ZLU/zrX/9qIlQIIYQQQojUkMuiEHnYtUcIIYQQQpx/5LIohBBCCCGEELkA1ZCJHAk9v+hlFg3SI3/xi1+keC6OiEIIIYQQQuQGJMiiNEOOFyzMfTPkvApNlWvVquWee+65LL3PDTfc4D766KO4BNn5ABt9vi8pjVEIIYQQQohYkCBzzo0bN87l9VI6enLdfvvt7ptvvgn14srJ4g3BValSJXe+yKiYzixRL4QQQggh8hc5RpDRPPeiiy7KlntTcJff1yC/kpvX/PrEJa5A4aLZPQwhhMgQB0c2z+4hCCFEtpJtph5ETx555BGzDi9ZsqRLSEhwO3bssLqhiy++2JUuXdp16NDBnThxIuycXr162Tn0feIYekP98MMPrnPnzu6SSy6xqMqiRYtC52BTjsU6vbaIutCLiohYZHQjGBnhPr179w71mypTpoylqAXZt2+f9bkqUqSIu+6669yyZcuSzZGGyW3atLGIFNehD9fBgweT3RebeGzVM9InC86cOeP69evnypUr5woXLmxr8corr9g9iY4B60Y0iHunBevasWNHex701ho7dmyyYyZOnGj9zVgHnkerVq1Cc/vggw9srbkfn+Dco0H0jp5ll19+uT0rrjtt2jTbx7lcA2t5mltzP+zluUcQfsd+n/kzZnqBnTt3LtXvHc2e4Z577rF7+N9jhe/G9OnT3dtvvx2aKxFJP+Z58+ZZs2jmdOONN7q9e/da/zfSMllbvvNfffVVuu4phBBCCCHyBtnqsshLLNEJ+juNHDnS+kbVrl3bbdq0yS1evNgdO3bMBE3kObxIb9iwwcRZ9+7dXevWre0lfcuWLdYjCiF36tQpO56GwDQenj9/vtu5c6cbPHiwGzBggL0kpzW2YsWKufXr17vRo0dbQ2Mvurgm/bQYO/vpc4UQCnL27Fl72UckYlDBHHn5btasmUVlPCtWrHB79uyxa9PQOCMgnui39fzzz7tdu3a5l156ye6JQFu4cKEdw72OHj2aTJRG4/HHHzeBg9BYunSpiQzW2MNzQriyNlyXZ4ZIBa7foEED17VrV7sfH8aRGk8++aQ9IwQ14580aZI968gxPfbYY27r1q12/ZYtW1o/Nfj888/dXXfdZaJn27Ztdj6CdNiwYSl+73h2iCNA/DFO/3us9O3b176nPFs/V76PnsTERDdo0CBbuwsvvNDdf//9JvZZI74bNJjme5ma0MY6NfgRQgghhBB5g2xNWSQCgtgBXpoRY8OHDw/tnzp1qr3EE1GoXLmybatZs6a93EL//v1NyPHSzos/8GLLi/j27dtd/fr1XaFChdzQoUND1yRStnbtWhNkkWIvSI0aNexF2o9z/PjxJp6aNm3qli9f7nbv3u2WLFlikS1g3EFXwLlz55pwmzJlikVJ/As/0TKEDcIREH0ck9G0OdaIOSHsmjRpYtsqVqwY2k+EDkqVKhVTDRlOhYiZWbNmuTvvvDMkZBC3nsOHD9v4W7RoYcKzQoUK9gx9Gihzoik0EcZY4HqcT+QIokWqiG7de++99jPPGRHIOBE4ROv4vvCsWPMqVaq4L774wsQy3wvfVDv4vQvCusQ61iCIXqJfCKdo5yPYEOfw6KOPunbt2tl36eabb7ZtRHBfffXVFK8/YsSIsO+wEEIIIYTIO2RrhKxu3bqhn4lorFq1yl5u/YcXajhw4ECYUPIULFjQlShRwlWvXj20jbQ5OH78eGjbhAkT7F6kwnHdyZMn28t/agTvA6S/+WsSveHF34sxIFoThPkQ+UCo+Pkgik6fPh02H8aeGTVMuP2xHo0aNXKZAWMkklevXr3QNsYfTKtEnCLCEH5EJWfPnh2KTMYD0U5SEjECQWB9+OGHyY4JrjPRJsQbzwP4X/Z7AQyIHsTlkSNHon7vzgfB75L/fkZ+Z4Pf10j4hweaCvoPqbBCCCGEECJvkK0RMqIrHl6aST8bNWpUsuMQQx4iXkF4+Q5u8y/jRKeAF3wiFNQ/8bKOQBozZoylGqZGtPv4a8YC8+HFH5ESCcIw2hpkhOywgWctScMj4kdKI1Eo6qlI+YvHyZEI46FDh9x7771nkT4icz179nRPP/10po47s9Y8VqJ9PyO3pfbdoh6OjxBCCCGEyHtka4QsSJ06ddwnn3xiaWqYUQQ/GXmBpk6Iep4ePXpYOhzXC0ao4qFq1aoWpaBWyLNu3bpk88H4gxTByPlkhasjERde6iNNLjw+CofJSSxcffXVJhqCwhXTDVIjgxClIkWSFEDSRDGyWLlyZeiesd4vKFY7depkqZLY5RPNDBJcZ8w6Nm/ebM8D+F/SUYMtDHj+CMdgqmU0mGt6xxoknrkKIYQQQgiRY2zviYTgmEh9jXc3JOWPCBc1VqTjxQP1QjNmzLB6L+rHZs6caREcfo4XBAg1bQgHom2YLAwcODDsGNwC2YezIqYXCAKiP2+88YbNLy2BkF4QsoznwQcfNFMPau24H6lw1MqRWkgkBuMQjC+IqJFGmRLso7YJEw3SQhGWzNHXYQHX+vTTT83IA/dGIluIQp/WyJgQdIg0n7IZPD8SImxEFatVq2b1WFzfi61g+inPlO3PPvusiUTmDIhuRBxmL9SaYTRCHWCfPn1Sva8fq6/rIhrFfNID5/Md456s1/lopbBjaIK79NJLs/w+QgghhBAiH0TIqMcimkGUAcMLIj5Yk5P6ltbLdGp069bNHBHbtm1r9VA48vHinhEYD02Ef/zxR7NY79Kli1nXB8HMYvXq1a58+fJ2fwQEAocasqx6icbkAtt55kf9HUYnWNfDFVdcYcYQ2MBTs4RgSQsEJXbtpJIiQm+55Zaw+iueDQITd0zmh2MhLo8IKiBVFCFNWwAiX2nV7RFlol6KmitEHuciyINg4sIHwblmzRr3zjvvhJwYmSOiEAdO9j/88MO25t4EJjVIaSVNktpAb0ySHlhrhCg1bcyV77IQQgghhBBpcUFSML9LiBwKUTaimtjdY/qRnyEiSwQOgw9FyIQQQgghcvf7Wo6JkAkhhBBCCCFEfkOCLAdBk+Cg7X/wQ/peSvtSqwVLCdIHU7teWumF8UAKYUr3Y19OgrTLlMYazTlTCCGEEEKIeMj3KYsPPPCA+/bbb91bb72VoetgmEFd2d133x33NahJ+/zzz1Pcl5q1Pe6N6QGHQtIAUzOpwEExM8FghPBtNAjlYhziwT6fZ0J/tewAQ5SzZ89G3UcNHuPN6POOF6UsCiGEEELkbNLzvpZjXBazi3HjxoXZpGcnCK70CqtYoE/Y7bffbo6Evj8YYisr7pUaCK6g6MopRBN/uFLm9Pq26xOXuAKFi563+wkhRLwcHNk8u4cghBA5lhyRsvjTTz9l271RrvE0Mc5La5Bfx44QJ1IohBBCCCFEvhJkjRs3Ntt1bO2xLE9ISHA7duxwv/vd76xGh5SwDh06uBMnToSdQ38pzqFHFMfQtwxb986dO1vzXyI+ixYtCp2DhT6250QviD5hS05ELDJlMZh2xn169+4d6oVWpkwZi6AEoeEztuxFihQxS3fs0iOhcTT9vxB7XId+ZMEUQX9f7PKx/Pe9u+KFvl39+vUz23b6aLEWr7zyit2T6BiwbqRWcu+0WLBggbUeYN3oq4XtvbfQT2nsR44csT5yzJdm3ljABxtLpwZW9jxTnqNvDxDE3xPrfmzlCf1SdxYUg6wBz44oHM8Gm356zgUjhcyf7wj2/awTDai55rZt22wfn1dffdWlB9/TDrt8zuc7FBzz8OHDbW58F+hJhwikvxvrRD+6adOmpet+QgghhBAi75BtEbLp06db3yn6NfEyTi8rXmg3bdrkFi9e7I4dO2aCJvIcBBx9phBn3bt3d61bt3YNGzZ0W7Zssf5lCLlTp07Z8TQp5oV3/vz5bufOndZ4eMCAAW7evHlpjg1BgZgYPXq0vUR70cU16SvG2NlP7y2EUBBqjxCZiAuMOpgjQrNZs2ZhAoJGxDQS5to0Qc4IHTt2tB5gNIXetWuXe+mll+yeCLSFCxfaMdzr6NGjyURpJByDsKLhMtdCyDDnYGpn5Ni///5716hRI6uBozcYAgdRy3qlBc8D0Ytw4fmXLVvWTZw4Mdlx3NOPh7nSAw0x5eF+zJXnx/cBUcpz+Prrr8OuQy82vnNcq2nTpu6xxx4zEw/mzYeedemB7yMsX77czmdcnpUrV7ovvvjCetI988wz1qi6RYsWJo75/iAq6ZWHmE0JhCZ5yMGPEEIIIYTIG2SLqQcRBF4qeWmGYcOGmXBZsmRJ6BheUBETvPRXrlzZziHixXHAz6QbIhRmzJhh27788kt7mV+7dq2rX79+1HsTmeM4IkDRTD0i7wM0f0Yw8hK/dOlS17x5czN9IDoECEiie97kgagLc+KFn4gJIMSIkHAfhCP35TzcDBF3GWHv3r0WpUIcEcmKpYYsNXguRJCIrkWrpYo29smTJ1sjaM4h8pMeENSI8QkTJoS28fyIkvm6Lu75z3/+0yKPNN0GxDCRJoolMT1B5BDduv/++0PCGHMSoqoc59eBZ0DEMiMGIkETl5RqyBgz9/z0009Dzc1p2E0ED4EW/B5PmTLF3XfffVHvxfiCwtNT7i/zVEMmhMgVqIZMCJHf+N/c0IeMF34P0ZRVq1aFWYvz4goHDhwIHVejRo3Qz9jAk0pHWp2HtDDv5ufhJZ97kebGdREOaVm6B+8DiDx/TUQWQtGLMWjQoEHY8cxn//79FiHz80GkIDCC82HsGRVjgJBgPYhQZQY1a9Z0d955p42PCCSpoYi5IJFjZwyIqvSKMb+m9erVC9sWuaZ+XF6M+WOIzCHSWFcE2M033xzaX6hQIRPTXD8IqZTnCyJvXoz572jwO+u/x8HvbCT9+/e3P2b/Yb5CCCGEECJvkG0ui6QEenipbtmypRs1alSy4xBDwRfsyChFcJuPRvk0uTlz5ljUZuzYsfbyjkAaM2ZMmnVN0e4TS+pdcD6IwGj9qhCG0dYgI6Rmhx8PiASibR9++KFFBF944QU3cOBAWzdfLxU59sweQ1aSWeseC2l9Z2P5flHrxkcIIYQQQuQ9coTtfZ06daz2J7N7X1G7RTpcjx49QtuCEap4qFq1qkUoqBXyYnHdunXJ5jN37lxLTTsffaKIuPBC/8EHH0RNWfSRLNLjYgWRQLSJD7V3pC6SotenT58Uo4qk3VGvld4oGWuK2KMOzhO5pj7yGOzHxjG+To7aQl+T6NMsiZhh6kHKYmpwXnrWJtr5kJFrxMOOoQnqQyaEEEIIkcvJEbb3PXv2tBd5jCR4gUY0UU+Ge2JGXnKvueYaM4ngWtRZPfnkk2Gue/GA4KGmrVOnTiYQqDUjehSkffv2JhCoU2L/Z599ZrVEOACmZt4QLwhZxoMJB7VQ/n7evASBgsDCfOOrr76yCF5qII68wQbpnZhUcB7CKSV4djhSUlOFKKJuCpFNPV9aPProo27q1KnmNshzwvjik08+SXYcdXg4MGLQ8t5779lx1ASSEkjUC5MXasWob+OYrl27msEL56S1fqwZaZc4e2KikR4Q3ohEb0ZDWqEQQgghhBC5RpBRj8VLPOILwwsiPkQ1MKAI1t+kF9zrMP3ANY8apZMnT4ZFy+KB8RApIlJDfVKXLl3M/j0IdU6YNpQvX97uj5DxVu5ZFdGYNGmSa9Wqlc2P+jvEiLepv+KKK8wUAndBapgQManBGBn/XXfdZeJz0KBBlvaJcUlqUSLSGxEnnMczxASF9Me04PkglnFJJNUTwxTEVSTUtSGyaTnAOb///e/DWhJwv3vvvdecNolSUseHGMfsIzU4BwdMDD9IKcXBMT0Q1cXdEmdLvstBwxAhhBBCCCFynMuiEOkl0g0zP5Me1x4hhBBCCHH+yRUui0IIIYQQQgiR35EgyyFQaxa0/Q9+SPtLaR8fDz24YukzRl1YatdLqy1APNbvKd0rmhNlPNEzatcyC8aU0niZS1bfXwghhBBC5B9yhMui+H+9sVJqTBx0FswMqHNKrQlysMdaZoABB46H0fC949ICsZkS48aNc5mZeUttWmRfNE+kZb0QQgghhBAZQYIsh4DgqlSp0nm5FyYUKd0L4ZSZrQfA29CnF1wVY2mcTX5uZkK/Oj45nesTl7gChf//RtlCCJETODiyeXYPQQghchVKWYwT+n6NGDHCGiUjpmrWrOkWLFhg+7Ccx2Z+xYoVFvnCdZF+aHv27Am7xj//+U934403uiJFiphN/j333BPa980331hfLhwCOR+Hw3379iWLGuHkyH7OxUUykrffftscB7lHxYoVzW3x3Llzof2ME4dGokJYx0c6RqYHxozlP06FrAmOiFjZe+jf1qZNG0urpFcZboQHDx5MlvrHGIjSXXvttW7AgAFRo1Ws91NPPRV2XvDZjB492kQnDZVZo+C80hpHauAESi82zi1RooQ5Q0ZG5xo3bmwtDtjH9WkHEHSD5Hh+Z1yMj7lyvBBCCCGEyH9IkMUJYmzGjBnuxRdftJ5Zf/3rX92f/vQna87soT8ZdvH08yLqRJ8wz7/+9S8TUVjEb9261cQbNvoeRAbnvfPOO9bLi5d4jvWpf/QKw0ofC3vSD7FsHzZsWLK6NEQdfb7oy4UtOyIuUnQhDhjLxx9/HDbG9IJ1PfdZtGiR27Vrlwk9hCYw7oSEBIs8MS7aHFCThd08kTAP64BwXbZsmfVNQ+Bt2LAhrKE36719+3Z3//33Rx1H//79zQLfj+e1114LpUbGOo6U4HmyhvRNW7NmjfXPow1CJNOnTzeBy3NCHCIemRPQn+3ZZ5+154HIxjmSNgFCCCGEECL/Idv7OKBxMJGP5cuXuwYNGoS205OMRsQPPfSQCST20zvL11E1b97c6sGIVhExI2I1a9asZNfnJZ3+X4gFjgOiX+XKlbMX/datW5sYwUYTYee57777rDkx9vC+iTX3R6B4uB+Rmy+++CIUIaPnGwIhoxBlQ4AhViLhvghGhBr3BAQQkSYECf3nEKGMH1ORYKpirVq1rFcYAguImq1cudKtW7cumSX+d999ZxG68ePH2/OIZxypQTQL8U0DaiDaSJSU/mnekp8IGZE0BJ8HsX3HHXeYUHzmmWdMjO3YsSOmmjS+b8Fm1dio8l0o95d5SlkUQuQ4lLIohBBOtvdZDQ2HEV5NmzYNc+AjYhaM5NSoUSP0c9myZe1/jx8/bv9LVMuLtUgQC0TUgql6pMeRwsc+f0xkKl9QHMK2bdssMhMcIw2jjx49auP3kFaZGdDMec6cOSagEH0ffvhh2FhYNyJTfiyIWpplB9eMSFFk3RhRMqJcwL8f0LiZbdFgXRAvKa1trOOIBn9QrF1w3XlO0dYv+Oz98/fPHkGNMEeQ8zyIsAXTSKNFY/mD9h/EmBBCCCGEyBvI1CMOvv/+e/tfolNXXHFF2D5qgvyLfTD64aMx1DdBZrompjZOasb++Mc/JttHlM5Dal1mQJ3boUOHLBpIeh6iqGfPnu7pp5+2sRBFimZzT0QrtbG0a9fO9evXz23ZssWEDDVgbdu2jTqGtNY11nFklMjIF8/fP3sEFWmZRFBZpx49ergxY8ZYumu0iBkRTurWIiNkQgghhBAi9yNBFgfXXXedCS9S6xo1apRsf1qRFh9BoV6qc+fOyfZVrVrVIibUHwVTFnmJ597+GPYH8Sl8Hsw8OOd8uTd6UdOpUyf73HrrrZbahyBjLHPnznWlSpVKM2wbyZVXXmnrjIhCkBGZ5DrRwEgEUcbaRktZzMg4iE4R6WLdb7vtNtvGc9q8ebNdNz0wxpYtW9oH0VqlShWr4Yt2Hb5rfIQQQgghRN5DgiwOSHfr27ev1RIR9bjlllssnY2aL17yY7F5T0xMtAjS1VdfbbVfvNgTWSIShKjA+Y90NmqNuN8TTzxh0Ti2A658N998s4kdti1ZssTqr4IMHjzYtWjRwtz8WrVq5QoUKGApe9QuRRqAZAbcj+gTzZNJG8SUA+EIpBgSBWKspFEisoimvfHGG5beyO+pwfmsGfVeqdW7EfljDbkmqY+s0VdffWVGIJigZHQcGKRQB8YzQkRRD+Zr9mIFUxBqzEh9xCGTujYEWnrbA+wYmpBuUSmEEEIIIXIWqiGLk7///e9mMkF9D6IDlz5SGDF4iAWMH+bPn28uitRcYfiAm6AHu3jEDYKK2jBqpxBsPqWtfv367uWXX7amyFjAL1261A0aNCjsHrgJIorYh70+5yBm4u0LlhYIINLriP4RQSpYsKDVlAHCY/Xq1SYOSaFkzRBI1G7FIioQlEQJqX0LWtxHg+fy2GOPmUDkPqQ3+vqtjI6D63bo0MEigDwXxHKwXUEsYCDCs0MsslakLtICgTpBIYQQQgiRv5DLohB52LVHCCGEEEKcf+SyKIQQQgghhBC5AAkyEcbDDz8cZpMf/KS1Ly+Q0vz4BPuKCSGEEEIIkRkoZTGHEmx2nBGwW6fPVVp1Vx5qrQixRoNwa2r7UnI+zE3QoywlMFXJinYFQ4YMsedMb7pYUMqiEEIIIUTOJj3va3JZzKFg1pEdWhlRlZqwykzR9f7777vbb7/dffPNN2Z0kV0iNiiIMqtFQEoiK70CWQghhBBC5G0kyFIBi3WcA7MDFHV+XwOROtcnLnEFChfN7mEIIYRxcGTz7B6CEELkSlRDFmFF/8gjj7i//OUvrmTJkmYbT8+u3/3ud1ZDVLp0abM8P3HiRNg5vXr1snOKFy9ux2Bp/sMPP1jTZ2zRibosWrQodA49qLBaxyKfFLhrr73WImKR0Z5gFIX70HuMXlmXXXaZK1OmjEVhguzbt8/s5unFRQPpZcuWJZvjf/7zH9emTRuLSHEd+nEdPHgw2X3/8Y9/uF//+tc2toxAPzL6gpUrV86aG7MWr7zyit2T6BiwbkSOuHdaLFiwwFWvXt3WDZv4Jk2a2FqzFtOnT3dvv/22XYsPETjg/pUrVzbL+4oVK5ot/tmzZ0M9wYYOHWr92fx5bAOibTSXptk1oWZaE3BcWqR0zauuusr2Y5PPNv+7EEIIIYTIvyhCFgEv9d27d7cmz7yQ8xLOSzn9u3788Ud7uUfQrFy5MuwchBJ9xObOnWvnk5bGi/eAAQPsXITc4cOHTRTQTJoGxPQhQ1R8+OGH7qGHHnJly5a1a6c2tj59+rj169e7tWvXmoChl1XTpk3tmvTVQhCyn3xVRGIQRAgik/5ZGFRceOGF1iCaHmrbt28PRcJWrFhhAiSaoEsvHTt2tLE+//zz1i/ts88+M0GLQFu4cKG799573Z49e+x+adVnHT161LVr186NHj3a1va7776zeZDaSaPuXbt2Wb4uPdwAwQmIYgQRAvPjjz+2htts45nRowzRTVNt+oEFo5OtW7e2MSGm2UaTbpp57927N3TtaKR0zebNm1vKJ+NjzenTFquo5eNJqY5PCCGEEELkPiTIIrjmmmvshR8QK7Vr13bDhw8P7Z86daqJCV7KiboAQsM3ZaYx8siRIy3Cxos/0KB40qRJJnpozkxzZyIoHiJliJZ58+alKshoIpyYmBga5/jx4008Ich48d+9e7dbsmSJCQ9g3ET3PIhFhNuUKVMsQgOIA6JlRJN++9vf2rZixYrZMRlNVWSNmBPCjkgWEKHyeFGDSImlhgxBdu7cOROevrk10TIP4gnhQvQwSLBhNlEpxBsNqxFknEP0E3EaPG/NmjUmsDE5IbIHTz/9tNWFEaVDQKdEStf0gpO5Ro4xNWg+Hvy+CCGEEEKIvIMEWQR169YN/UzK2apVq+zlOpIDBw6EBBlCyUPUg6hXUCgQtQJe7j0TJkwwcUfUjMgbtVq1atVKdWzB+wARNX9NokMIRS/GgEhYEOaDiyDRoSCnT5+2+XgYe2bUjWFowXo0atTIZQYIXyJUjI9IHwKyVatWlvKYGghRInTM8fvvvzdRl5bbDWvFsTzLIDyr4FqdDxD5REaDETKetRBCCCGEyP1IkEVAdMjDC3nLli3dqFGjkh2HGPIQ8QpC9Cm4zUejiE4B0RmiNGPHjjXRhEAaM2aMpRqmRrT7+GvGAvNBcM6ePTvZPuqkoq1BRshsi3jEHdE2UjyXLl3qXnjhBTdw4EBbN6KM0SDy2L59e4swIeJIHWT9Wfu01opn7OvQgmSGI2R6IELno3RCCCGEECJvIUGWCnXq1LE6J9LcSD/LLKhPa9iwoevRo0doW0ajLlWrVjXDDtL6vFhct25dsvkQLSJF8Hz0ryKShWD84IMPQimLQXwUDpOTWEGEUjfHh1RQUhep1yOCxPUir4V44xiEm+fQoUPJxhF5Hmv15Zdf2nOPx3wj2jW9qE7PfIUQQgghRN5GgiwVevbsaY6JGEl4d0NS/oiwUGMVqylDJNR/zZgxw+q9iOzMnDnTbdy4McUoTywgeEih7NSpk0XbSGsLihAgUsQ+nBWfeuopMxZBnLzxxhs2P37PTBAyjOfBBx8MmXpwP9IsqZVDKCGw3n33XXfXXXeFaq9SgkgYNXOkKiIq+f2rr74yMervx5piEkKqIdEw1pq0UJ7ZjTfe6P71r3+ZgIscJ2YjpFiyBkQsWU+ilzhOUlPI2n7xxRd2PoYiN9xwQ5pzj7wmUS62MwcEJb+nlW6ZGjuGJqgxtBBCCCFELke296lAPRbRLCIaiAAiPjgXkrJWoED8S9etWzczpsCNr169eu7kyZNh0bJ4YDwIDWqcbrrpJnOGxLo+CA6Pq1evduXLl7f7I2Sw36eGLKte7DEzoc6L+VWpUsWMTrCphyuuuMJSCZ944gmrs6PlQGowRsaPeEMgYdZB6qE3LuHa2PQjlkjB5Nn9/ve/d3/961/t2tToETHD9j4ITo+4HmLDz3mvv/66CcX33nvP2gjQvoD73XfffSYofU1gakS7JjBe0i6pAcMwRgghhBBC5G8uSMIzXAiRayD6SfSP1gaKkAkhhBBC5O73NUXIhBBCCCGEECKbkCATqULjZeq6on2ooUtpX2q1YClBrVdq12N/TqFatWopjjOai6UQQgghhBDRkKlHBnnggQfct99+aw2DMwI1S9SAYSKRk6AeC2OKaFCvlh5r+8aNG1sd13PPPZdizV5K9/L7cwo7d+50EydOtKbckcRSYyaEEEIIIQRIkGWQcePGubxchofgOnLkiJlTfPPNN1nagwuL+UqVKsUk3nICtBdgvAcPHjSHzK1bt6bZ3FsIIYQQQog8J8h++umnUE+r8w3Fevl9DXIjZ8+eTdZoO7dxfeISV6Bw0ewehhAiF3NwZPPsHoIQQuR7cmUNGdETbMyxoC9ZsqRLSEhwO3bsMPtzanhIGevQoYM7ceJE2Dm9evWyc+j9xDH0GMOCHVtz+kQR7Vi0aFHoHOzusYUn+kGkCEt1ImKRKYvBNEPu07t371DfsjJlyrghQ4aEnbNv3z6zUy9SpIi77rrrzAY9Epo806uLiBTXoXcYkZjI+2JtTyofY8sIZ86ccf369TM7dvpjsRavvPKK3ZPoGLBupFZy77RgXTt27GjPg0gSdu+RkPJHnzDWgeeBPb6fG82kWWvuxyc492gQvaPPGhbzPCuuO23aNNvHuVyDptiNGjWy+/k6r6lTp1o9GHNmnGlZ70fD94/Dxp778B0IPqPhw4fb/HiW9H87d+6ce/zxx+250qPMj1MIIYQQQuQ/cqUgg+nTp1tEiF5TI0eOdHfccYe9EG/atMktXrzYHTt2zARN5DkIuA0bNpg46969u2vdurVr2LCh27Jli/UaQ8idOnXKjv/555/thXn+/PlWMzR48GA3YMAAN2/evDTHVqxYMWtcTFNhXsK96OKa9ABj7Ox/8cUXTQhFRm8QmYhETDWYI8KGvlZEwjw0GKYJMtemuXJGQDzRK4sGzrt27XIvvfSS3ROBtnDhQjuGex09ejSZKI0GggNR9fbbb7ulS5e6999/39bYw3NCuLI2XJdnhkgFrk9TZvqKcT8+jCM16C3GM0JQM376n/Gsg9Dv7NFHH7X9rC/H0Pz7oYcech9//LF75513QimT6YHvEyxfvtzGSqNtz8qVK62hNP3TnnnmGZeYmOhatGhh4pbn//DDD1tfOtJCUxPLWKcGP0IIIYQQIm+Qa1MWiYAgdmDYsGEmxohEeIh88BK/d+9ea+oLNWvWtGbC0L9/fxNyvLTz4g8ILl7St2/f7urXr28pbTQuDkZC1q5da4IsUuwFqVGjhr14+3GOHz/exBMGELy079692y1ZsiRkUsG4fXNjIJKDcJsyZYpFXIAoChEWhA3CERB9HJPRVEXWiDkh7Jo0aWLbKlasGNpPJAdKlSoVUw3Z999/b9G1WbNmuTvvvDMkUhG3HhwTGT/iBOFZoUKFUKNk0kCZE42siTDGAtfjfExI4Kqrrkp2DNFRxLCH781jjz1mIs1z4403uvRCVA5KlCiRbLysHSKXxt1EMfnOIvgR9sHv4Zo1a6zxdDRGjBgR9j0UQgghhBB5h1wbIatbt27o523btrlVq1aFWY9XqVLF9h04cCBMKHmwbOcFunr16snc8Y4fPx7aNmHCBLsXL91cd/LkyWnarwfvA6TC+WsSnUEoBh0DiQYFYT779+83oeLnw4v96dOnw+bD2DOjbgxnQ9aDdL7MgDESyatXr15oG+MPplUiThFhCD+ikqQQ+shkPBDtnDNnjplqkC764YcfJjvGizXgeRC58oIxqyAdEjEW/I4Fv3P+exj8zkWCaKOpoP+QziqEEEIIIfIGuTZCRnQlGJFp2bKlGzVqVLLjEEOeSBMHok/BbT4aRXQKeMHv27ev1T8hmhBIY8aMsVSz1Ih2H3/NWGA+iMBo/ax8NCZyDTJCeqzrMwvWkhRGIn6kNBKdpNZu48aNcTk5EmE8dOiQe++99yzSh9AiHfHpp5+Oul7na85pfedi+X5Q38ZHCCGEEELkPXJthCxInTp13CeffGJpatQABT8ZES3UblFf1qNHD0uH43rBCFU8VK1a1SIc1Bp51q1bl2w+GH+QIhg5n6xwdSRigyCg5isaPgqHyUksXH311SY6gsIV0w1SIyNt7kmRJI2PNFHMN6i58veM9X5BsdqpUydLlcQun2hmaoKQ7wuppBklvesjhBBCCCFEro+QBSESgmNiu3btQu6GpPwR4aLGirSweKD+a8aMGVbvRf3YzJkzLYLjXfXiAQFCTRvCgWgbBg0DBw4MOwa3QPbhrIjpBbVXRH8wi2B+wVqszABhwngefPBBq3ei1o77kUZHrRyphURxMA656667LLpEGmVKsA93Sow9SMdDWDLHYOoe1/r000/NyAODCyJbiEKf1siYEHSINJ+yGTw/EiJsRBVJEcQEg+sjflODiBymGoyPCNt3331nIhzDl/TA+awJxiQ8G1wcz0c7hB1DE9yll16a5fcRQgghhBBZR56IkFGPxYs0EQoML4j4YOBA6ltqL/FpgfsdJhBt27a1eqiTJ09atCwjMJ4333zT/fjjj+6mm25yXbp0Mev6IJhZ4MpXvnx5uz/CAoFDDVlWvYBjZoLtPPOj/g6jE6zr4YorrjBTCVwKqYGKxRoeQXnrrbdaKiki9JZbbgmr++PZIDBxx2R+uE3i8oigAlJFEdK0BSDylVbdHlEqaq2o30PkcS6CPDUQoUTSsN/nvhiMEJlML0T6ELI4U/JdREgLIYQQQggRCxckJSUlxXSkECJHQFSVCBwGH4qQCSGEEELk7ve1PBEhE0IIIYQQQojciARZHoEG0kHb/+CH9L2U9qVWC5YSpA+mdr200gvjgVqvlO7HvswEd8uU7uVTKoUQQgghhMgMlLKYS3nggQfct99+69566y37nZq0zz//POqx7EvJ5h3jEmra7r777pjvfe7cOTPbSAkMOairykwwGCH0Gw3CwBhrZBaYexw7dizqPtwjMTnJTpSyKIQQQgiRs0nP+1qecFnMj4wbN84FtTSCC1v88wFi63zcix5lt99+u1nmI7gyU3SlBpb4fHI61ycucQUKF83uYQiRpzg4snl2D0EIIUQ+QymLGeCnn37KtnujuONpoJyX1iC3jT03r5UQQgghhMgaJMjSQePGjc3yHUv9kiVLuoSEBLdjxw7rYUV9EZbwHTp0cCdOnAg7h75WnEO/LY6hZxqW8p07d7ZIDNGmRYsWhc7Bvh+be/qdEfmiNxcRsciUxWCaIffp3bt3qA9bmTJlrM9WECzdsYSnTxZ28suWLUs2R5pW03sMscd1sHAPpif6+2LVj8W77xsWL/QM69evnytXrpwrXLiwrcUrr7xi9yQ6BqwbfdC4d1osWLDA2h6wbvRAw3Lf2/dHG/uAAQOspUEk9GKjB1wsUTzaF9CAnDW7+eabrYcbsP61atWyXng8S9YdSDWlpQLfBbZdf/311jdNCCGEEELkP5SymE6mT5/uunfvbn3PeLGmjxa9xJ599lmr1UJcIGhWrlwZdg5CacOGDW7u3Ll2PnVb99xzjwkCzkXIYYZBDzIaJNNgeP78+SYqPvzwQ/fQQw+5smXL2rVTG1ufPn2sofLatWtNgCAQmjZtatekpxkigP3ksyISg5w9e9ZEZoMGDcwkhNTEYcOGuWbNmrnt27dbry9YsWKF5cJGE3TppWPHjjZW35D6s88+M0GLQFu4cKG799573Z49e+x+KdXBeY4ePWrNwUePHm1rSy0Y8wimdkYb+4gRI9yBAwfc1Vdfbb9/8sknNl/un1YtHQKPnm30UCMCxjNGPHpoUM516LmGuQrPwTehnjVrlt1z586dqTYvR7Ty8aRUSyeEEEIIIXIfEmTpBBMMXvgBsVK7dm03fPjw0P6pU6eamNi7d6+rXLmybUNoDBo0yH6mefHIkSMtwsaLPAwePNgaMyMC6tevb8YRNGL2EF1BtMybNy9VQUZT5MTExNA4x48fbwIEQbZ8+XK3e/dut2TJEosOAeNGHHgQiwgGIjpeVEybNs0iP0SCaLoNRIM4xgu0eGGNmBPiiEgWVKxYMbSfCB1QOxZLeiaCDJGE8PTGG0TLgkQbO8/ntddec08++WTIZZGoWVp1cggjhC0Npb2Yo8l1EETajBkzrLk1LF261ETbrl27Qt+P4JyjgWAMfh+EEEIIIUTeQSmL6aRu3bqhn7dt2+ZWrVoVZotepUoV20fEJSiUPERCiHoFhQJRK+8k6JkwYYLdixd5rjt58uQ07eSD9wEiav6aCACEohdjQCQsCPMhokMapZ8Pouj06dNh82HsGRVj8NFHH9l6NGrUyGUGCKs777zTxte6dWtLDcUQJEi0sbdv394EGRBNI9rFtrRgbYhCElVs2bKlpZUiCoMgDL0Y83Mm+unFWCwg4hF+/kNaqRBCCCGEyBtIkKUTIiye77//3l7EeckOfnytloeIVxCiT8FtPhpFdArmzJnj+vbta3VkRFS4JvVmaZlCRLuPv2YsMB9EYOR8iGTdf//9UdcgI6SVgpheEHdE26jHo0buhRdesDox0iBTGztpjqRFbtmyxdJDETxt27aN6Z5EEIleNmzY0CKMCK1169aleL945kxtHWmWwY8QQgghhMgbKGUxA9SpU8fqgzK77xb1abzg9+jRI7QtGKGKB1LpEBpEcIicQVA4+PkgKkgRPB8v/USrEIwffPBBKGUxiI9kYXISK4hQ6ub4kApKhIp6PWrrUoKIFVE6UhWpAyTFMz0W+6St8iGSRdSRaBuppylFMY8cORKW0hovO4YmSJwJIYQQQuRyFCHLAD179nRff/21RVg2btxoookaLaJZ6RERkVD/tWnTJrsWL+7UNnH9jIDgQQB06tTJUhMxuxg4cGDYMaTpUduGsyL7iSxRO4Z7IyIis0HIMp4HH3zQGlz7+1FXBogpBBYOhF999ZVF8FIDsxLq4lg70jsx0uC8yLquaDB3IpMYqcSSrgiMFxFGhAxnRaKZREdTux/Cj+gpZiVE87gGEb3FixfHdE8hhBBCCJG3kCDLANRjEc1CfGF4QcQH50IMKAoUiH9psUTHmIK0OcwlTp48GRYtiwfGQ6SICBA27ThDYv8eBIfH1atXu/Lly9v9ERakTVJDllWRGMxMWrVqZfOj/g6jE29Tf8UVV5iZxRNPPGF1drQcSA3GyPjvuusuE58YqYwdOzbMuCQlGAPrfOrUqbB2AqnBemGUgrjifjhhItJ5fqlBVPXGG280IU9qJQ6cGRHwQgghhBAi93JBUtATXAiR48HdkcbgGHwoZVEIIYQQIne/rylCJoQQQgghhBDZhASZiAvs3knto9YsaPsf/OB6mNI+PkCNGPVjsUBdWGrXS6stQHpp3LixGYukdD/mnln3iWzSLYQQQggh8gdyWRRxQc8tsl2xZMcaPxrUq2WmtT01eyndy+/PbLD79029I6HGTQghhBBCiIwgQZaLoS9ZZjRojgdyYj2VKlU6L/ektUDkvbJ6DTBoOV/zSy/XJy5xBQoXze5hCJEnODiyeXYPQQghRD5FKYu5CFLbcBokvQ17+oSEBLdjxw5zESSFDifCDh06uBMnToSd06tXLzunePHidszLL79sTobY819yySUmOLBe9+D4h7vib37zG4tw0VyZiFi0lMXgfbDHxzHwsssuc2XKlHFDhgwJO8c3zC5SpIi5C2L7Hgm90tq0aWNCiOtgwX/w4MFk98UhkogYY8sIEydOtDYDjIm1wW0xJb755hvXsWNHW0ccFll35uTBpRHnRCJn7Md18/XXXw+7BuvONXhe9IPDBVIIIYQQQuRfJMhyGdOnT7eIEHb7I0eOdHfccYc1Jab3Fr2sjh07ZoIm8hwE3IYNG0ycde/e3bVu3dqaT2/ZssUs+xFyWL4DzZpplkxPrp07d1qD5QEDBoT6g6U2tmLFilk/sNGjR7unnnoqJLq4Jlb6jJ39L774ouvXr1/Y+WfPnjWRiUikPos5IlyaNWtmkTDPihUr3J49e+za9CiLF9YMEck4uR7rh2BMCcQg57zzzjvWe4yUTSz2GTfQHqBu3bruX//6lwllbPBZV9bd8/jjj1sj7Lffftv6ltF3jWcghBBCCCHyJ7K9z0UQhcJC07/ADxs2zIQLDaQ9NHAuV66cCQx6Y3EOES9vQMHPpBsijmbMmGHbvvzyS4vWIDLq168f9d5E5jhuwYIFIXHy7bffhgw5Iu8D9DtDMCIcER/Nmze3Bsq+1gsBRJSJ/mhEvWbNmmVz2rVrl5l9AEKMaBn3QThyX87DwCOjqYo0jiZKyJohAqOtd61atdxzzz1nkTDWE5GIkPURMdYaIYrAjUaLFi2sv9rTTz9tja1LlChh8/TH01gc8Yt44z7ROHPmjH08fAe4b7m/zFPKohCZhFIWhRBCZJftvWrIchlEYDzbtm1zq1atCjkWBjlw4IAJCKhRo0ZoO86HiALS6Tyk6sHx48dD2yZMmOCmTp1qwgdzDoQR4iQ1gvcBRJ6/JiILERE03mjQoEHY8cxn//79ycQRkSfm42HsmVE31rRpU1ehQgVXsWJFi8LxueeeeyzdMBLGTw0bjbo9rCMpk+wDBOnw4cMtkvj555/bmiGk/PWYA9uC1yAtM620yxEjRliDbCGEEEIIkfeQIMtlkBLoIeLSsmVLN2rUqGTHIYY8hQoVCttH9Cm4zUejSCuEOXPmuL59+1p9E6IJgTRmzBhLNUyNaPfx14wF5oPgnD17drJ9l19+edQ1yAjMi2gjaYNE8EjNpO5t48aNFpVLL6wRtXZEuhCNjJPavWC6ZTz079/f9enTJ1mETAghhBBC5H4kyHIxderUcQsXLnRXXXWVRW8yC5+W16NHj9C2YIQqHqpWrWqGHUePHg2JxXXr1iWbz9y5c12pUqXSDO1mFqxbkyZN7JOYmGhCbOXKlZbSGTn+c+fOmSgNpiySGopBiV83TEj+9Kc/2e+I0b1794b2X3311SZauUb58uVDRiEc06hRoxTHSGsBPkIIIYQQIu8hQZaL6dmzpzkm4uzn3Q1J+SPCNWXKFEtPjAdcB6kvozYNp8WZM2da1Iif4wXBQwplp06dLJJElGfgwIFhx7Rv3972IWow2qC2ipozar2YH79nJhiCfPrpp2bkgXPie++9ZyIqWgoha8K4unbt6l566SWLrj3xxBPmqMh2fww1dh9++KFd75lnnjGTFS/ISC3FvRJjD9IdEZ6sQYEC8Xnr7BiacN6EqxBCCCGEyBrkspiLoR6LqAy1SxhekCZHihxRnnhf8qFbt24WIWrbtq3VOxEJCkbL4oHxYN5BPRpmH126dDHr+iDUWq1evdqiR9yfqBQChhqyrBAerBNiD+MR7oXzIzb11apVi3r8tGnTLKUSow5SOfHDQcT5VE0aSBPlwykSQxCs/4OtAQDBeeutt1qqKSL1lltuCasLFEIIIYQQ+Qu5LAqRh117hBBCCCFEzn5fU4RMCCGEEEIIIbIJ1ZCJXA19z+hlFg3SI3/xi1+k6uoohBBCCCFEdiJBJtJFZEPoeMES3zeEzgg33HCD++ijj+ISZJHgVkkNHp/MHGNanK/7CCGEEEKInIcEmUgX9NnKSWWHCK5KlSpl9zCEEEIIIYSICwmyXAiNhi+66KJsuTfFifl9DXIK1ycucQUKF83uYQiRKzk4snl2D0EIIYQwZOqRC8BC/ZFHHrFUupIlS5qt+o4dO6x2it5WpUuXdh06dHAnTpwIO6dXr152Dj2xOIaeZT/88IPr3Lmz9dEisrRo0aLQOdjnYzNPvzEiT/TjIiIWmbIYTK3jPr179w71QcPqfciQIWHn7Nu3z3p9FSlSxHpyLVu2LNkcaRrdpk0bs6LnOvT2OnjwYLL7YpWP3X+0XmHp4fjx42Y9zzyZ7+zZs9M85+OPPzaLfM6hj9hDDz0UVodGr7amTZvaM0K40ux5y5Yt6V4LIYQQQgiRf5AgyyVMnz7dIkL0HRs5cqQJg9q1a7tNmza5xYsXWwNiBE3kOYiDDRs2mDjr3r27a926tWvYsKEJBXqXIeROnTplx9MUmebL8+fPdzt37nSDBw92AwYMcPPmzUtzbMWKFXPr1693o0ePtqbOXmhwTXqKMXb20+urX79+YeefPXvWRCYiEZMO5ojQbNasmUXCPCtWrHB79uyxa9PUOSMg8BCBq1atsmbOEydONJGWEghZxoi4RXixRsuXLzeh7Pnuu++s8fWaNWvcunXrrFH0XXfdZdtjXYtonDlzxqxTgx8hhBBCCJE3UMpiLoGXe8QODBs2zMTY8OHDQ/unTp3qypUr5/bu3esqV65s22rWrGnNiqF///4m5BBoXbt2tW0IrkmTJrnt27e7+vXrW4PjoUOHhq5J5Gjt2rUmyCLFXpAaNWq4xMTE0DjHjx9v4oloEaJl9+7dbsmSJRbZAsYddEacO3euiZUpU6aYwYVvwky07P333zfhCIg+jsloqiJrRGQQoXrjjTfatldeecWaQ6fEa6+9Zg2qZ8yYYeMA5kmUbdSoURaBRCQHmTx5ss3hgw8+sGbSsaxFNEaMGBH2XIQQQgghRN5BEbJcQt26dUM/b9u2zSI7RJH8p0qVKrbvwIEDYULJU7BgQUuzq169emgbIgKCkaEJEybYvS6//HK7LqLi8OHDqY4teB8oW7Zs6Jq7du0yoegFCDRo0CDseOazf/9+i5D5+ZC2iAAKzoexZ0bdGGO68MILw9aU9UM8pXYOAteLMbj55ptNSBK1A6KUiF1EKSmLNAEkpdGvXyxrEQ3ENE0F/YfInhBCCCGEyBsoQpZLCAoBXvJ9ZCYSxJCHiFcQok/BbT4ahaiAOXPmuL59+7qxY8eaUEAgjRkzxtLrUiPaffw1Y4H5II6i1XEhDKOtQU6EdMWTJ09a3V2FChVc4cKFbR2DaZfxwHX4CCGEEEKIvIcEWS6kTp06buHChdY3i0hPZkHtFvVlPXr0CG0LRqjigTRAIjpHjx4NiUXqqyLnQ9piqVKlLKqU1RANO3funNu8eXMoZZEoF/3VUpvHq6++arVkXhiyXgUKFAgZjPA7tWjUjQHzDhqtxLIWQgghhBAifyFBlgvp2bOnOSa2a9cu5G5Iyh8RLmqsSE+MB1LtqJGixon6sZkzZ5qBBT/HS5MmTaymjegR0TYMKQYOHBh2TPv27W0fzooYgmAscujQIffGG2/Y/Pg9M0FAYRjSrVs3q6FD1OJGmVoTacZInRzzwEXyq6++MqMUTFF86ifrx5rRrJp5Pv7442HXjGUt0sOOoQnnRcAKIYQQQoisQzVkuRBqkIjGYFOP4QW1VQgKaqCI2MQLAgUXwLZt27p69epZ+l0wWhYPjOfNN990P/74o7vppptcly5dzLo+SNGiRd3q1atd+fLl7f5EkrDfp4YsqwQHpiGsI9b03BMLeyJ0KcEYEapff/21RdVatWrl7rzzTjP28GAM8s0331jED6FGO4DgNWNZCyGEEEIIkb+4ICkpKSm7ByGEiB0ia5iGYPChCJkQQgghRO5+X1OETAghhBBCCCGyCdWQiVwJDaRT6t9FSmBq9WC4OgohhBBCCJETkCATIR544AFzGnzrrbcydB1s76mVuvvuu11WgXHGRx99FLMgo1aM+XXu3DnLxiSEEEIIIUR6kSATIeiflVtKChFclSpVivl4eqXR0yw95wghhBBCCJHVSJDlMGgifNFFF2XLvSk8zO9rkJu4PnGJK1C4aHYPQwjj4Mjm2T0EIYQQIlciU49spnHjxu6RRx4x2/qSJUu6hIQEt2PHDquPuvjii63HFRbqwQbDnEMPLM4pXry4HUNfMpoWk5J3ySWXWCRo0aJFoXOwyMdKnp5iRJfoxUVELAgpfcE0Q+6DdbvvdVamTBnrwRVk37597rbbbnNFihRx1113nVu2bFmyOdIMuU2bNmbLz3XoN3bw4MFk98UCHit632g5Xo4fP+5atmxp82S+s2fPjppW+dJLL7kWLVqYpT1W+2vXrrV+bsyb5s80yY5sjP3222+brT3zrVixohs6dKg1mfY888wz1oaA88uVK2dtA4I1azSXZh2w0OeePGN6otEsWgghhBBC5D8kyHIA06dPt4gQvcVGjhzp7rjjDle7dm23adMmt3jxYnfs2DETNJHnIOA2bNhg4qx79+6udevWJiK2bNli/ckQcqdOnbLjf/75Z2uwPH/+fLdz5043ePBgN2DAADdv3rw0x4a4WL9+vRs9erQ1bvaii2vSw4uxs//FF190/fr1Czv/7NmzJjIRiRhxMEcvQoiEeVasWOH27Nlj13733XcztJ4IPETgqlWr3IIFC9zEiRNNpEXy97//3XXs2NFq0apUqeLuv/9+68XWv39/W3vSNxHLHsbP8Y8++qitIYIOgRXsJUavseeff9598skntnYrV640QRuEZ/L0009bE2n6rx0+fNj17ds3xfmcOXPGrFODHyGEEEIIkTdQH7JshmgML9iIKBg2bJi9+BNB8Rw5csSiLQiWypUr2zlEvDgO+Jl0Q8TRjBkzbNuXX37pypYta1Gf+vXrR703YoPjEC3RTD0i7wM0NEYwIhyXLl3qmjdv7g4dOmSRLUBAEt3zph6zZs2yOe3atcuiUoAQI0rEfRCO3JfzECYZTVXcu3evRdgQqjRwht27d1s06tlnn7WoIjCWQYMGmSiDdevWuQYNGlhz5wcffNC2zZkzxyKOmIRAkyZNrBk0gs3D/BBcX3zxRdTxsLYPP/xwKMKJgOOaROKuvvpq24ZgROjyLKJBVJJIXCTl/jJPKYsix6CURSGEECK+PmSqIcsB1K1bN/Tztm3bLLJDFCkS0ucQZFCjRo3Q9oIFC7oSJUpYqpyHNEYIRoYmTJjgpk6dasIHkYEwqlWrVqpjC94HEHn+mogshKIXY4CoCcJ8EB9EyIKcPn06LB2QsWdG3RhjuvDCC8PWlOgXAjC1ufn1ilxDxskfFH9IzIUIXzAihmDlGKJepD4uX77cjRgxwkQg55HOGNwP/K8XY5FrGg0EYJ8+fUK/c13WXQghhBBC5H4kyHIApAR6qDei/mnUqFHJjuPFPegaGISIT3Cbj0aRVuijPaTFjR071kQTAmnMmDGWapga0e7jrxkLzAdxFK2OC9fDaGtwvoi2XqmtIXMhUkUkMhJqyqiLoyaN9FFEG/Vya9assdo9xK8XZNHWNLVAdeHChe0jhBBCCCHyHhJkOQwMIxYuXOiuuuoqi/RkFkR2qC/DZMITaViRXkgDpFYLQwovFkn9i5zP3LlzXalSpdIM12YGRMOISm3evDmUskiqJ6mYGYW5cK2UrPO5J+IN0UstGaRVoyeEEEIIIfI3EmQ5jJ49e5pjYrt27ULuhqT8EeGaMmWKpSfGwzXXXGP1ZdSm4TyIocTGjRvt53ihpooUyk6dOlm0jVS6gQMHhh3Tvn1724ezInVSGItQc/bGG2/Y/Pg9M6F+DMMQzDkmTZpkopa6schG0fGAEQoRsPLly7tWrVqZ6CKNEVdM6uQQapiYvPDCCxblRARjdJJV7BiacF5ErhBCCCGEyDrkspjDoB6LF3lqkzC8oKYJQUENlI+6xAMChVS7tm3bunr16rmTJ0+GRcvigfFg3kE9GmYfXbp0CauvAtL0cBJExHB/omqk8FFXlVViYtq0abaOjRo1sns+9NBDFqHLKLhF4gCJmQnRN8xSMAqpUKGC7a9Zs6bZ3pNuev3111uaJvVkQgghhBBCpIRcFoXIw649QgghhBAiZ7+vKUImhBBCCCGEENmEashEjoO+Z/QyiwbpkanVg+GEKIQQQgghRG5BgiyfQoNiatMyw30ws7nhhhvcRx99FJcgy2lENtsWQgghhBAiiASZyHEguFKyls/tYjMzuT5xiStQ+P/1NhPifHBwZPPsHoIQQgiR51ANmYgbLN7PJzhPpqcptRBCCCGEEDkdCbLzACIC+3N6fhH9wR59wYIFtu/99993F1xwgVuxYoWl6mETTwNnGhAH+ec//2lW60WKFHElS5Z099xzT2jfN9984zp27OiKFy9u51N/tW/fvmRRI6zn2c+52N5H8vbbb1vzY+5RsWJFN3ToUGuy7GGc9Pb6/e9/74oVK5bM4j49+Hn/61//cjVq1LB7YiNPT6/gmLH7f+edd9x1113nChcu7A4fPpzqfLlu586dzdGG6/MZMmRIzOuUElyjVq1aYduee+45a+AdFIx9+vSxMZcoUcL6rEWamDZu3Ng98sgj9sF5h2f55JNPJjtOCCGEEELkDyTIzgOIMZoy0yT4k08+cX/961/dn/70J/fBBx+EjqGh8tixY92mTZusmfGDDz4Y2odoQUTdddddbuvWrSbe6PsVrFPiPITL2rVr7eWeY30Ea/369db7CxFAbdbtt99ujYwjjTQQK48++qjbuXOne+mll0wQRYouhAlj+fjjj8PGGC+PP/64zZsm1Zdffrk1VA5G3k6dOmV9vWiKzdrRTyy1+SJmEUrYix49etQ+ffv2jWmdMgrzYM2mTp3q1qxZ477++mvr0xbJ9OnT7Rlv2LDBjRs3znqXMb+UOHPmjFmnBj9CCCGEECJvoD5kWQwv05dddplbvny5a9CgQWg7TZQRGzQtRiCx/84777R97733nmvevLkZWBA5QmQQsZo1a1ay6xPhqVy5sjWT5jgg+lWuXDl78W/durW7//77LWKEsPPcd999bvHixaE6qyZNmtj9+/fvHzqG+xHl+eKLL+x3ok3UZtEMOaMQyWLec+bMsWbVgIC58sorTdS0adPG/pdoFyKSqGKs841WQxbLeamBEMWYI2g2gvDjc/DgQfudZtSIbUQmEF0kKlq3bt2QqQcRsuPHj5u4ZD3hiSeeMJGIEE7p3kQrIyn3l3mqIRPnFdWQCSGEELGhPmQ5iP3795vwatq0qbv44otDHyJmBw4cCB1H2p6nbNmy9r+8uAMiwIu1SHbt2mXRlnr16oW2kS537bXX2j5/THA/BMUhbNu2zT311FNhY+zatatFmBi/h7TKzCQ4DoRrcNxw0UUXha1NLPONRrznxQp/bKxV8PrcL9p6kZrpxZhfAwQjKY/RQCRzff/5z3/+k+HxCiGEEEKInIFcFrMY3xeL6NQVV1wRto+aKC/KChUqFNruX9a9gcX5sHlnnERh/vjHPybbR5TOQ+3Y+YS5B8VLdlGgQIFkdV7ny9SE7wkfIYQQQgiR95Agy2KCZhSNGjVKtj8YJUsJIkTUjZG+F0nVqlUtNY46sWAqHqYg3Nsfw/4g69atC/sdMw/OyYjdfDwwDsxGvOnG3r17bbwpEct8iapFRptiOS81qG/78ssvTZR5gRhMXyQkTWST69922222jftt3rzZ1jZItGdxzTXXuIIFC7r0sGNoQpohcCGEEEIIkbORIMtiLrnkEjOVoLaIiNctt9xiaWfUMvEyXaFChTSvkZiYaCmLV199tdV+8aJPnVm/fv3sRf4Pf/iDpRdixMH9qEkiGsd26N27t7v55pvd008/bduWLFli9WNBBg8e7Fq0aGHiqFWrVhYRIo0R18NIA5DMhDRJUgdLly5txia4Dt59990pHh/LfHE+JOKHiKX2DEfFWM5LDWq/vvrqKzd69GhbH9Zv0aJFYYIIQ5SRI0favapUqWJmHdF6oSHOcWPs1q2b27Jli3vhhRfMEEQIIYQQQuRDMPUQWcvPP/+c9NxzzyVde+21SYUKFUq6/PLLkxISEpI++OCDpFWrVpEHl/TNN9+Ejt+6datt++yzz0LbFi5cmFSrVq2kiy66KKlkyZJJf/zjH0P7vv7666QOHTok/fKXv0z6xS9+Ydfeu3dv2BheeeWVpCuvvNL2t2zZMunpp5+244MsXrw4qWHDhnbMpZdemnTTTTclTZ48ObSfMb355puZsiZ+3v/85z+TqlWrZvPiftu2bQsdM23atGRjjHW+Dz/8cFKJEiXsHomJiTGflxqTJk1KKleuXFKxYsWSOnbsmPSPf/wjqUKFCqH9Z8+eTXr00Udt7X71q18l9enTx477wx/+EDqmUaNGST169LDxcVzx4sWTBgwYYN+RWPnvf/9r8+J/hRBCCCFEziM972tyWRTZgndZJE2Rvl35BSJt9DPDnfF8uPYIIYQQQojzj1wWhRBCCCGEECIXIEEm0gXNlX2N18MPPxxmkx/8pLWP6FhO4ne/+12K4x0+fHh2D08IIYQQQuRRlLIo0gVhV74ypBnSJ41wbDQIzaa2DxOPN998M1UDj6wE4w+aR/OBzz//3BpxR4P+aHzihcbRNIjeunWrpSsGxS2mH75pdKwoZVEIIYQQImeTnvc1uSzmQn766Sezds8O+GJ5SpUqZZ+USG1fTluDyB5xmTnOrOL6xCWuQOGiWXZ9ISI5OLJ5dg9BCCGEyHMoZTGXGEE88sgjFs3BFj4hIcHs6H2aHdGmDh06uBMnToSd06tXLzunePHidszLL7/sfvjhB+tnhu07PcewbvfQu+vPf/6zRXNoyHzttde6cePGpZiy6O+Drf7f/vY3iyKVKVPGDRkyJOycffv2WW8uGkzT82vZsmXJ5vif//zHtWnTxiJvXAcreiJLkff9xz/+4X7961/b2DKynocOHbJWBPQUCzaeph0B+7HKZ91Ya4xHgs+BD8KUZ/Hkk0+GNYwm8vb3v//ddezY0f415KGHHrL1hNq1a9u9uA5rNH36dPf222+HxoDRiRBCCCGEyF9IkOUSeHknIoRgoNfVHXfcYS/4mzZtsp5Yx44dM0ETeQ6iYcOGDSbOunfv7lq3bm2Nkel/9dvf/taE3KlTp+x4+qRdeeWVbv78+W7nzp3Wm2zAgAFu3rx5aY6tWLFi1vCYPl30FvOii2v+8Y9/tLGz/8UXX7T+aUHOnj1rwgeR+O9//9vmiNBs1qxZWISJvmI0cuba7777btxr+cYbb9g8GefRo0ft4xs90+8N0bh27Vq3Zs0a17Jly7Am08z1wgsvtDVFrNJrbMqUKWHXp98b/c9IUUSwcSwsX77c7sX96U3H82KOfgy+YbUQQgghhMg/KGUxl0CzYcQO0KgZMRY0m5g6daorV66c27t3r6tcubJtQxQMGjTIfu7fv78JOQQazZEBwTVp0iS3fft2V79+fVeoUCE3dOjQ0DWJ7CBMEGSRYi9IjRo1rHm1H+f48eNNPDVt2tREyO7du60ZNZEtYNxE9zxz58414Yaw8dGqadOmWbSMqBHCERB9HJPRVEUicAULFjQBSETPw/recMMNbuLEiaFt1apVCzuXNX722WdtnETpPv74Y/vdrykglh977LHQ79wLaIAdvB9RyDNnzoRtiwbH8PGkVJsnhBBCCCFyH4qQ5RLq1q0b+nnbtm1u1apVYU6AVapUsX0HDhwIE0pBUYAgqF69emgbaYyAOYdnwoQJdq/LL7/crjt58mR3+PDhVMcWvA+ULVs2dM1du3aZiPFiDBo0aBB2PPPZv3+/CSQ/H0TT6dOnw+bD2LOyds5HyFID4RpMcWQupGQGo2iIusxkxIgRliLpP6ynEEIIIYTIGyhClksgOuT5/vvvLZVu1KhRyY5DDHmIeAVBSAS3eWFBdArmzJljqXRjx441oYFAGjNmjKUapka0+/hrxgLzQQTOnj072T6EYbQ1yAqIWGUGmT1Oopt9+vQJi5BJlAkhhBBC5A0kyHIhderUcQsXLjQDCeqZMgtqt6hj6tGjR2hbMEIVD1WrVjXDDmqkvFhct25dsvmQtogr4/mycSfSFoxq+UgfqZbBtM1IIsUpcyFN06clpnQviLxftDFEo3DhwvYRQgghhBB5DwmyXEjPnj3NMbFdu3Yhd0NS/ohwUWOVmjhIDYTFjBkzrN6L+rGZM2e6jRs3hlwC46FJkyZW09apUyeLthHdGThwYNgx7du3t304K2K0geEGLoiYXzA/fs9sELOrV6929913n4kdauuIRJEWiSCleTWCidRQjFDYD6RvEq3q1q2bGaO88MILFlFMDYQm0TfMV5gLbpOkHjIG1hqjEtJJ2RYZbUyNHUMT1IdMCCGEECKXoxqyXAj1WESziK5geIGIwN4eE4wCBeJ/pIgMHBHbtm3r6tWr506ePBkWLYsHxkMDaJou33TTTa5Lly5mXR8Ei3nEUfny5e3+RNWw36eGLKsEB8IPW/2rr746lBaJcFy6dKnVtDFW0jaxpQ9GIbGz93NBGD/66KNmbZ8anP/888+7l156yZ4dwhMwAsEYhJozxsAzFUIIIYQQ+YsLkoJNlIQQKUL/sFq1arnnnnsu13R+F0IIIYQQOft9TREyIYQQQgghhMgmVEMmciU0kA72MgtCSmFqjom4OgohhBBCCJETkCATWcIDDzzgvv32W/fWW29l6DpY6FODdvfdd4dtp+6KvmHxCLJ4oUm1EEIIIYQQmYkEmcgSxo0b57KyPBHBValSJZdTGTJkiInRlESjEEIIIYQQIEGWh/npp59CPbDONxQx5sU1yM41jeT6xCWuQOGi2T0MkYc5OLJ5dg9BCCGEyPPI1COPuQA+8sgjZoFP36yEhAS3Y8cOq7W6+OKLXenSpV2HDh3ciRMnws7p1auXnVO8eHE7hh5nP/zwg+vcubO75JJLLBK1aNGi0DnY7WNLT38yIlVYtxMRi0xZDKYZcp/evXuH+qaVKVPGokhB9u3b52677Tbr03Xddde5ZcuWJZsjTabbtGljFv9cBwt57Osj74u1PhbzjC0j0Cvs73//u9nd45DjLe779etnNvlY9lesWNE9+eST7uzZs7bv1VdftebS2OeTcsmHbUAaJ9b/2NxzvTvuuMOOE0IIIYQQ+RMJsjzG9OnTLYJDT6uRI0faC3/t2rXdpk2brDHxsWPHTNBEnoOA27Bhg4mz7t27WzPkhg0bWvNjep0h5E6dOmXH//zzz9bgeP78+W7nzp1u8ODBbsCAAW7evHlpjq1YsWJu/fr1bvTo0dYLzIsurkkPMsbO/hdffNFETxAEDyITkYipB3NEaDZr1swiV54VK1ZYs2Wu/e6772Z4TZ9++mlXs2ZNt3XrVhNewBgQWcwfMYqIffbZZ20ffdwee+wxV61aNXf06FH7sA1Y1+PHj5vA3bx5s6tTp46788473ddff53i/c+cOWPWqcGPEEIIIYTIG6gPWR6CKBQv64goGDZsmAmXJUuWhI45cuSIK1eunAkWIjycQ8SL44CfSTdEHM2YMcO2ffnll65s2bJu7dq1rn79+lHvTWSO4xYsWBDV1CPyPkBzZQQjwpGGzM2bN3eHDh2yyBYgIInueVOPWbNm2Zx27dplUSdAiBEt4z4IR+7LeYcPH86U1EIiZAhaxpAaiLY5c+aY8E2phmzNmjU2RwRZ4cKFQ9uJQBI5TKnBNNci4hZJub/MU8qiyFKUsiiEEEJkfR8y1ZDlMerWrRv6mVS4VatWWRQpkgMHDpgggxo1aoS2FyxY0JUoUcJVr149tI00RkBIeCZMmOCmTp1qwgdXQ4QRTZNTI3gfQOT5ayKyEIpejEGDBg3Cjmc++/fvt+hUkNOnT9t8PIw9M+u8cHSMZO7cue7555+3+2Kjf+7cuTT/2Bg/x7K+QVi/4Pgj6d+/v+vTp0/YHzhrJYQQQgghcj8SZHkMUgI9vPy3bNnSjRo1KtlxiCFPoUKFwvYRfQpu89Eo0gqBSFDfvn3d2LFjTTQhkMaMGWOphqkR7T7+mrHAfBCcs2fPTraPmqxoa5AZRF6PSGH79u0takUKJf/6wZqwHmmNn3WPZp9PlC8liKYFI2pCCCGEECLvIEGWh6E+aeHChZZ2d+GFmfeoqd2ivqxHjx6hbalFeGKhatWqZthBvZUXi+vWrUs2HyJTpUqVSjMalZV8+OGHrkKFCm7gwIGhbaRaBiFCR4pm5PhJ6+RZ8EyEEEIIIYSQIMvD9OzZ08wm2rVrF3I3JOWPaM6UKVMsPTEerrnmGqsvozYNp8WZM2e6jRs32s/x0qRJE0uh7NSpk0XbSMsLCh4gKsU+nBUxBMFYBCH0xhtv2Pz4/XzA/EnVZB1vvPFG969//StZjRmC67PPPrMaMsZFFJE5ElGkHg5TE+b7xRdf2Pn33HNP1NTI1NgxNCFbhakQQgghhMg4clnMw1CPRTSLSA2GF9RWYW9PelyBAvE/+m7dupnpB86B9erVcydPngyLlsUD40HUUE+F2QfW8FjXB8FifvXq1a58+fJ2f6Jq2O9TQ3Y+hcnvf/9799e//tWMTKibI2Lm3Rc99957r7k/3n777ZZO+frrr1uK5nvvvWfW/rQUQJDdd999Jip9nZ4QQgghhMhfyGVRiDzs2iOEEEIIIXL2+5oiZEIIIYQQQgiRTaiGTORp6HtGL7NokB75i1/8IlVXRCGEEEIIIbISCTIRRmRD53ihXso3dM5OMMrwzZkbNWpk86N+y5tzYBLStGnTbB2jEEIIIYTIv0iQiTDGjRvn8lJZIRGwSpUqhfqgYbDhfwcs9oO/pxfcFDFK4SOEEEIIIUR6kSDLgfz000/Wxyo7oPgwv69BbuH6xCWuQOGi2T0MkUM5OLJ5dg9BCCGEEDEgU48cQOPGjc1CnShLyZIlXUJCgtuxY4fVPl188cVmid6hQwd34sSJsHN69epl5xQvXtyOoefYDz/8YCl59L0i8rNo0aLQOdjfYxNPvzAiR9dee61FxIKQ0hdMM+Q+vXv3DvUxK1OmjBsyZEjYOfv27TMr9yJFirjrrrvOLVu2LNkcafrcpk0bs9znOvQSO3jwYLL7YnWPXT9jywjHjx93LVu2tHky39mzZ0c9jkbUrDPHVaxY0S1YsCC074477rDnEuSrr74yobhixQpbGyzrscAnRZOPZ82aNe7WW2+165YrV87WkGfjmThxoqVMsmY8u1atWmVovkIIIYQQInciQZZDmD59ur3o0zds5MiRJgZq167tNm3a5BYvXuyOHTtmgibyHATchg0bTJx1797dtW7d2jVs2NBt2bLFeo8h5E6dOmXH//zzz9akeP78+W7nzp1u8ODBbsCAAW7evHlpjq1YsWJu/fr11tCYpsxedHFNeoIxdva/+OKLrl+/fmHnnz171kQmIhGTDeaI0KRPF5EwDyJnz549du133303Q+uJwEMErlq1ykQWAgiRFgn9w+gZtm3bNms8TV+wXbt22T56ob322mvuzJkzoeNnzZrlrrjiCns+NKRmPVkPhB0fOHDggM2N627fvt3NnTvXBJoXdzxTBBrnMV+eL4I2Jbg/1qnBjxBCCCGEyBuoD1kOgEgLL9mIKBg2bJgJlyVLloSOOXLkiEVaeIGnoTDnEPHiOOBn0g0RRzNmzLBtX375pdVIrV271tWvXz/qvREJHOcjQ5GmHpH3ARo3I0gQjkuXLnXNmze3SBGRLUBgEHXyph6IGOaE0PFRJIQY0TLug3Dkvpx3+PDhDKcq7t271yJsCNUbb7zRtu3evdsaST/77LOhei/G8vDDD7tJkyaFzmWd6tSpYwKOhtPMCZHpxXDNmjVtjRMTE1OsIUPIFSxY0L300kuhbQgyTEWIktEcmigmzxSRmhZEJIcOHZpse7m/zFPKokgRpSwKIYQQ2Yf6kOVC6tatG/qZaA2RHaJI/lOlSpVQ9MVTo0aN0M8IgBIlSrjq1auHtpEKB8HI0IQJE+xemFtw3cmTJ5sISo3gfQCR56+JyEIoejEGDRo0CDue+ezfv9/Eh58PaYsInuB8GHtm1I0xpgsvvDBsTVk/BGAkkWPldx8hI52QCOPUqVPtdwQzqaSIx9Rgvq+++mrY8yNCSDTxs88+M1fHChUqWIok1yed0kcxo9G/f3/7Y/YfIn9CCCGEECJvIFOPHAIpgcH+V9Q/jRo1KtlxiCEProFBiPgEt/loFEIA5syZ4/r27evGjh1rwgOBhO07qYapEe0+/pqxwHwQR9HquBCG0dYgp0C0q1atWhbNmjZtmkUGEVNpzbdbt26WlhhJ+fLlTXQi7t5//32LMJI6ShRs48aNUUVj4cKF7SOEEEIIIfIeEmQ5EFLmFi5caOlwRHoyC2q3qC/r0aNHaFswQhUPpAESsaF+yovFdevWJZsPdVSlSpVKM2SbGRANO3funNu8eXMoZZFUT1IxI2GsHTt2DPud2r1g1I5eZhimUE82fvz4sPMRV6R0Rs6XGr3U7PR5rk2aNLEP6Y8IsZUrV1o6pBBCCCGEyD9IkOVAevbsaQKgXbt2IXdDUv6IcE2ZMsXSE+MBVz/qy6hNw3lw5syZFpXh53hBUFDT1qlTJ4u2kS87cODAsGMwy2AfzooYWWCEQc0ZphjMj98zE+rHMNUgSkV9GOKHGi8cDyPB4ATBdcstt1gEj7qzV155JVmUjFo7Inj33HNP2D5E8+rVq80MhCgWJiuYmlCLxjmcy3kINMxKEHQYlnz66adm5IFDJjVlRBzT6yy5Y2jCeRG4QgghhBAi61ANWQ6EeiyiWUReMLwgSoOgIIpSoED8jwyBQgSmbdu2rl69eu7kyZNh0bJ4YDyYd/z4449m9oEAwbo+SNGiRU20kK7H/YmqYb9PDVlWCQrSC1lHjDS450MPPWQRukgwy0DoUieHWH399dfNuj8IwhhRx/9SVxYEgYl9/9VXXx1Kv+RaH3zwgZmLYH1PxI20RF9nx3NEjJL+yFpgGsJ9q1WrliVrIYQQQgghci5yWRQiDbzgIppIOmJucu0RQgghhBA5+31NKYtCpAD904giDho0KGSHL4QQQgghRGYiQSZyJPQ9o5dZNEiPjFYPFnQ5zAxIG7399tutRs73aRNCCCGEECIzkSDLRUQ2bY4XbOt90+acCkYbH330UVyCzDe0xq7+ueeei3sMXCNaRi929Qi1b775JqpNvRBCCCGEELEiQZaLGDduXFSBkJcIip3UbOMzk4yKN5pAY7oSzVY/K7k+cYkrULjoeb2nyNkcHNk8u4cghBBCiHQiQZZOfvrpJ+s9lR1QGJjf10AIIYQQQoi8hGzvY4ie0E+KCAg9phISEtyOHTusvuniiy92pUuXdh06dHAnTpwIO6dXr152Dn2mOIa+Yj/88IPr3Lmzu+SSSyz6s2jRotA5WNxjBU9PMNLx6ElFRCwyZTGYZsh9evfuHepVVqZMGTdkyJCwc/bt22f9rrBrx86dXliR0Ni5TZs2ln7HdegXhrNg5H2xs8e6Pb39siI5c+aM9eoqV66c9e5iLej9xT2JjgHrRmol904L1pXmzjwPmlOPHTs22TETJ060PmysA8+jVatWoblhUc9acz8+wbnHEtHjmeKg48/3z4AeZcOGDQuNrUKFCu6dd95xX331la0x27DI37RpUzpWTwghhBBC5CUkyGJg+vTpFhHC5GHkyJHWP4reUrxIL1682B07dswETeQ5CDgaDSPOunfv7lq3bu0aNmzotmzZYv3FEHKnTp2y42kMTINkGhXTRJi+VQMGDHDz5s1Lc2w0Hl6/fr0bPXq09cXyootr0oOLsbOfflcIoUgnQUQmIhEjDeaIUKCxMpEwz4oVK9yePXvs2jQ2zggIFPpuPf/8827Xrl3upZdesnsi0BYuXGjHcK+jR48mE6XRePzxx01Uvf32227p0qUmklhjD88J4cracF2eGSIVuH6DBg1c165d7X58GEes8DxJdcTO1J/ft2/f0P5nn33W3XzzzW7r1q2uefPm9syZ/5/+9CcbI3b6/J5aKioCFuvU4EcIIYQQQuQNlLIYA0RWEDtAxAMxNnz48ND+qVOn2ks8jYBx5IOaNWuaXTr079/fhBwCjRd/QHBNmjTJbd++3SzVCxUqZE2KPUTK1q5da4IsUuwFIcKSmJgYGuf48eNNPDVt2tQtX77c7d692y1ZsiTUlJhxB90L586da8JtypQpFt3xTZWJliFsEI6A6OOYjKYqskbMCWHXpEkT21axYsXQfiJ0QBPnWAwzcFQkujZr1ix35513hkQq4tZz+PBhG3+LFi1MeBKp4hn6NFDmRPNqIozphXO5BmsX7fy77rrLGnIHn/mNN95o4hwQyAhCRH1K9x8xYkTYd0MIIYQQQuQdFCGLgbp164Z+3rZtm1u1apVFdPynSpUqtu/AgQNhQslTsGBBV6JECVe9evXQNtLm4Pjx46FtEyZMsHtdfvnldt3JkyebmEiN4H2AlD1/TaJPCEUvxoCX/yDMZ//+/SZU/HwQRadPnw6bD2PPjLoxnBNZj0aNGrnMgDESyatXr15oG+MPplUiThFhCD8iVLNnzw5FJrOa4PPxzzyt70EkCHpSIv2HFFMhhBBCCJE3UIQsBoiuBCMyLVu2dKNGjUp2HGLIQ8QrCBGU4DYfjSI6BXPmzLFUN+qfEE0IpDFjxliqYWpEu4+/ZiwwH0QgIiUShGG0NcgIadnVZwWsJemBRPxIaSRSRZ3Xxo0bs9y2PtozT+17EA3q7PgIIYQQQoi8hwRZOqlTp47VOWHYcOGFmbd81G5Rj9SjR4/QtmCEKh6qVq1q0RTqmrxYXLduXbL5kLZIiiB1UFkN0SHEBzVfPmUxiI/CYXISC9RgIXAQruXLl7dtWOaTGhmMwvGsuB8fUjwRYitXrgzV2MV6v2hk9Px42TE04bw8MyGEEEIIkXUoZTGd9OzZ03399deuXbt2FmFBNFGjhdNeRl7Kqf/CfIJrISaefPJJu35GQHxQ09apUydLTcS0Y+DAgWHHtG/f3mrbcP1j/2effWaRJEwwjhw54jIbhCzjefDBB63Btb+fNy8htZCoEcYhuBESwUsNUixxp8TYA4GFAybOiQUK/P9fba6FgQjpkocOHXIzZswwUejTGhkTgg53Rdwy0xNh9OczTmr3OP98pUMKIYQQQojcjwRZOqEei2gW4gvDCyI+2NsTcQmKgPSC8QPRmrZt21o91MmTJ8OiZfHAeN588033448/uptuusl16dLFrOuDYGaxevVqiy5xf6JqCBxqyLIq+oKxBbbzzI/6O4xOsK6HK664wgwsnnjiCauvouVAWpDaeeutt1oqKSL0lltuCav749m88cYb5o7J/HCbxOWxWrVqtp9UUeraaAtAmmZadXuRENl8+OGH7dlxvjeAEUIIIYQQIi0uSErNb1sIkePA9h5nRww+lLIohBBCCJG739cUIRNCCCGEEEKIbEKCTKQbas2Ctv/BD6l/Ke3DfOPuu+9O171IH4y8DjVmuDXyc3rTC2OB9MOU5sC+lOrIaBAthBBCCCFEepDLokg3N9xwgxlkRIN6tZSs7b/77jtreJ3emr3Ie2GA8swzz1h/sWCPtcziqaeesrqyaChFUAghhBBCZCYSZPkMmihntMEzgqtSpUrufIBdfbR7YeMf7xjSWgNaAPA5n1DKiVFMelopXJ+4xBUoXDRLxyVyNgdHNs/uIQghhBAigyhlMY/TuHFjcyrECRJ7+4SEBLOG/93vfmcpeDgZdujQwezag+f06tXLzilevLgd8/LLL5sTIvb+NFpGDC1atCh0DmICd0YiYAg2LOXHjRsXNhbs6IMpi9wHe/2//e1v7rLLLnNlypSxhs1B9u3b52677TZXpEgRc0FctmxZsjnSa61Nmzbmpsh1sPDHwj7yvjhMElHzdvfxcvz4cXN0ZJ7MN7KpNvcmrTIY2fv2229tGxb/wP/yO2uIIySNn9esWZOhcQkhhBBCiNyHBFk+YPr06RYRwq5/5MiRZv9eu3Zt63u2ePFid+zYMRM0kecg4DZs2GDirHv37q5169Zm8b5lyxaz/EfI+Z5b9O668sor3fz5893OnTvd4MGD3YABA0L9xVIbW7FixawPGHbxpAt60cU1feNm9mNX369fv7Dzz549ayITkUhtG3NEaDZr1swiYR56hO3Zs8euTV+yjIDAQwSuWrXKLViwwE2cONFEWjxg788z2bVrl6tRo0aGxiWEEEIIIXIfSlnMB1Bz5XtjDRs2zMTY8OHDQ/unTp3qypUrZw2paSQNNWvWdIMGDbKf+/fvb6IBgUbPMEBw0U9s+/btrn79+mbYQf8wD5GjtWvXmiCLFHtBECGJiYmhcY4fP97EE/Vhy5cvd7t377Zm2b5WjHET3fPMnTvXhNuUKVMs4gTTpk2zaBlRKIQjIPo4JqPpmqwRUS2E6o033mjbXnnlFetvFg8IUOaaGmfOnLFP0EZVCCGEEELkDSTI8gHBJsnbtm2zyA5RpEgOHDgQEmTBaA3OiSVKlLAm2B7SGCEYGZowYYKJO5wPMfcgQlWrVq1UxxYZFaI2zF+TqBFCMWjc0aBBg7Djmc/+/fstQhaExtbMx8PYMyrG/Jio8wquKc2tEYDxGqSkxYgRI8LErhBCCCGEyDtIkOUDiA55vv/+e6t/GjVqVLLjEEMeIl5BiD4Ft/loFNEpmDNnjjkTjh071kQTAmnMmDGWapga0e7jrxkLzAdxFFnHBZdffnnUNchqChT4f5nAwZ7rpFZGI5ZxEaHs06dPWIQMoSqEEEIIIXI/EmT5jDp16riFCxda36z0OPqlBbVb1Jf16NEjtC0YoYoH0gCp1Tp69GhILK5bty7ZfEhbxBXxfFjSEw07d+6c27x5cyhlkdo0TDsihSDjJj0UUmoTEAsYfvARQgghhBB5DwmyfEbPnj3NMbFdu3Yhd0NS/ohwUWNFemI8UP81Y8YMq/eifmzmzJlu48aN6e47FqRJkyaWQtmpUyeLthEZGjhwYNgx7du3t304K1KPhbHIoUOH3BtvvGHz4/fMBIdGDEO6detmNXSIWtwog73X+Jm6OurumD8pmL4eLzPZMTRBfdGEEEIIIXI5clnMZ1CPRTQLm3oML6itQlBQA+VT7eIBgYIjYtu2bV29evXcyZMnw6Jl8cB43nzzTatHu+mmm1yXLl3Muj5I0aJF3erVq1358uXt/kTVsN+nhiyrxAqmIaxjo0aN7J4PPfRQsr5l1NIRSSOdkvXFTEUIIYQQQohILkgKFroIIXI8RAp/+ctfuv/+97+KkAkhhBBC5PL3NUXIhBBCCCGEECKbUA2ZyHfQQDrYyywI6ZHBerBoro5CCCGEEEJkFhJkuZAHHnjAXP3eeuutDF0Hi3lqtO6++26Xn6D3V0quh2kJMs+QIUNs/TPiniiEEEIIIYQEWS5k3LhxYT2u8iLvv/++u/32290333wTd9PllEBwVapUKUeKZCGEEEIIkb+QIIuTn376yV100UXZcm8KBPP7GuTnsXuuT1ziChQumt3DEJnIwZHNs3sIQgghhDjPyNQjRho3buweeeQRszAvWbKkS0hIcDt27LBapIsvvtiVLl3adejQwZ04cSLsnF69etk5xYsXt2PoAfbDDz+4zp07u0suucQiNYsWLQqdgx09tu30ryKSQ98rImKR0ZhgmiH36d27d6ivWJkyZSylLsi+ffvcbbfd5ooUKeKuu+46t2zZsmRzpAlzmzZtLCLFdejtdfDgwWT3xXoe23fGlhHOnDnj+vXr58qVK2eNj1mLV155xe5JdAxYN1IruXdaLFiwwGz8WbcSJUpYHzPWOrWxHzlyxHqyMd9ixYpZOuP69evTNQ/Wevr06e7tt9+2sfIhwsc8+HnevHnu1ltvtXHRTHrv3r3Wo4178d3hO/TVV1/FtYZCCCGEECJ3I0GWDnjpJqpCHy+a/t5xxx2udu3abtOmTW7x4sXu2LFjJmgiz0HAbdiwwcRZ9+7dXevWrV3Dhg3dli1brBcYQu7UqVN2/M8//2zNjOfPn+927tzpBg8e7AYMGGAv9WmNDUGBmBg9erQ1Sfaii2vSL4uxs//FF180IRTk7NmzJjIRiZheMEfEAk2QiSZ5VqxY4fbs2WPXfvfddzO0nh07dnSvv/66e/75592uXbvcSy+9ZPdEoC1cuNCO4V5Hjx5NJkoj4RiE1YMPPmjXQhAx52BqZ+TYMeigl9jnn3/u3nnnHbdt2zYTtaxXeujbt689d9aKcfDh+XoSExOtMTTPm0bS999/v92HObHWNObmOacmXLFODX6EEEIIIUTeQCmL6eCaa64xsQM0+kWMDR8+PKwZMGKCCEjlypVtW82aNe1lHPr3729CDoHWtWtX28aL+KRJk9z27dtd/fr1XaFChdzQoUND1yRStnbtWhNkkWIvSI0aNezF349z/PjxJkCaNm3qli9f7nbv3u2WLFli0SFg3EGnwblz55oQmTJlikV1fANkomWIG4QjIPo4JqPpfqwRc0IcEcmCihUrhvYTsQIaLsdSQ4YIohEzIqxChQq2jWhZkMixT5482SJTRKv8/eKpLUNEEv1COBGdjCbYELvw6KOPmnDk2dx88822jYjoq6++muL1R4wYEfadEEIIIYQQeQdFyNJB3bp1Qz8TTVm1apW9jPtPlSpVbN+BAwfChJKnYMGClkoXFAqkMcLx48dD2yZMmGD3uvzyy+26CIfDhw+nOrbgfaBs2bKhaxIxQih6MQYNGjQIO575EKkhQubng0g5ffp02HwYe2bUXuFOyHoQocoMEL533nmnjY8IJKmhGIIEiRw7Y0BUezGWVQSfjX/ekd+B4POPBCFPU0H/IbVUCCGEEELkDRQhSwdEWDyku7Vs2dKNGjUq2XGIIQ8RryBEn4LbfDTKp8nNmTPHIipjx4410YRAGjNmTJp1TdHuk57UO+aDCJw9e3ayfQjDaGuQEWKxlk8PiDuibR9++KFbunSpe+GFF9zAgQNt3YgyRht7Zo8hJaI978htqT0r6uv4CCGEEEKIvIciZHFSp04d98knn7irrrrK0tyCn4yIFmq3qD/q0aOHRW+4XjBCFQ9Vq1a1qAppfZ5169Ylmw/GH6QIRs4nK1wdiRAhQj744IOo+30kC5OTWEHYkAZIet/WrVvtGvRZSy1yRZTs66+/jmMGycebnrEKIYQQQggBipDFSc+ePS0tjnog725Iyh8RLuqUiNjEA/VfM2bMsHovIjszZ860Gicf5YkHarSoaevUqZNF2zCFIHoUpH379rYPZ0UMQTAWOXTokHvjjTdsfvyemSBkGQ8mHJh6kHLI/Ujdo1aOOjAEFuYbd911l0WzSKNMCSJh1GVR64ao5HfqwxCjKcGzo5YO90XqtIhsIuRI7YxM6YxlPjwzTENISz0frQl2DE1wl156aZbfRwghhBBCZB2KkMUJL+1Es4iKIAKI+GBvjwFFgQLxL2u3bt3MmKJt27auXr167uTJkxYtywiMh0jRjz/+6G666SbXpUsXs38PUrRoUbd69WpXvnx5uz9CBrMJasiy6qUfM5NWrVrZ/Ki/w+jE29RfccUVFul64oknrMaKlgOpwRgZP+IN8YmRCmmfQeOSaFEt0hsRcJzHM8R0JR4xzdix0sfKnhRPvhtCCCGEEEKkxQVJQV9wIUSOhwgnETgMPhQhE0IIIYTI3e9ripAJIYQQQgghRDYhQSbihqbGQdv/4Ie0v5T2pVYLlhLY/qd2vbTaAqSXatWqpXivaE6UQgghhBBCxINMPUSIBx54wH377bfurbfeiul46qVwKYxmTPLcc8+55s2bZ2rNXrR7Bfdn5jpQw/b2229H3e97iQUZMmSIrVtqYxRCCCGEECISCTIRYty4cS49JYU4H2KLHw1cElPaFw8XXnhhpl4vLTA5yer74SKJ2Qouj0IIIYQQIn8iQZbD+Omnn0I9uM4358OqPaevQW7i+sQlrkDhotk9DBEnB0dmXgRZCCGEELkX1ZBlM40bNzZLdyzzS5Ys6RISEtyOHTvMrp16JdLjOnTo4E6cOBF2Tq9eveyc4sWL2zH0RMMyvnPnzu6SSy6x6M6iRYtC52DPj409/cyIbGHRTkQsMlUvGK3hPr179w71WStTpoyl5gWhmfRtt93mihQp4q677jq3bNmyZHOkKTW9xWgJwHXodXbw4MFk98WKn9RDxpYR6N1GOiXrwJjvv/9+628WhKbeLVq0MNcbjrv11ltTbMBNHzis7EeNGhXahj0+6865vj1A5DlNmza1Z4rQbdSokduyZUtY3zK45557LFLmfxdCCCGEEPkLCbIcwPTp0y0iRO8qXvTvuOMOV7t2bbdp0ya3ePFid+zYMRM0kefwsr9hwwYTZ927d3etW7d2DRs2tBd/eqMh5E6dOmXH//zzz9bcef78+W7nzp1u8ODBbsCAAW7evHlpjq1YsWLWaHn06NHWNNqLLq5JzzLGzv4XX3zR9evXL+z8s2fPmshEuGACwhwRms2aNbNImIemzjRV5to0g84I3PPvf/+727Ztm9V1If4QfZ7PP//cRGThwoXdypUr3ebNm61B9blz55Jdi/0IK8SinxtrhjClqTTPiIbSEydODDvvu+++s8bXa9ascevWrbO6Onqdsd0LNpg2bZo7evRo6PdonDlzxqxTgx8hhBBCCJE3UB+ybIYoFC/YPnoybNgwEy5LliwJHXPkyBFXrlw5Eyw0PeYcIl4cB/xMFAZxNGPGDNv25ZdfmlBYu3atq1+/ftR7E5njuAULFkQ19Yi8D9BYGsGIcKSpMsYdhw4dCplqICCJ7vnaqFmzZtmcdu3aZZEgQIgRLeM+CEfuy3k4JWZFqiKi6cYbbzQxhBhEiM6ZM8fWs1ChQsmO9+uAoOrYsaObMmWKNer2IHoRzBMmTAhtY42JkqVk6oF4Zc6vvfaaRebSU0OG+KNJdiTl/jJPKYu5GKUsCiGEEHkX9SHLZdStWzf0M1GdVatWhdmsV6lSxfYFU+pq1KgR+hmL+RIlSrjq1asncwIMpuohILgX6Xdcd/LkyWnaxQfvA4g8f01EFkIx6HDYoEGDsOOZz/79+y1C5udD2iLiJTgfxp5ZYoyIV8uWLV358uXtvqQLgp8rookUxWhizEPEj4gj6Y9BMebnXa9evbBtkfMmqtm1a1eLjPHHyB/i999/H5c9f//+/e2P2X9IARVCCCGEEHkDmXrkAEgJ9PDSjpgI1isFxZAnUkwQbQlu89EoIjNARKhv375u7NixJh4QKmPGjDHhkRrR7uOvGQvMBxEYrXcXwjDaGmQE6uhIkeTDPbkHIojffYokNXRpcfXVV5vInTp1qkUBUxNv0SC6dvLkSavTw3GS9EjWPZimGSucy0cIIYQQQuQ9JMhyGHXq1HELFy40kwes3jMLardItevRo0doW0omFrFStWpVi9ZQA+XFIvVSkfOZO3euK1WqVJrh2sxg9+7dJoRIqSR651MWI6N+1MZRa5aS0KI+74033rC0Ter3qBvzxzJvhCzpjJ7IebPe1JVRNwasU9CYBbgeKaFCCCGEECL/IkGWw+jZs6c5JrZr1y7kbkjKHxEuaplIT4wHUueoL6M2DadFUvEwkuDneGnSpInVtBENItpGruzAgQPDjmnfvr3tw1kRQxCMRag5Q+wwP37PTEhTJPXxhRdecA8//LA5VmLwEVk7x/777rvP0gFJKURQUR8XdHhERGLqcfvtt9vz4Bkgkh999FGrM8PJ8eabb7ZIHK6NFStWDFtv7/bIujz++OPJInOIbsxMuAYRMBwz08OOoQnnReQKIYQQQoisQzVkOQzqsYiuEDnB8ILaKuztMYQoUCD+x9WtWzcz/aAeivonokjBaFk8MB5MKX788UcTM126dDE3wsgGy6tXrzahxP2JLnmb+KwQE6Qovvrqq+YmiQ0/kbKnn3467BhSERFapFNSX0ZKJSI4WrQM23yO/fjjj01c8lxYwyeffNIEJeciMHG5DPLKK6+4b775xiKEuF3SPgCBF4T0UVwlieRhEiKEEEIIIfIfclkUIg+79gghhBBCiPOPXBaFEEIIIYQQIhegGjKR46DvGb3MokF6ZGouiaQhCiGEEEIIkVuQIBNpEtkwOl5ibYSMEUZKDZbTEmTxgptirVq13HPPPZdp13z//ffNEIRaMmoAhRBCCCGEiESCTKQJvbTOZ6khgqtSpUrufILrY3p7jWWUgwcPmsvl1q1bTQyml+sTl7gChYtmydhE1nBwZPPsHoIQQgghchgSZLkEGgpj554dUJCY19eA9gJCCCGEEEKcb2TqkUMhhY5+WVje06Q4ISHBempRW3XxxRe70qVLm516sNkw5/Tq1cvOoacVx2Dn/sMPP7jOnTu7Sy65xCJPixYtCp2DjTs29ERqiEzRh4uIWGTKYjDNkPtg4+77pGENP2TIkLBz9u3b52677TZXpEgRs5/H3j0SmiXTdJl0Pq5DrzKiRpH3xUqfdgDBHmHxQKNm+oMxJtamVatWYXNi3YI9woYPH+4efPBBWzds+ydPnhzazzhJwaQ3GQ23ueb111/vPvjggxTvf+rUKXt+9B0jBdT3gMPynmsxBiGEEEIIkb+QIMvBTJ8+3SJC9CWjn9Ydd9xhL++bNm1yixcvdseOHTNBE3kOAm7Dhg0mzuiP1bp1axMNW7Zssd5mCDnEAfz888/WnJm+XTt37nSDBw92AwYMcPPmzUtzbMWKFXPr1693o0ePtqbPXnRxTXqOMXb2v/jii65fv35h5589e9ZEJmIHEw/miNBs1qyZRcI8NE7es2ePXfvdd9+Ney1ZM0Qk4+R6rB+CMTXoE0Y9GymF9GxjLTk3CA2fH3vsMTumQYMGrmXLltbjLRIEWNOmTW1tmAsilGcEy5cvd0ePHrW0yWicOXPGrFODHyGEEEIIkTeQIMvBEM1B7BAZ4iUeMUbUpkqVKvbz1KlT3apVq9zevXtD59SsWdMNGjTIzu3fv79FbhBoXbt2tW0ILgTD9u3b7XjqpoYOHWrCg4gNzY+JpqUlyGrUqOESExPtmh07drTzEU9eYOzevdvNmDHDxoPwYdxB5s6da+JkypQp1vyahtHTpk1zhw8fNjMMD6KPY6pVq2afeOG6XKtFixauQoUKtn4ItNS46667TIgRVURQso6sdxCimPfee6+Nf9KkSZbeSVPoIF9++aU1oC5btqz75z//ac2yfRNr36iaKGNKaZMjRoyw6/oPjaSFEEIIIUTeQIIsB1O3bt3Qz9u2bTMxQBTJfxBmcODAgTCh5ClYsKC97CN4PKTqwfHjx0PbJkyYYPdCIHBdUvMQMKkRvA8gNvw1d+3aZaKBNEMP0aMgzGf//v0WIfPzQZCcPn06bD6MPTPqxohOIcQqVqxoEcLZs2eHooSxzJGUQkRTcN0i53XhhReaMGX+kfdG1CFC45kLwpqmgv5DqqcQQgghhMgbyNQjB0NEJ9hfi3S4UaNGJTsOMeSJdApESAS38TsQnQJqoPr27WvpeYgLBNKYMWMs1TA1ot3HXzMWmA8iEGEUiY8cRa5BRmBepGwSfVu6dKlFCql727hxY4qW9Bmdo6d58+Zu4cKFlhIaFMexUrhwYfsIIYQQQoi8hwRZLqFOnTr2Uo/ZBJGYzILaLerLSM3zBCNU8UD6HlEc6qK8WFy3bl2y+RAxKlWqlLv00kvd+YB1a9KkiX1It0SIrVy50urd4oV5+Vq0c+fOuc2bN1saYxDq/4gA3nnnnSYIMTkBHy3DWCUedgxNOG9rJ4QQQgghsgalLOYSevbs6b7++mvXrl07i+ogmpYsWWL1XvG+0AM1YBhecC1q0Z588km7fkZA8FSuXNl16tTJUhMx7Rg4cGDYMdSqUZOFsyL7P/vsMxMr1HUdOXLEZTYYgjz//PPWcPrQoUNW30a0K6POjaR70uyamjmeEU2gcWaM5Omnn7Y5Y8zCsYAYxdnSG7SQjiiEEEIIIfIXEmS5BOqxiGYhvnBKJPUNm3aiPAUKxP8Yu3XrZhGitm3bunr16pnhRzBaFg+MB5Hy448/uptuusl16dLFrOuDYGyxevVqs5Pn/kTVsN+nhiwroj6sEy6GCCLuhfPj66+/niGjEB/94oN5yZo1a9w777xjQjMazz77rLliMgbELxE7ROJLL71kzxdxKoQQQggh8hcXJCUlJWX3IITIbdCHDFdK7O5r1ap1Xu+N7T1ui0TUlLIohBBCCJHzSM/7miJkQgghhBBCCJFNSJCJFHnggQfc3XffneHr4E741ltvZfg61JoFbf+DHyz+U9rHJxLMUZ577rlMH2MQXBzPd/RMCCGEEELkLuSyKFJk3LhxLidltNLjC1OOaFCvhkFGvOAIWbx48ZiPR9BldG1effVVqwP89ttvM3QdIYQQQgiRe5Egy+H89NNPmdIYOR7Ie81Ja4DgosFyVkDT59zG9YlLXIHCRbN7GPmGgyObZ/cQhBBCCJEHUcpiDqNx48bWx4rICW59CQkJbseOHe53v/udpd6VLl3adejQwZ04cSLsnF69etk5RHk45uWXX3Y//PCD2eLTFBkhs2jRotA5uDXiaogxBUIH+3ciYqmlLHIfbOn/9re/ucsuu8xEDGl5Qfbt22d9uYoUKWL9tpYtW5ZsjvQow20Q50Oug7sgJhmR98WZEffBjFrTHz9+3JpqM0/mG60ZdTBlkbHwO66Mt99+uzlC4qK4du3asOgW4+ccWgcwX54Vc0sJWhVUrFjRnu+qVavs2VDoyb34RK6lEEIIIYTI+0iQ5UCmT59uESFs7rFUxya9du3a1i/M96xC0ESeg4DbsGGDibPu3bu71q1bW9PnLVu2mFU+Qu7UqVN2PD24rrzySjd//ny3c+dON3jwYDdgwAA3b968NMdWrFgxt379ejd69Gj31FNPhUQX18TCnrGzH2v5fv36hZ1/9uxZEy6IRGrCmCNCs1mzZhYJ86xYscLt2bPHrk0PsYyAwEMoIYIWLFjgJk6caCItLeid1rdvX0uTpK8aPeBo/uxhLRGN9DRjHqQe3nfffVGvtX37dnfLLbe4+++/340fP97dfPPNVsOG6w7pkny4lxBCCCGEyF8oZTEHQsQFsQPDhg0zMTZ8+PDQ/qlTp7py5cpZLyuEAhDBGTRokP3cv39/E3IItK5du9o2BNekSZNMGNSvX98VKlTIDR06NHRNIkdEgBBkkWIvSI0aNVxiYmJonIgLxFPTpk3d8uXLrekxTaaJbAHjJrrnmTt3rgm3KVOmWFQIpk2bZtEmGkMjHAHRxzEZTddkjYgMIlRvvPFG2/bKK69YL7K0QCA1b/7/0tRYK3qW7d+/31WpUiUkLpk//du8WOW63Iv+a54PP/zQtWjRwgTeY489ZtuYFymhrEFa6ZJnzpyxT9BGVQghhBBC5A0UIcuB1K1bN/Tztm3bLLITdAz0goAUuKBQ8uA4WKJECWse7SGNEYKRoQkTJti9Lr/8crvu5MmT3eHDh1MdW/A+ULZs2dA1d+3aZULRizFo0KBB2PHMB1FDhMzPh7RFGkIH58PYM6N2jjHRgDm4pqwfAjAtgnNlnpHrx3W9yAtel3t6WE/EKoLYi7H0MmLECBNv/sMaCyGEEEKIvIEiZDkQokOe77//3uqfRo0alew4LxKAiFcQIi/BbT4aRXQK5syZYxGgsWPHmmhCII0ZM8ZSDVMj2n38NWOB+SCOotVxIQyjrUF2kdr6xQpzQqC+/vrr7sEHH4yrkTMRzz59+oRFyCTKhBBCCCHyBhJkOZw6deq4hQsXms06EZnMgpon6st69OgR2haMUMUD6XrUalEP5cXiunXrks2HtMVSpUrFJU7SC1Er6r42b94cimZRm5YZVvNcl7o+n57orxtMh8RIhBq4u+66y2rnli5dauIXiABirpIWhQsXto8QQgghhMh7SJDlcHr27GmOiRhKeHdDUv6IcFFjRXpiPFD/hRkF9V7Uj82cOdNt3LjRfo6XJk2aWE1bp06dLNpGJIe6qSDt27e3fTgrYgiCscihQ4fM0ZD58XtmgkMjhiHdunWzGjpELW6UGelZFoygYaDy/PPP23VxT6Q+L1g/5qN9//rXv6yWjg/GLKRqIrKJGFKDRw0gbo58YmXH0ITzImqFEEIIIUTWoRqyHA7pbkSziKRgeEFtFYKCWqUCBeJ/fAgUHBHbtm1rphQnT54Mi5bFA+N58803rUkzoqRLly7mQhgEwbF69WpXvnx5uz/RJOz3qSHLKnGBaQjr2KhRI7vnQw89ZBG6jMJccJHEORHXREQW0b9osA9zEZpJYxRCSwIilA8//LA9A1IbvZGLEEIIIYTIP1yQxBuiECJd0IcMYZwZqY/phcgj5h70MFOETAghhBAi55Ge9zVFyIQQQgghhBAim1ANmcjx0EA62MssCOmRqdWDUaMlhBBCCCFETkUpi8J44IEHLP3urbfeytB1sIenjuzuu+/OtLEhuj7//PO4BFmlSpVcTmLIkCG2xh999FHc11DKohBCCCFEziY972uKkAlj3LhxZjiRE0Fw5TRhJYQQQgghRGYgQZaD+Omnn6w3VXaAgs/va5DbuD5xiStQOHabfJE+Do5snt1DEEIIIUQ+QKYe2Ujjxo2tdxVufSVLlrTGwTt27LB6KWzSS5cu7Tp06OBOnDgRdg69rzinePHidgx9yrBR79y5szUdJpqExboHy3ys5ekxRrSJ3lxExCJTFoNphtynd+/eod5nZcqUsXS7IPv27XO33XabK1KkiLvuuuvcsmXLks2RRtFt2rQxm36uQ/+xgwcPJrsv9vhY0zO2jDBx4kTrscaYWJtWrVrZdnqulShRwp05cybseO7NGgPzq1WrlvVko0cYIvW+++5z3333Xeh4eojdcsstNh+u16JFi2QNtY8cOWJ945gvPchuuOEGt379+qjj5dyKFSva9yCnRiiFEEIIIUTWIUGWzUyfPt0iQvQaGzlypLvjjjtc7dq13aZNm+zl/9ixYyZoIs9BwG3YsMHEWffu3V3r1q2tr9WWLVusXxki49SpU3b8zz//bA2X58+f73bu3OkGDx7sBgwY4ObNm5fm2BAUiAl6ZNHI2YsurklPL8bO/hdffNF6cgU5e/asiUxEIsYczBGhSaNmImEeGiPv2bPHrv3uu+/GvZasGSKScXI91g/BCKwPwvSdd94JHX/8+HFr2Pzggw+GCSRqvBgHnw8++MCeiwfh26dPH7sX46b32j333GPr4U1E6HdGzRv32rZtm4lavz/I9u3bTdzRx2z8+PFWfxcNRCR5yMGPEEIIIYTIGyhlMZshmuMbAg8bNszE2PDhw0P7p06d6sqVK+f27t3rKleubNtq1qzpBg0aZD/379/fBAMCrWvXrrYNwTVp0iR74a9fv74rVKiQGzp0aOiaRMrWrl1rgixS7AWpUaOGS0xMDI0T0YAIadq0qVu+fLnbvXu3W7JkiUW2gHEH3RBpkowQmTJlSkhs0KSZ6NL7779vwhEQfRyT0VTFw4cP27WIWiECK1SoYOsJRAYRPtwfcQazZs2yBtVEAz2Mlx5jnA8IW+bsG1zfe++9Yffk+dDUGaF7/fXXu9dee8199dVXbuPGjRYhg2j1bx9++KGNc+DAge6xxx5LdV4jRowIe35CCCGEECLvoAhZNlO3bt3Qz0RTVq1aZVEk/6lSpYrtC6bFIZQ8BQsWtNS56tWrh7aRqucjQJ4JEybYvRAPXHfy5MkmYFIjeB8oW7Zs6Jq7du0yoejFGDRo0CDseOazf/9+Ezd+PoiU06dPh82HsWdG3RhCERFGCiBCavbs2aEoISBYly5dGnJsRHiRMhmMTJGq6MVY5Jx9mibpiNwDxxyOB7+WuCciAr0YiwbHMlaEc1pizItuHHr8hzRQIYQQQgiRN1CELJshouMh3a1ly5Zu1KhRyY5DGHiIeAVBUAS3eYHh0+TmzJnj+vbt68aOHWuiCcExZsyYFOuaUrtPtNS7lGA+iECEUSQIw2hrkBGYFymbRN8QXgge6sKIVhGVQygRXaSejOjcJ598YimL6ZkzzwfRR90eYpR9RMZ8CmZqFvzBuXPu66+/bumSaVmhFi5c2D5CCCGEECLvIUGWg6hTp45buHChRV0uvDDzHg21W9SX9ejRI7Qt0ogivVStWtUiNUePHg2JxXXr1iWbD2mLpUqVOm/9sli3Jk2a2Id0S4TYypUrrd4NunTp4p577jmLknEMUb5YOXnypNWmIcZuvfVW27ZmzZpkUUXSL7/++usUo2SINurT7rrrLquxQzwGo3JCCCGEECL/IEGWg+jZs6e97JMS590NSfkjwsVLPumJ8UD9F1Eh6r2oH8NFkKgRP8cLYoaatk6dOlm0DaMJ6qGCtG/f3vbhrIjRBsYihw4dcm+88YbNj98zE0TOp59+akYeOFC+9957FsEKOjdSR0a0kHVmTdID1yQ9lHRPRCiph0888UTYMTw7aulwb6T2i+O2bt1qEbFgSidRQaJz1NzxwYCElM70sGNoghpDCyGEEELkclRDloPgpZ1oFm6ApNRRW4W9PVEe3PzipVu3bhYhatu2ratXr55FeoLRsnhgPG+++ab78ccf3U033WSRJ2984SlatKhbvXq1GWdwf6Jq2O9TQ5YVQoJ1QuzhVMm9cH4kLbBatWqhY7Cyx5gD8RO0+Y91zojjzZs3W5riX//6VxOcQaiFI+JFVJAIGM8Q05VoYpox0J4Au/vmzZubg6MQQgghhMhfXJCk5kcin3HnnXeaSHv++eddboRoJMISgw9FyIQQQgghcvf7mlIWRb7hm2++McMPPjSQFkIIIYQQIruRIBM5ChpIB3uZBSE9MjUXQ1wdUwOXRUQZLpbBujIhhBBCCCGyCwkykS3Q/+vbb791b731Vtj2G264wXp5xSrIMCwh2kVfr7Q4ePCgy8450oC6Vq1a5vIohBBCCCEEqIZMZAvk0/LVw4gjI9AnDHOR9Bp0ZMccscKnz5m3uKe9AaYtfNKDasiEEEIIIXI2qiETMUEzY1wBswO+oHl9DSLnmFJfsni5PnGJK1C4aKZeUzh3cGTz7B6CEEIIIfIRsr3PR5Ay98gjj1hEpmTJktaUeMeOHVazhQV76dKlXYcOHdyJEyfCzunVq5edQx8ujqGHFxbtnTt3tmhPpUqVzL7dg20/9vb0OSPFkHqtcePGJUvnC0a1uE/v3r1D/dfKlCnjhgwZEnbOvn37rMdYkSJF3HXXXeeWLVuWbI40q27Tpo1FpbgOPdCCqYr+vlj002Ygo7VkpEuSNsmYWJtWrVqlOkcfDeNnerJhnU+Uj48QQgghhMh/SJDlM6ZPn24RIfqd0R+Lnl2YXWzatMmaEx87dswETeQ5CLgNGzaYOOvevbtr3bq1a9iwoduyZYv1TEPInTp1yo6nGTNNn+fPn+927tzpBg8e7AYMGODmzZuX5thomLx+/Xo3evRoaybtRRfXpJcZY2c/Pcb69esXdv7Zs2dNZCISMQdhjgjNZs2aWSTMs2LFCrdnzx67Ns2k44U1Q0QyTq7H+iEYY4F+aawR5x49etQ+KXHmzBkLewc/QgghhBAib6CUxXwG0RzEDgwbNszE2PDhw0P7p06d6sqVK+f27t3rKleubNtq1qzpBg0aZD/379/fhBwCrWvXrrYNwTVp0iS3fft2V79+fauTGjp0aOiaRMrWrl1rgixS7AWpUaOGS0xMDI1z/PjxJp4w7Fi+fLnbvXu3W7JkiUW2gHEHHRnnzp1rwm3KlCmhiNO0adMsWobVPcIREH0ck9FUxcOHD9u1WrRoYSKwQoUKtp6xQPSOZtGcRzQwNUaMGBG2nkIIIYQQIu+gCFk+o27duqGft23b5latWmVRJP+pUqWK7Ttw4ECYUPIgIkqUKOGqV68e2kaqHhw/fjy0bcKECXavyy+/3K47efJkEzCpEbwPlC1bNnTNXbt2mVD0YgwaNGgQdjzz2b9/v4kcPx+Ez+nTp8Pmw9gzo24MoYgIq1ixokUIZ8+eHYoSZiaIYApC/Ye0TCGEEEIIkTdQhCyfQUQn2LerZcuW1pcrEsSQh4hXEKJPwW0+GkV0CubMmeP69u3rxo4da6IJgTRmzBhLNUyNaPfx14wF5oMIRBhFgjCMtgYZgXmRskn0benSpRYppO5t48aNGXaPDFK4cGH7CCGEEEKIvIcEWT6mTp06buHChWa/fuGFmfdVoHaL+rIePXqEtgUjVPFQtWpViwxRa+XF4rp165LNh7TFUqVKnTc7eNatSZMm9iHdEiG2cuVKq3dLC6J0GKAIIYQQQoj8iwRZPqZnz57mmNiuXbuQuyEpf0S4qLEiPTEeqP+aMWOG1XtRPzZz5kyLGvFzvCB4qGnr1KmTRdswthg4cGDYMe3bt7d9OCtiloFpBk6GGGgwP37PTDAE+fTTT83IAwfK9957zyJ6sTo3IoRXr17t7rvvPouAUZeXHnYMTVAfMiGEEEKIXI5qyPIx1GMRzSJKg+EFtVXYshPlKVAg/q9Gt27dLELUtm1bV69ePXfy5MmwaFk8MB4aQP/444/upptucl26dDHr+iBFixY1gVO+fHm7P1E17PepIcsK4cI6IfZwquReOD++/vrrrlq1ajGdj2jEkv/qq68OS6kUQgghhBD5hwuSkpKSsnsQQuRFiDwSZZw1a1a2dX4XQgghhBDnn/S8rylCJkQmc+7cOeu/htV/rNEyIYQQQgiRP1ENmcjX0EA62MssCOmRv/jFL1J1dYzGjh07zNTk9ttvdw8//HCmjVUIIYQQQuQ9JMjEeeeBBx5w3377rXvrrbcydB1s8akru/vuu+O+xg033OA++uijuARZSkYd1OH5fmT+dz6ZNWYhhBBCCJF3kCAT551x48a5nFK6iOCqVKlSll0fd8lg3zNs+3FkBAw9cJ7cunWrq1WrVrqvfX3iElegcNFMHa9w7uDI5tk9BCGEEELkIyTI8ik//fST9cHKDihwzC9rEOmeWKZMmSy9nxBCCCGEyF3I1COf0LhxY/fII49Y6hz9rhISEqzWifqpiy++2JUuXdp16NDBnThxIuycXr162TlEdTiGvmU//PCD69y5s7vkkkssurRo0aLQOVjoYzVP5IfoEz25iIhFpiwGU/a4T+/evUO90BAtQ4YMCTtn37591u+rSJEi7rrrrnPLli1LNkcaR7dp08bs6LkO/ciIQkXeF7t8LP9j7ReWEsePH3ctW7a0eTLf2bNnJzuGlMXnnnsu9Dspiz5V0/dlq127tm1nHYQQQgghRP5CgiwfMX36dIsI0Xts5MiR1j8LMbBp0ya3ePFid+zYMRM0kecg4DZs2GDirHv37q5169ZmWrFlyxbrX4aQ8zVTNEamAfP8+fPNaXDw4MFuwIABbt68eWmOjdS+9evXu9GjR1uPLi+6uCZ9xRg7++n31a9fv7Dzz549ayITkYhRB3NEaDZr1swiYZ4VK1a4PXv22LVp7JwREHiIwFWrVrkFCxa4iRMnmkiLFdYUli9fbqmM9DSLxpkzZ8w6NfgRQgghhBB5A6Us5iOuueYaEzswbNgwE2PDhw8P7Z86daorV66c27t3r6tcubJtq1mzphs0aJD93L9/fxNyCLSuXbvaNgTXpEmT3Pbt2139+vVdoUKF3NChQ0PXJAqE/TuCLFLsBalRo4ZLTEwMjXP8+PEmnpo2bWqCZffu3W7JkiUW2QLGHXRHnDt3rgm3KVOmWLQJpk2bZtGy999/34QjIPo4JqOpiqwRkUFE1Y033mjbXnnlFWsQnd50xhIlSqSayjhixIiwNRVCCCGEEHkHCbJ8RN26dUM/b9u2zSI7RJEiOXDgQEiQIZQ8NDlGPFSvXj20jTRGCEaGJkyYYOLu8OHD5lRIhCot04rgfaBs2bKha+7atcuEohdj0KBBg7Djmc/+/fstQhbk9OnTNh8PY8+MujHGdOGFF4ataZUqVUwAZjYI4T59+oR+J0LGegghhBBCiNyPBFk+Iuj2Rw8t6p9GjRqV7DjEkIeIVxCiT8FtPhpFdArmzJnj+vbt68aOHWuiCYE0ZswYSzVMjWj38deMBeaDOIpWxxU01giuQW6hcOHC9hFCCCGEEHkPCbJ8Sp06ddzChQvNdIJIT2ZB7Rb1ZT169AhtC0ao4oE0QGq1qLPyYnHdunXJ5kPaYqlSpdyll17qshqiYefOnXObN28OpSxSm0Z/tVjxkTqMUOJhx9CE8zJXIYQQQgiRdcjUI5/Ss2dP9/XXX7t27dpZryxEEzVauCfGKxB8/RcmIVyLOqsnn3zSrp8Rmvx/7L0LuI11+v//GZQwTd9SYUpmUNKJklMplaRIB4okJBSRJBLKdihngymHlIQOjqmQ4yQqh8ghohxKqTE1pKZySPX8r9f9u57n/6y119577b3XPr9f1/WMvZ7j5/NZa67reXff9/u+/npLoWzTpo2lJmLa0bdv34hzWrZsabVtOCty/IsvvrDaMdwbv/76a5docGjEMOSBBx6w6B/CrH379ulqJI145HzfUOXHH39M+DiFEEIIIUTuRoKsgEI9FtEsxBeGF9RWYW9PDVShQhn/WSBQcERs3ry5q1mzpjt48GBEtCwjMJ558+ZZPVqNGjVM+GBdH6Z48eJu1apV7pxzzrHnE1XDfp8asqyKImEawjrWrVvXnnn//febyIoXIpP//Oc/3XPPPWf3QUwKIYQQQoiCxZ88z/NyehBC5FdIsRw0aJCJyESBqQfNtYmoKWVRCCGEECL3kZ73NdWQCZEF0JeNCCSpiBdeeGFOD0cIIYQQQuRSlLIosg0aKd92222Zvg8OjG+88Uam70OtGbb/sTYs/lM6FqtVAOYoY8aMCT5PmjTJ3XXXXZYGGm3Rn1XzEUIIIYQQeQ9FyES2MXbsWJebMmQvv/xyt3nz5pjHqFdLj0FHNI888ojVvYUFaP/+/U14pfRMIYQQQghR8JAgK2DQpDkRjZEzAnm0uWkNEFwVK1Z0eZWLkpa4QkWL5/Qw8ix7hzbK6SEIIYQQQihlMb9zzTXXuC5duljqHLbwDRo0cNu2bXM33XSTpd6VKlXKtWrVyh04cCDimoceesiuOfXUU+2c559/3v3yyy9mi0+zZ4TMokWLgmtwa8TV8O9//7sJHWzhiYillrLIc7Clf+yxx9xpp53mSpcubVGkMLt27XJXX321O+mkk9wFF1zgli1blmyO9Chr1qyZOURyH9wK9+7dm+y5ODPiZsjYMsN3331nTbWZJ/ONbkZN+iLcfvvtlo7I55deeskNGDDAbPvZx8Y+IYQQQghRsJEgKwBMnTrVIkKYTAwdOtRdd9117tJLL7V+YX4PLARN9DUIuA8//NDEWadOndydd95pTZ83btxoVvkIOcwr4I8//nBnn322mz17ttu+fbvr16+f69Onj5s1a1aaYytRooT18ho+fLgbOHBgILq4J3byjJ3jEydOdL169Yq4/vjx4yYyEYnUhDFHhCY9woiE+fzrX/+yxs3ce8GCBZlaTwQeInDFihVuzpw5bvz48SbSfPy+a9ji08yaz7QBePTRR83gg31s7BNCCCGEEAUbpSwWAGjWjNiBp556ysTY4MGDg+MvvviiK1u2rDVypgEzVKlSxT3xxBP2d+/evU3IIdA6dOhg+xBcEyZMcB9//LGrVauWO+GEEywC5EPkaM2aNSbIosVemEsuucQlJSUF43z22WdNPNWvX98tX77cffrpp9ZkmsgWMG6iez4zZ8404fbCCy9Y1MkXQkTLaAyNcAREH+dkNl2TNSIyiFCtXr267Zs8ebL1PfM544wz7F/GQNTPB6FI77Hwvng4duyYbWEbVSGEEEIIkT+QICsAVKtWLfiblDkiO7GcAvfs2RMIMoSSD46DJUuWtObRPqQxQjgyNG7cOBN3X331lZliEKGqWrVqqmMLP8fv2+Xfc8eOHSYUfTEG0Y6FzGf37t0WIQtDQ2jm48PYE1E7x5gQVeE1Pf/88018ZRVDhgyJELtCCCGEECL/IEFWACA65PPzzz9b/dOwYcOSnYcY8iHiFYboU3ifH40iOgUzZsxwPXr0cKNGjTLRhEAaMWKEpRqmRqzn+PeMB+aDOIqu4wpHqqLXIK9BhLJ79+4RETKEqhBCCCGEyPtIkBUwLrvsMjd37lwzmiDSkyio3aK+7MEHHwz2hSNUGYE0QGq1qLfyxeLatWuTzYe0xTPPPDPNLuiJgGjYb7/95j766KMgZZHatB9++CGZ0MToJAwRuuh98VC0aFHbhBBCCCFE/kOCrIDRuXNnc0xs0aJF4G5Iyh8RLmqsSE/MCNR/TZs2zeq9qB+bPn26mVnwd0a5/vrrLYWyTZs2Fm0jMtS3b9+Ic1q2bGnHcFbEEARjkS+//NK9/vrrNj8+JxIcGjEMeeCBB6yGDlGLG2V0zzIEL7VwV155pYkp3CrZ98UXX1gfMsZFFDEzQmvbgAbZIkKFEEIIIUTWIZfFAgb1WESziNRgeEFtFYKCGqhChTL+c0Cg4IiIc2DNmjXdwYMHI6JlGYHx0FyZerQaNWq49u3bm3V9mOLFi7tVq1a5c845x55PVA37fWrIskqsYBrCOtatW9eeef/991uELgypmzg6klqIiQo0bdrUxNy1115r6ZSvvfZaloxPCCGEEELkHf7keZ6X04MQQsQPkUKabP/444+KkAkhhBBC5PH3NUXIhBBCCCGEECKHUA2ZKHDQQDrcyywM6ZHR9WDRro5CCCGEEEIkCgkykWHuvfdecxd84403MnUfrO6pFbvttttcdnD55ZebsUZqgoz6MObXtm3bwLRk/Pjx2TI+IYQQQghRcJAgExlm7NixLi+WICK4KlasmOo52NZjvBE+L9ynTQghhBBCiEQgQZbH+fXXX62/VU5AoWJBX4Oc5KKkJa5Q0eKuILN3aKOcHoIQQgghRKaQqUce45prrnFdunQxq/rTTz/dNWjQwG3bts1qov785z+7UqVKuVatWrkDBw5EXPPQQw/ZNfTD4hx6kf3yyy+Wkkc/LCJBixYtCq7BFh/7ePqIEVGi/xYRsTCk9IXTDHlO165dg/5mpUuXdv3794+4ZteuXe7qq692J510krvgggvMGj4amkE3a9bMrPi5Dz3G9u7dm+y5WOBjP8/YMsN3333nGjdubPNkvq+88kqa12zdutVdd911dk3JkiXN+j5cX/buu++aVX+JEiVsHvQjoz8abNmyxazvWXdcd6pVq+Y2bNiQqTkIIYQQQoi8iQRZHmTq1KkWEaKf2NChQ00Y0OuKl/rFixe7b7/91gRN9DUIuA8//NDEWadOndydd97prrjiCrdx40brSYaQO3z4sJ3/xx9/WPPi2bNnu+3bt7t+/fq5Pn36uFmzZqU5NkTIunXr3PDhw61Zsy+6uCd9uxg7xydOnOh69eoVcf3x48dNZCJWMN9gjghN+ncRCfOh6fJnn31m916wYEGm1hOBhwhcsWKFmzNnjtWKIdJSAiHLGBG3NL9mjZYvX25CGX777TcTjNShffzxx27NmjUm2KiV85tZs7Zc+9FHH7nHH3/cUiRT4tixY2adGt6EEEIIIUT+QCmLeRAMJhA78NRTT5kYGzx4cHD8xRdftIbEO3fudOedd57tq1KlinviiSfs7969e5uQQ6B16NDB9iG4JkyYYAKiVq1aJhAGDBgQ3JPIEcICQRYt9sJccsklLikpKRjns88+a+Kpfv36Jlo+/fRTt2TJEotsAeMOOx7OnDnThNsLL7wQCBgaMRNlIuqEcAREH+dkNlWRNSIyiFCtXr267Zs8ebI1mE6JV1991RpPT5s2zcYBzJMo27Bhw2zt6Dlx8803uwoVKtjx8P2++uor17NnT3f++ecH65QaQ4YMifguhBBCCCFE/kERsjwIKW4+pL8R2SGK5G/+i/6ePXsihJJP4cKFLc3u4osvDvaRxgjhyNC4cePsWZhbcN9JkyaZmEiN8HN8Iwz/njt27DCh6IsxqF27dsT5zGf37t0WIfPnQ9oiAig8H8aeiLoxxlSkSJGINWX9EICpXYPA9cUYkJKIkCRqx3iJuhFFQ6SR6rl///7g3O7du7v27du766+/3oRxeF6xQEAj8PyNaJ4QQgghhMgfSJDlQcJCgLolXvqxcQ9vfq2WT3RKHNGn8D4/GoWogBkzZrgePXpYHdnSpUvtntSbhdMGYxHrOf4944H5II6i50Mk6+677465BrkRonpEFEkJJepHpHLt2rV2jLq6Tz75xDVq1Mi98847VkuH7X9KFC1a1GrNwpsQQgghhMgfKGUxj3PZZZe5uXPnur/97W8W6UkU1G4hJh588MFgX1qRnLQgbY/oDtEi30LeFynh+SBgzjzzzGwRHkTDqPmilstPWSTKRX+11Obx0ksvWS2ZLwxZr0KFCkUYjJBKykaEi0ggqY6kgwICje2RRx5xLVq0MAF3++23Z/l8hRBCCCFE7kKCLI/TuXNnc0zkpd53NyTljwgXNVakJ2YE6pqokaLei/qx6dOnmwkFf2cUUvQQIW3atHEjRowwc4q+fftGnIPhBcdwVsQQBPML3Alff/11mx+fEwkCCsOQBx54wGroELW4UeKemBKMkTo55kG067///a8ZpWCKQurnF198Yemdt9xyi6VnIvCIWLZu3doaT1M/dscdd9hafv3117auTZs2TffYtw1ooGiZEEIIIUQeRymLeRxe+InOYFOP4QW1VQgKaqCI2GQUBAqOiM2bN3c1a9Z0Bw8ejIiWZQTGQ2oeogRLeOqosK4PU7x4cbdq1Sp3zjnn2POJRpE2SQ1ZVokPolOsI66IPBNHRCJ0KcEYEarff/+9RdUQV/Xq1TNjD/845iWILAQo90M4s6YIZNYSccYxDFIwNZFphxBCCCFEweRPnud5OT0IIUT8EFmkKTcGH4qQCSGEEELk7fc1RciEEEIIIYQQIodQDZnI89BAOtzLLAzpkanVg+HqKIQQQgghRE4hQSYyBH22cCJ84403MnUfbPGpK7vtttsyfI/LL7/crPEzIsgyC+6W1OyxCSGEEEIIkV4kyESGoNlxbik/RHBVrFjR5UauueYaV7VqVTdmzJicHooQQgghhMiFSJDlYWjSfOKJJ+bIsylSLOhrkNNclLTEFSpa3BU09g5tlNNDEEIIIYRIGDL1yEMQbenSpYulx51++umuQYMGbtu2bVY/9ec//9l6YNEL68CBAxHX0COLa0499VQ7h75lNDVu27atO/nkky26tGjRouAaLPSxmqdPFtEnenUREYtOWQynGfKcrl27Br3QSpcubT26wtCL6+qrr3YnnXSSu+CCC9yyZcuSzZHG0VjBY9vPfehHtnfv3mTPxS4fq/pwI+aMMH78eOu5xphYGyzso9ebDQHKmj/55JOpRgbp/cbY//Wvf9lYV65caWtHaiYbczl06JD1MjvjjDNsfXk+1vtCCCGEEKLgIUGWx5g6dapFhOg9NnToUHfddde5Sy+91G3YsMEtXrzYffvttyZooq9BTHz44Ycmzjp16uTuvPNOd8UVV7iNGzda/zKE3OHDh+38P/74wxowz549223fvt3169fP9enTx82aNSvNsZUoUcKtW7fODR8+3Bo7+6KLe9Lji7FzfOLEia5Xr14R1x8/ftxEJiIRow7miNCkcTORMB/EDs2WufeCBQsyvJasGSKScXI/1g/BGD0nmkWzdgirf/zjHya6YsGcH3/8cbd06VLrS8b5tWvXdh06dHD79++3rWzZsibqWFdE8I4dO6whNd9PShw7dsysU8ObEEIIIYTIHyhlMY9BNIUXf3jqqadMjA0ePDg4/uKLL9pL/86dO63xMFSpUsU98cQT9nfv3r1NyCEAEAqA4EIUfPzxx65WrVruhBNOiGhUTKRszZo1JsiixV6YSy65xCUlJQXjpFEy4ql+/fpu+fLl1iyZhspEtoBxh90RZ86cacINwUM0CYgcEXF69913TTgCoo9zMpuq+NVXX9m9br75ZhOB5cqVs/UMw1qOHj3axkM0buvWrfbZXzsfxOX06dMtInbhhRfaPqJqjJFG0UQMw8/lOZiR+MYgqTFkyBA1jhZCCCGEyKcoQpbHqFatWvD3li1b3IoVKyyK5G/nn3++HduzZ0+EUPIpXLiwK1mypLv44ouDfaTqwXfffRfsGzdunD2LtDruO2nSJBMSqRF+DpQpUya4J5EgxI0vxoDoURjms3v3bhNH/nxIWzx69GjEfBh7IurGEIqIsPLly1uE8JVXXgmihD4IVF8c+mMm9ZK0Tp9Ro0ZZGuj7778fiLHUIEI5Y8YMM/sgxXP16tWpno+Ipqmgv5HWKYQQQggh8gcSZHkMIjrhHlqNGzc2y/fw5tdq+RDxCoPACO/zBQfRKUAs9OjRw+rISL/jntSbhdMGYxHrOf4944H5IAKj50O07+677465BpkB4UfK5muvvWbikUgh0UTs/NPDVVddZQItrZROH6KCX375pXvkkUfcv//9b0tvZL1TomjRotbhPbwJIYQQQoj8gVIW8zCXXXaZmzt3rqW8UeeUKKjdor7swQcfDPaFI1QZoXLlyhbZoY4K8QNr165NNh/SFs8888xsEx2s2/XXX28b6ZakR77zzjtW7wbUu4VhzKRjEmn0qVGjhhl/UOvG/cLiikheOJrmQ+SxTZs2tiHoevbs6UaOHJmlcxVCCCGEELkPCbI8TOfOnS1VrkWLFoG7ISl/RLiosQqLhvSA4Jg2bZrVe1E/Rm3U+vXr7e+MguChpg0BMmLECDOm6Nu3b8Q5OA9yDGdFjDYwFiGS9Prrr9v8+JxIMAT5/PPPLZqIA+Xbb79tEb2wcyNpmt27d3cPPPCARdOeeeYZS1GMBgHL9US/EGV+o2jEMqIOd0U/BRP3SSKBpDdi2ME4EKzpZduABoqWCSGEEELkcZSymIehHotoFhEYDC+orUIIEOUpVCjjXy3igwhR8+bNXc2aNd3BgwcjomUZgfHMmzfPHTlyxCJK7du3N+v6MJhfrFq1yp1zzjn2fEQKaZPUkGWF8GCdEHs4VfIsnB9JXwzXgbVu3ToYMwL44Ycfdvfff3/M+9WpU8ctXLjQDFQQbkC0DGGMzT9RMQQeUTPqwqi5QwxyHBEthBBCCCEKHn/yUmuqJEQBhj5kGG+MGTPG5SaILuLgiMGHImRCCCGEELmP9LyvKUImhBBCCCGEEDmEashEnoYG0uFeZmFINSxWrFiqro5CCCGEEELkJBJkIl3ce++9Zgv/xhtvZOo+WOJTU3bbbbdl6j40V8YaPyOCLBoMOKjB8w05aEadGhh1YHSyadMmS20UQgghhBAivUiQiXQxduxYl5vKDhFcFStWdLmVRAnPWFyUtMQVKlrc5Vf2Dm2U00MQQgghhMhyJMjyIDRoxqkvJ6A4saCvgRBCCCGEEIlCph55xO2PxsOk0p1++umuQYMGbtu2bVY7RW+rUqVKuVatWrkDBw5EXPPQQw/ZNfTY4hx6lv3yyy+ubdu27uSTT7bI0qJFi4JrsM/HZp40PCJP9OMiIhadshiO9vCcrl27Bn3QSpcubX22wuzatcvs3U866SSzf1+2bFmyOdI0ulmzZmZFz33oRUZKYPRzscrH7j/cKywjfPfdd65x48Y2T+b7yiuvxIxuTZgwwdaZ88qXL+/mzJmT4j1Zv/vuu8+df/75Zm9PCiTcfvvtdi//85YtW9y1115r3wGuO/Qk27BhQ6bmI4QQQggh8iYSZHmEqVOnWkSIvmNDhw613lmXXnqpvcgvXrzYffvttyZooq9BwH344Ycmzjp16uTuvPNOa2JMk2N6lyHkDh8+bOfTFJnmy7Nnz3bbt293/fr1c3369HGzZs1Kc2wlSpSwBsjDhw+3ps6+6OKe9BRj7Byn11evXr0irj9+/LiJTAQKJh3MEaF54403WiTM51//+pf77LPP7N40U84MCDxE4IoVK0xkjR8/3kRaNE8++aRr2rSpiSgaV991111ux44dyc6jwTNrSz0bc6CXGs20YcqUKW7//v3BZ+7DOvP5o48+co8//rg74YQTUhwr98Y6NbwJIYQQQoj8gVIW8wjnnnuuiR146qmnTIwNHjw4OP7iiy+6smXLup07d7rzzjvP9lWpUsWaFAONiBFyCLQOHTrYPgQXEaCPP/7Y1apVy0TBgAEDgnsSOVqzZo0JsmixF4YGx0lJScE4n332WRNP9evXd8uXL3effvqpW7JkiUW2gHGHnRFnzpxpwu2FF16wSJIvYoiWYayBcAREH+dkNlWRNSIyiFCtXr267Zs8ebI1h44GkUUTaxg0aJCJQZo+I+DCbo2NGjUy4YTA89M6aQQNzIPIoQ/Rs549e1okzV+z1BgyZEjE9yKEEEIIIfIPipDlEUhr8yFaw4s/USR/81/u9+zZEyGUfAoXLuxKlizpLr744mAfaYwQjgyNGzfOnoWY4L6TJk0yAZEa4edAmTJlgnsSTUIo+mIMateuHXE+89m9e7dFyPz5kLZ49OjRiPkw9kTUjTGmIkWKRKwp64dwiiZ6rHyOjpC1aNHCUkGXLl0aV41d9+7dTeRdf/31JpLDc4wFYpqmgv5GZE8IIYQQQuQPFCHLIxAdCkdkqH8aNmxYsvMQQz7RaXBEn8L7/GgU0SmYMWOG69Gjhxs1apQJDwTSiBEjLNUwNWI9x79nPDAfxFGsOi4/yhS9BrmJhg0bupdfftmiiaSSpgU1dnfffbdbuHChReqILrL21JrFomjRorYJIYQQQoj8hwRZHuSyyy5zc+fONZMIIj2Jgtot6ssefPDBYF9a0Zu0IA2QiA41VL5YXLt2bbL5kLZ45plnmslFVkM07LfffrP6LT9lkdo0+qtFw1hbt24d8Zl00TDU5l100UXulltuMZFVt27dCLGK2Uc0pJWyPfLIIxZhI0UzJUGWEtsGNMiW9RJCCCGEEFmHUhbzIJ07d3bff/+9vchjDIFookYL98RYL//xQi0TJiHcizorDC18I4qMQloewqNNmzaWmojhRd++fSPOweSC2jacFTn+xRdfWO0Y7o1ff/21SzQ4NGIY8sADD1j0D2FGCmGsJtIYnFCfx3oQyaLuDMfLaDBNobbv5ptvdu+//36wH9FMPd1//vMfd+jQIWtWzfXM78svvzQRzBrHql8TQgghhBD5HwmyPAj1WLzII74wvKC2Cnt7aqAKFcr4V4pAwRGxefPmrmbNmu7gwYMR0bKMwHhojIwQqVGjhgkfrOvDFC9e3K1atcqcCXk+4gT7fWrIsioCRESKdSSaxTPvv/9+i9BFg5kG6YTUyU2bNs299tprZt0fC74DzieFcfXq1baP9E+MQKijI7JGLR/rStQNoYpZCgYnMu0QQgghhCiY/MnzPC+nByFEboRaOMRkuO9abgDbe8xDMPhQyqIQQgghRO4jPe9ripAJIYQQQgghRA4hQSbS1Uw5EdEiIk9vvPFGpu5BrVnY9j+8kRaY0jG2WFDrNWbMGJfVJGLuQgghhBAi/yCXRRE3Y8eOdbklw/Xyyy93mzdvjnmMerVYBh35IV1RCCGEEELkLyTI8hi//vprQpojZ4R4mh5n1xoguCpWrOgKMhclLXGFihZ3+ZG9Qxvl9BCEEEIIIbIFpSzmcq655hqzScfBD2v4Bg0auG3btpkzH+l3pUqVcq1atXIHDhyIuAYbdq459dRT7Zznn3/e/fLLL2aNT8NnxAxNiX1wbMTZ8O9//7uJHazhiYillrLIc7Cmf+yxx9xpp53mSpcubU2Pw+zatctdffXV7qSTTjJ3QhwHo6FPGW6DuERyH+zv9+7dm+y5uDPijMjYMsN3331njbWZJ/ONbkhN+iLQF4xImf8Z5s+fb73LmA/fR7h3GOcNGjTI2hHQxPqss85y48aNS/Z8erLx/fH88uXLuzlz5mRqPkIIIYQQIu8iQZYHmDp1qkXFsLofOnSou+6668xCnZ5hixcvdt9++60JmuhrEAz0zUKc0bz4zjvvtMbPGzduNLt8hNzhw4ft/D/++MOdffbZ1ndr+/btrl+/fq5Pnz5u1qxZaY4N8UE/r+HDh7uBAwcGoot7YinP2Dk+ceJE16tXr4jrjx8/biITkUhdGHNEaNInjEiYD728aN7MvRcsWJCp9UTgIQJXrFhhYmj8+PEm0nz83mtY4yOe/M80fUaAYWu/adMmGxNW/mFGjBjhqlSpYscff/xx9/DDDycTofR3a9q0qfVlowfbXXfd5Xbs2JGpOQkhhBBCiLyJbO9zOUShsM1ERAHNhxEuNG/2oXkyfa4QLPS24hoiXpwH/E26IeKIXlpAo+IyZcq4NWvWuFq1asV8NpE5zvMjOAiZH374ITCliH4OIFAQjAjHpUuXukaNGlkDZCJbgIAkOuTXZ7388ss2JwQJ0ShAiBEt4zkIR57LdV999VWm0zVp8EyEDaFKpAs+/fRT6302evRoiyqmVEOGmCWixZhjQYSM+4Qjj4gtvr+33347uG/Hjh3dhAkTgnNY/8suu8yEYSyOHTtmmw/34/su222WUhaFEEIIIXIhsr3PZ1SrVi34m6gKkZ2wa+D5559vx/bs2ROcRyNjH1wHS5YsaQ2kfUhjhHBkiPQ6nnXGGWfYfSdNmmQiKDXCzwFEnn9PRBbCwRdjULt27Yjzmc/u3bstQubPh7RFmkKH58PYE1E7x5iKFCkSsaasHwIwLTARqVevXqrnRM+Pz9HRr3jOCTNkyBD7P7S/saZCCCGEECJ/IFOPPAApgT4///yz1T8NGzYs2XmIIZ8TTjgh4hiRmfA+PxpFWiHMmDHD9ejRw40aNcoEAgKJ9DtSDVMj1nP8e8YD80EcRddxAcIw1hrkFJl1bswovXv3dt27d08WIRNCCCGEEHkfCbI8Bqltc+fOtfQ4Ij2JgtotUvIefPDBYF84QpURSN+jVos6LF8srl27Ntl8Zs6c6c4888w0w7mJgGjYb7/95j766KMgZZFUT1Ixo4Um6ZjR0UDqxjBGSYno+fGZdYje17p164jP1ASmRNGiRW0TQgghhBD5DwmyPEbnzp3NMREnP9/dkJQ/IlwvvPCCpSdmhHPPPdfqy6hNw3lw+vTpZmbB3xnl+uuvt5q2Nm3aWLSNyE7fvn0jzsHUgmM4K2IIgrEINWevv/66zY/PiYT6MQxDHnjgAavjQtRSNxYd/ULwIr6uvPJKE0O4VSYlJVnKYoUKFaw2DGFHbVjYqARhi7kJtWeYeWCSghlIGPbRR61OnToWGaSebfLkyemey7YBDbJFxAohhBBCiKxDNWR5DOqxeOkneoPhBbVVCApqoAoVyvjXiUDB9KN58+auZs2a7uDBgxHRsozAeDDGoFEzZh/t27c36/owxYsXd6tWrXLnnHOOPZ9oEvb71JBlldjAPZF1rFu3rj3z/vvvtwhdGFI3EVSkBvrRK0xMEFNvvfWWq1q1qpmXIKbCPProo+Z+yTWYlfzjH/8wF8kwAwYMMAFNxA0R/Nprr1lLACGEEEIIUfCQy6IQCYKoGuLYd2rMDa49QgghhBAi+5HLohBCCCGEEELkAVRDJvIc9D2jl1ksSI9MzQ0RV0chhBBCCCFyCxJkIt1EN4jOKLGaL8cDhhj0BMuIIMtK9u7dm+Y51KFRfzZmzJhsGZMQQgghhMjdSJCJdDN27FiXk6WHCK6KFStm6TMknIQQQgghRHYgQZZH+fXXX92JJ56YI8+mQLGgr0Fu4KKkJa5Q0eIuL7N3aKOcHoIQQgghRI4iU488AhGbLl26mIPf6aefblbq27Zts1qqP//5z65UqVKuVatW7sCBAxHXPPTQQ3YNfbQ4hx5mv/zyizU3Pvnkky3StGjRouAa7PSxnaf/GJEo+nYREYtOWQynGfKcrl27Bn3RSpcu7fr37x9xza5du9zVV1/tTjrpJLN4x1I+GppIN2vWzCz8uQ+9ycJpgP5zsc7Htp6xZYbx48db/zXGxNrccccdwXNWrlxp8yatks0fxyeffOJuvvlmc8th/a666qqggbY/PmztzzjjDDunY8eOJhzD0L+M7xJhy3f55JNP5mjEUQghhBBC5BwSZHmIqVOnWkSIPmRDhw61Plj0u6Lv1eLFi923335rgib6Gl766ZeFOOvUqZO788473RVXXOE2btxovcwQcocPH7bz//jjD2vGTL+t7du3u379+rk+ffq4WbNmpTm2EiVKuHXr1lljZJo8+6KLe9Lvi7FzfOLEiRHNlOH48eMmMhE5mHYwR4QmTZzDgoZmzZ999pnde8GCBRleS9YMEck4uR/rh2AEhFjt2rVdhw4d3P79+22jH9k333xj59Ao+p133nEfffSRu++++0xghce3Y8cO9+6771p/MRpcI9Ci14qG1HwnPIteZTT1Toljx46ZdWp4E0IIIYQQ+QP1IcsjEIXiRRwRBTQdRrgsWbIkOOfrr7824YDAOO+88+waIl6cB/xNVAZxRENi+M9//uPKlCnj1qxZ42rVqhXz2URzOG/OnDkxTT2inwM0gkYwIhyXLl3qGjVq5L788kuLbAECiOieb+rx8ssv25wQM0SkACFGtIznIBx5Ltd99dVXmU5VRCgRJWTNEIHx1JAhTGnozPqecMIJya5hfPPnz7dIHw2vAfHZs2dP60FBo2zu+91331mkzZ/n448/bs2mEcCxINoYLeqgbLdZSlkUQgghhMiFqA9ZPqVatWrB31u2bHErVqywKJK/nX/++XbMT6GDSy65JPi7cOHCrmTJku7iiy8O9pGqB4gEn3HjxtmzSLvjvpMmTTIRlBrh5wAiz78nIguh6IsxIAIVhvns3r3bxJE/H9IWjx49GjEfxp6IurH69eu7cuXKufLly1uE8JVXXgmihCmBsyMpirHEmE+VKlUCMebPE6t9RJoPwtcXY/45pHQiamPRu3dv+z+zv4XvJYQQQggh8jYy9chDkBLow0t+48aN3bBhw5KdhxjyiRYPCIHwPl8YkFYIRIB69OjhRo0aZUIBgTRixAhLNUyNWM/x7xkPzAcRiDCKBmEYaw0yA/Mi2khqIRE8UjOJRK1fv96icrHIKTt9UiTZhBBCCCFE/kOCLI9y2WWXublz57q//e1vVo+UKKjdor7swQcfDPaFI1QZoXLlyhbVoRbLF4tr165NNp+ZM2e6M888M82wbqJg3a6//nrbkpKSTIhRG+bXu0VHrIgCUv9FvVtKUTIifeFeaMyTaB8RQp9occs5mIsQwRRCCCGEEAULCbI8SufOnc0xsUWLFoG7ISl/RLgwiMjoyz3CgPoyatNwWpw+fbpFjfg7oyB4qGlr06aNRdvIqe3bt2/EOS1btrRjOCtitIGxCDVn1HoxPz4nEgxBPv/8czPpwIHy7bfftoie79yI0EU44a7op09SS/fMM8+4u+66y9IIyQtGTFEv519H3RsulU888YRdi9DjOurHfEj/7N69u3vggQcsSsc9iUiml20DGmSbeBVCCCGEEFmDasjyKNRjEc0iioPhBbVV2NsT5Qm//KcXRAIRoubNm7uaNWu6gwcPRkTLMgLjwbyDyBHipX379mZdH4a6q1WrVrlzzjnHnk9UDWFDDVlWiA7WCbGH8QjPwnwDV8QLL7zQjpO2iajFop+USUQU9XdE0EivrFu3rqVYIorD0bJ69eqZqEXosYa33HJLshYArVu3DtYCYf3www+7+++/P+FzFEIIIYQQuR+5LAqRIKLdJ3ODa48QQgghhMh+5LIohBBCCCGEEHkA1ZCJPAt9z+hlFouwsUYsSDsUQgghhBAip1HKosg1qXpY5fuNouMB0fXNN99kSJBVrFgxXWOL1Sg6p1DKohBCCCFE7iY972uKkIlMM3bsWJcTuh7BlV5hJYQQQgghRG5CgiyfgN06vbNyAtR/QV+DnOCipCWuUNHiLq+xd2ijnB6CEEIIIUSuQaYeeRRS6OhvhdX96aef7ho0aOC2bdtmNVX0zSpVqpRr1aqVO3DgQMQ1Dz30kF1D7y3Owbb9l19+cW3btnUnn3yyRZwWLVoUXIOtPvbz9CEjIkW/LSJi0SmL4TRDntO1a9egP1rp0qWTWb/v2rXLrOFPOukks5ZftmxZsjnSTLpZs2ZmUc996FFGb6/o52KhTxsAvxdYRhk/frxZ1jMm1uaOO+5I8dxDhw6ZfT3riGU/686cfOih1rhxYzteokQJs9On15l/LX3XsNNnTXnmlClTMjV2IYQQQgiRN5Egy8NMnTrVIkL0Ixs6dKj11Lr00kvdhg0b3OLFi923335rgib6GgTchx9+aOKsU6dO7s4773RXXHGFNSmmpxlC7vDhw3Y+zZJpyjx79my3fft2169fP9enTx83a9asNMeGEKG58vDhw63Zsy+6uCe9xhg7x+kB1qtXr4jrjx8/biITkYh5B3NEaN54440WCfP517/+5T777DO7N82eMwprhohknNyP9UMwpgRikGveeustt2bNGkvZbNiwoY0b6C927Ngx6622detWN2zYMBs/PPnkk7aWCN8dO3a4CRMm2HeSEtyHPOTwJoQQQggh8gdKWczDEFlB7MBTTz1lYmzw4MHB8RdffNGVLVvW7dy505133nm2r0qVKu6JJ56wv3v37m1CDjHQoUMH24fgQiB8/PHHrlatWtb0eMCAAcE9iZQhQBBk0WIvzCWXXOKSkpKCcT777LMmnurXr++WL1/uPv30U7dkyRKLbAHjDjsmzpw504TbCy+8YGYfQBSJaNm7775rwhEQfZyT2VRFGj9zr5tvvtlEYLly5Ww9Y0EkDCGGSETIwiuvvGJrjbEJApf7NW3a1Bp2Q/ny5SOexb0vv/xy+/y3v/0t1bENGTIk4jsQQgghhBD5B0XI8jDVqlUL/t6yZYtbsWKFRWH87fzzz7dje/bsiRBKPoULF3YlS5YMRAOQqgffffddsG/cuHH2LFLsuO+kSZNMVKRG+DlQpkyZ4J5EhRAvvhiD2rVrR5zPfHbv3m3iyJ8PaYtHjx6NmA9jT0TdGEIREYZwIkKIwPKjhNEw/iJFiriaNWsG+1hHUiY5BkTbEMlXXnmlCVMErg9RyRkzZphrI2mdq1evTnVsCGccevyNVE4hhBBCCJE/kCDLwxDRCffVomZp8+bNEZtfq+VDxCsM0afwPj8aRXQKEA49evSwOrKlS5faPak3C6cNxiLWc/x7xgPzQQRGz4do39133x1zDTIDwo+Uzddee83EI5FCoonY+WeE9u3bu88//9zEHSmLRMOeeeYZO0YkkBqzRx55xP373/929erVszVOiaJFi5pdangTQgghhBD5AwmyfMJll13mPvnkE0t/w5gjvGVGtPhpeQ8++KCl2XG/cIQqI1SuXNmiPPv37w/2rV27Ntl8EJNnnnlmsvlklasjUa/rr7/e0kCJaGEg8s4778Qc/2+//Wb1bz4HDx602jMMSnyIAnbs2NG9/vrr7tFHHzUDFR+ijW3atHEvv/yy9TYj6iiEEEIIIQoeqiHLJ2AiwQt/ixYtAndDUv6IcFFjRXpiRqD+a9q0aVbvRf3Y9OnT3fr16+3vjILooaYNQTJixAgzqejbt2/EObgQcgxnRYw2MBYhqoS4YX58TiQYghDRIpqIMyKOiET0Yjk3siaMi7q75557zqJrjz/+uDvrrLNsP+BkSSSMeeKqSDopQg6IvhH9w3kRww6e7R9LD9sGNFC0TAghhBAij6MIWT6BeiyiWdjUY3hBbRWiABOMQoUy/jU/8MAD5ojYvHlzq5kiEkS0LDMwnnnz5rkjR464GjVqWHof1vVhsJLHofCcc86x5yNYSJukhiwrRAjrhNjDqZJn4fxI+iKiKRYYjCCqMAGh/g2XRUScn6rJ94BI5l44QyLMsNUHat6oC6PODgGIWEY4CyGEEEKIgsefPN4khRB5BiKKpG1i8KEImRBCCCFE3n5fU4RMCCGEEEIIIXII1ZCJfAMNpMO9zMKQHlmsWLFUXR2FEEIIIYTIbiTIRAQvvfSS1Z5l1O49J8FaHmv8jAgyIYQQQgghcgIJMpFnwIYed8dNmzZZU+VoEFzY4seCPmgYidx2220JGw8tBhCvbP5nnCDXrFnjatWqFZzHcYTiu+++G5yTEjhPIorj4aKkJa5Q0eIur7B3aKOcHoIQQgghRK5DgkwknOPHjydrDF1QOOmkk1yvXr3cypUrYx6nZQAOjLB69WrXtGlT61/mF3sqiieEEEIIUbCQqUcOQp+rIUOGWNSHF/EqVaq4OXPm2DGiKUR1/vWvf1kqHjbwNGjm5T3M/PnzXfXq1U0InH766e72228PjtH/qnXr1tZXi+upr6LZchiiMVjLc5xrsbWP5s0337RGzTyjfPnybsCAAdYY2YdxTpgwwd1yyy3WhDrawj49MGZ6kNE4mTWh5xcW8+D3PqNBNc+85pprApFTv359mz9uNnXr1nUbN24M7klUCpgf1/mf45lbern//vutyTUW+LFgXqVLl7aNXnFA82t/X1Y1vRZCCCGEELkTCbIcBDFG02V6Xn3yySfukUcecffcc09EdIWGyaNGjXIbNmxwRYoUcffdd19wbOHChSYyGjZsaGl8iDf6evnce++9dt1bb71laXR0OOBcIliwbt066+3VpUsXS6m79tpr3VNPPZXMKANR9/DDD7vt27dbI2REXLTo6t+/v41l69atEWNML08++aQ9Z9GiRW7Hjh0m9BBa8OGHH9q/y5cvd/v377e+YfDTTz9Zqt/7779vYggRxzzZ7ws2QNhxnf853rmlB0Rjx44drc8YgjsR0Dwa69TwJoQQQggh8gfqQ5ZD8JJNhARxQWNhH5okHz582CItCCSO16tXz44RdWnUqJEZVBDRIWJGVOfll19Odn8iYTQjplk05wHRr7Jly7qpU6e6O++80919993WGwFh53PXXXe5xYsXB6Ye119/vT0fgeHD8x577DH373//2z4TdaJOavTo0ZleF6JsCLAXX3wx3TVkPgghGj2/+uqr1rg5pRqyeOaW3hoy/ibCV6FCBTdu3DjXqlWriBqyMHzmOyYqyHhTArFL5C6ast1mqYZMCCGEECIXoj5keYDdu3eb8CLV7s9//nOwETHbs2dPcN4ll1wS/F2mTBn797vvvrN/ecn3xVo0RJeIqNWsWTPYV7JkSVepUiU75p8TPg5hcQhbtmxxAwcOjBhjhw4dLNLE+H1Iq0wEnTp1cjNmzDDBhTCiziotvv32WxsTkTF++PzosbH/6quvUr0u3rmlF9ISe/To4fr16+d+/fVXl1kQjPyf2d/27duX6XsKIYQQQojcgUw9cgi/7xXRqbPOOiviWNGiRQNRFjbHIMoDfipcdhhAME6iM02aNEl2jCidD7VjiYA6N1wIiQYuW7bMBGfnzp3dyJEjU7yGdEWif2PHjnXlypWz9UNYpiWG4p1bRujevbsbP368bZmF+bAJIYQQQoj8hwRZDnHBBRfYSzZRHEwooglHyVKC6Bl1Y23btk12rHLlymZOQZ1YOGURUxCe7Z/D8TDUYIXB8IJrUrKTzwqIMCGy2K666irXs2dPE2QnnniiHfddCn1Iy0T4UDcGRJAOHDgQcQ7CNvq6rJwb0Tbq4Ug3JA0zK9g2oEGaIXAhhBBCCJG7kSDLIU4++WRLa8PIg4hXnTp1LB0NccFLNpGetEhKSrIIEvVK1H4hwIgsYbtO+t6tt95qKXiYVfC8xx9/3KJx7IeuXbu6K6+80sQO+5YsWWL1Y2FIu6MOCyfGO+64wxUqVMhS/bZt25bMACQR8Lxq1aq5Cy+80OrsFixYYMLRdyMkKsgYzz77bItikaLIXKdPn25pk+TrIuCio4fUdyFemS9CGOfJrJ4bdYDU1VHLFp0aKoQQQgghBKiGLAcZNGiQRVFwW0R03HjjjZbC6Nu7pwW277NnzzYXRWqurrvuusCJ0HcVRNwgOkjhw78FweanQdK8+Pnnn7dUPyz3ly5d6p544omIZzRo0MBEEcew1+caREY8gjEjEAWjZoro39VXX+0KFy5sNWVATdw///lPE5h//etfA2E5efJkM8Yg4oWJBkIT8RYGp0pSIDE1wTY/O+bGOvMdHz16NCH3E0IIIYQQ+Q+5LAqRj117hBBCCCFE9iOXRSGEEEIIIYTIA0iQ5WFoYpxa/6qcgsbIYSv58JbWsXig4XW4n1hW8Morr9iYSDskVTI8TurbhBBCCCGESARKWczjgoymw34T59wCfdII08aCkG1qx6Jrv2JB6JefbVaK0Z9++sn6m9ELjfFOnDgxOIZIC9eZRTeIzurvRimLQgghhBC5m/S8r8llsYBz/PjxiF5niQBRlZqwSulYvE2U+XFnNbhSsvF/IFwws9P2P14uSlriChUtni3P2ju0UbY8RwghhBCioKGUxTjhpRw3RBwQsVTHlXDOnDl27N1337WmzdiqY71evHhx6/1Fj6sw8+fPNzc/7NpPP/10d/vttwfHcAls3bq12bFzPQ2Sd+3aFXE9URcs2jnOtfQVi+bNN980t0GeUb58eWt8jB2+D+OcMGGC9caimfPTTz+d4TVhzC1btrS+YawJ9vM4O/rQD6xZs2YWyTrttNPMFXHv3r3JUg8ZA66JlSpVcn369IlpEc96Dxw4MOK68HczfPhwE01Y2rNG4XmlNY54oDVAmTJlXMmSJa1RNULWd7qkkTXtC1hbNn4P9Ibjv4j4++hH5kfTcF5s0aKFrT9tCMaNG5eusQghhBBCiPyDBFmcIMamTZtmqWuffPKJvYDfc889buXKlcE5ffv2NXv1DRs2WN3RfffdFxzDzh4RRfPiTZs2mXirUaNGcByRwXVY2K9Zs8ZS8jjXf/GngXO7du1cly5d3ObNm921116brFfWe++9Z6Lu4Ycfdtu3bzd7eERctOhCHDCWrVu3RowxvWDZz3MWLVrkduzYYUIPoQmMG1t5okyMi/5q1F9h7R+OhLEOCFcs6bGgR+Bh3R9ujM16f/zxx+7uu++OOQ5s8ocOHRqMh75fpUqVStc4UmPFihU2Hv6dOnWqrSkbvP7669YTDbG4f/9+2xDjY8aMseiav4+ecz4jRowwgcnvgN5wfF/MXwghhBBCFDxUQxYHNCgmsrJ8+XLr5+XTvn17d/jwYWsAjEDiOI2agX5fjRo1ckeOHLFoFS/pRKxefvnlZPcnEnbeeeeZWOA8IPpFzywEwJ133mlihIgLws6HZtA0SfbrlK6//np7PgLFh+dRB/Xvf//bPhOtobaJfluZhSgbAuzFF19MdoznIhgRajwTEEBEqd544w13ww03mAhl/F999ZX1H/Ohp1rTpk1NYAFRs3feecetXbvWPnMdc+Y+1HoRoXv22Wft+8jIOFKDZxHxQpDREw2IttFE2u+Plp4aMs6l5xwiNvw9kmfMbyal3x+bD+fy2yjbbZZSFoUQQgghciGyvU8wu3fvNuFVv379CLc9ImbhSA7NjH1Ib/MNLoColi/WokEsEFELp+qRGkcKH8f8c6JT+cLiELZs2WKRmvAYO3ToYBEaxu9DWmUi6NSpk4kSBBSib/Xq1RFjYd2ITPljQdTSJDm8ZhdffHGEGAOiZES5gP9e8Nprr9m+WLAuiJWU1jbecaQGroq+GPO/W/97zQjR3xuf/e85pegs/4f2N8SYEEIIIYTIH8jUIw5+/vln+5foFDU/YahZ8l/sw+YYfjSG+iagxio7xknNWJMmTZIdI0rnQ+1SIqDOjfopIjuk3CGKqK+i3oqxVKtWzezjoyGildpYqK/q1auX27hxo0UYqQFr3rx5zDGkta7xjiM1ok1P+G797zU7IOLZvXv3ZBEyIYQQQgiR95Egi4MLLrjAhBepdXXr1k12PJ5IC9Ez6qUwe4iGFDaMN6gTC6csUlvFs/1zOB7GT+HzwcyDa7LTERBR06ZNG9uuuuoq17NnTxNkjGXmzJnmqJhea3ZqslhnRBSCjMhkSs6MGIkgyljbWCmLmRlHvBDh+/3339Pcl9L3xme+35Tgt8cmhBBCCCHyHxJkcUC6G6YMGHkQGalTp47lg1LzxUt+uCdVSiQlJVkEqUKFClYzhAAjskQkCFGB8x/phRhx8DzMHojGsR+6du3qrrzyShM77FuyZInVX4Xp16+fu/nmm81l8I477rA6J1L2tm3blswAJBHwPKJPpPSRNogphy8sSDHEvIKxkkaJyCKahgkG6Y18Tg2uZ82o90qt3o3IH2vIPRFBrNF///tfMwLBBCWz44gH6sJWrVpl3yvCibo69hGdQyhi4IEzJhvwu8EVEqdIIouzZ8+OqA2Ml20DGqgPmRBCCCFEHkc1ZHGCVTkmE9TzIDpw6eMlGhv8eMAenRdvXBSpubruuuvMTdAHu3jEDYKKmiJqpxBsfrpcrVq13PPPP+/Gjh1rL/hLly51TzzxRMQzcBNEFHEMe32uQczEIxgzAgKIdDqif1dffbXVWflGF4gPRArikBRK1gyBRO1WPCICQUmUkNq3sMV9LPheHn30UROIPIf0Rr/GK7PjiAeEHjb6iG0/DZJIZ8eOHW0s7EOA+TBWHDUvvfRSE8r/+Mc/7LsTQgghhBAFD7ksCpGNxHJkzErXHiGEEEIIkf3IZVEIIYQQQggh8gASZAUc0urCNvnhLa1j+YGU5sdGI2khhBBCCCGykgJl6hFuKJwZsD2fN29emrVNeQHqn6hpoh4NkxDCq2+++aZ7+umnzcCE/mmxyGiqXP/+/W39U7pvdpPaOKJbHCQiHZFas+jP1CFu2rTJaguFEEIIIUTBokAJMgwx8nvJ3LvvvuuuvfZad+jQIfd///d/aZ6PHfxHH31kApNry5cvby6BGE1giJGdFvphAxTEyZgxYzI8r3jJyPxeeuklE12IeyGEEEIIIfKUIMPGHHe+nIDCutxATq5BLOijVqZMmaAHmsgbXJS0xBUq+v+s9DPD3qGNEjIeIYQQQgiRC2vIiHZ06dLFIgpEXrD3pi/WTTfdZHU6pUqVcq1atXIHDhyIuOahhx6ya0499VQ7B8v3X375xRor06eLyMaiRYuCa2jCi5056V80Cq5UqZJFxKJTFsNphjyH/l70ozrttNNc6dKlLaUuzK5du8zSnX5XNGmmb1Q0+/btc82aNbPIDfeh51U4Nc1/LmmAf/3rX21smYGeX/TeKlu2rPW9Yi0mT55szySKBKwbqZU8OzU4zlrT9JrzSbuL5tlnn3UXXXRR8JmUQ86dOHFisO/6669PZsOfGtOnT7dnIZLp3/XTTz8F41m5cqV9dzyDLbV5+b8vNu7Fbwwb/HgjoYwB6/nWrVvb75EWAbQmoJcZ3yP7sPXHpt6P1PEbxDHHH1/4N4NN/3333We/Uaz2J02aFPE8Wh1gd8/v6fLLL7dURSGEEEIIUXDJFlOPqVOnWkSIhrhDhw61Hly8lPKSS93St99+a4Im+hpernmBRTB06tTJ3XnnnRbF2bhxo7vhhhtMyPECDNQ70eSXXl/bt2+3nlR9+vRxs2bNSnNsJUqUcOvWrbNeUdRU+aKLe9K7irFzHAGCEApz/PhxE4tD61kAAKUDSURBVJm8gGMCwRx5iadPGZEwHxoEf/bZZ3ZveoVlBsTDa6+95v75z3+6HTt2WDNpnolAmzt3rp3Ds/bv359MlEbDcb9hMuevX78+2Tl169a1NUWkAIKJ7wZx4q/BmjVrTBzFG5FD1LEObNyP34U/Hvqw0SSb8bClNS++wyJFithvhf2kW77wwgtxrye92mgojThq1KiR/a5Y43vuucd+a/QX4zMij98fqZTU0Pnjo2m4z6hRowKh9eCDD9rvljEDjaLpM4ewJ00UIRe+NjUBTm1feBNCCCGEEPkEL4upW7eud+mllwafBw0a5N1www0R5+zbt49whvfZZ58F19SpUyc4/ttvv3klSpTwWrVqFezbv3+/XbNmzZoUn925c2evadOmwec2bdp4t956a8TYws+B6tWre7169bK/lyxZ4hUpUsT75ptvguOLFi2y586bN88+T58+3atUqZL3xx9/BOccO3bMK1asmF3vP7dUqVK2P7OwRjx/2bJlMY+vWLHCjh86dCjue44ePdorV65cxD7W5uGHH7a/mVvJkiW92bNn2+eqVat6Q4YM8UqXLm2f33//fe+EE07wfvnllzSflZSU5BUvXtz73//+F+zr2bOnV7NmzZjPTmtenFu5cuWI9ef7Y188MO977rkn2e/qySefDPbxG2Mfx2DKlCneKaeckua9GNOZZ57pTZgwwT4/99xzto5HjhwJzuEY9960aVOqa8Y50VvZbrO8cr0WZHoTQgghhBCJ5ccff7T3Nf5Ni2yJkFWrVi34e8uWLW7FihUR9uLnn39+EDnxIU3Mp3Dhwq5kyZLu4osvDvaRxgjfffddsG/cuHH2rDPOOMPuS7oYqXipEX4OUEvl35PoE9EZ0gx9iN6EYT67d++2CJk/H9IWjx49GjEfxp6IujFcAVkPolbZBWl5pG0SEcPIgmgZ0R8iN59++qlFuKpXr24mIPGmCbJesdY8I9SqVcvGGP6OSDUljTUewr8B/3eV1m8tnnsxJtJgw78njpOuGB5rWvTu3dtSJP2NFFkhhBBCCJE/yBZTD1ICfUjbaty4sRs2bFiy83gx9znhhBMijvFyG97nv4CTVggzZsyw9C9SxnjJ5YV/xIgRlmqYGrGe498zHpgPIvCVV15JdgxhGGsNMgP1cTkB6YgIXNIySTclZc8XaQiy9AjEzK55oon1u0rttxbvvRI1N+oE2YQQQgghRP4j210WL7vsMqsHIkpC3U+ioHaL+h4iNz7hCFVGqFy5skUjqBPyxeLatWuTzWfmzJlmH5/R3lzpgcgNL/iIIIw0ovGjcPFGh+IFwYXJCjV6fq0Y/y5fvtzWnl5miYI5RI8/tXlFi26+o3PPPdciiVlBrPHF+3vCzIToqR8li/49CSGEEEKIgkW2C7LOnTubY2KLFi0Cd0NS/ohwYcSQ0ZdoXsCnTZvmlixZYk6LvPhiUMHfGQXBc95557k2bdpYtA0zhb59+0ac07JlSzuGI59vjvHll1+6119/3ebH50SCkGU8OPlh6lGlShV7HmlxGKPgEkhUBrOMhg0bWkSNNMrMQqodDoevvvpqYEqCICMqyfMwxUjkHBFZuCv6KaCpzYu01O7du7sHHnjATDieeeYZi5RmFYyPyChGLaw/qZrxpGvefffd9vvBsIQ0ROY3cuTIDI9j24AG2fIfAYQQQgghRNaRLTVkYajHIqJChAGnRCI+RF6wjC9UKOPD4WUcR8TmzZu7mjVruoMHD0ZEyzIC46Fh8pEjR1yNGjVc+/btzbo+DC/iq1atMotznk8UBPt9oiBZ9bI8YcIEd8cdd9j8qL/jBZ+WAHDWWWe5AQMGuMcff9xqn7CDTwSIoauuusr+rVOnTiDSmCOugolKyQREHsIcN0LSPhFcqc0LB0T/O0LwP/zww+7+++93WQWR2I4dO9pvjfHhzhkPCMj58+e7rVu3Wton4ixW6q4QQgghhCg4/Alnj5wehBAZhShd1apVzYq+oECklp5rGHwoQiaEEEIIkbff17I9QiaEEEIIIYQQ4v8hQZYD4FQYtv0Pb6TqpXQsI7VgpPuldr+02gKklwsvvDDFZ8VyosypdU5EXZ0QQgghhBCZRSmLOQD1Tt98802Kx1Kztq9YsWK6nvXbb7+ZeQTgbvnUU0+5TZs2BccT7XaJwcjx48djHqP2K9x/LCfXOSNrmVtQyqIQQgghRP55X8t2l0Xx/3qJZZcYQGz5z0IQEYHLymfjhpgREI04YiIWqQlLDxiNYL5y2223JXSdGcvgwYPNtIX/M9EknJq1nj17mvumz9SpU92zzz7rPvnkE1tfWiFwzs033xycQ7+2a6+91oxKPv744wg3UQxtqIG79957MzxWIYQQQgiRN5EgE+mC6Fd08+P8CPb6TZs2dQ0aNLBUywoVKlhrAfqwPfnkk9Z7zneERIwReUQQsj4vv/yytUEYO3ZsMpfLzz//3NoztG3bNtNjvChpiStUNG27/dTYO7RRpschhBBCCCEyjmrIsgiaNw8ZMsSiPkRq6Fc1Z86cIFpCVIc+VljGY52Plfpnn30WcQ8s0qtXr25NhE8//XR3++23B8cOHTpkdu/0BuP6m266ye3atSvi+pdeesns+DnOtbQCiObNN9+0iA7PKF++vFnLk+bowzix2b/lllvM2j7a9j89MGb6tmEVz5rQO27KlCl2zO8Xhx08z/SbT9NLrn79+jZ/wr40qKbXWDjlEpgf1/mf45lbShw+fNgEE/3O3nrrLetHx/hop0DfsOeeey5o6ky/M/rQIcyIxtH2gDWilQO90WgsHuahhx5ySUlJ7tixYxleRyGEEEIIkX+QIMsiEGNEQiZOnGipbI888oi755573MqVK4Nz6EPFC/2GDRsstZBmzz4LFy40kYEoIHUO8UafLR/S27gOwbBmzRpHKSDn+vVbNFamHxoRms2bN1u6HFGcaNMLRB19u7Zv325CAxEXLbr69+9vY6F/VniM6YXIEs9ZtGiR27Fjhwk9hBZ8+OGH9u/y5cvd/v37rbE2/PTTT9YI+/333zcBhIhjnuz3BRsg7LjO/xzv3GJBc/EDBw5YY+9YkGIIr732mpmD0AMvmkcffdS+C+r2wiDUEIU0r44XxBt5yOFNCCGEEELkD2TqkQXwAn3aaaeZuKhdu3awn8bSRF9oWoxA4ni9evXs2Ntvv+0aNWpkRhREdIiYEdUh/S0aImHUMNFgm/OA6Bc1TtQz3Xnnne7uu++2uieEnc9dd93lFi9e7H744Qf7TOSH5/fu3Ts4h+chRP7973/bZ6JOiIjRo0dnel2IsiHAXnzxxQzXkBF5RBC9+uqrQY1WrBqyeOaWEjR67tWrl/v+++8tApkSRCURgQjeWBDRIyI4fvz4oIaMKCHpjn369LH0Rc5Jq4YMQUx0L5qy3WYpZVEIIYQQIheiPmQ5zO7du014kWoXtlknYrZnz57gvEsuuST4u0yZMvYvdUrAS74v1qIhukREjRQ6n5IlS7pKlSrZMf+c8HEIi0PYsmWLGzhwYMQYO3ToYCKD8fuQVpkIOnXq5GbMmGGCC2G0evXqNK/59ttvbUxExvhR84P++eef07Trj3dusUjPf6PIyH/PIHLJ9zVs2LC4zkdU8n9mf4tOgxRCCCGEEHkXmXpkAQgGIDp11llnRRwrWrRoIMrC5hhEefwIEKRmfZ/IcRJ5adKkSbJjROl8qB1LBESUsMUnGrhs2TITnJ07d7a6rJQgXZHoHwYZODiyfgjLX3/9NSFzi4XvoPjpp58mE7HR55FKyVhOPPHEiGNE4fgvI2E3Rh/ENKmTRMSiTT9iwZzZhBBCCCFE/kMRsiwAa3NeoIniYPQQ3kgrjAeiZ9SNxQLjCOqQqBPzQbRgCsKz/XPCx4EarDAYXnBN9BjZChXKmp8Ghh6ILNIHSdObNGmS7fcFze+//x5xPmmZXbt2tboxmk6zrtR3hUHYRl+XmbndcMMNllpJ6mIs/JRPUkARfr7JRxhEJuPCqTEWpJUyn1ipiEIIIYQQouCgCFkWQPNjXPcw8iDiVadOHUs1Q1yQchdPry6c+IggYbfOiz8CjMgStU2k72GrTgoeYoDnPf744xaNYz8gYq688koTBuzDqIL6sTD9+vWzOiycGO+44w4TKqT6bdu2LZkBSCLgedWqVTMhQp0d1vIIRzjzzDMtKsgYzz77bItikaLIXKdPn25pk0Sc6O8VHT3EWRHxynwRbNR9ZWZuRARfeOEFE03UvbGWCDmE4KxZs0xok3pJ9AzTEMZElCxse09ED8GZmgAfOnSo2epnlG0DGqgxtBBCCCFEXgdTD5F4/vjjD2/MmDFepUqVvBNOOME744wzvAYNGngrV670VqxYQeGRd+jQoeD8TZs22b4vvvgi2Dd37lyvatWq3oknnuidfvrpXpMmTYJj33//vdeqVSvvlFNO8YoVK2b33rlzZ8QYJk+e7J199tl2vHHjxt7IkSPt/DCLFy/2rrjiCjvnL3/5i1ejRg1v0qRJwXHGNG/evISsyaBBg7zKlSvbs0477TTv1ltv9T7//PPg+PPPP++VLVvWK1SokFe3bl3bt3HjRu/yyy/3TjrpJO/cc8/1Zs+e7ZUrV84bPXp0cN1bb73lVaxY0StSpIgdi3duabF+/Xpbc767okWL2jPuv/9+b9euXcnWuVq1ajbGEiVKeFdddZWNKUys7xxuuOEG2z9lypS4x/Xjjz/aNfwrhBBCCCFyH+l5X5PLohD52LVHCCGEEEJkP3JZFEIIIYQQQog8gASZSBcdO3aMsJIPb2kdyy288sorKY6T+jYhhBBCCCGyizybsohlOG53b7zxRqbuE6upsEgZ+qQRgo0F4djUjmHckdPfC7+b//73v2a6EQucEeMxXfEbNvP7S6kxdFahlEUhhBBCiNxNet7X8qzLIi/UeVRL5mkQVakJq/SIrpwC0YVrYl7noqQlrlDR4pm6x96hjRI2HiGEEEIIkc0pi2k1581KUJz/93//53KaRK9BTq6pEEIIIYQQIhcLsmuuucZ16dLFdevWzRrn0kOJvk433XST1d+UKlXKtWrVKqJxL9c89NBDdg39oTjn+eefd7/88otr27at9dAiWrFo0aLgGpr8tmvXzv3973+3nlOVKlVKlmJG6lk4nY3n0C/qsccec6eddporXbq0pZSF2bVrl7v66qutxxUNlJctW5Zsjvv27XPNmjUzscd96OG1d+/eZM99+umn3V//+lcbW2agh9agQYNc69atLZx5//332/7333/fXXXVVTZ/elkxN9bMZ/z48daji7mwpvTaiv6e2BCufFdPPvlkRETx0KFD9ky+k+LFi9t3yPr4vPTSS7YG9C+jVxjf74033uj2798fnPPuu++6GjVqWN8uzqUP2Jdffhkcf/PNN61BM2MsX768NUGmn1q88CzGxRpw/Zw5cyKOb9261V133XV2vGTJkrZ2NGoO/466d+9uY+M4v43wGkybNs320xMtDN8vv+OMQP8y1os5n3/++fY9+fA7IhXz9ddfd9dee62te5UqVdyaNWsy9CwhhBBCCFEAI2RTp051J554ojU5prEtL8SXXnqp27BhgzX1/fbbb03QRF+DKPjwww9NnHXq1Mma7l5xxRVu48aN7oYbbrAX4MOHD9v5NFOmOfDs2bPd9u3brclvnz59rClvWmNDHKxbt84NHz7cDRw4MBBd3LNJkyY2do5PnDjRmiyHoakvIhOR+N5779kcfSESjlzRhPizzz6ze9PcOLPQvJkX802bNplw2rNnjz2zadOm7uOPP3YzZ840gYbAAtYagcb8GAfrjtCMXosiRYrYmiNm//GPf5hYCAtL7vPWW2+ZIECoNGzY0NbAh++DsdGYedWqVdYQmYbXgLBCuNStW9fGyD0QRAgOYP0QfDRO5jukgTUiDyEbL6wFa0BD55YtW1qD7B07dtgxxCnfFYJy/fr19ltZvnx5sEYwatQoe+aLL75o6/f9999bXZoPv0FEG2sQrpFbuHChu++++1xGzEL4rTJHxjl48GCbA99FmL59+9o6Unt23nnnuRYtWqQqVBGM5CGHNyGEEEIIkU9IT4MzmvVeeumlEY1+aWwbZt++fdYE7bPPPguuqVOnTnD8t99+s+a5NDX22b9/v12zZs2aFJ/duXNnr2nTpsHnNm3aWGPh8NjCz4Hq1at7vXr1sr+XLFlijYO/+eab4PiiRYsiGh9Pnz7dGjnT1Nnn2LFj1liY6/3nlipVyvYnAhoZ33bbbRH72rVrZw2Iw7z33nvWMPnIkSPWMJpGx//73/9i3pO1oAFzeB6sA/uABtLM+4MPPgiOHzhwwOY5a9Ys+0yjYs7ZvXt3cM64ceNs7nDw4EE7/u6778YcQ7169bzBgwdH7GN9y5QpE9e6cO+OHTtG7KtZs6bXqVMn+5sGz6eeeqr3888/B8cXLlxoa/Sf//zHPvOs4cOHB8ePHz9ujbLDvxvud9NNNwWfR40a5ZUvXz5i7VIiKSnJq1KlSvC5QoUK3quvvhpxDv8fqV27tv1N02/m9cILLwTHP/nkE9u3Y8eOVJ/DOdFb2W6zvHK9FmRqE0IIIYQQOdsYOt2mHtWqVQv+JnKxYsUKiyJFQ5SH//oPl1xySbC/cOHCliZ28cUXB/tIufOjEz7jxo2zyAZRmSNHjliEqmrVqqmOLfwcKFOmTHBPIhak/pFm6FO7du2I85nP7t27LUIW5ujRozYfH8ZOpC1RXH755cnGQdSJiIsPGoUo3xdffOHq169vToCk8RFJY7v99tstBc6nVq1aQbTKnysRIyJCrAXRs5o1awbH+U5Iv/QjUMD9KlSoEHM9SeckykaUivFcf/31FhnlHH8ORBjDETGezVoSeQuPNSWivx8++46GjJOoIhFRH1ImWSOihqQMkvIYniNzZq3DaYsdOnRw1atXd998840766yzLKLGvMJrFw9E7PiNkGrLPX2IfJE2mtLv1F8v1pUUx1j07t3bUi99iJDxWxZCCCGEEHmfdAuy8Asw9TqNGzd2w4YNS3ae/6Lpu9qF4WU3vM9/+eVlGmbMmGEpXQgIXsIRSCNGjLBUw9SI9Rz/nvHAfBCcYSHkc8YZZ8Rcg0QQfT/G8cADD1haYjTnnHOOiUFSPanhWrp0qaXJUS9H6l4ijU5irWdYzEyZMsXGSMokaZVPPPGEpXEiBpkDNWOkiUaDWMotkG6LsKOejNTZTz75xFIW04tfu0Z9ZFgE+v8RIkxqv/1YFC1a1DYhhBBCCJH/yJTtPYYNc+fONWMKog+JgsgK9WUPPvhgsC8cocoIGC1g2EHUxBeLa9euTTYfhAXW7TnZ34lxUHeVmjU7601Uii0pKcmE2DvvvBMIoGjxylwxAUEcsBZEbjiHdYaDBw9aZAmzk/QKGjaiOIjnV1991QQZc+B+mbGXZ8zUoYU/8yxgDkSziEz5gpbfTaFChSzSR1SK75k5+vV1zPmjjz6ysYVp3769GzNmjEXJWM+MRJ+I8hJ9/fzzz63eLTvYNqCB+pAJIYQQQhRk2/vOnTubUQKmBERnEE248uGeSHpaRkE4YDjBvXbu3GnGCNw/M/CiTQplmzZtLJ0O0wnMFcLwIo35CM6KHCc9kCgUUaCvv/7aZReYjaxevdoMKkjRw/0Qx0LfsAIjkX/+8592DFdDojtEWMKOj6R6kuaGKHrttdfcM888YwYb/voyR1LrMLtgPe655x5L2WN/PLA2iDDMPBgDkTrGiVAConaMiygZUSdSDIl8EkWLF4w6SFvlN4DoxKDEXwO+KyJtfJ84fZI6i2EM5jB+CizzxXiG5s2ffvqpCXyaiUdz99132/dLdCsjZh4+zHXIkCH23TBmXCCJImKoIoQQQgghRMIFGREBohKIL9K9qK3C3p5oDZGKjEK6HpGe5s2bW/oX0ZtwtCwjMB4c9qhHw6qdqEi04x91TbgJkhbI8xEX1ARR95SdkQhqjFauXGkv9VjfExVC4Pj1b6wv1uk4XDJGHCMRXRdeeGFwDyJL/lwRzogT31IfEAqkZ958880W2SIV8e23306WppgSrBUiBxdEhC735jl8d0BtGcIRoUaNFlGz0aNHW+1begQOIo71QNwxRz+Cx/MR7PwHAe6P7X+9evXcs88+G1z/6KOPmkBDtPmpr9TaRUM0jXlQCxlupZBe+E3hZMna8v8FHCiJ4tG+QQghhBBCiFj8CWePmEdEnoU+ZBigkIYn4gMxh6AlupXbwdQDEfnjjz8qZVEIIYQQIo+/ryWu8EuIPAgNsklLZQs3cRZCCCGEECLXpyyK/9cAmVS3WBsGGikd81sFYLGemTS5sFsftVJ5AVwsU1qTcNpldkA6KCmN1OCR4hmGsaQ0zlhOnPFACmMinTCFEEIIIUTeRhGyTEJfK783VjS84BcrVizV68eOHRthJZ8IiPZkNzhtUj/Ilha33HJLMmt4n3hr2BLF3r17bYtV50VN3fHjx2Ne5xuHJGpNhBBCCCFEwUSCzDlrOp3RRs8IrsxYu0c3Dc6LaxAvmL8QycNcI7r5dm4kPQYkOcFFSUtcoaJpN9hOib1DGyV0PEIIIYQQIv0UKqimF9inE7nA5h5HQKzTb7rpJktHI/qBO9+BAwcirsFWnWtOPfVUOwebdPpgYfOPwECYLVq0KEKA4NJI9AXhhi09EbEw0SmLPAeb/ccee8yddtpprnTp0tb0OQz28vTWwvYd10GaMUdDz7VmzZpZehz3wc6eSFD0c3GaxL0xbJmfkfXE+v6RRx4xweU3O/bT89566y0bJ82NseOnhUH9+vVt7RGkuBHS6DoM98CxEFdEHBWx6uc+4dovrO9p2M3achx3w3jAPp9URdaPCOemTZuSnRPP74HfEBtzYC60Z/CjnSmtiQ8OkThkcv8bb7zR+uMJIYQQQoiCR4EUZDB16lSLCGHbT68qLOR5Saf/2eLFi923335rgib6Gl68eaFHnHXq1Mndeeed1lwZQYH1Py/uhw8ftvOpSzr77LOtnxaNnrGu79Onj5s1a1aaY6PZMU2Nhw8f7gYOHBiILu6JJT9j5ziW9/QtC0OaHSITkUiNG3P0X/yJhPn861//sj5l3BuL+oyCBT/zZJwIi7C4YC2GDRtm4op+ZDTd/umnn6xuix5ofsPqhg0b2v5o23u+g48//tiOI8CwuQfED2uKAKbH2YQJE+y7SYuff/7ZrP4RiDSJRuz26NEj4hx6lcX7e6BBN78HhDb9xphnPGsycuRIN336dGuzgEiNHoMQQgghhCggeAWQunXrepdeemnwedCgQd4NN9wQcc6+ffsIdXifffZZcE2dOnWC47/99ptXokQJr1WrVsG+/fv32zVr1qxJ8dmdO3f2mjZtGnxu06aNd+utt0aMLfwcqF69uterVy/7e8mSJV6RIkW8b775Jji+aNEie+68efPs8/Tp071KlSp5f/zxR3DOsWPHvGLFitn1/nNLlSpl+xNBuXLlvNGjR0fsmzJlio1r8+bNqV77+++/eyeffLI3f/78YB/XPfHEE8Hnn3/+2fYxV2jcuLHXtm3bdI/zueee80qWLOkdOXIk2DdhwgS796ZNm9L1e6hcuXLEGvMdsS+eNdm9e3ewb9y4cfZdpMTRo0e9H3/8Mdj8sZTtNssr12tBhjchhBBCCJE18M7G+xr/pkWBjZDRFNlny5YtbsWKFREueueff74d27NnT3AeDYp9cFAsWbKkNQCONnr47rvvgn3jxo2zZ5Fax30nTZpkEZHUCD8HypQpE9yTaFDZsmWDJtFA0+MwzGf37t0WIfPnQ9oiDa7D82HsWV03xv2j50O0qUOHDhYZI92P3gxErqLXJXwdEUPO89eB6CRNo+m3Rnrn6tWr4xoP68d9SVdMbf3i+T3Q7Dqcish9SCclVTU1SMGsUKFCzO83FkOGDLF18je+fyGEEEIIkT8osKYevOD7IAYaN25sqXXR8LKckgMgL+Phff7LOWmFgGAgFW3UqFH2so5AGjFihKUapkas5/j3jAfmgwiMZc2OMIy1BlkF9V3R9VOkKx48eNDS/DDOoLaM9QmnU6a1DtR3UaOFEyIplzR27ty5s6UCZpZ4fw8ZJda8UnPa7N27t+vevXtEo0GJMiGEEEKI/EGBFWRhLrvsMjd37lyzKacmKFFQu0V92YMPPhjsC0dYMgJGEBh2UJPkiwPqsKLnM3PmTKvXSqszeCIjYWlFhsLrQhNm6sKA+YQNM+IFcYm4Y7vqqqtcz5490xRkrB+1W0QL/ShZrPWL5/cQLaz9ejiip+ldk9RAsLIJIYQQQoj8hwSZcxZZwTGxRYsWgbshKX9EuDBp8F+w0wsv59OmTTNHPZwWEQI4DMbqeRUv119/vTvvvPNMhBBtI1rSt2/fiHMwv+AYzoqYSmAuQTQJownmx+dEg3jBoOKuu+4y8ZCawQbrwlrgcMj4EVJp9WuLBoMUooA0bz527JiZkiC20uLuu++29SJlksgTzpPRIi7e3wMplkSuHnjgATN1eeaZZywampE1yQjbBjTINsEthBBCCCGyhgJbQxaGeiyiNkQzcEqktgp7eyzbCxXK+BLxoo4jYvPmza0RMml64WhZRmA88+bNs6bTNWrUcO3btzfr+ugaJYTAOeecY89HqGC/T1Qoq17gEX6IG2qjwmmRsZg8ebLZ1hOJwpUSm3+ieemB6BOCinowWgAgkhBMaUE92Pz5893WrVvNRRFxFp2aGO/voXXr1sH3gIh7+OGH3f3335+hNRFCCCGEEAWTP+HskdODECKvQZ8xDEXGjBmT7c8mqoi5x48//qgImRBCCCFELiQ972uKkAkhhBBCCCFEDiFBJgwaSIdt3sMb6YApHWPLTQwePDjFceLMKIQQQgghRG5Cph4i6M+F/fqmTZuSHaNOKr2mGzlFx44dXbNmzWIeS+Qc3n333YTdSwghhBBCFFwkyERgkkEkrGLFii4vgyMiW5iXXnrJTDl++OGHHBuXEEIIIYQQsZAgEwnj+PHjyZoeZyW4IBLVy4wTZnZC42uEb6K4KGmJK1S0eLqu2Tu0UcKeL4QQQgghMk/eeJPNZ/zxxx9uyJAh1o+MNLoqVaq4OXPmBKlwiIx//etf1qcLC3uaS3/22WcR98C6vXr16tbcmP5Wt99+e3AMS3ks2U899VS7ntqpXbt2JYsaYYvPca7Fkj+aN99806zpeUb58uXdgAED3G+//RYcZ5wTJkxwt9xyiytRokQy+/304M974cKFZmXPM2vVquW2bdsWMWas59966y13wQUXWG8veoGlNl/u27ZtW3O44f5s/fv3j3udUoL1ok/ZWWedZddijf/aa68lc2Ls0qWLRef4jho0aBCxbjyP75+19b9/IYQQQghRsJAgywEQYzSMnjhxovvkk0/cI4884u655x63cuXK4Bz6Y9FkeMOGDa5IkSLuvvvuC44hWhBRDRs2tJovxBu9sHzuvfdeuw7hsmbNGkdnA84lggXr1q2zvmSIhc2bN7trr73WPfXUU8lMPhAr9Nbavn27e+6550wQRYsuxA1joa9XeIwZhSbRzJsG2vTuaty4cTBuOHz4sPUNo0Eza0f/stTmi5jFmh670f3799vWo0ePuNYpNejpRmNqvgtEI/3H6Kn24YcfRpw3depUi4rR14zv2+fJJ590TZs2dVu2bLFG3jSPpo4vFjS+xjo1vAkhhBBCiHwCfchE9nH06FGvePHi3urVqyP2t2vXzmvRooW3YsUK+sJ5y5cvD44tXLjQ9h05csQ+165d22vZsmXM++/cudPO/eCDD4J9Bw4c8IoVK+bNmjXLPvOchg0bRlzXvHlz75RTTgk+16tXzxs8eHDEOdOnT/fKlCkTfOY53bp18xKBP+8ZM2YE+w4ePGjjnjlzpn2eMmWKnbN58+Z0zZfrwnOL97r00qhRI+/RRx8NPtetW9e79NJLk53Hczt27Bixr2bNml6nTp1i3jcpKcmuid7Kdpvlleu1IF2bEEIIIYTIen788Ud7X+PftFCELJvZvXu3RXnq168fYclOxGzPnj3BeaTt+ZQpU8b+/e677+xfolr16tWLeX+iLETUatasGewrWbKkq1SpUhCB4d/wcahdu3bEZyI3AwcOjBhjhw4dLMLE+H1Iq0wk4XFgzhEeNxBtCq9NPPONRUavC9evDRo0yFIVGSfrs2TJEkuhDEMULa15+p9Tem7v3r0t5dLf9u3bl+b4hBBCCCFE3kCmHtnMzz//bP+S6kb9URhqonxRFjbHoObIrz2D7LCgZ5zUjDVp0iTZMeq7fKgdy06Yu78eOcmIESPc2LFjLR0SUcY6UCuGcUeYRKwPvws2IYQQQgiR/1CELJsJm1FgMR/eypYtG9c9iBBRNxaLypUrm/EGdWJhAwpMQXi2f074OKxduzbiM2YeXBM9RrasdDUMjwPTjZ07d9p4UyKe+RJVI6KV3utSg5qwW2+91Wr/MGXBmIOxZmSe/ufU5imEEEIIIfInipBlMyeffLKZSmDkQcSrTp06lobGCz7GE+XKlUvzHklJSZayWKFCBTODQFi8/fbbrlevXu7cc881oUB6IUYcPO/xxx+3aBz7oWvXru7KK690I0eOtH2k2i1evDjiGf369XM333yzOTHecccdJsJIY8TAItoAJJGQJknqYKlSpczYBHfC2267LcXz45nv3/72N4v4IWIRT7gixnNdanA9zoirV682l8Z//OMf7ttvv41LzMHs2bMt3ZPv/5VXXjEzkMmTJ6djpZzbNqCB/WaEEEIIIUTeRRGyHIDaI1z2cFskKnLjjTdaCiM2+PGAnTov9LgDVq1a1V133XUR7n5Tpkyx2iUEFbVJ+Egg2Pw0SOzkn3/+eUu5Q6AsXbrUPfHEExHPwKJ9wYIFdgx7fa4ZPXp0XIIxMwwdOtScHRn/f/7zH7P3T6t3V1rzxWmxY8eOrnnz5ubcOHz48LiuSw3Wiygi68T3Ubp06VSFYzSkg86YMcOindQPYpkfr5gTQgghhBD5hz/h7JHTgxCCfmHY75OmSK+x/Aw1cPPmzUuXgAuD7f0pp5xikVVFyIQQQgghch/peV9ThEwIIYQQQgghcggJMpEwSAsM2+SHt7SO5TZuuummFMc7ePDgnB6eEEIIIYTIJyhlMRXuvfde98MPP7g33ngjR1PUMgumFliys2Ul9EkjPBsLQrWpHTvzzDNzVQrjN998444cORLzGH3H2HIKpSwKIYQQQuRu0vO+JpfFVMD0Ij/o1fXr12dLvzBEVbSwij4eLxhx0ISaH3JOEN0jLqPktLAUQgghhBC5m1wvyGi0m5bLXlaRU2Ig0WuAs2Beg/niXJiXOX78eJbe/6KkJa5Q0eJxn793aKMsHY8QQgghhMgHNWRYiHfp0sXS6+hBha04va/8mh76U7Vq1codOHAg4pqHHnrIrqEnFOdg6/7LL7+4tm3bWo8pGhovWrQouIZGwe3atTOr+WLFirlKlSpZRCw6ZTGcZshz6OH12GOPWcoagqF///4R1+zatctdffXV7qSTTjIb82XLliWb4759+1yzZs0sYsJ96Hu1d+/eZM99+umn3V//+lcbW2ZTFseMGWN/E/FjzPQXo0E192dO8TB+/Hjrv8XcWGP6k2XmO0grskSqJymj8NJLL9l6kT7qj4HfBmvpQ580olE8i9AwlvYbNmxI81nx3BsmTJhgvd8Qi3wn06dPjzjOeDnnlltusYgkPc4YD7AmHOe7BXqYXXzxxfbbo+/a9ddfb2slhBBCCCEKFrlOkMHUqVPtpZdmyfSlos/WpZdeai/XNDCmAS+CJvoaBBz9uBAGnTp1cnfeeaelvm3cuNHdcMMNJuQOHz5s59OU+eyzz7Z+Xtu3b7dGyH369HGzZs1Kc2y8bK9bt876WdHI2Bdd3LNJkyY2do5PnDjRmjVHR0142Uc0vPfeezZHhCa9yIiE+dDE+LPPPrN70w8sUcydO9f6idEMGfGICEEYpAVrj3BjvoyL7wHhmZnvIL1wHSKVvl2sG2KNxtg+LVu2tO+UFM2PPvrIGj3H01MsnntTA0h/tEcffdT+A8EDDzxgQnPFihUR90Hs3n777W7r1q3Wa4z1BtaMFExEP/+2aNHC3XfffW7Hjh0mPvndpJQee+zYMctDDm9CCCGEECKf4OUy6tat61166aXB50GDBnk33HBDxDn79u3jzdX77LPPgmvq1KkTHP/tt9+8EiVKeK1atQr27d+/365Zs2ZNis/u3Lmz17Rp0+BzmzZtvFtvvTVibOHnQPXq1b1evXrZ30uWLPGKFCniffPNN8HxRYsW2XPnzZtnn6dPn+5VqlTJ++OPP4Jzjh075hUrVsyu959bqlQp258IypUr540ePdr+HjVqlHfeeed5v/76a7ruMXfuXO8vf/mL97///S/m8UR9Bz4rVqywcw8dOmSfp0yZYp/Xrl0bnLNjxw7bt27dOvt88skney+99FK65hXvva+44gqvQ4cOEdfdeeedXsOGDYPPnN+tW7dU5wEfffSR7du7d29c40tKSrLzo7ey3WZ55XotiHsTQgghhBDZw48//mjva/ybFrkyQkaqWTgNjShE2Hb8/PPPt2N79uwJzrvkkkuCvwsXLmxpYOHIDyl0vhOgz7hx4+xZ1Fhx30mTJrmvvvoq1bGFnwNlypQJ7km0o2zZspYG6FO7du2I85nP7t27LULmz4e0xaNHj0bMh7FnRe0cESvcA8uXL28pdUR+fvvttzSvq1+/vitXrpxdR5TrlVdeSRbpysh3kB6KFCniqlevHnzmd0CqIesO3bt3d+3bt7f0PyKr4fXM7L3598orr4y4hs/+cZ/LL788zWdVqVLF1atXz9aG74PUTkw/UqJ3797m0ONv0amUQgghhBAi75IrBVnYEfDnn392jRs3dps3b47Y/Fotn+jUNOp1wvv47KcVwowZM1yPHj2sjmzp0qV2T1LQwmmDsYj1HP+e8cB8EIHR89m5c6e7++67Y65BIkEwkj5HPRj1Sw8++KCtY1oGFAhI0g5fe+01E6GkeCIs/BqvjHwHiYZ0wU8++cQ1atTIvfPOO1bDh+DMTuL53hCrpKJST8cYn3nmGatJ++KLL2KeT60fNXHhTQghhBBC5A9ypSALc9lll9lLNsYUmEKEt8yIFuqEqG1CkFCfxv3SE1GJReXKlS16QY2Qz9q1a5PNBzGJBXz0fLLL1REhhsj95z//afVLa9assZqneKJIRJ+onfv444/NiAThk10QyQubdCAsEYSsu895553nHnnkERPZ1GVNmTIlIffmX34zYfiMoEoNP8qJiUwYxCkRNurMNm3aZOdlt3gUQgghhBA5T663ve/cubOldGGC4LsbkvJHhOuFF16waENGwE0PA4clS5aY0yKOeZhB8HdGQawgCNq0aeNGjBhh5gt9+/aNOAfjCY7hrIhBBiYUX375pXv99ddtfnzOSnAURBzUrFnTFS9e3L388ssm0EhHTA2MRT7//HOLpuEY+Pbbb1ukK7MOkOmBaBtmIQhJxCFunLVq1XI1atSwNMyePXua8yPf4ddff23fZ9OmTTN9b+DeGMkg3vme58+fb9/Z8uXLU70v64r4Yv0aNmxoa81/YMC0BZMThDkGMP/9738jhGU8bBvQQNEyIYQQQog8Tq6PkFGPRSQCEcELLHU3WKtT31OoUMaHj0seEZTmzZubODl48KBFyzID4yHKgTjgRZ56Jpz7wiCCVq1aZbbzPJ+XcNImqSHLjpdr1g2BS3SGmi8EBeKCeq+0rkOA4HjJmHGQJH3xwgsvdNkFa4drJamdjJ/6u5kzZ9oxhDnfYevWrU0UI55olUAEKrP3BtoQ4JA4cuRImzMulUTfsPtPq8E0Y8DxkRo6hB7fM78BBBpjfeKJJ9yoUaNsvEIIIYQQomDxJ5w9cnoQQsQT2UOIh2vW8sK9swIir6S3YvChCJkQQgghRN5+X8v1ETIhhBBCCCGEyK9IkOUBaCAdtv0Pb6TqpXSMLRHPSM994qVjx44pPotjiYRUwJSeNXjw4IQ+SwghhBBCiPSglMU8ADVp33zzTYrHMIpICdwb430GhhZPPfWUuf5l9D7xQi8yQrmxIKyL2UWiYO2YXywwiWHLSyhlUQghhBAi/7yv5XqXRfH/bOoTLYhiPQPTCSJuWf0sQHAlUnSlZayRFXViOV17dlHSEleoaPG4zt07tFGWj0cIIYQQQqQfpSyKhJJWg+lEg/tmVjWaFkIIIYQQIquRIMshEBFDhgyxnllEp6pUqeLmzJljx2jWTO8qelVdfvnlZslOE2uaFYfBrr569erupJNOcqeffrq7/fbbg2OHDh0yC3h6hnE9dVQ0pI6O8GC/z3GuxTY+mjfffNOaWfOM8uXLm4U7TZR9GOeECRPcLbfcYo26o23+04M/74ULF5olP8+kF9i2bdsixowF/1tvvWVNmYsWLeq++uqrVOfLfdu2bWshY+7P1r9//7jXKaWxpnTPY8eOmYV+2bJlbXxEHCdPnhz3HIUQQgghRMFBgiyHQIzRmJp+XjQKfuSRR9w999zjVq5cGZxDU2n6U23YsMGaFd93333BMV7oEVH0sqLmC/HmNzGGe++9165DuKxZs8ZRKsi5fgSLZsT0P6Mv1ubNm921115r9WPRRh+IlYcfftht377dem8hiKJFF0KEsWzdujVijBmFJszMm8bOZ5xxhmvcuHFE5O3w4cNu2LBh1hictSP1MbX5ImbHjBlj+bv79++3rUePHnGtU0qkdk/WjB5t1OTt2LHD1i3aGCWtOYZB4JGHHN6EEEIIIUQ+AVMPkb0cPXrUK168uLd69eqI/e3atfNatGjhrVixAqMVb/ny5cGxhQsX2r4jR47Y59q1a3stW7aMef+dO3fauR988EGw78CBA16xYsW8WbNm2Wee07Bhw4jrmjdv7p1yyinB53r16nmDBw+OOGf69OlemTJlgs88p1u3bl4i8Oc9Y8aMYN/Bgwdt3DNnzrTPU6ZMsXM2b96crvlyXXhu8V6XGrHu+dlnn9k9ly1bluE5RpOUlGTXRG9lu83yyvVaENcmhBBCCCGyjx9//NHe1/g3LRQhywF2795tUZ769etHWLATMduzZ09wHiltPmXKlAncCYGoVr169WLen6gMEbWaNWsG+0qWLOkqVapkx/xzwsehdu3aEZ+3bNniBg4cGDHGDh06WDSI8fuQVplIwuPAATE8bjjxxBMj1iae+cYio9elBt8Lxih169bN1BzD9O7d21Ij/W3fvn0ZGpsQQgghhMh9yGUxB/j555+DtEMcAMNQc+SLshNOOCHYT90R+AYWqVndJ3Kc1Iw1adIk2TFqn3yoHctOmLu/HrmNrPhe+E2wCSGEEEKI/IcEWQ4QNqOIFUkJR8lSgggRdWMYS0RTuXJlM96gToxaJ8CwA1MQnu2fw/Ewa9eujfiMmQfXZIcNfvQ4MBvxTTd27txp402JeOZLVA1HxvRelxqx7nnxxRebaKYW8Prrr0/YHGOxbUAD9SETQgghhMjjSJDlACeffLIZQGDkwct7nTp1LBXtgw8+sBfscuXKpXmPpKQkS1msUKGCu+uuu0xYvP322+bud+6557pbb73V0gsxlOB5jz/+uEXj2A9du3Z1V155pRs5cqTtW7JkiVu8eHHEM/r16+duvvlmEw533HGHK1SokKUx4ggYbQCSSEiTJHWQvmgYm+Agedttt6V4fjzz/dvf/mYRP0QsjpY4KsZzXWrEuif72rRpY+YmmHqw/8svv7RU02bNmmV4jkIIIYQQIp+SLVVtIhl//PGHN2bMGK9SpUreCSec4J1xxhlegwYNvJUrVwbGD4cOHQrO37Rpk+374osvgn1z5871qlat6p144one6aef7jVp0iQ49v3333utWrUy0wkMI7g3JhZhJk+e7J199tl2vHHjxt7IkSOTmVQsXrzYu+KKK+ycv/zlL16NGjW8SZMmBccZ07x58xKyJv6858+f71144YU2L563ZcuWVI004p1vx44dvZIlS9ozMMqI97rUiHVPjFceeeQRMz9hDhUrVvRefPHFuOeYyCJRIYQQQgiR/aTnfe1P/E9Oi0Ih/B5d2O+TwkevsfxIIuaI7f0pp5xiUVWlLAohhBBC5D7S874ml0UhhBBCCCGEyCEkyERCwb4dK/mwVb6/dezYMeZ+/1gYXBTfeOMNl5PcdNNNKY538ODBcd2DRtr5NdonhBBCCCEyj1IWRcJ7rBGijRWaZR/HYsGxM888M0KQzZs3L0eNLr755ht35MiRmMfoHcaWFlz/008/RcwtsyhlUQghhBAid5Oe9zW5LOZDfv31V7NkzwnSsshPpDBJjUSsQXSPuIz2JcuqnnEXJS1xhYoWT/WcvUMbZcmzhRBCCCFEYlDKYj7gmmuucV26dHHdunUz+/QGDRqYNb2fcoe1eqtWrdyBAwcirnnooYfsmlNPPdXOef75590vv/xivc2wgEdcLVq0KLiGnlvt2rVzf//7301kkJ44duzYiLHce++9EVEtnoPF/mOPPWYRpdKlS7v+/ftHXLNr1y539dVXW7Np+n8tW7Ys2Rz37dtntvGk/3EfbOn37t2b7LlPP/20++tf/2pjywzjx483W3zGxNpg+w8LFiywMfj9xzZv3mzRPOzyfdq3b+/uueeemCmLzL1q1apu+vTpZpHPfzmhbQFRNCGEEEIIUfCQIMsnTJ061SJC9DIbOnSou+6669yll17qNmzYYP3Fvv3224g+WP41CLgPP/zQxFmnTp3cnXfeaU2SN27c6G644QYTcocPH7bz6Zl29tlnu9mzZ7vt27dbn7I+ffq4WbNmpTm2EiVKWAPm4cOHWw8uX3RxzyZNmtjYOT5x4kTrpRbm+PHjJjIRie+9957NEaF54403WiTMh35gNHXm3ginjMKaISIZJ/dj/RCMcNVVV5l42rRpk32mATRriHuiD/sQoilB42/q4xgjG+fznQkhhBBCiIKHUhbzCURzEDtA02bEWNh44sUXX3Rly5Z1O3fudOedd57to2nxE088YX/37t3bRAHigkbJgOCaMGGC+/jjj12tWrXcCSec4AYMGBDck0jZmjVrTJBFi70wl1xyiTWy9sf57LPPmniqX7++W758ufv000+tMTWRLWDcRPd8Zs6cacLthRdesGgUTJkyxSJPCCGEIyD6OCezqYpfffWV3Yum2IhAGnWznkBEiwgXz7388svtXxp8sy40iSZPmDq6unXrpnh/5kLkjHsDopf1ILoXi2PHjtnmk1IdnhBCCCGEyHsoQpZPqFatWvD3li1b3IoVKyJcAc8///wgOhMWSj6FCxd2JUuWdBdffHGwj1Q9+O6774J948aNs2edccYZdt9JkyaZgEmN8HOgTJkywT137NhhQtEXY1C7du2I85kPIgcB48+HtMWjR49GzIexJ6J2DqGICCtfvryJpVdeeSWIEgJiCyGGHw4ROyJ8lStXdu+//75Fu5gLwjMlSFX0xVj0esRiyJAhJgT9jfUSQgghhBD5A0XI8glEdHyI1DRu3NgNGzYs2Xm8/PsQ8QpD9Cm8z49GEdGBGTNmuB49erhRo0aZaEJUjBgxwlINUyPWc/x7xgPzQQQijKJBGMZag8zAvEjZRHQtXbrUIoXUfq1fv96icqQjEnFEKDI3xC77OJ+Gz6lFxzKyHkQvu3fvHhEhkygTQgghhMgfSJDlQy677DI3d+5ci8TQEyxRULtFfdmDDz4Y7AtHqDICkSUMO/bv3x+IxbVr1yabD2mLODRml80763b99dfbRrolQuydd96xaJhfRzZ69OhAfCHISPlEkD366KMJHUvRokVtE0IIIYQQ+Q8JsnxI586dzTGxRYsWgbshKX9EuKixIj0xI5CGN23aNKv3on4Mp0CiRvydURA81LS1adPGom1Ef/r27RtxTsuWLe0YzooYbWAs8uWXX7rXX3/d5sfnRILRxueff25GHjhQvv322xbB8p0b2UcaJhE76uGAc6mjw4AkrQhZotg2oIH6kAkhhBBC5HFUQ5YPoYaJaBbW7BheUFuFvT1RnkKFMv6VP/DAAxYhat68uatZs6Y7ePBgRLQsIzAeGkDTQLlGjRpmGR9tblG8eHG3atUqd8455wT1WtjvU0OWFYKEdULs4VTJs3B+fO2119yFF14YnIPoYn19N0VEL5b92Ppn1nJfCCGEEEIUHP7k4UwghMiXnd+FEEIIIUTufl9ThEwIIYQQQgghcgjVkIl8CXb04V5mYUiPLFasWKqujkIIIYQQQmQHEmT5HBoQUz/2ww8/uIIETZs3b96cIUEmhBBCCCFEdiFBJvKl2ERwVaxYMUfGJYQQQgghRLxIkIk0wco9uplxVoJ7Ic2SM+MIWRC4KGmJK1S0eMxje4c2yvbxCCGEEEKI9KM33gRCr6ohQ4ZYXy4iNFWqVHFz5syxY++++66JjH/961+WToeVO02WP/vss4h7zJ8/31WvXt2ddNJJ7vTTT3e33357cIymw61bt7Y+WFxPjdSuXbuSRY2wh+c412JNH82bb75pzZZ5Rvny5d2AAQPcb7/9FhxnnBMmTHC33HKLK1GiRDIb+vTgz3vhwoXWu4tn1qpVy23bti1izFjNv/XWW2YdTxPkr776KtX5ct+2bduacw33Z+vfv3/c65QS3KNq1aoR+8aMGWNNtn3uvfded9ttt7mRI0daM+uSJUta7zeEqw892vieTz75ZLPCv/vuu913330XHGeM9Fc744wz7LdCj7cpU6ZkeJ2FEEIIIUTeRIIsgSDGaJxM36pPPvnEPfLII+6ee+5xK1euDM6h6fGoUaPchg0bXJEiRdx9990XHEO0IKIaNmzoNm3aZOKN3lxhIcB1CJc1a9Y4OhZwri8E1q1bZ/25unTpYvVT1157rXvqqaeSmV0gVh5++GG3fft299xzz5kgihZdCBPGsnXr1ogxZpSePXvavGkkjQhp3LhxhIA5fPiwGzZsmDWuZu3OPPPMVOeLmEUoYSO6f/9+23r06BHXOiWCFStWuD179ti/U6dOtTVk8+FZgwYNclu2bHFvvPGG27t3r43L58knn7T1X7RokduxY4cJYAR4LI4dO2bWqeFNCCGEEELkE+hDJjLP0aNHveLFi3urV6+O2N+uXTuvRYsW3ooVK+j35i1fvjw4tnDhQtt35MgR+1y7dm2vZcuWMe+/c+dOO/eDDz4I9h04cMArVqyYN2vWLPvMcxo2bBhxXfPmzb1TTjkl+FyvXj1v8ODBEedMnz7dK1OmTPCZ53Tr1s1LBP68Z8yYEew7ePCgjXvmzJn2ecqUKXbO5s2b0zVfrgvPLd7rUiMpKcmrUqVKxL7Ro0d75cqVCz63adPGPv/222/BvjvvvNPWOiXWr19v4/rpp5/sc+PGjb22bdumOR5/TFwbvZXtNssr12tBzE0IIYQQQuQcP/74o72v8W9aKEKWIHbv3m1Rnvr167s///nPwUbEjEiKD2l7PqS7gZ/KRlSrXr16Me9PFIWIWs2aNYN9pMpVqlTJjvnnhI9D7dq1Iz4TsRk4cGDEGDt06GARJsbvQ7pdIgmP47TTTosYN5x44okRaxPPfGOR0evSy4UXXugKFy4c8V2GUxI/+ugjiwKSPkraYt26dW0/qZjQqVMnN2PGDEuPfOyxx9zq1atTfFbv3r0tNdPf9u3bl7B5CCGEEEKInEWmHgnC711F2uFZZ50VcYyaKF+Uhc0xqHvya88gO6zYGSc1Y02aNEl2jPouH2rHshPm7q9HToKRyP8LEv7/xEp1jDY5Yez+9/jLL7+4Bg0a2PbKK69YiiZCjM+//vqrnUNd25dffunefvttt2zZMhPi1KFRlxYNvx82IYQQQgiR/1CELEGEzSiwWw9vZcuWjeseRIioG4tF5cqVzXiDOjEfDDswBeHZ/jnh47B27dqIz5h5cE30GNmy0tUwPA4MLXbu3GnjTYl45ktUDUfG9F6XGoin//znPxGiLKV+Zinx6aef2jOHDh3qrrrqKnf++edHRM/Cz2rTpo17+eWXrR5u0qRJ6XqOEEIIIYTI+yhCliBIS8NUAiMPIiV16tSx9LIPPvjAjCfKlSuX5j2SkpIsUlKhQgV31113mbAggtKrVy9z4bv11lstvRAjDp73+OOPWzSO/dC1a1d35ZVXWpSFfUuWLHGLFy+OeEa/fv3czTffbKl0d9xxh4kw0hhxPYw2AEkkpEmSOliqVCkzNsHAAqfClIhnvjgfEvFDxOJoiaNiPNelxjXXXOP++9//uuHDh9v6sH4Yb/Adxgtri1h85plnXMeOHW1tMfiI/h6qVatmqY+YdixYsCBVgRqLbQMapGtcQgghhBAi96EIWQLhpRv3PNwWebm+8cYbLYURG/x4QAzMnj3b3AGpLbruuuvchx9+GBzHFp2XeAQVNVlEcRBsfvocdvLPP/+8Gzt2rAmUpUuXuieeeCLiGaTN8fLPMez1uWb06NFxCcbMQLQIZ0fGTwQKe39ES2qkNV+cFhE8zZs3t2gTIiqe61KD7238+PFu3Lhxtoasv+/eGC+MBcdFvkuicsw9OhWRuVMbRlT06quvtno0asqEEEIIIUTB4k84e+T0IET+hX5h2O+TpkivMZF5sL0/5ZRTLAKrCJkQQgghRN5+X1OETAghhBBCCCFyCAkykSakBYZt8sNbWsdyG7gbpjTewYMH5/TwhBBCCCFEAUMpiwUMapu6devmfvjhh7ivwSGQsGssCMGmduzMM890uYlvvvnGHTlyJOYx+qOx5XaUsiiEEEIIkX/e1+SyKNIEUZWasMotoisesRndIy4ruffee20sb7zxRrY9UwghhBBC5C0kyES6oVFyPI6FiYJeYzRezso+aXmRi5KWuEJFiwef9w5tlKPjEUIIIYQQ6UdvuFkI/ciwwMf2vlixYmajPmfOnMB9EJFBD63LL7/cemhh404D4zDYw2NPf9JJJ1nvrttvvz04hnNh69at3amnnmrXUx+1a9euZFEj+mJxnGtpWBzNm2++aQ2jeUb58uXdgAEDrAeaD+OcMGGCu+WWW1yJEiXc008/neE18edNOwAs33km1vv06gqPGUdG7P/DDbdTmy/3bdu2rYWFuT9b//79416nlPjyyy9d48aN7VrmTt8wLPR9PvnkE7PXJxRNzzMaQe/Zs8eePXXqVFtbfzyMce/evfY3Fvd838z/oosucitXrszwmgohhBBCiLyLBFkWghibNm2amzhxor240zT6nnvuiXj5pknyqFGj3IYNG1yRIkXcfffdFxxDtCCiGjZs6DZt2mTirUaNGhEpcVyHcFmzZo312+JcIliwbt06165dO9elSxe3efNms5+Pbv783nvvmVihR9j27dutmTKCKFp0ITAYy9atWyPGmFF69uxp816/fr317UL0+OOGw4cPu2HDhrkXXnjB1o60yNTmi7gZM2aMCaP9+/fb5vcPS2udUqNz587WuHnVqlU2d8aEAYhfj0YPMQTjO++84z766CNbG8Qsz27WrJn1ovPHwxjD83/00Ufte6VXGvOPJZaB55OHHN6EEEIIIUQ+AVMPkXiOHj3qFS9e3Fu9enXE/nbt2nktWrTwVqxYgZmKt3z58uDYwoULbd+RI0fsc+3atb2WLVvGvP/OnTvt3A8++CDYd+DAAa9YsWLerFmz7DPPadiwYcR1zZs390455ZTgc7169bzBgwdHnDN9+nSvTJkywWee061bNy8R+POeMWNGsO/gwYM27pkzZ9rnKVOm2DmbN29O13y5Ljy3eK9LjYsvvtjr379/zGO9e/f2/v73v3u//vprzONt2rTxbr311oh9X3zxhY1n6NChwb7jx497Z599tjds2LCY90lKSrJrorey3WZ55XotCDYhhBBCCJE7+PHHH+19jX/TQhGyLGL37t0W5alfv36EtToRM1LafEjb8ylTpkzgaghEterVqxfz/jt27LCIWs2aNYN9JUuWdJUqVbJj/jnh40A0JsyWLVvcwIEDI8bYoUMHi+gwfh/SKhNJeBw4G4bHDSeeeGLE2sQz31hk9Dqfrl27WlTxyiuvdElJSe7jjz8OjvH9kKKYkXq68PwZH+ub0nh69+5tqZj+tm/fvnQ/TwghhBBC5E5k6pFF/Pzzz0HaYbSzHyluvigLv8xTW+TXngF1Z9kxTmrGmjRpkuwY9U0+1E9lJ8zdX4+cpH379q5Bgwb2PS5dutTSUEm1fOihh7Ll+/F/L2xCCCGEECL/oQhZFhE2o6hYsWLEVrZs2bjuQYSIurFYVK5c2WqVqBPzoQYJUxCe7Z8TPg5r166N+IyZB9dEj5EtK10Nw+PAdGPnzp023pSIZ75E1XBkTO91acH3RZPr119/3eq+nn/++eD7oQYvpVq0WOOJNX/GR/1ZavMXQgghhBD5E0XIsggc9zB2wMiDiFedOnUs3eyDDz4w44ly5cqleQ9S5EhZrFChgrvrrrvsxR2Hv169erlzzz3X3XrrrZZeiBEHz3v88cctGsd+P92OVLuRI0faviVLlrjFixdHPKNfv37mEogT4x133GEijDRGXA+jDUASCWmSpA6WKlXKjE1wkLzttttSPD+e+f7tb3+ziB8iFkdLHBXjuS416GuGK+N5551nwnHFihWBcMIs5ZlnnrHvhrRCmv8htDBeISWS8bDmiD/mynGfcePG2di41+jRo+3e6TVL2TaggRpDCyGEEELkdbKlqq2A8scff3hjxozxKlWq5J1wwgneGWec4TVo0MBbuXJlYG5x6NCh4PxNmzbZPowffObOnetVrVrVO/HEE73TTz/da9KkSXDs+++/91q1amVGFphUcG9MLMJMnjzZDCM43rhxY2/kyJHJjC8WL17sXXHFFXbOX/7yF69GjRrepEmTguOMad68eQlZE3/e8+fP9y688EKbF8/bsmVLcE4sc45459uxY0evZMmS9gzMMOK9LiW6dOniVahQwStatKh9f9wHUxAfxn3DDTeYgcvJJ5/sXXXVVd6ePXvs2HfffefVr1/f+/Of/2zjYe6+qcerr75q82b+F1xwgffOO+9kSZGoEEIIIYTIftLzvvYn/ienRaEoONCLC/t9IkL0Gito0IeMvnTY3VetWjVD98D2nmgbEVdFyIQQQgghch/peV9TDZkQQgghhBBC5BASZCLdYHARtskPb2kdy21QH5bSeAcPHpzTwxNCCCGEEPkcpSwWIF566SUzqfjhhx8ydR/6pBGGjQUh2dSOnXnmmS438c0337gjR47EPEZ/NLashF5vrVq1csuWLXM//fRTXKmcSlkUQgghhMjdpOd9TS6LIt0gqlITVjkhujIqNqN7xGU3U6dONev81atXm9Nk2IkxLS5KWuIKFS0efN47tFEWjVIIIYQQQmQVEmQiXdBzK9zMOquhjxcNorOyJ1pOQoNwrO8vuuiinB6KEEIIIYTIAfLnW24ugN5jQ4YMMUe9YsWKWV+sOXPmBE6DiAz6ZV1++eXWL+uKK66wflVh5s+f76pXr+5OOukki57cfvvtwTFS21q3bu1OPfVUu55aqF27diWLGtFfjONcS0PkaN58801rDs0zypcv7wYMGGD9znwY54QJE9wtt9ziSpQo4Z5++ukMr4k/74ULF1pTZZ5Zq1Yt63kWHjMpe2+99VZEc+3U5st927ZtayFh7s/Wv3//uNcpNebOnesuvPBCGwd9xUaNGhVxfPz48dZPjLnQU41ebj7XXHON9SpjI/LFd/jkk0/SaiI4zv1WrVplY+azEEIIIYQoYGSDDX+B5KmnnvLOP/986/FFXyp6a9HL6t133w16cdWsWdM+f/LJJ9a/il5gPgsWLPAKFy7s9evXz9u+fbu3efNmb/DgwcHxW265xatcubK3atUqO0ZvrYoVK3q//vqrHV+7dq1XqFAhb9iwYd5nn33mjR071vu///u/iP5eXEvfsZdeesnGuHTpUu9vf/ub179//+AcxnnmmWd6L774op3z5ZdfZnhN/Hkzbp718ccfezfffLM90x8360TPNtbigw8+8D799FPvl19+SXW+x44ds35vzGX//v22/fTTT3GtU2ps2LDB1nDgwIG2hoyNPmb8C+vXr7fviJ5ie/fu9TZu3Gjr7FO3bl3rQfbwww/bPF5++WXrV+b3eDt48KDXoUMHr3bt2jZmPsfi6NGj1sPC3/bt22frWLbbLK9crwXBJoQQQggh8l4fMgmyLIAXaF68V69eHbG/Xbt2XosWLQJhsnz58uDYwoULbd+RI0fsMy/pLVu2jHl/mhpzLoLFh2bFiIVZs2bZZ57TsGHDiOuaN28eIcjq1asXIfJg+vTpXpkyZYLPPKdbt25eIvDnPWPGjGAfIoRxz5w50z4jdjgH8ZSe+cZqJh3Pdalx9913W2PnMD179rRGzn7TbkTg//73v5jXI8gQgzQI9+nVq5ft80GscV5q0OCaeURvEmRCCCGEEHlfkCllMQvYvXu3uefVr18/wkZ92rRpVjPkQ9qeT5kyZQIHQ9i8ebOrV69ezPvv2LHDFSlSxNWsWTPYV7JkSVepUiU75p8TPg61a9eO+LxlyxY3cODAiDF26NDB7d+/38bvQ1plIgmPAxfD8LjhxBNPjFibeOYbi4xeF77+yiuvjNjHZ1IeqW3j+y1XrpyleuKU+Morr0SsG5CSSTpieO7+9fHSu3dvS8f0t3379sV9rRBCCCGEyN3I1CML+Pnnn+1faqWiXfyoRfJFWdgcw39pp/YMqDvLjnFSM9akSZNkx6iJ8qF2LDth7mERk1s5+eST3caNG62GbenSpa5fv35Wu7Z+/fo0revTA78ZNiGEEEIIkf+QIMsCwmYUdevWTXY8HCVLCSJEmH5gVhENrnwYb6xbt87MQADDDkxBeLZ/DsfDrF27NuIzZh5cU7FiRZedMA7MRnzTjZ07d9p4UyKe+RJVi446xXNdanD9Bx98ELGPz+edd54rXLiwfSYCd/3119uWlJRkQuydd94JRG6s7wATEP/6zLBtQAP1IRNCCCGEyONIkGVR5KRHjx7ukUcesYhXnTp1LNWMl3leoElzSwte7klZrFChgrvrrrtMWLz99tuuV69e9kJ/6623Wnrhc889Z897/PHHLRrHfujataul140cOdL2LVmyxC1evDjiGUR0br75ZhNHuANiLU8aI66HTz31VJatD2mSpA7iSti3b19zH7zttttSPD+e+eKASMQPEYujJY6K8VyXGo8++qi5XA4aNMg1b97crVmzxj377LPmrAgLFixwn3/+ubv66qvNxZHvh++blEgfRHn37t3dAw88YNG0Z555JplToxBCCCGEKMBkS1VbAQQjB5z/KlWqZK6BZ5xxhjn8rVy5MjC3OHToUHD+pk2bbN8XX3wR7MM0omrVqt6JJ57onX766V6TJk2CY99//73XqlUrM7LApIJ7Y2IRZvLkyd7ZZ59txxs3buyNHDkymfEFLpA4GnIOBhU1atQIXACBMc2bNy8ha+LPe/78+d6FF15o8+J5W7ZsCc6JZc4R73w7duzolSxZ0p6BEUa816XGnDlzzMSD7/Ccc87xRowYERx77733zJDj1FNPtXtfcsklgTkJcOzBBx+0cbG2nNenT58Ik494TD0yUyQqhBBCCCGyn/S8r/2J/8lpUSgKBtRaXXvttZammMgaq9wKfcWqVq3qxowZk9D7/u9//7O+ZkRdlbIohBBCCJH7SM/7mlwWhRBCCCGEECKHkCDLA7z00ku5JqLUsWPHCJv88JbWsfRy7733plpbllluuummFMc7ePDgLHuuEEIIIYQQPkpZzCOCrFu3bu6HH37I6aFYnzRCsLEgHJvasTPPPDNdzyLEy88zq8ToN998444cORLzGP3R2DID1v3z5s1LuKhUyqIQQgghRO4mPe9rclksIBw/fjyi71lGQVSlJqziEV2//vqr2dSnBT/irCS6R1xe46KkJa5Q0eLB571DG+XoeIQQQgghRPpRymIU2JYPGTLE/f3vf7cGxVioz5kzJzClIOqBtfrll19u1ur0t6KvVZj58+ebXTrNlbF0v/3224NjGFq0bt3abNK5nrS5Xbt2JYuIYUXPca6ld1Y0b775pvUR4xnly5e3Bs9Y4/swzgkTJrhbbrnFGjs//fTTGV4TxtyyZUt3xhln2JpgJz9lypTg+L59+1yzZs0skkVUCUv5vXv3Jks9ZAx//etfzRa+T58+rmbNmsmexXpjix++LvzdDB8+3Pqm0eeNNQrPK61xpAbfbY0aNWytuJ6WAV9++aUdo9kz5hwvvviiPZOUxgcffND6njGe0qVLmxANjwUbfuD747vwP/v3woa/bNmy9h0zZv7riRBCCCGEKHhIkEWBGJs2bZqbOHGi++STT6yX2D333ONWrlwZnEPvLHpJbdiwwRoD33fffcGxhQsX2kt4w4YN3aZNm0y88aLvg8jgurfeesv6WpGSx7lEsPxGwu3atXNdunRxmzdvNlfC6J5g7733nom6hx9+2G3fvt1e7hFx0aKLl3/GsnXr1ogxppcnn3zSnrNo0SK3Y8cOE3oITWDcDRo0sB5fjIteawiWG2+80SJhPqwDwnXZsmXWvwuB9+GHH0Y0yWa9P/74Y3f33XfHHEfv3r3d0KFDg/G8+uqr1sssPeOIBUIW4UcTb57P93L//febkPJhnMyfXm6vvfaamzx5smvUqJH7+uuv7bcxbNgw98QTTwSNoNevX2//Ilz3798ffIbdu3e7WbNmmXDnfvxOEHhCCCGEEKIAkg02/HmGo0ePesWLF/dWr14dsb9du3ZeixYtgj5ay5cvD44tXLjQ9h05csQ+165d22vZsmXM+9P/inM/+OCDYN+BAwesh9WsWbPsM89p2LBhxHXNmzeP6M1Vr149b/DgwRHnTJ8+3StTpkzwmed069bNSwT0MGvbtm3MYzyXXmvh3lrHjh2zOS1ZssQ+t2nTxitVqpTtD1OlShVv4MCBwefevXt7NWvWDD5z3a233mp//+9///OKFi3qPf/88xkeR0ocPHjQ1uvdd9+NeZyeZvwuGIMP/cz+9re/eb///nuwj+cPGTIk1R5u3Ktw4cLe119/HexbtGiRV6hQIW///v0p/i7pYeFv+/bts3uX7TbLK9drQbAJIYQQQoi814dMEbIQRC4OHz7s6tevH+G4R8QsHMm55JJLgr/LlCkTmF0AUa169erFvD/RJSJq4VS9kiVLWgofx/xzolP5ateuHfF5y5YtltYXHmOHDh0sEsP4fUirTASdOnVyM2bMsFS7xx57zK1evTpiLKwbkSl/LKQLHj16NGLNLr744mR1Y0TJiHIB+oXIE/tiwbocO3YsxbWNdxyx4Dwil0TYGjdu7MaOHWtrGYaUQ+7tQ2TuggsucIUKFYrY5/8OUoO0x3D9Gt8v6ZjRqa/hqC31dP5GqqMQQgghhMgfyNQjxM8//xykHUYbPlCz5L/Yh80x/LQ2XqiBGqvsGCc1Y02aNEl2jJoyH+qhEgF1btRTvf3225ZyiCjq3LmzGzlypI2lWrVq7pVXXkl2HTVnqY2lRYsWrlevXm7jxo3mdkgNWPPmzWOOIa11jXccKUFqYdeuXS2FcObMmZZ+yFxr1aplx6MNUfjeY+3zfweJhFTN7t27R7j2SJQJIYQQQuQPJMhCEPFAeH311VdWTxRNWpEWP3pGvVTbtm2THatcubLVK1FnhBkIYNhBZIRn++f4dUg+a9eujfiMmQfXYG6RXSBq2rRpY9tVV13levbsaYKMsSBgMLVIrwX72WefbeuMiEKQEZlMyaURIxFEGWvbvn37ZMczMw6fSy+91DYEEFErone+IMsICDaMP6Lh9/Xvf//bDE7875dIG5HSWPCbZBNCCCGEEPkPCbIQpKT16NHDjDyIdNSpU8fc7zCI4CW/XLlyad4jKSnJIkgVKlRwd911lwkwIktEghAVOP+RXogRB897/PHHLRrHfiBKg8MfYod9S5YssahNmH79+rmbb77ZUt/uuOMOe5knZW/btm3JDEASAc8j+nThhRda2iCmHAhHIMVwxIgRNlbSKBFZRNNef/11S2/kc2pwPWuG8cbo0aNTPI/IH2vIPUl9ZI3++9//mhEIJiiZGccXX3zhJk2aZI6UiCTELs6XGKdkBtIcEZCMFUGFs6Y/F4Qt3zHRLr5znBZxa0wP2wY0UB8yIYQQQog8jmrIohg0aJC5+FG3g+jApY8URmzw4+Gaa65xs2fPNhdFaq6uu+46cxMMp8YhbhBURGGonUKw+elvRGSef/55q2PCAn7p0qWWPheGWidEEcew1+caxEw8gjEjIICIGhH9u/rqq13hwoWtpgywbV+1apWJQ1IoWTMEErVb8YgFBCVRQmrf0mqgzPfy6KOPmkDkOaQ3+jVbmRkH13766aeuadOm7rzzzjOHRVIyH3jgAZcZcOIk7ZH0QiJvPkQ2GSPumjfccIOt6/jx4zP1LCGEEEIIkTf5E84eOT0IIQoKtCJ44403zPwlOzq/CyGEEEKI7Cc972uKkAkhhBBCCCFEDiFBVkDo2LFjhE1+eEvrWH4gpfmx0UhaCCGEEEKInEApiwUEaq0IncaCMKp/bO7cuWYMsmnTpuBYSs6HeQl6lKUEpirZ0a4gUShlUQghhBAi/7yvyWWxgICoSk1Y+cdoboxpR3Za6sfL3r17zVwFsYhhSnrA4XLevHlpGoek10URJ8docff1118Hx7t162Zb+Pw1a9ZE2OlznJqyd999N2FjE0IIIYQQeQMJMpFwjh8/nqxpcn4Fi33aGPggZlPDt+9fuXJlpp99UdISV6hocft779BGmb6fEEIIIYTIflRDloPQ6wx7faI+pMxhcz9nzhw7RrTkT3/6k/Wxuvzyy82anWbS9MgKM3/+fLO+50X/9NNPd7fffntw7NChQ9ZLi/5XXH/TTTdZf60wL730klnFc5xrsaCP5s0337TGyzyjfPnybsCAAdZfzYdxTpgwwfp4lShRwj399NMZXhPGTE8xGlGzJkS2aBUAfusBLOR5Ji0GYP369dZUmvkTGqbZ9MaNG4N7EpkC5sd1/ud45pYW9JKjf5i/Me7UwFKfRtC0OhBCCCGEEEKCLAdBjE2bNs1NnDjRGhzTkPqee+6JiJ707dvX+llt2LDBFSlSxN13333BMfqjITLoZ0UaH+KtRo0awfF7773XrqMnGmlylAtyLhEsWLdunfXq6tKli6XMXXvttckaS2N4gah7+OGH3fbt262hNSIuWnRh585Ytm7dGjHG9EKvMZ6zaNEit2PHDhN6CC3w+7ktX77c7d+/35o+w08//WSNlt9//30TO4g45sl+X7ABwo7r/M/xzi2RICoxSqGvG4I8HmjGTR5yeBNCCCGEEPkETD1E9nP06FGvePHi3urVqyP2t2vXzmvRooW3YsUKzFa85cuXB8cWLlxo+44cOWKfa9eu7bVs2TLm/Xfu3GnnfvDBB8G+AwcOeMWKFfNmzZpln3lOw4YNI65r3ry5d8oppwSf69Wr5w0ePDjinOnTp3tlypQJPvOcbt26eYmgcePGXtu2bWMe++KLL+xZmzZtSvUev//+u3fyySd78+fPjxjjvHnzIs6LZ26pUa5cOe/EE0/0SpQoEWxjx46NOD569Ohkn7/77jsb37Rp02z/ww8/7NWtWzfF5yQlJdn4o7ey3WZ55XotsE0IIYQQQuQefvzxR3tf49+0UA1ZDrr+HT582FLtwvz666+WkudzySWXBH+XKVMmcEwkzZCoVrh+KQzRJSJqNWvWDPaVLFnSVapUyY7554RTHKF27dpu8eLFwectW7a4Dz74ICJq9Pvvv7ujR4/a+El1BNIqE0GnTp1c06ZNLeXwhhtuMBMOUjVT49tvv3VPPPGEpXmyNoyPsX311VepXhfv3FKjZ8+eFon08aN5qUFaY48ePVy/fv1c8+bN0zyfaFr37t2Dz0TIypYtm+Z1QgghhBAi9yNBlkP8/PPPQdohznxhihYt6vbs2WN/h80xqH8CP9UtO6zaGSd1VU2aNEl2jLorH2rHEgF1bjgRUmO1bNkyV69ePde5c2c3cuTIFK8hXZHat7Fjx7py5crZ+iEsEbeJmFtqIMAy4kiJwBo/frxtacF82IQQQgghRP5DgiyHuOCCC+wlmygOJhTR+IIsNYieUTfWtm3bZMcqV65s5hTUifkRJkQLpiA82z+H42GowQqD4QXXZKcNPhEkRBbbVVddZVEoBNmJJ54YRLHCEOVC2FA3Bvv27XMHDhyIOAdhG31dTszNh4bU1MtRe4cZihBCCCGEKJhIkOUQuPORtoaRBxGvOnXqWOM4xAXN44j0pEVSUpJFkCpUqODuuusuE2BElrBVx9ji1ltvtZRGzCp43uOPP27ROPZD165d3ZVXXmlih31LliyJSFcE0upuvvlmS5G84447XKFChSzVb9u2bckMQBIBz6tWrZq78MILzcxiwYIFJhz9XmlEBRnj2WefbVEsXBWZ6/Tp0y1tknQ+BFx09BBnRcQr80UI4zyZ3XOL5bg4evRo9+qrr0aklsbLtgEN1BhaCCGEECKPI5fFHGTQoEEWJcFtEdFx4403Wgqjb++eFti+z54921wUaZR83XXXBU6Evqsg4gbRQQof3hYINj8NkubEzz//vKX6Ybm/dOlSq8UK06BBAxNFHMNen2sQEfEIxoxAFIyaKaJ/V199tfX1mjFjhh2jJu6f//ynCcy//vWvgbCcPHmy2eUT8WrVqpUJzegm2DhVkgJJ7ZVfo5fdc4uG74HfADVrQgghhBCiYPInnD1yehBCiPghCkhkkIiqImRCCCGEEHn7fU0RMiGEEEIIIYTIISTIRMKh8TGmFbG2tI7lFl555ZUUx0l9mxBCCCGEEIlAKYsFkJdeesl169bN/fDDD1lyf3qBEaaNBSHb1I5F137lFD/99JP1N0up9iu76sxioZRFIYQQQojcTXre1+SyKBIOoio1YZXasb1795qpyaZNm8yoJD3Qp23evHnWTDqz4ErJhjsjfdEA50YcLR9++GHXvn374FwaUl977bUx77N//35XunRp+/v77793AwcOtDGynx5mGLlgfY/ToxBCCCGEKHhIkIkMcfz48Yim1fkZRBTtAw4fPmyulvxN+wCaWIehp1n0fwHxxSdiDBdHXCQnTpxoaY+IT1wtcXhcs2aNK1++fLrGdVHSEleoaHH7e+/QRpmepxBCCCGEyH5UQ5bF0GMMW3uiPkRYsJefM2dOEFkhqkN/LHpoFS9e3Jo482IfZv78+fbSTt8toiq33357cAy799atW1tfLa5HJOzatStZiiIRGI5zLQ2io3nzzTfNNp5nIAwGDBhgfc18GOeECROsiXGJEiXc008/neE1YcwtW7a0BtCsCX3EsOgH3/Ifa3qeibU/rF+/3tWvX9/mT/iXZtobN24M7kkkC5gf1/mf45lbWhApI8rFtfR4O+2008xCPxrEF+eFN3qbQd++fd2///1vt3z5cvuO+D6w9af3G8K2c+fOGV5PIYQQQgiRd5Egy2IQY9OmTbOoyCeffGKNoO+55x63cuXK4Bxe1umTtWHDBuu1dd999wXH6EuGyGjYsKGl8SHeatSoERy/99577Tp6kRFloSSQc4lgwbp161y7du1cly5d3ObNmy21Lrrp8XvvvWeijlS87du3W58vRFy06CK1jrFs3bo1Yozphd5rPGfRokVux44dJvQQWuD3UUO4kNb3+uuvBzVdbdq0ce+//75bu3atiTjmyX5fsAHCjuv8z/HOLV5xPXfuXBOURLrScx291BChfvqiD4L0wQcfNGFGFC0WNMgmDzm8CSGEEEKIfAKmHiJrOHr0qFe8eHFv9erVEfvbtWvntWjRwluxYgWGKt7y5cuDYwsXLrR9R44csc+1a9f2WrZsGfP+O3futHM/+OCDYN+BAwe8YsWKebNmzbLPPKdhw4YR1zVv3tw75ZRTgs/16tXzBg8eHHHO9OnTvTJlygSfeU63bt28RNC4cWOvbdu2MY998cUX9qxNmzaleo/ff//dO/nkk7358+dHjHHevHkR58Uzt9QoV66cd+KJJ3olSpTwihQpYs847bTTvF27dgXn+N8j54S3Cy64wI7/5z//seOjR4+O+YzXX3/djq9bty7m8aSkJDsevZXtNssr12uBbUIIIYQQIvfw448/2vsa/6aFasiykN27d1vdEal2YX799VdLyfO55JJLgr/LlCkTOBWS1kZUi5qlWBBdIqJWs2bNYF/JkiVdpUqV7Jh/TjjFEWrXru0WL14cfN6yZYv74IMPIqJGv//+uzt69KiNn1RHIK0yEXTq1Mk1bdrUUg5vuOEGM+EgVTM1cDyk3oo0T9aG8TG2r776KtXr4p1bavTs2dMikUTe+JuIVsWKFZOdRzSO9Eaf6Bq7tAxNU4q69e7d23Xv3j34TISsbNmyaY5bCCGEEELkfiTIspCff/45SDvEBCJM0aJF3Z49e5K9uFP/5Ke5+Slt2TFO6qqaNGmS7Bh1Vz7UjiUCaqhwLnz77betFqtevXpWQzVy5MgUryFdkdq3sWPHmuU864ewRNwmYm6pQTolAowNU4+LL77YxOkFF1wQcR71b//3f/+X7Hpq5djvi+SUhLVfPxcNc2UTQgghhBD5D9WQZSG8sPMiTRTHf6H3t3gjHETPqBuLReXKlc2cgjoxH0QLpiC+WOCc8HGgBisMhhdcEz1GNt+UItEgUhBZL7/8shszZoybNGlSRJSIKFYYolxdu3a1ujEcClnXAwcORJyDsI2+LtFz43tr3ry5Ra3ihec0a9bMvfrqq+4///lPxLEjR4648ePHWxQTsxIhhBBCCFGwUIQsCyF9rUePHmbkQcSrTp061hwOcYE9ejzNhZOSkiyCRP+ru+66ywQYkSXc/jC2uPXWWy2lEbMKnvf4449bNI79gIi58sorLfrEPswjwumK0K9fP3fzzTdbiuQdd9xhAoJUv23btiUzAEkEPK9atWomrDCsWLBggQlH36mQqCBjPPvssy2KhVBhrtOnT7fIFCl7pA5GRw9xVkS8Ml8EG86TWTE3DEIuuugiM1MJp3GSSkkqZBhSSBGKpEwyNtJXhw8fbtd/8cUXlobJmIj8pZdtAxqoMbQQQgghRF4nW6raCjB//PGHN2bMGK9SpUreCSec4J1xxhlegwYNvJUrVwZmEIcOHQrOx8yCfZhb+MydO9erWrWqmUucfvrpXpMmTYJj33//vdeqVSsz6cDMg3tj9hFm8uTJ3tlnn23HMdQYOXJkhKkHLF682LviiivsnL/85S9ejRo1vEmTJqVqmJFRBg0a5FWuXNmehUHGrbfe6n3++efB8eeff94rW7asV6hQIa9u3bq2b+PGjd7ll1/unXTSSd65557rzZ492ww3wkYZb731llexYkUz3+BYvHNLjehn+LDON910k/3tf4+xtjVr1gTX/Pe///Ueeughm1vhwoXtOOM6ePBglhWJCiGEEEKI7Cc972t/4n9yWhQKURCZPHmyGYTMnDnTjE3ihQghUUOirYqQCSGEEELkPtLzvqYaMiFyCPrD0Z8MUw9qyYQQQgghRMFDgkxkiI4dO7o///nPMbe0juUWXnnllRTHSX1bdoCZBwYh2eGmKYQQQgghch9KWUwwL730kuvWrZv74YcfXH4GAwtCsT6PPfaYfZ44caKFZcPHwnAM447M0r9/fzdhwgQbx7x589KV8ufz008/WX+zWGDEETZdoQ8Z3+kbb7zhchqlLAohhBBC5G7S874ml0WRIRBVYWE1ZcoUa3zs9+FKhOhKCVL86C2GEKtVq5a5KeKwiBBmixdcKcONnBPJ3r17ra/Ypk2bXNWqVbPkGRclLXGFihZ3e4c2ypL7CyGEEEKIrEeCLBdy/PjxiGbROQmNl/3eYKmRnT20/Iba2Pj7jbSFEEIIIYTIi+TpGjJ6ew0ZMsQiEdTgVKlSxc2ZM8eOvfvuu/ayTu8nekUVL17cXXHFFdYkOMz8+fNd9erVrd/V6aefbjU9PocOHXKtW7e2CAzX33TTTW7Xrl3JUhTpccVxrqUxczRvvvmmNSjmGeXLl7foDv3EfBgn6Xe33HKLK1GihPWsyiiMuWXLltZ4mTWhfxfRK599+/ZZk2IiWaeddpqJGqI54dQ80v8Yw1//+ldXqVIl16dPH1ezZs1kz2K9Bw4cGHFd+Luh3xYNmOkJxhqF55XWOFJLVWzcuLH9Tf8u1u6aa65xX375pfV747Mv0vhuuD9phqwD69+gQQN7dmrQXLp79+52LX3ESMeMzuylTxp95fxz6HXmC0XgNwmXXnppMEZYv3699SLjt4aIrVu3rtu4cWOa8xZCCCGEEPmTPC3IEGPTpk2zuqVPPvnEXsjvuecet3LlyuCcvn37ulGjRlkT3yJFirj77rsvOLZw4UITUQ0bNrTUMsRbjRo1guOIDK5766233Jo1a+ylnHOJYMG6devMKa9Lly5u8+bN7tprr03WbPi9994zUUcz4e3bt1sDZ4RCtOhCaDCWrVu3RowxvTz55JP2nEWLFllqH0KPl39g3AgS0vQYFw2qMbC48cYbLRLmwzogXJctW2ZNmxF4H374YYTgYL0//vhjd/fdd8ccB0YVQ4cODcbz6quvulKlSqVrHLGg0bYvMPfv32/b66+/bk2kEYf+Pp/Dhw/bWvM74TnUgdFgOzX4vfAdvfjii+79999333//vaVHhvnll19MtPH7YL0Qh3x/CFFgvWD58uXBGP26tTZt2th9165da0KR3xT7U4Lm2eQhhzchhBBCCJFP8PIoR48e9YoXL+6tXr06Yn+7du28Fi1aBM16ly9fHhxbuHCh7Tty5Ih9rl27tteyZcuY96e5Mud+8MEHwb4DBw5Yc+FZs2bZZ57TsGHDiOuaN28e0XS5Xr163uDBgyPOmT59ulemTJngM8/p1q2blwho/Ny2bduYx3guDappVu1z7Ngxm9OSJUvsc5s2bbxSpUrZ/jBVqlTxBg4cGHzu3bu3V7NmzeAz19HgGf73v/95RYsWtQbPGR1HatCgOvqnG6uB85QpU+y8tWvXBvt27Nhh+9atW5fi/fluhg8fHnw+fvy4Ndb25xcLmj5z361bt9pnGnvzmUbfqfH77797J598sjd//vwUz0lKSorZdLpst1leuV4LUr2/EEIIIYTI3Y2h82yEbPfu3Rb9IP0rbFdOJCQcybnkkkuCv8uUKWP/4swHRLXq1asX8/5El4iohVP1SE0jhY9j/jnRqXy1a9eO+LxlyxaL3ITH2KFDB4uaMH4f0ioTQadOnay3FUYSpNqtXr06YiysG5EpfyykCx49ejRizS6++OJkdWNEyYhyARrytddes32xYF2I6qS0tvGOIxHwHZKS6nP++edbmiFj/OqrryK+l8GDB5sTDt9N+HvlHtHfD6mrLVq0sBRUnHMwFQHumRq4OvL9ExkjZZFrf/7551SvI9rIuPwtrZRLIYQQQgiRd8izph68xPpph2eddVbEMWqW/Bf7sDmGX1vkp5VlR+8nxknNWJMmTZIdo6bJh9qxRECdG/VUb7/9tqUcIoo6d+7sRo4caWOpVq2a9d+Khpqz1MaC+OjVq5fVO9HEGFHQvHnzmGNIa13jHUdWQ40cotwHURgv1LFhi//888/bffhNXXTRRWmmXJKuSJ3h2LFj7Xp+q4j41K7jHDYhhBBCCJH/yLOC7IILLrCXVCILGCNEE0+khegZ9T9t27ZNdqxy5cpmvEGdGGYgwIs0tVU82z+H42GoCwqDmQfXYG6RXSBqePFnu+qqq1zPnj1NkDGWmTNnmiV9evtXUaPFOiOiEGREJlOytif6gyhjbdu3b5/seGbGkRJE9DDjiIbvkDovvzaQ74I6Mr47Il+xvhciqXyvV199dXCPjz76yMYd/h0gxlhfoCYsejwQPSbq2MaPH291Y4CwPXDgQIbmvG1AA/UhE0IIIYTI4+TZlEXS3TB4wMhj6tSpJsCI3jzzzDP2OR6SkpIs9Y5/SWHDUGPYsGGBqMD5j/QyXrZJs8MwhGgc+6Fr167mtofYIYXt2Weftc9h+vXrZ2mURMkwwuA5pBQ+8cQTWbAq/+95uDqSEsjzMOVAfAAphhh8MH7MNL744gtzo2QeX3/9dZr35nrGPnv27BTTFf3IH9E0Uib9FFKE6uTJkxMyjliQMrhq1Sr3zTffRAgcIqQPPfSQCSxEFUYt9C4Lm7dEgwELhiS4M3766afuwQcfjGj0jesm6auTJk2ydX7nnXfM4CMMYhNRyu+BNEVSDf3f1fTp0+13wJhYi+yI1AohhBBCiNxJnhVkMGjQIHPxw20R0YFLHymMvuV4WmBFjrjARZGaq+uuuy5wxwPc/Eitw9KctDJqp0gF9NMgebEnSkL6GRbwS5cuTSa0cBNEFHGMWiauGT16tKWrZQVEZqg5IvpHhKdw4cImogBrfkQLFvSkULJmuERSuxVPpOWOO+6w6BC1b2GL+1jwvTz66KMmEHkO6Y1+7V5mxxEL6vSwza9QoUJE2iPPQhziBnnllVdarRjRudRg3K1atbIII9874j/cDgFHRdYUgUeaIv9RYMSIERH3IPr2z3/+01w1SWn0RTyilNYERNt4BiI0K5toCyGEEEKI3M2fcPbI6UEIkRVgXd+tW7eI6FZ+ANt7DEGIuillUQghhBAib7+v5ekImRBCCCGEEELkZSTIciEdO3aMsGMPb2kdyymozUorjTEecMKkpiqlOVJzlt1QnzZmzJgcXRchhBBCCJE/UcpiLoRaK8KcsSDkmdqxnKpHIhzLT4keX5kVZLgQ4uIYC0xVstsEA0FG6iNbTq1LGKUsCiGEEELkbtLzvpZnbe/zM4iq1IRVSsfS6oGVlfCDSxTYzme0TQBrEN3UOidJ5LpEc1HSEleoaHG3d2ijLHuGEEIIIYTIWpSymIfBJbJLly4WucFGHkfHbdu2WXNo0vtKlSplTn5hG3iuwQaea7Bv5xycIn/55Rfrx4ajIGJo0aJFwTX00sIFEfdKolOVKlUyZ8nUUvN4Dg6CWN/TcLl06dKuf//+EdfQKgAnSGzy6e1GI+to6NPVrFkzizBxH9wKcVOMfu7TTz9tboaMLbPRSZo+M0/mG928mlYLuG76kMpIVC/c7oD1e+GFFzK8LkIIIYQQouAgQZbHoecaESEaDtM7C+v+Sy+91Joh+z2wEDTR1yDgsPhHnHXq1Mndeeed1gCbXm433HCDCTns7eGPP/6wxtC0CNi+fbtZ2ffp08fNmjUrzbGVKFHC+m0NHz7crOl90cU9sbxn7ByfOHGi2dOHOX78uIlMRCK1Y8wRoUl7g3A0kAbUNGrm3rQYyAwIKETgihUr3Jw5cyx90rfrB5pj05fOb/i8cuVKW0v6qAF90Oi7hvDKyLoIIYQQQogCBjVkIm9St25d79JLLw0+Dxo0yLvhhhsiztm3bx81gt5nn30WXFOnTp3g+G+//eaVKFHCa9WqVbBv//79ds2aNWtSfHbnzp29pk2bBp/btGnj3XrrrRFjCz8Hqlev7vXq1cv+XrJkiVekSBHvm2++CY4vWrTInjtv3jz7PH36dK9SpUreH3/8EZxz7Ngxr1ixYna9/9xSpUrZ/szCGvH8Dz/8MNi3Y8cO2zd69Gj7fOjQIa9QoULe+vXrbVynnXaaN2TIEK9mzZp2/OWXX/bOOuusDK9LLI4ePer9+OOPweZ/p2W7zfLK9VqQ6XkLIYQQQojEwjsb72v8mxaqIcvj0LjaZ8uWLRbZIYoUDVGb8847z/6mabQPjaNLlizpLr744mAfaYwQjgyNGzfOvfjii+6rr75yR44csQgVzbRTI/wcvzbMv+eOHTtc2bJlLc3QhybMYZjP7t27LUIWhgbSzMeHsSeibowx0dA5vKbnn39+hCEHf9MEnIgYz2S7//77XVJSkvv5558tYkYULaPrEgsanw8YMCBTcxNCCCGEELkTCbI8DqlvPggC6p+GDRuW7Dxe+n1OOOGEiGPUQIX38dlPK4QZM2ZY7dSoUaNMNCGQRowYYSl3qRHrOf4944H5II6i67jgjDPOiLkG2QHpiAiyokWLmviiFqxy5cqWyogge/TRRxO6Lr1793bdu3ePcO1BzAohhBBCiLyPBFk+4rLLLnNz5841m3YiPYmC2i3qyx588MFgXzhClREQMNRq7d+/PxCLa9euTTafmTNnmqtkdti7Ew377bff3EcffeSqV69u+6hN++GHHyLOQ4QRLWSNqWfzRdprr73mdu7cmWr9WEZA+LEJIYQQQoj8h0w98hGdO3d233//vWvRooVbv369iaYlS5aYe6JvQpERzj33XDMJ4V4IjieffNLunxmuv/56S6Fs06aNpSZi2tG3b9+Ic1q2bGmGGTgrcvyLL76wyBQuhV9//bVLNDg0IrAeeOABi/4hzNq3b5+s7xnOkD/99JMZiPjii3+J5CEu/dTQrGbbgAayvBdCCCGEyONIkOUjqMcimoX4wimR2irs7al7KlQo4181AgVHxObNm7uaNWu6gwcPRkTLMgLjmTdvntWj1ahRw4QP1vVhihcv7latWuXOOeccez5RNez3qSHLqojZlClTbB2JgvFM6sOi+77RLoC1JW2SqJov0kg7TKt+TAghhBBCiDB/wtkjYo8QIt90fhdCCCGEELn7fU0RMiGEEEIIIYTIIWTqIfIV1JrddNNNMY+RHhldDxbt6iiEEEIIIUR2IkFWAHjppZeslizaLTA/cvnll7vNmzfb34899piFiydOnBiXIMss/fv3dxMmTLCeYtTH3XbbbVn2LCGEEEIIkT+QIBP5CgRXxYoVA4MOSiTDjZ2zCppK07wZIVarVi0z/qD9AEKYTQghhBBCiFhIkIm4OH78eLKGxjnFr7/+6k488cQ0z6OQMrvw+7Jh0e831s5qLkpa4goVLS7reyGEEEKIPIxMPRIM1udDhgxxf//73y1aU6VKFTdnzhw7Rg8tXtb/9a9/WWodtu40XKb5cJj58+dbY+KTTjrJ+nDdfvvtwbFDhw651q1bWwSG66mX2rVrV7IURaziOc612NRH8+abb1rjZZ5Rvnx5i+7QFNmHcZJ+d8stt7gSJUoks6RPD4yZnmLYxLMm9DUjeuVDg+hmzZpZJOu0004zUbN3797g+L333mvpf4wBS3r6hfXp08cs+KNhvQcOHBhxXfi7GT58uEXQaLTMGoXnldY4UktVbNy4cWDnz9rRl+zLL790jzzyiH32RRrfDfd/4403bB1Y/wYNGtizhRBCCCFEwUOCLMEgxqZNm2Z1S5988om9kN9zzz1u5cqVwTk0QB41apQ1Wy5SpIi77777gmMLFy40EdWwYUO3adMmE2/06fJBZHDdW2+95dasWWMpeZxLBAtoaEyvri5dulgt1bXXXuueeuqpZMYXiLqHH37Ybd++3T333HMmFKJFF0KDsWzdujVijOmFRtI8Z9GiRZbah9BDaALjRpCcfPLJNi76qP35z3+2Bs1EwnxYB4TrsmXLrCEzAu/DDz8MIlPAen/88cfu7rvvjjmO3r17u6FDhwbjefXVV12pUqXSNY5Y9OjRIxCY+/fvt+311193Z599tolDf5/P4cOHba35nfAcavvuuuuuFO9/7Ngxq4ULb0IIIYQQIp9AHzKRGI4ePeoVL17cW716dcT+du3aeS1atPBWrFhBzzdv+fLlwbGFCxfaviNHjtjn2rVrey1btox5/507d9q5H3zwQbDvwIEDXrFixbxZs2bZZ57TsGHDiOuaN2/unXLKKcHnevXqeYMHD444Z/r06V6ZMmWCzzynW7duXiJo3Lix17Zt25jHeG6lSpW8P/74I9h37Ngxm9OSJUvsc5s2bbxSpUrZ/jBVqlTxBg4cGHzu3bu3V7NmzeAz191666329//+9z+vaNGi3vPPP5/hcaTGvHnzbM3ClCtXzhs9enTEvilTpth5a9euDfbt2LHD9q1bty7mvZOSkux49Fa22yyvXK8FaY5NCCGEEEJkLz/++KO9r/FvWihClkB2795t0Y/69etbdMXfiISEIzmXXHJJ8HeZMmXsX5z5gKhWvXr1Yt6f6BIRtXCqXsmSJS2Fj2P+OdGpfLVr1474vGXLFovchMfYoUMHi+Iwfh/SKhNBp06d3IwZM1zVqlXN+XD16tURY2HdiEz5YyFd8OjRoxFrdvHFFyerGyNKRpQL0JCvvfaa7YsF60KkKaW1jXcciYDvkJRUn/PPP9/SGP3vMFZkj6aC/qb0RiGEEEKI/INMPRKI38eKtMOzzjor4hg1S/6Lfdgcw68tor4JstKWPTxOasaaNGmS7Bg1TT7UjiUC6tyop3r77bct5RBR1LlzZzdy5EgbS7Vq1dwrr7yS7DpqzlIbS4sWLVyvXr3cxo0bzdIeodK8efOYY0hrXeMdR07Ab4dNCCGEEELkPyTIEsgFF1xgL85fffWVq1u3brLj8URaiJ5RL9W2bdtkxypXrmzGG9SJYQYCGHZQW8Wz/XM4Hmbt2rURnzHz4BrfHj47QNS0adPGtquuusr17NnTBBljmTlzpjvzzDPdX/7yl3Tdkxot1hkRhSAjMsl9YoGBBqKMtW3fvn2y45kZR0oQ0fv999+T7ec7pA7Qrw3ku6COjO9OCCGEEEIULJSymEBId8PgASOPqVOnmgAjevPMM8/Y53hISkqy1Dv+JYUNQ41hw4YFogLnP9IL33//fUuzwzCEaBz7oWvXrm7x4sUmdnBffPbZZ+1zmH79+lkaJVEyjDB4DimFTzzxRBasyv97Hq6OpATyPEw5fPFBiiEGH4wfM40vvvjC3CiZx9dff53mvbmesc+ePTvFdEU/8kc0jZRJP4UUoTp58uSEjCMW9CFbtWqV++abb9yBAweC/URIH3roIRPOH330kRm10LssbN4SD9sGNJDlvRBCCCFEHkeCLMEMGjTIXPxwW0R04NJHCiM2+PGAXTriAhdFaq6uu+46cxP0wc2P1Lqbb77ZasOonSIV0E+D5MX++eefd2PHjjUL+KVLlyYTWrgJIoo4Ri0T14wePdqVK1fOZQVEiqiDIvp39dVXu8KFC5uIAqz5ES1Y0JNCyZrhEkntVjyRqjvuuMOihNS+hS3uY8H38uijj5pA5DmkN/q1e5kdRyyo08M2v0KFChFpjzwLcYgb5JVXXmn1akTnhBBCCCFEweNPOHvk9CCEKCjQXqBbt26WophRsL2n6TUGH4lKrxRCCCGEEIkjPe9ripAJIYQQQgghRA4hQSbiomPHjhE2+eEtrWP5gZTmx0bNmRBCCCGEEBlBKYu5GMweSG174403MnUfrPXnzZuXZo1ValBrReg1FoRhUzuWkvNhRuEn+8ADD7g5c+a4Q4cOuU2bNlm9XVaCIUlKYKqSmXYF/fv3t++YHnTxoJRFIYQQQojcTXre12R7n4vBmCO36GVEVWrCKqOiCyfDa6+91oQVzZHjAddIarG4tnz58uaOmAjRmRrZ2SJACCGEEEIUHCTI0uDXX381l8CcAFVd0NcgFljWlylTJujFJoQQQgghRF5FNWQxbOe7dOliTnhEXrCI37Ztm7vpppusXqhUqVKuVatWEX2luIa+Ulxz6qmn2jlYz//yyy/W4Jn+ZERYFi1aFFxDw2Bs1bHDJ92tUqVKFhGLTlkMR3x4Dn2x6KV12mmnudKlS1u6Wxh6j2EtT98tmkUvW7Ys2Rz37dvnmjVr9v+1dx7QVVXZ/z8mVMMIAtIMRBRCky6E0IIGaQKRGhkIRVBKHMARBGkRUDqGMvRFmQksmoDSQxmkhuLQiwGkSAkwoTnUADn/9d3rd+//vZeXl5fy8kLy/ax1ybv33HvPOfck4e7svb9bwSOF+6D2FuTZbfv9/vvvVbFixWRsqeHp06ci8168eHEpnI1ngfpf6BPeMYDnBi8X+nYE2vGsUXwb56PWFzbQqlUr8xjAs0Eo45w5c6RvyM1j3nAdO8uCBQtUhQoVZNwwAvG9YYC+cG+UIMC9IZUfFRUl4Y1YKy8vLzEabQuCjxs3Tr5H8H1hSOsTQgghhJCsCQ0yO6CIMzxCe/fulZdn1AKrWrWq+vXXXyVc7ubNm/Jib3sNDDjUDIPB0Lt3b9WuXTt5IUdx6EaNGokhh3pZID4+Xnl7e0vNsdOnT0ttrCFDhqgVK1YkOTa86KOo8IQJE6TWlWF04Z6ooYWxo3327NliCFny7NkzMTJhDECMAnOEoYl6afCEGWzfvl1FR0fLvVGzLDV07txZil1PmzZNilDDiEGfMJJWrVol56CvmJiYBEapLWjHnPHscP6hQ4dkM2q0GccMYBzhma5bt07WDvlmffr0cWrcs2bNUqGhoerzzz+XAt2oDWcbuoi6c5gf8r/Kli0rtcWQ34a6a/h+QcippRGHscBQHDNmjLTDyJs5c2aSBi3ikC03QgghhBCSSYCoB/n/BAQE6KpVq5r7o0eP1o0aNbI658qVK0js0tHR0eY1devWNdufP3+uvby8dEhIiHksJiZGromKikq079DQUN2mTRtzv0uXLjooKMhqbJb9gBo1auhBgwbJ58jISJ0tWzZ97do1s33Tpk3S75o1a2Q/IiJClylTRsfHx5vnPH36VOfOnVuuN/otXLiwHE8teEbof+vWrXbbd+zYIe137951+p7h4eHax8fH6pjlHA3CwsK0p6envnr1qtXz8PDwkPVIimLFiumhQ4cm2o4+hw0bZu5jbXFs/vz55rGlS5fqXLlymfv+/v66T58+Vvfx8/PTlStXTrQfzAP3td3u37+f5BwIIYQQQkj6g/c0Z9/X6CGzQ/Xq1c3Px44dUzt27LCSOYcnBFiGolWqVMn87OnpqQoUKKAqVqxoHkOImqFWaDBjxgzp64033pD7zp07V0LxHGHZD4CHxbgnvE/wOiHM0MDf39/qfMwHXiN4yIz5IGwRYXOW88HY0yJvDJ4jPI+AgADlDkqUKCEqiJbPA55EeOQcgWd6/fp1FRgY6PR6GGtsu+54toZXC2vk5+dndQ/bNbIF3jaEWRobQk4JIYQQQkjmgKIedkBIoMGDBw9UixYt1Pjx4xOcB2PIIHv27FZtyC+yPIZ9AGMALFu2TA0YMEBNnjxZXshhIE2cOFFCDR1hrx/jns6A+cAIXLJkSYI2GIb2nkFqSI0cvDtxdtz21tjRuqcE5K9hI4QQQgghmQ8aZElQrVo1yXOCUES2bGn3uJC7hfwyy3wmW/GH5AJRCXhPkEdlGIv79+9PMJ/ly5eLTH161LCCtwjGyM6dO1XDhg0TtBteOIicpAYYQfbuAY8jPF2G1xDPw8PDI0mhEhjIWHPk0hnCI2kB1ghGN/LODGzXiBBCCCGEZB0YspgEEHW4c+eO6tChg4hFwGiKjIwU9cTUGBGlS5cWUQfc6+zZs2r48OFWYhQpAQaPr6+v6tKli4QmQrRj6NChVud07NhRxEegrIj2ixcvSj0vqDdevXpVpTUwajCeTz/9VIofG/0Z4iU+Pj7iRYJwyH//+1/x4KW0HxhPN27ckJpmBlCbtHwemCcEWaBQmRQQ34AHE2IkUK+EOMv06dNVaujXr58oN0KABOseFhamTp06lap7EkIIIYSQlxcaZEkAzwq8WTC+oJQIjw/k7SEZD09LSoESHxQRg4ODJafo9u3bTqv/JQbGg+LIjx8/VjVr1lQ9evQQ6XpLIM++a9cuya1C//DYGNLrrvKYQa2wbdu2Mj/k33322WdSEgAgv2vkyJFq8ODBkm9lqUiYHGA4QRESOXRQxDSAKiLm2axZM1k/5HwlpWpoAENuypQpcj6k7yFvD8MsNWC9YXyjdAFCRy9fviyKnIQQQgghJGvyCpQ93D0IQlwBPFzwykFYJDMBgRAUDYfAR3qEnRJCCCGEENe9r9FDRgghhBBCCCFugqIeJEmQe9W0aVO7bQiPdKRImNycMIhwlC9fPtF2FNFGuGVaAMn/xNi0aZOqV69emvRDCCGEEEJIYjBkMQPStWtXde/ePQm3Sw0Qy0BO2ccff5yq+8DounbtWrINMtQeGzhwoOTcOcvz58/VpUuXEvSBEgHI5UPuGUQ7kMOXWlCPzVbwBDl1w4YNk9y2tJDsd0XYJEMWCSGEEEIyNsl5X6OHLAMydepUlZHsZBgmEMdILjBCklvPDKUFbPuCKMiRI0dEHh4KkTDIXn/9dTlWpUoVlVJs+8E8YeilZK6JAUPyb3/7W5rdjxBCCCGEZC5okCVCXFycWSMrvYE1nRmegWWh6dSAUgPwXL377ruyb+tBy8ggLNJRaCQhhBBCCMnaUNTj/2jQoIFIriO8Dl6Yxo0bq5MnT0ruFF6oIckeEhKiYmNjra6B9wPXwGODc+bNmydhdahThuLC8LYgH8kA8vmQmS9ZsqR4ZFCgGB4x25BFyzBD9IP6WZBKz58/v9TQQiicJZBjr1+/vtTdQg4WJOBtQdFo1OCCFwj3QS0yS+PG6BdS+ZD7T6p4sjO1wSAbD+Dxw5iR/5UzZ065P+aUFJg7JO0h1Y8QTOzj2QHI2xvHLMcPGX0Yg3AP9+rVSwzLlABPHAo4Y21RLgDfC7ay91hvSO2jvVWrVuqHH36wCqfEnC29eMYYJ02aJMW7CxQoILXunj17lqIxEkIIIYSQlxsaZBb885//FI8QcpXGjRunPvjgA3npRwHnzZs3q5s3b4pBY3sNDLiDBw+KcYaaUu3atVO1a9eWQsKofQVD7tGjR3J+fHy88vb2VitXrhSBihEjRqghQ4aYhZIdjQ3hfwcOHFATJkxQo0aNMo0u3BO1tjB2tM+ePVsNGjTI6nq88MPIhJEIkQ7MEYZmkyZNrAwWFFeOjo6We6NYc1qxatUqFR4erubMmSNGDfKqUNMtKVavXi11y/z9/VVMTIzs41mDbdu2mccsx3/mzBkpPr106VJpg4GWEmA8Ye3Xrl2roqKixKhEPTPDeMIzhMGHYs8Iz/zwww8T1H2zx44dO8Trh69Y10WLFsmWGE+fPpU4ZMuNEEIIIYRkEiDqQbQOCAjQVatWNfdHjx6tGzVqZHXOlStXkNilo6OjzWvq1q1rtj9//lx7eXnpkJAQ81hMTIxcExUVlWjfoaGhuk2bNuZ+ly5ddFBQkNXYLPsBNWrU0IMGDZLPkZGROlu2bPratWtm+6ZNm6TfNWvWyH5ERIQuU6aMjo+PN895+vSpzp07t1xv9Fu4cGE5nhb4+Pjo8PBw+Tx58mTt6+ur4+Likn2ffv36yTMwuHjxosztyJEjVudh/Pnz59cPHz40j82aNUvnyZNHv3jxIsl+0Af6AmfPnpU+9u7da7bHxsbK81qxYoXsBwcH648++sjqHh07dtR58+Y198PCwnTlypWtxojngu8Vg3bt2sm9EgP3wFhst/v37yc5J0IIIYQQkv7gPc3Z9zV6yCyoXr26+fnYsWPiwTBygLCVLVtW2uDdMKhUqZL52dPTU0LQLD0/CGMEt27dMo/NmDFD+kJYHe47d+5ckXt3hGU/AOFuxj3hEULYHMIADeBRsgTzgaogPGTGfBC2+OTJE6v5YOyuyJ2D1xBqiW+//bZ4vKD+CEXFtKZy5coSPmj5HCC9j3DN5IBnCoERPz8/8xjWFmGcaAPwJNasWdPqOtt9e1SoUEG+V+ytpT2++eYbUegxtuTOhRBCCCGEZFwo6mGBpSIgXuJbtGihxo8fn+A8vEAbZM+e3aoNOU2Wx7BvhBWCZcuWifIe8qJgLMBAmjhxooQaOsJeP8Y9nQHzgRG4ZMkSh+IbyVVFdBYYjDBgEGaIcMg+ffrIvHfu3Jlgbpmd5K4lcu6wEUIIIYSQzAcNskSoVq2a5D1BmAKekrQCeUfIL4NBYmDpoUoJUCCE1wT5VIaxCIl42/ksX75cFSpUyG21qyBiAiMXG4Qs4HE8ceKEjC05GB48CKTYAk+gZW00PAd4A2EQJveZwoMHQxnrBW7fvi1GpVG4Gt6yQ4cOWV1nu08IIYQQQogjGLKYCDAY7ty5ozp06CAv2TCaIiMjRT3RniHgLKVLlxahCNzr7Nmzavjw4al+iW/YsKHy9fVVXbp0EYMEoh1Dhw5NUPQY4iNQVkT7xYsXRfgCSodXr15VrgaiFfPnzxflygsXLqjFixeL0eTj45Pse8GoxLWG0ArC+AwgUAIVSwimbNy4UYWFhYl6poeHR7LXCc8K4ZV79uyR59qpUycpGI3jACIu6APKihAqgWAJFDUNryghhBBCCCFJQYMsEZCPBW8WjC8oJSK3CvL2kDRP7su9JT179hRFxODgYMlPgtfF0luWEjAe5GTBM4Qcph49eiRQ+0NeFaTjITuP/uEBguGCHLL08JjhuUEivk6dOpIPh9DFdevWSV5WcoHHctq0aWIAYZ0MAwkEBgaKMYUSAHjGLVu2TFAiwFkWLlwoYZ7NmzeX8FKoLMIAM0IOMRcoWsIgQ+4aDMQvv/xSSg8QQgghhBDiDK9A2cOpMwnJ4ECm/t69eyKp7y7gUfvtt9/EC+kqIHuP4uHwDLor/JQQQgghhKTN+xpzyAhJBSjwjPpjEENBuCLqis2cOdPdwyKEEEIIIS8JDFnMgiCfCyGESQEvj6Xsv+UG2fbE2rA5i6M+knOfpEBZAUf9JFV2IDFQpBoGGUJaEb6IUEqEjBJCCCGEEOIMDFnMogYZ8uEQ3ucI5KRdu3Yt0TZDydAepUqVcmostn1AYOT9999XP//8s6gZOnsfADEN5NJ9/PHHCdqgmHjp0qVEr01MTRPHL1++nOD42LFj1eDBg819KHJOnz5dHTlyRPIOUW+tbdu2IiiCem/GXMeNG6eWLl0q90TJA8wVOW6oTeYsDFkkhBBCCMnYMGSRpAkwuBIziJ49e5Ym9cNs+zCMIoiPJMcYSwrcN6X3GzVqlOSGWQJjygCKlqhXB0GPMWPGiNAIVBfhMYuIiFD9+vVTT58+FTVMeOJQgw6CLlCIhGGHzxA5qVWrVqrnSQghhBBCXi4YsuhiUPAXL90lS5YU4wNqfD/++KO0QXYeXp3t27er9957T5QQUfMKta4sgRphjRo1RL0P0vWtWrUy2+7evas6d+6sXn/9dbm+adOmYgzYesRg4KAd10LZ0RZ4pFAPDH3AuzNy5EjxKhlgnLNmzRLVQuRL2ao4JgeMGTL8KEiNZwJVRCgaAjwnULVqVemzQYMGso/SAAgNxPzx14aAgAB1+PBhK08WwPxwnbHvzNySAsZXkSJFrDajgDZCFmGEwchCoWusH/rGWOE1QykCMGXKFBUVFaXWr1+v2rdvL3L/UMTEOYbiJZ3VhBBCCCFZEIQsEtfx3Xff6bJly+rNmzfr33//XS9cuFDnzJlT//LLL3rHjh14A9d+fn6yf+rUKV2vXj1du3Zt8/r169drT09PPWLECH369Gl99OhRPWbMGLO9ZcuWuly5cnrXrl3S1rhxY12qVCkdFxcn7fv379ceHh56/PjxOjo6Wk+dOlXny5dP582b17wHrn3ttdf0okWLZIxbtmzRb731lv7222/NczDOQoUK6QULFsg5ly9fTvEzCQ0N1VWqVNGHDh3SFy9e1Fu3btVr166VtoMHD0pf27Zt0zExMfr27dtyfPv27ToiIkKfOXNGnkP37t114cKF9Z9//intt27dkuvwfHEd9p2dmyN8fHx0eHh4ou19+/bVefLkMZ93YlSqVEk3atTIbtuSJUtk7EeOHHFqTPfv35fz8ZUQQgghhGQ8kvO+RoPMhTx58kS/+uqret++fVbHYUx06NDBNMhgfBhs2LBBjj1+/Fj2/f39dceOHe3e/+zZs3Lu3r17zWOxsbE6d+7cesWKFbKPfpo1a2Z1XXBwsJVBFhgYaGXkARg/RYsWNffRT//+/XVa0KJFC92tWze7bTDQnDFOXrx4of/yl7/odevWWY1xzZo1Vuc5M7ekDLIcOXJoLy8vqw2GHmjatKkYW0mRK1cu3a9fP7tthw8flrEvX7480e8j/DAb25UrV2iQEUIIIYRkEoOMOWQu5Pz58+rRo0cSvmZJXFychOQZoFCyQdGiReXrrVu3JMzw6NGjCfKXDM6cOSO5UchBMkCh5TJlykibcY5liCNAkWMUMTY4duyYFMG2DEOEMAWKRmP8CHUECKtMC3r37q3atGkjIYcoug0RDoT6OQL5VsOGDZMwTzwbjA9jS0od0dm5OWLgwIFS48ySN998U74mJ8wwpSGJCHlFmCUhhBBCCMl80CBzIQ8ePJCvGzZsMF/gDXLmzKl+//13+WwpjoH8JyP3DDhSMkzLceKFv3Xr1gnakHdlYORNpRbkuUFlcOPGjWrr1q0qMDBQhYaGSk2vxEAuFnLfpk6dKvlXeH4wLGHcpsXcHIG8tcQEQXx9fdWePXuSFDnBeYaRbItxHOfY45tvvlF///vfrVR7ihcv7tTYCSGEEEJIxoaiHi4Esu0wHODFwQu95ebsCzW8ZxD9sAfEICBOceDAAfMYjBaIgqBv4xzLdrB//36rfQhe4BrbMWLz8HDNtwgEPWBkLV68WAQv5s6dK8dz5MhherEsgZerb9++qlmzZiIRj+caGxtrdQ4MItvrXD23v/71r2L0JVYM2igt8Mknn4iSIjx2lsDwDg8Pl/WC4Is9MFfIpVpuhBBCCCEkc0APmQuBOt+AAQNEDh0v3nXr1pVaBDAu8FINT09ShIWFiQfpnXfekZd6GGDwLA0aNEjUCYOCgiSkcc6cOdIfamPBG4fjAEZMnTp1xPuEY5GRkVbhimDEiBGqefPmEiKJ2lkwVGA4nDx5Un333Xdp/lzQX/Xq1cWwghw8lAdhOIJChQqJVxBj9Pb2Fi8WVBUxV0jII2wSHiKEEdp6D6FuCOMV84URA+XJtJjb//73P3Xjxg2rYwh1xBoiXPTrr79WX331ldRTQ3goZO8RrgrZe6w5ZO/xPQC1xxYtWljJ3kOhER4yGGuGd5QQQgghhGQh0iWrLQsTHx+vp0yZosuUKaOzZ8+u33jjDVFC3LlzpynqcffuXfN8iFngGMQtDFatWiWqhBCXKFiwoG7durXZdufOHR0SEiIiHRDzwL0h9mHJ/Pnztbe3t7RDUGPSpElWoh4AKpBQd8Q5UCWsWbOmnjt3rkPBjJQyevRoUYZEX/nz59dBQUH6woULZvu8efN08eLFRR0yICDAFL547733RByjdOnSeuXKlQkUEKHUCIXJbNmySZuzc3ME7oO52249e/a0Og+CHPXr1xehEYh+QOhj1KhRVmv78OFDPXToUBkjvhcw9zZt2ugTJ04k6/lRZZEQQgghJGOTnPe1V/CPu41CQohrKr8TQgghhJCM/b7GHDJCCCGEEEIIcRM0yEiK6NWrl8qTJ4/dLam2jMKSJUsSHSfy2wghhBBCCHE1DFnM4ixatEj179/fVAN0FtQCgyvWHnDLOmqDcEdGAGIdENawBxQbnRFdcQcMWSSEEEIIydgk532NKoskRcCocmRYpYXRdenSJVWyZEl15MgRVaVKlWRdC8XCNWvWSNHpxIAqJTZngYoj6qcBKDxC+RIKij169LA6b968eeof//iH1JlD4W7MoX379lJPzPIe9kApABjJhBBCCCEka0CDjKSapIoiZyZGjRolZQYePXqkVq5cKZ9RZgDFrsGCBQvE4zht2jQVEBAgsv7Hjx8XmX1w6NAhs1bavn37VJs2baROmvGXk/QoBE4IIYQQQjIOzCFLR1CLbOzYseIxwYs3CgH/+OOP0vbLL7+IVwd1tFBrC3WuateuLS/rlqxbt07VqFFD6nMVLFhQ6l4Z3L17V3Xu3Fnqb+F6GAnnzp2zuh7eF9TkQjuuRSFpW1AvCwWV0cfbb7+tRo4cKfXPDDDOWbNmqZYtWyovLy/1/fffp/iZYMwdO3aUQtF4Jqg3tnDhQmnDcwJVq1aVPhs0aGAaNR9++KHMH65gGD6HDx827wkvFMD8cJ2x78zckgIetSJFisi1qAWXP39+tXXrVrN97dq14g3r3r27FJ9GLlqHDh3MZ4R54npsuNbwJhrHMB9CCCGEEJJ1oEGWjsAY+9e//iUFg0+dOiXFgjt16qR27txpnjN06FApHPzrr79KuNunn35qtm3YsEGMjGbNmkkYH4y3mjVrmu1du3aV62AUREVFocacnAsPFjhw4IAYCl988YU6evSoev/99xMUR969e7cYdQjFO336tBSchhFna3R9++23MpYTJ05YjTG5DB8+XPrZtGmTFEiGoQdDCxw8eFC+omhyTEyMWr16tZn7hdC+PXv2qP3794sRh3niuGGwARh2uM7Yd3ZuzhrXq1atEoMyR44c5nEYVRiTo7DE5AIvG+KQLTdCCCGEEJJJSIe6aERr/eTJE/3qq6/qffv2WR3v3r277tChg1kketu2bWbbhg0b5Njjx49l39/fX3fs2NHu/VEMGufu3bvXPBYbGyvFkFesWCH76KdZs2ZW1wUHB1sViQ4MDNRjxoyxOiciIkIXLVrU3Ec//fv312kBClV369bNbhuKY6MvFMt2xIsXL6Qg87p16xwWsnZmbkkViUZxbhR+RvFp9IHizufOnTPPuX79uq5Vq5a0+fr66i5dukjRaIzRFnuFwe0RFhZmtzg1C0MTQgghhLz8haHpIUsnzp8/L3lHCLWzlFeHxwziDwaVKlUyPxctWtRUNATwagUGBtq9P7xL8Kj5+fmZxwoUKKDKlCkjbcY5lu3A39/fav/YsWOSJ2U5RuRJwdOE8RsgrDIt6N27t1q2bJmIdnz99deSV5UUUEbEmOAZQ4gf8q8ePHig/vjjD4fXOTs3RwwcOFDW4d///rc8y/DwcAlNtFwzeCfhOYQnDuGQ8OY1adJEvGopAWIgUOgxtitXrqToPoQQQgghJONBUY90AgaDEXYIEQhLcubMaRplluIYyH8Cxot8egg+YJzIq2rdunWCNuRdGSB3LC1AnhvC+zZu3Ci5WDA4Q0ND1aRJkxK9BgYOct+mTp0q0vR4fjAs4+Li0mRujkA4JQwwbBD1qFixohin5cuXtzrv3Xffla1Pnz5Se61evXoSmoow0eSC+WEjhBBCCCGZDxpk6QRe2PFSDS8ORChssfSSJQa8Z8gb69atW4K2cuXKiTcGeWIQAwEwWiAKYhgLOAftliDfyRIIXuAaS6+Pq4HQBYwsbDBc4IWCQWbkZhmqhAZ79+5VM2fOlLwxAI9RbGys1TkwbG2vS+u5FS9eXAUHB4sHC2IhiWE8/4cPH6ZJv4QQQgghJPNAgyydgDrfgAEDRMgDHq+6detK+BmMC4TcOVOEOCwsTDxIqH/1ySefiAEGzxLU/hC+FxQUJCF4EKtAf4MHDxZvHI6Dvn37qjp16oixg2ORkZFq8+bNVn2MGDFCNW/eXJQY27Ztqzw8PCTUD7LttgIgaQH6q169uqgRQrxi/fr1Yjga6oPwCmKM3t7e4sVCiCLmGhERIZ4pCFzAgLP1HkJZEcYr5gtDGMqTrpgbwhLhCYOYCsaDEMxixYqpDz74QMaMcEjcG0anbXgoIYQQQgghzCFLR0aPHi2qglBbhNGBvCKEMBry7kkB2XeEyUFFETlXeOk3lAgNVUEYNzA68PIPbQsYbEYYZK1ataRoMUL9ILm/ZcsWNWzYMKs+GjduLEYR2iCvj2uQJ+WMwZgS4AWDhwnev/r16ytPT0/JKQPIiUM9LxiYMHIMw3L+/PmibgiPV0hIiBiatoWooVSJEEh4sSCb76q5wfvVqFEjMfZAw4YNxevYrl075evrK3XGYEjCOEROHyGEEEIIIZa8AmUPqyOEkAwNPKv58uWTUE2joDQhhBBCCMk4IIoLjoF79+4lWWeWIYuEvGQYxbzxQ04IIYQQQjIuqJNLg4y4HKgILl682G4bCl87akOR7IzAkiVLVM+ePe22IaQRhbwzCvnz55evEIhJ6gecZL6/tNEzmnXgmmdNuO5ZD6555gRBiDDGkHaTFAxZJKkGddLwy8Qe+MXiqM0298td4AcG9c3sgRw8V+XQpQQ8TxhiCF3kL+6sA9c968E1z5pw3bMeXHNCDxlJNTCqHBlWGcXocgRUKbERQgghhBCSnlBlkRBCCCGEEELcBA0yQl4yUFcNNenwlWQduO5ZD6551oTrnvXgmhPmkBFCCCGEEEKIm6CHjBBCCCGEEELcBA0yQgghhBBCCHETNMgIIYQQQgghxE3QICOEEEIIIYQQN0GDjJAMwIwZM9Rbb72lcuXKpfz8/NTBgwcdnr9y5UpVtmxZOb9ixYpq48aNVu3Q6hkxYoQqWrSoyp07t2rYsKE6d+6ci2dB3LXmz549U4MGDZLjXl5eqlixYqpz587q+vXr6TAT4s6fdUt69eqlXnnlFTVlyhQXjJxkpDU/c+aMatmypRQTxs98jRo11B9//OHCWRB3r/uDBw/UF198oby9veX/9fLly6vZs2e7eBYk3YDKIiHEfSxbtkznyJFDL1iwQJ86dUp/9tlnOl++fPrmzZt2z9+7d6/29PTUEyZM0KdPn9bDhg3T2bNn1ydOnDDPGTdunM6bN6/+6aef9LFjx3TLli11yZIl9ePHj9NxZiS91vzevXu6YcOGevny5fq3337TUVFRumbNmrp69erpPDOS3j/rBqtXr9aVK1fWxYoV0+Hh4ekwG+KuNT9//rzOnz+/HjhwoD58+LDs//zzz4nek2SOdcc93nnnHb1jxw598eJFPWfOHLkGa09efmiQEeJm8OIcGhpq7r948UJeqsaOHWv3/Pbt2+uPPvrI6pifn5/u2bOnfI6Pj9dFihTREydONNvxwp4zZ069dOlSl82DuG/N7XHw4EGUNNGXL19Ow5GTjLjuV69e1W+++aY+efKk9vHxoUGWydc8ODhYd+rUyYWjJhlx3StUqKBHjRpldU61atX00KFD03z8JP1hyCIhbiQuLk795z//kZBCAw8PD9mPioqyew2OW54PGjdubJ5/8eJFdePGDatzENaCkInE7kle7jW3x/379yV8LV++fGk4epLR1j0+Pl6FhISogQMHqgoVKrhwBiQjrDnWe8OGDcrX11eOFypUSH63//TTTy6eDXH3z3rt2rXV2rVr1bVr1yQtYceOHers2bOqUaNGLpwNSS9okBHiRmJjY9WLFy9U4cKFrY5jH0aVPXDc0fnG1+Tck7zca27LkydPJKesQ4cO6rXXXkvD0ZOMtu7jx49X2bJlU3379nXRyElGWvNbt25JLtG4ceNUkyZN1JYtW1SrVq1U69at1c6dO104G+Lun/Xp06dL3hhyyHLkyCHrjzy1+vXru2gmJD3Jlq69EUIIcSkQ+Gjfvr38BXXWrFnuHg5xIfgr/NSpU9Xhw4fFG0oyP/CQgaCgIPXll1/K5ypVqqh9+/aJwENAQICbR0hcBQyy/fv3i5fMx8dH7dq1S4WGhoqIk613jbx80ENGiBspWLCg8vT0VDdv3rQ6jv0iRYrYvQbHHZ1vfE3OPcnLvea2xtjly5fV1q1b6R3L5Ou+e/du8ZiUKFFCvGTYsPZfffWVqLuRzLfmuCfWGZ4SS8qVK0eVxUy87o8fP1ZDhgxRP/zwg2rRooWqVKmSKC4GBwerSZMmuXA2JL2gQUaIG0HYQfXq1dX27dut/gKKfX9/f7vX4Ljl+QAv38b5JUuWlF/iluf8+eef6sCBA4nek7zca25pjKG8wbZt21SBAgVcOAuSEdYduWPHjx9XR48eNTf8tRz5ZJGRkS6eEXHHmuOekLiPjo62Oge5RPCakMy57vj9jg25aJbA8DO8puQlxw1CIoQQG3lcKCAuWrRI5G4///xzkce9ceOGtIeEhOjBgwdbyeNmy5ZNT5o0SZ85c0aHhYXZlb3HPSCHe/z4cR0UFETZ+0y85nFxcVLawNvbWx89elTHxMSY29OnT902T+L6n3VbqLKY+dccJQ5wbO7cufrcuXN6+vTpIn++e/dut8yRpM+6BwQEiNIiZO8vXLigFy5cqHPlyqVnzpzpljmStIUGGSEZAPyHWqJECalbArnc/fv3W/0S7tKli9X5K1as0L6+vnI+fkFv2LDBqh3S98OHD9eFCxeW/xQCAwN1dHR0us2HpO+aoyYN/r5mb8N/3iTz/qzbQoMsa6z5/PnzdalSpeSFHPXnUHOSZO51xx/YunbtKvL5WPcyZcroyZMny//35OXnFfzjbi8dIYQQQgghhGRFmENGCCGEEEIIIW6CBhkhhBBCCCGEuAkaZIQQQgghhBDiJmiQEUIIIYQQQoiboEFGCCGEEEIIIW6CBhkhhBBCCCGEuAkaZIQQQgghhBDiJmiQEUIIIYQQQoiboEFGCCGEEEIIIW6CBhkhhBBCCCGEuAkaZIQQQgghhBDiJmiQEUIIIYQQQohyD/8PtpjIMUaBXmIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.002289\n",
       "encoder__proto_ospf             0.000760\n",
       "encoder__proto_sctp             0.000771\n",
       "encoder__proto_tcp              0.001969\n",
       "encoder__proto_udp              0.025228\n",
       "encoder__proto_unas             0.001297\n",
       "encoder__service_dns            0.047050\n",
       "encoder__service_ftp            0.000409\n",
       "encoder__service_ftp-data       0.000504\n",
       "encoder__service_http           0.004399\n",
       "encoder__service_smtp           0.000748\n",
       "encoder__state_CON              0.002900\n",
       "encoder__state_ECO              0.000027\n",
       "encoder__state_FIN              0.000637\n",
       "encoder__state_INT              0.007103\n",
       "encoder__state_REQ              0.000588\n",
       "encoder__state_RST              0.000013\n",
       "remainder__dur                  0.020717\n",
       "remainder__spkts                0.010003\n",
       "remainder__dpkts                0.009122\n",
       "remainder__sbytes               0.073659\n",
       "remainder__dbytes               0.021778\n",
       "remainder__rate                 0.024504\n",
       "remainder__sttl                 0.094045\n",
       "remainder__dttl                 0.012907\n",
       "remainder__sload                0.033948\n",
       "remainder__dload                0.033277\n",
       "remainder__sloss                0.007421\n",
       "remainder__dloss                0.010422\n",
       "remainder__sinpkt               0.017457\n",
       "remainder__dinpkt               0.014835\n",
       "remainder__sjit                 0.013910\n",
       "remainder__djit                 0.013893\n",
       "remainder__swin                 0.001573\n",
       "remainder__stcpb                0.007878\n",
       "remainder__dtcpb                0.008845\n",
       "remainder__dwin                 0.000472\n",
       "remainder__tcprtt               0.025203\n",
       "remainder__synack               0.014213\n",
       "remainder__ackdat               0.017930\n",
       "remainder__smean                0.050911\n",
       "remainder__dmean                0.026252\n",
       "remainder__trans_depth          0.002718\n",
       "remainder__response_body_len    0.002878\n",
       "remainder__ct_srv_src           0.034274\n",
       "remainder__ct_state_ttl         0.046782\n",
       "remainder__ct_dst_ltm           0.021318\n",
       "remainder__ct_src_dport_ltm     0.047888\n",
       "remainder__ct_dst_sport_ltm     0.049038\n",
       "remainder__ct_dst_src_ltm       0.043798\n",
       "remainder__is_ftp_login         0.000131\n",
       "remainder__ct_ftp_cmd           0.000190\n",
       "remainder__ct_flw_http_mthd     0.003045\n",
       "remainder__ct_src_ltm           0.018829\n",
       "remainder__ct_srv_dst           0.065233\n",
       "remainder__is_sm_ips_ports      0.002014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e2a8d00-9e61-4d23-b6d7-c0b81cf1312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,10, 4,10],grid_size = 4, spline_order = 2, scale_noise=0.1, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80abf3db-1a4d-4fd2-82fb-31725d75010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KAN([40, 5,4, 2,10],grid_size = 4, spline_order = 2, scale_noise=0.1, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.440901  [    0/175341]\n",
      "loss: 0.369255  [ 1600/175341]\n",
      "loss: 0.382019  [ 3200/175341]\n",
      "loss: 0.722621  [ 4800/175341]\n",
      "loss: 0.332475  [ 6400/175341]\n",
      "loss: 0.551266  [ 8000/175341]\n",
      "loss: 0.702944  [ 9600/175341]\n",
      "loss: 0.886902  [11200/175341]\n",
      "loss: 0.751396  [12800/175341]\n",
      "loss: 0.971538  [14400/175341]\n",
      "loss: 0.460648  [16000/175341]\n",
      "loss: 0.480023  [17600/175341]\n",
      "loss: 0.547781  [19200/175341]\n",
      "loss: 0.196432  [20800/175341]\n",
      "loss: 0.738696  [22400/175341]\n",
      "loss: 0.273961  [24000/175341]\n",
      "loss: 0.529558  [25600/175341]\n",
      "loss: 0.923234  [27200/175341]\n",
      "loss: 0.718976  [28800/175341]\n",
      "loss: 0.354098  [30400/175341]\n",
      "loss: 0.311513  [32000/175341]\n",
      "loss: 0.254862  [33600/175341]\n",
      "loss: 0.334361  [35200/175341]\n",
      "loss: 0.623524  [36800/175341]\n",
      "loss: 0.195947  [38400/175341]\n",
      "loss: 0.317851  [40000/175341]\n",
      "loss: 0.157714  [41600/175341]\n",
      "loss: 0.167166  [43200/175341]\n",
      "loss: 0.580817  [44800/175341]\n",
      "loss: 0.890907  [46400/175341]\n",
      "loss: 0.452711  [48000/175341]\n",
      "loss: 0.477400  [49600/175341]\n",
      "loss: 0.366636  [51200/175341]\n",
      "loss: 0.367640  [52800/175341]\n",
      "loss: 0.460399  [54400/175341]\n",
      "loss: 0.301383  [56000/175341]\n",
      "loss: 0.644834  [57600/175341]\n",
      "loss: 0.941367  [59200/175341]\n",
      "loss: 0.719833  [60800/175341]\n",
      "loss: 0.252438  [62400/175341]\n",
      "loss: 0.948712  [64000/175341]\n",
      "loss: 0.291951  [65600/175341]\n",
      "loss: 0.318341  [67200/175341]\n",
      "loss: 0.785863  [68800/175341]\n",
      "loss: 0.147019  [70400/175341]\n",
      "loss: 0.747192  [72000/175341]\n",
      "loss: 0.477321  [73600/175341]\n",
      "loss: 0.298580  [75200/175341]\n",
      "loss: 0.203355  [76800/175341]\n",
      "loss: 0.648609  [78400/175341]\n",
      "loss: 0.443481  [80000/175341]\n",
      "loss: 0.452686  [81600/175341]\n",
      "loss: 0.626274  [83200/175341]\n",
      "loss: 0.494829  [84800/175341]\n",
      "loss: 0.460528  [86400/175341]\n",
      "loss: 0.348027  [88000/175341]\n",
      "loss: 0.737270  [89600/175341]\n",
      "loss: 0.476735  [91200/175341]\n",
      "loss: 0.136813  [92800/175341]\n",
      "loss: 0.831259  [94400/175341]\n",
      "loss: 0.621058  [96000/175341]\n",
      "loss: 0.290042  [97600/175341]\n",
      "loss: 0.771890  [99200/175341]\n",
      "loss: 0.669189  [100800/175341]\n",
      "loss: 0.642838  [102400/175341]\n",
      "loss: 0.469314  [104000/175341]\n",
      "loss: 0.305808  [105600/175341]\n",
      "loss: 0.201118  [107200/175341]\n",
      "loss: 0.328621  [108800/175341]\n",
      "loss: 0.758146  [110400/175341]\n",
      "loss: 0.437552  [112000/175341]\n",
      "loss: 0.959045  [113600/175341]\n",
      "loss: 0.302384  [115200/175341]\n",
      "loss: 0.511196  [116800/175341]\n",
      "loss: 0.569833  [118400/175341]\n",
      "loss: 0.970216  [120000/175341]\n",
      "loss: 0.546396  [121600/175341]\n",
      "loss: 0.803729  [123200/175341]\n",
      "loss: 0.692670  [124800/175341]\n",
      "loss: 0.593176  [126400/175341]\n",
      "loss: 0.839668  [128000/175341]\n",
      "loss: 0.365682  [129600/175341]\n",
      "loss: 0.481112  [131200/175341]\n",
      "loss: 0.862177  [132800/175341]\n",
      "loss: 0.736738  [134400/175341]\n",
      "loss: 0.395687  [136000/175341]\n",
      "loss: 0.740845  [137600/175341]\n",
      "loss: 0.342121  [139200/175341]\n",
      "loss: 0.341914  [140800/175341]\n",
      "loss: 0.601872  [142400/175341]\n",
      "loss: 0.786773  [144000/175341]\n",
      "loss: 0.590963  [145600/175341]\n",
      "loss: 0.231239  [147200/175341]\n",
      "loss: 0.442073  [148800/175341]\n",
      "loss: 0.476053  [150400/175341]\n",
      "loss: 0.537302  [152000/175341]\n",
      "loss: 0.371641  [153600/175341]\n",
      "loss: 0.209021  [155200/175341]\n",
      "loss: 0.606803  [156800/175341]\n",
      "loss: 0.553542  [158400/175341]\n",
      "loss: 0.612649  [160000/175341]\n",
      "loss: 0.346321  [161600/175341]\n",
      "loss: 0.673950  [163200/175341]\n",
      "loss: 0.453481  [164800/175341]\n",
      "loss: 0.161237  [166400/175341]\n",
      "loss: 0.671878  [168000/175341]\n",
      "loss: 0.170227  [169600/175341]\n",
      "loss: 0.443004  [171200/175341]\n",
      "loss: 0.336383  [172800/175341]\n",
      "loss: 0.777946  [174400/175341]\n",
      "Train Accuracy: 79.6522%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.604462, F1-score: 74.31%, Macro_F1-Score:  37.03%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.539019  [    0/175341]\n",
      "loss: 0.445374  [ 1600/175341]\n",
      "loss: 0.510254  [ 3200/175341]\n",
      "loss: 0.406198  [ 4800/175341]\n",
      "loss: 0.686094  [ 6400/175341]\n",
      "loss: 0.124086  [ 8000/175341]\n",
      "loss: 0.316816  [ 9600/175341]\n",
      "loss: 0.291235  [11200/175341]\n",
      "loss: 0.562393  [12800/175341]\n",
      "loss: 0.698802  [14400/175341]\n",
      "loss: 0.306569  [16000/175341]\n",
      "loss: 0.711663  [17600/175341]\n",
      "loss: 0.262650  [19200/175341]\n",
      "loss: 0.169056  [20800/175341]\n",
      "loss: 0.934991  [22400/175341]\n",
      "loss: 0.397801  [24000/175341]\n",
      "loss: 0.457647  [25600/175341]\n",
      "loss: 0.488693  [27200/175341]\n",
      "loss: 0.207230  [28800/175341]\n",
      "loss: 0.800366  [30400/175341]\n",
      "loss: 0.378692  [32000/175341]\n",
      "loss: 0.581004  [33600/175341]\n",
      "loss: 0.232118  [35200/175341]\n",
      "loss: 0.826998  [36800/175341]\n",
      "loss: 0.505867  [38400/175341]\n",
      "loss: 0.909353  [40000/175341]\n",
      "loss: 0.318662  [41600/175341]\n",
      "loss: 0.281441  [43200/175341]\n",
      "loss: 0.219749  [44800/175341]\n",
      "loss: 0.333485  [46400/175341]\n",
      "loss: 0.354603  [48000/175341]\n",
      "loss: 0.200316  [49600/175341]\n",
      "loss: 0.480176  [51200/175341]\n",
      "loss: 0.411517  [52800/175341]\n",
      "loss: 0.327573  [54400/175341]\n",
      "loss: 0.343513  [56000/175341]\n",
      "loss: 0.269263  [57600/175341]\n",
      "loss: 0.098668  [59200/175341]\n",
      "loss: 0.587562  [60800/175341]\n",
      "loss: 0.839536  [62400/175341]\n",
      "loss: 0.426805  [64000/175341]\n",
      "loss: 0.502427  [65600/175341]\n",
      "loss: 0.916756  [67200/175341]\n",
      "loss: 0.709184  [68800/175341]\n",
      "loss: 0.373306  [70400/175341]\n",
      "loss: 0.483669  [72000/175341]\n",
      "loss: 0.428568  [73600/175341]\n",
      "loss: 0.730601  [75200/175341]\n",
      "loss: 0.450336  [76800/175341]\n",
      "loss: 0.833068  [78400/175341]\n",
      "loss: 0.582639  [80000/175341]\n",
      "loss: 0.559456  [81600/175341]\n",
      "loss: 0.405617  [83200/175341]\n",
      "loss: 0.517305  [84800/175341]\n",
      "loss: 0.224312  [86400/175341]\n",
      "loss: 0.678976  [88000/175341]\n",
      "loss: 0.263165  [89600/175341]\n",
      "loss: 1.321360  [91200/175341]\n",
      "loss: 0.649380  [92800/175341]\n",
      "loss: 0.457838  [94400/175341]\n",
      "loss: 0.205751  [96000/175341]\n",
      "loss: 0.512008  [97600/175341]\n",
      "loss: 0.659529  [99200/175341]\n",
      "loss: 0.275433  [100800/175341]\n",
      "loss: 0.424921  [102400/175341]\n",
      "loss: 0.599008  [104000/175341]\n",
      "loss: 0.478598  [105600/175341]\n",
      "loss: 0.382422  [107200/175341]\n",
      "loss: 0.665392  [108800/175341]\n",
      "loss: 0.420865  [110400/175341]\n",
      "loss: 0.105486  [112000/175341]\n",
      "loss: 0.331263  [113600/175341]\n",
      "loss: 0.750815  [115200/175341]\n",
      "loss: 1.370792  [116800/175341]\n",
      "loss: 0.791395  [118400/175341]\n",
      "loss: 0.443574  [120000/175341]\n",
      "loss: 0.446038  [121600/175341]\n",
      "loss: 0.433991  [123200/175341]\n",
      "loss: 0.542244  [124800/175341]\n",
      "loss: 0.922441  [126400/175341]\n",
      "loss: 0.447647  [128000/175341]\n",
      "loss: 0.424396  [129600/175341]\n",
      "loss: 0.642901  [131200/175341]\n",
      "loss: 0.420789  [132800/175341]\n",
      "loss: 0.847642  [134400/175341]\n",
      "loss: 0.635459  [136000/175341]\n",
      "loss: 0.411167  [137600/175341]\n",
      "loss: 0.364994  [139200/175341]\n",
      "loss: 0.372325  [140800/175341]\n",
      "loss: 0.698942  [142400/175341]\n",
      "loss: 0.361489  [144000/175341]\n",
      "loss: 0.417392  [145600/175341]\n",
      "loss: 0.629978  [147200/175341]\n",
      "loss: 0.428905  [148800/175341]\n",
      "loss: 0.770501  [150400/175341]\n",
      "loss: 0.820336  [152000/175341]\n",
      "loss: 0.399089  [153600/175341]\n",
      "loss: 0.665839  [155200/175341]\n",
      "loss: 0.687444  [156800/175341]\n",
      "loss: 0.768930  [158400/175341]\n",
      "loss: 0.549770  [160000/175341]\n",
      "loss: 0.537038  [161600/175341]\n",
      "loss: 0.909032  [163200/175341]\n",
      "loss: 0.249410  [164800/175341]\n",
      "loss: 0.302157  [166400/175341]\n",
      "loss: 0.376217  [168000/175341]\n",
      "loss: 0.751742  [169600/175341]\n",
      "loss: 0.470860  [171200/175341]\n",
      "loss: 0.332998  [172800/175341]\n",
      "loss: 0.098485  [174400/175341]\n",
      "Train Accuracy: 79.6425%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.614952, F1-score: 73.80%, Macro_F1-Score:  36.54%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.154187  [    0/175341]\n",
      "loss: 0.686250  [ 1600/175341]\n",
      "loss: 0.699818  [ 3200/175341]\n",
      "loss: 0.506628  [ 4800/175341]\n",
      "loss: 0.446294  [ 6400/175341]\n",
      "loss: 0.400685  [ 8000/175341]\n",
      "loss: 0.671105  [ 9600/175341]\n",
      "loss: 0.435856  [11200/175341]\n",
      "loss: 0.239513  [12800/175341]\n",
      "loss: 0.638093  [14400/175341]\n",
      "loss: 0.558356  [16000/175341]\n",
      "loss: 0.410482  [17600/175341]\n",
      "loss: 0.196168  [19200/175341]\n",
      "loss: 0.672351  [20800/175341]\n",
      "loss: 0.321868  [22400/175341]\n",
      "loss: 0.204646  [24000/175341]\n",
      "loss: 0.503894  [25600/175341]\n",
      "loss: 0.420553  [27200/175341]\n",
      "loss: 0.701366  [28800/175341]\n",
      "loss: 0.658880  [30400/175341]\n",
      "loss: 0.752717  [32000/175341]\n",
      "loss: 0.199701  [33600/175341]\n",
      "loss: 0.432664  [35200/175341]\n",
      "loss: 0.796213  [36800/175341]\n",
      "loss: 0.806739  [38400/175341]\n",
      "loss: 0.184454  [40000/175341]\n",
      "loss: 0.398361  [41600/175341]\n",
      "loss: 0.560087  [43200/175341]\n",
      "loss: 0.512518  [44800/175341]\n",
      "loss: 0.732635  [46400/175341]\n",
      "loss: 0.379147  [48000/175341]\n",
      "loss: 0.985953  [49600/175341]\n",
      "loss: 0.423174  [51200/175341]\n",
      "loss: 0.306287  [52800/175341]\n",
      "loss: 0.653781  [54400/175341]\n",
      "loss: 0.412582  [56000/175341]\n",
      "loss: 0.343303  [57600/175341]\n",
      "loss: 0.435639  [59200/175341]\n",
      "loss: 0.359310  [60800/175341]\n",
      "loss: 0.361086  [62400/175341]\n",
      "loss: 0.685552  [64000/175341]\n",
      "loss: 0.352823  [65600/175341]\n",
      "loss: 0.156533  [67200/175341]\n",
      "loss: 0.867185  [68800/175341]\n",
      "loss: 0.498110  [70400/175341]\n",
      "loss: 0.546985  [72000/175341]\n",
      "loss: 0.547222  [73600/175341]\n",
      "loss: 0.565091  [75200/175341]\n",
      "loss: 0.704607  [76800/175341]\n",
      "loss: 0.410876  [78400/175341]\n",
      "loss: 0.683037  [80000/175341]\n",
      "loss: 0.334081  [81600/175341]\n",
      "loss: 0.306573  [83200/175341]\n",
      "loss: 0.495779  [84800/175341]\n",
      "loss: 0.888831  [86400/175341]\n",
      "loss: 0.428200  [88000/175341]\n",
      "loss: 0.400962  [89600/175341]\n",
      "loss: 0.556456  [91200/175341]\n",
      "loss: 0.532555  [92800/175341]\n",
      "loss: 0.695132  [94400/175341]\n",
      "loss: 0.800939  [96000/175341]\n",
      "loss: 0.578150  [97600/175341]\n",
      "loss: 0.249937  [99200/175341]\n",
      "loss: 0.402979  [100800/175341]\n",
      "loss: 0.674552  [102400/175341]\n",
      "loss: 1.130689  [104000/175341]\n",
      "loss: 0.260430  [105600/175341]\n",
      "loss: 0.530660  [107200/175341]\n",
      "loss: 0.357192  [108800/175341]\n",
      "loss: 1.485574  [110400/175341]\n",
      "loss: 0.588865  [112000/175341]\n",
      "loss: 0.819905  [113600/175341]\n",
      "loss: 0.733694  [115200/175341]\n",
      "loss: 0.594929  [116800/175341]\n",
      "loss: 0.499329  [118400/175341]\n",
      "loss: 0.413888  [120000/175341]\n",
      "loss: 1.020768  [121600/175341]\n",
      "loss: 0.648387  [123200/175341]\n",
      "loss: 0.462020  [124800/175341]\n",
      "loss: 0.376049  [126400/175341]\n",
      "loss: 0.807850  [128000/175341]\n",
      "loss: 0.622539  [129600/175341]\n",
      "loss: 0.714104  [131200/175341]\n",
      "loss: 0.664287  [132800/175341]\n",
      "loss: 0.232227  [134400/175341]\n",
      "loss: 0.390650  [136000/175341]\n",
      "loss: 0.279472  [137600/175341]\n",
      "loss: 0.624993  [139200/175341]\n",
      "loss: 0.282560  [140800/175341]\n",
      "loss: 0.514587  [142400/175341]\n",
      "loss: 0.488203  [144000/175341]\n",
      "loss: 0.561035  [145600/175341]\n",
      "loss: 0.209500  [147200/175341]\n",
      "loss: 0.650763  [148800/175341]\n",
      "loss: 0.640499  [150400/175341]\n",
      "loss: 0.615407  [152000/175341]\n",
      "loss: 0.749167  [153600/175341]\n",
      "loss: 0.436463  [155200/175341]\n",
      "loss: 1.112082  [156800/175341]\n",
      "loss: 0.569762  [158400/175341]\n",
      "loss: 0.280662  [160000/175341]\n",
      "loss: 0.101336  [161600/175341]\n",
      "loss: 0.967741  [163200/175341]\n",
      "loss: 0.634884  [164800/175341]\n",
      "loss: 0.149439  [166400/175341]\n",
      "loss: 0.267305  [168000/175341]\n",
      "loss: 0.488545  [169600/175341]\n",
      "loss: 0.508753  [171200/175341]\n",
      "loss: 0.659395  [172800/175341]\n",
      "loss: 0.501415  [174400/175341]\n",
      "Train Accuracy: 79.7018%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.596655, F1-score: 74.96%, Macro_F1-Score:  36.76%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.589487  [    0/175341]\n",
      "loss: 1.053396  [ 1600/175341]\n",
      "loss: 0.975074  [ 3200/175341]\n",
      "loss: 0.633394  [ 4800/175341]\n",
      "loss: 0.803932  [ 6400/175341]\n",
      "loss: 0.117749  [ 8000/175341]\n",
      "loss: 0.579388  [ 9600/175341]\n",
      "loss: 0.716345  [11200/175341]\n",
      "loss: 0.741381  [12800/175341]\n",
      "loss: 0.499733  [14400/175341]\n",
      "loss: 0.824497  [16000/175341]\n",
      "loss: 0.587063  [17600/175341]\n",
      "loss: 0.599097  [19200/175341]\n",
      "loss: 0.481155  [20800/175341]\n",
      "loss: 0.366570  [22400/175341]\n",
      "loss: 0.668259  [24000/175341]\n",
      "loss: 0.460727  [25600/175341]\n",
      "loss: 0.442835  [27200/175341]\n",
      "loss: 0.729780  [28800/175341]\n",
      "loss: 0.272956  [30400/175341]\n",
      "loss: 0.330033  [32000/175341]\n",
      "loss: 0.253465  [33600/175341]\n",
      "loss: 0.378096  [35200/175341]\n",
      "loss: 0.439681  [36800/175341]\n",
      "loss: 0.272459  [38400/175341]\n",
      "loss: 0.225572  [40000/175341]\n",
      "loss: 0.373114  [41600/175341]\n",
      "loss: 0.348738  [43200/175341]\n",
      "loss: 0.121664  [44800/175341]\n",
      "loss: 0.160712  [46400/175341]\n",
      "loss: 0.628648  [48000/175341]\n",
      "loss: 0.366242  [49600/175341]\n",
      "loss: 1.120653  [51200/175341]\n",
      "loss: 0.388566  [52800/175341]\n",
      "loss: 0.372356  [54400/175341]\n",
      "loss: 0.411673  [56000/175341]\n",
      "loss: 0.329443  [57600/175341]\n",
      "loss: 0.194576  [59200/175341]\n",
      "loss: 0.697391  [60800/175341]\n",
      "loss: 0.430084  [62400/175341]\n",
      "loss: 0.370960  [64000/175341]\n",
      "loss: 0.455547  [65600/175341]\n",
      "loss: 0.773466  [67200/175341]\n",
      "loss: 0.414750  [68800/175341]\n",
      "loss: 0.806906  [70400/175341]\n",
      "loss: 0.446557  [72000/175341]\n",
      "loss: 0.405381  [73600/175341]\n",
      "loss: 0.859482  [75200/175341]\n",
      "loss: 0.298346  [76800/175341]\n",
      "loss: 0.286880  [78400/175341]\n",
      "loss: 0.203501  [80000/175341]\n",
      "loss: 0.488190  [81600/175341]\n",
      "loss: 0.286184  [83200/175341]\n",
      "loss: 0.640537  [84800/175341]\n",
      "loss: 0.923261  [86400/175341]\n",
      "loss: 0.371829  [88000/175341]\n",
      "loss: 0.347018  [89600/175341]\n",
      "loss: 0.665009  [91200/175341]\n",
      "loss: 0.494514  [92800/175341]\n",
      "loss: 0.472606  [94400/175341]\n",
      "loss: 0.530863  [96000/175341]\n",
      "loss: 1.027517  [97600/175341]\n",
      "loss: 0.321553  [99200/175341]\n",
      "loss: 0.791754  [100800/175341]\n",
      "loss: 0.457471  [102400/175341]\n",
      "loss: 0.190492  [104000/175341]\n",
      "loss: 0.327724  [105600/175341]\n",
      "loss: 0.895169  [107200/175341]\n",
      "loss: 0.347866  [108800/175341]\n",
      "loss: 0.467535  [110400/175341]\n",
      "loss: 0.475553  [112000/175341]\n",
      "loss: 0.616142  [113600/175341]\n",
      "loss: 0.489893  [115200/175341]\n",
      "loss: 0.557131  [116800/175341]\n",
      "loss: 0.723347  [118400/175341]\n",
      "loss: 0.431111  [120000/175341]\n",
      "loss: 0.139130  [121600/175341]\n",
      "loss: 0.598861  [123200/175341]\n",
      "loss: 0.294382  [124800/175341]\n",
      "loss: 0.828747  [126400/175341]\n",
      "loss: 0.336200  [128000/175341]\n",
      "loss: 0.250780  [129600/175341]\n",
      "loss: 0.698656  [131200/175341]\n",
      "loss: 0.210501  [132800/175341]\n",
      "loss: 0.704022  [134400/175341]\n",
      "loss: 0.064428  [136000/175341]\n",
      "loss: 0.267052  [137600/175341]\n",
      "loss: 0.255467  [139200/175341]\n",
      "loss: 0.674619  [140800/175341]\n",
      "loss: 0.421982  [142400/175341]\n",
      "loss: 0.441550  [144000/175341]\n",
      "loss: 0.911639  [145600/175341]\n",
      "loss: 0.138722  [147200/175341]\n",
      "loss: 0.399965  [148800/175341]\n",
      "loss: 0.313010  [150400/175341]\n",
      "loss: 0.597815  [152000/175341]\n",
      "loss: 0.773103  [153600/175341]\n",
      "loss: 1.023500  [155200/175341]\n",
      "loss: 1.180151  [156800/175341]\n",
      "loss: 0.702207  [158400/175341]\n",
      "loss: 0.640623  [160000/175341]\n",
      "loss: 0.332412  [161600/175341]\n",
      "loss: 0.422278  [163200/175341]\n",
      "loss: 0.439020  [164800/175341]\n",
      "loss: 0.249866  [166400/175341]\n",
      "loss: 0.591793  [168000/175341]\n",
      "loss: 0.334631  [169600/175341]\n",
      "loss: 0.537332  [171200/175341]\n",
      "loss: 0.349201  [172800/175341]\n",
      "loss: 0.399823  [174400/175341]\n",
      "Train Accuracy: 79.7429%\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.641487, F1-score: 72.66%, Macro_F1-Score:  36.58%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.856469  [    0/175341]\n",
      "loss: 0.706684  [ 1600/175341]\n",
      "loss: 0.595525  [ 3200/175341]\n",
      "loss: 0.253709  [ 4800/175341]\n",
      "loss: 0.697187  [ 6400/175341]\n",
      "loss: 0.664636  [ 8000/175341]\n",
      "loss: 0.614717  [ 9600/175341]\n",
      "loss: 0.397168  [11200/175341]\n",
      "loss: 0.918896  [12800/175341]\n",
      "loss: 0.609790  [14400/175341]\n",
      "loss: 0.868934  [16000/175341]\n",
      "loss: 0.627425  [17600/175341]\n",
      "loss: 0.542411  [19200/175341]\n",
      "loss: 0.437839  [20800/175341]\n",
      "loss: 0.327849  [22400/175341]\n",
      "loss: 0.250653  [24000/175341]\n",
      "loss: 0.399987  [25600/175341]\n",
      "loss: 0.299592  [27200/175341]\n",
      "loss: 0.657323  [28800/175341]\n",
      "loss: 0.575241  [30400/175341]\n",
      "loss: 0.570543  [32000/175341]\n",
      "loss: 0.261195  [33600/175341]\n",
      "loss: 1.056494  [35200/175341]\n",
      "loss: 0.608831  [36800/175341]\n",
      "loss: 0.379844  [38400/175341]\n",
      "loss: 0.549447  [40000/175341]\n",
      "loss: 0.584744  [41600/175341]\n",
      "loss: 0.570907  [43200/175341]\n",
      "loss: 0.710561  [44800/175341]\n",
      "loss: 0.917841  [46400/175341]\n",
      "loss: 0.384624  [48000/175341]\n",
      "loss: 0.388896  [49600/175341]\n",
      "loss: 0.987905  [51200/175341]\n",
      "loss: 0.663336  [52800/175341]\n",
      "loss: 0.453117  [54400/175341]\n",
      "loss: 0.706825  [56000/175341]\n",
      "loss: 0.245429  [57600/175341]\n",
      "loss: 0.839509  [59200/175341]\n",
      "loss: 0.245834  [60800/175341]\n",
      "loss: 0.721346  [62400/175341]\n",
      "loss: 0.461661  [64000/175341]\n",
      "loss: 0.733083  [65600/175341]\n",
      "loss: 0.451737  [67200/175341]\n",
      "loss: 0.442974  [68800/175341]\n",
      "loss: 0.769807  [70400/175341]\n",
      "loss: 0.398145  [72000/175341]\n",
      "loss: 0.440881  [73600/175341]\n",
      "loss: 0.158574  [75200/175341]\n",
      "loss: 0.387314  [76800/175341]\n",
      "loss: 0.377897  [78400/175341]\n",
      "loss: 0.705142  [80000/175341]\n",
      "loss: 0.150395  [81600/175341]\n",
      "loss: 0.416384  [83200/175341]\n",
      "loss: 0.448956  [84800/175341]\n",
      "loss: 0.365890  [86400/175341]\n",
      "loss: 0.509695  [88000/175341]\n",
      "loss: 0.773916  [89600/175341]\n",
      "loss: 0.375058  [91200/175341]\n",
      "loss: 0.126552  [92800/175341]\n",
      "loss: 0.762311  [94400/175341]\n",
      "loss: 0.343898  [96000/175341]\n",
      "loss: 0.597052  [97600/175341]\n",
      "loss: 0.361953  [99200/175341]\n",
      "loss: 0.451080  [100800/175341]\n",
      "loss: 0.292282  [102400/175341]\n",
      "loss: 0.231324  [104000/175341]\n",
      "loss: 0.462362  [105600/175341]\n",
      "loss: 0.741358  [107200/175341]\n",
      "loss: 0.153288  [108800/175341]\n",
      "loss: 0.479174  [110400/175341]\n",
      "loss: 0.355895  [112000/175341]\n",
      "loss: 0.737518  [113600/175341]\n",
      "loss: 0.391052  [115200/175341]\n",
      "loss: 1.064025  [116800/175341]\n",
      "loss: 0.264491  [118400/175341]\n",
      "loss: 0.265638  [120000/175341]\n",
      "loss: 0.748526  [121600/175341]\n",
      "loss: 0.954445  [123200/175341]\n",
      "loss: 0.205351  [124800/175341]\n",
      "loss: 0.401615  [126400/175341]\n",
      "loss: 0.634305  [128000/175341]\n",
      "loss: 0.360179  [129600/175341]\n",
      "loss: 0.379689  [131200/175341]\n",
      "loss: 0.485623  [132800/175341]\n",
      "loss: 0.850454  [134400/175341]\n",
      "loss: 0.557008  [136000/175341]\n",
      "loss: 0.805231  [137600/175341]\n",
      "loss: 0.306554  [139200/175341]\n",
      "loss: 0.551220  [140800/175341]\n",
      "loss: 0.631303  [142400/175341]\n",
      "loss: 0.504613  [144000/175341]\n",
      "loss: 0.787081  [145600/175341]\n",
      "loss: 0.302574  [147200/175341]\n",
      "loss: 0.278371  [148800/175341]\n",
      "loss: 1.010794  [150400/175341]\n",
      "loss: 0.373993  [152000/175341]\n",
      "loss: 0.817614  [153600/175341]\n",
      "loss: 1.126615  [155200/175341]\n",
      "loss: 0.451666  [156800/175341]\n",
      "loss: 0.467498  [158400/175341]\n",
      "loss: 1.011770  [160000/175341]\n",
      "loss: 0.774262  [161600/175341]\n",
      "loss: 0.817601  [163200/175341]\n",
      "loss: 0.110334  [164800/175341]\n",
      "loss: 0.622917  [166400/175341]\n",
      "loss: 0.477904  [168000/175341]\n",
      "loss: 0.912246  [169600/175341]\n",
      "loss: 0.337181  [171200/175341]\n",
      "loss: 0.322205  [172800/175341]\n",
      "loss: 0.317910  [174400/175341]\n",
      "Train Accuracy: 79.7919%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.600437, F1-score: 74.47%, Macro_F1-Score:  37.90%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.423396  [    0/175341]\n",
      "loss: 0.973321  [ 1600/175341]\n",
      "loss: 0.956587  [ 3200/175341]\n",
      "loss: 0.266168  [ 4800/175341]\n",
      "loss: 0.583616  [ 6400/175341]\n",
      "loss: 0.514220  [ 8000/175341]\n",
      "loss: 0.456852  [ 9600/175341]\n",
      "loss: 0.411576  [11200/175341]\n",
      "loss: 0.501587  [12800/175341]\n",
      "loss: 0.434919  [14400/175341]\n",
      "loss: 0.388498  [16000/175341]\n",
      "loss: 0.380618  [17600/175341]\n",
      "loss: 0.282822  [19200/175341]\n",
      "loss: 0.310278  [20800/175341]\n",
      "loss: 0.478699  [22400/175341]\n",
      "loss: 0.293603  [24000/175341]\n",
      "loss: 0.322821  [25600/175341]\n",
      "loss: 0.608241  [27200/175341]\n",
      "loss: 0.802846  [28800/175341]\n",
      "loss: 0.276492  [30400/175341]\n",
      "loss: 0.104898  [32000/175341]\n",
      "loss: 0.072057  [33600/175341]\n",
      "loss: 0.341157  [35200/175341]\n",
      "loss: 0.821414  [36800/175341]\n",
      "loss: 0.240253  [38400/175341]\n",
      "loss: 0.352072  [40000/175341]\n",
      "loss: 0.583081  [41600/175341]\n",
      "loss: 0.909457  [43200/175341]\n",
      "loss: 0.595774  [44800/175341]\n",
      "loss: 0.661879  [46400/175341]\n",
      "loss: 0.496033  [48000/175341]\n",
      "loss: 0.357995  [49600/175341]\n",
      "loss: 0.299293  [51200/175341]\n",
      "loss: 0.374228  [52800/175341]\n",
      "loss: 0.417652  [54400/175341]\n",
      "loss: 0.261726  [56000/175341]\n",
      "loss: 0.420156  [57600/175341]\n",
      "loss: 0.889443  [59200/175341]\n",
      "loss: 0.611510  [60800/175341]\n",
      "loss: 0.345708  [62400/175341]\n",
      "loss: 0.223552  [64000/175341]\n",
      "loss: 0.878254  [65600/175341]\n",
      "loss: 0.226797  [67200/175341]\n",
      "loss: 0.327273  [68800/175341]\n",
      "loss: 0.833254  [70400/175341]\n",
      "loss: 0.535151  [72000/175341]\n",
      "loss: 0.211547  [73600/175341]\n",
      "loss: 0.192347  [75200/175341]\n",
      "loss: 0.352383  [76800/175341]\n",
      "loss: 0.283011  [78400/175341]\n",
      "loss: 0.642145  [80000/175341]\n",
      "loss: 0.884092  [81600/175341]\n",
      "loss: 0.472744  [83200/175341]\n",
      "loss: 0.592498  [84800/175341]\n",
      "loss: 0.764233  [86400/175341]\n",
      "loss: 0.319662  [88000/175341]\n",
      "loss: 0.366590  [89600/175341]\n",
      "loss: 0.514019  [91200/175341]\n",
      "loss: 0.401688  [92800/175341]\n",
      "loss: 0.950594  [94400/175341]\n",
      "loss: 0.337473  [96000/175341]\n",
      "loss: 0.484038  [97600/175341]\n",
      "loss: 0.229055  [99200/175341]\n",
      "loss: 0.691220  [100800/175341]\n",
      "loss: 0.196334  [102400/175341]\n",
      "loss: 0.501890  [104000/175341]\n",
      "loss: 0.695463  [105600/175341]\n",
      "loss: 0.308823  [107200/175341]\n",
      "loss: 0.754433  [108800/175341]\n",
      "loss: 0.737831  [110400/175341]\n",
      "loss: 0.377583  [112000/175341]\n",
      "loss: 1.055949  [113600/175341]\n",
      "loss: 0.635823  [115200/175341]\n",
      "loss: 0.853794  [116800/175341]\n",
      "loss: 0.162437  [118400/175341]\n",
      "loss: 0.518945  [120000/175341]\n",
      "loss: 0.579511  [121600/175341]\n",
      "loss: 0.247903  [123200/175341]\n",
      "loss: 0.280737  [124800/175341]\n",
      "loss: 0.341089  [126400/175341]\n",
      "loss: 0.626946  [128000/175341]\n",
      "loss: 0.389184  [129600/175341]\n",
      "loss: 0.420170  [131200/175341]\n",
      "loss: 0.295827  [132800/175341]\n",
      "loss: 0.580299  [134400/175341]\n",
      "loss: 0.952573  [136000/175341]\n",
      "loss: 0.655533  [137600/175341]\n",
      "loss: 0.492455  [139200/175341]\n",
      "loss: 0.678517  [140800/175341]\n",
      "loss: 0.327280  [142400/175341]\n",
      "loss: 1.025218  [144000/175341]\n",
      "loss: 0.758770  [145600/175341]\n",
      "loss: 0.380050  [147200/175341]\n",
      "loss: 0.222554  [148800/175341]\n",
      "loss: 0.427083  [150400/175341]\n",
      "loss: 0.163614  [152000/175341]\n",
      "loss: 0.560183  [153600/175341]\n",
      "loss: 0.428136  [155200/175341]\n",
      "loss: 0.857086  [156800/175341]\n",
      "loss: 0.678838  [158400/175341]\n",
      "loss: 0.655305  [160000/175341]\n",
      "loss: 0.331937  [161600/175341]\n",
      "loss: 0.499250  [163200/175341]\n",
      "loss: 0.776188  [164800/175341]\n",
      "loss: 0.686694  [166400/175341]\n",
      "loss: 0.551347  [168000/175341]\n",
      "loss: 0.253015  [169600/175341]\n",
      "loss: 0.666097  [171200/175341]\n",
      "loss: 0.139016  [172800/175341]\n",
      "loss: 0.649316  [174400/175341]\n",
      "Train Accuracy: 79.8826%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.568558, F1-score: 76.44%, Macro_F1-Score:  38.65%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.800832  [    0/175341]\n",
      "loss: 0.693208  [ 1600/175341]\n",
      "loss: 0.965279  [ 3200/175341]\n",
      "loss: 0.178227  [ 4800/175341]\n",
      "loss: 1.105601  [ 6400/175341]\n",
      "loss: 0.345505  [ 8000/175341]\n",
      "loss: 0.534811  [ 9600/175341]\n",
      "loss: 0.336110  [11200/175341]\n",
      "loss: 0.562287  [12800/175341]\n",
      "loss: 0.335124  [14400/175341]\n",
      "loss: 0.296077  [16000/175341]\n",
      "loss: 0.327567  [17600/175341]\n",
      "loss: 0.601182  [19200/175341]\n",
      "loss: 0.455610  [20800/175341]\n",
      "loss: 0.570075  [22400/175341]\n",
      "loss: 0.880758  [24000/175341]\n",
      "loss: 0.388831  [25600/175341]\n",
      "loss: 0.695470  [27200/175341]\n",
      "loss: 0.423870  [28800/175341]\n",
      "loss: 0.679214  [30400/175341]\n",
      "loss: 0.537094  [32000/175341]\n",
      "loss: 0.557832  [33600/175341]\n",
      "loss: 0.661081  [35200/175341]\n",
      "loss: 0.294087  [36800/175341]\n",
      "loss: 0.853316  [38400/175341]\n",
      "loss: 0.713356  [40000/175341]\n",
      "loss: 0.706309  [41600/175341]\n",
      "loss: 0.575330  [43200/175341]\n",
      "loss: 0.434374  [44800/175341]\n",
      "loss: 0.859446  [46400/175341]\n",
      "loss: 0.227332  [48000/175341]\n",
      "loss: 0.243604  [49600/175341]\n",
      "loss: 0.464805  [51200/175341]\n",
      "loss: 0.390816  [52800/175341]\n",
      "loss: 0.358879  [54400/175341]\n",
      "loss: 0.211741  [56000/175341]\n",
      "loss: 0.335024  [57600/175341]\n",
      "loss: 0.657300  [59200/175341]\n",
      "loss: 0.643845  [60800/175341]\n",
      "loss: 0.408290  [62400/175341]\n",
      "loss: 0.677299  [64000/175341]\n",
      "loss: 0.235099  [65600/175341]\n",
      "loss: 0.361263  [67200/175341]\n",
      "loss: 0.671863  [68800/175341]\n",
      "loss: 0.506463  [70400/175341]\n",
      "loss: 0.279037  [72000/175341]\n",
      "loss: 0.395420  [73600/175341]\n",
      "loss: 0.631719  [75200/175341]\n",
      "loss: 0.531443  [76800/175341]\n",
      "loss: 0.624125  [78400/175341]\n",
      "loss: 0.601907  [80000/175341]\n",
      "loss: 0.511458  [81600/175341]\n",
      "loss: 0.745544  [83200/175341]\n",
      "loss: 0.383044  [84800/175341]\n",
      "loss: 0.297229  [86400/175341]\n",
      "loss: 0.258763  [88000/175341]\n",
      "loss: 0.484585  [89600/175341]\n",
      "loss: 0.666089  [91200/175341]\n",
      "loss: 0.734233  [92800/175341]\n",
      "loss: 0.578413  [94400/175341]\n",
      "loss: 0.341740  [96000/175341]\n",
      "loss: 0.376010  [97600/175341]\n",
      "loss: 0.835182  [99200/175341]\n",
      "loss: 0.505180  [100800/175341]\n",
      "loss: 0.346172  [102400/175341]\n",
      "loss: 0.446864  [104000/175341]\n",
      "loss: 0.584335  [105600/175341]\n",
      "loss: 0.590526  [107200/175341]\n",
      "loss: 0.305649  [108800/175341]\n",
      "loss: 0.729051  [110400/175341]\n",
      "loss: 0.509166  [112000/175341]\n",
      "loss: 0.570511  [113600/175341]\n",
      "loss: 0.312935  [115200/175341]\n",
      "loss: 0.393498  [116800/175341]\n",
      "loss: 1.042536  [118400/175341]\n",
      "loss: 0.212058  [120000/175341]\n",
      "loss: 0.610997  [121600/175341]\n",
      "loss: 0.306534  [123200/175341]\n",
      "loss: 0.270512  [124800/175341]\n",
      "loss: 0.166094  [126400/175341]\n",
      "loss: 0.396547  [128000/175341]\n",
      "loss: 0.528706  [129600/175341]\n",
      "loss: 0.609311  [131200/175341]\n",
      "loss: 0.131147  [132800/175341]\n",
      "loss: 0.678486  [134400/175341]\n",
      "loss: 0.484973  [136000/175341]\n",
      "loss: 0.356879  [137600/175341]\n",
      "loss: 0.239161  [139200/175341]\n",
      "loss: 0.413405  [140800/175341]\n",
      "loss: 0.687719  [142400/175341]\n",
      "loss: 0.145888  [144000/175341]\n",
      "loss: 0.727880  [145600/175341]\n",
      "loss: 0.359350  [147200/175341]\n",
      "loss: 1.023294  [148800/175341]\n",
      "loss: 0.505401  [150400/175341]\n",
      "loss: 0.523196  [152000/175341]\n",
      "loss: 0.573602  [153600/175341]\n",
      "loss: 0.437722  [155200/175341]\n",
      "loss: 0.332784  [156800/175341]\n",
      "loss: 0.266598  [158400/175341]\n",
      "loss: 0.216502  [160000/175341]\n",
      "loss: 0.523876  [161600/175341]\n",
      "loss: 0.474818  [163200/175341]\n",
      "loss: 0.821098  [164800/175341]\n",
      "loss: 0.414554  [166400/175341]\n",
      "loss: 0.183494  [168000/175341]\n",
      "loss: 0.383656  [169600/175341]\n",
      "loss: 0.165354  [171200/175341]\n",
      "loss: 0.547113  [172800/175341]\n",
      "loss: 0.328013  [174400/175341]\n",
      "Train Accuracy: 79.8781%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.587293, F1-score: 75.38%, Macro_F1-Score:  38.38%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.435177  [    0/175341]\n",
      "loss: 0.560426  [ 1600/175341]\n",
      "loss: 0.646340  [ 3200/175341]\n",
      "loss: 0.951516  [ 4800/175341]\n",
      "loss: 0.042571  [ 6400/175341]\n",
      "loss: 0.880653  [ 8000/175341]\n",
      "loss: 0.539225  [ 9600/175341]\n",
      "loss: 0.245124  [11200/175341]\n",
      "loss: 0.734389  [12800/175341]\n",
      "loss: 0.434173  [14400/175341]\n",
      "loss: 0.753772  [16000/175341]\n",
      "loss: 0.556725  [17600/175341]\n",
      "loss: 0.891146  [19200/175341]\n",
      "loss: 0.555756  [20800/175341]\n",
      "loss: 0.337922  [22400/175341]\n",
      "loss: 0.542230  [24000/175341]\n",
      "loss: 0.600249  [25600/175341]\n",
      "loss: 0.987740  [27200/175341]\n",
      "loss: 0.542037  [28800/175341]\n",
      "loss: 0.297394  [30400/175341]\n",
      "loss: 0.476329  [32000/175341]\n",
      "loss: 0.447394  [33600/175341]\n",
      "loss: 0.387664  [35200/175341]\n",
      "loss: 0.165358  [36800/175341]\n",
      "loss: 0.391822  [38400/175341]\n",
      "loss: 0.778247  [40000/175341]\n",
      "loss: 0.257214  [41600/175341]\n",
      "loss: 0.698820  [43200/175341]\n",
      "loss: 0.990071  [44800/175341]\n",
      "loss: 0.239988  [46400/175341]\n",
      "loss: 0.713791  [48000/175341]\n",
      "loss: 0.657940  [49600/175341]\n",
      "loss: 0.493489  [51200/175341]\n",
      "loss: 0.208871  [52800/175341]\n",
      "loss: 0.319625  [54400/175341]\n",
      "loss: 0.386306  [56000/175341]\n",
      "loss: 0.610854  [57600/175341]\n",
      "loss: 0.256794  [59200/175341]\n",
      "loss: 0.845794  [60800/175341]\n",
      "loss: 0.402587  [62400/175341]\n",
      "loss: 0.323615  [64000/175341]\n",
      "loss: 0.714895  [65600/175341]\n",
      "loss: 0.274006  [67200/175341]\n",
      "loss: 0.420166  [68800/175341]\n",
      "loss: 0.477512  [70400/175341]\n",
      "loss: 0.324874  [72000/175341]\n",
      "loss: 0.449324  [73600/175341]\n",
      "loss: 0.364417  [75200/175341]\n",
      "loss: 0.346138  [76800/175341]\n",
      "loss: 0.376210  [78400/175341]\n",
      "loss: 0.149602  [80000/175341]\n",
      "loss: 0.546165  [81600/175341]\n",
      "loss: 0.297452  [83200/175341]\n",
      "loss: 0.257509  [84800/175341]\n",
      "loss: 0.265018  [86400/175341]\n",
      "loss: 0.605940  [88000/175341]\n",
      "loss: 0.771891  [89600/175341]\n",
      "loss: 0.326626  [91200/175341]\n",
      "loss: 0.280363  [92800/175341]\n",
      "loss: 0.603411  [94400/175341]\n",
      "loss: 0.379598  [96000/175341]\n",
      "loss: 0.387709  [97600/175341]\n",
      "loss: 0.572449  [99200/175341]\n",
      "loss: 0.519651  [100800/175341]\n",
      "loss: 0.415842  [102400/175341]\n",
      "loss: 0.278722  [104000/175341]\n",
      "loss: 0.293578  [105600/175341]\n",
      "loss: 0.359412  [107200/175341]\n",
      "loss: 0.195729  [108800/175341]\n",
      "loss: 0.334690  [110400/175341]\n",
      "loss: 0.273495  [112000/175341]\n",
      "loss: 0.364084  [113600/175341]\n",
      "loss: 0.690468  [115200/175341]\n",
      "loss: 0.709304  [116800/175341]\n",
      "loss: 0.407447  [118400/175341]\n",
      "loss: 0.238807  [120000/175341]\n",
      "loss: 0.651731  [121600/175341]\n",
      "loss: 0.968237  [123200/175341]\n",
      "loss: 0.361475  [124800/175341]\n",
      "loss: 0.771549  [126400/175341]\n",
      "loss: 0.396378  [128000/175341]\n",
      "loss: 0.991195  [129600/175341]\n",
      "loss: 0.495561  [131200/175341]\n",
      "loss: 0.581979  [132800/175341]\n",
      "loss: 0.352224  [134400/175341]\n",
      "loss: 0.406660  [136000/175341]\n",
      "loss: 0.669541  [137600/175341]\n",
      "loss: 0.581558  [139200/175341]\n",
      "loss: 0.799712  [140800/175341]\n",
      "loss: 0.433160  [142400/175341]\n",
      "loss: 0.532123  [144000/175341]\n",
      "loss: 0.277426  [145600/175341]\n",
      "loss: 0.541936  [147200/175341]\n",
      "loss: 0.223442  [148800/175341]\n",
      "loss: 0.686649  [150400/175341]\n",
      "loss: 0.498111  [152000/175341]\n",
      "loss: 0.307521  [153600/175341]\n",
      "loss: 0.361221  [155200/175341]\n",
      "loss: 0.648082  [156800/175341]\n",
      "loss: 0.345596  [158400/175341]\n",
      "loss: 0.379107  [160000/175341]\n",
      "loss: 0.147157  [161600/175341]\n",
      "loss: 0.347591  [163200/175341]\n",
      "loss: 0.407492  [164800/175341]\n",
      "loss: 0.378375  [166400/175341]\n",
      "loss: 0.665664  [168000/175341]\n",
      "loss: 0.550034  [169600/175341]\n",
      "loss: 0.220851  [171200/175341]\n",
      "loss: 0.389448  [172800/175341]\n",
      "loss: 0.573692  [174400/175341]\n",
      "Train Accuracy: 79.9596%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.607067, F1-score: 73.48%, Macro_F1-Score:  38.26%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.758854  [    0/175341]\n",
      "loss: 0.559365  [ 1600/175341]\n",
      "loss: 0.239421  [ 3200/175341]\n",
      "loss: 0.440118  [ 4800/175341]\n",
      "loss: 0.371242  [ 6400/175341]\n",
      "loss: 0.421705  [ 8000/175341]\n",
      "loss: 0.274459  [ 9600/175341]\n",
      "loss: 0.755743  [11200/175341]\n",
      "loss: 0.571802  [12800/175341]\n",
      "loss: 0.511358  [14400/175341]\n",
      "loss: 0.718898  [16000/175341]\n",
      "loss: 0.203566  [17600/175341]\n",
      "loss: 0.390154  [19200/175341]\n",
      "loss: 0.365544  [20800/175341]\n",
      "loss: 0.313287  [22400/175341]\n",
      "loss: 0.508623  [24000/175341]\n",
      "loss: 0.393759  [25600/175341]\n",
      "loss: 0.516941  [27200/175341]\n",
      "loss: 0.282513  [28800/175341]\n",
      "loss: 0.370851  [30400/175341]\n",
      "loss: 0.339990  [32000/175341]\n",
      "loss: 0.324395  [33600/175341]\n",
      "loss: 0.249250  [35200/175341]\n",
      "loss: 0.902830  [36800/175341]\n",
      "loss: 0.466241  [38400/175341]\n",
      "loss: 0.298627  [40000/175341]\n",
      "loss: 0.884953  [41600/175341]\n",
      "loss: 0.701962  [43200/175341]\n",
      "loss: 0.288148  [44800/175341]\n",
      "loss: 0.676820  [46400/175341]\n",
      "loss: 0.374497  [48000/175341]\n",
      "loss: 0.610506  [49600/175341]\n",
      "loss: 0.446046  [51200/175341]\n",
      "loss: 0.452235  [52800/175341]\n",
      "loss: 0.176075  [54400/175341]\n",
      "loss: 0.502755  [56000/175341]\n",
      "loss: 1.076048  [57600/175341]\n",
      "loss: 0.754311  [59200/175341]\n",
      "loss: 0.477223  [60800/175341]\n",
      "loss: 0.774033  [62400/175341]\n",
      "loss: 0.554452  [64000/175341]\n",
      "loss: 0.535228  [65600/175341]\n",
      "loss: 0.324030  [67200/175341]\n",
      "loss: 0.479140  [68800/175341]\n",
      "loss: 0.468023  [70400/175341]\n",
      "loss: 0.388659  [72000/175341]\n",
      "loss: 0.764685  [73600/175341]\n",
      "loss: 0.624960  [75200/175341]\n",
      "loss: 0.179088  [76800/175341]\n",
      "loss: 0.504657  [78400/175341]\n",
      "loss: 0.273019  [80000/175341]\n",
      "loss: 0.534578  [81600/175341]\n",
      "loss: 0.594676  [83200/175341]\n",
      "loss: 0.332195  [84800/175341]\n",
      "loss: 0.307615  [86400/175341]\n",
      "loss: 0.186882  [88000/175341]\n",
      "loss: 0.323889  [89600/175341]\n",
      "loss: 0.491562  [91200/175341]\n",
      "loss: 0.420089  [92800/175341]\n",
      "loss: 0.546306  [94400/175341]\n",
      "loss: 0.246885  [96000/175341]\n",
      "loss: 0.921360  [97600/175341]\n",
      "loss: 0.620118  [99200/175341]\n",
      "loss: 0.236700  [100800/175341]\n",
      "loss: 0.894253  [102400/175341]\n",
      "loss: 0.486825  [104000/175341]\n",
      "loss: 0.496237  [105600/175341]\n",
      "loss: 0.375104  [107200/175341]\n",
      "loss: 0.448841  [108800/175341]\n",
      "loss: 0.415366  [110400/175341]\n",
      "loss: 0.525773  [112000/175341]\n",
      "loss: 0.540895  [113600/175341]\n",
      "loss: 0.939071  [115200/175341]\n",
      "loss: 0.564170  [116800/175341]\n",
      "loss: 0.292619  [118400/175341]\n",
      "loss: 0.823957  [120000/175341]\n",
      "loss: 0.250610  [121600/175341]\n",
      "loss: 0.611995  [123200/175341]\n",
      "loss: 0.394589  [124800/175341]\n",
      "loss: 0.220541  [126400/175341]\n",
      "loss: 0.206609  [128000/175341]\n",
      "loss: 0.579419  [129600/175341]\n",
      "loss: 0.382264  [131200/175341]\n",
      "loss: 0.586308  [132800/175341]\n",
      "loss: 0.292614  [134400/175341]\n",
      "loss: 0.505069  [136000/175341]\n",
      "loss: 1.324435  [137600/175341]\n",
      "loss: 0.895671  [139200/175341]\n",
      "loss: 0.601425  [140800/175341]\n",
      "loss: 0.765151  [142400/175341]\n",
      "loss: 0.455137  [144000/175341]\n",
      "loss: 0.591356  [145600/175341]\n",
      "loss: 0.916568  [147200/175341]\n",
      "loss: 0.676944  [148800/175341]\n",
      "loss: 0.233282  [150400/175341]\n",
      "loss: 0.242976  [152000/175341]\n",
      "loss: 0.621311  [153600/175341]\n",
      "loss: 0.421324  [155200/175341]\n",
      "loss: 0.472955  [156800/175341]\n",
      "loss: 0.321861  [158400/175341]\n",
      "loss: 0.214888  [160000/175341]\n",
      "loss: 0.531725  [161600/175341]\n",
      "loss: 0.724519  [163200/175341]\n",
      "loss: 0.431763  [164800/175341]\n",
      "loss: 0.525213  [166400/175341]\n",
      "loss: 0.215932  [168000/175341]\n",
      "loss: 0.544434  [169600/175341]\n",
      "loss: 0.442525  [171200/175341]\n",
      "loss: 0.716005  [172800/175341]\n",
      "loss: 0.747761  [174400/175341]\n",
      "Train Accuracy: 79.9431%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.607324, F1-score: 74.67%, Macro_F1-Score:  38.37%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.427516  [    0/175341]\n",
      "loss: 0.605232  [ 1600/175341]\n",
      "loss: 0.752289  [ 3200/175341]\n",
      "loss: 0.415885  [ 4800/175341]\n",
      "loss: 0.537838  [ 6400/175341]\n",
      "loss: 0.500308  [ 8000/175341]\n",
      "loss: 0.365522  [ 9600/175341]\n",
      "loss: 0.630073  [11200/175341]\n",
      "loss: 0.896985  [12800/175341]\n",
      "loss: 0.547708  [14400/175341]\n",
      "loss: 0.680326  [16000/175341]\n",
      "loss: 0.694462  [17600/175341]\n",
      "loss: 0.215161  [19200/175341]\n",
      "loss: 0.643016  [20800/175341]\n",
      "loss: 0.625471  [22400/175341]\n",
      "loss: 0.481092  [24000/175341]\n",
      "loss: 0.460172  [25600/175341]\n",
      "loss: 0.361956  [27200/175341]\n",
      "loss: 0.951628  [28800/175341]\n",
      "loss: 0.381460  [30400/175341]\n",
      "loss: 0.538956  [32000/175341]\n",
      "loss: 0.625100  [33600/175341]\n",
      "loss: 0.551542  [35200/175341]\n",
      "loss: 0.492156  [36800/175341]\n",
      "loss: 0.549731  [38400/175341]\n",
      "loss: 0.343483  [40000/175341]\n",
      "loss: 0.542302  [41600/175341]\n",
      "loss: 0.624828  [43200/175341]\n",
      "loss: 0.472939  [44800/175341]\n",
      "loss: 0.744447  [46400/175341]\n",
      "loss: 0.364734  [48000/175341]\n",
      "loss: 0.554882  [49600/175341]\n",
      "loss: 0.380433  [51200/175341]\n",
      "loss: 0.251121  [52800/175341]\n",
      "loss: 0.946465  [54400/175341]\n",
      "loss: 0.476555  [56000/175341]\n",
      "loss: 0.550051  [57600/175341]\n",
      "loss: 0.563014  [59200/175341]\n",
      "loss: 0.602999  [60800/175341]\n",
      "loss: 1.161745  [62400/175341]\n",
      "loss: 0.796179  [64000/175341]\n",
      "loss: 0.716346  [65600/175341]\n",
      "loss: 0.445184  [67200/175341]\n",
      "loss: 0.313784  [68800/175341]\n",
      "loss: 0.337923  [70400/175341]\n",
      "loss: 0.380712  [72000/175341]\n",
      "loss: 0.771195  [73600/175341]\n",
      "loss: 0.689658  [75200/175341]\n",
      "loss: 0.563984  [76800/175341]\n",
      "loss: 0.705021  [78400/175341]\n",
      "loss: 0.584820  [80000/175341]\n",
      "loss: 0.661699  [81600/175341]\n",
      "loss: 0.196297  [83200/175341]\n",
      "loss: 0.582768  [84800/175341]\n",
      "loss: 0.544450  [86400/175341]\n",
      "loss: 0.599595  [88000/175341]\n",
      "loss: 0.365839  [89600/175341]\n",
      "loss: 0.522220  [91200/175341]\n",
      "loss: 0.433058  [92800/175341]\n",
      "loss: 0.746457  [94400/175341]\n",
      "loss: 0.578899  [96000/175341]\n",
      "loss: 0.995053  [97600/175341]\n",
      "loss: 0.453462  [99200/175341]\n",
      "loss: 0.628621  [100800/175341]\n",
      "loss: 0.490674  [102400/175341]\n",
      "loss: 0.855163  [104000/175341]\n",
      "loss: 0.437877  [105600/175341]\n",
      "loss: 0.356175  [107200/175341]\n",
      "loss: 0.495346  [108800/175341]\n",
      "loss: 0.280428  [110400/175341]\n",
      "loss: 0.437509  [112000/175341]\n",
      "loss: 0.640741  [113600/175341]\n",
      "loss: 0.526439  [115200/175341]\n",
      "loss: 0.678101  [116800/175341]\n",
      "loss: 0.519122  [118400/175341]\n",
      "loss: 0.621963  [120000/175341]\n",
      "loss: 0.584252  [121600/175341]\n",
      "loss: 0.529535  [123200/175341]\n",
      "loss: 0.514614  [124800/175341]\n",
      "loss: 0.288924  [126400/175341]\n",
      "loss: 0.804693  [128000/175341]\n",
      "loss: 0.398218  [129600/175341]\n",
      "loss: 0.477716  [131200/175341]\n",
      "loss: 1.180678  [132800/175341]\n",
      "loss: 0.413591  [134400/175341]\n",
      "loss: 0.322477  [136000/175341]\n",
      "loss: 0.919603  [137600/175341]\n",
      "loss: 0.423444  [139200/175341]\n",
      "loss: 0.467334  [140800/175341]\n",
      "loss: 0.701012  [142400/175341]\n",
      "loss: 0.411998  [144000/175341]\n",
      "loss: 0.570700  [145600/175341]\n",
      "loss: 0.371369  [147200/175341]\n",
      "loss: 0.519970  [148800/175341]\n",
      "loss: 0.498338  [150400/175341]\n",
      "loss: 0.285766  [152000/175341]\n",
      "loss: 0.453474  [153600/175341]\n",
      "loss: 0.966544  [155200/175341]\n",
      "loss: 0.562510  [156800/175341]\n",
      "loss: 0.499663  [158400/175341]\n",
      "loss: 0.604633  [160000/175341]\n",
      "loss: 0.602060  [161600/175341]\n",
      "loss: 0.392977  [163200/175341]\n",
      "loss: 0.359262  [164800/175341]\n",
      "loss: 0.521611  [166400/175341]\n",
      "loss: 0.202440  [168000/175341]\n",
      "loss: 0.370749  [169600/175341]\n",
      "loss: 0.467618  [171200/175341]\n",
      "loss: 0.572702  [172800/175341]\n",
      "loss: 0.266901  [174400/175341]\n",
      "Train Accuracy: 79.9722%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.585941, F1-score: 75.66%, Macro_F1-Score:  38.53%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.373604  [    0/175341]\n",
      "loss: 0.724495  [ 1600/175341]\n",
      "loss: 0.158234  [ 3200/175341]\n",
      "loss: 0.762438  [ 4800/175341]\n",
      "loss: 0.760114  [ 6400/175341]\n",
      "loss: 0.791190  [ 8000/175341]\n",
      "loss: 0.725601  [ 9600/175341]\n",
      "loss: 0.770610  [11200/175341]\n",
      "loss: 0.394649  [12800/175341]\n",
      "loss: 0.631793  [14400/175341]\n",
      "loss: 0.118941  [16000/175341]\n",
      "loss: 0.587932  [17600/175341]\n",
      "loss: 0.450620  [19200/175341]\n",
      "loss: 0.459986  [20800/175341]\n",
      "loss: 0.418054  [22400/175341]\n",
      "loss: 0.534691  [24000/175341]\n",
      "loss: 0.367986  [25600/175341]\n",
      "loss: 0.420241  [27200/175341]\n",
      "loss: 0.388858  [28800/175341]\n",
      "loss: 0.543957  [30400/175341]\n",
      "loss: 0.610771  [32000/175341]\n",
      "loss: 0.241988  [33600/175341]\n",
      "loss: 0.799313  [35200/175341]\n",
      "loss: 0.326167  [36800/175341]\n",
      "loss: 0.366244  [38400/175341]\n",
      "loss: 0.604383  [40000/175341]\n",
      "loss: 0.595318  [41600/175341]\n",
      "loss: 0.324358  [43200/175341]\n",
      "loss: 0.488880  [44800/175341]\n",
      "loss: 0.295604  [46400/175341]\n",
      "loss: 0.972863  [48000/175341]\n",
      "loss: 0.424082  [49600/175341]\n",
      "loss: 0.749185  [51200/175341]\n",
      "loss: 0.544990  [52800/175341]\n",
      "loss: 0.487052  [54400/175341]\n",
      "loss: 0.551341  [56000/175341]\n",
      "loss: 0.368484  [57600/175341]\n",
      "loss: 0.225952  [59200/175341]\n",
      "loss: 0.848890  [60800/175341]\n",
      "loss: 0.188009  [62400/175341]\n",
      "loss: 0.385960  [64000/175341]\n",
      "loss: 0.467697  [65600/175341]\n",
      "loss: 0.687643  [67200/175341]\n",
      "loss: 0.652867  [68800/175341]\n",
      "loss: 0.387451  [70400/175341]\n",
      "loss: 0.847617  [72000/175341]\n",
      "loss: 0.766779  [73600/175341]\n",
      "loss: 0.611374  [75200/175341]\n",
      "loss: 0.442885  [76800/175341]\n",
      "loss: 0.407234  [78400/175341]\n",
      "loss: 0.510010  [80000/175341]\n",
      "loss: 0.332701  [81600/175341]\n",
      "loss: 0.789069  [83200/175341]\n",
      "loss: 0.171298  [84800/175341]\n",
      "loss: 0.818539  [86400/175341]\n",
      "loss: 0.508255  [88000/175341]\n",
      "loss: 0.451131  [89600/175341]\n",
      "loss: 0.635536  [91200/175341]\n",
      "loss: 0.764584  [92800/175341]\n",
      "loss: 0.254479  [94400/175341]\n",
      "loss: 0.681842  [96000/175341]\n",
      "loss: 0.882640  [97600/175341]\n",
      "loss: 0.288636  [99200/175341]\n",
      "loss: 0.134292  [100800/175341]\n",
      "loss: 0.544715  [102400/175341]\n",
      "loss: 0.609927  [104000/175341]\n",
      "loss: 0.585222  [105600/175341]\n",
      "loss: 0.310462  [107200/175341]\n",
      "loss: 0.900572  [108800/175341]\n",
      "loss: 0.475901  [110400/175341]\n",
      "loss: 0.854624  [112000/175341]\n",
      "loss: 0.273026  [113600/175341]\n",
      "loss: 0.699381  [115200/175341]\n",
      "loss: 0.593071  [116800/175341]\n",
      "loss: 0.451176  [118400/175341]\n",
      "loss: 0.307399  [120000/175341]\n",
      "loss: 0.303445  [121600/175341]\n",
      "loss: 0.255800  [123200/175341]\n",
      "loss: 0.902448  [124800/175341]\n",
      "loss: 0.264270  [126400/175341]\n",
      "loss: 0.389997  [128000/175341]\n",
      "loss: 0.361313  [129600/175341]\n",
      "loss: 0.369008  [131200/175341]\n",
      "loss: 0.418726  [132800/175341]\n",
      "loss: 0.267383  [134400/175341]\n",
      "loss: 0.300012  [136000/175341]\n",
      "loss: 0.455677  [137600/175341]\n",
      "loss: 0.445139  [139200/175341]\n",
      "loss: 0.835568  [140800/175341]\n",
      "loss: 0.218235  [142400/175341]\n",
      "loss: 0.567409  [144000/175341]\n",
      "loss: 0.432619  [145600/175341]\n",
      "loss: 0.463083  [147200/175341]\n",
      "loss: 0.404519  [148800/175341]\n",
      "loss: 0.488122  [150400/175341]\n",
      "loss: 0.718597  [152000/175341]\n",
      "loss: 0.383422  [153600/175341]\n",
      "loss: 0.466600  [155200/175341]\n",
      "loss: 0.798370  [156800/175341]\n",
      "loss: 0.326753  [158400/175341]\n",
      "loss: 0.705868  [160000/175341]\n",
      "loss: 1.145010  [161600/175341]\n",
      "loss: 0.904896  [163200/175341]\n",
      "loss: 1.002743  [164800/175341]\n",
      "loss: 0.934674  [166400/175341]\n",
      "loss: 0.498607  [168000/175341]\n",
      "loss: 0.395427  [169600/175341]\n",
      "loss: 0.491608  [171200/175341]\n",
      "loss: 0.596174  [172800/175341]\n",
      "loss: 0.834651  [174400/175341]\n",
      "Train Accuracy: 80.0429%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.601277, F1-score: 74.42%, Macro_F1-Score:  38.70%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.193234  [    0/175341]\n",
      "loss: 0.471958  [ 1600/175341]\n",
      "loss: 0.663824  [ 3200/175341]\n",
      "loss: 0.659014  [ 4800/175341]\n",
      "loss: 0.457092  [ 6400/175341]\n",
      "loss: 0.532208  [ 8000/175341]\n",
      "loss: 0.395015  [ 9600/175341]\n",
      "loss: 0.392138  [11200/175341]\n",
      "loss: 0.472649  [12800/175341]\n",
      "loss: 0.999454  [14400/175341]\n",
      "loss: 0.976593  [16000/175341]\n",
      "loss: 0.468747  [17600/175341]\n",
      "loss: 0.440396  [19200/175341]\n",
      "loss: 0.947895  [20800/175341]\n",
      "loss: 0.883628  [22400/175341]\n",
      "loss: 0.068867  [24000/175341]\n",
      "loss: 0.719588  [25600/175341]\n",
      "loss: 1.099122  [27200/175341]\n",
      "loss: 0.139062  [28800/175341]\n",
      "loss: 0.320892  [30400/175341]\n",
      "loss: 0.371794  [32000/175341]\n",
      "loss: 0.309933  [33600/175341]\n",
      "loss: 0.449894  [35200/175341]\n",
      "loss: 0.228685  [36800/175341]\n",
      "loss: 0.516123  [38400/175341]\n",
      "loss: 0.469293  [40000/175341]\n",
      "loss: 0.229413  [41600/175341]\n",
      "loss: 0.277214  [43200/175341]\n",
      "loss: 0.749240  [44800/175341]\n",
      "loss: 0.413346  [46400/175341]\n",
      "loss: 0.379566  [48000/175341]\n",
      "loss: 0.549012  [49600/175341]\n",
      "loss: 0.393982  [51200/175341]\n",
      "loss: 0.701622  [52800/175341]\n",
      "loss: 0.705339  [54400/175341]\n",
      "loss: 0.526132  [56000/175341]\n",
      "loss: 0.250335  [57600/175341]\n",
      "loss: 0.334477  [59200/175341]\n",
      "loss: 0.491681  [60800/175341]\n",
      "loss: 0.664270  [62400/175341]\n",
      "loss: 0.781097  [64000/175341]\n",
      "loss: 0.836597  [65600/175341]\n",
      "loss: 0.310458  [67200/175341]\n",
      "loss: 0.835923  [68800/175341]\n",
      "loss: 0.678476  [70400/175341]\n",
      "loss: 0.463620  [72000/175341]\n",
      "loss: 0.267667  [73600/175341]\n",
      "loss: 0.108949  [75200/175341]\n",
      "loss: 0.754324  [76800/175341]\n",
      "loss: 0.651481  [78400/175341]\n",
      "loss: 0.364114  [80000/175341]\n",
      "loss: 0.530908  [81600/175341]\n",
      "loss: 0.481498  [83200/175341]\n",
      "loss: 0.522738  [84800/175341]\n",
      "loss: 0.345138  [86400/175341]\n",
      "loss: 0.684068  [88000/175341]\n",
      "loss: 0.255320  [89600/175341]\n",
      "loss: 0.447103  [91200/175341]\n",
      "loss: 0.669203  [92800/175341]\n",
      "loss: 0.542195  [94400/175341]\n",
      "loss: 0.724554  [96000/175341]\n",
      "loss: 0.304170  [97600/175341]\n",
      "loss: 0.826708  [99200/175341]\n",
      "loss: 0.396360  [100800/175341]\n",
      "loss: 0.385875  [102400/175341]\n",
      "loss: 0.496649  [104000/175341]\n",
      "loss: 0.745951  [105600/175341]\n",
      "loss: 0.729459  [107200/175341]\n",
      "loss: 0.499408  [108800/175341]\n",
      "loss: 0.451719  [110400/175341]\n",
      "loss: 0.277878  [112000/175341]\n",
      "loss: 0.430055  [113600/175341]\n",
      "loss: 0.499918  [115200/175341]\n",
      "loss: 0.200235  [116800/175341]\n",
      "loss: 0.644776  [118400/175341]\n",
      "loss: 0.565576  [120000/175341]\n",
      "loss: 0.328155  [121600/175341]\n",
      "loss: 0.499852  [123200/175341]\n",
      "loss: 0.537245  [124800/175341]\n",
      "loss: 0.678407  [126400/175341]\n",
      "loss: 0.630522  [128000/175341]\n",
      "loss: 0.265869  [129600/175341]\n",
      "loss: 1.110935  [131200/175341]\n",
      "loss: 0.239293  [132800/175341]\n",
      "loss: 0.704057  [134400/175341]\n",
      "loss: 0.794072  [136000/175341]\n",
      "loss: 0.290037  [137600/175341]\n",
      "loss: 0.613486  [139200/175341]\n",
      "loss: 0.788306  [140800/175341]\n",
      "loss: 0.657681  [142400/175341]\n",
      "loss: 0.238216  [144000/175341]\n",
      "loss: 0.603150  [145600/175341]\n",
      "loss: 0.305113  [147200/175341]\n",
      "loss: 0.259202  [148800/175341]\n",
      "loss: 0.357189  [150400/175341]\n",
      "loss: 0.795029  [152000/175341]\n",
      "loss: 0.466835  [153600/175341]\n",
      "loss: 0.258253  [155200/175341]\n",
      "loss: 0.297824  [156800/175341]\n",
      "loss: 0.299563  [158400/175341]\n",
      "loss: 0.576364  [160000/175341]\n",
      "loss: 0.535488  [161600/175341]\n",
      "loss: 0.161493  [163200/175341]\n",
      "loss: 0.971203  [164800/175341]\n",
      "loss: 0.415561  [166400/175341]\n",
      "loss: 0.716596  [168000/175341]\n",
      "loss: 0.487812  [169600/175341]\n",
      "loss: 0.355610  [171200/175341]\n",
      "loss: 0.445873  [172800/175341]\n",
      "loss: 0.402639  [174400/175341]\n",
      "Train Accuracy: 80.0201%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.596327, F1-score: 75.37%, Macro_F1-Score:  38.92%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.052950  [    0/175341]\n",
      "loss: 0.687799  [ 1600/175341]\n",
      "loss: 0.923689  [ 3200/175341]\n",
      "loss: 0.566783  [ 4800/175341]\n",
      "loss: 0.439716  [ 6400/175341]\n",
      "loss: 0.661406  [ 8000/175341]\n",
      "loss: 0.436645  [ 9600/175341]\n",
      "loss: 0.528690  [11200/175341]\n",
      "loss: 0.426628  [12800/175341]\n",
      "loss: 0.446482  [14400/175341]\n",
      "loss: 0.592823  [16000/175341]\n",
      "loss: 0.342523  [17600/175341]\n",
      "loss: 0.254073  [19200/175341]\n",
      "loss: 0.258914  [20800/175341]\n",
      "loss: 0.807007  [22400/175341]\n",
      "loss: 0.201884  [24000/175341]\n",
      "loss: 0.608647  [25600/175341]\n",
      "loss: 0.596487  [27200/175341]\n",
      "loss: 0.327431  [28800/175341]\n",
      "loss: 0.593994  [30400/175341]\n",
      "loss: 0.783766  [32000/175341]\n",
      "loss: 0.353053  [33600/175341]\n",
      "loss: 1.071453  [35200/175341]\n",
      "loss: 0.555694  [36800/175341]\n",
      "loss: 0.699233  [38400/175341]\n",
      "loss: 0.665890  [40000/175341]\n",
      "loss: 0.320343  [41600/175341]\n",
      "loss: 0.758508  [43200/175341]\n",
      "loss: 0.472074  [44800/175341]\n",
      "loss: 0.548932  [46400/175341]\n",
      "loss: 0.264002  [48000/175341]\n",
      "loss: 0.510275  [49600/175341]\n",
      "loss: 0.477670  [51200/175341]\n",
      "loss: 0.156499  [52800/175341]\n",
      "loss: 0.743129  [54400/175341]\n",
      "loss: 0.595206  [56000/175341]\n",
      "loss: 0.496552  [57600/175341]\n",
      "loss: 0.403465  [59200/175341]\n",
      "loss: 0.514587  [60800/175341]\n",
      "loss: 0.482305  [62400/175341]\n",
      "loss: 0.252208  [64000/175341]\n",
      "loss: 0.292551  [65600/175341]\n",
      "loss: 0.378261  [67200/175341]\n",
      "loss: 0.724505  [68800/175341]\n",
      "loss: 0.502549  [70400/175341]\n",
      "loss: 0.584244  [72000/175341]\n",
      "loss: 0.824270  [73600/175341]\n",
      "loss: 0.436781  [75200/175341]\n",
      "loss: 0.188002  [76800/175341]\n",
      "loss: 0.914124  [78400/175341]\n",
      "loss: 0.295198  [80000/175341]\n",
      "loss: 0.270454  [81600/175341]\n",
      "loss: 0.487028  [83200/175341]\n",
      "loss: 0.462474  [84800/175341]\n",
      "loss: 0.376728  [86400/175341]\n",
      "loss: 0.441330  [88000/175341]\n",
      "loss: 1.009994  [89600/175341]\n",
      "loss: 0.204115  [91200/175341]\n",
      "loss: 0.428876  [92800/175341]\n",
      "loss: 0.533005  [94400/175341]\n",
      "loss: 0.591933  [96000/175341]\n",
      "loss: 0.456037  [97600/175341]\n",
      "loss: 0.597759  [99200/175341]\n",
      "loss: 0.231985  [100800/175341]\n",
      "loss: 0.481892  [102400/175341]\n",
      "loss: 0.462469  [104000/175341]\n",
      "loss: 0.479253  [105600/175341]\n",
      "loss: 0.428146  [107200/175341]\n",
      "loss: 0.544791  [108800/175341]\n",
      "loss: 0.357116  [110400/175341]\n",
      "loss: 0.957181  [112000/175341]\n",
      "loss: 0.318026  [113600/175341]\n",
      "loss: 0.659700  [115200/175341]\n",
      "loss: 0.301394  [116800/175341]\n",
      "loss: 0.543664  [118400/175341]\n",
      "loss: 0.391442  [120000/175341]\n",
      "loss: 0.232793  [121600/175341]\n",
      "loss: 0.637131  [123200/175341]\n",
      "loss: 0.382980  [124800/175341]\n",
      "loss: 0.186847  [126400/175341]\n",
      "loss: 0.768043  [128000/175341]\n",
      "loss: 0.818369  [129600/175341]\n",
      "loss: 0.757901  [131200/175341]\n",
      "loss: 0.505756  [132800/175341]\n",
      "loss: 0.314318  [134400/175341]\n",
      "loss: 0.816985  [136000/175341]\n",
      "loss: 0.767622  [137600/175341]\n",
      "loss: 0.692618  [139200/175341]\n",
      "loss: 0.508918  [140800/175341]\n",
      "loss: 0.287797  [142400/175341]\n",
      "loss: 0.298644  [144000/175341]\n",
      "loss: 0.300759  [145600/175341]\n",
      "loss: 0.252600  [147200/175341]\n",
      "loss: 0.448426  [148800/175341]\n",
      "loss: 0.618095  [150400/175341]\n",
      "loss: 0.244108  [152000/175341]\n",
      "loss: 0.866308  [153600/175341]\n",
      "loss: 0.601021  [155200/175341]\n",
      "loss: 0.811078  [156800/175341]\n",
      "loss: 0.591423  [158400/175341]\n",
      "loss: 0.721280  [160000/175341]\n",
      "loss: 0.794987  [161600/175341]\n",
      "loss: 0.403637  [163200/175341]\n",
      "loss: 0.807904  [164800/175341]\n",
      "loss: 0.889400  [166400/175341]\n",
      "loss: 0.742836  [168000/175341]\n",
      "loss: 0.321803  [169600/175341]\n",
      "loss: 0.418589  [171200/175341]\n",
      "loss: 0.507889  [172800/175341]\n",
      "loss: 0.636358  [174400/175341]\n",
      "Train Accuracy: 80.0885%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.597299, F1-score: 74.72%, Macro_F1-Score:  38.80%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.547834  [    0/175341]\n",
      "loss: 0.708080  [ 1600/175341]\n",
      "loss: 0.506683  [ 3200/175341]\n",
      "loss: 0.300665  [ 4800/175341]\n",
      "loss: 0.829215  [ 6400/175341]\n",
      "loss: 0.288969  [ 8000/175341]\n",
      "loss: 0.545591  [ 9600/175341]\n",
      "loss: 0.871802  [11200/175341]\n",
      "loss: 0.533928  [12800/175341]\n",
      "loss: 0.376655  [14400/175341]\n",
      "loss: 0.427329  [16000/175341]\n",
      "loss: 0.532143  [17600/175341]\n",
      "loss: 0.421598  [19200/175341]\n",
      "loss: 0.651303  [20800/175341]\n",
      "loss: 0.264488  [22400/175341]\n",
      "loss: 1.021419  [24000/175341]\n",
      "loss: 0.565215  [25600/175341]\n",
      "loss: 0.454013  [27200/175341]\n",
      "loss: 0.490168  [28800/175341]\n",
      "loss: 0.365240  [30400/175341]\n",
      "loss: 0.488441  [32000/175341]\n",
      "loss: 0.219429  [33600/175341]\n",
      "loss: 0.533248  [35200/175341]\n",
      "loss: 0.782829  [36800/175341]\n",
      "loss: 0.351380  [38400/175341]\n",
      "loss: 0.356509  [40000/175341]\n",
      "loss: 0.814620  [41600/175341]\n",
      "loss: 0.407123  [43200/175341]\n",
      "loss: 0.542306  [44800/175341]\n",
      "loss: 0.438742  [46400/175341]\n",
      "loss: 0.526604  [48000/175341]\n",
      "loss: 0.671629  [49600/175341]\n",
      "loss: 0.191672  [51200/175341]\n",
      "loss: 0.745003  [52800/175341]\n",
      "loss: 0.404829  [54400/175341]\n",
      "loss: 0.390301  [56000/175341]\n",
      "loss: 0.604648  [57600/175341]\n",
      "loss: 0.467950  [59200/175341]\n",
      "loss: 0.403164  [60800/175341]\n",
      "loss: 0.477251  [62400/175341]\n",
      "loss: 0.459823  [64000/175341]\n",
      "loss: 0.517748  [65600/175341]\n",
      "loss: 0.159291  [67200/175341]\n",
      "loss: 0.577320  [68800/175341]\n",
      "loss: 0.750175  [70400/175341]\n",
      "loss: 0.603586  [72000/175341]\n",
      "loss: 0.214437  [73600/175341]\n",
      "loss: 0.684700  [75200/175341]\n",
      "loss: 0.703992  [76800/175341]\n",
      "loss: 0.416545  [78400/175341]\n",
      "loss: 0.406007  [80000/175341]\n",
      "loss: 0.618368  [81600/175341]\n",
      "loss: 0.341162  [83200/175341]\n",
      "loss: 0.460850  [84800/175341]\n",
      "loss: 0.447795  [86400/175341]\n",
      "loss: 0.373367  [88000/175341]\n",
      "loss: 0.314313  [89600/175341]\n",
      "loss: 0.773933  [91200/175341]\n",
      "loss: 0.411897  [92800/175341]\n",
      "loss: 0.306189  [94400/175341]\n",
      "loss: 0.679380  [96000/175341]\n",
      "loss: 0.538605  [97600/175341]\n",
      "loss: 0.359550  [99200/175341]\n",
      "loss: 0.370181  [100800/175341]\n",
      "loss: 0.312845  [102400/175341]\n",
      "loss: 0.606062  [104000/175341]\n",
      "loss: 0.486135  [105600/175341]\n",
      "loss: 0.190505  [107200/175341]\n",
      "loss: 0.142144  [108800/175341]\n",
      "loss: 0.417052  [110400/175341]\n",
      "loss: 0.482298  [112000/175341]\n",
      "loss: 0.705552  [113600/175341]\n",
      "loss: 0.712097  [115200/175341]\n",
      "loss: 0.817946  [116800/175341]\n",
      "loss: 0.535189  [118400/175341]\n",
      "loss: 0.445980  [120000/175341]\n",
      "loss: 0.318504  [121600/175341]\n",
      "loss: 0.264332  [123200/175341]\n",
      "loss: 0.528136  [124800/175341]\n",
      "loss: 0.718607  [126400/175341]\n",
      "loss: 0.643988  [128000/175341]\n",
      "loss: 0.478130  [129600/175341]\n",
      "loss: 0.385948  [131200/175341]\n",
      "loss: 0.629284  [132800/175341]\n",
      "loss: 0.201998  [134400/175341]\n",
      "loss: 0.441095  [136000/175341]\n",
      "loss: 0.743871  [137600/175341]\n",
      "loss: 0.336195  [139200/175341]\n",
      "loss: 1.026284  [140800/175341]\n",
      "loss: 0.617173  [142400/175341]\n",
      "loss: 0.618887  [144000/175341]\n",
      "loss: 0.563863  [145600/175341]\n",
      "loss: 0.428143  [147200/175341]\n",
      "loss: 0.894466  [148800/175341]\n",
      "loss: 0.504694  [150400/175341]\n",
      "loss: 0.567978  [152000/175341]\n",
      "loss: 0.713300  [153600/175341]\n",
      "loss: 0.266670  [155200/175341]\n",
      "loss: 0.342145  [156800/175341]\n",
      "loss: 0.541133  [158400/175341]\n",
      "loss: 0.776659  [160000/175341]\n",
      "loss: 0.528256  [161600/175341]\n",
      "loss: 0.161714  [163200/175341]\n",
      "loss: 0.847610  [164800/175341]\n",
      "loss: 0.400930  [166400/175341]\n",
      "loss: 0.552268  [168000/175341]\n",
      "loss: 0.679028  [169600/175341]\n",
      "loss: 0.815862  [171200/175341]\n",
      "loss: 0.414330  [172800/175341]\n",
      "loss: 0.418352  [174400/175341]\n",
      "Train Accuracy: 80.1062%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.576305, F1-score: 75.99%, Macro_F1-Score:  39.22%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.569240  [    0/175341]\n",
      "loss: 0.725332  [ 1600/175341]\n",
      "loss: 0.519931  [ 3200/175341]\n",
      "loss: 0.878404  [ 4800/175341]\n",
      "loss: 0.479042  [ 6400/175341]\n",
      "loss: 0.555442  [ 8000/175341]\n",
      "loss: 0.858515  [ 9600/175341]\n",
      "loss: 0.718043  [11200/175341]\n",
      "loss: 0.705551  [12800/175341]\n",
      "loss: 0.554211  [14400/175341]\n",
      "loss: 0.371948  [16000/175341]\n",
      "loss: 0.474957  [17600/175341]\n",
      "loss: 0.200286  [19200/175341]\n",
      "loss: 0.198559  [20800/175341]\n",
      "loss: 0.222550  [22400/175341]\n",
      "loss: 0.830154  [24000/175341]\n",
      "loss: 0.327926  [25600/175341]\n",
      "loss: 0.825880  [27200/175341]\n",
      "loss: 0.442533  [28800/175341]\n",
      "loss: 0.291753  [30400/175341]\n",
      "loss: 0.766060  [32000/175341]\n",
      "loss: 1.055900  [33600/175341]\n",
      "loss: 0.652439  [35200/175341]\n",
      "loss: 0.453372  [36800/175341]\n",
      "loss: 0.619760  [38400/175341]\n",
      "loss: 0.348916  [40000/175341]\n",
      "loss: 0.816770  [41600/175341]\n",
      "loss: 0.315625  [43200/175341]\n",
      "loss: 0.511947  [44800/175341]\n",
      "loss: 0.234930  [46400/175341]\n",
      "loss: 0.393132  [48000/175341]\n",
      "loss: 1.008544  [49600/175341]\n",
      "loss: 0.980369  [51200/175341]\n",
      "loss: 0.486887  [52800/175341]\n",
      "loss: 0.322914  [54400/175341]\n",
      "loss: 0.407689  [56000/175341]\n",
      "loss: 0.684552  [57600/175341]\n",
      "loss: 0.563039  [59200/175341]\n",
      "loss: 0.228346  [60800/175341]\n",
      "loss: 0.321125  [62400/175341]\n",
      "loss: 0.200815  [64000/175341]\n",
      "loss: 0.428096  [65600/175341]\n",
      "loss: 0.480474  [67200/175341]\n",
      "loss: 0.231609  [68800/175341]\n",
      "loss: 0.478086  [70400/175341]\n",
      "loss: 0.809344  [72000/175341]\n",
      "loss: 0.362438  [73600/175341]\n",
      "loss: 0.127351  [75200/175341]\n",
      "loss: 0.591206  [76800/175341]\n",
      "loss: 0.362952  [78400/175341]\n",
      "loss: 0.392269  [80000/175341]\n",
      "loss: 0.607377  [81600/175341]\n",
      "loss: 0.356299  [83200/175341]\n",
      "loss: 0.132241  [84800/175341]\n",
      "loss: 0.593092  [86400/175341]\n",
      "loss: 0.529627  [88000/175341]\n",
      "loss: 0.358403  [89600/175341]\n",
      "loss: 0.388437  [91200/175341]\n",
      "loss: 0.437288  [92800/175341]\n",
      "loss: 0.563947  [94400/175341]\n",
      "loss: 0.294111  [96000/175341]\n",
      "loss: 0.753461  [97600/175341]\n",
      "loss: 1.025574  [99200/175341]\n",
      "loss: 0.687159  [100800/175341]\n",
      "loss: 0.417148  [102400/175341]\n",
      "loss: 0.389437  [104000/175341]\n",
      "loss: 0.694483  [105600/175341]\n",
      "loss: 0.314126  [107200/175341]\n",
      "loss: 0.392884  [108800/175341]\n",
      "loss: 0.418597  [110400/175341]\n",
      "loss: 0.711616  [112000/175341]\n",
      "loss: 0.696118  [113600/175341]\n",
      "loss: 0.191763  [115200/175341]\n",
      "loss: 0.429082  [116800/175341]\n",
      "loss: 0.442383  [118400/175341]\n",
      "loss: 0.173485  [120000/175341]\n",
      "loss: 0.763046  [121600/175341]\n",
      "loss: 0.325356  [123200/175341]\n",
      "loss: 0.475880  [124800/175341]\n",
      "loss: 0.369395  [126400/175341]\n",
      "loss: 0.625960  [128000/175341]\n",
      "loss: 0.355608  [129600/175341]\n",
      "loss: 0.245852  [131200/175341]\n",
      "loss: 0.492852  [132800/175341]\n",
      "loss: 1.194856  [134400/175341]\n",
      "loss: 0.411126  [136000/175341]\n",
      "loss: 0.246505  [137600/175341]\n",
      "loss: 0.600886  [139200/175341]\n",
      "loss: 0.482063  [140800/175341]\n",
      "loss: 0.461676  [142400/175341]\n",
      "loss: 0.305865  [144000/175341]\n",
      "loss: 0.824546  [145600/175341]\n",
      "loss: 0.378917  [147200/175341]\n",
      "loss: 0.812414  [148800/175341]\n",
      "loss: 0.751138  [150400/175341]\n",
      "loss: 0.513277  [152000/175341]\n",
      "loss: 0.469256  [153600/175341]\n",
      "loss: 0.455743  [155200/175341]\n",
      "loss: 0.404473  [156800/175341]\n",
      "loss: 0.688242  [158400/175341]\n",
      "loss: 0.330247  [160000/175341]\n",
      "loss: 0.780873  [161600/175341]\n",
      "loss: 0.838544  [163200/175341]\n",
      "loss: 0.591634  [164800/175341]\n",
      "loss: 0.617132  [166400/175341]\n",
      "loss: 1.135501  [168000/175341]\n",
      "loss: 0.531275  [169600/175341]\n",
      "loss: 0.672679  [171200/175341]\n",
      "loss: 0.534030  [172800/175341]\n",
      "loss: 0.226661  [174400/175341]\n",
      "Train Accuracy: 80.1176%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.607319, F1-score: 74.47%, Macro_F1-Score:  38.74%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.822773  [    0/175341]\n",
      "loss: 0.224260  [ 1600/175341]\n",
      "loss: 0.269693  [ 3200/175341]\n",
      "loss: 0.293512  [ 4800/175341]\n",
      "loss: 0.689439  [ 6400/175341]\n",
      "loss: 0.200272  [ 8000/175341]\n",
      "loss: 0.712610  [ 9600/175341]\n",
      "loss: 0.411341  [11200/175341]\n",
      "loss: 0.859208  [12800/175341]\n",
      "loss: 0.342905  [14400/175341]\n",
      "loss: 0.384255  [16000/175341]\n",
      "loss: 0.445322  [17600/175341]\n",
      "loss: 0.427980  [19200/175341]\n",
      "loss: 0.728971  [20800/175341]\n",
      "loss: 0.441349  [22400/175341]\n",
      "loss: 0.364206  [24000/175341]\n",
      "loss: 0.538899  [25600/175341]\n",
      "loss: 0.815676  [27200/175341]\n",
      "loss: 0.868809  [28800/175341]\n",
      "loss: 0.551666  [30400/175341]\n",
      "loss: 0.336927  [32000/175341]\n",
      "loss: 0.310338  [33600/175341]\n",
      "loss: 0.545963  [35200/175341]\n",
      "loss: 0.342092  [36800/175341]\n",
      "loss: 0.654461  [38400/175341]\n",
      "loss: 0.288459  [40000/175341]\n",
      "loss: 0.757061  [41600/175341]\n",
      "loss: 0.367822  [43200/175341]\n",
      "loss: 0.337739  [44800/175341]\n",
      "loss: 0.424719  [46400/175341]\n",
      "loss: 0.299790  [48000/175341]\n",
      "loss: 0.649143  [49600/175341]\n",
      "loss: 0.675894  [51200/175341]\n",
      "loss: 0.833574  [52800/175341]\n",
      "loss: 1.462688  [54400/175341]\n",
      "loss: 0.688829  [56000/175341]\n",
      "loss: 0.316497  [57600/175341]\n",
      "loss: 0.890547  [59200/175341]\n",
      "loss: 0.132760  [60800/175341]\n",
      "loss: 0.520240  [62400/175341]\n",
      "loss: 0.665892  [64000/175341]\n",
      "loss: 0.455980  [65600/175341]\n",
      "loss: 0.480443  [67200/175341]\n",
      "loss: 1.061563  [68800/175341]\n",
      "loss: 0.794736  [70400/175341]\n",
      "loss: 0.415386  [72000/175341]\n",
      "loss: 0.833684  [73600/175341]\n",
      "loss: 0.730525  [75200/175341]\n",
      "loss: 0.868115  [76800/175341]\n",
      "loss: 0.710782  [78400/175341]\n",
      "loss: 0.222713  [80000/175341]\n",
      "loss: 0.855905  [81600/175341]\n",
      "loss: 0.613719  [83200/175341]\n",
      "loss: 0.464307  [84800/175341]\n",
      "loss: 0.597494  [86400/175341]\n",
      "loss: 0.482009  [88000/175341]\n",
      "loss: 0.251169  [89600/175341]\n",
      "loss: 0.713101  [91200/175341]\n",
      "loss: 0.208722  [92800/175341]\n",
      "loss: 1.154901  [94400/175341]\n",
      "loss: 0.474827  [96000/175341]\n",
      "loss: 0.493617  [97600/175341]\n",
      "loss: 0.474885  [99200/175341]\n",
      "loss: 0.662004  [100800/175341]\n",
      "loss: 0.766549  [102400/175341]\n",
      "loss: 0.722404  [104000/175341]\n",
      "loss: 0.741996  [105600/175341]\n",
      "loss: 0.405722  [107200/175341]\n",
      "loss: 0.866669  [108800/175341]\n",
      "loss: 0.886675  [110400/175341]\n",
      "loss: 0.680261  [112000/175341]\n",
      "loss: 0.643915  [113600/175341]\n",
      "loss: 0.367603  [115200/175341]\n",
      "loss: 0.783981  [116800/175341]\n",
      "loss: 0.501327  [118400/175341]\n",
      "loss: 0.600920  [120000/175341]\n",
      "loss: 0.230453  [121600/175341]\n",
      "loss: 0.628314  [123200/175341]\n",
      "loss: 0.266849  [124800/175341]\n",
      "loss: 0.759849  [126400/175341]\n",
      "loss: 0.736956  [128000/175341]\n",
      "loss: 0.108589  [129600/175341]\n",
      "loss: 0.577985  [131200/175341]\n",
      "loss: 0.516900  [132800/175341]\n",
      "loss: 0.507220  [134400/175341]\n",
      "loss: 1.034147  [136000/175341]\n",
      "loss: 0.390995  [137600/175341]\n",
      "loss: 0.383632  [139200/175341]\n",
      "loss: 0.564688  [140800/175341]\n",
      "loss: 0.510717  [142400/175341]\n",
      "loss: 0.484315  [144000/175341]\n",
      "loss: 0.532919  [145600/175341]\n",
      "loss: 0.594577  [147200/175341]\n",
      "loss: 0.600395  [148800/175341]\n",
      "loss: 0.476105  [150400/175341]\n",
      "loss: 0.315340  [152000/175341]\n",
      "loss: 0.269243  [153600/175341]\n",
      "loss: 0.616665  [155200/175341]\n",
      "loss: 0.128458  [156800/175341]\n",
      "loss: 0.174327  [158400/175341]\n",
      "loss: 0.121997  [160000/175341]\n",
      "loss: 0.329221  [161600/175341]\n",
      "loss: 0.952535  [163200/175341]\n",
      "loss: 0.716069  [164800/175341]\n",
      "loss: 0.459833  [166400/175341]\n",
      "loss: 0.661193  [168000/175341]\n",
      "loss: 0.614266  [169600/175341]\n",
      "loss: 0.515582  [171200/175341]\n",
      "loss: 0.561992  [172800/175341]\n",
      "loss: 0.614555  [174400/175341]\n",
      "Train Accuracy: 80.1672%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.589638, F1-score: 75.29%, Macro_F1-Score:  39.06%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.595864  [    0/175341]\n",
      "loss: 0.384228  [ 1600/175341]\n",
      "loss: 1.117951  [ 3200/175341]\n",
      "loss: 0.644226  [ 4800/175341]\n",
      "loss: 0.778188  [ 6400/175341]\n",
      "loss: 0.404766  [ 8000/175341]\n",
      "loss: 0.219196  [ 9600/175341]\n",
      "loss: 0.414898  [11200/175341]\n",
      "loss: 0.531678  [12800/175341]\n",
      "loss: 0.402978  [14400/175341]\n",
      "loss: 0.390258  [16000/175341]\n",
      "loss: 0.654138  [17600/175341]\n",
      "loss: 0.311261  [19200/175341]\n",
      "loss: 0.535349  [20800/175341]\n",
      "loss: 0.351839  [22400/175341]\n",
      "loss: 0.736972  [24000/175341]\n",
      "loss: 0.494375  [25600/175341]\n",
      "loss: 0.898448  [27200/175341]\n",
      "loss: 0.649581  [28800/175341]\n",
      "loss: 0.666105  [30400/175341]\n",
      "loss: 0.722048  [32000/175341]\n",
      "loss: 1.088398  [33600/175341]\n",
      "loss: 0.369709  [35200/175341]\n",
      "loss: 0.675147  [36800/175341]\n",
      "loss: 0.183268  [38400/175341]\n",
      "loss: 0.761799  [40000/175341]\n",
      "loss: 0.675683  [41600/175341]\n",
      "loss: 0.219089  [43200/175341]\n",
      "loss: 0.546214  [44800/175341]\n",
      "loss: 0.297469  [46400/175341]\n",
      "loss: 0.330655  [48000/175341]\n",
      "loss: 0.439666  [49600/175341]\n",
      "loss: 0.237929  [51200/175341]\n",
      "loss: 0.442522  [52800/175341]\n",
      "loss: 0.573839  [54400/175341]\n",
      "loss: 0.382411  [56000/175341]\n",
      "loss: 0.332241  [57600/175341]\n",
      "loss: 0.408771  [59200/175341]\n",
      "loss: 0.305765  [60800/175341]\n",
      "loss: 0.359226  [62400/175341]\n",
      "loss: 0.495872  [64000/175341]\n",
      "loss: 0.605663  [65600/175341]\n",
      "loss: 0.619189  [67200/175341]\n",
      "loss: 0.384245  [68800/175341]\n",
      "loss: 0.666285  [70400/175341]\n",
      "loss: 0.309180  [72000/175341]\n",
      "loss: 0.308867  [73600/175341]\n",
      "loss: 0.610488  [75200/175341]\n",
      "loss: 0.681322  [76800/175341]\n",
      "loss: 0.602363  [78400/175341]\n",
      "loss: 0.626359  [80000/175341]\n",
      "loss: 0.403273  [81600/175341]\n",
      "loss: 0.572831  [83200/175341]\n",
      "loss: 0.594775  [84800/175341]\n",
      "loss: 0.734190  [86400/175341]\n",
      "loss: 0.343674  [88000/175341]\n",
      "loss: 0.334786  [89600/175341]\n",
      "loss: 0.970055  [91200/175341]\n",
      "loss: 0.526914  [92800/175341]\n",
      "loss: 0.471918  [94400/175341]\n",
      "loss: 0.568312  [96000/175341]\n",
      "loss: 0.518404  [97600/175341]\n",
      "loss: 0.669790  [99200/175341]\n",
      "loss: 0.304454  [100800/175341]\n",
      "loss: 0.718716  [102400/175341]\n",
      "loss: 0.430005  [104000/175341]\n",
      "loss: 0.578601  [105600/175341]\n",
      "loss: 0.440678  [107200/175341]\n",
      "loss: 0.394412  [108800/175341]\n",
      "loss: 0.510189  [110400/175341]\n",
      "loss: 0.374280  [112000/175341]\n",
      "loss: 0.669996  [113600/175341]\n",
      "loss: 0.828229  [115200/175341]\n",
      "loss: 0.727437  [116800/175341]\n",
      "loss: 0.415714  [118400/175341]\n",
      "loss: 0.493181  [120000/175341]\n",
      "loss: 0.598216  [121600/175341]\n",
      "loss: 0.432448  [123200/175341]\n",
      "loss: 0.532474  [124800/175341]\n",
      "loss: 0.431756  [126400/175341]\n",
      "loss: 0.418907  [128000/175341]\n",
      "loss: 0.464198  [129600/175341]\n",
      "loss: 0.522537  [131200/175341]\n",
      "loss: 0.299871  [132800/175341]\n",
      "loss: 0.646531  [134400/175341]\n",
      "loss: 0.547651  [136000/175341]\n",
      "loss: 0.673241  [137600/175341]\n",
      "loss: 0.772876  [139200/175341]\n",
      "loss: 0.657039  [140800/175341]\n",
      "loss: 0.657134  [142400/175341]\n",
      "loss: 0.380991  [144000/175341]\n",
      "loss: 0.581107  [145600/175341]\n",
      "loss: 0.603938  [147200/175341]\n",
      "loss: 0.436869  [148800/175341]\n",
      "loss: 0.158448  [150400/175341]\n",
      "loss: 0.133857  [152000/175341]\n",
      "loss: 0.411116  [153600/175341]\n",
      "loss: 0.823129  [155200/175341]\n",
      "loss: 0.163517  [156800/175341]\n",
      "loss: 0.646058  [158400/175341]\n",
      "loss: 0.406956  [160000/175341]\n",
      "loss: 0.358673  [161600/175341]\n",
      "loss: 1.091617  [163200/175341]\n",
      "loss: 0.540707  [164800/175341]\n",
      "loss: 0.372250  [166400/175341]\n",
      "loss: 0.635208  [168000/175341]\n",
      "loss: 0.059896  [169600/175341]\n",
      "loss: 0.388804  [171200/175341]\n",
      "loss: 0.608475  [172800/175341]\n",
      "loss: 0.512883  [174400/175341]\n",
      "Train Accuracy: 80.1524%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.618147, F1-score: 74.75%, Macro_F1-Score:  38.73%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.761067  [    0/175341]\n",
      "loss: 0.448454  [ 1600/175341]\n",
      "loss: 0.365507  [ 3200/175341]\n",
      "loss: 0.597771  [ 4800/175341]\n",
      "loss: 0.074635  [ 6400/175341]\n",
      "loss: 0.558947  [ 8000/175341]\n",
      "loss: 0.556032  [ 9600/175341]\n",
      "loss: 0.477742  [11200/175341]\n",
      "loss: 0.525590  [12800/175341]\n",
      "loss: 0.317845  [14400/175341]\n",
      "loss: 0.236292  [16000/175341]\n",
      "loss: 0.373049  [17600/175341]\n",
      "loss: 0.391283  [19200/175341]\n",
      "loss: 0.523679  [20800/175341]\n",
      "loss: 0.468694  [22400/175341]\n",
      "loss: 0.234480  [24000/175341]\n",
      "loss: 0.310601  [25600/175341]\n",
      "loss: 1.039995  [27200/175341]\n",
      "loss: 0.394118  [28800/175341]\n",
      "loss: 0.731938  [30400/175341]\n",
      "loss: 0.507172  [32000/175341]\n",
      "loss: 0.486239  [33600/175341]\n",
      "loss: 0.631953  [35200/175341]\n",
      "loss: 0.281553  [36800/175341]\n",
      "loss: 0.280979  [38400/175341]\n",
      "loss: 0.674527  [40000/175341]\n",
      "loss: 0.546429  [41600/175341]\n",
      "loss: 0.843423  [43200/175341]\n",
      "loss: 0.211235  [44800/175341]\n",
      "loss: 0.454182  [46400/175341]\n",
      "loss: 0.899567  [48000/175341]\n",
      "loss: 0.388517  [49600/175341]\n",
      "loss: 0.648181  [51200/175341]\n",
      "loss: 0.721020  [52800/175341]\n",
      "loss: 0.548859  [54400/175341]\n",
      "loss: 0.694783  [56000/175341]\n",
      "loss: 0.391921  [57600/175341]\n",
      "loss: 0.553014  [59200/175341]\n",
      "loss: 0.156769  [60800/175341]\n",
      "loss: 0.676195  [62400/175341]\n",
      "loss: 0.499167  [64000/175341]\n",
      "loss: 0.549871  [65600/175341]\n",
      "loss: 0.331206  [67200/175341]\n",
      "loss: 0.811364  [68800/175341]\n",
      "loss: 0.652113  [70400/175341]\n",
      "loss: 0.528481  [72000/175341]\n",
      "loss: 0.319296  [73600/175341]\n",
      "loss: 0.730892  [75200/175341]\n",
      "loss: 0.669575  [76800/175341]\n",
      "loss: 0.825346  [78400/175341]\n",
      "loss: 0.527990  [80000/175341]\n",
      "loss: 0.382356  [81600/175341]\n",
      "loss: 0.317038  [83200/175341]\n",
      "loss: 0.414747  [84800/175341]\n",
      "loss: 0.300078  [86400/175341]\n",
      "loss: 0.312422  [88000/175341]\n",
      "loss: 0.561223  [89600/175341]\n",
      "loss: 1.187261  [91200/175341]\n",
      "loss: 0.451798  [92800/175341]\n",
      "loss: 0.593590  [94400/175341]\n",
      "loss: 0.231146  [96000/175341]\n",
      "loss: 0.588851  [97600/175341]\n",
      "loss: 0.523558  [99200/175341]\n",
      "loss: 0.731855  [100800/175341]\n",
      "loss: 0.571567  [102400/175341]\n",
      "loss: 0.274349  [104000/175341]\n",
      "loss: 0.359133  [105600/175341]\n",
      "loss: 0.921935  [107200/175341]\n",
      "loss: 0.563748  [108800/175341]\n",
      "loss: 0.465058  [110400/175341]\n",
      "loss: 0.247905  [112000/175341]\n",
      "loss: 0.417506  [113600/175341]\n",
      "loss: 1.135648  [115200/175341]\n",
      "loss: 0.643772  [116800/175341]\n",
      "loss: 0.681690  [118400/175341]\n",
      "loss: 0.749955  [120000/175341]\n",
      "loss: 0.144962  [121600/175341]\n",
      "loss: 0.281541  [123200/175341]\n",
      "loss: 0.236622  [124800/175341]\n",
      "loss: 0.695336  [126400/175341]\n",
      "loss: 0.261936  [128000/175341]\n",
      "loss: 0.420337  [129600/175341]\n",
      "loss: 0.261368  [131200/175341]\n",
      "loss: 0.482264  [132800/175341]\n",
      "loss: 0.481916  [134400/175341]\n",
      "loss: 1.274730  [136000/175341]\n",
      "loss: 0.640092  [137600/175341]\n",
      "loss: 0.737503  [139200/175341]\n",
      "loss: 0.211779  [140800/175341]\n",
      "loss: 1.024292  [142400/175341]\n",
      "loss: 0.403901  [144000/175341]\n",
      "loss: 0.256080  [145600/175341]\n",
      "loss: 0.770320  [147200/175341]\n",
      "loss: 0.731695  [148800/175341]\n",
      "loss: 0.749639  [150400/175341]\n",
      "loss: 0.644453  [152000/175341]\n",
      "loss: 0.706061  [153600/175341]\n",
      "loss: 0.452070  [155200/175341]\n",
      "loss: 0.910312  [156800/175341]\n",
      "loss: 0.442436  [158400/175341]\n",
      "loss: 0.249805  [160000/175341]\n",
      "loss: 0.859909  [161600/175341]\n",
      "loss: 0.104587  [163200/175341]\n",
      "loss: 0.267122  [164800/175341]\n",
      "loss: 0.550293  [166400/175341]\n",
      "loss: 0.505736  [168000/175341]\n",
      "loss: 0.552118  [169600/175341]\n",
      "loss: 0.463006  [171200/175341]\n",
      "loss: 0.363439  [172800/175341]\n",
      "loss: 0.738604  [174400/175341]\n",
      "Train Accuracy: 80.2003%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.601484, F1-score: 75.33%, Macro_F1-Score:  38.74%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.407048  [    0/175341]\n",
      "loss: 0.308820  [ 1600/175341]\n",
      "loss: 0.764005  [ 3200/175341]\n",
      "loss: 0.464237  [ 4800/175341]\n",
      "loss: 0.425451  [ 6400/175341]\n",
      "loss: 0.142981  [ 8000/175341]\n",
      "loss: 0.745292  [ 9600/175341]\n",
      "loss: 0.514892  [11200/175341]\n",
      "loss: 0.939204  [12800/175341]\n",
      "loss: 0.499563  [14400/175341]\n",
      "loss: 0.520008  [16000/175341]\n",
      "loss: 0.489988  [17600/175341]\n",
      "loss: 0.459305  [19200/175341]\n",
      "loss: 0.434170  [20800/175341]\n",
      "loss: 0.823679  [22400/175341]\n",
      "loss: 0.566400  [24000/175341]\n",
      "loss: 0.945841  [25600/175341]\n",
      "loss: 0.673729  [27200/175341]\n",
      "loss: 0.809950  [28800/175341]\n",
      "loss: 0.973360  [30400/175341]\n",
      "loss: 0.326598  [32000/175341]\n",
      "loss: 0.356362  [33600/175341]\n",
      "loss: 0.214924  [35200/175341]\n",
      "loss: 0.728630  [36800/175341]\n",
      "loss: 0.229849  [38400/175341]\n",
      "loss: 0.564879  [40000/175341]\n",
      "loss: 0.283939  [41600/175341]\n",
      "loss: 0.827676  [43200/175341]\n",
      "loss: 0.328694  [44800/175341]\n",
      "loss: 0.622475  [46400/175341]\n",
      "loss: 0.265264  [48000/175341]\n",
      "loss: 0.577097  [49600/175341]\n",
      "loss: 0.541222  [51200/175341]\n",
      "loss: 0.347570  [52800/175341]\n",
      "loss: 0.654741  [54400/175341]\n",
      "loss: 1.255744  [56000/175341]\n",
      "loss: 0.619608  [57600/175341]\n",
      "loss: 0.561208  [59200/175341]\n",
      "loss: 0.785974  [60800/175341]\n",
      "loss: 0.431065  [62400/175341]\n",
      "loss: 0.441510  [64000/175341]\n",
      "loss: 0.748440  [65600/175341]\n",
      "loss: 0.567372  [67200/175341]\n",
      "loss: 0.333289  [68800/175341]\n",
      "loss: 0.590293  [70400/175341]\n",
      "loss: 0.462481  [72000/175341]\n",
      "loss: 0.394035  [73600/175341]\n",
      "loss: 0.266800  [75200/175341]\n",
      "loss: 0.379815  [76800/175341]\n",
      "loss: 0.435869  [78400/175341]\n",
      "loss: 0.360808  [80000/175341]\n",
      "loss: 0.424746  [81600/175341]\n",
      "loss: 0.394677  [83200/175341]\n",
      "loss: 0.313953  [84800/175341]\n",
      "loss: 0.909526  [86400/175341]\n",
      "loss: 0.427458  [88000/175341]\n",
      "loss: 1.159689  [89600/175341]\n",
      "loss: 0.133064  [91200/175341]\n",
      "loss: 0.505160  [92800/175341]\n",
      "loss: 0.625351  [94400/175341]\n",
      "loss: 0.342525  [96000/175341]\n",
      "loss: 0.482404  [97600/175341]\n",
      "loss: 0.408859  [99200/175341]\n",
      "loss: 0.386283  [100800/175341]\n",
      "loss: 0.455136  [102400/175341]\n",
      "loss: 0.615256  [104000/175341]\n",
      "loss: 0.329307  [105600/175341]\n",
      "loss: 1.263570  [107200/175341]\n",
      "loss: 0.616965  [108800/175341]\n",
      "loss: 0.249494  [110400/175341]\n",
      "loss: 0.565927  [112000/175341]\n",
      "loss: 0.300964  [113600/175341]\n",
      "loss: 0.474466  [115200/175341]\n",
      "loss: 0.584701  [116800/175341]\n",
      "loss: 0.371098  [118400/175341]\n",
      "loss: 0.902525  [120000/175341]\n",
      "loss: 0.620184  [121600/175341]\n",
      "loss: 0.190049  [123200/175341]\n",
      "loss: 0.344553  [124800/175341]\n",
      "loss: 0.637913  [126400/175341]\n",
      "loss: 0.472801  [128000/175341]\n",
      "loss: 1.034312  [129600/175341]\n",
      "loss: 0.514949  [131200/175341]\n",
      "loss: 0.414324  [132800/175341]\n",
      "loss: 0.293096  [134400/175341]\n",
      "loss: 0.665148  [136000/175341]\n",
      "loss: 0.319268  [137600/175341]\n",
      "loss: 0.517294  [139200/175341]\n",
      "loss: 0.620608  [140800/175341]\n",
      "loss: 0.425000  [142400/175341]\n",
      "loss: 0.365106  [144000/175341]\n",
      "loss: 0.351116  [145600/175341]\n",
      "loss: 0.496414  [147200/175341]\n",
      "loss: 0.359077  [148800/175341]\n",
      "loss: 0.184923  [150400/175341]\n",
      "loss: 0.423503  [152000/175341]\n",
      "loss: 0.693712  [153600/175341]\n",
      "loss: 0.157616  [155200/175341]\n",
      "loss: 0.520338  [156800/175341]\n",
      "loss: 0.678027  [158400/175341]\n",
      "loss: 0.328872  [160000/175341]\n",
      "loss: 0.607622  [161600/175341]\n",
      "loss: 0.291522  [163200/175341]\n",
      "loss: 0.350385  [164800/175341]\n",
      "loss: 0.334094  [166400/175341]\n",
      "loss: 0.314628  [168000/175341]\n",
      "loss: 0.305016  [169600/175341]\n",
      "loss: 0.554628  [171200/175341]\n",
      "loss: 0.528469  [172800/175341]\n",
      "loss: 0.578860  [174400/175341]\n",
      "Train Accuracy: 80.2254%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.600210, F1-score: 75.08%, Macro_F1-Score:  38.99%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.293668  [    0/175341]\n",
      "loss: 0.705346  [ 1600/175341]\n",
      "loss: 0.333184  [ 3200/175341]\n",
      "loss: 0.373886  [ 4800/175341]\n",
      "loss: 0.791820  [ 6400/175341]\n",
      "loss: 0.546620  [ 8000/175341]\n",
      "loss: 0.306211  [ 9600/175341]\n",
      "loss: 0.258267  [11200/175341]\n",
      "loss: 0.350412  [12800/175341]\n",
      "loss: 0.429280  [14400/175341]\n",
      "loss: 0.504199  [16000/175341]\n",
      "loss: 0.601083  [17600/175341]\n",
      "loss: 0.090128  [19200/175341]\n",
      "loss: 0.202860  [20800/175341]\n",
      "loss: 0.291743  [22400/175341]\n",
      "loss: 0.575620  [24000/175341]\n",
      "loss: 0.586651  [25600/175341]\n",
      "loss: 0.712942  [27200/175341]\n",
      "loss: 1.051049  [28800/175341]\n",
      "loss: 0.609903  [30400/175341]\n",
      "loss: 0.410622  [32000/175341]\n",
      "loss: 0.557933  [33600/175341]\n",
      "loss: 0.680990  [35200/175341]\n",
      "loss: 0.321968  [36800/175341]\n",
      "loss: 0.616190  [38400/175341]\n",
      "loss: 0.592035  [40000/175341]\n",
      "loss: 0.446863  [41600/175341]\n",
      "loss: 0.175686  [43200/175341]\n",
      "loss: 0.949058  [44800/175341]\n",
      "loss: 0.347205  [46400/175341]\n",
      "loss: 0.166009  [48000/175341]\n",
      "loss: 0.805666  [49600/175341]\n",
      "loss: 1.420170  [51200/175341]\n",
      "loss: 0.434176  [52800/175341]\n",
      "loss: 0.377014  [54400/175341]\n",
      "loss: 0.219086  [56000/175341]\n",
      "loss: 0.288616  [57600/175341]\n",
      "loss: 0.565826  [59200/175341]\n",
      "loss: 0.386085  [60800/175341]\n",
      "loss: 0.653069  [62400/175341]\n",
      "loss: 0.386656  [64000/175341]\n",
      "loss: 0.714504  [65600/175341]\n",
      "loss: 0.575451  [67200/175341]\n",
      "loss: 0.487711  [68800/175341]\n",
      "loss: 0.482250  [70400/175341]\n",
      "loss: 0.627810  [72000/175341]\n",
      "loss: 0.330998  [73600/175341]\n",
      "loss: 0.647811  [75200/175341]\n",
      "loss: 0.531015  [76800/175341]\n",
      "loss: 0.276569  [78400/175341]\n",
      "loss: 0.859651  [80000/175341]\n",
      "loss: 0.442628  [81600/175341]\n",
      "loss: 0.147182  [83200/175341]\n",
      "loss: 0.624701  [84800/175341]\n",
      "loss: 0.213338  [86400/175341]\n",
      "loss: 0.550638  [88000/175341]\n",
      "loss: 0.701525  [89600/175341]\n",
      "loss: 0.323741  [91200/175341]\n",
      "loss: 0.262257  [92800/175341]\n",
      "loss: 0.251506  [94400/175341]\n",
      "loss: 0.591934  [96000/175341]\n",
      "loss: 0.226132  [97600/175341]\n",
      "loss: 0.418683  [99200/175341]\n",
      "loss: 0.166954  [100800/175341]\n",
      "loss: 0.476424  [102400/175341]\n",
      "loss: 0.380939  [104000/175341]\n",
      "loss: 0.506392  [105600/175341]\n",
      "loss: 0.637647  [107200/175341]\n",
      "loss: 0.275065  [108800/175341]\n",
      "loss: 0.833149  [110400/175341]\n",
      "loss: 0.251202  [112000/175341]\n",
      "loss: 0.502469  [113600/175341]\n",
      "loss: 0.646856  [115200/175341]\n",
      "loss: 0.377431  [116800/175341]\n",
      "loss: 0.250109  [118400/175341]\n",
      "loss: 0.765882  [120000/175341]\n",
      "loss: 0.517105  [121600/175341]\n",
      "loss: 0.597172  [123200/175341]\n",
      "loss: 0.381794  [124800/175341]\n",
      "loss: 0.233047  [126400/175341]\n",
      "loss: 0.319878  [128000/175341]\n",
      "loss: 0.437492  [129600/175341]\n",
      "loss: 0.621119  [131200/175341]\n",
      "loss: 0.547179  [132800/175341]\n",
      "loss: 0.329811  [134400/175341]\n",
      "loss: 0.594525  [136000/175341]\n",
      "loss: 0.462075  [137600/175341]\n",
      "loss: 0.553326  [139200/175341]\n",
      "loss: 0.231131  [140800/175341]\n",
      "loss: 0.861610  [142400/175341]\n",
      "loss: 0.480202  [144000/175341]\n",
      "loss: 0.343621  [145600/175341]\n",
      "loss: 0.664345  [147200/175341]\n",
      "loss: 0.398374  [148800/175341]\n",
      "loss: 0.471157  [150400/175341]\n",
      "loss: 1.022424  [152000/175341]\n",
      "loss: 0.447268  [153600/175341]\n",
      "loss: 0.440739  [155200/175341]\n",
      "loss: 0.634220  [156800/175341]\n",
      "loss: 0.807420  [158400/175341]\n",
      "loss: 0.989275  [160000/175341]\n",
      "loss: 0.727611  [161600/175341]\n",
      "loss: 0.403192  [163200/175341]\n",
      "loss: 0.486104  [164800/175341]\n",
      "loss: 0.881796  [166400/175341]\n",
      "loss: 0.631224  [168000/175341]\n",
      "loss: 0.759322  [169600/175341]\n",
      "loss: 0.927981  [171200/175341]\n",
      "loss: 0.813248  [172800/175341]\n",
      "loss: 0.460864  [174400/175341]\n",
      "Train Accuracy: 80.2043%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.594239, F1-score: 75.35%, Macro_F1-Score:  39.19%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.108906  [    0/175341]\n",
      "loss: 0.257077  [ 1600/175341]\n",
      "loss: 0.157798  [ 3200/175341]\n",
      "loss: 0.374379  [ 4800/175341]\n",
      "loss: 0.543363  [ 6400/175341]\n",
      "loss: 0.352598  [ 8000/175341]\n",
      "loss: 0.152844  [ 9600/175341]\n",
      "loss: 0.585742  [11200/175341]\n",
      "loss: 0.134135  [12800/175341]\n",
      "loss: 0.201633  [14400/175341]\n",
      "loss: 0.398789  [16000/175341]\n",
      "loss: 0.625062  [17600/175341]\n",
      "loss: 0.622505  [19200/175341]\n",
      "loss: 0.392802  [20800/175341]\n",
      "loss: 0.523022  [22400/175341]\n",
      "loss: 0.254320  [24000/175341]\n",
      "loss: 0.472264  [25600/175341]\n",
      "loss: 0.597450  [27200/175341]\n",
      "loss: 0.435735  [28800/175341]\n",
      "loss: 0.232942  [30400/175341]\n",
      "loss: 0.614634  [32000/175341]\n",
      "loss: 0.314270  [33600/175341]\n",
      "loss: 0.456164  [35200/175341]\n",
      "loss: 0.712167  [36800/175341]\n",
      "loss: 0.887913  [38400/175341]\n",
      "loss: 1.154189  [40000/175341]\n",
      "loss: 0.498494  [41600/175341]\n",
      "loss: 0.567493  [43200/175341]\n",
      "loss: 0.120993  [44800/175341]\n",
      "loss: 0.500566  [46400/175341]\n",
      "loss: 0.371071  [48000/175341]\n",
      "loss: 0.726966  [49600/175341]\n",
      "loss: 0.380415  [51200/175341]\n",
      "loss: 0.366801  [52800/175341]\n",
      "loss: 0.560996  [54400/175341]\n",
      "loss: 0.639241  [56000/175341]\n",
      "loss: 0.286647  [57600/175341]\n",
      "loss: 0.326545  [59200/175341]\n",
      "loss: 0.336646  [60800/175341]\n",
      "loss: 0.655554  [62400/175341]\n",
      "loss: 0.854927  [64000/175341]\n",
      "loss: 0.659784  [65600/175341]\n",
      "loss: 0.645565  [67200/175341]\n",
      "loss: 0.430585  [68800/175341]\n",
      "loss: 0.165666  [70400/175341]\n",
      "loss: 0.775712  [72000/175341]\n",
      "loss: 0.662263  [73600/175341]\n",
      "loss: 0.544158  [75200/175341]\n",
      "loss: 0.770116  [76800/175341]\n",
      "loss: 0.239945  [78400/175341]\n",
      "loss: 0.297649  [80000/175341]\n",
      "loss: 0.649632  [81600/175341]\n",
      "loss: 0.498303  [83200/175341]\n",
      "loss: 0.865420  [84800/175341]\n",
      "loss: 0.717682  [86400/175341]\n",
      "loss: 0.528449  [88000/175341]\n",
      "loss: 0.620638  [89600/175341]\n",
      "loss: 0.240816  [91200/175341]\n",
      "loss: 0.509131  [92800/175341]\n",
      "loss: 0.321287  [94400/175341]\n",
      "loss: 0.401713  [96000/175341]\n",
      "loss: 0.618745  [97600/175341]\n",
      "loss: 0.138806  [99200/175341]\n",
      "loss: 0.710742  [100800/175341]\n",
      "loss: 0.487441  [102400/175341]\n",
      "loss: 0.725710  [104000/175341]\n",
      "loss: 0.218309  [105600/175341]\n",
      "loss: 0.517096  [107200/175341]\n",
      "loss: 0.724624  [108800/175341]\n",
      "loss: 0.538046  [110400/175341]\n",
      "loss: 0.543993  [112000/175341]\n",
      "loss: 0.251160  [113600/175341]\n",
      "loss: 0.282730  [115200/175341]\n",
      "loss: 0.528944  [116800/175341]\n",
      "loss: 0.936188  [118400/175341]\n",
      "loss: 0.331238  [120000/175341]\n",
      "loss: 0.573057  [121600/175341]\n",
      "loss: 0.483293  [123200/175341]\n",
      "loss: 0.729183  [124800/175341]\n",
      "loss: 0.438530  [126400/175341]\n",
      "loss: 0.197089  [128000/175341]\n",
      "loss: 0.609219  [129600/175341]\n",
      "loss: 0.295496  [131200/175341]\n",
      "loss: 0.377402  [132800/175341]\n",
      "loss: 0.521830  [134400/175341]\n",
      "loss: 0.430910  [136000/175341]\n",
      "loss: 0.416554  [137600/175341]\n",
      "loss: 0.485830  [139200/175341]\n",
      "loss: 0.299862  [140800/175341]\n",
      "loss: 0.737683  [142400/175341]\n",
      "loss: 0.289404  [144000/175341]\n",
      "loss: 0.920883  [145600/175341]\n",
      "loss: 0.264739  [147200/175341]\n",
      "loss: 0.059610  [148800/175341]\n",
      "loss: 0.492004  [150400/175341]\n",
      "loss: 0.973832  [152000/175341]\n",
      "loss: 0.241168  [153600/175341]\n",
      "loss: 0.389726  [155200/175341]\n",
      "loss: 0.490370  [156800/175341]\n",
      "loss: 0.564241  [158400/175341]\n",
      "loss: 0.413476  [160000/175341]\n",
      "loss: 0.581140  [161600/175341]\n",
      "loss: 0.470945  [163200/175341]\n",
      "loss: 0.265037  [164800/175341]\n",
      "loss: 0.615246  [166400/175341]\n",
      "loss: 0.182410  [168000/175341]\n",
      "loss: 0.653757  [169600/175341]\n",
      "loss: 0.361836  [171200/175341]\n",
      "loss: 0.530973  [172800/175341]\n",
      "loss: 0.252678  [174400/175341]\n",
      "Train Accuracy: 80.2562%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.619271, F1-score: 74.45%, Macro_F1-Score:  39.20%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.417899  [    0/175341]\n",
      "loss: 0.403108  [ 1600/175341]\n",
      "loss: 0.495599  [ 3200/175341]\n",
      "loss: 0.309336  [ 4800/175341]\n",
      "loss: 0.286517  [ 6400/175341]\n",
      "loss: 0.780772  [ 8000/175341]\n",
      "loss: 0.305834  [ 9600/175341]\n",
      "loss: 0.494233  [11200/175341]\n",
      "loss: 1.046178  [12800/175341]\n",
      "loss: 0.916555  [14400/175341]\n",
      "loss: 0.655828  [16000/175341]\n",
      "loss: 0.412674  [17600/175341]\n",
      "loss: 0.483051  [19200/175341]\n",
      "loss: 0.167324  [20800/175341]\n",
      "loss: 0.407861  [22400/175341]\n",
      "loss: 0.390251  [24000/175341]\n",
      "loss: 0.937504  [25600/175341]\n",
      "loss: 0.424538  [27200/175341]\n",
      "loss: 0.435250  [28800/175341]\n",
      "loss: 0.880482  [30400/175341]\n",
      "loss: 0.792072  [32000/175341]\n",
      "loss: 0.688006  [33600/175341]\n",
      "loss: 0.427732  [35200/175341]\n",
      "loss: 0.269201  [36800/175341]\n",
      "loss: 0.271902  [38400/175341]\n",
      "loss: 0.318881  [40000/175341]\n",
      "loss: 0.144173  [41600/175341]\n",
      "loss: 0.705454  [43200/175341]\n",
      "loss: 0.411628  [44800/175341]\n",
      "loss: 0.418548  [46400/175341]\n",
      "loss: 0.284046  [48000/175341]\n",
      "loss: 0.683942  [49600/175341]\n",
      "loss: 0.560562  [51200/175341]\n",
      "loss: 0.605458  [52800/175341]\n",
      "loss: 0.590827  [54400/175341]\n",
      "loss: 0.406153  [56000/175341]\n",
      "loss: 0.257239  [57600/175341]\n",
      "loss: 0.841661  [59200/175341]\n",
      "loss: 0.185098  [60800/175341]\n",
      "loss: 0.401458  [62400/175341]\n",
      "loss: 0.424411  [64000/175341]\n",
      "loss: 0.604780  [65600/175341]\n",
      "loss: 0.539722  [67200/175341]\n",
      "loss: 0.275180  [68800/175341]\n",
      "loss: 0.390734  [70400/175341]\n",
      "loss: 0.247386  [72000/175341]\n",
      "loss: 0.996523  [73600/175341]\n",
      "loss: 0.307119  [75200/175341]\n",
      "loss: 0.674863  [76800/175341]\n",
      "loss: 0.242459  [78400/175341]\n",
      "loss: 0.785191  [80000/175341]\n",
      "loss: 0.554343  [81600/175341]\n",
      "loss: 0.358241  [83200/175341]\n",
      "loss: 0.707628  [84800/175341]\n",
      "loss: 0.433806  [86400/175341]\n",
      "loss: 0.617466  [88000/175341]\n",
      "loss: 0.379874  [89600/175341]\n",
      "loss: 0.779226  [91200/175341]\n",
      "loss: 0.755783  [92800/175341]\n",
      "loss: 0.205134  [94400/175341]\n",
      "loss: 0.976038  [96000/175341]\n",
      "loss: 0.502126  [97600/175341]\n",
      "loss: 0.437198  [99200/175341]\n",
      "loss: 0.516679  [100800/175341]\n",
      "loss: 0.355934  [102400/175341]\n",
      "loss: 0.421251  [104000/175341]\n",
      "loss: 0.732555  [105600/175341]\n",
      "loss: 0.159975  [107200/175341]\n",
      "loss: 0.487712  [108800/175341]\n",
      "loss: 0.606668  [110400/175341]\n",
      "loss: 0.491583  [112000/175341]\n",
      "loss: 0.478931  [113600/175341]\n",
      "loss: 0.566433  [115200/175341]\n",
      "loss: 0.389728  [116800/175341]\n",
      "loss: 0.478522  [118400/175341]\n",
      "loss: 0.682937  [120000/175341]\n",
      "loss: 0.357890  [121600/175341]\n",
      "loss: 0.422837  [123200/175341]\n",
      "loss: 0.484891  [124800/175341]\n",
      "loss: 0.443720  [126400/175341]\n",
      "loss: 0.611531  [128000/175341]\n",
      "loss: 0.534450  [129600/175341]\n",
      "loss: 0.435453  [131200/175341]\n",
      "loss: 0.511482  [132800/175341]\n",
      "loss: 0.444780  [134400/175341]\n",
      "loss: 0.470841  [136000/175341]\n",
      "loss: 0.561846  [137600/175341]\n",
      "loss: 0.838396  [139200/175341]\n",
      "loss: 0.193948  [140800/175341]\n",
      "loss: 0.954843  [142400/175341]\n",
      "loss: 0.232946  [144000/175341]\n",
      "loss: 0.196323  [145600/175341]\n",
      "loss: 0.481528  [147200/175341]\n",
      "loss: 0.315827  [148800/175341]\n",
      "loss: 1.039757  [150400/175341]\n",
      "loss: 0.515748  [152000/175341]\n",
      "loss: 0.713359  [153600/175341]\n",
      "loss: 0.605590  [155200/175341]\n",
      "loss: 0.628608  [156800/175341]\n",
      "loss: 0.370470  [158400/175341]\n",
      "loss: 0.898194  [160000/175341]\n",
      "loss: 0.176036  [161600/175341]\n",
      "loss: 0.165215  [163200/175341]\n",
      "loss: 0.480745  [164800/175341]\n",
      "loss: 0.616498  [166400/175341]\n",
      "loss: 0.749668  [168000/175341]\n",
      "loss: 0.682278  [169600/175341]\n",
      "loss: 0.281055  [171200/175341]\n",
      "loss: 0.230784  [172800/175341]\n",
      "loss: 0.558044  [174400/175341]\n",
      "Train Accuracy: 80.2511%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.621279, F1-score: 74.15%, Macro_F1-Score:  38.24%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.268531  [    0/175341]\n",
      "loss: 0.473714  [ 1600/175341]\n",
      "loss: 0.313559  [ 3200/175341]\n",
      "loss: 0.803306  [ 4800/175341]\n",
      "loss: 0.581006  [ 6400/175341]\n",
      "loss: 0.114959  [ 8000/175341]\n",
      "loss: 0.696811  [ 9600/175341]\n",
      "loss: 0.147519  [11200/175341]\n",
      "loss: 0.483893  [12800/175341]\n",
      "loss: 0.423113  [14400/175341]\n",
      "loss: 0.762433  [16000/175341]\n",
      "loss: 0.181713  [17600/175341]\n",
      "loss: 0.400726  [19200/175341]\n",
      "loss: 0.485403  [20800/175341]\n",
      "loss: 0.622726  [22400/175341]\n",
      "loss: 0.449286  [24000/175341]\n",
      "loss: 0.371808  [25600/175341]\n",
      "loss: 0.497106  [27200/175341]\n",
      "loss: 0.551283  [28800/175341]\n",
      "loss: 0.575915  [30400/175341]\n",
      "loss: 0.618497  [32000/175341]\n",
      "loss: 0.443174  [33600/175341]\n",
      "loss: 0.673756  [35200/175341]\n",
      "loss: 0.295960  [36800/175341]\n",
      "loss: 1.262347  [38400/175341]\n",
      "loss: 0.407636  [40000/175341]\n",
      "loss: 0.253020  [41600/175341]\n",
      "loss: 0.668227  [43200/175341]\n",
      "loss: 0.250711  [44800/175341]\n",
      "loss: 0.507989  [46400/175341]\n",
      "loss: 0.493437  [48000/175341]\n",
      "loss: 0.839509  [49600/175341]\n",
      "loss: 0.445008  [51200/175341]\n",
      "loss: 0.403158  [52800/175341]\n",
      "loss: 0.439824  [54400/175341]\n",
      "loss: 0.327343  [56000/175341]\n",
      "loss: 0.656347  [57600/175341]\n",
      "loss: 0.455922  [59200/175341]\n",
      "loss: 0.192456  [60800/175341]\n",
      "loss: 0.963979  [62400/175341]\n",
      "loss: 0.469656  [64000/175341]\n",
      "loss: 0.661614  [65600/175341]\n",
      "loss: 0.563887  [67200/175341]\n",
      "loss: 0.686527  [68800/175341]\n",
      "loss: 0.461315  [70400/175341]\n",
      "loss: 0.216410  [72000/175341]\n",
      "loss: 0.202237  [73600/175341]\n",
      "loss: 0.826103  [75200/175341]\n",
      "loss: 0.394763  [76800/175341]\n",
      "loss: 0.230465  [78400/175341]\n",
      "loss: 0.335038  [80000/175341]\n",
      "loss: 0.204240  [81600/175341]\n",
      "loss: 0.555764  [83200/175341]\n",
      "loss: 0.667205  [84800/175341]\n",
      "loss: 0.390885  [86400/175341]\n",
      "loss: 0.554744  [88000/175341]\n",
      "loss: 0.388865  [89600/175341]\n",
      "loss: 0.639407  [91200/175341]\n",
      "loss: 0.230940  [92800/175341]\n",
      "loss: 0.474610  [94400/175341]\n",
      "loss: 0.200388  [96000/175341]\n",
      "loss: 0.704404  [97600/175341]\n",
      "loss: 0.926629  [99200/175341]\n",
      "loss: 0.565597  [100800/175341]\n",
      "loss: 0.414524  [102400/175341]\n",
      "loss: 0.398714  [104000/175341]\n",
      "loss: 0.763332  [105600/175341]\n",
      "loss: 0.643617  [107200/175341]\n",
      "loss: 0.356181  [108800/175341]\n",
      "loss: 0.217343  [110400/175341]\n",
      "loss: 0.543672  [112000/175341]\n",
      "loss: 0.220680  [113600/175341]\n",
      "loss: 0.625603  [115200/175341]\n",
      "loss: 0.688212  [116800/175341]\n",
      "loss: 0.611381  [118400/175341]\n",
      "loss: 0.496859  [120000/175341]\n",
      "loss: 0.520317  [121600/175341]\n",
      "loss: 0.331921  [123200/175341]\n",
      "loss: 1.079011  [124800/175341]\n",
      "loss: 0.726701  [126400/175341]\n",
      "loss: 0.553615  [128000/175341]\n",
      "loss: 0.189892  [129600/175341]\n",
      "loss: 0.629270  [131200/175341]\n",
      "loss: 0.503439  [132800/175341]\n",
      "loss: 0.385808  [134400/175341]\n",
      "loss: 0.359125  [136000/175341]\n",
      "loss: 0.412007  [137600/175341]\n",
      "loss: 0.311901  [139200/175341]\n",
      "loss: 0.573578  [140800/175341]\n",
      "loss: 0.655927  [142400/175341]\n",
      "loss: 0.312203  [144000/175341]\n",
      "loss: 0.580427  [145600/175341]\n",
      "loss: 1.443878  [147200/175341]\n",
      "loss: 0.592722  [148800/175341]\n",
      "loss: 0.336309  [150400/175341]\n",
      "loss: 0.528683  [152000/175341]\n",
      "loss: 0.414773  [153600/175341]\n",
      "loss: 0.231502  [155200/175341]\n",
      "loss: 0.436878  [156800/175341]\n",
      "loss: 0.371762  [158400/175341]\n",
      "loss: 0.720913  [160000/175341]\n",
      "loss: 0.746769  [161600/175341]\n",
      "loss: 0.715290  [163200/175341]\n",
      "loss: 0.428648  [164800/175341]\n",
      "loss: 0.500161  [166400/175341]\n",
      "loss: 0.509753  [168000/175341]\n",
      "loss: 0.436586  [169600/175341]\n",
      "loss: 0.579936  [171200/175341]\n",
      "loss: 0.471517  [172800/175341]\n",
      "loss: 0.637136  [174400/175341]\n",
      "Train Accuracy: 80.2568%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.590770, F1-score: 75.75%, Macro_F1-Score:  39.30%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.684192  [    0/175341]\n",
      "loss: 0.415446  [ 1600/175341]\n",
      "loss: 0.564233  [ 3200/175341]\n",
      "loss: 0.325262  [ 4800/175341]\n",
      "loss: 0.279336  [ 6400/175341]\n",
      "loss: 0.286960  [ 8000/175341]\n",
      "loss: 0.620914  [ 9600/175341]\n",
      "loss: 0.861471  [11200/175341]\n",
      "loss: 0.279414  [12800/175341]\n",
      "loss: 0.073671  [14400/175341]\n",
      "loss: 0.292451  [16000/175341]\n",
      "loss: 0.711907  [17600/175341]\n",
      "loss: 0.207787  [19200/175341]\n",
      "loss: 0.314432  [20800/175341]\n",
      "loss: 0.371871  [22400/175341]\n",
      "loss: 0.617886  [24000/175341]\n",
      "loss: 0.201180  [25600/175341]\n",
      "loss: 0.276004  [27200/175341]\n",
      "loss: 0.416582  [28800/175341]\n",
      "loss: 0.797179  [30400/175341]\n",
      "loss: 0.375560  [32000/175341]\n",
      "loss: 0.163524  [33600/175341]\n",
      "loss: 0.308855  [35200/175341]\n",
      "loss: 0.503033  [36800/175341]\n",
      "loss: 1.139328  [38400/175341]\n",
      "loss: 0.537165  [40000/175341]\n",
      "loss: 0.960819  [41600/175341]\n",
      "loss: 0.336909  [43200/175341]\n",
      "loss: 0.467675  [44800/175341]\n",
      "loss: 0.311319  [46400/175341]\n",
      "loss: 0.505374  [48000/175341]\n",
      "loss: 0.091310  [49600/175341]\n",
      "loss: 0.551410  [51200/175341]\n",
      "loss: 0.409335  [52800/175341]\n",
      "loss: 0.621726  [54400/175341]\n",
      "loss: 0.659058  [56000/175341]\n",
      "loss: 1.225640  [57600/175341]\n",
      "loss: 0.678360  [59200/175341]\n",
      "loss: 0.550904  [60800/175341]\n",
      "loss: 0.295417  [62400/175341]\n",
      "loss: 0.562639  [64000/175341]\n",
      "loss: 0.287904  [65600/175341]\n",
      "loss: 0.187164  [67200/175341]\n",
      "loss: 0.491996  [68800/175341]\n",
      "loss: 0.273770  [70400/175341]\n",
      "loss: 0.487901  [72000/175341]\n",
      "loss: 0.307201  [73600/175341]\n",
      "loss: 0.661302  [75200/175341]\n",
      "loss: 0.740211  [76800/175341]\n",
      "loss: 0.356750  [78400/175341]\n",
      "loss: 0.364687  [80000/175341]\n",
      "loss: 0.350831  [81600/175341]\n",
      "loss: 0.306975  [83200/175341]\n",
      "loss: 1.050417  [84800/175341]\n",
      "loss: 0.773542  [86400/175341]\n",
      "loss: 0.663694  [88000/175341]\n",
      "loss: 0.283078  [89600/175341]\n",
      "loss: 0.318086  [91200/175341]\n",
      "loss: 0.454941  [92800/175341]\n",
      "loss: 0.424686  [94400/175341]\n",
      "loss: 0.557086  [96000/175341]\n",
      "loss: 0.401814  [97600/175341]\n",
      "loss: 0.392413  [99200/175341]\n",
      "loss: 0.567228  [100800/175341]\n",
      "loss: 0.535347  [102400/175341]\n",
      "loss: 0.957021  [104000/175341]\n",
      "loss: 0.340428  [105600/175341]\n",
      "loss: 0.681348  [107200/175341]\n",
      "loss: 0.449878  [108800/175341]\n",
      "loss: 0.847379  [110400/175341]\n",
      "loss: 0.676147  [112000/175341]\n",
      "loss: 0.169759  [113600/175341]\n",
      "loss: 0.471592  [115200/175341]\n",
      "loss: 0.592092  [116800/175341]\n",
      "loss: 0.347545  [118400/175341]\n",
      "loss: 0.386277  [120000/175341]\n",
      "loss: 0.861293  [121600/175341]\n",
      "loss: 0.175692  [123200/175341]\n",
      "loss: 0.444401  [124800/175341]\n",
      "loss: 0.273860  [126400/175341]\n",
      "loss: 0.705800  [128000/175341]\n",
      "loss: 0.859870  [129600/175341]\n",
      "loss: 0.390770  [131200/175341]\n",
      "loss: 0.738625  [132800/175341]\n",
      "loss: 0.807544  [134400/175341]\n",
      "loss: 0.963102  [136000/175341]\n",
      "loss: 0.275912  [137600/175341]\n",
      "loss: 0.414156  [139200/175341]\n",
      "loss: 0.510088  [140800/175341]\n",
      "loss: 0.636815  [142400/175341]\n",
      "loss: 0.802551  [144000/175341]\n",
      "loss: 0.223752  [145600/175341]\n",
      "loss: 0.382311  [147200/175341]\n",
      "loss: 0.585580  [148800/175341]\n",
      "loss: 0.173732  [150400/175341]\n",
      "loss: 0.654931  [152000/175341]\n",
      "loss: 0.347617  [153600/175341]\n",
      "loss: 0.267170  [155200/175341]\n",
      "loss: 0.436537  [156800/175341]\n",
      "loss: 0.345278  [158400/175341]\n",
      "loss: 0.813748  [160000/175341]\n",
      "loss: 0.487669  [161600/175341]\n",
      "loss: 0.789421  [163200/175341]\n",
      "loss: 0.452798  [164800/175341]\n",
      "loss: 0.591981  [166400/175341]\n",
      "loss: 0.624889  [168000/175341]\n",
      "loss: 0.556963  [169600/175341]\n",
      "loss: 0.293175  [171200/175341]\n",
      "loss: 0.304689  [172800/175341]\n",
      "loss: 0.208872  [174400/175341]\n",
      "Train Accuracy: 80.3030%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.620253, F1-score: 74.99%, Macro_F1-Score:  38.78%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.523450  [    0/175341]\n",
      "loss: 0.398969  [ 1600/175341]\n",
      "loss: 0.687743  [ 3200/175341]\n",
      "loss: 0.364419  [ 4800/175341]\n",
      "loss: 0.296001  [ 6400/175341]\n",
      "loss: 0.549995  [ 8000/175341]\n",
      "loss: 0.151134  [ 9600/175341]\n",
      "loss: 0.565955  [11200/175341]\n",
      "loss: 0.415474  [12800/175341]\n",
      "loss: 0.338675  [14400/175341]\n",
      "loss: 0.683904  [16000/175341]\n",
      "loss: 0.489088  [17600/175341]\n",
      "loss: 0.402172  [19200/175341]\n",
      "loss: 0.881797  [20800/175341]\n",
      "loss: 0.475251  [22400/175341]\n",
      "loss: 0.478214  [24000/175341]\n",
      "loss: 0.755776  [25600/175341]\n",
      "loss: 0.349646  [27200/175341]\n",
      "loss: 0.408201  [28800/175341]\n",
      "loss: 0.415558  [30400/175341]\n",
      "loss: 0.364821  [32000/175341]\n",
      "loss: 0.463495  [33600/175341]\n",
      "loss: 0.680573  [35200/175341]\n",
      "loss: 0.341678  [36800/175341]\n",
      "loss: 0.658912  [38400/175341]\n",
      "loss: 0.508815  [40000/175341]\n",
      "loss: 0.538448  [41600/175341]\n",
      "loss: 0.658977  [43200/175341]\n",
      "loss: 0.315795  [44800/175341]\n",
      "loss: 0.269983  [46400/175341]\n",
      "loss: 0.537042  [48000/175341]\n",
      "loss: 0.602483  [49600/175341]\n",
      "loss: 0.514792  [51200/175341]\n",
      "loss: 0.494329  [52800/175341]\n",
      "loss: 0.452444  [54400/175341]\n",
      "loss: 0.864099  [56000/175341]\n",
      "loss: 0.269476  [57600/175341]\n",
      "loss: 0.583250  [59200/175341]\n",
      "loss: 0.203253  [60800/175341]\n",
      "loss: 1.227426  [62400/175341]\n",
      "loss: 0.810526  [64000/175341]\n",
      "loss: 0.761984  [65600/175341]\n",
      "loss: 0.398508  [67200/175341]\n",
      "loss: 0.278866  [68800/175341]\n",
      "loss: 0.475255  [70400/175341]\n",
      "loss: 0.626528  [72000/175341]\n",
      "loss: 0.633035  [73600/175341]\n",
      "loss: 0.457708  [75200/175341]\n",
      "loss: 0.421260  [76800/175341]\n",
      "loss: 0.521892  [78400/175341]\n",
      "loss: 0.256295  [80000/175341]\n",
      "loss: 0.583576  [81600/175341]\n",
      "loss: 0.266994  [83200/175341]\n",
      "loss: 0.375137  [84800/175341]\n",
      "loss: 0.699338  [86400/175341]\n",
      "loss: 0.219538  [88000/175341]\n",
      "loss: 1.560404  [89600/175341]\n",
      "loss: 0.321056  [91200/175341]\n",
      "loss: 0.562071  [92800/175341]\n",
      "loss: 0.583128  [94400/175341]\n",
      "loss: 0.122633  [96000/175341]\n",
      "loss: 0.702007  [97600/175341]\n",
      "loss: 0.486362  [99200/175341]\n",
      "loss: 0.707628  [100800/175341]\n",
      "loss: 0.544994  [102400/175341]\n",
      "loss: 0.614337  [104000/175341]\n",
      "loss: 0.352762  [105600/175341]\n",
      "loss: 0.658600  [107200/175341]\n",
      "loss: 0.401612  [108800/175341]\n",
      "loss: 0.214902  [110400/175341]\n",
      "loss: 0.397202  [112000/175341]\n",
      "loss: 0.506421  [113600/175341]\n",
      "loss: 0.710673  [115200/175341]\n",
      "loss: 0.556251  [116800/175341]\n",
      "loss: 0.416368  [118400/175341]\n",
      "loss: 0.532121  [120000/175341]\n",
      "loss: 0.473988  [121600/175341]\n",
      "loss: 0.862944  [123200/175341]\n",
      "loss: 0.867006  [124800/175341]\n",
      "loss: 1.008353  [126400/175341]\n",
      "loss: 0.197534  [128000/175341]\n",
      "loss: 0.404217  [129600/175341]\n",
      "loss: 0.315190  [131200/175341]\n",
      "loss: 0.583302  [132800/175341]\n",
      "loss: 0.600277  [134400/175341]\n",
      "loss: 0.610395  [136000/175341]\n",
      "loss: 0.543859  [137600/175341]\n",
      "loss: 0.710112  [139200/175341]\n",
      "loss: 0.488222  [140800/175341]\n",
      "loss: 0.585077  [142400/175341]\n",
      "loss: 0.430490  [144000/175341]\n",
      "loss: 0.512927  [145600/175341]\n",
      "loss: 0.604821  [147200/175341]\n",
      "loss: 0.180111  [148800/175341]\n",
      "loss: 0.669204  [150400/175341]\n",
      "loss: 0.358656  [152000/175341]\n",
      "loss: 0.789466  [153600/175341]\n",
      "loss: 0.386788  [155200/175341]\n",
      "loss: 0.689926  [156800/175341]\n",
      "loss: 0.339268  [158400/175341]\n",
      "loss: 0.215891  [160000/175341]\n",
      "loss: 0.700158  [161600/175341]\n",
      "loss: 0.362678  [163200/175341]\n",
      "loss: 0.541505  [164800/175341]\n",
      "loss: 0.172832  [166400/175341]\n",
      "loss: 0.288716  [168000/175341]\n",
      "loss: 0.535796  [169600/175341]\n",
      "loss: 0.748129  [171200/175341]\n",
      "loss: 0.194030  [172800/175341]\n",
      "loss: 0.708547  [174400/175341]\n",
      "Train Accuracy: 80.3395%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.591478, F1-score: 75.23%, Macro_F1-Score:  39.40%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.499035  [    0/175341]\n",
      "loss: 0.385315  [ 1600/175341]\n",
      "loss: 0.522736  [ 3200/175341]\n",
      "loss: 0.206233  [ 4800/175341]\n",
      "loss: 0.721563  [ 6400/175341]\n",
      "loss: 0.547156  [ 8000/175341]\n",
      "loss: 0.791015  [ 9600/175341]\n",
      "loss: 0.615774  [11200/175341]\n",
      "loss: 0.586663  [12800/175341]\n",
      "loss: 0.573535  [14400/175341]\n",
      "loss: 0.528486  [16000/175341]\n",
      "loss: 0.248813  [17600/175341]\n",
      "loss: 0.425005  [19200/175341]\n",
      "loss: 0.595298  [20800/175341]\n",
      "loss: 0.946919  [22400/175341]\n",
      "loss: 0.190885  [24000/175341]\n",
      "loss: 0.401398  [25600/175341]\n",
      "loss: 0.442293  [27200/175341]\n",
      "loss: 0.220525  [28800/175341]\n",
      "loss: 0.389077  [30400/175341]\n",
      "loss: 0.913169  [32000/175341]\n",
      "loss: 0.253292  [33600/175341]\n",
      "loss: 0.960369  [35200/175341]\n",
      "loss: 0.689945  [36800/175341]\n",
      "loss: 0.285547  [38400/175341]\n",
      "loss: 0.726464  [40000/175341]\n",
      "loss: 0.668953  [41600/175341]\n",
      "loss: 0.388377  [43200/175341]\n",
      "loss: 0.163066  [44800/175341]\n",
      "loss: 0.317819  [46400/175341]\n",
      "loss: 0.616677  [48000/175341]\n",
      "loss: 0.402880  [49600/175341]\n",
      "loss: 0.470103  [51200/175341]\n",
      "loss: 0.568975  [52800/175341]\n",
      "loss: 0.445528  [54400/175341]\n",
      "loss: 0.506824  [56000/175341]\n",
      "loss: 0.376662  [57600/175341]\n",
      "loss: 0.198603  [59200/175341]\n",
      "loss: 0.965950  [60800/175341]\n",
      "loss: 0.289616  [62400/175341]\n",
      "loss: 0.232802  [64000/175341]\n",
      "loss: 0.569586  [65600/175341]\n",
      "loss: 0.489461  [67200/175341]\n",
      "loss: 0.704343  [68800/175341]\n",
      "loss: 0.692942  [70400/175341]\n",
      "loss: 0.408931  [72000/175341]\n",
      "loss: 0.567513  [73600/175341]\n",
      "loss: 0.502930  [75200/175341]\n",
      "loss: 0.458549  [76800/175341]\n",
      "loss: 0.467546  [78400/175341]\n",
      "loss: 0.412961  [80000/175341]\n",
      "loss: 0.386047  [81600/175341]\n",
      "loss: 0.588263  [83200/175341]\n",
      "loss: 0.422070  [84800/175341]\n",
      "loss: 0.104614  [86400/175341]\n",
      "loss: 0.207081  [88000/175341]\n",
      "loss: 0.735477  [89600/175341]\n",
      "loss: 0.474077  [91200/175341]\n",
      "loss: 0.750151  [92800/175341]\n",
      "loss: 0.861635  [94400/175341]\n",
      "loss: 0.572618  [96000/175341]\n",
      "loss: 0.351182  [97600/175341]\n",
      "loss: 0.381955  [99200/175341]\n",
      "loss: 0.217228  [100800/175341]\n",
      "loss: 0.685260  [102400/175341]\n",
      "loss: 0.417588  [104000/175341]\n",
      "loss: 0.330705  [105600/175341]\n",
      "loss: 0.190017  [107200/175341]\n",
      "loss: 0.502343  [108800/175341]\n",
      "loss: 0.468463  [110400/175341]\n",
      "loss: 0.549477  [112000/175341]\n",
      "loss: 0.237784  [113600/175341]\n",
      "loss: 0.743648  [115200/175341]\n",
      "loss: 0.354062  [116800/175341]\n",
      "loss: 0.339653  [118400/175341]\n",
      "loss: 0.864274  [120000/175341]\n",
      "loss: 0.822930  [121600/175341]\n",
      "loss: 0.237913  [123200/175341]\n",
      "loss: 0.409998  [124800/175341]\n",
      "loss: 0.144625  [126400/175341]\n",
      "loss: 0.477355  [128000/175341]\n",
      "loss: 0.237683  [129600/175341]\n",
      "loss: 0.536737  [131200/175341]\n",
      "loss: 0.300860  [132800/175341]\n",
      "loss: 0.389175  [134400/175341]\n",
      "loss: 0.569773  [136000/175341]\n",
      "loss: 0.453387  [137600/175341]\n",
      "loss: 0.776610  [139200/175341]\n",
      "loss: 0.407975  [140800/175341]\n",
      "loss: 0.767654  [142400/175341]\n",
      "loss: 0.460280  [144000/175341]\n",
      "loss: 0.260105  [145600/175341]\n",
      "loss: 0.192126  [147200/175341]\n",
      "loss: 0.291866  [148800/175341]\n",
      "loss: 0.735462  [150400/175341]\n",
      "loss: 0.714248  [152000/175341]\n",
      "loss: 0.393682  [153600/175341]\n",
      "loss: 0.743547  [155200/175341]\n",
      "loss: 0.956133  [156800/175341]\n",
      "loss: 0.442075  [158400/175341]\n",
      "loss: 0.344180  [160000/175341]\n",
      "loss: 0.562024  [161600/175341]\n",
      "loss: 0.441748  [163200/175341]\n",
      "loss: 0.624161  [164800/175341]\n",
      "loss: 0.586962  [166400/175341]\n",
      "loss: 0.240404  [168000/175341]\n",
      "loss: 0.392383  [169600/175341]\n",
      "loss: 0.299017  [171200/175341]\n",
      "loss: 0.434760  [172800/175341]\n",
      "loss: 0.401685  [174400/175341]\n",
      "Train Accuracy: 80.3777%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.596497, F1-score: 75.85%, Macro_F1-Score:  39.54%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.429238  [    0/175341]\n",
      "loss: 0.053681  [ 1600/175341]\n",
      "loss: 0.401731  [ 3200/175341]\n",
      "loss: 0.604051  [ 4800/175341]\n",
      "loss: 0.258079  [ 6400/175341]\n",
      "loss: 0.391239  [ 8000/175341]\n",
      "loss: 0.474069  [ 9600/175341]\n",
      "loss: 0.351892  [11200/175341]\n",
      "loss: 0.776802  [12800/175341]\n",
      "loss: 0.103718  [14400/175341]\n",
      "loss: 0.195456  [16000/175341]\n",
      "loss: 0.796177  [17600/175341]\n",
      "loss: 0.776460  [19200/175341]\n",
      "loss: 0.708148  [20800/175341]\n",
      "loss: 0.984595  [22400/175341]\n",
      "loss: 0.522229  [24000/175341]\n",
      "loss: 0.882409  [25600/175341]\n",
      "loss: 1.048985  [27200/175341]\n",
      "loss: 0.577267  [28800/175341]\n",
      "loss: 0.364365  [30400/175341]\n",
      "loss: 1.139019  [32000/175341]\n",
      "loss: 0.299085  [33600/175341]\n",
      "loss: 0.713719  [35200/175341]\n",
      "loss: 0.628072  [36800/175341]\n",
      "loss: 0.470990  [38400/175341]\n",
      "loss: 0.478393  [40000/175341]\n",
      "loss: 0.619225  [41600/175341]\n",
      "loss: 0.642388  [43200/175341]\n",
      "loss: 0.324502  [44800/175341]\n",
      "loss: 0.750105  [46400/175341]\n",
      "loss: 0.682269  [48000/175341]\n",
      "loss: 0.550787  [49600/175341]\n",
      "loss: 0.517268  [51200/175341]\n",
      "loss: 0.550126  [52800/175341]\n",
      "loss: 0.174581  [54400/175341]\n",
      "loss: 0.601158  [56000/175341]\n",
      "loss: 0.650320  [57600/175341]\n",
      "loss: 0.314810  [59200/175341]\n",
      "loss: 0.292529  [60800/175341]\n",
      "loss: 0.623472  [62400/175341]\n",
      "loss: 0.149396  [64000/175341]\n",
      "loss: 0.610611  [65600/175341]\n",
      "loss: 0.442161  [67200/175341]\n",
      "loss: 0.947522  [68800/175341]\n",
      "loss: 0.460847  [70400/175341]\n",
      "loss: 0.276263  [72000/175341]\n",
      "loss: 0.484553  [73600/175341]\n",
      "loss: 0.633748  [75200/175341]\n",
      "loss: 0.312541  [76800/175341]\n",
      "loss: 0.244158  [78400/175341]\n",
      "loss: 0.571542  [80000/175341]\n",
      "loss: 0.061395  [81600/175341]\n",
      "loss: 0.267640  [83200/175341]\n",
      "loss: 0.295503  [84800/175341]\n",
      "loss: 0.398947  [86400/175341]\n",
      "loss: 0.616628  [88000/175341]\n",
      "loss: 1.059241  [89600/175341]\n",
      "loss: 0.626263  [91200/175341]\n",
      "loss: 0.524233  [92800/175341]\n",
      "loss: 0.633439  [94400/175341]\n",
      "loss: 0.890402  [96000/175341]\n",
      "loss: 0.202444  [97600/175341]\n",
      "loss: 0.672712  [99200/175341]\n",
      "loss: 0.526327  [100800/175341]\n",
      "loss: 0.486076  [102400/175341]\n",
      "loss: 0.413122  [104000/175341]\n",
      "loss: 0.402081  [105600/175341]\n",
      "loss: 0.390683  [107200/175341]\n",
      "loss: 0.392795  [108800/175341]\n",
      "loss: 0.707477  [110400/175341]\n",
      "loss: 0.214929  [112000/175341]\n",
      "loss: 0.917633  [113600/175341]\n",
      "loss: 0.323315  [115200/175341]\n",
      "loss: 0.446025  [116800/175341]\n",
      "loss: 0.467525  [118400/175341]\n",
      "loss: 0.912249  [120000/175341]\n",
      "loss: 1.082815  [121600/175341]\n",
      "loss: 0.375115  [123200/175341]\n",
      "loss: 0.751018  [124800/175341]\n",
      "loss: 0.711047  [126400/175341]\n",
      "loss: 0.512517  [128000/175341]\n",
      "loss: 0.315535  [129600/175341]\n",
      "loss: 0.409826  [131200/175341]\n",
      "loss: 0.431910  [132800/175341]\n",
      "loss: 0.637770  [134400/175341]\n",
      "loss: 0.179278  [136000/175341]\n",
      "loss: 0.753060  [137600/175341]\n",
      "loss: 0.814154  [139200/175341]\n",
      "loss: 0.453156  [140800/175341]\n",
      "loss: 0.280453  [142400/175341]\n",
      "loss: 0.558148  [144000/175341]\n",
      "loss: 0.270037  [145600/175341]\n",
      "loss: 0.348518  [147200/175341]\n",
      "loss: 0.767968  [148800/175341]\n",
      "loss: 0.665231  [150400/175341]\n",
      "loss: 0.313473  [152000/175341]\n",
      "loss: 1.182899  [153600/175341]\n",
      "loss: 0.521490  [155200/175341]\n",
      "loss: 0.192218  [156800/175341]\n",
      "loss: 0.109320  [158400/175341]\n",
      "loss: 0.360136  [160000/175341]\n",
      "loss: 0.129020  [161600/175341]\n",
      "loss: 0.329805  [163200/175341]\n",
      "loss: 0.679171  [164800/175341]\n",
      "loss: 0.482052  [166400/175341]\n",
      "loss: 0.507570  [168000/175341]\n",
      "loss: 0.251729  [169600/175341]\n",
      "loss: 0.475923  [171200/175341]\n",
      "loss: 0.692388  [172800/175341]\n",
      "loss: 0.665476  [174400/175341]\n",
      "Train Accuracy: 80.3885%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.636288, F1-score: 74.52%, Macro_F1-Score:  37.76%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.684622  [    0/175341]\n",
      "loss: 0.098505  [ 1600/175341]\n",
      "loss: 0.473590  [ 3200/175341]\n",
      "loss: 0.740979  [ 4800/175341]\n",
      "loss: 0.419464  [ 6400/175341]\n",
      "loss: 0.448372  [ 8000/175341]\n",
      "loss: 0.266386  [ 9600/175341]\n",
      "loss: 0.247286  [11200/175341]\n",
      "loss: 0.365758  [12800/175341]\n",
      "loss: 0.401699  [14400/175341]\n",
      "loss: 0.602507  [16000/175341]\n",
      "loss: 0.360654  [17600/175341]\n",
      "loss: 0.495803  [19200/175341]\n",
      "loss: 0.656587  [20800/175341]\n",
      "loss: 0.614719  [22400/175341]\n",
      "loss: 0.299044  [24000/175341]\n",
      "loss: 0.336330  [25600/175341]\n",
      "loss: 0.769056  [27200/175341]\n",
      "loss: 0.815962  [28800/175341]\n",
      "loss: 0.414475  [30400/175341]\n",
      "loss: 0.621940  [32000/175341]\n",
      "loss: 0.382062  [33600/175341]\n",
      "loss: 0.447220  [35200/175341]\n",
      "loss: 0.739639  [36800/175341]\n",
      "loss: 0.332946  [38400/175341]\n",
      "loss: 0.717410  [40000/175341]\n",
      "loss: 1.120867  [41600/175341]\n",
      "loss: 0.487466  [43200/175341]\n",
      "loss: 0.542744  [44800/175341]\n",
      "loss: 0.550874  [46400/175341]\n",
      "loss: 0.418725  [48000/175341]\n",
      "loss: 0.716799  [49600/175341]\n",
      "loss: 0.587028  [51200/175341]\n",
      "loss: 0.551643  [52800/175341]\n",
      "loss: 0.487349  [54400/175341]\n",
      "loss: 0.320248  [56000/175341]\n",
      "loss: 0.498083  [57600/175341]\n",
      "loss: 0.617593  [59200/175341]\n",
      "loss: 0.501935  [60800/175341]\n",
      "loss: 1.142800  [62400/175341]\n",
      "loss: 0.328239  [64000/175341]\n",
      "loss: 0.531025  [65600/175341]\n",
      "loss: 0.671918  [67200/175341]\n",
      "loss: 0.306286  [68800/175341]\n",
      "loss: 0.336439  [70400/175341]\n",
      "loss: 0.302060  [72000/175341]\n",
      "loss: 0.463086  [73600/175341]\n",
      "loss: 0.874024  [75200/175341]\n",
      "loss: 0.547076  [76800/175341]\n",
      "loss: 0.449128  [78400/175341]\n",
      "loss: 0.931409  [80000/175341]\n",
      "loss: 0.622964  [81600/175341]\n",
      "loss: 0.833133  [83200/175341]\n",
      "loss: 0.372686  [84800/175341]\n",
      "loss: 0.500829  [86400/175341]\n",
      "loss: 0.529641  [88000/175341]\n",
      "loss: 0.465756  [89600/175341]\n",
      "loss: 0.401737  [91200/175341]\n",
      "loss: 0.168036  [92800/175341]\n",
      "loss: 0.399709  [94400/175341]\n",
      "loss: 0.515442  [96000/175341]\n",
      "loss: 0.641294  [97600/175341]\n",
      "loss: 0.431353  [99200/175341]\n",
      "loss: 0.692780  [100800/175341]\n",
      "loss: 0.263640  [102400/175341]\n",
      "loss: 0.349030  [104000/175341]\n",
      "loss: 0.643953  [105600/175341]\n",
      "loss: 0.458525  [107200/175341]\n",
      "loss: 0.442972  [108800/175341]\n",
      "loss: 0.749825  [110400/175341]\n",
      "loss: 0.225534  [112000/175341]\n",
      "loss: 0.274458  [113600/175341]\n",
      "loss: 0.790552  [115200/175341]\n",
      "loss: 0.833210  [116800/175341]\n",
      "loss: 0.529085  [118400/175341]\n",
      "loss: 0.398188  [120000/175341]\n",
      "loss: 0.511145  [121600/175341]\n",
      "loss: 0.460489  [123200/175341]\n",
      "loss: 0.649927  [124800/175341]\n",
      "loss: 0.893225  [126400/175341]\n",
      "loss: 0.324948  [128000/175341]\n",
      "loss: 0.481684  [129600/175341]\n",
      "loss: 0.373766  [131200/175341]\n",
      "loss: 0.314212  [132800/175341]\n",
      "loss: 0.825185  [134400/175341]\n",
      "loss: 0.454720  [136000/175341]\n",
      "loss: 0.944294  [137600/175341]\n",
      "loss: 0.736687  [139200/175341]\n",
      "loss: 0.210837  [140800/175341]\n",
      "loss: 0.358227  [142400/175341]\n",
      "loss: 0.653207  [144000/175341]\n",
      "loss: 0.786409  [145600/175341]\n",
      "loss: 0.656742  [147200/175341]\n",
      "loss: 0.633327  [148800/175341]\n",
      "loss: 1.137091  [150400/175341]\n",
      "loss: 0.763846  [152000/175341]\n",
      "loss: 0.763689  [153600/175341]\n",
      "loss: 0.455965  [155200/175341]\n",
      "loss: 0.580520  [156800/175341]\n",
      "loss: 0.175349  [158400/175341]\n",
      "loss: 0.345817  [160000/175341]\n",
      "loss: 0.313689  [161600/175341]\n",
      "loss: 0.558138  [163200/175341]\n",
      "loss: 0.451381  [164800/175341]\n",
      "loss: 0.700209  [166400/175341]\n",
      "loss: 0.454198  [168000/175341]\n",
      "loss: 0.456343  [169600/175341]\n",
      "loss: 0.854791  [171200/175341]\n",
      "loss: 0.821925  [172800/175341]\n",
      "loss: 0.337779  [174400/175341]\n",
      "Train Accuracy: 80.3925%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.595762, F1-score: 75.20%, Macro_F1-Score:  39.45%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.459284  [    0/175341]\n",
      "loss: 0.194244  [ 1600/175341]\n",
      "loss: 0.365589  [ 3200/175341]\n",
      "loss: 0.267365  [ 4800/175341]\n",
      "loss: 0.775447  [ 6400/175341]\n",
      "loss: 0.298313  [ 8000/175341]\n",
      "loss: 0.736238  [ 9600/175341]\n",
      "loss: 0.502300  [11200/175341]\n",
      "loss: 0.597919  [12800/175341]\n",
      "loss: 0.223039  [14400/175341]\n",
      "loss: 0.385608  [16000/175341]\n",
      "loss: 0.125151  [17600/175341]\n",
      "loss: 0.737977  [19200/175341]\n",
      "loss: 0.444895  [20800/175341]\n",
      "loss: 0.518316  [22400/175341]\n",
      "loss: 0.368270  [24000/175341]\n",
      "loss: 0.782303  [25600/175341]\n",
      "loss: 0.499827  [27200/175341]\n",
      "loss: 0.324982  [28800/175341]\n",
      "loss: 0.729104  [30400/175341]\n",
      "loss: 0.293284  [32000/175341]\n",
      "loss: 0.437564  [33600/175341]\n",
      "loss: 0.683633  [35200/175341]\n",
      "loss: 0.448705  [36800/175341]\n",
      "loss: 0.811716  [38400/175341]\n",
      "loss: 0.204425  [40000/175341]\n",
      "loss: 0.456974  [41600/175341]\n",
      "loss: 0.461786  [43200/175341]\n",
      "loss: 0.244302  [44800/175341]\n",
      "loss: 0.402444  [46400/175341]\n",
      "loss: 0.336685  [48000/175341]\n",
      "loss: 0.422789  [49600/175341]\n",
      "loss: 0.270202  [51200/175341]\n",
      "loss: 0.541822  [52800/175341]\n",
      "loss: 0.323579  [54400/175341]\n",
      "loss: 0.430698  [56000/175341]\n",
      "loss: 0.693974  [57600/175341]\n",
      "loss: 0.815489  [59200/175341]\n",
      "loss: 0.199514  [60800/175341]\n",
      "loss: 0.565618  [62400/175341]\n",
      "loss: 0.977507  [64000/175341]\n",
      "loss: 0.738315  [65600/175341]\n",
      "loss: 0.281438  [67200/175341]\n",
      "loss: 0.675657  [68800/175341]\n",
      "loss: 0.575759  [70400/175341]\n",
      "loss: 0.816109  [72000/175341]\n",
      "loss: 0.767374  [73600/175341]\n",
      "loss: 0.316971  [75200/175341]\n",
      "loss: 0.186194  [76800/175341]\n",
      "loss: 0.485781  [78400/175341]\n",
      "loss: 0.621077  [80000/175341]\n",
      "loss: 0.164133  [81600/175341]\n",
      "loss: 0.369262  [83200/175341]\n",
      "loss: 0.256503  [84800/175341]\n",
      "loss: 0.610192  [86400/175341]\n",
      "loss: 0.406361  [88000/175341]\n",
      "loss: 0.333048  [89600/175341]\n",
      "loss: 0.270908  [91200/175341]\n",
      "loss: 0.695890  [92800/175341]\n",
      "loss: 0.978526  [94400/175341]\n",
      "loss: 0.224231  [96000/175341]\n",
      "loss: 0.733858  [97600/175341]\n",
      "loss: 0.497419  [99200/175341]\n",
      "loss: 0.720878  [100800/175341]\n",
      "loss: 1.019884  [102400/175341]\n",
      "loss: 0.526870  [104000/175341]\n",
      "loss: 0.524776  [105600/175341]\n",
      "loss: 0.680386  [107200/175341]\n",
      "loss: 0.335968  [108800/175341]\n",
      "loss: 0.654402  [110400/175341]\n",
      "loss: 0.589940  [112000/175341]\n",
      "loss: 0.734368  [113600/175341]\n",
      "loss: 0.361705  [115200/175341]\n",
      "loss: 0.586336  [116800/175341]\n",
      "loss: 0.405148  [118400/175341]\n",
      "loss: 0.506591  [120000/175341]\n",
      "loss: 0.342143  [121600/175341]\n",
      "loss: 0.739490  [123200/175341]\n",
      "loss: 0.194116  [124800/175341]\n",
      "loss: 0.283762  [126400/175341]\n",
      "loss: 0.236713  [128000/175341]\n",
      "loss: 0.326132  [129600/175341]\n",
      "loss: 0.229803  [131200/175341]\n",
      "loss: 0.458286  [132800/175341]\n",
      "loss: 0.694089  [134400/175341]\n",
      "loss: 0.615162  [136000/175341]\n",
      "loss: 0.325422  [137600/175341]\n",
      "loss: 0.327768  [139200/175341]\n",
      "loss: 0.374197  [140800/175341]\n",
      "loss: 0.231169  [142400/175341]\n",
      "loss: 0.699786  [144000/175341]\n",
      "loss: 0.920102  [145600/175341]\n",
      "loss: 0.410106  [147200/175341]\n",
      "loss: 0.406174  [148800/175341]\n",
      "loss: 0.418375  [150400/175341]\n",
      "loss: 0.292061  [152000/175341]\n",
      "loss: 0.311653  [153600/175341]\n",
      "loss: 0.464408  [155200/175341]\n",
      "loss: 0.353386  [156800/175341]\n",
      "loss: 0.074100  [158400/175341]\n",
      "loss: 0.203358  [160000/175341]\n",
      "loss: 0.640291  [161600/175341]\n",
      "loss: 0.494457  [163200/175341]\n",
      "loss: 0.544702  [164800/175341]\n",
      "loss: 0.521079  [166400/175341]\n",
      "loss: 0.361427  [168000/175341]\n",
      "loss: 0.415620  [169600/175341]\n",
      "loss: 0.624734  [171200/175341]\n",
      "loss: 0.949944  [172800/175341]\n",
      "loss: 0.504613  [174400/175341]\n",
      "Train Accuracy: 80.4290%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.636928, F1-score: 74.09%, Macro_F1-Score:  38.69%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.758001  [    0/175341]\n",
      "loss: 0.324679  [ 1600/175341]\n",
      "loss: 0.533702  [ 3200/175341]\n",
      "loss: 1.134318  [ 4800/175341]\n",
      "loss: 0.325064  [ 6400/175341]\n",
      "loss: 0.489075  [ 8000/175341]\n",
      "loss: 0.451991  [ 9600/175341]\n",
      "loss: 1.134435  [11200/175341]\n",
      "loss: 0.713045  [12800/175341]\n",
      "loss: 0.142622  [14400/175341]\n",
      "loss: 0.501990  [16000/175341]\n",
      "loss: 0.440705  [17600/175341]\n",
      "loss: 0.568802  [19200/175341]\n",
      "loss: 0.474386  [20800/175341]\n",
      "loss: 0.657451  [22400/175341]\n",
      "loss: 0.444946  [24000/175341]\n",
      "loss: 0.593911  [25600/175341]\n",
      "loss: 0.227694  [27200/175341]\n",
      "loss: 0.740807  [28800/175341]\n",
      "loss: 0.397482  [30400/175341]\n",
      "loss: 0.349991  [32000/175341]\n",
      "loss: 0.717273  [33600/175341]\n",
      "loss: 0.339659  [35200/175341]\n",
      "loss: 0.515111  [36800/175341]\n",
      "loss: 0.672530  [38400/175341]\n",
      "loss: 0.326202  [40000/175341]\n",
      "loss: 0.389449  [41600/175341]\n",
      "loss: 0.428853  [43200/175341]\n",
      "loss: 0.827639  [44800/175341]\n",
      "loss: 0.879466  [46400/175341]\n",
      "loss: 0.611170  [48000/175341]\n",
      "loss: 0.814852  [49600/175341]\n",
      "loss: 0.260663  [51200/175341]\n",
      "loss: 0.368658  [52800/175341]\n",
      "loss: 0.393008  [54400/175341]\n",
      "loss: 0.521505  [56000/175341]\n",
      "loss: 0.336552  [57600/175341]\n",
      "loss: 0.245087  [59200/175341]\n",
      "loss: 0.450765  [60800/175341]\n",
      "loss: 0.507933  [62400/175341]\n",
      "loss: 0.508499  [64000/175341]\n",
      "loss: 0.973326  [65600/175341]\n",
      "loss: 0.678672  [67200/175341]\n",
      "loss: 0.176380  [68800/175341]\n",
      "loss: 0.669051  [70400/175341]\n",
      "loss: 0.150462  [72000/175341]\n",
      "loss: 0.610581  [73600/175341]\n",
      "loss: 0.219622  [75200/175341]\n",
      "loss: 0.487471  [76800/175341]\n",
      "loss: 0.143843  [78400/175341]\n",
      "loss: 0.251958  [80000/175341]\n",
      "loss: 0.338386  [81600/175341]\n",
      "loss: 0.255839  [83200/175341]\n",
      "loss: 0.228047  [84800/175341]\n",
      "loss: 0.520653  [86400/175341]\n",
      "loss: 0.563824  [88000/175341]\n",
      "loss: 0.349892  [89600/175341]\n",
      "loss: 0.132932  [91200/175341]\n",
      "loss: 0.359407  [92800/175341]\n",
      "loss: 0.841778  [94400/175341]\n",
      "loss: 0.795522  [96000/175341]\n",
      "loss: 0.701516  [97600/175341]\n",
      "loss: 0.884275  [99200/175341]\n",
      "loss: 0.687910  [100800/175341]\n",
      "loss: 0.626480  [102400/175341]\n",
      "loss: 0.319147  [104000/175341]\n",
      "loss: 0.513809  [105600/175341]\n",
      "loss: 0.517501  [107200/175341]\n",
      "loss: 0.630678  [108800/175341]\n",
      "loss: 0.539846  [110400/175341]\n",
      "loss: 0.552010  [112000/175341]\n",
      "loss: 0.350629  [113600/175341]\n",
      "loss: 0.395949  [115200/175341]\n",
      "loss: 0.495693  [116800/175341]\n",
      "loss: 0.636030  [118400/175341]\n",
      "loss: 0.203954  [120000/175341]\n",
      "loss: 0.358344  [121600/175341]\n",
      "loss: 0.537585  [123200/175341]\n",
      "loss: 0.336173  [124800/175341]\n",
      "loss: 0.287665  [126400/175341]\n",
      "loss: 0.361498  [128000/175341]\n",
      "loss: 0.450984  [129600/175341]\n",
      "loss: 0.286433  [131200/175341]\n",
      "loss: 0.517817  [132800/175341]\n",
      "loss: 0.566561  [134400/175341]\n",
      "loss: 0.282429  [136000/175341]\n",
      "loss: 0.544725  [137600/175341]\n",
      "loss: 0.186537  [139200/175341]\n",
      "loss: 0.409487  [140800/175341]\n",
      "loss: 0.330517  [142400/175341]\n",
      "loss: 0.679796  [144000/175341]\n",
      "loss: 0.640305  [145600/175341]\n",
      "loss: 0.930286  [147200/175341]\n",
      "loss: 0.533672  [148800/175341]\n",
      "loss: 0.474132  [150400/175341]\n",
      "loss: 0.169373  [152000/175341]\n",
      "loss: 0.841794  [153600/175341]\n",
      "loss: 0.487536  [155200/175341]\n",
      "loss: 0.405103  [156800/175341]\n",
      "loss: 0.480470  [158400/175341]\n",
      "loss: 0.157023  [160000/175341]\n",
      "loss: 0.866002  [161600/175341]\n",
      "loss: 0.461066  [163200/175341]\n",
      "loss: 0.411638  [164800/175341]\n",
      "loss: 0.309060  [166400/175341]\n",
      "loss: 0.770156  [168000/175341]\n",
      "loss: 0.477499  [169600/175341]\n",
      "loss: 0.320735  [171200/175341]\n",
      "loss: 0.273840  [172800/175341]\n",
      "loss: 0.650203  [174400/175341]\n",
      "Train Accuracy: 80.4621%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.581733, F1-score: 75.87%, Macro_F1-Score:  39.78%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.272923  [    0/175341]\n",
      "loss: 0.333500  [ 1600/175341]\n",
      "loss: 0.415540  [ 3200/175341]\n",
      "loss: 0.463882  [ 4800/175341]\n",
      "loss: 0.319130  [ 6400/175341]\n",
      "loss: 0.784191  [ 8000/175341]\n",
      "loss: 0.313951  [ 9600/175341]\n",
      "loss: 0.625432  [11200/175341]\n",
      "loss: 0.636210  [12800/175341]\n",
      "loss: 0.235940  [14400/175341]\n",
      "loss: 0.274511  [16000/175341]\n",
      "loss: 0.340401  [17600/175341]\n",
      "loss: 0.617040  [19200/175341]\n",
      "loss: 0.770716  [20800/175341]\n",
      "loss: 0.473149  [22400/175341]\n",
      "loss: 0.267608  [24000/175341]\n",
      "loss: 0.437334  [25600/175341]\n",
      "loss: 0.617774  [27200/175341]\n",
      "loss: 0.215979  [28800/175341]\n",
      "loss: 0.630640  [30400/175341]\n",
      "loss: 0.761457  [32000/175341]\n",
      "loss: 0.421898  [33600/175341]\n",
      "loss: 0.442101  [35200/175341]\n",
      "loss: 0.551601  [36800/175341]\n",
      "loss: 0.741613  [38400/175341]\n",
      "loss: 0.446631  [40000/175341]\n",
      "loss: 0.603062  [41600/175341]\n",
      "loss: 0.334749  [43200/175341]\n",
      "loss: 0.321033  [44800/175341]\n",
      "loss: 0.737326  [46400/175341]\n",
      "loss: 0.370328  [48000/175341]\n",
      "loss: 0.645580  [49600/175341]\n",
      "loss: 0.795782  [51200/175341]\n",
      "loss: 0.435033  [52800/175341]\n",
      "loss: 0.216301  [54400/175341]\n",
      "loss: 0.402938  [56000/175341]\n",
      "loss: 0.461369  [57600/175341]\n",
      "loss: 1.000244  [59200/175341]\n",
      "loss: 0.343523  [60800/175341]\n",
      "loss: 0.309999  [62400/175341]\n",
      "loss: 0.170851  [64000/175341]\n",
      "loss: 0.426570  [65600/175341]\n",
      "loss: 0.603189  [67200/175341]\n",
      "loss: 0.611986  [68800/175341]\n",
      "loss: 0.358680  [70400/175341]\n",
      "loss: 0.319762  [72000/175341]\n",
      "loss: 0.710517  [73600/175341]\n",
      "loss: 0.157996  [75200/175341]\n",
      "loss: 0.929046  [76800/175341]\n",
      "loss: 0.882241  [78400/175341]\n",
      "loss: 0.646760  [80000/175341]\n",
      "loss: 0.487607  [81600/175341]\n",
      "loss: 0.880058  [83200/175341]\n",
      "loss: 0.631283  [84800/175341]\n",
      "loss: 0.738478  [86400/175341]\n",
      "loss: 0.436762  [88000/175341]\n",
      "loss: 0.580913  [89600/175341]\n",
      "loss: 0.257680  [91200/175341]\n",
      "loss: 0.370671  [92800/175341]\n",
      "loss: 0.152743  [94400/175341]\n",
      "loss: 0.715512  [96000/175341]\n",
      "loss: 0.359329  [97600/175341]\n",
      "loss: 0.428002  [99200/175341]\n",
      "loss: 0.899244  [100800/175341]\n",
      "loss: 0.660860  [102400/175341]\n",
      "loss: 0.887126  [104000/175341]\n",
      "loss: 0.756450  [105600/175341]\n",
      "loss: 0.316391  [107200/175341]\n",
      "loss: 0.410130  [108800/175341]\n",
      "loss: 0.385798  [110400/175341]\n",
      "loss: 0.544760  [112000/175341]\n",
      "loss: 0.387029  [113600/175341]\n",
      "loss: 0.527477  [115200/175341]\n",
      "loss: 0.648007  [116800/175341]\n",
      "loss: 0.424227  [118400/175341]\n",
      "loss: 0.508107  [120000/175341]\n",
      "loss: 0.620324  [121600/175341]\n",
      "loss: 0.256771  [123200/175341]\n",
      "loss: 0.632831  [124800/175341]\n",
      "loss: 0.527234  [126400/175341]\n",
      "loss: 0.489035  [128000/175341]\n",
      "loss: 0.795779  [129600/175341]\n",
      "loss: 0.537520  [131200/175341]\n",
      "loss: 0.337496  [132800/175341]\n",
      "loss: 0.681908  [134400/175341]\n",
      "loss: 0.262453  [136000/175341]\n",
      "loss: 0.440754  [137600/175341]\n",
      "loss: 0.322446  [139200/175341]\n",
      "loss: 0.803618  [140800/175341]\n",
      "loss: 0.857357  [142400/175341]\n",
      "loss: 0.374768  [144000/175341]\n",
      "loss: 0.173839  [145600/175341]\n",
      "loss: 0.526146  [147200/175341]\n",
      "loss: 0.561814  [148800/175341]\n",
      "loss: 0.745681  [150400/175341]\n",
      "loss: 0.569786  [152000/175341]\n",
      "loss: 0.581036  [153600/175341]\n",
      "loss: 0.342925  [155200/175341]\n",
      "loss: 0.427547  [156800/175341]\n",
      "loss: 0.273447  [158400/175341]\n",
      "loss: 0.328152  [160000/175341]\n",
      "loss: 0.495477  [161600/175341]\n",
      "loss: 0.069743  [163200/175341]\n",
      "loss: 0.633672  [164800/175341]\n",
      "loss: 0.695692  [166400/175341]\n",
      "loss: 0.522280  [168000/175341]\n",
      "loss: 0.352100  [169600/175341]\n",
      "loss: 0.638499  [171200/175341]\n",
      "loss: 0.368857  [172800/175341]\n",
      "loss: 0.320410  [174400/175341]\n",
      "Train Accuracy: 80.5088%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.586421, F1-score: 75.89%, Macro_F1-Score:  39.78%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.567447  [    0/175341]\n",
      "loss: 0.568255  [ 1600/175341]\n",
      "loss: 0.530618  [ 3200/175341]\n",
      "loss: 0.592279  [ 4800/175341]\n",
      "loss: 0.622394  [ 6400/175341]\n",
      "loss: 0.601507  [ 8000/175341]\n",
      "loss: 0.578754  [ 9600/175341]\n",
      "loss: 0.087221  [11200/175341]\n",
      "loss: 0.407974  [12800/175341]\n",
      "loss: 0.925214  [14400/175341]\n",
      "loss: 0.248255  [16000/175341]\n",
      "loss: 0.714269  [17600/175341]\n",
      "loss: 0.057868  [19200/175341]\n",
      "loss: 0.659072  [20800/175341]\n",
      "loss: 0.876665  [22400/175341]\n",
      "loss: 0.658518  [24000/175341]\n",
      "loss: 0.448610  [25600/175341]\n",
      "loss: 0.612154  [27200/175341]\n",
      "loss: 0.596064  [28800/175341]\n",
      "loss: 0.702248  [30400/175341]\n",
      "loss: 1.114493  [32000/175341]\n",
      "loss: 0.465929  [33600/175341]\n",
      "loss: 0.202084  [35200/175341]\n",
      "loss: 0.385752  [36800/175341]\n",
      "loss: 0.344637  [38400/175341]\n",
      "loss: 0.423790  [40000/175341]\n",
      "loss: 0.377967  [41600/175341]\n",
      "loss: 0.103104  [43200/175341]\n",
      "loss: 0.316811  [44800/175341]\n",
      "loss: 0.471163  [46400/175341]\n",
      "loss: 0.661751  [48000/175341]\n",
      "loss: 0.675672  [49600/175341]\n",
      "loss: 0.776265  [51200/175341]\n",
      "loss: 0.638346  [52800/175341]\n",
      "loss: 0.676887  [54400/175341]\n",
      "loss: 0.585901  [56000/175341]\n",
      "loss: 0.032659  [57600/175341]\n",
      "loss: 0.328122  [59200/175341]\n",
      "loss: 0.520498  [60800/175341]\n",
      "loss: 0.320588  [62400/175341]\n",
      "loss: 0.307805  [64000/175341]\n",
      "loss: 0.288605  [65600/175341]\n",
      "loss: 0.471267  [67200/175341]\n",
      "loss: 0.133869  [68800/175341]\n",
      "loss: 0.531318  [70400/175341]\n",
      "loss: 0.400922  [72000/175341]\n",
      "loss: 0.182269  [73600/175341]\n",
      "loss: 0.540599  [75200/175341]\n",
      "loss: 0.637316  [76800/175341]\n",
      "loss: 0.625263  [78400/175341]\n",
      "loss: 0.470536  [80000/175341]\n",
      "loss: 0.637380  [81600/175341]\n",
      "loss: 0.718202  [83200/175341]\n",
      "loss: 0.536305  [84800/175341]\n",
      "loss: 0.468228  [86400/175341]\n",
      "loss: 0.229157  [88000/175341]\n",
      "loss: 0.606102  [89600/175341]\n",
      "loss: 0.664021  [91200/175341]\n",
      "loss: 0.107678  [92800/175341]\n",
      "loss: 0.269504  [94400/175341]\n",
      "loss: 0.667900  [96000/175341]\n",
      "loss: 0.555428  [97600/175341]\n",
      "loss: 0.708245  [99200/175341]\n",
      "loss: 0.176118  [100800/175341]\n",
      "loss: 0.483541  [102400/175341]\n",
      "loss: 0.657362  [104000/175341]\n",
      "loss: 0.179792  [105600/175341]\n",
      "loss: 0.689314  [107200/175341]\n",
      "loss: 0.694872  [108800/175341]\n",
      "loss: 0.545565  [110400/175341]\n",
      "loss: 0.592326  [112000/175341]\n",
      "loss: 0.510472  [113600/175341]\n",
      "loss: 0.637883  [115200/175341]\n",
      "loss: 0.555024  [116800/175341]\n",
      "loss: 0.341252  [118400/175341]\n",
      "loss: 1.169845  [120000/175341]\n",
      "loss: 0.779473  [121600/175341]\n",
      "loss: 0.575184  [123200/175341]\n",
      "loss: 0.715031  [124800/175341]\n",
      "loss: 0.388537  [126400/175341]\n",
      "loss: 0.566951  [128000/175341]\n",
      "loss: 0.716285  [129600/175341]\n",
      "loss: 0.339035  [131200/175341]\n",
      "loss: 1.092508  [132800/175341]\n",
      "loss: 0.419280  [134400/175341]\n",
      "loss: 0.962493  [136000/175341]\n",
      "loss: 0.285255  [137600/175341]\n",
      "loss: 0.774382  [139200/175341]\n",
      "loss: 0.620296  [140800/175341]\n",
      "loss: 0.808939  [142400/175341]\n",
      "loss: 0.245943  [144000/175341]\n",
      "loss: 0.698482  [145600/175341]\n",
      "loss: 0.537542  [147200/175341]\n",
      "loss: 0.335495  [148800/175341]\n",
      "loss: 0.808474  [150400/175341]\n",
      "loss: 0.269971  [152000/175341]\n",
      "loss: 0.352117  [153600/175341]\n",
      "loss: 0.496890  [155200/175341]\n",
      "loss: 0.849752  [156800/175341]\n",
      "loss: 0.501071  [158400/175341]\n",
      "loss: 0.435335  [160000/175341]\n",
      "loss: 0.129793  [161600/175341]\n",
      "loss: 0.711491  [163200/175341]\n",
      "loss: 0.211180  [164800/175341]\n",
      "loss: 0.692800  [166400/175341]\n",
      "loss: 0.927587  [168000/175341]\n",
      "loss: 0.607705  [169600/175341]\n",
      "loss: 0.223618  [171200/175341]\n",
      "loss: 0.428292  [172800/175341]\n",
      "loss: 0.450711  [174400/175341]\n",
      "Train Accuracy: 80.5237%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.591221, F1-score: 75.72%, Macro_F1-Score:  39.92%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.591657  [    0/175341]\n",
      "loss: 0.206400  [ 1600/175341]\n",
      "loss: 0.933974  [ 3200/175341]\n",
      "loss: 0.534476  [ 4800/175341]\n",
      "loss: 0.485904  [ 6400/175341]\n",
      "loss: 0.145555  [ 8000/175341]\n",
      "loss: 0.331665  [ 9600/175341]\n",
      "loss: 0.806783  [11200/175341]\n",
      "loss: 0.590101  [12800/175341]\n",
      "loss: 0.546415  [14400/175341]\n",
      "loss: 0.492316  [16000/175341]\n",
      "loss: 0.730592  [17600/175341]\n",
      "loss: 0.274828  [19200/175341]\n",
      "loss: 0.337526  [20800/175341]\n",
      "loss: 0.221629  [22400/175341]\n",
      "loss: 0.292926  [24000/175341]\n",
      "loss: 0.190006  [25600/175341]\n",
      "loss: 0.577770  [27200/175341]\n",
      "loss: 0.557826  [28800/175341]\n",
      "loss: 0.420966  [30400/175341]\n",
      "loss: 0.277882  [32000/175341]\n",
      "loss: 0.485790  [33600/175341]\n",
      "loss: 0.740177  [35200/175341]\n",
      "loss: 0.648865  [36800/175341]\n",
      "loss: 0.150440  [38400/175341]\n",
      "loss: 0.784591  [40000/175341]\n",
      "loss: 0.314839  [41600/175341]\n",
      "loss: 0.310635  [43200/175341]\n",
      "loss: 0.619730  [44800/175341]\n",
      "loss: 0.713232  [46400/175341]\n",
      "loss: 0.457504  [48000/175341]\n",
      "loss: 0.487128  [49600/175341]\n",
      "loss: 0.371555  [51200/175341]\n",
      "loss: 0.708491  [52800/175341]\n",
      "loss: 0.580615  [54400/175341]\n",
      "loss: 0.523896  [56000/175341]\n",
      "loss: 0.378558  [57600/175341]\n",
      "loss: 0.451319  [59200/175341]\n",
      "loss: 0.224337  [60800/175341]\n",
      "loss: 0.451815  [62400/175341]\n",
      "loss: 0.394278  [64000/175341]\n",
      "loss: 0.519935  [65600/175341]\n",
      "loss: 0.839800  [67200/175341]\n",
      "loss: 0.754146  [68800/175341]\n",
      "loss: 0.534332  [70400/175341]\n",
      "loss: 0.554504  [72000/175341]\n",
      "loss: 0.309199  [73600/175341]\n",
      "loss: 0.705038  [75200/175341]\n",
      "loss: 0.389196  [76800/175341]\n",
      "loss: 0.770633  [78400/175341]\n",
      "loss: 0.200791  [80000/175341]\n",
      "loss: 0.492511  [81600/175341]\n",
      "loss: 0.645292  [83200/175341]\n",
      "loss: 0.197979  [84800/175341]\n",
      "loss: 0.327381  [86400/175341]\n",
      "loss: 0.854908  [88000/175341]\n",
      "loss: 0.346102  [89600/175341]\n",
      "loss: 0.620834  [91200/175341]\n",
      "loss: 0.928990  [92800/175341]\n",
      "loss: 0.473912  [94400/175341]\n",
      "loss: 0.290670  [96000/175341]\n",
      "loss: 0.222232  [97600/175341]\n",
      "loss: 0.390813  [99200/175341]\n",
      "loss: 0.385198  [100800/175341]\n",
      "loss: 0.399174  [102400/175341]\n",
      "loss: 0.580975  [104000/175341]\n",
      "loss: 0.556593  [105600/175341]\n",
      "loss: 0.317861  [107200/175341]\n",
      "loss: 0.587017  [108800/175341]\n",
      "loss: 0.253995  [110400/175341]\n",
      "loss: 0.962702  [112000/175341]\n",
      "loss: 0.546451  [113600/175341]\n",
      "loss: 0.488459  [115200/175341]\n",
      "loss: 0.186352  [116800/175341]\n",
      "loss: 0.594713  [118400/175341]\n",
      "loss: 0.322868  [120000/175341]\n",
      "loss: 0.599790  [121600/175341]\n",
      "loss: 0.434130  [123200/175341]\n",
      "loss: 0.711441  [124800/175341]\n",
      "loss: 0.479776  [126400/175341]\n",
      "loss: 0.448165  [128000/175341]\n",
      "loss: 0.568298  [129600/175341]\n",
      "loss: 0.727905  [131200/175341]\n",
      "loss: 0.527070  [132800/175341]\n",
      "loss: 0.695340  [134400/175341]\n",
      "loss: 0.193643  [136000/175341]\n",
      "loss: 0.540684  [137600/175341]\n",
      "loss: 0.732978  [139200/175341]\n",
      "loss: 0.686757  [140800/175341]\n",
      "loss: 0.239680  [142400/175341]\n",
      "loss: 0.612670  [144000/175341]\n",
      "loss: 0.694734  [145600/175341]\n",
      "loss: 0.719982  [147200/175341]\n",
      "loss: 0.497627  [148800/175341]\n",
      "loss: 1.156985  [150400/175341]\n",
      "loss: 0.418592  [152000/175341]\n",
      "loss: 0.123882  [153600/175341]\n",
      "loss: 0.560491  [155200/175341]\n",
      "loss: 0.221943  [156800/175341]\n",
      "loss: 0.065241  [158400/175341]\n",
      "loss: 0.306860  [160000/175341]\n",
      "loss: 0.471360  [161600/175341]\n",
      "loss: 0.714077  [163200/175341]\n",
      "loss: 0.361669  [164800/175341]\n",
      "loss: 0.508078  [166400/175341]\n",
      "loss: 0.408505  [168000/175341]\n",
      "loss: 0.429876  [169600/175341]\n",
      "loss: 0.555538  [171200/175341]\n",
      "loss: 0.658438  [172800/175341]\n",
      "loss: 0.707497  [174400/175341]\n",
      "Train Accuracy: 80.5248%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.592741, F1-score: 74.98%, Macro_F1-Score:  39.75%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.683919  [    0/175341]\n",
      "loss: 0.557442  [ 1600/175341]\n",
      "loss: 0.306286  [ 3200/175341]\n",
      "loss: 0.188341  [ 4800/175341]\n",
      "loss: 0.879577  [ 6400/175341]\n",
      "loss: 0.931200  [ 8000/175341]\n",
      "loss: 0.583456  [ 9600/175341]\n",
      "loss: 0.550812  [11200/175341]\n",
      "loss: 0.481005  [12800/175341]\n",
      "loss: 0.769852  [14400/175341]\n",
      "loss: 0.633857  [16000/175341]\n",
      "loss: 0.645605  [17600/175341]\n",
      "loss: 0.237757  [19200/175341]\n",
      "loss: 0.256913  [20800/175341]\n",
      "loss: 0.201808  [22400/175341]\n",
      "loss: 0.381495  [24000/175341]\n",
      "loss: 0.941066  [25600/175341]\n",
      "loss: 0.539817  [27200/175341]\n",
      "loss: 0.712937  [28800/175341]\n",
      "loss: 0.503460  [30400/175341]\n",
      "loss: 0.286439  [32000/175341]\n",
      "loss: 0.269352  [33600/175341]\n",
      "loss: 0.479409  [35200/175341]\n",
      "loss: 0.391955  [36800/175341]\n",
      "loss: 0.422496  [38400/175341]\n",
      "loss: 0.188877  [40000/175341]\n",
      "loss: 0.572115  [41600/175341]\n",
      "loss: 0.295658  [43200/175341]\n",
      "loss: 0.482388  [44800/175341]\n",
      "loss: 0.436911  [46400/175341]\n",
      "loss: 0.341606  [48000/175341]\n",
      "loss: 1.009285  [49600/175341]\n",
      "loss: 0.215260  [51200/175341]\n",
      "loss: 0.680246  [52800/175341]\n",
      "loss: 0.457233  [54400/175341]\n",
      "loss: 0.269291  [56000/175341]\n",
      "loss: 0.618638  [57600/175341]\n",
      "loss: 0.338407  [59200/175341]\n",
      "loss: 0.213706  [60800/175341]\n",
      "loss: 0.792987  [62400/175341]\n",
      "loss: 0.422040  [64000/175341]\n",
      "loss: 0.507992  [65600/175341]\n",
      "loss: 1.197023  [67200/175341]\n",
      "loss: 0.377436  [68800/175341]\n",
      "loss: 0.291731  [70400/175341]\n",
      "loss: 0.412591  [72000/175341]\n",
      "loss: 0.286026  [73600/175341]\n",
      "loss: 0.230130  [75200/175341]\n",
      "loss: 0.400452  [76800/175341]\n",
      "loss: 0.397651  [78400/175341]\n",
      "loss: 0.705930  [80000/175341]\n",
      "loss: 0.372503  [81600/175341]\n",
      "loss: 0.147055  [83200/175341]\n",
      "loss: 0.469968  [84800/175341]\n",
      "loss: 0.330406  [86400/175341]\n",
      "loss: 0.286113  [88000/175341]\n",
      "loss: 0.056675  [89600/175341]\n",
      "loss: 0.310964  [91200/175341]\n",
      "loss: 1.299971  [92800/175341]\n",
      "loss: 0.740603  [94400/175341]\n",
      "loss: 0.425827  [96000/175341]\n",
      "loss: 0.184004  [97600/175341]\n",
      "loss: 0.328390  [99200/175341]\n",
      "loss: 0.488759  [100800/175341]\n",
      "loss: 0.790290  [102400/175341]\n",
      "loss: 0.910280  [104000/175341]\n",
      "loss: 0.541029  [105600/175341]\n",
      "loss: 0.921992  [107200/175341]\n",
      "loss: 0.773653  [108800/175341]\n",
      "loss: 0.339364  [110400/175341]\n",
      "loss: 0.409548  [112000/175341]\n",
      "loss: 0.786371  [113600/175341]\n",
      "loss: 0.843990  [115200/175341]\n",
      "loss: 0.225270  [116800/175341]\n",
      "loss: 0.882906  [118400/175341]\n",
      "loss: 0.488745  [120000/175341]\n",
      "loss: 0.343632  [121600/175341]\n",
      "loss: 0.664580  [123200/175341]\n",
      "loss: 0.658881  [124800/175341]\n",
      "loss: 0.183890  [126400/175341]\n",
      "loss: 0.438434  [128000/175341]\n",
      "loss: 0.594448  [129600/175341]\n",
      "loss: 0.327742  [131200/175341]\n",
      "loss: 0.324012  [132800/175341]\n",
      "loss: 0.565249  [134400/175341]\n",
      "loss: 0.561800  [136000/175341]\n",
      "loss: 0.481613  [137600/175341]\n",
      "loss: 0.280370  [139200/175341]\n",
      "loss: 0.666723  [140800/175341]\n",
      "loss: 0.857098  [142400/175341]\n",
      "loss: 0.234262  [144000/175341]\n",
      "loss: 0.379273  [145600/175341]\n",
      "loss: 0.109380  [147200/175341]\n",
      "loss: 0.291360  [148800/175341]\n",
      "loss: 0.696810  [150400/175341]\n",
      "loss: 0.517673  [152000/175341]\n",
      "loss: 0.289237  [153600/175341]\n",
      "loss: 0.541370  [155200/175341]\n",
      "loss: 0.225793  [156800/175341]\n",
      "loss: 0.837044  [158400/175341]\n",
      "loss: 0.409879  [160000/175341]\n",
      "loss: 0.524499  [161600/175341]\n",
      "loss: 0.355333  [163200/175341]\n",
      "loss: 0.777737  [164800/175341]\n",
      "loss: 0.361196  [166400/175341]\n",
      "loss: 0.485472  [168000/175341]\n",
      "loss: 0.523388  [169600/175341]\n",
      "loss: 0.423004  [171200/175341]\n",
      "loss: 0.929755  [172800/175341]\n",
      "loss: 0.549523  [174400/175341]\n",
      "Train Accuracy: 80.5539%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.595545, F1-score: 75.64%, Macro_F1-Score:  39.57%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.563348  [    0/175341]\n",
      "loss: 0.431877  [ 1600/175341]\n",
      "loss: 0.498169  [ 3200/175341]\n",
      "loss: 0.332024  [ 4800/175341]\n",
      "loss: 0.424209  [ 6400/175341]\n",
      "loss: 0.583669  [ 8000/175341]\n",
      "loss: 0.327839  [ 9600/175341]\n",
      "loss: 0.136682  [11200/175341]\n",
      "loss: 0.638605  [12800/175341]\n",
      "loss: 0.575333  [14400/175341]\n",
      "loss: 0.403328  [16000/175341]\n",
      "loss: 0.722205  [17600/175341]\n",
      "loss: 0.449157  [19200/175341]\n",
      "loss: 0.536455  [20800/175341]\n",
      "loss: 0.297820  [22400/175341]\n",
      "loss: 0.462326  [24000/175341]\n",
      "loss: 0.076055  [25600/175341]\n",
      "loss: 0.578688  [27200/175341]\n",
      "loss: 0.847392  [28800/175341]\n",
      "loss: 0.160296  [30400/175341]\n",
      "loss: 0.835014  [32000/175341]\n",
      "loss: 0.577408  [33600/175341]\n",
      "loss: 0.434846  [35200/175341]\n",
      "loss: 0.259831  [36800/175341]\n",
      "loss: 0.343188  [38400/175341]\n",
      "loss: 0.286844  [40000/175341]\n",
      "loss: 0.265374  [41600/175341]\n",
      "loss: 0.987892  [43200/175341]\n",
      "loss: 0.538888  [44800/175341]\n",
      "loss: 0.425654  [46400/175341]\n",
      "loss: 0.627403  [48000/175341]\n",
      "loss: 0.252389  [49600/175341]\n",
      "loss: 0.670911  [51200/175341]\n",
      "loss: 0.291384  [52800/175341]\n",
      "loss: 0.501873  [54400/175341]\n",
      "loss: 0.299981  [56000/175341]\n",
      "loss: 0.161812  [57600/175341]\n",
      "loss: 0.199903  [59200/175341]\n",
      "loss: 0.560523  [60800/175341]\n",
      "loss: 0.575485  [62400/175341]\n",
      "loss: 0.753222  [64000/175341]\n",
      "loss: 0.363164  [65600/175341]\n",
      "loss: 0.534408  [67200/175341]\n",
      "loss: 0.471740  [68800/175341]\n",
      "loss: 0.243905  [70400/175341]\n",
      "loss: 0.435972  [72000/175341]\n",
      "loss: 0.442658  [73600/175341]\n",
      "loss: 0.471183  [75200/175341]\n",
      "loss: 0.693278  [76800/175341]\n",
      "loss: 0.539773  [78400/175341]\n",
      "loss: 0.430944  [80000/175341]\n",
      "loss: 0.260401  [81600/175341]\n",
      "loss: 0.643183  [83200/175341]\n",
      "loss: 0.334211  [84800/175341]\n",
      "loss: 0.546641  [86400/175341]\n",
      "loss: 0.604004  [88000/175341]\n",
      "loss: 0.287621  [89600/175341]\n",
      "loss: 0.333003  [91200/175341]\n",
      "loss: 0.339635  [92800/175341]\n",
      "loss: 0.437088  [94400/175341]\n",
      "loss: 0.463837  [96000/175341]\n",
      "loss: 0.371900  [97600/175341]\n",
      "loss: 0.410771  [99200/175341]\n",
      "loss: 0.687187  [100800/175341]\n",
      "loss: 0.225300  [102400/175341]\n",
      "loss: 0.271676  [104000/175341]\n",
      "loss: 0.399655  [105600/175341]\n",
      "loss: 0.499711  [107200/175341]\n",
      "loss: 0.408473  [108800/175341]\n",
      "loss: 0.511711  [110400/175341]\n",
      "loss: 0.650367  [112000/175341]\n",
      "loss: 0.318270  [113600/175341]\n",
      "loss: 0.533785  [115200/175341]\n",
      "loss: 0.616060  [116800/175341]\n",
      "loss: 0.518290  [118400/175341]\n",
      "loss: 0.517319  [120000/175341]\n",
      "loss: 0.331677  [121600/175341]\n",
      "loss: 0.286693  [123200/175341]\n",
      "loss: 0.500979  [124800/175341]\n",
      "loss: 0.401432  [126400/175341]\n",
      "loss: 0.137672  [128000/175341]\n",
      "loss: 0.408386  [129600/175341]\n",
      "loss: 0.616122  [131200/175341]\n",
      "loss: 0.499851  [132800/175341]\n",
      "loss: 0.354467  [134400/175341]\n",
      "loss: 0.521626  [136000/175341]\n",
      "loss: 0.641099  [137600/175341]\n",
      "loss: 0.939692  [139200/175341]\n",
      "loss: 0.717797  [140800/175341]\n",
      "loss: 0.439542  [142400/175341]\n",
      "loss: 0.074874  [144000/175341]\n",
      "loss: 0.348386  [145600/175341]\n",
      "loss: 0.293260  [147200/175341]\n",
      "loss: 0.227413  [148800/175341]\n",
      "loss: 0.229812  [150400/175341]\n",
      "loss: 0.405763  [152000/175341]\n",
      "loss: 0.703186  [153600/175341]\n",
      "loss: 0.393036  [155200/175341]\n",
      "loss: 0.323539  [156800/175341]\n",
      "loss: 0.654495  [158400/175341]\n",
      "loss: 0.362691  [160000/175341]\n",
      "loss: 0.622408  [161600/175341]\n",
      "loss: 0.310688  [163200/175341]\n",
      "loss: 0.358363  [164800/175341]\n",
      "loss: 0.364313  [166400/175341]\n",
      "loss: 1.050925  [168000/175341]\n",
      "loss: 0.609875  [169600/175341]\n",
      "loss: 0.562326  [171200/175341]\n",
      "loss: 0.352040  [172800/175341]\n",
      "loss: 0.379021  [174400/175341]\n",
      "Train Accuracy: 80.5556%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.599481, F1-score: 75.79%, Macro_F1-Score:  39.87%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.320722  [    0/175341]\n",
      "loss: 0.401108  [ 1600/175341]\n",
      "loss: 0.264026  [ 3200/175341]\n",
      "loss: 0.875419  [ 4800/175341]\n",
      "loss: 0.285364  [ 6400/175341]\n",
      "loss: 0.778328  [ 8000/175341]\n",
      "loss: 0.771171  [ 9600/175341]\n",
      "loss: 0.804659  [11200/175341]\n",
      "loss: 0.634502  [12800/175341]\n",
      "loss: 0.179587  [14400/175341]\n",
      "loss: 0.562634  [16000/175341]\n",
      "loss: 0.662612  [17600/175341]\n",
      "loss: 0.563082  [19200/175341]\n",
      "loss: 0.322071  [20800/175341]\n",
      "loss: 0.426231  [22400/175341]\n",
      "loss: 0.411065  [24000/175341]\n",
      "loss: 0.582129  [25600/175341]\n",
      "loss: 0.657785  [27200/175341]\n",
      "loss: 0.358989  [28800/175341]\n",
      "loss: 0.657896  [30400/175341]\n",
      "loss: 0.947660  [32000/175341]\n",
      "loss: 0.565343  [33600/175341]\n",
      "loss: 0.496593  [35200/175341]\n",
      "loss: 0.355033  [36800/175341]\n",
      "loss: 0.659119  [38400/175341]\n",
      "loss: 0.366125  [40000/175341]\n",
      "loss: 0.405489  [41600/175341]\n",
      "loss: 1.105584  [43200/175341]\n",
      "loss: 0.322408  [44800/175341]\n",
      "loss: 0.422205  [46400/175341]\n",
      "loss: 0.540124  [48000/175341]\n",
      "loss: 0.450835  [49600/175341]\n",
      "loss: 0.473794  [51200/175341]\n",
      "loss: 0.477770  [52800/175341]\n",
      "loss: 0.629339  [54400/175341]\n",
      "loss: 0.811093  [56000/175341]\n",
      "loss: 0.394898  [57600/175341]\n",
      "loss: 0.377363  [59200/175341]\n",
      "loss: 0.578158  [60800/175341]\n",
      "loss: 0.764475  [62400/175341]\n",
      "loss: 0.494967  [64000/175341]\n",
      "loss: 0.397531  [65600/175341]\n",
      "loss: 0.598373  [67200/175341]\n",
      "loss: 0.750704  [68800/175341]\n",
      "loss: 1.002073  [70400/175341]\n",
      "loss: 0.187424  [72000/175341]\n",
      "loss: 0.601033  [73600/175341]\n",
      "loss: 0.281557  [75200/175341]\n",
      "loss: 0.814356  [76800/175341]\n",
      "loss: 0.587486  [78400/175341]\n",
      "loss: 0.340486  [80000/175341]\n",
      "loss: 0.358591  [81600/175341]\n",
      "loss: 0.601123  [83200/175341]\n",
      "loss: 0.503796  [84800/175341]\n",
      "loss: 0.968074  [86400/175341]\n",
      "loss: 0.163703  [88000/175341]\n",
      "loss: 0.585911  [89600/175341]\n",
      "loss: 0.440000  [91200/175341]\n",
      "loss: 0.545880  [92800/175341]\n",
      "loss: 0.298624  [94400/175341]\n",
      "loss: 0.358112  [96000/175341]\n",
      "loss: 0.585582  [97600/175341]\n",
      "loss: 0.667316  [99200/175341]\n",
      "loss: 0.272366  [100800/175341]\n",
      "loss: 0.529445  [102400/175341]\n",
      "loss: 0.658843  [104000/175341]\n",
      "loss: 0.521325  [105600/175341]\n",
      "loss: 0.198142  [107200/175341]\n",
      "loss: 1.122199  [108800/175341]\n",
      "loss: 0.407767  [110400/175341]\n",
      "loss: 0.123083  [112000/175341]\n",
      "loss: 0.378888  [113600/175341]\n",
      "loss: 0.230328  [115200/175341]\n",
      "loss: 0.501009  [116800/175341]\n",
      "loss: 0.736853  [118400/175341]\n",
      "loss: 0.389945  [120000/175341]\n",
      "loss: 0.347179  [121600/175341]\n",
      "loss: 0.319071  [123200/175341]\n",
      "loss: 0.482067  [124800/175341]\n",
      "loss: 0.723474  [126400/175341]\n",
      "loss: 0.677408  [128000/175341]\n",
      "loss: 0.394967  [129600/175341]\n",
      "loss: 0.164667  [131200/175341]\n",
      "loss: 0.512635  [132800/175341]\n",
      "loss: 0.397996  [134400/175341]\n",
      "loss: 0.540586  [136000/175341]\n",
      "loss: 0.378582  [137600/175341]\n",
      "loss: 0.465329  [139200/175341]\n",
      "loss: 0.436030  [140800/175341]\n",
      "loss: 0.810264  [142400/175341]\n",
      "loss: 0.457026  [144000/175341]\n",
      "loss: 0.393713  [145600/175341]\n",
      "loss: 0.194901  [147200/175341]\n",
      "loss: 0.314260  [148800/175341]\n",
      "loss: 0.245507  [150400/175341]\n",
      "loss: 0.653100  [152000/175341]\n",
      "loss: 0.485895  [153600/175341]\n",
      "loss: 0.305674  [155200/175341]\n",
      "loss: 0.232049  [156800/175341]\n",
      "loss: 0.347401  [158400/175341]\n",
      "loss: 0.227584  [160000/175341]\n",
      "loss: 0.422652  [161600/175341]\n",
      "loss: 0.813922  [163200/175341]\n",
      "loss: 0.547026  [164800/175341]\n",
      "loss: 0.134667  [166400/175341]\n",
      "loss: 0.457853  [168000/175341]\n",
      "loss: 0.756476  [169600/175341]\n",
      "loss: 0.274067  [171200/175341]\n",
      "loss: 0.499508  [172800/175341]\n",
      "loss: 0.380625  [174400/175341]\n",
      "Train Accuracy: 80.5642%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.605596, F1-score: 74.73%, Macro_F1-Score:  39.30%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.362655  [    0/175341]\n",
      "loss: 1.096467  [ 1600/175341]\n",
      "loss: 0.505913  [ 3200/175341]\n",
      "loss: 0.261787  [ 4800/175341]\n",
      "loss: 0.441397  [ 6400/175341]\n",
      "loss: 0.421380  [ 8000/175341]\n",
      "loss: 0.520352  [ 9600/175341]\n",
      "loss: 0.228933  [11200/175341]\n",
      "loss: 0.628776  [12800/175341]\n",
      "loss: 0.384878  [14400/175341]\n",
      "loss: 0.248250  [16000/175341]\n",
      "loss: 0.705657  [17600/175341]\n",
      "loss: 0.828294  [19200/175341]\n",
      "loss: 0.583451  [20800/175341]\n",
      "loss: 0.319389  [22400/175341]\n",
      "loss: 0.727449  [24000/175341]\n",
      "loss: 0.411899  [25600/175341]\n",
      "loss: 0.629814  [27200/175341]\n",
      "loss: 0.701956  [28800/175341]\n",
      "loss: 0.669382  [30400/175341]\n",
      "loss: 0.646095  [32000/175341]\n",
      "loss: 0.386929  [33600/175341]\n",
      "loss: 0.238198  [35200/175341]\n",
      "loss: 0.409643  [36800/175341]\n",
      "loss: 0.590601  [38400/175341]\n",
      "loss: 0.506918  [40000/175341]\n",
      "loss: 0.611189  [41600/175341]\n",
      "loss: 0.428272  [43200/175341]\n",
      "loss: 0.492237  [44800/175341]\n",
      "loss: 0.237626  [46400/175341]\n",
      "loss: 0.684855  [48000/175341]\n",
      "loss: 0.517433  [49600/175341]\n",
      "loss: 0.491942  [51200/175341]\n",
      "loss: 0.442480  [52800/175341]\n",
      "loss: 0.323931  [54400/175341]\n",
      "loss: 0.592950  [56000/175341]\n",
      "loss: 0.679835  [57600/175341]\n",
      "loss: 0.495049  [59200/175341]\n",
      "loss: 0.213789  [60800/175341]\n",
      "loss: 0.565503  [62400/175341]\n",
      "loss: 0.381447  [64000/175341]\n",
      "loss: 0.333342  [65600/175341]\n",
      "loss: 0.535038  [67200/175341]\n",
      "loss: 0.494945  [68800/175341]\n",
      "loss: 0.348240  [70400/175341]\n",
      "loss: 0.686886  [72000/175341]\n",
      "loss: 0.321677  [73600/175341]\n",
      "loss: 0.393138  [75200/175341]\n",
      "loss: 0.327060  [76800/175341]\n",
      "loss: 0.783259  [78400/175341]\n",
      "loss: 0.389677  [80000/175341]\n",
      "loss: 0.495188  [81600/175341]\n",
      "loss: 0.249855  [83200/175341]\n",
      "loss: 1.016312  [84800/175341]\n",
      "loss: 0.737968  [86400/175341]\n",
      "loss: 0.331286  [88000/175341]\n",
      "loss: 0.472536  [89600/175341]\n",
      "loss: 0.410880  [91200/175341]\n",
      "loss: 0.539802  [92800/175341]\n",
      "loss: 0.516336  [94400/175341]\n",
      "loss: 0.163920  [96000/175341]\n",
      "loss: 0.292598  [97600/175341]\n",
      "loss: 0.398013  [99200/175341]\n",
      "loss: 0.451416  [100800/175341]\n",
      "loss: 0.786143  [102400/175341]\n",
      "loss: 0.674250  [104000/175341]\n",
      "loss: 0.407158  [105600/175341]\n",
      "loss: 0.459297  [107200/175341]\n",
      "loss: 0.363084  [108800/175341]\n",
      "loss: 0.314855  [110400/175341]\n",
      "loss: 0.518700  [112000/175341]\n",
      "loss: 0.401233  [113600/175341]\n",
      "loss: 0.526660  [115200/175341]\n",
      "loss: 0.465504  [116800/175341]\n",
      "loss: 0.276301  [118400/175341]\n",
      "loss: 0.459211  [120000/175341]\n",
      "loss: 0.378715  [121600/175341]\n",
      "loss: 0.394120  [123200/175341]\n",
      "loss: 0.503455  [124800/175341]\n",
      "loss: 0.854837  [126400/175341]\n",
      "loss: 0.930869  [128000/175341]\n",
      "loss: 0.346799  [129600/175341]\n",
      "loss: 0.357498  [131200/175341]\n",
      "loss: 0.552455  [132800/175341]\n",
      "loss: 0.373009  [134400/175341]\n",
      "loss: 0.520428  [136000/175341]\n",
      "loss: 0.853735  [137600/175341]\n",
      "loss: 0.492290  [139200/175341]\n",
      "loss: 0.505005  [140800/175341]\n",
      "loss: 0.425131  [142400/175341]\n",
      "loss: 0.248252  [144000/175341]\n",
      "loss: 0.542432  [145600/175341]\n",
      "loss: 0.331980  [147200/175341]\n",
      "loss: 0.703516  [148800/175341]\n",
      "loss: 0.292131  [150400/175341]\n",
      "loss: 0.801497  [152000/175341]\n",
      "loss: 0.419533  [153600/175341]\n",
      "loss: 0.482503  [155200/175341]\n",
      "loss: 0.412428  [156800/175341]\n",
      "loss: 0.394443  [158400/175341]\n",
      "loss: 0.517584  [160000/175341]\n",
      "loss: 0.600759  [161600/175341]\n",
      "loss: 0.385943  [163200/175341]\n",
      "loss: 0.620114  [164800/175341]\n",
      "loss: 0.663201  [166400/175341]\n",
      "loss: 1.199380  [168000/175341]\n",
      "loss: 0.314328  [169600/175341]\n",
      "loss: 0.460228  [171200/175341]\n",
      "loss: 0.365950  [172800/175341]\n",
      "loss: 0.397364  [174400/175341]\n",
      "Train Accuracy: 80.5642%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.564306, F1-score: 76.18%, Macro_F1-Score:  40.76%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.345022  [    0/175341]\n",
      "loss: 0.682522  [ 1600/175341]\n",
      "loss: 0.514138  [ 3200/175341]\n",
      "loss: 0.573997  [ 4800/175341]\n",
      "loss: 0.430841  [ 6400/175341]\n",
      "loss: 0.486276  [ 8000/175341]\n",
      "loss: 0.352514  [ 9600/175341]\n",
      "loss: 0.558745  [11200/175341]\n",
      "loss: 0.247353  [12800/175341]\n",
      "loss: 0.285893  [14400/175341]\n",
      "loss: 0.722231  [16000/175341]\n",
      "loss: 0.285671  [17600/175341]\n",
      "loss: 0.725795  [19200/175341]\n",
      "loss: 0.212401  [20800/175341]\n",
      "loss: 0.473004  [22400/175341]\n",
      "loss: 0.534125  [24000/175341]\n",
      "loss: 0.720799  [25600/175341]\n",
      "loss: 0.500885  [27200/175341]\n",
      "loss: 0.187732  [28800/175341]\n",
      "loss: 0.304555  [30400/175341]\n",
      "loss: 0.647805  [32000/175341]\n",
      "loss: 0.553494  [33600/175341]\n",
      "loss: 0.369826  [35200/175341]\n",
      "loss: 0.621551  [36800/175341]\n",
      "loss: 0.546732  [38400/175341]\n",
      "loss: 0.445805  [40000/175341]\n",
      "loss: 0.854671  [41600/175341]\n",
      "loss: 0.579044  [43200/175341]\n",
      "loss: 0.494968  [44800/175341]\n",
      "loss: 0.333748  [46400/175341]\n",
      "loss: 0.643087  [48000/175341]\n",
      "loss: 0.570159  [49600/175341]\n",
      "loss: 0.331589  [51200/175341]\n",
      "loss: 0.115840  [52800/175341]\n",
      "loss: 0.310339  [54400/175341]\n",
      "loss: 0.299139  [56000/175341]\n",
      "loss: 0.360772  [57600/175341]\n",
      "loss: 0.780731  [59200/175341]\n",
      "loss: 0.659825  [60800/175341]\n",
      "loss: 1.012062  [62400/175341]\n",
      "loss: 0.735092  [64000/175341]\n",
      "loss: 0.180124  [65600/175341]\n",
      "loss: 0.580307  [67200/175341]\n",
      "loss: 0.976441  [68800/175341]\n",
      "loss: 0.516178  [70400/175341]\n",
      "loss: 0.263984  [72000/175341]\n",
      "loss: 0.680148  [73600/175341]\n",
      "loss: 0.339423  [75200/175341]\n",
      "loss: 0.405613  [76800/175341]\n",
      "loss: 0.316029  [78400/175341]\n",
      "loss: 0.462548  [80000/175341]\n",
      "loss: 0.318344  [81600/175341]\n",
      "loss: 0.368181  [83200/175341]\n",
      "loss: 0.407102  [84800/175341]\n",
      "loss: 0.211765  [86400/175341]\n",
      "loss: 0.902102  [88000/175341]\n",
      "loss: 0.434183  [89600/175341]\n",
      "loss: 0.173834  [91200/175341]\n",
      "loss: 0.685943  [92800/175341]\n",
      "loss: 0.465853  [94400/175341]\n",
      "loss: 0.502655  [96000/175341]\n",
      "loss: 0.365976  [97600/175341]\n",
      "loss: 0.436230  [99200/175341]\n",
      "loss: 0.772067  [100800/175341]\n",
      "loss: 0.681250  [102400/175341]\n",
      "loss: 0.458554  [104000/175341]\n",
      "loss: 0.311948  [105600/175341]\n",
      "loss: 0.661146  [107200/175341]\n",
      "loss: 0.204781  [108800/175341]\n",
      "loss: 0.535958  [110400/175341]\n",
      "loss: 0.271395  [112000/175341]\n",
      "loss: 0.194471  [113600/175341]\n",
      "loss: 0.844096  [115200/175341]\n",
      "loss: 0.542956  [116800/175341]\n",
      "loss: 0.595419  [118400/175341]\n",
      "loss: 0.658723  [120000/175341]\n",
      "loss: 0.588081  [121600/175341]\n",
      "loss: 0.753640  [123200/175341]\n",
      "loss: 0.841861  [124800/175341]\n",
      "loss: 0.415482  [126400/175341]\n",
      "loss: 0.150757  [128000/175341]\n",
      "loss: 0.503172  [129600/175341]\n",
      "loss: 0.428176  [131200/175341]\n",
      "loss: 0.514242  [132800/175341]\n",
      "loss: 0.670747  [134400/175341]\n",
      "loss: 0.493756  [136000/175341]\n",
      "loss: 0.684691  [137600/175341]\n",
      "loss: 0.331463  [139200/175341]\n",
      "loss: 0.343657  [140800/175341]\n",
      "loss: 0.371372  [142400/175341]\n",
      "loss: 0.702140  [144000/175341]\n",
      "loss: 0.407062  [145600/175341]\n",
      "loss: 0.519071  [147200/175341]\n",
      "loss: 0.329002  [148800/175341]\n",
      "loss: 0.273568  [150400/175341]\n",
      "loss: 0.686776  [152000/175341]\n",
      "loss: 0.453404  [153600/175341]\n",
      "loss: 0.367290  [155200/175341]\n",
      "loss: 0.472406  [156800/175341]\n",
      "loss: 0.512491  [158400/175341]\n",
      "loss: 0.312848  [160000/175341]\n",
      "loss: 0.776688  [161600/175341]\n",
      "loss: 0.170247  [163200/175341]\n",
      "loss: 0.383308  [164800/175341]\n",
      "loss: 0.350751  [166400/175341]\n",
      "loss: 0.457786  [168000/175341]\n",
      "loss: 0.284786  [169600/175341]\n",
      "loss: 0.391594  [171200/175341]\n",
      "loss: 0.442645  [172800/175341]\n",
      "loss: 0.303364  [174400/175341]\n",
      "Train Accuracy: 80.5967%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.579475, F1-score: 76.04%, Macro_F1-Score:  40.35%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.086883  [    0/175341]\n",
      "loss: 0.215478  [ 1600/175341]\n",
      "loss: 0.519759  [ 3200/175341]\n",
      "loss: 0.474261  [ 4800/175341]\n",
      "loss: 0.265382  [ 6400/175341]\n",
      "loss: 0.448718  [ 8000/175341]\n",
      "loss: 0.181990  [ 9600/175341]\n",
      "loss: 0.174057  [11200/175341]\n",
      "loss: 0.240994  [12800/175341]\n",
      "loss: 0.403605  [14400/175341]\n",
      "loss: 0.821853  [16000/175341]\n",
      "loss: 0.832954  [17600/175341]\n",
      "loss: 0.663760  [19200/175341]\n",
      "loss: 0.453259  [20800/175341]\n",
      "loss: 0.516054  [22400/175341]\n",
      "loss: 0.425115  [24000/175341]\n",
      "loss: 0.216516  [25600/175341]\n",
      "loss: 0.521794  [27200/175341]\n",
      "loss: 0.427597  [28800/175341]\n",
      "loss: 0.427630  [30400/175341]\n",
      "loss: 0.443064  [32000/175341]\n",
      "loss: 0.395588  [33600/175341]\n",
      "loss: 0.630157  [35200/175341]\n",
      "loss: 0.481255  [36800/175341]\n",
      "loss: 0.607474  [38400/175341]\n",
      "loss: 0.576316  [40000/175341]\n",
      "loss: 0.731564  [41600/175341]\n",
      "loss: 0.501327  [43200/175341]\n",
      "loss: 0.292410  [44800/175341]\n",
      "loss: 0.607312  [46400/175341]\n",
      "loss: 0.567264  [48000/175341]\n",
      "loss: 0.461528  [49600/175341]\n",
      "loss: 0.637876  [51200/175341]\n",
      "loss: 0.571239  [52800/175341]\n",
      "loss: 0.797191  [54400/175341]\n",
      "loss: 0.365136  [56000/175341]\n",
      "loss: 0.433298  [57600/175341]\n",
      "loss: 0.279152  [59200/175341]\n",
      "loss: 0.509822  [60800/175341]\n",
      "loss: 0.384800  [62400/175341]\n",
      "loss: 0.513253  [64000/175341]\n",
      "loss: 0.480519  [65600/175341]\n",
      "loss: 0.290558  [67200/175341]\n",
      "loss: 0.974914  [68800/175341]\n",
      "loss: 0.531399  [70400/175341]\n",
      "loss: 0.454909  [72000/175341]\n",
      "loss: 0.729931  [73600/175341]\n",
      "loss: 0.511626  [75200/175341]\n",
      "loss: 0.184592  [76800/175341]\n",
      "loss: 0.512375  [78400/175341]\n",
      "loss: 0.577556  [80000/175341]\n",
      "loss: 0.549857  [81600/175341]\n",
      "loss: 0.249902  [83200/175341]\n",
      "loss: 0.579650  [84800/175341]\n",
      "loss: 0.509153  [86400/175341]\n",
      "loss: 0.368366  [88000/175341]\n",
      "loss: 0.158356  [89600/175341]\n",
      "loss: 0.563917  [91200/175341]\n",
      "loss: 0.378176  [92800/175341]\n",
      "loss: 0.511060  [94400/175341]\n",
      "loss: 0.535543  [96000/175341]\n",
      "loss: 0.609747  [97600/175341]\n",
      "loss: 0.346306  [99200/175341]\n",
      "loss: 0.179484  [100800/175341]\n",
      "loss: 0.360965  [102400/175341]\n",
      "loss: 0.382480  [104000/175341]\n",
      "loss: 0.417517  [105600/175341]\n",
      "loss: 0.454932  [107200/175341]\n",
      "loss: 0.350030  [108800/175341]\n",
      "loss: 0.794275  [110400/175341]\n",
      "loss: 0.588340  [112000/175341]\n",
      "loss: 0.442827  [113600/175341]\n",
      "loss: 0.159757  [115200/175341]\n",
      "loss: 0.605606  [116800/175341]\n",
      "loss: 0.334813  [118400/175341]\n",
      "loss: 0.435709  [120000/175341]\n",
      "loss: 0.177914  [121600/175341]\n",
      "loss: 0.241764  [123200/175341]\n",
      "loss: 0.202394  [124800/175341]\n",
      "loss: 0.546642  [126400/175341]\n",
      "loss: 0.901176  [128000/175341]\n",
      "loss: 0.192226  [129600/175341]\n",
      "loss: 0.540941  [131200/175341]\n",
      "loss: 0.210860  [132800/175341]\n",
      "loss: 0.574759  [134400/175341]\n",
      "loss: 0.359188  [136000/175341]\n",
      "loss: 0.619714  [137600/175341]\n",
      "loss: 0.682007  [139200/175341]\n",
      "loss: 0.500469  [140800/175341]\n",
      "loss: 0.752859  [142400/175341]\n",
      "loss: 0.642097  [144000/175341]\n",
      "loss: 0.238171  [145600/175341]\n",
      "loss: 0.358950  [147200/175341]\n",
      "loss: 0.335049  [148800/175341]\n",
      "loss: 0.297940  [150400/175341]\n",
      "loss: 0.473005  [152000/175341]\n",
      "loss: 0.393703  [153600/175341]\n",
      "loss: 0.412132  [155200/175341]\n",
      "loss: 0.322918  [156800/175341]\n",
      "loss: 0.443630  [158400/175341]\n",
      "loss: 0.502840  [160000/175341]\n",
      "loss: 0.503779  [161600/175341]\n",
      "loss: 0.651431  [163200/175341]\n",
      "loss: 0.748885  [164800/175341]\n",
      "loss: 0.112488  [166400/175341]\n",
      "loss: 0.305225  [168000/175341]\n",
      "loss: 0.822127  [169600/175341]\n",
      "loss: 0.310601  [171200/175341]\n",
      "loss: 0.672126  [172800/175341]\n",
      "loss: 0.233063  [174400/175341]\n",
      "Train Accuracy: 80.5921%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.565651, F1-score: 76.69%, Macro_F1-Score:  40.28%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.851033  [    0/175341]\n",
      "loss: 0.295978  [ 1600/175341]\n",
      "loss: 0.525661  [ 3200/175341]\n",
      "loss: 0.236171  [ 4800/175341]\n",
      "loss: 0.480075  [ 6400/175341]\n",
      "loss: 0.530350  [ 8000/175341]\n",
      "loss: 0.639807  [ 9600/175341]\n",
      "loss: 0.434993  [11200/175341]\n",
      "loss: 0.615137  [12800/175341]\n",
      "loss: 0.526790  [14400/175341]\n",
      "loss: 0.559869  [16000/175341]\n",
      "loss: 0.527869  [17600/175341]\n",
      "loss: 1.177484  [19200/175341]\n",
      "loss: 0.138453  [20800/175341]\n",
      "loss: 0.574907  [22400/175341]\n",
      "loss: 0.473022  [24000/175341]\n",
      "loss: 0.326567  [25600/175341]\n",
      "loss: 0.552416  [27200/175341]\n",
      "loss: 0.463745  [28800/175341]\n",
      "loss: 0.461932  [30400/175341]\n",
      "loss: 1.202795  [32000/175341]\n",
      "loss: 0.199065  [33600/175341]\n",
      "loss: 0.286546  [35200/175341]\n",
      "loss: 0.226725  [36800/175341]\n",
      "loss: 0.224052  [38400/175341]\n",
      "loss: 0.366749  [40000/175341]\n",
      "loss: 0.697284  [41600/175341]\n",
      "loss: 0.121575  [43200/175341]\n",
      "loss: 0.536754  [44800/175341]\n",
      "loss: 0.560498  [46400/175341]\n",
      "loss: 0.442964  [48000/175341]\n",
      "loss: 0.339497  [49600/175341]\n",
      "loss: 0.703569  [51200/175341]\n",
      "loss: 0.806347  [52800/175341]\n",
      "loss: 0.545275  [54400/175341]\n",
      "loss: 0.607998  [56000/175341]\n",
      "loss: 0.810499  [57600/175341]\n",
      "loss: 1.426858  [59200/175341]\n",
      "loss: 0.343392  [60800/175341]\n",
      "loss: 0.258783  [62400/175341]\n",
      "loss: 0.210461  [64000/175341]\n",
      "loss: 0.855159  [65600/175341]\n",
      "loss: 0.636653  [67200/175341]\n",
      "loss: 0.772875  [68800/175341]\n",
      "loss: 0.651263  [70400/175341]\n",
      "loss: 0.622007  [72000/175341]\n",
      "loss: 0.268793  [73600/175341]\n",
      "loss: 0.611782  [75200/175341]\n",
      "loss: 0.328863  [76800/175341]\n",
      "loss: 0.446651  [78400/175341]\n",
      "loss: 0.258620  [80000/175341]\n",
      "loss: 0.699654  [81600/175341]\n",
      "loss: 0.462316  [83200/175341]\n",
      "loss: 0.324474  [84800/175341]\n",
      "loss: 0.374668  [86400/175341]\n",
      "loss: 0.535857  [88000/175341]\n",
      "loss: 0.282928  [89600/175341]\n",
      "loss: 0.509478  [91200/175341]\n",
      "loss: 0.610875  [92800/175341]\n",
      "loss: 0.477443  [94400/175341]\n",
      "loss: 0.903589  [96000/175341]\n",
      "loss: 0.490973  [97600/175341]\n",
      "loss: 0.324393  [99200/175341]\n",
      "loss: 0.821459  [100800/175341]\n",
      "loss: 0.351067  [102400/175341]\n",
      "loss: 0.511394  [104000/175341]\n",
      "loss: 0.331815  [105600/175341]\n",
      "loss: 0.423394  [107200/175341]\n",
      "loss: 0.775598  [108800/175341]\n",
      "loss: 0.382050  [110400/175341]\n",
      "loss: 0.274429  [112000/175341]\n",
      "loss: 0.454503  [113600/175341]\n",
      "loss: 0.550200  [115200/175341]\n",
      "loss: 0.453605  [116800/175341]\n",
      "loss: 0.440716  [118400/175341]\n",
      "loss: 0.298569  [120000/175341]\n",
      "loss: 0.734363  [121600/175341]\n",
      "loss: 0.424440  [123200/175341]\n",
      "loss: 0.582845  [124800/175341]\n",
      "loss: 0.384988  [126400/175341]\n",
      "loss: 0.635340  [128000/175341]\n",
      "loss: 0.360846  [129600/175341]\n",
      "loss: 0.382949  [131200/175341]\n",
      "loss: 0.748280  [132800/175341]\n",
      "loss: 0.588781  [134400/175341]\n",
      "loss: 0.533816  [136000/175341]\n",
      "loss: 0.427558  [137600/175341]\n",
      "loss: 0.484623  [139200/175341]\n",
      "loss: 0.150552  [140800/175341]\n",
      "loss: 0.598211  [142400/175341]\n",
      "loss: 0.522608  [144000/175341]\n",
      "loss: 0.331278  [145600/175341]\n",
      "loss: 0.333769  [147200/175341]\n",
      "loss: 0.537979  [148800/175341]\n",
      "loss: 0.971163  [150400/175341]\n",
      "loss: 0.328342  [152000/175341]\n",
      "loss: 0.418295  [153600/175341]\n",
      "loss: 0.245870  [155200/175341]\n",
      "loss: 0.253980  [156800/175341]\n",
      "loss: 0.703949  [158400/175341]\n",
      "loss: 0.792610  [160000/175341]\n",
      "loss: 0.629927  [161600/175341]\n",
      "loss: 0.357155  [163200/175341]\n",
      "loss: 0.143593  [164800/175341]\n",
      "loss: 1.116359  [166400/175341]\n",
      "loss: 0.715650  [168000/175341]\n",
      "loss: 0.457587  [169600/175341]\n",
      "loss: 0.442589  [171200/175341]\n",
      "loss: 0.290344  [172800/175341]\n",
      "loss: 0.283873  [174400/175341]\n",
      "Train Accuracy: 80.6069%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.583719, F1-score: 75.59%, Macro_F1-Score:  40.40%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b427034-602d-4252-af52-9c7d0908f074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.399120  [    0/175341]\n",
      "loss: 0.948359  [ 1600/175341]\n",
      "loss: 0.530783  [ 3200/175341]\n",
      "loss: 0.173955  [ 4800/175341]\n",
      "loss: 0.204503  [ 6400/175341]\n",
      "loss: 0.408930  [ 8000/175341]\n",
      "loss: 0.437123  [ 9600/175341]\n",
      "loss: 0.204299  [11200/175341]\n",
      "loss: 0.326054  [12800/175341]\n",
      "loss: 0.502620  [14400/175341]\n",
      "loss: 0.935823  [16000/175341]\n",
      "loss: 0.704344  [17600/175341]\n",
      "loss: 0.290815  [19200/175341]\n",
      "loss: 0.772285  [20800/175341]\n",
      "loss: 0.484590  [22400/175341]\n",
      "loss: 0.473952  [24000/175341]\n",
      "loss: 0.257846  [25600/175341]\n",
      "loss: 0.832144  [27200/175341]\n",
      "loss: 0.710474  [28800/175341]\n",
      "loss: 0.441708  [30400/175341]\n",
      "loss: 0.583425  [32000/175341]\n",
      "loss: 0.540367  [33600/175341]\n",
      "loss: 0.438702  [35200/175341]\n",
      "loss: 0.408068  [36800/175341]\n",
      "loss: 0.255854  [38400/175341]\n",
      "loss: 0.692375  [40000/175341]\n",
      "loss: 0.826537  [41600/175341]\n",
      "loss: 0.818552  [43200/175341]\n",
      "loss: 0.734640  [44800/175341]\n",
      "loss: 0.365135  [46400/175341]\n",
      "loss: 0.567682  [48000/175341]\n",
      "loss: 0.372808  [49600/175341]\n",
      "loss: 0.340612  [51200/175341]\n",
      "loss: 0.363323  [52800/175341]\n",
      "loss: 0.565548  [54400/175341]\n",
      "loss: 0.397432  [56000/175341]\n",
      "loss: 0.362169  [57600/175341]\n",
      "loss: 0.420885  [59200/175341]\n",
      "loss: 0.848867  [60800/175341]\n",
      "loss: 0.684580  [62400/175341]\n",
      "loss: 0.514188  [64000/175341]\n",
      "loss: 0.579508  [65600/175341]\n",
      "loss: 0.578340  [67200/175341]\n",
      "loss: 0.380718  [68800/175341]\n",
      "loss: 0.384799  [70400/175341]\n",
      "loss: 0.689359  [72000/175341]\n",
      "loss: 0.799395  [73600/175341]\n",
      "loss: 0.315291  [75200/175341]\n",
      "loss: 0.682575  [76800/175341]\n",
      "loss: 0.447962  [78400/175341]\n",
      "loss: 0.345542  [80000/175341]\n",
      "loss: 0.425350  [81600/175341]\n",
      "loss: 0.395208  [83200/175341]\n",
      "loss: 0.514949  [84800/175341]\n",
      "loss: 0.642924  [86400/175341]\n",
      "loss: 0.480307  [88000/175341]\n",
      "loss: 0.764781  [89600/175341]\n",
      "loss: 0.455895  [91200/175341]\n",
      "loss: 0.318897  [92800/175341]\n",
      "loss: 0.548881  [94400/175341]\n",
      "loss: 0.363805  [96000/175341]\n",
      "loss: 0.229626  [97600/175341]\n",
      "loss: 0.357452  [99200/175341]\n",
      "loss: 0.447021  [100800/175341]\n",
      "loss: 0.620395  [102400/175341]\n",
      "loss: 0.421373  [104000/175341]\n",
      "loss: 0.693306  [105600/175341]\n",
      "loss: 0.542491  [107200/175341]\n",
      "loss: 0.552414  [108800/175341]\n",
      "loss: 0.496041  [110400/175341]\n",
      "loss: 0.619750  [112000/175341]\n",
      "loss: 0.340581  [113600/175341]\n",
      "loss: 0.125862  [115200/175341]\n",
      "loss: 0.356123  [116800/175341]\n",
      "loss: 0.890996  [118400/175341]\n",
      "loss: 0.517296  [120000/175341]\n",
      "loss: 0.335973  [121600/175341]\n",
      "loss: 0.363998  [123200/175341]\n",
      "loss: 0.168415  [124800/175341]\n",
      "loss: 0.705781  [126400/175341]\n",
      "loss: 0.473044  [128000/175341]\n",
      "loss: 0.512709  [129600/175341]\n",
      "loss: 0.415568  [131200/175341]\n",
      "loss: 0.978243  [132800/175341]\n",
      "loss: 0.329532  [134400/175341]\n",
      "loss: 0.508287  [136000/175341]\n",
      "loss: 0.327171  [137600/175341]\n",
      "loss: 0.432298  [139200/175341]\n",
      "loss: 0.352663  [140800/175341]\n",
      "loss: 0.536674  [142400/175341]\n",
      "loss: 0.168978  [144000/175341]\n",
      "loss: 0.127940  [145600/175341]\n",
      "loss: 0.392865  [147200/175341]\n",
      "loss: 0.347982  [148800/175341]\n",
      "loss: 0.499207  [150400/175341]\n",
      "loss: 0.280096  [152000/175341]\n",
      "loss: 0.390979  [153600/175341]\n",
      "loss: 0.237936  [155200/175341]\n",
      "loss: 0.188026  [156800/175341]\n",
      "loss: 0.749940  [158400/175341]\n",
      "loss: 0.287158  [160000/175341]\n",
      "loss: 0.347452  [161600/175341]\n",
      "loss: 0.819133  [163200/175341]\n",
      "loss: 0.584194  [164800/175341]\n",
      "loss: 0.531319  [166400/175341]\n",
      "loss: 0.268880  [168000/175341]\n",
      "loss: 0.298968  [169600/175341]\n",
      "loss: 0.955748  [171200/175341]\n",
      "loss: 0.230429  [172800/175341]\n",
      "loss: 1.177809  [174400/175341]\n",
      "Train Accuracy: 80.6172%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.586802, F1-score: 75.80%, Macro_F1-Score:  39.57%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.535257  [    0/175341]\n",
      "loss: 0.362695  [ 1600/175341]\n",
      "loss: 1.255249  [ 3200/175341]\n",
      "loss: 0.430307  [ 4800/175341]\n",
      "loss: 0.665028  [ 6400/175341]\n",
      "loss: 0.649699  [ 8000/175341]\n",
      "loss: 0.652029  [ 9600/175341]\n",
      "loss: 0.336979  [11200/175341]\n",
      "loss: 0.681816  [12800/175341]\n",
      "loss: 0.537125  [14400/175341]\n",
      "loss: 0.538802  [16000/175341]\n",
      "loss: 0.376617  [17600/175341]\n",
      "loss: 0.701286  [19200/175341]\n",
      "loss: 0.386877  [20800/175341]\n",
      "loss: 0.229329  [22400/175341]\n",
      "loss: 0.321245  [24000/175341]\n",
      "loss: 0.345136  [25600/175341]\n",
      "loss: 0.325489  [27200/175341]\n",
      "loss: 0.606739  [28800/175341]\n",
      "loss: 0.506836  [30400/175341]\n",
      "loss: 0.456178  [32000/175341]\n",
      "loss: 0.574106  [33600/175341]\n",
      "loss: 0.334102  [35200/175341]\n",
      "loss: 0.502489  [36800/175341]\n",
      "loss: 0.419354  [38400/175341]\n",
      "loss: 0.681615  [40000/175341]\n",
      "loss: 0.655168  [41600/175341]\n",
      "loss: 0.698145  [43200/175341]\n",
      "loss: 0.683584  [44800/175341]\n",
      "loss: 0.252591  [46400/175341]\n",
      "loss: 0.208132  [48000/175341]\n",
      "loss: 0.579235  [49600/175341]\n",
      "loss: 0.926282  [51200/175341]\n",
      "loss: 0.622353  [52800/175341]\n",
      "loss: 0.387669  [54400/175341]\n",
      "loss: 0.879477  [56000/175341]\n",
      "loss: 0.444223  [57600/175341]\n",
      "loss: 0.372072  [59200/175341]\n",
      "loss: 1.023947  [60800/175341]\n",
      "loss: 0.661558  [62400/175341]\n",
      "loss: 0.516274  [64000/175341]\n",
      "loss: 0.327490  [65600/175341]\n",
      "loss: 0.309154  [67200/175341]\n",
      "loss: 0.525897  [68800/175341]\n",
      "loss: 0.502869  [70400/175341]\n",
      "loss: 0.683407  [72000/175341]\n",
      "loss: 0.304963  [73600/175341]\n",
      "loss: 0.521311  [75200/175341]\n",
      "loss: 0.201087  [76800/175341]\n",
      "loss: 0.401878  [78400/175341]\n",
      "loss: 0.376839  [80000/175341]\n",
      "loss: 0.331288  [81600/175341]\n",
      "loss: 0.390262  [83200/175341]\n",
      "loss: 0.242742  [84800/175341]\n",
      "loss: 0.555441  [86400/175341]\n",
      "loss: 0.238443  [88000/175341]\n",
      "loss: 0.834281  [89600/175341]\n",
      "loss: 0.104567  [91200/175341]\n",
      "loss: 0.747654  [92800/175341]\n",
      "loss: 0.370631  [94400/175341]\n",
      "loss: 0.606940  [96000/175341]\n",
      "loss: 0.362598  [97600/175341]\n",
      "loss: 1.179281  [99200/175341]\n",
      "loss: 0.285429  [100800/175341]\n",
      "loss: 0.590333  [102400/175341]\n",
      "loss: 0.255522  [104000/175341]\n",
      "loss: 0.416987  [105600/175341]\n",
      "loss: 0.459360  [107200/175341]\n",
      "loss: 0.343881  [108800/175341]\n",
      "loss: 0.404906  [110400/175341]\n",
      "loss: 0.460916  [112000/175341]\n",
      "loss: 0.389772  [113600/175341]\n",
      "loss: 0.583390  [115200/175341]\n",
      "loss: 0.362603  [116800/175341]\n",
      "loss: 0.576336  [118400/175341]\n",
      "loss: 0.164625  [120000/175341]\n",
      "loss: 0.527062  [121600/175341]\n",
      "loss: 0.539137  [123200/175341]\n",
      "loss: 0.929602  [124800/175341]\n",
      "loss: 0.770380  [126400/175341]\n",
      "loss: 0.408314  [128000/175341]\n",
      "loss: 0.156847  [129600/175341]\n",
      "loss: 0.483883  [131200/175341]\n",
      "loss: 0.253079  [132800/175341]\n",
      "loss: 0.283401  [134400/175341]\n",
      "loss: 0.333065  [136000/175341]\n",
      "loss: 0.840096  [137600/175341]\n",
      "loss: 0.966481  [139200/175341]\n",
      "loss: 0.622514  [140800/175341]\n",
      "loss: 0.547501  [142400/175341]\n",
      "loss: 0.190010  [144000/175341]\n",
      "loss: 0.641355  [145600/175341]\n",
      "loss: 0.713342  [147200/175341]\n",
      "loss: 0.540508  [148800/175341]\n",
      "loss: 0.377263  [150400/175341]\n",
      "loss: 0.591505  [152000/175341]\n",
      "loss: 0.181226  [153600/175341]\n",
      "loss: 0.257312  [155200/175341]\n",
      "loss: 0.533163  [156800/175341]\n",
      "loss: 0.746463  [158400/175341]\n",
      "loss: 0.477018  [160000/175341]\n",
      "loss: 0.747184  [161600/175341]\n",
      "loss: 0.531107  [163200/175341]\n",
      "loss: 0.446676  [164800/175341]\n",
      "loss: 0.320181  [166400/175341]\n",
      "loss: 0.954259  [168000/175341]\n",
      "loss: 0.170945  [169600/175341]\n",
      "loss: 0.577013  [171200/175341]\n",
      "loss: 0.460842  [172800/175341]\n",
      "loss: 0.868837  [174400/175341]\n",
      "Train Accuracy: 80.6446%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.584793, F1-score: 75.30%, Macro_F1-Score:  39.69%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.553556  [    0/175341]\n",
      "loss: 0.310749  [ 1600/175341]\n",
      "loss: 0.716496  [ 3200/175341]\n",
      "loss: 0.483352  [ 4800/175341]\n",
      "loss: 0.651518  [ 6400/175341]\n",
      "loss: 0.700670  [ 8000/175341]\n",
      "loss: 0.649304  [ 9600/175341]\n",
      "loss: 0.405988  [11200/175341]\n",
      "loss: 0.155753  [12800/175341]\n",
      "loss: 0.822001  [14400/175341]\n",
      "loss: 0.218557  [16000/175341]\n",
      "loss: 0.375333  [17600/175341]\n",
      "loss: 0.619291  [19200/175341]\n",
      "loss: 0.697056  [20800/175341]\n",
      "loss: 0.526308  [22400/175341]\n",
      "loss: 0.317175  [24000/175341]\n",
      "loss: 0.578485  [25600/175341]\n",
      "loss: 0.529439  [27200/175341]\n",
      "loss: 0.625332  [28800/175341]\n",
      "loss: 0.796749  [30400/175341]\n",
      "loss: 0.997009  [32000/175341]\n",
      "loss: 0.892057  [33600/175341]\n",
      "loss: 0.512170  [35200/175341]\n",
      "loss: 0.765343  [36800/175341]\n",
      "loss: 0.711115  [38400/175341]\n",
      "loss: 0.903991  [40000/175341]\n",
      "loss: 0.252631  [41600/175341]\n",
      "loss: 0.125515  [43200/175341]\n",
      "loss: 0.415755  [44800/175341]\n",
      "loss: 0.446677  [46400/175341]\n",
      "loss: 0.409791  [48000/175341]\n",
      "loss: 0.663200  [49600/175341]\n",
      "loss: 0.563289  [51200/175341]\n",
      "loss: 0.439046  [52800/175341]\n",
      "loss: 0.250108  [54400/175341]\n",
      "loss: 1.103234  [56000/175341]\n",
      "loss: 0.406353  [57600/175341]\n",
      "loss: 0.435666  [59200/175341]\n",
      "loss: 0.526034  [60800/175341]\n",
      "loss: 0.326724  [62400/175341]\n",
      "loss: 0.535310  [64000/175341]\n",
      "loss: 0.283913  [65600/175341]\n",
      "loss: 0.395163  [67200/175341]\n",
      "loss: 0.313760  [68800/175341]\n",
      "loss: 0.978434  [70400/175341]\n",
      "loss: 0.418045  [72000/175341]\n",
      "loss: 0.736282  [73600/175341]\n",
      "loss: 0.308077  [75200/175341]\n",
      "loss: 0.456833  [76800/175341]\n",
      "loss: 0.425907  [78400/175341]\n",
      "loss: 0.414765  [80000/175341]\n",
      "loss: 0.405231  [81600/175341]\n",
      "loss: 0.483140  [83200/175341]\n",
      "loss: 0.569461  [84800/175341]\n",
      "loss: 0.336166  [86400/175341]\n",
      "loss: 0.331360  [88000/175341]\n",
      "loss: 0.403906  [89600/175341]\n",
      "loss: 0.241703  [91200/175341]\n",
      "loss: 0.802668  [92800/175341]\n",
      "loss: 0.291236  [94400/175341]\n",
      "loss: 0.350706  [96000/175341]\n",
      "loss: 0.230066  [97600/175341]\n",
      "loss: 0.324915  [99200/175341]\n",
      "loss: 0.374685  [100800/175341]\n",
      "loss: 0.619668  [102400/175341]\n",
      "loss: 0.325133  [104000/175341]\n",
      "loss: 0.392456  [105600/175341]\n",
      "loss: 0.682510  [107200/175341]\n",
      "loss: 0.331093  [108800/175341]\n",
      "loss: 0.473859  [110400/175341]\n",
      "loss: 0.451575  [112000/175341]\n",
      "loss: 0.124906  [113600/175341]\n",
      "loss: 0.748556  [115200/175341]\n",
      "loss: 0.275759  [116800/175341]\n",
      "loss: 0.895074  [118400/175341]\n",
      "loss: 0.707837  [120000/175341]\n",
      "loss: 0.402164  [121600/175341]\n",
      "loss: 0.462043  [123200/175341]\n",
      "loss: 0.320272  [124800/175341]\n",
      "loss: 0.287187  [126400/175341]\n",
      "loss: 0.455266  [128000/175341]\n",
      "loss: 0.518070  [129600/175341]\n",
      "loss: 0.327497  [131200/175341]\n",
      "loss: 0.583008  [132800/175341]\n",
      "loss: 0.354294  [134400/175341]\n",
      "loss: 0.642246  [136000/175341]\n",
      "loss: 0.396898  [137600/175341]\n",
      "loss: 0.373853  [139200/175341]\n",
      "loss: 0.544791  [140800/175341]\n",
      "loss: 0.481455  [142400/175341]\n",
      "loss: 0.556284  [144000/175341]\n",
      "loss: 0.520711  [145600/175341]\n",
      "loss: 0.036158  [147200/175341]\n",
      "loss: 0.337478  [148800/175341]\n",
      "loss: 0.257419  [150400/175341]\n",
      "loss: 0.461682  [152000/175341]\n",
      "loss: 0.219189  [153600/175341]\n",
      "loss: 0.699165  [155200/175341]\n",
      "loss: 0.665658  [156800/175341]\n",
      "loss: 1.035390  [158400/175341]\n",
      "loss: 0.385110  [160000/175341]\n",
      "loss: 0.652760  [161600/175341]\n",
      "loss: 0.590458  [163200/175341]\n",
      "loss: 0.548957  [164800/175341]\n",
      "loss: 0.191038  [166400/175341]\n",
      "loss: 0.431687  [168000/175341]\n",
      "loss: 0.772871  [169600/175341]\n",
      "loss: 0.332132  [171200/175341]\n",
      "loss: 0.811649  [172800/175341]\n",
      "loss: 0.291691  [174400/175341]\n",
      "Train Accuracy: 80.6680%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.609324, F1-score: 74.48%, Macro_F1-Score:  39.84%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.038521  [    0/175341]\n",
      "loss: 0.352603  [ 1600/175341]\n",
      "loss: 0.376014  [ 3200/175341]\n",
      "loss: 0.571279  [ 4800/175341]\n",
      "loss: 0.311377  [ 6400/175341]\n",
      "loss: 1.396073  [ 8000/175341]\n",
      "loss: 0.588144  [ 9600/175341]\n",
      "loss: 0.495247  [11200/175341]\n",
      "loss: 0.258364  [12800/175341]\n",
      "loss: 0.380036  [14400/175341]\n",
      "loss: 0.676057  [16000/175341]\n",
      "loss: 0.648095  [17600/175341]\n",
      "loss: 0.309722  [19200/175341]\n",
      "loss: 0.652826  [20800/175341]\n",
      "loss: 0.429301  [22400/175341]\n",
      "loss: 0.497237  [24000/175341]\n",
      "loss: 0.248144  [25600/175341]\n",
      "loss: 0.585592  [27200/175341]\n",
      "loss: 0.726882  [28800/175341]\n",
      "loss: 0.620490  [30400/175341]\n",
      "loss: 0.747824  [32000/175341]\n",
      "loss: 0.739914  [33600/175341]\n",
      "loss: 0.811516  [35200/175341]\n",
      "loss: 0.506673  [36800/175341]\n",
      "loss: 0.350325  [38400/175341]\n",
      "loss: 0.527175  [40000/175341]\n",
      "loss: 0.337550  [41600/175341]\n",
      "loss: 0.244944  [43200/175341]\n",
      "loss: 0.550357  [44800/175341]\n",
      "loss: 0.337752  [46400/175341]\n",
      "loss: 0.569686  [48000/175341]\n",
      "loss: 0.401254  [49600/175341]\n",
      "loss: 0.675276  [51200/175341]\n",
      "loss: 0.607241  [52800/175341]\n",
      "loss: 0.204404  [54400/175341]\n",
      "loss: 0.631223  [56000/175341]\n",
      "loss: 0.583909  [57600/175341]\n",
      "loss: 0.388750  [59200/175341]\n",
      "loss: 0.332834  [60800/175341]\n",
      "loss: 0.323821  [62400/175341]\n",
      "loss: 0.457954  [64000/175341]\n",
      "loss: 0.757495  [65600/175341]\n",
      "loss: 0.214458  [67200/175341]\n",
      "loss: 0.817523  [68800/175341]\n",
      "loss: 0.444847  [70400/175341]\n",
      "loss: 0.548235  [72000/175341]\n",
      "loss: 0.913091  [73600/175341]\n",
      "loss: 0.494141  [75200/175341]\n",
      "loss: 0.522442  [76800/175341]\n",
      "loss: 0.429680  [78400/175341]\n",
      "loss: 0.297565  [80000/175341]\n",
      "loss: 0.702673  [81600/175341]\n",
      "loss: 0.749183  [83200/175341]\n",
      "loss: 0.334117  [84800/175341]\n",
      "loss: 0.329234  [86400/175341]\n",
      "loss: 0.500872  [88000/175341]\n",
      "loss: 0.396482  [89600/175341]\n",
      "loss: 0.164049  [91200/175341]\n",
      "loss: 1.019858  [92800/175341]\n",
      "loss: 0.339754  [94400/175341]\n",
      "loss: 0.266741  [96000/175341]\n",
      "loss: 0.361079  [97600/175341]\n",
      "loss: 0.508995  [99200/175341]\n",
      "loss: 1.048131  [100800/175341]\n",
      "loss: 0.806955  [102400/175341]\n",
      "loss: 0.763386  [104000/175341]\n",
      "loss: 0.219649  [105600/175341]\n",
      "loss: 0.981340  [107200/175341]\n",
      "loss: 0.471800  [108800/175341]\n",
      "loss: 0.556189  [110400/175341]\n",
      "loss: 0.306558  [112000/175341]\n",
      "loss: 0.746281  [113600/175341]\n",
      "loss: 0.380076  [115200/175341]\n",
      "loss: 0.860629  [116800/175341]\n",
      "loss: 0.467992  [118400/175341]\n",
      "loss: 0.448817  [120000/175341]\n",
      "loss: 0.188239  [121600/175341]\n",
      "loss: 0.415739  [123200/175341]\n",
      "loss: 0.119175  [124800/175341]\n",
      "loss: 0.319041  [126400/175341]\n",
      "loss: 0.452613  [128000/175341]\n",
      "loss: 0.343466  [129600/175341]\n",
      "loss: 0.245193  [131200/175341]\n",
      "loss: 0.287997  [132800/175341]\n",
      "loss: 0.362644  [134400/175341]\n",
      "loss: 0.472130  [136000/175341]\n",
      "loss: 0.253413  [137600/175341]\n",
      "loss: 0.257553  [139200/175341]\n",
      "loss: 0.554569  [140800/175341]\n",
      "loss: 0.501021  [142400/175341]\n",
      "loss: 0.568414  [144000/175341]\n",
      "loss: 0.108066  [145600/175341]\n",
      "loss: 0.646824  [147200/175341]\n",
      "loss: 0.441087  [148800/175341]\n",
      "loss: 0.289435  [150400/175341]\n",
      "loss: 0.474802  [152000/175341]\n",
      "loss: 0.659048  [153600/175341]\n",
      "loss: 0.230606  [155200/175341]\n",
      "loss: 0.677420  [156800/175341]\n",
      "loss: 0.529936  [158400/175341]\n",
      "loss: 0.635795  [160000/175341]\n",
      "loss: 0.131536  [161600/175341]\n",
      "loss: 0.299583  [163200/175341]\n",
      "loss: 0.402031  [164800/175341]\n",
      "loss: 0.640694  [166400/175341]\n",
      "loss: 0.598499  [168000/175341]\n",
      "loss: 0.516643  [169600/175341]\n",
      "loss: 0.872967  [171200/175341]\n",
      "loss: 0.440947  [172800/175341]\n",
      "loss: 0.694006  [174400/175341]\n",
      "Train Accuracy: 80.6891%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.573057, F1-score: 76.87%, Macro_F1-Score:  40.97%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.348216  [    0/175341]\n",
      "loss: 0.682534  [ 1600/175341]\n",
      "loss: 0.404975  [ 3200/175341]\n",
      "loss: 0.647034  [ 4800/175341]\n",
      "loss: 0.439116  [ 6400/175341]\n",
      "loss: 0.723930  [ 8000/175341]\n",
      "loss: 0.502530  [ 9600/175341]\n",
      "loss: 0.487886  [11200/175341]\n",
      "loss: 0.724117  [12800/175341]\n",
      "loss: 0.285938  [14400/175341]\n",
      "loss: 0.772726  [16000/175341]\n",
      "loss: 0.593433  [17600/175341]\n",
      "loss: 0.760719  [19200/175341]\n",
      "loss: 0.760413  [20800/175341]\n",
      "loss: 0.362125  [22400/175341]\n",
      "loss: 0.438170  [24000/175341]\n",
      "loss: 0.377854  [25600/175341]\n",
      "loss: 0.782507  [27200/175341]\n",
      "loss: 0.501393  [28800/175341]\n",
      "loss: 0.512264  [30400/175341]\n",
      "loss: 0.393460  [32000/175341]\n",
      "loss: 0.339544  [33600/175341]\n",
      "loss: 0.475423  [35200/175341]\n",
      "loss: 0.624620  [36800/175341]\n",
      "loss: 0.900668  [38400/175341]\n",
      "loss: 0.180226  [40000/175341]\n",
      "loss: 0.502162  [41600/175341]\n",
      "loss: 0.383856  [43200/175341]\n",
      "loss: 0.335104  [44800/175341]\n",
      "loss: 0.628398  [46400/175341]\n",
      "loss: 0.499492  [48000/175341]\n",
      "loss: 0.422055  [49600/175341]\n",
      "loss: 0.582613  [51200/175341]\n",
      "loss: 0.340576  [52800/175341]\n",
      "loss: 0.875015  [54400/175341]\n",
      "loss: 0.738042  [56000/175341]\n",
      "loss: 0.565098  [57600/175341]\n",
      "loss: 0.406308  [59200/175341]\n",
      "loss: 0.340742  [60800/175341]\n",
      "loss: 0.664611  [62400/175341]\n",
      "loss: 0.786076  [64000/175341]\n",
      "loss: 0.867646  [65600/175341]\n",
      "loss: 1.131255  [67200/175341]\n",
      "loss: 0.694981  [68800/175341]\n",
      "loss: 0.522196  [70400/175341]\n",
      "loss: 0.174288  [72000/175341]\n",
      "loss: 0.348013  [73600/175341]\n",
      "loss: 0.388254  [75200/175341]\n",
      "loss: 0.403206  [76800/175341]\n",
      "loss: 0.211766  [78400/175341]\n",
      "loss: 0.548647  [80000/175341]\n",
      "loss: 0.509906  [81600/175341]\n",
      "loss: 0.289454  [83200/175341]\n",
      "loss: 0.748586  [84800/175341]\n",
      "loss: 0.418043  [86400/175341]\n",
      "loss: 0.302471  [88000/175341]\n",
      "loss: 0.361713  [89600/175341]\n",
      "loss: 0.501357  [91200/175341]\n",
      "loss: 0.379307  [92800/175341]\n",
      "loss: 0.499535  [94400/175341]\n",
      "loss: 0.253311  [96000/175341]\n",
      "loss: 0.289954  [97600/175341]\n",
      "loss: 0.216085  [99200/175341]\n",
      "loss: 0.100806  [100800/175341]\n",
      "loss: 0.655171  [102400/175341]\n",
      "loss: 0.238931  [104000/175341]\n",
      "loss: 1.185944  [105600/175341]\n",
      "loss: 0.831391  [107200/175341]\n",
      "loss: 0.441847  [108800/175341]\n",
      "loss: 0.454598  [110400/175341]\n",
      "loss: 0.761058  [112000/175341]\n",
      "loss: 0.766919  [113600/175341]\n",
      "loss: 0.173284  [115200/175341]\n",
      "loss: 0.496995  [116800/175341]\n",
      "loss: 0.372070  [118400/175341]\n",
      "loss: 0.863910  [120000/175341]\n",
      "loss: 0.542291  [121600/175341]\n",
      "loss: 0.571545  [123200/175341]\n",
      "loss: 0.759316  [124800/175341]\n",
      "loss: 0.269308  [126400/175341]\n",
      "loss: 0.474749  [128000/175341]\n",
      "loss: 0.245206  [129600/175341]\n",
      "loss: 0.476600  [131200/175341]\n",
      "loss: 0.623249  [132800/175341]\n",
      "loss: 0.384146  [134400/175341]\n",
      "loss: 0.576074  [136000/175341]\n",
      "loss: 0.421924  [137600/175341]\n",
      "loss: 0.372053  [139200/175341]\n",
      "loss: 0.517642  [140800/175341]\n",
      "loss: 0.721554  [142400/175341]\n",
      "loss: 1.101196  [144000/175341]\n",
      "loss: 1.022915  [145600/175341]\n",
      "loss: 0.612917  [147200/175341]\n",
      "loss: 0.624536  [148800/175341]\n",
      "loss: 0.316216  [150400/175341]\n",
      "loss: 0.412111  [152000/175341]\n",
      "loss: 0.498205  [153600/175341]\n",
      "loss: 0.291690  [155200/175341]\n",
      "loss: 0.407680  [156800/175341]\n",
      "loss: 0.668477  [158400/175341]\n",
      "loss: 0.168140  [160000/175341]\n",
      "loss: 0.451919  [161600/175341]\n",
      "loss: 0.407734  [163200/175341]\n",
      "loss: 0.199890  [164800/175341]\n",
      "loss: 0.377617  [166400/175341]\n",
      "loss: 0.283364  [168000/175341]\n",
      "loss: 0.758301  [169600/175341]\n",
      "loss: 0.439277  [171200/175341]\n",
      "loss: 0.899249  [172800/175341]\n",
      "loss: 0.663416  [174400/175341]\n",
      "Train Accuracy: 80.6896%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.603675, F1-score: 74.85%, Macro_F1-Score:  39.80%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.182323  [    0/175341]\n",
      "loss: 0.234011  [ 1600/175341]\n",
      "loss: 0.247688  [ 3200/175341]\n",
      "loss: 0.714315  [ 4800/175341]\n",
      "loss: 0.499546  [ 6400/175341]\n",
      "loss: 0.778543  [ 8000/175341]\n",
      "loss: 0.664335  [ 9600/175341]\n",
      "loss: 0.364120  [11200/175341]\n",
      "loss: 0.484986  [12800/175341]\n",
      "loss: 0.364677  [14400/175341]\n",
      "loss: 0.354410  [16000/175341]\n",
      "loss: 0.274685  [17600/175341]\n",
      "loss: 0.853559  [19200/175341]\n",
      "loss: 0.421382  [20800/175341]\n",
      "loss: 0.377113  [22400/175341]\n",
      "loss: 0.601577  [24000/175341]\n",
      "loss: 0.222889  [25600/175341]\n",
      "loss: 0.925275  [27200/175341]\n",
      "loss: 0.227776  [28800/175341]\n",
      "loss: 0.488733  [30400/175341]\n",
      "loss: 0.330877  [32000/175341]\n",
      "loss: 0.375724  [33600/175341]\n",
      "loss: 0.403034  [35200/175341]\n",
      "loss: 0.521614  [36800/175341]\n",
      "loss: 0.765842  [38400/175341]\n",
      "loss: 0.328552  [40000/175341]\n",
      "loss: 0.180124  [41600/175341]\n",
      "loss: 0.632544  [43200/175341]\n",
      "loss: 0.491134  [44800/175341]\n",
      "loss: 0.428560  [46400/175341]\n",
      "loss: 0.552185  [48000/175341]\n",
      "loss: 0.798769  [49600/175341]\n",
      "loss: 0.561401  [51200/175341]\n",
      "loss: 0.532111  [52800/175341]\n",
      "loss: 0.399892  [54400/175341]\n",
      "loss: 0.230307  [56000/175341]\n",
      "loss: 0.548303  [57600/175341]\n",
      "loss: 0.526833  [59200/175341]\n",
      "loss: 0.267446  [60800/175341]\n",
      "loss: 0.905983  [62400/175341]\n",
      "loss: 0.691892  [64000/175341]\n",
      "loss: 0.673878  [65600/175341]\n",
      "loss: 0.618472  [67200/175341]\n",
      "loss: 0.604872  [68800/175341]\n",
      "loss: 0.729396  [70400/175341]\n",
      "loss: 0.191012  [72000/175341]\n",
      "loss: 0.701589  [73600/175341]\n",
      "loss: 0.486474  [75200/175341]\n",
      "loss: 0.488265  [76800/175341]\n",
      "loss: 0.587037  [78400/175341]\n",
      "loss: 0.246878  [80000/175341]\n",
      "loss: 0.439450  [81600/175341]\n",
      "loss: 0.281709  [83200/175341]\n",
      "loss: 0.486822  [84800/175341]\n",
      "loss: 0.442033  [86400/175341]\n",
      "loss: 0.669854  [88000/175341]\n",
      "loss: 0.269012  [89600/175341]\n",
      "loss: 0.350970  [91200/175341]\n",
      "loss: 0.608699  [92800/175341]\n",
      "loss: 0.569401  [94400/175341]\n",
      "loss: 0.357303  [96000/175341]\n",
      "loss: 0.762603  [97600/175341]\n",
      "loss: 0.622547  [99200/175341]\n",
      "loss: 0.909226  [100800/175341]\n",
      "loss: 0.575686  [102400/175341]\n",
      "loss: 0.409665  [104000/175341]\n",
      "loss: 0.286014  [105600/175341]\n",
      "loss: 0.315670  [107200/175341]\n",
      "loss: 0.175741  [108800/175341]\n",
      "loss: 0.434941  [110400/175341]\n",
      "loss: 0.146977  [112000/175341]\n",
      "loss: 0.660932  [113600/175341]\n",
      "loss: 0.425855  [115200/175341]\n",
      "loss: 0.297698  [116800/175341]\n",
      "loss: 0.646704  [118400/175341]\n",
      "loss: 0.471597  [120000/175341]\n",
      "loss: 0.601837  [121600/175341]\n",
      "loss: 0.537913  [123200/175341]\n",
      "loss: 0.671692  [124800/175341]\n",
      "loss: 1.024535  [126400/175341]\n",
      "loss: 0.149251  [128000/175341]\n",
      "loss: 0.180130  [129600/175341]\n",
      "loss: 0.345187  [131200/175341]\n",
      "loss: 0.451908  [132800/175341]\n",
      "loss: 0.677850  [134400/175341]\n",
      "loss: 1.077109  [136000/175341]\n",
      "loss: 0.064909  [137600/175341]\n",
      "loss: 0.788912  [139200/175341]\n",
      "loss: 0.324242  [140800/175341]\n",
      "loss: 0.336177  [142400/175341]\n",
      "loss: 0.464055  [144000/175341]\n",
      "loss: 0.648646  [145600/175341]\n",
      "loss: 0.467509  [147200/175341]\n",
      "loss: 0.786364  [148800/175341]\n",
      "loss: 0.383129  [150400/175341]\n",
      "loss: 0.173727  [152000/175341]\n",
      "loss: 0.410714  [153600/175341]\n",
      "loss: 0.252264  [155200/175341]\n",
      "loss: 0.555857  [156800/175341]\n",
      "loss: 0.781293  [158400/175341]\n",
      "loss: 0.633913  [160000/175341]\n",
      "loss: 0.713550  [161600/175341]\n",
      "loss: 0.256754  [163200/175341]\n",
      "loss: 0.604697  [164800/175341]\n",
      "loss: 0.584527  [166400/175341]\n",
      "loss: 0.719917  [168000/175341]\n",
      "loss: 0.315853  [169600/175341]\n",
      "loss: 0.397491  [171200/175341]\n",
      "loss: 0.285292  [172800/175341]\n",
      "loss: 0.309510  [174400/175341]\n",
      "Train Accuracy: 80.7176%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.580395, F1-score: 75.78%, Macro_F1-Score:  40.22%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.840964  [    0/175341]\n",
      "loss: 0.430676  [ 1600/175341]\n",
      "loss: 0.229243  [ 3200/175341]\n",
      "loss: 0.363616  [ 4800/175341]\n",
      "loss: 0.389396  [ 6400/175341]\n",
      "loss: 0.723894  [ 8000/175341]\n",
      "loss: 0.822220  [ 9600/175341]\n",
      "loss: 0.458544  [11200/175341]\n",
      "loss: 0.422150  [12800/175341]\n",
      "loss: 0.542937  [14400/175341]\n",
      "loss: 0.109904  [16000/175341]\n",
      "loss: 0.329168  [17600/175341]\n",
      "loss: 0.479550  [19200/175341]\n",
      "loss: 0.340767  [20800/175341]\n",
      "loss: 0.413030  [22400/175341]\n",
      "loss: 0.413251  [24000/175341]\n",
      "loss: 0.449666  [25600/175341]\n",
      "loss: 0.344721  [27200/175341]\n",
      "loss: 0.550668  [28800/175341]\n",
      "loss: 0.564329  [30400/175341]\n",
      "loss: 0.502627  [32000/175341]\n",
      "loss: 0.231998  [33600/175341]\n",
      "loss: 0.380098  [35200/175341]\n",
      "loss: 0.277463  [36800/175341]\n",
      "loss: 0.742026  [38400/175341]\n",
      "loss: 0.454832  [40000/175341]\n",
      "loss: 0.494024  [41600/175341]\n",
      "loss: 0.641942  [43200/175341]\n",
      "loss: 0.729947  [44800/175341]\n",
      "loss: 0.632416  [46400/175341]\n",
      "loss: 0.382055  [48000/175341]\n",
      "loss: 0.656173  [49600/175341]\n",
      "loss: 0.257617  [51200/175341]\n",
      "loss: 0.365741  [52800/175341]\n",
      "loss: 0.310768  [54400/175341]\n",
      "loss: 0.552537  [56000/175341]\n",
      "loss: 0.480210  [57600/175341]\n",
      "loss: 0.939065  [59200/175341]\n",
      "loss: 0.222741  [60800/175341]\n",
      "loss: 0.719268  [62400/175341]\n",
      "loss: 0.519347  [64000/175341]\n",
      "loss: 0.652659  [65600/175341]\n",
      "loss: 0.745692  [67200/175341]\n",
      "loss: 0.345981  [68800/175341]\n",
      "loss: 0.733492  [70400/175341]\n",
      "loss: 0.210500  [72000/175341]\n",
      "loss: 0.670082  [73600/175341]\n",
      "loss: 0.658299  [75200/175341]\n",
      "loss: 0.293574  [76800/175341]\n",
      "loss: 0.231781  [78400/175341]\n",
      "loss: 0.481692  [80000/175341]\n",
      "loss: 0.579413  [81600/175341]\n",
      "loss: 0.338371  [83200/175341]\n",
      "loss: 0.683145  [84800/175341]\n",
      "loss: 0.254000  [86400/175341]\n",
      "loss: 0.510537  [88000/175341]\n",
      "loss: 0.495058  [89600/175341]\n",
      "loss: 0.506256  [91200/175341]\n",
      "loss: 0.189741  [92800/175341]\n",
      "loss: 0.451511  [94400/175341]\n",
      "loss: 0.495533  [96000/175341]\n",
      "loss: 0.586829  [97600/175341]\n",
      "loss: 0.490036  [99200/175341]\n",
      "loss: 0.307651  [100800/175341]\n",
      "loss: 0.594032  [102400/175341]\n",
      "loss: 0.048140  [104000/175341]\n",
      "loss: 0.639236  [105600/175341]\n",
      "loss: 0.503212  [107200/175341]\n",
      "loss: 0.286855  [108800/175341]\n",
      "loss: 0.397251  [110400/175341]\n",
      "loss: 0.335487  [112000/175341]\n",
      "loss: 0.295695  [113600/175341]\n",
      "loss: 0.545203  [115200/175341]\n",
      "loss: 0.428740  [116800/175341]\n",
      "loss: 0.358741  [118400/175341]\n",
      "loss: 0.637921  [120000/175341]\n",
      "loss: 0.375665  [121600/175341]\n",
      "loss: 0.235056  [123200/175341]\n",
      "loss: 0.654730  [124800/175341]\n",
      "loss: 0.220754  [126400/175341]\n",
      "loss: 0.619293  [128000/175341]\n",
      "loss: 0.738866  [129600/175341]\n",
      "loss: 0.292161  [131200/175341]\n",
      "loss: 0.922512  [132800/175341]\n",
      "loss: 0.581989  [134400/175341]\n",
      "loss: 0.391691  [136000/175341]\n",
      "loss: 0.278237  [137600/175341]\n",
      "loss: 0.491718  [139200/175341]\n",
      "loss: 0.301748  [140800/175341]\n",
      "loss: 0.397885  [142400/175341]\n",
      "loss: 0.321328  [144000/175341]\n",
      "loss: 0.450568  [145600/175341]\n",
      "loss: 0.459141  [147200/175341]\n",
      "loss: 0.147153  [148800/175341]\n",
      "loss: 0.293182  [150400/175341]\n",
      "loss: 0.395313  [152000/175341]\n",
      "loss: 0.210854  [153600/175341]\n",
      "loss: 0.454512  [155200/175341]\n",
      "loss: 0.244710  [156800/175341]\n",
      "loss: 0.366749  [158400/175341]\n",
      "loss: 0.360198  [160000/175341]\n",
      "loss: 0.503967  [161600/175341]\n",
      "loss: 0.733178  [163200/175341]\n",
      "loss: 0.418138  [164800/175341]\n",
      "loss: 0.355285  [166400/175341]\n",
      "loss: 1.120114  [168000/175341]\n",
      "loss: 0.930234  [169600/175341]\n",
      "loss: 0.457789  [171200/175341]\n",
      "loss: 0.695888  [172800/175341]\n",
      "loss: 0.419404  [174400/175341]\n",
      "Train Accuracy: 80.6925%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.562637, F1-score: 76.75%, Macro_F1-Score:  41.06%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.314534  [    0/175341]\n",
      "loss: 0.660178  [ 1600/175341]\n",
      "loss: 0.364248  [ 3200/175341]\n",
      "loss: 0.292879  [ 4800/175341]\n",
      "loss: 0.808397  [ 6400/175341]\n",
      "loss: 0.185541  [ 8000/175341]\n",
      "loss: 0.975911  [ 9600/175341]\n",
      "loss: 0.159553  [11200/175341]\n",
      "loss: 0.606161  [12800/175341]\n",
      "loss: 0.703520  [14400/175341]\n",
      "loss: 0.401546  [16000/175341]\n",
      "loss: 0.322359  [17600/175341]\n",
      "loss: 0.397747  [19200/175341]\n",
      "loss: 0.657756  [20800/175341]\n",
      "loss: 0.171880  [22400/175341]\n",
      "loss: 0.456950  [24000/175341]\n",
      "loss: 0.352047  [25600/175341]\n",
      "loss: 0.205298  [27200/175341]\n",
      "loss: 0.202334  [28800/175341]\n",
      "loss: 0.149671  [30400/175341]\n",
      "loss: 0.294930  [32000/175341]\n",
      "loss: 0.341837  [33600/175341]\n",
      "loss: 0.351370  [35200/175341]\n",
      "loss: 0.239062  [36800/175341]\n",
      "loss: 0.155691  [38400/175341]\n",
      "loss: 0.309211  [40000/175341]\n",
      "loss: 0.211010  [41600/175341]\n",
      "loss: 0.523809  [43200/175341]\n",
      "loss: 1.053521  [44800/175341]\n",
      "loss: 0.391114  [46400/175341]\n",
      "loss: 1.159216  [48000/175341]\n",
      "loss: 0.278832  [49600/175341]\n",
      "loss: 0.289141  [51200/175341]\n",
      "loss: 0.400513  [52800/175341]\n",
      "loss: 0.404454  [54400/175341]\n",
      "loss: 0.309473  [56000/175341]\n",
      "loss: 0.442005  [57600/175341]\n",
      "loss: 0.363127  [59200/175341]\n",
      "loss: 0.706898  [60800/175341]\n",
      "loss: 0.592790  [62400/175341]\n",
      "loss: 0.629605  [64000/175341]\n",
      "loss: 0.839774  [65600/175341]\n",
      "loss: 0.217808  [67200/175341]\n",
      "loss: 0.600271  [68800/175341]\n",
      "loss: 0.688052  [70400/175341]\n",
      "loss: 0.900952  [72000/175341]\n",
      "loss: 0.775526  [73600/175341]\n",
      "loss: 0.416401  [75200/175341]\n",
      "loss: 0.153592  [76800/175341]\n",
      "loss: 0.265096  [78400/175341]\n",
      "loss: 0.410360  [80000/175341]\n",
      "loss: 0.425081  [81600/175341]\n",
      "loss: 0.344809  [83200/175341]\n",
      "loss: 0.797747  [84800/175341]\n",
      "loss: 0.284213  [86400/175341]\n",
      "loss: 0.501479  [88000/175341]\n",
      "loss: 0.611992  [89600/175341]\n",
      "loss: 0.364964  [91200/175341]\n",
      "loss: 0.279844  [92800/175341]\n",
      "loss: 0.527762  [94400/175341]\n",
      "loss: 0.244957  [96000/175341]\n",
      "loss: 0.156443  [97600/175341]\n",
      "loss: 0.670717  [99200/175341]\n",
      "loss: 0.541969  [100800/175341]\n",
      "loss: 0.413438  [102400/175341]\n",
      "loss: 0.518546  [104000/175341]\n",
      "loss: 0.950620  [105600/175341]\n",
      "loss: 0.393801  [107200/175341]\n",
      "loss: 0.533408  [108800/175341]\n",
      "loss: 0.617219  [110400/175341]\n",
      "loss: 0.266390  [112000/175341]\n",
      "loss: 0.680029  [113600/175341]\n",
      "loss: 0.142344  [115200/175341]\n",
      "loss: 0.719830  [116800/175341]\n",
      "loss: 0.204279  [118400/175341]\n",
      "loss: 0.313827  [120000/175341]\n",
      "loss: 0.437480  [121600/175341]\n",
      "loss: 0.449056  [123200/175341]\n",
      "loss: 0.242171  [124800/175341]\n",
      "loss: 0.406332  [126400/175341]\n",
      "loss: 0.562098  [128000/175341]\n",
      "loss: 0.487686  [129600/175341]\n",
      "loss: 0.858992  [131200/175341]\n",
      "loss: 0.298985  [132800/175341]\n",
      "loss: 0.443877  [134400/175341]\n",
      "loss: 0.455052  [136000/175341]\n",
      "loss: 0.498708  [137600/175341]\n",
      "loss: 0.570665  [139200/175341]\n",
      "loss: 0.472798  [140800/175341]\n",
      "loss: 0.653488  [142400/175341]\n",
      "loss: 0.274222  [144000/175341]\n",
      "loss: 0.148101  [145600/175341]\n",
      "loss: 0.496390  [147200/175341]\n",
      "loss: 0.839381  [148800/175341]\n",
      "loss: 0.448269  [150400/175341]\n",
      "loss: 0.371532  [152000/175341]\n",
      "loss: 0.183503  [153600/175341]\n",
      "loss: 0.371600  [155200/175341]\n",
      "loss: 0.497860  [156800/175341]\n",
      "loss: 0.678806  [158400/175341]\n",
      "loss: 0.452734  [160000/175341]\n",
      "loss: 0.445884  [161600/175341]\n",
      "loss: 0.679195  [163200/175341]\n",
      "loss: 0.397148  [164800/175341]\n",
      "loss: 0.602442  [166400/175341]\n",
      "loss: 0.542961  [168000/175341]\n",
      "loss: 0.213085  [169600/175341]\n",
      "loss: 0.500402  [171200/175341]\n",
      "loss: 0.194730  [172800/175341]\n",
      "loss: 0.700345  [174400/175341]\n",
      "Train Accuracy: 80.7107%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.622509, F1-score: 74.28%, Macro_F1-Score:  38.41%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.599222  [    0/175341]\n",
      "loss: 0.715256  [ 1600/175341]\n",
      "loss: 0.317986  [ 3200/175341]\n",
      "loss: 0.508688  [ 4800/175341]\n",
      "loss: 0.408749  [ 6400/175341]\n",
      "loss: 0.318382  [ 8000/175341]\n",
      "loss: 0.682672  [ 9600/175341]\n",
      "loss: 0.902170  [11200/175341]\n",
      "loss: 0.251526  [12800/175341]\n",
      "loss: 0.533270  [14400/175341]\n",
      "loss: 0.317017  [16000/175341]\n",
      "loss: 0.684543  [17600/175341]\n",
      "loss: 0.370851  [19200/175341]\n",
      "loss: 0.508177  [20800/175341]\n",
      "loss: 0.505410  [22400/175341]\n",
      "loss: 0.678765  [24000/175341]\n",
      "loss: 0.443074  [25600/175341]\n",
      "loss: 1.547229  [27200/175341]\n",
      "loss: 0.504835  [28800/175341]\n",
      "loss: 0.621880  [30400/175341]\n",
      "loss: 0.233402  [32000/175341]\n",
      "loss: 0.283480  [33600/175341]\n",
      "loss: 0.255779  [35200/175341]\n",
      "loss: 0.453378  [36800/175341]\n",
      "loss: 1.169605  [38400/175341]\n",
      "loss: 0.347974  [40000/175341]\n",
      "loss: 0.760046  [41600/175341]\n",
      "loss: 0.341239  [43200/175341]\n",
      "loss: 0.667643  [44800/175341]\n",
      "loss: 0.644171  [46400/175341]\n",
      "loss: 0.468179  [48000/175341]\n",
      "loss: 1.067725  [49600/175341]\n",
      "loss: 0.822051  [51200/175341]\n",
      "loss: 0.337832  [52800/175341]\n",
      "loss: 0.607322  [54400/175341]\n",
      "loss: 0.315458  [56000/175341]\n",
      "loss: 0.348031  [57600/175341]\n",
      "loss: 0.241063  [59200/175341]\n",
      "loss: 0.207376  [60800/175341]\n",
      "loss: 0.865297  [62400/175341]\n",
      "loss: 0.283099  [64000/175341]\n",
      "loss: 0.400850  [65600/175341]\n",
      "loss: 0.895385  [67200/175341]\n",
      "loss: 0.519311  [68800/175341]\n",
      "loss: 0.451350  [70400/175341]\n",
      "loss: 1.175889  [72000/175341]\n",
      "loss: 0.426947  [73600/175341]\n",
      "loss: 0.206448  [75200/175341]\n",
      "loss: 0.830750  [76800/175341]\n",
      "loss: 0.231065  [78400/175341]\n",
      "loss: 0.496420  [80000/175341]\n",
      "loss: 0.849667  [81600/175341]\n",
      "loss: 0.275062  [83200/175341]\n",
      "loss: 0.303734  [84800/175341]\n",
      "loss: 0.821308  [86400/175341]\n",
      "loss: 0.338018  [88000/175341]\n",
      "loss: 0.288376  [89600/175341]\n",
      "loss: 0.429039  [91200/175341]\n",
      "loss: 0.401968  [92800/175341]\n",
      "loss: 0.741394  [94400/175341]\n",
      "loss: 0.347819  [96000/175341]\n",
      "loss: 0.427946  [97600/175341]\n",
      "loss: 0.499084  [99200/175341]\n",
      "loss: 0.168742  [100800/175341]\n",
      "loss: 0.519154  [102400/175341]\n",
      "loss: 0.476667  [104000/175341]\n",
      "loss: 0.929743  [105600/175341]\n",
      "loss: 0.664828  [107200/175341]\n",
      "loss: 0.345237  [108800/175341]\n",
      "loss: 0.996328  [110400/175341]\n",
      "loss: 0.462812  [112000/175341]\n",
      "loss: 0.692903  [113600/175341]\n",
      "loss: 0.486016  [115200/175341]\n",
      "loss: 0.888011  [116800/175341]\n",
      "loss: 0.351631  [118400/175341]\n",
      "loss: 0.291351  [120000/175341]\n",
      "loss: 0.702974  [121600/175341]\n",
      "loss: 0.223362  [123200/175341]\n",
      "loss: 0.405743  [124800/175341]\n",
      "loss: 1.003537  [126400/175341]\n",
      "loss: 0.366300  [128000/175341]\n",
      "loss: 0.642604  [129600/175341]\n",
      "loss: 0.193995  [131200/175341]\n",
      "loss: 0.380841  [132800/175341]\n",
      "loss: 0.328433  [134400/175341]\n",
      "loss: 0.528658  [136000/175341]\n",
      "loss: 0.734122  [137600/175341]\n",
      "loss: 0.335900  [139200/175341]\n",
      "loss: 0.218162  [140800/175341]\n",
      "loss: 0.354587  [142400/175341]\n",
      "loss: 0.958554  [144000/175341]\n",
      "loss: 0.505726  [145600/175341]\n",
      "loss: 0.404100  [147200/175341]\n",
      "loss: 0.472533  [148800/175341]\n",
      "loss: 0.669719  [150400/175341]\n",
      "loss: 0.516498  [152000/175341]\n",
      "loss: 0.544360  [153600/175341]\n",
      "loss: 0.374682  [155200/175341]\n",
      "loss: 0.435344  [156800/175341]\n",
      "loss: 0.840741  [158400/175341]\n",
      "loss: 0.129922  [160000/175341]\n",
      "loss: 0.267423  [161600/175341]\n",
      "loss: 0.387571  [163200/175341]\n",
      "loss: 0.374350  [164800/175341]\n",
      "loss: 0.266406  [166400/175341]\n",
      "loss: 0.686118  [168000/175341]\n",
      "loss: 0.463723  [169600/175341]\n",
      "loss: 0.756581  [171200/175341]\n",
      "loss: 0.465320  [172800/175341]\n",
      "loss: 0.849067  [174400/175341]\n",
      "Train Accuracy: 80.7507%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.588709, F1-score: 75.47%, Macro_F1-Score:  39.87%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.366000  [    0/175341]\n",
      "loss: 0.349414  [ 1600/175341]\n",
      "loss: 0.615299  [ 3200/175341]\n",
      "loss: 0.343699  [ 4800/175341]\n",
      "loss: 0.505198  [ 6400/175341]\n",
      "loss: 0.753180  [ 8000/175341]\n",
      "loss: 0.609274  [ 9600/175341]\n",
      "loss: 0.404908  [11200/175341]\n",
      "loss: 0.832932  [12800/175341]\n",
      "loss: 0.557666  [14400/175341]\n",
      "loss: 0.506887  [16000/175341]\n",
      "loss: 0.562292  [17600/175341]\n",
      "loss: 0.860891  [19200/175341]\n",
      "loss: 0.606455  [20800/175341]\n",
      "loss: 0.806587  [22400/175341]\n",
      "loss: 0.356915  [24000/175341]\n",
      "loss: 0.123292  [25600/175341]\n",
      "loss: 0.594827  [27200/175341]\n",
      "loss: 0.479242  [28800/175341]\n",
      "loss: 0.446100  [30400/175341]\n",
      "loss: 0.762533  [32000/175341]\n",
      "loss: 0.607310  [33600/175341]\n",
      "loss: 0.502178  [35200/175341]\n",
      "loss: 0.518682  [36800/175341]\n",
      "loss: 0.427830  [38400/175341]\n",
      "loss: 0.689413  [40000/175341]\n",
      "loss: 0.193516  [41600/175341]\n",
      "loss: 0.691380  [43200/175341]\n",
      "loss: 0.527475  [44800/175341]\n",
      "loss: 0.333348  [46400/175341]\n",
      "loss: 1.121816  [48000/175341]\n",
      "loss: 0.558734  [49600/175341]\n",
      "loss: 0.636321  [51200/175341]\n",
      "loss: 0.323265  [52800/175341]\n",
      "loss: 0.850072  [54400/175341]\n",
      "loss: 1.016580  [56000/175341]\n",
      "loss: 0.389931  [57600/175341]\n",
      "loss: 0.746021  [59200/175341]\n",
      "loss: 0.093816  [60800/175341]\n",
      "loss: 0.488847  [62400/175341]\n",
      "loss: 0.361475  [64000/175341]\n",
      "loss: 0.233596  [65600/175341]\n",
      "loss: 0.521509  [67200/175341]\n",
      "loss: 0.396805  [68800/175341]\n",
      "loss: 1.238615  [70400/175341]\n",
      "loss: 0.451516  [72000/175341]\n",
      "loss: 0.402122  [73600/175341]\n",
      "loss: 0.348914  [75200/175341]\n",
      "loss: 0.355223  [76800/175341]\n",
      "loss: 0.395304  [78400/175341]\n",
      "loss: 0.722324  [80000/175341]\n",
      "loss: 0.257194  [81600/175341]\n",
      "loss: 0.643802  [83200/175341]\n",
      "loss: 0.551852  [84800/175341]\n",
      "loss: 0.812223  [86400/175341]\n",
      "loss: 0.370748  [88000/175341]\n",
      "loss: 0.362663  [89600/175341]\n",
      "loss: 0.335244  [91200/175341]\n",
      "loss: 0.289512  [92800/175341]\n",
      "loss: 0.679706  [94400/175341]\n",
      "loss: 0.626555  [96000/175341]\n",
      "loss: 0.630646  [97600/175341]\n",
      "loss: 0.556980  [99200/175341]\n",
      "loss: 0.252754  [100800/175341]\n",
      "loss: 0.954637  [102400/175341]\n",
      "loss: 0.385137  [104000/175341]\n",
      "loss: 0.592158  [105600/175341]\n",
      "loss: 0.603678  [107200/175341]\n",
      "loss: 0.527312  [108800/175341]\n",
      "loss: 0.528536  [110400/175341]\n",
      "loss: 0.719169  [112000/175341]\n",
      "loss: 0.222338  [113600/175341]\n",
      "loss: 1.197530  [115200/175341]\n",
      "loss: 0.302405  [116800/175341]\n",
      "loss: 0.689714  [118400/175341]\n",
      "loss: 0.483006  [120000/175341]\n",
      "loss: 0.754564  [121600/175341]\n",
      "loss: 0.369614  [123200/175341]\n",
      "loss: 0.488112  [124800/175341]\n",
      "loss: 0.525035  [126400/175341]\n",
      "loss: 0.614899  [128000/175341]\n",
      "loss: 0.579245  [129600/175341]\n",
      "loss: 0.597182  [131200/175341]\n",
      "loss: 0.455559  [132800/175341]\n",
      "loss: 0.389561  [134400/175341]\n",
      "loss: 0.401731  [136000/175341]\n",
      "loss: 0.157849  [137600/175341]\n",
      "loss: 0.566230  [139200/175341]\n",
      "loss: 0.825889  [140800/175341]\n",
      "loss: 0.485097  [142400/175341]\n",
      "loss: 0.071836  [144000/175341]\n",
      "loss: 0.393242  [145600/175341]\n",
      "loss: 0.708892  [147200/175341]\n",
      "loss: 0.503817  [148800/175341]\n",
      "loss: 0.453195  [150400/175341]\n",
      "loss: 0.357644  [152000/175341]\n",
      "loss: 0.499194  [153600/175341]\n",
      "loss: 0.521235  [155200/175341]\n",
      "loss: 0.424987  [156800/175341]\n",
      "loss: 0.649171  [158400/175341]\n",
      "loss: 0.600483  [160000/175341]\n",
      "loss: 0.399846  [161600/175341]\n",
      "loss: 1.056989  [163200/175341]\n",
      "loss: 0.683224  [164800/175341]\n",
      "loss: 0.458866  [166400/175341]\n",
      "loss: 0.888967  [168000/175341]\n",
      "loss: 0.299025  [169600/175341]\n",
      "loss: 0.308493  [171200/175341]\n",
      "loss: 0.491417  [172800/175341]\n",
      "loss: 0.332690  [174400/175341]\n",
      "Train Accuracy: 80.3708%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.677246, F1-score: 73.62%, Macro_F1-Score:  36.98%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.430790  [    0/175341]\n",
      "loss: 0.579761  [ 1600/175341]\n",
      "loss: 0.567686  [ 3200/175341]\n",
      "loss: 0.546590  [ 4800/175341]\n",
      "loss: 0.491771  [ 6400/175341]\n",
      "loss: 0.878378  [ 8000/175341]\n",
      "loss: 0.519236  [ 9600/175341]\n",
      "loss: 0.324460  [11200/175341]\n",
      "loss: 0.411070  [12800/175341]\n",
      "loss: 0.236569  [14400/175341]\n",
      "loss: 0.751728  [16000/175341]\n",
      "loss: 0.229638  [17600/175341]\n",
      "loss: 0.214319  [19200/175341]\n",
      "loss: 0.386088  [20800/175341]\n",
      "loss: 0.128535  [22400/175341]\n",
      "loss: 0.330615  [24000/175341]\n",
      "loss: 0.392795  [25600/175341]\n",
      "loss: 0.527682  [27200/175341]\n",
      "loss: 0.493358  [28800/175341]\n",
      "loss: 0.241169  [30400/175341]\n",
      "loss: 0.688499  [32000/175341]\n",
      "loss: 0.299341  [33600/175341]\n",
      "loss: 0.549697  [35200/175341]\n",
      "loss: 0.309016  [36800/175341]\n",
      "loss: 1.269702  [38400/175341]\n",
      "loss: 0.656039  [40000/175341]\n",
      "loss: 0.457879  [41600/175341]\n",
      "loss: 0.415373  [43200/175341]\n",
      "loss: 0.735473  [44800/175341]\n",
      "loss: 0.692596  [46400/175341]\n",
      "loss: 0.482920  [48000/175341]\n",
      "loss: 0.331168  [49600/175341]\n",
      "loss: 0.558358  [51200/175341]\n",
      "loss: 0.436177  [52800/175341]\n",
      "loss: 0.270766  [54400/175341]\n",
      "loss: 0.443646  [56000/175341]\n",
      "loss: 0.399944  [57600/175341]\n",
      "loss: 0.208040  [59200/175341]\n",
      "loss: 0.812406  [60800/175341]\n",
      "loss: 0.625212  [62400/175341]\n",
      "loss: 0.469059  [64000/175341]\n",
      "loss: 0.194674  [65600/175341]\n",
      "loss: 1.041779  [67200/175341]\n",
      "loss: 0.618197  [68800/175341]\n",
      "loss: 0.505917  [70400/175341]\n",
      "loss: 0.401757  [72000/175341]\n",
      "loss: 0.692947  [73600/175341]\n",
      "loss: 0.359733  [75200/175341]\n",
      "loss: 0.768379  [76800/175341]\n",
      "loss: 0.604291  [78400/175341]\n",
      "loss: 0.453539  [80000/175341]\n",
      "loss: 0.468127  [81600/175341]\n",
      "loss: 0.905002  [83200/175341]\n",
      "loss: 0.584635  [84800/175341]\n",
      "loss: 0.113080  [86400/175341]\n",
      "loss: 0.293515  [88000/175341]\n",
      "loss: 0.156077  [89600/175341]\n",
      "loss: 0.667872  [91200/175341]\n",
      "loss: 0.663926  [92800/175341]\n",
      "loss: 0.575892  [94400/175341]\n",
      "loss: 0.666488  [96000/175341]\n",
      "loss: 0.396357  [97600/175341]\n",
      "loss: 0.426160  [99200/175341]\n",
      "loss: 0.267566  [100800/175341]\n",
      "loss: 0.365298  [102400/175341]\n",
      "loss: 0.339268  [104000/175341]\n",
      "loss: 0.821888  [105600/175341]\n",
      "loss: 0.221525  [107200/175341]\n",
      "loss: 0.378012  [108800/175341]\n",
      "loss: 0.201680  [110400/175341]\n",
      "loss: 0.846149  [112000/175341]\n",
      "loss: 0.476348  [113600/175341]\n",
      "loss: 0.538045  [115200/175341]\n",
      "loss: 0.680620  [116800/175341]\n",
      "loss: 0.562440  [118400/175341]\n",
      "loss: 0.193669  [120000/175341]\n",
      "loss: 0.449487  [121600/175341]\n",
      "loss: 0.569097  [123200/175341]\n",
      "loss: 0.375746  [124800/175341]\n",
      "loss: 0.470464  [126400/175341]\n",
      "loss: 0.536205  [128000/175341]\n",
      "loss: 0.455794  [129600/175341]\n",
      "loss: 0.521942  [131200/175341]\n",
      "loss: 0.701757  [132800/175341]\n",
      "loss: 0.986212  [134400/175341]\n",
      "loss: 0.573436  [136000/175341]\n",
      "loss: 0.707492  [137600/175341]\n",
      "loss: 0.562190  [139200/175341]\n",
      "loss: 0.527612  [140800/175341]\n",
      "loss: 0.404595  [142400/175341]\n",
      "loss: 0.273739  [144000/175341]\n",
      "loss: 0.392572  [145600/175341]\n",
      "loss: 0.674893  [147200/175341]\n",
      "loss: 0.333078  [148800/175341]\n",
      "loss: 0.385837  [150400/175341]\n",
      "loss: 0.399160  [152000/175341]\n",
      "loss: 0.695257  [153600/175341]\n",
      "loss: 0.435816  [155200/175341]\n",
      "loss: 0.631782  [156800/175341]\n",
      "loss: 0.832209  [158400/175341]\n",
      "loss: 0.683473  [160000/175341]\n",
      "loss: 0.912317  [161600/175341]\n",
      "loss: 0.256539  [163200/175341]\n",
      "loss: 0.221776  [164800/175341]\n",
      "loss: 0.437104  [166400/175341]\n",
      "loss: 0.401875  [168000/175341]\n",
      "loss: 0.433251  [169600/175341]\n",
      "loss: 0.786593  [171200/175341]\n",
      "loss: 0.454415  [172800/175341]\n",
      "loss: 0.111971  [174400/175341]\n",
      "Train Accuracy: 80.3201%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.579294, F1-score: 75.84%, Macro_F1-Score:  39.70%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.237749  [    0/175341]\n",
      "loss: 1.153493  [ 1600/175341]\n",
      "loss: 0.540457  [ 3200/175341]\n",
      "loss: 0.563764  [ 4800/175341]\n",
      "loss: 0.593688  [ 6400/175341]\n",
      "loss: 0.494528  [ 8000/175341]\n",
      "loss: 0.577951  [ 9600/175341]\n",
      "loss: 0.249447  [11200/175341]\n",
      "loss: 0.698277  [12800/175341]\n",
      "loss: 0.578427  [14400/175341]\n",
      "loss: 0.809984  [16000/175341]\n",
      "loss: 0.630990  [17600/175341]\n",
      "loss: 0.360774  [19200/175341]\n",
      "loss: 0.737219  [20800/175341]\n",
      "loss: 0.307806  [22400/175341]\n",
      "loss: 0.319598  [24000/175341]\n",
      "loss: 0.179365  [25600/175341]\n",
      "loss: 0.547063  [27200/175341]\n",
      "loss: 0.277612  [28800/175341]\n",
      "loss: 0.487503  [30400/175341]\n",
      "loss: 0.406738  [32000/175341]\n",
      "loss: 0.110578  [33600/175341]\n",
      "loss: 0.511924  [35200/175341]\n",
      "loss: 0.744526  [36800/175341]\n",
      "loss: 0.708963  [38400/175341]\n",
      "loss: 0.110160  [40000/175341]\n",
      "loss: 0.516859  [41600/175341]\n",
      "loss: 0.383728  [43200/175341]\n",
      "loss: 0.511563  [44800/175341]\n",
      "loss: 0.551194  [46400/175341]\n",
      "loss: 0.877760  [48000/175341]\n",
      "loss: 0.450822  [49600/175341]\n",
      "loss: 0.289578  [51200/175341]\n",
      "loss: 0.730290  [52800/175341]\n",
      "loss: 0.770027  [54400/175341]\n",
      "loss: 0.300890  [56000/175341]\n",
      "loss: 0.426622  [57600/175341]\n",
      "loss: 0.539841  [59200/175341]\n",
      "loss: 0.586401  [60800/175341]\n",
      "loss: 0.361442  [62400/175341]\n",
      "loss: 0.518910  [64000/175341]\n",
      "loss: 0.346816  [65600/175341]\n",
      "loss: 0.419469  [67200/175341]\n",
      "loss: 0.177663  [68800/175341]\n",
      "loss: 0.442106  [70400/175341]\n",
      "loss: 0.502899  [72000/175341]\n",
      "loss: 0.381736  [73600/175341]\n",
      "loss: 0.711197  [75200/175341]\n",
      "loss: 0.423922  [76800/175341]\n",
      "loss: 0.483893  [78400/175341]\n",
      "loss: 0.807993  [80000/175341]\n",
      "loss: 0.416018  [81600/175341]\n",
      "loss: 0.517797  [83200/175341]\n",
      "loss: 0.641476  [84800/175341]\n",
      "loss: 0.612756  [86400/175341]\n",
      "loss: 0.575441  [88000/175341]\n",
      "loss: 0.460912  [89600/175341]\n",
      "loss: 1.164955  [91200/175341]\n",
      "loss: 0.340700  [92800/175341]\n",
      "loss: 0.395100  [94400/175341]\n",
      "loss: 0.337178  [96000/175341]\n",
      "loss: 0.793681  [97600/175341]\n",
      "loss: 0.264268  [99200/175341]\n",
      "loss: 0.848479  [100800/175341]\n",
      "loss: 0.505478  [102400/175341]\n",
      "loss: 0.594694  [104000/175341]\n",
      "loss: 0.594051  [105600/175341]\n",
      "loss: 0.611111  [107200/175341]\n",
      "loss: 0.379800  [108800/175341]\n",
      "loss: 0.909576  [110400/175341]\n",
      "loss: 0.538954  [112000/175341]\n",
      "loss: 0.164638  [113600/175341]\n",
      "loss: 0.668833  [115200/175341]\n",
      "loss: 0.407323  [116800/175341]\n",
      "loss: 0.499735  [118400/175341]\n",
      "loss: 0.785599  [120000/175341]\n",
      "loss: 0.646657  [121600/175341]\n",
      "loss: 0.849510  [123200/175341]\n",
      "loss: 0.311743  [124800/175341]\n",
      "loss: 0.410144  [126400/175341]\n",
      "loss: 0.721215  [128000/175341]\n",
      "loss: 0.323268  [129600/175341]\n",
      "loss: 0.257150  [131200/175341]\n",
      "loss: 0.499964  [132800/175341]\n",
      "loss: 0.264371  [134400/175341]\n",
      "loss: 0.221836  [136000/175341]\n",
      "loss: 0.591034  [137600/175341]\n",
      "loss: 0.560918  [139200/175341]\n",
      "loss: 0.591208  [140800/175341]\n",
      "loss: 0.394651  [142400/175341]\n",
      "loss: 0.465613  [144000/175341]\n",
      "loss: 0.577683  [145600/175341]\n",
      "loss: 0.249001  [147200/175341]\n",
      "loss: 0.455041  [148800/175341]\n",
      "loss: 0.529447  [150400/175341]\n",
      "loss: 0.051262  [152000/175341]\n",
      "loss: 0.669444  [153600/175341]\n",
      "loss: 0.333521  [155200/175341]\n",
      "loss: 0.843730  [156800/175341]\n",
      "loss: 0.543479  [158400/175341]\n",
      "loss: 0.400781  [160000/175341]\n",
      "loss: 0.510717  [161600/175341]\n",
      "loss: 0.466271  [163200/175341]\n",
      "loss: 0.351238  [164800/175341]\n",
      "loss: 0.368257  [166400/175341]\n",
      "loss: 0.414179  [168000/175341]\n",
      "loss: 0.934855  [169600/175341]\n",
      "loss: 0.767331  [171200/175341]\n",
      "loss: 0.719470  [172800/175341]\n",
      "loss: 0.376001  [174400/175341]\n",
      "Train Accuracy: 80.4438%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.611857, F1-score: 74.71%, Macro_F1-Score:  39.37%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.669998  [    0/175341]\n",
      "loss: 0.364886  [ 1600/175341]\n",
      "loss: 0.351334  [ 3200/175341]\n",
      "loss: 0.233771  [ 4800/175341]\n",
      "loss: 0.300620  [ 6400/175341]\n",
      "loss: 0.160841  [ 8000/175341]\n",
      "loss: 0.630646  [ 9600/175341]\n",
      "loss: 0.590949  [11200/175341]\n",
      "loss: 0.546997  [12800/175341]\n",
      "loss: 0.228960  [14400/175341]\n",
      "loss: 0.440754  [16000/175341]\n",
      "loss: 0.305947  [17600/175341]\n",
      "loss: 0.237649  [19200/175341]\n",
      "loss: 0.724421  [20800/175341]\n",
      "loss: 0.347728  [22400/175341]\n",
      "loss: 0.924212  [24000/175341]\n",
      "loss: 1.022652  [25600/175341]\n",
      "loss: 0.836527  [27200/175341]\n",
      "loss: 0.618156  [28800/175341]\n",
      "loss: 0.466467  [30400/175341]\n",
      "loss: 0.743459  [32000/175341]\n",
      "loss: 0.633782  [33600/175341]\n",
      "loss: 0.486066  [35200/175341]\n",
      "loss: 0.592430  [36800/175341]\n",
      "loss: 0.800758  [38400/175341]\n",
      "loss: 0.239905  [40000/175341]\n",
      "loss: 0.451668  [41600/175341]\n",
      "loss: 0.594669  [43200/175341]\n",
      "loss: 0.591480  [44800/175341]\n",
      "loss: 0.442615  [46400/175341]\n",
      "loss: 0.707871  [48000/175341]\n",
      "loss: 0.526823  [49600/175341]\n",
      "loss: 1.150440  [51200/175341]\n",
      "loss: 0.509589  [52800/175341]\n",
      "loss: 0.457502  [54400/175341]\n",
      "loss: 0.356835  [56000/175341]\n",
      "loss: 0.230249  [57600/175341]\n",
      "loss: 0.715300  [59200/175341]\n",
      "loss: 1.168470  [60800/175341]\n",
      "loss: 0.264917  [62400/175341]\n",
      "loss: 0.545414  [64000/175341]\n",
      "loss: 0.744561  [65600/175341]\n",
      "loss: 0.207790  [67200/175341]\n",
      "loss: 0.542873  [68800/175341]\n",
      "loss: 0.335942  [70400/175341]\n",
      "loss: 0.644821  [72000/175341]\n",
      "loss: 0.428510  [73600/175341]\n",
      "loss: 0.973885  [75200/175341]\n",
      "loss: 0.341344  [76800/175341]\n",
      "loss: 0.459796  [78400/175341]\n",
      "loss: 0.266108  [80000/175341]\n",
      "loss: 0.622253  [81600/175341]\n",
      "loss: 0.197497  [83200/175341]\n",
      "loss: 0.328665  [84800/175341]\n",
      "loss: 0.450680  [86400/175341]\n",
      "loss: 0.518947  [88000/175341]\n",
      "loss: 0.448566  [89600/175341]\n",
      "loss: 0.156774  [91200/175341]\n",
      "loss: 0.640531  [92800/175341]\n",
      "loss: 0.417568  [94400/175341]\n",
      "loss: 0.780763  [96000/175341]\n",
      "loss: 0.762655  [97600/175341]\n",
      "loss: 0.458759  [99200/175341]\n",
      "loss: 0.659482  [100800/175341]\n",
      "loss: 0.671331  [102400/175341]\n",
      "loss: 0.161927  [104000/175341]\n",
      "loss: 0.876065  [105600/175341]\n",
      "loss: 0.448765  [107200/175341]\n",
      "loss: 0.444407  [108800/175341]\n",
      "loss: 0.405519  [110400/175341]\n",
      "loss: 0.520953  [112000/175341]\n",
      "loss: 0.413619  [113600/175341]\n",
      "loss: 0.654472  [115200/175341]\n",
      "loss: 0.286384  [116800/175341]\n",
      "loss: 0.259902  [118400/175341]\n",
      "loss: 0.412880  [120000/175341]\n",
      "loss: 0.147054  [121600/175341]\n",
      "loss: 0.614647  [123200/175341]\n",
      "loss: 0.312449  [124800/175341]\n",
      "loss: 0.470834  [126400/175341]\n",
      "loss: 0.971512  [128000/175341]\n",
      "loss: 0.554849  [129600/175341]\n",
      "loss: 0.244418  [131200/175341]\n",
      "loss: 0.640726  [132800/175341]\n",
      "loss: 0.918752  [134400/175341]\n",
      "loss: 0.610034  [136000/175341]\n",
      "loss: 0.671077  [137600/175341]\n",
      "loss: 0.582682  [139200/175341]\n",
      "loss: 0.160685  [140800/175341]\n",
      "loss: 0.436774  [142400/175341]\n",
      "loss: 0.455603  [144000/175341]\n",
      "loss: 0.302288  [145600/175341]\n",
      "loss: 0.406631  [147200/175341]\n",
      "loss: 0.481058  [148800/175341]\n",
      "loss: 0.269768  [150400/175341]\n",
      "loss: 0.527817  [152000/175341]\n",
      "loss: 0.660347  [153600/175341]\n",
      "loss: 0.221878  [155200/175341]\n",
      "loss: 0.546168  [156800/175341]\n",
      "loss: 0.207211  [158400/175341]\n",
      "loss: 0.283364  [160000/175341]\n",
      "loss: 0.214351  [161600/175341]\n",
      "loss: 0.768003  [163200/175341]\n",
      "loss: 0.480911  [164800/175341]\n",
      "loss: 0.451094  [166400/175341]\n",
      "loss: 0.577025  [168000/175341]\n",
      "loss: 0.210613  [169600/175341]\n",
      "loss: 0.622643  [171200/175341]\n",
      "loss: 0.373721  [172800/175341]\n",
      "loss: 0.537383  [174400/175341]\n",
      "Train Accuracy: 80.5539%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.593712, F1-score: 75.44%, Macro_F1-Score:  40.02%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.551367  [    0/175341]\n",
      "loss: 0.825849  [ 1600/175341]\n",
      "loss: 0.236166  [ 3200/175341]\n",
      "loss: 0.270740  [ 4800/175341]\n",
      "loss: 0.344989  [ 6400/175341]\n",
      "loss: 0.697413  [ 8000/175341]\n",
      "loss: 0.729929  [ 9600/175341]\n",
      "loss: 0.715008  [11200/175341]\n",
      "loss: 0.821281  [12800/175341]\n",
      "loss: 0.461471  [14400/175341]\n",
      "loss: 0.276256  [16000/175341]\n",
      "loss: 0.219950  [17600/175341]\n",
      "loss: 0.565176  [19200/175341]\n",
      "loss: 0.309515  [20800/175341]\n",
      "loss: 0.959211  [22400/175341]\n",
      "loss: 0.399119  [24000/175341]\n",
      "loss: 0.167322  [25600/175341]\n",
      "loss: 0.581835  [27200/175341]\n",
      "loss: 0.546132  [28800/175341]\n",
      "loss: 0.496975  [30400/175341]\n",
      "loss: 0.398438  [32000/175341]\n",
      "loss: 0.785384  [33600/175341]\n",
      "loss: 0.323711  [35200/175341]\n",
      "loss: 0.164276  [36800/175341]\n",
      "loss: 0.555910  [38400/175341]\n",
      "loss: 0.149317  [40000/175341]\n",
      "loss: 0.739855  [41600/175341]\n",
      "loss: 0.534165  [43200/175341]\n",
      "loss: 0.452030  [44800/175341]\n",
      "loss: 0.439404  [46400/175341]\n",
      "loss: 0.373100  [48000/175341]\n",
      "loss: 0.175099  [49600/175341]\n",
      "loss: 0.449919  [51200/175341]\n",
      "loss: 0.483100  [52800/175341]\n",
      "loss: 0.330522  [54400/175341]\n",
      "loss: 0.423941  [56000/175341]\n",
      "loss: 0.667818  [57600/175341]\n",
      "loss: 0.592898  [59200/175341]\n",
      "loss: 0.271311  [60800/175341]\n",
      "loss: 0.357330  [62400/175341]\n",
      "loss: 0.618251  [64000/175341]\n",
      "loss: 0.296990  [65600/175341]\n",
      "loss: 0.331629  [67200/175341]\n",
      "loss: 0.322557  [68800/175341]\n",
      "loss: 0.288030  [70400/175341]\n",
      "loss: 0.423221  [72000/175341]\n",
      "loss: 0.458394  [73600/175341]\n",
      "loss: 0.522397  [75200/175341]\n",
      "loss: 0.552426  [76800/175341]\n",
      "loss: 0.397138  [78400/175341]\n",
      "loss: 0.490689  [80000/175341]\n",
      "loss: 0.447025  [81600/175341]\n",
      "loss: 0.225660  [83200/175341]\n",
      "loss: 0.553615  [84800/175341]\n",
      "loss: 0.388476  [86400/175341]\n",
      "loss: 0.490801  [88000/175341]\n",
      "loss: 0.786500  [89600/175341]\n",
      "loss: 0.656825  [91200/175341]\n",
      "loss: 0.374329  [92800/175341]\n",
      "loss: 0.367075  [94400/175341]\n",
      "loss: 0.830228  [96000/175341]\n",
      "loss: 0.523908  [97600/175341]\n",
      "loss: 0.580862  [99200/175341]\n",
      "loss: 0.366343  [100800/175341]\n",
      "loss: 0.836674  [102400/175341]\n",
      "loss: 0.256409  [104000/175341]\n",
      "loss: 0.261718  [105600/175341]\n",
      "loss: 0.487429  [107200/175341]\n",
      "loss: 0.602946  [108800/175341]\n",
      "loss: 0.567029  [110400/175341]\n",
      "loss: 0.182830  [112000/175341]\n",
      "loss: 0.245338  [113600/175341]\n",
      "loss: 0.223860  [115200/175341]\n",
      "loss: 0.472823  [116800/175341]\n",
      "loss: 0.408266  [118400/175341]\n",
      "loss: 0.443035  [120000/175341]\n",
      "loss: 0.273834  [121600/175341]\n",
      "loss: 0.347473  [123200/175341]\n",
      "loss: 0.440102  [124800/175341]\n",
      "loss: 0.310699  [126400/175341]\n",
      "loss: 0.651231  [128000/175341]\n",
      "loss: 0.771991  [129600/175341]\n",
      "loss: 0.645064  [131200/175341]\n",
      "loss: 0.263885  [132800/175341]\n",
      "loss: 0.360275  [134400/175341]\n",
      "loss: 0.723224  [136000/175341]\n",
      "loss: 0.485013  [137600/175341]\n",
      "loss: 0.312949  [139200/175341]\n",
      "loss: 0.575723  [140800/175341]\n",
      "loss: 0.541880  [142400/175341]\n",
      "loss: 0.565969  [144000/175341]\n",
      "loss: 0.525918  [145600/175341]\n",
      "loss: 0.210831  [147200/175341]\n",
      "loss: 0.399465  [148800/175341]\n",
      "loss: 0.417007  [150400/175341]\n",
      "loss: 0.269807  [152000/175341]\n",
      "loss: 0.398428  [153600/175341]\n",
      "loss: 0.546768  [155200/175341]\n",
      "loss: 0.508207  [156800/175341]\n",
      "loss: 0.673050  [158400/175341]\n",
      "loss: 0.504970  [160000/175341]\n",
      "loss: 0.785033  [161600/175341]\n",
      "loss: 0.758046  [163200/175341]\n",
      "loss: 0.763110  [164800/175341]\n",
      "loss: 0.377292  [166400/175341]\n",
      "loss: 0.574518  [168000/175341]\n",
      "loss: 0.451106  [169600/175341]\n",
      "loss: 0.414609  [171200/175341]\n",
      "loss: 0.443003  [172800/175341]\n",
      "loss: 0.411772  [174400/175341]\n",
      "Train Accuracy: 80.7290%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.577182, F1-score: 76.09%, Macro_F1-Score:  39.56%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.636703  [    0/175341]\n",
      "loss: 0.466925  [ 1600/175341]\n",
      "loss: 0.258108  [ 3200/175341]\n",
      "loss: 0.839903  [ 4800/175341]\n",
      "loss: 0.612353  [ 6400/175341]\n",
      "loss: 0.202975  [ 8000/175341]\n",
      "loss: 0.735345  [ 9600/175341]\n",
      "loss: 0.568811  [11200/175341]\n",
      "loss: 0.305828  [12800/175341]\n",
      "loss: 0.650210  [14400/175341]\n",
      "loss: 0.314059  [16000/175341]\n",
      "loss: 0.264972  [17600/175341]\n",
      "loss: 0.754700  [19200/175341]\n",
      "loss: 0.295543  [20800/175341]\n",
      "loss: 0.568679  [22400/175341]\n",
      "loss: 0.363177  [24000/175341]\n",
      "loss: 0.205143  [25600/175341]\n",
      "loss: 0.424125  [27200/175341]\n",
      "loss: 0.148592  [28800/175341]\n",
      "loss: 0.506822  [30400/175341]\n",
      "loss: 0.387921  [32000/175341]\n",
      "loss: 1.223464  [33600/175341]\n",
      "loss: 0.495781  [35200/175341]\n",
      "loss: 0.412516  [36800/175341]\n",
      "loss: 0.310624  [38400/175341]\n",
      "loss: 0.833663  [40000/175341]\n",
      "loss: 0.864257  [41600/175341]\n",
      "loss: 1.115471  [43200/175341]\n",
      "loss: 0.397952  [44800/175341]\n",
      "loss: 0.348436  [46400/175341]\n",
      "loss: 0.402162  [48000/175341]\n",
      "loss: 0.306004  [49600/175341]\n",
      "loss: 0.582023  [51200/175341]\n",
      "loss: 0.564621  [52800/175341]\n",
      "loss: 0.601747  [54400/175341]\n",
      "loss: 0.453920  [56000/175341]\n",
      "loss: 1.148637  [57600/175341]\n",
      "loss: 0.487832  [59200/175341]\n",
      "loss: 0.697268  [60800/175341]\n",
      "loss: 0.338909  [62400/175341]\n",
      "loss: 0.240827  [64000/175341]\n",
      "loss: 0.142401  [65600/175341]\n",
      "loss: 0.485364  [67200/175341]\n",
      "loss: 0.478277  [68800/175341]\n",
      "loss: 0.218377  [70400/175341]\n",
      "loss: 0.397633  [72000/175341]\n",
      "loss: 0.325473  [73600/175341]\n",
      "loss: 0.379496  [75200/175341]\n",
      "loss: 0.512596  [76800/175341]\n",
      "loss: 0.336234  [78400/175341]\n",
      "loss: 0.380591  [80000/175341]\n",
      "loss: 0.281236  [81600/175341]\n",
      "loss: 0.703564  [83200/175341]\n",
      "loss: 0.758146  [84800/175341]\n",
      "loss: 0.245586  [86400/175341]\n",
      "loss: 0.144240  [88000/175341]\n",
      "loss: 0.768009  [89600/175341]\n",
      "loss: 0.391754  [91200/175341]\n",
      "loss: 0.579421  [92800/175341]\n",
      "loss: 0.405591  [94400/175341]\n",
      "loss: 0.862555  [96000/175341]\n",
      "loss: 0.563210  [97600/175341]\n",
      "loss: 0.269237  [99200/175341]\n",
      "loss: 0.580625  [100800/175341]\n",
      "loss: 0.572708  [102400/175341]\n",
      "loss: 0.290054  [104000/175341]\n",
      "loss: 0.447798  [105600/175341]\n",
      "loss: 0.460286  [107200/175341]\n",
      "loss: 0.543024  [108800/175341]\n",
      "loss: 0.421476  [110400/175341]\n",
      "loss: 0.389368  [112000/175341]\n",
      "loss: 0.408495  [113600/175341]\n",
      "loss: 0.346247  [115200/175341]\n",
      "loss: 0.795376  [116800/175341]\n",
      "loss: 0.479954  [118400/175341]\n",
      "loss: 0.717168  [120000/175341]\n",
      "loss: 0.231662  [121600/175341]\n",
      "loss: 0.607490  [123200/175341]\n",
      "loss: 0.540694  [124800/175341]\n",
      "loss: 0.700467  [126400/175341]\n",
      "loss: 0.338676  [128000/175341]\n",
      "loss: 0.387733  [129600/175341]\n",
      "loss: 0.589303  [131200/175341]\n",
      "loss: 0.384031  [132800/175341]\n",
      "loss: 0.486941  [134400/175341]\n",
      "loss: 0.246082  [136000/175341]\n",
      "loss: 0.610620  [137600/175341]\n",
      "loss: 0.347286  [139200/175341]\n",
      "loss: 0.951544  [140800/175341]\n",
      "loss: 0.429737  [142400/175341]\n",
      "loss: 0.281735  [144000/175341]\n",
      "loss: 0.418475  [145600/175341]\n",
      "loss: 0.572903  [147200/175341]\n",
      "loss: 0.772480  [148800/175341]\n",
      "loss: 0.486650  [150400/175341]\n",
      "loss: 0.859226  [152000/175341]\n",
      "loss: 0.468394  [153600/175341]\n",
      "loss: 0.319693  [155200/175341]\n",
      "loss: 0.790689  [156800/175341]\n",
      "loss: 0.261530  [158400/175341]\n",
      "loss: 0.872361  [160000/175341]\n",
      "loss: 0.362774  [161600/175341]\n",
      "loss: 0.462705  [163200/175341]\n",
      "loss: 0.666074  [164800/175341]\n",
      "loss: 0.618465  [166400/175341]\n",
      "loss: 0.428325  [168000/175341]\n",
      "loss: 0.322779  [169600/175341]\n",
      "loss: 0.328737  [171200/175341]\n",
      "loss: 0.743824  [172800/175341]\n",
      "loss: 0.628201  [174400/175341]\n",
      "Train Accuracy: 80.6029%\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.629410, F1-score: 73.67%, Macro_F1-Score:  38.86%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.616723  [    0/175341]\n",
      "loss: 0.580718  [ 1600/175341]\n",
      "loss: 0.636174  [ 3200/175341]\n",
      "loss: 0.242328  [ 4800/175341]\n",
      "loss: 0.353993  [ 6400/175341]\n",
      "loss: 0.428001  [ 8000/175341]\n",
      "loss: 0.568380  [ 9600/175341]\n",
      "loss: 0.421302  [11200/175341]\n",
      "loss: 0.164566  [12800/175341]\n",
      "loss: 0.316903  [14400/175341]\n",
      "loss: 0.494208  [16000/175341]\n",
      "loss: 0.380633  [17600/175341]\n",
      "loss: 0.591988  [19200/175341]\n",
      "loss: 0.565848  [20800/175341]\n",
      "loss: 0.496033  [22400/175341]\n",
      "loss: 0.583937  [24000/175341]\n",
      "loss: 0.337087  [25600/175341]\n",
      "loss: 0.430389  [27200/175341]\n",
      "loss: 0.436010  [28800/175341]\n",
      "loss: 0.430841  [30400/175341]\n",
      "loss: 0.557045  [32000/175341]\n",
      "loss: 0.568673  [33600/175341]\n",
      "loss: 0.843805  [35200/175341]\n",
      "loss: 0.549603  [36800/175341]\n",
      "loss: 0.354912  [38400/175341]\n",
      "loss: 0.677650  [40000/175341]\n",
      "loss: 0.551759  [41600/175341]\n",
      "loss: 0.268383  [43200/175341]\n",
      "loss: 0.796628  [44800/175341]\n",
      "loss: 0.604969  [46400/175341]\n",
      "loss: 0.271402  [48000/175341]\n",
      "loss: 0.340475  [49600/175341]\n",
      "loss: 0.429444  [51200/175341]\n",
      "loss: 0.419003  [52800/175341]\n",
      "loss: 0.258417  [54400/175341]\n",
      "loss: 0.503773  [56000/175341]\n",
      "loss: 0.362671  [57600/175341]\n",
      "loss: 1.273525  [59200/175341]\n",
      "loss: 0.381984  [60800/175341]\n",
      "loss: 0.745456  [62400/175341]\n",
      "loss: 0.451137  [64000/175341]\n",
      "loss: 0.492495  [65600/175341]\n",
      "loss: 0.549033  [67200/175341]\n",
      "loss: 0.221334  [68800/175341]\n",
      "loss: 0.707762  [70400/175341]\n",
      "loss: 0.539155  [72000/175341]\n",
      "loss: 0.456315  [73600/175341]\n",
      "loss: 0.325128  [75200/175341]\n",
      "loss: 0.618205  [76800/175341]\n",
      "loss: 0.327132  [78400/175341]\n",
      "loss: 0.431428  [80000/175341]\n",
      "loss: 0.373697  [81600/175341]\n",
      "loss: 0.312969  [83200/175341]\n",
      "loss: 0.404199  [84800/175341]\n",
      "loss: 0.443740  [86400/175341]\n",
      "loss: 0.522164  [88000/175341]\n",
      "loss: 0.521017  [89600/175341]\n",
      "loss: 0.612902  [91200/175341]\n",
      "loss: 0.593016  [92800/175341]\n",
      "loss: 0.415048  [94400/175341]\n",
      "loss: 0.488147  [96000/175341]\n",
      "loss: 0.663441  [97600/175341]\n",
      "loss: 0.768505  [99200/175341]\n",
      "loss: 0.368671  [100800/175341]\n",
      "loss: 0.605617  [102400/175341]\n",
      "loss: 0.369044  [104000/175341]\n",
      "loss: 0.547544  [105600/175341]\n",
      "loss: 0.544467  [107200/175341]\n",
      "loss: 0.616112  [108800/175341]\n",
      "loss: 0.792771  [110400/175341]\n",
      "loss: 0.348342  [112000/175341]\n",
      "loss: 0.212861  [113600/175341]\n",
      "loss: 0.243988  [115200/175341]\n",
      "loss: 0.413036  [116800/175341]\n",
      "loss: 0.130833  [118400/175341]\n",
      "loss: 0.509564  [120000/175341]\n",
      "loss: 0.268954  [121600/175341]\n",
      "loss: 0.276634  [123200/175341]\n",
      "loss: 0.512976  [124800/175341]\n",
      "loss: 0.390867  [126400/175341]\n",
      "loss: 0.177870  [128000/175341]\n",
      "loss: 1.009711  [129600/175341]\n",
      "loss: 0.601475  [131200/175341]\n",
      "loss: 0.276827  [132800/175341]\n",
      "loss: 0.522603  [134400/175341]\n",
      "loss: 0.652863  [136000/175341]\n",
      "loss: 0.404416  [137600/175341]\n",
      "loss: 0.343893  [139200/175341]\n",
      "loss: 0.281054  [140800/175341]\n",
      "loss: 0.395250  [142400/175341]\n",
      "loss: 0.281496  [144000/175341]\n",
      "loss: 0.672801  [145600/175341]\n",
      "loss: 0.506343  [147200/175341]\n",
      "loss: 0.563033  [148800/175341]\n",
      "loss: 0.823519  [150400/175341]\n",
      "loss: 0.669146  [152000/175341]\n",
      "loss: 0.537972  [153600/175341]\n",
      "loss: 0.592987  [155200/175341]\n",
      "loss: 0.372115  [156800/175341]\n",
      "loss: 0.694395  [158400/175341]\n",
      "loss: 0.238834  [160000/175341]\n",
      "loss: 0.303647  [161600/175341]\n",
      "loss: 0.881568  [163200/175341]\n",
      "loss: 0.481518  [164800/175341]\n",
      "loss: 0.340959  [166400/175341]\n",
      "loss: 0.112124  [168000/175341]\n",
      "loss: 0.636031  [169600/175341]\n",
      "loss: 0.738143  [171200/175341]\n",
      "loss: 0.699409  [172800/175341]\n",
      "loss: 0.552246  [174400/175341]\n",
      "Train Accuracy: 80.5014%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.608500, F1-score: 75.06%, Macro_F1-Score:  38.92%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.774769  [    0/175341]\n",
      "loss: 0.528639  [ 1600/175341]\n",
      "loss: 0.084521  [ 3200/175341]\n",
      "loss: 0.371182  [ 4800/175341]\n",
      "loss: 0.488232  [ 6400/175341]\n",
      "loss: 0.470353  [ 8000/175341]\n",
      "loss: 0.707359  [ 9600/175341]\n",
      "loss: 0.540008  [11200/175341]\n",
      "loss: 0.485274  [12800/175341]\n",
      "loss: 0.402777  [14400/175341]\n",
      "loss: 0.458786  [16000/175341]\n",
      "loss: 0.283507  [17600/175341]\n",
      "loss: 0.579545  [19200/175341]\n",
      "loss: 0.526256  [20800/175341]\n",
      "loss: 0.222418  [22400/175341]\n",
      "loss: 0.450810  [24000/175341]\n",
      "loss: 0.910050  [25600/175341]\n",
      "loss: 0.619752  [27200/175341]\n",
      "loss: 0.507994  [28800/175341]\n",
      "loss: 0.688620  [30400/175341]\n",
      "loss: 0.550412  [32000/175341]\n",
      "loss: 0.784212  [33600/175341]\n",
      "loss: 0.554858  [35200/175341]\n",
      "loss: 0.466685  [36800/175341]\n",
      "loss: 0.353074  [38400/175341]\n",
      "loss: 0.408046  [40000/175341]\n",
      "loss: 0.291666  [41600/175341]\n",
      "loss: 0.642902  [43200/175341]\n",
      "loss: 0.689090  [44800/175341]\n",
      "loss: 0.321472  [46400/175341]\n",
      "loss: 0.285659  [48000/175341]\n",
      "loss: 0.581370  [49600/175341]\n",
      "loss: 0.631247  [51200/175341]\n",
      "loss: 0.730978  [52800/175341]\n",
      "loss: 0.352455  [54400/175341]\n",
      "loss: 0.192161  [56000/175341]\n",
      "loss: 0.530694  [57600/175341]\n",
      "loss: 0.492328  [59200/175341]\n",
      "loss: 0.387403  [60800/175341]\n",
      "loss: 0.605975  [62400/175341]\n",
      "loss: 1.051175  [64000/175341]\n",
      "loss: 0.165948  [65600/175341]\n",
      "loss: 0.363089  [67200/175341]\n",
      "loss: 0.316888  [68800/175341]\n",
      "loss: 0.854523  [70400/175341]\n",
      "loss: 0.612210  [72000/175341]\n",
      "loss: 0.249349  [73600/175341]\n",
      "loss: 0.542896  [75200/175341]\n",
      "loss: 0.386246  [76800/175341]\n",
      "loss: 0.182133  [78400/175341]\n",
      "loss: 0.465237  [80000/175341]\n",
      "loss: 0.529292  [81600/175341]\n",
      "loss: 0.516622  [83200/175341]\n",
      "loss: 0.429936  [84800/175341]\n",
      "loss: 0.144245  [86400/175341]\n",
      "loss: 0.655530  [88000/175341]\n",
      "loss: 0.734062  [89600/175341]\n",
      "loss: 0.749231  [91200/175341]\n",
      "loss: 0.418077  [92800/175341]\n",
      "loss: 0.603946  [94400/175341]\n",
      "loss: 0.409664  [96000/175341]\n",
      "loss: 0.414903  [97600/175341]\n",
      "loss: 0.473789  [99200/175341]\n",
      "loss: 0.243962  [100800/175341]\n",
      "loss: 0.406944  [102400/175341]\n",
      "loss: 0.383581  [104000/175341]\n",
      "loss: 0.336935  [105600/175341]\n",
      "loss: 0.309652  [107200/175341]\n",
      "loss: 0.757419  [108800/175341]\n",
      "loss: 0.340449  [110400/175341]\n",
      "loss: 0.605452  [112000/175341]\n",
      "loss: 0.385068  [113600/175341]\n",
      "loss: 0.345836  [115200/175341]\n",
      "loss: 0.370503  [116800/175341]\n",
      "loss: 0.397649  [118400/175341]\n",
      "loss: 0.602816  [120000/175341]\n",
      "loss: 0.712871  [121600/175341]\n",
      "loss: 0.465540  [123200/175341]\n",
      "loss: 0.798654  [124800/175341]\n",
      "loss: 0.555930  [126400/175341]\n",
      "loss: 0.585175  [128000/175341]\n",
      "loss: 0.415654  [129600/175341]\n",
      "loss: 0.818509  [131200/175341]\n",
      "loss: 0.438109  [132800/175341]\n",
      "loss: 0.664226  [134400/175341]\n",
      "loss: 0.430492  [136000/175341]\n",
      "loss: 0.765678  [137600/175341]\n",
      "loss: 0.848605  [139200/175341]\n",
      "loss: 0.438090  [140800/175341]\n",
      "loss: 0.522117  [142400/175341]\n",
      "loss: 0.479605  [144000/175341]\n",
      "loss: 0.299019  [145600/175341]\n",
      "loss: 0.790725  [147200/175341]\n",
      "loss: 0.618293  [148800/175341]\n",
      "loss: 0.373141  [150400/175341]\n",
      "loss: 0.533875  [152000/175341]\n",
      "loss: 0.413181  [153600/175341]\n",
      "loss: 0.325091  [155200/175341]\n",
      "loss: 0.240490  [156800/175341]\n",
      "loss: 0.840003  [158400/175341]\n",
      "loss: 0.860444  [160000/175341]\n",
      "loss: 0.460757  [161600/175341]\n",
      "loss: 0.371285  [163200/175341]\n",
      "loss: 0.529914  [164800/175341]\n",
      "loss: 0.776836  [166400/175341]\n",
      "loss: 0.757357  [168000/175341]\n",
      "loss: 0.527725  [169600/175341]\n",
      "loss: 0.569225  [171200/175341]\n",
      "loss: 0.437425  [172800/175341]\n",
      "loss: 0.724286  [174400/175341]\n",
      "Train Accuracy: 80.5305%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.621301, F1-score: 74.55%, Macro_F1-Score:  38.73%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.806466  [    0/175341]\n",
      "loss: 0.556048  [ 1600/175341]\n",
      "loss: 0.871803  [ 3200/175341]\n",
      "loss: 0.549972  [ 4800/175341]\n",
      "loss: 0.505198  [ 6400/175341]\n",
      "loss: 0.291261  [ 8000/175341]\n",
      "loss: 0.320837  [ 9600/175341]\n",
      "loss: 0.388806  [11200/175341]\n",
      "loss: 0.247595  [12800/175341]\n",
      "loss: 0.448553  [14400/175341]\n",
      "loss: 0.766057  [16000/175341]\n",
      "loss: 0.486547  [17600/175341]\n",
      "loss: 0.775463  [19200/175341]\n",
      "loss: 0.683381  [20800/175341]\n",
      "loss: 0.372449  [22400/175341]\n",
      "loss: 0.659642  [24000/175341]\n",
      "loss: 0.646234  [25600/175341]\n",
      "loss: 0.419859  [27200/175341]\n",
      "loss: 0.288934  [28800/175341]\n",
      "loss: 0.378134  [30400/175341]\n",
      "loss: 0.381616  [32000/175341]\n",
      "loss: 0.309000  [33600/175341]\n",
      "loss: 0.684039  [35200/175341]\n",
      "loss: 0.332184  [36800/175341]\n",
      "loss: 0.359602  [38400/175341]\n",
      "loss: 0.324360  [40000/175341]\n",
      "loss: 0.610332  [41600/175341]\n",
      "loss: 0.649230  [43200/175341]\n",
      "loss: 0.247231  [44800/175341]\n",
      "loss: 0.305741  [46400/175341]\n",
      "loss: 0.862736  [48000/175341]\n",
      "loss: 0.363635  [49600/175341]\n",
      "loss: 0.281749  [51200/175341]\n",
      "loss: 0.352119  [52800/175341]\n",
      "loss: 0.894285  [54400/175341]\n",
      "loss: 0.703704  [56000/175341]\n",
      "loss: 0.537441  [57600/175341]\n",
      "loss: 0.541172  [59200/175341]\n",
      "loss: 0.501814  [60800/175341]\n",
      "loss: 0.406449  [62400/175341]\n",
      "loss: 0.736457  [64000/175341]\n",
      "loss: 0.504338  [65600/175341]\n",
      "loss: 0.650177  [67200/175341]\n",
      "loss: 0.644123  [68800/175341]\n",
      "loss: 0.518339  [70400/175341]\n",
      "loss: 0.222596  [72000/175341]\n",
      "loss: 0.586520  [73600/175341]\n",
      "loss: 0.612421  [75200/175341]\n",
      "loss: 0.350684  [76800/175341]\n",
      "loss: 0.555007  [78400/175341]\n",
      "loss: 0.537339  [80000/175341]\n",
      "loss: 0.514258  [81600/175341]\n",
      "loss: 0.689449  [83200/175341]\n",
      "loss: 0.322622  [84800/175341]\n",
      "loss: 0.290113  [86400/175341]\n",
      "loss: 0.543126  [88000/175341]\n",
      "loss: 0.246379  [89600/175341]\n",
      "loss: 0.547686  [91200/175341]\n",
      "loss: 0.453973  [92800/175341]\n",
      "loss: 0.644719  [94400/175341]\n",
      "loss: 0.394922  [96000/175341]\n",
      "loss: 0.621227  [97600/175341]\n",
      "loss: 0.328127  [99200/175341]\n",
      "loss: 1.022350  [100800/175341]\n",
      "loss: 0.644623  [102400/175341]\n",
      "loss: 0.304635  [104000/175341]\n",
      "loss: 0.121339  [105600/175341]\n",
      "loss: 0.966368  [107200/175341]\n",
      "loss: 0.224327  [108800/175341]\n",
      "loss: 0.333116  [110400/175341]\n",
      "loss: 0.298864  [112000/175341]\n",
      "loss: 0.317041  [113600/175341]\n",
      "loss: 0.224840  [115200/175341]\n",
      "loss: 0.884199  [116800/175341]\n",
      "loss: 0.283006  [118400/175341]\n",
      "loss: 0.265418  [120000/175341]\n",
      "loss: 0.321952  [121600/175341]\n",
      "loss: 0.535211  [123200/175341]\n",
      "loss: 0.394230  [124800/175341]\n",
      "loss: 0.361197  [126400/175341]\n",
      "loss: 0.123015  [128000/175341]\n",
      "loss: 0.775713  [129600/175341]\n",
      "loss: 0.471629  [131200/175341]\n",
      "loss: 0.284583  [132800/175341]\n",
      "loss: 0.559247  [134400/175341]\n",
      "loss: 0.458016  [136000/175341]\n",
      "loss: 0.377323  [137600/175341]\n",
      "loss: 0.325117  [139200/175341]\n",
      "loss: 0.323816  [140800/175341]\n",
      "loss: 0.456181  [142400/175341]\n",
      "loss: 0.623809  [144000/175341]\n",
      "loss: 0.295307  [145600/175341]\n",
      "loss: 0.718401  [147200/175341]\n",
      "loss: 0.429456  [148800/175341]\n",
      "loss: 0.312833  [150400/175341]\n",
      "loss: 0.412226  [152000/175341]\n",
      "loss: 0.155463  [153600/175341]\n",
      "loss: 0.379717  [155200/175341]\n",
      "loss: 0.401677  [156800/175341]\n",
      "loss: 0.671509  [158400/175341]\n",
      "loss: 0.416892  [160000/175341]\n",
      "loss: 0.379418  [161600/175341]\n",
      "loss: 0.342184  [163200/175341]\n",
      "loss: 0.569260  [164800/175341]\n",
      "loss: 0.574712  [166400/175341]\n",
      "loss: 0.321203  [168000/175341]\n",
      "loss: 0.535031  [169600/175341]\n",
      "loss: 0.325588  [171200/175341]\n",
      "loss: 0.420919  [172800/175341]\n",
      "loss: 0.405658  [174400/175341]\n",
      "Train Accuracy: 80.5619%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.567253, F1-score: 76.57%, Macro_F1-Score:  40.97%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.675794  [    0/175341]\n",
      "loss: 0.368805  [ 1600/175341]\n",
      "loss: 0.639936  [ 3200/175341]\n",
      "loss: 0.564500  [ 4800/175341]\n",
      "loss: 0.444816  [ 6400/175341]\n",
      "loss: 0.304619  [ 8000/175341]\n",
      "loss: 0.668815  [ 9600/175341]\n",
      "loss: 0.441638  [11200/175341]\n",
      "loss: 0.551215  [12800/175341]\n",
      "loss: 0.323545  [14400/175341]\n",
      "loss: 0.482165  [16000/175341]\n",
      "loss: 0.421347  [17600/175341]\n",
      "loss: 0.239454  [19200/175341]\n",
      "loss: 0.758410  [20800/175341]\n",
      "loss: 0.470648  [22400/175341]\n",
      "loss: 0.489092  [24000/175341]\n",
      "loss: 0.594679  [25600/175341]\n",
      "loss: 0.339899  [27200/175341]\n",
      "loss: 0.070277  [28800/175341]\n",
      "loss: 0.529473  [30400/175341]\n",
      "loss: 0.420003  [32000/175341]\n",
      "loss: 0.923318  [33600/175341]\n",
      "loss: 0.408070  [35200/175341]\n",
      "loss: 0.537030  [36800/175341]\n",
      "loss: 0.368605  [38400/175341]\n",
      "loss: 0.524619  [40000/175341]\n",
      "loss: 0.299459  [41600/175341]\n",
      "loss: 0.391271  [43200/175341]\n",
      "loss: 0.253380  [44800/175341]\n",
      "loss: 0.356869  [46400/175341]\n",
      "loss: 0.431126  [48000/175341]\n",
      "loss: 0.396910  [49600/175341]\n",
      "loss: 0.326299  [51200/175341]\n",
      "loss: 0.766775  [52800/175341]\n",
      "loss: 0.939383  [54400/175341]\n",
      "loss: 0.303116  [56000/175341]\n",
      "loss: 0.436495  [57600/175341]\n",
      "loss: 0.263592  [59200/175341]\n",
      "loss: 1.032709  [60800/175341]\n",
      "loss: 0.515975  [62400/175341]\n",
      "loss: 0.757371  [64000/175341]\n",
      "loss: 0.090931  [65600/175341]\n",
      "loss: 0.330189  [67200/175341]\n",
      "loss: 0.559208  [68800/175341]\n",
      "loss: 0.284139  [70400/175341]\n",
      "loss: 0.335769  [72000/175341]\n",
      "loss: 0.418234  [73600/175341]\n",
      "loss: 0.322721  [75200/175341]\n",
      "loss: 0.188727  [76800/175341]\n",
      "loss: 1.274134  [78400/175341]\n",
      "loss: 0.235926  [80000/175341]\n",
      "loss: 0.829191  [81600/175341]\n",
      "loss: 0.313586  [83200/175341]\n",
      "loss: 0.256727  [84800/175341]\n",
      "loss: 0.554067  [86400/175341]\n",
      "loss: 0.397655  [88000/175341]\n",
      "loss: 0.413231  [89600/175341]\n",
      "loss: 0.562104  [91200/175341]\n",
      "loss: 0.322357  [92800/175341]\n",
      "loss: 0.328370  [94400/175341]\n",
      "loss: 0.475356  [96000/175341]\n",
      "loss: 0.424537  [97600/175341]\n",
      "loss: 0.604534  [99200/175341]\n",
      "loss: 0.657093  [100800/175341]\n",
      "loss: 0.618714  [102400/175341]\n",
      "loss: 0.471923  [104000/175341]\n",
      "loss: 0.385874  [105600/175341]\n",
      "loss: 0.268005  [107200/175341]\n",
      "loss: 0.230078  [108800/175341]\n",
      "loss: 0.589285  [110400/175341]\n",
      "loss: 0.340910  [112000/175341]\n",
      "loss: 0.482374  [113600/175341]\n",
      "loss: 0.292936  [115200/175341]\n",
      "loss: 0.790400  [116800/175341]\n",
      "loss: 0.396501  [118400/175341]\n",
      "loss: 0.367158  [120000/175341]\n",
      "loss: 1.022557  [121600/175341]\n",
      "loss: 0.301392  [123200/175341]\n",
      "loss: 0.428688  [124800/175341]\n",
      "loss: 0.787275  [126400/175341]\n",
      "loss: 0.538463  [128000/175341]\n",
      "loss: 0.766401  [129600/175341]\n",
      "loss: 0.152025  [131200/175341]\n",
      "loss: 0.516160  [132800/175341]\n",
      "loss: 0.395279  [134400/175341]\n",
      "loss: 0.346000  [136000/175341]\n",
      "loss: 0.392624  [137600/175341]\n",
      "loss: 0.556231  [139200/175341]\n",
      "loss: 0.375792  [140800/175341]\n",
      "loss: 0.386597  [142400/175341]\n",
      "loss: 0.341613  [144000/175341]\n",
      "loss: 0.246270  [145600/175341]\n",
      "loss: 0.269015  [147200/175341]\n",
      "loss: 0.225094  [148800/175341]\n",
      "loss: 0.499856  [150400/175341]\n",
      "loss: 0.361243  [152000/175341]\n",
      "loss: 0.490803  [153600/175341]\n",
      "loss: 0.745878  [155200/175341]\n",
      "loss: 0.765567  [156800/175341]\n",
      "loss: 0.539626  [158400/175341]\n",
      "loss: 0.887338  [160000/175341]\n",
      "loss: 0.174172  [161600/175341]\n",
      "loss: 0.327900  [163200/175341]\n",
      "loss: 0.457844  [164800/175341]\n",
      "loss: 0.633305  [166400/175341]\n",
      "loss: 0.354545  [168000/175341]\n",
      "loss: 0.245772  [169600/175341]\n",
      "loss: 0.420679  [171200/175341]\n",
      "loss: 0.438668  [172800/175341]\n",
      "loss: 0.400729  [174400/175341]\n",
      "Train Accuracy: 80.7906%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.602297, F1-score: 75.47%, Macro_F1-Score:  38.87%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.512603  [    0/175341]\n",
      "loss: 0.109511  [ 1600/175341]\n",
      "loss: 0.276547  [ 3200/175341]\n",
      "loss: 0.515898  [ 4800/175341]\n",
      "loss: 0.325550  [ 6400/175341]\n",
      "loss: 0.330421  [ 8000/175341]\n",
      "loss: 0.477272  [ 9600/175341]\n",
      "loss: 0.620951  [11200/175341]\n",
      "loss: 0.447638  [12800/175341]\n",
      "loss: 0.317986  [14400/175341]\n",
      "loss: 0.103444  [16000/175341]\n",
      "loss: 0.242531  [17600/175341]\n",
      "loss: 0.251554  [19200/175341]\n",
      "loss: 0.635922  [20800/175341]\n",
      "loss: 0.486140  [22400/175341]\n",
      "loss: 0.369377  [24000/175341]\n",
      "loss: 0.178228  [25600/175341]\n",
      "loss: 1.007213  [27200/175341]\n",
      "loss: 0.705552  [28800/175341]\n",
      "loss: 0.250847  [30400/175341]\n",
      "loss: 0.532715  [32000/175341]\n",
      "loss: 0.340084  [33600/175341]\n",
      "loss: 0.097820  [35200/175341]\n",
      "loss: 0.283066  [36800/175341]\n",
      "loss: 0.687798  [38400/175341]\n",
      "loss: 0.196458  [40000/175341]\n",
      "loss: 0.224422  [41600/175341]\n",
      "loss: 0.303225  [43200/175341]\n",
      "loss: 0.256113  [44800/175341]\n",
      "loss: 0.347328  [46400/175341]\n",
      "loss: 0.472793  [48000/175341]\n",
      "loss: 0.832460  [49600/175341]\n",
      "loss: 0.490858  [51200/175341]\n",
      "loss: 0.180272  [52800/175341]\n",
      "loss: 0.443526  [54400/175341]\n",
      "loss: 0.281767  [56000/175341]\n",
      "loss: 0.210610  [57600/175341]\n",
      "loss: 0.363756  [59200/175341]\n",
      "loss: 0.231332  [60800/175341]\n",
      "loss: 0.195943  [62400/175341]\n",
      "loss: 0.474792  [64000/175341]\n",
      "loss: 1.002010  [65600/175341]\n",
      "loss: 0.368204  [67200/175341]\n",
      "loss: 0.331728  [68800/175341]\n",
      "loss: 0.553542  [70400/175341]\n",
      "loss: 0.238684  [72000/175341]\n",
      "loss: 0.582540  [73600/175341]\n",
      "loss: 0.433986  [75200/175341]\n",
      "loss: 0.548068  [76800/175341]\n",
      "loss: 0.563371  [78400/175341]\n",
      "loss: 0.691731  [80000/175341]\n",
      "loss: 0.284607  [81600/175341]\n",
      "loss: 0.395290  [83200/175341]\n",
      "loss: 0.629810  [84800/175341]\n",
      "loss: 0.487300  [86400/175341]\n",
      "loss: 0.220344  [88000/175341]\n",
      "loss: 0.438843  [89600/175341]\n",
      "loss: 0.435758  [91200/175341]\n",
      "loss: 0.596842  [92800/175341]\n",
      "loss: 0.533285  [94400/175341]\n",
      "loss: 0.450565  [96000/175341]\n",
      "loss: 0.335859  [97600/175341]\n",
      "loss: 0.308361  [99200/175341]\n",
      "loss: 0.536081  [100800/175341]\n",
      "loss: 0.372870  [102400/175341]\n",
      "loss: 0.385293  [104000/175341]\n",
      "loss: 0.469487  [105600/175341]\n",
      "loss: 0.519220  [107200/175341]\n",
      "loss: 0.502418  [108800/175341]\n",
      "loss: 0.313672  [110400/175341]\n",
      "loss: 0.317985  [112000/175341]\n",
      "loss: 0.497716  [113600/175341]\n",
      "loss: 0.525213  [115200/175341]\n",
      "loss: 0.401760  [116800/175341]\n",
      "loss: 0.374790  [118400/175341]\n",
      "loss: 0.301315  [120000/175341]\n",
      "loss: 0.751296  [121600/175341]\n",
      "loss: 0.453844  [123200/175341]\n",
      "loss: 0.508657  [124800/175341]\n",
      "loss: 0.584830  [126400/175341]\n",
      "loss: 0.648182  [128000/175341]\n",
      "loss: 0.801593  [129600/175341]\n",
      "loss: 0.430957  [131200/175341]\n",
      "loss: 0.978846  [132800/175341]\n",
      "loss: 0.651851  [134400/175341]\n",
      "loss: 0.381561  [136000/175341]\n",
      "loss: 0.413151  [137600/175341]\n",
      "loss: 0.315170  [139200/175341]\n",
      "loss: 0.492355  [140800/175341]\n",
      "loss: 0.134092  [142400/175341]\n",
      "loss: 0.214578  [144000/175341]\n",
      "loss: 0.609801  [145600/175341]\n",
      "loss: 0.911184  [147200/175341]\n",
      "loss: 0.287142  [148800/175341]\n",
      "loss: 0.504028  [150400/175341]\n",
      "loss: 0.322762  [152000/175341]\n",
      "loss: 0.539858  [153600/175341]\n",
      "loss: 0.806882  [155200/175341]\n",
      "loss: 0.962269  [156800/175341]\n",
      "loss: 0.395292  [158400/175341]\n",
      "loss: 0.040663  [160000/175341]\n",
      "loss: 0.200573  [161600/175341]\n",
      "loss: 0.591588  [163200/175341]\n",
      "loss: 0.584272  [164800/175341]\n",
      "loss: 0.609460  [166400/175341]\n",
      "loss: 0.689357  [168000/175341]\n",
      "loss: 0.595591  [169600/175341]\n",
      "loss: 0.464683  [171200/175341]\n",
      "loss: 0.530370  [172800/175341]\n",
      "loss: 0.412657  [174400/175341]\n",
      "Train Accuracy: 80.7894%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.565602, F1-score: 75.98%, Macro_F1-Score:  40.35%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115f1b7-87f8-41a5-8992-af1752600f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a33c4-8087-4b5c-80cf-c10334d2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 15,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada289ca-73cc-4ac6-8f1c-89c10d83b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6d93-0220-4085-bb3f-8965b97240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e34344-c96e-42c9-b318-96c5e291cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units1: int, hidden_units2: int, hidden_units3: int, hidden_units4: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units2),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units3),\n",
    "        )\n",
    "\n",
    "        # **New Block Added Here**\n",
    "        self.linear_relu_stack3 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units4),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack4 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        x = self.linear_relu_stack3(x)  # Pass through new block\n",
    "        logits = self.linear_relu_stack4(x)  # Final output layer\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = NeuralNetwork(40,128,64,32,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model_test, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model_test, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
