{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbdef64-5462-455b-96fa-863d4b36bdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2c096b-e87b-4597-a60e-d432d11ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = {\n",
    "    0: 20000,  # Class 0 should have 56000 samples\n",
    "    1: 20000,  # Class 1 should have 56000 samples\n",
    "    2: 30000,  # Class 2 should have 56000 samples\n",
    "    3: 34000,  # Class 3 should have 56000 samples\n",
    "    4: 30000,  # Class 4 should have 56000 samples\n",
    "    5: 41000,  # Class 5 should have 56000 samples\n",
    "    6: 56000,  # Class 6 should have 56000 samples\n",
    "    7: 30000,  # Class 7 should have 56000 samples\n",
    "    8: 20000,  # Class 8 should have 56000 samples\n",
    "    9: 10000,  # Class 9 should have 56000 samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45addaf4-c17e-4cb2-94b3-a5c6629a8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 6\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96863f09-6fdd-47a1-aa2a-0c75a380e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 5\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011c575f-d547-4b41-84c0-1af4ba290d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"majority\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af5f4e3-48a5-4a60-b951-1cb02855acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102d84fa-c8e2-425c-84f8-25b8a6272498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy = \"minority\", random_state=42, n_neighbors= 3)\n",
    "#train_data_X,train_data_y = adasyn.fit_resample(train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "#smote_enn = SMOTEENN(sampling_strategy=\"not majority\", enn = enn, random_state=42,n_neighbors = 5)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "#train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_data_y)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "#print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.002289\n",
       "encoder__proto_ospf             0.000760\n",
       "encoder__proto_sctp             0.000771\n",
       "encoder__proto_tcp              0.001969\n",
       "encoder__proto_udp              0.025228\n",
       "encoder__proto_unas             0.001297\n",
       "encoder__service_dns            0.047050\n",
       "encoder__service_ftp            0.000409\n",
       "encoder__service_ftp-data       0.000504\n",
       "encoder__service_http           0.004399\n",
       "encoder__service_smtp           0.000748\n",
       "encoder__state_CON              0.002900\n",
       "encoder__state_ECO              0.000027\n",
       "encoder__state_FIN              0.000637\n",
       "encoder__state_INT              0.007103\n",
       "encoder__state_REQ              0.000588\n",
       "encoder__state_RST              0.000013\n",
       "remainder__dur                  0.020717\n",
       "remainder__spkts                0.010003\n",
       "remainder__dpkts                0.009122\n",
       "remainder__sbytes               0.073659\n",
       "remainder__dbytes               0.021778\n",
       "remainder__rate                 0.024504\n",
       "remainder__sttl                 0.094045\n",
       "remainder__dttl                 0.012907\n",
       "remainder__sload                0.033948\n",
       "remainder__dload                0.033277\n",
       "remainder__sloss                0.007421\n",
       "remainder__dloss                0.010422\n",
       "remainder__sinpkt               0.017457\n",
       "remainder__dinpkt               0.014835\n",
       "remainder__sjit                 0.013910\n",
       "remainder__djit                 0.013893\n",
       "remainder__swin                 0.001573\n",
       "remainder__stcpb                0.007878\n",
       "remainder__dtcpb                0.008845\n",
       "remainder__dwin                 0.000472\n",
       "remainder__tcprtt               0.025203\n",
       "remainder__synack               0.014213\n",
       "remainder__ackdat               0.017930\n",
       "remainder__smean                0.050911\n",
       "remainder__dmean                0.026252\n",
       "remainder__trans_depth          0.002718\n",
       "remainder__response_body_len    0.002878\n",
       "remainder__ct_srv_src           0.034274\n",
       "remainder__ct_state_ttl         0.046782\n",
       "remainder__ct_dst_ltm           0.021318\n",
       "remainder__ct_src_dport_ltm     0.047888\n",
       "remainder__ct_dst_sport_ltm     0.049038\n",
       "remainder__ct_dst_src_ltm       0.043798\n",
       "remainder__is_ftp_login         0.000131\n",
       "remainder__ct_ftp_cmd           0.000190\n",
       "remainder__ct_flw_http_mthd     0.003045\n",
       "remainder__ct_src_ltm           0.018829\n",
       "remainder__ct_srv_dst           0.065233\n",
       "remainder__is_sm_ips_ports      0.002014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e2a8d00-9e61-4d23-b6d7-c0b81cf1312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,10, 4,10],grid_size = 4, spline_order = 2, scale_noise=0.1, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301879  [    0/175341]\n",
      "loss: 2.227663  [ 1600/175341]\n",
      "loss: 2.198478  [ 3200/175341]\n",
      "loss: 2.151745  [ 4800/175341]\n",
      "loss: 2.093129  [ 6400/175341]\n",
      "loss: 1.928080  [ 8000/175341]\n",
      "loss: 1.831158  [ 9600/175341]\n",
      "loss: 1.700692  [11200/175341]\n",
      "loss: 1.655609  [12800/175341]\n",
      "loss: 1.732935  [14400/175341]\n",
      "loss: 1.581114  [16000/175341]\n",
      "loss: 1.568261  [17600/175341]\n",
      "loss: 1.428969  [19200/175341]\n",
      "loss: 1.472398  [20800/175341]\n",
      "loss: 1.330044  [22400/175341]\n",
      "loss: 1.257519  [24000/175341]\n",
      "loss: 1.535965  [25600/175341]\n",
      "loss: 1.122771  [27200/175341]\n",
      "loss: 0.983768  [28800/175341]\n",
      "loss: 1.164093  [30400/175341]\n",
      "loss: 1.055241  [32000/175341]\n",
      "loss: 0.718565  [33600/175341]\n",
      "loss: 1.012914  [35200/175341]\n",
      "loss: 1.037233  [36800/175341]\n",
      "loss: 0.918020  [38400/175341]\n",
      "loss: 1.033825  [40000/175341]\n",
      "loss: 0.958920  [41600/175341]\n",
      "loss: 0.976249  [43200/175341]\n",
      "loss: 1.022316  [44800/175341]\n",
      "loss: 0.861836  [46400/175341]\n",
      "loss: 0.635374  [48000/175341]\n",
      "loss: 1.117821  [49600/175341]\n",
      "loss: 1.120094  [51200/175341]\n",
      "loss: 1.015710  [52800/175341]\n",
      "loss: 1.127750  [54400/175341]\n",
      "loss: 0.830780  [56000/175341]\n",
      "loss: 1.100877  [57600/175341]\n",
      "loss: 0.415997  [59200/175341]\n",
      "loss: 0.778029  [60800/175341]\n",
      "loss: 0.761825  [62400/175341]\n",
      "loss: 1.201670  [64000/175341]\n",
      "loss: 0.572373  [65600/175341]\n",
      "loss: 0.977664  [67200/175341]\n",
      "loss: 0.719787  [68800/175341]\n",
      "loss: 0.566397  [70400/175341]\n",
      "loss: 0.817028  [72000/175341]\n",
      "loss: 0.964980  [73600/175341]\n",
      "loss: 0.956389  [75200/175341]\n",
      "loss: 0.776581  [76800/175341]\n",
      "loss: 0.696360  [78400/175341]\n",
      "loss: 0.505218  [80000/175341]\n",
      "loss: 0.754707  [81600/175341]\n",
      "loss: 1.053856  [83200/175341]\n",
      "loss: 0.905944  [84800/175341]\n",
      "loss: 0.914886  [86400/175341]\n",
      "loss: 1.016291  [88000/175341]\n",
      "loss: 0.806884  [89600/175341]\n",
      "loss: 0.757681  [91200/175341]\n",
      "loss: 1.240229  [92800/175341]\n",
      "loss: 1.065907  [94400/175341]\n",
      "loss: 0.851067  [96000/175341]\n",
      "loss: 0.665753  [97600/175341]\n",
      "loss: 0.686270  [99200/175341]\n",
      "loss: 0.645443  [100800/175341]\n",
      "loss: 0.364727  [102400/175341]\n",
      "loss: 0.710757  [104000/175341]\n",
      "loss: 0.458346  [105600/175341]\n",
      "loss: 0.764160  [107200/175341]\n",
      "loss: 0.879955  [108800/175341]\n",
      "loss: 0.240529  [110400/175341]\n",
      "loss: 0.700972  [112000/175341]\n",
      "loss: 0.917262  [113600/175341]\n",
      "loss: 0.464230  [115200/175341]\n",
      "loss: 0.602122  [116800/175341]\n",
      "loss: 0.758808  [118400/175341]\n",
      "loss: 0.653145  [120000/175341]\n",
      "loss: 0.807384  [121600/175341]\n",
      "loss: 0.705196  [123200/175341]\n",
      "loss: 0.299150  [124800/175341]\n",
      "loss: 0.508339  [126400/175341]\n",
      "loss: 0.444578  [128000/175341]\n",
      "loss: 0.802915  [129600/175341]\n",
      "loss: 0.825077  [131200/175341]\n",
      "loss: 0.755730  [132800/175341]\n",
      "loss: 0.783656  [134400/175341]\n",
      "loss: 0.578693  [136000/175341]\n",
      "loss: 0.228273  [137600/175341]\n",
      "loss: 0.560495  [139200/175341]\n",
      "loss: 0.546302  [140800/175341]\n",
      "loss: 0.879581  [142400/175341]\n",
      "loss: 0.733946  [144000/175341]\n",
      "loss: 0.608599  [145600/175341]\n",
      "loss: 0.549059  [147200/175341]\n",
      "loss: 1.170859  [148800/175341]\n",
      "loss: 0.640253  [150400/175341]\n",
      "loss: 0.403701  [152000/175341]\n",
      "loss: 0.298002  [153600/175341]\n",
      "loss: 0.831346  [155200/175341]\n",
      "loss: 1.027716  [156800/175341]\n",
      "loss: 0.440097  [158400/175341]\n",
      "loss: 0.893218  [160000/175341]\n",
      "loss: 0.775959  [161600/175341]\n",
      "loss: 0.829710  [163200/175341]\n",
      "loss: 0.741769  [164800/175341]\n",
      "loss: 0.582788  [166400/175341]\n",
      "loss: 0.739039  [168000/175341]\n",
      "loss: 0.648038  [169600/175341]\n",
      "loss: 0.489036  [171200/175341]\n",
      "loss: 0.765168  [172800/175341]\n",
      "loss: 0.745504  [174400/175341]\n",
      "Train Accuracy: 68.2841%\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.719479, F1-score: 68.64%, Macro_F1-Score:  30.67%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.419324  [    0/175341]\n",
      "loss: 0.883945  [ 1600/175341]\n",
      "loss: 0.708738  [ 3200/175341]\n",
      "loss: 0.558637  [ 4800/175341]\n",
      "loss: 0.293793  [ 6400/175341]\n",
      "loss: 0.902895  [ 8000/175341]\n",
      "loss: 0.857807  [ 9600/175341]\n",
      "loss: 0.504784  [11200/175341]\n",
      "loss: 0.835794  [12800/175341]\n",
      "loss: 1.005455  [14400/175341]\n",
      "loss: 0.342681  [16000/175341]\n",
      "loss: 0.797111  [17600/175341]\n",
      "loss: 0.511435  [19200/175341]\n",
      "loss: 0.262740  [20800/175341]\n",
      "loss: 0.894633  [22400/175341]\n",
      "loss: 0.443637  [24000/175341]\n",
      "loss: 0.515768  [25600/175341]\n",
      "loss: 0.369088  [27200/175341]\n",
      "loss: 0.497926  [28800/175341]\n",
      "loss: 0.396299  [30400/175341]\n",
      "loss: 0.504410  [32000/175341]\n",
      "loss: 0.925686  [33600/175341]\n",
      "loss: 0.509871  [35200/175341]\n",
      "loss: 0.430228  [36800/175341]\n",
      "loss: 0.557005  [38400/175341]\n",
      "loss: 0.731561  [40000/175341]\n",
      "loss: 0.294079  [41600/175341]\n",
      "loss: 0.328228  [43200/175341]\n",
      "loss: 0.961988  [44800/175341]\n",
      "loss: 0.564635  [46400/175341]\n",
      "loss: 0.784666  [48000/175341]\n",
      "loss: 0.687836  [49600/175341]\n",
      "loss: 0.576888  [51200/175341]\n",
      "loss: 0.415639  [52800/175341]\n",
      "loss: 0.437531  [54400/175341]\n",
      "loss: 0.711091  [56000/175341]\n",
      "loss: 0.607880  [57600/175341]\n",
      "loss: 0.593742  [59200/175341]\n",
      "loss: 0.062081  [60800/175341]\n",
      "loss: 0.399721  [62400/175341]\n",
      "loss: 0.512175  [64000/175341]\n",
      "loss: 0.511839  [65600/175341]\n",
      "loss: 0.679825  [67200/175341]\n",
      "loss: 0.922677  [68800/175341]\n",
      "loss: 0.942189  [70400/175341]\n",
      "loss: 1.134125  [72000/175341]\n",
      "loss: 0.654706  [73600/175341]\n",
      "loss: 0.360597  [75200/175341]\n",
      "loss: 0.769837  [76800/175341]\n",
      "loss: 0.689098  [78400/175341]\n",
      "loss: 0.551087  [80000/175341]\n",
      "loss: 0.411540  [81600/175341]\n",
      "loss: 1.009182  [83200/175341]\n",
      "loss: 0.631348  [84800/175341]\n",
      "loss: 0.505093  [86400/175341]\n",
      "loss: 0.918301  [88000/175341]\n",
      "loss: 0.271917  [89600/175341]\n",
      "loss: 0.904590  [91200/175341]\n",
      "loss: 0.493839  [92800/175341]\n",
      "loss: 0.473393  [94400/175341]\n",
      "loss: 0.849321  [96000/175341]\n",
      "loss: 0.935478  [97600/175341]\n",
      "loss: 0.371213  [99200/175341]\n",
      "loss: 0.546732  [100800/175341]\n",
      "loss: 0.429191  [102400/175341]\n",
      "loss: 0.399625  [104000/175341]\n",
      "loss: 0.790924  [105600/175341]\n",
      "loss: 0.427678  [107200/175341]\n",
      "loss: 1.252316  [108800/175341]\n",
      "loss: 0.483906  [110400/175341]\n",
      "loss: 0.514668  [112000/175341]\n",
      "loss: 0.837546  [113600/175341]\n",
      "loss: 0.120719  [115200/175341]\n",
      "loss: 0.597490  [116800/175341]\n",
      "loss: 0.929792  [118400/175341]\n",
      "loss: 0.320669  [120000/175341]\n",
      "loss: 0.555075  [121600/175341]\n",
      "loss: 0.639748  [123200/175341]\n",
      "loss: 0.495014  [124800/175341]\n",
      "loss: 0.406677  [126400/175341]\n",
      "loss: 0.963467  [128000/175341]\n",
      "loss: 0.569449  [129600/175341]\n",
      "loss: 0.294837  [131200/175341]\n",
      "loss: 0.586358  [132800/175341]\n",
      "loss: 0.553569  [134400/175341]\n",
      "loss: 1.078426  [136000/175341]\n",
      "loss: 0.462614  [137600/175341]\n",
      "loss: 0.781460  [139200/175341]\n",
      "loss: 0.453258  [140800/175341]\n",
      "loss: 0.857038  [142400/175341]\n",
      "loss: 0.641617  [144000/175341]\n",
      "loss: 0.829252  [145600/175341]\n",
      "loss: 0.603878  [147200/175341]\n",
      "loss: 0.905918  [148800/175341]\n",
      "loss: 0.692884  [150400/175341]\n",
      "loss: 0.365788  [152000/175341]\n",
      "loss: 0.724122  [153600/175341]\n",
      "loss: 0.194680  [155200/175341]\n",
      "loss: 0.406748  [156800/175341]\n",
      "loss: 0.570607  [158400/175341]\n",
      "loss: 0.348775  [160000/175341]\n",
      "loss: 0.483036  [161600/175341]\n",
      "loss: 0.431365  [163200/175341]\n",
      "loss: 0.816269  [164800/175341]\n",
      "loss: 0.763302  [166400/175341]\n",
      "loss: 1.036420  [168000/175341]\n",
      "loss: 0.534880  [169600/175341]\n",
      "loss: 0.557059  [171200/175341]\n",
      "loss: 0.614105  [172800/175341]\n",
      "loss: 0.711616  [174400/175341]\n",
      "Train Accuracy: 76.5417%\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.656382, F1-score: 71.04%, Macro_F1-Score:  32.65%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.487270  [    0/175341]\n",
      "loss: 0.310747  [ 1600/175341]\n",
      "loss: 0.387275  [ 3200/175341]\n",
      "loss: 0.692833  [ 4800/175341]\n",
      "loss: 0.922295  [ 6400/175341]\n",
      "loss: 0.680786  [ 8000/175341]\n",
      "loss: 0.663082  [ 9600/175341]\n",
      "loss: 1.035991  [11200/175341]\n",
      "loss: 0.693734  [12800/175341]\n",
      "loss: 0.744440  [14400/175341]\n",
      "loss: 0.879365  [16000/175341]\n",
      "loss: 0.829672  [17600/175341]\n",
      "loss: 0.338534  [19200/175341]\n",
      "loss: 0.325495  [20800/175341]\n",
      "loss: 0.910440  [22400/175341]\n",
      "loss: 0.893775  [24000/175341]\n",
      "loss: 0.955087  [25600/175341]\n",
      "loss: 0.427286  [27200/175341]\n",
      "loss: 0.497728  [28800/175341]\n",
      "loss: 0.687764  [30400/175341]\n",
      "loss: 0.323607  [32000/175341]\n",
      "loss: 0.792449  [33600/175341]\n",
      "loss: 0.612043  [35200/175341]\n",
      "loss: 0.782185  [36800/175341]\n",
      "loss: 0.583219  [38400/175341]\n",
      "loss: 0.619746  [40000/175341]\n",
      "loss: 0.304501  [41600/175341]\n",
      "loss: 0.782780  [43200/175341]\n",
      "loss: 0.607175  [44800/175341]\n",
      "loss: 0.946639  [46400/175341]\n",
      "loss: 0.401092  [48000/175341]\n",
      "loss: 0.715223  [49600/175341]\n",
      "loss: 0.866679  [51200/175341]\n",
      "loss: 0.364858  [52800/175341]\n",
      "loss: 0.372477  [54400/175341]\n",
      "loss: 0.376084  [56000/175341]\n",
      "loss: 0.358696  [57600/175341]\n",
      "loss: 0.655259  [59200/175341]\n",
      "loss: 1.037299  [60800/175341]\n",
      "loss: 0.393752  [62400/175341]\n",
      "loss: 0.945479  [64000/175341]\n",
      "loss: 0.602776  [65600/175341]\n",
      "loss: 0.371379  [67200/175341]\n",
      "loss: 0.515093  [68800/175341]\n",
      "loss: 0.533999  [70400/175341]\n",
      "loss: 0.423279  [72000/175341]\n",
      "loss: 0.596039  [73600/175341]\n",
      "loss: 0.620726  [75200/175341]\n",
      "loss: 0.317716  [76800/175341]\n",
      "loss: 0.976506  [78400/175341]\n",
      "loss: 0.580430  [80000/175341]\n",
      "loss: 0.750250  [81600/175341]\n",
      "loss: 0.423754  [83200/175341]\n",
      "loss: 0.349265  [84800/175341]\n",
      "loss: 0.463958  [86400/175341]\n",
      "loss: 0.344613  [88000/175341]\n",
      "loss: 0.504800  [89600/175341]\n",
      "loss: 0.366902  [91200/175341]\n",
      "loss: 0.628994  [92800/175341]\n",
      "loss: 0.703160  [94400/175341]\n",
      "loss: 0.608362  [96000/175341]\n",
      "loss: 0.928033  [97600/175341]\n",
      "loss: 0.723261  [99200/175341]\n",
      "loss: 0.670883  [100800/175341]\n",
      "loss: 0.570643  [102400/175341]\n",
      "loss: 0.744867  [104000/175341]\n",
      "loss: 0.489151  [105600/175341]\n",
      "loss: 0.350674  [107200/175341]\n",
      "loss: 0.728245  [108800/175341]\n",
      "loss: 0.323461  [110400/175341]\n",
      "loss: 0.395447  [112000/175341]\n",
      "loss: 0.504234  [113600/175341]\n",
      "loss: 0.395595  [115200/175341]\n",
      "loss: 0.457768  [116800/175341]\n",
      "loss: 0.462452  [118400/175341]\n",
      "loss: 0.787617  [120000/175341]\n",
      "loss: 0.485036  [121600/175341]\n",
      "loss: 0.663357  [123200/175341]\n",
      "loss: 0.708406  [124800/175341]\n",
      "loss: 0.656257  [126400/175341]\n",
      "loss: 0.405119  [128000/175341]\n",
      "loss: 0.602601  [129600/175341]\n",
      "loss: 0.872372  [131200/175341]\n",
      "loss: 0.486165  [132800/175341]\n",
      "loss: 0.701255  [134400/175341]\n",
      "loss: 0.792559  [136000/175341]\n",
      "loss: 0.416005  [137600/175341]\n",
      "loss: 0.406532  [139200/175341]\n",
      "loss: 0.289023  [140800/175341]\n",
      "loss: 0.246724  [142400/175341]\n",
      "loss: 0.768028  [144000/175341]\n",
      "loss: 0.827724  [145600/175341]\n",
      "loss: 0.331554  [147200/175341]\n",
      "loss: 0.825231  [148800/175341]\n",
      "loss: 0.496146  [150400/175341]\n",
      "loss: 0.713649  [152000/175341]\n",
      "loss: 0.553863  [153600/175341]\n",
      "loss: 0.183555  [155200/175341]\n",
      "loss: 0.683289  [156800/175341]\n",
      "loss: 0.673050  [158400/175341]\n",
      "loss: 0.594220  [160000/175341]\n",
      "loss: 0.426470  [161600/175341]\n",
      "loss: 0.676861  [163200/175341]\n",
      "loss: 0.152157  [164800/175341]\n",
      "loss: 0.613621  [166400/175341]\n",
      "loss: 0.398145  [168000/175341]\n",
      "loss: 0.612366  [169600/175341]\n",
      "loss: 0.711480  [171200/175341]\n",
      "loss: 0.381437  [172800/175341]\n",
      "loss: 0.690173  [174400/175341]\n",
      "Train Accuracy: 77.1884%\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.636033, F1-score: 71.16%, Macro_F1-Score:  33.06%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.829969  [    0/175341]\n",
      "loss: 0.427772  [ 1600/175341]\n",
      "loss: 0.095593  [ 3200/175341]\n",
      "loss: 0.706548  [ 4800/175341]\n",
      "loss: 0.456227  [ 6400/175341]\n",
      "loss: 0.527148  [ 8000/175341]\n",
      "loss: 0.571231  [ 9600/175341]\n",
      "loss: 0.861518  [11200/175341]\n",
      "loss: 0.559421  [12800/175341]\n",
      "loss: 1.304061  [14400/175341]\n",
      "loss: 0.686805  [16000/175341]\n",
      "loss: 0.414566  [17600/175341]\n",
      "loss: 0.578327  [19200/175341]\n",
      "loss: 0.426244  [20800/175341]\n",
      "loss: 0.627855  [22400/175341]\n",
      "loss: 0.520086  [24000/175341]\n",
      "loss: 1.210452  [25600/175341]\n",
      "loss: 0.665288  [27200/175341]\n",
      "loss: 0.568821  [28800/175341]\n",
      "loss: 0.708414  [30400/175341]\n",
      "loss: 0.642201  [32000/175341]\n",
      "loss: 0.350604  [33600/175341]\n",
      "loss: 1.117056  [35200/175341]\n",
      "loss: 0.742078  [36800/175341]\n",
      "loss: 0.461298  [38400/175341]\n",
      "loss: 0.968677  [40000/175341]\n",
      "loss: 0.334463  [41600/175341]\n",
      "loss: 0.425058  [43200/175341]\n",
      "loss: 0.927324  [44800/175341]\n",
      "loss: 0.469726  [46400/175341]\n",
      "loss: 0.355696  [48000/175341]\n",
      "loss: 0.186414  [49600/175341]\n",
      "loss: 0.329537  [51200/175341]\n",
      "loss: 0.282908  [52800/175341]\n",
      "loss: 0.320990  [54400/175341]\n",
      "loss: 0.304818  [56000/175341]\n",
      "loss: 0.560336  [57600/175341]\n",
      "loss: 0.692075  [59200/175341]\n",
      "loss: 0.919687  [60800/175341]\n",
      "loss: 0.585577  [62400/175341]\n",
      "loss: 0.650993  [64000/175341]\n",
      "loss: 0.548466  [65600/175341]\n",
      "loss: 0.634339  [67200/175341]\n",
      "loss: 0.677479  [68800/175341]\n",
      "loss: 0.872793  [70400/175341]\n",
      "loss: 0.349657  [72000/175341]\n",
      "loss: 0.358022  [73600/175341]\n",
      "loss: 0.545284  [75200/175341]\n",
      "loss: 0.585915  [76800/175341]\n",
      "loss: 0.784961  [78400/175341]\n",
      "loss: 0.845275  [80000/175341]\n",
      "loss: 0.596176  [81600/175341]\n",
      "loss: 0.850814  [83200/175341]\n",
      "loss: 0.923085  [84800/175341]\n",
      "loss: 0.494653  [86400/175341]\n",
      "loss: 0.369622  [88000/175341]\n",
      "loss: 0.518226  [89600/175341]\n",
      "loss: 0.707410  [91200/175341]\n",
      "loss: 0.376276  [92800/175341]\n",
      "loss: 0.548775  [94400/175341]\n",
      "loss: 0.613201  [96000/175341]\n",
      "loss: 0.466818  [97600/175341]\n",
      "loss: 0.402249  [99200/175341]\n",
      "loss: 0.804333  [100800/175341]\n",
      "loss: 0.582420  [102400/175341]\n",
      "loss: 0.945553  [104000/175341]\n",
      "loss: 0.463456  [105600/175341]\n",
      "loss: 0.709561  [107200/175341]\n",
      "loss: 0.295760  [108800/175341]\n",
      "loss: 0.453744  [110400/175341]\n",
      "loss: 0.403252  [112000/175341]\n",
      "loss: 0.443763  [113600/175341]\n",
      "loss: 0.899472  [115200/175341]\n",
      "loss: 0.682491  [116800/175341]\n",
      "loss: 0.860798  [118400/175341]\n",
      "loss: 0.502875  [120000/175341]\n",
      "loss: 0.376776  [121600/175341]\n",
      "loss: 0.573139  [123200/175341]\n",
      "loss: 0.632273  [124800/175341]\n",
      "loss: 0.223422  [126400/175341]\n",
      "loss: 0.437890  [128000/175341]\n",
      "loss: 0.399425  [129600/175341]\n",
      "loss: 0.442152  [131200/175341]\n",
      "loss: 0.486095  [132800/175341]\n",
      "loss: 0.302248  [134400/175341]\n",
      "loss: 0.889163  [136000/175341]\n",
      "loss: 0.390751  [137600/175341]\n",
      "loss: 0.535853  [139200/175341]\n",
      "loss: 0.427416  [140800/175341]\n",
      "loss: 0.771852  [142400/175341]\n",
      "loss: 0.371998  [144000/175341]\n",
      "loss: 0.243489  [145600/175341]\n",
      "loss: 0.454678  [147200/175341]\n",
      "loss: 0.538396  [148800/175341]\n",
      "loss: 0.760448  [150400/175341]\n",
      "loss: 0.706566  [152000/175341]\n",
      "loss: 0.363837  [153600/175341]\n",
      "loss: 0.554286  [155200/175341]\n",
      "loss: 0.796562  [156800/175341]\n",
      "loss: 0.654588  [158400/175341]\n",
      "loss: 0.617407  [160000/175341]\n",
      "loss: 0.058802  [161600/175341]\n",
      "loss: 0.456679  [163200/175341]\n",
      "loss: 0.428992  [164800/175341]\n",
      "loss: 0.468095  [166400/175341]\n",
      "loss: 0.279975  [168000/175341]\n",
      "loss: 0.586734  [169600/175341]\n",
      "loss: 0.163805  [171200/175341]\n",
      "loss: 0.396214  [172800/175341]\n",
      "loss: 0.569042  [174400/175341]\n",
      "Train Accuracy: 77.5603%\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.631737, F1-score: 72.19%, Macro_F1-Score:  33.30%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.856039  [    0/175341]\n",
      "loss: 0.318018  [ 1600/175341]\n",
      "loss: 0.369796  [ 3200/175341]\n",
      "loss: 0.411694  [ 4800/175341]\n",
      "loss: 0.798105  [ 6400/175341]\n",
      "loss: 0.298384  [ 8000/175341]\n",
      "loss: 0.750366  [ 9600/175341]\n",
      "loss: 0.844201  [11200/175341]\n",
      "loss: 0.124038  [12800/175341]\n",
      "loss: 0.353143  [14400/175341]\n",
      "loss: 0.755377  [16000/175341]\n",
      "loss: 0.779319  [17600/175341]\n",
      "loss: 0.508264  [19200/175341]\n",
      "loss: 0.474146  [20800/175341]\n",
      "loss: 0.109677  [22400/175341]\n",
      "loss: 0.576202  [24000/175341]\n",
      "loss: 0.844174  [25600/175341]\n",
      "loss: 0.510708  [27200/175341]\n",
      "loss: 0.509387  [28800/175341]\n",
      "loss: 0.777835  [30400/175341]\n",
      "loss: 0.580226  [32000/175341]\n",
      "loss: 0.433127  [33600/175341]\n",
      "loss: 0.659023  [35200/175341]\n",
      "loss: 0.107600  [36800/175341]\n",
      "loss: 0.357923  [38400/175341]\n",
      "loss: 0.701625  [40000/175341]\n",
      "loss: 0.582755  [41600/175341]\n",
      "loss: 0.379674  [43200/175341]\n",
      "loss: 0.494088  [44800/175341]\n",
      "loss: 0.720353  [46400/175341]\n",
      "loss: 0.583695  [48000/175341]\n",
      "loss: 0.364753  [49600/175341]\n",
      "loss: 0.814855  [51200/175341]\n",
      "loss: 0.426027  [52800/175341]\n",
      "loss: 0.651926  [54400/175341]\n",
      "loss: 0.630126  [56000/175341]\n",
      "loss: 0.463091  [57600/175341]\n",
      "loss: 0.718400  [59200/175341]\n",
      "loss: 0.548454  [60800/175341]\n",
      "loss: 1.240057  [62400/175341]\n",
      "loss: 0.559400  [64000/175341]\n",
      "loss: 0.350203  [65600/175341]\n",
      "loss: 1.175210  [67200/175341]\n",
      "loss: 0.229346  [68800/175341]\n",
      "loss: 0.815824  [70400/175341]\n",
      "loss: 0.469709  [72000/175341]\n",
      "loss: 0.413126  [73600/175341]\n",
      "loss: 0.158111  [75200/175341]\n",
      "loss: 0.773449  [76800/175341]\n",
      "loss: 0.451086  [78400/175341]\n",
      "loss: 0.486811  [80000/175341]\n",
      "loss: 0.488830  [81600/175341]\n",
      "loss: 0.391624  [83200/175341]\n",
      "loss: 0.519430  [84800/175341]\n",
      "loss: 0.280045  [86400/175341]\n",
      "loss: 0.513435  [88000/175341]\n",
      "loss: 0.670831  [89600/175341]\n",
      "loss: 0.331449  [91200/175341]\n",
      "loss: 0.469990  [92800/175341]\n",
      "loss: 0.791227  [94400/175341]\n",
      "loss: 0.566851  [96000/175341]\n",
      "loss: 0.771474  [97600/175341]\n",
      "loss: 0.853393  [99200/175341]\n",
      "loss: 0.461061  [100800/175341]\n",
      "loss: 0.390469  [102400/175341]\n",
      "loss: 0.473278  [104000/175341]\n",
      "loss: 0.830119  [105600/175341]\n",
      "loss: 0.534673  [107200/175341]\n",
      "loss: 0.721599  [108800/175341]\n",
      "loss: 0.633185  [110400/175341]\n",
      "loss: 0.464641  [112000/175341]\n",
      "loss: 0.639009  [113600/175341]\n",
      "loss: 0.353325  [115200/175341]\n",
      "loss: 0.426948  [116800/175341]\n",
      "loss: 0.545967  [118400/175341]\n",
      "loss: 1.165011  [120000/175341]\n",
      "loss: 0.717318  [121600/175341]\n",
      "loss: 0.841756  [123200/175341]\n",
      "loss: 0.668777  [124800/175341]\n",
      "loss: 0.663939  [126400/175341]\n",
      "loss: 0.558890  [128000/175341]\n",
      "loss: 1.040273  [129600/175341]\n",
      "loss: 0.778593  [131200/175341]\n",
      "loss: 0.771430  [132800/175341]\n",
      "loss: 0.808372  [134400/175341]\n",
      "loss: 0.743472  [136000/175341]\n",
      "loss: 0.669716  [137600/175341]\n",
      "loss: 0.863500  [139200/175341]\n",
      "loss: 0.342282  [140800/175341]\n",
      "loss: 0.507968  [142400/175341]\n",
      "loss: 0.557203  [144000/175341]\n",
      "loss: 0.796629  [145600/175341]\n",
      "loss: 0.756800  [147200/175341]\n",
      "loss: 0.776137  [148800/175341]\n",
      "loss: 0.734707  [150400/175341]\n",
      "loss: 0.338415  [152000/175341]\n",
      "loss: 0.535261  [153600/175341]\n",
      "loss: 0.271463  [155200/175341]\n",
      "loss: 0.404973  [156800/175341]\n",
      "loss: 0.484884  [158400/175341]\n",
      "loss: 0.273936  [160000/175341]\n",
      "loss: 1.165586  [161600/175341]\n",
      "loss: 0.279937  [163200/175341]\n",
      "loss: 0.791497  [164800/175341]\n",
      "loss: 0.299553  [166400/175341]\n",
      "loss: 0.337959  [168000/175341]\n",
      "loss: 0.470026  [169600/175341]\n",
      "loss: 0.566082  [171200/175341]\n",
      "loss: 0.649854  [172800/175341]\n",
      "loss: 0.453645  [174400/175341]\n",
      "Train Accuracy: 77.8637%\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.635155, F1-score: 72.02%, Macro_F1-Score:  33.60%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.845357  [    0/175341]\n",
      "loss: 0.329497  [ 1600/175341]\n",
      "loss: 0.261743  [ 3200/175341]\n",
      "loss: 0.392005  [ 4800/175341]\n",
      "loss: 0.311732  [ 6400/175341]\n",
      "loss: 0.260968  [ 8000/175341]\n",
      "loss: 0.351851  [ 9600/175341]\n",
      "loss: 0.354004  [11200/175341]\n",
      "loss: 0.810402  [12800/175341]\n",
      "loss: 0.773948  [14400/175341]\n",
      "loss: 0.174102  [16000/175341]\n",
      "loss: 0.263542  [17600/175341]\n",
      "loss: 0.475030  [19200/175341]\n",
      "loss: 0.671175  [20800/175341]\n",
      "loss: 0.528941  [22400/175341]\n",
      "loss: 0.446519  [24000/175341]\n",
      "loss: 0.455249  [25600/175341]\n",
      "loss: 0.865293  [27200/175341]\n",
      "loss: 0.409675  [28800/175341]\n",
      "loss: 0.410381  [30400/175341]\n",
      "loss: 0.556151  [32000/175341]\n",
      "loss: 0.426616  [33600/175341]\n",
      "loss: 0.666331  [35200/175341]\n",
      "loss: 1.012272  [36800/175341]\n",
      "loss: 1.041443  [38400/175341]\n",
      "loss: 0.678783  [40000/175341]\n",
      "loss: 0.514678  [41600/175341]\n",
      "loss: 0.523458  [43200/175341]\n",
      "loss: 0.752329  [44800/175341]\n",
      "loss: 0.407444  [46400/175341]\n",
      "loss: 0.843263  [48000/175341]\n",
      "loss: 0.532240  [49600/175341]\n",
      "loss: 0.315836  [51200/175341]\n",
      "loss: 0.381868  [52800/175341]\n",
      "loss: 0.785434  [54400/175341]\n",
      "loss: 0.560068  [56000/175341]\n",
      "loss: 0.760445  [57600/175341]\n",
      "loss: 1.103651  [59200/175341]\n",
      "loss: 0.372316  [60800/175341]\n",
      "loss: 0.570659  [62400/175341]\n",
      "loss: 0.416205  [64000/175341]\n",
      "loss: 0.593165  [65600/175341]\n",
      "loss: 0.415276  [67200/175341]\n",
      "loss: 0.233967  [68800/175341]\n",
      "loss: 0.475610  [70400/175341]\n",
      "loss: 0.661931  [72000/175341]\n",
      "loss: 0.246250  [73600/175341]\n",
      "loss: 1.135236  [75200/175341]\n",
      "loss: 0.337766  [76800/175341]\n",
      "loss: 0.475992  [78400/175341]\n",
      "loss: 0.783080  [80000/175341]\n",
      "loss: 0.397984  [81600/175341]\n",
      "loss: 0.353351  [83200/175341]\n",
      "loss: 0.460566  [84800/175341]\n",
      "loss: 0.504904  [86400/175341]\n",
      "loss: 0.505901  [88000/175341]\n",
      "loss: 0.500756  [89600/175341]\n",
      "loss: 0.780078  [91200/175341]\n",
      "loss: 0.895653  [92800/175341]\n",
      "loss: 0.587277  [94400/175341]\n",
      "loss: 0.515559  [96000/175341]\n",
      "loss: 0.738526  [97600/175341]\n",
      "loss: 0.622203  [99200/175341]\n",
      "loss: 0.871231  [100800/175341]\n",
      "loss: 0.567185  [102400/175341]\n",
      "loss: 0.356243  [104000/175341]\n",
      "loss: 0.312164  [105600/175341]\n",
      "loss: 0.361901  [107200/175341]\n",
      "loss: 0.467443  [108800/175341]\n",
      "loss: 0.700504  [110400/175341]\n",
      "loss: 0.909842  [112000/175341]\n",
      "loss: 0.366348  [113600/175341]\n",
      "loss: 0.602606  [115200/175341]\n",
      "loss: 0.460397  [116800/175341]\n",
      "loss: 0.524937  [118400/175341]\n",
      "loss: 0.683187  [120000/175341]\n",
      "loss: 0.742149  [121600/175341]\n",
      "loss: 0.267193  [123200/175341]\n",
      "loss: 0.280776  [124800/175341]\n",
      "loss: 0.226882  [126400/175341]\n",
      "loss: 0.391058  [128000/175341]\n",
      "loss: 0.579434  [129600/175341]\n",
      "loss: 0.344317  [131200/175341]\n",
      "loss: 0.260884  [132800/175341]\n",
      "loss: 0.747358  [134400/175341]\n",
      "loss: 0.845260  [136000/175341]\n",
      "loss: 0.165845  [137600/175341]\n",
      "loss: 0.467371  [139200/175341]\n",
      "loss: 0.445944  [140800/175341]\n",
      "loss: 0.275921  [142400/175341]\n",
      "loss: 0.534440  [144000/175341]\n",
      "loss: 0.354609  [145600/175341]\n",
      "loss: 0.233865  [147200/175341]\n",
      "loss: 0.469366  [148800/175341]\n",
      "loss: 0.916700  [150400/175341]\n",
      "loss: 0.651498  [152000/175341]\n",
      "loss: 1.221495  [153600/175341]\n",
      "loss: 0.290209  [155200/175341]\n",
      "loss: 0.540436  [156800/175341]\n",
      "loss: 0.227769  [158400/175341]\n",
      "loss: 0.464718  [160000/175341]\n",
      "loss: 0.686092  [161600/175341]\n",
      "loss: 1.081129  [163200/175341]\n",
      "loss: 0.623048  [164800/175341]\n",
      "loss: 0.424021  [166400/175341]\n",
      "loss: 0.300975  [168000/175341]\n",
      "loss: 0.640730  [169600/175341]\n",
      "loss: 0.644900  [171200/175341]\n",
      "loss: 0.416913  [172800/175341]\n",
      "loss: 0.647003  [174400/175341]\n",
      "Train Accuracy: 78.0873%\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.627184, F1-score: 72.29%, Macro_F1-Score:  33.81%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.508865  [    0/175341]\n",
      "loss: 0.485919  [ 1600/175341]\n",
      "loss: 0.822516  [ 3200/175341]\n",
      "loss: 0.654173  [ 4800/175341]\n",
      "loss: 0.295036  [ 6400/175341]\n",
      "loss: 0.201838  [ 8000/175341]\n",
      "loss: 0.550384  [ 9600/175341]\n",
      "loss: 0.362409  [11200/175341]\n",
      "loss: 0.267561  [12800/175341]\n",
      "loss: 0.672815  [14400/175341]\n",
      "loss: 0.704896  [16000/175341]\n",
      "loss: 0.871177  [17600/175341]\n",
      "loss: 0.529426  [19200/175341]\n",
      "loss: 0.444897  [20800/175341]\n",
      "loss: 0.280184  [22400/175341]\n",
      "loss: 0.290307  [24000/175341]\n",
      "loss: 0.775209  [25600/175341]\n",
      "loss: 0.371361  [27200/175341]\n",
      "loss: 0.490925  [28800/175341]\n",
      "loss: 1.086054  [30400/175341]\n",
      "loss: 0.456760  [32000/175341]\n",
      "loss: 0.345566  [33600/175341]\n",
      "loss: 1.104625  [35200/175341]\n",
      "loss: 0.257573  [36800/175341]\n",
      "loss: 0.426947  [38400/175341]\n",
      "loss: 0.380488  [40000/175341]\n",
      "loss: 0.509073  [41600/175341]\n",
      "loss: 0.450755  [43200/175341]\n",
      "loss: 0.548409  [44800/175341]\n",
      "loss: 0.324707  [46400/175341]\n",
      "loss: 0.300720  [48000/175341]\n",
      "loss: 0.622345  [49600/175341]\n",
      "loss: 0.466114  [51200/175341]\n",
      "loss: 1.540674  [52800/175341]\n",
      "loss: 0.844818  [54400/175341]\n",
      "loss: 0.279482  [56000/175341]\n",
      "loss: 0.426756  [57600/175341]\n",
      "loss: 0.404278  [59200/175341]\n",
      "loss: 0.510397  [60800/175341]\n",
      "loss: 0.954158  [62400/175341]\n",
      "loss: 0.851545  [64000/175341]\n",
      "loss: 0.622993  [65600/175341]\n",
      "loss: 0.929404  [67200/175341]\n",
      "loss: 0.536401  [68800/175341]\n",
      "loss: 0.583064  [70400/175341]\n",
      "loss: 0.899929  [72000/175341]\n",
      "loss: 0.795027  [73600/175341]\n",
      "loss: 0.934568  [75200/175341]\n",
      "loss: 0.596853  [76800/175341]\n",
      "loss: 0.319672  [78400/175341]\n",
      "loss: 0.343709  [80000/175341]\n",
      "loss: 0.646288  [81600/175341]\n",
      "loss: 0.704039  [83200/175341]\n",
      "loss: 0.630373  [84800/175341]\n",
      "loss: 0.306707  [86400/175341]\n",
      "loss: 0.543359  [88000/175341]\n",
      "loss: 0.386602  [89600/175341]\n",
      "loss: 0.205901  [91200/175341]\n",
      "loss: 0.777912  [92800/175341]\n",
      "loss: 0.601076  [94400/175341]\n",
      "loss: 0.305408  [96000/175341]\n",
      "loss: 0.811017  [97600/175341]\n",
      "loss: 0.336530  [99200/175341]\n",
      "loss: 1.023838  [100800/175341]\n",
      "loss: 0.326059  [102400/175341]\n",
      "loss: 0.351557  [104000/175341]\n",
      "loss: 0.570688  [105600/175341]\n",
      "loss: 0.818001  [107200/175341]\n",
      "loss: 0.230478  [108800/175341]\n",
      "loss: 0.565545  [110400/175341]\n",
      "loss: 0.638902  [112000/175341]\n",
      "loss: 0.950757  [113600/175341]\n",
      "loss: 0.453375  [115200/175341]\n",
      "loss: 0.812963  [116800/175341]\n",
      "loss: 0.340881  [118400/175341]\n",
      "loss: 0.848447  [120000/175341]\n",
      "loss: 0.774945  [121600/175341]\n",
      "loss: 0.373331  [123200/175341]\n",
      "loss: 0.409008  [124800/175341]\n",
      "loss: 0.789400  [126400/175341]\n",
      "loss: 0.582595  [128000/175341]\n",
      "loss: 0.422569  [129600/175341]\n",
      "loss: 0.615325  [131200/175341]\n",
      "loss: 0.459254  [132800/175341]\n",
      "loss: 0.208541  [134400/175341]\n",
      "loss: 0.585915  [136000/175341]\n",
      "loss: 0.150833  [137600/175341]\n",
      "loss: 0.547962  [139200/175341]\n",
      "loss: 0.482727  [140800/175341]\n",
      "loss: 0.177073  [142400/175341]\n",
      "loss: 0.599512  [144000/175341]\n",
      "loss: 0.592571  [145600/175341]\n",
      "loss: 0.931218  [147200/175341]\n",
      "loss: 0.597911  [148800/175341]\n",
      "loss: 0.638554  [150400/175341]\n",
      "loss: 0.534233  [152000/175341]\n",
      "loss: 0.531941  [153600/175341]\n",
      "loss: 0.549345  [155200/175341]\n",
      "loss: 0.418471  [156800/175341]\n",
      "loss: 0.386137  [158400/175341]\n",
      "loss: 0.312079  [160000/175341]\n",
      "loss: 0.384997  [161600/175341]\n",
      "loss: 0.763719  [163200/175341]\n",
      "loss: 0.382570  [164800/175341]\n",
      "loss: 0.540690  [166400/175341]\n",
      "loss: 0.492264  [168000/175341]\n",
      "loss: 0.826889  [169600/175341]\n",
      "loss: 0.599385  [171200/175341]\n",
      "loss: 1.102644  [172800/175341]\n",
      "loss: 0.239784  [174400/175341]\n",
      "Train Accuracy: 78.3040%\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.624743, F1-score: 72.62%, Macro_F1-Score:  33.90%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.506607  [    0/175341]\n",
      "loss: 0.550102  [ 1600/175341]\n",
      "loss: 0.406270  [ 3200/175341]\n",
      "loss: 0.615924  [ 4800/175341]\n",
      "loss: 0.823476  [ 6400/175341]\n",
      "loss: 0.518051  [ 8000/175341]\n",
      "loss: 0.315576  [ 9600/175341]\n",
      "loss: 0.373305  [11200/175341]\n",
      "loss: 0.735605  [12800/175341]\n",
      "loss: 0.556696  [14400/175341]\n",
      "loss: 0.759948  [16000/175341]\n",
      "loss: 0.568525  [17600/175341]\n",
      "loss: 0.681255  [19200/175341]\n",
      "loss: 0.546450  [20800/175341]\n",
      "loss: 0.428493  [22400/175341]\n",
      "loss: 0.596241  [24000/175341]\n",
      "loss: 0.450981  [25600/175341]\n",
      "loss: 0.857273  [27200/175341]\n",
      "loss: 0.725767  [28800/175341]\n",
      "loss: 0.269093  [30400/175341]\n",
      "loss: 0.344391  [32000/175341]\n",
      "loss: 0.800741  [33600/175341]\n",
      "loss: 0.295382  [35200/175341]\n",
      "loss: 0.389673  [36800/175341]\n",
      "loss: 0.462857  [38400/175341]\n",
      "loss: 0.536107  [40000/175341]\n",
      "loss: 0.452830  [41600/175341]\n",
      "loss: 0.680896  [43200/175341]\n",
      "loss: 0.463181  [44800/175341]\n",
      "loss: 0.549764  [46400/175341]\n",
      "loss: 0.704656  [48000/175341]\n",
      "loss: 0.558983  [49600/175341]\n",
      "loss: 0.424472  [51200/175341]\n",
      "loss: 0.604282  [52800/175341]\n",
      "loss: 0.572569  [54400/175341]\n",
      "loss: 0.599550  [56000/175341]\n",
      "loss: 0.581492  [57600/175341]\n",
      "loss: 0.486036  [59200/175341]\n",
      "loss: 0.483336  [60800/175341]\n",
      "loss: 0.741087  [62400/175341]\n",
      "loss: 0.308214  [64000/175341]\n",
      "loss: 0.427121  [65600/175341]\n",
      "loss: 0.695803  [67200/175341]\n",
      "loss: 0.691673  [68800/175341]\n",
      "loss: 0.908246  [70400/175341]\n",
      "loss: 0.599091  [72000/175341]\n",
      "loss: 0.720352  [73600/175341]\n",
      "loss: 0.409071  [75200/175341]\n",
      "loss: 0.667201  [76800/175341]\n",
      "loss: 0.375237  [78400/175341]\n",
      "loss: 0.260284  [80000/175341]\n",
      "loss: 0.893675  [81600/175341]\n",
      "loss: 0.187918  [83200/175341]\n",
      "loss: 0.396201  [84800/175341]\n",
      "loss: 0.377860  [86400/175341]\n",
      "loss: 0.715017  [88000/175341]\n",
      "loss: 0.484109  [89600/175341]\n",
      "loss: 0.170852  [91200/175341]\n",
      "loss: 0.546364  [92800/175341]\n",
      "loss: 0.434738  [94400/175341]\n",
      "loss: 0.273152  [96000/175341]\n",
      "loss: 0.290488  [97600/175341]\n",
      "loss: 0.553743  [99200/175341]\n",
      "loss: 0.841631  [100800/175341]\n",
      "loss: 0.522200  [102400/175341]\n",
      "loss: 0.543548  [104000/175341]\n",
      "loss: 0.899138  [105600/175341]\n",
      "loss: 0.696776  [107200/175341]\n",
      "loss: 0.704927  [108800/175341]\n",
      "loss: 0.329427  [110400/175341]\n",
      "loss: 0.735751  [112000/175341]\n",
      "loss: 0.481944  [113600/175341]\n",
      "loss: 0.439001  [115200/175341]\n",
      "loss: 0.356646  [116800/175341]\n",
      "loss: 0.466170  [118400/175341]\n",
      "loss: 0.635649  [120000/175341]\n",
      "loss: 0.798803  [121600/175341]\n",
      "loss: 0.642030  [123200/175341]\n",
      "loss: 0.562825  [124800/175341]\n",
      "loss: 0.254595  [126400/175341]\n",
      "loss: 0.524536  [128000/175341]\n",
      "loss: 0.646826  [129600/175341]\n",
      "loss: 0.722600  [131200/175341]\n",
      "loss: 0.462092  [132800/175341]\n",
      "loss: 0.371933  [134400/175341]\n",
      "loss: 0.436876  [136000/175341]\n",
      "loss: 0.185473  [137600/175341]\n",
      "loss: 0.825062  [139200/175341]\n",
      "loss: 0.322812  [140800/175341]\n",
      "loss: 0.305482  [142400/175341]\n",
      "loss: 0.497896  [144000/175341]\n",
      "loss: 0.436316  [145600/175341]\n",
      "loss: 0.168829  [147200/175341]\n",
      "loss: 0.361929  [148800/175341]\n",
      "loss: 0.436020  [150400/175341]\n",
      "loss: 0.724863  [152000/175341]\n",
      "loss: 0.284835  [153600/175341]\n",
      "loss: 0.516430  [155200/175341]\n",
      "loss: 0.648214  [156800/175341]\n",
      "loss: 0.430117  [158400/175341]\n",
      "loss: 0.332541  [160000/175341]\n",
      "loss: 0.266145  [161600/175341]\n",
      "loss: 0.518410  [163200/175341]\n",
      "loss: 0.414023  [164800/175341]\n",
      "loss: 0.768368  [166400/175341]\n",
      "loss: 0.466701  [168000/175341]\n",
      "loss: 0.340744  [169600/175341]\n",
      "loss: 0.765168  [171200/175341]\n",
      "loss: 0.284469  [172800/175341]\n",
      "loss: 0.527968  [174400/175341]\n",
      "Train Accuracy: 78.4762%\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.640324, F1-score: 71.70%, Macro_F1-Score:  34.14%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.399790  [    0/175341]\n",
      "loss: 0.375521  [ 1600/175341]\n",
      "loss: 0.619286  [ 3200/175341]\n",
      "loss: 0.710616  [ 4800/175341]\n",
      "loss: 0.469650  [ 6400/175341]\n",
      "loss: 0.718281  [ 8000/175341]\n",
      "loss: 0.551463  [ 9600/175341]\n",
      "loss: 0.664432  [11200/175341]\n",
      "loss: 0.342338  [12800/175341]\n",
      "loss: 0.590249  [14400/175341]\n",
      "loss: 0.561480  [16000/175341]\n",
      "loss: 0.600591  [17600/175341]\n",
      "loss: 0.652657  [19200/175341]\n",
      "loss: 0.539791  [20800/175341]\n",
      "loss: 0.264963  [22400/175341]\n",
      "loss: 0.419226  [24000/175341]\n",
      "loss: 0.919238  [25600/175341]\n",
      "loss: 0.653761  [27200/175341]\n",
      "loss: 0.470533  [28800/175341]\n",
      "loss: 0.721365  [30400/175341]\n",
      "loss: 0.692088  [32000/175341]\n",
      "loss: 0.716795  [33600/175341]\n",
      "loss: 0.845406  [35200/175341]\n",
      "loss: 0.644329  [36800/175341]\n",
      "loss: 0.172606  [38400/175341]\n",
      "loss: 0.450246  [40000/175341]\n",
      "loss: 0.572737  [41600/175341]\n",
      "loss: 0.384372  [43200/175341]\n",
      "loss: 0.319983  [44800/175341]\n",
      "loss: 0.514080  [46400/175341]\n",
      "loss: 0.660961  [48000/175341]\n",
      "loss: 0.413204  [49600/175341]\n",
      "loss: 0.602461  [51200/175341]\n",
      "loss: 0.580291  [52800/175341]\n",
      "loss: 0.729518  [54400/175341]\n",
      "loss: 0.690613  [56000/175341]\n",
      "loss: 0.553557  [57600/175341]\n",
      "loss: 0.363960  [59200/175341]\n",
      "loss: 0.294487  [60800/175341]\n",
      "loss: 0.489717  [62400/175341]\n",
      "loss: 0.151682  [64000/175341]\n",
      "loss: 0.506403  [65600/175341]\n",
      "loss: 0.681478  [67200/175341]\n",
      "loss: 0.719410  [68800/175341]\n",
      "loss: 0.646233  [70400/175341]\n",
      "loss: 0.719016  [72000/175341]\n",
      "loss: 0.301618  [73600/175341]\n",
      "loss: 0.485915  [75200/175341]\n",
      "loss: 0.695121  [76800/175341]\n",
      "loss: 0.783261  [78400/175341]\n",
      "loss: 1.289781  [80000/175341]\n",
      "loss: 0.471534  [81600/175341]\n",
      "loss: 0.744383  [83200/175341]\n",
      "loss: 0.860791  [84800/175341]\n",
      "loss: 0.317323  [86400/175341]\n",
      "loss: 0.332611  [88000/175341]\n",
      "loss: 0.263158  [89600/175341]\n",
      "loss: 0.396110  [91200/175341]\n",
      "loss: 0.319152  [92800/175341]\n",
      "loss: 0.195106  [94400/175341]\n",
      "loss: 0.417229  [96000/175341]\n",
      "loss: 0.489175  [97600/175341]\n",
      "loss: 0.545047  [99200/175341]\n",
      "loss: 0.406689  [100800/175341]\n",
      "loss: 0.450667  [102400/175341]\n",
      "loss: 0.365809  [104000/175341]\n",
      "loss: 0.497186  [105600/175341]\n",
      "loss: 0.498530  [107200/175341]\n",
      "loss: 0.344773  [108800/175341]\n",
      "loss: 0.186576  [110400/175341]\n",
      "loss: 0.823076  [112000/175341]\n",
      "loss: 0.367153  [113600/175341]\n",
      "loss: 0.269562  [115200/175341]\n",
      "loss: 0.177110  [116800/175341]\n",
      "loss: 0.633582  [118400/175341]\n",
      "loss: 0.306956  [120000/175341]\n",
      "loss: 0.693507  [121600/175341]\n",
      "loss: 0.547433  [123200/175341]\n",
      "loss: 0.629831  [124800/175341]\n",
      "loss: 0.889681  [126400/175341]\n",
      "loss: 0.515353  [128000/175341]\n",
      "loss: 0.667120  [129600/175341]\n",
      "loss: 0.468319  [131200/175341]\n",
      "loss: 0.246047  [132800/175341]\n",
      "loss: 0.635051  [134400/175341]\n",
      "loss: 0.409513  [136000/175341]\n",
      "loss: 0.731225  [137600/175341]\n",
      "loss: 0.340286  [139200/175341]\n",
      "loss: 0.229592  [140800/175341]\n",
      "loss: 0.284688  [142400/175341]\n",
      "loss: 1.328810  [144000/175341]\n",
      "loss: 0.248177  [145600/175341]\n",
      "loss: 0.487375  [147200/175341]\n",
      "loss: 0.283094  [148800/175341]\n",
      "loss: 0.329759  [150400/175341]\n",
      "loss: 0.444369  [152000/175341]\n",
      "loss: 0.704390  [153600/175341]\n",
      "loss: 0.588406  [155200/175341]\n",
      "loss: 0.192826  [156800/175341]\n",
      "loss: 0.231639  [158400/175341]\n",
      "loss: 0.569310  [160000/175341]\n",
      "loss: 0.442714  [161600/175341]\n",
      "loss: 0.316420  [163200/175341]\n",
      "loss: 0.695862  [164800/175341]\n",
      "loss: 0.276499  [166400/175341]\n",
      "loss: 0.394548  [168000/175341]\n",
      "loss: 0.583022  [169600/175341]\n",
      "loss: 0.753799  [171200/175341]\n",
      "loss: 0.533125  [172800/175341]\n",
      "loss: 0.674292  [174400/175341]\n",
      "Train Accuracy: 78.5760%\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.623834, F1-score: 72.17%, Macro_F1-Score:  33.91%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.806032  [    0/175341]\n",
      "loss: 0.543910  [ 1600/175341]\n",
      "loss: 0.338611  [ 3200/175341]\n",
      "loss: 0.176648  [ 4800/175341]\n",
      "loss: 0.424336  [ 6400/175341]\n",
      "loss: 0.519733  [ 8000/175341]\n",
      "loss: 0.694217  [ 9600/175341]\n",
      "loss: 0.320732  [11200/175341]\n",
      "loss: 0.589608  [12800/175341]\n",
      "loss: 0.495952  [14400/175341]\n",
      "loss: 0.416215  [16000/175341]\n",
      "loss: 0.423783  [17600/175341]\n",
      "loss: 0.795202  [19200/175341]\n",
      "loss: 0.537970  [20800/175341]\n",
      "loss: 0.158816  [22400/175341]\n",
      "loss: 0.363976  [24000/175341]\n",
      "loss: 0.736227  [25600/175341]\n",
      "loss: 0.160735  [27200/175341]\n",
      "loss: 0.488817  [28800/175341]\n",
      "loss: 0.568419  [30400/175341]\n",
      "loss: 0.253942  [32000/175341]\n",
      "loss: 0.272218  [33600/175341]\n",
      "loss: 0.354543  [35200/175341]\n",
      "loss: 0.620383  [36800/175341]\n",
      "loss: 0.296184  [38400/175341]\n",
      "loss: 0.528138  [40000/175341]\n",
      "loss: 1.204608  [41600/175341]\n",
      "loss: 0.533492  [43200/175341]\n",
      "loss: 1.145668  [44800/175341]\n",
      "loss: 0.202227  [46400/175341]\n",
      "loss: 0.856894  [48000/175341]\n",
      "loss: 1.105113  [49600/175341]\n",
      "loss: 0.508749  [51200/175341]\n",
      "loss: 0.188281  [52800/175341]\n",
      "loss: 0.871665  [54400/175341]\n",
      "loss: 0.354375  [56000/175341]\n",
      "loss: 0.269071  [57600/175341]\n",
      "loss: 0.359314  [59200/175341]\n",
      "loss: 0.500229  [60800/175341]\n",
      "loss: 0.927332  [62400/175341]\n",
      "loss: 0.383103  [64000/175341]\n",
      "loss: 0.646957  [65600/175341]\n",
      "loss: 0.603015  [67200/175341]\n",
      "loss: 0.112228  [68800/175341]\n",
      "loss: 0.731303  [70400/175341]\n",
      "loss: 0.203464  [72000/175341]\n",
      "loss: 0.390072  [73600/175341]\n",
      "loss: 0.545954  [75200/175341]\n",
      "loss: 0.426709  [76800/175341]\n",
      "loss: 0.396001  [78400/175341]\n",
      "loss: 0.320315  [80000/175341]\n",
      "loss: 0.545944  [81600/175341]\n",
      "loss: 0.277859  [83200/175341]\n",
      "loss: 0.565817  [84800/175341]\n",
      "loss: 0.467290  [86400/175341]\n",
      "loss: 0.423782  [88000/175341]\n",
      "loss: 0.421420  [89600/175341]\n",
      "loss: 0.436463  [91200/175341]\n",
      "loss: 0.935434  [92800/175341]\n",
      "loss: 0.532476  [94400/175341]\n",
      "loss: 0.364655  [96000/175341]\n",
      "loss: 0.138017  [97600/175341]\n",
      "loss: 0.637898  [99200/175341]\n",
      "loss: 0.230428  [100800/175341]\n",
      "loss: 0.512351  [102400/175341]\n",
      "loss: 0.577892  [104000/175341]\n",
      "loss: 0.694685  [105600/175341]\n",
      "loss: 0.652839  [107200/175341]\n",
      "loss: 0.474676  [108800/175341]\n",
      "loss: 0.489214  [110400/175341]\n",
      "loss: 0.275277  [112000/175341]\n",
      "loss: 0.473674  [113600/175341]\n",
      "loss: 0.270038  [115200/175341]\n",
      "loss: 0.527814  [116800/175341]\n",
      "loss: 0.833498  [118400/175341]\n",
      "loss: 0.604702  [120000/175341]\n",
      "loss: 0.225789  [121600/175341]\n",
      "loss: 0.872870  [123200/175341]\n",
      "loss: 0.699834  [124800/175341]\n",
      "loss: 0.955987  [126400/175341]\n",
      "loss: 0.535235  [128000/175341]\n",
      "loss: 0.529931  [129600/175341]\n",
      "loss: 0.871345  [131200/175341]\n",
      "loss: 0.562181  [132800/175341]\n",
      "loss: 0.539124  [134400/175341]\n",
      "loss: 0.460993  [136000/175341]\n",
      "loss: 0.271193  [137600/175341]\n",
      "loss: 0.225272  [139200/175341]\n",
      "loss: 0.471875  [140800/175341]\n",
      "loss: 0.827042  [142400/175341]\n",
      "loss: 0.544841  [144000/175341]\n",
      "loss: 0.395150  [145600/175341]\n",
      "loss: 0.361498  [147200/175341]\n",
      "loss: 0.446751  [148800/175341]\n",
      "loss: 0.340070  [150400/175341]\n",
      "loss: 0.200901  [152000/175341]\n",
      "loss: 0.522441  [153600/175341]\n",
      "loss: 1.044107  [155200/175341]\n",
      "loss: 0.737366  [156800/175341]\n",
      "loss: 0.416111  [158400/175341]\n",
      "loss: 0.376030  [160000/175341]\n",
      "loss: 1.650676  [161600/175341]\n",
      "loss: 0.527222  [163200/175341]\n",
      "loss: 0.516639  [164800/175341]\n",
      "loss: 0.534620  [166400/175341]\n",
      "loss: 0.851513  [168000/175341]\n",
      "loss: 0.317153  [169600/175341]\n",
      "loss: 1.114966  [171200/175341]\n",
      "loss: 0.501230  [172800/175341]\n",
      "loss: 0.465588  [174400/175341]\n",
      "Train Accuracy: 78.7169%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.589889, F1-score: 73.44%, Macro_F1-Score:  34.12%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.451639  [    0/175341]\n",
      "loss: 0.895889  [ 1600/175341]\n",
      "loss: 0.667342  [ 3200/175341]\n",
      "loss: 0.208970  [ 4800/175341]\n",
      "loss: 0.782572  [ 6400/175341]\n",
      "loss: 0.711739  [ 8000/175341]\n",
      "loss: 0.220074  [ 9600/175341]\n",
      "loss: 0.330618  [11200/175341]\n",
      "loss: 0.281340  [12800/175341]\n",
      "loss: 0.456455  [14400/175341]\n",
      "loss: 0.398243  [16000/175341]\n",
      "loss: 1.003286  [17600/175341]\n",
      "loss: 0.220212  [19200/175341]\n",
      "loss: 0.522126  [20800/175341]\n",
      "loss: 0.773528  [22400/175341]\n",
      "loss: 0.365267  [24000/175341]\n",
      "loss: 0.402508  [25600/175341]\n",
      "loss: 0.418959  [27200/175341]\n",
      "loss: 0.509555  [28800/175341]\n",
      "loss: 0.350370  [30400/175341]\n",
      "loss: 0.276576  [32000/175341]\n",
      "loss: 0.535127  [33600/175341]\n",
      "loss: 0.781890  [35200/175341]\n",
      "loss: 0.621017  [36800/175341]\n",
      "loss: 0.605056  [38400/175341]\n",
      "loss: 0.390304  [40000/175341]\n",
      "loss: 0.887835  [41600/175341]\n",
      "loss: 0.463731  [43200/175341]\n",
      "loss: 0.477440  [44800/175341]\n",
      "loss: 0.181425  [46400/175341]\n",
      "loss: 0.309195  [48000/175341]\n",
      "loss: 0.702841  [49600/175341]\n",
      "loss: 0.383136  [51200/175341]\n",
      "loss: 0.908481  [52800/175341]\n",
      "loss: 0.546948  [54400/175341]\n",
      "loss: 0.333594  [56000/175341]\n",
      "loss: 0.145900  [57600/175341]\n",
      "loss: 0.855198  [59200/175341]\n",
      "loss: 0.341456  [60800/175341]\n",
      "loss: 0.748205  [62400/175341]\n",
      "loss: 0.581083  [64000/175341]\n",
      "loss: 0.417075  [65600/175341]\n",
      "loss: 0.305334  [67200/175341]\n",
      "loss: 1.138734  [68800/175341]\n",
      "loss: 0.681725  [70400/175341]\n",
      "loss: 0.398066  [72000/175341]\n",
      "loss: 0.387162  [73600/175341]\n",
      "loss: 0.423385  [75200/175341]\n",
      "loss: 0.377344  [76800/175341]\n",
      "loss: 0.822231  [78400/175341]\n",
      "loss: 0.460865  [80000/175341]\n",
      "loss: 0.773283  [81600/175341]\n",
      "loss: 0.538398  [83200/175341]\n",
      "loss: 0.849724  [84800/175341]\n",
      "loss: 0.342564  [86400/175341]\n",
      "loss: 0.673962  [88000/175341]\n",
      "loss: 0.585256  [89600/175341]\n",
      "loss: 0.531315  [91200/175341]\n",
      "loss: 0.455215  [92800/175341]\n",
      "loss: 1.286109  [94400/175341]\n",
      "loss: 0.853413  [96000/175341]\n",
      "loss: 0.463617  [97600/175341]\n",
      "loss: 0.453417  [99200/175341]\n",
      "loss: 0.637589  [100800/175341]\n",
      "loss: 0.585069  [102400/175341]\n",
      "loss: 0.538686  [104000/175341]\n",
      "loss: 0.826951  [105600/175341]\n",
      "loss: 0.483575  [107200/175341]\n",
      "loss: 0.640219  [108800/175341]\n",
      "loss: 0.379556  [110400/175341]\n",
      "loss: 0.601501  [112000/175341]\n",
      "loss: 0.935126  [113600/175341]\n",
      "loss: 0.772177  [115200/175341]\n",
      "loss: 0.845448  [116800/175341]\n",
      "loss: 0.292122  [118400/175341]\n",
      "loss: 0.597201  [120000/175341]\n",
      "loss: 0.507735  [121600/175341]\n",
      "loss: 0.563264  [123200/175341]\n",
      "loss: 0.416477  [124800/175341]\n",
      "loss: 0.665043  [126400/175341]\n",
      "loss: 0.475875  [128000/175341]\n",
      "loss: 0.645552  [129600/175341]\n",
      "loss: 0.670754  [131200/175341]\n",
      "loss: 0.985554  [132800/175341]\n",
      "loss: 0.714016  [134400/175341]\n",
      "loss: 0.602340  [136000/175341]\n",
      "loss: 0.326993  [137600/175341]\n",
      "loss: 0.450969  [139200/175341]\n",
      "loss: 0.888672  [140800/175341]\n",
      "loss: 0.503703  [142400/175341]\n",
      "loss: 0.374484  [144000/175341]\n",
      "loss: 0.577447  [145600/175341]\n",
      "loss: 0.728205  [147200/175341]\n",
      "loss: 0.572466  [148800/175341]\n",
      "loss: 0.178772  [150400/175341]\n",
      "loss: 0.904677  [152000/175341]\n",
      "loss: 0.364410  [153600/175341]\n",
      "loss: 0.431747  [155200/175341]\n",
      "loss: 0.826608  [156800/175341]\n",
      "loss: 0.675039  [158400/175341]\n",
      "loss: 0.639588  [160000/175341]\n",
      "loss: 0.765145  [161600/175341]\n",
      "loss: 0.182102  [163200/175341]\n",
      "loss: 0.518553  [164800/175341]\n",
      "loss: 0.688942  [166400/175341]\n",
      "loss: 0.517030  [168000/175341]\n",
      "loss: 0.579310  [169600/175341]\n",
      "loss: 0.574567  [171200/175341]\n",
      "loss: 0.548954  [172800/175341]\n",
      "loss: 0.830903  [174400/175341]\n",
      "Train Accuracy: 78.9091%\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.610764, F1-score: 72.94%, Macro_F1-Score:  33.83%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.938382  [    0/175341]\n",
      "loss: 0.533510  [ 1600/175341]\n",
      "loss: 0.304663  [ 3200/175341]\n",
      "loss: 0.636556  [ 4800/175341]\n",
      "loss: 0.836883  [ 6400/175341]\n",
      "loss: 0.577745  [ 8000/175341]\n",
      "loss: 0.464000  [ 9600/175341]\n",
      "loss: 0.445509  [11200/175341]\n",
      "loss: 0.345011  [12800/175341]\n",
      "loss: 0.716996  [14400/175341]\n",
      "loss: 0.344242  [16000/175341]\n",
      "loss: 0.532192  [17600/175341]\n",
      "loss: 0.951257  [19200/175341]\n",
      "loss: 0.399879  [20800/175341]\n",
      "loss: 0.425838  [22400/175341]\n",
      "loss: 0.384294  [24000/175341]\n",
      "loss: 0.504331  [25600/175341]\n",
      "loss: 0.525129  [27200/175341]\n",
      "loss: 0.204663  [28800/175341]\n",
      "loss: 0.528496  [30400/175341]\n",
      "loss: 0.297161  [32000/175341]\n",
      "loss: 0.440317  [33600/175341]\n",
      "loss: 0.577999  [35200/175341]\n",
      "loss: 0.329822  [36800/175341]\n",
      "loss: 0.635375  [38400/175341]\n",
      "loss: 0.322694  [40000/175341]\n",
      "loss: 0.768386  [41600/175341]\n",
      "loss: 0.494705  [43200/175341]\n",
      "loss: 0.213883  [44800/175341]\n",
      "loss: 0.905774  [46400/175341]\n",
      "loss: 0.585323  [48000/175341]\n",
      "loss: 0.625003  [49600/175341]\n",
      "loss: 0.288939  [51200/175341]\n",
      "loss: 0.813778  [52800/175341]\n",
      "loss: 0.561116  [54400/175341]\n",
      "loss: 0.542351  [56000/175341]\n",
      "loss: 0.645847  [57600/175341]\n",
      "loss: 0.491698  [59200/175341]\n",
      "loss: 0.509336  [60800/175341]\n",
      "loss: 0.414948  [62400/175341]\n",
      "loss: 0.615856  [64000/175341]\n",
      "loss: 0.469771  [65600/175341]\n",
      "loss: 0.272490  [67200/175341]\n",
      "loss: 0.625568  [68800/175341]\n",
      "loss: 0.515347  [70400/175341]\n",
      "loss: 0.541237  [72000/175341]\n",
      "loss: 0.552303  [73600/175341]\n",
      "loss: 0.758987  [75200/175341]\n",
      "loss: 0.363319  [76800/175341]\n",
      "loss: 0.716917  [78400/175341]\n",
      "loss: 0.808504  [80000/175341]\n",
      "loss: 0.869340  [81600/175341]\n",
      "loss: 0.291368  [83200/175341]\n",
      "loss: 0.318318  [84800/175341]\n",
      "loss: 0.440119  [86400/175341]\n",
      "loss: 0.467209  [88000/175341]\n",
      "loss: 0.542044  [89600/175341]\n",
      "loss: 0.290804  [91200/175341]\n",
      "loss: 0.276604  [92800/175341]\n",
      "loss: 0.616874  [94400/175341]\n",
      "loss: 0.395910  [96000/175341]\n",
      "loss: 0.636611  [97600/175341]\n",
      "loss: 0.584013  [99200/175341]\n",
      "loss: 0.334323  [100800/175341]\n",
      "loss: 0.410947  [102400/175341]\n",
      "loss: 0.381244  [104000/175341]\n",
      "loss: 0.651677  [105600/175341]\n",
      "loss: 0.451554  [107200/175341]\n",
      "loss: 0.571153  [108800/175341]\n",
      "loss: 0.403593  [110400/175341]\n",
      "loss: 0.474610  [112000/175341]\n",
      "loss: 0.587409  [113600/175341]\n",
      "loss: 0.300395  [115200/175341]\n",
      "loss: 0.517489  [116800/175341]\n",
      "loss: 0.355675  [118400/175341]\n",
      "loss: 0.545032  [120000/175341]\n",
      "loss: 0.663907  [121600/175341]\n",
      "loss: 0.305009  [123200/175341]\n",
      "loss: 0.289403  [124800/175341]\n",
      "loss: 0.603655  [126400/175341]\n",
      "loss: 0.727872  [128000/175341]\n",
      "loss: 0.408503  [129600/175341]\n",
      "loss: 0.460720  [131200/175341]\n",
      "loss: 0.359890  [132800/175341]\n",
      "loss: 0.722900  [134400/175341]\n",
      "loss: 0.505339  [136000/175341]\n",
      "loss: 0.562318  [137600/175341]\n",
      "loss: 0.824603  [139200/175341]\n",
      "loss: 0.489439  [140800/175341]\n",
      "loss: 0.470227  [142400/175341]\n",
      "loss: 0.852981  [144000/175341]\n",
      "loss: 0.199340  [145600/175341]\n",
      "loss: 0.329102  [147200/175341]\n",
      "loss: 0.442981  [148800/175341]\n",
      "loss: 0.932790  [150400/175341]\n",
      "loss: 0.611513  [152000/175341]\n",
      "loss: 1.014704  [153600/175341]\n",
      "loss: 0.914387  [155200/175341]\n",
      "loss: 0.501204  [156800/175341]\n",
      "loss: 0.568086  [158400/175341]\n",
      "loss: 0.657312  [160000/175341]\n",
      "loss: 0.602611  [161600/175341]\n",
      "loss: 0.308475  [163200/175341]\n",
      "loss: 0.344795  [164800/175341]\n",
      "loss: 0.336369  [166400/175341]\n",
      "loss: 0.840899  [168000/175341]\n",
      "loss: 0.601917  [169600/175341]\n",
      "loss: 0.525973  [171200/175341]\n",
      "loss: 0.551291  [172800/175341]\n",
      "loss: 0.390382  [174400/175341]\n",
      "Train Accuracy: 78.9553%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.590198, F1-score: 73.95%, Macro_F1-Score:  34.32%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.408277  [    0/175341]\n",
      "loss: 0.430689  [ 1600/175341]\n",
      "loss: 0.691933  [ 3200/175341]\n",
      "loss: 1.421391  [ 4800/175341]\n",
      "loss: 0.677547  [ 6400/175341]\n",
      "loss: 0.522823  [ 8000/175341]\n",
      "loss: 0.867632  [ 9600/175341]\n",
      "loss: 0.751115  [11200/175341]\n",
      "loss: 0.843959  [12800/175341]\n",
      "loss: 0.737381  [14400/175341]\n",
      "loss: 0.846133  [16000/175341]\n",
      "loss: 1.016514  [17600/175341]\n",
      "loss: 0.335555  [19200/175341]\n",
      "loss: 0.641260  [20800/175341]\n",
      "loss: 0.778752  [22400/175341]\n",
      "loss: 0.825905  [24000/175341]\n",
      "loss: 0.579874  [25600/175341]\n",
      "loss: 0.481122  [27200/175341]\n",
      "loss: 0.297932  [28800/175341]\n",
      "loss: 0.633823  [30400/175341]\n",
      "loss: 0.886257  [32000/175341]\n",
      "loss: 0.550519  [33600/175341]\n",
      "loss: 0.262169  [35200/175341]\n",
      "loss: 0.418120  [36800/175341]\n",
      "loss: 0.466557  [38400/175341]\n",
      "loss: 0.957189  [40000/175341]\n",
      "loss: 0.631241  [41600/175341]\n",
      "loss: 0.704891  [43200/175341]\n",
      "loss: 0.348021  [44800/175341]\n",
      "loss: 0.438314  [46400/175341]\n",
      "loss: 0.981032  [48000/175341]\n",
      "loss: 0.568054  [49600/175341]\n",
      "loss: 0.934003  [51200/175341]\n",
      "loss: 0.536978  [52800/175341]\n",
      "loss: 0.420991  [54400/175341]\n",
      "loss: 0.502440  [56000/175341]\n",
      "loss: 0.397903  [57600/175341]\n",
      "loss: 0.368545  [59200/175341]\n",
      "loss: 0.514603  [60800/175341]\n",
      "loss: 0.398901  [62400/175341]\n",
      "loss: 0.438285  [64000/175341]\n",
      "loss: 0.751246  [65600/175341]\n",
      "loss: 0.727098  [67200/175341]\n",
      "loss: 0.509509  [68800/175341]\n",
      "loss: 0.502992  [70400/175341]\n",
      "loss: 0.723240  [72000/175341]\n",
      "loss: 0.836436  [73600/175341]\n",
      "loss: 0.572926  [75200/175341]\n",
      "loss: 0.700754  [76800/175341]\n",
      "loss: 0.298714  [78400/175341]\n",
      "loss: 0.336165  [80000/175341]\n",
      "loss: 0.798392  [81600/175341]\n",
      "loss: 0.699608  [83200/175341]\n",
      "loss: 0.876288  [84800/175341]\n",
      "loss: 0.214092  [86400/175341]\n",
      "loss: 0.595702  [88000/175341]\n",
      "loss: 0.663153  [89600/175341]\n",
      "loss: 0.706260  [91200/175341]\n",
      "loss: 0.870735  [92800/175341]\n",
      "loss: 0.328355  [94400/175341]\n",
      "loss: 0.454119  [96000/175341]\n",
      "loss: 0.669384  [97600/175341]\n",
      "loss: 0.447259  [99200/175341]\n",
      "loss: 1.214163  [100800/175341]\n",
      "loss: 0.410212  [102400/175341]\n",
      "loss: 0.360105  [104000/175341]\n",
      "loss: 0.542992  [105600/175341]\n",
      "loss: 0.379735  [107200/175341]\n",
      "loss: 1.124506  [108800/175341]\n",
      "loss: 0.503367  [110400/175341]\n",
      "loss: 0.501936  [112000/175341]\n",
      "loss: 0.173098  [113600/175341]\n",
      "loss: 0.713441  [115200/175341]\n",
      "loss: 0.298201  [116800/175341]\n",
      "loss: 0.420359  [118400/175341]\n",
      "loss: 0.249832  [120000/175341]\n",
      "loss: 0.586873  [121600/175341]\n",
      "loss: 0.479137  [123200/175341]\n",
      "loss: 0.465319  [124800/175341]\n",
      "loss: 0.272917  [126400/175341]\n",
      "loss: 0.237785  [128000/175341]\n",
      "loss: 0.154823  [129600/175341]\n",
      "loss: 0.412807  [131200/175341]\n",
      "loss: 0.677122  [132800/175341]\n",
      "loss: 0.407776  [134400/175341]\n",
      "loss: 0.505403  [136000/175341]\n",
      "loss: 0.165105  [137600/175341]\n",
      "loss: 1.072271  [139200/175341]\n",
      "loss: 0.322880  [140800/175341]\n",
      "loss: 0.806604  [142400/175341]\n",
      "loss: 0.736648  [144000/175341]\n",
      "loss: 0.734284  [145600/175341]\n",
      "loss: 0.494044  [147200/175341]\n",
      "loss: 0.596260  [148800/175341]\n",
      "loss: 0.312126  [150400/175341]\n",
      "loss: 0.539853  [152000/175341]\n",
      "loss: 0.488319  [153600/175341]\n",
      "loss: 0.467176  [155200/175341]\n",
      "loss: 0.601225  [156800/175341]\n",
      "loss: 1.004954  [158400/175341]\n",
      "loss: 0.495707  [160000/175341]\n",
      "loss: 0.458845  [161600/175341]\n",
      "loss: 0.453320  [163200/175341]\n",
      "loss: 0.405321  [164800/175341]\n",
      "loss: 0.300227  [166400/175341]\n",
      "loss: 0.592652  [168000/175341]\n",
      "loss: 0.782824  [169600/175341]\n",
      "loss: 0.293606  [171200/175341]\n",
      "loss: 0.737427  [172800/175341]\n",
      "loss: 0.223473  [174400/175341]\n",
      "Train Accuracy: 79.0945%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.599564, F1-score: 74.04%, Macro_F1-Score:  34.24%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.582724  [    0/175341]\n",
      "loss: 0.305244  [ 1600/175341]\n",
      "loss: 0.544369  [ 3200/175341]\n",
      "loss: 0.514349  [ 4800/175341]\n",
      "loss: 0.519690  [ 6400/175341]\n",
      "loss: 0.765382  [ 8000/175341]\n",
      "loss: 0.591686  [ 9600/175341]\n",
      "loss: 0.670380  [11200/175341]\n",
      "loss: 0.514072  [12800/175341]\n",
      "loss: 1.031311  [14400/175341]\n",
      "loss: 0.681626  [16000/175341]\n",
      "loss: 0.844637  [17600/175341]\n",
      "loss: 0.479241  [19200/175341]\n",
      "loss: 0.519282  [20800/175341]\n",
      "loss: 0.341272  [22400/175341]\n",
      "loss: 0.704134  [24000/175341]\n",
      "loss: 1.055883  [25600/175341]\n",
      "loss: 0.419982  [27200/175341]\n",
      "loss: 0.556207  [28800/175341]\n",
      "loss: 0.345620  [30400/175341]\n",
      "loss: 1.007099  [32000/175341]\n",
      "loss: 0.579711  [33600/175341]\n",
      "loss: 0.787668  [35200/175341]\n",
      "loss: 0.425933  [36800/175341]\n",
      "loss: 0.475975  [38400/175341]\n",
      "loss: 0.660577  [40000/175341]\n",
      "loss: 0.111961  [41600/175341]\n",
      "loss: 0.300482  [43200/175341]\n",
      "loss: 0.747549  [44800/175341]\n",
      "loss: 0.357113  [46400/175341]\n",
      "loss: 0.585927  [48000/175341]\n",
      "loss: 0.384155  [49600/175341]\n",
      "loss: 0.358945  [51200/175341]\n",
      "loss: 0.221662  [52800/175341]\n",
      "loss: 0.359084  [54400/175341]\n",
      "loss: 0.468273  [56000/175341]\n",
      "loss: 0.463787  [57600/175341]\n",
      "loss: 0.443461  [59200/175341]\n",
      "loss: 0.696337  [60800/175341]\n",
      "loss: 0.384455  [62400/175341]\n",
      "loss: 0.977880  [64000/175341]\n",
      "loss: 0.764568  [65600/175341]\n",
      "loss: 0.358754  [67200/175341]\n",
      "loss: 0.404378  [68800/175341]\n",
      "loss: 0.347780  [70400/175341]\n",
      "loss: 0.657123  [72000/175341]\n",
      "loss: 0.281855  [73600/175341]\n",
      "loss: 0.155756  [75200/175341]\n",
      "loss: 0.631612  [76800/175341]\n",
      "loss: 0.421121  [78400/175341]\n",
      "loss: 0.210983  [80000/175341]\n",
      "loss: 0.459120  [81600/175341]\n",
      "loss: 0.381903  [83200/175341]\n",
      "loss: 0.408503  [84800/175341]\n",
      "loss: 0.711337  [86400/175341]\n",
      "loss: 0.428794  [88000/175341]\n",
      "loss: 0.658494  [89600/175341]\n",
      "loss: 0.359015  [91200/175341]\n",
      "loss: 0.754490  [92800/175341]\n",
      "loss: 0.603686  [94400/175341]\n",
      "loss: 0.217124  [96000/175341]\n",
      "loss: 0.545897  [97600/175341]\n",
      "loss: 0.467669  [99200/175341]\n",
      "loss: 0.349385  [100800/175341]\n",
      "loss: 0.299482  [102400/175341]\n",
      "loss: 0.779938  [104000/175341]\n",
      "loss: 0.810605  [105600/175341]\n",
      "loss: 0.508343  [107200/175341]\n",
      "loss: 0.519531  [108800/175341]\n",
      "loss: 0.409230  [110400/175341]\n",
      "loss: 0.299906  [112000/175341]\n",
      "loss: 0.581090  [113600/175341]\n",
      "loss: 0.776122  [115200/175341]\n",
      "loss: 0.469602  [116800/175341]\n",
      "loss: 0.462276  [118400/175341]\n",
      "loss: 0.444645  [120000/175341]\n",
      "loss: 0.708660  [121600/175341]\n",
      "loss: 0.299532  [123200/175341]\n",
      "loss: 0.661095  [124800/175341]\n",
      "loss: 0.958749  [126400/175341]\n",
      "loss: 1.009456  [128000/175341]\n",
      "loss: 0.788639  [129600/175341]\n",
      "loss: 0.647739  [131200/175341]\n",
      "loss: 0.746240  [132800/175341]\n",
      "loss: 0.453136  [134400/175341]\n",
      "loss: 0.516493  [136000/175341]\n",
      "loss: 0.657980  [137600/175341]\n",
      "loss: 0.236037  [139200/175341]\n",
      "loss: 0.921761  [140800/175341]\n",
      "loss: 0.588828  [142400/175341]\n",
      "loss: 0.523081  [144000/175341]\n",
      "loss: 0.471407  [145600/175341]\n",
      "loss: 0.389262  [147200/175341]\n",
      "loss: 0.324443  [148800/175341]\n",
      "loss: 0.606346  [150400/175341]\n",
      "loss: 0.627509  [152000/175341]\n",
      "loss: 0.350332  [153600/175341]\n",
      "loss: 0.405873  [155200/175341]\n",
      "loss: 0.618265  [156800/175341]\n",
      "loss: 0.522700  [158400/175341]\n",
      "loss: 0.452721  [160000/175341]\n",
      "loss: 0.282130  [161600/175341]\n",
      "loss: 0.307308  [163200/175341]\n",
      "loss: 0.445714  [164800/175341]\n",
      "loss: 0.325375  [166400/175341]\n",
      "loss: 0.868445  [168000/175341]\n",
      "loss: 1.002413  [169600/175341]\n",
      "loss: 0.230649  [171200/175341]\n",
      "loss: 0.374508  [172800/175341]\n",
      "loss: 0.610189  [174400/175341]\n",
      "Train Accuracy: 79.1424%\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.597564, F1-score: 73.41%, Macro_F1-Score:  37.03%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.664673  [    0/175341]\n",
      "loss: 0.243294  [ 1600/175341]\n",
      "loss: 0.445735  [ 3200/175341]\n",
      "loss: 0.830346  [ 4800/175341]\n",
      "loss: 0.921082  [ 6400/175341]\n",
      "loss: 0.279942  [ 8000/175341]\n",
      "loss: 0.422135  [ 9600/175341]\n",
      "loss: 0.846830  [11200/175341]\n",
      "loss: 0.375012  [12800/175341]\n",
      "loss: 0.700633  [14400/175341]\n",
      "loss: 0.666516  [16000/175341]\n",
      "loss: 0.541954  [17600/175341]\n",
      "loss: 0.323194  [19200/175341]\n",
      "loss: 0.620974  [20800/175341]\n",
      "loss: 1.069087  [22400/175341]\n",
      "loss: 0.931653  [24000/175341]\n",
      "loss: 0.624694  [25600/175341]\n",
      "loss: 0.412139  [27200/175341]\n",
      "loss: 0.788166  [28800/175341]\n",
      "loss: 0.626457  [30400/175341]\n",
      "loss: 0.281703  [32000/175341]\n",
      "loss: 0.391487  [33600/175341]\n",
      "loss: 0.351946  [35200/175341]\n",
      "loss: 0.376043  [36800/175341]\n",
      "loss: 1.343158  [38400/175341]\n",
      "loss: 0.823366  [40000/175341]\n",
      "loss: 0.900229  [41600/175341]\n",
      "loss: 0.850502  [43200/175341]\n",
      "loss: 0.206087  [44800/175341]\n",
      "loss: 0.854642  [46400/175341]\n",
      "loss: 0.183443  [48000/175341]\n",
      "loss: 0.567915  [49600/175341]\n",
      "loss: 0.297423  [51200/175341]\n",
      "loss: 0.649075  [52800/175341]\n",
      "loss: 0.673741  [54400/175341]\n",
      "loss: 0.673828  [56000/175341]\n",
      "loss: 0.492837  [57600/175341]\n",
      "loss: 0.288121  [59200/175341]\n",
      "loss: 0.350612  [60800/175341]\n",
      "loss: 0.699931  [62400/175341]\n",
      "loss: 0.368981  [64000/175341]\n",
      "loss: 0.876198  [65600/175341]\n",
      "loss: 0.382309  [67200/175341]\n",
      "loss: 0.386497  [68800/175341]\n",
      "loss: 0.736247  [70400/175341]\n",
      "loss: 0.498833  [72000/175341]\n",
      "loss: 0.743145  [73600/175341]\n",
      "loss: 0.411261  [75200/175341]\n",
      "loss: 0.833917  [76800/175341]\n",
      "loss: 0.306586  [78400/175341]\n",
      "loss: 0.299400  [80000/175341]\n",
      "loss: 0.685077  [81600/175341]\n",
      "loss: 0.760253  [83200/175341]\n",
      "loss: 0.501099  [84800/175341]\n",
      "loss: 0.533216  [86400/175341]\n",
      "loss: 0.343399  [88000/175341]\n",
      "loss: 0.768217  [89600/175341]\n",
      "loss: 0.954437  [91200/175341]\n",
      "loss: 0.418765  [92800/175341]\n",
      "loss: 0.238617  [94400/175341]\n",
      "loss: 0.334209  [96000/175341]\n",
      "loss: 0.605183  [97600/175341]\n",
      "loss: 0.322972  [99200/175341]\n",
      "loss: 0.256740  [100800/175341]\n",
      "loss: 0.215428  [102400/175341]\n",
      "loss: 0.606867  [104000/175341]\n",
      "loss: 0.474784  [105600/175341]\n",
      "loss: 0.776263  [107200/175341]\n",
      "loss: 0.423580  [108800/175341]\n",
      "loss: 0.533466  [110400/175341]\n",
      "loss: 0.294290  [112000/175341]\n",
      "loss: 0.606215  [113600/175341]\n",
      "loss: 0.461885  [115200/175341]\n",
      "loss: 0.245168  [116800/175341]\n",
      "loss: 0.271372  [118400/175341]\n",
      "loss: 0.552851  [120000/175341]\n",
      "loss: 0.473259  [121600/175341]\n",
      "loss: 0.333870  [123200/175341]\n",
      "loss: 0.335194  [124800/175341]\n",
      "loss: 0.551082  [126400/175341]\n",
      "loss: 0.288411  [128000/175341]\n",
      "loss: 0.443336  [129600/175341]\n",
      "loss: 0.671312  [131200/175341]\n",
      "loss: 0.536118  [132800/175341]\n",
      "loss: 0.402349  [134400/175341]\n",
      "loss: 0.263364  [136000/175341]\n",
      "loss: 0.404693  [137600/175341]\n",
      "loss: 1.022191  [139200/175341]\n",
      "loss: 0.658343  [140800/175341]\n",
      "loss: 0.462992  [142400/175341]\n",
      "loss: 0.648911  [144000/175341]\n",
      "loss: 0.286841  [145600/175341]\n",
      "loss: 0.380219  [147200/175341]\n",
      "loss: 0.510544  [148800/175341]\n",
      "loss: 0.373083  [150400/175341]\n",
      "loss: 0.606505  [152000/175341]\n",
      "loss: 0.707293  [153600/175341]\n",
      "loss: 1.064249  [155200/175341]\n",
      "loss: 0.319010  [156800/175341]\n",
      "loss: 0.614219  [158400/175341]\n",
      "loss: 0.720857  [160000/175341]\n",
      "loss: 0.557198  [161600/175341]\n",
      "loss: 0.727515  [163200/175341]\n",
      "loss: 0.325553  [164800/175341]\n",
      "loss: 0.348009  [166400/175341]\n",
      "loss: 0.690356  [168000/175341]\n",
      "loss: 0.469628  [169600/175341]\n",
      "loss: 0.679590  [171200/175341]\n",
      "loss: 0.615690  [172800/175341]\n",
      "loss: 0.590286  [174400/175341]\n",
      "Train Accuracy: 79.2775%\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.616083, F1-score: 72.20%, Macro_F1-Score:  34.57%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.772543  [    0/175341]\n",
      "loss: 0.664355  [ 1600/175341]\n",
      "loss: 0.801989  [ 3200/175341]\n",
      "loss: 0.449613  [ 4800/175341]\n",
      "loss: 0.641877  [ 6400/175341]\n",
      "loss: 0.639667  [ 8000/175341]\n",
      "loss: 0.500820  [ 9600/175341]\n",
      "loss: 0.756571  [11200/175341]\n",
      "loss: 0.575246  [12800/175341]\n",
      "loss: 0.396912  [14400/175341]\n",
      "loss: 0.512470  [16000/175341]\n",
      "loss: 0.328874  [17600/175341]\n",
      "loss: 0.404342  [19200/175341]\n",
      "loss: 0.388288  [20800/175341]\n",
      "loss: 0.472953  [22400/175341]\n",
      "loss: 0.407987  [24000/175341]\n",
      "loss: 0.540709  [25600/175341]\n",
      "loss: 0.351433  [27200/175341]\n",
      "loss: 0.477872  [28800/175341]\n",
      "loss: 0.372977  [30400/175341]\n",
      "loss: 0.410646  [32000/175341]\n",
      "loss: 0.586823  [33600/175341]\n",
      "loss: 0.972827  [35200/175341]\n",
      "loss: 0.452451  [36800/175341]\n",
      "loss: 0.779397  [38400/175341]\n",
      "loss: 0.721761  [40000/175341]\n",
      "loss: 0.452639  [41600/175341]\n",
      "loss: 0.719385  [43200/175341]\n",
      "loss: 0.429090  [44800/175341]\n",
      "loss: 0.081284  [46400/175341]\n",
      "loss: 0.363489  [48000/175341]\n",
      "loss: 0.590022  [49600/175341]\n",
      "loss: 0.489262  [51200/175341]\n",
      "loss: 0.276448  [52800/175341]\n",
      "loss: 0.249420  [54400/175341]\n",
      "loss: 0.582842  [56000/175341]\n",
      "loss: 0.476932  [57600/175341]\n",
      "loss: 0.307510  [59200/175341]\n",
      "loss: 0.431727  [60800/175341]\n",
      "loss: 0.536480  [62400/175341]\n",
      "loss: 0.382090  [64000/175341]\n",
      "loss: 0.488333  [65600/175341]\n",
      "loss: 0.649233  [67200/175341]\n",
      "loss: 0.377657  [68800/175341]\n",
      "loss: 0.522850  [70400/175341]\n",
      "loss: 0.622956  [72000/175341]\n",
      "loss: 0.758930  [73600/175341]\n",
      "loss: 0.285951  [75200/175341]\n",
      "loss: 0.755032  [76800/175341]\n",
      "loss: 0.505189  [78400/175341]\n",
      "loss: 0.421320  [80000/175341]\n",
      "loss: 0.224004  [81600/175341]\n",
      "loss: 0.584673  [83200/175341]\n",
      "loss: 0.549038  [84800/175341]\n",
      "loss: 0.325616  [86400/175341]\n",
      "loss: 0.564088  [88000/175341]\n",
      "loss: 0.481399  [89600/175341]\n",
      "loss: 0.164044  [91200/175341]\n",
      "loss: 0.700792  [92800/175341]\n",
      "loss: 0.896173  [94400/175341]\n",
      "loss: 0.585768  [96000/175341]\n",
      "loss: 0.315899  [97600/175341]\n",
      "loss: 0.509018  [99200/175341]\n",
      "loss: 0.474469  [100800/175341]\n",
      "loss: 0.470165  [102400/175341]\n",
      "loss: 0.507256  [104000/175341]\n",
      "loss: 0.241661  [105600/175341]\n",
      "loss: 0.636568  [107200/175341]\n",
      "loss: 0.518969  [108800/175341]\n",
      "loss: 0.664331  [110400/175341]\n",
      "loss: 0.774343  [112000/175341]\n",
      "loss: 0.566164  [113600/175341]\n",
      "loss: 0.560691  [115200/175341]\n",
      "loss: 0.641782  [116800/175341]\n",
      "loss: 0.339325  [118400/175341]\n",
      "loss: 0.391444  [120000/175341]\n",
      "loss: 0.386535  [121600/175341]\n",
      "loss: 0.620925  [123200/175341]\n",
      "loss: 0.604746  [124800/175341]\n",
      "loss: 0.353485  [126400/175341]\n",
      "loss: 0.751694  [128000/175341]\n",
      "loss: 0.354047  [129600/175341]\n",
      "loss: 0.333276  [131200/175341]\n",
      "loss: 0.463391  [132800/175341]\n",
      "loss: 0.327785  [134400/175341]\n",
      "loss: 0.274981  [136000/175341]\n",
      "loss: 0.432261  [137600/175341]\n",
      "loss: 0.525438  [139200/175341]\n",
      "loss: 0.801111  [140800/175341]\n",
      "loss: 0.655066  [142400/175341]\n",
      "loss: 0.476043  [144000/175341]\n",
      "loss: 0.868283  [145600/175341]\n",
      "loss: 0.479304  [147200/175341]\n",
      "loss: 0.535069  [148800/175341]\n",
      "loss: 0.577321  [150400/175341]\n",
      "loss: 0.119074  [152000/175341]\n",
      "loss: 0.683803  [153600/175341]\n",
      "loss: 0.403232  [155200/175341]\n",
      "loss: 0.385268  [156800/175341]\n",
      "loss: 0.543012  [158400/175341]\n",
      "loss: 0.533238  [160000/175341]\n",
      "loss: 0.559274  [161600/175341]\n",
      "loss: 0.577979  [163200/175341]\n",
      "loss: 0.465381  [164800/175341]\n",
      "loss: 0.499655  [166400/175341]\n",
      "loss: 0.298100  [168000/175341]\n",
      "loss: 0.524833  [169600/175341]\n",
      "loss: 0.344218  [171200/175341]\n",
      "loss: 0.640444  [172800/175341]\n",
      "loss: 0.879701  [174400/175341]\n",
      "Train Accuracy: 79.3688%\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.609917, F1-score: 72.66%, Macro_F1-Score:  35.70%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.614319  [    0/175341]\n",
      "loss: 0.324618  [ 1600/175341]\n",
      "loss: 0.442215  [ 3200/175341]\n",
      "loss: 0.805968  [ 4800/175341]\n",
      "loss: 0.326019  [ 6400/175341]\n",
      "loss: 0.514552  [ 8000/175341]\n",
      "loss: 0.669056  [ 9600/175341]\n",
      "loss: 0.318707  [11200/175341]\n",
      "loss: 0.428094  [12800/175341]\n",
      "loss: 0.802278  [14400/175341]\n",
      "loss: 0.562255  [16000/175341]\n",
      "loss: 0.337538  [17600/175341]\n",
      "loss: 0.141521  [19200/175341]\n",
      "loss: 0.631526  [20800/175341]\n",
      "loss: 0.922589  [22400/175341]\n",
      "loss: 0.839519  [24000/175341]\n",
      "loss: 0.999113  [25600/175341]\n",
      "loss: 0.794640  [27200/175341]\n",
      "loss: 0.740214  [28800/175341]\n",
      "loss: 0.842752  [30400/175341]\n",
      "loss: 0.328232  [32000/175341]\n",
      "loss: 0.529626  [33600/175341]\n",
      "loss: 0.548675  [35200/175341]\n",
      "loss: 0.647620  [36800/175341]\n",
      "loss: 0.298179  [38400/175341]\n",
      "loss: 0.254748  [40000/175341]\n",
      "loss: 0.789846  [41600/175341]\n",
      "loss: 0.379903  [43200/175341]\n",
      "loss: 0.270591  [44800/175341]\n",
      "loss: 0.448898  [46400/175341]\n",
      "loss: 0.792856  [48000/175341]\n",
      "loss: 0.569318  [49600/175341]\n",
      "loss: 0.389562  [51200/175341]\n",
      "loss: 0.784839  [52800/175341]\n",
      "loss: 0.439890  [54400/175341]\n",
      "loss: 0.315120  [56000/175341]\n",
      "loss: 0.548517  [57600/175341]\n",
      "loss: 0.519814  [59200/175341]\n",
      "loss: 0.166674  [60800/175341]\n",
      "loss: 0.632645  [62400/175341]\n",
      "loss: 0.617999  [64000/175341]\n",
      "loss: 0.300450  [65600/175341]\n",
      "loss: 0.922077  [67200/175341]\n",
      "loss: 0.806594  [68800/175341]\n",
      "loss: 0.106562  [70400/175341]\n",
      "loss: 0.676624  [72000/175341]\n",
      "loss: 0.622057  [73600/175341]\n",
      "loss: 0.861612  [75200/175341]\n",
      "loss: 0.416746  [76800/175341]\n",
      "loss: 0.559750  [78400/175341]\n",
      "loss: 0.535325  [80000/175341]\n",
      "loss: 0.336341  [81600/175341]\n",
      "loss: 0.513322  [83200/175341]\n",
      "loss: 0.624176  [84800/175341]\n",
      "loss: 0.577538  [86400/175341]\n",
      "loss: 0.786824  [88000/175341]\n",
      "loss: 0.281145  [89600/175341]\n",
      "loss: 0.617404  [91200/175341]\n",
      "loss: 0.643194  [92800/175341]\n",
      "loss: 0.720891  [94400/175341]\n",
      "loss: 0.505365  [96000/175341]\n",
      "loss: 0.267686  [97600/175341]\n",
      "loss: 0.784725  [99200/175341]\n",
      "loss: 0.396691  [100800/175341]\n",
      "loss: 0.574093  [102400/175341]\n",
      "loss: 1.259119  [104000/175341]\n",
      "loss: 0.733914  [105600/175341]\n",
      "loss: 0.685450  [107200/175341]\n",
      "loss: 0.755763  [108800/175341]\n",
      "loss: 0.441202  [110400/175341]\n",
      "loss: 0.320255  [112000/175341]\n",
      "loss: 0.451269  [113600/175341]\n",
      "loss: 0.453682  [115200/175341]\n",
      "loss: 0.730786  [116800/175341]\n",
      "loss: 0.657954  [118400/175341]\n",
      "loss: 1.096597  [120000/175341]\n",
      "loss: 1.100959  [121600/175341]\n",
      "loss: 0.276830  [123200/175341]\n",
      "loss: 0.439623  [124800/175341]\n",
      "loss: 0.477309  [126400/175341]\n",
      "loss: 0.635368  [128000/175341]\n",
      "loss: 0.075324  [129600/175341]\n",
      "loss: 0.533516  [131200/175341]\n",
      "loss: 0.144089  [132800/175341]\n",
      "loss: 0.525890  [134400/175341]\n",
      "loss: 0.742803  [136000/175341]\n",
      "loss: 0.409888  [137600/175341]\n",
      "loss: 0.894969  [139200/175341]\n",
      "loss: 0.442209  [140800/175341]\n",
      "loss: 0.679487  [142400/175341]\n",
      "loss: 0.362570  [144000/175341]\n",
      "loss: 0.808186  [145600/175341]\n",
      "loss: 0.856641  [147200/175341]\n",
      "loss: 1.023463  [148800/175341]\n",
      "loss: 0.351004  [150400/175341]\n",
      "loss: 0.335902  [152000/175341]\n",
      "loss: 0.767342  [153600/175341]\n",
      "loss: 0.441240  [155200/175341]\n",
      "loss: 0.283372  [156800/175341]\n",
      "loss: 0.661819  [158400/175341]\n",
      "loss: 0.419021  [160000/175341]\n",
      "loss: 0.497376  [161600/175341]\n",
      "loss: 0.442317  [163200/175341]\n",
      "loss: 0.532896  [164800/175341]\n",
      "loss: 0.768931  [166400/175341]\n",
      "loss: 0.609412  [168000/175341]\n",
      "loss: 0.607855  [169600/175341]\n",
      "loss: 0.294289  [171200/175341]\n",
      "loss: 0.988868  [172800/175341]\n",
      "loss: 0.414119  [174400/175341]\n",
      "Train Accuracy: 79.4395%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.600006, F1-score: 73.59%, Macro_F1-Score:  35.78%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.718162  [    0/175341]\n",
      "loss: 0.511527  [ 1600/175341]\n",
      "loss: 0.412249  [ 3200/175341]\n",
      "loss: 1.061631  [ 4800/175341]\n",
      "loss: 0.366435  [ 6400/175341]\n",
      "loss: 0.463614  [ 8000/175341]\n",
      "loss: 0.615175  [ 9600/175341]\n",
      "loss: 0.668299  [11200/175341]\n",
      "loss: 0.518234  [12800/175341]\n",
      "loss: 0.633978  [14400/175341]\n",
      "loss: 0.452093  [16000/175341]\n",
      "loss: 0.416081  [17600/175341]\n",
      "loss: 0.470903  [19200/175341]\n",
      "loss: 0.501650  [20800/175341]\n",
      "loss: 0.337089  [22400/175341]\n",
      "loss: 0.636236  [24000/175341]\n",
      "loss: 0.350836  [25600/175341]\n",
      "loss: 0.827955  [27200/175341]\n",
      "loss: 0.491876  [28800/175341]\n",
      "loss: 0.616408  [30400/175341]\n",
      "loss: 0.840424  [32000/175341]\n",
      "loss: 0.344227  [33600/175341]\n",
      "loss: 0.677435  [35200/175341]\n",
      "loss: 0.463415  [36800/175341]\n",
      "loss: 0.165920  [38400/175341]\n",
      "loss: 0.793475  [40000/175341]\n",
      "loss: 0.271340  [41600/175341]\n",
      "loss: 0.613096  [43200/175341]\n",
      "loss: 0.485788  [44800/175341]\n",
      "loss: 0.835122  [46400/175341]\n",
      "loss: 0.385693  [48000/175341]\n",
      "loss: 0.336438  [49600/175341]\n",
      "loss: 0.454668  [51200/175341]\n",
      "loss: 0.345466  [52800/175341]\n",
      "loss: 0.374740  [54400/175341]\n",
      "loss: 0.632450  [56000/175341]\n",
      "loss: 0.597329  [57600/175341]\n",
      "loss: 0.301381  [59200/175341]\n",
      "loss: 0.359503  [60800/175341]\n",
      "loss: 0.292261  [62400/175341]\n",
      "loss: 0.746790  [64000/175341]\n",
      "loss: 0.682940  [65600/175341]\n",
      "loss: 0.499744  [67200/175341]\n",
      "loss: 0.601924  [68800/175341]\n",
      "loss: 0.622791  [70400/175341]\n",
      "loss: 0.674304  [72000/175341]\n",
      "loss: 0.524631  [73600/175341]\n",
      "loss: 0.509386  [75200/175341]\n",
      "loss: 0.589361  [76800/175341]\n",
      "loss: 1.223280  [78400/175341]\n",
      "loss: 0.601808  [80000/175341]\n",
      "loss: 0.753020  [81600/175341]\n",
      "loss: 0.417038  [83200/175341]\n",
      "loss: 0.267007  [84800/175341]\n",
      "loss: 0.639714  [86400/175341]\n",
      "loss: 0.506856  [88000/175341]\n",
      "loss: 0.389802  [89600/175341]\n",
      "loss: 0.541459  [91200/175341]\n",
      "loss: 0.345952  [92800/175341]\n",
      "loss: 0.467410  [94400/175341]\n",
      "loss: 0.404807  [96000/175341]\n",
      "loss: 0.632321  [97600/175341]\n",
      "loss: 0.540855  [99200/175341]\n",
      "loss: 0.528750  [100800/175341]\n",
      "loss: 0.549231  [102400/175341]\n",
      "loss: 0.153626  [104000/175341]\n",
      "loss: 0.379419  [105600/175341]\n",
      "loss: 0.480003  [107200/175341]\n",
      "loss: 0.310798  [108800/175341]\n",
      "loss: 0.418100  [110400/175341]\n",
      "loss: 0.716502  [112000/175341]\n",
      "loss: 0.586559  [113600/175341]\n",
      "loss: 0.529474  [115200/175341]\n",
      "loss: 0.880875  [116800/175341]\n",
      "loss: 0.688339  [118400/175341]\n",
      "loss: 0.154331  [120000/175341]\n",
      "loss: 0.372818  [121600/175341]\n",
      "loss: 0.173508  [123200/175341]\n",
      "loss: 0.197667  [124800/175341]\n",
      "loss: 0.814495  [126400/175341]\n",
      "loss: 0.785441  [128000/175341]\n",
      "loss: 0.311614  [129600/175341]\n",
      "loss: 0.771012  [131200/175341]\n",
      "loss: 0.359680  [132800/175341]\n",
      "loss: 0.293076  [134400/175341]\n",
      "loss: 0.313295  [136000/175341]\n",
      "loss: 0.260043  [137600/175341]\n",
      "loss: 0.869634  [139200/175341]\n",
      "loss: 0.584471  [140800/175341]\n",
      "loss: 0.707015  [142400/175341]\n",
      "loss: 0.620242  [144000/175341]\n",
      "loss: 0.701064  [145600/175341]\n",
      "loss: 0.647489  [147200/175341]\n",
      "loss: 0.386659  [148800/175341]\n",
      "loss: 0.555574  [150400/175341]\n",
      "loss: 0.684920  [152000/175341]\n",
      "loss: 0.573186  [153600/175341]\n",
      "loss: 0.479643  [155200/175341]\n",
      "loss: 0.473950  [156800/175341]\n",
      "loss: 0.903244  [158400/175341]\n",
      "loss: 0.453589  [160000/175341]\n",
      "loss: 0.601587  [161600/175341]\n",
      "loss: 0.294094  [163200/175341]\n",
      "loss: 0.325982  [164800/175341]\n",
      "loss: 0.216896  [166400/175341]\n",
      "loss: 0.335718  [168000/175341]\n",
      "loss: 0.473622  [169600/175341]\n",
      "loss: 0.924857  [171200/175341]\n",
      "loss: 0.500797  [172800/175341]\n",
      "loss: 0.879463  [174400/175341]\n",
      "Train Accuracy: 79.4811%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.602028, F1-score: 73.71%, Macro_F1-Score:  35.76%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.562375  [    0/175341]\n",
      "loss: 0.376755  [ 1600/175341]\n",
      "loss: 0.298607  [ 3200/175341]\n",
      "loss: 0.381879  [ 4800/175341]\n",
      "loss: 0.655976  [ 6400/175341]\n",
      "loss: 0.401788  [ 8000/175341]\n",
      "loss: 0.680071  [ 9600/175341]\n",
      "loss: 0.543569  [11200/175341]\n",
      "loss: 0.549463  [12800/175341]\n",
      "loss: 0.253459  [14400/175341]\n",
      "loss: 0.485332  [16000/175341]\n",
      "loss: 0.226207  [17600/175341]\n",
      "loss: 0.478959  [19200/175341]\n",
      "loss: 0.272978  [20800/175341]\n",
      "loss: 0.516988  [22400/175341]\n",
      "loss: 0.386019  [24000/175341]\n",
      "loss: 0.386883  [25600/175341]\n",
      "loss: 0.261858  [27200/175341]\n",
      "loss: 0.557496  [28800/175341]\n",
      "loss: 0.415173  [30400/175341]\n",
      "loss: 0.750420  [32000/175341]\n",
      "loss: 0.480116  [33600/175341]\n",
      "loss: 0.534693  [35200/175341]\n",
      "loss: 1.133391  [36800/175341]\n",
      "loss: 0.254103  [38400/175341]\n",
      "loss: 0.410739  [40000/175341]\n",
      "loss: 0.464262  [41600/175341]\n",
      "loss: 0.618960  [43200/175341]\n",
      "loss: 0.395481  [44800/175341]\n",
      "loss: 0.255361  [46400/175341]\n",
      "loss: 0.336062  [48000/175341]\n",
      "loss: 0.526200  [49600/175341]\n",
      "loss: 0.620546  [51200/175341]\n",
      "loss: 0.640844  [52800/175341]\n",
      "loss: 0.315123  [54400/175341]\n",
      "loss: 0.334046  [56000/175341]\n",
      "loss: 0.312889  [57600/175341]\n",
      "loss: 0.589390  [59200/175341]\n",
      "loss: 0.624891  [60800/175341]\n",
      "loss: 0.952924  [62400/175341]\n",
      "loss: 0.594985  [64000/175341]\n",
      "loss: 0.781089  [65600/175341]\n",
      "loss: 0.417604  [67200/175341]\n",
      "loss: 0.687857  [68800/175341]\n",
      "loss: 0.531318  [70400/175341]\n",
      "loss: 0.171425  [72000/175341]\n",
      "loss: 0.597333  [73600/175341]\n",
      "loss: 0.324890  [75200/175341]\n",
      "loss: 0.406852  [76800/175341]\n",
      "loss: 0.401942  [78400/175341]\n",
      "loss: 0.174594  [80000/175341]\n",
      "loss: 0.463202  [81600/175341]\n",
      "loss: 0.357055  [83200/175341]\n",
      "loss: 0.190698  [84800/175341]\n",
      "loss: 0.461764  [86400/175341]\n",
      "loss: 0.221433  [88000/175341]\n",
      "loss: 0.414514  [89600/175341]\n",
      "loss: 0.402198  [91200/175341]\n",
      "loss: 0.316256  [92800/175341]\n",
      "loss: 0.616269  [94400/175341]\n",
      "loss: 0.219529  [96000/175341]\n",
      "loss: 0.400843  [97600/175341]\n",
      "loss: 0.498120  [99200/175341]\n",
      "loss: 0.317533  [100800/175341]\n",
      "loss: 1.042703  [102400/175341]\n",
      "loss: 0.893088  [104000/175341]\n",
      "loss: 0.434307  [105600/175341]\n",
      "loss: 0.120665  [107200/175341]\n",
      "loss: 1.092232  [108800/175341]\n",
      "loss: 0.569296  [110400/175341]\n",
      "loss: 0.707397  [112000/175341]\n",
      "loss: 0.718969  [113600/175341]\n",
      "loss: 0.767185  [115200/175341]\n",
      "loss: 0.546131  [116800/175341]\n",
      "loss: 0.491972  [118400/175341]\n",
      "loss: 0.335181  [120000/175341]\n",
      "loss: 0.697611  [121600/175341]\n",
      "loss: 0.721138  [123200/175341]\n",
      "loss: 0.333008  [124800/175341]\n",
      "loss: 0.812919  [126400/175341]\n",
      "loss: 0.542517  [128000/175341]\n",
      "loss: 0.533635  [129600/175341]\n",
      "loss: 0.862137  [131200/175341]\n",
      "loss: 0.606816  [132800/175341]\n",
      "loss: 0.305313  [134400/175341]\n",
      "loss: 0.487107  [136000/175341]\n",
      "loss: 0.316622  [137600/175341]\n",
      "loss: 0.736860  [139200/175341]\n",
      "loss: 0.299396  [140800/175341]\n",
      "loss: 0.316385  [142400/175341]\n",
      "loss: 0.646600  [144000/175341]\n",
      "loss: 0.387083  [145600/175341]\n",
      "loss: 0.164661  [147200/175341]\n",
      "loss: 0.306635  [148800/175341]\n",
      "loss: 0.303062  [150400/175341]\n",
      "loss: 0.462583  [152000/175341]\n",
      "loss: 0.516414  [153600/175341]\n",
      "loss: 0.382918  [155200/175341]\n",
      "loss: 0.589963  [156800/175341]\n",
      "loss: 0.489519  [158400/175341]\n",
      "loss: 0.553083  [160000/175341]\n",
      "loss: 0.529829  [161600/175341]\n",
      "loss: 0.416261  [163200/175341]\n",
      "loss: 0.388207  [164800/175341]\n",
      "loss: 0.694759  [166400/175341]\n",
      "loss: 0.249171  [168000/175341]\n",
      "loss: 0.479027  [169600/175341]\n",
      "loss: 0.460471  [171200/175341]\n",
      "loss: 0.273876  [172800/175341]\n",
      "loss: 0.302233  [174400/175341]\n",
      "Train Accuracy: 79.5644%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.581097, F1-score: 74.51%, Macro_F1-Score:  37.19%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.346739  [    0/175341]\n",
      "loss: 0.567657  [ 1600/175341]\n",
      "loss: 0.318027  [ 3200/175341]\n",
      "loss: 0.473743  [ 4800/175341]\n",
      "loss: 0.565311  [ 6400/175341]\n",
      "loss: 0.520928  [ 8000/175341]\n",
      "loss: 0.255114  [ 9600/175341]\n",
      "loss: 0.652687  [11200/175341]\n",
      "loss: 0.414538  [12800/175341]\n",
      "loss: 0.982608  [14400/175341]\n",
      "loss: 0.731645  [16000/175341]\n",
      "loss: 1.184221  [17600/175341]\n",
      "loss: 0.954777  [19200/175341]\n",
      "loss: 0.293862  [20800/175341]\n",
      "loss: 0.427794  [22400/175341]\n",
      "loss: 0.431449  [24000/175341]\n",
      "loss: 0.456119  [25600/175341]\n",
      "loss: 0.506633  [27200/175341]\n",
      "loss: 0.223867  [28800/175341]\n",
      "loss: 0.274028  [30400/175341]\n",
      "loss: 0.379228  [32000/175341]\n",
      "loss: 0.312047  [33600/175341]\n",
      "loss: 0.523942  [35200/175341]\n",
      "loss: 0.486661  [36800/175341]\n",
      "loss: 0.295580  [38400/175341]\n",
      "loss: 0.493502  [40000/175341]\n",
      "loss: 0.614020  [41600/175341]\n",
      "loss: 0.763438  [43200/175341]\n",
      "loss: 0.493703  [44800/175341]\n",
      "loss: 0.154891  [46400/175341]\n",
      "loss: 0.603578  [48000/175341]\n",
      "loss: 1.095449  [49600/175341]\n",
      "loss: 0.178140  [51200/175341]\n",
      "loss: 0.302865  [52800/175341]\n",
      "loss: 0.943692  [54400/175341]\n",
      "loss: 0.458797  [56000/175341]\n",
      "loss: 0.673316  [57600/175341]\n",
      "loss: 0.582563  [59200/175341]\n",
      "loss: 0.424356  [60800/175341]\n",
      "loss: 0.356904  [62400/175341]\n",
      "loss: 0.740464  [64000/175341]\n",
      "loss: 0.522924  [65600/175341]\n",
      "loss: 0.670187  [67200/175341]\n",
      "loss: 0.410475  [68800/175341]\n",
      "loss: 0.755204  [70400/175341]\n",
      "loss: 0.404956  [72000/175341]\n",
      "loss: 0.652704  [73600/175341]\n",
      "loss: 0.432051  [75200/175341]\n",
      "loss: 0.729962  [76800/175341]\n",
      "loss: 0.991502  [78400/175341]\n",
      "loss: 0.463505  [80000/175341]\n",
      "loss: 0.167387  [81600/175341]\n",
      "loss: 0.313480  [83200/175341]\n",
      "loss: 0.280982  [84800/175341]\n",
      "loss: 0.432223  [86400/175341]\n",
      "loss: 0.212120  [88000/175341]\n",
      "loss: 0.300703  [89600/175341]\n",
      "loss: 0.863331  [91200/175341]\n",
      "loss: 0.376510  [92800/175341]\n",
      "loss: 0.448307  [94400/175341]\n",
      "loss: 0.571855  [96000/175341]\n",
      "loss: 0.388111  [97600/175341]\n",
      "loss: 0.598740  [99200/175341]\n",
      "loss: 0.558309  [100800/175341]\n",
      "loss: 0.980419  [102400/175341]\n",
      "loss: 0.791478  [104000/175341]\n",
      "loss: 0.389752  [105600/175341]\n",
      "loss: 0.585023  [107200/175341]\n",
      "loss: 0.886698  [108800/175341]\n",
      "loss: 0.409377  [110400/175341]\n",
      "loss: 0.408530  [112000/175341]\n",
      "loss: 0.564194  [113600/175341]\n",
      "loss: 0.750502  [115200/175341]\n",
      "loss: 0.330793  [116800/175341]\n",
      "loss: 0.471068  [118400/175341]\n",
      "loss: 0.769314  [120000/175341]\n",
      "loss: 0.851852  [121600/175341]\n",
      "loss: 0.629768  [123200/175341]\n",
      "loss: 0.412256  [124800/175341]\n",
      "loss: 0.518987  [126400/175341]\n",
      "loss: 0.530469  [128000/175341]\n",
      "loss: 0.467937  [129600/175341]\n",
      "loss: 0.147396  [131200/175341]\n",
      "loss: 0.701718  [132800/175341]\n",
      "loss: 0.426114  [134400/175341]\n",
      "loss: 0.670842  [136000/175341]\n",
      "loss: 0.531202  [137600/175341]\n",
      "loss: 0.521661  [139200/175341]\n",
      "loss: 0.342992  [140800/175341]\n",
      "loss: 0.529622  [142400/175341]\n",
      "loss: 0.468219  [144000/175341]\n",
      "loss: 0.680273  [145600/175341]\n",
      "loss: 0.812119  [147200/175341]\n",
      "loss: 0.510780  [148800/175341]\n",
      "loss: 0.355874  [150400/175341]\n",
      "loss: 0.743946  [152000/175341]\n",
      "loss: 0.533891  [153600/175341]\n",
      "loss: 0.435278  [155200/175341]\n",
      "loss: 0.508085  [156800/175341]\n",
      "loss: 0.443636  [158400/175341]\n",
      "loss: 0.475808  [160000/175341]\n",
      "loss: 0.565994  [161600/175341]\n",
      "loss: 0.108295  [163200/175341]\n",
      "loss: 0.498792  [164800/175341]\n",
      "loss: 0.543272  [166400/175341]\n",
      "loss: 0.225813  [168000/175341]\n",
      "loss: 0.380350  [169600/175341]\n",
      "loss: 0.416770  [171200/175341]\n",
      "loss: 0.242086  [172800/175341]\n",
      "loss: 0.572335  [174400/175341]\n",
      "Train Accuracy: 79.6037%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.575609, F1-score: 75.04%, Macro_F1-Score:  37.74%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.439293  [    0/175341]\n",
      "loss: 0.343965  [ 1600/175341]\n",
      "loss: 0.290831  [ 3200/175341]\n",
      "loss: 0.216299  [ 4800/175341]\n",
      "loss: 0.548707  [ 6400/175341]\n",
      "loss: 0.398525  [ 8000/175341]\n",
      "loss: 0.487390  [ 9600/175341]\n",
      "loss: 0.628763  [11200/175341]\n",
      "loss: 0.300704  [12800/175341]\n",
      "loss: 0.262559  [14400/175341]\n",
      "loss: 0.655471  [16000/175341]\n",
      "loss: 0.744379  [17600/175341]\n",
      "loss: 0.400500  [19200/175341]\n",
      "loss: 0.651234  [20800/175341]\n",
      "loss: 0.477704  [22400/175341]\n",
      "loss: 0.878185  [24000/175341]\n",
      "loss: 0.487945  [25600/175341]\n",
      "loss: 0.560343  [27200/175341]\n",
      "loss: 0.460351  [28800/175341]\n",
      "loss: 0.397774  [30400/175341]\n",
      "loss: 0.306312  [32000/175341]\n",
      "loss: 0.435928  [33600/175341]\n",
      "loss: 0.772850  [35200/175341]\n",
      "loss: 0.549935  [36800/175341]\n",
      "loss: 0.518643  [38400/175341]\n",
      "loss: 0.496102  [40000/175341]\n",
      "loss: 0.220831  [41600/175341]\n",
      "loss: 0.932426  [43200/175341]\n",
      "loss: 0.598302  [44800/175341]\n",
      "loss: 0.544864  [46400/175341]\n",
      "loss: 0.197535  [48000/175341]\n",
      "loss: 0.666837  [49600/175341]\n",
      "loss: 0.804824  [51200/175341]\n",
      "loss: 0.467769  [52800/175341]\n",
      "loss: 0.710077  [54400/175341]\n",
      "loss: 0.749399  [56000/175341]\n",
      "loss: 0.641455  [57600/175341]\n",
      "loss: 0.464777  [59200/175341]\n",
      "loss: 0.696530  [60800/175341]\n",
      "loss: 0.641568  [62400/175341]\n",
      "loss: 0.786044  [64000/175341]\n",
      "loss: 0.509734  [65600/175341]\n",
      "loss: 0.470046  [67200/175341]\n",
      "loss: 0.540041  [68800/175341]\n",
      "loss: 0.294771  [70400/175341]\n",
      "loss: 0.779983  [72000/175341]\n",
      "loss: 0.606879  [73600/175341]\n",
      "loss: 0.497765  [75200/175341]\n",
      "loss: 0.221906  [76800/175341]\n",
      "loss: 0.448179  [78400/175341]\n",
      "loss: 0.938377  [80000/175341]\n",
      "loss: 0.726907  [81600/175341]\n",
      "loss: 0.750093  [83200/175341]\n",
      "loss: 0.871102  [84800/175341]\n",
      "loss: 0.281760  [86400/175341]\n",
      "loss: 0.282026  [88000/175341]\n",
      "loss: 0.602844  [89600/175341]\n",
      "loss: 0.368670  [91200/175341]\n",
      "loss: 0.447937  [92800/175341]\n",
      "loss: 0.342885  [94400/175341]\n",
      "loss: 0.451366  [96000/175341]\n",
      "loss: 0.797433  [97600/175341]\n",
      "loss: 0.380248  [99200/175341]\n",
      "loss: 0.413866  [100800/175341]\n",
      "loss: 0.366571  [102400/175341]\n",
      "loss: 0.584668  [104000/175341]\n",
      "loss: 0.500098  [105600/175341]\n",
      "loss: 0.567637  [107200/175341]\n",
      "loss: 0.354329  [108800/175341]\n",
      "loss: 0.668970  [110400/175341]\n",
      "loss: 0.388658  [112000/175341]\n",
      "loss: 0.694273  [113600/175341]\n",
      "loss: 0.412875  [115200/175341]\n",
      "loss: 0.402282  [116800/175341]\n",
      "loss: 0.199735  [118400/175341]\n",
      "loss: 0.360102  [120000/175341]\n",
      "loss: 0.353619  [121600/175341]\n",
      "loss: 0.277293  [123200/175341]\n",
      "loss: 0.419001  [124800/175341]\n",
      "loss: 0.903642  [126400/175341]\n",
      "loss: 0.798302  [128000/175341]\n",
      "loss: 0.453727  [129600/175341]\n",
      "loss: 0.433058  [131200/175341]\n",
      "loss: 0.226788  [132800/175341]\n",
      "loss: 0.335308  [134400/175341]\n",
      "loss: 0.545228  [136000/175341]\n",
      "loss: 0.948254  [137600/175341]\n",
      "loss: 0.118419  [139200/175341]\n",
      "loss: 0.558151  [140800/175341]\n",
      "loss: 1.052064  [142400/175341]\n",
      "loss: 0.450414  [144000/175341]\n",
      "loss: 0.467290  [145600/175341]\n",
      "loss: 0.128329  [147200/175341]\n",
      "loss: 0.505639  [148800/175341]\n",
      "loss: 0.333358  [150400/175341]\n",
      "loss: 0.423343  [152000/175341]\n",
      "loss: 0.479318  [153600/175341]\n",
      "loss: 0.364329  [155200/175341]\n",
      "loss: 0.390088  [156800/175341]\n",
      "loss: 1.006989  [158400/175341]\n",
      "loss: 0.598533  [160000/175341]\n",
      "loss: 0.517198  [161600/175341]\n",
      "loss: 0.683523  [163200/175341]\n",
      "loss: 0.485742  [164800/175341]\n",
      "loss: 0.191288  [166400/175341]\n",
      "loss: 0.678575  [168000/175341]\n",
      "loss: 0.565719  [169600/175341]\n",
      "loss: 0.859287  [171200/175341]\n",
      "loss: 0.456715  [172800/175341]\n",
      "loss: 0.498272  [174400/175341]\n",
      "Train Accuracy: 79.6682%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.593622, F1-score: 74.02%, Macro_F1-Score:  37.51%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.497772  [    0/175341]\n",
      "loss: 0.529106  [ 1600/175341]\n",
      "loss: 0.511545  [ 3200/175341]\n",
      "loss: 0.316062  [ 4800/175341]\n",
      "loss: 0.406072  [ 6400/175341]\n",
      "loss: 0.534970  [ 8000/175341]\n",
      "loss: 0.300890  [ 9600/175341]\n",
      "loss: 0.219146  [11200/175341]\n",
      "loss: 0.382798  [12800/175341]\n",
      "loss: 0.875311  [14400/175341]\n",
      "loss: 1.267303  [16000/175341]\n",
      "loss: 0.999004  [17600/175341]\n",
      "loss: 0.592609  [19200/175341]\n",
      "loss: 0.178861  [20800/175341]\n",
      "loss: 0.357682  [22400/175341]\n",
      "loss: 0.491181  [24000/175341]\n",
      "loss: 0.449254  [25600/175341]\n",
      "loss: 0.835290  [27200/175341]\n",
      "loss: 0.924121  [28800/175341]\n",
      "loss: 0.498422  [30400/175341]\n",
      "loss: 0.330505  [32000/175341]\n",
      "loss: 0.802386  [33600/175341]\n",
      "loss: 0.360353  [35200/175341]\n",
      "loss: 0.690708  [36800/175341]\n",
      "loss: 0.883131  [38400/175341]\n",
      "loss: 0.467325  [40000/175341]\n",
      "loss: 0.446512  [41600/175341]\n",
      "loss: 0.792194  [43200/175341]\n",
      "loss: 0.504403  [44800/175341]\n",
      "loss: 0.532562  [46400/175341]\n",
      "loss: 0.495567  [48000/175341]\n",
      "loss: 0.854441  [49600/175341]\n",
      "loss: 0.346820  [51200/175341]\n",
      "loss: 0.593376  [52800/175341]\n",
      "loss: 0.509957  [54400/175341]\n",
      "loss: 0.508702  [56000/175341]\n",
      "loss: 0.573343  [57600/175341]\n",
      "loss: 0.221846  [59200/175341]\n",
      "loss: 0.216693  [60800/175341]\n",
      "loss: 0.289631  [62400/175341]\n",
      "loss: 0.493797  [64000/175341]\n",
      "loss: 0.520840  [65600/175341]\n",
      "loss: 0.919483  [67200/175341]\n",
      "loss: 0.408664  [68800/175341]\n",
      "loss: 0.596521  [70400/175341]\n",
      "loss: 0.458472  [72000/175341]\n",
      "loss: 0.350693  [73600/175341]\n",
      "loss: 0.715393  [75200/175341]\n",
      "loss: 0.776183  [76800/175341]\n",
      "loss: 0.609674  [78400/175341]\n",
      "loss: 0.518515  [80000/175341]\n",
      "loss: 0.777531  [81600/175341]\n",
      "loss: 0.859944  [83200/175341]\n",
      "loss: 0.187535  [84800/175341]\n",
      "loss: 0.754363  [86400/175341]\n",
      "loss: 0.552267  [88000/175341]\n",
      "loss: 0.177246  [89600/175341]\n",
      "loss: 0.587492  [91200/175341]\n",
      "loss: 0.923696  [92800/175341]\n",
      "loss: 0.242883  [94400/175341]\n",
      "loss: 0.488986  [96000/175341]\n",
      "loss: 0.708658  [97600/175341]\n",
      "loss: 0.431509  [99200/175341]\n",
      "loss: 0.690958  [100800/175341]\n",
      "loss: 0.296110  [102400/175341]\n",
      "loss: 1.086911  [104000/175341]\n",
      "loss: 0.368620  [105600/175341]\n",
      "loss: 0.450939  [107200/175341]\n",
      "loss: 0.508057  [108800/175341]\n",
      "loss: 0.374726  [110400/175341]\n",
      "loss: 0.547583  [112000/175341]\n",
      "loss: 0.316431  [113600/175341]\n",
      "loss: 0.633657  [115200/175341]\n",
      "loss: 0.254655  [116800/175341]\n",
      "loss: 0.639117  [118400/175341]\n",
      "loss: 0.367942  [120000/175341]\n",
      "loss: 0.649492  [121600/175341]\n",
      "loss: 0.835992  [123200/175341]\n",
      "loss: 0.635452  [124800/175341]\n",
      "loss: 0.758090  [126400/175341]\n",
      "loss: 0.661239  [128000/175341]\n",
      "loss: 0.369039  [129600/175341]\n",
      "loss: 0.444422  [131200/175341]\n",
      "loss: 0.340970  [132800/175341]\n",
      "loss: 0.182124  [134400/175341]\n",
      "loss: 0.889157  [136000/175341]\n",
      "loss: 0.535489  [137600/175341]\n",
      "loss: 0.515687  [139200/175341]\n",
      "loss: 0.813745  [140800/175341]\n",
      "loss: 0.794413  [142400/175341]\n",
      "loss: 0.808518  [144000/175341]\n",
      "loss: 0.511470  [145600/175341]\n",
      "loss: 0.868062  [147200/175341]\n",
      "loss: 0.790379  [148800/175341]\n",
      "loss: 0.328038  [150400/175341]\n",
      "loss: 0.298280  [152000/175341]\n",
      "loss: 0.254414  [153600/175341]\n",
      "loss: 0.823658  [155200/175341]\n",
      "loss: 0.099169  [156800/175341]\n",
      "loss: 0.773653  [158400/175341]\n",
      "loss: 0.848386  [160000/175341]\n",
      "loss: 0.870277  [161600/175341]\n",
      "loss: 0.466617  [163200/175341]\n",
      "loss: 0.522602  [164800/175341]\n",
      "loss: 0.253430  [166400/175341]\n",
      "loss: 0.397739  [168000/175341]\n",
      "loss: 0.540642  [169600/175341]\n",
      "loss: 0.331623  [171200/175341]\n",
      "loss: 0.585494  [172800/175341]\n",
      "loss: 0.446489  [174400/175341]\n",
      "Train Accuracy: 79.7184%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.589219, F1-score: 74.41%, Macro_F1-Score:  37.25%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.848317  [    0/175341]\n",
      "loss: 0.653765  [ 1600/175341]\n",
      "loss: 0.194097  [ 3200/175341]\n",
      "loss: 0.479797  [ 4800/175341]\n",
      "loss: 0.158617  [ 6400/175341]\n",
      "loss: 0.559600  [ 8000/175341]\n",
      "loss: 0.561746  [ 9600/175341]\n",
      "loss: 0.635896  [11200/175341]\n",
      "loss: 0.883830  [12800/175341]\n",
      "loss: 0.576558  [14400/175341]\n",
      "loss: 0.728996  [16000/175341]\n",
      "loss: 0.444383  [17600/175341]\n",
      "loss: 0.273238  [19200/175341]\n",
      "loss: 0.398294  [20800/175341]\n",
      "loss: 0.460729  [22400/175341]\n",
      "loss: 0.479450  [24000/175341]\n",
      "loss: 0.387560  [25600/175341]\n",
      "loss: 0.373910  [27200/175341]\n",
      "loss: 0.209454  [28800/175341]\n",
      "loss: 0.357108  [30400/175341]\n",
      "loss: 0.659065  [32000/175341]\n",
      "loss: 0.555304  [33600/175341]\n",
      "loss: 0.676080  [35200/175341]\n",
      "loss: 0.351042  [36800/175341]\n",
      "loss: 0.348420  [38400/175341]\n",
      "loss: 0.227228  [40000/175341]\n",
      "loss: 0.253280  [41600/175341]\n",
      "loss: 0.297275  [43200/175341]\n",
      "loss: 0.498206  [44800/175341]\n",
      "loss: 0.528575  [46400/175341]\n",
      "loss: 0.415284  [48000/175341]\n",
      "loss: 0.701022  [49600/175341]\n",
      "loss: 0.321870  [51200/175341]\n",
      "loss: 0.327218  [52800/175341]\n",
      "loss: 0.278359  [54400/175341]\n",
      "loss: 0.432973  [56000/175341]\n",
      "loss: 0.430247  [57600/175341]\n",
      "loss: 0.171871  [59200/175341]\n",
      "loss: 0.816028  [60800/175341]\n",
      "loss: 0.638577  [62400/175341]\n",
      "loss: 0.721276  [64000/175341]\n",
      "loss: 0.371849  [65600/175341]\n",
      "loss: 0.327087  [67200/175341]\n",
      "loss: 0.588186  [68800/175341]\n",
      "loss: 0.865478  [70400/175341]\n",
      "loss: 0.820364  [72000/175341]\n",
      "loss: 0.413416  [73600/175341]\n",
      "loss: 0.843174  [75200/175341]\n",
      "loss: 0.483849  [76800/175341]\n",
      "loss: 0.786948  [78400/175341]\n",
      "loss: 0.637323  [80000/175341]\n",
      "loss: 0.558823  [81600/175341]\n",
      "loss: 0.321984  [83200/175341]\n",
      "loss: 0.423008  [84800/175341]\n",
      "loss: 0.654444  [86400/175341]\n",
      "loss: 0.653385  [88000/175341]\n",
      "loss: 0.753355  [89600/175341]\n",
      "loss: 0.597554  [91200/175341]\n",
      "loss: 0.522390  [92800/175341]\n",
      "loss: 0.480391  [94400/175341]\n",
      "loss: 0.258937  [96000/175341]\n",
      "loss: 0.639014  [97600/175341]\n",
      "loss: 0.356266  [99200/175341]\n",
      "loss: 0.489560  [100800/175341]\n",
      "loss: 0.674110  [102400/175341]\n",
      "loss: 0.452784  [104000/175341]\n",
      "loss: 0.443968  [105600/175341]\n",
      "loss: 0.333005  [107200/175341]\n",
      "loss: 0.763560  [108800/175341]\n",
      "loss: 0.105837  [110400/175341]\n",
      "loss: 0.512181  [112000/175341]\n",
      "loss: 0.402697  [113600/175341]\n",
      "loss: 0.313470  [115200/175341]\n",
      "loss: 0.947503  [116800/175341]\n",
      "loss: 0.669838  [118400/175341]\n",
      "loss: 0.832546  [120000/175341]\n",
      "loss: 0.384417  [121600/175341]\n",
      "loss: 0.296323  [123200/175341]\n",
      "loss: 0.410758  [124800/175341]\n",
      "loss: 0.334439  [126400/175341]\n",
      "loss: 0.579947  [128000/175341]\n",
      "loss: 0.711152  [129600/175341]\n",
      "loss: 0.456328  [131200/175341]\n",
      "loss: 0.614912  [132800/175341]\n",
      "loss: 0.876549  [134400/175341]\n",
      "loss: 0.361825  [136000/175341]\n",
      "loss: 0.570922  [137600/175341]\n",
      "loss: 0.452011  [139200/175341]\n",
      "loss: 0.401622  [140800/175341]\n",
      "loss: 0.732958  [142400/175341]\n",
      "loss: 0.557403  [144000/175341]\n",
      "loss: 0.542757  [145600/175341]\n",
      "loss: 0.603119  [147200/175341]\n",
      "loss: 0.387338  [148800/175341]\n",
      "loss: 0.664726  [150400/175341]\n",
      "loss: 0.316071  [152000/175341]\n",
      "loss: 0.653212  [153600/175341]\n",
      "loss: 0.177936  [155200/175341]\n",
      "loss: 0.266864  [156800/175341]\n",
      "loss: 0.923365  [158400/175341]\n",
      "loss: 0.795707  [160000/175341]\n",
      "loss: 0.402659  [161600/175341]\n",
      "loss: 0.391670  [163200/175341]\n",
      "loss: 0.487731  [164800/175341]\n",
      "loss: 0.352359  [166400/175341]\n",
      "loss: 0.785520  [168000/175341]\n",
      "loss: 0.635170  [169600/175341]\n",
      "loss: 0.860116  [171200/175341]\n",
      "loss: 0.453122  [172800/175341]\n",
      "loss: 0.415074  [174400/175341]\n",
      "Train Accuracy: 79.7720%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.580740, F1-score: 74.66%, Macro_F1-Score:  37.49%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.619800  [    0/175341]\n",
      "loss: 1.156219  [ 1600/175341]\n",
      "loss: 0.512988  [ 3200/175341]\n",
      "loss: 0.398687  [ 4800/175341]\n",
      "loss: 0.586810  [ 6400/175341]\n",
      "loss: 0.642246  [ 8000/175341]\n",
      "loss: 0.249750  [ 9600/175341]\n",
      "loss: 0.610653  [11200/175341]\n",
      "loss: 0.295960  [12800/175341]\n",
      "loss: 0.513332  [14400/175341]\n",
      "loss: 0.365306  [16000/175341]\n",
      "loss: 0.480904  [17600/175341]\n",
      "loss: 0.556459  [19200/175341]\n",
      "loss: 0.585667  [20800/175341]\n",
      "loss: 0.661273  [22400/175341]\n",
      "loss: 0.204592  [24000/175341]\n",
      "loss: 0.748019  [25600/175341]\n",
      "loss: 0.513388  [27200/175341]\n",
      "loss: 0.657811  [28800/175341]\n",
      "loss: 0.467638  [30400/175341]\n",
      "loss: 0.365472  [32000/175341]\n",
      "loss: 0.531836  [33600/175341]\n",
      "loss: 0.356566  [35200/175341]\n",
      "loss: 0.665321  [36800/175341]\n",
      "loss: 0.250517  [38400/175341]\n",
      "loss: 0.777387  [40000/175341]\n",
      "loss: 0.611388  [41600/175341]\n",
      "loss: 0.408324  [43200/175341]\n",
      "loss: 0.292867  [44800/175341]\n",
      "loss: 0.663688  [46400/175341]\n",
      "loss: 0.736706  [48000/175341]\n",
      "loss: 0.656549  [49600/175341]\n",
      "loss: 0.152413  [51200/175341]\n",
      "loss: 0.694582  [52800/175341]\n",
      "loss: 0.245789  [54400/175341]\n",
      "loss: 0.548933  [56000/175341]\n",
      "loss: 0.339804  [57600/175341]\n",
      "loss: 0.878848  [59200/175341]\n",
      "loss: 0.270731  [60800/175341]\n",
      "loss: 0.463865  [62400/175341]\n",
      "loss: 0.240286  [64000/175341]\n",
      "loss: 0.528246  [65600/175341]\n",
      "loss: 0.269170  [67200/175341]\n",
      "loss: 0.518862  [68800/175341]\n",
      "loss: 0.472021  [70400/175341]\n",
      "loss: 0.680099  [72000/175341]\n",
      "loss: 0.378244  [73600/175341]\n",
      "loss: 0.487326  [75200/175341]\n",
      "loss: 0.560793  [76800/175341]\n",
      "loss: 0.586938  [78400/175341]\n",
      "loss: 0.915179  [80000/175341]\n",
      "loss: 0.157952  [81600/175341]\n",
      "loss: 0.723115  [83200/175341]\n",
      "loss: 0.926666  [84800/175341]\n",
      "loss: 0.738566  [86400/175341]\n",
      "loss: 0.414512  [88000/175341]\n",
      "loss: 0.432650  [89600/175341]\n",
      "loss: 0.537864  [91200/175341]\n",
      "loss: 0.605902  [92800/175341]\n",
      "loss: 0.552312  [94400/175341]\n",
      "loss: 0.988068  [96000/175341]\n",
      "loss: 0.274028  [97600/175341]\n",
      "loss: 0.819973  [99200/175341]\n",
      "loss: 0.317681  [100800/175341]\n",
      "loss: 0.354967  [102400/175341]\n",
      "loss: 0.175594  [104000/175341]\n",
      "loss: 0.554695  [105600/175341]\n",
      "loss: 0.198213  [107200/175341]\n",
      "loss: 0.405730  [108800/175341]\n",
      "loss: 0.420925  [110400/175341]\n",
      "loss: 0.612997  [112000/175341]\n",
      "loss: 0.550374  [113600/175341]\n",
      "loss: 0.825587  [115200/175341]\n",
      "loss: 0.637004  [116800/175341]\n",
      "loss: 0.449577  [118400/175341]\n",
      "loss: 0.716884  [120000/175341]\n",
      "loss: 0.393505  [121600/175341]\n",
      "loss: 0.366955  [123200/175341]\n",
      "loss: 0.835363  [124800/175341]\n",
      "loss: 0.439536  [126400/175341]\n",
      "loss: 0.240267  [128000/175341]\n",
      "loss: 0.500458  [129600/175341]\n",
      "loss: 0.477516  [131200/175341]\n",
      "loss: 0.416689  [132800/175341]\n",
      "loss: 0.625935  [134400/175341]\n",
      "loss: 0.286098  [136000/175341]\n",
      "loss: 0.248668  [137600/175341]\n",
      "loss: 0.385167  [139200/175341]\n",
      "loss: 0.525667  [140800/175341]\n",
      "loss: 0.519985  [142400/175341]\n",
      "loss: 0.586428  [144000/175341]\n",
      "loss: 0.673020  [145600/175341]\n",
      "loss: 0.220473  [147200/175341]\n",
      "loss: 0.464002  [148800/175341]\n",
      "loss: 0.723213  [150400/175341]\n",
      "loss: 0.316679  [152000/175341]\n",
      "loss: 0.687287  [153600/175341]\n",
      "loss: 0.767545  [155200/175341]\n",
      "loss: 0.674449  [156800/175341]\n",
      "loss: 0.670630  [158400/175341]\n",
      "loss: 0.252277  [160000/175341]\n",
      "loss: 0.153109  [161600/175341]\n",
      "loss: 0.472776  [163200/175341]\n",
      "loss: 0.324388  [164800/175341]\n",
      "loss: 0.527868  [166400/175341]\n",
      "loss: 0.422167  [168000/175341]\n",
      "loss: 0.635443  [169600/175341]\n",
      "loss: 0.371169  [171200/175341]\n",
      "loss: 0.325932  [172800/175341]\n",
      "loss: 0.484559  [174400/175341]\n",
      "Train Accuracy: 79.7423%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.584461, F1-score: 74.30%, Macro_F1-Score:  38.20%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.512688  [    0/175341]\n",
      "loss: 0.213747  [ 1600/175341]\n",
      "loss: 0.773675  [ 3200/175341]\n",
      "loss: 0.350707  [ 4800/175341]\n",
      "loss: 0.377348  [ 6400/175341]\n",
      "loss: 0.905144  [ 8000/175341]\n",
      "loss: 0.562561  [ 9600/175341]\n",
      "loss: 0.465604  [11200/175341]\n",
      "loss: 0.331243  [12800/175341]\n",
      "loss: 0.383229  [14400/175341]\n",
      "loss: 0.148721  [16000/175341]\n",
      "loss: 0.632310  [17600/175341]\n",
      "loss: 0.760851  [19200/175341]\n",
      "loss: 0.441999  [20800/175341]\n",
      "loss: 0.498071  [22400/175341]\n",
      "loss: 0.718604  [24000/175341]\n",
      "loss: 0.431780  [25600/175341]\n",
      "loss: 0.322248  [27200/175341]\n",
      "loss: 0.468883  [28800/175341]\n",
      "loss: 0.531135  [30400/175341]\n",
      "loss: 0.348937  [32000/175341]\n",
      "loss: 0.966906  [33600/175341]\n",
      "loss: 0.636027  [35200/175341]\n",
      "loss: 0.583905  [36800/175341]\n",
      "loss: 0.592131  [38400/175341]\n",
      "loss: 0.639992  [40000/175341]\n",
      "loss: 0.356811  [41600/175341]\n",
      "loss: 0.383716  [43200/175341]\n",
      "loss: 0.211846  [44800/175341]\n",
      "loss: 0.490281  [46400/175341]\n",
      "loss: 0.638999  [48000/175341]\n",
      "loss: 0.115831  [49600/175341]\n",
      "loss: 0.495047  [51200/175341]\n",
      "loss: 0.594555  [52800/175341]\n",
      "loss: 0.627463  [54400/175341]\n",
      "loss: 0.563988  [56000/175341]\n",
      "loss: 0.765144  [57600/175341]\n",
      "loss: 0.677233  [59200/175341]\n",
      "loss: 0.232808  [60800/175341]\n",
      "loss: 0.179699  [62400/175341]\n",
      "loss: 0.392167  [64000/175341]\n",
      "loss: 0.301999  [65600/175341]\n",
      "loss: 0.635538  [67200/175341]\n",
      "loss: 0.441751  [68800/175341]\n",
      "loss: 0.872339  [70400/175341]\n",
      "loss: 0.348791  [72000/175341]\n",
      "loss: 0.541331  [73600/175341]\n",
      "loss: 0.724844  [75200/175341]\n",
      "loss: 0.633647  [76800/175341]\n",
      "loss: 0.248015  [78400/175341]\n",
      "loss: 0.505866  [80000/175341]\n",
      "loss: 0.633941  [81600/175341]\n",
      "loss: 0.681915  [83200/175341]\n",
      "loss: 0.564713  [84800/175341]\n",
      "loss: 0.554430  [86400/175341]\n",
      "loss: 0.251607  [88000/175341]\n",
      "loss: 0.554390  [89600/175341]\n",
      "loss: 0.957720  [91200/175341]\n",
      "loss: 0.705264  [92800/175341]\n",
      "loss: 0.740157  [94400/175341]\n",
      "loss: 0.333022  [96000/175341]\n",
      "loss: 0.434159  [97600/175341]\n",
      "loss: 0.537332  [99200/175341]\n",
      "loss: 0.549412  [100800/175341]\n",
      "loss: 0.552414  [102400/175341]\n",
      "loss: 0.281209  [104000/175341]\n",
      "loss: 0.545631  [105600/175341]\n",
      "loss: 0.308103  [107200/175341]\n",
      "loss: 0.416842  [108800/175341]\n",
      "loss: 0.275423  [110400/175341]\n",
      "loss: 0.910786  [112000/175341]\n",
      "loss: 0.167745  [113600/175341]\n",
      "loss: 0.238829  [115200/175341]\n",
      "loss: 0.366673  [116800/175341]\n",
      "loss: 0.205811  [118400/175341]\n",
      "loss: 0.429436  [120000/175341]\n",
      "loss: 0.480735  [121600/175341]\n",
      "loss: 0.469473  [123200/175341]\n",
      "loss: 0.717261  [124800/175341]\n",
      "loss: 1.044628  [126400/175341]\n",
      "loss: 0.605917  [128000/175341]\n",
      "loss: 0.392846  [129600/175341]\n",
      "loss: 0.367256  [131200/175341]\n",
      "loss: 0.335703  [132800/175341]\n",
      "loss: 0.772772  [134400/175341]\n",
      "loss: 0.517089  [136000/175341]\n",
      "loss: 0.407324  [137600/175341]\n",
      "loss: 0.707765  [139200/175341]\n",
      "loss: 0.636424  [140800/175341]\n",
      "loss: 0.235554  [142400/175341]\n",
      "loss: 0.662277  [144000/175341]\n",
      "loss: 0.643426  [145600/175341]\n",
      "loss: 0.584968  [147200/175341]\n",
      "loss: 0.651689  [148800/175341]\n",
      "loss: 0.653724  [150400/175341]\n",
      "loss: 0.383816  [152000/175341]\n",
      "loss: 0.922693  [153600/175341]\n",
      "loss: 0.499681  [155200/175341]\n",
      "loss: 0.494229  [156800/175341]\n",
      "loss: 0.795372  [158400/175341]\n",
      "loss: 0.322753  [160000/175341]\n",
      "loss: 0.535374  [161600/175341]\n",
      "loss: 0.418914  [163200/175341]\n",
      "loss: 0.247155  [164800/175341]\n",
      "loss: 0.197814  [166400/175341]\n",
      "loss: 0.477437  [168000/175341]\n",
      "loss: 0.812777  [169600/175341]\n",
      "loss: 0.777168  [171200/175341]\n",
      "loss: 1.425968  [172800/175341]\n",
      "loss: 0.359501  [174400/175341]\n",
      "Train Accuracy: 79.8108%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.587007, F1-score: 74.23%, Macro_F1-Score:  38.24%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.660982  [    0/175341]\n",
      "loss: 0.757325  [ 1600/175341]\n",
      "loss: 0.725256  [ 3200/175341]\n",
      "loss: 0.817503  [ 4800/175341]\n",
      "loss: 0.564585  [ 6400/175341]\n",
      "loss: 0.285824  [ 8000/175341]\n",
      "loss: 0.248364  [ 9600/175341]\n",
      "loss: 0.838232  [11200/175341]\n",
      "loss: 0.492514  [12800/175341]\n",
      "loss: 0.525738  [14400/175341]\n",
      "loss: 0.551076  [16000/175341]\n",
      "loss: 0.509240  [17600/175341]\n",
      "loss: 0.625352  [19200/175341]\n",
      "loss: 0.494813  [20800/175341]\n",
      "loss: 0.838184  [22400/175341]\n",
      "loss: 0.305963  [24000/175341]\n",
      "loss: 0.291127  [25600/175341]\n",
      "loss: 0.563443  [27200/175341]\n",
      "loss: 0.417387  [28800/175341]\n",
      "loss: 0.196432  [30400/175341]\n",
      "loss: 0.288246  [32000/175341]\n",
      "loss: 0.415211  [33600/175341]\n",
      "loss: 0.353109  [35200/175341]\n",
      "loss: 0.224941  [36800/175341]\n",
      "loss: 0.322295  [38400/175341]\n",
      "loss: 0.659884  [40000/175341]\n",
      "loss: 0.094326  [41600/175341]\n",
      "loss: 0.644159  [43200/175341]\n",
      "loss: 0.756153  [44800/175341]\n",
      "loss: 0.508029  [46400/175341]\n",
      "loss: 0.255066  [48000/175341]\n",
      "loss: 0.348486  [49600/175341]\n",
      "loss: 0.260356  [51200/175341]\n",
      "loss: 0.543993  [52800/175341]\n",
      "loss: 0.090008  [54400/175341]\n",
      "loss: 0.732048  [56000/175341]\n",
      "loss: 0.348577  [57600/175341]\n",
      "loss: 0.833814  [59200/175341]\n",
      "loss: 0.223613  [60800/175341]\n",
      "loss: 0.403297  [62400/175341]\n",
      "loss: 0.339224  [64000/175341]\n",
      "loss: 0.497987  [65600/175341]\n",
      "loss: 0.363237  [67200/175341]\n",
      "loss: 0.673230  [68800/175341]\n",
      "loss: 0.392651  [70400/175341]\n",
      "loss: 0.150251  [72000/175341]\n",
      "loss: 0.268135  [73600/175341]\n",
      "loss: 0.739489  [75200/175341]\n",
      "loss: 0.595859  [76800/175341]\n",
      "loss: 0.200623  [78400/175341]\n",
      "loss: 0.807807  [80000/175341]\n",
      "loss: 0.550608  [81600/175341]\n",
      "loss: 0.395478  [83200/175341]\n",
      "loss: 0.220096  [84800/175341]\n",
      "loss: 0.367491  [86400/175341]\n",
      "loss: 0.464280  [88000/175341]\n",
      "loss: 0.487664  [89600/175341]\n",
      "loss: 0.725417  [91200/175341]\n",
      "loss: 0.450721  [92800/175341]\n",
      "loss: 0.194788  [94400/175341]\n",
      "loss: 0.618446  [96000/175341]\n",
      "loss: 0.483341  [97600/175341]\n",
      "loss: 0.286675  [99200/175341]\n",
      "loss: 0.672119  [100800/175341]\n",
      "loss: 0.534807  [102400/175341]\n",
      "loss: 0.720095  [104000/175341]\n",
      "loss: 0.359769  [105600/175341]\n",
      "loss: 0.266996  [107200/175341]\n",
      "loss: 0.272018  [108800/175341]\n",
      "loss: 0.290180  [110400/175341]\n",
      "loss: 0.313046  [112000/175341]\n",
      "loss: 0.244466  [113600/175341]\n",
      "loss: 0.514783  [115200/175341]\n",
      "loss: 0.731824  [116800/175341]\n",
      "loss: 0.487940  [118400/175341]\n",
      "loss: 0.552281  [120000/175341]\n",
      "loss: 0.450982  [121600/175341]\n",
      "loss: 0.218899  [123200/175341]\n",
      "loss: 0.618135  [124800/175341]\n",
      "loss: 0.366340  [126400/175341]\n",
      "loss: 0.397012  [128000/175341]\n",
      "loss: 0.439383  [129600/175341]\n",
      "loss: 0.445404  [131200/175341]\n",
      "loss: 0.349808  [132800/175341]\n",
      "loss: 0.394883  [134400/175341]\n",
      "loss: 0.711269  [136000/175341]\n",
      "loss: 0.353021  [137600/175341]\n",
      "loss: 0.730952  [139200/175341]\n",
      "loss: 0.604001  [140800/175341]\n",
      "loss: 0.474118  [142400/175341]\n",
      "loss: 0.631765  [144000/175341]\n",
      "loss: 0.354240  [145600/175341]\n",
      "loss: 0.899406  [147200/175341]\n",
      "loss: 0.601927  [148800/175341]\n",
      "loss: 0.665795  [150400/175341]\n",
      "loss: 0.799500  [152000/175341]\n",
      "loss: 0.255997  [153600/175341]\n",
      "loss: 0.335402  [155200/175341]\n",
      "loss: 0.338330  [156800/175341]\n",
      "loss: 0.842665  [158400/175341]\n",
      "loss: 0.381334  [160000/175341]\n",
      "loss: 0.631740  [161600/175341]\n",
      "loss: 0.330940  [163200/175341]\n",
      "loss: 0.393213  [164800/175341]\n",
      "loss: 0.613451  [166400/175341]\n",
      "loss: 0.371803  [168000/175341]\n",
      "loss: 0.519852  [169600/175341]\n",
      "loss: 0.186512  [171200/175341]\n",
      "loss: 0.526437  [172800/175341]\n",
      "loss: 0.374237  [174400/175341]\n",
      "Train Accuracy: 79.8707%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.587857, F1-score: 74.68%, Macro_F1-Score:  37.76%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.205207  [    0/175341]\n",
      "loss: 0.603918  [ 1600/175341]\n",
      "loss: 0.467567  [ 3200/175341]\n",
      "loss: 0.608116  [ 4800/175341]\n",
      "loss: 0.297922  [ 6400/175341]\n",
      "loss: 0.408085  [ 8000/175341]\n",
      "loss: 0.281089  [ 9600/175341]\n",
      "loss: 0.495395  [11200/175341]\n",
      "loss: 0.686435  [12800/175341]\n",
      "loss: 0.271916  [14400/175341]\n",
      "loss: 1.074074  [16000/175341]\n",
      "loss: 0.399019  [17600/175341]\n",
      "loss: 0.780207  [19200/175341]\n",
      "loss: 0.849982  [20800/175341]\n",
      "loss: 0.696009  [22400/175341]\n",
      "loss: 0.588660  [24000/175341]\n",
      "loss: 0.363920  [25600/175341]\n",
      "loss: 0.549888  [27200/175341]\n",
      "loss: 0.463293  [28800/175341]\n",
      "loss: 0.333667  [30400/175341]\n",
      "loss: 0.592413  [32000/175341]\n",
      "loss: 0.679824  [33600/175341]\n",
      "loss: 0.378614  [35200/175341]\n",
      "loss: 0.627513  [36800/175341]\n",
      "loss: 0.367825  [38400/175341]\n",
      "loss: 0.460930  [40000/175341]\n",
      "loss: 0.596279  [41600/175341]\n",
      "loss: 0.464826  [43200/175341]\n",
      "loss: 0.702972  [44800/175341]\n",
      "loss: 0.429356  [46400/175341]\n",
      "loss: 0.364352  [48000/175341]\n",
      "loss: 0.536017  [49600/175341]\n",
      "loss: 0.241055  [51200/175341]\n",
      "loss: 1.251050  [52800/175341]\n",
      "loss: 0.526720  [54400/175341]\n",
      "loss: 0.218482  [56000/175341]\n",
      "loss: 0.834583  [57600/175341]\n",
      "loss: 0.588653  [59200/175341]\n",
      "loss: 0.753741  [60800/175341]\n",
      "loss: 0.836572  [62400/175341]\n",
      "loss: 0.631782  [64000/175341]\n",
      "loss: 0.403574  [65600/175341]\n",
      "loss: 0.868809  [67200/175341]\n",
      "loss: 0.308503  [68800/175341]\n",
      "loss: 0.528608  [70400/175341]\n",
      "loss: 0.195662  [72000/175341]\n",
      "loss: 0.270824  [73600/175341]\n",
      "loss: 0.422195  [75200/175341]\n",
      "loss: 0.371703  [76800/175341]\n",
      "loss: 0.219220  [78400/175341]\n",
      "loss: 0.214039  [80000/175341]\n",
      "loss: 0.657433  [81600/175341]\n",
      "loss: 0.626384  [83200/175341]\n",
      "loss: 0.599360  [84800/175341]\n",
      "loss: 0.189294  [86400/175341]\n",
      "loss: 0.494931  [88000/175341]\n",
      "loss: 0.754197  [89600/175341]\n",
      "loss: 0.389379  [91200/175341]\n",
      "loss: 0.261683  [92800/175341]\n",
      "loss: 0.503648  [94400/175341]\n",
      "loss: 0.585616  [96000/175341]\n",
      "loss: 0.267891  [97600/175341]\n",
      "loss: 0.569954  [99200/175341]\n",
      "loss: 0.396753  [100800/175341]\n",
      "loss: 0.432098  [102400/175341]\n",
      "loss: 0.537098  [104000/175341]\n",
      "loss: 0.661301  [105600/175341]\n",
      "loss: 0.470908  [107200/175341]\n",
      "loss: 0.472316  [108800/175341]\n",
      "loss: 0.304738  [110400/175341]\n",
      "loss: 0.964310  [112000/175341]\n",
      "loss: 0.648698  [113600/175341]\n",
      "loss: 0.036573  [115200/175341]\n",
      "loss: 0.575807  [116800/175341]\n",
      "loss: 0.381954  [118400/175341]\n",
      "loss: 0.748340  [120000/175341]\n",
      "loss: 0.599269  [121600/175341]\n",
      "loss: 0.485874  [123200/175341]\n",
      "loss: 0.217136  [124800/175341]\n",
      "loss: 0.488895  [126400/175341]\n",
      "loss: 0.692742  [128000/175341]\n",
      "loss: 0.507538  [129600/175341]\n",
      "loss: 0.907246  [131200/175341]\n",
      "loss: 0.463369  [132800/175341]\n",
      "loss: 0.317045  [134400/175341]\n",
      "loss: 0.690618  [136000/175341]\n",
      "loss: 0.748272  [137600/175341]\n",
      "loss: 0.226236  [139200/175341]\n",
      "loss: 0.745723  [140800/175341]\n",
      "loss: 0.365487  [142400/175341]\n",
      "loss: 0.592170  [144000/175341]\n",
      "loss: 0.462829  [145600/175341]\n",
      "loss: 0.439462  [147200/175341]\n",
      "loss: 0.811217  [148800/175341]\n",
      "loss: 0.171667  [150400/175341]\n",
      "loss: 0.237400  [152000/175341]\n",
      "loss: 0.295225  [153600/175341]\n",
      "loss: 0.634504  [155200/175341]\n",
      "loss: 0.737120  [156800/175341]\n",
      "loss: 0.170182  [158400/175341]\n",
      "loss: 0.559146  [160000/175341]\n",
      "loss: 0.288349  [161600/175341]\n",
      "loss: 0.460151  [163200/175341]\n",
      "loss: 0.445997  [164800/175341]\n",
      "loss: 0.512808  [166400/175341]\n",
      "loss: 0.678413  [168000/175341]\n",
      "loss: 0.666250  [169600/175341]\n",
      "loss: 0.628100  [171200/175341]\n",
      "loss: 0.481002  [172800/175341]\n",
      "loss: 0.547184  [174400/175341]\n",
      "Train Accuracy: 79.8786%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.573309, F1-score: 74.94%, Macro_F1-Score:  38.84%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.282829  [    0/175341]\n",
      "loss: 0.419459  [ 1600/175341]\n",
      "loss: 0.150570  [ 3200/175341]\n",
      "loss: 0.080977  [ 4800/175341]\n",
      "loss: 0.636729  [ 6400/175341]\n",
      "loss: 0.281249  [ 8000/175341]\n",
      "loss: 0.760428  [ 9600/175341]\n",
      "loss: 0.349828  [11200/175341]\n",
      "loss: 0.265624  [12800/175341]\n",
      "loss: 0.903751  [14400/175341]\n",
      "loss: 0.312605  [16000/175341]\n",
      "loss: 0.471178  [17600/175341]\n",
      "loss: 0.713506  [19200/175341]\n",
      "loss: 0.282899  [20800/175341]\n",
      "loss: 0.496229  [22400/175341]\n",
      "loss: 0.682553  [24000/175341]\n",
      "loss: 0.800193  [25600/175341]\n",
      "loss: 0.772248  [27200/175341]\n",
      "loss: 0.830515  [28800/175341]\n",
      "loss: 0.458383  [30400/175341]\n",
      "loss: 0.331826  [32000/175341]\n",
      "loss: 0.297942  [33600/175341]\n",
      "loss: 0.567586  [35200/175341]\n",
      "loss: 0.283649  [36800/175341]\n",
      "loss: 0.399341  [38400/175341]\n",
      "loss: 0.947857  [40000/175341]\n",
      "loss: 0.195034  [41600/175341]\n",
      "loss: 0.534915  [43200/175341]\n",
      "loss: 0.953259  [44800/175341]\n",
      "loss: 0.270950  [46400/175341]\n",
      "loss: 0.453086  [48000/175341]\n",
      "loss: 0.115340  [49600/175341]\n",
      "loss: 0.796639  [51200/175341]\n",
      "loss: 0.296891  [52800/175341]\n",
      "loss: 0.269012  [54400/175341]\n",
      "loss: 0.545563  [56000/175341]\n",
      "loss: 0.816788  [57600/175341]\n",
      "loss: 0.702619  [59200/175341]\n",
      "loss: 0.844704  [60800/175341]\n",
      "loss: 0.752863  [62400/175341]\n",
      "loss: 0.234386  [64000/175341]\n",
      "loss: 0.452822  [65600/175341]\n",
      "loss: 0.120827  [67200/175341]\n",
      "loss: 0.487999  [68800/175341]\n",
      "loss: 0.508370  [70400/175341]\n",
      "loss: 0.377603  [72000/175341]\n",
      "loss: 0.214389  [73600/175341]\n",
      "loss: 0.384753  [75200/175341]\n",
      "loss: 0.528928  [76800/175341]\n",
      "loss: 0.210422  [78400/175341]\n",
      "loss: 0.841348  [80000/175341]\n",
      "loss: 0.421107  [81600/175341]\n",
      "loss: 0.245240  [83200/175341]\n",
      "loss: 0.537760  [84800/175341]\n",
      "loss: 0.283020  [86400/175341]\n",
      "loss: 0.492753  [88000/175341]\n",
      "loss: 0.785498  [89600/175341]\n",
      "loss: 0.842137  [91200/175341]\n",
      "loss: 0.175092  [92800/175341]\n",
      "loss: 0.238783  [94400/175341]\n",
      "loss: 0.280312  [96000/175341]\n",
      "loss: 0.737777  [97600/175341]\n",
      "loss: 0.770301  [99200/175341]\n",
      "loss: 0.466770  [100800/175341]\n",
      "loss: 0.425122  [102400/175341]\n",
      "loss: 0.520287  [104000/175341]\n",
      "loss: 0.355568  [105600/175341]\n",
      "loss: 0.784613  [107200/175341]\n",
      "loss: 0.323967  [108800/175341]\n",
      "loss: 0.875636  [110400/175341]\n",
      "loss: 0.559326  [112000/175341]\n",
      "loss: 0.122275  [113600/175341]\n",
      "loss: 0.872615  [115200/175341]\n",
      "loss: 0.443319  [116800/175341]\n",
      "loss: 0.540885  [118400/175341]\n",
      "loss: 0.650304  [120000/175341]\n",
      "loss: 0.730783  [121600/175341]\n",
      "loss: 0.113063  [123200/175341]\n",
      "loss: 0.678186  [124800/175341]\n",
      "loss: 0.733251  [126400/175341]\n",
      "loss: 0.379604  [128000/175341]\n",
      "loss: 0.528830  [129600/175341]\n",
      "loss: 0.573562  [131200/175341]\n",
      "loss: 0.499856  [132800/175341]\n",
      "loss: 0.371612  [134400/175341]\n",
      "loss: 0.320621  [136000/175341]\n",
      "loss: 0.227830  [137600/175341]\n",
      "loss: 0.614607  [139200/175341]\n",
      "loss: 0.636362  [140800/175341]\n",
      "loss: 0.319051  [142400/175341]\n",
      "loss: 0.273691  [144000/175341]\n",
      "loss: 1.278797  [145600/175341]\n",
      "loss: 0.557091  [147200/175341]\n",
      "loss: 1.088533  [148800/175341]\n",
      "loss: 0.369630  [150400/175341]\n",
      "loss: 0.945755  [152000/175341]\n",
      "loss: 0.425466  [153600/175341]\n",
      "loss: 0.683835  [155200/175341]\n",
      "loss: 0.856079  [156800/175341]\n",
      "loss: 0.795054  [158400/175341]\n",
      "loss: 0.268677  [160000/175341]\n",
      "loss: 0.290757  [161600/175341]\n",
      "loss: 0.859109  [163200/175341]\n",
      "loss: 0.477792  [164800/175341]\n",
      "loss: 0.279183  [166400/175341]\n",
      "loss: 0.504112  [168000/175341]\n",
      "loss: 0.856750  [169600/175341]\n",
      "loss: 0.393110  [171200/175341]\n",
      "loss: 0.477807  [172800/175341]\n",
      "loss: 0.677033  [174400/175341]\n",
      "Train Accuracy: 79.8758%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.566154, F1-score: 75.76%, Macro_F1-Score:  39.04%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.350182  [    0/175341]\n",
      "loss: 0.343457  [ 1600/175341]\n",
      "loss: 0.480900  [ 3200/175341]\n",
      "loss: 0.454585  [ 4800/175341]\n",
      "loss: 0.637704  [ 6400/175341]\n",
      "loss: 0.416121  [ 8000/175341]\n",
      "loss: 0.331304  [ 9600/175341]\n",
      "loss: 0.490227  [11200/175341]\n",
      "loss: 1.095849  [12800/175341]\n",
      "loss: 0.346822  [14400/175341]\n",
      "loss: 0.383918  [16000/175341]\n",
      "loss: 0.256306  [17600/175341]\n",
      "loss: 0.508846  [19200/175341]\n",
      "loss: 0.365904  [20800/175341]\n",
      "loss: 0.554113  [22400/175341]\n",
      "loss: 0.533664  [24000/175341]\n",
      "loss: 0.460873  [25600/175341]\n",
      "loss: 0.402638  [27200/175341]\n",
      "loss: 0.544575  [28800/175341]\n",
      "loss: 0.692124  [30400/175341]\n",
      "loss: 0.785596  [32000/175341]\n",
      "loss: 0.578114  [33600/175341]\n",
      "loss: 0.592025  [35200/175341]\n",
      "loss: 0.578533  [36800/175341]\n",
      "loss: 0.777456  [38400/175341]\n",
      "loss: 1.051555  [40000/175341]\n",
      "loss: 1.028651  [41600/175341]\n",
      "loss: 0.477741  [43200/175341]\n",
      "loss: 0.530771  [44800/175341]\n",
      "loss: 0.433612  [46400/175341]\n",
      "loss: 0.472542  [48000/175341]\n",
      "loss: 0.460324  [49600/175341]\n",
      "loss: 0.369450  [51200/175341]\n",
      "loss: 0.393597  [52800/175341]\n",
      "loss: 0.333505  [54400/175341]\n",
      "loss: 0.797696  [56000/175341]\n",
      "loss: 0.443926  [57600/175341]\n",
      "loss: 0.546212  [59200/175341]\n",
      "loss: 0.462293  [60800/175341]\n",
      "loss: 0.361668  [62400/175341]\n",
      "loss: 0.496977  [64000/175341]\n",
      "loss: 0.415054  [65600/175341]\n",
      "loss: 0.671179  [67200/175341]\n",
      "loss: 0.553418  [68800/175341]\n",
      "loss: 0.384758  [70400/175341]\n",
      "loss: 0.576144  [72000/175341]\n",
      "loss: 0.410620  [73600/175341]\n",
      "loss: 0.630803  [75200/175341]\n",
      "loss: 0.591081  [76800/175341]\n",
      "loss: 0.402381  [78400/175341]\n",
      "loss: 0.805540  [80000/175341]\n",
      "loss: 0.411441  [81600/175341]\n",
      "loss: 0.283647  [83200/175341]\n",
      "loss: 1.008899  [84800/175341]\n",
      "loss: 0.687472  [86400/175341]\n",
      "loss: 0.706843  [88000/175341]\n",
      "loss: 0.568377  [89600/175341]\n",
      "loss: 0.153025  [91200/175341]\n",
      "loss: 0.926448  [92800/175341]\n",
      "loss: 0.360779  [94400/175341]\n",
      "loss: 0.214118  [96000/175341]\n",
      "loss: 0.826823  [97600/175341]\n",
      "loss: 0.607519  [99200/175341]\n",
      "loss: 0.337173  [100800/175341]\n",
      "loss: 0.921943  [102400/175341]\n",
      "loss: 0.405966  [104000/175341]\n",
      "loss: 0.282157  [105600/175341]\n",
      "loss: 0.682142  [107200/175341]\n",
      "loss: 0.529251  [108800/175341]\n",
      "loss: 0.828399  [110400/175341]\n",
      "loss: 0.882375  [112000/175341]\n",
      "loss: 0.514260  [113600/175341]\n",
      "loss: 0.313842  [115200/175341]\n",
      "loss: 0.325800  [116800/175341]\n",
      "loss: 0.248052  [118400/175341]\n",
      "loss: 0.811208  [120000/175341]\n",
      "loss: 0.743488  [121600/175341]\n",
      "loss: 0.459602  [123200/175341]\n",
      "loss: 0.257048  [124800/175341]\n",
      "loss: 0.823176  [126400/175341]\n",
      "loss: 0.590382  [128000/175341]\n",
      "loss: 0.396266  [129600/175341]\n",
      "loss: 0.364736  [131200/175341]\n",
      "loss: 0.456028  [132800/175341]\n",
      "loss: 0.399824  [134400/175341]\n",
      "loss: 0.658467  [136000/175341]\n",
      "loss: 0.422064  [137600/175341]\n",
      "loss: 0.344509  [139200/175341]\n",
      "loss: 0.470167  [140800/175341]\n",
      "loss: 0.635692  [142400/175341]\n",
      "loss: 0.621551  [144000/175341]\n",
      "loss: 0.442661  [145600/175341]\n",
      "loss: 0.416379  [147200/175341]\n",
      "loss: 0.374107  [148800/175341]\n",
      "loss: 0.352544  [150400/175341]\n",
      "loss: 0.405067  [152000/175341]\n",
      "loss: 0.117892  [153600/175341]\n",
      "loss: 0.436590  [155200/175341]\n",
      "loss: 0.381346  [156800/175341]\n",
      "loss: 0.475386  [158400/175341]\n",
      "loss: 1.179375  [160000/175341]\n",
      "loss: 0.418734  [161600/175341]\n",
      "loss: 0.405349  [163200/175341]\n",
      "loss: 0.907347  [164800/175341]\n",
      "loss: 0.453380  [166400/175341]\n",
      "loss: 0.531776  [168000/175341]\n",
      "loss: 0.391555  [169600/175341]\n",
      "loss: 1.326134  [171200/175341]\n",
      "loss: 0.511458  [172800/175341]\n",
      "loss: 0.405190  [174400/175341]\n",
      "Train Accuracy: 79.9368%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.575137, F1-score: 75.09%, Macro_F1-Score:  38.92%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.389262  [    0/175341]\n",
      "loss: 0.482662  [ 1600/175341]\n",
      "loss: 0.558043  [ 3200/175341]\n",
      "loss: 0.859446  [ 4800/175341]\n",
      "loss: 0.407065  [ 6400/175341]\n",
      "loss: 0.960928  [ 8000/175341]\n",
      "loss: 0.789036  [ 9600/175341]\n",
      "loss: 0.622245  [11200/175341]\n",
      "loss: 0.403059  [12800/175341]\n",
      "loss: 0.436946  [14400/175341]\n",
      "loss: 0.311812  [16000/175341]\n",
      "loss: 0.319379  [17600/175341]\n",
      "loss: 0.836128  [19200/175341]\n",
      "loss: 0.288336  [20800/175341]\n",
      "loss: 0.390365  [22400/175341]\n",
      "loss: 0.696070  [24000/175341]\n",
      "loss: 0.206314  [25600/175341]\n",
      "loss: 0.322800  [27200/175341]\n",
      "loss: 0.618193  [28800/175341]\n",
      "loss: 0.507697  [30400/175341]\n",
      "loss: 0.263183  [32000/175341]\n",
      "loss: 0.783941  [33600/175341]\n",
      "loss: 0.740937  [35200/175341]\n",
      "loss: 0.562618  [36800/175341]\n",
      "loss: 0.340593  [38400/175341]\n",
      "loss: 0.108644  [40000/175341]\n",
      "loss: 0.565207  [41600/175341]\n",
      "loss: 0.501475  [43200/175341]\n",
      "loss: 0.526964  [44800/175341]\n",
      "loss: 0.348559  [46400/175341]\n",
      "loss: 0.522512  [48000/175341]\n",
      "loss: 0.493609  [49600/175341]\n",
      "loss: 0.279873  [51200/175341]\n",
      "loss: 0.635796  [52800/175341]\n",
      "loss: 0.244443  [54400/175341]\n",
      "loss: 0.676601  [56000/175341]\n",
      "loss: 0.592276  [57600/175341]\n",
      "loss: 0.438089  [59200/175341]\n",
      "loss: 0.353437  [60800/175341]\n",
      "loss: 0.500346  [62400/175341]\n",
      "loss: 0.221059  [64000/175341]\n",
      "loss: 0.400090  [65600/175341]\n",
      "loss: 0.340515  [67200/175341]\n",
      "loss: 0.316287  [68800/175341]\n",
      "loss: 0.661170  [70400/175341]\n",
      "loss: 0.683436  [72000/175341]\n",
      "loss: 0.608208  [73600/175341]\n",
      "loss: 0.406738  [75200/175341]\n",
      "loss: 0.229049  [76800/175341]\n",
      "loss: 0.407198  [78400/175341]\n",
      "loss: 0.247919  [80000/175341]\n",
      "loss: 0.350757  [81600/175341]\n",
      "loss: 0.057482  [83200/175341]\n",
      "loss: 0.431149  [84800/175341]\n",
      "loss: 0.580126  [86400/175341]\n",
      "loss: 0.507950  [88000/175341]\n",
      "loss: 0.225346  [89600/175341]\n",
      "loss: 0.436522  [91200/175341]\n",
      "loss: 0.364647  [92800/175341]\n",
      "loss: 0.387712  [94400/175341]\n",
      "loss: 0.818083  [96000/175341]\n",
      "loss: 0.413710  [97600/175341]\n",
      "loss: 0.304385  [99200/175341]\n",
      "loss: 0.348651  [100800/175341]\n",
      "loss: 0.772447  [102400/175341]\n",
      "loss: 0.905572  [104000/175341]\n",
      "loss: 0.544012  [105600/175341]\n",
      "loss: 0.503254  [107200/175341]\n",
      "loss: 0.582474  [108800/175341]\n",
      "loss: 0.456219  [110400/175341]\n",
      "loss: 0.743786  [112000/175341]\n",
      "loss: 0.286086  [113600/175341]\n",
      "loss: 0.440003  [115200/175341]\n",
      "loss: 0.283000  [116800/175341]\n",
      "loss: 0.653121  [118400/175341]\n",
      "loss: 0.754963  [120000/175341]\n",
      "loss: 0.493085  [121600/175341]\n",
      "loss: 0.523417  [123200/175341]\n",
      "loss: 0.956655  [124800/175341]\n",
      "loss: 0.186158  [126400/175341]\n",
      "loss: 0.936108  [128000/175341]\n",
      "loss: 0.373927  [129600/175341]\n",
      "loss: 0.322951  [131200/175341]\n",
      "loss: 0.183878  [132800/175341]\n",
      "loss: 0.449966  [134400/175341]\n",
      "loss: 0.147469  [136000/175341]\n",
      "loss: 0.714088  [137600/175341]\n",
      "loss: 0.488194  [139200/175341]\n",
      "loss: 0.599111  [140800/175341]\n",
      "loss: 0.727120  [142400/175341]\n",
      "loss: 0.311449  [144000/175341]\n",
      "loss: 0.538578  [145600/175341]\n",
      "loss: 0.420693  [147200/175341]\n",
      "loss: 1.104726  [148800/175341]\n",
      "loss: 0.521058  [150400/175341]\n",
      "loss: 0.464989  [152000/175341]\n",
      "loss: 0.668595  [153600/175341]\n",
      "loss: 0.342239  [155200/175341]\n",
      "loss: 0.341393  [156800/175341]\n",
      "loss: 0.934976  [158400/175341]\n",
      "loss: 0.330125  [160000/175341]\n",
      "loss: 0.632358  [161600/175341]\n",
      "loss: 0.224306  [163200/175341]\n",
      "loss: 0.847959  [164800/175341]\n",
      "loss: 0.608321  [166400/175341]\n",
      "loss: 0.612332  [168000/175341]\n",
      "loss: 0.465596  [169600/175341]\n",
      "loss: 0.288845  [171200/175341]\n",
      "loss: 0.689123  [172800/175341]\n",
      "loss: 0.495719  [174400/175341]\n",
      "Train Accuracy: 79.9317%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.587076, F1-score: 74.59%, Macro_F1-Score:  38.04%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.563046  [    0/175341]\n",
      "loss: 0.497782  [ 1600/175341]\n",
      "loss: 0.946073  [ 3200/175341]\n",
      "loss: 0.785811  [ 4800/175341]\n",
      "loss: 0.419501  [ 6400/175341]\n",
      "loss: 0.472310  [ 8000/175341]\n",
      "loss: 0.325101  [ 9600/175341]\n",
      "loss: 0.314208  [11200/175341]\n",
      "loss: 0.419394  [12800/175341]\n",
      "loss: 0.483610  [14400/175341]\n",
      "loss: 0.405385  [16000/175341]\n",
      "loss: 0.181169  [17600/175341]\n",
      "loss: 0.418236  [19200/175341]\n",
      "loss: 0.742063  [20800/175341]\n",
      "loss: 0.600751  [22400/175341]\n",
      "loss: 0.235254  [24000/175341]\n",
      "loss: 0.348560  [25600/175341]\n",
      "loss: 0.529591  [27200/175341]\n",
      "loss: 0.316410  [28800/175341]\n",
      "loss: 0.588609  [30400/175341]\n",
      "loss: 0.734331  [32000/175341]\n",
      "loss: 0.519833  [33600/175341]\n",
      "loss: 0.281887  [35200/175341]\n",
      "loss: 0.767410  [36800/175341]\n",
      "loss: 0.730312  [38400/175341]\n",
      "loss: 0.499906  [40000/175341]\n",
      "loss: 0.296570  [41600/175341]\n",
      "loss: 0.650038  [43200/175341]\n",
      "loss: 0.305291  [44800/175341]\n",
      "loss: 0.637298  [46400/175341]\n",
      "loss: 0.570186  [48000/175341]\n",
      "loss: 0.746951  [49600/175341]\n",
      "loss: 0.526423  [51200/175341]\n",
      "loss: 0.580434  [52800/175341]\n",
      "loss: 0.512068  [54400/175341]\n",
      "loss: 0.337856  [56000/175341]\n",
      "loss: 0.487157  [57600/175341]\n",
      "loss: 0.477266  [59200/175341]\n",
      "loss: 0.492136  [60800/175341]\n",
      "loss: 0.140684  [62400/175341]\n",
      "loss: 0.456153  [64000/175341]\n",
      "loss: 0.890487  [65600/175341]\n",
      "loss: 0.779511  [67200/175341]\n",
      "loss: 0.506195  [68800/175341]\n",
      "loss: 0.556479  [70400/175341]\n",
      "loss: 0.104782  [72000/175341]\n",
      "loss: 0.241244  [73600/175341]\n",
      "loss: 0.390758  [75200/175341]\n",
      "loss: 0.600402  [76800/175341]\n",
      "loss: 0.412952  [78400/175341]\n",
      "loss: 0.539305  [80000/175341]\n",
      "loss: 0.123353  [81600/175341]\n",
      "loss: 0.667975  [83200/175341]\n",
      "loss: 0.566186  [84800/175341]\n",
      "loss: 0.381609  [86400/175341]\n",
      "loss: 0.464896  [88000/175341]\n",
      "loss: 0.704942  [89600/175341]\n",
      "loss: 0.352354  [91200/175341]\n",
      "loss: 0.338744  [92800/175341]\n",
      "loss: 0.480205  [94400/175341]\n",
      "loss: 0.666932  [96000/175341]\n",
      "loss: 0.684287  [97600/175341]\n",
      "loss: 0.384719  [99200/175341]\n",
      "loss: 0.218717  [100800/175341]\n",
      "loss: 0.269771  [102400/175341]\n",
      "loss: 0.238026  [104000/175341]\n",
      "loss: 0.453474  [105600/175341]\n",
      "loss: 0.510932  [107200/175341]\n",
      "loss: 0.175919  [108800/175341]\n",
      "loss: 0.562278  [110400/175341]\n",
      "loss: 0.590745  [112000/175341]\n",
      "loss: 0.414855  [113600/175341]\n",
      "loss: 0.230410  [115200/175341]\n",
      "loss: 0.803075  [116800/175341]\n",
      "loss: 0.202613  [118400/175341]\n",
      "loss: 0.645315  [120000/175341]\n",
      "loss: 0.141899  [121600/175341]\n",
      "loss: 0.908150  [123200/175341]\n",
      "loss: 0.337988  [124800/175341]\n",
      "loss: 0.430437  [126400/175341]\n",
      "loss: 0.740778  [128000/175341]\n",
      "loss: 0.675020  [129600/175341]\n",
      "loss: 0.291796  [131200/175341]\n",
      "loss: 0.840485  [132800/175341]\n",
      "loss: 0.182815  [134400/175341]\n",
      "loss: 0.297084  [136000/175341]\n",
      "loss: 0.345700  [137600/175341]\n",
      "loss: 0.605496  [139200/175341]\n",
      "loss: 0.433462  [140800/175341]\n",
      "loss: 0.225574  [142400/175341]\n",
      "loss: 0.655437  [144000/175341]\n",
      "loss: 0.744226  [145600/175341]\n",
      "loss: 0.396797  [147200/175341]\n",
      "loss: 0.393492  [148800/175341]\n",
      "loss: 0.433346  [150400/175341]\n",
      "loss: 0.515673  [152000/175341]\n",
      "loss: 0.491579  [153600/175341]\n",
      "loss: 0.999505  [155200/175341]\n",
      "loss: 0.739926  [156800/175341]\n",
      "loss: 0.545536  [158400/175341]\n",
      "loss: 0.368340  [160000/175341]\n",
      "loss: 0.668494  [161600/175341]\n",
      "loss: 0.223223  [163200/175341]\n",
      "loss: 0.228089  [164800/175341]\n",
      "loss: 0.441175  [166400/175341]\n",
      "loss: 0.423429  [168000/175341]\n",
      "loss: 0.273802  [169600/175341]\n",
      "loss: 0.541317  [171200/175341]\n",
      "loss: 0.472353  [172800/175341]\n",
      "loss: 0.663132  [174400/175341]\n",
      "Train Accuracy: 79.9625%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.576755, F1-score: 74.93%, Macro_F1-Score:  38.91%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.739555  [    0/175341]\n",
      "loss: 0.626768  [ 1600/175341]\n",
      "loss: 0.478447  [ 3200/175341]\n",
      "loss: 0.469472  [ 4800/175341]\n",
      "loss: 0.300408  [ 6400/175341]\n",
      "loss: 0.586681  [ 8000/175341]\n",
      "loss: 0.481851  [ 9600/175341]\n",
      "loss: 0.138356  [11200/175341]\n",
      "loss: 0.509080  [12800/175341]\n",
      "loss: 0.754908  [14400/175341]\n",
      "loss: 0.462789  [16000/175341]\n",
      "loss: 0.449928  [17600/175341]\n",
      "loss: 0.799488  [19200/175341]\n",
      "loss: 0.409343  [20800/175341]\n",
      "loss: 0.362467  [22400/175341]\n",
      "loss: 0.014813  [24000/175341]\n",
      "loss: 0.446099  [25600/175341]\n",
      "loss: 0.192493  [27200/175341]\n",
      "loss: 0.471092  [28800/175341]\n",
      "loss: 0.563669  [30400/175341]\n",
      "loss: 0.431837  [32000/175341]\n",
      "loss: 0.555893  [33600/175341]\n",
      "loss: 0.263111  [35200/175341]\n",
      "loss: 0.352832  [36800/175341]\n",
      "loss: 0.363199  [38400/175341]\n",
      "loss: 0.486004  [40000/175341]\n",
      "loss: 0.733946  [41600/175341]\n",
      "loss: 0.191503  [43200/175341]\n",
      "loss: 0.227569  [44800/175341]\n",
      "loss: 0.564337  [46400/175341]\n",
      "loss: 0.525279  [48000/175341]\n",
      "loss: 0.423964  [49600/175341]\n",
      "loss: 0.407531  [51200/175341]\n",
      "loss: 0.841455  [52800/175341]\n",
      "loss: 0.603183  [54400/175341]\n",
      "loss: 0.279581  [56000/175341]\n",
      "loss: 0.216633  [57600/175341]\n",
      "loss: 0.477623  [59200/175341]\n",
      "loss: 0.576761  [60800/175341]\n",
      "loss: 0.526050  [62400/175341]\n",
      "loss: 0.448399  [64000/175341]\n",
      "loss: 0.370929  [65600/175341]\n",
      "loss: 0.710200  [67200/175341]\n",
      "loss: 0.437016  [68800/175341]\n",
      "loss: 0.331550  [70400/175341]\n",
      "loss: 0.492004  [72000/175341]\n",
      "loss: 0.652465  [73600/175341]\n",
      "loss: 0.542589  [75200/175341]\n",
      "loss: 0.182019  [76800/175341]\n",
      "loss: 0.629864  [78400/175341]\n",
      "loss: 0.501052  [80000/175341]\n",
      "loss: 0.849781  [81600/175341]\n",
      "loss: 0.569543  [83200/175341]\n",
      "loss: 0.595934  [84800/175341]\n",
      "loss: 0.865427  [86400/175341]\n",
      "loss: 0.469473  [88000/175341]\n",
      "loss: 0.587253  [89600/175341]\n",
      "loss: 0.531956  [91200/175341]\n",
      "loss: 0.564858  [92800/175341]\n",
      "loss: 0.735687  [94400/175341]\n",
      "loss: 0.195949  [96000/175341]\n",
      "loss: 0.844608  [97600/175341]\n",
      "loss: 0.417731  [99200/175341]\n",
      "loss: 0.802112  [100800/175341]\n",
      "loss: 0.244558  [102400/175341]\n",
      "loss: 0.421122  [104000/175341]\n",
      "loss: 0.447377  [105600/175341]\n",
      "loss: 0.356215  [107200/175341]\n",
      "loss: 0.555258  [108800/175341]\n",
      "loss: 0.399588  [110400/175341]\n",
      "loss: 0.437985  [112000/175341]\n",
      "loss: 0.377016  [113600/175341]\n",
      "loss: 0.281726  [115200/175341]\n",
      "loss: 0.282255  [116800/175341]\n",
      "loss: 0.582492  [118400/175341]\n",
      "loss: 0.686844  [120000/175341]\n",
      "loss: 0.398959  [121600/175341]\n",
      "loss: 0.931948  [123200/175341]\n",
      "loss: 0.653069  [124800/175341]\n",
      "loss: 0.265997  [126400/175341]\n",
      "loss: 0.476667  [128000/175341]\n",
      "loss: 0.172912  [129600/175341]\n",
      "loss: 0.895280  [131200/175341]\n",
      "loss: 0.418054  [132800/175341]\n",
      "loss: 0.449399  [134400/175341]\n",
      "loss: 0.395701  [136000/175341]\n",
      "loss: 0.407119  [137600/175341]\n",
      "loss: 0.659969  [139200/175341]\n",
      "loss: 0.194088  [140800/175341]\n",
      "loss: 0.243240  [142400/175341]\n",
      "loss: 0.416347  [144000/175341]\n",
      "loss: 0.238852  [145600/175341]\n",
      "loss: 0.594584  [147200/175341]\n",
      "loss: 0.282454  [148800/175341]\n",
      "loss: 0.311921  [150400/175341]\n",
      "loss: 0.503140  [152000/175341]\n",
      "loss: 0.442525  [153600/175341]\n",
      "loss: 0.434833  [155200/175341]\n",
      "loss: 0.752857  [156800/175341]\n",
      "loss: 0.255633  [158400/175341]\n",
      "loss: 0.582052  [160000/175341]\n",
      "loss: 0.256327  [161600/175341]\n",
      "loss: 0.832951  [163200/175341]\n",
      "loss: 0.364075  [164800/175341]\n",
      "loss: 0.757508  [166400/175341]\n",
      "loss: 0.498149  [168000/175341]\n",
      "loss: 0.428849  [169600/175341]\n",
      "loss: 0.344591  [171200/175341]\n",
      "loss: 0.436533  [172800/175341]\n",
      "loss: 0.343093  [174400/175341]\n",
      "Train Accuracy: 79.9819%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.575252, F1-score: 74.89%, Macro_F1-Score:  38.99%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.400095  [    0/175341]\n",
      "loss: 0.696486  [ 1600/175341]\n",
      "loss: 0.587693  [ 3200/175341]\n",
      "loss: 0.403145  [ 4800/175341]\n",
      "loss: 0.783593  [ 6400/175341]\n",
      "loss: 0.153766  [ 8000/175341]\n",
      "loss: 0.561968  [ 9600/175341]\n",
      "loss: 0.322513  [11200/175341]\n",
      "loss: 0.478693  [12800/175341]\n",
      "loss: 0.179701  [14400/175341]\n",
      "loss: 0.426420  [16000/175341]\n",
      "loss: 0.293498  [17600/175341]\n",
      "loss: 0.732641  [19200/175341]\n",
      "loss: 0.458036  [20800/175341]\n",
      "loss: 0.894093  [22400/175341]\n",
      "loss: 0.259243  [24000/175341]\n",
      "loss: 0.650210  [25600/175341]\n",
      "loss: 0.204352  [27200/175341]\n",
      "loss: 0.680787  [28800/175341]\n",
      "loss: 0.703716  [30400/175341]\n",
      "loss: 0.126775  [32000/175341]\n",
      "loss: 0.725731  [33600/175341]\n",
      "loss: 0.226258  [35200/175341]\n",
      "loss: 0.892898  [36800/175341]\n",
      "loss: 0.519916  [38400/175341]\n",
      "loss: 0.257191  [40000/175341]\n",
      "loss: 0.703817  [41600/175341]\n",
      "loss: 0.221075  [43200/175341]\n",
      "loss: 0.517377  [44800/175341]\n",
      "loss: 0.289032  [46400/175341]\n",
      "loss: 0.588670  [48000/175341]\n",
      "loss: 0.525939  [49600/175341]\n",
      "loss: 0.437567  [51200/175341]\n",
      "loss: 0.522154  [52800/175341]\n",
      "loss: 0.545547  [54400/175341]\n",
      "loss: 0.536249  [56000/175341]\n",
      "loss: 0.390016  [57600/175341]\n",
      "loss: 0.293112  [59200/175341]\n",
      "loss: 0.810339  [60800/175341]\n",
      "loss: 0.441214  [62400/175341]\n",
      "loss: 0.358739  [64000/175341]\n",
      "loss: 0.271397  [65600/175341]\n",
      "loss: 0.171102  [67200/175341]\n",
      "loss: 0.331146  [68800/175341]\n",
      "loss: 0.315192  [70400/175341]\n",
      "loss: 0.896049  [72000/175341]\n",
      "loss: 0.720485  [73600/175341]\n",
      "loss: 0.296747  [75200/175341]\n",
      "loss: 0.340642  [76800/175341]\n",
      "loss: 0.351667  [78400/175341]\n",
      "loss: 0.393801  [80000/175341]\n",
      "loss: 0.689620  [81600/175341]\n",
      "loss: 0.154956  [83200/175341]\n",
      "loss: 0.646605  [84800/175341]\n",
      "loss: 0.888507  [86400/175341]\n",
      "loss: 0.491465  [88000/175341]\n",
      "loss: 0.141446  [89600/175341]\n",
      "loss: 0.464199  [91200/175341]\n",
      "loss: 0.311282  [92800/175341]\n",
      "loss: 0.365620  [94400/175341]\n",
      "loss: 0.872940  [96000/175341]\n",
      "loss: 0.637196  [97600/175341]\n",
      "loss: 0.636634  [99200/175341]\n",
      "loss: 0.305982  [100800/175341]\n",
      "loss: 0.765555  [102400/175341]\n",
      "loss: 0.802967  [104000/175341]\n",
      "loss: 0.353434  [105600/175341]\n",
      "loss: 0.792783  [107200/175341]\n",
      "loss: 0.287183  [108800/175341]\n",
      "loss: 0.620352  [110400/175341]\n",
      "loss: 0.561793  [112000/175341]\n",
      "loss: 0.699692  [113600/175341]\n",
      "loss: 0.229997  [115200/175341]\n",
      "loss: 0.824513  [116800/175341]\n",
      "loss: 0.454834  [118400/175341]\n",
      "loss: 0.540907  [120000/175341]\n",
      "loss: 0.864780  [121600/175341]\n",
      "loss: 0.829836  [123200/175341]\n",
      "loss: 0.575085  [124800/175341]\n",
      "loss: 0.278373  [126400/175341]\n",
      "loss: 0.474655  [128000/175341]\n",
      "loss: 0.733025  [129600/175341]\n",
      "loss: 0.668373  [131200/175341]\n",
      "loss: 0.445888  [132800/175341]\n",
      "loss: 0.423579  [134400/175341]\n",
      "loss: 0.243294  [136000/175341]\n",
      "loss: 0.405686  [137600/175341]\n",
      "loss: 0.788180  [139200/175341]\n",
      "loss: 0.640948  [140800/175341]\n",
      "loss: 0.786126  [142400/175341]\n",
      "loss: 0.641223  [144000/175341]\n",
      "loss: 0.652858  [145600/175341]\n",
      "loss: 0.694071  [147200/175341]\n",
      "loss: 0.526972  [148800/175341]\n",
      "loss: 0.500861  [150400/175341]\n",
      "loss: 0.508458  [152000/175341]\n",
      "loss: 0.840805  [153600/175341]\n",
      "loss: 0.462383  [155200/175341]\n",
      "loss: 0.609897  [156800/175341]\n",
      "loss: 0.630640  [158400/175341]\n",
      "loss: 0.708424  [160000/175341]\n",
      "loss: 0.443066  [161600/175341]\n",
      "loss: 0.340915  [163200/175341]\n",
      "loss: 0.523350  [164800/175341]\n",
      "loss: 0.836061  [166400/175341]\n",
      "loss: 0.216275  [168000/175341]\n",
      "loss: 0.171892  [169600/175341]\n",
      "loss: 0.320819  [171200/175341]\n",
      "loss: 0.568908  [172800/175341]\n",
      "loss: 0.849560  [174400/175341]\n",
      "Train Accuracy: 80.0195%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.574670, F1-score: 75.09%, Macro_F1-Score:  38.75%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.745505  [    0/175341]\n",
      "loss: 0.852912  [ 1600/175341]\n",
      "loss: 0.451904  [ 3200/175341]\n",
      "loss: 0.502077  [ 4800/175341]\n",
      "loss: 0.632196  [ 6400/175341]\n",
      "loss: 0.298348  [ 8000/175341]\n",
      "loss: 0.384443  [ 9600/175341]\n",
      "loss: 0.379243  [11200/175341]\n",
      "loss: 0.782945  [12800/175341]\n",
      "loss: 0.321717  [14400/175341]\n",
      "loss: 0.447072  [16000/175341]\n",
      "loss: 0.946223  [17600/175341]\n",
      "loss: 0.646693  [19200/175341]\n",
      "loss: 0.491404  [20800/175341]\n",
      "loss: 0.997737  [22400/175341]\n",
      "loss: 0.401251  [24000/175341]\n",
      "loss: 0.628460  [25600/175341]\n",
      "loss: 0.648376  [27200/175341]\n",
      "loss: 0.611770  [28800/175341]\n",
      "loss: 0.703121  [30400/175341]\n",
      "loss: 0.544744  [32000/175341]\n",
      "loss: 0.498389  [33600/175341]\n",
      "loss: 0.343619  [35200/175341]\n",
      "loss: 0.318139  [36800/175341]\n",
      "loss: 0.440435  [38400/175341]\n",
      "loss: 0.274954  [40000/175341]\n",
      "loss: 0.300826  [41600/175341]\n",
      "loss: 0.637542  [43200/175341]\n",
      "loss: 1.187795  [44800/175341]\n",
      "loss: 0.280763  [46400/175341]\n",
      "loss: 0.670818  [48000/175341]\n",
      "loss: 0.553793  [49600/175341]\n",
      "loss: 0.622103  [51200/175341]\n",
      "loss: 0.473352  [52800/175341]\n",
      "loss: 0.427916  [54400/175341]\n",
      "loss: 0.441002  [56000/175341]\n",
      "loss: 0.887146  [57600/175341]\n",
      "loss: 0.702336  [59200/175341]\n",
      "loss: 0.753420  [60800/175341]\n",
      "loss: 0.348035  [62400/175341]\n",
      "loss: 0.867988  [64000/175341]\n",
      "loss: 0.718164  [65600/175341]\n",
      "loss: 0.504649  [67200/175341]\n",
      "loss: 1.442140  [68800/175341]\n",
      "loss: 0.257063  [70400/175341]\n",
      "loss: 0.397606  [72000/175341]\n",
      "loss: 0.449504  [73600/175341]\n",
      "loss: 0.345362  [75200/175341]\n",
      "loss: 0.450828  [76800/175341]\n",
      "loss: 0.182444  [78400/175341]\n",
      "loss: 0.613052  [80000/175341]\n",
      "loss: 0.384181  [81600/175341]\n",
      "loss: 0.688903  [83200/175341]\n",
      "loss: 0.241667  [84800/175341]\n",
      "loss: 0.628480  [86400/175341]\n",
      "loss: 0.614414  [88000/175341]\n",
      "loss: 0.324544  [89600/175341]\n",
      "loss: 0.319508  [91200/175341]\n",
      "loss: 0.387287  [92800/175341]\n",
      "loss: 0.593568  [94400/175341]\n",
      "loss: 0.758203  [96000/175341]\n",
      "loss: 0.359350  [97600/175341]\n",
      "loss: 0.706370  [99200/175341]\n",
      "loss: 1.069019  [100800/175341]\n",
      "loss: 0.919362  [102400/175341]\n",
      "loss: 0.462126  [104000/175341]\n",
      "loss: 0.956442  [105600/175341]\n",
      "loss: 0.197009  [107200/175341]\n",
      "loss: 0.806491  [108800/175341]\n",
      "loss: 0.251321  [110400/175341]\n",
      "loss: 0.356541  [112000/175341]\n",
      "loss: 0.833504  [113600/175341]\n",
      "loss: 0.556302  [115200/175341]\n",
      "loss: 0.469363  [116800/175341]\n",
      "loss: 0.910478  [118400/175341]\n",
      "loss: 0.316568  [120000/175341]\n",
      "loss: 0.586530  [121600/175341]\n",
      "loss: 0.242664  [123200/175341]\n",
      "loss: 0.526487  [124800/175341]\n",
      "loss: 1.031552  [126400/175341]\n",
      "loss: 0.303558  [128000/175341]\n",
      "loss: 0.353311  [129600/175341]\n",
      "loss: 0.435502  [131200/175341]\n",
      "loss: 0.532300  [132800/175341]\n",
      "loss: 0.479302  [134400/175341]\n",
      "loss: 0.338653  [136000/175341]\n",
      "loss: 0.333809  [137600/175341]\n",
      "loss: 0.418705  [139200/175341]\n",
      "loss: 0.499000  [140800/175341]\n",
      "loss: 0.398545  [142400/175341]\n",
      "loss: 0.446342  [144000/175341]\n",
      "loss: 0.476042  [145600/175341]\n",
      "loss: 0.842825  [147200/175341]\n",
      "loss: 0.181159  [148800/175341]\n",
      "loss: 0.320269  [150400/175341]\n",
      "loss: 0.595807  [152000/175341]\n",
      "loss: 1.148167  [153600/175341]\n",
      "loss: 0.685282  [155200/175341]\n",
      "loss: 0.491143  [156800/175341]\n",
      "loss: 0.415415  [158400/175341]\n",
      "loss: 0.396739  [160000/175341]\n",
      "loss: 0.641667  [161600/175341]\n",
      "loss: 0.616291  [163200/175341]\n",
      "loss: 0.218192  [164800/175341]\n",
      "loss: 0.498702  [166400/175341]\n",
      "loss: 0.554102  [168000/175341]\n",
      "loss: 0.432281  [169600/175341]\n",
      "loss: 0.170468  [171200/175341]\n",
      "loss: 0.364050  [172800/175341]\n",
      "loss: 0.598926  [174400/175341]\n",
      "Train Accuracy: 79.9830%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.565763, F1-score: 75.89%, Macro_F1-Score:  39.58%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.471889  [    0/175341]\n",
      "loss: 0.438911  [ 1600/175341]\n",
      "loss: 0.604075  [ 3200/175341]\n",
      "loss: 0.548014  [ 4800/175341]\n",
      "loss: 0.344833  [ 6400/175341]\n",
      "loss: 0.680420  [ 8000/175341]\n",
      "loss: 0.183403  [ 9600/175341]\n",
      "loss: 0.777708  [11200/175341]\n",
      "loss: 0.411315  [12800/175341]\n",
      "loss: 0.343215  [14400/175341]\n",
      "loss: 0.378452  [16000/175341]\n",
      "loss: 0.218682  [17600/175341]\n",
      "loss: 0.216513  [19200/175341]\n",
      "loss: 0.407745  [20800/175341]\n",
      "loss: 0.321166  [22400/175341]\n",
      "loss: 0.727795  [24000/175341]\n",
      "loss: 0.857012  [25600/175341]\n",
      "loss: 0.532262  [27200/175341]\n",
      "loss: 0.156904  [28800/175341]\n",
      "loss: 0.430807  [30400/175341]\n",
      "loss: 0.385008  [32000/175341]\n",
      "loss: 0.759369  [33600/175341]\n",
      "loss: 0.426290  [35200/175341]\n",
      "loss: 0.345675  [36800/175341]\n",
      "loss: 0.287658  [38400/175341]\n",
      "loss: 0.584384  [40000/175341]\n",
      "loss: 0.458037  [41600/175341]\n",
      "loss: 0.696248  [43200/175341]\n",
      "loss: 0.642364  [44800/175341]\n",
      "loss: 0.426243  [46400/175341]\n",
      "loss: 0.540898  [48000/175341]\n",
      "loss: 0.463838  [49600/175341]\n",
      "loss: 0.662080  [51200/175341]\n",
      "loss: 0.541068  [52800/175341]\n",
      "loss: 0.609831  [54400/175341]\n",
      "loss: 0.424187  [56000/175341]\n",
      "loss: 0.421579  [57600/175341]\n",
      "loss: 0.988609  [59200/175341]\n",
      "loss: 0.541184  [60800/175341]\n",
      "loss: 0.539021  [62400/175341]\n",
      "loss: 0.362657  [64000/175341]\n",
      "loss: 0.400819  [65600/175341]\n",
      "loss: 0.854579  [67200/175341]\n",
      "loss: 0.558254  [68800/175341]\n",
      "loss: 0.842499  [70400/175341]\n",
      "loss: 0.506876  [72000/175341]\n",
      "loss: 0.825677  [73600/175341]\n",
      "loss: 0.580576  [75200/175341]\n",
      "loss: 0.410555  [76800/175341]\n",
      "loss: 0.181249  [78400/175341]\n",
      "loss: 0.496221  [80000/175341]\n",
      "loss: 0.705545  [81600/175341]\n",
      "loss: 0.421268  [83200/175341]\n",
      "loss: 0.689596  [84800/175341]\n",
      "loss: 0.356391  [86400/175341]\n",
      "loss: 0.317058  [88000/175341]\n",
      "loss: 0.462051  [89600/175341]\n",
      "loss: 0.814151  [91200/175341]\n",
      "loss: 0.324554  [92800/175341]\n",
      "loss: 0.312779  [94400/175341]\n",
      "loss: 0.644006  [96000/175341]\n",
      "loss: 0.245684  [97600/175341]\n",
      "loss: 0.418598  [99200/175341]\n",
      "loss: 0.176974  [100800/175341]\n",
      "loss: 0.435251  [102400/175341]\n",
      "loss: 0.193171  [104000/175341]\n",
      "loss: 0.554999  [105600/175341]\n",
      "loss: 0.420843  [107200/175341]\n",
      "loss: 0.465874  [108800/175341]\n",
      "loss: 0.521195  [110400/175341]\n",
      "loss: 0.373286  [112000/175341]\n",
      "loss: 0.237287  [113600/175341]\n",
      "loss: 0.687965  [115200/175341]\n",
      "loss: 0.848156  [116800/175341]\n",
      "loss: 0.715614  [118400/175341]\n",
      "loss: 0.807568  [120000/175341]\n",
      "loss: 0.677806  [121600/175341]\n",
      "loss: 0.211123  [123200/175341]\n",
      "loss: 0.202608  [124800/175341]\n",
      "loss: 0.610942  [126400/175341]\n",
      "loss: 0.297616  [128000/175341]\n",
      "loss: 0.543087  [129600/175341]\n",
      "loss: 0.415122  [131200/175341]\n",
      "loss: 1.078089  [132800/175341]\n",
      "loss: 0.856976  [134400/175341]\n",
      "loss: 0.575516  [136000/175341]\n",
      "loss: 1.012306  [137600/175341]\n",
      "loss: 0.641620  [139200/175341]\n",
      "loss: 0.554698  [140800/175341]\n",
      "loss: 0.780582  [142400/175341]\n",
      "loss: 0.467462  [144000/175341]\n",
      "loss: 0.585690  [145600/175341]\n",
      "loss: 0.652854  [147200/175341]\n",
      "loss: 0.604837  [148800/175341]\n",
      "loss: 0.730040  [150400/175341]\n",
      "loss: 0.627592  [152000/175341]\n",
      "loss: 0.188412  [153600/175341]\n",
      "loss: 1.105965  [155200/175341]\n",
      "loss: 0.577421  [156800/175341]\n",
      "loss: 0.603958  [158400/175341]\n",
      "loss: 0.697838  [160000/175341]\n",
      "loss: 0.235764  [161600/175341]\n",
      "loss: 0.284291  [163200/175341]\n",
      "loss: 0.464081  [164800/175341]\n",
      "loss: 0.211674  [166400/175341]\n",
      "loss: 0.437268  [168000/175341]\n",
      "loss: 0.495001  [169600/175341]\n",
      "loss: 0.918966  [171200/175341]\n",
      "loss: 0.271165  [172800/175341]\n",
      "loss: 0.653567  [174400/175341]\n",
      "Train Accuracy: 79.9950%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.581979, F1-score: 75.21%, Macro_F1-Score:  38.24%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.452796  [    0/175341]\n",
      "loss: 0.441420  [ 1600/175341]\n",
      "loss: 0.507824  [ 3200/175341]\n",
      "loss: 0.216144  [ 4800/175341]\n",
      "loss: 0.487999  [ 6400/175341]\n",
      "loss: 0.505733  [ 8000/175341]\n",
      "loss: 0.547488  [ 9600/175341]\n",
      "loss: 0.362807  [11200/175341]\n",
      "loss: 0.270148  [12800/175341]\n",
      "loss: 0.329901  [14400/175341]\n",
      "loss: 0.678900  [16000/175341]\n",
      "loss: 0.310767  [17600/175341]\n",
      "loss: 0.876382  [19200/175341]\n",
      "loss: 0.218495  [20800/175341]\n",
      "loss: 0.446020  [22400/175341]\n",
      "loss: 0.866313  [24000/175341]\n",
      "loss: 0.422444  [25600/175341]\n",
      "loss: 0.510456  [27200/175341]\n",
      "loss: 0.590787  [28800/175341]\n",
      "loss: 0.582505  [30400/175341]\n",
      "loss: 0.594018  [32000/175341]\n",
      "loss: 0.292837  [33600/175341]\n",
      "loss: 0.373191  [35200/175341]\n",
      "loss: 0.462202  [36800/175341]\n",
      "loss: 0.291063  [38400/175341]\n",
      "loss: 0.213193  [40000/175341]\n",
      "loss: 0.556724  [41600/175341]\n",
      "loss: 0.432598  [43200/175341]\n",
      "loss: 0.314004  [44800/175341]\n",
      "loss: 0.581770  [46400/175341]\n",
      "loss: 0.577243  [48000/175341]\n",
      "loss: 0.582245  [49600/175341]\n",
      "loss: 0.411215  [51200/175341]\n",
      "loss: 0.505948  [52800/175341]\n",
      "loss: 0.310020  [54400/175341]\n",
      "loss: 0.558245  [56000/175341]\n",
      "loss: 0.296382  [57600/175341]\n",
      "loss: 1.030185  [59200/175341]\n",
      "loss: 0.800631  [60800/175341]\n",
      "loss: 0.457218  [62400/175341]\n",
      "loss: 0.539600  [64000/175341]\n",
      "loss: 0.329318  [65600/175341]\n",
      "loss: 0.615882  [67200/175341]\n",
      "loss: 0.452168  [68800/175341]\n",
      "loss: 0.466631  [70400/175341]\n",
      "loss: 0.121214  [72000/175341]\n",
      "loss: 0.112081  [73600/175341]\n",
      "loss: 0.750053  [75200/175341]\n",
      "loss: 0.247800  [76800/175341]\n",
      "loss: 0.640969  [78400/175341]\n",
      "loss: 0.345361  [80000/175341]\n",
      "loss: 0.590226  [81600/175341]\n",
      "loss: 0.233571  [83200/175341]\n",
      "loss: 0.434505  [84800/175341]\n",
      "loss: 0.438249  [86400/175341]\n",
      "loss: 0.628585  [88000/175341]\n",
      "loss: 0.224716  [89600/175341]\n",
      "loss: 0.510403  [91200/175341]\n",
      "loss: 0.461537  [92800/175341]\n",
      "loss: 0.263072  [94400/175341]\n",
      "loss: 0.511808  [96000/175341]\n",
      "loss: 0.314337  [97600/175341]\n",
      "loss: 0.195903  [99200/175341]\n",
      "loss: 0.449380  [100800/175341]\n",
      "loss: 0.574758  [102400/175341]\n",
      "loss: 0.349038  [104000/175341]\n",
      "loss: 0.272978  [105600/175341]\n",
      "loss: 0.507932  [107200/175341]\n",
      "loss: 0.189814  [108800/175341]\n",
      "loss: 0.437007  [110400/175341]\n",
      "loss: 0.253374  [112000/175341]\n",
      "loss: 0.491404  [113600/175341]\n",
      "loss: 0.501749  [115200/175341]\n",
      "loss: 0.886120  [116800/175341]\n",
      "loss: 0.384998  [118400/175341]\n",
      "loss: 0.521261  [120000/175341]\n",
      "loss: 0.511350  [121600/175341]\n",
      "loss: 0.365825  [123200/175341]\n",
      "loss: 0.277077  [124800/175341]\n",
      "loss: 0.673086  [126400/175341]\n",
      "loss: 0.606243  [128000/175341]\n",
      "loss: 0.402913  [129600/175341]\n",
      "loss: 0.381400  [131200/175341]\n",
      "loss: 0.391352  [132800/175341]\n",
      "loss: 0.722647  [134400/175341]\n",
      "loss: 0.528174  [136000/175341]\n",
      "loss: 0.768972  [137600/175341]\n",
      "loss: 0.292247  [139200/175341]\n",
      "loss: 0.197315  [140800/175341]\n",
      "loss: 0.151144  [142400/175341]\n",
      "loss: 0.889743  [144000/175341]\n",
      "loss: 0.288170  [145600/175341]\n",
      "loss: 0.444706  [147200/175341]\n",
      "loss: 0.144372  [148800/175341]\n",
      "loss: 0.321037  [150400/175341]\n",
      "loss: 0.223607  [152000/175341]\n",
      "loss: 0.060986  [153600/175341]\n",
      "loss: 0.603285  [155200/175341]\n",
      "loss: 1.048426  [156800/175341]\n",
      "loss: 0.391499  [158400/175341]\n",
      "loss: 0.770577  [160000/175341]\n",
      "loss: 0.476093  [161600/175341]\n",
      "loss: 0.536987  [163200/175341]\n",
      "loss: 0.917297  [164800/175341]\n",
      "loss: 0.681892  [166400/175341]\n",
      "loss: 0.389914  [168000/175341]\n",
      "loss: 0.696952  [169600/175341]\n",
      "loss: 0.402388  [171200/175341]\n",
      "loss: 0.309299  [172800/175341]\n",
      "loss: 0.256932  [174400/175341]\n",
      "Train Accuracy: 80.0463%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.575151, F1-score: 75.35%, Macro_F1-Score:  39.19%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.703762  [    0/175341]\n",
      "loss: 0.519699  [ 1600/175341]\n",
      "loss: 0.323255  [ 3200/175341]\n",
      "loss: 0.914034  [ 4800/175341]\n",
      "loss: 0.343565  [ 6400/175341]\n",
      "loss: 0.483788  [ 8000/175341]\n",
      "loss: 0.321177  [ 9600/175341]\n",
      "loss: 0.303440  [11200/175341]\n",
      "loss: 0.533325  [12800/175341]\n",
      "loss: 0.350050  [14400/175341]\n",
      "loss: 0.151199  [16000/175341]\n",
      "loss: 0.468660  [17600/175341]\n",
      "loss: 0.197048  [19200/175341]\n",
      "loss: 0.368244  [20800/175341]\n",
      "loss: 0.613581  [22400/175341]\n",
      "loss: 0.311956  [24000/175341]\n",
      "loss: 0.303316  [25600/175341]\n",
      "loss: 0.751284  [27200/175341]\n",
      "loss: 0.307286  [28800/175341]\n",
      "loss: 0.941151  [30400/175341]\n",
      "loss: 0.434643  [32000/175341]\n",
      "loss: 1.181837  [33600/175341]\n",
      "loss: 0.507072  [35200/175341]\n",
      "loss: 0.281049  [36800/175341]\n",
      "loss: 0.606434  [38400/175341]\n",
      "loss: 0.140027  [40000/175341]\n",
      "loss: 1.104136  [41600/175341]\n",
      "loss: 0.991017  [43200/175341]\n",
      "loss: 0.192833  [44800/175341]\n",
      "loss: 0.678111  [46400/175341]\n",
      "loss: 0.500223  [48000/175341]\n",
      "loss: 0.810897  [49600/175341]\n",
      "loss: 0.412825  [51200/175341]\n",
      "loss: 0.176943  [52800/175341]\n",
      "loss: 0.565579  [54400/175341]\n",
      "loss: 0.158426  [56000/175341]\n",
      "loss: 0.726781  [57600/175341]\n",
      "loss: 0.469474  [59200/175341]\n",
      "loss: 0.412799  [60800/175341]\n",
      "loss: 0.729998  [62400/175341]\n",
      "loss: 0.290764  [64000/175341]\n",
      "loss: 0.345259  [65600/175341]\n",
      "loss: 0.308173  [67200/175341]\n",
      "loss: 0.937015  [68800/175341]\n",
      "loss: 0.473016  [70400/175341]\n",
      "loss: 0.303189  [72000/175341]\n",
      "loss: 0.392096  [73600/175341]\n",
      "loss: 0.387114  [75200/175341]\n",
      "loss: 0.580619  [76800/175341]\n",
      "loss: 0.845852  [78400/175341]\n",
      "loss: 0.443121  [80000/175341]\n",
      "loss: 0.281161  [81600/175341]\n",
      "loss: 0.615670  [83200/175341]\n",
      "loss: 0.330910  [84800/175341]\n",
      "loss: 0.562763  [86400/175341]\n",
      "loss: 0.370626  [88000/175341]\n",
      "loss: 0.419538  [89600/175341]\n",
      "loss: 0.900458  [91200/175341]\n",
      "loss: 0.650646  [92800/175341]\n",
      "loss: 0.411456  [94400/175341]\n",
      "loss: 0.827064  [96000/175341]\n",
      "loss: 0.380256  [97600/175341]\n",
      "loss: 0.381127  [99200/175341]\n",
      "loss: 0.289086  [100800/175341]\n",
      "loss: 0.514903  [102400/175341]\n",
      "loss: 0.476549  [104000/175341]\n",
      "loss: 0.468106  [105600/175341]\n",
      "loss: 0.175739  [107200/175341]\n",
      "loss: 0.686465  [108800/175341]\n",
      "loss: 0.804495  [110400/175341]\n",
      "loss: 0.460107  [112000/175341]\n",
      "loss: 0.343255  [113600/175341]\n",
      "loss: 0.390281  [115200/175341]\n",
      "loss: 1.259320  [116800/175341]\n",
      "loss: 0.383474  [118400/175341]\n",
      "loss: 0.709173  [120000/175341]\n",
      "loss: 0.251250  [121600/175341]\n",
      "loss: 0.543025  [123200/175341]\n",
      "loss: 0.303944  [124800/175341]\n",
      "loss: 0.590287  [126400/175341]\n",
      "loss: 0.256427  [128000/175341]\n",
      "loss: 0.737040  [129600/175341]\n",
      "loss: 0.380610  [131200/175341]\n",
      "loss: 0.161113  [132800/175341]\n",
      "loss: 0.400582  [134400/175341]\n",
      "loss: 0.582268  [136000/175341]\n",
      "loss: 0.625799  [137600/175341]\n",
      "loss: 0.414238  [139200/175341]\n",
      "loss: 0.342162  [140800/175341]\n",
      "loss: 0.380764  [142400/175341]\n",
      "loss: 0.267696  [144000/175341]\n",
      "loss: 0.850191  [145600/175341]\n",
      "loss: 0.575893  [147200/175341]\n",
      "loss: 1.002991  [148800/175341]\n",
      "loss: 0.429801  [150400/175341]\n",
      "loss: 0.486616  [152000/175341]\n",
      "loss: 0.603955  [153600/175341]\n",
      "loss: 0.395004  [155200/175341]\n",
      "loss: 0.427924  [156800/175341]\n",
      "loss: 0.345363  [158400/175341]\n",
      "loss: 0.531278  [160000/175341]\n",
      "loss: 0.353330  [161600/175341]\n",
      "loss: 0.536076  [163200/175341]\n",
      "loss: 0.448944  [164800/175341]\n",
      "loss: 0.697884  [166400/175341]\n",
      "loss: 0.642800  [168000/175341]\n",
      "loss: 0.204266  [169600/175341]\n",
      "loss: 0.496577  [171200/175341]\n",
      "loss: 0.458654  [172800/175341]\n",
      "loss: 0.435713  [174400/175341]\n",
      "Train Accuracy: 80.0338%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.578506, F1-score: 74.95%, Macro_F1-Score:  39.22%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.314793  [    0/175341]\n",
      "loss: 0.502218  [ 1600/175341]\n",
      "loss: 0.221809  [ 3200/175341]\n",
      "loss: 0.611785  [ 4800/175341]\n",
      "loss: 0.432623  [ 6400/175341]\n",
      "loss: 0.461441  [ 8000/175341]\n",
      "loss: 0.265949  [ 9600/175341]\n",
      "loss: 0.549658  [11200/175341]\n",
      "loss: 0.102095  [12800/175341]\n",
      "loss: 0.308106  [14400/175341]\n",
      "loss: 0.553198  [16000/175341]\n",
      "loss: 0.508609  [17600/175341]\n",
      "loss: 0.469453  [19200/175341]\n",
      "loss: 0.566899  [20800/175341]\n",
      "loss: 0.284124  [22400/175341]\n",
      "loss: 0.363492  [24000/175341]\n",
      "loss: 0.558035  [25600/175341]\n",
      "loss: 0.375464  [27200/175341]\n",
      "loss: 0.872185  [28800/175341]\n",
      "loss: 0.772292  [30400/175341]\n",
      "loss: 0.773556  [32000/175341]\n",
      "loss: 0.638973  [33600/175341]\n",
      "loss: 0.761436  [35200/175341]\n",
      "loss: 0.320131  [36800/175341]\n",
      "loss: 0.228749  [38400/175341]\n",
      "loss: 0.558774  [40000/175341]\n",
      "loss: 0.936529  [41600/175341]\n",
      "loss: 0.537239  [43200/175341]\n",
      "loss: 0.590883  [44800/175341]\n",
      "loss: 0.763229  [46400/175341]\n",
      "loss: 1.032835  [48000/175341]\n",
      "loss: 0.721440  [49600/175341]\n",
      "loss: 0.776269  [51200/175341]\n",
      "loss: 0.233778  [52800/175341]\n",
      "loss: 0.496583  [54400/175341]\n",
      "loss: 0.847656  [56000/175341]\n",
      "loss: 0.400826  [57600/175341]\n",
      "loss: 0.566760  [59200/175341]\n",
      "loss: 1.229384  [60800/175341]\n",
      "loss: 0.596882  [62400/175341]\n",
      "loss: 0.461923  [64000/175341]\n",
      "loss: 0.374560  [65600/175341]\n",
      "loss: 0.445456  [67200/175341]\n",
      "loss: 0.676780  [68800/175341]\n",
      "loss: 0.258569  [70400/175341]\n",
      "loss: 0.555700  [72000/175341]\n",
      "loss: 0.313236  [73600/175341]\n",
      "loss: 0.444663  [75200/175341]\n",
      "loss: 0.648128  [76800/175341]\n",
      "loss: 0.575743  [78400/175341]\n",
      "loss: 0.668772  [80000/175341]\n",
      "loss: 0.400623  [81600/175341]\n",
      "loss: 0.900396  [83200/175341]\n",
      "loss: 0.472281  [84800/175341]\n",
      "loss: 0.459780  [86400/175341]\n",
      "loss: 0.518708  [88000/175341]\n",
      "loss: 0.612806  [89600/175341]\n",
      "loss: 0.201963  [91200/175341]\n",
      "loss: 0.875521  [92800/175341]\n",
      "loss: 0.203409  [94400/175341]\n",
      "loss: 0.478302  [96000/175341]\n",
      "loss: 0.602147  [97600/175341]\n",
      "loss: 0.595587  [99200/175341]\n",
      "loss: 0.314594  [100800/175341]\n",
      "loss: 0.554145  [102400/175341]\n",
      "loss: 0.729406  [104000/175341]\n",
      "loss: 0.876371  [105600/175341]\n",
      "loss: 0.631956  [107200/175341]\n",
      "loss: 0.758073  [108800/175341]\n",
      "loss: 0.349818  [110400/175341]\n",
      "loss: 0.602294  [112000/175341]\n",
      "loss: 0.506503  [113600/175341]\n",
      "loss: 0.596911  [115200/175341]\n",
      "loss: 0.618370  [116800/175341]\n",
      "loss: 0.526309  [118400/175341]\n",
      "loss: 0.182213  [120000/175341]\n",
      "loss: 0.507405  [121600/175341]\n",
      "loss: 0.565806  [123200/175341]\n",
      "loss: 0.707317  [124800/175341]\n",
      "loss: 0.613714  [126400/175341]\n",
      "loss: 0.940223  [128000/175341]\n",
      "loss: 0.476817  [129600/175341]\n",
      "loss: 0.888093  [131200/175341]\n",
      "loss: 0.733915  [132800/175341]\n",
      "loss: 0.467768  [134400/175341]\n",
      "loss: 0.431005  [136000/175341]\n",
      "loss: 0.494066  [137600/175341]\n",
      "loss: 0.771254  [139200/175341]\n",
      "loss: 0.500598  [140800/175341]\n",
      "loss: 0.632814  [142400/175341]\n",
      "loss: 0.500243  [144000/175341]\n",
      "loss: 0.134513  [145600/175341]\n",
      "loss: 0.568039  [147200/175341]\n",
      "loss: 0.763738  [148800/175341]\n",
      "loss: 0.823851  [150400/175341]\n",
      "loss: 0.255547  [152000/175341]\n",
      "loss: 0.520258  [153600/175341]\n",
      "loss: 0.670696  [155200/175341]\n",
      "loss: 0.364736  [156800/175341]\n",
      "loss: 0.174819  [158400/175341]\n",
      "loss: 0.393077  [160000/175341]\n",
      "loss: 0.427211  [161600/175341]\n",
      "loss: 0.535307  [163200/175341]\n",
      "loss: 0.522619  [164800/175341]\n",
      "loss: 0.514459  [166400/175341]\n",
      "loss: 0.720374  [168000/175341]\n",
      "loss: 0.203487  [169600/175341]\n",
      "loss: 0.586454  [171200/175341]\n",
      "loss: 0.704289  [172800/175341]\n",
      "loss: 0.596643  [174400/175341]\n",
      "Train Accuracy: 80.0794%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.577496, F1-score: 75.60%, Macro_F1-Score:  39.22%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.527512  [    0/175341]\n",
      "loss: 0.363540  [ 1600/175341]\n",
      "loss: 0.289708  [ 3200/175341]\n",
      "loss: 0.494552  [ 4800/175341]\n",
      "loss: 0.567000  [ 6400/175341]\n",
      "loss: 0.556268  [ 8000/175341]\n",
      "loss: 0.583582  [ 9600/175341]\n",
      "loss: 0.336158  [11200/175341]\n",
      "loss: 0.348350  [12800/175341]\n",
      "loss: 0.540315  [14400/175341]\n",
      "loss: 0.416438  [16000/175341]\n",
      "loss: 0.378859  [17600/175341]\n",
      "loss: 0.416925  [19200/175341]\n",
      "loss: 0.549384  [20800/175341]\n",
      "loss: 0.145586  [22400/175341]\n",
      "loss: 0.113242  [24000/175341]\n",
      "loss: 0.267939  [25600/175341]\n",
      "loss: 0.432876  [27200/175341]\n",
      "loss: 0.594089  [28800/175341]\n",
      "loss: 0.763383  [30400/175341]\n",
      "loss: 0.599768  [32000/175341]\n",
      "loss: 0.452882  [33600/175341]\n",
      "loss: 0.115168  [35200/175341]\n",
      "loss: 0.576912  [36800/175341]\n",
      "loss: 0.213304  [38400/175341]\n",
      "loss: 0.469674  [40000/175341]\n",
      "loss: 0.277745  [41600/175341]\n",
      "loss: 0.142032  [43200/175341]\n",
      "loss: 1.119923  [44800/175341]\n",
      "loss: 1.088196  [46400/175341]\n",
      "loss: 0.519000  [48000/175341]\n",
      "loss: 0.634427  [49600/175341]\n",
      "loss: 0.247087  [51200/175341]\n",
      "loss: 0.295120  [52800/175341]\n",
      "loss: 0.642196  [54400/175341]\n",
      "loss: 0.408959  [56000/175341]\n",
      "loss: 0.564638  [57600/175341]\n",
      "loss: 0.682514  [59200/175341]\n",
      "loss: 0.344793  [60800/175341]\n",
      "loss: 0.502774  [62400/175341]\n",
      "loss: 0.378879  [64000/175341]\n",
      "loss: 0.599641  [65600/175341]\n",
      "loss: 0.184564  [67200/175341]\n",
      "loss: 0.541185  [68800/175341]\n",
      "loss: 0.658556  [70400/175341]\n",
      "loss: 0.462338  [72000/175341]\n",
      "loss: 0.589805  [73600/175341]\n",
      "loss: 0.672208  [75200/175341]\n",
      "loss: 0.695257  [76800/175341]\n",
      "loss: 0.454401  [78400/175341]\n",
      "loss: 0.467296  [80000/175341]\n",
      "loss: 0.162311  [81600/175341]\n",
      "loss: 0.234300  [83200/175341]\n",
      "loss: 0.643160  [84800/175341]\n",
      "loss: 0.419682  [86400/175341]\n",
      "loss: 0.498550  [88000/175341]\n",
      "loss: 0.923827  [89600/175341]\n",
      "loss: 0.394402  [91200/175341]\n",
      "loss: 0.248987  [92800/175341]\n",
      "loss: 0.442658  [94400/175341]\n",
      "loss: 0.309746  [96000/175341]\n",
      "loss: 0.414303  [97600/175341]\n",
      "loss: 0.432834  [99200/175341]\n",
      "loss: 0.415193  [100800/175341]\n",
      "loss: 0.355801  [102400/175341]\n",
      "loss: 0.384708  [104000/175341]\n",
      "loss: 0.571812  [105600/175341]\n",
      "loss: 0.366404  [107200/175341]\n",
      "loss: 0.257465  [108800/175341]\n",
      "loss: 0.572411  [110400/175341]\n",
      "loss: 0.469600  [112000/175341]\n",
      "loss: 0.452857  [113600/175341]\n",
      "loss: 0.169177  [115200/175341]\n",
      "loss: 0.460691  [116800/175341]\n",
      "loss: 0.185929  [118400/175341]\n",
      "loss: 0.312399  [120000/175341]\n",
      "loss: 0.644659  [121600/175341]\n",
      "loss: 0.314070  [123200/175341]\n",
      "loss: 0.552903  [124800/175341]\n",
      "loss: 0.615197  [126400/175341]\n",
      "loss: 0.758145  [128000/175341]\n",
      "loss: 0.487649  [129600/175341]\n",
      "loss: 0.677149  [131200/175341]\n",
      "loss: 0.616872  [132800/175341]\n",
      "loss: 0.464851  [134400/175341]\n",
      "loss: 0.868979  [136000/175341]\n",
      "loss: 0.462733  [137600/175341]\n",
      "loss: 1.192297  [139200/175341]\n",
      "loss: 0.453805  [140800/175341]\n",
      "loss: 0.269423  [142400/175341]\n",
      "loss: 0.292194  [144000/175341]\n",
      "loss: 0.523955  [145600/175341]\n",
      "loss: 0.544986  [147200/175341]\n",
      "loss: 0.443040  [148800/175341]\n",
      "loss: 0.695574  [150400/175341]\n",
      "loss: 0.511969  [152000/175341]\n",
      "loss: 0.309185  [153600/175341]\n",
      "loss: 0.229102  [155200/175341]\n",
      "loss: 0.378014  [156800/175341]\n",
      "loss: 0.382279  [158400/175341]\n",
      "loss: 0.863035  [160000/175341]\n",
      "loss: 0.846418  [161600/175341]\n",
      "loss: 0.751285  [163200/175341]\n",
      "loss: 0.660730  [164800/175341]\n",
      "loss: 0.526833  [166400/175341]\n",
      "loss: 0.473197  [168000/175341]\n",
      "loss: 0.650572  [169600/175341]\n",
      "loss: 1.055890  [171200/175341]\n",
      "loss: 0.254969  [172800/175341]\n",
      "loss: 0.222760  [174400/175341]\n",
      "Train Accuracy: 80.0817%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.590688, F1-score: 75.05%, Macro_F1-Score:  38.30%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.169688  [    0/175341]\n",
      "loss: 0.238163  [ 1600/175341]\n",
      "loss: 0.699834  [ 3200/175341]\n",
      "loss: 0.232905  [ 4800/175341]\n",
      "loss: 0.694886  [ 6400/175341]\n",
      "loss: 0.303403  [ 8000/175341]\n",
      "loss: 0.439943  [ 9600/175341]\n",
      "loss: 0.775036  [11200/175341]\n",
      "loss: 0.572725  [12800/175341]\n",
      "loss: 0.151100  [14400/175341]\n",
      "loss: 0.441036  [16000/175341]\n",
      "loss: 0.448389  [17600/175341]\n",
      "loss: 0.685803  [19200/175341]\n",
      "loss: 0.785370  [20800/175341]\n",
      "loss: 0.702906  [22400/175341]\n",
      "loss: 0.467284  [24000/175341]\n",
      "loss: 0.822906  [25600/175341]\n",
      "loss: 0.745814  [27200/175341]\n",
      "loss: 0.577826  [28800/175341]\n",
      "loss: 0.852904  [30400/175341]\n",
      "loss: 0.980746  [32000/175341]\n",
      "loss: 0.273332  [33600/175341]\n",
      "loss: 0.930256  [35200/175341]\n",
      "loss: 0.152928  [36800/175341]\n",
      "loss: 0.726109  [38400/175341]\n",
      "loss: 0.633811  [40000/175341]\n",
      "loss: 0.596587  [41600/175341]\n",
      "loss: 0.478733  [43200/175341]\n",
      "loss: 0.663035  [44800/175341]\n",
      "loss: 0.264463  [46400/175341]\n",
      "loss: 0.355691  [48000/175341]\n",
      "loss: 0.510877  [49600/175341]\n",
      "loss: 0.332483  [51200/175341]\n",
      "loss: 0.686998  [52800/175341]\n",
      "loss: 0.619436  [54400/175341]\n",
      "loss: 0.490529  [56000/175341]\n",
      "loss: 0.566476  [57600/175341]\n",
      "loss: 0.705706  [59200/175341]\n",
      "loss: 0.431679  [60800/175341]\n",
      "loss: 0.475278  [62400/175341]\n",
      "loss: 0.302755  [64000/175341]\n",
      "loss: 0.484632  [65600/175341]\n",
      "loss: 0.348576  [67200/175341]\n",
      "loss: 1.021308  [68800/175341]\n",
      "loss: 0.499435  [70400/175341]\n",
      "loss: 0.404822  [72000/175341]\n",
      "loss: 0.583779  [73600/175341]\n",
      "loss: 1.004582  [75200/175341]\n",
      "loss: 0.335306  [76800/175341]\n",
      "loss: 0.323813  [78400/175341]\n",
      "loss: 0.328000  [80000/175341]\n",
      "loss: 0.291249  [81600/175341]\n",
      "loss: 0.551146  [83200/175341]\n",
      "loss: 0.269257  [84800/175341]\n",
      "loss: 0.293355  [86400/175341]\n",
      "loss: 0.426240  [88000/175341]\n",
      "loss: 0.633935  [89600/175341]\n",
      "loss: 0.553428  [91200/175341]\n",
      "loss: 0.320091  [92800/175341]\n",
      "loss: 0.677093  [94400/175341]\n",
      "loss: 0.542330  [96000/175341]\n",
      "loss: 0.507616  [97600/175341]\n",
      "loss: 0.655085  [99200/175341]\n",
      "loss: 0.470410  [100800/175341]\n",
      "loss: 0.938162  [102400/175341]\n",
      "loss: 0.523065  [104000/175341]\n",
      "loss: 0.546564  [105600/175341]\n",
      "loss: 0.288598  [107200/175341]\n",
      "loss: 0.290278  [108800/175341]\n",
      "loss: 0.479153  [110400/175341]\n",
      "loss: 0.706689  [112000/175341]\n",
      "loss: 0.350994  [113600/175341]\n",
      "loss: 0.755527  [115200/175341]\n",
      "loss: 0.416461  [116800/175341]\n",
      "loss: 0.519891  [118400/175341]\n",
      "loss: 0.422353  [120000/175341]\n",
      "loss: 0.504629  [121600/175341]\n",
      "loss: 0.741205  [123200/175341]\n",
      "loss: 0.728263  [124800/175341]\n",
      "loss: 1.322949  [126400/175341]\n",
      "loss: 0.335183  [128000/175341]\n",
      "loss: 0.271750  [129600/175341]\n",
      "loss: 0.479873  [131200/175341]\n",
      "loss: 0.918381  [132800/175341]\n",
      "loss: 0.826365  [134400/175341]\n",
      "loss: 0.298952  [136000/175341]\n",
      "loss: 0.726361  [137600/175341]\n",
      "loss: 0.497568  [139200/175341]\n",
      "loss: 0.166665  [140800/175341]\n",
      "loss: 0.337850  [142400/175341]\n",
      "loss: 0.444016  [144000/175341]\n",
      "loss: 0.195615  [145600/175341]\n",
      "loss: 0.519432  [147200/175341]\n",
      "loss: 1.048502  [148800/175341]\n",
      "loss: 0.327789  [150400/175341]\n",
      "loss: 1.081998  [152000/175341]\n",
      "loss: 0.411040  [153600/175341]\n",
      "loss: 0.181798  [155200/175341]\n",
      "loss: 0.551846  [156800/175341]\n",
      "loss: 0.241427  [158400/175341]\n",
      "loss: 0.317654  [160000/175341]\n",
      "loss: 0.494280  [161600/175341]\n",
      "loss: 0.384603  [163200/175341]\n",
      "loss: 0.209397  [164800/175341]\n",
      "loss: 0.284735  [166400/175341]\n",
      "loss: 0.356160  [168000/175341]\n",
      "loss: 0.413090  [169600/175341]\n",
      "loss: 0.456110  [171200/175341]\n",
      "loss: 0.487484  [172800/175341]\n",
      "loss: 0.606583  [174400/175341]\n",
      "Train Accuracy: 80.0902%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.587320, F1-score: 74.40%, Macro_F1-Score:  39.42%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b427034-602d-4252-af52-9c7d0908f074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.375620  [    0/175341]\n",
      "loss: 0.282123  [ 1600/175341]\n",
      "loss: 0.475874  [ 3200/175341]\n",
      "loss: 0.347285  [ 4800/175341]\n",
      "loss: 0.561632  [ 6400/175341]\n",
      "loss: 0.401702  [ 8000/175341]\n",
      "loss: 0.671679  [ 9600/175341]\n",
      "loss: 0.827716  [11200/175341]\n",
      "loss: 0.343303  [12800/175341]\n",
      "loss: 0.499682  [14400/175341]\n",
      "loss: 0.367987  [16000/175341]\n",
      "loss: 0.538561  [17600/175341]\n",
      "loss: 0.662486  [19200/175341]\n",
      "loss: 0.669395  [20800/175341]\n",
      "loss: 0.547318  [22400/175341]\n",
      "loss: 0.425650  [24000/175341]\n",
      "loss: 0.316645  [25600/175341]\n",
      "loss: 0.376105  [27200/175341]\n",
      "loss: 0.173211  [28800/175341]\n",
      "loss: 0.406879  [30400/175341]\n",
      "loss: 0.922225  [32000/175341]\n",
      "loss: 0.250959  [33600/175341]\n",
      "loss: 0.767932  [35200/175341]\n",
      "loss: 0.568927  [36800/175341]\n",
      "loss: 0.699224  [38400/175341]\n",
      "loss: 0.223173  [40000/175341]\n",
      "loss: 0.245862  [41600/175341]\n",
      "loss: 0.435467  [43200/175341]\n",
      "loss: 0.226678  [44800/175341]\n",
      "loss: 0.408083  [46400/175341]\n",
      "loss: 0.555415  [48000/175341]\n",
      "loss: 0.309306  [49600/175341]\n",
      "loss: 0.368645  [51200/175341]\n",
      "loss: 0.419752  [52800/175341]\n",
      "loss: 0.227210  [54400/175341]\n",
      "loss: 0.490980  [56000/175341]\n",
      "loss: 0.309596  [57600/175341]\n",
      "loss: 0.489311  [59200/175341]\n",
      "loss: 0.919304  [60800/175341]\n",
      "loss: 0.637033  [62400/175341]\n",
      "loss: 0.281730  [64000/175341]\n",
      "loss: 0.650037  [65600/175341]\n",
      "loss: 0.345709  [67200/175341]\n",
      "loss: 0.403924  [68800/175341]\n",
      "loss: 0.552267  [70400/175341]\n",
      "loss: 0.242821  [72000/175341]\n",
      "loss: 0.571248  [73600/175341]\n",
      "loss: 0.398487  [75200/175341]\n",
      "loss: 0.579189  [76800/175341]\n",
      "loss: 0.282726  [78400/175341]\n",
      "loss: 0.371714  [80000/175341]\n",
      "loss: 0.733218  [81600/175341]\n",
      "loss: 0.502194  [83200/175341]\n",
      "loss: 0.295923  [84800/175341]\n",
      "loss: 0.604151  [86400/175341]\n",
      "loss: 0.516630  [88000/175341]\n",
      "loss: 0.448985  [89600/175341]\n",
      "loss: 0.316635  [91200/175341]\n",
      "loss: 0.247065  [92800/175341]\n",
      "loss: 0.493296  [94400/175341]\n",
      "loss: 0.333196  [96000/175341]\n",
      "loss: 1.011081  [97600/175341]\n",
      "loss: 0.245478  [99200/175341]\n",
      "loss: 0.359091  [100800/175341]\n",
      "loss: 0.528787  [102400/175341]\n",
      "loss: 0.331338  [104000/175341]\n",
      "loss: 0.676462  [105600/175341]\n",
      "loss: 0.289775  [107200/175341]\n",
      "loss: 0.770478  [108800/175341]\n",
      "loss: 0.659609  [110400/175341]\n",
      "loss: 0.382877  [112000/175341]\n",
      "loss: 0.220600  [113600/175341]\n",
      "loss: 0.803381  [115200/175341]\n",
      "loss: 0.581448  [116800/175341]\n",
      "loss: 0.885886  [118400/175341]\n",
      "loss: 0.406233  [120000/175341]\n",
      "loss: 0.429501  [121600/175341]\n",
      "loss: 0.450611  [123200/175341]\n",
      "loss: 0.248620  [124800/175341]\n",
      "loss: 0.613767  [126400/175341]\n",
      "loss: 0.358901  [128000/175341]\n",
      "loss: 0.325194  [129600/175341]\n",
      "loss: 0.102867  [131200/175341]\n",
      "loss: 0.521009  [132800/175341]\n",
      "loss: 0.728827  [134400/175341]\n",
      "loss: 0.541686  [136000/175341]\n",
      "loss: 0.655367  [137600/175341]\n",
      "loss: 0.849395  [139200/175341]\n",
      "loss: 0.451763  [140800/175341]\n",
      "loss: 0.342995  [142400/175341]\n",
      "loss: 0.745894  [144000/175341]\n",
      "loss: 0.872405  [145600/175341]\n",
      "loss: 0.212771  [147200/175341]\n",
      "loss: 0.196142  [148800/175341]\n",
      "loss: 0.397781  [150400/175341]\n",
      "loss: 0.186719  [152000/175341]\n",
      "loss: 0.270965  [153600/175341]\n",
      "loss: 0.371543  [155200/175341]\n",
      "loss: 0.391045  [156800/175341]\n",
      "loss: 0.558009  [158400/175341]\n",
      "loss: 0.543627  [160000/175341]\n",
      "loss: 0.421854  [161600/175341]\n",
      "loss: 0.413211  [163200/175341]\n",
      "loss: 0.575768  [164800/175341]\n",
      "loss: 0.396890  [166400/175341]\n",
      "loss: 0.433680  [168000/175341]\n",
      "loss: 0.052923  [169600/175341]\n",
      "loss: 0.400107  [171200/175341]\n",
      "loss: 0.435471  [172800/175341]\n",
      "loss: 0.627943  [174400/175341]\n",
      "Train Accuracy: 80.0834%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.584576, F1-score: 74.92%, Macro_F1-Score:  39.13%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.278406  [    0/175341]\n",
      "loss: 0.437672  [ 1600/175341]\n",
      "loss: 0.413727  [ 3200/175341]\n",
      "loss: 0.669093  [ 4800/175341]\n",
      "loss: 0.961967  [ 6400/175341]\n",
      "loss: 0.790533  [ 8000/175341]\n",
      "loss: 0.428893  [ 9600/175341]\n",
      "loss: 0.605452  [11200/175341]\n",
      "loss: 0.776881  [12800/175341]\n",
      "loss: 0.201513  [14400/175341]\n",
      "loss: 0.686755  [16000/175341]\n",
      "loss: 0.189333  [17600/175341]\n",
      "loss: 0.610913  [19200/175341]\n",
      "loss: 0.552874  [20800/175341]\n",
      "loss: 0.306977  [22400/175341]\n",
      "loss: 0.612833  [24000/175341]\n",
      "loss: 0.560401  [25600/175341]\n",
      "loss: 0.231701  [27200/175341]\n",
      "loss: 0.577549  [28800/175341]\n",
      "loss: 0.364868  [30400/175341]\n",
      "loss: 0.339190  [32000/175341]\n",
      "loss: 0.661658  [33600/175341]\n",
      "loss: 0.558630  [35200/175341]\n",
      "loss: 0.409479  [36800/175341]\n",
      "loss: 0.604917  [38400/175341]\n",
      "loss: 0.262084  [40000/175341]\n",
      "loss: 0.208989  [41600/175341]\n",
      "loss: 0.568005  [43200/175341]\n",
      "loss: 0.306332  [44800/175341]\n",
      "loss: 0.433874  [46400/175341]\n",
      "loss: 0.392687  [48000/175341]\n",
      "loss: 0.392313  [49600/175341]\n",
      "loss: 0.485145  [51200/175341]\n",
      "loss: 0.456706  [52800/175341]\n",
      "loss: 0.471852  [54400/175341]\n",
      "loss: 0.745230  [56000/175341]\n",
      "loss: 0.755039  [57600/175341]\n",
      "loss: 0.499241  [59200/175341]\n",
      "loss: 0.379594  [60800/175341]\n",
      "loss: 0.213931  [62400/175341]\n",
      "loss: 0.761847  [64000/175341]\n",
      "loss: 0.651015  [65600/175341]\n",
      "loss: 1.068456  [67200/175341]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115f1b7-87f8-41a5-8992-af1752600f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a33c4-8087-4b5c-80cf-c10334d2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 15,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada289ca-73cc-4ac6-8f1c-89c10d83b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6d93-0220-4085-bb3f-8965b97240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e34344-c96e-42c9-b318-96c5e291cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units1: int, hidden_units2: int, hidden_units3: int, hidden_units4: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units2),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units3),\n",
    "        )\n",
    "\n",
    "        # **New Block Added Here**\n",
    "        self.linear_relu_stack3 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units4),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack4 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        x = self.linear_relu_stack3(x)  # Pass through new block\n",
    "        logits = self.linear_relu_stack4(x)  # Final output layer\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = NeuralNetwork(40,128,64,32,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model_test, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model_test, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
