{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"all\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m smote_enn \u001b[38;5;241m=\u001b[39m SMOTEENN(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, enn \u001b[38;5;241m=\u001b[39m enn, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply SMOTEENN\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_data_X, train_data_y \u001b[38;5;241m=\u001b[39m \u001b[43msmote_enn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check the class distribution after resampling\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:105\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    111\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:160\u001b[0m, in \u001b[0;36mSMOTEENN._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy\n\u001b[0;32m    159\u001b[0m X_res, y_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmote_\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_res\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\base.py:105\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    111\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_edited_nearest_neighbours.py:168\u001b[0m, in \u001b[0;36mEditedNearestNeighbours._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    166\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[0;32m    167\u001b[0m y_class \u001b[38;5;241m=\u001b[39m _safe_indexing(y, target_class_indices)\n\u001b[1;32m--> 168\u001b[0m nnhood_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    169\u001b[0m nnhood_label \u001b[38;5;241m=\u001b[39m y[nnhood_idx]\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind_sel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    862\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    865\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    866\u001b[0m     )\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    881\u001b[0m ):\n\u001b[0;32m    882\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    883\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    884\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    294\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    295\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    302\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:59\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\threadpoolctl.py:592\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "smote_enn = SMOTEENN(sampling_strategy=\"auto\", enn = enn, random_state=42)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_labels_encoded)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=25) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306367  [    0/481450]\n",
      "loss: 2.304133  [ 3200/481450]\n",
      "loss: 2.297894  [ 6400/481450]\n",
      "loss: 2.294239  [ 9600/481450]\n",
      "loss: 2.293010  [12800/481450]\n",
      "loss: 2.294630  [16000/481450]\n",
      "loss: 2.291759  [19200/481450]\n",
      "loss: 2.258715  [22400/481450]\n",
      "loss: 2.281751  [25600/481450]\n",
      "loss: 2.213294  [28800/481450]\n",
      "loss: 2.157953  [32000/481450]\n",
      "loss: 2.132128  [35200/481450]\n",
      "loss: 2.056380  [38400/481450]\n",
      "loss: 1.993598  [41600/481450]\n",
      "loss: 2.037606  [44800/481450]\n",
      "loss: 1.933084  [48000/481450]\n",
      "loss: 1.902035  [51200/481450]\n",
      "loss: 1.932992  [54400/481450]\n",
      "loss: 1.783628  [57600/481450]\n",
      "loss: 1.859918  [60800/481450]\n",
      "loss: 1.726900  [64000/481450]\n",
      "loss: 1.751661  [67200/481450]\n",
      "loss: 1.753732  [70400/481450]\n",
      "loss: 1.749011  [73600/481450]\n",
      "loss: 1.622834  [76800/481450]\n",
      "loss: 1.654081  [80000/481450]\n",
      "loss: 1.555909  [83200/481450]\n",
      "loss: 1.648704  [86400/481450]\n",
      "loss: 1.520402  [89600/481450]\n",
      "loss: 1.494369  [92800/481450]\n",
      "loss: 1.496597  [96000/481450]\n",
      "loss: 1.464365  [99200/481450]\n",
      "loss: 1.423508  [102400/481450]\n",
      "loss: 1.596869  [105600/481450]\n",
      "loss: 1.401448  [108800/481450]\n",
      "loss: 1.342897  [112000/481450]\n",
      "loss: 1.428689  [115200/481450]\n",
      "loss: 1.419925  [118400/481450]\n",
      "loss: 1.260768  [121600/481450]\n",
      "loss: 1.365284  [124800/481450]\n",
      "loss: 1.404295  [128000/481450]\n",
      "loss: 1.190488  [131200/481450]\n",
      "loss: 1.082269  [134400/481450]\n",
      "loss: 1.180974  [137600/481450]\n",
      "loss: 1.137061  [140800/481450]\n",
      "loss: 1.255072  [144000/481450]\n",
      "loss: 1.280643  [147200/481450]\n",
      "loss: 1.165350  [150400/481450]\n",
      "loss: 1.128417  [153600/481450]\n",
      "loss: 0.855302  [156800/481450]\n",
      "loss: 0.812567  [160000/481450]\n",
      "loss: 1.252156  [163200/481450]\n",
      "loss: 1.068294  [166400/481450]\n",
      "loss: 0.922093  [169600/481450]\n",
      "loss: 1.075138  [172800/481450]\n",
      "loss: 0.874635  [176000/481450]\n",
      "loss: 0.948802  [179200/481450]\n",
      "loss: 0.975281  [182400/481450]\n",
      "loss: 0.982467  [185600/481450]\n",
      "loss: 1.057923  [188800/481450]\n",
      "loss: 0.943242  [192000/481450]\n",
      "loss: 1.042360  [195200/481450]\n",
      "loss: 0.807654  [198400/481450]\n",
      "loss: 1.178262  [201600/481450]\n",
      "loss: 1.039080  [204800/481450]\n",
      "loss: 0.975051  [208000/481450]\n",
      "loss: 0.797708  [211200/481450]\n",
      "loss: 0.775088  [214400/481450]\n",
      "loss: 0.892778  [217600/481450]\n",
      "loss: 0.950593  [220800/481450]\n",
      "loss: 0.995704  [224000/481450]\n",
      "loss: 0.930232  [227200/481450]\n",
      "loss: 0.882543  [230400/481450]\n",
      "loss: 0.803611  [233600/481450]\n",
      "loss: 0.748107  [236800/481450]\n",
      "loss: 0.823961  [240000/481450]\n",
      "loss: 0.885103  [243200/481450]\n",
      "loss: 0.746891  [246400/481450]\n",
      "loss: 0.881002  [249600/481450]\n",
      "loss: 0.879453  [252800/481450]\n",
      "loss: 0.928353  [256000/481450]\n",
      "loss: 0.547119  [259200/481450]\n",
      "loss: 0.794856  [262400/481450]\n",
      "loss: 0.735078  [265600/481450]\n",
      "loss: 0.690576  [268800/481450]\n",
      "loss: 0.791713  [272000/481450]\n",
      "loss: 0.869659  [275200/481450]\n",
      "loss: 0.840766  [278400/481450]\n",
      "loss: 0.716648  [281600/481450]\n",
      "loss: 0.768382  [284800/481450]\n",
      "loss: 0.545468  [288000/481450]\n",
      "loss: 0.872690  [291200/481450]\n",
      "loss: 0.826619  [294400/481450]\n",
      "loss: 0.847056  [297600/481450]\n",
      "loss: 0.654451  [300800/481450]\n",
      "loss: 1.005961  [304000/481450]\n",
      "loss: 0.758457  [307200/481450]\n",
      "loss: 0.860635  [310400/481450]\n",
      "loss: 0.602703  [313600/481450]\n",
      "loss: 0.669126  [316800/481450]\n",
      "loss: 1.008739  [320000/481450]\n",
      "loss: 0.775535  [323200/481450]\n",
      "loss: 0.796331  [326400/481450]\n",
      "loss: 0.620879  [329600/481450]\n",
      "loss: 0.856719  [332800/481450]\n",
      "loss: 0.633773  [336000/481450]\n",
      "loss: 0.610015  [339200/481450]\n",
      "loss: 0.512934  [342400/481450]\n",
      "loss: 0.543727  [345600/481450]\n",
      "loss: 0.716247  [348800/481450]\n",
      "loss: 0.841256  [352000/481450]\n",
      "loss: 0.857334  [355200/481450]\n",
      "loss: 0.975227  [358400/481450]\n",
      "loss: 0.722352  [361600/481450]\n",
      "loss: 0.561238  [364800/481450]\n",
      "loss: 0.859301  [368000/481450]\n",
      "loss: 0.811852  [371200/481450]\n",
      "loss: 0.632307  [374400/481450]\n",
      "loss: 0.663390  [377600/481450]\n",
      "loss: 0.661015  [380800/481450]\n",
      "loss: 0.626912  [384000/481450]\n",
      "loss: 0.830757  [387200/481450]\n",
      "loss: 0.824899  [390400/481450]\n",
      "loss: 0.650700  [393600/481450]\n",
      "loss: 0.499439  [396800/481450]\n",
      "loss: 0.653548  [400000/481450]\n",
      "loss: 0.731687  [403200/481450]\n",
      "loss: 0.996662  [406400/481450]\n",
      "loss: 0.678615  [409600/481450]\n",
      "loss: 0.580467  [412800/481450]\n",
      "loss: 0.476464  [416000/481450]\n",
      "loss: 0.615233  [419200/481450]\n",
      "loss: 0.716704  [422400/481450]\n",
      "loss: 0.828314  [425600/481450]\n",
      "loss: 0.744968  [428800/481450]\n",
      "loss: 0.514197  [432000/481450]\n",
      "loss: 0.633656  [435200/481450]\n",
      "loss: 0.592020  [438400/481450]\n",
      "loss: 0.472851  [441600/481450]\n",
      "loss: 0.518288  [444800/481450]\n",
      "loss: 0.652020  [448000/481450]\n",
      "loss: 0.670883  [451200/481450]\n",
      "loss: 0.671142  [454400/481450]\n",
      "loss: 0.569627  [457600/481450]\n",
      "loss: 0.501377  [460800/481450]\n",
      "loss: 0.574202  [464000/481450]\n",
      "loss: 0.510977  [467200/481450]\n",
      "loss: 0.908300  [470400/481450]\n",
      "loss: 0.875350  [473600/481450]\n",
      "loss: 0.625198  [476800/481450]\n",
      "loss: 0.455782  [480000/481450]\n",
      "Train Accuracy: 62.3926%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.594706, F1-score: 80.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.557794  [    0/481450]\n",
      "loss: 0.703320  [ 3200/481450]\n",
      "loss: 0.579759  [ 6400/481450]\n",
      "loss: 0.439117  [ 9600/481450]\n",
      "loss: 0.542094  [12800/481450]\n",
      "loss: 0.894564  [16000/481450]\n",
      "loss: 0.857356  [19200/481450]\n",
      "loss: 0.545533  [22400/481450]\n",
      "loss: 0.591782  [25600/481450]\n",
      "loss: 0.580753  [28800/481450]\n",
      "loss: 0.717353  [32000/481450]\n",
      "loss: 0.337054  [35200/481450]\n",
      "loss: 0.431033  [38400/481450]\n",
      "loss: 0.940711  [41600/481450]\n",
      "loss: 0.549631  [44800/481450]\n",
      "loss: 0.543314  [48000/481450]\n",
      "loss: 0.448041  [51200/481450]\n",
      "loss: 0.711517  [54400/481450]\n",
      "loss: 0.570732  [57600/481450]\n",
      "loss: 0.544091  [60800/481450]\n",
      "loss: 0.953807  [64000/481450]\n",
      "loss: 0.747723  [67200/481450]\n",
      "loss: 0.615590  [70400/481450]\n",
      "loss: 0.406011  [73600/481450]\n",
      "loss: 0.282164  [76800/481450]\n",
      "loss: 0.461356  [80000/481450]\n",
      "loss: 0.409172  [83200/481450]\n",
      "loss: 0.554900  [86400/481450]\n",
      "loss: 0.655743  [89600/481450]\n",
      "loss: 0.569093  [92800/481450]\n",
      "loss: 0.656567  [96000/481450]\n",
      "loss: 0.643837  [99200/481450]\n",
      "loss: 0.461320  [102400/481450]\n",
      "loss: 0.571920  [105600/481450]\n",
      "loss: 0.678911  [108800/481450]\n",
      "loss: 0.388218  [112000/481450]\n",
      "loss: 0.580619  [115200/481450]\n",
      "loss: 0.547854  [118400/481450]\n",
      "loss: 0.548533  [121600/481450]\n",
      "loss: 0.592482  [124800/481450]\n",
      "loss: 0.704020  [128000/481450]\n",
      "loss: 0.542950  [131200/481450]\n",
      "loss: 0.329963  [134400/481450]\n",
      "loss: 0.381792  [137600/481450]\n",
      "loss: 0.462404  [140800/481450]\n",
      "loss: 0.422352  [144000/481450]\n",
      "loss: 0.444156  [147200/481450]\n",
      "loss: 0.472004  [150400/481450]\n",
      "loss: 0.617432  [153600/481450]\n",
      "loss: 0.358813  [156800/481450]\n",
      "loss: 0.495874  [160000/481450]\n",
      "loss: 0.534643  [163200/481450]\n",
      "loss: 0.783249  [166400/481450]\n",
      "loss: 0.413029  [169600/481450]\n",
      "loss: 0.520261  [172800/481450]\n",
      "loss: 0.372397  [176000/481450]\n",
      "loss: 0.447243  [179200/481450]\n",
      "loss: 0.335316  [182400/481450]\n",
      "loss: 0.730708  [185600/481450]\n",
      "loss: 0.571753  [188800/481450]\n",
      "loss: 0.345237  [192000/481450]\n",
      "loss: 0.411565  [195200/481450]\n",
      "loss: 0.654835  [198400/481450]\n",
      "loss: 0.494413  [201600/481450]\n",
      "loss: 0.252515  [204800/481450]\n",
      "loss: 0.674506  [208000/481450]\n",
      "loss: 0.483153  [211200/481450]\n",
      "loss: 0.630107  [214400/481450]\n",
      "loss: 0.356822  [217600/481450]\n",
      "loss: 0.549951  [220800/481450]\n",
      "loss: 0.472647  [224000/481450]\n",
      "loss: 0.428646  [227200/481450]\n",
      "loss: 0.371250  [230400/481450]\n",
      "loss: 0.672740  [233600/481450]\n",
      "loss: 0.457616  [236800/481450]\n",
      "loss: 0.522830  [240000/481450]\n",
      "loss: 0.562546  [243200/481450]\n",
      "loss: 0.431905  [246400/481450]\n",
      "loss: 0.481606  [249600/481450]\n",
      "loss: 0.362056  [252800/481450]\n",
      "loss: 0.327519  [256000/481450]\n",
      "loss: 0.307785  [259200/481450]\n",
      "loss: 0.464719  [262400/481450]\n",
      "loss: 0.539711  [265600/481450]\n",
      "loss: 0.529728  [268800/481450]\n",
      "loss: 0.427240  [272000/481450]\n",
      "loss: 0.797124  [275200/481450]\n",
      "loss: 0.524390  [278400/481450]\n",
      "loss: 0.322839  [281600/481450]\n",
      "loss: 0.562008  [284800/481450]\n",
      "loss: 0.245596  [288000/481450]\n",
      "loss: 0.452374  [291200/481450]\n",
      "loss: 0.433382  [294400/481450]\n",
      "loss: 0.413295  [297600/481450]\n",
      "loss: 0.476352  [300800/481450]\n",
      "loss: 0.193892  [304000/481450]\n",
      "loss: 0.442151  [307200/481450]\n",
      "loss: 0.396943  [310400/481450]\n",
      "loss: 0.330259  [313600/481450]\n",
      "loss: 0.784217  [316800/481450]\n",
      "loss: 0.418683  [320000/481450]\n",
      "loss: 0.285655  [323200/481450]\n",
      "loss: 0.598416  [326400/481450]\n",
      "loss: 0.322908  [329600/481450]\n",
      "loss: 0.362176  [332800/481450]\n",
      "loss: 0.616570  [336000/481450]\n",
      "loss: 0.715215  [339200/481450]\n",
      "loss: 0.549902  [342400/481450]\n",
      "loss: 0.268243  [345600/481450]\n",
      "loss: 0.518778  [348800/481450]\n",
      "loss: 0.316710  [352000/481450]\n",
      "loss: 0.284108  [355200/481450]\n",
      "loss: 0.358198  [358400/481450]\n",
      "loss: 0.328598  [361600/481450]\n",
      "loss: 0.286163  [364800/481450]\n",
      "loss: 0.433024  [368000/481450]\n",
      "loss: 0.467497  [371200/481450]\n",
      "loss: 0.569380  [374400/481450]\n",
      "loss: 0.480737  [377600/481450]\n",
      "loss: 0.423669  [380800/481450]\n",
      "loss: 0.822450  [384000/481450]\n",
      "loss: 0.391290  [387200/481450]\n",
      "loss: 0.555252  [390400/481450]\n",
      "loss: 0.317106  [393600/481450]\n",
      "loss: 0.499473  [396800/481450]\n",
      "loss: 0.520916  [400000/481450]\n",
      "loss: 0.372970  [403200/481450]\n",
      "loss: 0.383539  [406400/481450]\n",
      "loss: 0.757045  [409600/481450]\n",
      "loss: 0.713838  [412800/481450]\n",
      "loss: 0.555808  [416000/481450]\n",
      "loss: 0.550557  [419200/481450]\n",
      "loss: 0.454939  [422400/481450]\n",
      "loss: 0.269431  [425600/481450]\n",
      "loss: 0.240097  [428800/481450]\n",
      "loss: 0.566077  [432000/481450]\n",
      "loss: 0.524944  [435200/481450]\n",
      "loss: 0.465255  [438400/481450]\n",
      "loss: 1.010519  [441600/481450]\n",
      "loss: 0.637622  [444800/481450]\n",
      "loss: 0.469068  [448000/481450]\n",
      "loss: 0.323514  [451200/481450]\n",
      "loss: 0.169544  [454400/481450]\n",
      "loss: 0.134317  [457600/481450]\n",
      "loss: 0.419482  [460800/481450]\n",
      "loss: 0.537873  [464000/481450]\n",
      "loss: 0.353584  [467200/481450]\n",
      "loss: 0.243835  [470400/481450]\n",
      "loss: 0.271080  [473600/481450]\n",
      "loss: 0.245908  [476800/481450]\n",
      "loss: 0.439728  [480000/481450]\n",
      "Train Accuracy: 81.6654%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.755197, F1-score: 79.23% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.551525  [    0/481450]\n",
      "loss: 0.904138  [ 3200/481450]\n",
      "loss: 0.320675  [ 6400/481450]\n",
      "loss: 0.463757  [ 9600/481450]\n",
      "loss: 0.428013  [12800/481450]\n",
      "loss: 0.383397  [16000/481450]\n",
      "loss: 0.428408  [19200/481450]\n",
      "loss: 0.373653  [22400/481450]\n",
      "loss: 0.259516  [25600/481450]\n",
      "loss: 0.335117  [28800/481450]\n",
      "loss: 0.764580  [32000/481450]\n",
      "loss: 0.408928  [35200/481450]\n",
      "loss: 0.365838  [38400/481450]\n",
      "loss: 0.438799  [41600/481450]\n",
      "loss: 0.828317  [44800/481450]\n",
      "loss: 0.597631  [48000/481450]\n",
      "loss: 0.499950  [51200/481450]\n",
      "loss: 0.515783  [54400/481450]\n",
      "loss: 0.552362  [57600/481450]\n",
      "loss: 0.367032  [60800/481450]\n",
      "loss: 0.365312  [64000/481450]\n",
      "loss: 0.431054  [67200/481450]\n",
      "loss: 0.624206  [70400/481450]\n",
      "loss: 0.470158  [73600/481450]\n",
      "loss: 0.612460  [76800/481450]\n",
      "loss: 0.318562  [80000/481450]\n",
      "loss: 0.440195  [83200/481450]\n",
      "loss: 0.398661  [86400/481450]\n",
      "loss: 0.507635  [89600/481450]\n",
      "loss: 0.293095  [92800/481450]\n",
      "loss: 0.369828  [96000/481450]\n",
      "loss: 0.322760  [99200/481450]\n",
      "loss: 0.421001  [102400/481450]\n",
      "loss: 0.168119  [105600/481450]\n",
      "loss: 0.553238  [108800/481450]\n",
      "loss: 0.304819  [112000/481450]\n",
      "loss: 0.444947  [115200/481450]\n",
      "loss: 0.607041  [118400/481450]\n",
      "loss: 0.700249  [121600/481450]\n",
      "loss: 0.384428  [124800/481450]\n",
      "loss: 0.433081  [128000/481450]\n",
      "loss: 0.327477  [131200/481450]\n",
      "loss: 0.323850  [134400/481450]\n",
      "loss: 0.315886  [137600/481450]\n",
      "loss: 0.315815  [140800/481450]\n",
      "loss: 0.603689  [144000/481450]\n",
      "loss: 0.446987  [147200/481450]\n",
      "loss: 0.463501  [150400/481450]\n",
      "loss: 0.273994  [153600/481450]\n",
      "loss: 0.432089  [156800/481450]\n",
      "loss: 0.646111  [160000/481450]\n",
      "loss: 0.261999  [163200/481450]\n",
      "loss: 0.594787  [166400/481450]\n",
      "loss: 0.543988  [169600/481450]\n",
      "loss: 0.261814  [172800/481450]\n",
      "loss: 0.369024  [176000/481450]\n",
      "loss: 0.649658  [179200/481450]\n",
      "loss: 0.236865  [182400/481450]\n",
      "loss: 0.333026  [185600/481450]\n",
      "loss: 0.438567  [188800/481450]\n",
      "loss: 0.337657  [192000/481450]\n",
      "loss: 0.452009  [195200/481450]\n",
      "loss: 0.269232  [198400/481450]\n",
      "loss: 0.552310  [201600/481450]\n",
      "loss: 0.801118  [204800/481450]\n",
      "loss: 0.387382  [208000/481450]\n",
      "loss: 0.411948  [211200/481450]\n",
      "loss: 0.308402  [214400/481450]\n",
      "loss: 0.444229  [217600/481450]\n",
      "loss: 0.299193  [220800/481450]\n",
      "loss: 0.204050  [224000/481450]\n",
      "loss: 0.392476  [227200/481450]\n",
      "loss: 0.342130  [230400/481450]\n",
      "loss: 0.661535  [233600/481450]\n",
      "loss: 0.831007  [236800/481450]\n",
      "loss: 0.258305  [240000/481450]\n",
      "loss: 0.510929  [243200/481450]\n",
      "loss: 0.452430  [246400/481450]\n",
      "loss: 0.435445  [249600/481450]\n",
      "loss: 0.267553  [252800/481450]\n",
      "loss: 0.408211  [256000/481450]\n",
      "loss: 0.467984  [259200/481450]\n",
      "loss: 0.175782  [262400/481450]\n",
      "loss: 0.570092  [265600/481450]\n",
      "loss: 0.352579  [268800/481450]\n",
      "loss: 0.708714  [272000/481450]\n",
      "loss: 0.346743  [275200/481450]\n",
      "loss: 0.430373  [278400/481450]\n",
      "loss: 0.493009  [281600/481450]\n",
      "loss: 0.212483  [284800/481450]\n",
      "loss: 0.344761  [288000/481450]\n",
      "loss: 0.225687  [291200/481450]\n",
      "loss: 0.570436  [294400/481450]\n",
      "loss: 0.231040  [297600/481450]\n",
      "loss: 0.304599  [300800/481450]\n",
      "loss: 0.511050  [304000/481450]\n",
      "loss: 0.245721  [307200/481450]\n",
      "loss: 0.413825  [310400/481450]\n",
      "loss: 0.525868  [313600/481450]\n",
      "loss: 0.432846  [316800/481450]\n",
      "loss: 0.386294  [320000/481450]\n",
      "loss: 0.345527  [323200/481450]\n",
      "loss: 0.630391  [326400/481450]\n",
      "loss: 0.285970  [329600/481450]\n",
      "loss: 0.454974  [332800/481450]\n",
      "loss: 0.276706  [336000/481450]\n",
      "loss: 0.317968  [339200/481450]\n",
      "loss: 0.558259  [342400/481450]\n",
      "loss: 0.490272  [345600/481450]\n",
      "loss: 0.275813  [348800/481450]\n",
      "loss: 0.256627  [352000/481450]\n",
      "loss: 0.394456  [355200/481450]\n",
      "loss: 0.570927  [358400/481450]\n",
      "loss: 0.384300  [361600/481450]\n",
      "loss: 0.566688  [364800/481450]\n",
      "loss: 0.153884  [368000/481450]\n",
      "loss: 0.233837  [371200/481450]\n",
      "loss: 0.283120  [374400/481450]\n",
      "loss: 0.395697  [377600/481450]\n",
      "loss: 0.418851  [380800/481450]\n",
      "loss: 0.383558  [384000/481450]\n",
      "loss: 0.209850  [387200/481450]\n",
      "loss: 0.439723  [390400/481450]\n",
      "loss: 0.518268  [393600/481450]\n",
      "loss: 0.517628  [396800/481450]\n",
      "loss: 0.419957  [400000/481450]\n",
      "loss: 0.466081  [403200/481450]\n",
      "loss: 0.294193  [406400/481450]\n",
      "loss: 0.561364  [409600/481450]\n",
      "loss: 0.318054  [412800/481450]\n",
      "loss: 0.335849  [416000/481450]\n",
      "loss: 0.271655  [419200/481450]\n",
      "loss: 0.222327  [422400/481450]\n",
      "loss: 0.669460  [425600/481450]\n",
      "loss: 0.603224  [428800/481450]\n",
      "loss: 0.339609  [432000/481450]\n",
      "loss: 0.202220  [435200/481450]\n",
      "loss: 0.347285  [438400/481450]\n",
      "loss: 0.390454  [441600/481450]\n",
      "loss: 0.580184  [444800/481450]\n",
      "loss: 0.462124  [448000/481450]\n",
      "loss: 0.351822  [451200/481450]\n",
      "loss: 0.489358  [454400/481450]\n",
      "loss: 0.278123  [457600/481450]\n",
      "loss: 0.480375  [460800/481450]\n",
      "loss: 0.535702  [464000/481450]\n",
      "loss: 0.297501  [467200/481450]\n",
      "loss: 0.315968  [470400/481450]\n",
      "loss: 0.353895  [473600/481450]\n",
      "loss: 0.296512  [476800/481450]\n",
      "loss: 0.303326  [480000/481450]\n",
      "Train Accuracy: 84.5573%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.811996, F1-score: 79.96% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.239192  [    0/481450]\n",
      "loss: 0.366300  [ 3200/481450]\n",
      "loss: 0.177258  [ 6400/481450]\n",
      "loss: 0.714126  [ 9600/481450]\n",
      "loss: 0.322908  [12800/481450]\n",
      "loss: 0.383352  [16000/481450]\n",
      "loss: 0.503995  [19200/481450]\n",
      "loss: 0.287172  [22400/481450]\n",
      "loss: 0.512793  [25600/481450]\n",
      "loss: 0.510639  [28800/481450]\n",
      "loss: 0.546862  [32000/481450]\n",
      "loss: 0.263758  [35200/481450]\n",
      "loss: 0.191515  [38400/481450]\n",
      "loss: 0.472446  [41600/481450]\n",
      "loss: 0.485314  [44800/481450]\n",
      "loss: 0.277787  [48000/481450]\n",
      "loss: 0.282423  [51200/481450]\n",
      "loss: 0.313429  [54400/481450]\n",
      "loss: 0.376169  [57600/481450]\n",
      "loss: 0.258315  [60800/481450]\n",
      "loss: 0.417206  [64000/481450]\n",
      "loss: 0.381193  [67200/481450]\n",
      "loss: 0.122945  [70400/481450]\n",
      "loss: 0.354251  [73600/481450]\n",
      "loss: 0.381391  [76800/481450]\n",
      "loss: 0.566859  [80000/481450]\n",
      "loss: 0.581050  [83200/481450]\n",
      "loss: 0.338000  [86400/481450]\n",
      "loss: 0.461926  [89600/481450]\n",
      "loss: 0.349149  [92800/481450]\n",
      "loss: 0.226273  [96000/481450]\n",
      "loss: 0.328814  [99200/481450]\n",
      "loss: 0.336865  [102400/481450]\n",
      "loss: 0.501783  [105600/481450]\n",
      "loss: 0.444101  [108800/481450]\n",
      "loss: 0.715011  [112000/481450]\n",
      "loss: 0.366202  [115200/481450]\n",
      "loss: 0.210532  [118400/481450]\n",
      "loss: 0.286412  [121600/481450]\n",
      "loss: 0.354037  [124800/481450]\n",
      "loss: 0.221723  [128000/481450]\n",
      "loss: 0.344946  [131200/481450]\n",
      "loss: 0.221336  [134400/481450]\n",
      "loss: 0.468167  [137600/481450]\n",
      "loss: 0.353677  [140800/481450]\n",
      "loss: 0.224149  [144000/481450]\n",
      "loss: 0.278066  [147200/481450]\n",
      "loss: 0.348167  [150400/481450]\n",
      "loss: 0.544758  [153600/481450]\n",
      "loss: 0.404894  [156800/481450]\n",
      "loss: 0.320656  [160000/481450]\n",
      "loss: 0.603934  [163200/481450]\n",
      "loss: 0.653088  [166400/481450]\n",
      "loss: 0.448094  [169600/481450]\n",
      "loss: 0.431476  [172800/481450]\n",
      "loss: 0.466896  [176000/481450]\n",
      "loss: 0.416902  [179200/481450]\n",
      "loss: 0.183760  [182400/481450]\n",
      "loss: 0.238624  [185600/481450]\n",
      "loss: 0.349842  [188800/481450]\n",
      "loss: 0.180840  [192000/481450]\n",
      "loss: 0.231598  [195200/481450]\n",
      "loss: 0.365321  [198400/481450]\n",
      "loss: 0.225660  [201600/481450]\n",
      "loss: 0.355034  [204800/481450]\n",
      "loss: 0.414430  [208000/481450]\n",
      "loss: 0.109930  [211200/481450]\n",
      "loss: 0.412229  [214400/481450]\n",
      "loss: 0.345114  [217600/481450]\n",
      "loss: 0.353190  [220800/481450]\n",
      "loss: 0.596220  [224000/481450]\n",
      "loss: 0.582473  [227200/481450]\n",
      "loss: 0.294048  [230400/481450]\n",
      "loss: 0.260967  [233600/481450]\n",
      "loss: 0.415496  [236800/481450]\n",
      "loss: 0.361836  [240000/481450]\n",
      "loss: 0.378402  [243200/481450]\n",
      "loss: 0.550943  [246400/481450]\n",
      "loss: 0.273852  [249600/481450]\n",
      "loss: 0.360063  [252800/481450]\n",
      "loss: 0.391107  [256000/481450]\n",
      "loss: 0.337410  [259200/481450]\n",
      "loss: 0.303402  [262400/481450]\n",
      "loss: 0.573124  [265600/481450]\n",
      "loss: 0.464645  [268800/481450]\n",
      "loss: 0.366693  [272000/481450]\n",
      "loss: 0.302659  [275200/481450]\n",
      "loss: 0.266604  [278400/481450]\n",
      "loss: 0.505580  [281600/481450]\n",
      "loss: 0.356431  [284800/481450]\n",
      "loss: 0.450527  [288000/481450]\n",
      "loss: 0.246329  [291200/481450]\n",
      "loss: 0.460383  [294400/481450]\n",
      "loss: 0.280143  [297600/481450]\n",
      "loss: 0.386674  [300800/481450]\n",
      "loss: 0.278365  [304000/481450]\n",
      "loss: 0.393918  [307200/481450]\n",
      "loss: 0.246421  [310400/481450]\n",
      "loss: 0.326343  [313600/481450]\n",
      "loss: 0.435965  [316800/481450]\n",
      "loss: 0.254165  [320000/481450]\n",
      "loss: 0.381016  [323200/481450]\n",
      "loss: 0.312315  [326400/481450]\n",
      "loss: 0.400338  [329600/481450]\n",
      "loss: 0.298564  [332800/481450]\n",
      "loss: 0.516251  [336000/481450]\n",
      "loss: 0.431648  [339200/481450]\n",
      "loss: 0.618115  [342400/481450]\n",
      "loss: 0.241977  [345600/481450]\n",
      "loss: 0.167955  [348800/481450]\n",
      "loss: 0.159823  [352000/481450]\n",
      "loss: 0.258205  [355200/481450]\n",
      "loss: 0.191538  [358400/481450]\n",
      "loss: 0.382853  [361600/481450]\n",
      "loss: 0.283410  [364800/481450]\n",
      "loss: 0.324253  [368000/481450]\n",
      "loss: 0.332723  [371200/481450]\n",
      "loss: 0.314537  [374400/481450]\n",
      "loss: 0.370949  [377600/481450]\n",
      "loss: 0.135440  [380800/481450]\n",
      "loss: 0.437877  [384000/481450]\n",
      "loss: 0.347712  [387200/481450]\n",
      "loss: 0.180870  [390400/481450]\n",
      "loss: 0.296337  [393600/481450]\n",
      "loss: 0.296500  [396800/481450]\n",
      "loss: 0.327763  [400000/481450]\n",
      "loss: 0.222641  [403200/481450]\n",
      "loss: 0.247930  [406400/481450]\n",
      "loss: 0.279854  [409600/481450]\n",
      "loss: 0.385986  [412800/481450]\n",
      "loss: 0.201703  [416000/481450]\n",
      "loss: 0.342590  [419200/481450]\n",
      "loss: 0.485097  [422400/481450]\n",
      "loss: 0.451371  [425600/481450]\n",
      "loss: 0.628112  [428800/481450]\n",
      "loss: 0.214884  [432000/481450]\n",
      "loss: 0.602448  [435200/481450]\n",
      "loss: 0.044693  [438400/481450]\n",
      "loss: 0.342837  [441600/481450]\n",
      "loss: 0.323808  [444800/481450]\n",
      "loss: 0.683302  [448000/481450]\n",
      "loss: 0.345833  [451200/481450]\n",
      "loss: 0.272098  [454400/481450]\n",
      "loss: 0.380026  [457600/481450]\n",
      "loss: 0.391500  [460800/481450]\n",
      "loss: 0.438593  [464000/481450]\n",
      "loss: 0.400398  [467200/481450]\n",
      "loss: 0.303408  [470400/481450]\n",
      "loss: 0.356970  [473600/481450]\n",
      "loss: 0.429435  [476800/481450]\n",
      "loss: 0.163699  [480000/481450]\n",
      "Train Accuracy: 85.7483%\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.822251, F1-score: 80.87% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.274851  [    0/481450]\n",
      "loss: 0.227143  [ 3200/481450]\n",
      "loss: 0.286221  [ 6400/481450]\n",
      "loss: 0.359536  [ 9600/481450]\n",
      "loss: 0.412644  [12800/481450]\n",
      "loss: 0.607048  [16000/481450]\n",
      "loss: 0.184201  [19200/481450]\n",
      "loss: 0.280082  [22400/481450]\n",
      "loss: 0.279483  [25600/481450]\n",
      "loss: 0.323288  [28800/481450]\n",
      "loss: 0.287587  [32000/481450]\n",
      "loss: 0.190525  [35200/481450]\n",
      "loss: 0.140478  [38400/481450]\n",
      "loss: 0.398756  [41600/481450]\n",
      "loss: 0.248287  [44800/481450]\n",
      "loss: 0.379522  [48000/481450]\n",
      "loss: 0.255665  [51200/481450]\n",
      "loss: 0.169422  [54400/481450]\n",
      "loss: 0.440934  [57600/481450]\n",
      "loss: 0.306924  [60800/481450]\n",
      "loss: 0.367311  [64000/481450]\n",
      "loss: 0.387948  [67200/481450]\n",
      "loss: 0.320224  [70400/481450]\n",
      "loss: 0.292507  [73600/481450]\n",
      "loss: 0.273556  [76800/481450]\n",
      "loss: 0.357887  [80000/481450]\n",
      "loss: 0.380741  [83200/481450]\n",
      "loss: 0.235292  [86400/481450]\n",
      "loss: 0.323773  [89600/481450]\n",
      "loss: 0.278276  [92800/481450]\n",
      "loss: 0.317258  [96000/481450]\n",
      "loss: 0.451004  [99200/481450]\n",
      "loss: 0.369339  [102400/481450]\n",
      "loss: 0.242162  [105600/481450]\n",
      "loss: 0.238101  [108800/481450]\n",
      "loss: 0.342211  [112000/481450]\n",
      "loss: 0.389430  [115200/481450]\n",
      "loss: 0.343791  [118400/481450]\n",
      "loss: 0.362546  [121600/481450]\n",
      "loss: 0.196389  [124800/481450]\n",
      "loss: 0.711174  [128000/481450]\n",
      "loss: 0.324288  [131200/481450]\n",
      "loss: 0.236575  [134400/481450]\n",
      "loss: 0.354344  [137600/481450]\n",
      "loss: 0.469032  [140800/481450]\n",
      "loss: 0.632998  [144000/481450]\n",
      "loss: 0.476119  [147200/481450]\n",
      "loss: 0.366150  [150400/481450]\n",
      "loss: 0.269293  [153600/481450]\n",
      "loss: 0.372102  [156800/481450]\n",
      "loss: 0.451899  [160000/481450]\n",
      "loss: 0.423510  [163200/481450]\n",
      "loss: 0.194072  [166400/481450]\n",
      "loss: 0.390356  [169600/481450]\n",
      "loss: 0.428107  [172800/481450]\n",
      "loss: 0.378927  [176000/481450]\n",
      "loss: 0.637119  [179200/481450]\n",
      "loss: 0.240568  [182400/481450]\n",
      "loss: 0.484042  [185600/481450]\n",
      "loss: 0.290518  [188800/481450]\n",
      "loss: 0.451415  [192000/481450]\n",
      "loss: 0.357440  [195200/481450]\n",
      "loss: 0.503948  [198400/481450]\n",
      "loss: 0.413113  [201600/481450]\n",
      "loss: 0.311955  [204800/481450]\n",
      "loss: 0.374946  [208000/481450]\n",
      "loss: 0.192614  [211200/481450]\n",
      "loss: 0.291376  [214400/481450]\n",
      "loss: 0.555063  [217600/481450]\n",
      "loss: 0.547239  [220800/481450]\n",
      "loss: 0.338166  [224000/481450]\n",
      "loss: 0.442819  [227200/481450]\n",
      "loss: 0.202642  [230400/481450]\n",
      "loss: 0.186799  [233600/481450]\n",
      "loss: 0.285941  [236800/481450]\n",
      "loss: 0.255879  [240000/481450]\n",
      "loss: 0.123759  [243200/481450]\n",
      "loss: 0.434165  [246400/481450]\n",
      "loss: 0.580291  [249600/481450]\n",
      "loss: 0.274573  [252800/481450]\n",
      "loss: 0.268281  [256000/481450]\n",
      "loss: 0.294056  [259200/481450]\n",
      "loss: 0.277457  [262400/481450]\n",
      "loss: 0.378392  [265600/481450]\n",
      "loss: 0.392663  [268800/481450]\n",
      "loss: 0.398515  [272000/481450]\n",
      "loss: 0.441296  [275200/481450]\n",
      "loss: 0.368957  [278400/481450]\n",
      "loss: 0.533911  [281600/481450]\n",
      "loss: 0.580036  [284800/481450]\n",
      "loss: 0.399725  [288000/481450]\n",
      "loss: 0.235851  [291200/481450]\n",
      "loss: 0.457086  [294400/481450]\n",
      "loss: 0.339689  [297600/481450]\n",
      "loss: 0.330250  [300800/481450]\n",
      "loss: 0.246596  [304000/481450]\n",
      "loss: 0.274566  [307200/481450]\n",
      "loss: 0.195511  [310400/481450]\n",
      "loss: 0.268706  [313600/481450]\n",
      "loss: 0.214741  [316800/481450]\n",
      "loss: 0.261267  [320000/481450]\n",
      "loss: 0.374924  [323200/481450]\n",
      "loss: 0.543343  [326400/481450]\n",
      "loss: 0.329501  [329600/481450]\n",
      "loss: 0.424433  [332800/481450]\n",
      "loss: 0.221627  [336000/481450]\n",
      "loss: 0.164426  [339200/481450]\n",
      "loss: 0.439470  [342400/481450]\n",
      "loss: 0.298788  [345600/481450]\n",
      "loss: 0.187420  [348800/481450]\n",
      "loss: 0.347690  [352000/481450]\n",
      "loss: 0.384815  [355200/481450]\n",
      "loss: 0.207166  [358400/481450]\n",
      "loss: 0.336369  [361600/481450]\n",
      "loss: 0.369853  [364800/481450]\n",
      "loss: 0.479487  [368000/481450]\n",
      "loss: 0.261158  [371200/481450]\n",
      "loss: 0.270299  [374400/481450]\n",
      "loss: 0.236649  [377600/481450]\n",
      "loss: 0.234933  [380800/481450]\n",
      "loss: 0.350414  [384000/481450]\n",
      "loss: 0.227115  [387200/481450]\n",
      "loss: 0.218802  [390400/481450]\n",
      "loss: 0.496048  [393600/481450]\n",
      "loss: 0.393474  [396800/481450]\n",
      "loss: 0.277765  [400000/481450]\n",
      "loss: 0.600083  [403200/481450]\n",
      "loss: 0.266296  [406400/481450]\n",
      "loss: 0.660802  [409600/481450]\n",
      "loss: 0.615044  [412800/481450]\n",
      "loss: 0.210487  [416000/481450]\n",
      "loss: 0.238900  [419200/481450]\n",
      "loss: 0.118611  [422400/481450]\n",
      "loss: 0.186113  [425600/481450]\n",
      "loss: 0.353085  [428800/481450]\n",
      "loss: 0.422882  [432000/481450]\n",
      "loss: 0.158240  [435200/481450]\n",
      "loss: 0.167720  [438400/481450]\n",
      "loss: 0.409457  [441600/481450]\n",
      "loss: 0.649205  [444800/481450]\n",
      "loss: 0.461713  [448000/481450]\n",
      "loss: 0.430338  [451200/481450]\n",
      "loss: 0.654396  [454400/481450]\n",
      "loss: 0.346144  [457600/481450]\n",
      "loss: 0.470010  [460800/481450]\n",
      "loss: 0.215812  [464000/481450]\n",
      "loss: 0.382284  [467200/481450]\n",
      "loss: 0.222224  [470400/481450]\n",
      "loss: 0.419452  [473600/481450]\n",
      "loss: 0.264493  [476800/481450]\n",
      "loss: 0.151025  [480000/481450]\n",
      "Train Accuracy: 86.6470%\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.830657, F1-score: 81.82% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.449436  [    0/481450]\n",
      "loss: 0.410520  [ 3200/481450]\n",
      "loss: 0.520203  [ 6400/481450]\n",
      "loss: 0.489165  [ 9600/481450]\n",
      "loss: 0.190378  [12800/481450]\n",
      "loss: 0.285093  [16000/481450]\n",
      "loss: 0.208771  [19200/481450]\n",
      "loss: 0.404630  [22400/481450]\n",
      "loss: 0.176654  [25600/481450]\n",
      "loss: 0.181718  [28800/481450]\n",
      "loss: 0.170955  [32000/481450]\n",
      "loss: 0.452860  [35200/481450]\n",
      "loss: 0.246462  [38400/481450]\n",
      "loss: 0.192578  [41600/481450]\n",
      "loss: 0.432067  [44800/481450]\n",
      "loss: 0.488361  [48000/481450]\n",
      "loss: 0.415225  [51200/481450]\n",
      "loss: 0.376209  [54400/481450]\n",
      "loss: 0.259226  [57600/481450]\n",
      "loss: 0.394715  [60800/481450]\n",
      "loss: 0.292737  [64000/481450]\n",
      "loss: 0.227282  [67200/481450]\n",
      "loss: 0.317435  [70400/481450]\n",
      "loss: 0.308801  [73600/481450]\n",
      "loss: 0.136800  [76800/481450]\n",
      "loss: 0.148773  [80000/481450]\n",
      "loss: 0.421967  [83200/481450]\n",
      "loss: 0.242489  [86400/481450]\n",
      "loss: 0.266654  [89600/481450]\n",
      "loss: 0.532742  [92800/481450]\n",
      "loss: 0.321958  [96000/481450]\n",
      "loss: 0.213777  [99200/481450]\n",
      "loss: 0.394337  [102400/481450]\n",
      "loss: 0.373404  [105600/481450]\n",
      "loss: 0.214227  [108800/481450]\n",
      "loss: 0.383873  [112000/481450]\n",
      "loss: 0.253668  [115200/481450]\n",
      "loss: 0.447206  [118400/481450]\n",
      "loss: 0.399787  [121600/481450]\n",
      "loss: 0.100947  [124800/481450]\n",
      "loss: 0.285169  [128000/481450]\n",
      "loss: 0.191793  [131200/481450]\n",
      "loss: 0.302142  [134400/481450]\n",
      "loss: 0.247346  [137600/481450]\n",
      "loss: 0.380260  [140800/481450]\n",
      "loss: 0.221342  [144000/481450]\n",
      "loss: 0.241039  [147200/481450]\n",
      "loss: 0.354948  [150400/481450]\n",
      "loss: 0.210445  [153600/481450]\n",
      "loss: 0.414381  [156800/481450]\n",
      "loss: 0.381417  [160000/481450]\n",
      "loss: 0.273363  [163200/481450]\n",
      "loss: 0.645460  [166400/481450]\n",
      "loss: 0.164560  [169600/481450]\n",
      "loss: 0.415295  [172800/481450]\n",
      "loss: 0.452834  [176000/481450]\n",
      "loss: 0.269956  [179200/481450]\n",
      "loss: 0.411472  [182400/481450]\n",
      "loss: 0.281939  [185600/481450]\n",
      "loss: 0.312781  [188800/481450]\n",
      "loss: 0.376521  [192000/481450]\n",
      "loss: 0.382840  [195200/481450]\n",
      "loss: 0.364971  [198400/481450]\n",
      "loss: 0.407982  [201600/481450]\n",
      "loss: 0.309273  [204800/481450]\n",
      "loss: 0.420530  [208000/481450]\n",
      "loss: 0.266091  [211200/481450]\n",
      "loss: 0.305050  [214400/481450]\n",
      "loss: 0.238424  [217600/481450]\n",
      "loss: 0.241090  [220800/481450]\n",
      "loss: 0.199530  [224000/481450]\n",
      "loss: 0.402483  [227200/481450]\n",
      "loss: 0.247466  [230400/481450]\n",
      "loss: 0.226377  [233600/481450]\n",
      "loss: 0.179627  [236800/481450]\n",
      "loss: 0.198176  [240000/481450]\n",
      "loss: 0.191589  [243200/481450]\n",
      "loss: 0.375238  [246400/481450]\n",
      "loss: 0.360097  [249600/481450]\n",
      "loss: 0.251204  [252800/481450]\n",
      "loss: 0.268961  [256000/481450]\n",
      "loss: 0.249072  [259200/481450]\n",
      "loss: 0.313088  [262400/481450]\n",
      "loss: 0.425463  [265600/481450]\n",
      "loss: 0.259339  [268800/481450]\n",
      "loss: 0.292841  [272000/481450]\n",
      "loss: 0.554170  [275200/481450]\n",
      "loss: 0.307973  [278400/481450]\n",
      "loss: 0.526412  [281600/481450]\n",
      "loss: 0.596878  [284800/481450]\n",
      "loss: 0.307934  [288000/481450]\n",
      "loss: 0.606692  [291200/481450]\n",
      "loss: 0.502376  [294400/481450]\n",
      "loss: 0.514911  [297600/481450]\n",
      "loss: 0.333996  [300800/481450]\n",
      "loss: 0.334260  [304000/481450]\n",
      "loss: 0.304920  [307200/481450]\n",
      "loss: 0.404366  [310400/481450]\n",
      "loss: 0.188887  [313600/481450]\n",
      "loss: 0.240812  [316800/481450]\n",
      "loss: 0.416513  [320000/481450]\n",
      "loss: 0.258617  [323200/481450]\n",
      "loss: 0.218875  [326400/481450]\n",
      "loss: 0.367505  [329600/481450]\n",
      "loss: 0.581178  [332800/481450]\n",
      "loss: 0.688232  [336000/481450]\n",
      "loss: 0.387182  [339200/481450]\n",
      "loss: 0.206057  [342400/481450]\n",
      "loss: 0.289631  [345600/481450]\n",
      "loss: 0.230550  [348800/481450]\n",
      "loss: 0.193746  [352000/481450]\n",
      "loss: 0.429839  [355200/481450]\n",
      "loss: 0.253235  [358400/481450]\n",
      "loss: 0.376367  [361600/481450]\n",
      "loss: 0.418304  [364800/481450]\n",
      "loss: 0.346390  [368000/481450]\n",
      "loss: 0.228743  [371200/481450]\n",
      "loss: 0.471813  [374400/481450]\n",
      "loss: 0.165781  [377600/481450]\n",
      "loss: 0.234296  [380800/481450]\n",
      "loss: 0.242947  [384000/481450]\n",
      "loss: 0.254290  [387200/481450]\n",
      "loss: 0.573087  [390400/481450]\n",
      "loss: 0.282434  [393600/481450]\n",
      "loss: 0.307945  [396800/481450]\n",
      "loss: 0.475868  [400000/481450]\n",
      "loss: 0.330808  [403200/481450]\n",
      "loss: 0.353982  [406400/481450]\n",
      "loss: 0.333320  [409600/481450]\n",
      "loss: 0.749790  [412800/481450]\n",
      "loss: 0.237670  [416000/481450]\n",
      "loss: 0.479063  [419200/481450]\n",
      "loss: 0.234558  [422400/481450]\n",
      "loss: 0.398511  [425600/481450]\n",
      "loss: 0.169065  [428800/481450]\n",
      "loss: 0.398115  [432000/481450]\n",
      "loss: 0.285045  [435200/481450]\n",
      "loss: 0.650078  [438400/481450]\n",
      "loss: 0.433061  [441600/481450]\n",
      "loss: 0.391671  [444800/481450]\n",
      "loss: 0.353189  [448000/481450]\n",
      "loss: 0.133013  [451200/481450]\n",
      "loss: 0.194990  [454400/481450]\n",
      "loss: 0.386959  [457600/481450]\n",
      "loss: 0.166667  [460800/481450]\n",
      "loss: 0.233728  [464000/481450]\n",
      "loss: 0.325365  [467200/481450]\n",
      "loss: 0.360098  [470400/481450]\n",
      "loss: 0.186762  [473600/481450]\n",
      "loss: 0.346984  [476800/481450]\n",
      "loss: 0.442719  [480000/481450]\n",
      "Train Accuracy: 87.1881%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.777186, F1-score: 83.00% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.243218  [    0/481450]\n",
      "loss: 0.369209  [ 3200/481450]\n",
      "loss: 0.228891  [ 6400/481450]\n",
      "loss: 0.264959  [ 9600/481450]\n",
      "loss: 0.366954  [12800/481450]\n",
      "loss: 0.143270  [16000/481450]\n",
      "loss: 0.398916  [19200/481450]\n",
      "loss: 0.200308  [22400/481450]\n",
      "loss: 0.190592  [25600/481450]\n",
      "loss: 0.392712  [28800/481450]\n",
      "loss: 0.777182  [32000/481450]\n",
      "loss: 0.299634  [35200/481450]\n",
      "loss: 0.346268  [38400/481450]\n",
      "loss: 0.334023  [41600/481450]\n",
      "loss: 0.416363  [44800/481450]\n",
      "loss: 0.179015  [48000/481450]\n",
      "loss: 0.259911  [51200/481450]\n",
      "loss: 0.180337  [54400/481450]\n",
      "loss: 0.400783  [57600/481450]\n",
      "loss: 0.176370  [60800/481450]\n",
      "loss: 0.545610  [64000/481450]\n",
      "loss: 0.074071  [67200/481450]\n",
      "loss: 0.466075  [70400/481450]\n",
      "loss: 0.238593  [73600/481450]\n",
      "loss: 0.608699  [76800/481450]\n",
      "loss: 0.408699  [80000/481450]\n",
      "loss: 0.208551  [83200/481450]\n",
      "loss: 0.219280  [86400/481450]\n",
      "loss: 0.245330  [89600/481450]\n",
      "loss: 0.349061  [92800/481450]\n",
      "loss: 0.175477  [96000/481450]\n",
      "loss: 0.195992  [99200/481450]\n",
      "loss: 0.290650  [102400/481450]\n",
      "loss: 0.278567  [105600/481450]\n",
      "loss: 0.546165  [108800/481450]\n",
      "loss: 0.289493  [112000/481450]\n",
      "loss: 0.159788  [115200/481450]\n",
      "loss: 0.325543  [118400/481450]\n",
      "loss: 0.303807  [121600/481450]\n",
      "loss: 0.376658  [124800/481450]\n",
      "loss: 0.454727  [128000/481450]\n",
      "loss: 0.333830  [131200/481450]\n",
      "loss: 0.296728  [134400/481450]\n",
      "loss: 0.220515  [137600/481450]\n",
      "loss: 0.228004  [140800/481450]\n",
      "loss: 0.073667  [144000/481450]\n",
      "loss: 0.361114  [147200/481450]\n",
      "loss: 0.361256  [150400/481450]\n",
      "loss: 0.241537  [153600/481450]\n",
      "loss: 0.635264  [156800/481450]\n",
      "loss: 0.306199  [160000/481450]\n",
      "loss: 0.602433  [163200/481450]\n",
      "loss: 0.338954  [166400/481450]\n",
      "loss: 0.144359  [169600/481450]\n",
      "loss: 0.672121  [172800/481450]\n",
      "loss: 0.180323  [176000/481450]\n",
      "loss: 0.159831  [179200/481450]\n",
      "loss: 0.242805  [182400/481450]\n",
      "loss: 0.195854  [185600/481450]\n",
      "loss: 0.170234  [188800/481450]\n",
      "loss: 0.508066  [192000/481450]\n",
      "loss: 0.345725  [195200/481450]\n",
      "loss: 0.201197  [198400/481450]\n",
      "loss: 0.189476  [201600/481450]\n",
      "loss: 0.120657  [204800/481450]\n",
      "loss: 0.491196  [208000/481450]\n",
      "loss: 0.233251  [211200/481450]\n",
      "loss: 0.637968  [214400/481450]\n",
      "loss: 0.296468  [217600/481450]\n",
      "loss: 0.353169  [220800/481450]\n",
      "loss: 0.216433  [224000/481450]\n",
      "loss: 0.310944  [227200/481450]\n",
      "loss: 0.136789  [230400/481450]\n",
      "loss: 0.238867  [233600/481450]\n",
      "loss: 0.169708  [236800/481450]\n",
      "loss: 0.439528  [240000/481450]\n",
      "loss: 0.403471  [243200/481450]\n",
      "loss: 0.457478  [246400/481450]\n",
      "loss: 0.344358  [249600/481450]\n",
      "loss: 0.234361  [252800/481450]\n",
      "loss: 0.346001  [256000/481450]\n",
      "loss: 0.213332  [259200/481450]\n",
      "loss: 0.206798  [262400/481450]\n",
      "loss: 0.329675  [265600/481450]\n",
      "loss: 0.288834  [268800/481450]\n",
      "loss: 0.374268  [272000/481450]\n",
      "loss: 0.096566  [275200/481450]\n",
      "loss: 0.290817  [278400/481450]\n",
      "loss: 0.365403  [281600/481450]\n",
      "loss: 0.098869  [284800/481450]\n",
      "loss: 0.597401  [288000/481450]\n",
      "loss: 0.420876  [291200/481450]\n",
      "loss: 0.175082  [294400/481450]\n",
      "loss: 0.159187  [297600/481450]\n",
      "loss: 0.385490  [300800/481450]\n",
      "loss: 0.334906  [304000/481450]\n",
      "loss: 0.304412  [307200/481450]\n",
      "loss: 0.211804  [310400/481450]\n",
      "loss: 0.345480  [313600/481450]\n",
      "loss: 0.459562  [316800/481450]\n",
      "loss: 0.307322  [320000/481450]\n",
      "loss: 0.294606  [323200/481450]\n",
      "loss: 0.174438  [326400/481450]\n",
      "loss: 0.288859  [329600/481450]\n",
      "loss: 0.418397  [332800/481450]\n",
      "loss: 0.535806  [336000/481450]\n",
      "loss: 0.243713  [339200/481450]\n",
      "loss: 0.415780  [342400/481450]\n",
      "loss: 0.439013  [345600/481450]\n",
      "loss: 0.306570  [348800/481450]\n",
      "loss: 0.353510  [352000/481450]\n",
      "loss: 0.188624  [355200/481450]\n",
      "loss: 0.517902  [358400/481450]\n",
      "loss: 0.143118  [361600/481450]\n",
      "loss: 0.214913  [364800/481450]\n",
      "loss: 0.340114  [368000/481450]\n",
      "loss: 0.280254  [371200/481450]\n",
      "loss: 0.482955  [374400/481450]\n",
      "loss: 0.419895  [377600/481450]\n",
      "loss: 0.439766  [380800/481450]\n",
      "loss: 0.156882  [384000/481450]\n",
      "loss: 0.187330  [387200/481450]\n",
      "loss: 0.475834  [390400/481450]\n",
      "loss: 0.228059  [393600/481450]\n",
      "loss: 0.409962  [396800/481450]\n",
      "loss: 0.261554  [400000/481450]\n",
      "loss: 0.263748  [403200/481450]\n",
      "loss: 0.246019  [406400/481450]\n",
      "loss: 0.301628  [409600/481450]\n",
      "loss: 0.647365  [412800/481450]\n",
      "loss: 0.309654  [416000/481450]\n",
      "loss: 0.398439  [419200/481450]\n",
      "loss: 0.388824  [422400/481450]\n",
      "loss: 0.304317  [425600/481450]\n",
      "loss: 0.355299  [428800/481450]\n",
      "loss: 0.212736  [432000/481450]\n",
      "loss: 0.217279  [435200/481450]\n",
      "loss: 0.531699  [438400/481450]\n",
      "loss: 0.305655  [441600/481450]\n",
      "loss: 0.520802  [444800/481450]\n",
      "loss: 0.231874  [448000/481450]\n",
      "loss: 0.299195  [451200/481450]\n",
      "loss: 0.422941  [454400/481450]\n",
      "loss: 0.463765  [457600/481450]\n",
      "loss: 0.314356  [460800/481450]\n",
      "loss: 0.367191  [464000/481450]\n",
      "loss: 0.281521  [467200/481450]\n",
      "loss: 0.315729  [470400/481450]\n",
      "loss: 0.511571  [473600/481450]\n",
      "loss: 0.487159  [476800/481450]\n",
      "loss: 0.251518  [480000/481450]\n",
      "Train Accuracy: 87.5325%\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.729520, F1-score: 84.19% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.338627  [    0/481450]\n",
      "loss: 0.341250  [ 3200/481450]\n",
      "loss: 0.249033  [ 6400/481450]\n",
      "loss: 0.607668  [ 9600/481450]\n",
      "loss: 0.354185  [12800/481450]\n",
      "loss: 0.133979  [16000/481450]\n",
      "loss: 0.221783  [19200/481450]\n",
      "loss: 0.127117  [22400/481450]\n",
      "loss: 0.209898  [25600/481450]\n",
      "loss: 0.131518  [28800/481450]\n",
      "loss: 0.278885  [32000/481450]\n",
      "loss: 0.063491  [35200/481450]\n",
      "loss: 0.106896  [38400/481450]\n",
      "loss: 0.409613  [41600/481450]\n",
      "loss: 0.221211  [44800/481450]\n",
      "loss: 0.254386  [48000/481450]\n",
      "loss: 0.432795  [51200/481450]\n",
      "loss: 0.141913  [54400/481450]\n",
      "loss: 0.345983  [57600/481450]\n",
      "loss: 0.429552  [60800/481450]\n",
      "loss: 0.345305  [64000/481450]\n",
      "loss: 0.229039  [67200/481450]\n",
      "loss: 0.255003  [70400/481450]\n",
      "loss: 0.239338  [73600/481450]\n",
      "loss: 0.415767  [76800/481450]\n",
      "loss: 0.293386  [80000/481450]\n",
      "loss: 0.200443  [83200/481450]\n",
      "loss: 0.316248  [86400/481450]\n",
      "loss: 0.277776  [89600/481450]\n",
      "loss: 0.113361  [92800/481450]\n",
      "loss: 0.192135  [96000/481450]\n",
      "loss: 0.266495  [99200/481450]\n",
      "loss: 0.241461  [102400/481450]\n",
      "loss: 0.241273  [105600/481450]\n",
      "loss: 0.379910  [108800/481450]\n",
      "loss: 0.231353  [112000/481450]\n",
      "loss: 0.221939  [115200/481450]\n",
      "loss: 0.350047  [118400/481450]\n",
      "loss: 0.515080  [121600/481450]\n",
      "loss: 0.417632  [124800/481450]\n",
      "loss: 0.221681  [128000/481450]\n",
      "loss: 0.212533  [131200/481450]\n",
      "loss: 0.357285  [134400/481450]\n",
      "loss: 0.252453  [137600/481450]\n",
      "loss: 0.384512  [140800/481450]\n",
      "loss: 0.158682  [144000/481450]\n",
      "loss: 0.674349  [147200/481450]\n",
      "loss: 0.448884  [150400/481450]\n",
      "loss: 0.201838  [153600/481450]\n",
      "loss: 0.252540  [156800/481450]\n",
      "loss: 0.208407  [160000/481450]\n",
      "loss: 0.506205  [163200/481450]\n",
      "loss: 0.279541  [166400/481450]\n",
      "loss: 0.301093  [169600/481450]\n",
      "loss: 0.138296  [172800/481450]\n",
      "loss: 0.497400  [176000/481450]\n",
      "loss: 0.311511  [179200/481450]\n",
      "loss: 0.415889  [182400/481450]\n",
      "loss: 0.172792  [185600/481450]\n",
      "loss: 0.297251  [188800/481450]\n",
      "loss: 0.294551  [192000/481450]\n",
      "loss: 0.229489  [195200/481450]\n",
      "loss: 0.217189  [198400/481450]\n",
      "loss: 0.284097  [201600/481450]\n",
      "loss: 0.089502  [204800/481450]\n",
      "loss: 0.217689  [208000/481450]\n",
      "loss: 0.329952  [211200/481450]\n",
      "loss: 0.254947  [214400/481450]\n",
      "loss: 0.382506  [217600/481450]\n",
      "loss: 0.378531  [220800/481450]\n",
      "loss: 0.214557  [224000/481450]\n",
      "loss: 0.226072  [227200/481450]\n",
      "loss: 0.531792  [230400/481450]\n",
      "loss: 0.384952  [233600/481450]\n",
      "loss: 0.226396  [236800/481450]\n",
      "loss: 0.503561  [240000/481450]\n",
      "loss: 0.268523  [243200/481450]\n",
      "loss: 0.373597  [246400/481450]\n",
      "loss: 0.203304  [249600/481450]\n",
      "loss: 0.288680  [252800/481450]\n",
      "loss: 0.258949  [256000/481450]\n",
      "loss: 0.519735  [259200/481450]\n",
      "loss: 0.254519  [262400/481450]\n",
      "loss: 0.087163  [265600/481450]\n",
      "loss: 0.298857  [268800/481450]\n",
      "loss: 0.301082  [272000/481450]\n",
      "loss: 0.278621  [275200/481450]\n",
      "loss: 0.244144  [278400/481450]\n",
      "loss: 0.122959  [281600/481450]\n",
      "loss: 0.452200  [284800/481450]\n",
      "loss: 0.170593  [288000/481450]\n",
      "loss: 0.260446  [291200/481450]\n",
      "loss: 0.254721  [294400/481450]\n",
      "loss: 0.218733  [297600/481450]\n",
      "loss: 0.226140  [300800/481450]\n",
      "loss: 0.350521  [304000/481450]\n",
      "loss: 0.267893  [307200/481450]\n",
      "loss: 0.426531  [310400/481450]\n",
      "loss: 0.357529  [313600/481450]\n",
      "loss: 0.154970  [316800/481450]\n",
      "loss: 0.159670  [320000/481450]\n",
      "loss: 0.231440  [323200/481450]\n",
      "loss: 0.202911  [326400/481450]\n",
      "loss: 0.138087  [329600/481450]\n",
      "loss: 0.143045  [332800/481450]\n",
      "loss: 0.253451  [336000/481450]\n",
      "loss: 0.511351  [339200/481450]\n",
      "loss: 0.309040  [342400/481450]\n",
      "loss: 0.297444  [345600/481450]\n",
      "loss: 0.437395  [348800/481450]\n",
      "loss: 0.262887  [352000/481450]\n",
      "loss: 0.510890  [355200/481450]\n",
      "loss: 0.201053  [358400/481450]\n",
      "loss: 0.243941  [361600/481450]\n",
      "loss: 0.383655  [364800/481450]\n",
      "loss: 0.116279  [368000/481450]\n",
      "loss: 0.153631  [371200/481450]\n",
      "loss: 0.269260  [374400/481450]\n",
      "loss: 0.491734  [377600/481450]\n",
      "loss: 0.393323  [380800/481450]\n",
      "loss: 0.284969  [384000/481450]\n",
      "loss: 0.465997  [387200/481450]\n",
      "loss: 0.312116  [390400/481450]\n",
      "loss: 0.308177  [393600/481450]\n",
      "loss: 0.237114  [396800/481450]\n",
      "loss: 0.334355  [400000/481450]\n",
      "loss: 0.319476  [403200/481450]\n",
      "loss: 0.359626  [406400/481450]\n",
      "loss: 0.364025  [409600/481450]\n",
      "loss: 0.295397  [412800/481450]\n",
      "loss: 0.338047  [416000/481450]\n",
      "loss: 0.141505  [419200/481450]\n",
      "loss: 0.368356  [422400/481450]\n",
      "loss: 0.493884  [425600/481450]\n",
      "loss: 0.557497  [428800/481450]\n",
      "loss: 0.236322  [432000/481450]\n",
      "loss: 0.405003  [435200/481450]\n",
      "loss: 0.321846  [438400/481450]\n",
      "loss: 0.282417  [441600/481450]\n",
      "loss: 0.349327  [444800/481450]\n",
      "loss: 0.272288  [448000/481450]\n",
      "loss: 0.302696  [451200/481450]\n",
      "loss: 0.212744  [454400/481450]\n",
      "loss: 0.179519  [457600/481450]\n",
      "loss: 0.103370  [460800/481450]\n",
      "loss: 0.366475  [464000/481450]\n",
      "loss: 0.199513  [467200/481450]\n",
      "loss: 0.396709  [470400/481450]\n",
      "loss: 0.201607  [473600/481450]\n",
      "loss: 0.308717  [476800/481450]\n",
      "loss: 0.192616  [480000/481450]\n",
      "Train Accuracy: 87.9466%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.689033, F1-score: 85.45% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.177970  [    0/481450]\n",
      "loss: 0.228337  [ 3200/481450]\n",
      "loss: 0.408560  [ 6400/481450]\n",
      "loss: 0.355390  [ 9600/481450]\n",
      "loss: 0.356118  [12800/481450]\n",
      "loss: 0.265197  [16000/481450]\n",
      "loss: 0.198511  [19200/481450]\n",
      "loss: 0.164532  [22400/481450]\n",
      "loss: 0.215287  [25600/481450]\n",
      "loss: 0.279885  [28800/481450]\n",
      "loss: 0.295264  [32000/481450]\n",
      "loss: 0.150702  [35200/481450]\n",
      "loss: 0.373805  [38400/481450]\n",
      "loss: 0.321938  [41600/481450]\n",
      "loss: 0.284817  [44800/481450]\n",
      "loss: 0.272463  [48000/481450]\n",
      "loss: 0.168794  [51200/481450]\n",
      "loss: 0.159978  [54400/481450]\n",
      "loss: 0.260319  [57600/481450]\n",
      "loss: 0.312277  [60800/481450]\n",
      "loss: 0.432951  [64000/481450]\n",
      "loss: 0.427536  [67200/481450]\n",
      "loss: 0.443078  [70400/481450]\n",
      "loss: 0.226451  [73600/481450]\n",
      "loss: 0.314638  [76800/481450]\n",
      "loss: 0.285940  [80000/481450]\n",
      "loss: 0.392399  [83200/481450]\n",
      "loss: 0.307973  [86400/481450]\n",
      "loss: 0.214969  [89600/481450]\n",
      "loss: 0.097227  [92800/481450]\n",
      "loss: 0.200863  [96000/481450]\n",
      "loss: 0.258988  [99200/481450]\n",
      "loss: 0.274299  [102400/481450]\n",
      "loss: 0.107029  [105600/481450]\n",
      "loss: 0.342576  [108800/481450]\n",
      "loss: 0.368945  [112000/481450]\n",
      "loss: 0.264714  [115200/481450]\n",
      "loss: 0.172131  [118400/481450]\n",
      "loss: 0.339661  [121600/481450]\n",
      "loss: 0.153878  [124800/481450]\n",
      "loss: 0.457033  [128000/481450]\n",
      "loss: 0.419070  [131200/481450]\n",
      "loss: 0.318783  [134400/481450]\n",
      "loss: 0.227248  [137600/481450]\n",
      "loss: 0.441521  [140800/481450]\n",
      "loss: 0.096022  [144000/481450]\n",
      "loss: 0.324631  [147200/481450]\n",
      "loss: 0.191005  [150400/481450]\n",
      "loss: 0.347766  [153600/481450]\n",
      "loss: 0.425450  [156800/481450]\n",
      "loss: 0.241676  [160000/481450]\n",
      "loss: 0.325530  [163200/481450]\n",
      "loss: 0.249962  [166400/481450]\n",
      "loss: 0.101624  [169600/481450]\n",
      "loss: 0.502650  [172800/481450]\n",
      "loss: 0.247303  [176000/481450]\n",
      "loss: 0.281119  [179200/481450]\n",
      "loss: 0.243324  [182400/481450]\n",
      "loss: 0.189711  [185600/481450]\n",
      "loss: 0.189326  [188800/481450]\n",
      "loss: 0.231906  [192000/481450]\n",
      "loss: 0.323592  [195200/481450]\n",
      "loss: 0.373464  [198400/481450]\n",
      "loss: 0.285403  [201600/481450]\n",
      "loss: 0.203183  [204800/481450]\n",
      "loss: 0.275844  [208000/481450]\n",
      "loss: 0.207941  [211200/481450]\n",
      "loss: 0.511942  [214400/481450]\n",
      "loss: 0.165718  [217600/481450]\n",
      "loss: 0.094736  [220800/481450]\n",
      "loss: 0.303640  [224000/481450]\n",
      "loss: 0.258706  [227200/481450]\n",
      "loss: 0.314628  [230400/481450]\n",
      "loss: 0.438026  [233600/481450]\n",
      "loss: 0.271088  [236800/481450]\n",
      "loss: 0.358679  [240000/481450]\n",
      "loss: 0.065821  [243200/481450]\n",
      "loss: 0.453686  [246400/481450]\n",
      "loss: 0.248615  [249600/481450]\n",
      "loss: 0.523237  [252800/481450]\n",
      "loss: 0.336138  [256000/481450]\n",
      "loss: 0.321394  [259200/481450]\n",
      "loss: 0.309771  [262400/481450]\n",
      "loss: 0.262295  [265600/481450]\n",
      "loss: 0.234655  [268800/481450]\n",
      "loss: 0.252549  [272000/481450]\n",
      "loss: 0.213119  [275200/481450]\n",
      "loss: 0.178191  [278400/481450]\n",
      "loss: 0.371062  [281600/481450]\n",
      "loss: 0.151109  [284800/481450]\n",
      "loss: 0.252088  [288000/481450]\n",
      "loss: 0.388355  [291200/481450]\n",
      "loss: 0.330077  [294400/481450]\n",
      "loss: 0.230033  [297600/481450]\n",
      "loss: 0.270794  [300800/481450]\n",
      "loss: 0.261003  [304000/481450]\n",
      "loss: 0.173864  [307200/481450]\n",
      "loss: 0.264639  [310400/481450]\n",
      "loss: 0.467578  [313600/481450]\n",
      "loss: 0.410490  [316800/481450]\n",
      "loss: 0.340935  [320000/481450]\n",
      "loss: 0.108548  [323200/481450]\n",
      "loss: 0.288221  [326400/481450]\n",
      "loss: 0.347669  [329600/481450]\n",
      "loss: 0.198212  [332800/481450]\n",
      "loss: 0.149638  [336000/481450]\n",
      "loss: 0.545069  [339200/481450]\n",
      "loss: 0.135122  [342400/481450]\n",
      "loss: 0.114529  [345600/481450]\n",
      "loss: 0.640766  [348800/481450]\n",
      "loss: 0.219242  [352000/481450]\n",
      "loss: 0.354251  [355200/481450]\n",
      "loss: 0.281308  [358400/481450]\n",
      "loss: 0.356076  [361600/481450]\n",
      "loss: 0.474647  [364800/481450]\n",
      "loss: 0.189136  [368000/481450]\n",
      "loss: 0.346147  [371200/481450]\n",
      "loss: 0.285722  [374400/481450]\n",
      "loss: 0.493067  [377600/481450]\n",
      "loss: 0.385686  [380800/481450]\n",
      "loss: 0.464610  [384000/481450]\n",
      "loss: 0.329352  [387200/481450]\n",
      "loss: 0.184744  [390400/481450]\n",
      "loss: 0.206475  [393600/481450]\n",
      "loss: 0.103511  [396800/481450]\n",
      "loss: 0.276845  [400000/481450]\n",
      "loss: 0.197682  [403200/481450]\n",
      "loss: 0.528040  [406400/481450]\n",
      "loss: 0.403261  [409600/481450]\n",
      "loss: 0.215155  [412800/481450]\n",
      "loss: 0.215944  [416000/481450]\n",
      "loss: 0.402181  [419200/481450]\n",
      "loss: 0.343760  [422400/481450]\n",
      "loss: 0.148025  [425600/481450]\n",
      "loss: 0.212814  [428800/481450]\n",
      "loss: 0.261606  [432000/481450]\n",
      "loss: 0.195025  [435200/481450]\n",
      "loss: 0.314133  [438400/481450]\n",
      "loss: 0.276315  [441600/481450]\n",
      "loss: 0.284059  [444800/481450]\n",
      "loss: 0.346632  [448000/481450]\n",
      "loss: 0.316056  [451200/481450]\n",
      "loss: 0.276796  [454400/481450]\n",
      "loss: 0.197400  [457600/481450]\n",
      "loss: 0.213251  [460800/481450]\n",
      "loss: 0.327056  [464000/481450]\n",
      "loss: 0.220281  [467200/481450]\n",
      "loss: 0.173441  [470400/481450]\n",
      "loss: 0.203351  [473600/481450]\n",
      "loss: 0.178026  [476800/481450]\n",
      "loss: 0.265063  [480000/481450]\n",
      "Train Accuracy: 88.2630%\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.686485, F1-score: 85.88% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.381155  [    0/481450]\n",
      "loss: 0.335968  [ 3200/481450]\n",
      "loss: 0.106259  [ 6400/481450]\n",
      "loss: 0.439751  [ 9600/481450]\n",
      "loss: 0.537386  [12800/481450]\n",
      "loss: 0.496215  [16000/481450]\n",
      "loss: 0.391112  [19200/481450]\n",
      "loss: 0.442874  [22400/481450]\n",
      "loss: 0.472340  [25600/481450]\n",
      "loss: 0.343956  [28800/481450]\n",
      "loss: 0.377126  [32000/481450]\n",
      "loss: 0.400730  [35200/481450]\n",
      "loss: 0.224782  [38400/481450]\n",
      "loss: 0.392300  [41600/481450]\n",
      "loss: 0.417773  [44800/481450]\n",
      "loss: 0.275801  [48000/481450]\n",
      "loss: 0.166965  [51200/481450]\n",
      "loss: 0.294064  [54400/481450]\n",
      "loss: 0.389089  [57600/481450]\n",
      "loss: 0.370266  [60800/481450]\n",
      "loss: 0.221054  [64000/481450]\n",
      "loss: 0.410181  [67200/481450]\n",
      "loss: 0.281449  [70400/481450]\n",
      "loss: 0.307691  [73600/481450]\n",
      "loss: 0.330540  [76800/481450]\n",
      "loss: 0.255370  [80000/481450]\n",
      "loss: 0.277058  [83200/481450]\n",
      "loss: 0.265274  [86400/481450]\n",
      "loss: 0.385216  [89600/481450]\n",
      "loss: 0.322381  [92800/481450]\n",
      "loss: 0.408881  [96000/481450]\n",
      "loss: 0.165818  [99200/481450]\n",
      "loss: 0.330109  [102400/481450]\n",
      "loss: 0.175295  [105600/481450]\n",
      "loss: 0.688335  [108800/481450]\n",
      "loss: 0.452819  [112000/481450]\n",
      "loss: 0.304458  [115200/481450]\n",
      "loss: 0.273195  [118400/481450]\n",
      "loss: 0.254842  [121600/481450]\n",
      "loss: 0.278009  [124800/481450]\n",
      "loss: 0.142694  [128000/481450]\n",
      "loss: 0.277401  [131200/481450]\n",
      "loss: 0.262128  [134400/481450]\n",
      "loss: 0.286720  [137600/481450]\n",
      "loss: 0.296633  [140800/481450]\n",
      "loss: 0.546024  [144000/481450]\n",
      "loss: 0.377389  [147200/481450]\n",
      "loss: 0.160421  [150400/481450]\n",
      "loss: 0.204968  [153600/481450]\n",
      "loss: 0.257473  [156800/481450]\n",
      "loss: 0.198975  [160000/481450]\n",
      "loss: 0.213493  [163200/481450]\n",
      "loss: 0.294998  [166400/481450]\n",
      "loss: 0.132103  [169600/481450]\n",
      "loss: 0.262420  [172800/481450]\n",
      "loss: 0.216634  [176000/481450]\n",
      "loss: 0.268352  [179200/481450]\n",
      "loss: 0.234606  [182400/481450]\n",
      "loss: 0.329648  [185600/481450]\n",
      "loss: 0.339667  [188800/481450]\n",
      "loss: 0.189611  [192000/481450]\n",
      "loss: 0.467018  [195200/481450]\n",
      "loss: 0.144739  [198400/481450]\n",
      "loss: 0.277076  [201600/481450]\n",
      "loss: 0.194391  [204800/481450]\n",
      "loss: 0.367817  [208000/481450]\n",
      "loss: 0.283008  [211200/481450]\n",
      "loss: 0.199850  [214400/481450]\n",
      "loss: 0.352454  [217600/481450]\n",
      "loss: 0.247588  [220800/481450]\n",
      "loss: 0.240492  [224000/481450]\n",
      "loss: 0.272671  [227200/481450]\n",
      "loss: 0.272480  [230400/481450]\n",
      "loss: 0.296544  [233600/481450]\n",
      "loss: 0.462362  [236800/481450]\n",
      "loss: 0.154971  [240000/481450]\n",
      "loss: 0.658540  [243200/481450]\n",
      "loss: 0.252176  [246400/481450]\n",
      "loss: 0.246530  [249600/481450]\n",
      "loss: 0.084827  [252800/481450]\n",
      "loss: 0.270157  [256000/481450]\n",
      "loss: 0.222944  [259200/481450]\n",
      "loss: 0.244160  [262400/481450]\n",
      "loss: 0.543360  [265600/481450]\n",
      "loss: 0.408361  [268800/481450]\n",
      "loss: 0.597770  [272000/481450]\n",
      "loss: 0.160950  [275200/481450]\n",
      "loss: 0.364055  [278400/481450]\n",
      "loss: 0.276980  [281600/481450]\n",
      "loss: 0.182115  [284800/481450]\n",
      "loss: 0.117002  [288000/481450]\n",
      "loss: 0.427423  [291200/481450]\n",
      "loss: 0.183772  [294400/481450]\n",
      "loss: 0.512507  [297600/481450]\n",
      "loss: 0.046387  [300800/481450]\n",
      "loss: 0.178411  [304000/481450]\n",
      "loss: 0.569667  [307200/481450]\n",
      "loss: 0.165051  [310400/481450]\n",
      "loss: 0.203012  [313600/481450]\n",
      "loss: 0.131481  [316800/481450]\n",
      "loss: 0.265927  [320000/481450]\n",
      "loss: 0.110860  [323200/481450]\n",
      "loss: 0.330259  [326400/481450]\n",
      "loss: 0.204307  [329600/481450]\n",
      "loss: 0.212445  [332800/481450]\n",
      "loss: 0.179367  [336000/481450]\n",
      "loss: 0.081724  [339200/481450]\n",
      "loss: 0.370558  [342400/481450]\n",
      "loss: 0.304264  [345600/481450]\n",
      "loss: 0.197493  [348800/481450]\n",
      "loss: 0.440887  [352000/481450]\n",
      "loss: 0.227383  [355200/481450]\n",
      "loss: 0.495316  [358400/481450]\n",
      "loss: 0.351595  [361600/481450]\n",
      "loss: 0.457531  [364800/481450]\n",
      "loss: 0.304794  [368000/481450]\n",
      "loss: 0.296452  [371200/481450]\n",
      "loss: 0.189699  [374400/481450]\n",
      "loss: 0.144138  [377600/481450]\n",
      "loss: 0.304712  [380800/481450]\n",
      "loss: 0.367958  [384000/481450]\n",
      "loss: 0.190443  [387200/481450]\n",
      "loss: 0.389041  [390400/481450]\n",
      "loss: 0.621069  [393600/481450]\n",
      "loss: 0.170178  [396800/481450]\n",
      "loss: 0.142074  [400000/481450]\n",
      "loss: 0.215465  [403200/481450]\n",
      "loss: 0.140687  [406400/481450]\n",
      "loss: 0.302064  [409600/481450]\n",
      "loss: 0.225046  [412800/481450]\n",
      "loss: 0.233906  [416000/481450]\n",
      "loss: 0.192311  [419200/481450]\n",
      "loss: 0.184094  [422400/481450]\n",
      "loss: 0.098840  [425600/481450]\n",
      "loss: 0.309125  [428800/481450]\n",
      "loss: 0.286937  [432000/481450]\n",
      "loss: 0.174918  [435200/481450]\n",
      "loss: 0.094904  [438400/481450]\n",
      "loss: 0.277685  [441600/481450]\n",
      "loss: 0.094837  [444800/481450]\n",
      "loss: 0.110695  [448000/481450]\n",
      "loss: 0.093060  [451200/481450]\n",
      "loss: 0.284822  [454400/481450]\n",
      "loss: 0.272357  [457600/481450]\n",
      "loss: 0.214153  [460800/481450]\n",
      "loss: 0.069858  [464000/481450]\n",
      "loss: 0.312337  [467200/481450]\n",
      "loss: 0.289797  [470400/481450]\n",
      "loss: 0.465075  [473600/481450]\n",
      "loss: 0.304830  [476800/481450]\n",
      "loss: 0.394117  [480000/481450]\n",
      "Train Accuracy: 88.6451%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.714583, F1-score: 85.63% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.244472  [    0/481450]\n",
      "loss: 0.231306  [ 3200/481450]\n",
      "loss: 0.325406  [ 6400/481450]\n",
      "loss: 0.333481  [ 9600/481450]\n",
      "loss: 0.210789  [12800/481450]\n",
      "loss: 0.285179  [16000/481450]\n",
      "loss: 0.388796  [19200/481450]\n",
      "loss: 0.099680  [22400/481450]\n",
      "loss: 0.291982  [25600/481450]\n",
      "loss: 0.252777  [28800/481450]\n",
      "loss: 0.235531  [32000/481450]\n",
      "loss: 0.253938  [35200/481450]\n",
      "loss: 0.212056  [38400/481450]\n",
      "loss: 0.405618  [41600/481450]\n",
      "loss: 0.382987  [44800/481450]\n",
      "loss: 0.151942  [48000/481450]\n",
      "loss: 0.325915  [51200/481450]\n",
      "loss: 0.178513  [54400/481450]\n",
      "loss: 0.214984  [57600/481450]\n",
      "loss: 0.231985  [60800/481450]\n",
      "loss: 0.373741  [64000/481450]\n",
      "loss: 0.313417  [67200/481450]\n",
      "loss: 0.450301  [70400/481450]\n",
      "loss: 0.263337  [73600/481450]\n",
      "loss: 0.162386  [76800/481450]\n",
      "loss: 0.818223  [80000/481450]\n",
      "loss: 0.258427  [83200/481450]\n",
      "loss: 0.272471  [86400/481450]\n",
      "loss: 0.119308  [89600/481450]\n",
      "loss: 0.208199  [92800/481450]\n",
      "loss: 0.181828  [96000/481450]\n",
      "loss: 0.242799  [99200/481450]\n",
      "loss: 0.447364  [102400/481450]\n",
      "loss: 0.296290  [105600/481450]\n",
      "loss: 0.176648  [108800/481450]\n",
      "loss: 0.144140  [112000/481450]\n",
      "loss: 0.448265  [115200/481450]\n",
      "loss: 0.180893  [118400/481450]\n",
      "loss: 0.361181  [121600/481450]\n",
      "loss: 0.367824  [124800/481450]\n",
      "loss: 0.150942  [128000/481450]\n",
      "loss: 0.240820  [131200/481450]\n",
      "loss: 0.310812  [134400/481450]\n",
      "loss: 0.285857  [137600/481450]\n",
      "loss: 0.358268  [140800/481450]\n",
      "loss: 0.284077  [144000/481450]\n",
      "loss: 0.059100  [147200/481450]\n",
      "loss: 0.185673  [150400/481450]\n",
      "loss: 0.064428  [153600/481450]\n",
      "loss: 0.411271  [156800/481450]\n",
      "loss: 0.095616  [160000/481450]\n",
      "loss: 0.452215  [163200/481450]\n",
      "loss: 0.059698  [166400/481450]\n",
      "loss: 0.011405  [169600/481450]\n",
      "loss: 0.252031  [172800/481450]\n",
      "loss: 0.355274  [176000/481450]\n",
      "loss: 0.107377  [179200/481450]\n",
      "loss: 0.189602  [182400/481450]\n",
      "loss: 0.289829  [185600/481450]\n",
      "loss: 0.229641  [188800/481450]\n",
      "loss: 0.130488  [192000/481450]\n",
      "loss: 0.178970  [195200/481450]\n",
      "loss: 0.277395  [198400/481450]\n",
      "loss: 0.282369  [201600/481450]\n",
      "loss: 0.347388  [204800/481450]\n",
      "loss: 0.189734  [208000/481450]\n",
      "loss: 0.223280  [211200/481450]\n",
      "loss: 0.400860  [214400/481450]\n",
      "loss: 0.332376  [217600/481450]\n",
      "loss: 0.223414  [220800/481450]\n",
      "loss: 0.266585  [224000/481450]\n",
      "loss: 0.287346  [227200/481450]\n",
      "loss: 0.247450  [230400/481450]\n",
      "loss: 0.062035  [233600/481450]\n",
      "loss: 0.335728  [236800/481450]\n",
      "loss: 0.152630  [240000/481450]\n",
      "loss: 0.260389  [243200/481450]\n",
      "loss: 0.191863  [246400/481450]\n",
      "loss: 0.185421  [249600/481450]\n",
      "loss: 0.214599  [252800/481450]\n",
      "loss: 0.205711  [256000/481450]\n",
      "loss: 0.245110  [259200/481450]\n",
      "loss: 0.436031  [262400/481450]\n",
      "loss: 0.182297  [265600/481450]\n",
      "loss: 0.146445  [268800/481450]\n",
      "loss: 0.353736  [272000/481450]\n",
      "loss: 0.247067  [275200/481450]\n",
      "loss: 0.320950  [278400/481450]\n",
      "loss: 0.171328  [281600/481450]\n",
      "loss: 0.396010  [284800/481450]\n",
      "loss: 0.245962  [288000/481450]\n",
      "loss: 0.394496  [291200/481450]\n",
      "loss: 0.154048  [294400/481450]\n",
      "loss: 0.300787  [297600/481450]\n",
      "loss: 0.360016  [300800/481450]\n",
      "loss: 0.339109  [304000/481450]\n",
      "loss: 0.198939  [307200/481450]\n",
      "loss: 0.199639  [310400/481450]\n",
      "loss: 0.366093  [313600/481450]\n",
      "loss: 0.442401  [316800/481450]\n",
      "loss: 0.124812  [320000/481450]\n",
      "loss: 0.346708  [323200/481450]\n",
      "loss: 0.236895  [326400/481450]\n",
      "loss: 0.164609  [329600/481450]\n",
      "loss: 0.205484  [332800/481450]\n",
      "loss: 0.218282  [336000/481450]\n",
      "loss: 0.238038  [339200/481450]\n",
      "loss: 0.286481  [342400/481450]\n",
      "loss: 0.231919  [345600/481450]\n",
      "loss: 0.358335  [348800/481450]\n",
      "loss: 0.153005  [352000/481450]\n",
      "loss: 0.398470  [355200/481450]\n",
      "loss: 0.363128  [358400/481450]\n",
      "loss: 0.301191  [361600/481450]\n",
      "loss: 0.146244  [364800/481450]\n",
      "loss: 0.218535  [368000/481450]\n",
      "loss: 0.255283  [371200/481450]\n",
      "loss: 0.279868  [374400/481450]\n",
      "loss: 0.160079  [377600/481450]\n",
      "loss: 0.459321  [380800/481450]\n",
      "loss: 0.495581  [384000/481450]\n",
      "loss: 0.159921  [387200/481450]\n",
      "loss: 0.163120  [390400/481450]\n",
      "loss: 0.338254  [393600/481450]\n",
      "loss: 0.088445  [396800/481450]\n",
      "loss: 0.349190  [400000/481450]\n",
      "loss: 0.311421  [403200/481450]\n",
      "loss: 0.373793  [406400/481450]\n",
      "loss: 0.152068  [409600/481450]\n",
      "loss: 0.321918  [412800/481450]\n",
      "loss: 0.425603  [416000/481450]\n",
      "loss: 0.381141  [419200/481450]\n",
      "loss: 0.151096  [422400/481450]\n",
      "loss: 0.136676  [425600/481450]\n",
      "loss: 0.235677  [428800/481450]\n",
      "loss: 0.356068  [432000/481450]\n",
      "loss: 0.190134  [435200/481450]\n",
      "loss: 0.489751  [438400/481450]\n",
      "loss: 0.307024  [441600/481450]\n",
      "loss: 0.220971  [444800/481450]\n",
      "loss: 0.229646  [448000/481450]\n",
      "loss: 0.208705  [451200/481450]\n",
      "loss: 0.307054  [454400/481450]\n",
      "loss: 0.191045  [457600/481450]\n",
      "loss: 0.392976  [460800/481450]\n",
      "loss: 0.218415  [464000/481450]\n",
      "loss: 0.434941  [467200/481450]\n",
      "loss: 0.122539  [470400/481450]\n",
      "loss: 0.267469  [473600/481450]\n",
      "loss: 0.337598  [476800/481450]\n",
      "loss: 0.462164  [480000/481450]\n",
      "Train Accuracy: 89.1509%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.726574, F1-score: 85.38% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.310965  [    0/481450]\n",
      "loss: 0.116938  [ 3200/481450]\n",
      "loss: 0.361809  [ 6400/481450]\n",
      "loss: 0.323753  [ 9600/481450]\n",
      "loss: 0.279951  [12800/481450]\n",
      "loss: 0.260444  [16000/481450]\n",
      "loss: 0.262991  [19200/481450]\n",
      "loss: 0.224426  [22400/481450]\n",
      "loss: 0.238556  [25600/481450]\n",
      "loss: 0.216343  [28800/481450]\n",
      "loss: 0.371618  [32000/481450]\n",
      "loss: 0.375698  [35200/481450]\n",
      "loss: 0.332870  [38400/481450]\n",
      "loss: 0.319416  [41600/481450]\n",
      "loss: 0.458010  [44800/481450]\n",
      "loss: 0.331854  [48000/481450]\n",
      "loss: 0.144553  [51200/481450]\n",
      "loss: 0.412590  [54400/481450]\n",
      "loss: 0.125698  [57600/481450]\n",
      "loss: 0.295023  [60800/481450]\n",
      "loss: 0.308675  [64000/481450]\n",
      "loss: 0.216019  [67200/481450]\n",
      "loss: 0.329100  [70400/481450]\n",
      "loss: 0.048841  [73600/481450]\n",
      "loss: 0.641275  [76800/481450]\n",
      "loss: 0.272616  [80000/481450]\n",
      "loss: 0.254923  [83200/481450]\n",
      "loss: 0.358147  [86400/481450]\n",
      "loss: 0.266644  [89600/481450]\n",
      "loss: 0.280895  [92800/481450]\n",
      "loss: 0.184829  [96000/481450]\n",
      "loss: 0.149937  [99200/481450]\n",
      "loss: 0.128550  [102400/481450]\n",
      "loss: 0.250622  [105600/481450]\n",
      "loss: 0.233196  [108800/481450]\n",
      "loss: 0.239028  [112000/481450]\n",
      "loss: 0.260770  [115200/481450]\n",
      "loss: 0.310971  [118400/481450]\n",
      "loss: 0.227214  [121600/481450]\n",
      "loss: 0.325881  [124800/481450]\n",
      "loss: 0.221638  [128000/481450]\n",
      "loss: 0.120668  [131200/481450]\n",
      "loss: 0.474899  [134400/481450]\n",
      "loss: 0.360984  [137600/481450]\n",
      "loss: 0.295098  [140800/481450]\n",
      "loss: 0.161171  [144000/481450]\n",
      "loss: 0.325252  [147200/481450]\n",
      "loss: 0.383658  [150400/481450]\n",
      "loss: 0.246655  [153600/481450]\n",
      "loss: 0.212558  [156800/481450]\n",
      "loss: 0.253092  [160000/481450]\n",
      "loss: 0.329263  [163200/481450]\n",
      "loss: 0.327138  [166400/481450]\n",
      "loss: 0.375265  [169600/481450]\n",
      "loss: 0.302858  [172800/481450]\n",
      "loss: 0.156113  [176000/481450]\n",
      "loss: 0.145467  [179200/481450]\n",
      "loss: 0.381602  [182400/481450]\n",
      "loss: 0.229095  [185600/481450]\n",
      "loss: 0.305815  [188800/481450]\n",
      "loss: 0.125247  [192000/481450]\n",
      "loss: 0.216276  [195200/481450]\n",
      "loss: 0.220519  [198400/481450]\n",
      "loss: 0.315738  [201600/481450]\n",
      "loss: 0.202412  [204800/481450]\n",
      "loss: 0.308638  [208000/481450]\n",
      "loss: 0.282944  [211200/481450]\n",
      "loss: 0.243870  [214400/481450]\n",
      "loss: 0.095877  [217600/481450]\n",
      "loss: 0.396015  [220800/481450]\n",
      "loss: 0.278652  [224000/481450]\n",
      "loss: 0.073353  [227200/481450]\n",
      "loss: 0.095879  [230400/481450]\n",
      "loss: 0.264397  [233600/481450]\n",
      "loss: 0.542866  [236800/481450]\n",
      "loss: 0.282144  [240000/481450]\n",
      "loss: 0.489146  [243200/481450]\n",
      "loss: 0.136187  [246400/481450]\n",
      "loss: 0.224160  [249600/481450]\n",
      "loss: 0.464220  [252800/481450]\n",
      "loss: 0.238018  [256000/481450]\n",
      "loss: 0.268171  [259200/481450]\n",
      "loss: 0.172810  [262400/481450]\n",
      "loss: 0.240935  [265600/481450]\n",
      "loss: 0.356624  [268800/481450]\n",
      "loss: 0.165086  [272000/481450]\n",
      "loss: 0.198393  [275200/481450]\n",
      "loss: 0.382273  [278400/481450]\n",
      "loss: 0.249904  [281600/481450]\n",
      "loss: 0.185606  [284800/481450]\n",
      "loss: 0.132861  [288000/481450]\n",
      "loss: 0.187338  [291200/481450]\n",
      "loss: 0.335145  [294400/481450]\n",
      "loss: 0.306010  [297600/481450]\n",
      "loss: 0.208309  [300800/481450]\n",
      "loss: 0.090499  [304000/481450]\n",
      "loss: 0.337037  [307200/481450]\n",
      "loss: 0.179037  [310400/481450]\n",
      "loss: 0.213940  [313600/481450]\n",
      "loss: 0.268003  [316800/481450]\n",
      "loss: 0.126438  [320000/481450]\n",
      "loss: 0.382998  [323200/481450]\n",
      "loss: 0.221656  [326400/481450]\n",
      "loss: 0.346921  [329600/481450]\n",
      "loss: 0.385439  [332800/481450]\n",
      "loss: 0.150752  [336000/481450]\n",
      "loss: 0.248623  [339200/481450]\n",
      "loss: 0.070842  [342400/481450]\n",
      "loss: 0.136368  [345600/481450]\n",
      "loss: 0.266890  [348800/481450]\n",
      "loss: 0.648097  [352000/481450]\n",
      "loss: 0.279183  [355200/481450]\n",
      "loss: 0.405557  [358400/481450]\n",
      "loss: 0.262867  [361600/481450]\n",
      "loss: 0.242994  [364800/481450]\n",
      "loss: 0.278634  [368000/481450]\n",
      "loss: 0.152511  [371200/481450]\n",
      "loss: 0.237240  [374400/481450]\n",
      "loss: 0.166302  [377600/481450]\n",
      "loss: 0.177519  [380800/481450]\n",
      "loss: 0.184818  [384000/481450]\n",
      "loss: 0.257145  [387200/481450]\n",
      "loss: 0.300747  [390400/481450]\n",
      "loss: 0.333799  [393600/481450]\n",
      "loss: 0.361284  [396800/481450]\n",
      "loss: 0.181442  [400000/481450]\n",
      "loss: 0.374215  [403200/481450]\n",
      "loss: 0.207789  [406400/481450]\n",
      "loss: 0.159832  [409600/481450]\n",
      "loss: 0.449965  [412800/481450]\n",
      "loss: 0.109440  [416000/481450]\n",
      "loss: 0.143747  [419200/481450]\n",
      "loss: 0.242690  [422400/481450]\n",
      "loss: 0.166577  [425600/481450]\n",
      "loss: 0.292902  [428800/481450]\n",
      "loss: 0.257432  [432000/481450]\n",
      "loss: 0.206955  [435200/481450]\n",
      "loss: 0.254984  [438400/481450]\n",
      "loss: 0.240133  [441600/481450]\n",
      "loss: 0.434885  [444800/481450]\n",
      "loss: 0.227447  [448000/481450]\n",
      "loss: 0.309366  [451200/481450]\n",
      "loss: 0.263040  [454400/481450]\n",
      "loss: 0.244234  [457600/481450]\n",
      "loss: 0.253883  [460800/481450]\n",
      "loss: 0.323593  [464000/481450]\n",
      "loss: 0.177051  [467200/481450]\n",
      "loss: 0.278137  [470400/481450]\n",
      "loss: 0.273629  [473600/481450]\n",
      "loss: 0.172704  [476800/481450]\n",
      "loss: 0.188160  [480000/481450]\n",
      "Train Accuracy: 89.4882%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.764151, F1-score: 85.38% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.217728  [    0/481450]\n",
      "loss: 0.093889  [ 3200/481450]\n",
      "loss: 0.393950  [ 6400/481450]\n",
      "loss: 0.229710  [ 9600/481450]\n",
      "loss: 0.150471  [12800/481450]\n",
      "loss: 0.268731  [16000/481450]\n",
      "loss: 0.127093  [19200/481450]\n",
      "loss: 0.396216  [22400/481450]\n",
      "loss: 0.532216  [25600/481450]\n",
      "loss: 0.255278  [28800/481450]\n",
      "loss: 0.326349  [32000/481450]\n",
      "loss: 0.130047  [35200/481450]\n",
      "loss: 0.513970  [38400/481450]\n",
      "loss: 0.185464  [41600/481450]\n",
      "loss: 0.253250  [44800/481450]\n",
      "loss: 0.361782  [48000/481450]\n",
      "loss: 0.276861  [51200/481450]\n",
      "loss: 0.350932  [54400/481450]\n",
      "loss: 0.166324  [57600/481450]\n",
      "loss: 0.214055  [60800/481450]\n",
      "loss: 0.281395  [64000/481450]\n",
      "loss: 0.432464  [67200/481450]\n",
      "loss: 0.048012  [70400/481450]\n",
      "loss: 0.310680  [73600/481450]\n",
      "loss: 0.212381  [76800/481450]\n",
      "loss: 0.122849  [80000/481450]\n",
      "loss: 0.230657  [83200/481450]\n",
      "loss: 0.241729  [86400/481450]\n",
      "loss: 0.364474  [89600/481450]\n",
      "loss: 0.190340  [92800/481450]\n",
      "loss: 0.331898  [96000/481450]\n",
      "loss: 0.360310  [99200/481450]\n",
      "loss: 0.105838  [102400/481450]\n",
      "loss: 0.388737  [105600/481450]\n",
      "loss: 0.264927  [108800/481450]\n",
      "loss: 0.190600  [112000/481450]\n",
      "loss: 0.422901  [115200/481450]\n",
      "loss: 0.396224  [118400/481450]\n",
      "loss: 0.213245  [121600/481450]\n",
      "loss: 0.364609  [124800/481450]\n",
      "loss: 0.082142  [128000/481450]\n",
      "loss: 0.203465  [131200/481450]\n",
      "loss: 0.360864  [134400/481450]\n",
      "loss: 0.293267  [137600/481450]\n",
      "loss: 0.185464  [140800/481450]\n",
      "loss: 0.450218  [144000/481450]\n",
      "loss: 0.315783  [147200/481450]\n",
      "loss: 0.174019  [150400/481450]\n",
      "loss: 0.124360  [153600/481450]\n",
      "loss: 0.249920  [156800/481450]\n",
      "loss: 0.427073  [160000/481450]\n",
      "loss: 0.201825  [163200/481450]\n",
      "loss: 0.290499  [166400/481450]\n",
      "loss: 0.315696  [169600/481450]\n",
      "loss: 0.159115  [172800/481450]\n",
      "loss: 0.218661  [176000/481450]\n",
      "loss: 0.118858  [179200/481450]\n",
      "loss: 0.337749  [182400/481450]\n",
      "loss: 0.309607  [185600/481450]\n",
      "loss: 0.262971  [188800/481450]\n",
      "loss: 0.371006  [192000/481450]\n",
      "loss: 0.306082  [195200/481450]\n",
      "loss: 0.532185  [198400/481450]\n",
      "loss: 0.311497  [201600/481450]\n",
      "loss: 0.136653  [204800/481450]\n",
      "loss: 0.261583  [208000/481450]\n",
      "loss: 0.109183  [211200/481450]\n",
      "loss: 0.200679  [214400/481450]\n",
      "loss: 0.471490  [217600/481450]\n",
      "loss: 0.179656  [220800/481450]\n",
      "loss: 0.335624  [224000/481450]\n",
      "loss: 0.287459  [227200/481450]\n",
      "loss: 0.294498  [230400/481450]\n",
      "loss: 0.181736  [233600/481450]\n",
      "loss: 0.293612  [236800/481450]\n",
      "loss: 0.405832  [240000/481450]\n",
      "loss: 0.337040  [243200/481450]\n",
      "loss: 0.151359  [246400/481450]\n",
      "loss: 0.155032  [249600/481450]\n",
      "loss: 0.123836  [252800/481450]\n",
      "loss: 0.265630  [256000/481450]\n",
      "loss: 0.387098  [259200/481450]\n",
      "loss: 0.208527  [262400/481450]\n",
      "loss: 0.458346  [265600/481450]\n",
      "loss: 0.197407  [268800/481450]\n",
      "loss: 0.233559  [272000/481450]\n",
      "loss: 0.130514  [275200/481450]\n",
      "loss: 0.256367  [278400/481450]\n",
      "loss: 0.327762  [281600/481450]\n",
      "loss: 0.646881  [284800/481450]\n",
      "loss: 0.317795  [288000/481450]\n",
      "loss: 0.367567  [291200/481450]\n",
      "loss: 0.138276  [294400/481450]\n",
      "loss: 0.281350  [297600/481450]\n",
      "loss: 0.447434  [300800/481450]\n",
      "loss: 0.379733  [304000/481450]\n",
      "loss: 0.334205  [307200/481450]\n",
      "loss: 0.309330  [310400/481450]\n",
      "loss: 0.261295  [313600/481450]\n",
      "loss: 0.254519  [316800/481450]\n",
      "loss: 0.267908  [320000/481450]\n",
      "loss: 0.441367  [323200/481450]\n",
      "loss: 0.267932  [326400/481450]\n",
      "loss: 0.124825  [329600/481450]\n",
      "loss: 0.252681  [332800/481450]\n",
      "loss: 0.542871  [336000/481450]\n",
      "loss: 0.187071  [339200/481450]\n",
      "loss: 0.319440  [342400/481450]\n",
      "loss: 0.295904  [345600/481450]\n",
      "loss: 0.074197  [348800/481450]\n",
      "loss: 0.455598  [352000/481450]\n",
      "loss: 0.327378  [355200/481450]\n",
      "loss: 0.156848  [358400/481450]\n",
      "loss: 0.407826  [361600/481450]\n",
      "loss: 0.185432  [364800/481450]\n",
      "loss: 0.215547  [368000/481450]\n",
      "loss: 0.308438  [371200/481450]\n",
      "loss: 0.162165  [374400/481450]\n",
      "loss: 0.275007  [377600/481450]\n",
      "loss: 0.320107  [380800/481450]\n",
      "loss: 0.372074  [384000/481450]\n",
      "loss: 0.317153  [387200/481450]\n",
      "loss: 0.507604  [390400/481450]\n",
      "loss: 0.056035  [393600/481450]\n",
      "loss: 0.092250  [396800/481450]\n",
      "loss: 0.358578  [400000/481450]\n",
      "loss: 0.273712  [403200/481450]\n",
      "loss: 0.111664  [406400/481450]\n",
      "loss: 0.249633  [409600/481450]\n",
      "loss: 0.451177  [412800/481450]\n",
      "loss: 0.265599  [416000/481450]\n",
      "loss: 0.353574  [419200/481450]\n",
      "loss: 0.574854  [422400/481450]\n",
      "loss: 0.226331  [425600/481450]\n",
      "loss: 0.250801  [428800/481450]\n",
      "loss: 0.274126  [432000/481450]\n",
      "loss: 0.182967  [435200/481450]\n",
      "loss: 0.256748  [438400/481450]\n",
      "loss: 0.102339  [441600/481450]\n",
      "loss: 0.296606  [444800/481450]\n",
      "loss: 0.404572  [448000/481450]\n",
      "loss: 0.340015  [451200/481450]\n",
      "loss: 0.325265  [454400/481450]\n",
      "loss: 0.223022  [457600/481450]\n",
      "loss: 0.202182  [460800/481450]\n",
      "loss: 0.161114  [464000/481450]\n",
      "loss: 0.239260  [467200/481450]\n",
      "loss: 0.379739  [470400/481450]\n",
      "loss: 0.180458  [473600/481450]\n",
      "loss: 0.334477  [476800/481450]\n",
      "loss: 0.288174  [480000/481450]\n",
      "Train Accuracy: 89.7348%\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.768779, F1-score: 85.67% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.180142  [    0/481450]\n",
      "loss: 0.239852  [ 3200/481450]\n",
      "loss: 0.260718  [ 6400/481450]\n",
      "loss: 0.393783  [ 9600/481450]\n",
      "loss: 0.258376  [12800/481450]\n",
      "loss: 0.150933  [16000/481450]\n",
      "loss: 0.223958  [19200/481450]\n",
      "loss: 0.102195  [22400/481450]\n",
      "loss: 0.253183  [25600/481450]\n",
      "loss: 0.227412  [28800/481450]\n",
      "loss: 0.217773  [32000/481450]\n",
      "loss: 0.708974  [35200/481450]\n",
      "loss: 0.150063  [38400/481450]\n",
      "loss: 0.534888  [41600/481450]\n",
      "loss: 0.266430  [44800/481450]\n",
      "loss: 0.276328  [48000/481450]\n",
      "loss: 0.291535  [51200/481450]\n",
      "loss: 0.109845  [54400/481450]\n",
      "loss: 0.187393  [57600/481450]\n",
      "loss: 0.144551  [60800/481450]\n",
      "loss: 0.296754  [64000/481450]\n",
      "loss: 0.336890  [67200/481450]\n",
      "loss: 0.369437  [70400/481450]\n",
      "loss: 0.274524  [73600/481450]\n",
      "loss: 0.135390  [76800/481450]\n",
      "loss: 0.241392  [80000/481450]\n",
      "loss: 0.334282  [83200/481450]\n",
      "loss: 0.134118  [86400/481450]\n",
      "loss: 0.278849  [89600/481450]\n",
      "loss: 0.512850  [92800/481450]\n",
      "loss: 0.095685  [96000/481450]\n",
      "loss: 0.108597  [99200/481450]\n",
      "loss: 0.190195  [102400/481450]\n",
      "loss: 0.138825  [105600/481450]\n",
      "loss: 0.249439  [108800/481450]\n",
      "loss: 0.455323  [112000/481450]\n",
      "loss: 0.222787  [115200/481450]\n",
      "loss: 0.230829  [118400/481450]\n",
      "loss: 0.335402  [121600/481450]\n",
      "loss: 0.162873  [124800/481450]\n",
      "loss: 0.345097  [128000/481450]\n",
      "loss: 0.612169  [131200/481450]\n",
      "loss: 0.310313  [134400/481450]\n",
      "loss: 0.349964  [137600/481450]\n",
      "loss: 0.223092  [140800/481450]\n",
      "loss: 0.273566  [144000/481450]\n",
      "loss: 0.083041  [147200/481450]\n",
      "loss: 0.280817  [150400/481450]\n",
      "loss: 0.464717  [153600/481450]\n",
      "loss: 0.127659  [156800/481450]\n",
      "loss: 0.202613  [160000/481450]\n",
      "loss: 0.167428  [163200/481450]\n",
      "loss: 0.346206  [166400/481450]\n",
      "loss: 0.198725  [169600/481450]\n",
      "loss: 0.189545  [172800/481450]\n",
      "loss: 0.312920  [176000/481450]\n",
      "loss: 0.341386  [179200/481450]\n",
      "loss: 0.208726  [182400/481450]\n",
      "loss: 0.836812  [185600/481450]\n",
      "loss: 0.172966  [188800/481450]\n",
      "loss: 0.372561  [192000/481450]\n",
      "loss: 0.104604  [195200/481450]\n",
      "loss: 0.223166  [198400/481450]\n",
      "loss: 0.579408  [201600/481450]\n",
      "loss: 0.123077  [204800/481450]\n",
      "loss: 0.241234  [208000/481450]\n",
      "loss: 0.317043  [211200/481450]\n",
      "loss: 0.339216  [214400/481450]\n",
      "loss: 0.177345  [217600/481450]\n",
      "loss: 0.262589  [220800/481450]\n",
      "loss: 0.070991  [224000/481450]\n",
      "loss: 0.375112  [227200/481450]\n",
      "loss: 0.179432  [230400/481450]\n",
      "loss: 0.144164  [233600/481450]\n",
      "loss: 0.219243  [236800/481450]\n",
      "loss: 0.301126  [240000/481450]\n",
      "loss: 0.197122  [243200/481450]\n",
      "loss: 0.373800  [246400/481450]\n",
      "loss: 0.321198  [249600/481450]\n",
      "loss: 0.332906  [252800/481450]\n",
      "loss: 0.148225  [256000/481450]\n",
      "loss: 0.168625  [259200/481450]\n",
      "loss: 0.529583  [262400/481450]\n",
      "loss: 0.273259  [265600/481450]\n",
      "loss: 0.044104  [268800/481450]\n",
      "loss: 0.305193  [272000/481450]\n",
      "loss: 0.213306  [275200/481450]\n",
      "loss: 0.343966  [278400/481450]\n",
      "loss: 0.306114  [281600/481450]\n",
      "loss: 0.116347  [284800/481450]\n",
      "loss: 0.166360  [288000/481450]\n",
      "loss: 0.193771  [291200/481450]\n",
      "loss: 0.237644  [294400/481450]\n",
      "loss: 0.164884  [297600/481450]\n",
      "loss: 0.251628  [300800/481450]\n",
      "loss: 0.221109  [304000/481450]\n",
      "loss: 0.273997  [307200/481450]\n",
      "loss: 0.250746  [310400/481450]\n",
      "loss: 0.318535  [313600/481450]\n",
      "loss: 0.216932  [316800/481450]\n",
      "loss: 0.105103  [320000/481450]\n",
      "loss: 0.211738  [323200/481450]\n",
      "loss: 0.184888  [326400/481450]\n",
      "loss: 0.367727  [329600/481450]\n",
      "loss: 0.221142  [332800/481450]\n",
      "loss: 0.173029  [336000/481450]\n",
      "loss: 0.148117  [339200/481450]\n",
      "loss: 0.235394  [342400/481450]\n",
      "loss: 0.159994  [345600/481450]\n",
      "loss: 0.292421  [348800/481450]\n",
      "loss: 0.356483  [352000/481450]\n",
      "loss: 0.505777  [355200/481450]\n",
      "loss: 0.247824  [358400/481450]\n",
      "loss: 0.186689  [361600/481450]\n",
      "loss: 0.317915  [364800/481450]\n",
      "loss: 0.165394  [368000/481450]\n",
      "loss: 0.206830  [371200/481450]\n",
      "loss: 0.225156  [374400/481450]\n",
      "loss: 0.254747  [377600/481450]\n",
      "loss: 0.307289  [380800/481450]\n",
      "loss: 0.436320  [384000/481450]\n",
      "loss: 0.076883  [387200/481450]\n",
      "loss: 0.253660  [390400/481450]\n",
      "loss: 0.497684  [393600/481450]\n",
      "loss: 0.308905  [396800/481450]\n",
      "loss: 0.232479  [400000/481450]\n",
      "loss: 0.306238  [403200/481450]\n",
      "loss: 0.281868  [406400/481450]\n",
      "loss: 0.079830  [409600/481450]\n",
      "loss: 0.337616  [412800/481450]\n",
      "loss: 0.616752  [416000/481450]\n",
      "loss: 0.257738  [419200/481450]\n",
      "loss: 0.295317  [422400/481450]\n",
      "loss: 0.246139  [425600/481450]\n",
      "loss: 0.124075  [428800/481450]\n",
      "loss: 0.208702  [432000/481450]\n",
      "loss: 0.127973  [435200/481450]\n",
      "loss: 0.179482  [438400/481450]\n",
      "loss: 0.459361  [441600/481450]\n",
      "loss: 0.212445  [444800/481450]\n",
      "loss: 0.249680  [448000/481450]\n",
      "loss: 0.220061  [451200/481450]\n",
      "loss: 0.219653  [454400/481450]\n",
      "loss: 0.132582  [457600/481450]\n",
      "loss: 0.258556  [460800/481450]\n",
      "loss: 0.146814  [464000/481450]\n",
      "loss: 0.173075  [467200/481450]\n",
      "loss: 0.108629  [470400/481450]\n",
      "loss: 0.331752  [473600/481450]\n",
      "loss: 0.137873  [476800/481450]\n",
      "loss: 0.295051  [480000/481450]\n",
      "Train Accuracy: 89.9042%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.757323, F1-score: 85.60% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.155085  [    0/481450]\n",
      "loss: 0.074140  [ 3200/481450]\n",
      "loss: 0.299499  [ 6400/481450]\n",
      "loss: 0.130096  [ 9600/481450]\n",
      "loss: 0.288419  [12800/481450]\n",
      "loss: 0.185937  [16000/481450]\n",
      "loss: 0.163558  [19200/481450]\n",
      "loss: 0.429469  [22400/481450]\n",
      "loss: 0.245284  [25600/481450]\n",
      "loss: 0.295991  [28800/481450]\n",
      "loss: 0.056457  [32000/481450]\n",
      "loss: 0.145287  [35200/481450]\n",
      "loss: 0.249437  [38400/481450]\n",
      "loss: 0.347598  [41600/481450]\n",
      "loss: 0.299347  [44800/481450]\n",
      "loss: 0.346633  [48000/481450]\n",
      "loss: 0.230323  [51200/481450]\n",
      "loss: 0.105958  [54400/481450]\n",
      "loss: 0.223923  [57600/481450]\n",
      "loss: 0.302284  [60800/481450]\n",
      "loss: 0.374047  [64000/481450]\n",
      "loss: 0.201026  [67200/481450]\n",
      "loss: 0.445696  [70400/481450]\n",
      "loss: 0.259343  [73600/481450]\n",
      "loss: 0.200668  [76800/481450]\n",
      "loss: 0.078152  [80000/481450]\n",
      "loss: 0.111708  [83200/481450]\n",
      "loss: 0.206139  [86400/481450]\n",
      "loss: 0.213013  [89600/481450]\n",
      "loss: 0.199693  [92800/481450]\n",
      "loss: 0.272968  [96000/481450]\n",
      "loss: 0.272790  [99200/481450]\n",
      "loss: 0.128070  [102400/481450]\n",
      "loss: 0.156114  [105600/481450]\n",
      "loss: 0.364656  [108800/481450]\n",
      "loss: 0.327471  [112000/481450]\n",
      "loss: 0.559836  [115200/481450]\n",
      "loss: 0.072758  [118400/481450]\n",
      "loss: 0.344329  [121600/481450]\n",
      "loss: 0.285602  [124800/481450]\n",
      "loss: 0.240000  [128000/481450]\n",
      "loss: 0.185304  [131200/481450]\n",
      "loss: 0.130390  [134400/481450]\n",
      "loss: 0.266191  [137600/481450]\n",
      "loss: 0.274098  [140800/481450]\n",
      "loss: 0.219367  [144000/481450]\n",
      "loss: 0.155919  [147200/481450]\n",
      "loss: 0.145126  [150400/481450]\n",
      "loss: 0.267054  [153600/481450]\n",
      "loss: 0.098560  [156800/481450]\n",
      "loss: 0.168048  [160000/481450]\n",
      "loss: 0.141977  [163200/481450]\n",
      "loss: 0.304782  [166400/481450]\n",
      "loss: 0.245967  [169600/481450]\n",
      "loss: 0.282222  [172800/481450]\n",
      "loss: 0.204164  [176000/481450]\n",
      "loss: 0.358297  [179200/481450]\n",
      "loss: 0.318884  [182400/481450]\n",
      "loss: 0.182011  [185600/481450]\n",
      "loss: 0.144439  [188800/481450]\n",
      "loss: 0.232027  [192000/481450]\n",
      "loss: 0.111233  [195200/481450]\n",
      "loss: 0.088712  [198400/481450]\n",
      "loss: 0.334311  [201600/481450]\n",
      "loss: 0.162286  [204800/481450]\n",
      "loss: 0.147404  [208000/481450]\n",
      "loss: 0.263730  [211200/481450]\n",
      "loss: 0.101891  [214400/481450]\n",
      "loss: 0.175661  [217600/481450]\n",
      "loss: 0.218195  [220800/481450]\n",
      "loss: 0.133179  [224000/481450]\n",
      "loss: 0.431308  [227200/481450]\n",
      "loss: 0.212714  [230400/481450]\n",
      "loss: 0.209841  [233600/481450]\n",
      "loss: 0.225133  [236800/481450]\n",
      "loss: 0.117436  [240000/481450]\n",
      "loss: 0.110797  [243200/481450]\n",
      "loss: 0.350781  [246400/481450]\n",
      "loss: 0.205542  [249600/481450]\n",
      "loss: 0.206997  [252800/481450]\n",
      "loss: 0.159512  [256000/481450]\n",
      "loss: 0.348172  [259200/481450]\n",
      "loss: 0.367545  [262400/481450]\n",
      "loss: 0.300971  [265600/481450]\n",
      "loss: 0.265993  [268800/481450]\n",
      "loss: 0.200902  [272000/481450]\n",
      "loss: 0.530240  [275200/481450]\n",
      "loss: 0.206209  [278400/481450]\n",
      "loss: 0.242300  [281600/481450]\n",
      "loss: 0.598204  [284800/481450]\n",
      "loss: 0.136342  [288000/481450]\n",
      "loss: 0.252433  [291200/481450]\n",
      "loss: 0.148714  [294400/481450]\n",
      "loss: 0.192391  [297600/481450]\n",
      "loss: 0.367105  [300800/481450]\n",
      "loss: 0.126080  [304000/481450]\n",
      "loss: 0.101047  [307200/481450]\n",
      "loss: 0.158508  [310400/481450]\n",
      "loss: 0.226432  [313600/481450]\n",
      "loss: 0.287413  [316800/481450]\n",
      "loss: 0.800247  [320000/481450]\n",
      "loss: 0.247934  [323200/481450]\n",
      "loss: 0.135274  [326400/481450]\n",
      "loss: 0.339539  [329600/481450]\n",
      "loss: 0.357725  [332800/481450]\n",
      "loss: 0.212419  [336000/481450]\n",
      "loss: 0.124083  [339200/481450]\n",
      "loss: 0.287231  [342400/481450]\n",
      "loss: 0.083500  [345600/481450]\n",
      "loss: 0.197857  [348800/481450]\n",
      "loss: 0.239326  [352000/481450]\n",
      "loss: 0.270165  [355200/481450]\n",
      "loss: 0.236806  [358400/481450]\n",
      "loss: 0.148211  [361600/481450]\n",
      "loss: 0.313445  [364800/481450]\n",
      "loss: 0.237138  [368000/481450]\n",
      "loss: 0.214850  [371200/481450]\n",
      "loss: 0.217881  [374400/481450]\n",
      "loss: 0.224767  [377600/481450]\n",
      "loss: 0.343877  [380800/481450]\n",
      "loss: 0.208234  [384000/481450]\n",
      "loss: 0.112791  [387200/481450]\n",
      "loss: 0.162575  [390400/481450]\n",
      "loss: 0.285339  [393600/481450]\n",
      "loss: 0.268000  [396800/481450]\n",
      "loss: 0.248240  [400000/481450]\n",
      "loss: 0.353295  [403200/481450]\n",
      "loss: 0.084909  [406400/481450]\n",
      "loss: 0.143755  [409600/481450]\n",
      "loss: 0.169424  [412800/481450]\n",
      "loss: 0.295900  [416000/481450]\n",
      "loss: 0.195587  [419200/481450]\n",
      "loss: 0.255908  [422400/481450]\n",
      "loss: 0.384041  [425600/481450]\n",
      "loss: 0.216537  [428800/481450]\n",
      "loss: 0.358881  [432000/481450]\n",
      "loss: 0.218199  [435200/481450]\n",
      "loss: 0.219585  [438400/481450]\n",
      "loss: 0.263662  [441600/481450]\n",
      "loss: 0.273155  [444800/481450]\n",
      "loss: 0.117378  [448000/481450]\n",
      "loss: 0.226487  [451200/481450]\n",
      "loss: 0.300087  [454400/481450]\n",
      "loss: 0.257347  [457600/481450]\n",
      "loss: 0.270552  [460800/481450]\n",
      "loss: 0.210840  [464000/481450]\n",
      "loss: 0.172039  [467200/481450]\n",
      "loss: 0.226312  [470400/481450]\n",
      "loss: 0.275306  [473600/481450]\n",
      "loss: 0.259804  [476800/481450]\n",
      "loss: 0.191352  [480000/481450]\n",
      "Train Accuracy: 90.0424%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.758729, F1-score: 85.60% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.261755  [    0/481450]\n",
      "loss: 0.312532  [ 3200/481450]\n",
      "loss: 0.224691  [ 6400/481450]\n",
      "loss: 0.391143  [ 9600/481450]\n",
      "loss: 0.128718  [12800/481450]\n",
      "loss: 0.364092  [16000/481450]\n",
      "loss: 0.114532  [19200/481450]\n",
      "loss: 0.137761  [22400/481450]\n",
      "loss: 0.196940  [25600/481450]\n",
      "loss: 0.190530  [28800/481450]\n",
      "loss: 0.124312  [32000/481450]\n",
      "loss: 0.226783  [35200/481450]\n",
      "loss: 0.223365  [38400/481450]\n",
      "loss: 0.123930  [41600/481450]\n",
      "loss: 0.209235  [44800/481450]\n",
      "loss: 0.402861  [48000/481450]\n",
      "loss: 0.218320  [51200/481450]\n",
      "loss: 0.094199  [54400/481450]\n",
      "loss: 0.452533  [57600/481450]\n",
      "loss: 0.219356  [60800/481450]\n",
      "loss: 0.016567  [64000/481450]\n",
      "loss: 0.275529  [67200/481450]\n",
      "loss: 0.483102  [70400/481450]\n",
      "loss: 0.328759  [73600/481450]\n",
      "loss: 0.312766  [76800/481450]\n",
      "loss: 0.241970  [80000/481450]\n",
      "loss: 0.209931  [83200/481450]\n",
      "loss: 0.402909  [86400/481450]\n",
      "loss: 0.245859  [89600/481450]\n",
      "loss: 0.225943  [92800/481450]\n",
      "loss: 0.305480  [96000/481450]\n",
      "loss: 0.212771  [99200/481450]\n",
      "loss: 0.110829  [102400/481450]\n",
      "loss: 0.290394  [105600/481450]\n",
      "loss: 0.311280  [108800/481450]\n",
      "loss: 0.234078  [112000/481450]\n",
      "loss: 0.470472  [115200/481450]\n",
      "loss: 0.177111  [118400/481450]\n",
      "loss: 0.145360  [121600/481450]\n",
      "loss: 0.384821  [124800/481450]\n",
      "loss: 0.177515  [128000/481450]\n",
      "loss: 0.328244  [131200/481450]\n",
      "loss: 0.138280  [134400/481450]\n",
      "loss: 0.257093  [137600/481450]\n",
      "loss: 0.172802  [140800/481450]\n",
      "loss: 0.450662  [144000/481450]\n",
      "loss: 0.176766  [147200/481450]\n",
      "loss: 0.467074  [150400/481450]\n",
      "loss: 0.230760  [153600/481450]\n",
      "loss: 0.208141  [156800/481450]\n",
      "loss: 0.189323  [160000/481450]\n",
      "loss: 0.149895  [163200/481450]\n",
      "loss: 0.423341  [166400/481450]\n",
      "loss: 0.471685  [169600/481450]\n",
      "loss: 0.242280  [172800/481450]\n",
      "loss: 0.295403  [176000/481450]\n",
      "loss: 0.218728  [179200/481450]\n",
      "loss: 0.132532  [182400/481450]\n",
      "loss: 0.176667  [185600/481450]\n",
      "loss: 0.466576  [188800/481450]\n",
      "loss: 0.123657  [192000/481450]\n",
      "loss: 0.125188  [195200/481450]\n",
      "loss: 0.179849  [198400/481450]\n",
      "loss: 0.271350  [201600/481450]\n",
      "loss: 0.168043  [204800/481450]\n",
      "loss: 0.142347  [208000/481450]\n",
      "loss: 0.198352  [211200/481450]\n",
      "loss: 0.097108  [214400/481450]\n",
      "loss: 0.286464  [217600/481450]\n",
      "loss: 0.110661  [220800/481450]\n",
      "loss: 0.096427  [224000/481450]\n",
      "loss: 0.337571  [227200/481450]\n",
      "loss: 0.250675  [230400/481450]\n",
      "loss: 0.215602  [233600/481450]\n",
      "loss: 0.195860  [236800/481450]\n",
      "loss: 0.183885  [240000/481450]\n",
      "loss: 0.234930  [243200/481450]\n",
      "loss: 0.288841  [246400/481450]\n",
      "loss: 0.279015  [249600/481450]\n",
      "loss: 0.201947  [252800/481450]\n",
      "loss: 0.186454  [256000/481450]\n",
      "loss: 0.086028  [259200/481450]\n",
      "loss: 0.392998  [262400/481450]\n",
      "loss: 0.172610  [265600/481450]\n",
      "loss: 0.171310  [268800/481450]\n",
      "loss: 0.315991  [272000/481450]\n",
      "loss: 0.238629  [275200/481450]\n",
      "loss: 0.111648  [278400/481450]\n",
      "loss: 0.191009  [281600/481450]\n",
      "loss: 0.129396  [284800/481450]\n",
      "loss: 0.304076  [288000/481450]\n",
      "loss: 0.288318  [291200/481450]\n",
      "loss: 0.205532  [294400/481450]\n",
      "loss: 0.109801  [297600/481450]\n",
      "loss: 0.144961  [300800/481450]\n",
      "loss: 0.386828  [304000/481450]\n",
      "loss: 0.521413  [307200/481450]\n",
      "loss: 0.339403  [310400/481450]\n",
      "loss: 0.223802  [313600/481450]\n",
      "loss: 0.357751  [316800/481450]\n",
      "loss: 0.182927  [320000/481450]\n",
      "loss: 0.276148  [323200/481450]\n",
      "loss: 0.186450  [326400/481450]\n",
      "loss: 0.633570  [329600/481450]\n",
      "loss: 0.164758  [332800/481450]\n",
      "loss: 0.336505  [336000/481450]\n",
      "loss: 0.210887  [339200/481450]\n",
      "loss: 0.173771  [342400/481450]\n",
      "loss: 0.477948  [345600/481450]\n",
      "loss: 0.161018  [348800/481450]\n",
      "loss: 0.292830  [352000/481450]\n",
      "loss: 0.387676  [355200/481450]\n",
      "loss: 0.206937  [358400/481450]\n",
      "loss: 0.242181  [361600/481450]\n",
      "loss: 0.174146  [364800/481450]\n",
      "loss: 0.321658  [368000/481450]\n",
      "loss: 0.199994  [371200/481450]\n",
      "loss: 0.246655  [374400/481450]\n",
      "loss: 0.431050  [377600/481450]\n",
      "loss: 0.230743  [380800/481450]\n",
      "loss: 0.154843  [384000/481450]\n",
      "loss: 0.428938  [387200/481450]\n",
      "loss: 0.255706  [390400/481450]\n",
      "loss: 0.153848  [393600/481450]\n",
      "loss: 0.368127  [396800/481450]\n",
      "loss: 0.374535  [400000/481450]\n",
      "loss: 0.150325  [403200/481450]\n",
      "loss: 0.231231  [406400/481450]\n",
      "loss: 0.155636  [409600/481450]\n",
      "loss: 0.132632  [412800/481450]\n",
      "loss: 0.187081  [416000/481450]\n",
      "loss: 0.182259  [419200/481450]\n",
      "loss: 0.237901  [422400/481450]\n",
      "loss: 0.300286  [425600/481450]\n",
      "loss: 0.153306  [428800/481450]\n",
      "loss: 0.167256  [432000/481450]\n",
      "loss: 0.076625  [435200/481450]\n",
      "loss: 0.252486  [438400/481450]\n",
      "loss: 0.073635  [441600/481450]\n",
      "loss: 0.270920  [444800/481450]\n",
      "loss: 0.361507  [448000/481450]\n",
      "loss: 0.297666  [451200/481450]\n",
      "loss: 0.133876  [454400/481450]\n",
      "loss: 0.252392  [457600/481450]\n",
      "loss: 0.287896  [460800/481450]\n",
      "loss: 0.123666  [464000/481450]\n",
      "loss: 0.096995  [467200/481450]\n",
      "loss: 0.064098  [470400/481450]\n",
      "loss: 0.254300  [473600/481450]\n",
      "loss: 0.165767  [476800/481450]\n",
      "loss: 0.276225  [480000/481450]\n",
      "Train Accuracy: 90.2198%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.775049, F1-score: 85.40% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.294348  [    0/481450]\n",
      "loss: 0.170006  [ 3200/481450]\n",
      "loss: 0.268365  [ 6400/481450]\n",
      "loss: 0.172905  [ 9600/481450]\n",
      "loss: 0.772798  [12800/481450]\n",
      "loss: 0.403838  [16000/481450]\n",
      "loss: 0.235987  [19200/481450]\n",
      "loss: 0.266354  [22400/481450]\n",
      "loss: 0.214499  [25600/481450]\n",
      "loss: 0.275565  [28800/481450]\n",
      "loss: 0.388180  [32000/481450]\n",
      "loss: 0.127508  [35200/481450]\n",
      "loss: 0.194358  [38400/481450]\n",
      "loss: 0.235801  [41600/481450]\n",
      "loss: 0.130898  [44800/481450]\n",
      "loss: 0.299926  [48000/481450]\n",
      "loss: 0.287508  [51200/481450]\n",
      "loss: 0.228670  [54400/481450]\n",
      "loss: 0.277091  [57600/481450]\n",
      "loss: 0.146508  [60800/481450]\n",
      "loss: 0.529064  [64000/481450]\n",
      "loss: 0.309376  [67200/481450]\n",
      "loss: 0.152443  [70400/481450]\n",
      "loss: 0.140209  [73600/481450]\n",
      "loss: 0.190966  [76800/481450]\n",
      "loss: 0.266564  [80000/481450]\n",
      "loss: 0.101871  [83200/481450]\n",
      "loss: 0.194629  [86400/481450]\n",
      "loss: 0.249431  [89600/481450]\n",
      "loss: 0.127449  [92800/481450]\n",
      "loss: 0.301833  [96000/481450]\n",
      "loss: 0.176244  [99200/481450]\n",
      "loss: 0.132914  [102400/481450]\n",
      "loss: 0.447344  [105600/481450]\n",
      "loss: 0.177081  [108800/481450]\n",
      "loss: 0.204179  [112000/481450]\n",
      "loss: 0.064699  [115200/481450]\n",
      "loss: 0.333107  [118400/481450]\n",
      "loss: 0.244852  [121600/481450]\n",
      "loss: 0.209627  [124800/481450]\n",
      "loss: 0.211995  [128000/481450]\n",
      "loss: 0.229676  [131200/481450]\n",
      "loss: 0.236378  [134400/481450]\n",
      "loss: 0.119737  [137600/481450]\n",
      "loss: 0.233238  [140800/481450]\n",
      "loss: 0.242046  [144000/481450]\n",
      "loss: 0.119054  [147200/481450]\n",
      "loss: 0.192832  [150400/481450]\n",
      "loss: 0.200164  [153600/481450]\n",
      "loss: 0.274378  [156800/481450]\n",
      "loss: 0.291510  [160000/481450]\n",
      "loss: 0.177180  [163200/481450]\n",
      "loss: 0.096032  [166400/481450]\n",
      "loss: 0.232164  [169600/481450]\n",
      "loss: 0.140329  [172800/481450]\n",
      "loss: 0.333911  [176000/481450]\n",
      "loss: 0.206170  [179200/481450]\n",
      "loss: 0.144428  [182400/481450]\n",
      "loss: 0.201597  [185600/481450]\n",
      "loss: 0.258180  [188800/481450]\n",
      "loss: 0.329212  [192000/481450]\n",
      "loss: 0.216486  [195200/481450]\n",
      "loss: 0.311704  [198400/481450]\n",
      "loss: 0.290903  [201600/481450]\n",
      "loss: 0.171670  [204800/481450]\n",
      "loss: 0.260206  [208000/481450]\n",
      "loss: 0.230946  [211200/481450]\n",
      "loss: 0.476095  [214400/481450]\n",
      "loss: 0.147159  [217600/481450]\n",
      "loss: 0.302824  [220800/481450]\n",
      "loss: 0.154315  [224000/481450]\n",
      "loss: 0.323307  [227200/481450]\n",
      "loss: 0.363613  [230400/481450]\n",
      "loss: 0.179577  [233600/481450]\n",
      "loss: 0.342021  [236800/481450]\n",
      "loss: 0.263590  [240000/481450]\n",
      "loss: 0.268414  [243200/481450]\n",
      "loss: 0.213243  [246400/481450]\n",
      "loss: 0.323688  [249600/481450]\n",
      "loss: 0.249633  [252800/481450]\n",
      "loss: 0.201859  [256000/481450]\n",
      "loss: 0.223009  [259200/481450]\n",
      "loss: 0.206263  [262400/481450]\n",
      "loss: 0.296701  [265600/481450]\n",
      "loss: 0.227380  [268800/481450]\n",
      "loss: 0.181258  [272000/481450]\n",
      "loss: 0.194186  [275200/481450]\n",
      "loss: 0.211199  [278400/481450]\n",
      "loss: 0.268796  [281600/481450]\n",
      "loss: 0.128138  [284800/481450]\n",
      "loss: 0.445760  [288000/481450]\n",
      "loss: 0.219989  [291200/481450]\n",
      "loss: 0.198923  [294400/481450]\n",
      "loss: 0.055372  [297600/481450]\n",
      "loss: 0.096842  [300800/481450]\n",
      "loss: 0.319080  [304000/481450]\n",
      "loss: 0.252300  [307200/481450]\n",
      "loss: 0.206773  [310400/481450]\n",
      "loss: 0.161726  [313600/481450]\n",
      "loss: 0.278746  [316800/481450]\n",
      "loss: 0.145454  [320000/481450]\n",
      "loss: 0.253586  [323200/481450]\n",
      "loss: 0.384331  [326400/481450]\n",
      "loss: 0.373107  [329600/481450]\n",
      "loss: 0.206333  [332800/481450]\n",
      "loss: 0.337872  [336000/481450]\n",
      "loss: 0.238748  [339200/481450]\n",
      "loss: 0.269303  [342400/481450]\n",
      "loss: 0.314782  [345600/481450]\n",
      "loss: 0.468157  [348800/481450]\n",
      "loss: 0.436524  [352000/481450]\n",
      "loss: 0.213069  [355200/481450]\n",
      "loss: 0.201964  [358400/481450]\n",
      "loss: 0.174937  [361600/481450]\n",
      "loss: 0.266482  [364800/481450]\n",
      "loss: 0.310722  [368000/481450]\n",
      "loss: 0.374035  [371200/481450]\n",
      "loss: 0.148147  [374400/481450]\n",
      "loss: 0.222255  [377600/481450]\n",
      "loss: 0.223548  [380800/481450]\n",
      "loss: 0.165106  [384000/481450]\n",
      "loss: 0.121978  [387200/481450]\n",
      "loss: 0.219588  [390400/481450]\n",
      "loss: 0.159083  [393600/481450]\n",
      "loss: 0.370129  [396800/481450]\n",
      "loss: 0.299021  [400000/481450]\n",
      "loss: 0.402990  [403200/481450]\n",
      "loss: 0.286780  [406400/481450]\n",
      "loss: 0.201756  [409600/481450]\n",
      "loss: 0.207724  [412800/481450]\n",
      "loss: 0.257755  [416000/481450]\n",
      "loss: 0.254569  [419200/481450]\n",
      "loss: 0.351247  [422400/481450]\n",
      "loss: 0.232311  [425600/481450]\n",
      "loss: 0.368502  [428800/481450]\n",
      "loss: 0.398198  [432000/481450]\n",
      "loss: 0.263837  [435200/481450]\n",
      "loss: 0.337330  [438400/481450]\n",
      "loss: 0.319684  [441600/481450]\n",
      "loss: 0.299495  [444800/481450]\n",
      "loss: 0.459719  [448000/481450]\n",
      "loss: 0.264479  [451200/481450]\n",
      "loss: 0.149815  [454400/481450]\n",
      "loss: 0.274347  [457600/481450]\n",
      "loss: 0.295465  [460800/481450]\n",
      "loss: 0.170627  [464000/481450]\n",
      "loss: 0.231617  [467200/481450]\n",
      "loss: 0.272382  [470400/481450]\n",
      "loss: 0.269901  [473600/481450]\n",
      "loss: 0.169852  [476800/481450]\n",
      "loss: 0.135990  [480000/481450]\n",
      "Train Accuracy: 90.3101%\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.812963, F1-score: 85.13% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.340027  [    0/481450]\n",
      "loss: 0.104602  [ 3200/481450]\n",
      "loss: 0.250550  [ 6400/481450]\n",
      "loss: 0.187225  [ 9600/481450]\n",
      "loss: 0.303004  [12800/481450]\n",
      "loss: 0.153820  [16000/481450]\n",
      "loss: 0.276194  [19200/481450]\n",
      "loss: 0.145767  [22400/481450]\n",
      "loss: 0.203011  [25600/481450]\n",
      "loss: 0.273502  [28800/481450]\n",
      "loss: 0.097935  [32000/481450]\n",
      "loss: 0.091734  [35200/481450]\n",
      "loss: 0.096401  [38400/481450]\n",
      "loss: 0.380512  [41600/481450]\n",
      "loss: 0.231370  [44800/481450]\n",
      "loss: 0.253049  [48000/481450]\n",
      "loss: 0.158972  [51200/481450]\n",
      "loss: 0.176755  [54400/481450]\n",
      "loss: 0.127146  [57600/481450]\n",
      "loss: 0.249694  [60800/481450]\n",
      "loss: 0.372946  [64000/481450]\n",
      "loss: 0.133106  [67200/481450]\n",
      "loss: 0.255481  [70400/481450]\n",
      "loss: 0.290658  [73600/481450]\n",
      "loss: 0.133395  [76800/481450]\n",
      "loss: 0.131287  [80000/481450]\n",
      "loss: 0.455152  [83200/481450]\n",
      "loss: 0.186663  [86400/481450]\n",
      "loss: 0.106455  [89600/481450]\n",
      "loss: 0.093863  [92800/481450]\n",
      "loss: 0.247332  [96000/481450]\n",
      "loss: 0.231884  [99200/481450]\n",
      "loss: 0.145731  [102400/481450]\n",
      "loss: 0.223916  [105600/481450]\n",
      "loss: 0.195544  [108800/481450]\n",
      "loss: 0.176310  [112000/481450]\n",
      "loss: 0.197000  [115200/481450]\n",
      "loss: 0.203192  [118400/481450]\n",
      "loss: 0.174812  [121600/481450]\n",
      "loss: 0.242386  [124800/481450]\n",
      "loss: 0.517247  [128000/481450]\n",
      "loss: 0.129201  [131200/481450]\n",
      "loss: 0.412874  [134400/481450]\n",
      "loss: 0.627913  [137600/481450]\n",
      "loss: 0.350925  [140800/481450]\n",
      "loss: 0.176028  [144000/481450]\n",
      "loss: 0.214803  [147200/481450]\n",
      "loss: 0.152056  [150400/481450]\n",
      "loss: 0.110939  [153600/481450]\n",
      "loss: 0.159337  [156800/481450]\n",
      "loss: 0.154428  [160000/481450]\n",
      "loss: 0.303473  [163200/481450]\n",
      "loss: 0.217050  [166400/481450]\n",
      "loss: 0.263179  [169600/481450]\n",
      "loss: 0.243180  [172800/481450]\n",
      "loss: 0.313490  [176000/481450]\n",
      "loss: 0.221161  [179200/481450]\n",
      "loss: 0.162294  [182400/481450]\n",
      "loss: 0.215970  [185600/481450]\n",
      "loss: 0.331823  [188800/481450]\n",
      "loss: 0.344281  [192000/481450]\n",
      "loss: 0.228330  [195200/481450]\n",
      "loss: 0.391066  [198400/481450]\n",
      "loss: 0.275821  [201600/481450]\n",
      "loss: 0.379676  [204800/481450]\n",
      "loss: 0.423000  [208000/481450]\n",
      "loss: 0.126360  [211200/481450]\n",
      "loss: 0.353716  [214400/481450]\n",
      "loss: 0.368588  [217600/481450]\n",
      "loss: 0.141684  [220800/481450]\n",
      "loss: 0.060310  [224000/481450]\n",
      "loss: 0.582769  [227200/481450]\n",
      "loss: 0.213747  [230400/481450]\n",
      "loss: 0.553317  [233600/481450]\n",
      "loss: 0.217902  [236800/481450]\n",
      "loss: 0.335341  [240000/481450]\n",
      "loss: 0.209780  [243200/481450]\n",
      "loss: 0.265538  [246400/481450]\n",
      "loss: 0.219671  [249600/481450]\n",
      "loss: 0.086553  [252800/481450]\n",
      "loss: 0.214671  [256000/481450]\n",
      "loss: 0.287641  [259200/481450]\n",
      "loss: 0.141098  [262400/481450]\n",
      "loss: 0.444149  [265600/481450]\n",
      "loss: 0.131826  [268800/481450]\n",
      "loss: 0.114047  [272000/481450]\n",
      "loss: 0.226570  [275200/481450]\n",
      "loss: 0.263699  [278400/481450]\n",
      "loss: 0.361539  [281600/481450]\n",
      "loss: 0.196175  [284800/481450]\n",
      "loss: 0.130614  [288000/481450]\n",
      "loss: 0.125510  [291200/481450]\n",
      "loss: 0.128624  [294400/481450]\n",
      "loss: 0.140379  [297600/481450]\n",
      "loss: 0.236734  [300800/481450]\n",
      "loss: 0.484711  [304000/481450]\n",
      "loss: 0.136298  [307200/481450]\n",
      "loss: 0.274135  [310400/481450]\n",
      "loss: 0.105069  [313600/481450]\n",
      "loss: 0.376263  [316800/481450]\n",
      "loss: 0.427566  [320000/481450]\n",
      "loss: 0.367121  [323200/481450]\n",
      "loss: 0.261537  [326400/481450]\n",
      "loss: 0.302268  [329600/481450]\n",
      "loss: 0.182536  [332800/481450]\n",
      "loss: 0.202335  [336000/481450]\n",
      "loss: 0.306912  [339200/481450]\n",
      "loss: 0.114274  [342400/481450]\n",
      "loss: 0.126710  [345600/481450]\n",
      "loss: 0.385507  [348800/481450]\n",
      "loss: 0.211557  [352000/481450]\n",
      "loss: 0.239329  [355200/481450]\n",
      "loss: 0.226262  [358400/481450]\n",
      "loss: 0.320415  [361600/481450]\n",
      "loss: 0.125316  [364800/481450]\n",
      "loss: 0.194262  [368000/481450]\n",
      "loss: 0.278122  [371200/481450]\n",
      "loss: 0.340258  [374400/481450]\n",
      "loss: 0.162250  [377600/481450]\n",
      "loss: 0.351006  [380800/481450]\n",
      "loss: 0.214960  [384000/481450]\n",
      "loss: 0.340874  [387200/481450]\n",
      "loss: 0.144323  [390400/481450]\n",
      "loss: 0.207741  [393600/481450]\n",
      "loss: 0.277332  [396800/481450]\n",
      "loss: 0.415344  [400000/481450]\n",
      "loss: 0.394362  [403200/481450]\n",
      "loss: 0.195870  [406400/481450]\n",
      "loss: 0.342496  [409600/481450]\n",
      "loss: 0.317928  [412800/481450]\n",
      "loss: 0.240895  [416000/481450]\n",
      "loss: 0.241952  [419200/481450]\n",
      "loss: 0.415815  [422400/481450]\n",
      "loss: 0.184299  [425600/481450]\n",
      "loss: 0.228657  [428800/481450]\n",
      "loss: 0.136263  [432000/481450]\n",
      "loss: 0.129962  [435200/481450]\n",
      "loss: 0.143887  [438400/481450]\n",
      "loss: 0.213560  [441600/481450]\n",
      "loss: 0.327052  [444800/481450]\n",
      "loss: 0.360251  [448000/481450]\n",
      "loss: 0.143193  [451200/481450]\n",
      "loss: 0.344602  [454400/481450]\n",
      "loss: 0.194586  [457600/481450]\n",
      "loss: 0.096688  [460800/481450]\n",
      "loss: 0.316733  [464000/481450]\n",
      "loss: 0.172011  [467200/481450]\n",
      "loss: 0.267582  [470400/481450]\n",
      "loss: 0.341176  [473600/481450]\n",
      "loss: 0.220998  [476800/481450]\n",
      "loss: 0.153816  [480000/481450]\n",
      "Train Accuracy: 90.4254%\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.819137, F1-score: 85.35% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.206447  [    0/481450]\n",
      "loss: 0.095173  [ 3200/481450]\n",
      "loss: 0.167290  [ 6400/481450]\n",
      "loss: 0.198475  [ 9600/481450]\n",
      "loss: 0.073495  [12800/481450]\n",
      "loss: 0.232844  [16000/481450]\n",
      "loss: 0.323984  [19200/481450]\n",
      "loss: 0.131700  [22400/481450]\n",
      "loss: 0.338105  [25600/481450]\n",
      "loss: 0.179880  [28800/481450]\n",
      "loss: 0.335339  [32000/481450]\n",
      "loss: 0.081896  [35200/481450]\n",
      "loss: 0.130361  [38400/481450]\n",
      "loss: 0.179850  [41600/481450]\n",
      "loss: 0.177776  [44800/481450]\n",
      "loss: 0.269681  [48000/481450]\n",
      "loss: 0.171631  [51200/481450]\n",
      "loss: 0.087772  [54400/481450]\n",
      "loss: 0.256893  [57600/481450]\n",
      "loss: 0.267244  [60800/481450]\n",
      "loss: 0.481218  [64000/481450]\n",
      "loss: 0.212642  [67200/481450]\n",
      "loss: 0.232884  [70400/481450]\n",
      "loss: 0.132787  [73600/481450]\n",
      "loss: 0.156079  [76800/481450]\n",
      "loss: 0.289059  [80000/481450]\n",
      "loss: 0.259288  [83200/481450]\n",
      "loss: 0.091834  [86400/481450]\n",
      "loss: 0.308227  [89600/481450]\n",
      "loss: 0.236041  [92800/481450]\n",
      "loss: 0.128072  [96000/481450]\n",
      "loss: 0.201480  [99200/481450]\n",
      "loss: 0.168775  [102400/481450]\n",
      "loss: 0.131095  [105600/481450]\n",
      "loss: 0.114518  [108800/481450]\n",
      "loss: 0.286707  [112000/481450]\n",
      "loss: 0.105199  [115200/481450]\n",
      "loss: 0.146617  [118400/481450]\n",
      "loss: 0.274697  [121600/481450]\n",
      "loss: 0.206740  [124800/481450]\n",
      "loss: 0.145590  [128000/481450]\n",
      "loss: 0.159139  [131200/481450]\n",
      "loss: 0.195864  [134400/481450]\n",
      "loss: 0.237096  [137600/481450]\n",
      "loss: 0.165842  [140800/481450]\n",
      "loss: 0.135364  [144000/481450]\n",
      "loss: 0.229570  [147200/481450]\n",
      "loss: 0.344264  [150400/481450]\n",
      "loss: 0.223512  [153600/481450]\n",
      "loss: 0.149187  [156800/481450]\n",
      "loss: 0.141434  [160000/481450]\n",
      "loss: 0.177611  [163200/481450]\n",
      "loss: 0.196319  [166400/481450]\n",
      "loss: 0.247779  [169600/481450]\n",
      "loss: 0.378073  [172800/481450]\n",
      "loss: 0.269634  [176000/481450]\n",
      "loss: 0.088052  [179200/481450]\n",
      "loss: 0.479307  [182400/481450]\n",
      "loss: 0.117250  [185600/481450]\n",
      "loss: 0.313155  [188800/481450]\n",
      "loss: 0.159781  [192000/481450]\n",
      "loss: 0.370855  [195200/481450]\n",
      "loss: 0.230991  [198400/481450]\n",
      "loss: 0.171995  [201600/481450]\n",
      "loss: 0.219070  [204800/481450]\n",
      "loss: 0.263897  [208000/481450]\n",
      "loss: 0.090454  [211200/481450]\n",
      "loss: 0.412530  [214400/481450]\n",
      "loss: 0.213990  [217600/481450]\n",
      "loss: 0.426507  [220800/481450]\n",
      "loss: 0.346384  [224000/481450]\n",
      "loss: 0.264043  [227200/481450]\n",
      "loss: 0.117833  [230400/481450]\n",
      "loss: 0.242398  [233600/481450]\n",
      "loss: 0.159031  [236800/481450]\n",
      "loss: 0.181291  [240000/481450]\n",
      "loss: 0.394733  [243200/481450]\n",
      "loss: 0.389873  [246400/481450]\n",
      "loss: 0.273762  [249600/481450]\n",
      "loss: 0.105568  [252800/481450]\n",
      "loss: 0.170401  [256000/481450]\n",
      "loss: 0.194314  [259200/481450]\n",
      "loss: 0.473265  [262400/481450]\n",
      "loss: 0.333107  [265600/481450]\n",
      "loss: 0.360043  [268800/481450]\n",
      "loss: 0.375783  [272000/481450]\n",
      "loss: 0.115112  [275200/481450]\n",
      "loss: 0.154270  [278400/481450]\n",
      "loss: 0.322790  [281600/481450]\n",
      "loss: 0.336556  [284800/481450]\n",
      "loss: 0.337409  [288000/481450]\n",
      "loss: 0.150529  [291200/481450]\n",
      "loss: 0.219393  [294400/481450]\n",
      "loss: 0.626465  [297600/481450]\n",
      "loss: 0.157318  [300800/481450]\n",
      "loss: 0.174992  [304000/481450]\n",
      "loss: 0.344384  [307200/481450]\n",
      "loss: 0.410820  [310400/481450]\n",
      "loss: 0.329077  [313600/481450]\n",
      "loss: 0.218032  [316800/481450]\n",
      "loss: 0.192318  [320000/481450]\n",
      "loss: 0.137880  [323200/481450]\n",
      "loss: 0.160386  [326400/481450]\n",
      "loss: 0.167222  [329600/481450]\n",
      "loss: 0.074804  [332800/481450]\n",
      "loss: 0.132355  [336000/481450]\n",
      "loss: 0.464658  [339200/481450]\n",
      "loss: 0.229560  [342400/481450]\n",
      "loss: 0.262819  [345600/481450]\n",
      "loss: 0.275017  [348800/481450]\n",
      "loss: 0.103028  [352000/481450]\n",
      "loss: 0.404757  [355200/481450]\n",
      "loss: 0.275418  [358400/481450]\n",
      "loss: 0.305471  [361600/481450]\n",
      "loss: 0.223084  [364800/481450]\n",
      "loss: 0.187716  [368000/481450]\n",
      "loss: 0.338729  [371200/481450]\n",
      "loss: 0.196672  [374400/481450]\n",
      "loss: 0.162851  [377600/481450]\n",
      "loss: 0.280514  [380800/481450]\n",
      "loss: 0.248463  [384000/481450]\n",
      "loss: 0.352300  [387200/481450]\n",
      "loss: 0.276548  [390400/481450]\n",
      "loss: 0.301268  [393600/481450]\n",
      "loss: 0.093878  [396800/481450]\n",
      "loss: 0.142633  [400000/481450]\n",
      "loss: 0.258315  [403200/481450]\n",
      "loss: 0.278717  [406400/481450]\n",
      "loss: 0.334946  [409600/481450]\n",
      "loss: 0.337041  [412800/481450]\n",
      "loss: 0.249680  [416000/481450]\n",
      "loss: 0.163351  [419200/481450]\n",
      "loss: 0.183166  [422400/481450]\n",
      "loss: 0.104744  [425600/481450]\n",
      "loss: 0.347171  [428800/481450]\n",
      "loss: 0.166387  [432000/481450]\n",
      "loss: 0.130485  [435200/481450]\n",
      "loss: 0.354733  [438400/481450]\n",
      "loss: 0.210613  [441600/481450]\n",
      "loss: 0.141769  [444800/481450]\n",
      "loss: 0.195026  [448000/481450]\n",
      "loss: 0.074301  [451200/481450]\n",
      "loss: 0.267051  [454400/481450]\n",
      "loss: 0.474242  [457600/481450]\n",
      "loss: 0.289335  [460800/481450]\n",
      "loss: 0.415604  [464000/481450]\n",
      "loss: 0.249941  [467200/481450]\n",
      "loss: 0.229536  [470400/481450]\n",
      "loss: 0.213994  [473600/481450]\n",
      "loss: 0.125909  [476800/481450]\n",
      "loss: 0.240275  [480000/481450]\n",
      "Train Accuracy: 90.5467%\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.818205, F1-score: 85.66% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.276681  [    0/481450]\n",
      "loss: 0.086052  [ 3200/481450]\n",
      "loss: 0.205607  [ 6400/481450]\n",
      "loss: 0.244650  [ 9600/481450]\n",
      "loss: 0.033581  [12800/481450]\n",
      "loss: 0.264437  [16000/481450]\n",
      "loss: 0.262237  [19200/481450]\n",
      "loss: 0.439786  [22400/481450]\n",
      "loss: 0.137305  [25600/481450]\n",
      "loss: 0.190549  [28800/481450]\n",
      "loss: 0.257774  [32000/481450]\n",
      "loss: 0.439433  [35200/481450]\n",
      "loss: 0.325077  [38400/481450]\n",
      "loss: 0.464086  [41600/481450]\n",
      "loss: 0.137868  [44800/481450]\n",
      "loss: 0.228793  [48000/481450]\n",
      "loss: 0.099704  [51200/481450]\n",
      "loss: 0.183760  [54400/481450]\n",
      "loss: 0.088334  [57600/481450]\n",
      "loss: 0.145446  [60800/481450]\n",
      "loss: 0.251130  [64000/481450]\n",
      "loss: 0.141997  [67200/481450]\n",
      "loss: 0.260257  [70400/481450]\n",
      "loss: 0.322958  [73600/481450]\n",
      "loss: 0.175401  [76800/481450]\n",
      "loss: 0.220108  [80000/481450]\n",
      "loss: 0.209593  [83200/481450]\n",
      "loss: 0.438484  [86400/481450]\n",
      "loss: 0.255931  [89600/481450]\n",
      "loss: 0.331223  [92800/481450]\n",
      "loss: 0.578102  [96000/481450]\n",
      "loss: 0.196901  [99200/481450]\n",
      "loss: 0.382690  [102400/481450]\n",
      "loss: 0.137299  [105600/481450]\n",
      "loss: 0.200511  [108800/481450]\n",
      "loss: 0.356732  [112000/481450]\n",
      "loss: 0.028359  [115200/481450]\n",
      "loss: 0.105895  [118400/481450]\n",
      "loss: 0.192795  [121600/481450]\n",
      "loss: 0.243193  [124800/481450]\n",
      "loss: 0.133255  [128000/481450]\n",
      "loss: 0.153520  [131200/481450]\n",
      "loss: 0.267056  [134400/481450]\n",
      "loss: 0.236939  [137600/481450]\n",
      "loss: 0.165416  [140800/481450]\n",
      "loss: 0.589168  [144000/481450]\n",
      "loss: 0.054791  [147200/481450]\n",
      "loss: 0.271665  [150400/481450]\n",
      "loss: 0.153842  [153600/481450]\n",
      "loss: 0.249680  [156800/481450]\n",
      "loss: 0.373588  [160000/481450]\n",
      "loss: 0.290724  [163200/481450]\n",
      "loss: 0.186538  [166400/481450]\n",
      "loss: 0.141824  [169600/481450]\n",
      "loss: 0.265213  [172800/481450]\n",
      "loss: 0.108174  [176000/481450]\n",
      "loss: 0.139700  [179200/481450]\n",
      "loss: 0.194829  [182400/481450]\n",
      "loss: 0.037774  [185600/481450]\n",
      "loss: 0.167724  [188800/481450]\n",
      "loss: 0.158994  [192000/481450]\n",
      "loss: 0.313393  [195200/481450]\n",
      "loss: 0.073880  [198400/481450]\n",
      "loss: 0.414597  [201600/481450]\n",
      "loss: 0.085818  [204800/481450]\n",
      "loss: 0.189904  [208000/481450]\n",
      "loss: 0.185830  [211200/481450]\n",
      "loss: 0.128857  [214400/481450]\n",
      "loss: 0.165328  [217600/481450]\n",
      "loss: 0.088752  [220800/481450]\n",
      "loss: 0.119880  [224000/481450]\n",
      "loss: 0.377716  [227200/481450]\n",
      "loss: 0.118466  [230400/481450]\n",
      "loss: 0.164325  [233600/481450]\n",
      "loss: 0.378578  [236800/481450]\n",
      "loss: 0.179542  [240000/481450]\n",
      "loss: 0.157156  [243200/481450]\n",
      "loss: 0.165018  [246400/481450]\n",
      "loss: 0.135496  [249600/481450]\n",
      "loss: 0.078550  [252800/481450]\n",
      "loss: 0.078565  [256000/481450]\n",
      "loss: 0.464292  [259200/481450]\n",
      "loss: 0.188570  [262400/481450]\n",
      "loss: 0.156053  [265600/481450]\n",
      "loss: 0.164074  [268800/481450]\n",
      "loss: 0.235725  [272000/481450]\n",
      "loss: 0.221168  [275200/481450]\n",
      "loss: 0.345481  [278400/481450]\n",
      "loss: 0.158989  [281600/481450]\n",
      "loss: 0.068129  [284800/481450]\n",
      "loss: 0.033615  [288000/481450]\n",
      "loss: 0.113071  [291200/481450]\n",
      "loss: 0.232483  [294400/481450]\n",
      "loss: 0.349106  [297600/481450]\n",
      "loss: 0.246813  [300800/481450]\n",
      "loss: 0.275028  [304000/481450]\n",
      "loss: 0.315069  [307200/481450]\n",
      "loss: 0.294881  [310400/481450]\n",
      "loss: 0.084955  [313600/481450]\n",
      "loss: 0.103748  [316800/481450]\n",
      "loss: 0.223076  [320000/481450]\n",
      "loss: 0.644876  [323200/481450]\n",
      "loss: 0.045545  [326400/481450]\n",
      "loss: 0.401320  [329600/481450]\n",
      "loss: 0.221748  [332800/481450]\n",
      "loss: 0.104518  [336000/481450]\n",
      "loss: 0.055317  [339200/481450]\n",
      "loss: 0.290311  [342400/481450]\n",
      "loss: 0.096721  [345600/481450]\n",
      "loss: 0.301603  [348800/481450]\n",
      "loss: 0.232652  [352000/481450]\n",
      "loss: 0.213252  [355200/481450]\n",
      "loss: 0.349334  [358400/481450]\n",
      "loss: 0.281945  [361600/481450]\n",
      "loss: 0.299972  [364800/481450]\n",
      "loss: 0.384832  [368000/481450]\n",
      "loss: 0.251429  [371200/481450]\n",
      "loss: 0.235037  [374400/481450]\n",
      "loss: 0.058526  [377600/481450]\n",
      "loss: 0.304228  [380800/481450]\n",
      "loss: 0.335959  [384000/481450]\n",
      "loss: 0.541051  [387200/481450]\n",
      "loss: 0.241189  [390400/481450]\n",
      "loss: 0.266864  [393600/481450]\n",
      "loss: 0.351544  [396800/481450]\n",
      "loss: 0.143912  [400000/481450]\n",
      "loss: 0.116154  [403200/481450]\n",
      "loss: 0.105153  [406400/481450]\n",
      "loss: 0.375140  [409600/481450]\n",
      "loss: 0.318338  [412800/481450]\n",
      "loss: 0.129067  [416000/481450]\n",
      "loss: 0.162307  [419200/481450]\n",
      "loss: 0.040983  [422400/481450]\n",
      "loss: 0.226090  [425600/481450]\n",
      "loss: 0.134035  [428800/481450]\n",
      "loss: 0.127347  [432000/481450]\n",
      "loss: 0.145631  [435200/481450]\n",
      "loss: 0.022731  [438400/481450]\n",
      "loss: 0.150453  [441600/481450]\n",
      "loss: 0.253901  [444800/481450]\n",
      "loss: 0.225962  [448000/481450]\n",
      "loss: 0.378385  [451200/481450]\n",
      "loss: 0.204081  [454400/481450]\n",
      "loss: 0.274953  [457600/481450]\n",
      "loss: 0.281215  [460800/481450]\n",
      "loss: 0.076489  [464000/481450]\n",
      "loss: 0.222031  [467200/481450]\n",
      "loss: 0.207191  [470400/481450]\n",
      "loss: 0.201174  [473600/481450]\n",
      "loss: 0.074778  [476800/481450]\n",
      "loss: 0.238783  [480000/481450]\n",
      "Train Accuracy: 90.6165%\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.830471, F1-score: 85.53% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,scheduler):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.407161  [    0/481450]\n",
      "loss: 0.248885  [ 3200/481450]\n",
      "loss: 0.302694  [ 6400/481450]\n",
      "loss: 0.290368  [ 9600/481450]\n",
      "loss: 0.248356  [12800/481450]\n",
      "loss: 0.347584  [16000/481450]\n",
      "loss: 0.260450  [19200/481450]\n",
      "loss: 0.153373  [22400/481450]\n",
      "loss: 0.211395  [25600/481450]\n",
      "loss: 0.280048  [28800/481450]\n",
      "loss: 0.551072  [32000/481450]\n",
      "loss: 0.242100  [35200/481450]\n",
      "loss: 0.376839  [38400/481450]\n",
      "loss: 0.530344  [41600/481450]\n",
      "loss: 0.047740  [44800/481450]\n",
      "loss: 0.382037  [48000/481450]\n",
      "loss: 0.133002  [51200/481450]\n",
      "loss: 0.175415  [54400/481450]\n",
      "loss: 0.266727  [57600/481450]\n",
      "loss: 0.356649  [60800/481450]\n",
      "loss: 0.295790  [64000/481450]\n",
      "loss: 0.259346  [67200/481450]\n",
      "loss: 0.229986  [70400/481450]\n",
      "loss: 0.121399  [73600/481450]\n",
      "loss: 0.195167  [76800/481450]\n",
      "loss: 0.277340  [80000/481450]\n",
      "loss: 0.217338  [83200/481450]\n",
      "loss: 0.231709  [86400/481450]\n",
      "loss: 0.324257  [89600/481450]\n",
      "loss: 0.237235  [92800/481450]\n",
      "loss: 0.321577  [96000/481450]\n",
      "loss: 0.341236  [99200/481450]\n",
      "loss: 0.145905  [102400/481450]\n",
      "loss: 0.330484  [105600/481450]\n",
      "loss: 0.294633  [108800/481450]\n",
      "loss: 0.231578  [112000/481450]\n",
      "loss: 0.222409  [115200/481450]\n",
      "loss: 0.461695  [118400/481450]\n",
      "loss: 0.374904  [121600/481450]\n",
      "loss: 0.114602  [124800/481450]\n",
      "loss: 0.353723  [128000/481450]\n",
      "loss: 0.183473  [131200/481450]\n",
      "loss: 0.436538  [134400/481450]\n",
      "loss: 0.075138  [137600/481450]\n",
      "loss: 0.300795  [140800/481450]\n",
      "loss: 0.529032  [144000/481450]\n",
      "loss: 0.270004  [147200/481450]\n",
      "loss: 0.366657  [150400/481450]\n",
      "loss: 0.311112  [153600/481450]\n",
      "loss: 0.211928  [156800/481450]\n",
      "loss: 0.268379  [160000/481450]\n",
      "loss: 0.611624  [163200/481450]\n",
      "loss: 0.400922  [166400/481450]\n",
      "loss: 0.316510  [169600/481450]\n",
      "loss: 0.549950  [172800/481450]\n",
      "loss: 0.154515  [176000/481450]\n",
      "loss: 0.431937  [179200/481450]\n",
      "loss: 0.287123  [182400/481450]\n",
      "loss: 0.369408  [185600/481450]\n",
      "loss: 0.239024  [188800/481450]\n",
      "loss: 0.154488  [192000/481450]\n",
      "loss: 0.500535  [195200/481450]\n",
      "loss: 0.262849  [198400/481450]\n",
      "loss: 0.297963  [201600/481450]\n",
      "loss: 0.255245  [204800/481450]\n",
      "loss: 0.232667  [208000/481450]\n",
      "loss: 0.243727  [211200/481450]\n",
      "loss: 0.390564  [214400/481450]\n",
      "loss: 0.401098  [217600/481450]\n",
      "loss: 0.323042  [220800/481450]\n",
      "loss: 0.279806  [224000/481450]\n",
      "loss: 0.169533  [227200/481450]\n",
      "loss: 0.267745  [230400/481450]\n",
      "loss: 0.258768  [233600/481450]\n",
      "loss: 0.283328  [236800/481450]\n",
      "loss: 0.362440  [240000/481450]\n",
      "loss: 0.299089  [243200/481450]\n",
      "loss: 0.230293  [246400/481450]\n",
      "loss: 0.178301  [249600/481450]\n",
      "loss: 0.198777  [252800/481450]\n",
      "loss: 0.397015  [256000/481450]\n",
      "loss: 0.160599  [259200/481450]\n",
      "loss: 0.179553  [262400/481450]\n",
      "loss: 0.232552  [265600/481450]\n",
      "loss: 0.423253  [268800/481450]\n",
      "loss: 0.337919  [272000/481450]\n",
      "loss: 0.583646  [275200/481450]\n",
      "loss: 0.085314  [278400/481450]\n",
      "loss: 0.299244  [281600/481450]\n",
      "loss: 0.263522  [284800/481450]\n",
      "loss: 0.263142  [288000/481450]\n",
      "loss: 0.225995  [291200/481450]\n",
      "loss: 0.201337  [294400/481450]\n",
      "loss: 0.242055  [297600/481450]\n",
      "loss: 0.544868  [300800/481450]\n",
      "loss: 0.288992  [304000/481450]\n",
      "loss: 0.232154  [307200/481450]\n",
      "loss: 0.327430  [310400/481450]\n",
      "loss: 0.363444  [313600/481450]\n",
      "loss: 0.120751  [316800/481450]\n",
      "loss: 0.183201  [320000/481450]\n",
      "loss: 0.429792  [323200/481450]\n",
      "loss: 0.318540  [326400/481450]\n",
      "loss: 0.253163  [329600/481450]\n",
      "loss: 0.262380  [332800/481450]\n",
      "loss: 0.065487  [336000/481450]\n",
      "loss: 0.188800  [339200/481450]\n",
      "loss: 0.328083  [342400/481450]\n",
      "loss: 0.235447  [345600/481450]\n",
      "loss: 0.232013  [348800/481450]\n",
      "loss: 0.342847  [352000/481450]\n",
      "loss: 0.583130  [355200/481450]\n",
      "loss: 0.245348  [358400/481450]\n",
      "loss: 0.412512  [361600/481450]\n",
      "loss: 0.357966  [364800/481450]\n",
      "loss: 0.363364  [368000/481450]\n",
      "loss: 0.430090  [371200/481450]\n",
      "loss: 0.153763  [374400/481450]\n",
      "loss: 0.200360  [377600/481450]\n",
      "loss: 0.485753  [380800/481450]\n",
      "loss: 0.338388  [384000/481450]\n",
      "loss: 0.066615  [387200/481450]\n",
      "loss: 0.148382  [390400/481450]\n",
      "loss: 0.192716  [393600/481450]\n",
      "loss: 0.297759  [396800/481450]\n",
      "loss: 0.452961  [400000/481450]\n",
      "loss: 0.243701  [403200/481450]\n",
      "loss: 0.454874  [406400/481450]\n",
      "loss: 0.168116  [409600/481450]\n",
      "loss: 0.150864  [412800/481450]\n",
      "loss: 0.392234  [416000/481450]\n",
      "loss: 0.319392  [419200/481450]\n",
      "loss: 0.288071  [422400/481450]\n",
      "loss: 0.215188  [425600/481450]\n",
      "loss: 0.714944  [428800/481450]\n",
      "loss: 0.113952  [432000/481450]\n",
      "loss: 0.210401  [435200/481450]\n",
      "loss: 0.425279  [438400/481450]\n",
      "loss: 0.233624  [441600/481450]\n",
      "loss: 0.507988  [444800/481450]\n",
      "loss: 0.320946  [448000/481450]\n",
      "loss: 0.155006  [451200/481450]\n",
      "loss: 0.334917  [454400/481450]\n",
      "loss: 0.357269  [457600/481450]\n",
      "loss: 0.281123  [460800/481450]\n",
      "loss: 0.294178  [464000/481450]\n",
      "loss: 0.239332  [467200/481450]\n",
      "loss: 0.181043  [470400/481450]\n",
      "loss: 0.162510  [473600/481450]\n",
      "loss: 0.343567  [476800/481450]\n",
      "loss: 0.146843  [480000/481450]\n",
      "Train Accuracy: 89.0082%\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.739847, F1-score: 82.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.136654  [    0/481450]\n",
      "loss: 0.368329  [ 3200/481450]\n",
      "loss: 0.243074  [ 6400/481450]\n",
      "loss: 0.204710  [ 9600/481450]\n",
      "loss: 0.167110  [12800/481450]\n",
      "loss: 0.313853  [16000/481450]\n",
      "loss: 0.122421  [19200/481450]\n",
      "loss: 0.276794  [22400/481450]\n",
      "loss: 0.138305  [25600/481450]\n",
      "loss: 0.157936  [28800/481450]\n",
      "loss: 0.232937  [32000/481450]\n",
      "loss: 0.459152  [35200/481450]\n",
      "loss: 0.398669  [38400/481450]\n",
      "loss: 0.200324  [41600/481450]\n",
      "loss: 0.217362  [44800/481450]\n",
      "loss: 0.241058  [48000/481450]\n",
      "loss: 0.239651  [51200/481450]\n",
      "loss: 0.150701  [54400/481450]\n",
      "loss: 0.304414  [57600/481450]\n",
      "loss: 0.268291  [60800/481450]\n",
      "loss: 0.491986  [64000/481450]\n",
      "loss: 0.110887  [67200/481450]\n",
      "loss: 0.231932  [70400/481450]\n",
      "loss: 0.207168  [73600/481450]\n",
      "loss: 0.204157  [76800/481450]\n",
      "loss: 0.202611  [80000/481450]\n",
      "loss: 0.259685  [83200/481450]\n",
      "loss: 0.274524  [86400/481450]\n",
      "loss: 0.111567  [89600/481450]\n",
      "loss: 0.202459  [92800/481450]\n",
      "loss: 0.215723  [96000/481450]\n",
      "loss: 0.280482  [99200/481450]\n",
      "loss: 0.270081  [102400/481450]\n",
      "loss: 0.660749  [105600/481450]\n",
      "loss: 0.171807  [108800/481450]\n",
      "loss: 0.102703  [112000/481450]\n",
      "loss: 0.143221  [115200/481450]\n",
      "loss: 0.259784  [118400/481450]\n",
      "loss: 0.203459  [121600/481450]\n",
      "loss: 0.263079  [124800/481450]\n",
      "loss: 0.329582  [128000/481450]\n",
      "loss: 0.375748  [131200/481450]\n",
      "loss: 0.048494  [134400/481450]\n",
      "loss: 0.102307  [137600/481450]\n",
      "loss: 0.253004  [140800/481450]\n",
      "loss: 0.282339  [144000/481450]\n",
      "loss: 0.156198  [147200/481450]\n",
      "loss: 0.138395  [150400/481450]\n",
      "loss: 0.215177  [153600/481450]\n",
      "loss: 0.276198  [156800/481450]\n",
      "loss: 0.252224  [160000/481450]\n",
      "loss: 0.174219  [163200/481450]\n",
      "loss: 0.351637  [166400/481450]\n",
      "loss: 0.238186  [169600/481450]\n",
      "loss: 0.459725  [172800/481450]\n",
      "loss: 0.327901  [176000/481450]\n",
      "loss: 0.251471  [179200/481450]\n",
      "loss: 0.110704  [182400/481450]\n",
      "loss: 0.299679  [185600/481450]\n",
      "loss: 0.149147  [188800/481450]\n",
      "loss: 0.433021  [192000/481450]\n",
      "loss: 0.467700  [195200/481450]\n",
      "loss: 0.121317  [198400/481450]\n",
      "loss: 0.422833  [201600/481450]\n",
      "loss: 0.268249  [204800/481450]\n",
      "loss: 0.243460  [208000/481450]\n",
      "loss: 0.293902  [211200/481450]\n",
      "loss: 0.362537  [214400/481450]\n",
      "loss: 0.227617  [217600/481450]\n",
      "loss: 0.116834  [220800/481450]\n",
      "loss: 0.464069  [224000/481450]\n",
      "loss: 0.476940  [227200/481450]\n",
      "loss: 0.234702  [230400/481450]\n",
      "loss: 0.290182  [233600/481450]\n",
      "loss: 0.245807  [236800/481450]\n",
      "loss: 0.180711  [240000/481450]\n",
      "loss: 0.374632  [243200/481450]\n",
      "loss: 0.409156  [246400/481450]\n",
      "loss: 0.240222  [249600/481450]\n",
      "loss: 0.212800  [252800/481450]\n",
      "loss: 0.251950  [256000/481450]\n",
      "loss: 0.190250  [259200/481450]\n",
      "loss: 0.226821  [262400/481450]\n",
      "loss: 0.397436  [265600/481450]\n",
      "loss: 0.574393  [268800/481450]\n",
      "loss: 0.468974  [272000/481450]\n",
      "loss: 0.237940  [275200/481450]\n",
      "loss: 0.234878  [278400/481450]\n",
      "loss: 0.268609  [281600/481450]\n",
      "loss: 0.139865  [284800/481450]\n",
      "loss: 0.392539  [288000/481450]\n",
      "loss: 0.142212  [291200/481450]\n",
      "loss: 0.238843  [294400/481450]\n",
      "loss: 0.160315  [297600/481450]\n",
      "loss: 0.289717  [300800/481450]\n",
      "loss: 0.104080  [304000/481450]\n",
      "loss: 0.395855  [307200/481450]\n",
      "loss: 0.194121  [310400/481450]\n",
      "loss: 0.199162  [313600/481450]\n",
      "loss: 0.426136  [316800/481450]\n",
      "loss: 0.193576  [320000/481450]\n",
      "loss: 0.212434  [323200/481450]\n",
      "loss: 0.267520  [326400/481450]\n",
      "loss: 0.287209  [329600/481450]\n",
      "loss: 0.308981  [332800/481450]\n",
      "loss: 0.243371  [336000/481450]\n",
      "loss: 0.321738  [339200/481450]\n",
      "loss: 0.246937  [342400/481450]\n",
      "loss: 0.391448  [345600/481450]\n",
      "loss: 0.187862  [348800/481450]\n",
      "loss: 0.298328  [352000/481450]\n",
      "loss: 0.262802  [355200/481450]\n",
      "loss: 0.522074  [358400/481450]\n",
      "loss: 0.137834  [361600/481450]\n",
      "loss: 0.166795  [364800/481450]\n",
      "loss: 0.158742  [368000/481450]\n",
      "loss: 0.141143  [371200/481450]\n",
      "loss: 0.307698  [374400/481450]\n",
      "loss: 0.514790  [377600/481450]\n",
      "loss: 0.322999  [380800/481450]\n",
      "loss: 0.335801  [384000/481450]\n",
      "loss: 0.286333  [387200/481450]\n",
      "loss: 0.125826  [390400/481450]\n",
      "loss: 0.144957  [393600/481450]\n",
      "loss: 0.254528  [396800/481450]\n",
      "loss: 0.249322  [400000/481450]\n",
      "loss: 0.229459  [403200/481450]\n",
      "loss: 0.385144  [406400/481450]\n",
      "loss: 0.233041  [409600/481450]\n",
      "loss: 0.222417  [412800/481450]\n",
      "loss: 0.096200  [416000/481450]\n",
      "loss: 0.331538  [419200/481450]\n",
      "loss: 0.193939  [422400/481450]\n",
      "loss: 0.149129  [425600/481450]\n",
      "loss: 0.049958  [428800/481450]\n",
      "loss: 0.282548  [432000/481450]\n",
      "loss: 0.100730  [435200/481450]\n",
      "loss: 0.604804  [438400/481450]\n",
      "loss: 0.440074  [441600/481450]\n",
      "loss: 0.211893  [444800/481450]\n",
      "loss: 0.174431  [448000/481450]\n",
      "loss: 0.163588  [451200/481450]\n",
      "loss: 0.199490  [454400/481450]\n",
      "loss: 0.122595  [457600/481450]\n",
      "loss: 0.191945  [460800/481450]\n",
      "loss: 0.209528  [464000/481450]\n",
      "loss: 0.124475  [467200/481450]\n",
      "loss: 0.137436  [470400/481450]\n",
      "loss: 0.281482  [473600/481450]\n",
      "loss: 0.265184  [476800/481450]\n",
      "loss: 0.368491  [480000/481450]\n",
      "Train Accuracy: 89.0323%\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.742177, F1-score: 82.20% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.353654  [    0/481450]\n",
      "loss: 0.282764  [ 3200/481450]\n",
      "loss: 0.119998  [ 6400/481450]\n",
      "loss: 0.309782  [ 9600/481450]\n",
      "loss: 0.307538  [12800/481450]\n",
      "loss: 0.401140  [16000/481450]\n",
      "loss: 0.149842  [19200/481450]\n",
      "loss: 0.245336  [22400/481450]\n",
      "loss: 0.506552  [25600/481450]\n",
      "loss: 0.278543  [28800/481450]\n",
      "loss: 0.244985  [32000/481450]\n",
      "loss: 0.352022  [35200/481450]\n",
      "loss: 0.140961  [38400/481450]\n",
      "loss: 0.170154  [41600/481450]\n",
      "loss: 0.125209  [44800/481450]\n",
      "loss: 0.223389  [48000/481450]\n",
      "loss: 0.206618  [51200/481450]\n",
      "loss: 0.035887  [54400/481450]\n",
      "loss: 0.241296  [57600/481450]\n",
      "loss: 0.384697  [60800/481450]\n",
      "loss: 0.407663  [64000/481450]\n",
      "loss: 0.306269  [67200/481450]\n",
      "loss: 0.245748  [70400/481450]\n",
      "loss: 0.180087  [73600/481450]\n",
      "loss: 0.248832  [76800/481450]\n",
      "loss: 0.343468  [80000/481450]\n",
      "loss: 0.182839  [83200/481450]\n",
      "loss: 0.700092  [86400/481450]\n",
      "loss: 0.287006  [89600/481450]\n",
      "loss: 0.179207  [92800/481450]\n",
      "loss: 0.307025  [96000/481450]\n",
      "loss: 0.154231  [99200/481450]\n",
      "loss: 0.124484  [102400/481450]\n",
      "loss: 0.251109  [105600/481450]\n",
      "loss: 0.214698  [108800/481450]\n",
      "loss: 0.231565  [112000/481450]\n",
      "loss: 0.086544  [115200/481450]\n",
      "loss: 0.208553  [118400/481450]\n",
      "loss: 0.124867  [121600/481450]\n",
      "loss: 0.147413  [124800/481450]\n",
      "loss: 0.127426  [128000/481450]\n",
      "loss: 0.401878  [131200/481450]\n",
      "loss: 0.349625  [134400/481450]\n",
      "loss: 0.151354  [137600/481450]\n",
      "loss: 0.234258  [140800/481450]\n",
      "loss: 0.210037  [144000/481450]\n",
      "loss: 0.272929  [147200/481450]\n",
      "loss: 0.155207  [150400/481450]\n",
      "loss: 0.315878  [153600/481450]\n",
      "loss: 0.262768  [156800/481450]\n",
      "loss: 0.285958  [160000/481450]\n",
      "loss: 0.242586  [163200/481450]\n",
      "loss: 0.235528  [166400/481450]\n",
      "loss: 0.256415  [169600/481450]\n",
      "loss: 0.575909  [172800/481450]\n",
      "loss: 0.137471  [176000/481450]\n",
      "loss: 0.204600  [179200/481450]\n",
      "loss: 0.490567  [182400/481450]\n",
      "loss: 0.238055  [185600/481450]\n",
      "loss: 0.310483  [188800/481450]\n",
      "loss: 0.420595  [192000/481450]\n",
      "loss: 0.184580  [195200/481450]\n",
      "loss: 0.220616  [198400/481450]\n",
      "loss: 0.168860  [201600/481450]\n",
      "loss: 0.215252  [204800/481450]\n",
      "loss: 0.119973  [208000/481450]\n",
      "loss: 0.255692  [211200/481450]\n",
      "loss: 0.454876  [214400/481450]\n",
      "loss: 0.503721  [217600/481450]\n",
      "loss: 0.296183  [220800/481450]\n",
      "loss: 0.465931  [224000/481450]\n",
      "loss: 0.115768  [227200/481450]\n",
      "loss: 0.228265  [230400/481450]\n",
      "loss: 0.221039  [233600/481450]\n",
      "loss: 0.132875  [236800/481450]\n",
      "loss: 0.349381  [240000/481450]\n",
      "loss: 0.235903  [243200/481450]\n",
      "loss: 0.286615  [246400/481450]\n",
      "loss: 0.195384  [249600/481450]\n",
      "loss: 0.511977  [252800/481450]\n",
      "loss: 0.414239  [256000/481450]\n",
      "loss: 0.269433  [259200/481450]\n",
      "loss: 0.113485  [262400/481450]\n",
      "loss: 0.362263  [265600/481450]\n",
      "loss: 0.356240  [268800/481450]\n",
      "loss: 0.273358  [272000/481450]\n",
      "loss: 0.195170  [275200/481450]\n",
      "loss: 0.195011  [278400/481450]\n",
      "loss: 0.330156  [281600/481450]\n",
      "loss: 0.160957  [284800/481450]\n",
      "loss: 0.226154  [288000/481450]\n",
      "loss: 0.350448  [291200/481450]\n",
      "loss: 0.178663  [294400/481450]\n",
      "loss: 0.303872  [297600/481450]\n",
      "loss: 0.392151  [300800/481450]\n",
      "loss: 0.123496  [304000/481450]\n",
      "loss: 0.276022  [307200/481450]\n",
      "loss: 0.442038  [310400/481450]\n",
      "loss: 0.411876  [313600/481450]\n",
      "loss: 0.218775  [316800/481450]\n",
      "loss: 0.412377  [320000/481450]\n",
      "loss: 0.304874  [323200/481450]\n",
      "loss: 0.214845  [326400/481450]\n",
      "loss: 0.266381  [329600/481450]\n",
      "loss: 0.165160  [332800/481450]\n",
      "loss: 0.210390  [336000/481450]\n",
      "loss: 0.162752  [339200/481450]\n",
      "loss: 0.321381  [342400/481450]\n",
      "loss: 0.349603  [345600/481450]\n",
      "loss: 0.422335  [348800/481450]\n",
      "loss: 0.369955  [352000/481450]\n",
      "loss: 0.230219  [355200/481450]\n",
      "loss: 0.279677  [358400/481450]\n",
      "loss: 0.269737  [361600/481450]\n",
      "loss: 0.176767  [364800/481450]\n",
      "loss: 0.488402  [368000/481450]\n",
      "loss: 0.222755  [371200/481450]\n",
      "loss: 0.306492  [374400/481450]\n",
      "loss: 0.232470  [377600/481450]\n",
      "loss: 0.216635  [380800/481450]\n",
      "loss: 0.021140  [384000/481450]\n",
      "loss: 0.253383  [387200/481450]\n",
      "loss: 0.324815  [390400/481450]\n",
      "loss: 0.274279  [393600/481450]\n",
      "loss: 0.445904  [396800/481450]\n",
      "loss: 0.354249  [400000/481450]\n",
      "loss: 0.261466  [403200/481450]\n",
      "loss: 0.182915  [406400/481450]\n",
      "loss: 0.486879  [409600/481450]\n",
      "loss: 0.266901  [412800/481450]\n",
      "loss: 0.211130  [416000/481450]\n",
      "loss: 0.153459  [419200/481450]\n",
      "loss: 0.209149  [422400/481450]\n",
      "loss: 0.350481  [425600/481450]\n",
      "loss: 0.177949  [428800/481450]\n",
      "loss: 0.418566  [432000/481450]\n",
      "loss: 0.598584  [435200/481450]\n",
      "loss: 0.322073  [438400/481450]\n",
      "loss: 0.274822  [441600/481450]\n",
      "loss: 0.305949  [444800/481450]\n",
      "loss: 0.290227  [448000/481450]\n",
      "loss: 0.399039  [451200/481450]\n",
      "loss: 0.075848  [454400/481450]\n",
      "loss: 0.181616  [457600/481450]\n",
      "loss: 0.115744  [460800/481450]\n",
      "loss: 0.216387  [464000/481450]\n",
      "loss: 0.076668  [467200/481450]\n",
      "loss: 0.212578  [470400/481450]\n",
      "loss: 0.138881  [473600/481450]\n",
      "loss: 0.293833  [476800/481450]\n",
      "loss: 0.320613  [480000/481450]\n",
      "Train Accuracy: 89.0167%\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.744210, F1-score: 82.19% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.315768  [    0/481450]\n",
      "loss: 0.207276  [ 3200/481450]\n",
      "loss: 0.275716  [ 6400/481450]\n",
      "loss: 0.266081  [ 9600/481450]\n",
      "loss: 0.360925  [12800/481450]\n",
      "loss: 0.209385  [16000/481450]\n",
      "loss: 0.191606  [19200/481450]\n",
      "loss: 0.130085  [22400/481450]\n",
      "loss: 0.327546  [25600/481450]\n",
      "loss: 0.344450  [28800/481450]\n",
      "loss: 0.308983  [32000/481450]\n",
      "loss: 0.420548  [35200/481450]\n",
      "loss: 0.365133  [38400/481450]\n",
      "loss: 0.104479  [41600/481450]\n",
      "loss: 0.419627  [44800/481450]\n",
      "loss: 0.144679  [48000/481450]\n",
      "loss: 0.436840  [51200/481450]\n",
      "loss: 0.338163  [54400/481450]\n",
      "loss: 0.196831  [57600/481450]\n",
      "loss: 0.256985  [60800/481450]\n",
      "loss: 0.172007  [64000/481450]\n",
      "loss: 0.055231  [67200/481450]\n",
      "loss: 0.282152  [70400/481450]\n",
      "loss: 0.325316  [73600/481450]\n",
      "loss: 0.227169  [76800/481450]\n",
      "loss: 0.410891  [80000/481450]\n",
      "loss: 0.212634  [83200/481450]\n",
      "loss: 0.232881  [86400/481450]\n",
      "loss: 0.137844  [89600/481450]\n",
      "loss: 0.307503  [92800/481450]\n",
      "loss: 0.217774  [96000/481450]\n",
      "loss: 0.317477  [99200/481450]\n",
      "loss: 0.322800  [102400/481450]\n",
      "loss: 0.209162  [105600/481450]\n",
      "loss: 0.259884  [108800/481450]\n",
      "loss: 0.244020  [112000/481450]\n",
      "loss: 0.218403  [115200/481450]\n",
      "loss: 0.209972  [118400/481450]\n",
      "loss: 0.185491  [121600/481450]\n",
      "loss: 0.227760  [124800/481450]\n",
      "loss: 0.326925  [128000/481450]\n",
      "loss: 0.239525  [131200/481450]\n",
      "loss: 0.301897  [134400/481450]\n",
      "loss: 0.105420  [137600/481450]\n",
      "loss: 0.159273  [140800/481450]\n",
      "loss: 0.321107  [144000/481450]\n",
      "loss: 0.262982  [147200/481450]\n",
      "loss: 0.443158  [150400/481450]\n",
      "loss: 0.154105  [153600/481450]\n",
      "loss: 0.364064  [156800/481450]\n",
      "loss: 0.197416  [160000/481450]\n",
      "loss: 0.201845  [163200/481450]\n",
      "loss: 0.207336  [166400/481450]\n",
      "loss: 0.237910  [169600/481450]\n",
      "loss: 0.163122  [172800/481450]\n",
      "loss: 0.311310  [176000/481450]\n",
      "loss: 0.216801  [179200/481450]\n",
      "loss: 0.119549  [182400/481450]\n",
      "loss: 0.182893  [185600/481450]\n",
      "loss: 0.202500  [188800/481450]\n",
      "loss: 0.351896  [192000/481450]\n",
      "loss: 0.381991  [195200/481450]\n",
      "loss: 0.344359  [198400/481450]\n",
      "loss: 0.404133  [201600/481450]\n",
      "loss: 0.359226  [204800/481450]\n",
      "loss: 0.337267  [208000/481450]\n",
      "loss: 0.253518  [211200/481450]\n",
      "loss: 0.257511  [214400/481450]\n",
      "loss: 0.205326  [217600/481450]\n",
      "loss: 0.607623  [220800/481450]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 14\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,scheduler)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
