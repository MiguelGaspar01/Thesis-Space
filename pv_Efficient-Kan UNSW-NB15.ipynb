{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"all\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after resampling: Counter({'0': 1})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "smote_enn = SMOTEENN(sampling_strategy=\"auto\", enn = enn, random_state=42)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_labels_encoded)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.000311\n",
       "encoder__proto_ospf             0.001044\n",
       "encoder__proto_sctp             0.001308\n",
       "encoder__proto_tcp              0.000619\n",
       "encoder__proto_udp              0.016380\n",
       "encoder__proto_unas             0.002054\n",
       "encoder__service_dns            0.010718\n",
       "encoder__service_ftp            0.000065\n",
       "encoder__service_ftp-data       0.000163\n",
       "encoder__service_http           0.014931\n",
       "encoder__service_smtp           0.000242\n",
       "encoder__state_CON              0.000703\n",
       "encoder__state_ECO              0.000002\n",
       "encoder__state_FIN              0.000307\n",
       "encoder__state_INT              0.001511\n",
       "encoder__state_REQ              0.000159\n",
       "encoder__state_RST              0.000004\n",
       "remainder__dur                  0.019442\n",
       "remainder__spkts                0.017945\n",
       "remainder__dpkts                0.012584\n",
       "remainder__sbytes               0.068248\n",
       "remainder__dbytes               0.024869\n",
       "remainder__rate                 0.020755\n",
       "remainder__sttl                 0.045384\n",
       "remainder__dttl                 0.027071\n",
       "remainder__sload                0.026382\n",
       "remainder__dload                0.025673\n",
       "remainder__sloss                0.009783\n",
       "remainder__dloss                0.007623\n",
       "remainder__sinpkt               0.018103\n",
       "remainder__dinpkt               0.008567\n",
       "remainder__sjit                 0.018018\n",
       "remainder__djit                 0.006013\n",
       "remainder__swin                 0.005968\n",
       "remainder__stcpb                0.004121\n",
       "remainder__dtcpb                0.002351\n",
       "remainder__dwin                 0.004511\n",
       "remainder__tcprtt               0.017455\n",
       "remainder__synack               0.009202\n",
       "remainder__ackdat               0.009084\n",
       "remainder__smean                0.069373\n",
       "remainder__dmean                0.026209\n",
       "remainder__trans_depth          0.054308\n",
       "remainder__response_body_len    0.004144\n",
       "remainder__ct_srv_src           0.023065\n",
       "remainder__ct_state_ttl         0.028406\n",
       "remainder__ct_dst_ltm           0.042347\n",
       "remainder__ct_src_dport_ltm     0.046356\n",
       "remainder__ct_dst_sport_ltm     0.032865\n",
       "remainder__ct_dst_src_ltm       0.059473\n",
       "remainder__is_ftp_login         0.000113\n",
       "remainder__ct_ftp_cmd           0.000225\n",
       "remainder__ct_flw_http_mthd     0.056274\n",
       "remainder__ct_src_ltm           0.038922\n",
       "remainder__ct_srv_dst           0.057708\n",
       "remainder__is_sm_ips_ports      0.000540\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.286148  [    0/481450]\n",
      "loss: 2.303674  [ 3200/481450]\n",
      "loss: 2.244631  [ 6400/481450]\n",
      "loss: 2.213986  [ 9600/481450]\n",
      "loss: 2.136511  [12800/481450]\n",
      "loss: 2.161938  [16000/481450]\n",
      "loss: 2.113140  [19200/481450]\n",
      "loss: 1.968425  [22400/481450]\n",
      "loss: 2.088135  [25600/481450]\n",
      "loss: 2.063952  [28800/481450]\n",
      "loss: 2.090575  [32000/481450]\n",
      "loss: 1.949890  [35200/481450]\n",
      "loss: 1.957240  [38400/481450]\n",
      "loss: 1.855549  [41600/481450]\n",
      "loss: 1.948665  [44800/481450]\n",
      "loss: 1.776588  [48000/481450]\n",
      "loss: 1.802523  [51200/481450]\n",
      "loss: 1.592133  [54400/481450]\n",
      "loss: 1.724606  [57600/481450]\n",
      "loss: 1.631809  [60800/481450]\n",
      "loss: 1.875248  [64000/481450]\n",
      "loss: 1.739676  [67200/481450]\n",
      "loss: 1.412294  [70400/481450]\n",
      "loss: 1.457785  [73600/481450]\n",
      "loss: 1.822690  [76800/481450]\n",
      "loss: 1.638276  [80000/481450]\n",
      "loss: 1.328319  [83200/481450]\n",
      "loss: 1.246202  [86400/481450]\n",
      "loss: 1.710122  [89600/481450]\n",
      "loss: 1.127563  [92800/481450]\n",
      "loss: 1.537654  [96000/481450]\n",
      "loss: 1.522860  [99200/481450]\n",
      "loss: 1.199517  [102400/481450]\n",
      "loss: 1.477077  [105600/481450]\n",
      "loss: 1.393306  [108800/481450]\n",
      "loss: 1.360568  [112000/481450]\n",
      "loss: 1.581322  [115200/481450]\n",
      "loss: 1.048778  [118400/481450]\n",
      "loss: 1.365826  [121600/481450]\n",
      "loss: 1.557499  [124800/481450]\n",
      "loss: 1.262963  [128000/481450]\n",
      "loss: 1.141964  [131200/481450]\n",
      "loss: 1.179148  [134400/481450]\n",
      "loss: 0.997949  [137600/481450]\n",
      "loss: 1.137917  [140800/481450]\n",
      "loss: 1.135623  [144000/481450]\n",
      "loss: 1.167950  [147200/481450]\n",
      "loss: 1.108058  [150400/481450]\n",
      "loss: 0.954491  [153600/481450]\n",
      "loss: 1.022826  [156800/481450]\n",
      "loss: 1.185036  [160000/481450]\n",
      "loss: 0.908510  [163200/481450]\n",
      "loss: 0.734866  [166400/481450]\n",
      "loss: 1.018397  [169600/481450]\n",
      "loss: 0.833350  [172800/481450]\n",
      "loss: 0.945043  [176000/481450]\n",
      "loss: 1.076432  [179200/481450]\n",
      "loss: 1.225631  [182400/481450]\n",
      "loss: 0.872076  [185600/481450]\n",
      "loss: 1.061877  [188800/481450]\n",
      "loss: 0.972615  [192000/481450]\n",
      "loss: 0.992188  [195200/481450]\n",
      "loss: 0.994091  [198400/481450]\n",
      "loss: 0.830557  [201600/481450]\n",
      "loss: 1.022319  [204800/481450]\n",
      "loss: 1.037866  [208000/481450]\n",
      "loss: 0.726292  [211200/481450]\n",
      "loss: 0.852934  [214400/481450]\n",
      "loss: 0.890242  [217600/481450]\n",
      "loss: 0.794399  [220800/481450]\n",
      "loss: 0.863860  [224000/481450]\n",
      "loss: 0.726670  [227200/481450]\n",
      "loss: 0.825233  [230400/481450]\n",
      "loss: 0.784781  [233600/481450]\n",
      "loss: 0.874370  [236800/481450]\n",
      "loss: 0.998757  [240000/481450]\n",
      "loss: 0.867681  [243200/481450]\n",
      "loss: 0.827286  [246400/481450]\n",
      "loss: 0.897808  [249600/481450]\n",
      "loss: 0.627497  [252800/481450]\n",
      "loss: 0.794034  [256000/481450]\n",
      "loss: 0.838228  [259200/481450]\n",
      "loss: 0.822405  [262400/481450]\n",
      "loss: 0.867765  [265600/481450]\n",
      "loss: 0.848949  [268800/481450]\n",
      "loss: 0.741565  [272000/481450]\n",
      "loss: 0.710693  [275200/481450]\n",
      "loss: 0.751927  [278400/481450]\n",
      "loss: 0.728341  [281600/481450]\n",
      "loss: 0.648252  [284800/481450]\n",
      "loss: 0.588218  [288000/481450]\n",
      "loss: 0.915062  [291200/481450]\n",
      "loss: 0.866613  [294400/481450]\n",
      "loss: 0.683510  [297600/481450]\n",
      "loss: 0.552146  [300800/481450]\n",
      "loss: 0.675212  [304000/481450]\n",
      "loss: 0.709518  [307200/481450]\n",
      "loss: 0.773829  [310400/481450]\n",
      "loss: 0.588911  [313600/481450]\n",
      "loss: 0.547217  [316800/481450]\n",
      "loss: 0.633796  [320000/481450]\n",
      "loss: 0.747699  [323200/481450]\n",
      "loss: 0.812706  [326400/481450]\n",
      "loss: 0.647954  [329600/481450]\n",
      "loss: 0.645753  [332800/481450]\n",
      "loss: 0.588837  [336000/481450]\n",
      "loss: 0.564841  [339200/481450]\n",
      "loss: 0.650624  [342400/481450]\n",
      "loss: 0.429643  [345600/481450]\n",
      "loss: 0.446935  [348800/481450]\n",
      "loss: 0.570832  [352000/481450]\n",
      "loss: 0.695662  [355200/481450]\n",
      "loss: 0.654269  [358400/481450]\n",
      "loss: 0.461660  [361600/481450]\n",
      "loss: 0.678248  [364800/481450]\n",
      "loss: 0.549520  [368000/481450]\n",
      "loss: 0.541192  [371200/481450]\n",
      "loss: 0.475682  [374400/481450]\n",
      "loss: 0.713420  [377600/481450]\n",
      "loss: 0.539505  [380800/481450]\n",
      "loss: 0.512058  [384000/481450]\n",
      "loss: 0.687054  [387200/481450]\n",
      "loss: 0.634265  [390400/481450]\n",
      "loss: 0.637846  [393600/481450]\n",
      "loss: 0.572007  [396800/481450]\n",
      "loss: 0.423196  [400000/481450]\n",
      "loss: 0.439583  [403200/481450]\n",
      "loss: 0.508161  [406400/481450]\n",
      "loss: 0.445223  [409600/481450]\n",
      "loss: 0.397496  [412800/481450]\n",
      "loss: 0.456420  [416000/481450]\n",
      "loss: 0.457184  [419200/481450]\n",
      "loss: 0.612458  [422400/481450]\n",
      "loss: 0.768707  [425600/481450]\n",
      "loss: 0.600309  [428800/481450]\n",
      "loss: 0.434370  [432000/481450]\n",
      "loss: 0.442690  [435200/481450]\n",
      "loss: 0.532435  [438400/481450]\n",
      "loss: 0.464136  [441600/481450]\n",
      "loss: 0.664463  [444800/481450]\n",
      "loss: 0.327452  [448000/481450]\n",
      "loss: 0.458998  [451200/481450]\n",
      "loss: 0.701104  [454400/481450]\n",
      "loss: 0.364597  [457600/481450]\n",
      "loss: 0.454606  [460800/481450]\n",
      "loss: 0.361700  [464000/481450]\n",
      "loss: 0.483387  [467200/481450]\n",
      "loss: 0.538737  [470400/481450]\n",
      "loss: 0.480643  [473600/481450]\n",
      "loss: 0.590878  [476800/481450]\n",
      "loss: 0.390816  [480000/481450]\n",
      "Train Accuracy: 65.9204%\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.477187, F1-score: 86.34% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.347697  [    0/481450]\n",
      "loss: 0.406869  [ 3200/481450]\n",
      "loss: 0.402254  [ 6400/481450]\n",
      "loss: 0.756165  [ 9600/481450]\n",
      "loss: 0.521004  [12800/481450]\n",
      "loss: 0.487663  [16000/481450]\n",
      "loss: 0.475713  [19200/481450]\n",
      "loss: 0.478939  [22400/481450]\n",
      "loss: 0.491000  [25600/481450]\n",
      "loss: 0.444077  [28800/481450]\n",
      "loss: 0.579775  [32000/481450]\n",
      "loss: 0.363112  [35200/481450]\n",
      "loss: 0.646612  [38400/481450]\n",
      "loss: 0.524246  [41600/481450]\n",
      "loss: 0.641063  [44800/481450]\n",
      "loss: 0.568425  [48000/481450]\n",
      "loss: 0.380618  [51200/481450]\n",
      "loss: 0.503694  [54400/481450]\n",
      "loss: 0.286124  [57600/481450]\n",
      "loss: 0.926986  [60800/481450]\n",
      "loss: 0.644139  [64000/481450]\n",
      "loss: 0.318760  [67200/481450]\n",
      "loss: 0.503642  [70400/481450]\n",
      "loss: 0.455527  [73600/481450]\n",
      "loss: 0.492244  [76800/481450]\n",
      "loss: 0.386114  [80000/481450]\n",
      "loss: 0.443081  [83200/481450]\n",
      "loss: 0.349275  [86400/481450]\n",
      "loss: 0.363015  [89600/481450]\n",
      "loss: 0.436061  [92800/481450]\n",
      "loss: 0.244715  [96000/481450]\n",
      "loss: 0.519856  [99200/481450]\n",
      "loss: 0.398015  [102400/481450]\n",
      "loss: 0.312587  [105600/481450]\n",
      "loss: 0.330561  [108800/481450]\n",
      "loss: 0.472746  [112000/481450]\n",
      "loss: 0.468163  [115200/481450]\n",
      "loss: 0.545673  [118400/481450]\n",
      "loss: 0.330653  [121600/481450]\n",
      "loss: 0.292682  [124800/481450]\n",
      "loss: 0.543220  [128000/481450]\n",
      "loss: 0.285432  [131200/481450]\n",
      "loss: 0.468382  [134400/481450]\n",
      "loss: 0.497390  [137600/481450]\n",
      "loss: 0.400116  [140800/481450]\n",
      "loss: 0.429233  [144000/481450]\n",
      "loss: 0.250573  [147200/481450]\n",
      "loss: 0.402318  [150400/481450]\n",
      "loss: 0.305029  [153600/481450]\n",
      "loss: 0.417448  [156800/481450]\n",
      "loss: 0.403754  [160000/481450]\n",
      "loss: 0.231297  [163200/481450]\n",
      "loss: 0.497007  [166400/481450]\n",
      "loss: 0.558052  [169600/481450]\n",
      "loss: 0.353314  [172800/481450]\n",
      "loss: 0.300887  [176000/481450]\n",
      "loss: 0.340892  [179200/481450]\n",
      "loss: 0.534238  [182400/481450]\n",
      "loss: 0.571003  [185600/481450]\n",
      "loss: 0.412600  [188800/481450]\n",
      "loss: 0.369385  [192000/481450]\n",
      "loss: 0.618293  [195200/481450]\n",
      "loss: 0.528130  [198400/481450]\n",
      "loss: 0.514833  [201600/481450]\n",
      "loss: 0.561161  [204800/481450]\n",
      "loss: 0.529643  [208000/481450]\n",
      "loss: 0.441795  [211200/481450]\n",
      "loss: 0.360724  [214400/481450]\n",
      "loss: 0.308598  [217600/481450]\n",
      "loss: 0.390827  [220800/481450]\n",
      "loss: 0.443485  [224000/481450]\n",
      "loss: 0.485716  [227200/481450]\n",
      "loss: 0.510967  [230400/481450]\n",
      "loss: 0.612287  [233600/481450]\n",
      "loss: 0.447195  [236800/481450]\n",
      "loss: 0.659718  [240000/481450]\n",
      "loss: 0.290682  [243200/481450]\n",
      "loss: 0.329775  [246400/481450]\n",
      "loss: 0.396198  [249600/481450]\n",
      "loss: 0.230072  [252800/481450]\n",
      "loss: 0.334847  [256000/481450]\n",
      "loss: 0.361319  [259200/481450]\n",
      "loss: 0.523922  [262400/481450]\n",
      "loss: 0.439451  [265600/481450]\n",
      "loss: 0.265075  [268800/481450]\n",
      "loss: 0.532503  [272000/481450]\n",
      "loss: 0.533219  [275200/481450]\n",
      "loss: 0.358765  [278400/481450]\n",
      "loss: 0.399118  [281600/481450]\n",
      "loss: 0.310639  [284800/481450]\n",
      "loss: 0.508443  [288000/481450]\n",
      "loss: 0.576806  [291200/481450]\n",
      "loss: 0.644003  [294400/481450]\n",
      "loss: 0.250579  [297600/481450]\n",
      "loss: 0.439338  [300800/481450]\n",
      "loss: 0.580799  [304000/481450]\n",
      "loss: 0.420437  [307200/481450]\n",
      "loss: 0.445269  [310400/481450]\n",
      "loss: 0.165394  [313600/481450]\n",
      "loss: 0.376048  [316800/481450]\n",
      "loss: 0.670994  [320000/481450]\n",
      "loss: 0.367769  [323200/481450]\n",
      "loss: 0.438535  [326400/481450]\n",
      "loss: 0.735772  [329600/481450]\n",
      "loss: 0.530204  [332800/481450]\n",
      "loss: 0.510785  [336000/481450]\n",
      "loss: 0.492958  [339200/481450]\n",
      "loss: 0.301302  [342400/481450]\n",
      "loss: 0.458838  [345600/481450]\n",
      "loss: 0.406227  [348800/481450]\n",
      "loss: 0.600830  [352000/481450]\n",
      "loss: 0.454394  [355200/481450]\n",
      "loss: 0.441063  [358400/481450]\n",
      "loss: 0.530164  [361600/481450]\n",
      "loss: 0.379167  [364800/481450]\n",
      "loss: 0.572176  [368000/481450]\n",
      "loss: 0.216915  [371200/481450]\n",
      "loss: 0.431000  [374400/481450]\n",
      "loss: 0.385660  [377600/481450]\n",
      "loss: 0.415272  [380800/481450]\n",
      "loss: 0.282139  [384000/481450]\n",
      "loss: 0.316621  [387200/481450]\n",
      "loss: 0.477875  [390400/481450]\n",
      "loss: 0.470658  [393600/481450]\n",
      "loss: 0.285728  [396800/481450]\n",
      "loss: 0.280560  [400000/481450]\n",
      "loss: 0.507431  [403200/481450]\n",
      "loss: 0.351668  [406400/481450]\n",
      "loss: 0.414339  [409600/481450]\n",
      "loss: 0.471558  [412800/481450]\n",
      "loss: 0.289917  [416000/481450]\n",
      "loss: 0.316453  [419200/481450]\n",
      "loss: 0.348914  [422400/481450]\n",
      "loss: 0.338427  [425600/481450]\n",
      "loss: 0.257903  [428800/481450]\n",
      "loss: 0.506042  [432000/481450]\n",
      "loss: 0.657184  [435200/481450]\n",
      "loss: 0.247558  [438400/481450]\n",
      "loss: 0.259661  [441600/481450]\n",
      "loss: 0.192562  [444800/481450]\n",
      "loss: 0.348479  [448000/481450]\n",
      "loss: 0.309197  [451200/481450]\n",
      "loss: 0.322215  [454400/481450]\n",
      "loss: 0.308780  [457600/481450]\n",
      "loss: 0.278342  [460800/481450]\n",
      "loss: 0.172398  [464000/481450]\n",
      "loss: 0.424917  [467200/481450]\n",
      "loss: 0.268824  [470400/481450]\n",
      "loss: 0.413994  [473600/481450]\n",
      "loss: 0.332538  [476800/481450]\n",
      "loss: 0.562455  [480000/481450]\n",
      "Train Accuracy: 84.2364%\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.530767, F1-score: 84.76% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.268099  [    0/481450]\n",
      "loss: 0.408044  [ 3200/481450]\n",
      "loss: 0.466695  [ 6400/481450]\n",
      "loss: 0.302987  [ 9600/481450]\n",
      "loss: 0.454599  [12800/481450]\n",
      "loss: 0.345418  [16000/481450]\n",
      "loss: 0.309323  [19200/481450]\n",
      "loss: 0.422571  [22400/481450]\n",
      "loss: 0.314565  [25600/481450]\n",
      "loss: 0.274775  [28800/481450]\n",
      "loss: 0.246978  [32000/481450]\n",
      "loss: 0.415948  [35200/481450]\n",
      "loss: 0.431042  [38400/481450]\n",
      "loss: 0.439702  [41600/481450]\n",
      "loss: 0.372503  [44800/481450]\n",
      "loss: 0.594877  [48000/481450]\n",
      "loss: 0.344368  [51200/481450]\n",
      "loss: 0.412284  [54400/481450]\n",
      "loss: 0.624530  [57600/481450]\n",
      "loss: 0.266409  [60800/481450]\n",
      "loss: 0.210776  [64000/481450]\n",
      "loss: 0.237076  [67200/481450]\n",
      "loss: 0.561488  [70400/481450]\n",
      "loss: 0.125825  [73600/481450]\n",
      "loss: 0.467965  [76800/481450]\n",
      "loss: 0.637244  [80000/481450]\n",
      "loss: 0.535318  [83200/481450]\n",
      "loss: 0.573507  [86400/481450]\n",
      "loss: 0.436064  [89600/481450]\n",
      "loss: 0.508913  [92800/481450]\n",
      "loss: 0.145343  [96000/481450]\n",
      "loss: 0.232459  [99200/481450]\n",
      "loss: 0.420117  [102400/481450]\n",
      "loss: 0.457411  [105600/481450]\n",
      "loss: 0.464287  [108800/481450]\n",
      "loss: 0.531096  [112000/481450]\n",
      "loss: 0.522891  [115200/481450]\n",
      "loss: 0.253125  [118400/481450]\n",
      "loss: 0.263015  [121600/481450]\n",
      "loss: 0.370981  [124800/481450]\n",
      "loss: 0.262226  [128000/481450]\n",
      "loss: 0.269752  [131200/481450]\n",
      "loss: 0.328141  [134400/481450]\n",
      "loss: 0.420665  [137600/481450]\n",
      "loss: 0.413145  [140800/481450]\n",
      "loss: 0.392194  [144000/481450]\n",
      "loss: 0.324929  [147200/481450]\n",
      "loss: 0.547000  [150400/481450]\n",
      "loss: 0.517810  [153600/481450]\n",
      "loss: 0.410865  [156800/481450]\n",
      "loss: 0.341931  [160000/481450]\n",
      "loss: 0.281355  [163200/481450]\n",
      "loss: 0.147846  [166400/481450]\n",
      "loss: 0.196970  [169600/481450]\n",
      "loss: 0.406287  [172800/481450]\n",
      "loss: 0.474128  [176000/481450]\n",
      "loss: 0.359928  [179200/481450]\n",
      "loss: 0.296246  [182400/481450]\n",
      "loss: 0.271937  [185600/481450]\n",
      "loss: 0.304376  [188800/481450]\n",
      "loss: 0.419719  [192000/481450]\n",
      "loss: 0.273399  [195200/481450]\n",
      "loss: 0.353076  [198400/481450]\n",
      "loss: 0.342133  [201600/481450]\n",
      "loss: 0.214424  [204800/481450]\n",
      "loss: 0.605602  [208000/481450]\n",
      "loss: 0.421560  [211200/481450]\n",
      "loss: 0.418301  [214400/481450]\n",
      "loss: 0.285175  [217600/481450]\n",
      "loss: 0.255074  [220800/481450]\n",
      "loss: 0.475458  [224000/481450]\n",
      "loss: 0.531678  [227200/481450]\n",
      "loss: 0.256498  [230400/481450]\n",
      "loss: 0.507168  [233600/481450]\n",
      "loss: 0.238777  [236800/481450]\n",
      "loss: 0.349812  [240000/481450]\n",
      "loss: 0.404987  [243200/481450]\n",
      "loss: 0.270168  [246400/481450]\n",
      "loss: 0.451184  [249600/481450]\n",
      "loss: 0.329414  [252800/481450]\n",
      "loss: 0.457712  [256000/481450]\n",
      "loss: 0.419143  [259200/481450]\n",
      "loss: 0.528595  [262400/481450]\n",
      "loss: 0.399293  [265600/481450]\n",
      "loss: 0.382414  [268800/481450]\n",
      "loss: 0.259842  [272000/481450]\n",
      "loss: 0.316784  [275200/481450]\n",
      "loss: 0.384055  [278400/481450]\n",
      "loss: 0.398730  [281600/481450]\n",
      "loss: 0.375585  [284800/481450]\n",
      "loss: 0.196177  [288000/481450]\n",
      "loss: 0.342112  [291200/481450]\n",
      "loss: 0.810799  [294400/481450]\n",
      "loss: 0.488658  [297600/481450]\n",
      "loss: 0.427256  [300800/481450]\n",
      "loss: 0.433929  [304000/481450]\n",
      "loss: 0.568076  [307200/481450]\n",
      "loss: 0.363980  [310400/481450]\n",
      "loss: 0.182798  [313600/481450]\n",
      "loss: 0.474884  [316800/481450]\n",
      "loss: 0.403793  [320000/481450]\n",
      "loss: 0.381895  [323200/481450]\n",
      "loss: 0.440207  [326400/481450]\n",
      "loss: 0.231944  [329600/481450]\n",
      "loss: 0.377359  [332800/481450]\n",
      "loss: 0.311724  [336000/481450]\n",
      "loss: 0.467512  [339200/481450]\n",
      "loss: 0.432748  [342400/481450]\n",
      "loss: 0.429982  [345600/481450]\n",
      "loss: 0.385284  [348800/481450]\n",
      "loss: 0.297078  [352000/481450]\n",
      "loss: 0.360209  [355200/481450]\n",
      "loss: 0.311537  [358400/481450]\n",
      "loss: 0.352745  [361600/481450]\n",
      "loss: 0.284889  [364800/481450]\n",
      "loss: 0.405818  [368000/481450]\n",
      "loss: 0.594126  [371200/481450]\n",
      "loss: 0.203668  [374400/481450]\n",
      "loss: 0.356840  [377600/481450]\n",
      "loss: 0.265287  [380800/481450]\n",
      "loss: 0.142132  [384000/481450]\n",
      "loss: 0.276258  [387200/481450]\n",
      "loss: 0.352194  [390400/481450]\n",
      "loss: 0.192388  [393600/481450]\n",
      "loss: 0.647106  [396800/481450]\n",
      "loss: 0.361819  [400000/481450]\n",
      "loss: 0.247049  [403200/481450]\n",
      "loss: 0.348966  [406400/481450]\n",
      "loss: 0.408087  [409600/481450]\n",
      "loss: 0.291606  [412800/481450]\n",
      "loss: 0.268914  [416000/481450]\n",
      "loss: 0.468108  [419200/481450]\n",
      "loss: 0.492903  [422400/481450]\n",
      "loss: 0.315499  [425600/481450]\n",
      "loss: 0.277045  [428800/481450]\n",
      "loss: 0.307756  [432000/481450]\n",
      "loss: 0.450169  [435200/481450]\n",
      "loss: 0.196466  [438400/481450]\n",
      "loss: 0.304614  [441600/481450]\n",
      "loss: 0.162424  [444800/481450]\n",
      "loss: 0.368309  [448000/481450]\n",
      "loss: 0.322437  [451200/481450]\n",
      "loss: 0.562515  [454400/481450]\n",
      "loss: 0.406140  [457600/481450]\n",
      "loss: 0.519855  [460800/481450]\n",
      "loss: 0.188103  [464000/481450]\n",
      "loss: 0.413980  [467200/481450]\n",
      "loss: 0.483231  [470400/481450]\n",
      "loss: 0.453015  [473600/481450]\n",
      "loss: 0.333250  [476800/481450]\n",
      "loss: 0.420896  [480000/481450]\n",
      "Train Accuracy: 85.9992%\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.638626, F1-score: 84.89% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.168351  [    0/481450]\n",
      "loss: 0.411680  [ 3200/481450]\n",
      "loss: 0.569934  [ 6400/481450]\n",
      "loss: 0.239213  [ 9600/481450]\n",
      "loss: 0.352793  [12800/481450]\n",
      "loss: 0.235607  [16000/481450]\n",
      "loss: 0.442681  [19200/481450]\n",
      "loss: 0.323237  [22400/481450]\n",
      "loss: 0.273721  [25600/481450]\n",
      "loss: 0.229780  [28800/481450]\n",
      "loss: 0.337289  [32000/481450]\n",
      "loss: 0.478899  [35200/481450]\n",
      "loss: 0.260504  [38400/481450]\n",
      "loss: 0.432837  [41600/481450]\n",
      "loss: 0.473993  [44800/481450]\n",
      "loss: 0.297671  [48000/481450]\n",
      "loss: 0.313293  [51200/481450]\n",
      "loss: 0.422865  [54400/481450]\n",
      "loss: 0.333424  [57600/481450]\n",
      "loss: 0.295497  [60800/481450]\n",
      "loss: 0.449251  [64000/481450]\n",
      "loss: 0.284237  [67200/481450]\n",
      "loss: 0.413724  [70400/481450]\n",
      "loss: 0.250795  [73600/481450]\n",
      "loss: 0.470742  [76800/481450]\n",
      "loss: 0.180214  [80000/481450]\n",
      "loss: 0.347423  [83200/481450]\n",
      "loss: 0.563007  [86400/481450]\n",
      "loss: 0.234929  [89600/481450]\n",
      "loss: 0.255383  [92800/481450]\n",
      "loss: 0.468120  [96000/481450]\n",
      "loss: 0.515550  [99200/481450]\n",
      "loss: 0.745728  [102400/481450]\n",
      "loss: 0.226221  [105600/481450]\n",
      "loss: 0.286683  [108800/481450]\n",
      "loss: 0.282363  [112000/481450]\n",
      "loss: 0.305748  [115200/481450]\n",
      "loss: 0.474668  [118400/481450]\n",
      "loss: 0.298143  [121600/481450]\n",
      "loss: 0.193112  [124800/481450]\n",
      "loss: 0.265355  [128000/481450]\n",
      "loss: 0.294401  [131200/481450]\n",
      "loss: 0.415578  [134400/481450]\n",
      "loss: 0.309938  [137600/481450]\n",
      "loss: 0.319465  [140800/481450]\n",
      "loss: 0.422368  [144000/481450]\n",
      "loss: 0.320979  [147200/481450]\n",
      "loss: 0.287809  [150400/481450]\n",
      "loss: 0.442414  [153600/481450]\n",
      "loss: 0.299215  [156800/481450]\n",
      "loss: 0.336649  [160000/481450]\n",
      "loss: 0.240869  [163200/481450]\n",
      "loss: 0.247089  [166400/481450]\n",
      "loss: 0.270685  [169600/481450]\n",
      "loss: 0.508573  [172800/481450]\n",
      "loss: 0.385862  [176000/481450]\n",
      "loss: 0.075027  [179200/481450]\n",
      "loss: 0.139446  [182400/481450]\n",
      "loss: 0.417256  [185600/481450]\n",
      "loss: 0.309025  [188800/481450]\n",
      "loss: 0.441942  [192000/481450]\n",
      "loss: 0.072383  [195200/481450]\n",
      "loss: 0.063540  [198400/481450]\n",
      "loss: 0.308980  [201600/481450]\n",
      "loss: 0.233048  [204800/481450]\n",
      "loss: 0.252806  [208000/481450]\n",
      "loss: 0.399234  [211200/481450]\n",
      "loss: 0.499888  [214400/481450]\n",
      "loss: 0.379879  [217600/481450]\n",
      "loss: 0.355342  [220800/481450]\n",
      "loss: 0.261293  [224000/481450]\n",
      "loss: 0.243059  [227200/481450]\n",
      "loss: 0.489766  [230400/481450]\n",
      "loss: 0.392614  [233600/481450]\n",
      "loss: 0.248934  [236800/481450]\n",
      "loss: 0.355164  [240000/481450]\n",
      "loss: 0.374447  [243200/481450]\n",
      "loss: 0.418148  [246400/481450]\n",
      "loss: 0.190197  [249600/481450]\n",
      "loss: 0.229199  [252800/481450]\n",
      "loss: 0.563674  [256000/481450]\n",
      "loss: 0.299891  [259200/481450]\n",
      "loss: 0.361360  [262400/481450]\n",
      "loss: 0.388991  [265600/481450]\n",
      "loss: 0.326265  [268800/481450]\n",
      "loss: 0.406220  [272000/481450]\n",
      "loss: 0.308824  [275200/481450]\n",
      "loss: 0.262264  [278400/481450]\n",
      "loss: 0.328662  [281600/481450]\n",
      "loss: 0.161205  [284800/481450]\n",
      "loss: 0.187996  [288000/481450]\n",
      "loss: 0.333899  [291200/481450]\n",
      "loss: 0.245752  [294400/481450]\n",
      "loss: 0.344333  [297600/481450]\n",
      "loss: 0.233446  [300800/481450]\n",
      "loss: 0.260500  [304000/481450]\n",
      "loss: 0.267283  [307200/481450]\n",
      "loss: 0.234655  [310400/481450]\n",
      "loss: 0.202048  [313600/481450]\n",
      "loss: 0.298743  [316800/481450]\n",
      "loss: 0.270221  [320000/481450]\n",
      "loss: 0.245656  [323200/481450]\n",
      "loss: 0.494949  [326400/481450]\n",
      "loss: 0.224765  [329600/481450]\n",
      "loss: 0.326369  [332800/481450]\n",
      "loss: 0.275179  [336000/481450]\n",
      "loss: 0.218677  [339200/481450]\n",
      "loss: 0.397394  [342400/481450]\n",
      "loss: 0.253169  [345600/481450]\n",
      "loss: 0.216148  [348800/481450]\n",
      "loss: 0.384588  [352000/481450]\n",
      "loss: 0.142324  [355200/481450]\n",
      "loss: 0.361322  [358400/481450]\n",
      "loss: 0.254863  [361600/481450]\n",
      "loss: 0.357692  [364800/481450]\n",
      "loss: 0.364957  [368000/481450]\n",
      "loss: 0.358832  [371200/481450]\n",
      "loss: 0.229646  [374400/481450]\n",
      "loss: 0.157979  [377600/481450]\n",
      "loss: 0.344020  [380800/481450]\n",
      "loss: 0.262749  [384000/481450]\n",
      "loss: 0.495011  [387200/481450]\n",
      "loss: 0.442078  [390400/481450]\n",
      "loss: 0.305257  [393600/481450]\n",
      "loss: 0.321553  [396800/481450]\n",
      "loss: 0.311602  [400000/481450]\n",
      "loss: 0.342484  [403200/481450]\n",
      "loss: 0.306412  [406400/481450]\n",
      "loss: 0.202523  [409600/481450]\n",
      "loss: 0.120040  [412800/481450]\n",
      "loss: 0.244422  [416000/481450]\n",
      "loss: 0.297269  [419200/481450]\n",
      "loss: 0.400057  [422400/481450]\n",
      "loss: 0.387972  [425600/481450]\n",
      "loss: 0.437895  [428800/481450]\n",
      "loss: 0.256633  [432000/481450]\n",
      "loss: 0.186271  [435200/481450]\n",
      "loss: 0.256038  [438400/481450]\n",
      "loss: 0.322247  [441600/481450]\n",
      "loss: 0.290859  [444800/481450]\n",
      "loss: 0.164316  [448000/481450]\n",
      "loss: 0.162126  [451200/481450]\n",
      "loss: 0.368963  [454400/481450]\n",
      "loss: 0.441136  [457600/481450]\n",
      "loss: 0.293541  [460800/481450]\n",
      "loss: 0.682912  [464000/481450]\n",
      "loss: 0.366050  [467200/481450]\n",
      "loss: 0.280040  [470400/481450]\n",
      "loss: 0.437315  [473600/481450]\n",
      "loss: 0.145076  [476800/481450]\n",
      "loss: 0.141104  [480000/481450]\n",
      "Train Accuracy: 86.8983%\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.670357, F1-score: 85.31% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.211478  [    0/481450]\n",
      "loss: 0.279485  [ 3200/481450]\n",
      "loss: 0.107877  [ 6400/481450]\n",
      "loss: 0.208032  [ 9600/481450]\n",
      "loss: 0.250710  [12800/481450]\n",
      "loss: 0.252048  [16000/481450]\n",
      "loss: 0.449224  [19200/481450]\n",
      "loss: 0.567949  [22400/481450]\n",
      "loss: 0.244336  [25600/481450]\n",
      "loss: 0.202413  [28800/481450]\n",
      "loss: 0.243207  [32000/481450]\n",
      "loss: 0.165499  [35200/481450]\n",
      "loss: 0.295174  [38400/481450]\n",
      "loss: 0.317907  [41600/481450]\n",
      "loss: 0.347158  [44800/481450]\n",
      "loss: 0.331677  [48000/481450]\n",
      "loss: 0.255609  [51200/481450]\n",
      "loss: 0.457014  [54400/481450]\n",
      "loss: 0.350656  [57600/481450]\n",
      "loss: 0.267029  [60800/481450]\n",
      "loss: 0.203810  [64000/481450]\n",
      "loss: 0.311679  [67200/481450]\n",
      "loss: 0.192327  [70400/481450]\n",
      "loss: 0.217163  [73600/481450]\n",
      "loss: 0.236518  [76800/481450]\n",
      "loss: 0.239185  [80000/481450]\n",
      "loss: 0.153757  [83200/481450]\n",
      "loss: 0.146748  [86400/481450]\n",
      "loss: 0.220017  [89600/481450]\n",
      "loss: 0.137972  [92800/481450]\n",
      "loss: 0.251098  [96000/481450]\n",
      "loss: 0.254665  [99200/481450]\n",
      "loss: 0.334566  [102400/481450]\n",
      "loss: 0.167263  [105600/481450]\n",
      "loss: 0.286675  [108800/481450]\n",
      "loss: 0.293693  [112000/481450]\n",
      "loss: 0.379755  [115200/481450]\n",
      "loss: 0.207770  [118400/481450]\n",
      "loss: 0.158963  [121600/481450]\n",
      "loss: 0.556555  [124800/481450]\n",
      "loss: 0.207122  [128000/481450]\n",
      "loss: 0.417647  [131200/481450]\n",
      "loss: 0.419471  [134400/481450]\n",
      "loss: 0.376845  [137600/481450]\n",
      "loss: 0.245582  [140800/481450]\n",
      "loss: 0.221048  [144000/481450]\n",
      "loss: 0.203987  [147200/481450]\n",
      "loss: 0.348326  [150400/481450]\n",
      "loss: 0.143479  [153600/481450]\n",
      "loss: 0.099444  [156800/481450]\n",
      "loss: 0.320307  [160000/481450]\n",
      "loss: 0.228608  [163200/481450]\n",
      "loss: 0.207735  [166400/481450]\n",
      "loss: 0.419508  [169600/481450]\n",
      "loss: 0.262931  [172800/481450]\n",
      "loss: 0.315610  [176000/481450]\n",
      "loss: 0.376434  [179200/481450]\n",
      "loss: 0.129880  [182400/481450]\n",
      "loss: 0.382481  [185600/481450]\n",
      "loss: 0.260326  [188800/481450]\n",
      "loss: 0.539131  [192000/481450]\n",
      "loss: 0.373776  [195200/481450]\n",
      "loss: 0.130773  [198400/481450]\n",
      "loss: 0.472482  [201600/481450]\n",
      "loss: 0.436940  [204800/481450]\n",
      "loss: 0.269732  [208000/481450]\n",
      "loss: 0.219434  [211200/481450]\n",
      "loss: 0.273450  [214400/481450]\n",
      "loss: 0.262609  [217600/481450]\n",
      "loss: 0.240918  [220800/481450]\n",
      "loss: 0.314219  [224000/481450]\n",
      "loss: 0.262355  [227200/481450]\n",
      "loss: 0.242748  [230400/481450]\n",
      "loss: 0.258280  [233600/481450]\n",
      "loss: 0.382283  [236800/481450]\n",
      "loss: 0.188609  [240000/481450]\n",
      "loss: 0.307760  [243200/481450]\n",
      "loss: 0.452002  [246400/481450]\n",
      "loss: 0.476813  [249600/481450]\n",
      "loss: 0.411632  [252800/481450]\n",
      "loss: 0.378951  [256000/481450]\n",
      "loss: 0.322038  [259200/481450]\n",
      "loss: 0.290907  [262400/481450]\n",
      "loss: 0.353836  [265600/481450]\n",
      "loss: 0.423038  [268800/481450]\n",
      "loss: 0.261479  [272000/481450]\n",
      "loss: 0.416962  [275200/481450]\n",
      "loss: 0.137344  [278400/481450]\n",
      "loss: 0.327696  [281600/481450]\n",
      "loss: 0.259129  [284800/481450]\n",
      "loss: 0.302591  [288000/481450]\n",
      "loss: 0.286856  [291200/481450]\n",
      "loss: 0.348945  [294400/481450]\n",
      "loss: 0.169848  [297600/481450]\n",
      "loss: 0.237025  [300800/481450]\n",
      "loss: 0.193312  [304000/481450]\n",
      "loss: 0.283008  [307200/481450]\n",
      "loss: 0.449985  [310400/481450]\n",
      "loss: 0.453743  [313600/481450]\n",
      "loss: 0.198639  [316800/481450]\n",
      "loss: 0.261617  [320000/481450]\n",
      "loss: 0.555419  [323200/481450]\n",
      "loss: 0.177918  [326400/481450]\n",
      "loss: 0.294918  [329600/481450]\n",
      "loss: 0.079306  [332800/481450]\n",
      "loss: 0.249402  [336000/481450]\n",
      "loss: 0.293679  [339200/481450]\n",
      "loss: 0.219307  [342400/481450]\n",
      "loss: 0.328029  [345600/481450]\n",
      "loss: 0.366039  [348800/481450]\n",
      "loss: 0.475776  [352000/481450]\n",
      "loss: 0.143624  [355200/481450]\n",
      "loss: 0.091966  [358400/481450]\n",
      "loss: 0.273446  [361600/481450]\n",
      "loss: 0.330029  [364800/481450]\n",
      "loss: 0.365399  [368000/481450]\n",
      "loss: 0.234473  [371200/481450]\n",
      "loss: 0.420510  [374400/481450]\n",
      "loss: 0.263582  [377600/481450]\n",
      "loss: 0.271607  [380800/481450]\n",
      "loss: 0.569660  [384000/481450]\n",
      "loss: 0.430155  [387200/481450]\n",
      "loss: 0.293512  [390400/481450]\n",
      "loss: 0.195078  [393600/481450]\n",
      "loss: 0.185794  [396800/481450]\n",
      "loss: 0.169500  [400000/481450]\n",
      "loss: 0.139543  [403200/481450]\n",
      "loss: 0.326431  [406400/481450]\n",
      "loss: 0.330342  [409600/481450]\n",
      "loss: 0.318819  [412800/481450]\n",
      "loss: 0.139607  [416000/481450]\n",
      "loss: 0.290020  [419200/481450]\n",
      "loss: 0.295132  [422400/481450]\n",
      "loss: 0.178815  [425600/481450]\n",
      "loss: 0.186797  [428800/481450]\n",
      "loss: 0.313142  [432000/481450]\n",
      "loss: 0.337146  [435200/481450]\n",
      "loss: 0.282514  [438400/481450]\n",
      "loss: 0.380566  [441600/481450]\n",
      "loss: 0.241949  [444800/481450]\n",
      "loss: 0.382037  [448000/481450]\n",
      "loss: 0.247771  [451200/481450]\n",
      "loss: 0.504154  [454400/481450]\n",
      "loss: 0.110395  [457600/481450]\n",
      "loss: 0.083115  [460800/481450]\n",
      "loss: 0.153408  [464000/481450]\n",
      "loss: 0.332548  [467200/481450]\n",
      "loss: 0.348454  [470400/481450]\n",
      "loss: 0.570639  [473600/481450]\n",
      "loss: 0.361669  [476800/481450]\n",
      "loss: 0.170737  [480000/481450]\n",
      "Train Accuracy: 87.7539%\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.732989, F1-score: 84.84% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.280757  [    0/481450]\n",
      "loss: 0.200196  [ 3200/481450]\n",
      "loss: 0.132458  [ 6400/481450]\n",
      "loss: 0.330048  [ 9600/481450]\n",
      "loss: 0.341822  [12800/481450]\n",
      "loss: 0.145951  [16000/481450]\n",
      "loss: 0.471318  [19200/481450]\n",
      "loss: 0.179301  [22400/481450]\n",
      "loss: 0.219296  [25600/481450]\n",
      "loss: 0.230050  [28800/481450]\n",
      "loss: 0.166771  [32000/481450]\n",
      "loss: 0.334640  [35200/481450]\n",
      "loss: 0.569283  [38400/481450]\n",
      "loss: 0.373363  [41600/481450]\n",
      "loss: 0.414706  [44800/481450]\n",
      "loss: 0.122149  [48000/481450]\n",
      "loss: 0.126770  [51200/481450]\n",
      "loss: 0.098571  [54400/481450]\n",
      "loss: 0.368337  [57600/481450]\n",
      "loss: 0.215625  [60800/481450]\n",
      "loss: 0.320232  [64000/481450]\n",
      "loss: 0.123877  [67200/481450]\n",
      "loss: 0.214099  [70400/481450]\n",
      "loss: 0.345407  [73600/481450]\n",
      "loss: 0.212658  [76800/481450]\n",
      "loss: 0.432292  [80000/481450]\n",
      "loss: 0.239768  [83200/481450]\n",
      "loss: 0.308196  [86400/481450]\n",
      "loss: 0.756273  [89600/481450]\n",
      "loss: 0.294692  [92800/481450]\n",
      "loss: 0.188391  [96000/481450]\n",
      "loss: 0.653234  [99200/481450]\n",
      "loss: 0.438447  [102400/481450]\n",
      "loss: 0.347765  [105600/481450]\n",
      "loss: 0.424361  [108800/481450]\n",
      "loss: 0.134219  [112000/481450]\n",
      "loss: 0.280850  [115200/481450]\n",
      "loss: 0.278097  [118400/481450]\n",
      "loss: 0.317499  [121600/481450]\n",
      "loss: 0.308142  [124800/481450]\n",
      "loss: 0.364063  [128000/481450]\n",
      "loss: 0.405530  [131200/481450]\n",
      "loss: 0.357159  [134400/481450]\n",
      "loss: 0.267273  [137600/481450]\n",
      "loss: 0.263884  [140800/481450]\n",
      "loss: 0.452620  [144000/481450]\n",
      "loss: 0.454393  [147200/481450]\n",
      "loss: 0.276234  [150400/481450]\n",
      "loss: 0.224203  [153600/481450]\n",
      "loss: 0.286279  [156800/481450]\n",
      "loss: 0.192670  [160000/481450]\n",
      "loss: 0.317507  [163200/481450]\n",
      "loss: 0.253574  [166400/481450]\n",
      "loss: 0.378915  [169600/481450]\n",
      "loss: 0.337740  [172800/481450]\n",
      "loss: 0.232939  [176000/481450]\n",
      "loss: 0.132085  [179200/481450]\n",
      "loss: 0.219779  [182400/481450]\n",
      "loss: 0.327672  [185600/481450]\n",
      "loss: 0.440059  [188800/481450]\n",
      "loss: 0.433124  [192000/481450]\n",
      "loss: 0.115676  [195200/481450]\n",
      "loss: 0.309237  [198400/481450]\n",
      "loss: 0.329570  [201600/481450]\n",
      "loss: 0.152108  [204800/481450]\n",
      "loss: 0.570257  [208000/481450]\n",
      "loss: 0.174265  [211200/481450]\n",
      "loss: 0.454112  [214400/481450]\n",
      "loss: 0.115993  [217600/481450]\n",
      "loss: 0.205698  [220800/481450]\n",
      "loss: 0.250785  [224000/481450]\n",
      "loss: 0.147179  [227200/481450]\n",
      "loss: 0.198875  [230400/481450]\n",
      "loss: 0.314438  [233600/481450]\n",
      "loss: 0.304273  [236800/481450]\n",
      "loss: 0.118457  [240000/481450]\n",
      "loss: 0.221373  [243200/481450]\n",
      "loss: 0.691399  [246400/481450]\n",
      "loss: 0.217940  [249600/481450]\n",
      "loss: 0.303000  [252800/481450]\n",
      "loss: 0.330760  [256000/481450]\n",
      "loss: 0.364372  [259200/481450]\n",
      "loss: 0.262736  [262400/481450]\n",
      "loss: 0.192093  [265600/481450]\n",
      "loss: 0.294877  [268800/481450]\n",
      "loss: 0.515655  [272000/481450]\n",
      "loss: 0.088953  [275200/481450]\n",
      "loss: 0.627519  [278400/481450]\n",
      "loss: 0.388741  [281600/481450]\n",
      "loss: 0.150212  [284800/481450]\n",
      "loss: 0.195817  [288000/481450]\n",
      "loss: 0.291542  [291200/481450]\n",
      "loss: 0.477359  [294400/481450]\n",
      "loss: 0.207806  [297600/481450]\n",
      "loss: 0.354701  [300800/481450]\n",
      "loss: 0.267126  [304000/481450]\n",
      "loss: 0.282813  [307200/481450]\n",
      "loss: 0.152131  [310400/481450]\n",
      "loss: 0.176629  [313600/481450]\n",
      "loss: 0.415325  [316800/481450]\n",
      "loss: 0.269927  [320000/481450]\n",
      "loss: 0.178086  [323200/481450]\n",
      "loss: 0.213746  [326400/481450]\n",
      "loss: 0.296419  [329600/481450]\n",
      "loss: 0.223420  [332800/481450]\n",
      "loss: 0.236751  [336000/481450]\n",
      "loss: 0.517801  [339200/481450]\n",
      "loss: 0.308296  [342400/481450]\n",
      "loss: 0.172638  [345600/481450]\n",
      "loss: 0.299926  [348800/481450]\n",
      "loss: 0.154848  [352000/481450]\n",
      "loss: 0.402067  [355200/481450]\n",
      "loss: 0.200766  [358400/481450]\n",
      "loss: 0.339464  [361600/481450]\n",
      "loss: 0.171259  [364800/481450]\n",
      "loss: 0.428643  [368000/481450]\n",
      "loss: 0.294211  [371200/481450]\n",
      "loss: 0.313595  [374400/481450]\n",
      "loss: 0.241622  [377600/481450]\n",
      "loss: 0.273902  [380800/481450]\n",
      "loss: 0.284129  [384000/481450]\n",
      "loss: 0.220055  [387200/481450]\n",
      "loss: 0.229306  [390400/481450]\n",
      "loss: 0.371364  [393600/481450]\n",
      "loss: 0.161972  [396800/481450]\n",
      "loss: 0.743955  [400000/481450]\n",
      "loss: 0.244154  [403200/481450]\n",
      "loss: 0.420614  [406400/481450]\n",
      "loss: 0.228882  [409600/481450]\n",
      "loss: 0.460116  [412800/481450]\n",
      "loss: 0.243600  [416000/481450]\n",
      "loss: 0.108503  [419200/481450]\n",
      "loss: 0.133389  [422400/481450]\n",
      "loss: 0.387590  [425600/481450]\n",
      "loss: 0.471252  [428800/481450]\n",
      "loss: 0.318347  [432000/481450]\n",
      "loss: 0.413977  [435200/481450]\n",
      "loss: 0.138757  [438400/481450]\n",
      "loss: 0.427321  [441600/481450]\n",
      "loss: 0.429214  [444800/481450]\n",
      "loss: 0.297611  [448000/481450]\n",
      "loss: 0.627470  [451200/481450]\n",
      "loss: 0.251737  [454400/481450]\n",
      "loss: 0.463981  [457600/481450]\n",
      "loss: 0.092890  [460800/481450]\n",
      "loss: 0.380333  [464000/481450]\n",
      "loss: 0.196691  [467200/481450]\n",
      "loss: 0.083530  [470400/481450]\n",
      "loss: 0.513289  [473600/481450]\n",
      "loss: 0.212229  [476800/481450]\n",
      "loss: 0.198645  [480000/481450]\n",
      "Train Accuracy: 88.4783%\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.778633, F1-score: 83.72% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.070408  [    0/481450]\n",
      "loss: 0.227820  [ 3200/481450]\n",
      "loss: 0.412066  [ 6400/481450]\n",
      "loss: 0.104129  [ 9600/481450]\n",
      "loss: 0.194308  [12800/481450]\n",
      "loss: 0.317888  [16000/481450]\n",
      "loss: 0.095548  [19200/481450]\n",
      "loss: 0.292291  [22400/481450]\n",
      "loss: 0.318776  [25600/481450]\n",
      "loss: 0.311565  [28800/481450]\n",
      "loss: 0.216307  [32000/481450]\n",
      "loss: 0.319152  [35200/481450]\n",
      "loss: 0.299677  [38400/481450]\n",
      "loss: 0.130747  [41600/481450]\n",
      "loss: 0.189353  [44800/481450]\n",
      "loss: 0.281444  [48000/481450]\n",
      "loss: 0.355924  [51200/481450]\n",
      "loss: 0.520703  [54400/481450]\n",
      "loss: 0.286082  [57600/481450]\n",
      "loss: 0.089461  [60800/481450]\n",
      "loss: 0.311262  [64000/481450]\n",
      "loss: 0.442491  [67200/481450]\n",
      "loss: 0.333074  [70400/481450]\n",
      "loss: 0.138383  [73600/481450]\n",
      "loss: 0.504687  [76800/481450]\n",
      "loss: 0.260732  [80000/481450]\n",
      "loss: 0.174553  [83200/481450]\n",
      "loss: 0.289082  [86400/481450]\n",
      "loss: 0.499821  [89600/481450]\n",
      "loss: 0.437840  [92800/481450]\n",
      "loss: 0.349462  [96000/481450]\n",
      "loss: 0.256859  [99200/481450]\n",
      "loss: 0.464018  [102400/481450]\n",
      "loss: 0.175583  [105600/481450]\n",
      "loss: 0.255428  [108800/481450]\n",
      "loss: 0.403096  [112000/481450]\n",
      "loss: 0.385314  [115200/481450]\n",
      "loss: 0.160279  [118400/481450]\n",
      "loss: 0.188457  [121600/481450]\n",
      "loss: 0.267035  [124800/481450]\n",
      "loss: 0.167614  [128000/481450]\n",
      "loss: 0.164589  [131200/481450]\n",
      "loss: 0.164819  [134400/481450]\n",
      "loss: 0.155711  [137600/481450]\n",
      "loss: 0.209069  [140800/481450]\n",
      "loss: 0.462493  [144000/481450]\n",
      "loss: 0.306319  [147200/481450]\n",
      "loss: 0.191369  [150400/481450]\n",
      "loss: 0.236170  [153600/481450]\n",
      "loss: 0.251376  [156800/481450]\n",
      "loss: 0.440384  [160000/481450]\n",
      "loss: 0.403583  [163200/481450]\n",
      "loss: 0.203211  [166400/481450]\n",
      "loss: 0.252275  [169600/481450]\n",
      "loss: 0.270249  [172800/481450]\n",
      "loss: 0.160085  [176000/481450]\n",
      "loss: 0.206198  [179200/481450]\n",
      "loss: 0.284173  [182400/481450]\n",
      "loss: 0.295896  [185600/481450]\n",
      "loss: 0.354244  [188800/481450]\n",
      "loss: 0.243528  [192000/481450]\n",
      "loss: 0.174971  [195200/481450]\n",
      "loss: 0.149934  [198400/481450]\n",
      "loss: 0.310667  [201600/481450]\n",
      "loss: 0.282444  [204800/481450]\n",
      "loss: 0.065648  [208000/481450]\n",
      "loss: 0.560236  [211200/481450]\n",
      "loss: 0.146884  [214400/481450]\n",
      "loss: 0.141892  [217600/481450]\n",
      "loss: 0.233809  [220800/481450]\n",
      "loss: 0.238407  [224000/481450]\n",
      "loss: 0.291613  [227200/481450]\n",
      "loss: 0.126504  [230400/481450]\n",
      "loss: 0.264820  [233600/481450]\n",
      "loss: 0.766292  [236800/481450]\n",
      "loss: 0.324360  [240000/481450]\n",
      "loss: 0.099117  [243200/481450]\n",
      "loss: 0.171659  [246400/481450]\n",
      "loss: 0.266761  [249600/481450]\n",
      "loss: 0.251754  [252800/481450]\n",
      "loss: 0.175679  [256000/481450]\n",
      "loss: 0.234832  [259200/481450]\n",
      "loss: 0.185792  [262400/481450]\n",
      "loss: 0.472679  [265600/481450]\n",
      "loss: 0.300342  [268800/481450]\n",
      "loss: 0.217090  [272000/481450]\n",
      "loss: 0.256246  [275200/481450]\n",
      "loss: 0.141970  [278400/481450]\n",
      "loss: 0.398621  [281600/481450]\n",
      "loss: 0.302993  [284800/481450]\n",
      "loss: 0.101581  [288000/481450]\n",
      "loss: 0.419911  [291200/481450]\n",
      "loss: 0.275392  [294400/481450]\n",
      "loss: 0.263280  [297600/481450]\n",
      "loss: 0.271111  [300800/481450]\n",
      "loss: 0.103996  [304000/481450]\n",
      "loss: 0.163799  [307200/481450]\n",
      "loss: 0.138283  [310400/481450]\n",
      "loss: 0.388090  [313600/481450]\n",
      "loss: 0.417937  [316800/481450]\n",
      "loss: 0.178019  [320000/481450]\n",
      "loss: 0.172863  [323200/481450]\n",
      "loss: 0.325642  [326400/481450]\n",
      "loss: 0.379414  [329600/481450]\n",
      "loss: 0.345558  [332800/481450]\n",
      "loss: 0.069878  [336000/481450]\n",
      "loss: 0.269910  [339200/481450]\n",
      "loss: 0.614441  [342400/481450]\n",
      "loss: 0.316851  [345600/481450]\n",
      "loss: 0.155030  [348800/481450]\n",
      "loss: 0.395544  [352000/481450]\n",
      "loss: 0.055760  [355200/481450]\n",
      "loss: 0.396257  [358400/481450]\n",
      "loss: 0.200816  [361600/481450]\n",
      "loss: 0.253928  [364800/481450]\n",
      "loss: 0.210295  [368000/481450]\n",
      "loss: 0.326361  [371200/481450]\n",
      "loss: 0.469157  [374400/481450]\n",
      "loss: 0.125747  [377600/481450]\n",
      "loss: 0.175411  [380800/481450]\n",
      "loss: 0.298222  [384000/481450]\n",
      "loss: 0.097223  [387200/481450]\n",
      "loss: 0.260952  [390400/481450]\n",
      "loss: 0.267814  [393600/481450]\n",
      "loss: 0.288731  [396800/481450]\n",
      "loss: 0.079819  [400000/481450]\n",
      "loss: 0.418294  [403200/481450]\n",
      "loss: 0.195456  [406400/481450]\n",
      "loss: 0.213420  [409600/481450]\n",
      "loss: 0.421128  [412800/481450]\n",
      "loss: 0.538939  [416000/481450]\n",
      "loss: 0.286947  [419200/481450]\n",
      "loss: 0.319673  [422400/481450]\n",
      "loss: 0.209413  [425600/481450]\n",
      "loss: 0.420998  [428800/481450]\n",
      "loss: 0.287755  [432000/481450]\n",
      "loss: 0.509113  [435200/481450]\n",
      "loss: 0.262695  [438400/481450]\n",
      "loss: 0.296092  [441600/481450]\n",
      "loss: 0.393262  [444800/481450]\n",
      "loss: 0.265759  [448000/481450]\n",
      "loss: 0.319066  [451200/481450]\n",
      "loss: 0.301311  [454400/481450]\n",
      "loss: 0.231654  [457600/481450]\n",
      "loss: 0.168040  [460800/481450]\n",
      "loss: 0.244238  [464000/481450]\n",
      "loss: 0.344817  [467200/481450]\n",
      "loss: 0.296219  [470400/481450]\n",
      "loss: 0.189309  [473600/481450]\n",
      "loss: 0.282217  [476800/481450]\n",
      "loss: 0.292145  [480000/481450]\n",
      "Train Accuracy: 89.1170%\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.727578, F1-score: 84.27% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.338541  [    0/481450]\n",
      "loss: 0.284004  [ 3200/481450]\n",
      "loss: 0.137493  [ 6400/481450]\n",
      "loss: 0.177716  [ 9600/481450]\n",
      "loss: 0.122866  [12800/481450]\n",
      "loss: 0.424549  [16000/481450]\n",
      "loss: 0.278446  [19200/481450]\n",
      "loss: 0.284368  [22400/481450]\n",
      "loss: 0.287765  [25600/481450]\n",
      "loss: 0.122516  [28800/481450]\n",
      "loss: 0.068472  [32000/481450]\n",
      "loss: 0.178017  [35200/481450]\n",
      "loss: 0.117487  [38400/481450]\n",
      "loss: 0.315012  [41600/481450]\n",
      "loss: 0.327926  [44800/481450]\n",
      "loss: 0.366876  [48000/481450]\n",
      "loss: 0.185738  [51200/481450]\n",
      "loss: 0.285775  [54400/481450]\n",
      "loss: 0.123150  [57600/481450]\n",
      "loss: 0.383806  [60800/481450]\n",
      "loss: 0.205880  [64000/481450]\n",
      "loss: 0.286395  [67200/481450]\n",
      "loss: 0.536447  [70400/481450]\n",
      "loss: 0.300974  [73600/481450]\n",
      "loss: 0.229922  [76800/481450]\n",
      "loss: 0.218962  [80000/481450]\n",
      "loss: 0.482278  [83200/481450]\n",
      "loss: 0.203971  [86400/481450]\n",
      "loss: 0.323765  [89600/481450]\n",
      "loss: 0.225378  [92800/481450]\n",
      "loss: 0.245795  [96000/481450]\n",
      "loss: 0.173926  [99200/481450]\n",
      "loss: 0.164975  [102400/481450]\n",
      "loss: 0.304834  [105600/481450]\n",
      "loss: 0.256465  [108800/481450]\n",
      "loss: 0.221648  [112000/481450]\n",
      "loss: 0.207552  [115200/481450]\n",
      "loss: 0.206422  [118400/481450]\n",
      "loss: 0.065819  [121600/481450]\n",
      "loss: 0.260657  [124800/481450]\n",
      "loss: 0.344701  [128000/481450]\n",
      "loss: 0.151313  [131200/481450]\n",
      "loss: 0.206972  [134400/481450]\n",
      "loss: 0.172493  [137600/481450]\n",
      "loss: 0.237149  [140800/481450]\n",
      "loss: 0.247182  [144000/481450]\n",
      "loss: 0.173110  [147200/481450]\n",
      "loss: 0.299651  [150400/481450]\n",
      "loss: 0.117598  [153600/481450]\n",
      "loss: 0.358134  [156800/481450]\n",
      "loss: 0.336952  [160000/481450]\n",
      "loss: 0.191585  [163200/481450]\n",
      "loss: 0.200339  [166400/481450]\n",
      "loss: 0.112869  [169600/481450]\n",
      "loss: 0.167783  [172800/481450]\n",
      "loss: 0.227373  [176000/481450]\n",
      "loss: 0.487622  [179200/481450]\n",
      "loss: 0.150285  [182400/481450]\n",
      "loss: 0.109649  [185600/481450]\n",
      "loss: 0.216802  [188800/481450]\n",
      "loss: 0.277294  [192000/481450]\n",
      "loss: 0.379251  [195200/481450]\n",
      "loss: 0.209953  [198400/481450]\n",
      "loss: 0.178198  [201600/481450]\n",
      "loss: 0.283653  [204800/481450]\n",
      "loss: 0.109635  [208000/481450]\n",
      "loss: 0.184366  [211200/481450]\n",
      "loss: 0.388319  [214400/481450]\n",
      "loss: 0.354349  [217600/481450]\n",
      "loss: 0.188729  [220800/481450]\n",
      "loss: 0.382974  [224000/481450]\n",
      "loss: 0.278110  [227200/481450]\n",
      "loss: 0.352787  [230400/481450]\n",
      "loss: 0.427040  [233600/481450]\n",
      "loss: 0.020727  [236800/481450]\n",
      "loss: 0.346269  [240000/481450]\n",
      "loss: 0.358895  [243200/481450]\n",
      "loss: 0.190788  [246400/481450]\n",
      "loss: 0.273560  [249600/481450]\n",
      "loss: 0.432207  [252800/481450]\n",
      "loss: 0.163456  [256000/481450]\n",
      "loss: 0.395390  [259200/481450]\n",
      "loss: 0.159771  [262400/481450]\n",
      "loss: 0.297035  [265600/481450]\n",
      "loss: 0.166278  [268800/481450]\n",
      "loss: 0.443458  [272000/481450]\n",
      "loss: 0.235869  [275200/481450]\n",
      "loss: 0.380637  [278400/481450]\n",
      "loss: 0.329964  [281600/481450]\n",
      "loss: 0.507474  [284800/481450]\n",
      "loss: 0.204351  [288000/481450]\n",
      "loss: 0.252441  [291200/481450]\n",
      "loss: 0.352367  [294400/481450]\n",
      "loss: 0.357307  [297600/481450]\n",
      "loss: 0.136918  [300800/481450]\n",
      "loss: 0.153179  [304000/481450]\n",
      "loss: 0.321064  [307200/481450]\n",
      "loss: 0.595729  [310400/481450]\n",
      "loss: 0.722131  [313600/481450]\n",
      "loss: 0.303861  [316800/481450]\n",
      "loss: 0.343481  [320000/481450]\n",
      "loss: 0.229928  [323200/481450]\n",
      "loss: 0.340583  [326400/481450]\n",
      "loss: 0.280957  [329600/481450]\n",
      "loss: 0.376676  [332800/481450]\n",
      "loss: 0.189642  [336000/481450]\n",
      "loss: 0.182533  [339200/481450]\n",
      "loss: 0.443263  [342400/481450]\n",
      "loss: 0.098710  [345600/481450]\n",
      "loss: 0.203143  [348800/481450]\n",
      "loss: 0.062542  [352000/481450]\n",
      "loss: 0.205142  [355200/481450]\n",
      "loss: 0.399920  [358400/481450]\n",
      "loss: 0.409594  [361600/481450]\n",
      "loss: 0.175840  [364800/481450]\n",
      "loss: 0.152094  [368000/481450]\n",
      "loss: 0.228085  [371200/481450]\n",
      "loss: 0.302924  [374400/481450]\n",
      "loss: 0.270249  [377600/481450]\n",
      "loss: 0.357615  [380800/481450]\n",
      "loss: 0.072525  [384000/481450]\n",
      "loss: 0.200972  [387200/481450]\n",
      "loss: 0.272829  [390400/481450]\n",
      "loss: 0.219902  [393600/481450]\n",
      "loss: 0.423500  [396800/481450]\n",
      "loss: 0.137672  [400000/481450]\n",
      "loss: 0.368956  [403200/481450]\n",
      "loss: 0.162288  [406400/481450]\n",
      "loss: 0.371407  [409600/481450]\n",
      "loss: 0.235954  [412800/481450]\n",
      "loss: 0.149404  [416000/481450]\n",
      "loss: 0.125002  [419200/481450]\n",
      "loss: 0.223814  [422400/481450]\n",
      "loss: 0.175329  [425600/481450]\n",
      "loss: 0.265321  [428800/481450]\n",
      "loss: 0.309342  [432000/481450]\n",
      "loss: 0.225842  [435200/481450]\n",
      "loss: 0.390386  [438400/481450]\n",
      "loss: 0.316424  [441600/481450]\n",
      "loss: 0.145460  [444800/481450]\n",
      "loss: 0.350479  [448000/481450]\n",
      "loss: 0.077106  [451200/481450]\n",
      "loss: 0.264373  [454400/481450]\n",
      "loss: 0.155034  [457600/481450]\n",
      "loss: 0.373424  [460800/481450]\n",
      "loss: 0.259647  [464000/481450]\n",
      "loss: 0.174641  [467200/481450]\n",
      "loss: 0.323686  [470400/481450]\n",
      "loss: 0.445564  [473600/481450]\n",
      "loss: 0.161147  [476800/481450]\n",
      "loss: 0.121263  [480000/481450]\n",
      "Train Accuracy: 89.6180%\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.767595, F1-score: 84.65% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.428022  [    0/481450]\n",
      "loss: 0.469221  [ 3200/481450]\n",
      "loss: 0.159385  [ 6400/481450]\n",
      "loss: 0.151867  [ 9600/481450]\n",
      "loss: 0.357800  [12800/481450]\n",
      "loss: 0.244808  [16000/481450]\n",
      "loss: 0.207490  [19200/481450]\n",
      "loss: 0.187601  [22400/481450]\n",
      "loss: 0.123888  [25600/481450]\n",
      "loss: 0.100339  [28800/481450]\n",
      "loss: 0.180020  [32000/481450]\n",
      "loss: 0.297581  [35200/481450]\n",
      "loss: 0.160248  [38400/481450]\n",
      "loss: 0.378629  [41600/481450]\n",
      "loss: 0.361045  [44800/481450]\n",
      "loss: 0.178882  [48000/481450]\n",
      "loss: 0.146496  [51200/481450]\n",
      "loss: 0.307164  [54400/481450]\n",
      "loss: 0.396137  [57600/481450]\n",
      "loss: 0.311504  [60800/481450]\n",
      "loss: 0.090067  [64000/481450]\n",
      "loss: 0.109834  [67200/481450]\n",
      "loss: 0.078234  [70400/481450]\n",
      "loss: 0.227792  [73600/481450]\n",
      "loss: 0.567007  [76800/481450]\n",
      "loss: 0.297627  [80000/481450]\n",
      "loss: 0.279362  [83200/481450]\n",
      "loss: 0.253080  [86400/481450]\n",
      "loss: 0.321212  [89600/481450]\n",
      "loss: 0.135757  [92800/481450]\n",
      "loss: 0.312324  [96000/481450]\n",
      "loss: 0.327376  [99200/481450]\n",
      "loss: 0.403482  [102400/481450]\n",
      "loss: 0.186706  [105600/481450]\n",
      "loss: 0.178973  [108800/481450]\n",
      "loss: 0.155088  [112000/481450]\n",
      "loss: 0.316456  [115200/481450]\n",
      "loss: 0.298007  [118400/481450]\n",
      "loss: 0.137381  [121600/481450]\n",
      "loss: 0.128911  [124800/481450]\n",
      "loss: 0.112391  [128000/481450]\n",
      "loss: 0.430402  [131200/481450]\n",
      "loss: 0.192966  [134400/481450]\n",
      "loss: 0.192919  [137600/481450]\n",
      "loss: 0.283559  [140800/481450]\n",
      "loss: 0.157228  [144000/481450]\n",
      "loss: 0.141118  [147200/481450]\n",
      "loss: 0.163846  [150400/481450]\n",
      "loss: 0.274262  [153600/481450]\n",
      "loss: 0.061742  [156800/481450]\n",
      "loss: 0.096043  [160000/481450]\n",
      "loss: 0.339054  [163200/481450]\n",
      "loss: 0.136195  [166400/481450]\n",
      "loss: 0.186168  [169600/481450]\n",
      "loss: 0.279096  [172800/481450]\n",
      "loss: 0.191010  [176000/481450]\n",
      "loss: 0.218495  [179200/481450]\n",
      "loss: 0.425235  [182400/481450]\n",
      "loss: 0.250677  [185600/481450]\n",
      "loss: 0.284479  [188800/481450]\n",
      "loss: 0.199533  [192000/481450]\n",
      "loss: 0.163475  [195200/481450]\n",
      "loss: 0.346522  [198400/481450]\n",
      "loss: 0.327881  [201600/481450]\n",
      "loss: 0.303554  [204800/481450]\n",
      "loss: 0.305265  [208000/481450]\n",
      "loss: 0.133383  [211200/481450]\n",
      "loss: 0.123960  [214400/481450]\n",
      "loss: 0.194955  [217600/481450]\n",
      "loss: 0.126665  [220800/481450]\n",
      "loss: 0.144404  [224000/481450]\n",
      "loss: 0.422430  [227200/481450]\n",
      "loss: 0.224993  [230400/481450]\n",
      "loss: 0.252118  [233600/481450]\n",
      "loss: 0.183446  [236800/481450]\n",
      "loss: 0.351831  [240000/481450]\n",
      "loss: 0.157780  [243200/481450]\n",
      "loss: 0.130206  [246400/481450]\n",
      "loss: 0.228781  [249600/481450]\n",
      "loss: 0.252837  [252800/481450]\n",
      "loss: 0.086639  [256000/481450]\n",
      "loss: 0.179824  [259200/481450]\n",
      "loss: 0.094414  [262400/481450]\n",
      "loss: 0.272361  [265600/481450]\n",
      "loss: 0.251802  [268800/481450]\n",
      "loss: 0.369481  [272000/481450]\n",
      "loss: 0.294650  [275200/481450]\n",
      "loss: 0.172241  [278400/481450]\n",
      "loss: 0.047793  [281600/481450]\n",
      "loss: 0.235690  [284800/481450]\n",
      "loss: 0.333405  [288000/481450]\n",
      "loss: 0.376975  [291200/481450]\n",
      "loss: 0.262744  [294400/481450]\n",
      "loss: 0.286307  [297600/481450]\n",
      "loss: 0.190663  [300800/481450]\n",
      "loss: 0.205138  [304000/481450]\n",
      "loss: 0.108208  [307200/481450]\n",
      "loss: 0.410406  [310400/481450]\n",
      "loss: 0.434352  [313600/481450]\n",
      "loss: 0.360107  [316800/481450]\n",
      "loss: 0.162885  [320000/481450]\n",
      "loss: 0.267984  [323200/481450]\n",
      "loss: 0.161256  [326400/481450]\n",
      "loss: 0.160586  [329600/481450]\n",
      "loss: 0.098307  [332800/481450]\n",
      "loss: 0.250838  [336000/481450]\n",
      "loss: 0.289131  [339200/481450]\n",
      "loss: 0.209996  [342400/481450]\n",
      "loss: 0.321852  [345600/481450]\n",
      "loss: 0.114139  [348800/481450]\n",
      "loss: 0.143683  [352000/481450]\n",
      "loss: 0.251156  [355200/481450]\n",
      "loss: 0.057011  [358400/481450]\n",
      "loss: 0.245782  [361600/481450]\n",
      "loss: 0.203028  [364800/481450]\n",
      "loss: 0.112727  [368000/481450]\n",
      "loss: 0.256540  [371200/481450]\n",
      "loss: 0.235665  [374400/481450]\n",
      "loss: 0.163733  [377600/481450]\n",
      "loss: 0.122388  [380800/481450]\n",
      "loss: 0.423128  [384000/481450]\n",
      "loss: 0.107457  [387200/481450]\n",
      "loss: 0.315875  [390400/481450]\n",
      "loss: 0.326813  [393600/481450]\n",
      "loss: 0.334690  [396800/481450]\n",
      "loss: 0.130135  [400000/481450]\n",
      "loss: 0.255633  [403200/481450]\n",
      "loss: 0.015379  [406400/481450]\n",
      "loss: 0.400221  [409600/481450]\n",
      "loss: 0.172897  [412800/481450]\n",
      "loss: 0.285095  [416000/481450]\n",
      "loss: 0.276892  [419200/481450]\n",
      "loss: 0.184191  [422400/481450]\n",
      "loss: 0.284755  [425600/481450]\n",
      "loss: 0.257017  [428800/481450]\n",
      "loss: 0.508988  [432000/481450]\n",
      "loss: 0.226363  [435200/481450]\n",
      "loss: 0.381119  [438400/481450]\n",
      "loss: 0.311636  [441600/481450]\n",
      "loss: 0.330206  [444800/481450]\n",
      "loss: 0.477100  [448000/481450]\n",
      "loss: 0.167329  [451200/481450]\n",
      "loss: 0.370669  [454400/481450]\n",
      "loss: 0.283276  [457600/481450]\n",
      "loss: 0.316356  [460800/481450]\n",
      "loss: 0.157865  [464000/481450]\n",
      "loss: 0.410129  [467200/481450]\n",
      "loss: 0.154468  [470400/481450]\n",
      "loss: 0.168220  [473600/481450]\n",
      "loss: 0.174988  [476800/481450]\n",
      "loss: 0.246801  [480000/481450]\n",
      "Train Accuracy: 89.9948%\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.851199, F1-score: 83.53% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.238236  [    0/481450]\n",
      "loss: 0.311727  [ 3200/481450]\n",
      "loss: 0.411290  [ 6400/481450]\n",
      "loss: 0.151912  [ 9600/481450]\n",
      "loss: 0.072784  [12800/481450]\n",
      "loss: 0.038976  [16000/481450]\n",
      "loss: 0.328681  [19200/481450]\n",
      "loss: 0.217366  [22400/481450]\n",
      "loss: 0.347637  [25600/481450]\n",
      "loss: 0.202361  [28800/481450]\n",
      "loss: 0.253543  [32000/481450]\n",
      "loss: 0.370773  [35200/481450]\n",
      "loss: 0.296187  [38400/481450]\n",
      "loss: 0.353979  [41600/481450]\n",
      "loss: 0.167955  [44800/481450]\n",
      "loss: 0.244723  [48000/481450]\n",
      "loss: 0.189755  [51200/481450]\n",
      "loss: 0.182961  [54400/481450]\n",
      "loss: 0.360304  [57600/481450]\n",
      "loss: 0.218837  [60800/481450]\n",
      "loss: 0.251578  [64000/481450]\n",
      "loss: 0.039646  [67200/481450]\n",
      "loss: 0.115957  [70400/481450]\n",
      "loss: 0.262076  [73600/481450]\n",
      "loss: 0.185217  [76800/481450]\n",
      "loss: 0.234973  [80000/481450]\n",
      "loss: 0.071455  [83200/481450]\n",
      "loss: 0.092023  [86400/481450]\n",
      "loss: 0.307056  [89600/481450]\n",
      "loss: 0.137159  [92800/481450]\n",
      "loss: 0.100393  [96000/481450]\n",
      "loss: 0.311170  [99200/481450]\n",
      "loss: 0.291891  [102400/481450]\n",
      "loss: 0.409057  [105600/481450]\n",
      "loss: 0.126407  [108800/481450]\n",
      "loss: 0.161237  [112000/481450]\n",
      "loss: 0.229777  [115200/481450]\n",
      "loss: 0.114690  [118400/481450]\n",
      "loss: 0.231235  [121600/481450]\n",
      "loss: 0.164843  [124800/481450]\n",
      "loss: 0.136378  [128000/481450]\n",
      "loss: 0.276373  [131200/481450]\n",
      "loss: 0.414164  [134400/481450]\n",
      "loss: 0.546484  [137600/481450]\n",
      "loss: 0.112106  [140800/481450]\n",
      "loss: 0.298742  [144000/481450]\n",
      "loss: 0.370562  [147200/481450]\n",
      "loss: 0.226364  [150400/481450]\n",
      "loss: 0.138310  [153600/481450]\n",
      "loss: 0.243119  [156800/481450]\n",
      "loss: 0.258294  [160000/481450]\n",
      "loss: 0.190145  [163200/481450]\n",
      "loss: 0.079076  [166400/481450]\n",
      "loss: 0.433265  [169600/481450]\n",
      "loss: 0.535024  [172800/481450]\n",
      "loss: 0.137718  [176000/481450]\n",
      "loss: 0.286339  [179200/481450]\n",
      "loss: 0.301696  [182400/481450]\n",
      "loss: 0.148642  [185600/481450]\n",
      "loss: 0.224442  [188800/481450]\n",
      "loss: 0.185162  [192000/481450]\n",
      "loss: 0.142659  [195200/481450]\n",
      "loss: 0.166811  [198400/481450]\n",
      "loss: 0.260826  [201600/481450]\n",
      "loss: 0.199997  [204800/481450]\n",
      "loss: 0.362852  [208000/481450]\n",
      "loss: 0.287255  [211200/481450]\n",
      "loss: 0.139329  [214400/481450]\n",
      "loss: 0.225290  [217600/481450]\n",
      "loss: 0.197465  [220800/481450]\n",
      "loss: 0.263960  [224000/481450]\n",
      "loss: 0.289587  [227200/481450]\n",
      "loss: 0.171723  [230400/481450]\n",
      "loss: 0.220492  [233600/481450]\n",
      "loss: 0.366563  [236800/481450]\n",
      "loss: 0.217622  [240000/481450]\n",
      "loss: 0.168550  [243200/481450]\n",
      "loss: 0.215441  [246400/481450]\n",
      "loss: 0.302754  [249600/481450]\n",
      "loss: 0.304533  [252800/481450]\n",
      "loss: 0.254599  [256000/481450]\n",
      "loss: 0.164258  [259200/481450]\n",
      "loss: 0.289819  [262400/481450]\n",
      "loss: 0.592465  [265600/481450]\n",
      "loss: 0.221871  [268800/481450]\n",
      "loss: 0.236578  [272000/481450]\n",
      "loss: 0.158331  [275200/481450]\n",
      "loss: 0.307136  [278400/481450]\n",
      "loss: 0.356128  [281600/481450]\n",
      "loss: 0.396313  [284800/481450]\n",
      "loss: 0.157172  [288000/481450]\n",
      "loss: 0.301525  [291200/481450]\n",
      "loss: 0.327087  [294400/481450]\n",
      "loss: 0.147499  [297600/481450]\n",
      "loss: 0.133456  [300800/481450]\n",
      "loss: 0.302689  [304000/481450]\n",
      "loss: 0.217168  [307200/481450]\n",
      "loss: 0.383633  [310400/481450]\n",
      "loss: 0.145314  [313600/481450]\n",
      "loss: 0.284647  [316800/481450]\n",
      "loss: 0.282070  [320000/481450]\n",
      "loss: 0.273121  [323200/481450]\n",
      "loss: 0.124957  [326400/481450]\n",
      "loss: 0.258425  [329600/481450]\n",
      "loss: 0.308784  [332800/481450]\n",
      "loss: 0.371863  [336000/481450]\n",
      "loss: 0.308014  [339200/481450]\n",
      "loss: 0.188385  [342400/481450]\n",
      "loss: 0.171341  [345600/481450]\n",
      "loss: 0.256902  [348800/481450]\n",
      "loss: 0.099834  [352000/481450]\n",
      "loss: 0.117633  [355200/481450]\n",
      "loss: 0.170612  [358400/481450]\n",
      "loss: 0.242934  [361600/481450]\n",
      "loss: 0.188034  [364800/481450]\n",
      "loss: 0.179938  [368000/481450]\n",
      "loss: 0.245973  [371200/481450]\n",
      "loss: 0.330667  [374400/481450]\n",
      "loss: 0.228784  [377600/481450]\n",
      "loss: 0.235733  [380800/481450]\n",
      "loss: 0.191169  [384000/481450]\n",
      "loss: 0.197580  [387200/481450]\n",
      "loss: 0.229271  [390400/481450]\n",
      "loss: 0.199436  [393600/481450]\n",
      "loss: 0.393675  [396800/481450]\n",
      "loss: 0.269439  [400000/481450]\n",
      "loss: 0.120684  [403200/481450]\n",
      "loss: 0.171016  [406400/481450]\n",
      "loss: 0.358270  [409600/481450]\n",
      "loss: 0.325470  [412800/481450]\n",
      "loss: 0.227535  [416000/481450]\n",
      "loss: 0.131159  [419200/481450]\n",
      "loss: 0.436366  [422400/481450]\n",
      "loss: 0.120515  [425600/481450]\n",
      "loss: 0.073629  [428800/481450]\n",
      "loss: 0.125378  [432000/481450]\n",
      "loss: 0.154174  [435200/481450]\n",
      "loss: 0.172517  [438400/481450]\n",
      "loss: 0.187901  [441600/481450]\n",
      "loss: 0.224345  [444800/481450]\n",
      "loss: 0.275979  [448000/481450]\n",
      "loss: 0.333912  [451200/481450]\n",
      "loss: 0.156525  [454400/481450]\n",
      "loss: 0.095212  [457600/481450]\n",
      "loss: 0.209275  [460800/481450]\n",
      "loss: 0.367899  [464000/481450]\n",
      "loss: 0.205857  [467200/481450]\n",
      "loss: 0.183564  [470400/481450]\n",
      "loss: 0.108494  [473600/481450]\n",
      "loss: 0.272421  [476800/481450]\n",
      "loss: 0.317925  [480000/481450]\n",
      "Train Accuracy: 90.3259%\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.952393, F1-score: 82.38% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.472291  [    0/481450]\n",
      "loss: 0.289138  [ 3200/481450]\n",
      "loss: 0.257957  [ 6400/481450]\n",
      "loss: 0.217745  [ 9600/481450]\n",
      "loss: 0.139480  [12800/481450]\n",
      "loss: 0.316925  [16000/481450]\n",
      "loss: 0.122197  [19200/481450]\n",
      "loss: 0.086358  [22400/481450]\n",
      "loss: 0.185040  [25600/481450]\n",
      "loss: 0.068677  [28800/481450]\n",
      "loss: 0.299547  [32000/481450]\n",
      "loss: 0.285089  [35200/481450]\n",
      "loss: 0.193735  [38400/481450]\n",
      "loss: 0.169670  [41600/481450]\n",
      "loss: 0.246978  [44800/481450]\n",
      "loss: 0.185677  [48000/481450]\n",
      "loss: 0.239651  [51200/481450]\n",
      "loss: 0.511634  [54400/481450]\n",
      "loss: 0.180889  [57600/481450]\n",
      "loss: 0.251086  [60800/481450]\n",
      "loss: 0.308578  [64000/481450]\n",
      "loss: 0.204529  [67200/481450]\n",
      "loss: 0.111509  [70400/481450]\n",
      "loss: 0.269767  [73600/481450]\n",
      "loss: 0.173334  [76800/481450]\n",
      "loss: 0.135904  [80000/481450]\n",
      "loss: 0.177542  [83200/481450]\n",
      "loss: 0.109801  [86400/481450]\n",
      "loss: 0.196617  [89600/481450]\n",
      "loss: 0.098519  [92800/481450]\n",
      "loss: 0.324671  [96000/481450]\n",
      "loss: 0.253447  [99200/481450]\n",
      "loss: 0.190717  [102400/481450]\n",
      "loss: 0.212734  [105600/481450]\n",
      "loss: 0.213687  [108800/481450]\n",
      "loss: 0.298777  [112000/481450]\n",
      "loss: 0.245228  [115200/481450]\n",
      "loss: 0.103861  [118400/481450]\n",
      "loss: 0.229890  [121600/481450]\n",
      "loss: 0.157564  [124800/481450]\n",
      "loss: 0.119887  [128000/481450]\n",
      "loss: 0.145941  [131200/481450]\n",
      "loss: 0.708818  [134400/481450]\n",
      "loss: 0.223357  [137600/481450]\n",
      "loss: 0.243960  [140800/481450]\n",
      "loss: 0.387430  [144000/481450]\n",
      "loss: 0.014127  [147200/481450]\n",
      "loss: 0.329682  [150400/481450]\n",
      "loss: 0.207647  [153600/481450]\n",
      "loss: 0.244492  [156800/481450]\n",
      "loss: 0.087817  [160000/481450]\n",
      "loss: 0.224834  [163200/481450]\n",
      "loss: 0.217948  [166400/481450]\n",
      "loss: 0.193362  [169600/481450]\n",
      "loss: 0.211115  [172800/481450]\n",
      "loss: 0.430318  [176000/481450]\n",
      "loss: 0.116045  [179200/481450]\n",
      "loss: 0.220223  [182400/481450]\n",
      "loss: 0.229391  [185600/481450]\n",
      "loss: 0.478895  [188800/481450]\n",
      "loss: 0.154563  [192000/481450]\n",
      "loss: 0.191574  [195200/481450]\n",
      "loss: 0.136667  [198400/481450]\n",
      "loss: 0.266364  [201600/481450]\n",
      "loss: 0.126639  [204800/481450]\n",
      "loss: 0.254135  [208000/481450]\n",
      "loss: 0.164327  [211200/481450]\n",
      "loss: 0.056747  [214400/481450]\n",
      "loss: 0.323220  [217600/481450]\n",
      "loss: 0.155529  [220800/481450]\n",
      "loss: 0.106556  [224000/481450]\n",
      "loss: 0.163795  [227200/481450]\n",
      "loss: 0.124092  [230400/481450]\n",
      "loss: 0.271068  [233600/481450]\n",
      "loss: 0.075916  [236800/481450]\n",
      "loss: 0.290810  [240000/481450]\n",
      "loss: 0.206180  [243200/481450]\n",
      "loss: 0.216089  [246400/481450]\n",
      "loss: 0.504198  [249600/481450]\n",
      "loss: 0.192443  [252800/481450]\n",
      "loss: 0.207002  [256000/481450]\n",
      "loss: 0.129161  [259200/481450]\n",
      "loss: 0.173134  [262400/481450]\n",
      "loss: 0.205952  [265600/481450]\n",
      "loss: 0.214298  [268800/481450]\n",
      "loss: 0.165754  [272000/481450]\n",
      "loss: 0.218453  [275200/481450]\n",
      "loss: 0.291152  [278400/481450]\n",
      "loss: 0.142172  [281600/481450]\n",
      "loss: 0.517488  [284800/481450]\n",
      "loss: 0.262656  [288000/481450]\n",
      "loss: 0.246533  [291200/481450]\n",
      "loss: 0.210327  [294400/481450]\n",
      "loss: 0.271765  [297600/481450]\n",
      "loss: 0.444191  [300800/481450]\n",
      "loss: 0.317198  [304000/481450]\n",
      "loss: 0.305804  [307200/481450]\n",
      "loss: 0.220395  [310400/481450]\n",
      "loss: 0.145196  [313600/481450]\n",
      "loss: 0.111907  [316800/481450]\n",
      "loss: 0.103051  [320000/481450]\n",
      "loss: 0.130001  [323200/481450]\n",
      "loss: 0.171326  [326400/481450]\n",
      "loss: 0.147135  [329600/481450]\n",
      "loss: 0.148939  [332800/481450]\n",
      "loss: 0.141189  [336000/481450]\n",
      "loss: 0.250726  [339200/481450]\n",
      "loss: 0.250666  [342400/481450]\n",
      "loss: 0.124902  [345600/481450]\n",
      "loss: 0.196808  [348800/481450]\n",
      "loss: 0.221884  [352000/481450]\n",
      "loss: 0.295776  [355200/481450]\n",
      "loss: 0.125329  [358400/481450]\n",
      "loss: 0.110895  [361600/481450]\n",
      "loss: 0.175507  [364800/481450]\n",
      "loss: 0.227854  [368000/481450]\n",
      "loss: 0.372117  [371200/481450]\n",
      "loss: 0.118290  [374400/481450]\n",
      "loss: 0.329555  [377600/481450]\n",
      "loss: 0.140444  [380800/481450]\n",
      "loss: 0.148543  [384000/481450]\n",
      "loss: 0.061697  [387200/481450]\n",
      "loss: 0.358594  [390400/481450]\n",
      "loss: 0.184702  [393600/481450]\n",
      "loss: 0.241799  [396800/481450]\n",
      "loss: 0.263326  [400000/481450]\n",
      "loss: 0.120486  [403200/481450]\n",
      "loss: 0.215146  [406400/481450]\n",
      "loss: 0.253828  [409600/481450]\n",
      "loss: 0.284287  [412800/481450]\n",
      "loss: 0.339304  [416000/481450]\n",
      "loss: 0.161627  [419200/481450]\n",
      "loss: 0.121706  [422400/481450]\n",
      "loss: 0.105991  [425600/481450]\n",
      "loss: 0.125368  [428800/481450]\n",
      "loss: 0.175672  [432000/481450]\n",
      "loss: 0.178683  [435200/481450]\n",
      "loss: 0.097455  [438400/481450]\n",
      "loss: 0.308250  [441600/481450]\n",
      "loss: 0.298849  [444800/481450]\n",
      "loss: 0.248040  [448000/481450]\n",
      "loss: 0.265012  [451200/481450]\n",
      "loss: 0.287905  [454400/481450]\n",
      "loss: 0.134540  [457600/481450]\n",
      "loss: 0.073744  [460800/481450]\n",
      "loss: 0.098701  [464000/481450]\n",
      "loss: 0.142036  [467200/481450]\n",
      "loss: 0.293688  [470400/481450]\n",
      "loss: 0.187478  [473600/481450]\n",
      "loss: 0.096525  [476800/481450]\n",
      "loss: 0.193879  [480000/481450]\n",
      "Train Accuracy: 90.5591%\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.950271, F1-score: 82.93% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.284039  [    0/481450]\n",
      "loss: 0.186868  [ 3200/481450]\n",
      "loss: 0.302091  [ 6400/481450]\n",
      "loss: 0.153376  [ 9600/481450]\n",
      "loss: 0.048326  [12800/481450]\n",
      "loss: 0.181611  [16000/481450]\n",
      "loss: 0.467618  [19200/481450]\n",
      "loss: 0.109104  [22400/481450]\n",
      "loss: 0.189778  [25600/481450]\n",
      "loss: 0.191073  [28800/481450]\n",
      "loss: 0.279871  [32000/481450]\n",
      "loss: 0.184322  [35200/481450]\n",
      "loss: 0.020207  [38400/481450]\n",
      "loss: 0.182793  [41600/481450]\n",
      "loss: 0.190294  [44800/481450]\n",
      "loss: 0.257921  [48000/481450]\n",
      "loss: 0.205194  [51200/481450]\n",
      "loss: 0.356766  [54400/481450]\n",
      "loss: 0.212979  [57600/481450]\n",
      "loss: 0.219223  [60800/481450]\n",
      "loss: 0.177030  [64000/481450]\n",
      "loss: 0.223642  [67200/481450]\n",
      "loss: 0.142550  [70400/481450]\n",
      "loss: 0.173567  [73600/481450]\n",
      "loss: 0.104694  [76800/481450]\n",
      "loss: 0.207864  [80000/481450]\n",
      "loss: 0.177496  [83200/481450]\n",
      "loss: 0.309007  [86400/481450]\n",
      "loss: 0.276866  [89600/481450]\n",
      "loss: 0.168356  [92800/481450]\n",
      "loss: 0.336604  [96000/481450]\n",
      "loss: 0.071329  [99200/481450]\n",
      "loss: 0.194278  [102400/481450]\n",
      "loss: 0.272219  [105600/481450]\n",
      "loss: 0.256860  [108800/481450]\n",
      "loss: 0.371564  [112000/481450]\n",
      "loss: 0.630562  [115200/481450]\n",
      "loss: 0.202321  [118400/481450]\n",
      "loss: 0.272409  [121600/481450]\n",
      "loss: 0.126529  [124800/481450]\n",
      "loss: 0.159161  [128000/481450]\n",
      "loss: 0.101032  [131200/481450]\n",
      "loss: 0.145746  [134400/481450]\n",
      "loss: 0.121041  [137600/481450]\n",
      "loss: 0.150735  [140800/481450]\n",
      "loss: 0.297246  [144000/481450]\n",
      "loss: 0.476536  [147200/481450]\n",
      "loss: 0.315537  [150400/481450]\n",
      "loss: 0.256219  [153600/481450]\n",
      "loss: 0.346118  [156800/481450]\n",
      "loss: 0.230143  [160000/481450]\n",
      "loss: 0.087143  [163200/481450]\n",
      "loss: 0.286163  [166400/481450]\n",
      "loss: 0.253076  [169600/481450]\n",
      "loss: 0.057945  [172800/481450]\n",
      "loss: 0.272948  [176000/481450]\n",
      "loss: 0.387180  [179200/481450]\n",
      "loss: 0.153374  [182400/481450]\n",
      "loss: 0.173440  [185600/481450]\n",
      "loss: 0.295597  [188800/481450]\n",
      "loss: 0.135701  [192000/481450]\n",
      "loss: 0.314171  [195200/481450]\n",
      "loss: 0.147686  [198400/481450]\n",
      "loss: 0.176966  [201600/481450]\n",
      "loss: 0.292127  [204800/481450]\n",
      "loss: 0.234205  [208000/481450]\n",
      "loss: 0.027543  [211200/481450]\n",
      "loss: 0.256217  [214400/481450]\n",
      "loss: 0.320875  [217600/481450]\n",
      "loss: 0.279218  [220800/481450]\n",
      "loss: 0.190143  [224000/481450]\n",
      "loss: 0.250465  [227200/481450]\n",
      "loss: 0.193421  [230400/481450]\n",
      "loss: 0.244375  [233600/481450]\n",
      "loss: 0.326119  [236800/481450]\n",
      "loss: 0.210373  [240000/481450]\n",
      "loss: 0.224537  [243200/481450]\n",
      "loss: 0.301116  [246400/481450]\n",
      "loss: 0.234261  [249600/481450]\n",
      "loss: 0.197813  [252800/481450]\n",
      "loss: 0.210670  [256000/481450]\n",
      "loss: 0.232871  [259200/481450]\n",
      "loss: 0.364729  [262400/481450]\n",
      "loss: 0.140098  [265600/481450]\n",
      "loss: 0.303034  [268800/481450]\n",
      "loss: 0.294948  [272000/481450]\n",
      "loss: 0.023359  [275200/481450]\n",
      "loss: 0.166384  [278400/481450]\n",
      "loss: 0.246606  [281600/481450]\n",
      "loss: 0.167302  [284800/481450]\n",
      "loss: 0.048537  [288000/481450]\n",
      "loss: 0.218617  [291200/481450]\n",
      "loss: 0.384413  [294400/481450]\n",
      "loss: 0.459576  [297600/481450]\n",
      "loss: 0.261568  [300800/481450]\n",
      "loss: 0.397229  [304000/481450]\n",
      "loss: 0.213974  [307200/481450]\n",
      "loss: 0.208929  [310400/481450]\n",
      "loss: 0.081322  [313600/481450]\n",
      "loss: 0.283430  [316800/481450]\n",
      "loss: 0.302055  [320000/481450]\n",
      "loss: 0.062153  [323200/481450]\n",
      "loss: 0.141510  [326400/481450]\n",
      "loss: 0.246406  [329600/481450]\n",
      "loss: 0.211627  [332800/481450]\n",
      "loss: 0.088368  [336000/481450]\n",
      "loss: 0.297291  [339200/481450]\n",
      "loss: 0.285973  [342400/481450]\n",
      "loss: 0.232894  [345600/481450]\n",
      "loss: 0.169091  [348800/481450]\n",
      "loss: 0.445673  [352000/481450]\n",
      "loss: 0.290881  [355200/481450]\n",
      "loss: 0.292096  [358400/481450]\n",
      "loss: 0.153937  [361600/481450]\n",
      "loss: 0.200464  [364800/481450]\n",
      "loss: 0.036206  [368000/481450]\n",
      "loss: 0.299230  [371200/481450]\n",
      "loss: 0.022591  [374400/481450]\n",
      "loss: 0.174971  [377600/481450]\n",
      "loss: 0.185636  [380800/481450]\n",
      "loss: 0.175252  [384000/481450]\n",
      "loss: 0.080767  [387200/481450]\n",
      "loss: 0.314127  [390400/481450]\n",
      "loss: 0.316232  [393600/481450]\n",
      "loss: 0.179858  [396800/481450]\n",
      "loss: 0.230013  [400000/481450]\n",
      "loss: 0.161557  [403200/481450]\n",
      "loss: 0.346199  [406400/481450]\n",
      "loss: 0.374913  [409600/481450]\n",
      "loss: 0.213336  [412800/481450]\n",
      "loss: 0.219299  [416000/481450]\n",
      "loss: 0.246696  [419200/481450]\n",
      "loss: 0.080021  [422400/481450]\n",
      "loss: 0.179487  [425600/481450]\n",
      "loss: 0.358142  [428800/481450]\n",
      "loss: 0.123664  [432000/481450]\n",
      "loss: 0.318176  [435200/481450]\n",
      "loss: 0.180597  [438400/481450]\n",
      "loss: 0.133309  [441600/481450]\n",
      "loss: 0.388542  [444800/481450]\n",
      "loss: 0.262538  [448000/481450]\n",
      "loss: 0.101227  [451200/481450]\n",
      "loss: 0.031402  [454400/481450]\n",
      "loss: 0.124924  [457600/481450]\n",
      "loss: 0.169544  [460800/481450]\n",
      "loss: 0.154108  [464000/481450]\n",
      "loss: 0.134654  [467200/481450]\n",
      "loss: 0.234204  [470400/481450]\n",
      "loss: 0.411881  [473600/481450]\n",
      "loss: 0.050813  [476800/481450]\n",
      "loss: 0.309135  [480000/481450]\n",
      "Train Accuracy: 90.7263%\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 1.050372, F1-score: 82.58% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.163254  [    0/481450]\n",
      "loss: 0.183766  [ 3200/481450]\n",
      "loss: 0.172482  [ 6400/481450]\n",
      "loss: 0.328970  [ 9600/481450]\n",
      "loss: 0.076940  [12800/481450]\n",
      "loss: 0.161002  [16000/481450]\n",
      "loss: 0.161255  [19200/481450]\n",
      "loss: 0.198333  [22400/481450]\n",
      "loss: 0.307769  [25600/481450]\n",
      "loss: 0.204544  [28800/481450]\n",
      "loss: 0.205340  [32000/481450]\n",
      "loss: 0.227572  [35200/481450]\n",
      "loss: 0.186695  [38400/481450]\n",
      "loss: 0.157670  [41600/481450]\n",
      "loss: 0.199673  [44800/481450]\n",
      "loss: 0.270640  [48000/481450]\n",
      "loss: 0.280949  [51200/481450]\n",
      "loss: 0.262913  [54400/481450]\n",
      "loss: 0.045351  [57600/481450]\n",
      "loss: 0.490263  [60800/481450]\n",
      "loss: 0.320857  [64000/481450]\n",
      "loss: 0.215046  [67200/481450]\n",
      "loss: 0.055268  [70400/481450]\n",
      "loss: 0.118266  [73600/481450]\n",
      "loss: 0.304148  [76800/481450]\n",
      "loss: 0.138119  [80000/481450]\n",
      "loss: 0.280414  [83200/481450]\n",
      "loss: 0.129921  [86400/481450]\n",
      "loss: 0.187642  [89600/481450]\n",
      "loss: 0.318358  [92800/481450]\n",
      "loss: 0.417560  [96000/481450]\n",
      "loss: 0.237853  [99200/481450]\n",
      "loss: 0.281480  [102400/481450]\n",
      "loss: 0.166298  [105600/481450]\n",
      "loss: 0.417151  [108800/481450]\n",
      "loss: 0.129786  [112000/481450]\n",
      "loss: 0.133247  [115200/481450]\n",
      "loss: 0.317654  [118400/481450]\n",
      "loss: 0.174972  [121600/481450]\n",
      "loss: 0.221220  [124800/481450]\n",
      "loss: 0.233837  [128000/481450]\n",
      "loss: 0.305422  [131200/481450]\n",
      "loss: 0.428875  [134400/481450]\n",
      "loss: 0.293292  [137600/481450]\n",
      "loss: 0.084240  [140800/481450]\n",
      "loss: 0.162141  [144000/481450]\n",
      "loss: 0.120708  [147200/481450]\n",
      "loss: 0.120141  [150400/481450]\n",
      "loss: 0.243575  [153600/481450]\n",
      "loss: 0.465922  [156800/481450]\n",
      "loss: 0.362885  [160000/481450]\n",
      "loss: 0.071516  [163200/481450]\n",
      "loss: 0.292683  [166400/481450]\n",
      "loss: 0.096483  [169600/481450]\n",
      "loss: 0.292154  [172800/481450]\n",
      "loss: 0.051649  [176000/481450]\n",
      "loss: 0.250205  [179200/481450]\n",
      "loss: 0.355128  [182400/481450]\n",
      "loss: 0.237053  [185600/481450]\n",
      "loss: 0.225831  [188800/481450]\n",
      "loss: 0.224425  [192000/481450]\n",
      "loss: 0.262281  [195200/481450]\n",
      "loss: 0.512857  [198400/481450]\n",
      "loss: 0.260011  [201600/481450]\n",
      "loss: 0.135530  [204800/481450]\n",
      "loss: 0.255560  [208000/481450]\n",
      "loss: 0.235731  [211200/481450]\n",
      "loss: 0.221049  [214400/481450]\n",
      "loss: 0.243569  [217600/481450]\n",
      "loss: 0.265629  [220800/481450]\n",
      "loss: 0.204590  [224000/481450]\n",
      "loss: 0.141691  [227200/481450]\n",
      "loss: 0.253037  [230400/481450]\n",
      "loss: 0.218676  [233600/481450]\n",
      "loss: 0.322777  [236800/481450]\n",
      "loss: 0.143426  [240000/481450]\n",
      "loss: 0.317263  [243200/481450]\n",
      "loss: 0.194013  [246400/481450]\n",
      "loss: 0.368095  [249600/481450]\n",
      "loss: 0.143560  [252800/481450]\n",
      "loss: 0.175217  [256000/481450]\n",
      "loss: 0.482210  [259200/481450]\n",
      "loss: 0.111081  [262400/481450]\n",
      "loss: 0.154268  [265600/481450]\n",
      "loss: 0.281607  [268800/481450]\n",
      "loss: 0.351395  [272000/481450]\n",
      "loss: 0.339389  [275200/481450]\n",
      "loss: 0.156422  [278400/481450]\n",
      "loss: 0.306428  [281600/481450]\n",
      "loss: 0.173942  [284800/481450]\n",
      "loss: 0.284152  [288000/481450]\n",
      "loss: 0.161632  [291200/481450]\n",
      "loss: 0.039529  [294400/481450]\n",
      "loss: 0.210347  [297600/481450]\n",
      "loss: 0.127362  [300800/481450]\n",
      "loss: 0.079236  [304000/481450]\n",
      "loss: 0.189993  [307200/481450]\n",
      "loss: 0.289026  [310400/481450]\n",
      "loss: 0.109060  [313600/481450]\n",
      "loss: 0.291170  [316800/481450]\n",
      "loss: 0.105121  [320000/481450]\n",
      "loss: 0.421044  [323200/481450]\n",
      "loss: 0.321224  [326400/481450]\n",
      "loss: 0.166276  [329600/481450]\n",
      "loss: 0.367789  [332800/481450]\n",
      "loss: 0.144199  [336000/481450]\n",
      "loss: 0.336257  [339200/481450]\n",
      "loss: 0.360861  [342400/481450]\n",
      "loss: 0.435308  [345600/481450]\n",
      "loss: 0.230558  [348800/481450]\n",
      "loss: 0.133412  [352000/481450]\n",
      "loss: 0.077214  [355200/481450]\n",
      "loss: 0.164322  [358400/481450]\n",
      "loss: 0.109066  [361600/481450]\n",
      "loss: 0.155433  [364800/481450]\n",
      "loss: 0.130551  [368000/481450]\n",
      "loss: 0.362666  [371200/481450]\n",
      "loss: 0.229034  [374400/481450]\n",
      "loss: 0.245682  [377600/481450]\n",
      "loss: 0.313118  [380800/481450]\n",
      "loss: 0.335251  [384000/481450]\n",
      "loss: 0.079769  [387200/481450]\n",
      "loss: 0.223137  [390400/481450]\n",
      "loss: 0.229320  [393600/481450]\n",
      "loss: 0.272816  [396800/481450]\n",
      "loss: 0.121440  [400000/481450]\n",
      "loss: 0.206179  [403200/481450]\n",
      "loss: 0.259540  [406400/481450]\n",
      "loss: 0.231067  [409600/481450]\n",
      "loss: 0.303685  [412800/481450]\n",
      "loss: 0.241046  [416000/481450]\n",
      "loss: 0.163833  [419200/481450]\n",
      "loss: 0.106874  [422400/481450]\n",
      "loss: 0.137756  [425600/481450]\n",
      "loss: 0.173025  [428800/481450]\n",
      "loss: 0.039008  [432000/481450]\n",
      "loss: 0.178539  [435200/481450]\n",
      "loss: 0.120686  [438400/481450]\n",
      "loss: 0.132260  [441600/481450]\n",
      "loss: 0.067519  [444800/481450]\n",
      "loss: 0.127106  [448000/481450]\n",
      "loss: 0.193326  [451200/481450]\n",
      "loss: 0.179320  [454400/481450]\n",
      "loss: 0.253901  [457600/481450]\n",
      "loss: 0.101011  [460800/481450]\n",
      "loss: 0.292381  [464000/481450]\n",
      "loss: 0.315879  [467200/481450]\n",
      "loss: 0.165378  [470400/481450]\n",
      "loss: 0.138427  [473600/481450]\n",
      "loss: 0.154351  [476800/481450]\n",
      "loss: 0.149244  [480000/481450]\n",
      "Train Accuracy: 90.8734%\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 1.046010, F1-score: 83.03% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.344381  [    0/481450]\n",
      "loss: 0.286063  [ 3200/481450]\n",
      "loss: 0.127544  [ 6400/481450]\n",
      "loss: 0.373511  [ 9600/481450]\n",
      "loss: 0.163098  [12800/481450]\n",
      "loss: 0.352375  [16000/481450]\n",
      "loss: 0.166347  [19200/481450]\n",
      "loss: 0.197897  [22400/481450]\n",
      "loss: 0.090833  [25600/481450]\n",
      "loss: 0.186154  [28800/481450]\n",
      "loss: 0.441531  [32000/481450]\n",
      "loss: 0.164707  [35200/481450]\n",
      "loss: 0.285373  [38400/481450]\n",
      "loss: 0.268963  [41600/481450]\n",
      "loss: 0.190585  [44800/481450]\n",
      "loss: 0.167650  [48000/481450]\n",
      "loss: 0.217261  [51200/481450]\n",
      "loss: 0.238925  [54400/481450]\n",
      "loss: 0.165318  [57600/481450]\n",
      "loss: 0.099185  [60800/481450]\n",
      "loss: 0.344007  [64000/481450]\n",
      "loss: 0.358989  [67200/481450]\n",
      "loss: 0.192965  [70400/481450]\n",
      "loss: 0.130420  [73600/481450]\n",
      "loss: 0.624781  [76800/481450]\n",
      "loss: 0.170679  [80000/481450]\n",
      "loss: 0.385574  [83200/481450]\n",
      "loss: 0.489510  [86400/481450]\n",
      "loss: 0.304017  [89600/481450]\n",
      "loss: 0.140733  [92800/481450]\n",
      "loss: 0.115000  [96000/481450]\n",
      "loss: 0.557511  [99200/481450]\n",
      "loss: 0.295956  [102400/481450]\n",
      "loss: 0.246920  [105600/481450]\n",
      "loss: 0.217070  [108800/481450]\n",
      "loss: 0.683772  [112000/481450]\n",
      "loss: 0.240641  [115200/481450]\n",
      "loss: 0.151984  [118400/481450]\n",
      "loss: 0.211645  [121600/481450]\n",
      "loss: 0.150434  [124800/481450]\n",
      "loss: 0.308551  [128000/481450]\n",
      "loss: 0.226815  [131200/481450]\n",
      "loss: 0.185584  [134400/481450]\n",
      "loss: 0.294961  [137600/481450]\n",
      "loss: 0.317953  [140800/481450]\n",
      "loss: 0.449248  [144000/481450]\n",
      "loss: 0.206729  [147200/481450]\n",
      "loss: 0.216805  [150400/481450]\n",
      "loss: 0.117865  [153600/481450]\n",
      "loss: 0.493083  [156800/481450]\n",
      "loss: 0.183804  [160000/481450]\n",
      "loss: 0.369448  [163200/481450]\n",
      "loss: 0.202376  [166400/481450]\n",
      "loss: 0.234641  [169600/481450]\n",
      "loss: 0.196706  [172800/481450]\n",
      "loss: 0.485747  [176000/481450]\n",
      "loss: 0.170262  [179200/481450]\n",
      "loss: 0.115981  [182400/481450]\n",
      "loss: 0.239845  [185600/481450]\n",
      "loss: 0.074591  [188800/481450]\n",
      "loss: 0.190858  [192000/481450]\n",
      "loss: 0.234477  [195200/481450]\n",
      "loss: 0.124047  [198400/481450]\n",
      "loss: 0.170518  [201600/481450]\n",
      "loss: 0.344852  [204800/481450]\n",
      "loss: 0.330308  [208000/481450]\n",
      "loss: 0.113991  [211200/481450]\n",
      "loss: 0.127486  [214400/481450]\n",
      "loss: 0.142972  [217600/481450]\n",
      "loss: 0.221746  [220800/481450]\n",
      "loss: 0.113867  [224000/481450]\n",
      "loss: 0.212537  [227200/481450]\n",
      "loss: 0.098386  [230400/481450]\n",
      "loss: 0.066939  [233600/481450]\n",
      "loss: 0.290710  [236800/481450]\n",
      "loss: 0.333349  [240000/481450]\n",
      "loss: 0.223572  [243200/481450]\n",
      "loss: 0.221004  [246400/481450]\n",
      "loss: 0.300318  [249600/481450]\n",
      "loss: 0.237491  [252800/481450]\n",
      "loss: 0.112982  [256000/481450]\n",
      "loss: 0.257761  [259200/481450]\n",
      "loss: 0.117774  [262400/481450]\n",
      "loss: 0.242263  [265600/481450]\n",
      "loss: 0.222846  [268800/481450]\n",
      "loss: 0.190483  [272000/481450]\n",
      "loss: 0.212128  [275200/481450]\n",
      "loss: 0.150346  [278400/481450]\n",
      "loss: 0.199471  [281600/481450]\n",
      "loss: 0.133987  [284800/481450]\n",
      "loss: 0.067226  [288000/481450]\n",
      "loss: 0.267856  [291200/481450]\n",
      "loss: 0.154595  [294400/481450]\n",
      "loss: 0.160004  [297600/481450]\n",
      "loss: 0.222741  [300800/481450]\n",
      "loss: 0.318584  [304000/481450]\n",
      "loss: 0.164096  [307200/481450]\n",
      "loss: 0.148446  [310400/481450]\n",
      "loss: 0.218578  [313600/481450]\n",
      "loss: 0.216909  [316800/481450]\n",
      "loss: 0.004463  [320000/481450]\n",
      "loss: 0.193201  [323200/481450]\n",
      "loss: 0.164271  [326400/481450]\n",
      "loss: 0.078031  [329600/481450]\n",
      "loss: 0.087936  [332800/481450]\n",
      "loss: 0.232859  [336000/481450]\n",
      "loss: 0.110465  [339200/481450]\n",
      "loss: 0.283632  [342400/481450]\n",
      "loss: 0.285165  [345600/481450]\n",
      "loss: 0.697700  [348800/481450]\n",
      "loss: 0.151074  [352000/481450]\n",
      "loss: 0.246111  [355200/481450]\n",
      "loss: 0.273773  [358400/481450]\n",
      "loss: 0.120097  [361600/481450]\n",
      "loss: 0.366210  [364800/481450]\n",
      "loss: 0.202036  [368000/481450]\n",
      "loss: 0.113978  [371200/481450]\n",
      "loss: 0.366334  [374400/481450]\n",
      "loss: 0.145191  [377600/481450]\n",
      "loss: 0.314878  [380800/481450]\n",
      "loss: 0.112991  [384000/481450]\n",
      "loss: 0.185347  [387200/481450]\n",
      "loss: 0.257826  [390400/481450]\n",
      "loss: 0.291619  [393600/481450]\n",
      "loss: 0.172741  [396800/481450]\n",
      "loss: 0.364789  [400000/481450]\n",
      "loss: 0.157195  [403200/481450]\n",
      "loss: 0.191782  [406400/481450]\n",
      "loss: 0.368274  [409600/481450]\n",
      "loss: 0.184763  [412800/481450]\n",
      "loss: 0.115731  [416000/481450]\n",
      "loss: 0.160474  [419200/481450]\n",
      "loss: 0.095053  [422400/481450]\n",
      "loss: 0.290832  [425600/481450]\n",
      "loss: 0.252323  [428800/481450]\n",
      "loss: 0.487456  [432000/481450]\n",
      "loss: 0.339996  [435200/481450]\n",
      "loss: 0.055860  [438400/481450]\n",
      "loss: 0.060004  [441600/481450]\n",
      "loss: 0.231012  [444800/481450]\n",
      "loss: 0.187210  [448000/481450]\n",
      "loss: 0.155906  [451200/481450]\n",
      "loss: 0.246579  [454400/481450]\n",
      "loss: 0.337636  [457600/481450]\n",
      "loss: 0.336697  [460800/481450]\n",
      "loss: 0.482539  [464000/481450]\n",
      "loss: 0.136954  [467200/481450]\n",
      "loss: 0.039758  [470400/481450]\n",
      "loss: 0.059385  [473600/481450]\n",
      "loss: 0.089324  [476800/481450]\n",
      "loss: 0.180950  [480000/481450]\n",
      "Train Accuracy: 91.0169%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 1.116099, F1-score: 83.10% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.068855  [    0/481450]\n",
      "loss: 0.265877  [ 3200/481450]\n",
      "loss: 0.224429  [ 6400/481450]\n",
      "loss: 0.394636  [ 9600/481450]\n",
      "loss: 0.357750  [12800/481450]\n",
      "loss: 0.084187  [16000/481450]\n",
      "loss: 0.209484  [19200/481450]\n",
      "loss: 0.281998  [22400/481450]\n",
      "loss: 0.296498  [25600/481450]\n",
      "loss: 0.153824  [28800/481450]\n",
      "loss: 0.237564  [32000/481450]\n",
      "loss: 0.129332  [35200/481450]\n",
      "loss: 0.130316  [38400/481450]\n",
      "loss: 0.192477  [41600/481450]\n",
      "loss: 0.246935  [44800/481450]\n",
      "loss: 0.130094  [48000/481450]\n",
      "loss: 0.160064  [51200/481450]\n",
      "loss: 0.185242  [54400/481450]\n",
      "loss: 0.152129  [57600/481450]\n",
      "loss: 0.190702  [60800/481450]\n",
      "loss: 0.087289  [64000/481450]\n",
      "loss: 0.202753  [67200/481450]\n",
      "loss: 0.309268  [70400/481450]\n",
      "loss: 0.209066  [73600/481450]\n",
      "loss: 0.300468  [76800/481450]\n",
      "loss: 0.254262  [80000/481450]\n",
      "loss: 0.170311  [83200/481450]\n",
      "loss: 0.210704  [86400/481450]\n",
      "loss: 0.290284  [89600/481450]\n",
      "loss: 0.229595  [92800/481450]\n",
      "loss: 0.128166  [96000/481450]\n",
      "loss: 0.240543  [99200/481450]\n",
      "loss: 0.158991  [102400/481450]\n",
      "loss: 0.057881  [105600/481450]\n",
      "loss: 0.165271  [108800/481450]\n",
      "loss: 0.085876  [112000/481450]\n",
      "loss: 0.324823  [115200/481450]\n",
      "loss: 0.104688  [118400/481450]\n",
      "loss: 0.182970  [121600/481450]\n",
      "loss: 0.081042  [124800/481450]\n",
      "loss: 0.568633  [128000/481450]\n",
      "loss: 0.163413  [131200/481450]\n",
      "loss: 0.234842  [134400/481450]\n",
      "loss: 0.248168  [137600/481450]\n",
      "loss: 0.235061  [140800/481450]\n",
      "loss: 0.261777  [144000/481450]\n",
      "loss: 0.194632  [147200/481450]\n",
      "loss: 0.195724  [150400/481450]\n",
      "loss: 0.344232  [153600/481450]\n",
      "loss: 0.286040  [156800/481450]\n",
      "loss: 0.210617  [160000/481450]\n",
      "loss: 0.331185  [163200/481450]\n",
      "loss: 0.104257  [166400/481450]\n",
      "loss: 0.464078  [169600/481450]\n",
      "loss: 0.313656  [172800/481450]\n",
      "loss: 0.291956  [176000/481450]\n",
      "loss: 0.227550  [179200/481450]\n",
      "loss: 0.279886  [182400/481450]\n",
      "loss: 0.476366  [185600/481450]\n",
      "loss: 0.354646  [188800/481450]\n",
      "loss: 0.279426  [192000/481450]\n",
      "loss: 0.169757  [195200/481450]\n",
      "loss: 0.144762  [198400/481450]\n",
      "loss: 0.180766  [201600/481450]\n",
      "loss: 0.334273  [204800/481450]\n",
      "loss: 0.286442  [208000/481450]\n",
      "loss: 0.166513  [211200/481450]\n",
      "loss: 0.186257  [214400/481450]\n",
      "loss: 0.084445  [217600/481450]\n",
      "loss: 0.092538  [220800/481450]\n",
      "loss: 0.144814  [224000/481450]\n",
      "loss: 0.371731  [227200/481450]\n",
      "loss: 0.089399  [230400/481450]\n",
      "loss: 0.320226  [233600/481450]\n",
      "loss: 0.175436  [236800/481450]\n",
      "loss: 0.163447  [240000/481450]\n",
      "loss: 0.275446  [243200/481450]\n",
      "loss: 0.108201  [246400/481450]\n",
      "loss: 0.195405  [249600/481450]\n",
      "loss: 0.325664  [252800/481450]\n",
      "loss: 0.014883  [256000/481450]\n",
      "loss: 0.150664  [259200/481450]\n",
      "loss: 0.261742  [262400/481450]\n",
      "loss: 0.189766  [265600/481450]\n",
      "loss: 0.254829  [268800/481450]\n",
      "loss: 0.337132  [272000/481450]\n",
      "loss: 0.361579  [275200/481450]\n",
      "loss: 0.192122  [278400/481450]\n",
      "loss: 0.177492  [281600/481450]\n",
      "loss: 0.205246  [284800/481450]\n",
      "loss: 0.199208  [288000/481450]\n",
      "loss: 0.207123  [291200/481450]\n",
      "loss: 0.152365  [294400/481450]\n",
      "loss: 0.139211  [297600/481450]\n",
      "loss: 0.101020  [300800/481450]\n",
      "loss: 0.012985  [304000/481450]\n",
      "loss: 0.285030  [307200/481450]\n",
      "loss: 0.289946  [310400/481450]\n",
      "loss: 0.159979  [313600/481450]\n",
      "loss: 0.241045  [316800/481450]\n",
      "loss: 0.541311  [320000/481450]\n",
      "loss: 0.273455  [323200/481450]\n",
      "loss: 0.205017  [326400/481450]\n",
      "loss: 0.317654  [329600/481450]\n",
      "loss: 0.345834  [332800/481450]\n",
      "loss: 0.220816  [336000/481450]\n",
      "loss: 0.185209  [339200/481450]\n",
      "loss: 0.102229  [342400/481450]\n",
      "loss: 0.155331  [345600/481450]\n",
      "loss: 0.114109  [348800/481450]\n",
      "loss: 0.295761  [352000/481450]\n",
      "loss: 0.175585  [355200/481450]\n",
      "loss: 0.195493  [358400/481450]\n",
      "loss: 0.198734  [361600/481450]\n",
      "loss: 0.188711  [364800/481450]\n",
      "loss: 0.444813  [368000/481450]\n",
      "loss: 0.136044  [371200/481450]\n",
      "loss: 0.109863  [374400/481450]\n",
      "loss: 0.318910  [377600/481450]\n",
      "loss: 0.109164  [380800/481450]\n",
      "loss: 0.289863  [384000/481450]\n",
      "loss: 0.205111  [387200/481450]\n",
      "loss: 0.265915  [390400/481450]\n",
      "loss: 0.118551  [393600/481450]\n",
      "loss: 0.126768  [396800/481450]\n",
      "loss: 0.245494  [400000/481450]\n",
      "loss: 0.186042  [403200/481450]\n",
      "loss: 0.195724  [406400/481450]\n",
      "loss: 0.288364  [409600/481450]\n",
      "loss: 0.144505  [412800/481450]\n",
      "loss: 0.156725  [416000/481450]\n",
      "loss: 0.202989  [419200/481450]\n",
      "loss: 0.317606  [422400/481450]\n",
      "loss: 0.131769  [425600/481450]\n",
      "loss: 0.163016  [428800/481450]\n",
      "loss: 0.285042  [432000/481450]\n",
      "loss: 0.113743  [435200/481450]\n",
      "loss: 0.324892  [438400/481450]\n",
      "loss: 0.263745  [441600/481450]\n",
      "loss: 0.066338  [444800/481450]\n",
      "loss: 0.173876  [448000/481450]\n",
      "loss: 0.338319  [451200/481450]\n",
      "loss: 0.186648  [454400/481450]\n",
      "loss: 0.321994  [457600/481450]\n",
      "loss: 0.159905  [460800/481450]\n",
      "loss: 0.278161  [464000/481450]\n",
      "loss: 0.144789  [467200/481450]\n",
      "loss: 0.297002  [470400/481450]\n",
      "loss: 0.115544  [473600/481450]\n",
      "loss: 0.371930  [476800/481450]\n",
      "loss: 0.179775  [480000/481450]\n",
      "Train Accuracy: 91.1114%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 1.121049, F1-score: 82.81% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.193970  [    0/481450]\n",
      "loss: 0.247240  [ 3200/481450]\n",
      "loss: 0.144182  [ 6400/481450]\n",
      "loss: 0.411409  [ 9600/481450]\n",
      "loss: 0.068441  [12800/481450]\n",
      "loss: 0.040667  [16000/481450]\n",
      "loss: 0.227308  [19200/481450]\n",
      "loss: 0.418315  [22400/481450]\n",
      "loss: 0.035662  [25600/481450]\n",
      "loss: 0.093629  [28800/481450]\n",
      "loss: 0.421443  [32000/481450]\n",
      "loss: 0.175183  [35200/481450]\n",
      "loss: 0.162368  [38400/481450]\n",
      "loss: 0.225891  [41600/481450]\n",
      "loss: 0.140987  [44800/481450]\n",
      "loss: 0.529438  [48000/481450]\n",
      "loss: 0.314720  [51200/481450]\n",
      "loss: 0.167695  [54400/481450]\n",
      "loss: 0.257880  [57600/481450]\n",
      "loss: 0.069148  [60800/481450]\n",
      "loss: 0.050057  [64000/481450]\n",
      "loss: 0.129037  [67200/481450]\n",
      "loss: 0.313961  [70400/481450]\n",
      "loss: 0.199316  [73600/481450]\n",
      "loss: 0.318088  [76800/481450]\n",
      "loss: 0.172370  [80000/481450]\n",
      "loss: 0.312193  [83200/481450]\n",
      "loss: 0.230829  [86400/481450]\n",
      "loss: 0.118294  [89600/481450]\n",
      "loss: 0.231040  [92800/481450]\n",
      "loss: 0.300937  [96000/481450]\n",
      "loss: 0.109076  [99200/481450]\n",
      "loss: 0.052121  [102400/481450]\n",
      "loss: 0.207514  [105600/481450]\n",
      "loss: 0.345358  [108800/481450]\n",
      "loss: 0.199530  [112000/481450]\n",
      "loss: 0.126192  [115200/481450]\n",
      "loss: 0.185058  [118400/481450]\n",
      "loss: 0.290190  [121600/481450]\n",
      "loss: 0.233502  [124800/481450]\n",
      "loss: 0.208137  [128000/481450]\n",
      "loss: 0.379228  [131200/481450]\n",
      "loss: 0.128356  [134400/481450]\n",
      "loss: 0.126051  [137600/481450]\n",
      "loss: 0.296758  [140800/481450]\n",
      "loss: 0.193909  [144000/481450]\n",
      "loss: 0.297494  [147200/481450]\n",
      "loss: 0.157652  [150400/481450]\n",
      "loss: 0.155385  [153600/481450]\n",
      "loss: 0.239256  [156800/481450]\n",
      "loss: 0.192140  [160000/481450]\n",
      "loss: 0.322368  [163200/481450]\n",
      "loss: 0.177799  [166400/481450]\n",
      "loss: 0.162878  [169600/481450]\n",
      "loss: 0.217310  [172800/481450]\n",
      "loss: 0.142504  [176000/481450]\n",
      "loss: 0.074831  [179200/481450]\n",
      "loss: 0.218498  [182400/481450]\n",
      "loss: 0.031844  [185600/481450]\n",
      "loss: 0.333920  [188800/481450]\n",
      "loss: 0.215992  [192000/481450]\n",
      "loss: 0.157708  [195200/481450]\n",
      "loss: 0.139045  [198400/481450]\n",
      "loss: 0.172445  [201600/481450]\n",
      "loss: 0.295739  [204800/481450]\n",
      "loss: 0.161577  [208000/481450]\n",
      "loss: 0.356441  [211200/481450]\n",
      "loss: 0.363894  [214400/481450]\n",
      "loss: 0.223625  [217600/481450]\n",
      "loss: 0.151824  [220800/481450]\n",
      "loss: 0.120301  [224000/481450]\n",
      "loss: 0.258964  [227200/481450]\n",
      "loss: 0.117632  [230400/481450]\n",
      "loss: 0.322893  [233600/481450]\n",
      "loss: 0.490747  [236800/481450]\n",
      "loss: 0.183430  [240000/481450]\n",
      "loss: 0.150647  [243200/481450]\n",
      "loss: 0.305473  [246400/481450]\n",
      "loss: 0.124913  [249600/481450]\n",
      "loss: 0.187231  [252800/481450]\n",
      "loss: 0.163563  [256000/481450]\n",
      "loss: 0.341579  [259200/481450]\n",
      "loss: 0.202693  [262400/481450]\n",
      "loss: 0.360378  [265600/481450]\n",
      "loss: 0.163806  [268800/481450]\n",
      "loss: 0.252934  [272000/481450]\n",
      "loss: 0.220250  [275200/481450]\n",
      "loss: 0.223044  [278400/481450]\n",
      "loss: 0.117509  [281600/481450]\n",
      "loss: 0.162398  [284800/481450]\n",
      "loss: 0.424752  [288000/481450]\n",
      "loss: 0.147217  [291200/481450]\n",
      "loss: 0.312339  [294400/481450]\n",
      "loss: 0.145353  [297600/481450]\n",
      "loss: 0.220080  [300800/481450]\n",
      "loss: 0.257573  [304000/481450]\n",
      "loss: 0.203558  [307200/481450]\n",
      "loss: 0.289097  [310400/481450]\n",
      "loss: 0.096442  [313600/481450]\n",
      "loss: 0.222359  [316800/481450]\n",
      "loss: 0.229911  [320000/481450]\n",
      "loss: 0.262708  [323200/481450]\n",
      "loss: 0.288320  [326400/481450]\n",
      "loss: 0.156938  [329600/481450]\n",
      "loss: 0.197038  [332800/481450]\n",
      "loss: 0.215670  [336000/481450]\n",
      "loss: 0.485957  [339200/481450]\n",
      "loss: 0.176525  [342400/481450]\n",
      "loss: 0.213426  [345600/481450]\n",
      "loss: 0.228995  [348800/481450]\n",
      "loss: 0.227498  [352000/481450]\n",
      "loss: 0.270717  [355200/481450]\n",
      "loss: 0.126660  [358400/481450]\n",
      "loss: 0.136106  [361600/481450]\n",
      "loss: 0.323381  [364800/481450]\n",
      "loss: 0.250436  [368000/481450]\n",
      "loss: 0.138654  [371200/481450]\n",
      "loss: 0.222310  [374400/481450]\n",
      "loss: 0.139957  [377600/481450]\n",
      "loss: 0.105838  [380800/481450]\n",
      "loss: 0.709455  [384000/481450]\n",
      "loss: 0.199188  [387200/481450]\n",
      "loss: 0.111331  [390400/481450]\n",
      "loss: 0.286683  [393600/481450]\n",
      "loss: 0.139394  [396800/481450]\n",
      "loss: 0.158645  [400000/481450]\n",
      "loss: 0.303210  [403200/481450]\n",
      "loss: 0.149743  [406400/481450]\n",
      "loss: 0.187891  [409600/481450]\n",
      "loss: 0.372219  [412800/481450]\n",
      "loss: 0.166122  [416000/481450]\n",
      "loss: 0.142080  [419200/481450]\n",
      "loss: 0.145084  [422400/481450]\n",
      "loss: 0.136854  [425600/481450]\n",
      "loss: 0.239941  [428800/481450]\n",
      "loss: 0.304882  [432000/481450]\n",
      "loss: 0.262946  [435200/481450]\n",
      "loss: 0.371229  [438400/481450]\n",
      "loss: 0.277884  [441600/481450]\n",
      "loss: 0.139939  [444800/481450]\n",
      "loss: 0.108794  [448000/481450]\n",
      "loss: 0.361739  [451200/481450]\n",
      "loss: 0.105326  [454400/481450]\n",
      "loss: 0.196288  [457600/481450]\n",
      "loss: 0.257302  [460800/481450]\n",
      "loss: 0.340025  [464000/481450]\n",
      "loss: 0.231999  [467200/481450]\n",
      "loss: 0.129740  [470400/481450]\n",
      "loss: 0.238847  [473600/481450]\n",
      "loss: 0.039306  [476800/481450]\n",
      "loss: 0.129033  [480000/481450]\n",
      "Train Accuracy: 91.1904%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 1.096391, F1-score: 82.97% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.267159  [    0/481450]\n",
      "loss: 0.505399  [ 3200/481450]\n",
      "loss: 0.094743  [ 6400/481450]\n",
      "loss: 0.370475  [ 9600/481450]\n",
      "loss: 0.157047  [12800/481450]\n",
      "loss: 0.272752  [16000/481450]\n",
      "loss: 0.196846  [19200/481450]\n",
      "loss: 0.253546  [22400/481450]\n",
      "loss: 0.283454  [25600/481450]\n",
      "loss: 0.230220  [28800/481450]\n",
      "loss: 0.378413  [32000/481450]\n",
      "loss: 0.169791  [35200/481450]\n",
      "loss: 0.152405  [38400/481450]\n",
      "loss: 0.089892  [41600/481450]\n",
      "loss: 0.149595  [44800/481450]\n",
      "loss: 0.259914  [48000/481450]\n",
      "loss: 0.099813  [51200/481450]\n",
      "loss: 0.131646  [54400/481450]\n",
      "loss: 0.117641  [57600/481450]\n",
      "loss: 0.131594  [60800/481450]\n",
      "loss: 0.129928  [64000/481450]\n",
      "loss: 0.439288  [67200/481450]\n",
      "loss: 0.227572  [70400/481450]\n",
      "loss: 0.284980  [73600/481450]\n",
      "loss: 0.279557  [76800/481450]\n",
      "loss: 0.214610  [80000/481450]\n",
      "loss: 0.304770  [83200/481450]\n",
      "loss: 0.273592  [86400/481450]\n",
      "loss: 0.152418  [89600/481450]\n",
      "loss: 0.340006  [92800/481450]\n",
      "loss: 0.255142  [96000/481450]\n",
      "loss: 0.030130  [99200/481450]\n",
      "loss: 0.457284  [102400/481450]\n",
      "loss: 0.184577  [105600/481450]\n",
      "loss: 0.183914  [108800/481450]\n",
      "loss: 0.144164  [112000/481450]\n",
      "loss: 0.173153  [115200/481450]\n",
      "loss: 0.225029  [118400/481450]\n",
      "loss: 0.199695  [121600/481450]\n",
      "loss: 0.129231  [124800/481450]\n",
      "loss: 0.063230  [128000/481450]\n",
      "loss: 0.171349  [131200/481450]\n",
      "loss: 0.192400  [134400/481450]\n",
      "loss: 0.227305  [137600/481450]\n",
      "loss: 0.148203  [140800/481450]\n",
      "loss: 0.074631  [144000/481450]\n",
      "loss: 0.031350  [147200/481450]\n",
      "loss: 0.166899  [150400/481450]\n",
      "loss: 0.123944  [153600/481450]\n",
      "loss: 0.330621  [156800/481450]\n",
      "loss: 0.301974  [160000/481450]\n",
      "loss: 0.164662  [163200/481450]\n",
      "loss: 0.141181  [166400/481450]\n",
      "loss: 0.530280  [169600/481450]\n",
      "loss: 0.392479  [172800/481450]\n",
      "loss: 0.164167  [176000/481450]\n",
      "loss: 0.318554  [179200/481450]\n",
      "loss: 0.192997  [182400/481450]\n",
      "loss: 0.224359  [185600/481450]\n",
      "loss: 0.222899  [188800/481450]\n",
      "loss: 0.349492  [192000/481450]\n",
      "loss: 0.227343  [195200/481450]\n",
      "loss: 0.170738  [198400/481450]\n",
      "loss: 0.106347  [201600/481450]\n",
      "loss: 0.270717  [204800/481450]\n",
      "loss: 0.134652  [208000/481450]\n",
      "loss: 0.224002  [211200/481450]\n",
      "loss: 0.247099  [214400/481450]\n",
      "loss: 0.299615  [217600/481450]\n",
      "loss: 0.145111  [220800/481450]\n",
      "loss: 0.324134  [224000/481450]\n",
      "loss: 0.214691  [227200/481450]\n",
      "loss: 0.168533  [230400/481450]\n",
      "loss: 0.094387  [233600/481450]\n",
      "loss: 0.349523  [236800/481450]\n",
      "loss: 0.360540  [240000/481450]\n",
      "loss: 0.247932  [243200/481450]\n",
      "loss: 0.429548  [246400/481450]\n",
      "loss: 0.259303  [249600/481450]\n",
      "loss: 0.246224  [252800/481450]\n",
      "loss: 0.259511  [256000/481450]\n",
      "loss: 0.138586  [259200/481450]\n",
      "loss: 0.081430  [262400/481450]\n",
      "loss: 0.331711  [265600/481450]\n",
      "loss: 0.159979  [268800/481450]\n",
      "loss: 0.207272  [272000/481450]\n",
      "loss: 0.283099  [275200/481450]\n",
      "loss: 0.103394  [278400/481450]\n",
      "loss: 0.152908  [281600/481450]\n",
      "loss: 0.076624  [284800/481450]\n",
      "loss: 0.189833  [288000/481450]\n",
      "loss: 0.265709  [291200/481450]\n",
      "loss: 0.149500  [294400/481450]\n",
      "loss: 0.209359  [297600/481450]\n",
      "loss: 0.203176  [300800/481450]\n",
      "loss: 0.154493  [304000/481450]\n",
      "loss: 0.183387  [307200/481450]\n",
      "loss: 0.260930  [310400/481450]\n",
      "loss: 0.287328  [313600/481450]\n",
      "loss: 0.237227  [316800/481450]\n",
      "loss: 0.238929  [320000/481450]\n",
      "loss: 0.228716  [323200/481450]\n",
      "loss: 0.272847  [326400/481450]\n",
      "loss: 0.322246  [329600/481450]\n",
      "loss: 0.214730  [332800/481450]\n",
      "loss: 0.121503  [336000/481450]\n",
      "loss: 0.206136  [339200/481450]\n",
      "loss: 0.467604  [342400/481450]\n",
      "loss: 0.094567  [345600/481450]\n",
      "loss: 0.138511  [348800/481450]\n",
      "loss: 0.577367  [352000/481450]\n",
      "loss: 0.195844  [355200/481450]\n",
      "loss: 0.068781  [358400/481450]\n",
      "loss: 0.142703  [361600/481450]\n",
      "loss: 0.218515  [364800/481450]\n",
      "loss: 0.078398  [368000/481450]\n",
      "loss: 0.167475  [371200/481450]\n",
      "loss: 0.574067  [374400/481450]\n",
      "loss: 0.158606  [377600/481450]\n",
      "loss: 0.174515  [380800/481450]\n",
      "loss: 0.174517  [384000/481450]\n",
      "loss: 0.414817  [387200/481450]\n",
      "loss: 0.213830  [390400/481450]\n",
      "loss: 0.127309  [393600/481450]\n",
      "loss: 0.227273  [396800/481450]\n",
      "loss: 0.283726  [400000/481450]\n",
      "loss: 0.206531  [403200/481450]\n",
      "loss: 0.250951  [406400/481450]\n",
      "loss: 0.200572  [409600/481450]\n",
      "loss: 0.196120  [412800/481450]\n",
      "loss: 0.230741  [416000/481450]\n",
      "loss: 0.157280  [419200/481450]\n",
      "loss: 0.167756  [422400/481450]\n",
      "loss: 0.374235  [425600/481450]\n",
      "loss: 0.318018  [428800/481450]\n",
      "loss: 0.191676  [432000/481450]\n",
      "loss: 0.319309  [435200/481450]\n",
      "loss: 0.288888  [438400/481450]\n",
      "loss: 0.182101  [441600/481450]\n",
      "loss: 0.339552  [444800/481450]\n",
      "loss: 0.239930  [448000/481450]\n",
      "loss: 0.205684  [451200/481450]\n",
      "loss: 0.219015  [454400/481450]\n",
      "loss: 0.237689  [457600/481450]\n",
      "loss: 0.290183  [460800/481450]\n",
      "loss: 0.260962  [464000/481450]\n",
      "loss: 0.086068  [467200/481450]\n",
      "loss: 0.229147  [470400/481450]\n",
      "loss: 0.164227  [473600/481450]\n",
      "loss: 0.160439  [476800/481450]\n",
      "loss: 0.380036  [480000/481450]\n",
      "Train Accuracy: 91.3019%\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 1.074064, F1-score: 83.24% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.233208  [    0/481450]\n",
      "loss: 0.231228  [ 3200/481450]\n",
      "loss: 0.151644  [ 6400/481450]\n",
      "loss: 0.211626  [ 9600/481450]\n",
      "loss: 0.347943  [12800/481450]\n",
      "loss: 0.202511  [16000/481450]\n",
      "loss: 0.226612  [19200/481450]\n",
      "loss: 0.185854  [22400/481450]\n",
      "loss: 0.165024  [25600/481450]\n",
      "loss: 0.320961  [28800/481450]\n",
      "loss: 0.294563  [32000/481450]\n",
      "loss: 0.101375  [35200/481450]\n",
      "loss: 0.168258  [38400/481450]\n",
      "loss: 0.199929  [41600/481450]\n",
      "loss: 0.079739  [44800/481450]\n",
      "loss: 0.084835  [48000/481450]\n",
      "loss: 0.277692  [51200/481450]\n",
      "loss: 0.115764  [54400/481450]\n",
      "loss: 0.161045  [57600/481450]\n",
      "loss: 0.711360  [60800/481450]\n",
      "loss: 0.108036  [64000/481450]\n",
      "loss: 0.227059  [67200/481450]\n",
      "loss: 0.060815  [70400/481450]\n",
      "loss: 0.332325  [73600/481450]\n",
      "loss: 0.025612  [76800/481450]\n",
      "loss: 0.174744  [80000/481450]\n",
      "loss: 0.199911  [83200/481450]\n",
      "loss: 0.249482  [86400/481450]\n",
      "loss: 0.242073  [89600/481450]\n",
      "loss: 0.235779  [92800/481450]\n",
      "loss: 0.138846  [96000/481450]\n",
      "loss: 0.134981  [99200/481450]\n",
      "loss: 0.349744  [102400/481450]\n",
      "loss: 0.159292  [105600/481450]\n",
      "loss: 0.310530  [108800/481450]\n",
      "loss: 0.218086  [112000/481450]\n",
      "loss: 0.080761  [115200/481450]\n",
      "loss: 0.124636  [118400/481450]\n",
      "loss: 0.121838  [121600/481450]\n",
      "loss: 0.402816  [124800/481450]\n",
      "loss: 0.140126  [128000/481450]\n",
      "loss: 0.258508  [131200/481450]\n",
      "loss: 0.422294  [134400/481450]\n",
      "loss: 0.292329  [137600/481450]\n",
      "loss: 0.153626  [140800/481450]\n",
      "loss: 0.135945  [144000/481450]\n",
      "loss: 0.101650  [147200/481450]\n",
      "loss: 0.220754  [150400/481450]\n",
      "loss: 0.409094  [153600/481450]\n",
      "loss: 0.139742  [156800/481450]\n",
      "loss: 0.174680  [160000/481450]\n",
      "loss: 0.050461  [163200/481450]\n",
      "loss: 0.143096  [166400/481450]\n",
      "loss: 0.107143  [169600/481450]\n",
      "loss: 0.340681  [172800/481450]\n",
      "loss: 0.289154  [176000/481450]\n",
      "loss: 0.147733  [179200/481450]\n",
      "loss: 0.122624  [182400/481450]\n",
      "loss: 0.249529  [185600/481450]\n",
      "loss: 0.278234  [188800/481450]\n",
      "loss: 0.111221  [192000/481450]\n",
      "loss: 0.122121  [195200/481450]\n",
      "loss: 0.328162  [198400/481450]\n",
      "loss: 0.142754  [201600/481450]\n",
      "loss: 0.157129  [204800/481450]\n",
      "loss: 0.162400  [208000/481450]\n",
      "loss: 0.229098  [211200/481450]\n",
      "loss: 0.592983  [214400/481450]\n",
      "loss: 0.181065  [217600/481450]\n",
      "loss: 0.172513  [220800/481450]\n",
      "loss: 0.192744  [224000/481450]\n",
      "loss: 0.253591  [227200/481450]\n",
      "loss: 0.134742  [230400/481450]\n",
      "loss: 0.276058  [233600/481450]\n",
      "loss: 0.147531  [236800/481450]\n",
      "loss: 0.166856  [240000/481450]\n",
      "loss: 0.226650  [243200/481450]\n",
      "loss: 0.421790  [246400/481450]\n",
      "loss: 0.107942  [249600/481450]\n",
      "loss: 0.231915  [252800/481450]\n",
      "loss: 0.108389  [256000/481450]\n",
      "loss: 0.100448  [259200/481450]\n",
      "loss: 0.216137  [262400/481450]\n",
      "loss: 0.225923  [265600/481450]\n",
      "loss: 0.206915  [268800/481450]\n",
      "loss: 0.197162  [272000/481450]\n",
      "loss: 0.243759  [275200/481450]\n",
      "loss: 0.224875  [278400/481450]\n",
      "loss: 0.227548  [281600/481450]\n",
      "loss: 0.207527  [284800/481450]\n",
      "loss: 0.192216  [288000/481450]\n",
      "loss: 0.122263  [291200/481450]\n",
      "loss: 0.107713  [294400/481450]\n",
      "loss: 0.053993  [297600/481450]\n",
      "loss: 0.144388  [300800/481450]\n",
      "loss: 0.187453  [304000/481450]\n",
      "loss: 0.108735  [307200/481450]\n",
      "loss: 0.068905  [310400/481450]\n",
      "loss: 0.088417  [313600/481450]\n",
      "loss: 0.305576  [316800/481450]\n",
      "loss: 0.212501  [320000/481450]\n",
      "loss: 0.248408  [323200/481450]\n",
      "loss: 0.016797  [326400/481450]\n",
      "loss: 0.217810  [329600/481450]\n",
      "loss: 0.245841  [332800/481450]\n",
      "loss: 0.298675  [336000/481450]\n",
      "loss: 0.215579  [339200/481450]\n",
      "loss: 0.088116  [342400/481450]\n",
      "loss: 0.088229  [345600/481450]\n",
      "loss: 0.159461  [348800/481450]\n",
      "loss: 0.134625  [352000/481450]\n",
      "loss: 0.323300  [355200/481450]\n",
      "loss: 0.087293  [358400/481450]\n",
      "loss: 0.402684  [361600/481450]\n",
      "loss: 0.207588  [364800/481450]\n",
      "loss: 0.365751  [368000/481450]\n",
      "loss: 0.062241  [371200/481450]\n",
      "loss: 0.331996  [374400/481450]\n",
      "loss: 0.156180  [377600/481450]\n",
      "loss: 0.206512  [380800/481450]\n",
      "loss: 0.151263  [384000/481450]\n",
      "loss: 0.266751  [387200/481450]\n",
      "loss: 0.379160  [390400/481450]\n",
      "loss: 0.218094  [393600/481450]\n",
      "loss: 0.169452  [396800/481450]\n",
      "loss: 0.088208  [400000/481450]\n",
      "loss: 0.194879  [403200/481450]\n",
      "loss: 0.164528  [406400/481450]\n",
      "loss: 0.243170  [409600/481450]\n",
      "loss: 0.438474  [412800/481450]\n",
      "loss: 0.116967  [416000/481450]\n",
      "loss: 0.227331  [419200/481450]\n",
      "loss: 0.309946  [422400/481450]\n",
      "loss: 0.089183  [425600/481450]\n",
      "loss: 0.278777  [428800/481450]\n",
      "loss: 0.374879  [432000/481450]\n",
      "loss: 0.289440  [435200/481450]\n",
      "loss: 0.115660  [438400/481450]\n",
      "loss: 0.159837  [441600/481450]\n",
      "loss: 0.164152  [444800/481450]\n",
      "loss: 0.221159  [448000/481450]\n",
      "loss: 0.098519  [451200/481450]\n",
      "loss: 0.084874  [454400/481450]\n",
      "loss: 0.209116  [457600/481450]\n",
      "loss: 0.238859  [460800/481450]\n",
      "loss: 0.229544  [464000/481450]\n",
      "loss: 0.176154  [467200/481450]\n",
      "loss: 0.131316  [470400/481450]\n",
      "loss: 0.156954  [473600/481450]\n",
      "loss: 0.063401  [476800/481450]\n",
      "loss: 0.224895  [480000/481450]\n",
      "Train Accuracy: 91.4055%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 1.156452, F1-score: 83.10% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.183480  [    0/481450]\n",
      "loss: 0.117106  [ 3200/481450]\n",
      "loss: 0.195601  [ 6400/481450]\n",
      "loss: 0.338440  [ 9600/481450]\n",
      "loss: 0.159438  [12800/481450]\n",
      "loss: 0.175130  [16000/481450]\n",
      "loss: 0.290016  [19200/481450]\n",
      "loss: 0.169874  [22400/481450]\n",
      "loss: 0.122698  [25600/481450]\n",
      "loss: 0.284402  [28800/481450]\n",
      "loss: 0.172939  [32000/481450]\n",
      "loss: 0.423108  [35200/481450]\n",
      "loss: 0.217557  [38400/481450]\n",
      "loss: 0.221809  [41600/481450]\n",
      "loss: 0.131528  [44800/481450]\n",
      "loss: 0.070443  [48000/481450]\n",
      "loss: 0.094115  [51200/481450]\n",
      "loss: 0.080134  [54400/481450]\n",
      "loss: 0.239543  [57600/481450]\n",
      "loss: 0.192700  [60800/481450]\n",
      "loss: 0.213405  [64000/481450]\n",
      "loss: 0.188338  [67200/481450]\n",
      "loss: 0.234439  [70400/481450]\n",
      "loss: 0.149610  [73600/481450]\n",
      "loss: 0.002537  [76800/481450]\n",
      "loss: 0.273139  [80000/481450]\n",
      "loss: 0.205643  [83200/481450]\n",
      "loss: 0.278229  [86400/481450]\n",
      "loss: 0.179102  [89600/481450]\n",
      "loss: 0.260004  [92800/481450]\n",
      "loss: 0.050357  [96000/481450]\n",
      "loss: 0.165408  [99200/481450]\n",
      "loss: 0.029833  [102400/481450]\n",
      "loss: 0.091036  [105600/481450]\n",
      "loss: 0.103310  [108800/481450]\n",
      "loss: 0.063353  [112000/481450]\n",
      "loss: 0.179959  [115200/481450]\n",
      "loss: 0.354271  [118400/481450]\n",
      "loss: 0.333843  [121600/481450]\n",
      "loss: 0.408852  [124800/481450]\n",
      "loss: 0.257604  [128000/481450]\n",
      "loss: 0.201774  [131200/481450]\n",
      "loss: 0.092935  [134400/481450]\n",
      "loss: 0.269037  [137600/481450]\n",
      "loss: 0.204024  [140800/481450]\n",
      "loss: 0.314378  [144000/481450]\n",
      "loss: 0.151438  [147200/481450]\n",
      "loss: 0.082943  [150400/481450]\n",
      "loss: 0.068685  [153600/481450]\n",
      "loss: 0.155094  [156800/481450]\n",
      "loss: 0.176585  [160000/481450]\n",
      "loss: 0.287619  [163200/481450]\n",
      "loss: 0.171221  [166400/481450]\n",
      "loss: 0.114965  [169600/481450]\n",
      "loss: 0.377101  [172800/481450]\n",
      "loss: 0.150792  [176000/481450]\n",
      "loss: 0.359064  [179200/481450]\n",
      "loss: 0.134885  [182400/481450]\n",
      "loss: 0.020533  [185600/481450]\n",
      "loss: 0.202966  [188800/481450]\n",
      "loss: 0.482479  [192000/481450]\n",
      "loss: 0.091523  [195200/481450]\n",
      "loss: 0.216519  [198400/481450]\n",
      "loss: 0.154766  [201600/481450]\n",
      "loss: 0.119552  [204800/481450]\n",
      "loss: 0.259111  [208000/481450]\n",
      "loss: 0.268263  [211200/481450]\n",
      "loss: 0.158910  [214400/481450]\n",
      "loss: 0.143963  [217600/481450]\n",
      "loss: 0.311791  [220800/481450]\n",
      "loss: 0.157586  [224000/481450]\n",
      "loss: 0.199978  [227200/481450]\n",
      "loss: 0.208937  [230400/481450]\n",
      "loss: 0.024825  [233600/481450]\n",
      "loss: 0.224372  [236800/481450]\n",
      "loss: 0.087662  [240000/481450]\n",
      "loss: 0.130555  [243200/481450]\n",
      "loss: 0.283472  [246400/481450]\n",
      "loss: 0.248568  [249600/481450]\n",
      "loss: 0.208711  [252800/481450]\n",
      "loss: 0.166651  [256000/481450]\n",
      "loss: 0.121720  [259200/481450]\n",
      "loss: 0.105715  [262400/481450]\n",
      "loss: 0.149407  [265600/481450]\n",
      "loss: 0.044673  [268800/481450]\n",
      "loss: 0.074389  [272000/481450]\n",
      "loss: 0.197936  [275200/481450]\n",
      "loss: 0.257077  [278400/481450]\n",
      "loss: 0.222830  [281600/481450]\n",
      "loss: 0.367355  [284800/481450]\n",
      "loss: 0.217795  [288000/481450]\n",
      "loss: 0.367393  [291200/481450]\n",
      "loss: 0.226893  [294400/481450]\n",
      "loss: 0.146314  [297600/481450]\n",
      "loss: 0.260630  [300800/481450]\n",
      "loss: 0.373226  [304000/481450]\n",
      "loss: 0.238916  [307200/481450]\n",
      "loss: 0.184166  [310400/481450]\n",
      "loss: 0.215612  [313600/481450]\n",
      "loss: 0.226926  [316800/481450]\n",
      "loss: 0.472352  [320000/481450]\n",
      "loss: 0.151649  [323200/481450]\n",
      "loss: 0.254623  [326400/481450]\n",
      "loss: 0.052618  [329600/481450]\n",
      "loss: 0.018483  [332800/481450]\n",
      "loss: 0.173776  [336000/481450]\n",
      "loss: 0.436932  [339200/481450]\n",
      "loss: 0.210272  [342400/481450]\n",
      "loss: 0.212664  [345600/481450]\n",
      "loss: 0.049560  [348800/481450]\n",
      "loss: 0.164947  [352000/481450]\n",
      "loss: 0.145613  [355200/481450]\n",
      "loss: 0.311389  [358400/481450]\n",
      "loss: 0.240494  [361600/481450]\n",
      "loss: 0.206520  [364800/481450]\n",
      "loss: 0.298098  [368000/481450]\n",
      "loss: 0.176553  [371200/481450]\n",
      "loss: 0.175124  [374400/481450]\n",
      "loss: 0.227455  [377600/481450]\n",
      "loss: 0.130071  [380800/481450]\n",
      "loss: 0.048697  [384000/481450]\n",
      "loss: 0.230986  [387200/481450]\n",
      "loss: 0.136306  [390400/481450]\n",
      "loss: 0.305255  [393600/481450]\n",
      "loss: 0.329259  [396800/481450]\n",
      "loss: 0.085653  [400000/481450]\n",
      "loss: 0.332103  [403200/481450]\n",
      "loss: 0.085449  [406400/481450]\n",
      "loss: 0.205061  [409600/481450]\n",
      "loss: 0.218267  [412800/481450]\n",
      "loss: 0.228704  [416000/481450]\n",
      "loss: 0.177418  [419200/481450]\n",
      "loss: 0.238085  [422400/481450]\n",
      "loss: 0.090739  [425600/481450]\n",
      "loss: 0.157295  [428800/481450]\n",
      "loss: 0.140794  [432000/481450]\n",
      "loss: 0.120240  [435200/481450]\n",
      "loss: 0.414110  [438400/481450]\n",
      "loss: 0.407094  [441600/481450]\n",
      "loss: 0.339643  [444800/481450]\n",
      "loss: 0.337002  [448000/481450]\n",
      "loss: 0.143059  [451200/481450]\n",
      "loss: 0.255416  [454400/481450]\n",
      "loss: 0.158242  [457600/481450]\n",
      "loss: 0.344914  [460800/481450]\n",
      "loss: 0.163374  [464000/481450]\n",
      "loss: 0.216074  [467200/481450]\n",
      "loss: 0.075646  [470400/481450]\n",
      "loss: 0.098956  [473600/481450]\n",
      "loss: 0.152167  [476800/481450]\n",
      "loss: 0.192642  [480000/481450]\n",
      "Train Accuracy: 91.4652%\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 1.177121, F1-score: 82.87% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.185950  [    0/481450]\n",
      "loss: 0.643832  [ 3200/481450]\n",
      "loss: 0.221147  [ 6400/481450]\n",
      "loss: 0.143138  [ 9600/481450]\n",
      "loss: 0.183490  [12800/481450]\n",
      "loss: 0.094099  [16000/481450]\n",
      "loss: 0.098726  [19200/481450]\n",
      "loss: 0.205396  [22400/481450]\n",
      "loss: 0.183877  [25600/481450]\n",
      "loss: 0.261532  [28800/481450]\n",
      "loss: 0.178288  [32000/481450]\n",
      "loss: 0.174488  [35200/481450]\n",
      "loss: 0.139427  [38400/481450]\n",
      "loss: 0.128828  [41600/481450]\n",
      "loss: 0.251325  [44800/481450]\n",
      "loss: 0.403387  [48000/481450]\n",
      "loss: 0.074371  [51200/481450]\n",
      "loss: 0.157245  [54400/481450]\n",
      "loss: 0.220761  [57600/481450]\n",
      "loss: 0.111631  [60800/481450]\n",
      "loss: 0.245610  [64000/481450]\n",
      "loss: 0.171206  [67200/481450]\n",
      "loss: 0.164267  [70400/481450]\n",
      "loss: 0.191391  [73600/481450]\n",
      "loss: 0.113181  [76800/481450]\n",
      "loss: 0.297408  [80000/481450]\n",
      "loss: 0.361824  [83200/481450]\n",
      "loss: 0.100618  [86400/481450]\n",
      "loss: 0.420354  [89600/481450]\n",
      "loss: 0.111757  [92800/481450]\n",
      "loss: 0.442908  [96000/481450]\n",
      "loss: 0.227395  [99200/481450]\n",
      "loss: 0.342185  [102400/481450]\n",
      "loss: 0.156871  [105600/481450]\n",
      "loss: 0.248933  [108800/481450]\n",
      "loss: 0.155753  [112000/481450]\n",
      "loss: 0.113789  [115200/481450]\n",
      "loss: 0.225732  [118400/481450]\n",
      "loss: 0.321053  [121600/481450]\n",
      "loss: 0.476247  [124800/481450]\n",
      "loss: 0.102681  [128000/481450]\n",
      "loss: 0.152752  [131200/481450]\n",
      "loss: 0.344839  [134400/481450]\n",
      "loss: 0.299558  [137600/481450]\n",
      "loss: 0.225510  [140800/481450]\n",
      "loss: 0.115849  [144000/481450]\n",
      "loss: 0.267413  [147200/481450]\n",
      "loss: 0.168620  [150400/481450]\n",
      "loss: 0.111289  [153600/481450]\n",
      "loss: 0.189344  [156800/481450]\n",
      "loss: 0.116290  [160000/481450]\n",
      "loss: 0.272569  [163200/481450]\n",
      "loss: 0.166018  [166400/481450]\n",
      "loss: 0.314585  [169600/481450]\n",
      "loss: 0.187843  [172800/481450]\n",
      "loss: 0.168470  [176000/481450]\n",
      "loss: 0.447847  [179200/481450]\n",
      "loss: 0.274649  [182400/481450]\n",
      "loss: 0.256293  [185600/481450]\n",
      "loss: 0.168772  [188800/481450]\n",
      "loss: 0.133551  [192000/481450]\n",
      "loss: 0.199357  [195200/481450]\n",
      "loss: 0.203471  [198400/481450]\n",
      "loss: 0.226194  [201600/481450]\n",
      "loss: 0.184776  [204800/481450]\n",
      "loss: 0.169569  [208000/481450]\n",
      "loss: 0.262106  [211200/481450]\n",
      "loss: 0.310640  [214400/481450]\n",
      "loss: 0.152829  [217600/481450]\n",
      "loss: 0.184438  [220800/481450]\n",
      "loss: 0.319132  [224000/481450]\n",
      "loss: 0.156168  [227200/481450]\n",
      "loss: 0.215502  [230400/481450]\n",
      "loss: 0.076860  [233600/481450]\n",
      "loss: 0.311432  [236800/481450]\n",
      "loss: 0.169632  [240000/481450]\n",
      "loss: 0.183018  [243200/481450]\n",
      "loss: 0.115874  [246400/481450]\n",
      "loss: 0.312398  [249600/481450]\n",
      "loss: 0.161333  [252800/481450]\n",
      "loss: 0.222779  [256000/481450]\n",
      "loss: 0.226125  [259200/481450]\n",
      "loss: 0.134124  [262400/481450]\n",
      "loss: 0.053800  [265600/481450]\n",
      "loss: 0.289819  [268800/481450]\n",
      "loss: 0.143230  [272000/481450]\n",
      "loss: 0.084356  [275200/481450]\n",
      "loss: 0.228531  [278400/481450]\n",
      "loss: 0.411697  [281600/481450]\n",
      "loss: 0.096475  [284800/481450]\n",
      "loss: 0.335543  [288000/481450]\n",
      "loss: 0.282051  [291200/481450]\n",
      "loss: 0.215000  [294400/481450]\n",
      "loss: 0.267975  [297600/481450]\n",
      "loss: 0.181461  [300800/481450]\n",
      "loss: 0.561476  [304000/481450]\n",
      "loss: 0.318610  [307200/481450]\n",
      "loss: 0.351187  [310400/481450]\n",
      "loss: 0.244081  [313600/481450]\n",
      "loss: 0.209911  [316800/481450]\n",
      "loss: 0.181162  [320000/481450]\n",
      "loss: 0.228971  [323200/481450]\n",
      "loss: 0.148075  [326400/481450]\n",
      "loss: 0.058823  [329600/481450]\n",
      "loss: 0.193414  [332800/481450]\n",
      "loss: 0.247396  [336000/481450]\n",
      "loss: 0.265847  [339200/481450]\n",
      "loss: 0.088939  [342400/481450]\n",
      "loss: 0.380471  [345600/481450]\n",
      "loss: 0.306402  [348800/481450]\n",
      "loss: 0.245707  [352000/481450]\n",
      "loss: 0.233764  [355200/481450]\n",
      "loss: 0.068188  [358400/481450]\n",
      "loss: 0.335652  [361600/481450]\n",
      "loss: 0.203406  [364800/481450]\n",
      "loss: 0.196112  [368000/481450]\n",
      "loss: 0.151712  [371200/481450]\n",
      "loss: 0.107083  [374400/481450]\n",
      "loss: 0.200319  [377600/481450]\n",
      "loss: 0.100386  [380800/481450]\n",
      "loss: 0.141312  [384000/481450]\n",
      "loss: 0.222341  [387200/481450]\n",
      "loss: 0.202136  [390400/481450]\n",
      "loss: 0.444749  [393600/481450]\n",
      "loss: 0.215169  [396800/481450]\n",
      "loss: 0.141284  [400000/481450]\n",
      "loss: 0.179483  [403200/481450]\n",
      "loss: 0.150095  [406400/481450]\n",
      "loss: 0.162014  [409600/481450]\n",
      "loss: 0.330423  [412800/481450]\n",
      "loss: 0.133058  [416000/481450]\n",
      "loss: 0.137938  [419200/481450]\n",
      "loss: 0.209157  [422400/481450]\n",
      "loss: 0.027087  [425600/481450]\n",
      "loss: 0.310149  [428800/481450]\n",
      "loss: 0.243380  [432000/481450]\n",
      "loss: 0.269668  [435200/481450]\n",
      "loss: 0.341458  [438400/481450]\n",
      "loss: 0.184045  [441600/481450]\n",
      "loss: 0.144366  [444800/481450]\n",
      "loss: 0.060858  [448000/481450]\n",
      "loss: 0.322405  [451200/481450]\n",
      "loss: 0.121508  [454400/481450]\n",
      "loss: 0.397389  [457600/481450]\n",
      "loss: 0.135738  [460800/481450]\n",
      "loss: 0.222513  [464000/481450]\n",
      "loss: 0.187459  [467200/481450]\n",
      "loss: 0.513684  [470400/481450]\n",
      "loss: 0.271021  [473600/481450]\n",
      "loss: 0.203274  [476800/481450]\n",
      "loss: 0.205707  [480000/481450]\n",
      "Train Accuracy: 91.5273%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 1.075463, F1-score: 83.23% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303873  [    0/481450]\n",
      "loss: 2.302816  [ 3200/481450]\n",
      "loss: 2.297785  [ 6400/481450]\n",
      "loss: 2.296031  [ 9600/481450]\n",
      "loss: 2.288023  [12800/481450]\n",
      "loss: 2.277872  [16000/481450]\n",
      "loss: 2.235938  [19200/481450]\n",
      "loss: 2.178953  [22400/481450]\n",
      "loss: 2.070969  [25600/481450]\n",
      "loss: 2.067049  [28800/481450]\n",
      "loss: 2.037387  [32000/481450]\n",
      "loss: 1.985152  [35200/481450]\n",
      "loss: 1.862195  [38400/481450]\n",
      "loss: 1.924645  [41600/481450]\n",
      "loss: 1.855682  [44800/481450]\n",
      "loss: 1.714819  [48000/481450]\n",
      "loss: 2.010747  [51200/481450]\n",
      "loss: 2.028619  [54400/481450]\n",
      "loss: 1.706535  [57600/481450]\n",
      "loss: 1.761786  [60800/481450]\n",
      "loss: 1.650376  [64000/481450]\n",
      "loss: 1.897968  [67200/481450]\n",
      "loss: 1.528954  [70400/481450]\n",
      "loss: 1.595967  [73600/481450]\n",
      "loss: 1.435996  [76800/481450]\n",
      "loss: 1.479889  [80000/481450]\n",
      "loss: 1.480312  [83200/481450]\n",
      "loss: 1.585973  [86400/481450]\n",
      "loss: 1.508904  [89600/481450]\n",
      "loss: 1.325055  [92800/481450]\n",
      "loss: 1.312539  [96000/481450]\n",
      "loss: 1.546192  [99200/481450]\n",
      "loss: 1.231118  [102400/481450]\n",
      "loss: 1.474196  [105600/481450]\n",
      "loss: 1.494893  [108800/481450]\n",
      "loss: 1.316959  [112000/481450]\n",
      "loss: 1.681700  [115200/481450]\n",
      "loss: 1.529127  [118400/481450]\n",
      "loss: 1.268122  [121600/481450]\n",
      "loss: 1.506580  [124800/481450]\n",
      "loss: 1.656167  [128000/481450]\n",
      "loss: 1.308796  [131200/481450]\n",
      "loss: 1.398517  [134400/481450]\n",
      "loss: 1.100171  [137600/481450]\n",
      "loss: 1.232177  [140800/481450]\n",
      "loss: 1.238866  [144000/481450]\n",
      "loss: 1.457727  [147200/481450]\n",
      "loss: 1.261930  [150400/481450]\n",
      "loss: 1.221967  [153600/481450]\n",
      "loss: 1.230982  [156800/481450]\n",
      "loss: 1.278268  [160000/481450]\n",
      "loss: 1.132662  [163200/481450]\n",
      "loss: 1.073883  [166400/481450]\n",
      "loss: 1.171490  [169600/481450]\n",
      "loss: 1.105073  [172800/481450]\n",
      "loss: 1.059875  [176000/481450]\n",
      "loss: 1.145698  [179200/481450]\n",
      "loss: 1.036083  [182400/481450]\n",
      "loss: 1.110263  [185600/481450]\n",
      "loss: 1.335220  [188800/481450]\n",
      "loss: 1.021522  [192000/481450]\n",
      "loss: 1.016870  [195200/481450]\n",
      "loss: 1.008809  [198400/481450]\n",
      "loss: 1.150310  [201600/481450]\n",
      "loss: 1.143446  [204800/481450]\n",
      "loss: 1.131747  [208000/481450]\n",
      "loss: 1.188166  [211200/481450]\n",
      "loss: 1.147888  [214400/481450]\n",
      "loss: 0.800938  [217600/481450]\n",
      "loss: 1.025549  [220800/481450]\n",
      "loss: 0.814390  [224000/481450]\n",
      "loss: 1.295744  [227200/481450]\n",
      "loss: 0.929968  [230400/481450]\n",
      "loss: 1.140617  [233600/481450]\n",
      "loss: 0.779464  [236800/481450]\n",
      "loss: 0.847682  [240000/481450]\n",
      "loss: 0.903409  [243200/481450]\n",
      "loss: 0.573645  [246400/481450]\n",
      "loss: 1.029950  [249600/481450]\n",
      "loss: 0.762832  [252800/481450]\n",
      "loss: 1.033998  [256000/481450]\n",
      "loss: 0.933532  [259200/481450]\n",
      "loss: 0.856361  [262400/481450]\n",
      "loss: 0.628264  [265600/481450]\n",
      "loss: 0.747080  [268800/481450]\n",
      "loss: 0.887821  [272000/481450]\n",
      "loss: 0.649923  [275200/481450]\n",
      "loss: 0.793809  [278400/481450]\n",
      "loss: 0.866693  [281600/481450]\n",
      "loss: 1.026863  [284800/481450]\n",
      "loss: 0.807795  [288000/481450]\n",
      "loss: 1.123703  [291200/481450]\n",
      "loss: 0.950396  [294400/481450]\n",
      "loss: 0.741085  [297600/481450]\n",
      "loss: 0.646328  [300800/481450]\n",
      "loss: 0.784540  [304000/481450]\n",
      "loss: 0.541354  [307200/481450]\n",
      "loss: 0.612877  [310400/481450]\n",
      "loss: 0.565532  [313600/481450]\n",
      "loss: 0.617424  [316800/481450]\n",
      "loss: 0.787419  [320000/481450]\n",
      "loss: 0.870635  [323200/481450]\n",
      "loss: 0.830984  [326400/481450]\n",
      "loss: 0.681614  [329600/481450]\n",
      "loss: 0.819576  [332800/481450]\n",
      "loss: 0.603740  [336000/481450]\n",
      "loss: 0.707299  [339200/481450]\n",
      "loss: 0.743055  [342400/481450]\n",
      "loss: 0.496641  [345600/481450]\n",
      "loss: 0.693308  [348800/481450]\n",
      "loss: 0.721531  [352000/481450]\n",
      "loss: 0.748250  [355200/481450]\n",
      "loss: 0.760275  [358400/481450]\n",
      "loss: 0.885153  [361600/481450]\n",
      "loss: 0.698536  [364800/481450]\n",
      "loss: 0.876220  [368000/481450]\n",
      "loss: 0.858084  [371200/481450]\n",
      "loss: 0.808324  [374400/481450]\n",
      "loss: 0.904600  [377600/481450]\n",
      "loss: 0.775363  [380800/481450]\n",
      "loss: 0.883015  [384000/481450]\n",
      "loss: 0.774155  [387200/481450]\n",
      "loss: 0.617692  [390400/481450]\n",
      "loss: 0.812352  [393600/481450]\n",
      "loss: 0.673213  [396800/481450]\n",
      "loss: 0.502179  [400000/481450]\n",
      "loss: 0.866659  [403200/481450]\n",
      "loss: 0.557522  [406400/481450]\n",
      "loss: 0.773617  [409600/481450]\n",
      "loss: 0.447325  [412800/481450]\n",
      "loss: 0.627371  [416000/481450]\n",
      "loss: 0.613677  [419200/481450]\n",
      "loss: 0.651694  [422400/481450]\n",
      "loss: 0.775910  [425600/481450]\n",
      "loss: 0.704129  [428800/481450]\n",
      "loss: 0.584952  [432000/481450]\n",
      "loss: 0.811646  [435200/481450]\n",
      "loss: 0.604666  [438400/481450]\n",
      "loss: 0.665494  [441600/481450]\n",
      "loss: 0.692758  [444800/481450]\n",
      "loss: 0.663030  [448000/481450]\n",
      "loss: 0.675393  [451200/481450]\n",
      "loss: 0.521713  [454400/481450]\n",
      "loss: 0.919646  [457600/481450]\n",
      "loss: 0.462332  [460800/481450]\n",
      "loss: 0.669725  [464000/481450]\n",
      "loss: 0.462278  [467200/481450]\n",
      "loss: 0.510200  [470400/481450]\n",
      "loss: 0.603693  [473600/481450]\n",
      "loss: 0.602505  [476800/481450]\n",
      "loss: 0.633448  [480000/481450]\n",
      "Train Accuracy: 62.1811%\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.617605, F1-score: 82.11% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.564160  [    0/481450]\n",
      "loss: 0.501400  [ 3200/481450]\n",
      "loss: 0.543068  [ 6400/481450]\n",
      "loss: 0.620027  [ 9600/481450]\n",
      "loss: 0.698540  [12800/481450]\n",
      "loss: 0.714395  [16000/481450]\n",
      "loss: 0.762351  [19200/481450]\n",
      "loss: 0.770652  [22400/481450]\n",
      "loss: 0.686026  [25600/481450]\n",
      "loss: 0.585501  [28800/481450]\n",
      "loss: 0.582488  [32000/481450]\n",
      "loss: 0.546628  [35200/481450]\n",
      "loss: 0.473546  [38400/481450]\n",
      "loss: 0.642924  [41600/481450]\n",
      "loss: 0.662224  [44800/481450]\n",
      "loss: 0.523117  [48000/481450]\n",
      "loss: 0.543241  [51200/481450]\n",
      "loss: 0.692559  [54400/481450]\n",
      "loss: 0.590855  [57600/481450]\n",
      "loss: 0.717613  [60800/481450]\n",
      "loss: 0.558585  [64000/481450]\n",
      "loss: 0.403257  [67200/481450]\n",
      "loss: 0.484515  [70400/481450]\n",
      "loss: 0.623852  [73600/481450]\n",
      "loss: 0.473937  [76800/481450]\n",
      "loss: 0.498273  [80000/481450]\n",
      "loss: 0.477172  [83200/481450]\n",
      "loss: 0.644184  [86400/481450]\n",
      "loss: 0.858913  [89600/481450]\n",
      "loss: 0.441991  [92800/481450]\n",
      "loss: 0.474304  [96000/481450]\n",
      "loss: 0.552009  [99200/481450]\n",
      "loss: 0.531494  [102400/481450]\n",
      "loss: 0.294158  [105600/481450]\n",
      "loss: 0.587179  [108800/481450]\n",
      "loss: 0.492244  [112000/481450]\n",
      "loss: 0.524260  [115200/481450]\n",
      "loss: 0.571141  [118400/481450]\n",
      "loss: 0.471821  [121600/481450]\n",
      "loss: 0.663453  [124800/481450]\n",
      "loss: 0.344800  [128000/481450]\n",
      "loss: 0.558365  [131200/481450]\n",
      "loss: 0.520408  [134400/481450]\n",
      "loss: 0.370737  [137600/481450]\n",
      "loss: 0.654038  [140800/481450]\n",
      "loss: 0.225805  [144000/481450]\n",
      "loss: 0.370672  [147200/481450]\n",
      "loss: 0.579952  [150400/481450]\n",
      "loss: 0.452032  [153600/481450]\n",
      "loss: 0.580801  [156800/481450]\n",
      "loss: 0.421348  [160000/481450]\n",
      "loss: 0.531510  [163200/481450]\n",
      "loss: 0.336323  [166400/481450]\n",
      "loss: 0.417078  [169600/481450]\n",
      "loss: 0.439486  [172800/481450]\n",
      "loss: 0.587727  [176000/481450]\n",
      "loss: 0.377145  [179200/481450]\n",
      "loss: 0.482465  [182400/481450]\n",
      "loss: 0.540299  [185600/481450]\n",
      "loss: 0.504759  [188800/481450]\n",
      "loss: 0.939687  [192000/481450]\n",
      "loss: 0.559904  [195200/481450]\n",
      "loss: 0.559324  [198400/481450]\n",
      "loss: 0.390650  [201600/481450]\n",
      "loss: 0.319613  [204800/481450]\n",
      "loss: 0.631125  [208000/481450]\n",
      "loss: 0.379922  [211200/481450]\n",
      "loss: 0.470432  [214400/481450]\n",
      "loss: 0.697380  [217600/481450]\n",
      "loss: 0.483873  [220800/481450]\n",
      "loss: 0.570428  [224000/481450]\n",
      "loss: 0.592842  [227200/481450]\n",
      "loss: 0.565133  [230400/481450]\n",
      "loss: 0.581268  [233600/481450]\n",
      "loss: 0.156740  [236800/481450]\n",
      "loss: 0.362389  [240000/481450]\n",
      "loss: 0.357143  [243200/481450]\n",
      "loss: 0.393589  [246400/481450]\n",
      "loss: 0.357797  [249600/481450]\n",
      "loss: 0.356319  [252800/481450]\n",
      "loss: 0.244575  [256000/481450]\n",
      "loss: 0.485555  [259200/481450]\n",
      "loss: 0.455749  [262400/481450]\n",
      "loss: 0.457227  [265600/481450]\n",
      "loss: 0.510907  [268800/481450]\n",
      "loss: 0.229796  [272000/481450]\n",
      "loss: 0.489738  [275200/481450]\n",
      "loss: 0.303385  [278400/481450]\n",
      "loss: 0.392328  [281600/481450]\n",
      "loss: 0.248430  [284800/481450]\n",
      "loss: 0.467333  [288000/481450]\n",
      "loss: 0.451846  [291200/481450]\n",
      "loss: 0.576203  [294400/481450]\n",
      "loss: 0.394502  [297600/481450]\n",
      "loss: 0.769004  [300800/481450]\n",
      "loss: 0.369792  [304000/481450]\n",
      "loss: 0.746180  [307200/481450]\n",
      "loss: 0.335058  [310400/481450]\n",
      "loss: 0.449125  [313600/481450]\n",
      "loss: 0.815511  [316800/481450]\n",
      "loss: 0.551572  [320000/481450]\n",
      "loss: 0.429078  [323200/481450]\n",
      "loss: 0.736431  [326400/481450]\n",
      "loss: 0.655041  [329600/481450]\n",
      "loss: 0.373423  [332800/481450]\n",
      "loss: 0.451881  [336000/481450]\n",
      "loss: 0.397157  [339200/481450]\n",
      "loss: 0.660216  [342400/481450]\n",
      "loss: 0.684946  [345600/481450]\n",
      "loss: 0.512235  [348800/481450]\n",
      "loss: 0.567810  [352000/481450]\n",
      "loss: 0.331990  [355200/481450]\n",
      "loss: 0.448139  [358400/481450]\n",
      "loss: 0.420008  [361600/481450]\n",
      "loss: 0.514960  [364800/481450]\n",
      "loss: 0.441924  [368000/481450]\n",
      "loss: 0.464416  [371200/481450]\n",
      "loss: 0.427173  [374400/481450]\n",
      "loss: 0.623438  [377600/481450]\n",
      "loss: 0.435355  [380800/481450]\n",
      "loss: 0.484409  [384000/481450]\n",
      "loss: 0.569751  [387200/481450]\n",
      "loss: 0.320499  [390400/481450]\n",
      "loss: 0.434205  [393600/481450]\n",
      "loss: 0.728910  [396800/481450]\n",
      "loss: 0.744317  [400000/481450]\n",
      "loss: 0.367702  [403200/481450]\n",
      "loss: 0.476635  [406400/481450]\n",
      "loss: 0.426254  [409600/481450]\n",
      "loss: 0.484233  [412800/481450]\n",
      "loss: 0.488971  [416000/481450]\n",
      "loss: 0.365012  [419200/481450]\n",
      "loss: 0.657597  [422400/481450]\n",
      "loss: 0.596684  [425600/481450]\n",
      "loss: 0.397815  [428800/481450]\n",
      "loss: 0.605564  [432000/481450]\n",
      "loss: 0.267354  [435200/481450]\n",
      "loss: 0.412495  [438400/481450]\n",
      "loss: 0.700551  [441600/481450]\n",
      "loss: 0.489357  [444800/481450]\n",
      "loss: 0.309933  [448000/481450]\n",
      "loss: 0.428647  [451200/481450]\n",
      "loss: 0.367225  [454400/481450]\n",
      "loss: 0.482403  [457600/481450]\n",
      "loss: 0.373690  [460800/481450]\n",
      "loss: 0.343778  [464000/481450]\n",
      "loss: 0.651008  [467200/481450]\n",
      "loss: 0.670919  [470400/481450]\n",
      "loss: 0.468633  [473600/481450]\n",
      "loss: 0.459203  [476800/481450]\n",
      "loss: 0.657335  [480000/481450]\n",
      "Train Accuracy: 80.9648%\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.552664, F1-score: 82.36% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.405726  [    0/481450]\n",
      "loss: 0.325214  [ 3200/481450]\n",
      "loss: 0.503985  [ 6400/481450]\n",
      "loss: 0.497425  [ 9600/481450]\n",
      "loss: 0.228459  [12800/481450]\n",
      "loss: 0.659756  [16000/481450]\n",
      "loss: 0.289993  [19200/481450]\n",
      "loss: 0.407252  [22400/481450]\n",
      "loss: 0.278160  [25600/481450]\n",
      "loss: 0.332673  [28800/481450]\n",
      "loss: 0.396970  [32000/481450]\n",
      "loss: 0.312453  [35200/481450]\n",
      "loss: 0.379049  [38400/481450]\n",
      "loss: 0.423386  [41600/481450]\n",
      "loss: 0.287655  [44800/481450]\n",
      "loss: 0.642869  [48000/481450]\n",
      "loss: 0.794826  [51200/481450]\n",
      "loss: 0.284913  [54400/481450]\n",
      "loss: 0.453777  [57600/481450]\n",
      "loss: 0.483607  [60800/481450]\n",
      "loss: 0.325212  [64000/481450]\n",
      "loss: 0.486196  [67200/481450]\n",
      "loss: 0.420256  [70400/481450]\n",
      "loss: 0.287554  [73600/481450]\n",
      "loss: 0.458415  [76800/481450]\n",
      "loss: 0.445502  [80000/481450]\n",
      "loss: 0.416336  [83200/481450]\n",
      "loss: 0.353295  [86400/481450]\n",
      "loss: 0.318656  [89600/481450]\n",
      "loss: 0.659878  [92800/481450]\n",
      "loss: 0.290639  [96000/481450]\n",
      "loss: 0.458490  [99200/481450]\n",
      "loss: 0.453602  [102400/481450]\n",
      "loss: 0.271794  [105600/481450]\n",
      "loss: 0.335813  [108800/481450]\n",
      "loss: 0.173087  [112000/481450]\n",
      "loss: 0.414376  [115200/481450]\n",
      "loss: 0.340914  [118400/481450]\n",
      "loss: 0.398083  [121600/481450]\n",
      "loss: 0.486441  [124800/481450]\n",
      "loss: 0.262067  [128000/481450]\n",
      "loss: 0.413472  [131200/481450]\n",
      "loss: 0.318532  [134400/481450]\n",
      "loss: 0.791157  [137600/481450]\n",
      "loss: 0.468370  [140800/481450]\n",
      "loss: 0.374505  [144000/481450]\n",
      "loss: 0.422454  [147200/481450]\n",
      "loss: 0.491600  [150400/481450]\n",
      "loss: 0.380875  [153600/481450]\n",
      "loss: 0.460730  [156800/481450]\n",
      "loss: 0.405906  [160000/481450]\n",
      "loss: 0.368868  [163200/481450]\n",
      "loss: 0.503606  [166400/481450]\n",
      "loss: 0.432976  [169600/481450]\n",
      "loss: 0.537621  [172800/481450]\n",
      "loss: 0.554145  [176000/481450]\n",
      "loss: 0.467484  [179200/481450]\n",
      "loss: 0.434592  [182400/481450]\n",
      "loss: 0.282261  [185600/481450]\n",
      "loss: 0.173953  [188800/481450]\n",
      "loss: 0.575742  [192000/481450]\n",
      "loss: 0.247651  [195200/481450]\n",
      "loss: 0.369191  [198400/481450]\n",
      "loss: 0.385346  [201600/481450]\n",
      "loss: 0.428571  [204800/481450]\n",
      "loss: 0.376220  [208000/481450]\n",
      "loss: 0.415404  [211200/481450]\n",
      "loss: 0.467076  [214400/481450]\n",
      "loss: 0.576974  [217600/481450]\n",
      "loss: 0.307704  [220800/481450]\n",
      "loss: 0.375133  [224000/481450]\n",
      "loss: 0.421875  [227200/481450]\n",
      "loss: 0.397084  [230400/481450]\n",
      "loss: 0.259349  [233600/481450]\n",
      "loss: 0.334620  [236800/481450]\n",
      "loss: 0.430742  [240000/481450]\n",
      "loss: 0.287184  [243200/481450]\n",
      "loss: 0.555221  [246400/481450]\n",
      "loss: 0.167711  [249600/481450]\n",
      "loss: 0.501434  [252800/481450]\n",
      "loss: 0.464254  [256000/481450]\n",
      "loss: 0.330087  [259200/481450]\n",
      "loss: 0.404158  [262400/481450]\n",
      "loss: 0.391532  [265600/481450]\n",
      "loss: 0.325307  [268800/481450]\n",
      "loss: 0.657714  [272000/481450]\n",
      "loss: 0.423447  [275200/481450]\n",
      "loss: 0.281330  [278400/481450]\n",
      "loss: 0.555872  [281600/481450]\n",
      "loss: 0.242222  [284800/481450]\n",
      "loss: 0.358640  [288000/481450]\n",
      "loss: 0.322149  [291200/481450]\n",
      "loss: 0.313594  [294400/481450]\n",
      "loss: 0.585517  [297600/481450]\n",
      "loss: 0.274007  [300800/481450]\n",
      "loss: 0.504407  [304000/481450]\n",
      "loss: 0.641743  [307200/481450]\n",
      "loss: 0.299442  [310400/481450]\n",
      "loss: 0.426545  [313600/481450]\n",
      "loss: 0.490388  [316800/481450]\n",
      "loss: 0.865823  [320000/481450]\n",
      "loss: 0.234797  [323200/481450]\n",
      "loss: 0.353660  [326400/481450]\n",
      "loss: 0.279889  [329600/481450]\n",
      "loss: 0.666394  [332800/481450]\n",
      "loss: 0.477233  [336000/481450]\n",
      "loss: 0.465080  [339200/481450]\n",
      "loss: 0.505927  [342400/481450]\n",
      "loss: 0.408563  [345600/481450]\n",
      "loss: 0.452241  [348800/481450]\n",
      "loss: 0.387817  [352000/481450]\n",
      "loss: 0.259747  [355200/481450]\n",
      "loss: 0.252581  [358400/481450]\n",
      "loss: 0.652287  [361600/481450]\n",
      "loss: 0.427796  [364800/481450]\n",
      "loss: 0.537170  [368000/481450]\n",
      "loss: 0.411418  [371200/481450]\n",
      "loss: 0.416742  [374400/481450]\n",
      "loss: 0.438060  [377600/481450]\n",
      "loss: 0.238618  [380800/481450]\n",
      "loss: 0.327507  [384000/481450]\n",
      "loss: 0.600937  [387200/481450]\n",
      "loss: 0.378873  [390400/481450]\n",
      "loss: 0.203687  [393600/481450]\n",
      "loss: 0.270985  [396800/481450]\n",
      "loss: 0.350158  [400000/481450]\n",
      "loss: 0.795691  [403200/481450]\n",
      "loss: 0.449272  [406400/481450]\n",
      "loss: 0.396139  [409600/481450]\n",
      "loss: 0.544792  [412800/481450]\n",
      "loss: 0.268424  [416000/481450]\n",
      "loss: 0.269682  [419200/481450]\n",
      "loss: 0.318136  [422400/481450]\n",
      "loss: 0.288944  [425600/481450]\n",
      "loss: 0.257570  [428800/481450]\n",
      "loss: 0.346423  [432000/481450]\n",
      "loss: 0.436898  [435200/481450]\n",
      "loss: 0.429152  [438400/481450]\n",
      "loss: 0.371790  [441600/481450]\n",
      "loss: 0.498210  [444800/481450]\n",
      "loss: 0.412438  [448000/481450]\n",
      "loss: 0.381998  [451200/481450]\n",
      "loss: 0.450167  [454400/481450]\n",
      "loss: 0.498056  [457600/481450]\n",
      "loss: 0.477489  [460800/481450]\n",
      "loss: 0.355349  [464000/481450]\n",
      "loss: 0.326290  [467200/481450]\n",
      "loss: 0.398600  [470400/481450]\n",
      "loss: 0.465258  [473600/481450]\n",
      "loss: 0.729446  [476800/481450]\n",
      "loss: 0.526953  [480000/481450]\n",
      "Train Accuracy: 83.4625%\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.630287, F1-score: 80.96% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.375080  [    0/481450]\n",
      "loss: 0.446128  [ 3200/481450]\n",
      "loss: 0.421201  [ 6400/481450]\n",
      "loss: 0.390591  [ 9600/481450]\n",
      "loss: 0.221623  [12800/481450]\n",
      "loss: 0.447497  [16000/481450]\n",
      "loss: 0.614379  [19200/481450]\n",
      "loss: 0.385302  [22400/481450]\n",
      "loss: 0.355858  [25600/481450]\n",
      "loss: 0.431601  [28800/481450]\n",
      "loss: 0.446059  [32000/481450]\n",
      "loss: 0.456558  [35200/481450]\n",
      "loss: 0.399099  [38400/481450]\n",
      "loss: 0.433197  [41600/481450]\n",
      "loss: 0.269663  [44800/481450]\n",
      "loss: 0.422387  [48000/481450]\n",
      "loss: 0.333181  [51200/481450]\n",
      "loss: 0.498198  [54400/481450]\n",
      "loss: 0.340039  [57600/481450]\n",
      "loss: 0.385041  [60800/481450]\n",
      "loss: 0.484063  [64000/481450]\n",
      "loss: 0.486310  [67200/481450]\n",
      "loss: 0.139373  [70400/481450]\n",
      "loss: 0.252222  [73600/481450]\n",
      "loss: 0.332428  [76800/481450]\n",
      "loss: 0.295929  [80000/481450]\n",
      "loss: 0.179142  [83200/481450]\n",
      "loss: 0.307429  [86400/481450]\n",
      "loss: 0.640207  [89600/481450]\n",
      "loss: 0.428035  [92800/481450]\n",
      "loss: 0.254925  [96000/481450]\n",
      "loss: 0.492751  [99200/481450]\n",
      "loss: 0.477515  [102400/481450]\n",
      "loss: 0.614776  [105600/481450]\n",
      "loss: 0.387638  [108800/481450]\n",
      "loss: 0.434519  [112000/481450]\n",
      "loss: 0.574795  [115200/481450]\n",
      "loss: 0.340494  [118400/481450]\n",
      "loss: 0.248567  [121600/481450]\n",
      "loss: 0.467805  [124800/481450]\n",
      "loss: 0.332177  [128000/481450]\n",
      "loss: 0.321855  [131200/481450]\n",
      "loss: 0.365120  [134400/481450]\n",
      "loss: 0.269082  [137600/481450]\n",
      "loss: 0.446234  [140800/481450]\n",
      "loss: 0.448630  [144000/481450]\n",
      "loss: 0.385322  [147200/481450]\n",
      "loss: 0.355608  [150400/481450]\n",
      "loss: 0.231873  [153600/481450]\n",
      "loss: 0.486915  [156800/481450]\n",
      "loss: 0.458352  [160000/481450]\n",
      "loss: 0.586897  [163200/481450]\n",
      "loss: 0.518636  [166400/481450]\n",
      "loss: 0.322561  [169600/481450]\n",
      "loss: 0.203665  [172800/481450]\n",
      "loss: 0.388589  [176000/481450]\n",
      "loss: 0.299579  [179200/481450]\n",
      "loss: 0.651444  [182400/481450]\n",
      "loss: 0.278981  [185600/481450]\n",
      "loss: 0.431724  [188800/481450]\n",
      "loss: 0.456179  [192000/481450]\n",
      "loss: 0.557451  [195200/481450]\n",
      "loss: 0.357372  [198400/481450]\n",
      "loss: 0.453964  [201600/481450]\n",
      "loss: 0.510915  [204800/481450]\n",
      "loss: 0.412583  [208000/481450]\n",
      "loss: 0.262410  [211200/481450]\n",
      "loss: 0.327734  [214400/481450]\n",
      "loss: 0.481631  [217600/481450]\n",
      "loss: 0.411182  [220800/481450]\n",
      "loss: 0.380644  [224000/481450]\n",
      "loss: 0.590544  [227200/481450]\n",
      "loss: 0.359393  [230400/481450]\n",
      "loss: 0.359238  [233600/481450]\n",
      "loss: 0.431547  [236800/481450]\n",
      "loss: 0.279384  [240000/481450]\n",
      "loss: 0.401043  [243200/481450]\n",
      "loss: 0.365726  [246400/481450]\n",
      "loss: 0.272352  [249600/481450]\n",
      "loss: 0.274613  [252800/481450]\n",
      "loss: 0.515284  [256000/481450]\n",
      "loss: 0.709265  [259200/481450]\n",
      "loss: 0.438873  [262400/481450]\n",
      "loss: 0.397332  [265600/481450]\n",
      "loss: 0.662855  [268800/481450]\n",
      "loss: 0.360598  [272000/481450]\n",
      "loss: 0.423967  [275200/481450]\n",
      "loss: 0.198934  [278400/481450]\n",
      "loss: 0.197805  [281600/481450]\n",
      "loss: 0.186831  [284800/481450]\n",
      "loss: 0.285786  [288000/481450]\n",
      "loss: 0.649011  [291200/481450]\n",
      "loss: 0.408582  [294400/481450]\n",
      "loss: 0.196713  [297600/481450]\n",
      "loss: 0.591960  [300800/481450]\n",
      "loss: 0.340921  [304000/481450]\n",
      "loss: 0.629707  [307200/481450]\n",
      "loss: 0.281518  [310400/481450]\n",
      "loss: 0.381229  [313600/481450]\n",
      "loss: 0.281924  [316800/481450]\n",
      "loss: 0.383906  [320000/481450]\n",
      "loss: 0.483146  [323200/481450]\n",
      "loss: 0.591276  [326400/481450]\n",
      "loss: 0.575243  [329600/481450]\n",
      "loss: 0.514739  [332800/481450]\n",
      "loss: 0.363320  [336000/481450]\n",
      "loss: 0.349838  [339200/481450]\n",
      "loss: 0.255401  [342400/481450]\n",
      "loss: 0.684211  [345600/481450]\n",
      "loss: 0.298538  [348800/481450]\n",
      "loss: 0.181315  [352000/481450]\n",
      "loss: 0.267444  [355200/481450]\n",
      "loss: 0.317409  [358400/481450]\n",
      "loss: 0.339766  [361600/481450]\n",
      "loss: 0.263118  [364800/481450]\n",
      "loss: 0.318326  [368000/481450]\n",
      "loss: 0.338847  [371200/481450]\n",
      "loss: 0.270707  [374400/481450]\n",
      "loss: 0.405199  [377600/481450]\n",
      "loss: 0.409997  [380800/481450]\n",
      "loss: 0.517773  [384000/481450]\n",
      "loss: 0.251037  [387200/481450]\n",
      "loss: 0.484033  [390400/481450]\n",
      "loss: 0.327673  [393600/481450]\n",
      "loss: 0.199566  [396800/481450]\n",
      "loss: 0.297180  [400000/481450]\n",
      "loss: 0.357831  [403200/481450]\n",
      "loss: 0.254409  [406400/481450]\n",
      "loss: 0.717402  [409600/481450]\n",
      "loss: 0.457120  [412800/481450]\n",
      "loss: 0.306176  [416000/481450]\n",
      "loss: 0.212321  [419200/481450]\n",
      "loss: 0.524770  [422400/481450]\n",
      "loss: 0.697487  [425600/481450]\n",
      "loss: 0.451591  [428800/481450]\n",
      "loss: 0.634668  [432000/481450]\n",
      "loss: 0.390448  [435200/481450]\n",
      "loss: 0.455640  [438400/481450]\n",
      "loss: 0.464531  [441600/481450]\n",
      "loss: 0.411215  [444800/481450]\n",
      "loss: 0.446906  [448000/481450]\n",
      "loss: 0.374529  [451200/481450]\n",
      "loss: 0.086278  [454400/481450]\n",
      "loss: 0.340402  [457600/481450]\n",
      "loss: 0.447986  [460800/481450]\n",
      "loss: 0.419011  [464000/481450]\n",
      "loss: 0.401048  [467200/481450]\n",
      "loss: 0.532877  [470400/481450]\n",
      "loss: 0.547216  [473600/481450]\n",
      "loss: 0.527165  [476800/481450]\n",
      "loss: 0.346752  [480000/481450]\n",
      "Train Accuracy: 84.5124%\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.603063, F1-score: 81.57% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.220983  [    0/481450]\n",
      "loss: 0.329930  [ 3200/481450]\n",
      "loss: 0.208471  [ 6400/481450]\n",
      "loss: 0.495156  [ 9600/481450]\n",
      "loss: 0.333022  [12800/481450]\n",
      "loss: 0.295482  [16000/481450]\n",
      "loss: 0.409786  [19200/481450]\n",
      "loss: 0.535087  [22400/481450]\n",
      "loss: 0.624230  [25600/481450]\n",
      "loss: 0.256426  [28800/481450]\n",
      "loss: 0.656212  [32000/481450]\n",
      "loss: 0.372179  [35200/481450]\n",
      "loss: 0.236710  [38400/481450]\n",
      "loss: 0.164927  [41600/481450]\n",
      "loss: 0.491815  [44800/481450]\n",
      "loss: 0.205165  [48000/481450]\n",
      "loss: 0.541486  [51200/481450]\n",
      "loss: 0.283875  [54400/481450]\n",
      "loss: 0.383279  [57600/481450]\n",
      "loss: 0.305283  [60800/481450]\n",
      "loss: 0.647001  [64000/481450]\n",
      "loss: 0.371707  [67200/481450]\n",
      "loss: 0.383337  [70400/481450]\n",
      "loss: 0.429075  [73600/481450]\n",
      "loss: 0.462073  [76800/481450]\n",
      "loss: 0.417481  [80000/481450]\n",
      "loss: 0.221391  [83200/481450]\n",
      "loss: 0.289954  [86400/481450]\n",
      "loss: 0.350431  [89600/481450]\n",
      "loss: 0.329865  [92800/481450]\n",
      "loss: 0.495991  [96000/481450]\n",
      "loss: 0.329192  [99200/481450]\n",
      "loss: 0.389125  [102400/481450]\n",
      "loss: 0.252600  [105600/481450]\n",
      "loss: 0.372835  [108800/481450]\n",
      "loss: 0.504367  [112000/481450]\n",
      "loss: 0.406078  [115200/481450]\n",
      "loss: 0.252213  [118400/481450]\n",
      "loss: 0.470661  [121600/481450]\n",
      "loss: 0.615862  [124800/481450]\n",
      "loss: 0.204230  [128000/481450]\n",
      "loss: 0.632095  [131200/481450]\n",
      "loss: 0.375410  [134400/481450]\n",
      "loss: 0.118815  [137600/481450]\n",
      "loss: 0.213956  [140800/481450]\n",
      "loss: 0.220140  [144000/481450]\n",
      "loss: 0.189263  [147200/481450]\n",
      "loss: 0.389325  [150400/481450]\n",
      "loss: 0.297064  [153600/481450]\n",
      "loss: 0.308207  [156800/481450]\n",
      "loss: 0.474815  [160000/481450]\n",
      "loss: 0.306694  [163200/481450]\n",
      "loss: 0.272674  [166400/481450]\n",
      "loss: 0.292244  [169600/481450]\n",
      "loss: 0.357481  [172800/481450]\n",
      "loss: 0.193735  [176000/481450]\n",
      "loss: 0.392088  [179200/481450]\n",
      "loss: 0.192817  [182400/481450]\n",
      "loss: 0.312850  [185600/481450]\n",
      "loss: 0.308315  [188800/481450]\n",
      "loss: 0.596738  [192000/481450]\n",
      "loss: 0.537079  [195200/481450]\n",
      "loss: 0.217593  [198400/481450]\n",
      "loss: 0.260682  [201600/481450]\n",
      "loss: 0.243107  [204800/481450]\n",
      "loss: 0.367787  [208000/481450]\n",
      "loss: 0.287314  [211200/481450]\n",
      "loss: 0.366625  [214400/481450]\n",
      "loss: 0.338434  [217600/481450]\n",
      "loss: 0.411108  [220800/481450]\n",
      "loss: 0.310147  [224000/481450]\n",
      "loss: 0.268104  [227200/481450]\n",
      "loss: 0.423054  [230400/481450]\n",
      "loss: 0.234233  [233600/481450]\n",
      "loss: 0.139309  [236800/481450]\n",
      "loss: 0.292480  [240000/481450]\n",
      "loss: 0.365576  [243200/481450]\n",
      "loss: 0.493380  [246400/481450]\n",
      "loss: 0.292942  [249600/481450]\n",
      "loss: 0.394894  [252800/481450]\n",
      "loss: 0.322843  [256000/481450]\n",
      "loss: 0.412747  [259200/481450]\n",
      "loss: 0.457077  [262400/481450]\n",
      "loss: 0.496655  [265600/481450]\n",
      "loss: 0.509222  [268800/481450]\n",
      "loss: 0.483721  [272000/481450]\n",
      "loss: 0.345149  [275200/481450]\n",
      "loss: 0.357964  [278400/481450]\n",
      "loss: 0.233928  [281600/481450]\n",
      "loss: 0.496698  [284800/481450]\n",
      "loss: 0.382595  [288000/481450]\n",
      "loss: 0.158300  [291200/481450]\n",
      "loss: 0.346045  [294400/481450]\n",
      "loss: 0.324094  [297600/481450]\n",
      "loss: 0.453090  [300800/481450]\n",
      "loss: 0.382257  [304000/481450]\n",
      "loss: 0.357067  [307200/481450]\n",
      "loss: 0.402796  [310400/481450]\n",
      "loss: 0.451794  [313600/481450]\n",
      "loss: 0.640983  [316800/481450]\n",
      "loss: 0.280053  [320000/481450]\n",
      "loss: 0.355470  [323200/481450]\n",
      "loss: 0.447125  [326400/481450]\n",
      "loss: 0.413233  [329600/481450]\n",
      "loss: 0.259351  [332800/481450]\n",
      "loss: 0.449005  [336000/481450]\n",
      "loss: 0.577767  [339200/481450]\n",
      "loss: 0.209293  [342400/481450]\n",
      "loss: 0.546447  [345600/481450]\n",
      "loss: 0.251858  [348800/481450]\n",
      "loss: 0.230727  [352000/481450]\n",
      "loss: 0.263113  [355200/481450]\n",
      "loss: 0.472972  [358400/481450]\n",
      "loss: 0.392025  [361600/481450]\n",
      "loss: 0.307722  [364800/481450]\n",
      "loss: 0.372065  [368000/481450]\n",
      "loss: 0.209838  [371200/481450]\n",
      "loss: 0.391788  [374400/481450]\n",
      "loss: 0.468760  [377600/481450]\n",
      "loss: 0.252290  [380800/481450]\n",
      "loss: 0.374442  [384000/481450]\n",
      "loss: 0.441059  [387200/481450]\n",
      "loss: 0.643470  [390400/481450]\n",
      "loss: 0.474209  [393600/481450]\n",
      "loss: 0.207976  [396800/481450]\n",
      "loss: 0.322660  [400000/481450]\n",
      "loss: 0.455929  [403200/481450]\n",
      "loss: 0.480905  [406400/481450]\n",
      "loss: 0.341295  [409600/481450]\n",
      "loss: 0.393096  [412800/481450]\n",
      "loss: 0.272625  [416000/481450]\n",
      "loss: 0.125378  [419200/481450]\n",
      "loss: 0.359950  [422400/481450]\n",
      "loss: 0.301779  [425600/481450]\n",
      "loss: 0.350157  [428800/481450]\n",
      "loss: 0.305701  [432000/481450]\n",
      "loss: 0.437744  [435200/481450]\n",
      "loss: 0.261899  [438400/481450]\n",
      "loss: 0.599716  [441600/481450]\n",
      "loss: 0.280815  [444800/481450]\n",
      "loss: 0.288589  [448000/481450]\n",
      "loss: 0.156098  [451200/481450]\n",
      "loss: 0.534891  [454400/481450]\n",
      "loss: 0.328557  [457600/481450]\n",
      "loss: 0.503293  [460800/481450]\n",
      "loss: 0.270560  [464000/481450]\n",
      "loss: 0.371628  [467200/481450]\n",
      "loss: 0.543373  [470400/481450]\n",
      "loss: 0.334911  [473600/481450]\n",
      "loss: 0.330146  [476800/481450]\n",
      "loss: 0.324761  [480000/481450]\n",
      "Train Accuracy: 85.2685%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.809947, F1-score: 77.15% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.289118  [    0/481450]\n",
      "loss: 0.357825  [ 3200/481450]\n",
      "loss: 0.257647  [ 6400/481450]\n",
      "loss: 0.340055  [ 9600/481450]\n",
      "loss: 0.358327  [12800/481450]\n",
      "loss: 0.417490  [16000/481450]\n",
      "loss: 0.381055  [19200/481450]\n",
      "loss: 0.505459  [22400/481450]\n",
      "loss: 0.407577  [25600/481450]\n",
      "loss: 0.298119  [28800/481450]\n",
      "loss: 0.225314  [32000/481450]\n",
      "loss: 0.336511  [35200/481450]\n",
      "loss: 0.410700  [38400/481450]\n",
      "loss: 0.312428  [41600/481450]\n",
      "loss: 0.341671  [44800/481450]\n",
      "loss: 0.303376  [48000/481450]\n",
      "loss: 0.257779  [51200/481450]\n",
      "loss: 0.460130  [54400/481450]\n",
      "loss: 0.310060  [57600/481450]\n",
      "loss: 0.541169  [60800/481450]\n",
      "loss: 0.309177  [64000/481450]\n",
      "loss: 0.369566  [67200/481450]\n",
      "loss: 0.351236  [70400/481450]\n",
      "loss: 0.264097  [73600/481450]\n",
      "loss: 0.264063  [76800/481450]\n",
      "loss: 0.308891  [80000/481450]\n",
      "loss: 0.331310  [83200/481450]\n",
      "loss: 0.417585  [86400/481450]\n",
      "loss: 0.463835  [89600/481450]\n",
      "loss: 0.243365  [92800/481450]\n",
      "loss: 0.312825  [96000/481450]\n",
      "loss: 0.310738  [99200/481450]\n",
      "loss: 0.267453  [102400/481450]\n",
      "loss: 0.164673  [105600/481450]\n",
      "loss: 0.491335  [108800/481450]\n",
      "loss: 0.371768  [112000/481450]\n",
      "loss: 0.391516  [115200/481450]\n",
      "loss: 0.432700  [118400/481450]\n",
      "loss: 0.136939  [121600/481450]\n",
      "loss: 0.275681  [124800/481450]\n",
      "loss: 0.124259  [128000/481450]\n",
      "loss: 0.240406  [131200/481450]\n",
      "loss: 0.636645  [134400/481450]\n",
      "loss: 0.489997  [137600/481450]\n",
      "loss: 0.446392  [140800/481450]\n",
      "loss: 0.273222  [144000/481450]\n",
      "loss: 0.281606  [147200/481450]\n",
      "loss: 0.479894  [150400/481450]\n",
      "loss: 0.329383  [153600/481450]\n",
      "loss: 0.468004  [156800/481450]\n",
      "loss: 0.244714  [160000/481450]\n",
      "loss: 0.144722  [163200/481450]\n",
      "loss: 0.415517  [166400/481450]\n",
      "loss: 0.311593  [169600/481450]\n",
      "loss: 0.126721  [172800/481450]\n",
      "loss: 0.169094  [176000/481450]\n",
      "loss: 0.263058  [179200/481450]\n",
      "loss: 0.464066  [182400/481450]\n",
      "loss: 0.157626  [185600/481450]\n",
      "loss: 0.233380  [188800/481450]\n",
      "loss: 0.338517  [192000/481450]\n",
      "loss: 0.283759  [195200/481450]\n",
      "loss: 0.235701  [198400/481450]\n",
      "loss: 0.445770  [201600/481450]\n",
      "loss: 0.371752  [204800/481450]\n",
      "loss: 0.436521  [208000/481450]\n",
      "loss: 0.273549  [211200/481450]\n",
      "loss: 0.163906  [214400/481450]\n",
      "loss: 0.716161  [217600/481450]\n",
      "loss: 0.281489  [220800/481450]\n",
      "loss: 0.442590  [224000/481450]\n",
      "loss: 0.202500  [227200/481450]\n",
      "loss: 0.445261  [230400/481450]\n",
      "loss: 0.318773  [233600/481450]\n",
      "loss: 0.182502  [236800/481450]\n",
      "loss: 0.525683  [240000/481450]\n",
      "loss: 0.296960  [243200/481450]\n",
      "loss: 0.328290  [246400/481450]\n",
      "loss: 0.237411  [249600/481450]\n",
      "loss: 0.257346  [252800/481450]\n",
      "loss: 0.428395  [256000/481450]\n",
      "loss: 0.454368  [259200/481450]\n",
      "loss: 0.369042  [262400/481450]\n",
      "loss: 0.397977  [265600/481450]\n",
      "loss: 0.403307  [268800/481450]\n",
      "loss: 0.292926  [272000/481450]\n",
      "loss: 0.572482  [275200/481450]\n",
      "loss: 0.477534  [278400/481450]\n",
      "loss: 0.261991  [281600/481450]\n",
      "loss: 0.604403  [284800/481450]\n",
      "loss: 0.371707  [288000/481450]\n",
      "loss: 0.395790  [291200/481450]\n",
      "loss: 0.310069  [294400/481450]\n",
      "loss: 0.124595  [297600/481450]\n",
      "loss: 0.224985  [300800/481450]\n",
      "loss: 0.382811  [304000/481450]\n",
      "loss: 0.288226  [307200/481450]\n",
      "loss: 0.283911  [310400/481450]\n",
      "loss: 0.264552  [313600/481450]\n",
      "loss: 0.286736  [316800/481450]\n",
      "loss: 0.395042  [320000/481450]\n",
      "loss: 0.632418  [323200/481450]\n",
      "loss: 0.245809  [326400/481450]\n",
      "loss: 0.316754  [329600/481450]\n",
      "loss: 0.366048  [332800/481450]\n",
      "loss: 0.260967  [336000/481450]\n",
      "loss: 0.450439  [339200/481450]\n",
      "loss: 0.229013  [342400/481450]\n",
      "loss: 0.372865  [345600/481450]\n",
      "loss: 0.225862  [348800/481450]\n",
      "loss: 0.423956  [352000/481450]\n",
      "loss: 0.283322  [355200/481450]\n",
      "loss: 0.715559  [358400/481450]\n",
      "loss: 0.513449  [361600/481450]\n",
      "loss: 0.206210  [364800/481450]\n",
      "loss: 0.359834  [368000/481450]\n",
      "loss: 0.259223  [371200/481450]\n",
      "loss: 0.253777  [374400/481450]\n",
      "loss: 0.139557  [377600/481450]\n",
      "loss: 0.438806  [380800/481450]\n",
      "loss: 0.291465  [384000/481450]\n",
      "loss: 0.249759  [387200/481450]\n",
      "loss: 0.286415  [390400/481450]\n",
      "loss: 0.352619  [393600/481450]\n",
      "loss: 0.476766  [396800/481450]\n",
      "loss: 0.391496  [400000/481450]\n",
      "loss: 0.368939  [403200/481450]\n",
      "loss: 0.302801  [406400/481450]\n",
      "loss: 0.325692  [409600/481450]\n",
      "loss: 0.336475  [412800/481450]\n",
      "loss: 0.375713  [416000/481450]\n",
      "loss: 0.554570  [419200/481450]\n",
      "loss: 0.211024  [422400/481450]\n",
      "loss: 0.330451  [425600/481450]\n",
      "loss: 0.236618  [428800/481450]\n",
      "loss: 0.322901  [432000/481450]\n",
      "loss: 0.269425  [435200/481450]\n",
      "loss: 0.382462  [438400/481450]\n",
      "loss: 0.255620  [441600/481450]\n",
      "loss: 0.226802  [444800/481450]\n",
      "loss: 0.207496  [448000/481450]\n",
      "loss: 0.307447  [451200/481450]\n",
      "loss: 0.388480  [454400/481450]\n",
      "loss: 0.225653  [457600/481450]\n",
      "loss: 0.436859  [460800/481450]\n",
      "loss: 0.485935  [464000/481450]\n",
      "loss: 0.214621  [467200/481450]\n",
      "loss: 0.250462  [470400/481450]\n",
      "loss: 0.396459  [473600/481450]\n",
      "loss: 0.451415  [476800/481450]\n",
      "loss: 0.565654  [480000/481450]\n",
      "Train Accuracy: 85.7094%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.760265, F1-score: 78.54% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.295207  [    0/481450]\n",
      "loss: 0.505324  [ 3200/481450]\n",
      "loss: 0.495112  [ 6400/481450]\n",
      "loss: 0.163507  [ 9600/481450]\n",
      "loss: 0.349751  [12800/481450]\n",
      "loss: 0.270061  [16000/481450]\n",
      "loss: 0.383576  [19200/481450]\n",
      "loss: 0.580127  [22400/481450]\n",
      "loss: 0.434331  [25600/481450]\n",
      "loss: 0.440183  [28800/481450]\n",
      "loss: 0.480264  [32000/481450]\n",
      "loss: 0.380933  [35200/481450]\n",
      "loss: 0.289375  [38400/481450]\n",
      "loss: 0.398234  [41600/481450]\n",
      "loss: 0.216058  [44800/481450]\n",
      "loss: 0.410101  [48000/481450]\n",
      "loss: 0.405224  [51200/481450]\n",
      "loss: 0.324922  [54400/481450]\n",
      "loss: 0.245774  [57600/481450]\n",
      "loss: 0.470748  [60800/481450]\n",
      "loss: 0.191435  [64000/481450]\n",
      "loss: 0.284413  [67200/481450]\n",
      "loss: 0.380744  [70400/481450]\n",
      "loss: 0.166771  [73600/481450]\n",
      "loss: 0.352768  [76800/481450]\n",
      "loss: 0.258371  [80000/481450]\n",
      "loss: 0.279316  [83200/481450]\n",
      "loss: 0.224054  [86400/481450]\n",
      "loss: 0.331132  [89600/481450]\n",
      "loss: 0.356595  [92800/481450]\n",
      "loss: 0.273720  [96000/481450]\n",
      "loss: 0.519391  [99200/481450]\n",
      "loss: 0.537293  [102400/481450]\n",
      "loss: 0.300216  [105600/481450]\n",
      "loss: 0.418111  [108800/481450]\n",
      "loss: 0.458083  [112000/481450]\n",
      "loss: 0.409699  [115200/481450]\n",
      "loss: 0.354708  [118400/481450]\n",
      "loss: 0.239718  [121600/481450]\n",
      "loss: 0.659416  [124800/481450]\n",
      "loss: 0.398055  [128000/481450]\n",
      "loss: 0.252677  [131200/481450]\n",
      "loss: 0.241689  [134400/481450]\n",
      "loss: 0.471155  [137600/481450]\n",
      "loss: 0.496923  [140800/481450]\n",
      "loss: 0.181985  [144000/481450]\n",
      "loss: 0.260405  [147200/481450]\n",
      "loss: 0.238193  [150400/481450]\n",
      "loss: 0.446258  [153600/481450]\n",
      "loss: 0.396850  [156800/481450]\n",
      "loss: 0.177128  [160000/481450]\n",
      "loss: 0.396334  [163200/481450]\n",
      "loss: 0.270820  [166400/481450]\n",
      "loss: 0.524272  [169600/481450]\n",
      "loss: 0.336150  [172800/481450]\n",
      "loss: 0.481624  [176000/481450]\n",
      "loss: 0.170347  [179200/481450]\n",
      "loss: 0.454840  [182400/481450]\n",
      "loss: 0.297567  [185600/481450]\n",
      "loss: 0.514612  [188800/481450]\n",
      "loss: 0.377960  [192000/481450]\n",
      "loss: 0.384340  [195200/481450]\n",
      "loss: 0.595442  [198400/481450]\n",
      "loss: 0.329658  [201600/481450]\n",
      "loss: 0.457248  [204800/481450]\n",
      "loss: 0.284027  [208000/481450]\n",
      "loss: 0.406990  [211200/481450]\n",
      "loss: 0.201321  [214400/481450]\n",
      "loss: 0.180657  [217600/481450]\n",
      "loss: 0.508982  [220800/481450]\n",
      "loss: 0.429807  [224000/481450]\n",
      "loss: 0.453521  [227200/481450]\n",
      "loss: 0.353122  [230400/481450]\n",
      "loss: 0.261137  [233600/481450]\n",
      "loss: 0.473282  [236800/481450]\n",
      "loss: 0.364390  [240000/481450]\n",
      "loss: 0.280855  [243200/481450]\n",
      "loss: 0.386447  [246400/481450]\n",
      "loss: 0.249336  [249600/481450]\n",
      "loss: 0.357753  [252800/481450]\n",
      "loss: 0.160562  [256000/481450]\n",
      "loss: 0.485036  [259200/481450]\n",
      "loss: 0.304581  [262400/481450]\n",
      "loss: 0.491364  [265600/481450]\n",
      "loss: 0.371802  [268800/481450]\n",
      "loss: 0.282622  [272000/481450]\n",
      "loss: 0.438079  [275200/481450]\n",
      "loss: 0.319603  [278400/481450]\n",
      "loss: 0.213591  [281600/481450]\n",
      "loss: 0.266836  [284800/481450]\n",
      "loss: 0.287982  [288000/481450]\n",
      "loss: 0.397397  [291200/481450]\n",
      "loss: 0.385880  [294400/481450]\n",
      "loss: 0.235019  [297600/481450]\n",
      "loss: 0.248951  [300800/481450]\n",
      "loss: 0.280700  [304000/481450]\n",
      "loss: 0.276943  [307200/481450]\n",
      "loss: 0.283327  [310400/481450]\n",
      "loss: 0.392703  [313600/481450]\n",
      "loss: 0.325540  [316800/481450]\n",
      "loss: 0.409495  [320000/481450]\n",
      "loss: 0.141560  [323200/481450]\n",
      "loss: 0.219090  [326400/481450]\n",
      "loss: 0.429180  [329600/481450]\n",
      "loss: 0.384402  [332800/481450]\n",
      "loss: 0.169622  [336000/481450]\n",
      "loss: 0.345658  [339200/481450]\n",
      "loss: 0.209444  [342400/481450]\n",
      "loss: 0.316838  [345600/481450]\n",
      "loss: 0.271513  [348800/481450]\n",
      "loss: 0.334662  [352000/481450]\n",
      "loss: 0.175868  [355200/481450]\n",
      "loss: 0.578911  [358400/481450]\n",
      "loss: 0.404441  [361600/481450]\n",
      "loss: 0.512091  [364800/481450]\n",
      "loss: 0.208637  [368000/481450]\n",
      "loss: 0.182580  [371200/481450]\n",
      "loss: 0.247353  [374400/481450]\n",
      "loss: 0.346305  [377600/481450]\n",
      "loss: 0.491299  [380800/481450]\n",
      "loss: 0.256038  [384000/481450]\n",
      "loss: 0.565723  [387200/481450]\n",
      "loss: 0.385403  [390400/481450]\n",
      "loss: 0.426939  [393600/481450]\n",
      "loss: 0.295290  [396800/481450]\n",
      "loss: 0.215141  [400000/481450]\n",
      "loss: 0.222214  [403200/481450]\n",
      "loss: 0.300599  [406400/481450]\n",
      "loss: 0.227014  [409600/481450]\n",
      "loss: 0.147992  [412800/481450]\n",
      "loss: 0.306028  [416000/481450]\n",
      "loss: 0.428378  [419200/481450]\n",
      "loss: 0.452596  [422400/481450]\n",
      "loss: 0.462428  [425600/481450]\n",
      "loss: 0.297042  [428800/481450]\n",
      "loss: 0.324221  [432000/481450]\n",
      "loss: 0.552618  [435200/481450]\n",
      "loss: 0.244583  [438400/481450]\n",
      "loss: 0.381465  [441600/481450]\n",
      "loss: 0.159077  [444800/481450]\n",
      "loss: 0.341869  [448000/481450]\n",
      "loss: 0.591235  [451200/481450]\n",
      "loss: 0.721037  [454400/481450]\n",
      "loss: 0.308380  [457600/481450]\n",
      "loss: 0.298029  [460800/481450]\n",
      "loss: 0.384470  [464000/481450]\n",
      "loss: 0.419463  [467200/481450]\n",
      "loss: 0.290692  [470400/481450]\n",
      "loss: 0.320669  [473600/481450]\n",
      "loss: 0.239279  [476800/481450]\n",
      "loss: 0.237607  [480000/481450]\n",
      "Train Accuracy: 86.1321%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.793809, F1-score: 78.14% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.288160  [    0/481450]\n",
      "loss: 0.145464  [ 3200/481450]\n",
      "loss: 0.209356  [ 6400/481450]\n",
      "loss: 0.660226  [ 9600/481450]\n",
      "loss: 0.505461  [12800/481450]\n",
      "loss: 0.353147  [16000/481450]\n",
      "loss: 0.285192  [19200/481450]\n",
      "loss: 0.499817  [22400/481450]\n",
      "loss: 0.216709  [25600/481450]\n",
      "loss: 0.475532  [28800/481450]\n",
      "loss: 0.307492  [32000/481450]\n",
      "loss: 0.185453  [35200/481450]\n",
      "loss: 0.468623  [38400/481450]\n",
      "loss: 0.260626  [41600/481450]\n",
      "loss: 0.546769  [44800/481450]\n",
      "loss: 0.252240  [48000/481450]\n",
      "loss: 0.220574  [51200/481450]\n",
      "loss: 0.249641  [54400/481450]\n",
      "loss: 0.307314  [57600/481450]\n",
      "loss: 0.326041  [60800/481450]\n",
      "loss: 0.313301  [64000/481450]\n",
      "loss: 0.272020  [67200/481450]\n",
      "loss: 0.316814  [70400/481450]\n",
      "loss: 0.385838  [73600/481450]\n",
      "loss: 0.130041  [76800/481450]\n",
      "loss: 0.281551  [80000/481450]\n",
      "loss: 0.595080  [83200/481450]\n",
      "loss: 0.452724  [86400/481450]\n",
      "loss: 0.281270  [89600/481450]\n",
      "loss: 0.123156  [92800/481450]\n",
      "loss: 0.341632  [96000/481450]\n",
      "loss: 0.370870  [99200/481450]\n",
      "loss: 0.183142  [102400/481450]\n",
      "loss: 0.395184  [105600/481450]\n",
      "loss: 0.545027  [108800/481450]\n",
      "loss: 0.368283  [112000/481450]\n",
      "loss: 0.361957  [115200/481450]\n",
      "loss: 0.502682  [118400/481450]\n",
      "loss: 0.582624  [121600/481450]\n",
      "loss: 0.111951  [124800/481450]\n",
      "loss: 0.296670  [128000/481450]\n",
      "loss: 0.318873  [131200/481450]\n",
      "loss: 0.305370  [134400/481450]\n",
      "loss: 0.299532  [137600/481450]\n",
      "loss: 0.316253  [140800/481450]\n",
      "loss: 0.356495  [144000/481450]\n",
      "loss: 0.294751  [147200/481450]\n",
      "loss: 0.335781  [150400/481450]\n",
      "loss: 0.175722  [153600/481450]\n",
      "loss: 0.291017  [156800/481450]\n",
      "loss: 0.347047  [160000/481450]\n",
      "loss: 0.352013  [163200/481450]\n",
      "loss: 0.330094  [166400/481450]\n",
      "loss: 0.307434  [169600/481450]\n",
      "loss: 0.455092  [172800/481450]\n",
      "loss: 0.226037  [176000/481450]\n",
      "loss: 0.104746  [179200/481450]\n",
      "loss: 0.446912  [182400/481450]\n",
      "loss: 0.364188  [185600/481450]\n",
      "loss: 0.320099  [188800/481450]\n",
      "loss: 0.175571  [192000/481450]\n",
      "loss: 0.236600  [195200/481450]\n",
      "loss: 0.737166  [198400/481450]\n",
      "loss: 0.518313  [201600/481450]\n",
      "loss: 0.169606  [204800/481450]\n",
      "loss: 0.441939  [208000/481450]\n",
      "loss: 0.329098  [211200/481450]\n",
      "loss: 0.174064  [214400/481450]\n",
      "loss: 0.057084  [217600/481450]\n",
      "loss: 0.302519  [220800/481450]\n",
      "loss: 0.302047  [224000/481450]\n",
      "loss: 0.272688  [227200/481450]\n",
      "loss: 0.195381  [230400/481450]\n",
      "loss: 0.313342  [233600/481450]\n",
      "loss: 0.217263  [236800/481450]\n",
      "loss: 0.359483  [240000/481450]\n",
      "loss: 0.241952  [243200/481450]\n",
      "loss: 0.371827  [246400/481450]\n",
      "loss: 0.709781  [249600/481450]\n",
      "loss: 0.221645  [252800/481450]\n",
      "loss: 0.348542  [256000/481450]\n",
      "loss: 0.321075  [259200/481450]\n",
      "loss: 0.223776  [262400/481450]\n",
      "loss: 0.294758  [265600/481450]\n",
      "loss: 0.280464  [268800/481450]\n",
      "loss: 0.149590  [272000/481450]\n",
      "loss: 0.267712  [275200/481450]\n",
      "loss: 0.371342  [278400/481450]\n",
      "loss: 0.216764  [281600/481450]\n",
      "loss: 0.341405  [284800/481450]\n",
      "loss: 0.140198  [288000/481450]\n",
      "loss: 0.167712  [291200/481450]\n",
      "loss: 0.293762  [294400/481450]\n",
      "loss: 0.395931  [297600/481450]\n",
      "loss: 0.284718  [300800/481450]\n",
      "loss: 0.325817  [304000/481450]\n",
      "loss: 0.625019  [307200/481450]\n",
      "loss: 0.299030  [310400/481450]\n",
      "loss: 0.397543  [313600/481450]\n",
      "loss: 0.398768  [316800/481450]\n",
      "loss: 0.332649  [320000/481450]\n",
      "loss: 0.411193  [323200/481450]\n",
      "loss: 0.363210  [326400/481450]\n",
      "loss: 0.187233  [329600/481450]\n",
      "loss: 0.213968  [332800/481450]\n",
      "loss: 0.238785  [336000/481450]\n",
      "loss: 0.309993  [339200/481450]\n",
      "loss: 0.341807  [342400/481450]\n",
      "loss: 0.241532  [345600/481450]\n",
      "loss: 0.374081  [348800/481450]\n",
      "loss: 0.336028  [352000/481450]\n",
      "loss: 0.205565  [355200/481450]\n",
      "loss: 0.374380  [358400/481450]\n",
      "loss: 0.368357  [361600/481450]\n",
      "loss: 0.632454  [364800/481450]\n",
      "loss: 0.184751  [368000/481450]\n",
      "loss: 0.388287  [371200/481450]\n",
      "loss: 0.264610  [374400/481450]\n",
      "loss: 0.290527  [377600/481450]\n",
      "loss: 0.322412  [380800/481450]\n",
      "loss: 0.368887  [384000/481450]\n",
      "loss: 0.295678  [387200/481450]\n",
      "loss: 0.161847  [390400/481450]\n",
      "loss: 0.374620  [393600/481450]\n",
      "loss: 0.219744  [396800/481450]\n",
      "loss: 0.630896  [400000/481450]\n",
      "loss: 0.348976  [403200/481450]\n",
      "loss: 0.196368  [406400/481450]\n",
      "loss: 0.428710  [409600/481450]\n",
      "loss: 0.512820  [412800/481450]\n",
      "loss: 0.102381  [416000/481450]\n",
      "loss: 0.118384  [419200/481450]\n",
      "loss: 0.353301  [422400/481450]\n",
      "loss: 0.175055  [425600/481450]\n",
      "loss: 0.306801  [428800/481450]\n",
      "loss: 0.243383  [432000/481450]\n",
      "loss: 0.375737  [435200/481450]\n",
      "loss: 0.206414  [438400/481450]\n",
      "loss: 0.612354  [441600/481450]\n",
      "loss: 0.223358  [444800/481450]\n",
      "loss: 0.188415  [448000/481450]\n",
      "loss: 0.326135  [451200/481450]\n",
      "loss: 0.344231  [454400/481450]\n",
      "loss: 0.284062  [457600/481450]\n",
      "loss: 0.406950  [460800/481450]\n",
      "loss: 0.511353  [464000/481450]\n",
      "loss: 0.221630  [467200/481450]\n",
      "loss: 0.338766  [470400/481450]\n",
      "loss: 0.429073  [473600/481450]\n",
      "loss: 0.311526  [476800/481450]\n",
      "loss: 0.341933  [480000/481450]\n",
      "Train Accuracy: 86.5525%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.751867, F1-score: 78.92% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.253921  [    0/481450]\n",
      "loss: 0.224495  [ 3200/481450]\n",
      "loss: 0.365556  [ 6400/481450]\n",
      "loss: 0.488570  [ 9600/481450]\n",
      "loss: 0.242597  [12800/481450]\n",
      "loss: 0.182086  [16000/481450]\n",
      "loss: 0.254196  [19200/481450]\n",
      "loss: 0.283156  [22400/481450]\n",
      "loss: 0.132827  [25600/481450]\n",
      "loss: 0.212454  [28800/481450]\n",
      "loss: 0.304519  [32000/481450]\n",
      "loss: 0.297874  [35200/481450]\n",
      "loss: 0.200968  [38400/481450]\n",
      "loss: 0.746570  [41600/481450]\n",
      "loss: 0.511871  [44800/481450]\n",
      "loss: 0.192925  [48000/481450]\n",
      "loss: 0.186111  [51200/481450]\n",
      "loss: 0.311500  [54400/481450]\n",
      "loss: 0.261404  [57600/481450]\n",
      "loss: 0.215800  [60800/481450]\n",
      "loss: 0.180784  [64000/481450]\n",
      "loss: 0.213233  [67200/481450]\n",
      "loss: 0.497175  [70400/481450]\n",
      "loss: 0.349166  [73600/481450]\n",
      "loss: 0.366398  [76800/481450]\n",
      "loss: 0.334948  [80000/481450]\n",
      "loss: 0.426446  [83200/481450]\n",
      "loss: 0.195635  [86400/481450]\n",
      "loss: 0.449826  [89600/481450]\n",
      "loss: 0.188100  [92800/481450]\n",
      "loss: 0.170710  [96000/481450]\n",
      "loss: 0.133882  [99200/481450]\n",
      "loss: 0.276368  [102400/481450]\n",
      "loss: 0.328121  [105600/481450]\n",
      "loss: 0.248723  [108800/481450]\n",
      "loss: 0.548089  [112000/481450]\n",
      "loss: 0.370947  [115200/481450]\n",
      "loss: 0.194675  [118400/481450]\n",
      "loss: 0.256349  [121600/481450]\n",
      "loss: 0.203623  [124800/481450]\n",
      "loss: 0.223887  [128000/481450]\n",
      "loss: 0.261637  [131200/481450]\n",
      "loss: 0.305999  [134400/481450]\n",
      "loss: 0.258249  [137600/481450]\n",
      "loss: 0.306793  [140800/481450]\n",
      "loss: 0.244784  [144000/481450]\n",
      "loss: 0.213852  [147200/481450]\n",
      "loss: 0.461506  [150400/481450]\n",
      "loss: 0.375055  [153600/481450]\n",
      "loss: 0.292180  [156800/481450]\n",
      "loss: 0.404876  [160000/481450]\n",
      "loss: 0.475557  [163200/481450]\n",
      "loss: 0.677231  [166400/481450]\n",
      "loss: 0.341363  [169600/481450]\n",
      "loss: 0.132885  [172800/481450]\n",
      "loss: 0.192447  [176000/481450]\n",
      "loss: 0.172235  [179200/481450]\n",
      "loss: 0.219543  [182400/481450]\n",
      "loss: 0.381772  [185600/481450]\n",
      "loss: 0.303607  [188800/481450]\n",
      "loss: 0.191122  [192000/481450]\n",
      "loss: 0.299217  [195200/481450]\n",
      "loss: 0.196868  [198400/481450]\n",
      "loss: 0.397156  [201600/481450]\n",
      "loss: 0.254311  [204800/481450]\n",
      "loss: 0.405645  [208000/481450]\n",
      "loss: 0.293421  [211200/481450]\n",
      "loss: 0.444281  [214400/481450]\n",
      "loss: 0.227247  [217600/481450]\n",
      "loss: 0.239219  [220800/481450]\n",
      "loss: 0.358889  [224000/481450]\n",
      "loss: 0.360872  [227200/481450]\n",
      "loss: 0.576113  [230400/481450]\n",
      "loss: 0.382882  [233600/481450]\n",
      "loss: 0.368728  [236800/481450]\n",
      "loss: 0.406157  [240000/481450]\n",
      "loss: 0.298475  [243200/481450]\n",
      "loss: 0.247045  [246400/481450]\n",
      "loss: 0.428718  [249600/481450]\n",
      "loss: 0.529147  [252800/481450]\n",
      "loss: 0.470248  [256000/481450]\n",
      "loss: 0.386497  [259200/481450]\n",
      "loss: 0.277226  [262400/481450]\n",
      "loss: 0.572001  [265600/481450]\n",
      "loss: 0.565855  [268800/481450]\n",
      "loss: 0.174081  [272000/481450]\n",
      "loss: 0.427597  [275200/481450]\n",
      "loss: 0.412476  [278400/481450]\n",
      "loss: 0.296629  [281600/481450]\n",
      "loss: 0.325433  [284800/481450]\n",
      "loss: 0.341789  [288000/481450]\n",
      "loss: 0.739076  [291200/481450]\n",
      "loss: 0.376803  [294400/481450]\n",
      "loss: 0.188799  [297600/481450]\n",
      "loss: 0.276427  [300800/481450]\n",
      "loss: 0.152068  [304000/481450]\n",
      "loss: 0.325819  [307200/481450]\n",
      "loss: 0.338405  [310400/481450]\n",
      "loss: 0.342655  [313600/481450]\n",
      "loss: 0.380886  [316800/481450]\n",
      "loss: 0.420636  [320000/481450]\n",
      "loss: 0.215581  [323200/481450]\n",
      "loss: 0.453793  [326400/481450]\n",
      "loss: 0.468868  [329600/481450]\n",
      "loss: 0.203538  [332800/481450]\n",
      "loss: 0.321437  [336000/481450]\n",
      "loss: 0.483278  [339200/481450]\n",
      "loss: 0.275751  [342400/481450]\n",
      "loss: 0.388493  [345600/481450]\n",
      "loss: 0.048635  [348800/481450]\n",
      "loss: 0.317646  [352000/481450]\n",
      "loss: 0.277876  [355200/481450]\n",
      "loss: 0.200878  [358400/481450]\n",
      "loss: 0.545929  [361600/481450]\n",
      "loss: 0.284645  [364800/481450]\n",
      "loss: 0.279325  [368000/481450]\n",
      "loss: 0.120035  [371200/481450]\n",
      "loss: 0.385788  [374400/481450]\n",
      "loss: 0.365505  [377600/481450]\n",
      "loss: 0.345338  [380800/481450]\n",
      "loss: 0.450213  [384000/481450]\n",
      "loss: 0.209203  [387200/481450]\n",
      "loss: 0.313036  [390400/481450]\n",
      "loss: 0.383049  [393600/481450]\n",
      "loss: 0.397964  [396800/481450]\n",
      "loss: 0.321336  [400000/481450]\n",
      "loss: 0.195741  [403200/481450]\n",
      "loss: 0.120407  [406400/481450]\n",
      "loss: 0.445892  [409600/481450]\n",
      "loss: 0.329635  [412800/481450]\n",
      "loss: 0.324724  [416000/481450]\n",
      "loss: 0.221328  [419200/481450]\n",
      "loss: 0.399782  [422400/481450]\n",
      "loss: 0.437672  [425600/481450]\n",
      "loss: 0.376912  [428800/481450]\n",
      "loss: 0.155541  [432000/481450]\n",
      "loss: 0.276075  [435200/481450]\n",
      "loss: 0.295296  [438400/481450]\n",
      "loss: 0.144280  [441600/481450]\n",
      "loss: 0.357690  [444800/481450]\n",
      "loss: 0.183706  [448000/481450]\n",
      "loss: 0.229489  [451200/481450]\n",
      "loss: 0.285996  [454400/481450]\n",
      "loss: 0.212967  [457600/481450]\n",
      "loss: 0.377710  [460800/481450]\n",
      "loss: 0.175565  [464000/481450]\n",
      "loss: 0.477595  [467200/481450]\n",
      "loss: 0.376562  [470400/481450]\n",
      "loss: 0.225267  [473600/481450]\n",
      "loss: 0.298433  [476800/481450]\n",
      "loss: 0.246503  [480000/481450]\n",
      "Train Accuracy: 87.0128%\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.587572, F1-score: 82.43% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.427593  [    0/481450]\n",
      "loss: 0.255467  [ 3200/481450]\n",
      "loss: 0.315774  [ 6400/481450]\n",
      "loss: 0.480200  [ 9600/481450]\n",
      "loss: 0.327716  [12800/481450]\n",
      "loss: 0.158441  [16000/481450]\n",
      "loss: 0.351111  [19200/481450]\n",
      "loss: 0.338027  [22400/481450]\n",
      "loss: 0.290272  [25600/481450]\n",
      "loss: 0.137045  [28800/481450]\n",
      "loss: 0.268262  [32000/481450]\n",
      "loss: 0.462034  [35200/481450]\n",
      "loss: 0.372698  [38400/481450]\n",
      "loss: 0.430658  [41600/481450]\n",
      "loss: 0.310987  [44800/481450]\n",
      "loss: 0.041161  [48000/481450]\n",
      "loss: 0.481672  [51200/481450]\n",
      "loss: 0.222217  [54400/481450]\n",
      "loss: 0.359744  [57600/481450]\n",
      "loss: 0.618016  [60800/481450]\n",
      "loss: 0.209504  [64000/481450]\n",
      "loss: 0.242471  [67200/481450]\n",
      "loss: 0.394332  [70400/481450]\n",
      "loss: 0.457287  [73600/481450]\n",
      "loss: 0.374038  [76800/481450]\n",
      "loss: 0.131869  [80000/481450]\n",
      "loss: 0.240543  [83200/481450]\n",
      "loss: 0.536495  [86400/481450]\n",
      "loss: 0.358104  [89600/481450]\n",
      "loss: 0.376876  [92800/481450]\n",
      "loss: 0.197131  [96000/481450]\n",
      "loss: 0.352977  [99200/481450]\n",
      "loss: 0.260690  [102400/481450]\n",
      "loss: 0.331199  [105600/481450]\n",
      "loss: 0.190375  [108800/481450]\n",
      "loss: 0.511498  [112000/481450]\n",
      "loss: 0.282981  [115200/481450]\n",
      "loss: 0.771755  [118400/481450]\n",
      "loss: 0.438058  [121600/481450]\n",
      "loss: 0.429683  [124800/481450]\n",
      "loss: 0.406047  [128000/481450]\n",
      "loss: 0.173681  [131200/481450]\n",
      "loss: 0.212575  [134400/481450]\n",
      "loss: 0.375361  [137600/481450]\n",
      "loss: 0.252116  [140800/481450]\n",
      "loss: 0.262298  [144000/481450]\n",
      "loss: 0.389325  [147200/481450]\n",
      "loss: 0.286971  [150400/481450]\n",
      "loss: 0.128564  [153600/481450]\n",
      "loss: 0.227701  [156800/481450]\n",
      "loss: 0.258018  [160000/481450]\n",
      "loss: 0.308761  [163200/481450]\n",
      "loss: 0.068609  [166400/481450]\n",
      "loss: 0.458878  [169600/481450]\n",
      "loss: 0.286619  [172800/481450]\n",
      "loss: 0.258400  [176000/481450]\n",
      "loss: 0.663798  [179200/481450]\n",
      "loss: 0.336485  [182400/481450]\n",
      "loss: 0.260652  [185600/481450]\n",
      "loss: 0.125635  [188800/481450]\n",
      "loss: 0.286275  [192000/481450]\n",
      "loss: 0.202216  [195200/481450]\n",
      "loss: 0.377732  [198400/481450]\n",
      "loss: 0.309093  [201600/481450]\n",
      "loss: 0.298252  [204800/481450]\n",
      "loss: 0.432613  [208000/481450]\n",
      "loss: 0.146732  [211200/481450]\n",
      "loss: 0.276133  [214400/481450]\n",
      "loss: 0.610117  [217600/481450]\n",
      "loss: 0.255580  [220800/481450]\n",
      "loss: 0.339862  [224000/481450]\n",
      "loss: 0.345836  [227200/481450]\n",
      "loss: 0.148091  [230400/481450]\n",
      "loss: 0.340103  [233600/481450]\n",
      "loss: 0.165992  [236800/481450]\n",
      "loss: 0.247470  [240000/481450]\n",
      "loss: 0.250901  [243200/481450]\n",
      "loss: 0.269208  [246400/481450]\n",
      "loss: 0.253684  [249600/481450]\n",
      "loss: 0.222014  [252800/481450]\n",
      "loss: 0.523653  [256000/481450]\n",
      "loss: 0.201576  [259200/481450]\n",
      "loss: 0.297727  [262400/481450]\n",
      "loss: 0.322908  [265600/481450]\n",
      "loss: 0.352921  [268800/481450]\n",
      "loss: 0.274263  [272000/481450]\n",
      "loss: 0.462742  [275200/481450]\n",
      "loss: 0.370401  [278400/481450]\n",
      "loss: 0.413259  [281600/481450]\n",
      "loss: 0.146943  [284800/481450]\n",
      "loss: 0.293948  [288000/481450]\n",
      "loss: 0.462201  [291200/481450]\n",
      "loss: 0.470409  [294400/481450]\n",
      "loss: 0.333574  [297600/481450]\n",
      "loss: 0.366264  [300800/481450]\n",
      "loss: 0.227564  [304000/481450]\n",
      "loss: 0.271183  [307200/481450]\n",
      "loss: 0.295159  [310400/481450]\n",
      "loss: 0.228874  [313600/481450]\n",
      "loss: 0.340469  [316800/481450]\n",
      "loss: 0.303144  [320000/481450]\n",
      "loss: 0.471553  [323200/481450]\n",
      "loss: 0.439541  [326400/481450]\n",
      "loss: 0.325878  [329600/481450]\n",
      "loss: 0.327401  [332800/481450]\n",
      "loss: 0.351539  [336000/481450]\n",
      "loss: 0.353887  [339200/481450]\n",
      "loss: 0.222156  [342400/481450]\n",
      "loss: 0.319340  [345600/481450]\n",
      "loss: 0.151604  [348800/481450]\n",
      "loss: 0.133993  [352000/481450]\n",
      "loss: 0.315286  [355200/481450]\n",
      "loss: 0.478826  [358400/481450]\n",
      "loss: 0.040784  [361600/481450]\n",
      "loss: 0.623288  [364800/481450]\n",
      "loss: 0.438752  [368000/481450]\n",
      "loss: 0.349671  [371200/481450]\n",
      "loss: 0.407383  [374400/481450]\n",
      "loss: 0.441665  [377600/481450]\n",
      "loss: 0.308552  [380800/481450]\n",
      "loss: 0.484988  [384000/481450]\n",
      "loss: 0.301108  [387200/481450]\n",
      "loss: 0.197499  [390400/481450]\n",
      "loss: 0.207277  [393600/481450]\n",
      "loss: 0.366826  [396800/481450]\n",
      "loss: 0.483109  [400000/481450]\n",
      "loss: 0.556628  [403200/481450]\n",
      "loss: 0.323429  [406400/481450]\n",
      "loss: 0.386207  [409600/481450]\n",
      "loss: 0.108341  [412800/481450]\n",
      "loss: 0.412559  [416000/481450]\n",
      "loss: 0.374188  [419200/481450]\n",
      "loss: 0.370271  [422400/481450]\n",
      "loss: 0.324048  [425600/481450]\n",
      "loss: 0.379607  [428800/481450]\n",
      "loss: 0.190673  [432000/481450]\n",
      "loss: 0.371479  [435200/481450]\n",
      "loss: 0.356496  [438400/481450]\n",
      "loss: 0.354765  [441600/481450]\n",
      "loss: 0.448211  [444800/481450]\n",
      "loss: 0.424620  [448000/481450]\n",
      "loss: 0.240512  [451200/481450]\n",
      "loss: 0.178601  [454400/481450]\n",
      "loss: 0.516066  [457600/481450]\n",
      "loss: 0.201595  [460800/481450]\n",
      "loss: 0.309586  [464000/481450]\n",
      "loss: 0.280328  [467200/481450]\n",
      "loss: 0.437790  [470400/481450]\n",
      "loss: 0.466617  [473600/481450]\n",
      "loss: 0.269677  [476800/481450]\n",
      "loss: 0.260899  [480000/481450]\n",
      "Train Accuracy: 87.4047%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.635968, F1-score: 83.06% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.160823  [    0/481450]\n",
      "loss: 0.559044  [ 3200/481450]\n",
      "loss: 0.236022  [ 6400/481450]\n",
      "loss: 0.333669  [ 9600/481450]\n",
      "loss: 0.112028  [12800/481450]\n",
      "loss: 0.465336  [16000/481450]\n",
      "loss: 0.277035  [19200/481450]\n",
      "loss: 0.242763  [22400/481450]\n",
      "loss: 0.207965  [25600/481450]\n",
      "loss: 0.348230  [28800/481450]\n",
      "loss: 0.415930  [32000/481450]\n",
      "loss: 0.169562  [35200/481450]\n",
      "loss: 0.347031  [38400/481450]\n",
      "loss: 0.199845  [41600/481450]\n",
      "loss: 0.300729  [44800/481450]\n",
      "loss: 0.265424  [48000/481450]\n",
      "loss: 0.188944  [51200/481450]\n",
      "loss: 0.288931  [54400/481450]\n",
      "loss: 0.418969  [57600/481450]\n",
      "loss: 0.544084  [60800/481450]\n",
      "loss: 0.386907  [64000/481450]\n",
      "loss: 0.488378  [67200/481450]\n",
      "loss: 0.531477  [70400/481450]\n",
      "loss: 0.192566  [73600/481450]\n",
      "loss: 0.250230  [76800/481450]\n",
      "loss: 0.546273  [80000/481450]\n",
      "loss: 0.307574  [83200/481450]\n",
      "loss: 0.383162  [86400/481450]\n",
      "loss: 0.198080  [89600/481450]\n",
      "loss: 0.223326  [92800/481450]\n",
      "loss: 0.229291  [96000/481450]\n",
      "loss: 0.281401  [99200/481450]\n",
      "loss: 0.352031  [102400/481450]\n",
      "loss: 0.462937  [105600/481450]\n",
      "loss: 0.364784  [108800/481450]\n",
      "loss: 0.257155  [112000/481450]\n",
      "loss: 0.408386  [115200/481450]\n",
      "loss: 0.364750  [118400/481450]\n",
      "loss: 0.358994  [121600/481450]\n",
      "loss: 0.492568  [124800/481450]\n",
      "loss: 0.301462  [128000/481450]\n",
      "loss: 0.577042  [131200/481450]\n",
      "loss: 0.390288  [134400/481450]\n",
      "loss: 0.436212  [137600/481450]\n",
      "loss: 0.063991  [140800/481450]\n",
      "loss: 0.298202  [144000/481450]\n",
      "loss: 0.440200  [147200/481450]\n",
      "loss: 0.495983  [150400/481450]\n",
      "loss: 0.410725  [153600/481450]\n",
      "loss: 0.354335  [156800/481450]\n",
      "loss: 0.473168  [160000/481450]\n",
      "loss: 0.415230  [163200/481450]\n",
      "loss: 0.290880  [166400/481450]\n",
      "loss: 0.403445  [169600/481450]\n",
      "loss: 0.237849  [172800/481450]\n",
      "loss: 0.326348  [176000/481450]\n",
      "loss: 0.320568  [179200/481450]\n",
      "loss: 0.287217  [182400/481450]\n",
      "loss: 0.104338  [185600/481450]\n",
      "loss: 0.553913  [188800/481450]\n",
      "loss: 0.159429  [192000/481450]\n",
      "loss: 0.290949  [195200/481450]\n",
      "loss: 0.179779  [198400/481450]\n",
      "loss: 0.131189  [201600/481450]\n",
      "loss: 0.245985  [204800/481450]\n",
      "loss: 0.337707  [208000/481450]\n",
      "loss: 0.365818  [211200/481450]\n",
      "loss: 0.457456  [214400/481450]\n",
      "loss: 0.455280  [217600/481450]\n",
      "loss: 0.307855  [220800/481450]\n",
      "loss: 0.320631  [224000/481450]\n",
      "loss: 0.423020  [227200/481450]\n",
      "loss: 0.202121  [230400/481450]\n",
      "loss: 0.251367  [233600/481450]\n",
      "loss: 0.208096  [236800/481450]\n",
      "loss: 0.458192  [240000/481450]\n",
      "loss: 0.181587  [243200/481450]\n",
      "loss: 0.397415  [246400/481450]\n",
      "loss: 0.393110  [249600/481450]\n",
      "loss: 0.329109  [252800/481450]\n",
      "loss: 0.375887  [256000/481450]\n",
      "loss: 0.565571  [259200/481450]\n",
      "loss: 0.251024  [262400/481450]\n",
      "loss: 0.571536  [265600/481450]\n",
      "loss: 0.250125  [268800/481450]\n",
      "loss: 0.152393  [272000/481450]\n",
      "loss: 0.239329  [275200/481450]\n",
      "loss: 0.267603  [278400/481450]\n",
      "loss: 0.193583  [281600/481450]\n",
      "loss: 0.462403  [284800/481450]\n",
      "loss: 0.261682  [288000/481450]\n",
      "loss: 0.378599  [291200/481450]\n",
      "loss: 0.224954  [294400/481450]\n",
      "loss: 0.302569  [297600/481450]\n",
      "loss: 0.402441  [300800/481450]\n",
      "loss: 0.332118  [304000/481450]\n",
      "loss: 0.365109  [307200/481450]\n",
      "loss: 0.222871  [310400/481450]\n",
      "loss: 0.178192  [313600/481450]\n",
      "loss: 0.332528  [316800/481450]\n",
      "loss: 0.155529  [320000/481450]\n",
      "loss: 0.228705  [323200/481450]\n",
      "loss: 0.195009  [326400/481450]\n",
      "loss: 0.585707  [329600/481450]\n",
      "loss: 0.239491  [332800/481450]\n",
      "loss: 0.289595  [336000/481450]\n",
      "loss: 0.248216  [339200/481450]\n",
      "loss: 0.393081  [342400/481450]\n",
      "loss: 0.453739  [345600/481450]\n",
      "loss: 0.048621  [348800/481450]\n",
      "loss: 0.346140  [352000/481450]\n",
      "loss: 0.287440  [355200/481450]\n",
      "loss: 0.192703  [358400/481450]\n",
      "loss: 0.357491  [361600/481450]\n",
      "loss: 0.231085  [364800/481450]\n",
      "loss: 0.242858  [368000/481450]\n",
      "loss: 0.400469  [371200/481450]\n",
      "loss: 0.330776  [374400/481450]\n",
      "loss: 0.352031  [377600/481450]\n",
      "loss: 0.468113  [380800/481450]\n",
      "loss: 0.227595  [384000/481450]\n",
      "loss: 0.131211  [387200/481450]\n",
      "loss: 0.522202  [390400/481450]\n",
      "loss: 0.449910  [393600/481450]\n",
      "loss: 0.183307  [396800/481450]\n",
      "loss: 0.293532  [400000/481450]\n",
      "loss: 0.208090  [403200/481450]\n",
      "loss: 0.347450  [406400/481450]\n",
      "loss: 0.215694  [409600/481450]\n",
      "loss: 0.445594  [412800/481450]\n",
      "loss: 0.259005  [416000/481450]\n",
      "loss: 0.215353  [419200/481450]\n",
      "loss: 0.472623  [422400/481450]\n",
      "loss: 0.506940  [425600/481450]\n",
      "loss: 0.469169  [428800/481450]\n",
      "loss: 0.422311  [432000/481450]\n",
      "loss: 0.134284  [435200/481450]\n",
      "loss: 0.233688  [438400/481450]\n",
      "loss: 0.524874  [441600/481450]\n",
      "loss: 0.387429  [444800/481450]\n",
      "loss: 0.290550  [448000/481450]\n",
      "loss: 0.305517  [451200/481450]\n",
      "loss: 0.355097  [454400/481450]\n",
      "loss: 0.173061  [457600/481450]\n",
      "loss: 0.155849  [460800/481450]\n",
      "loss: 0.344811  [464000/481450]\n",
      "loss: 0.110394  [467200/481450]\n",
      "loss: 0.330225  [470400/481450]\n",
      "loss: 0.241586  [473600/481450]\n",
      "loss: 0.290092  [476800/481450]\n",
      "loss: 0.334270  [480000/481450]\n",
      "Train Accuracy: 87.7057%\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.696134, F1-score: 83.30% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.222603  [    0/481450]\n",
      "loss: 0.282730  [ 3200/481450]\n",
      "loss: 0.383208  [ 6400/481450]\n",
      "loss: 0.281340  [ 9600/481450]\n",
      "loss: 0.109638  [12800/481450]\n",
      "loss: 0.555395  [16000/481450]\n",
      "loss: 0.280424  [19200/481450]\n",
      "loss: 0.303959  [22400/481450]\n",
      "loss: 0.250952  [25600/481450]\n",
      "loss: 0.143322  [28800/481450]\n",
      "loss: 0.248372  [32000/481450]\n",
      "loss: 0.353707  [35200/481450]\n",
      "loss: 0.236866  [38400/481450]\n",
      "loss: 0.441330  [41600/481450]\n",
      "loss: 0.207498  [44800/481450]\n",
      "loss: 0.258489  [48000/481450]\n",
      "loss: 0.197515  [51200/481450]\n",
      "loss: 0.382242  [54400/481450]\n",
      "loss: 0.223090  [57600/481450]\n",
      "loss: 0.275405  [60800/481450]\n",
      "loss: 0.376493  [64000/481450]\n",
      "loss: 0.191542  [67200/481450]\n",
      "loss: 0.304260  [70400/481450]\n",
      "loss: 0.438819  [73600/481450]\n",
      "loss: 0.265218  [76800/481450]\n",
      "loss: 0.622904  [80000/481450]\n",
      "loss: 0.358842  [83200/481450]\n",
      "loss: 0.119415  [86400/481450]\n",
      "loss: 0.486501  [89600/481450]\n",
      "loss: 0.206165  [92800/481450]\n",
      "loss: 0.280721  [96000/481450]\n",
      "loss: 0.132187  [99200/481450]\n",
      "loss: 0.134143  [102400/481450]\n",
      "loss: 0.373057  [105600/481450]\n",
      "loss: 0.205072  [108800/481450]\n",
      "loss: 0.287829  [112000/481450]\n",
      "loss: 0.241221  [115200/481450]\n",
      "loss: 0.201610  [118400/481450]\n",
      "loss: 0.459573  [121600/481450]\n",
      "loss: 0.362794  [124800/481450]\n",
      "loss: 0.461262  [128000/481450]\n",
      "loss: 0.120697  [131200/481450]\n",
      "loss: 0.269158  [134400/481450]\n",
      "loss: 0.483019  [137600/481450]\n",
      "loss: 0.242718  [140800/481450]\n",
      "loss: 0.378421  [144000/481450]\n",
      "loss: 0.286510  [147200/481450]\n",
      "loss: 0.349330  [150400/481450]\n",
      "loss: 0.162449  [153600/481450]\n",
      "loss: 0.111844  [156800/481450]\n",
      "loss: 0.413897  [160000/481450]\n",
      "loss: 0.398848  [163200/481450]\n",
      "loss: 0.241041  [166400/481450]\n",
      "loss: 0.187767  [169600/481450]\n",
      "loss: 0.577785  [172800/481450]\n",
      "loss: 0.188876  [176000/481450]\n",
      "loss: 0.350673  [179200/481450]\n",
      "loss: 0.307255  [182400/481450]\n",
      "loss: 0.135380  [185600/481450]\n",
      "loss: 0.266123  [188800/481450]\n",
      "loss: 0.501113  [192000/481450]\n",
      "loss: 0.199339  [195200/481450]\n",
      "loss: 0.338799  [198400/481450]\n",
      "loss: 0.242537  [201600/481450]\n",
      "loss: 0.292994  [204800/481450]\n",
      "loss: 0.483129  [208000/481450]\n",
      "loss: 0.247428  [211200/481450]\n",
      "loss: 0.430015  [214400/481450]\n",
      "loss: 0.263333  [217600/481450]\n",
      "loss: 0.248774  [220800/481450]\n",
      "loss: 0.389415  [224000/481450]\n",
      "loss: 0.401672  [227200/481450]\n",
      "loss: 0.193680  [230400/481450]\n",
      "loss: 0.228596  [233600/481450]\n",
      "loss: 0.210971  [236800/481450]\n",
      "loss: 0.181319  [240000/481450]\n",
      "loss: 0.147759  [243200/481450]\n",
      "loss: 0.070949  [246400/481450]\n",
      "loss: 0.299580  [249600/481450]\n",
      "loss: 0.300195  [252800/481450]\n",
      "loss: 0.306841  [256000/481450]\n",
      "loss: 0.305600  [259200/481450]\n",
      "loss: 0.398301  [262400/481450]\n",
      "loss: 0.291415  [265600/481450]\n",
      "loss: 0.406751  [268800/481450]\n",
      "loss: 0.374682  [272000/481450]\n",
      "loss: 0.231439  [275200/481450]\n",
      "loss: 0.355251  [278400/481450]\n",
      "loss: 0.208159  [281600/481450]\n",
      "loss: 0.197966  [284800/481450]\n",
      "loss: 0.255259  [288000/481450]\n",
      "loss: 0.211643  [291200/481450]\n",
      "loss: 0.422668  [294400/481450]\n",
      "loss: 0.317727  [297600/481450]\n",
      "loss: 0.542088  [300800/481450]\n",
      "loss: 0.330508  [304000/481450]\n",
      "loss: 0.316103  [307200/481450]\n",
      "loss: 0.398216  [310400/481450]\n",
      "loss: 0.384182  [313600/481450]\n",
      "loss: 0.622180  [316800/481450]\n",
      "loss: 0.189130  [320000/481450]\n",
      "loss: 0.156193  [323200/481450]\n",
      "loss: 0.169449  [326400/481450]\n",
      "loss: 0.426701  [329600/481450]\n",
      "loss: 0.225329  [332800/481450]\n",
      "loss: 0.279607  [336000/481450]\n",
      "loss: 0.444056  [339200/481450]\n",
      "loss: 0.143235  [342400/481450]\n",
      "loss: 0.293017  [345600/481450]\n",
      "loss: 0.557140  [348800/481450]\n",
      "loss: 0.261319  [352000/481450]\n",
      "loss: 0.184673  [355200/481450]\n",
      "loss: 0.457875  [358400/481450]\n",
      "loss: 0.280348  [361600/481450]\n",
      "loss: 0.278561  [364800/481450]\n",
      "loss: 0.156551  [368000/481450]\n",
      "loss: 0.261523  [371200/481450]\n",
      "loss: 0.217382  [374400/481450]\n",
      "loss: 0.448210  [377600/481450]\n",
      "loss: 0.264456  [380800/481450]\n",
      "loss: 0.374853  [384000/481450]\n",
      "loss: 0.329661  [387200/481450]\n",
      "loss: 0.326478  [390400/481450]\n",
      "loss: 0.356339  [393600/481450]\n",
      "loss: 0.392016  [396800/481450]\n",
      "loss: 0.289476  [400000/481450]\n",
      "loss: 0.413596  [403200/481450]\n",
      "loss: 0.105291  [406400/481450]\n",
      "loss: 0.222328  [409600/481450]\n",
      "loss: 0.153602  [412800/481450]\n",
      "loss: 0.278695  [416000/481450]\n",
      "loss: 0.345937  [419200/481450]\n",
      "loss: 0.203906  [422400/481450]\n",
      "loss: 0.230771  [425600/481450]\n",
      "loss: 0.227658  [428800/481450]\n",
      "loss: 0.258815  [432000/481450]\n",
      "loss: 0.482382  [435200/481450]\n",
      "loss: 0.232265  [438400/481450]\n",
      "loss: 0.246588  [441600/481450]\n",
      "loss: 0.380574  [444800/481450]\n",
      "loss: 0.276884  [448000/481450]\n",
      "loss: 0.319344  [451200/481450]\n",
      "loss: 0.294935  [454400/481450]\n",
      "loss: 0.398537  [457600/481450]\n",
      "loss: 0.222453  [460800/481450]\n",
      "loss: 0.423355  [464000/481450]\n",
      "loss: 0.542467  [467200/481450]\n",
      "loss: 0.247346  [470400/481450]\n",
      "loss: 0.194813  [473600/481450]\n",
      "loss: 0.419055  [476800/481450]\n",
      "loss: 0.283642  [480000/481450]\n",
      "Train Accuracy: 87.8758%\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.657049, F1-score: 84.84% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.419702  [    0/481450]\n",
      "loss: 0.197737  [ 3200/481450]\n",
      "loss: 0.219143  [ 6400/481450]\n",
      "loss: 0.196394  [ 9600/481450]\n",
      "loss: 0.219094  [12800/481450]\n",
      "loss: 0.347015  [16000/481450]\n",
      "loss: 0.316044  [19200/481450]\n",
      "loss: 0.203778  [22400/481450]\n",
      "loss: 0.272968  [25600/481450]\n",
      "loss: 0.323087  [28800/481450]\n",
      "loss: 0.153578  [32000/481450]\n",
      "loss: 0.388682  [35200/481450]\n",
      "loss: 0.177610  [38400/481450]\n",
      "loss: 0.256738  [41600/481450]\n",
      "loss: 0.302946  [44800/481450]\n",
      "loss: 0.291084  [48000/481450]\n",
      "loss: 0.336028  [51200/481450]\n",
      "loss: 0.097373  [54400/481450]\n",
      "loss: 0.255851  [57600/481450]\n",
      "loss: 0.228311  [60800/481450]\n",
      "loss: 0.181717  [64000/481450]\n",
      "loss: 0.486021  [67200/481450]\n",
      "loss: 0.210267  [70400/481450]\n",
      "loss: 0.314952  [73600/481450]\n",
      "loss: 0.308502  [76800/481450]\n",
      "loss: 0.341873  [80000/481450]\n",
      "loss: 0.477124  [83200/481450]\n",
      "loss: 0.337258  [86400/481450]\n",
      "loss: 0.201487  [89600/481450]\n",
      "loss: 0.388156  [92800/481450]\n",
      "loss: 0.322105  [96000/481450]\n",
      "loss: 0.080960  [99200/481450]\n",
      "loss: 0.245475  [102400/481450]\n",
      "loss: 0.434326  [105600/481450]\n",
      "loss: 0.386413  [108800/481450]\n",
      "loss: 0.256292  [112000/481450]\n",
      "loss: 0.281817  [115200/481450]\n",
      "loss: 0.359093  [118400/481450]\n",
      "loss: 0.277592  [121600/481450]\n",
      "loss: 0.283109  [124800/481450]\n",
      "loss: 0.208345  [128000/481450]\n",
      "loss: 0.255298  [131200/481450]\n",
      "loss: 0.339650  [134400/481450]\n",
      "loss: 0.159482  [137600/481450]\n",
      "loss: 0.298013  [140800/481450]\n",
      "loss: 0.419061  [144000/481450]\n",
      "loss: 0.292406  [147200/481450]\n",
      "loss: 0.185373  [150400/481450]\n",
      "loss: 0.461093  [153600/481450]\n",
      "loss: 0.512537  [156800/481450]\n",
      "loss: 0.163285  [160000/481450]\n",
      "loss: 0.206241  [163200/481450]\n",
      "loss: 0.191409  [166400/481450]\n",
      "loss: 0.248651  [169600/481450]\n",
      "loss: 0.129881  [172800/481450]\n",
      "loss: 0.640453  [176000/481450]\n",
      "loss: 0.144791  [179200/481450]\n",
      "loss: 0.531367  [182400/481450]\n",
      "loss: 0.208940  [185600/481450]\n",
      "loss: 0.236590  [188800/481450]\n",
      "loss: 0.320493  [192000/481450]\n",
      "loss: 0.210651  [195200/481450]\n",
      "loss: 0.452167  [198400/481450]\n",
      "loss: 0.208630  [201600/481450]\n",
      "loss: 0.268688  [204800/481450]\n",
      "loss: 0.296589  [208000/481450]\n",
      "loss: 0.500989  [211200/481450]\n",
      "loss: 0.380444  [214400/481450]\n",
      "loss: 0.252760  [217600/481450]\n",
      "loss: 0.161706  [220800/481450]\n",
      "loss: 0.454852  [224000/481450]\n",
      "loss: 0.331506  [227200/481450]\n",
      "loss: 0.171391  [230400/481450]\n",
      "loss: 0.288653  [233600/481450]\n",
      "loss: 0.176811  [236800/481450]\n",
      "loss: 0.253659  [240000/481450]\n",
      "loss: 0.160208  [243200/481450]\n",
      "loss: 0.525454  [246400/481450]\n",
      "loss: 0.432065  [249600/481450]\n",
      "loss: 0.332082  [252800/481450]\n",
      "loss: 0.180302  [256000/481450]\n",
      "loss: 0.526030  [259200/481450]\n",
      "loss: 0.150323  [262400/481450]\n",
      "loss: 0.253940  [265600/481450]\n",
      "loss: 0.208982  [268800/481450]\n",
      "loss: 0.116121  [272000/481450]\n",
      "loss: 0.521497  [275200/481450]\n",
      "loss: 0.101758  [278400/481450]\n",
      "loss: 0.280859  [281600/481450]\n",
      "loss: 0.378604  [284800/481450]\n",
      "loss: 0.313144  [288000/481450]\n",
      "loss: 0.397361  [291200/481450]\n",
      "loss: 0.230849  [294400/481450]\n",
      "loss: 0.262313  [297600/481450]\n",
      "loss: 0.447111  [300800/481450]\n",
      "loss: 0.269083  [304000/481450]\n",
      "loss: 0.102912  [307200/481450]\n",
      "loss: 0.349605  [310400/481450]\n",
      "loss: 0.357992  [313600/481450]\n",
      "loss: 0.217093  [316800/481450]\n",
      "loss: 0.244980  [320000/481450]\n",
      "loss: 0.361331  [323200/481450]\n",
      "loss: 0.309847  [326400/481450]\n",
      "loss: 0.232385  [329600/481450]\n",
      "loss: 0.289040  [332800/481450]\n",
      "loss: 0.316747  [336000/481450]\n",
      "loss: 0.494073  [339200/481450]\n",
      "loss: 0.484442  [342400/481450]\n",
      "loss: 0.261134  [345600/481450]\n",
      "loss: 0.200713  [348800/481450]\n",
      "loss: 0.304667  [352000/481450]\n",
      "loss: 0.365115  [355200/481450]\n",
      "loss: 0.408029  [358400/481450]\n",
      "loss: 0.241719  [361600/481450]\n",
      "loss: 0.295249  [364800/481450]\n",
      "loss: 0.190951  [368000/481450]\n",
      "loss: 0.226904  [371200/481450]\n",
      "loss: 0.207620  [374400/481450]\n",
      "loss: 0.421675  [377600/481450]\n",
      "loss: 0.258471  [380800/481450]\n",
      "loss: 0.247753  [384000/481450]\n",
      "loss: 0.264601  [387200/481450]\n",
      "loss: 0.359390  [390400/481450]\n",
      "loss: 0.187171  [393600/481450]\n",
      "loss: 0.202272  [396800/481450]\n",
      "loss: 0.172943  [400000/481450]\n",
      "loss: 0.276916  [403200/481450]\n",
      "loss: 0.400542  [406400/481450]\n",
      "loss: 0.196586  [409600/481450]\n",
      "loss: 0.588161  [412800/481450]\n",
      "loss: 0.400603  [416000/481450]\n",
      "loss: 0.176980  [419200/481450]\n",
      "loss: 0.157921  [422400/481450]\n",
      "loss: 0.166580  [425600/481450]\n",
      "loss: 0.342404  [428800/481450]\n",
      "loss: 0.264355  [432000/481450]\n",
      "loss: 0.439234  [435200/481450]\n",
      "loss: 0.317926  [438400/481450]\n",
      "loss: 0.396040  [441600/481450]\n",
      "loss: 0.224627  [444800/481450]\n",
      "loss: 0.392844  [448000/481450]\n",
      "loss: 0.414730  [451200/481450]\n",
      "loss: 0.212887  [454400/481450]\n",
      "loss: 0.284401  [457600/481450]\n",
      "loss: 0.294633  [460800/481450]\n",
      "loss: 0.274651  [464000/481450]\n",
      "loss: 0.244890  [467200/481450]\n",
      "loss: 0.440279  [470400/481450]\n",
      "loss: 0.293791  [473600/481450]\n",
      "loss: 0.638425  [476800/481450]\n",
      "loss: 0.240648  [480000/481450]\n",
      "Train Accuracy: 88.1319%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.617638, F1-score: 85.58% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.410978  [    0/481450]\n",
      "loss: 0.308753  [ 3200/481450]\n",
      "loss: 0.167475  [ 6400/481450]\n",
      "loss: 0.084385  [ 9600/481450]\n",
      "loss: 0.436976  [12800/481450]\n",
      "loss: 0.180575  [16000/481450]\n",
      "loss: 0.480494  [19200/481450]\n",
      "loss: 0.147481  [22400/481450]\n",
      "loss: 0.221523  [25600/481450]\n",
      "loss: 0.120790  [28800/481450]\n",
      "loss: 0.313212  [32000/481450]\n",
      "loss: 0.522912  [35200/481450]\n",
      "loss: 0.318531  [38400/481450]\n",
      "loss: 0.253984  [41600/481450]\n",
      "loss: 0.257137  [44800/481450]\n",
      "loss: 0.161340  [48000/481450]\n",
      "loss: 0.249833  [51200/481450]\n",
      "loss: 0.193897  [54400/481450]\n",
      "loss: 0.107891  [57600/481450]\n",
      "loss: 0.540851  [60800/481450]\n",
      "loss: 0.172848  [64000/481450]\n",
      "loss: 0.029686  [67200/481450]\n",
      "loss: 0.143304  [70400/481450]\n",
      "loss: 0.248292  [73600/481450]\n",
      "loss: 0.348622  [76800/481450]\n",
      "loss: 0.157416  [80000/481450]\n",
      "loss: 0.510198  [83200/481450]\n",
      "loss: 0.394262  [86400/481450]\n",
      "loss: 0.179118  [89600/481450]\n",
      "loss: 0.127442  [92800/481450]\n",
      "loss: 0.157055  [96000/481450]\n",
      "loss: 0.334463  [99200/481450]\n",
      "loss: 0.223188  [102400/481450]\n",
      "loss: 0.236613  [105600/481450]\n",
      "loss: 0.293459  [108800/481450]\n",
      "loss: 0.323098  [112000/481450]\n",
      "loss: 0.272755  [115200/481450]\n",
      "loss: 0.241188  [118400/481450]\n",
      "loss: 0.188368  [121600/481450]\n",
      "loss: 0.232509  [124800/481450]\n",
      "loss: 0.249754  [128000/481450]\n",
      "loss: 0.335935  [131200/481450]\n",
      "loss: 0.314202  [134400/481450]\n",
      "loss: 0.222475  [137600/481450]\n",
      "loss: 0.319002  [140800/481450]\n",
      "loss: 0.269964  [144000/481450]\n",
      "loss: 0.215440  [147200/481450]\n",
      "loss: 0.381957  [150400/481450]\n",
      "loss: 0.237652  [153600/481450]\n",
      "loss: 0.288838  [156800/481450]\n",
      "loss: 0.337784  [160000/481450]\n",
      "loss: 0.380904  [163200/481450]\n",
      "loss: 0.200456  [166400/481450]\n",
      "loss: 0.299178  [169600/481450]\n",
      "loss: 0.210661  [172800/481450]\n",
      "loss: 0.404057  [176000/481450]\n",
      "loss: 0.157118  [179200/481450]\n",
      "loss: 0.283459  [182400/481450]\n",
      "loss: 0.469074  [185600/481450]\n",
      "loss: 0.375861  [188800/481450]\n",
      "loss: 0.088970  [192000/481450]\n",
      "loss: 0.289402  [195200/481450]\n",
      "loss: 0.249992  [198400/481450]\n",
      "loss: 0.262674  [201600/481450]\n",
      "loss: 0.325830  [204800/481450]\n",
      "loss: 0.309007  [208000/481450]\n",
      "loss: 0.383569  [211200/481450]\n",
      "loss: 0.351242  [214400/481450]\n",
      "loss: 0.229807  [217600/481450]\n",
      "loss: 0.280269  [220800/481450]\n",
      "loss: 0.431216  [224000/481450]\n",
      "loss: 0.323381  [227200/481450]\n",
      "loss: 0.316708  [230400/481450]\n",
      "loss: 0.098215  [233600/481450]\n",
      "loss: 0.269144  [236800/481450]\n",
      "loss: 0.299465  [240000/481450]\n",
      "loss: 0.163386  [243200/481450]\n",
      "loss: 0.255622  [246400/481450]\n",
      "loss: 0.172888  [249600/481450]\n",
      "loss: 0.600578  [252800/481450]\n",
      "loss: 0.204769  [256000/481450]\n",
      "loss: 0.212122  [259200/481450]\n",
      "loss: 0.391204  [262400/481450]\n",
      "loss: 0.530261  [265600/481450]\n",
      "loss: 0.344890  [268800/481450]\n",
      "loss: 0.304351  [272000/481450]\n",
      "loss: 0.421921  [275200/481450]\n",
      "loss: 0.368618  [278400/481450]\n",
      "loss: 0.298915  [281600/481450]\n",
      "loss: 0.401398  [284800/481450]\n",
      "loss: 0.315793  [288000/481450]\n",
      "loss: 0.205786  [291200/481450]\n",
      "loss: 0.389021  [294400/481450]\n",
      "loss: 0.208388  [297600/481450]\n",
      "loss: 0.441057  [300800/481450]\n",
      "loss: 0.286722  [304000/481450]\n",
      "loss: 0.268268  [307200/481450]\n",
      "loss: 0.329712  [310400/481450]\n",
      "loss: 0.399835  [313600/481450]\n",
      "loss: 0.445098  [316800/481450]\n",
      "loss: 0.369633  [320000/481450]\n",
      "loss: 0.214813  [323200/481450]\n",
      "loss: 0.082512  [326400/481450]\n",
      "loss: 0.387063  [329600/481450]\n",
      "loss: 0.353488  [332800/481450]\n",
      "loss: 0.079272  [336000/481450]\n",
      "loss: 0.086270  [339200/481450]\n",
      "loss: 0.508493  [342400/481450]\n",
      "loss: 0.248172  [345600/481450]\n",
      "loss: 0.389397  [348800/481450]\n",
      "loss: 0.413723  [352000/481450]\n",
      "loss: 0.532035  [355200/481450]\n",
      "loss: 0.268273  [358400/481450]\n",
      "loss: 0.272568  [361600/481450]\n",
      "loss: 0.273443  [364800/481450]\n",
      "loss: 0.197192  [368000/481450]\n",
      "loss: 0.165167  [371200/481450]\n",
      "loss: 0.366437  [374400/481450]\n",
      "loss: 0.437194  [377600/481450]\n",
      "loss: 0.122127  [380800/481450]\n",
      "loss: 0.333772  [384000/481450]\n",
      "loss: 0.316382  [387200/481450]\n",
      "loss: 0.108802  [390400/481450]\n",
      "loss: 0.196091  [393600/481450]\n",
      "loss: 0.189035  [396800/481450]\n",
      "loss: 0.315811  [400000/481450]\n",
      "loss: 0.186116  [403200/481450]\n",
      "loss: 0.144386  [406400/481450]\n",
      "loss: 0.196073  [409600/481450]\n",
      "loss: 0.246700  [412800/481450]\n",
      "loss: 0.179605  [416000/481450]\n",
      "loss: 0.373398  [419200/481450]\n",
      "loss: 0.500601  [422400/481450]\n",
      "loss: 0.231533  [425600/481450]\n",
      "loss: 0.230687  [428800/481450]\n",
      "loss: 0.384986  [432000/481450]\n",
      "loss: 0.243760  [435200/481450]\n",
      "loss: 0.424164  [438400/481450]\n",
      "loss: 0.520046  [441600/481450]\n",
      "loss: 0.344315  [444800/481450]\n",
      "loss: 0.305853  [448000/481450]\n",
      "loss: 0.454836  [451200/481450]\n",
      "loss: 0.358472  [454400/481450]\n",
      "loss: 0.508460  [457600/481450]\n",
      "loss: 0.324390  [460800/481450]\n",
      "loss: 0.272574  [464000/481450]\n",
      "loss: 0.310520  [467200/481450]\n",
      "loss: 0.339054  [470400/481450]\n",
      "loss: 0.457200  [473600/481450]\n",
      "loss: 0.232500  [476800/481450]\n",
      "loss: 0.359617  [480000/481450]\n",
      "Train Accuracy: 88.2964%\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.776986, F1-score: 83.32% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.288287  [    0/481450]\n",
      "loss: 0.219179  [ 3200/481450]\n",
      "loss: 0.125564  [ 6400/481450]\n",
      "loss: 0.206708  [ 9600/481450]\n",
      "loss: 0.257457  [12800/481450]\n",
      "loss: 0.194814  [16000/481450]\n",
      "loss: 0.456337  [19200/481450]\n",
      "loss: 0.254336  [22400/481450]\n",
      "loss: 0.503128  [25600/481450]\n",
      "loss: 0.184528  [28800/481450]\n",
      "loss: 0.334679  [32000/481450]\n",
      "loss: 0.534953  [35200/481450]\n",
      "loss: 0.276846  [38400/481450]\n",
      "loss: 0.333226  [41600/481450]\n",
      "loss: 0.287585  [44800/481450]\n",
      "loss: 0.160000  [48000/481450]\n",
      "loss: 0.284135  [51200/481450]\n",
      "loss: 0.308967  [54400/481450]\n",
      "loss: 0.500894  [57600/481450]\n",
      "loss: 0.419715  [60800/481450]\n",
      "loss: 0.227920  [64000/481450]\n",
      "loss: 0.338203  [67200/481450]\n",
      "loss: 0.227041  [70400/481450]\n",
      "loss: 0.539020  [73600/481450]\n",
      "loss: 0.182038  [76800/481450]\n",
      "loss: 0.264838  [80000/481450]\n",
      "loss: 0.335221  [83200/481450]\n",
      "loss: 0.133770  [86400/481450]\n",
      "loss: 0.178069  [89600/481450]\n",
      "loss: 0.200077  [92800/481450]\n",
      "loss: 0.158882  [96000/481450]\n",
      "loss: 0.292857  [99200/481450]\n",
      "loss: 0.376068  [102400/481450]\n",
      "loss: 0.603767  [105600/481450]\n",
      "loss: 0.369965  [108800/481450]\n",
      "loss: 0.419608  [112000/481450]\n",
      "loss: 0.117959  [115200/481450]\n",
      "loss: 0.142868  [118400/481450]\n",
      "loss: 0.162063  [121600/481450]\n",
      "loss: 0.167199  [124800/481450]\n",
      "loss: 0.360902  [128000/481450]\n",
      "loss: 0.190382  [131200/481450]\n",
      "loss: 0.237228  [134400/481450]\n",
      "loss: 0.060819  [137600/481450]\n",
      "loss: 0.274858  [140800/481450]\n",
      "loss: 0.258602  [144000/481450]\n",
      "loss: 0.140221  [147200/481450]\n",
      "loss: 0.332479  [150400/481450]\n",
      "loss: 0.249458  [153600/481450]\n",
      "loss: 0.332819  [156800/481450]\n",
      "loss: 0.266406  [160000/481450]\n",
      "loss: 0.381792  [163200/481450]\n",
      "loss: 0.261406  [166400/481450]\n",
      "loss: 0.508879  [169600/481450]\n",
      "loss: 0.095263  [172800/481450]\n",
      "loss: 0.305379  [176000/481450]\n",
      "loss: 0.463228  [179200/481450]\n",
      "loss: 0.324559  [182400/481450]\n",
      "loss: 0.191286  [185600/481450]\n",
      "loss: 0.185741  [188800/481450]\n",
      "loss: 0.299126  [192000/481450]\n",
      "loss: 0.519121  [195200/481450]\n",
      "loss: 0.435503  [198400/481450]\n",
      "loss: 0.355334  [201600/481450]\n",
      "loss: 0.259254  [204800/481450]\n",
      "loss: 0.297162  [208000/481450]\n",
      "loss: 0.135451  [211200/481450]\n",
      "loss: 0.246844  [214400/481450]\n",
      "loss: 0.383366  [217600/481450]\n",
      "loss: 0.272461  [220800/481450]\n",
      "loss: 0.287839  [224000/481450]\n",
      "loss: 0.127811  [227200/481450]\n",
      "loss: 0.202380  [230400/481450]\n",
      "loss: 0.299473  [233600/481450]\n",
      "loss: 0.299711  [236800/481450]\n",
      "loss: 0.420473  [240000/481450]\n",
      "loss: 0.465000  [243200/481450]\n",
      "loss: 0.183332  [246400/481450]\n",
      "loss: 0.408144  [249600/481450]\n",
      "loss: 0.179906  [252800/481450]\n",
      "loss: 0.152128  [256000/481450]\n",
      "loss: 0.449475  [259200/481450]\n",
      "loss: 0.303110  [262400/481450]\n",
      "loss: 0.399872  [265600/481450]\n",
      "loss: 0.376060  [268800/481450]\n",
      "loss: 0.329767  [272000/481450]\n",
      "loss: 0.289173  [275200/481450]\n",
      "loss: 0.295880  [278400/481450]\n",
      "loss: 0.189157  [281600/481450]\n",
      "loss: 0.383494  [284800/481450]\n",
      "loss: 0.226341  [288000/481450]\n",
      "loss: 0.260196  [291200/481450]\n",
      "loss: 0.301958  [294400/481450]\n",
      "loss: 0.173082  [297600/481450]\n",
      "loss: 0.347434  [300800/481450]\n",
      "loss: 0.123622  [304000/481450]\n",
      "loss: 0.022020  [307200/481450]\n",
      "loss: 0.172770  [310400/481450]\n",
      "loss: 0.444790  [313600/481450]\n",
      "loss: 0.438458  [316800/481450]\n",
      "loss: 0.344540  [320000/481450]\n",
      "loss: 0.246164  [323200/481450]\n",
      "loss: 0.352688  [326400/481450]\n",
      "loss: 0.229708  [329600/481450]\n",
      "loss: 0.282830  [332800/481450]\n",
      "loss: 0.290970  [336000/481450]\n",
      "loss: 0.097920  [339200/481450]\n",
      "loss: 0.225426  [342400/481450]\n",
      "loss: 0.298965  [345600/481450]\n",
      "loss: 0.278712  [348800/481450]\n",
      "loss: 0.214651  [352000/481450]\n",
      "loss: 0.339864  [355200/481450]\n",
      "loss: 0.368561  [358400/481450]\n",
      "loss: 0.193872  [361600/481450]\n",
      "loss: 0.236549  [364800/481450]\n",
      "loss: 0.245025  [368000/481450]\n",
      "loss: 0.443819  [371200/481450]\n",
      "loss: 0.320842  [374400/481450]\n",
      "loss: 0.432676  [377600/481450]\n",
      "loss: 0.356991  [380800/481450]\n",
      "loss: 0.241929  [384000/481450]\n",
      "loss: 0.314309  [387200/481450]\n",
      "loss: 0.285188  [390400/481450]\n",
      "loss: 0.275123  [393600/481450]\n",
      "loss: 0.132734  [396800/481450]\n",
      "loss: 0.242486  [400000/481450]\n",
      "loss: 0.169777  [403200/481450]\n",
      "loss: 0.399032  [406400/481450]\n",
      "loss: 0.193572  [409600/481450]\n",
      "loss: 0.416889  [412800/481450]\n",
      "loss: 0.146187  [416000/481450]\n",
      "loss: 0.243995  [419200/481450]\n",
      "loss: 0.241721  [422400/481450]\n",
      "loss: 0.365133  [425600/481450]\n",
      "loss: 0.232938  [428800/481450]\n",
      "loss: 0.094756  [432000/481450]\n",
      "loss: 0.190170  [435200/481450]\n",
      "loss: 0.196000  [438400/481450]\n",
      "loss: 0.652522  [441600/481450]\n",
      "loss: 0.347500  [444800/481450]\n",
      "loss: 0.369677  [448000/481450]\n",
      "loss: 0.199330  [451200/481450]\n",
      "loss: 0.191191  [454400/481450]\n",
      "loss: 0.377739  [457600/481450]\n",
      "loss: 0.127689  [460800/481450]\n",
      "loss: 0.410464  [464000/481450]\n",
      "loss: 0.226463  [467200/481450]\n",
      "loss: 0.306218  [470400/481450]\n",
      "loss: 0.591022  [473600/481450]\n",
      "loss: 0.535132  [476800/481450]\n",
      "loss: 0.157863  [480000/481450]\n",
      "Train Accuracy: 88.5016%\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.807938, F1-score: 83.47% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.122746  [    0/481450]\n",
      "loss: 0.215623  [ 3200/481450]\n",
      "loss: 0.414922  [ 6400/481450]\n",
      "loss: 0.250231  [ 9600/481450]\n",
      "loss: 0.134833  [12800/481450]\n",
      "loss: 0.263815  [16000/481450]\n",
      "loss: 0.316679  [19200/481450]\n",
      "loss: 0.374953  [22400/481450]\n",
      "loss: 0.101133  [25600/481450]\n",
      "loss: 0.163702  [28800/481450]\n",
      "loss: 0.371572  [32000/481450]\n",
      "loss: 0.158033  [35200/481450]\n",
      "loss: 0.501733  [38400/481450]\n",
      "loss: 0.478814  [41600/481450]\n",
      "loss: 0.248739  [44800/481450]\n",
      "loss: 0.303209  [48000/481450]\n",
      "loss: 0.215870  [51200/481450]\n",
      "loss: 0.326735  [54400/481450]\n",
      "loss: 0.272975  [57600/481450]\n",
      "loss: 0.298537  [60800/481450]\n",
      "loss: 0.070616  [64000/481450]\n",
      "loss: 0.263178  [67200/481450]\n",
      "loss: 0.307636  [70400/481450]\n",
      "loss: 0.272246  [73600/481450]\n",
      "loss: 0.170403  [76800/481450]\n",
      "loss: 0.220865  [80000/481450]\n",
      "loss: 0.193662  [83200/481450]\n",
      "loss: 0.286426  [86400/481450]\n",
      "loss: 0.356758  [89600/481450]\n",
      "loss: 0.162671  [92800/481450]\n",
      "loss: 0.534817  [96000/481450]\n",
      "loss: 0.360223  [99200/481450]\n",
      "loss: 0.192448  [102400/481450]\n",
      "loss: 0.277760  [105600/481450]\n",
      "loss: 0.335556  [108800/481450]\n",
      "loss: 0.166234  [112000/481450]\n",
      "loss: 0.163876  [115200/481450]\n",
      "loss: 0.322006  [118400/481450]\n",
      "loss: 0.253125  [121600/481450]\n",
      "loss: 0.246167  [124800/481450]\n",
      "loss: 0.294976  [128000/481450]\n",
      "loss: 0.196069  [131200/481450]\n",
      "loss: 0.228404  [134400/481450]\n",
      "loss: 0.256983  [137600/481450]\n",
      "loss: 0.226219  [140800/481450]\n",
      "loss: 0.317055  [144000/481450]\n",
      "loss: 0.335476  [147200/481450]\n",
      "loss: 0.363854  [150400/481450]\n",
      "loss: 0.372068  [153600/481450]\n",
      "loss: 0.237529  [156800/481450]\n",
      "loss: 0.282373  [160000/481450]\n",
      "loss: 0.098014  [163200/481450]\n",
      "loss: 0.239561  [166400/481450]\n",
      "loss: 0.489912  [169600/481450]\n",
      "loss: 0.187711  [172800/481450]\n",
      "loss: 0.221449  [176000/481450]\n",
      "loss: 0.300615  [179200/481450]\n",
      "loss: 0.463787  [182400/481450]\n",
      "loss: 0.263695  [185600/481450]\n",
      "loss: 0.574346  [188800/481450]\n",
      "loss: 0.197581  [192000/481450]\n",
      "loss: 0.432770  [195200/481450]\n",
      "loss: 0.318172  [198400/481450]\n",
      "loss: 0.213224  [201600/481450]\n",
      "loss: 0.354338  [204800/481450]\n",
      "loss: 0.196979  [208000/481450]\n",
      "loss: 0.149445  [211200/481450]\n",
      "loss: 0.298606  [214400/481450]\n",
      "loss: 0.270292  [217600/481450]\n",
      "loss: 0.207125  [220800/481450]\n",
      "loss: 0.268330  [224000/481450]\n",
      "loss: 0.392885  [227200/481450]\n",
      "loss: 0.301244  [230400/481450]\n",
      "loss: 0.314058  [233600/481450]\n",
      "loss: 0.180342  [236800/481450]\n",
      "loss: 0.353542  [240000/481450]\n",
      "loss: 0.255975  [243200/481450]\n",
      "loss: 0.272107  [246400/481450]\n",
      "loss: 0.327494  [249600/481450]\n",
      "loss: 0.190180  [252800/481450]\n",
      "loss: 0.307002  [256000/481450]\n",
      "loss: 0.280267  [259200/481450]\n",
      "loss: 0.233330  [262400/481450]\n",
      "loss: 0.420074  [265600/481450]\n",
      "loss: 0.340150  [268800/481450]\n",
      "loss: 0.265006  [272000/481450]\n",
      "loss: 0.291578  [275200/481450]\n",
      "loss: 0.257146  [278400/481450]\n",
      "loss: 0.173760  [281600/481450]\n",
      "loss: 0.227446  [284800/481450]\n",
      "loss: 0.666499  [288000/481450]\n",
      "loss: 0.467039  [291200/481450]\n",
      "loss: 0.367183  [294400/481450]\n",
      "loss: 0.307032  [297600/481450]\n",
      "loss: 0.287123  [300800/481450]\n",
      "loss: 0.313148  [304000/481450]\n",
      "loss: 0.192094  [307200/481450]\n",
      "loss: 0.382892  [310400/481450]\n",
      "loss: 0.534031  [313600/481450]\n",
      "loss: 0.229715  [316800/481450]\n",
      "loss: 0.427744  [320000/481450]\n",
      "loss: 0.212574  [323200/481450]\n",
      "loss: 0.408805  [326400/481450]\n",
      "loss: 0.212746  [329600/481450]\n",
      "loss: 0.190548  [332800/481450]\n",
      "loss: 0.405739  [336000/481450]\n",
      "loss: 0.478449  [339200/481450]\n",
      "loss: 0.352566  [342400/481450]\n",
      "loss: 0.207002  [345600/481450]\n",
      "loss: 0.116277  [348800/481450]\n",
      "loss: 0.486337  [352000/481450]\n",
      "loss: 0.107279  [355200/481450]\n",
      "loss: 0.471350  [358400/481450]\n",
      "loss: 0.289559  [361600/481450]\n",
      "loss: 0.193146  [364800/481450]\n",
      "loss: 0.046581  [368000/481450]\n",
      "loss: 0.196360  [371200/481450]\n",
      "loss: 0.193679  [374400/481450]\n",
      "loss: 0.308524  [377600/481450]\n",
      "loss: 0.169503  [380800/481450]\n",
      "loss: 0.584437  [384000/481450]\n",
      "loss: 0.337711  [387200/481450]\n",
      "loss: 0.319189  [390400/481450]\n",
      "loss: 0.123544  [393600/481450]\n",
      "loss: 0.261248  [396800/481450]\n",
      "loss: 0.197619  [400000/481450]\n",
      "loss: 0.191916  [403200/481450]\n",
      "loss: 0.267237  [406400/481450]\n",
      "loss: 0.271961  [409600/481450]\n",
      "loss: 0.140479  [412800/481450]\n",
      "loss: 0.119705  [416000/481450]\n",
      "loss: 0.149639  [419200/481450]\n",
      "loss: 0.092491  [422400/481450]\n",
      "loss: 0.222221  [425600/481450]\n",
      "loss: 0.236586  [428800/481450]\n",
      "loss: 0.231296  [432000/481450]\n",
      "loss: 0.357139  [435200/481450]\n",
      "loss: 0.234078  [438400/481450]\n",
      "loss: 0.341478  [441600/481450]\n",
      "loss: 0.176900  [444800/481450]\n",
      "loss: 0.258525  [448000/481450]\n",
      "loss: 0.333640  [451200/481450]\n",
      "loss: 0.448299  [454400/481450]\n",
      "loss: 0.272456  [457600/481450]\n",
      "loss: 0.057607  [460800/481450]\n",
      "loss: 0.220267  [464000/481450]\n",
      "loss: 0.237267  [467200/481450]\n",
      "loss: 0.159726  [470400/481450]\n",
      "loss: 0.426864  [473600/481450]\n",
      "loss: 0.314115  [476800/481450]\n",
      "loss: 0.375733  [480000/481450]\n",
      "Train Accuracy: 88.7139%\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.841931, F1-score: 83.65% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.090716  [    0/481450]\n",
      "loss: 0.187900  [ 3200/481450]\n",
      "loss: 0.251528  [ 6400/481450]\n",
      "loss: 0.151796  [ 9600/481450]\n",
      "loss: 0.194067  [12800/481450]\n",
      "loss: 0.348512  [16000/481450]\n",
      "loss: 0.269599  [19200/481450]\n",
      "loss: 0.180058  [22400/481450]\n",
      "loss: 0.290940  [25600/481450]\n",
      "loss: 0.311301  [28800/481450]\n",
      "loss: 0.209445  [32000/481450]\n",
      "loss: 0.117623  [35200/481450]\n",
      "loss: 0.219297  [38400/481450]\n",
      "loss: 0.110509  [41600/481450]\n",
      "loss: 0.178695  [44800/481450]\n",
      "loss: 0.403911  [48000/481450]\n",
      "loss: 0.410592  [51200/481450]\n",
      "loss: 0.366055  [54400/481450]\n",
      "loss: 0.281659  [57600/481450]\n",
      "loss: 0.257625  [60800/481450]\n",
      "loss: 0.432802  [64000/481450]\n",
      "loss: 0.253833  [67200/481450]\n",
      "loss: 0.292274  [70400/481450]\n",
      "loss: 0.134803  [73600/481450]\n",
      "loss: 0.400396  [76800/481450]\n",
      "loss: 0.378931  [80000/481450]\n",
      "loss: 0.489001  [83200/481450]\n",
      "loss: 0.202834  [86400/481450]\n",
      "loss: 0.400967  [89600/481450]\n",
      "loss: 0.324774  [92800/481450]\n",
      "loss: 0.235229  [96000/481450]\n",
      "loss: 0.111248  [99200/481450]\n",
      "loss: 0.450283  [102400/481450]\n",
      "loss: 0.311730  [105600/481450]\n",
      "loss: 0.255213  [108800/481450]\n",
      "loss: 0.231105  [112000/481450]\n",
      "loss: 0.116040  [115200/481450]\n",
      "loss: 0.155263  [118400/481450]\n",
      "loss: 0.344555  [121600/481450]\n",
      "loss: 0.275043  [124800/481450]\n",
      "loss: 0.239857  [128000/481450]\n",
      "loss: 0.539247  [131200/481450]\n",
      "loss: 0.144974  [134400/481450]\n",
      "loss: 0.556034  [137600/481450]\n",
      "loss: 0.185979  [140800/481450]\n",
      "loss: 0.261467  [144000/481450]\n",
      "loss: 0.073315  [147200/481450]\n",
      "loss: 0.364827  [150400/481450]\n",
      "loss: 0.288023  [153600/481450]\n",
      "loss: 0.362954  [156800/481450]\n",
      "loss: 0.219818  [160000/481450]\n",
      "loss: 0.355441  [163200/481450]\n",
      "loss: 0.381723  [166400/481450]\n",
      "loss: 0.289901  [169600/481450]\n",
      "loss: 0.327546  [172800/481450]\n",
      "loss: 0.235349  [176000/481450]\n",
      "loss: 0.151790  [179200/481450]\n",
      "loss: 0.450365  [182400/481450]\n",
      "loss: 0.312548  [185600/481450]\n",
      "loss: 0.277761  [188800/481450]\n",
      "loss: 0.253125  [192000/481450]\n",
      "loss: 0.142892  [195200/481450]\n",
      "loss: 0.082443  [198400/481450]\n",
      "loss: 0.158401  [201600/481450]\n",
      "loss: 0.385154  [204800/481450]\n",
      "loss: 0.180405  [208000/481450]\n",
      "loss: 0.236066  [211200/481450]\n",
      "loss: 0.462840  [214400/481450]\n",
      "loss: 0.343126  [217600/481450]\n",
      "loss: 0.295828  [220800/481450]\n",
      "loss: 0.110404  [224000/481450]\n",
      "loss: 0.399525  [227200/481450]\n",
      "loss: 0.250710  [230400/481450]\n",
      "loss: 0.213930  [233600/481450]\n",
      "loss: 0.239344  [236800/481450]\n",
      "loss: 0.357594  [240000/481450]\n",
      "loss: 0.137947  [243200/481450]\n",
      "loss: 0.146846  [246400/481450]\n",
      "loss: 0.149002  [249600/481450]\n",
      "loss: 0.053622  [252800/481450]\n",
      "loss: 0.390444  [256000/481450]\n",
      "loss: 0.355196  [259200/481450]\n",
      "loss: 0.215443  [262400/481450]\n",
      "loss: 0.124900  [265600/481450]\n",
      "loss: 0.400441  [268800/481450]\n",
      "loss: 0.165463  [272000/481450]\n",
      "loss: 0.303652  [275200/481450]\n",
      "loss: 0.221794  [278400/481450]\n",
      "loss: 0.563121  [281600/481450]\n",
      "loss: 0.327119  [284800/481450]\n",
      "loss: 0.410849  [288000/481450]\n",
      "loss: 0.316092  [291200/481450]\n",
      "loss: 0.433305  [294400/481450]\n",
      "loss: 0.300378  [297600/481450]\n",
      "loss: 0.206841  [300800/481450]\n",
      "loss: 0.104496  [304000/481450]\n",
      "loss: 0.299767  [307200/481450]\n",
      "loss: 0.324508  [310400/481450]\n",
      "loss: 0.196771  [313600/481450]\n",
      "loss: 0.368021  [316800/481450]\n",
      "loss: 0.360693  [320000/481450]\n",
      "loss: 0.349146  [323200/481450]\n",
      "loss: 0.442516  [326400/481450]\n",
      "loss: 0.288998  [329600/481450]\n",
      "loss: 0.130399  [332800/481450]\n",
      "loss: 0.217343  [336000/481450]\n",
      "loss: 0.245125  [339200/481450]\n",
      "loss: 0.214231  [342400/481450]\n",
      "loss: 0.158270  [345600/481450]\n",
      "loss: 0.307955  [348800/481450]\n",
      "loss: 0.279860  [352000/481450]\n",
      "loss: 0.093746  [355200/481450]\n",
      "loss: 0.216795  [358400/481450]\n",
      "loss: 0.272546  [361600/481450]\n",
      "loss: 0.220367  [364800/481450]\n",
      "loss: 0.281958  [368000/481450]\n",
      "loss: 0.242428  [371200/481450]\n",
      "loss: 0.202413  [374400/481450]\n",
      "loss: 0.200383  [377600/481450]\n",
      "loss: 0.366620  [380800/481450]\n",
      "loss: 0.248499  [384000/481450]\n",
      "loss: 0.467959  [387200/481450]\n",
      "loss: 0.106490  [390400/481450]\n",
      "loss: 0.469560  [393600/481450]\n",
      "loss: 0.470573  [396800/481450]\n",
      "loss: 0.232302  [400000/481450]\n",
      "loss: 0.535819  [403200/481450]\n",
      "loss: 0.307215  [406400/481450]\n",
      "loss: 0.266064  [409600/481450]\n",
      "loss: 0.250360  [412800/481450]\n",
      "loss: 0.237940  [416000/481450]\n",
      "loss: 0.188357  [419200/481450]\n",
      "loss: 0.183805  [422400/481450]\n",
      "loss: 0.306633  [425600/481450]\n",
      "loss: 0.438836  [428800/481450]\n",
      "loss: 0.238265  [432000/481450]\n",
      "loss: 0.437386  [435200/481450]\n",
      "loss: 0.278258  [438400/481450]\n",
      "loss: 0.213132  [441600/481450]\n",
      "loss: 0.264277  [444800/481450]\n",
      "loss: 0.155455  [448000/481450]\n",
      "loss: 0.348289  [451200/481450]\n",
      "loss: 0.196396  [454400/481450]\n",
      "loss: 0.293321  [457600/481450]\n",
      "loss: 0.297256  [460800/481450]\n",
      "loss: 0.201303  [464000/481450]\n",
      "loss: 0.296146  [467200/481450]\n",
      "loss: 0.184090  [470400/481450]\n",
      "loss: 0.204369  [473600/481450]\n",
      "loss: 0.294745  [476800/481450]\n",
      "loss: 0.239878  [480000/481450]\n",
      "Train Accuracy: 88.8973%\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.845474, F1-score: 83.44% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.207347  [    0/481450]\n",
      "loss: 0.395004  [ 3200/481450]\n",
      "loss: 0.398434  [ 6400/481450]\n",
      "loss: 0.145629  [ 9600/481450]\n",
      "loss: 0.164222  [12800/481450]\n",
      "loss: 0.101131  [16000/481450]\n",
      "loss: 0.272649  [19200/481450]\n",
      "loss: 0.181153  [22400/481450]\n",
      "loss: 0.130215  [25600/481450]\n",
      "loss: 0.113769  [28800/481450]\n",
      "loss: 0.151279  [32000/481450]\n",
      "loss: 0.200232  [35200/481450]\n",
      "loss: 0.119722  [38400/481450]\n",
      "loss: 0.287066  [41600/481450]\n",
      "loss: 0.358984  [44800/481450]\n",
      "loss: 0.129240  [48000/481450]\n",
      "loss: 0.276531  [51200/481450]\n",
      "loss: 0.230379  [54400/481450]\n",
      "loss: 0.314709  [57600/481450]\n",
      "loss: 0.256552  [60800/481450]\n",
      "loss: 0.170958  [64000/481450]\n",
      "loss: 0.157699  [67200/481450]\n",
      "loss: 0.434228  [70400/481450]\n",
      "loss: 0.306986  [73600/481450]\n",
      "loss: 0.250225  [76800/481450]\n",
      "loss: 0.267891  [80000/481450]\n",
      "loss: 0.236077  [83200/481450]\n",
      "loss: 0.262517  [86400/481450]\n",
      "loss: 0.092527  [89600/481450]\n",
      "loss: 0.248561  [92800/481450]\n",
      "loss: 0.297663  [96000/481450]\n",
      "loss: 0.205752  [99200/481450]\n",
      "loss: 0.274364  [102400/481450]\n",
      "loss: 0.138021  [105600/481450]\n",
      "loss: 0.185147  [108800/481450]\n",
      "loss: 0.186641  [112000/481450]\n",
      "loss: 0.220456  [115200/481450]\n",
      "loss: 0.260507  [118400/481450]\n",
      "loss: 0.256045  [121600/481450]\n",
      "loss: 0.060885  [124800/481450]\n",
      "loss: 0.438441  [128000/481450]\n",
      "loss: 0.317299  [131200/481450]\n",
      "loss: 0.141701  [134400/481450]\n",
      "loss: 0.344727  [137600/481450]\n",
      "loss: 0.281689  [140800/481450]\n",
      "loss: 0.114075  [144000/481450]\n",
      "loss: 0.348541  [147200/481450]\n",
      "loss: 0.271843  [150400/481450]\n",
      "loss: 0.255180  [153600/481450]\n",
      "loss: 0.298940  [156800/481450]\n",
      "loss: 0.387748  [160000/481450]\n",
      "loss: 0.163998  [163200/481450]\n",
      "loss: 0.208055  [166400/481450]\n",
      "loss: 0.292125  [169600/481450]\n",
      "loss: 0.443180  [172800/481450]\n",
      "loss: 0.505044  [176000/481450]\n",
      "loss: 0.047718  [179200/481450]\n",
      "loss: 0.149958  [182400/481450]\n",
      "loss: 0.338545  [185600/481450]\n",
      "loss: 0.132025  [188800/481450]\n",
      "loss: 0.323609  [192000/481450]\n",
      "loss: 0.146740  [195200/481450]\n",
      "loss: 0.111597  [198400/481450]\n",
      "loss: 0.374859  [201600/481450]\n",
      "loss: 0.314577  [204800/481450]\n",
      "loss: 0.173140  [208000/481450]\n",
      "loss: 0.245438  [211200/481450]\n",
      "loss: 0.341961  [214400/481450]\n",
      "loss: 0.175943  [217600/481450]\n",
      "loss: 0.187378  [220800/481450]\n",
      "loss: 0.225391  [224000/481450]\n",
      "loss: 0.364490  [227200/481450]\n",
      "loss: 0.470996  [230400/481450]\n",
      "loss: 0.175802  [233600/481450]\n",
      "loss: 0.307061  [236800/481450]\n",
      "loss: 0.309030  [240000/481450]\n",
      "loss: 0.291167  [243200/481450]\n",
      "loss: 0.209101  [246400/481450]\n",
      "loss: 0.284721  [249600/481450]\n",
      "loss: 0.150007  [252800/481450]\n",
      "loss: 0.338656  [256000/481450]\n",
      "loss: 0.165893  [259200/481450]\n",
      "loss: 0.292196  [262400/481450]\n",
      "loss: 0.231956  [265600/481450]\n",
      "loss: 0.350105  [268800/481450]\n",
      "loss: 0.237483  [272000/481450]\n",
      "loss: 0.200889  [275200/481450]\n",
      "loss: 0.473004  [278400/481450]\n",
      "loss: 0.378839  [281600/481450]\n",
      "loss: 0.331323  [284800/481450]\n",
      "loss: 0.371372  [288000/481450]\n",
      "loss: 0.205545  [291200/481450]\n",
      "loss: 0.316822  [294400/481450]\n",
      "loss: 0.200808  [297600/481450]\n",
      "loss: 0.367725  [300800/481450]\n",
      "loss: 0.154533  [304000/481450]\n",
      "loss: 0.606861  [307200/481450]\n",
      "loss: 0.209998  [310400/481450]\n",
      "loss: 0.167105  [313600/481450]\n",
      "loss: 0.467967  [316800/481450]\n",
      "loss: 0.160302  [320000/481450]\n",
      "loss: 0.249032  [323200/481450]\n",
      "loss: 0.195518  [326400/481450]\n",
      "loss: 0.185120  [329600/481450]\n",
      "loss: 0.420992  [332800/481450]\n",
      "loss: 0.269057  [336000/481450]\n",
      "loss: 0.382325  [339200/481450]\n",
      "loss: 0.588071  [342400/481450]\n",
      "loss: 0.228074  [345600/481450]\n",
      "loss: 0.365401  [348800/481450]\n",
      "loss: 0.307650  [352000/481450]\n",
      "loss: 0.185283  [355200/481450]\n",
      "loss: 0.106322  [358400/481450]\n",
      "loss: 0.246884  [361600/481450]\n",
      "loss: 0.432767  [364800/481450]\n",
      "loss: 0.280109  [368000/481450]\n",
      "loss: 0.266514  [371200/481450]\n",
      "loss: 0.245778  [374400/481450]\n",
      "loss: 0.572144  [377600/481450]\n",
      "loss: 0.346311  [380800/481450]\n",
      "loss: 0.377256  [384000/481450]\n",
      "loss: 0.225918  [387200/481450]\n",
      "loss: 0.398236  [390400/481450]\n",
      "loss: 0.388441  [393600/481450]\n",
      "loss: 0.317304  [396800/481450]\n",
      "loss: 0.197446  [400000/481450]\n",
      "loss: 0.176069  [403200/481450]\n",
      "loss: 0.144275  [406400/481450]\n",
      "loss: 0.168287  [409600/481450]\n",
      "loss: 0.255620  [412800/481450]\n",
      "loss: 0.233557  [416000/481450]\n",
      "loss: 0.142752  [419200/481450]\n",
      "loss: 0.105755  [422400/481450]\n",
      "loss: 0.410454  [425600/481450]\n",
      "loss: 0.185635  [428800/481450]\n",
      "loss: 0.177601  [432000/481450]\n",
      "loss: 0.146650  [435200/481450]\n",
      "loss: 0.266679  [438400/481450]\n",
      "loss: 0.202788  [441600/481450]\n",
      "loss: 0.295343  [444800/481450]\n",
      "loss: 0.077589  [448000/481450]\n",
      "loss: 0.542917  [451200/481450]\n",
      "loss: 0.250118  [454400/481450]\n",
      "loss: 0.320731  [457600/481450]\n",
      "loss: 0.065248  [460800/481450]\n",
      "loss: 0.279739  [464000/481450]\n",
      "loss: 0.271470  [467200/481450]\n",
      "loss: 0.251485  [470400/481450]\n",
      "loss: 0.214186  [473600/481450]\n",
      "loss: 0.169512  [476800/481450]\n",
      "loss: 0.218964  [480000/481450]\n",
      "Train Accuracy: 89.0647%\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 1.012203, F1-score: 81.99% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.222312  [    0/481450]\n",
      "loss: 0.158334  [ 3200/481450]\n",
      "loss: 0.453450  [ 6400/481450]\n",
      "loss: 0.221788  [ 9600/481450]\n",
      "loss: 0.170892  [12800/481450]\n",
      "loss: 0.347131  [16000/481450]\n",
      "loss: 0.423991  [19200/481450]\n",
      "loss: 0.189018  [22400/481450]\n",
      "loss: 0.358343  [25600/481450]\n",
      "loss: 0.457758  [28800/481450]\n",
      "loss: 0.205771  [32000/481450]\n",
      "loss: 0.448963  [35200/481450]\n",
      "loss: 0.234933  [38400/481450]\n",
      "loss: 0.217584  [41600/481450]\n",
      "loss: 0.158740  [44800/481450]\n",
      "loss: 0.029259  [48000/481450]\n",
      "loss: 0.232384  [51200/481450]\n",
      "loss: 0.275685  [54400/481450]\n",
      "loss: 0.478907  [57600/481450]\n",
      "loss: 0.407141  [60800/481450]\n",
      "loss: 0.260987  [64000/481450]\n",
      "loss: 0.214941  [67200/481450]\n",
      "loss: 0.425761  [70400/481450]\n",
      "loss: 0.100932  [73600/481450]\n",
      "loss: 0.227371  [76800/481450]\n",
      "loss: 0.302450  [80000/481450]\n",
      "loss: 0.119159  [83200/481450]\n",
      "loss: 0.155241  [86400/481450]\n",
      "loss: 0.158131  [89600/481450]\n",
      "loss: 0.229445  [92800/481450]\n",
      "loss: 0.461768  [96000/481450]\n",
      "loss: 0.194163  [99200/481450]\n",
      "loss: 0.424487  [102400/481450]\n",
      "loss: 0.176796  [105600/481450]\n",
      "loss: 0.163926  [108800/481450]\n",
      "loss: 0.525061  [112000/481450]\n",
      "loss: 0.329186  [115200/481450]\n",
      "loss: 0.105665  [118400/481450]\n",
      "loss: 0.233444  [121600/481450]\n",
      "loss: 0.426899  [124800/481450]\n",
      "loss: 0.074696  [128000/481450]\n",
      "loss: 0.706970  [131200/481450]\n",
      "loss: 0.260470  [134400/481450]\n",
      "loss: 0.192008  [137600/481450]\n",
      "loss: 0.225015  [140800/481450]\n",
      "loss: 0.329060  [144000/481450]\n",
      "loss: 0.177381  [147200/481450]\n",
      "loss: 0.226892  [150400/481450]\n",
      "loss: 0.471016  [153600/481450]\n",
      "loss: 0.260305  [156800/481450]\n",
      "loss: 0.266896  [160000/481450]\n",
      "loss: 0.274861  [163200/481450]\n",
      "loss: 0.274557  [166400/481450]\n",
      "loss: 0.270364  [169600/481450]\n",
      "loss: 0.231943  [172800/481450]\n",
      "loss: 0.330699  [176000/481450]\n",
      "loss: 0.107174  [179200/481450]\n",
      "loss: 0.311667  [182400/481450]\n",
      "loss: 0.235153  [185600/481450]\n",
      "loss: 0.210230  [188800/481450]\n",
      "loss: 0.037643  [192000/481450]\n",
      "loss: 0.358244  [195200/481450]\n",
      "loss: 0.184958  [198400/481450]\n",
      "loss: 0.323540  [201600/481450]\n",
      "loss: 0.239003  [204800/481450]\n",
      "loss: 0.204256  [208000/481450]\n",
      "loss: 0.220203  [211200/481450]\n",
      "loss: 0.338185  [214400/481450]\n",
      "loss: 0.366407  [217600/481450]\n",
      "loss: 0.166860  [220800/481450]\n",
      "loss: 0.222930  [224000/481450]\n",
      "loss: 0.396659  [227200/481450]\n",
      "loss: 0.484183  [230400/481450]\n",
      "loss: 0.308224  [233600/481450]\n",
      "loss: 0.181764  [236800/481450]\n",
      "loss: 0.173644  [240000/481450]\n",
      "loss: 0.135488  [243200/481450]\n",
      "loss: 0.265590  [246400/481450]\n",
      "loss: 0.383134  [249600/481450]\n",
      "loss: 0.137941  [252800/481450]\n",
      "loss: 0.349710  [256000/481450]\n",
      "loss: 0.145713  [259200/481450]\n",
      "loss: 0.326304  [262400/481450]\n",
      "loss: 0.447749  [265600/481450]\n",
      "loss: 0.255021  [268800/481450]\n",
      "loss: 0.241298  [272000/481450]\n",
      "loss: 0.124709  [275200/481450]\n",
      "loss: 0.152582  [278400/481450]\n",
      "loss: 0.110490  [281600/481450]\n",
      "loss: 0.198560  [284800/481450]\n",
      "loss: 0.599560  [288000/481450]\n",
      "loss: 0.285688  [291200/481450]\n",
      "loss: 0.333801  [294400/481450]\n",
      "loss: 0.093679  [297600/481450]\n",
      "loss: 0.279410  [300800/481450]\n",
      "loss: 0.443746  [304000/481450]\n",
      "loss: 0.256443  [307200/481450]\n",
      "loss: 0.222470  [310400/481450]\n",
      "loss: 0.350088  [313600/481450]\n",
      "loss: 0.159186  [316800/481450]\n",
      "loss: 0.220674  [320000/481450]\n",
      "loss: 0.394989  [323200/481450]\n",
      "loss: 0.282659  [326400/481450]\n",
      "loss: 0.161538  [329600/481450]\n",
      "loss: 0.310852  [332800/481450]\n",
      "loss: 0.293299  [336000/481450]\n",
      "loss: 0.072950  [339200/481450]\n",
      "loss: 0.356085  [342400/481450]\n",
      "loss: 0.240457  [345600/481450]\n",
      "loss: 0.222873  [348800/481450]\n",
      "loss: 0.281363  [352000/481450]\n",
      "loss: 0.187168  [355200/481450]\n",
      "loss: 0.180079  [358400/481450]\n",
      "loss: 0.189123  [361600/481450]\n",
      "loss: 0.603109  [364800/481450]\n",
      "loss: 0.135563  [368000/481450]\n",
      "loss: 0.294611  [371200/481450]\n",
      "loss: 0.397381  [374400/481450]\n",
      "loss: 0.457494  [377600/481450]\n",
      "loss: 0.421603  [380800/481450]\n",
      "loss: 0.330648  [384000/481450]\n",
      "loss: 0.421901  [387200/481450]\n",
      "loss: 0.153297  [390400/481450]\n",
      "loss: 0.250428  [393600/481450]\n",
      "loss: 0.253138  [396800/481450]\n",
      "loss: 0.124435  [400000/481450]\n",
      "loss: 0.192725  [403200/481450]\n",
      "loss: 0.266787  [406400/481450]\n",
      "loss: 0.198147  [409600/481450]\n",
      "loss: 0.084164  [412800/481450]\n",
      "loss: 0.182614  [416000/481450]\n",
      "loss: 0.272681  [419200/481450]\n",
      "loss: 0.052304  [422400/481450]\n",
      "loss: 0.226054  [425600/481450]\n",
      "loss: 0.404864  [428800/481450]\n",
      "loss: 0.323614  [432000/481450]\n",
      "loss: 0.260524  [435200/481450]\n",
      "loss: 0.460764  [438400/481450]\n",
      "loss: 0.216725  [441600/481450]\n",
      "loss: 0.348142  [444800/481450]\n",
      "loss: 0.482045  [448000/481450]\n",
      "loss: 0.276621  [451200/481450]\n",
      "loss: 0.309578  [454400/481450]\n",
      "loss: 0.128676  [457600/481450]\n",
      "loss: 0.345335  [460800/481450]\n",
      "loss: 0.119491  [464000/481450]\n",
      "loss: 0.300517  [467200/481450]\n",
      "loss: 0.157967  [470400/481450]\n",
      "loss: 0.131975  [473600/481450]\n",
      "loss: 0.185865  [476800/481450]\n",
      "loss: 0.284372  [480000/481450]\n",
      "Train Accuracy: 89.2703%\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.972039, F1-score: 83.19% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.158529  [    0/481450]\n",
      "loss: 0.161080  [ 3200/481450]\n",
      "loss: 0.426777  [ 6400/481450]\n",
      "loss: 0.255369  [ 9600/481450]\n",
      "loss: 0.135004  [12800/481450]\n",
      "loss: 0.368821  [16000/481450]\n",
      "loss: 0.128625  [19200/481450]\n",
      "loss: 0.296972  [22400/481450]\n",
      "loss: 0.533509  [25600/481450]\n",
      "loss: 0.158693  [28800/481450]\n",
      "loss: 0.257913  [32000/481450]\n",
      "loss: 0.289223  [35200/481450]\n",
      "loss: 0.261086  [38400/481450]\n",
      "loss: 0.156167  [41600/481450]\n",
      "loss: 0.075663  [44800/481450]\n",
      "loss: 0.335727  [48000/481450]\n",
      "loss: 0.073397  [51200/481450]\n",
      "loss: 0.265686  [54400/481450]\n",
      "loss: 0.139397  [57600/481450]\n",
      "loss: 0.268948  [60800/481450]\n",
      "loss: 0.153422  [64000/481450]\n",
      "loss: 0.389914  [67200/481450]\n",
      "loss: 0.607349  [70400/481450]\n",
      "loss: 0.182592  [73600/481450]\n",
      "loss: 0.103660  [76800/481450]\n",
      "loss: 0.436422  [80000/481450]\n",
      "loss: 0.135069  [83200/481450]\n",
      "loss: 0.186620  [86400/481450]\n",
      "loss: 0.166823  [89600/481450]\n",
      "loss: 0.197075  [92800/481450]\n",
      "loss: 0.413842  [96000/481450]\n",
      "loss: 0.350320  [99200/481450]\n",
      "loss: 0.241091  [102400/481450]\n",
      "loss: 0.254306  [105600/481450]\n",
      "loss: 0.261810  [108800/481450]\n",
      "loss: 0.239858  [112000/481450]\n",
      "loss: 0.443927  [115200/481450]\n",
      "loss: 0.303524  [118400/481450]\n",
      "loss: 0.417834  [121600/481450]\n",
      "loss: 0.315514  [124800/481450]\n",
      "loss: 0.310612  [128000/481450]\n",
      "loss: 0.433198  [131200/481450]\n",
      "loss: 0.354068  [134400/481450]\n",
      "loss: 0.236323  [137600/481450]\n",
      "loss: 0.359887  [140800/481450]\n",
      "loss: 0.348747  [144000/481450]\n",
      "loss: 0.157563  [147200/481450]\n",
      "loss: 0.422103  [150400/481450]\n",
      "loss: 0.136228  [153600/481450]\n",
      "loss: 0.430720  [156800/481450]\n",
      "loss: 0.205940  [160000/481450]\n",
      "loss: 0.238926  [163200/481450]\n",
      "loss: 0.365227  [166400/481450]\n",
      "loss: 0.175999  [169600/481450]\n",
      "loss: 0.229696  [172800/481450]\n",
      "loss: 0.442350  [176000/481450]\n",
      "loss: 0.475554  [179200/481450]\n",
      "loss: 0.301120  [182400/481450]\n",
      "loss: 0.098896  [185600/481450]\n",
      "loss: 0.531704  [188800/481450]\n",
      "loss: 0.263490  [192000/481450]\n",
      "loss: 0.548691  [195200/481450]\n",
      "loss: 0.350287  [198400/481450]\n",
      "loss: 0.343318  [201600/481450]\n",
      "loss: 0.186382  [204800/481450]\n",
      "loss: 0.293545  [208000/481450]\n",
      "loss: 0.207812  [211200/481450]\n",
      "loss: 0.165895  [214400/481450]\n",
      "loss: 0.067269  [217600/481450]\n",
      "loss: 0.139483  [220800/481450]\n",
      "loss: 0.400761  [224000/481450]\n",
      "loss: 0.396305  [227200/481450]\n",
      "loss: 0.445556  [230400/481450]\n",
      "loss: 0.572113  [233600/481450]\n",
      "loss: 0.146663  [236800/481450]\n",
      "loss: 0.453253  [240000/481450]\n",
      "loss: 0.316457  [243200/481450]\n",
      "loss: 0.185728  [246400/481450]\n",
      "loss: 0.249502  [249600/481450]\n",
      "loss: 0.278673  [252800/481450]\n",
      "loss: 0.347953  [256000/481450]\n",
      "loss: 0.412747  [259200/481450]\n",
      "loss: 0.093979  [262400/481450]\n",
      "loss: 0.181308  [265600/481450]\n",
      "loss: 0.083652  [268800/481450]\n",
      "loss: 0.113610  [272000/481450]\n",
      "loss: 0.148934  [275200/481450]\n",
      "loss: 0.381703  [278400/481450]\n",
      "loss: 0.141818  [281600/481450]\n",
      "loss: 0.243327  [284800/481450]\n",
      "loss: 0.144975  [288000/481450]\n",
      "loss: 0.239004  [291200/481450]\n",
      "loss: 0.244424  [294400/481450]\n",
      "loss: 0.340123  [297600/481450]\n",
      "loss: 0.429485  [300800/481450]\n",
      "loss: 0.080111  [304000/481450]\n",
      "loss: 0.434062  [307200/481450]\n",
      "loss: 0.376479  [310400/481450]\n",
      "loss: 0.436463  [313600/481450]\n",
      "loss: 0.531430  [316800/481450]\n",
      "loss: 0.247111  [320000/481450]\n",
      "loss: 0.252397  [323200/481450]\n",
      "loss: 0.300943  [326400/481450]\n",
      "loss: 0.075179  [329600/481450]\n",
      "loss: 0.324634  [332800/481450]\n",
      "loss: 0.015293  [336000/481450]\n",
      "loss: 0.109186  [339200/481450]\n",
      "loss: 0.328840  [342400/481450]\n",
      "loss: 0.226443  [345600/481450]\n",
      "loss: 0.209183  [348800/481450]\n",
      "loss: 0.311104  [352000/481450]\n",
      "loss: 0.203098  [355200/481450]\n",
      "loss: 0.308207  [358400/481450]\n",
      "loss: 0.257091  [361600/481450]\n",
      "loss: 0.258565  [364800/481450]\n",
      "loss: 0.342562  [368000/481450]\n",
      "loss: 0.234499  [371200/481450]\n",
      "loss: 0.114552  [374400/481450]\n",
      "loss: 0.257111  [377600/481450]\n",
      "loss: 0.306855  [380800/481450]\n",
      "loss: 0.166722  [384000/481450]\n",
      "loss: 0.076117  [387200/481450]\n",
      "loss: 0.707705  [390400/481450]\n",
      "loss: 0.395798  [393600/481450]\n",
      "loss: 0.449848  [396800/481450]\n",
      "loss: 0.433027  [400000/481450]\n",
      "loss: 0.227658  [403200/481450]\n",
      "loss: 0.226686  [406400/481450]\n",
      "loss: 0.166475  [409600/481450]\n",
      "loss: 0.112394  [412800/481450]\n",
      "loss: 0.179634  [416000/481450]\n",
      "loss: 0.339852  [419200/481450]\n",
      "loss: 0.280183  [422400/481450]\n",
      "loss: 0.280544  [425600/481450]\n",
      "loss: 0.213414  [428800/481450]\n",
      "loss: 0.150180  [432000/481450]\n",
      "loss: 0.159845  [435200/481450]\n",
      "loss: 0.314096  [438400/481450]\n",
      "loss: 0.158996  [441600/481450]\n",
      "loss: 0.190860  [444800/481450]\n",
      "loss: 0.151472  [448000/481450]\n",
      "loss: 0.276212  [451200/481450]\n",
      "loss: 0.194762  [454400/481450]\n",
      "loss: 0.249449  [457600/481450]\n",
      "loss: 0.365374  [460800/481450]\n",
      "loss: 0.355856  [464000/481450]\n",
      "loss: 0.164278  [467200/481450]\n",
      "loss: 0.380978  [470400/481450]\n",
      "loss: 0.342393  [473600/481450]\n",
      "loss: 0.188582  [476800/481450]\n",
      "loss: 0.193723  [480000/481450]\n",
      "Train Accuracy: 89.3769%\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 1.087928, F1-score: 81.92% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a37a33c4-8087-4b5c-80cf-c10334d2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 15,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ada289ca-73cc-4ac6-8f1c-89c10d83b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301274  [    0/481450]\n",
      "loss: 2.301294  [ 3200/481450]\n",
      "loss: 2.304498  [ 6400/481450]\n",
      "loss: 2.282434  [ 9600/481450]\n",
      "loss: 2.293563  [12800/481450]\n",
      "loss: 2.281860  [16000/481450]\n",
      "loss: 2.257150  [19200/481450]\n",
      "loss: 2.191207  [22400/481450]\n",
      "loss: 2.114937  [25600/481450]\n",
      "loss: 2.208935  [28800/481450]\n",
      "loss: 2.202476  [32000/481450]\n",
      "loss: 2.015805  [35200/481450]\n",
      "loss: 1.955566  [38400/481450]\n",
      "loss: 2.077241  [41600/481450]\n",
      "loss: 1.912466  [44800/481450]\n",
      "loss: 1.915923  [48000/481450]\n",
      "loss: 1.718047  [51200/481450]\n",
      "loss: 1.702236  [54400/481450]\n",
      "loss: 1.732891  [57600/481450]\n",
      "loss: 1.761831  [60800/481450]\n",
      "loss: 1.929140  [64000/481450]\n",
      "loss: 1.679223  [67200/481450]\n",
      "loss: 1.805556  [70400/481450]\n",
      "loss: 1.505187  [73600/481450]\n",
      "loss: 1.397231  [76800/481450]\n",
      "loss: 1.350367  [80000/481450]\n",
      "loss: 1.515566  [83200/481450]\n",
      "loss: 1.460700  [86400/481450]\n",
      "loss: 1.365919  [89600/481450]\n",
      "loss: 1.382560  [92800/481450]\n",
      "loss: 1.491446  [96000/481450]\n",
      "loss: 1.400762  [99200/481450]\n",
      "loss: 1.309344  [102400/481450]\n",
      "loss: 1.383598  [105600/481450]\n",
      "loss: 1.418424  [108800/481450]\n",
      "loss: 1.407600  [112000/481450]\n",
      "loss: 1.456261  [115200/481450]\n",
      "loss: 1.492739  [118400/481450]\n",
      "loss: 1.293789  [121600/481450]\n",
      "loss: 1.349197  [124800/481450]\n",
      "loss: 1.273367  [128000/481450]\n",
      "loss: 1.079574  [131200/481450]\n",
      "loss: 1.305293  [134400/481450]\n",
      "loss: 0.992954  [137600/481450]\n",
      "loss: 1.170173  [140800/481450]\n",
      "loss: 1.313309  [144000/481450]\n",
      "loss: 1.171298  [147200/481450]\n",
      "loss: 1.173410  [150400/481450]\n",
      "loss: 1.172598  [153600/481450]\n",
      "loss: 1.048985  [156800/481450]\n",
      "loss: 0.960430  [160000/481450]\n",
      "loss: 0.820628  [163200/481450]\n",
      "loss: 1.199270  [166400/481450]\n",
      "loss: 1.015558  [169600/481450]\n",
      "loss: 0.936016  [172800/481450]\n",
      "loss: 1.121892  [176000/481450]\n",
      "loss: 1.004682  [179200/481450]\n",
      "loss: 1.342807  [182400/481450]\n",
      "loss: 1.177406  [185600/481450]\n",
      "loss: 1.321299  [188800/481450]\n",
      "loss: 1.189105  [192000/481450]\n",
      "loss: 1.184116  [195200/481450]\n",
      "loss: 1.107222  [198400/481450]\n",
      "loss: 1.072215  [201600/481450]\n",
      "loss: 0.987947  [204800/481450]\n",
      "loss: 0.930279  [208000/481450]\n",
      "loss: 0.815905  [211200/481450]\n",
      "loss: 1.191824  [214400/481450]\n",
      "loss: 0.933697  [217600/481450]\n",
      "loss: 0.868134  [220800/481450]\n",
      "loss: 0.857519  [224000/481450]\n",
      "loss: 1.090598  [227200/481450]\n",
      "loss: 0.882806  [230400/481450]\n",
      "loss: 0.869877  [233600/481450]\n",
      "loss: 0.816996  [236800/481450]\n",
      "loss: 0.691638  [240000/481450]\n",
      "loss: 0.681204  [243200/481450]\n",
      "loss: 0.842011  [246400/481450]\n",
      "loss: 0.866472  [249600/481450]\n",
      "loss: 0.786009  [252800/481450]\n",
      "loss: 0.882952  [256000/481450]\n",
      "loss: 0.896226  [259200/481450]\n",
      "loss: 0.875926  [262400/481450]\n",
      "loss: 0.784560  [265600/481450]\n",
      "loss: 0.961161  [268800/481450]\n",
      "loss: 0.887267  [272000/481450]\n",
      "loss: 0.734422  [275200/481450]\n",
      "loss: 0.775591  [278400/481450]\n",
      "loss: 0.894979  [281600/481450]\n",
      "loss: 0.804137  [284800/481450]\n",
      "loss: 0.706074  [288000/481450]\n",
      "loss: 0.674842  [291200/481450]\n",
      "loss: 0.709442  [294400/481450]\n",
      "loss: 0.825608  [297600/481450]\n",
      "loss: 0.804832  [300800/481450]\n",
      "loss: 0.632242  [304000/481450]\n",
      "loss: 0.837860  [307200/481450]\n",
      "loss: 0.548442  [310400/481450]\n",
      "loss: 0.625988  [313600/481450]\n",
      "loss: 0.579767  [316800/481450]\n",
      "loss: 0.698697  [320000/481450]\n",
      "loss: 0.735570  [323200/481450]\n",
      "loss: 0.410578  [326400/481450]\n",
      "loss: 0.622662  [329600/481450]\n",
      "loss: 0.752302  [332800/481450]\n",
      "loss: 0.647198  [336000/481450]\n",
      "loss: 0.681186  [339200/481450]\n",
      "loss: 0.453575  [342400/481450]\n",
      "loss: 0.775689  [345600/481450]\n",
      "loss: 0.680043  [348800/481450]\n",
      "loss: 0.575144  [352000/481450]\n",
      "loss: 0.655931  [355200/481450]\n",
      "loss: 0.679995  [358400/481450]\n",
      "loss: 0.811496  [361600/481450]\n",
      "loss: 0.580781  [364800/481450]\n",
      "loss: 0.723627  [368000/481450]\n",
      "loss: 0.568490  [371200/481450]\n",
      "loss: 0.494275  [374400/481450]\n",
      "loss: 0.683913  [377600/481450]\n",
      "loss: 0.407711  [380800/481450]\n",
      "loss: 0.640488  [384000/481450]\n",
      "loss: 0.634664  [387200/481450]\n",
      "loss: 0.673353  [390400/481450]\n",
      "loss: 0.668053  [393600/481450]\n",
      "loss: 0.515610  [396800/481450]\n",
      "loss: 0.713452  [400000/481450]\n",
      "loss: 0.703059  [403200/481450]\n",
      "loss: 0.588014  [406400/481450]\n",
      "loss: 0.450438  [409600/481450]\n",
      "loss: 0.619251  [412800/481450]\n",
      "loss: 0.507235  [416000/481450]\n",
      "loss: 0.660813  [419200/481450]\n",
      "loss: 0.729782  [422400/481450]\n",
      "loss: 0.404555  [425600/481450]\n",
      "loss: 0.566957  [428800/481450]\n",
      "loss: 0.596270  [432000/481450]\n",
      "loss: 0.460168  [435200/481450]\n",
      "loss: 0.908239  [438400/481450]\n",
      "loss: 0.593750  [441600/481450]\n",
      "loss: 0.636297  [444800/481450]\n",
      "loss: 0.646008  [448000/481450]\n",
      "loss: 0.448659  [451200/481450]\n",
      "loss: 0.531743  [454400/481450]\n",
      "loss: 0.617441  [457600/481450]\n",
      "loss: 0.326568  [460800/481450]\n",
      "loss: 0.524480  [464000/481450]\n",
      "loss: 0.685516  [467200/481450]\n",
      "loss: 0.542386  [470400/481450]\n",
      "loss: 0.476451  [473600/481450]\n",
      "loss: 0.487522  [476800/481450]\n",
      "loss: 0.748966  [480000/481450]\n",
      "Train Accuracy: 62.2372%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.395806, F1-score: 88.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.587289  [    0/481450]\n",
      "loss: 0.438215  [ 3200/481450]\n",
      "loss: 0.435070  [ 6400/481450]\n",
      "loss: 0.542492  [ 9600/481450]\n",
      "loss: 0.449340  [12800/481450]\n",
      "loss: 0.699953  [16000/481450]\n",
      "loss: 0.498226  [19200/481450]\n",
      "loss: 0.323338  [22400/481450]\n",
      "loss: 0.438128  [25600/481450]\n",
      "loss: 0.563131  [28800/481450]\n",
      "loss: 0.535274  [32000/481450]\n",
      "loss: 0.484155  [35200/481450]\n",
      "loss: 0.538041  [38400/481450]\n",
      "loss: 0.455687  [41600/481450]\n",
      "loss: 0.483983  [44800/481450]\n",
      "loss: 0.367825  [48000/481450]\n",
      "loss: 0.835974  [51200/481450]\n",
      "loss: 0.308691  [54400/481450]\n",
      "loss: 0.499450  [57600/481450]\n",
      "loss: 0.426939  [60800/481450]\n",
      "loss: 0.635557  [64000/481450]\n",
      "loss: 0.467355  [67200/481450]\n",
      "loss: 0.272447  [70400/481450]\n",
      "loss: 0.517937  [73600/481450]\n",
      "loss: 0.401017  [76800/481450]\n",
      "loss: 0.685099  [80000/481450]\n",
      "loss: 0.773927  [83200/481450]\n",
      "loss: 0.616483  [86400/481450]\n",
      "loss: 0.667198  [89600/481450]\n",
      "loss: 0.384587  [92800/481450]\n",
      "loss: 0.534406  [96000/481450]\n",
      "loss: 0.822595  [99200/481450]\n",
      "loss: 0.493634  [102400/481450]\n",
      "loss: 0.495802  [105600/481450]\n",
      "loss: 0.591562  [108800/481450]\n",
      "loss: 0.341262  [112000/481450]\n",
      "loss: 0.466152  [115200/481450]\n",
      "loss: 0.450525  [118400/481450]\n",
      "loss: 0.505431  [121600/481450]\n",
      "loss: 0.567801  [124800/481450]\n",
      "loss: 0.346636  [128000/481450]\n",
      "loss: 0.463042  [131200/481450]\n",
      "loss: 0.652027  [134400/481450]\n",
      "loss: 0.544597  [137600/481450]\n",
      "loss: 0.408357  [140800/481450]\n",
      "loss: 0.277963  [144000/481450]\n",
      "loss: 0.655361  [147200/481450]\n",
      "loss: 0.524326  [150400/481450]\n",
      "loss: 0.458218  [153600/481450]\n",
      "loss: 0.393824  [156800/481450]\n",
      "loss: 0.573394  [160000/481450]\n",
      "loss: 0.504708  [163200/481450]\n",
      "loss: 0.372752  [166400/481450]\n",
      "loss: 0.428040  [169600/481450]\n",
      "loss: 0.541288  [172800/481450]\n",
      "loss: 0.421922  [176000/481450]\n",
      "loss: 0.398594  [179200/481450]\n",
      "loss: 0.530507  [182400/481450]\n",
      "loss: 0.454714  [185600/481450]\n",
      "loss: 0.410437  [188800/481450]\n",
      "loss: 0.353362  [192000/481450]\n",
      "loss: 0.523393  [195200/481450]\n",
      "loss: 0.447974  [198400/481450]\n",
      "loss: 0.348984  [201600/481450]\n",
      "loss: 0.448028  [204800/481450]\n",
      "loss: 0.646912  [208000/481450]\n",
      "loss: 0.318897  [211200/481450]\n",
      "loss: 0.378912  [214400/481450]\n",
      "loss: 0.325778  [217600/481450]\n",
      "loss: 0.516092  [220800/481450]\n",
      "loss: 0.616258  [224000/481450]\n",
      "loss: 0.614584  [227200/481450]\n",
      "loss: 0.541972  [230400/481450]\n",
      "loss: 0.250267  [233600/481450]\n",
      "loss: 0.410207  [236800/481450]\n",
      "loss: 0.439521  [240000/481450]\n",
      "loss: 0.510156  [243200/481450]\n",
      "loss: 0.490664  [246400/481450]\n",
      "loss: 0.381896  [249600/481450]\n",
      "loss: 0.478573  [252800/481450]\n",
      "loss: 0.550238  [256000/481450]\n",
      "loss: 0.257010  [259200/481450]\n",
      "loss: 0.436749  [262400/481450]\n",
      "loss: 0.338624  [265600/481450]\n",
      "loss: 0.569103  [268800/481450]\n",
      "loss: 0.392915  [272000/481450]\n",
      "loss: 0.486468  [275200/481450]\n",
      "loss: 0.414062  [278400/481450]\n",
      "loss: 0.433829  [281600/481450]\n",
      "loss: 0.358183  [284800/481450]\n",
      "loss: 0.326893  [288000/481450]\n",
      "loss: 0.637289  [291200/481450]\n",
      "loss: 0.523263  [294400/481450]\n",
      "loss: 0.388439  [297600/481450]\n",
      "loss: 0.696213  [300800/481450]\n",
      "loss: 0.554625  [304000/481450]\n",
      "loss: 0.316397  [307200/481450]\n",
      "loss: 0.564706  [310400/481450]\n",
      "loss: 0.387243  [313600/481450]\n",
      "loss: 0.410815  [316800/481450]\n",
      "loss: 0.166865  [320000/481450]\n",
      "loss: 0.330119  [323200/481450]\n",
      "loss: 0.339655  [326400/481450]\n",
      "loss: 0.741182  [329600/481450]\n",
      "loss: 0.433171  [332800/481450]\n",
      "loss: 0.758477  [336000/481450]\n",
      "loss: 0.499796  [339200/481450]\n",
      "loss: 0.476009  [342400/481450]\n",
      "loss: 0.270292  [345600/481450]\n",
      "loss: 0.316320  [348800/481450]\n",
      "loss: 0.406990  [352000/481450]\n",
      "loss: 0.380981  [355200/481450]\n",
      "loss: 0.323782  [358400/481450]\n",
      "loss: 0.481759  [361600/481450]\n",
      "loss: 0.279876  [364800/481450]\n",
      "loss: 0.447050  [368000/481450]\n",
      "loss: 0.441475  [371200/481450]\n",
      "loss: 0.414648  [374400/481450]\n",
      "loss: 0.350610  [377600/481450]\n",
      "loss: 0.417638  [380800/481450]\n",
      "loss: 0.574354  [384000/481450]\n",
      "loss: 0.219653  [387200/481450]\n",
      "loss: 0.299063  [390400/481450]\n",
      "loss: 0.205644  [393600/481450]\n",
      "loss: 0.252118  [396800/481450]\n",
      "loss: 0.474224  [400000/481450]\n",
      "loss: 0.287474  [403200/481450]\n",
      "loss: 0.468450  [406400/481450]\n",
      "loss: 0.603605  [409600/481450]\n",
      "loss: 0.568972  [412800/481450]\n",
      "loss: 0.551705  [416000/481450]\n",
      "loss: 0.419680  [419200/481450]\n",
      "loss: 0.421716  [422400/481450]\n",
      "loss: 0.219237  [425600/481450]\n",
      "loss: 0.404308  [428800/481450]\n",
      "loss: 0.428407  [432000/481450]\n",
      "loss: 0.376361  [435200/481450]\n",
      "loss: 0.284326  [438400/481450]\n",
      "loss: 0.731609  [441600/481450]\n",
      "loss: 0.402897  [444800/481450]\n",
      "loss: 0.287498  [448000/481450]\n",
      "loss: 0.258248  [451200/481450]\n",
      "loss: 0.396096  [454400/481450]\n",
      "loss: 0.507970  [457600/481450]\n",
      "loss: 0.608203  [460800/481450]\n",
      "loss: 0.462216  [464000/481450]\n",
      "loss: 0.329657  [467200/481450]\n",
      "loss: 0.547430  [470400/481450]\n",
      "loss: 0.532891  [473600/481450]\n",
      "loss: 0.455834  [476800/481450]\n",
      "loss: 0.286336  [480000/481450]\n",
      "Train Accuracy: 82.3153%\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.334399, F1-score: 89.23% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.294012  [    0/481450]\n",
      "loss: 0.442567  [ 3200/481450]\n",
      "loss: 0.262710  [ 6400/481450]\n",
      "loss: 0.346951  [ 9600/481450]\n",
      "loss: 0.285318  [12800/481450]\n",
      "loss: 0.403031  [16000/481450]\n",
      "loss: 0.252990  [19200/481450]\n",
      "loss: 0.496006  [22400/481450]\n",
      "loss: 0.451252  [25600/481450]\n",
      "loss: 0.255152  [28800/481450]\n",
      "loss: 0.711611  [32000/481450]\n",
      "loss: 0.356346  [35200/481450]\n",
      "loss: 0.343269  [38400/481450]\n",
      "loss: 0.460651  [41600/481450]\n",
      "loss: 0.200365  [44800/481450]\n",
      "loss: 0.261423  [48000/481450]\n",
      "loss: 0.519059  [51200/481450]\n",
      "loss: 0.324031  [54400/481450]\n",
      "loss: 0.160280  [57600/481450]\n",
      "loss: 0.181116  [60800/481450]\n",
      "loss: 0.337468  [64000/481450]\n",
      "loss: 0.472576  [67200/481450]\n",
      "loss: 0.480961  [70400/481450]\n",
      "loss: 0.484909  [73600/481450]\n",
      "loss: 0.747318  [76800/481450]\n",
      "loss: 0.341016  [80000/481450]\n",
      "loss: 0.368455  [83200/481450]\n",
      "loss: 0.288564  [86400/481450]\n",
      "loss: 0.589868  [89600/481450]\n",
      "loss: 0.564354  [92800/481450]\n",
      "loss: 0.476689  [96000/481450]\n",
      "loss: 0.587904  [99200/481450]\n",
      "loss: 0.186838  [102400/481450]\n",
      "loss: 0.271215  [105600/481450]\n",
      "loss: 0.313050  [108800/481450]\n",
      "loss: 0.270158  [112000/481450]\n",
      "loss: 0.567042  [115200/481450]\n",
      "loss: 0.448272  [118400/481450]\n",
      "loss: 0.413763  [121600/481450]\n",
      "loss: 0.644117  [124800/481450]\n",
      "loss: 0.213354  [128000/481450]\n",
      "loss: 0.366552  [131200/481450]\n",
      "loss: 0.329950  [134400/481450]\n",
      "loss: 0.461736  [137600/481450]\n",
      "loss: 0.368878  [140800/481450]\n",
      "loss: 0.438750  [144000/481450]\n",
      "loss: 0.511559  [147200/481450]\n",
      "loss: 0.268781  [150400/481450]\n",
      "loss: 0.329569  [153600/481450]\n",
      "loss: 0.481030  [156800/481450]\n",
      "loss: 0.635498  [160000/481450]\n",
      "loss: 0.256897  [163200/481450]\n",
      "loss: 0.616313  [166400/481450]\n",
      "loss: 0.384993  [169600/481450]\n",
      "loss: 0.370058  [172800/481450]\n",
      "loss: 0.263370  [176000/481450]\n",
      "loss: 0.412185  [179200/481450]\n",
      "loss: 0.351464  [182400/481450]\n",
      "loss: 0.381508  [185600/481450]\n",
      "loss: 0.290527  [188800/481450]\n",
      "loss: 0.371508  [192000/481450]\n",
      "loss: 0.330315  [195200/481450]\n",
      "loss: 0.534584  [198400/481450]\n",
      "loss: 0.306063  [201600/481450]\n",
      "loss: 0.336278  [204800/481450]\n",
      "loss: 0.382638  [208000/481450]\n",
      "loss: 0.863005  [211200/481450]\n",
      "loss: 0.318867  [214400/481450]\n",
      "loss: 0.507584  [217600/481450]\n",
      "loss: 0.634312  [220800/481450]\n",
      "loss: 0.452989  [224000/481450]\n",
      "loss: 0.345232  [227200/481450]\n",
      "loss: 0.284968  [230400/481450]\n",
      "loss: 0.162841  [233600/481450]\n",
      "loss: 0.459970  [236800/481450]\n",
      "loss: 0.463111  [240000/481450]\n",
      "loss: 0.422865  [243200/481450]\n",
      "loss: 0.392000  [246400/481450]\n",
      "loss: 0.256082  [249600/481450]\n",
      "loss: 0.493309  [252800/481450]\n",
      "loss: 0.649149  [256000/481450]\n",
      "loss: 0.375630  [259200/481450]\n",
      "loss: 0.699728  [262400/481450]\n",
      "loss: 0.841375  [265600/481450]\n",
      "loss: 0.493966  [268800/481450]\n",
      "loss: 0.435333  [272000/481450]\n",
      "loss: 0.435004  [275200/481450]\n",
      "loss: 0.383350  [278400/481450]\n",
      "loss: 0.332406  [281600/481450]\n",
      "loss: 0.379820  [284800/481450]\n",
      "loss: 0.431208  [288000/481450]\n",
      "loss: 0.293180  [291200/481450]\n",
      "loss: 0.496076  [294400/481450]\n",
      "loss: 0.286925  [297600/481450]\n",
      "loss: 0.236933  [300800/481450]\n",
      "loss: 0.376831  [304000/481450]\n",
      "loss: 0.408748  [307200/481450]\n",
      "loss: 0.256934  [310400/481450]\n",
      "loss: 0.378946  [313600/481450]\n",
      "loss: 0.415312  [316800/481450]\n",
      "loss: 0.370576  [320000/481450]\n",
      "loss: 0.454676  [323200/481450]\n",
      "loss: 0.271736  [326400/481450]\n",
      "loss: 0.371053  [329600/481450]\n",
      "loss: 0.392913  [332800/481450]\n",
      "loss: 0.331210  [336000/481450]\n",
      "loss: 0.377178  [339200/481450]\n",
      "loss: 0.322170  [342400/481450]\n",
      "loss: 0.210019  [345600/481450]\n",
      "loss: 0.704808  [348800/481450]\n",
      "loss: 0.432798  [352000/481450]\n",
      "loss: 0.229867  [355200/481450]\n",
      "loss: 0.255173  [358400/481450]\n",
      "loss: 0.706982  [361600/481450]\n",
      "loss: 0.261731  [364800/481450]\n",
      "loss: 0.383346  [368000/481450]\n",
      "loss: 0.568804  [371200/481450]\n",
      "loss: 0.425442  [374400/481450]\n",
      "loss: 0.282357  [377600/481450]\n",
      "loss: 0.352457  [380800/481450]\n",
      "loss: 0.305416  [384000/481450]\n",
      "loss: 0.511198  [387200/481450]\n",
      "loss: 0.462419  [390400/481450]\n",
      "loss: 0.152249  [393600/481450]\n",
      "loss: 0.252492  [396800/481450]\n",
      "loss: 0.596237  [400000/481450]\n",
      "loss: 0.457844  [403200/481450]\n",
      "loss: 0.500605  [406400/481450]\n",
      "loss: 0.653480  [409600/481450]\n",
      "loss: 0.196005  [412800/481450]\n",
      "loss: 0.201168  [416000/481450]\n",
      "loss: 0.262529  [419200/481450]\n",
      "loss: 0.285360  [422400/481450]\n",
      "loss: 0.556011  [425600/481450]\n",
      "loss: 0.369309  [428800/481450]\n",
      "loss: 0.426458  [432000/481450]\n",
      "loss: 0.418127  [435200/481450]\n",
      "loss: 0.222582  [438400/481450]\n",
      "loss: 0.286828  [441600/481450]\n",
      "loss: 0.460622  [444800/481450]\n",
      "loss: 0.532429  [448000/481450]\n",
      "loss: 0.359403  [451200/481450]\n",
      "loss: 0.463896  [454400/481450]\n",
      "loss: 0.336319  [457600/481450]\n",
      "loss: 0.225755  [460800/481450]\n",
      "loss: 0.325361  [464000/481450]\n",
      "loss: 0.580629  [467200/481450]\n",
      "loss: 0.232652  [470400/481450]\n",
      "loss: 0.319211  [473600/481450]\n",
      "loss: 0.330872  [476800/481450]\n",
      "loss: 0.356055  [480000/481450]\n",
      "Train Accuracy: 84.5749%\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.325968, F1-score: 89.71% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.603321  [    0/481450]\n",
      "loss: 0.236854  [ 3200/481450]\n",
      "loss: 0.536890  [ 6400/481450]\n",
      "loss: 0.224384  [ 9600/481450]\n",
      "loss: 0.438545  [12800/481450]\n",
      "loss: 0.577012  [16000/481450]\n",
      "loss: 0.279532  [19200/481450]\n",
      "loss: 0.260466  [22400/481450]\n",
      "loss: 0.549298  [25600/481450]\n",
      "loss: 0.154142  [28800/481450]\n",
      "loss: 0.381011  [32000/481450]\n",
      "loss: 0.306144  [35200/481450]\n",
      "loss: 0.452824  [38400/481450]\n",
      "loss: 0.460970  [41600/481450]\n",
      "loss: 0.119135  [44800/481450]\n",
      "loss: 0.299050  [48000/481450]\n",
      "loss: 0.357191  [51200/481450]\n",
      "loss: 0.644073  [54400/481450]\n",
      "loss: 0.530140  [57600/481450]\n",
      "loss: 0.325613  [60800/481450]\n",
      "loss: 0.416907  [64000/481450]\n",
      "loss: 0.398565  [67200/481450]\n",
      "loss: 0.283468  [70400/481450]\n",
      "loss: 0.417734  [73600/481450]\n",
      "loss: 0.268070  [76800/481450]\n",
      "loss: 0.325017  [80000/481450]\n",
      "loss: 0.393485  [83200/481450]\n",
      "loss: 0.240035  [86400/481450]\n",
      "loss: 0.449144  [89600/481450]\n",
      "loss: 0.239616  [92800/481450]\n",
      "loss: 0.248311  [96000/481450]\n",
      "loss: 0.244359  [99200/481450]\n",
      "loss: 0.324528  [102400/481450]\n",
      "loss: 0.251458  [105600/481450]\n",
      "loss: 0.419354  [108800/481450]\n",
      "loss: 0.467891  [112000/481450]\n",
      "loss: 0.391163  [115200/481450]\n",
      "loss: 0.192533  [118400/481450]\n",
      "loss: 0.134840  [121600/481450]\n",
      "loss: 0.630864  [124800/481450]\n",
      "loss: 0.511320  [128000/481450]\n",
      "loss: 0.529715  [131200/481450]\n",
      "loss: 0.278338  [134400/481450]\n",
      "loss: 0.308211  [137600/481450]\n",
      "loss: 0.248679  [140800/481450]\n",
      "loss: 0.471776  [144000/481450]\n",
      "loss: 0.205169  [147200/481450]\n",
      "loss: 0.454800  [150400/481450]\n",
      "loss: 0.263543  [153600/481450]\n",
      "loss: 0.214998  [156800/481450]\n",
      "loss: 0.217007  [160000/481450]\n",
      "loss: 0.291745  [163200/481450]\n",
      "loss: 0.383499  [166400/481450]\n",
      "loss: 0.468480  [169600/481450]\n",
      "loss: 0.449553  [172800/481450]\n",
      "loss: 0.280991  [176000/481450]\n",
      "loss: 0.109948  [179200/481450]\n",
      "loss: 0.345305  [182400/481450]\n",
      "loss: 0.482310  [185600/481450]\n",
      "loss: 0.625125  [188800/481450]\n",
      "loss: 0.266567  [192000/481450]\n",
      "loss: 0.508878  [195200/481450]\n",
      "loss: 0.328750  [198400/481450]\n",
      "loss: 0.274897  [201600/481450]\n",
      "loss: 0.262745  [204800/481450]\n",
      "loss: 0.355162  [208000/481450]\n",
      "loss: 0.279154  [211200/481450]\n",
      "loss: 0.456570  [214400/481450]\n",
      "loss: 0.272158  [217600/481450]\n",
      "loss: 0.506473  [220800/481450]\n",
      "loss: 0.431570  [224000/481450]\n",
      "loss: 0.403587  [227200/481450]\n",
      "loss: 0.263941  [230400/481450]\n",
      "loss: 0.168817  [233600/481450]\n",
      "loss: 0.314083  [236800/481450]\n",
      "loss: 0.529686  [240000/481450]\n",
      "loss: 0.288349  [243200/481450]\n",
      "loss: 0.386098  [246400/481450]\n",
      "loss: 0.342504  [249600/481450]\n",
      "loss: 0.278273  [252800/481450]\n",
      "loss: 0.382392  [256000/481450]\n",
      "loss: 0.294284  [259200/481450]\n",
      "loss: 0.366879  [262400/481450]\n",
      "loss: 0.413271  [265600/481450]\n",
      "loss: 0.225684  [268800/481450]\n",
      "loss: 0.444788  [272000/481450]\n",
      "loss: 0.300465  [275200/481450]\n",
      "loss: 0.373515  [278400/481450]\n",
      "loss: 0.432070  [281600/481450]\n",
      "loss: 0.254676  [284800/481450]\n",
      "loss: 0.288144  [288000/481450]\n",
      "loss: 0.398315  [291200/481450]\n",
      "loss: 0.378523  [294400/481450]\n",
      "loss: 0.317879  [297600/481450]\n",
      "loss: 0.502995  [300800/481450]\n",
      "loss: 0.172658  [304000/481450]\n",
      "loss: 0.267317  [307200/481450]\n",
      "loss: 0.322216  [310400/481450]\n",
      "loss: 0.432770  [313600/481450]\n",
      "loss: 0.499738  [316800/481450]\n",
      "loss: 0.516935  [320000/481450]\n",
      "loss: 0.331825  [323200/481450]\n",
      "loss: 0.342061  [326400/481450]\n",
      "loss: 0.302047  [329600/481450]\n",
      "loss: 0.598792  [332800/481450]\n",
      "loss: 0.293508  [336000/481450]\n",
      "loss: 0.275969  [339200/481450]\n",
      "loss: 0.454842  [342400/481450]\n",
      "loss: 0.253587  [345600/481450]\n",
      "loss: 0.231006  [348800/481450]\n",
      "loss: 0.347860  [352000/481450]\n",
      "loss: 0.398180  [355200/481450]\n",
      "loss: 0.417985  [358400/481450]\n",
      "loss: 0.516405  [361600/481450]\n",
      "loss: 0.347694  [364800/481450]\n",
      "loss: 0.211688  [368000/481450]\n",
      "loss: 0.359661  [371200/481450]\n",
      "loss: 0.591381  [374400/481450]\n",
      "loss: 0.240996  [377600/481450]\n",
      "loss: 0.209168  [380800/481450]\n",
      "loss: 0.177819  [384000/481450]\n",
      "loss: 0.102465  [387200/481450]\n",
      "loss: 0.517568  [390400/481450]\n",
      "loss: 0.459049  [393600/481450]\n",
      "loss: 0.180572  [396800/481450]\n",
      "loss: 0.309190  [400000/481450]\n",
      "loss: 0.443992  [403200/481450]\n",
      "loss: 0.417190  [406400/481450]\n",
      "loss: 0.447309  [409600/481450]\n",
      "loss: 0.285819  [412800/481450]\n",
      "loss: 0.459662  [416000/481450]\n",
      "loss: 0.372270  [419200/481450]\n",
      "loss: 0.246856  [422400/481450]\n",
      "loss: 0.480222  [425600/481450]\n",
      "loss: 0.412871  [428800/481450]\n",
      "loss: 0.431533  [432000/481450]\n",
      "loss: 0.264318  [435200/481450]\n",
      "loss: 0.368964  [438400/481450]\n",
      "loss: 0.241637  [441600/481450]\n",
      "loss: 0.103098  [444800/481450]\n",
      "loss: 0.235156  [448000/481450]\n",
      "loss: 0.343057  [451200/481450]\n",
      "loss: 0.360075  [454400/481450]\n",
      "loss: 0.204758  [457600/481450]\n",
      "loss: 0.123316  [460800/481450]\n",
      "loss: 0.308545  [464000/481450]\n",
      "loss: 0.139396  [467200/481450]\n",
      "loss: 0.540768  [470400/481450]\n",
      "loss: 0.176816  [473600/481450]\n",
      "loss: 0.484078  [476800/481450]\n",
      "loss: 0.161906  [480000/481450]\n",
      "Train Accuracy: 86.0615%\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.313676, F1-score: 89.51% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.179889  [    0/481450]\n",
      "loss: 0.396618  [ 3200/481450]\n",
      "loss: 0.159200  [ 6400/481450]\n",
      "loss: 0.222594  [ 9600/481450]\n",
      "loss: 0.218392  [12800/481450]\n",
      "loss: 0.285267  [16000/481450]\n",
      "loss: 0.334533  [19200/481450]\n",
      "loss: 0.206249  [22400/481450]\n",
      "loss: 0.489739  [25600/481450]\n",
      "loss: 0.566476  [28800/481450]\n",
      "loss: 0.194473  [32000/481450]\n",
      "loss: 0.315081  [35200/481450]\n",
      "loss: 0.307185  [38400/481450]\n",
      "loss: 0.552956  [41600/481450]\n",
      "loss: 0.347475  [44800/481450]\n",
      "loss: 0.468873  [48000/481450]\n",
      "loss: 0.483659  [51200/481450]\n",
      "loss: 0.248660  [54400/481450]\n",
      "loss: 0.263250  [57600/481450]\n",
      "loss: 0.239442  [60800/481450]\n",
      "loss: 0.363360  [64000/481450]\n",
      "loss: 0.182611  [67200/481450]\n",
      "loss: 0.141283  [70400/481450]\n",
      "loss: 0.525361  [73600/481450]\n",
      "loss: 0.249626  [76800/481450]\n",
      "loss: 0.509410  [80000/481450]\n",
      "loss: 0.453054  [83200/481450]\n",
      "loss: 0.340868  [86400/481450]\n",
      "loss: 0.342896  [89600/481450]\n",
      "loss: 0.150043  [92800/481450]\n",
      "loss: 0.122135  [96000/481450]\n",
      "loss: 0.382243  [99200/481450]\n",
      "loss: 0.141060  [102400/481450]\n",
      "loss: 0.103066  [105600/481450]\n",
      "loss: 0.180580  [108800/481450]\n",
      "loss: 0.150249  [112000/481450]\n",
      "loss: 0.271060  [115200/481450]\n",
      "loss: 0.312661  [118400/481450]\n",
      "loss: 0.338191  [121600/481450]\n",
      "loss: 0.356141  [124800/481450]\n",
      "loss: 0.233314  [128000/481450]\n",
      "loss: 0.317940  [131200/481450]\n",
      "loss: 0.111086  [134400/481450]\n",
      "loss: 0.331561  [137600/481450]\n",
      "loss: 0.270432  [140800/481450]\n",
      "loss: 0.453729  [144000/481450]\n",
      "loss: 0.210249  [147200/481450]\n",
      "loss: 0.270813  [150400/481450]\n",
      "loss: 0.240370  [153600/481450]\n",
      "loss: 0.208014  [156800/481450]\n",
      "loss: 0.514510  [160000/481450]\n",
      "loss: 0.240422  [163200/481450]\n",
      "loss: 0.414595  [166400/481450]\n",
      "loss: 0.403557  [169600/481450]\n",
      "loss: 0.422849  [172800/481450]\n",
      "loss: 0.349616  [176000/481450]\n",
      "loss: 0.379554  [179200/481450]\n",
      "loss: 0.507523  [182400/481450]\n",
      "loss: 0.543084  [185600/481450]\n",
      "loss: 0.299107  [188800/481450]\n",
      "loss: 0.271656  [192000/481450]\n",
      "loss: 0.202462  [195200/481450]\n",
      "loss: 0.218977  [198400/481450]\n",
      "loss: 0.453891  [201600/481450]\n",
      "loss: 0.214547  [204800/481450]\n",
      "loss: 0.296089  [208000/481450]\n",
      "loss: 0.423339  [211200/481450]\n",
      "loss: 0.476949  [214400/481450]\n",
      "loss: 0.248649  [217600/481450]\n",
      "loss: 0.201877  [220800/481450]\n",
      "loss: 0.319703  [224000/481450]\n",
      "loss: 0.365358  [227200/481450]\n",
      "loss: 0.384864  [230400/481450]\n",
      "loss: 0.262321  [233600/481450]\n",
      "loss: 0.271271  [236800/481450]\n",
      "loss: 0.495500  [240000/481450]\n",
      "loss: 0.264600  [243200/481450]\n",
      "loss: 0.470100  [246400/481450]\n",
      "loss: 0.241119  [249600/481450]\n",
      "loss: 0.580726  [252800/481450]\n",
      "loss: 0.348074  [256000/481450]\n",
      "loss: 0.200332  [259200/481450]\n",
      "loss: 0.121761  [262400/481450]\n",
      "loss: 0.637323  [265600/481450]\n",
      "loss: 0.392447  [268800/481450]\n",
      "loss: 0.369549  [272000/481450]\n",
      "loss: 0.197392  [275200/481450]\n",
      "loss: 0.401511  [278400/481450]\n",
      "loss: 0.349511  [281600/481450]\n",
      "loss: 0.201821  [284800/481450]\n",
      "loss: 0.409355  [288000/481450]\n",
      "loss: 0.444785  [291200/481450]\n",
      "loss: 0.335500  [294400/481450]\n",
      "loss: 0.233272  [297600/481450]\n",
      "loss: 0.155585  [300800/481450]\n",
      "loss: 0.181217  [304000/481450]\n",
      "loss: 0.451657  [307200/481450]\n",
      "loss: 0.272526  [310400/481450]\n",
      "loss: 0.412055  [313600/481450]\n",
      "loss: 0.394415  [316800/481450]\n",
      "loss: 0.189753  [320000/481450]\n",
      "loss: 0.485146  [323200/481450]\n",
      "loss: 0.262304  [326400/481450]\n",
      "loss: 0.534584  [329600/481450]\n",
      "loss: 0.535752  [332800/481450]\n",
      "loss: 0.488113  [336000/481450]\n",
      "loss: 0.499202  [339200/481450]\n",
      "loss: 0.220303  [342400/481450]\n",
      "loss: 0.416419  [345600/481450]\n",
      "loss: 0.493940  [348800/481450]\n",
      "loss: 0.170235  [352000/481450]\n",
      "loss: 0.395325  [355200/481450]\n",
      "loss: 0.297208  [358400/481450]\n",
      "loss: 0.254363  [361600/481450]\n",
      "loss: 0.340168  [364800/481450]\n",
      "loss: 0.380989  [368000/481450]\n",
      "loss: 0.271449  [371200/481450]\n",
      "loss: 0.429684  [374400/481450]\n",
      "loss: 0.303246  [377600/481450]\n",
      "loss: 0.581874  [380800/481450]\n",
      "loss: 0.210638  [384000/481450]\n",
      "loss: 0.172727  [387200/481450]\n",
      "loss: 0.155750  [390400/481450]\n",
      "loss: 0.500969  [393600/481450]\n",
      "loss: 0.434537  [396800/481450]\n",
      "loss: 0.417598  [400000/481450]\n",
      "loss: 0.627590  [403200/481450]\n",
      "loss: 0.371591  [406400/481450]\n",
      "loss: 0.215890  [409600/481450]\n",
      "loss: 0.423493  [412800/481450]\n",
      "loss: 0.232403  [416000/481450]\n",
      "loss: 0.351048  [419200/481450]\n",
      "loss: 0.354247  [422400/481450]\n",
      "loss: 0.257680  [425600/481450]\n",
      "loss: 0.187827  [428800/481450]\n",
      "loss: 0.050802  [432000/481450]\n",
      "loss: 0.201155  [435200/481450]\n",
      "loss: 0.347531  [438400/481450]\n",
      "loss: 0.382351  [441600/481450]\n",
      "loss: 0.222447  [444800/481450]\n",
      "loss: 0.465314  [448000/481450]\n",
      "loss: 0.344630  [451200/481450]\n",
      "loss: 0.243687  [454400/481450]\n",
      "loss: 0.100313  [457600/481450]\n",
      "loss: 0.454608  [460800/481450]\n",
      "loss: 0.462955  [464000/481450]\n",
      "loss: 0.231178  [467200/481450]\n",
      "loss: 0.436590  [470400/481450]\n",
      "loss: 0.387370  [473600/481450]\n",
      "loss: 0.105928  [476800/481450]\n",
      "loss: 0.409420  [480000/481450]\n",
      "Train Accuracy: 87.2485%\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.307623, F1-score: 89.96% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.241397  [    0/481450]\n",
      "loss: 0.270448  [ 3200/481450]\n",
      "loss: 0.124250  [ 6400/481450]\n",
      "loss: 0.569470  [ 9600/481450]\n",
      "loss: 0.139897  [12800/481450]\n",
      "loss: 0.267227  [16000/481450]\n",
      "loss: 0.283594  [19200/481450]\n",
      "loss: 0.192432  [22400/481450]\n",
      "loss: 0.157722  [25600/481450]\n",
      "loss: 0.338033  [28800/481450]\n",
      "loss: 0.132649  [32000/481450]\n",
      "loss: 0.254340  [35200/481450]\n",
      "loss: 0.428112  [38400/481450]\n",
      "loss: 0.452691  [41600/481450]\n",
      "loss: 0.166135  [44800/481450]\n",
      "loss: 0.283093  [48000/481450]\n",
      "loss: 0.301883  [51200/481450]\n",
      "loss: 0.205897  [54400/481450]\n",
      "loss: 0.422244  [57600/481450]\n",
      "loss: 0.473295  [60800/481450]\n",
      "loss: 0.278787  [64000/481450]\n",
      "loss: 0.385304  [67200/481450]\n",
      "loss: 0.594893  [70400/481450]\n",
      "loss: 0.220259  [73600/481450]\n",
      "loss: 0.188248  [76800/481450]\n",
      "loss: 0.400372  [80000/481450]\n",
      "loss: 0.361675  [83200/481450]\n",
      "loss: 0.401538  [86400/481450]\n",
      "loss: 0.395202  [89600/481450]\n",
      "loss: 0.411925  [92800/481450]\n",
      "loss: 0.319366  [96000/481450]\n",
      "loss: 0.254629  [99200/481450]\n",
      "loss: 0.250789  [102400/481450]\n",
      "loss: 0.315550  [105600/481450]\n",
      "loss: 0.209999  [108800/481450]\n",
      "loss: 0.228863  [112000/481450]\n",
      "loss: 0.510025  [115200/481450]\n",
      "loss: 0.217095  [118400/481450]\n",
      "loss: 0.307103  [121600/481450]\n",
      "loss: 0.417527  [124800/481450]\n",
      "loss: 0.362120  [128000/481450]\n",
      "loss: 0.273150  [131200/481450]\n",
      "loss: 0.337969  [134400/481450]\n",
      "loss: 0.268840  [137600/481450]\n",
      "loss: 0.272221  [140800/481450]\n",
      "loss: 0.530918  [144000/481450]\n",
      "loss: 0.235543  [147200/481450]\n",
      "loss: 0.205946  [150400/481450]\n",
      "loss: 0.261977  [153600/481450]\n",
      "loss: 0.341440  [156800/481450]\n",
      "loss: 0.210628  [160000/481450]\n",
      "loss: 0.156507  [163200/481450]\n",
      "loss: 0.319163  [166400/481450]\n",
      "loss: 0.354341  [169600/481450]\n",
      "loss: 0.144244  [172800/481450]\n",
      "loss: 0.257006  [176000/481450]\n",
      "loss: 0.501571  [179200/481450]\n",
      "loss: 0.395236  [182400/481450]\n",
      "loss: 0.324440  [185600/481450]\n",
      "loss: 0.344487  [188800/481450]\n",
      "loss: 0.393860  [192000/481450]\n",
      "loss: 0.146884  [195200/481450]\n",
      "loss: 0.224045  [198400/481450]\n",
      "loss: 0.149629  [201600/481450]\n",
      "loss: 0.218176  [204800/481450]\n",
      "loss: 0.468245  [208000/481450]\n",
      "loss: 0.498820  [211200/481450]\n",
      "loss: 0.325026  [214400/481450]\n",
      "loss: 0.323441  [217600/481450]\n",
      "loss: 0.149454  [220800/481450]\n",
      "loss: 0.330699  [224000/481450]\n",
      "loss: 0.286967  [227200/481450]\n",
      "loss: 0.194461  [230400/481450]\n",
      "loss: 0.310510  [233600/481450]\n",
      "loss: 0.276843  [236800/481450]\n",
      "loss: 0.301436  [240000/481450]\n",
      "loss: 0.136991  [243200/481450]\n",
      "loss: 0.420302  [246400/481450]\n",
      "loss: 0.269460  [249600/481450]\n",
      "loss: 0.209339  [252800/481450]\n",
      "loss: 0.510174  [256000/481450]\n",
      "loss: 0.227771  [259200/481450]\n",
      "loss: 0.198869  [262400/481450]\n",
      "loss: 0.141153  [265600/481450]\n",
      "loss: 0.105910  [268800/481450]\n",
      "loss: 0.370346  [272000/481450]\n",
      "loss: 0.186660  [275200/481450]\n",
      "loss: 0.261920  [278400/481450]\n",
      "loss: 0.260189  [281600/481450]\n",
      "loss: 0.390326  [284800/481450]\n",
      "loss: 0.522447  [288000/481450]\n",
      "loss: 0.310259  [291200/481450]\n",
      "loss: 0.323572  [294400/481450]\n",
      "loss: 0.315623  [297600/481450]\n",
      "loss: 0.137819  [300800/481450]\n",
      "loss: 0.348665  [304000/481450]\n",
      "loss: 0.413645  [307200/481450]\n",
      "loss: 0.259049  [310400/481450]\n",
      "loss: 0.249123  [313600/481450]\n",
      "loss: 0.378036  [316800/481450]\n",
      "loss: 0.275639  [320000/481450]\n",
      "loss: 0.204774  [323200/481450]\n",
      "loss: 0.217316  [326400/481450]\n",
      "loss: 0.238271  [329600/481450]\n",
      "loss: 0.288540  [332800/481450]\n",
      "loss: 0.274852  [336000/481450]\n",
      "loss: 0.306598  [339200/481450]\n",
      "loss: 0.258070  [342400/481450]\n",
      "loss: 0.209639  [345600/481450]\n",
      "loss: 0.179812  [348800/481450]\n",
      "loss: 0.331901  [352000/481450]\n",
      "loss: 0.292493  [355200/481450]\n",
      "loss: 0.278381  [358400/481450]\n",
      "loss: 0.458773  [361600/481450]\n",
      "loss: 0.403654  [364800/481450]\n",
      "loss: 0.379540  [368000/481450]\n",
      "loss: 0.426917  [371200/481450]\n",
      "loss: 0.182544  [374400/481450]\n",
      "loss: 0.185511  [377600/481450]\n",
      "loss: 0.149350  [380800/481450]\n",
      "loss: 0.140849  [384000/481450]\n",
      "loss: 0.197102  [387200/481450]\n",
      "loss: 0.161482  [390400/481450]\n",
      "loss: 0.196603  [393600/481450]\n",
      "loss: 0.515972  [396800/481450]\n",
      "loss: 0.156673  [400000/481450]\n",
      "loss: 0.089224  [403200/481450]\n",
      "loss: 0.406393  [406400/481450]\n",
      "loss: 0.344261  [409600/481450]\n",
      "loss: 0.246169  [412800/481450]\n",
      "loss: 0.390375  [416000/481450]\n",
      "loss: 0.223990  [419200/481450]\n",
      "loss: 0.245093  [422400/481450]\n",
      "loss: 0.205442  [425600/481450]\n",
      "loss: 0.230866  [428800/481450]\n",
      "loss: 0.230973  [432000/481450]\n",
      "loss: 0.358867  [435200/481450]\n",
      "loss: 0.158112  [438400/481450]\n",
      "loss: 0.365664  [441600/481450]\n",
      "loss: 0.326526  [444800/481450]\n",
      "loss: 0.222829  [448000/481450]\n",
      "loss: 0.119872  [451200/481450]\n",
      "loss: 0.302074  [454400/481450]\n",
      "loss: 0.370812  [457600/481450]\n",
      "loss: 0.375042  [460800/481450]\n",
      "loss: 0.043830  [464000/481450]\n",
      "loss: 0.106952  [467200/481450]\n",
      "loss: 0.266016  [470400/481450]\n",
      "loss: 0.112335  [473600/481450]\n",
      "loss: 0.305611  [476800/481450]\n",
      "loss: 0.230543  [480000/481450]\n",
      "Train Accuracy: 88.2206%\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.330973, F1-score: 88.63% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.149115  [    0/481450]\n",
      "loss: 0.220670  [ 3200/481450]\n",
      "loss: 0.342752  [ 6400/481450]\n",
      "loss: 0.273929  [ 9600/481450]\n",
      "loss: 0.313728  [12800/481450]\n",
      "loss: 0.461020  [16000/481450]\n",
      "loss: 0.123139  [19200/481450]\n",
      "loss: 0.136923  [22400/481450]\n",
      "loss: 0.484221  [25600/481450]\n",
      "loss: 0.415651  [28800/481450]\n",
      "loss: 0.242720  [32000/481450]\n",
      "loss: 0.354794  [35200/481450]\n",
      "loss: 0.298927  [38400/481450]\n",
      "loss: 0.121521  [41600/481450]\n",
      "loss: 0.281047  [44800/481450]\n",
      "loss: 0.288117  [48000/481450]\n",
      "loss: 0.289594  [51200/481450]\n",
      "loss: 0.288295  [54400/481450]\n",
      "loss: 0.344151  [57600/481450]\n",
      "loss: 0.281454  [60800/481450]\n",
      "loss: 0.281052  [64000/481450]\n",
      "loss: 0.367981  [67200/481450]\n",
      "loss: 0.652753  [70400/481450]\n",
      "loss: 0.428624  [73600/481450]\n",
      "loss: 0.426547  [76800/481450]\n",
      "loss: 0.246027  [80000/481450]\n",
      "loss: 0.163584  [83200/481450]\n",
      "loss: 0.543156  [86400/481450]\n",
      "loss: 0.208155  [89600/481450]\n",
      "loss: 0.183853  [92800/481450]\n",
      "loss: 0.254272  [96000/481450]\n",
      "loss: 0.554337  [99200/481450]\n",
      "loss: 0.516896  [102400/481450]\n",
      "loss: 0.408387  [105600/481450]\n",
      "loss: 0.142262  [108800/481450]\n",
      "loss: 0.425615  [112000/481450]\n",
      "loss: 0.179613  [115200/481450]\n",
      "loss: 0.351424  [118400/481450]\n",
      "loss: 0.194608  [121600/481450]\n",
      "loss: 0.271370  [124800/481450]\n",
      "loss: 0.307230  [128000/481450]\n",
      "loss: 0.474880  [131200/481450]\n",
      "loss: 0.252534  [134400/481450]\n",
      "loss: 0.256096  [137600/481450]\n",
      "loss: 0.190135  [140800/481450]\n",
      "loss: 0.269698  [144000/481450]\n",
      "loss: 0.308329  [147200/481450]\n",
      "loss: 0.173727  [150400/481450]\n",
      "loss: 0.154224  [153600/481450]\n",
      "loss: 0.207709  [156800/481450]\n",
      "loss: 0.338999  [160000/481450]\n",
      "loss: 0.173787  [163200/481450]\n",
      "loss: 0.289552  [166400/481450]\n",
      "loss: 0.137208  [169600/481450]\n",
      "loss: 0.196734  [172800/481450]\n",
      "loss: 0.372567  [176000/481450]\n",
      "loss: 0.150884  [179200/481450]\n",
      "loss: 0.368713  [182400/481450]\n",
      "loss: 0.309661  [185600/481450]\n",
      "loss: 0.182071  [188800/481450]\n",
      "loss: 0.310075  [192000/481450]\n",
      "loss: 0.220823  [195200/481450]\n",
      "loss: 0.371327  [198400/481450]\n",
      "loss: 0.358891  [201600/481450]\n",
      "loss: 0.322711  [204800/481450]\n",
      "loss: 0.156582  [208000/481450]\n",
      "loss: 0.284075  [211200/481450]\n",
      "loss: 0.216485  [214400/481450]\n",
      "loss: 0.599608  [217600/481450]\n",
      "loss: 0.496179  [220800/481450]\n",
      "loss: 0.217080  [224000/481450]\n",
      "loss: 0.131542  [227200/481450]\n",
      "loss: 0.268043  [230400/481450]\n",
      "loss: 0.194972  [233600/481450]\n",
      "loss: 0.240782  [236800/481450]\n",
      "loss: 0.107449  [240000/481450]\n",
      "loss: 0.438202  [243200/481450]\n",
      "loss: 0.149021  [246400/481450]\n",
      "loss: 0.347432  [249600/481450]\n",
      "loss: 0.326108  [252800/481450]\n",
      "loss: 0.222662  [256000/481450]\n",
      "loss: 0.444298  [259200/481450]\n",
      "loss: 0.260828  [262400/481450]\n",
      "loss: 0.434716  [265600/481450]\n",
      "loss: 0.181062  [268800/481450]\n",
      "loss: 0.265933  [272000/481450]\n",
      "loss: 0.271770  [275200/481450]\n",
      "loss: 0.416128  [278400/481450]\n",
      "loss: 0.524366  [281600/481450]\n",
      "loss: 0.245068  [284800/481450]\n",
      "loss: 0.223723  [288000/481450]\n",
      "loss: 0.197395  [291200/481450]\n",
      "loss: 0.134384  [294400/481450]\n",
      "loss: 0.372713  [297600/481450]\n",
      "loss: 0.329207  [300800/481450]\n",
      "loss: 0.195058  [304000/481450]\n",
      "loss: 0.158144  [307200/481450]\n",
      "loss: 0.182471  [310400/481450]\n",
      "loss: 0.466791  [313600/481450]\n",
      "loss: 0.441468  [316800/481450]\n",
      "loss: 0.276170  [320000/481450]\n",
      "loss: 0.191833  [323200/481450]\n",
      "loss: 0.223098  [326400/481450]\n",
      "loss: 0.222862  [329600/481450]\n",
      "loss: 0.328418  [332800/481450]\n",
      "loss: 0.232427  [336000/481450]\n",
      "loss: 0.064258  [339200/481450]\n",
      "loss: 0.292745  [342400/481450]\n",
      "loss: 0.738070  [345600/481450]\n",
      "loss: 0.232763  [348800/481450]\n",
      "loss: 0.099262  [352000/481450]\n",
      "loss: 0.220454  [355200/481450]\n",
      "loss: 0.230530  [358400/481450]\n",
      "loss: 0.285580  [361600/481450]\n",
      "loss: 0.214507  [364800/481450]\n",
      "loss: 0.298787  [368000/481450]\n",
      "loss: 0.142741  [371200/481450]\n",
      "loss: 0.096280  [374400/481450]\n",
      "loss: 0.218875  [377600/481450]\n",
      "loss: 0.380714  [380800/481450]\n",
      "loss: 0.216569  [384000/481450]\n",
      "loss: 0.196559  [387200/481450]\n",
      "loss: 0.243035  [390400/481450]\n",
      "loss: 0.108888  [393600/481450]\n",
      "loss: 0.221974  [396800/481450]\n",
      "loss: 0.160957  [400000/481450]\n",
      "loss: 0.255085  [403200/481450]\n",
      "loss: 0.216877  [406400/481450]\n",
      "loss: 0.153239  [409600/481450]\n",
      "loss: 0.290005  [412800/481450]\n",
      "loss: 0.099678  [416000/481450]\n",
      "loss: 0.026172  [419200/481450]\n",
      "loss: 0.257125  [422400/481450]\n",
      "loss: 0.188586  [425600/481450]\n",
      "loss: 0.137812  [428800/481450]\n",
      "loss: 0.107631  [432000/481450]\n",
      "loss: 0.110637  [435200/481450]\n",
      "loss: 0.313747  [438400/481450]\n",
      "loss: 0.109816  [441600/481450]\n",
      "loss: 0.288545  [444800/481450]\n",
      "loss: 0.275580  [448000/481450]\n",
      "loss: 0.264889  [451200/481450]\n",
      "loss: 0.033163  [454400/481450]\n",
      "loss: 0.267489  [457600/481450]\n",
      "loss: 0.284431  [460800/481450]\n",
      "loss: 0.068266  [464000/481450]\n",
      "loss: 0.390398  [467200/481450]\n",
      "loss: 0.231940  [470400/481450]\n",
      "loss: 0.295520  [473600/481450]\n",
      "loss: 0.133757  [476800/481450]\n",
      "loss: 0.206438  [480000/481450]\n",
      "Train Accuracy: 88.7629%\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.342761, F1-score: 88.38% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.283409  [    0/481450]\n",
      "loss: 0.179956  [ 3200/481450]\n",
      "loss: 0.182247  [ 6400/481450]\n",
      "loss: 0.280961  [ 9600/481450]\n",
      "loss: 0.223416  [12800/481450]\n",
      "loss: 0.260639  [16000/481450]\n",
      "loss: 0.305058  [19200/481450]\n",
      "loss: 0.144366  [22400/481450]\n",
      "loss: 0.258696  [25600/481450]\n",
      "loss: 0.176170  [28800/481450]\n",
      "loss: 0.303513  [32000/481450]\n",
      "loss: 0.115754  [35200/481450]\n",
      "loss: 0.404388  [38400/481450]\n",
      "loss: 0.447172  [41600/481450]\n",
      "loss: 0.132710  [44800/481450]\n",
      "loss: 0.351570  [48000/481450]\n",
      "loss: 0.275908  [51200/481450]\n",
      "loss: 0.262550  [54400/481450]\n",
      "loss: 0.215729  [57600/481450]\n",
      "loss: 0.353370  [60800/481450]\n",
      "loss: 0.536622  [64000/481450]\n",
      "loss: 0.393571  [67200/481450]\n",
      "loss: 0.153281  [70400/481450]\n",
      "loss: 0.131309  [73600/481450]\n",
      "loss: 0.191276  [76800/481450]\n",
      "loss: 0.203885  [80000/481450]\n",
      "loss: 0.206785  [83200/481450]\n",
      "loss: 0.275556  [86400/481450]\n",
      "loss: 0.284322  [89600/481450]\n",
      "loss: 0.346538  [92800/481450]\n",
      "loss: 0.474549  [96000/481450]\n",
      "loss: 0.224037  [99200/481450]\n",
      "loss: 0.410800  [102400/481450]\n",
      "loss: 0.277541  [105600/481450]\n",
      "loss: 0.176411  [108800/481450]\n",
      "loss: 0.348523  [112000/481450]\n",
      "loss: 0.292382  [115200/481450]\n",
      "loss: 0.485340  [118400/481450]\n",
      "loss: 0.313244  [121600/481450]\n",
      "loss: 0.222105  [124800/481450]\n",
      "loss: 0.191132  [128000/481450]\n",
      "loss: 0.297280  [131200/481450]\n",
      "loss: 0.320154  [134400/481450]\n",
      "loss: 0.290583  [137600/481450]\n",
      "loss: 0.279846  [140800/481450]\n",
      "loss: 0.192966  [144000/481450]\n",
      "loss: 0.215804  [147200/481450]\n",
      "loss: 0.370025  [150400/481450]\n",
      "loss: 0.230882  [153600/481450]\n",
      "loss: 0.321544  [156800/481450]\n",
      "loss: 0.299991  [160000/481450]\n",
      "loss: 0.481345  [163200/481450]\n",
      "loss: 0.375504  [166400/481450]\n",
      "loss: 0.194254  [169600/481450]\n",
      "loss: 0.410505  [172800/481450]\n",
      "loss: 0.225457  [176000/481450]\n",
      "loss: 0.353070  [179200/481450]\n",
      "loss: 0.255152  [182400/481450]\n",
      "loss: 0.286768  [185600/481450]\n",
      "loss: 0.431174  [188800/481450]\n",
      "loss: 0.246622  [192000/481450]\n",
      "loss: 0.307688  [195200/481450]\n",
      "loss: 0.466906  [198400/481450]\n",
      "loss: 0.241768  [201600/481450]\n",
      "loss: 0.358095  [204800/481450]\n",
      "loss: 0.368355  [208000/481450]\n",
      "loss: 0.309543  [211200/481450]\n",
      "loss: 0.474252  [214400/481450]\n",
      "loss: 0.309493  [217600/481450]\n",
      "loss: 0.189636  [220800/481450]\n",
      "loss: 0.454305  [224000/481450]\n",
      "loss: 0.110022  [227200/481450]\n",
      "loss: 0.209953  [230400/481450]\n",
      "loss: 0.386495  [233600/481450]\n",
      "loss: 0.405410  [236800/481450]\n",
      "loss: 0.264385  [240000/481450]\n",
      "loss: 0.424580  [243200/481450]\n",
      "loss: 0.314505  [246400/481450]\n",
      "loss: 0.302539  [249600/481450]\n",
      "loss: 0.114942  [252800/481450]\n",
      "loss: 0.406451  [256000/481450]\n",
      "loss: 0.401629  [259200/481450]\n",
      "loss: 0.410040  [262400/481450]\n",
      "loss: 0.302030  [265600/481450]\n",
      "loss: 0.376167  [268800/481450]\n",
      "loss: 0.145048  [272000/481450]\n",
      "loss: 0.509365  [275200/481450]\n",
      "loss: 0.134840  [278400/481450]\n",
      "loss: 0.267669  [281600/481450]\n",
      "loss: 0.476182  [284800/481450]\n",
      "loss: 0.342504  [288000/481450]\n",
      "loss: 0.396264  [291200/481450]\n",
      "loss: 0.403848  [294400/481450]\n",
      "loss: 0.254553  [297600/481450]\n",
      "loss: 0.415184  [300800/481450]\n",
      "loss: 0.269072  [304000/481450]\n",
      "loss: 0.472846  [307200/481450]\n",
      "loss: 0.117384  [310400/481450]\n",
      "loss: 0.454507  [313600/481450]\n",
      "loss: 0.276487  [316800/481450]\n",
      "loss: 0.226244  [320000/481450]\n",
      "loss: 0.180812  [323200/481450]\n",
      "loss: 0.199663  [326400/481450]\n",
      "loss: 0.224353  [329600/481450]\n",
      "loss: 0.198832  [332800/481450]\n",
      "loss: 0.240841  [336000/481450]\n",
      "loss: 0.192658  [339200/481450]\n",
      "loss: 0.291951  [342400/481450]\n",
      "loss: 0.350595  [345600/481450]\n",
      "loss: 0.144886  [348800/481450]\n",
      "loss: 0.215727  [352000/481450]\n",
      "loss: 0.414628  [355200/481450]\n",
      "loss: 0.457316  [358400/481450]\n",
      "loss: 0.350649  [361600/481450]\n",
      "loss: 0.103091  [364800/481450]\n",
      "loss: 0.244398  [368000/481450]\n",
      "loss: 0.451810  [371200/481450]\n",
      "loss: 0.230772  [374400/481450]\n",
      "loss: 0.194691  [377600/481450]\n",
      "loss: 0.256936  [380800/481450]\n",
      "loss: 0.329797  [384000/481450]\n",
      "loss: 0.122120  [387200/481450]\n",
      "loss: 0.176898  [390400/481450]\n",
      "loss: 0.268768  [393600/481450]\n",
      "loss: 0.208834  [396800/481450]\n",
      "loss: 0.212955  [400000/481450]\n",
      "loss: 0.232662  [403200/481450]\n",
      "loss: 0.185768  [406400/481450]\n",
      "loss: 0.366365  [409600/481450]\n",
      "loss: 0.198370  [412800/481450]\n",
      "loss: 0.167242  [416000/481450]\n",
      "loss: 0.484085  [419200/481450]\n",
      "loss: 0.211228  [422400/481450]\n",
      "loss: 0.304361  [425600/481450]\n",
      "loss: 0.226358  [428800/481450]\n",
      "loss: 0.251514  [432000/481450]\n",
      "loss: 0.153038  [435200/481450]\n",
      "loss: 0.332135  [438400/481450]\n",
      "loss: 0.190577  [441600/481450]\n",
      "loss: 0.353947  [444800/481450]\n",
      "loss: 0.429421  [448000/481450]\n",
      "loss: 0.219091  [451200/481450]\n",
      "loss: 0.200554  [454400/481450]\n",
      "loss: 0.092962  [457600/481450]\n",
      "loss: 0.441362  [460800/481450]\n",
      "loss: 0.262024  [464000/481450]\n",
      "loss: 0.153215  [467200/481450]\n",
      "loss: 0.469120  [470400/481450]\n",
      "loss: 0.320774  [473600/481450]\n",
      "loss: 0.288802  [476800/481450]\n",
      "loss: 0.353833  [480000/481450]\n",
      "Train Accuracy: 89.2248%\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.349639, F1-score: 88.29% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.268423  [    0/481450]\n",
      "loss: 0.281512  [ 3200/481450]\n",
      "loss: 0.171518  [ 6400/481450]\n",
      "loss: 0.277206  [ 9600/481450]\n",
      "loss: 0.281676  [12800/481450]\n",
      "loss: 0.052977  [16000/481450]\n",
      "loss: 0.236670  [19200/481450]\n",
      "loss: 0.225853  [22400/481450]\n",
      "loss: 0.117922  [25600/481450]\n",
      "loss: 0.200570  [28800/481450]\n",
      "loss: 0.329582  [32000/481450]\n",
      "loss: 0.214974  [35200/481450]\n",
      "loss: 0.312374  [38400/481450]\n",
      "loss: 0.362182  [41600/481450]\n",
      "loss: 0.188525  [44800/481450]\n",
      "loss: 0.363414  [48000/481450]\n",
      "loss: 0.120489  [51200/481450]\n",
      "loss: 0.311056  [54400/481450]\n",
      "loss: 0.109294  [57600/481450]\n",
      "loss: 0.243626  [60800/481450]\n",
      "loss: 0.352190  [64000/481450]\n",
      "loss: 0.341895  [67200/481450]\n",
      "loss: 0.208551  [70400/481450]\n",
      "loss: 0.230102  [73600/481450]\n",
      "loss: 0.380838  [76800/481450]\n",
      "loss: 0.233280  [80000/481450]\n",
      "loss: 0.192400  [83200/481450]\n",
      "loss: 0.194260  [86400/481450]\n",
      "loss: 0.198489  [89600/481450]\n",
      "loss: 0.391806  [92800/481450]\n",
      "loss: 0.334147  [96000/481450]\n",
      "loss: 0.226710  [99200/481450]\n",
      "loss: 0.322861  [102400/481450]\n",
      "loss: 0.249380  [105600/481450]\n",
      "loss: 0.202057  [108800/481450]\n",
      "loss: 0.292916  [112000/481450]\n",
      "loss: 0.518430  [115200/481450]\n",
      "loss: 0.394139  [118400/481450]\n",
      "loss: 0.242889  [121600/481450]\n",
      "loss: 0.249934  [124800/481450]\n",
      "loss: 0.302013  [128000/481450]\n",
      "loss: 0.181740  [131200/481450]\n",
      "loss: 0.362828  [134400/481450]\n",
      "loss: 0.192056  [137600/481450]\n",
      "loss: 0.381008  [140800/481450]\n",
      "loss: 0.390574  [144000/481450]\n",
      "loss: 0.078530  [147200/481450]\n",
      "loss: 0.216722  [150400/481450]\n",
      "loss: 0.349868  [153600/481450]\n",
      "loss: 0.412913  [156800/481450]\n",
      "loss: 0.193267  [160000/481450]\n",
      "loss: 0.298259  [163200/481450]\n",
      "loss: 0.242177  [166400/481450]\n",
      "loss: 0.121089  [169600/481450]\n",
      "loss: 0.158238  [172800/481450]\n",
      "loss: 0.301248  [176000/481450]\n",
      "loss: 0.046365  [179200/481450]\n",
      "loss: 0.263403  [182400/481450]\n",
      "loss: 0.281018  [185600/481450]\n",
      "loss: 0.186255  [188800/481450]\n",
      "loss: 0.231028  [192000/481450]\n",
      "loss: 0.325873  [195200/481450]\n",
      "loss: 0.177940  [198400/481450]\n",
      "loss: 0.505675  [201600/481450]\n",
      "loss: 0.149694  [204800/481450]\n",
      "loss: 0.384211  [208000/481450]\n",
      "loss: 0.343445  [211200/481450]\n",
      "loss: 0.417344  [214400/481450]\n",
      "loss: 0.371126  [217600/481450]\n",
      "loss: 0.358515  [220800/481450]\n",
      "loss: 0.413005  [224000/481450]\n",
      "loss: 0.121252  [227200/481450]\n",
      "loss: 0.156204  [230400/481450]\n",
      "loss: 0.139178  [233600/481450]\n",
      "loss: 0.289723  [236800/481450]\n",
      "loss: 0.235996  [240000/481450]\n",
      "loss: 0.200873  [243200/481450]\n",
      "loss: 0.292802  [246400/481450]\n",
      "loss: 0.144452  [249600/481450]\n",
      "loss: 0.195255  [252800/481450]\n",
      "loss: 0.329610  [256000/481450]\n",
      "loss: 0.296220  [259200/481450]\n",
      "loss: 0.339281  [262400/481450]\n",
      "loss: 0.214904  [265600/481450]\n",
      "loss: 0.105763  [268800/481450]\n",
      "loss: 0.134809  [272000/481450]\n",
      "loss: 0.162672  [275200/481450]\n",
      "loss: 0.210626  [278400/481450]\n",
      "loss: 0.250413  [281600/481450]\n",
      "loss: 0.221363  [284800/481450]\n",
      "loss: 0.370917  [288000/481450]\n",
      "loss: 0.241027  [291200/481450]\n",
      "loss: 0.202415  [294400/481450]\n",
      "loss: 0.109896  [297600/481450]\n",
      "loss: 0.376937  [300800/481450]\n",
      "loss: 0.307341  [304000/481450]\n",
      "loss: 0.171454  [307200/481450]\n",
      "loss: 0.397236  [310400/481450]\n",
      "loss: 0.317441  [313600/481450]\n",
      "loss: 0.323212  [316800/481450]\n",
      "loss: 0.339335  [320000/481450]\n",
      "loss: 0.278892  [323200/481450]\n",
      "loss: 0.187011  [326400/481450]\n",
      "loss: 0.132250  [329600/481450]\n",
      "loss: 0.223581  [332800/481450]\n",
      "loss: 0.114895  [336000/481450]\n",
      "loss: 0.275880  [339200/481450]\n",
      "loss: 0.212656  [342400/481450]\n",
      "loss: 0.358966  [345600/481450]\n",
      "loss: 0.121996  [348800/481450]\n",
      "loss: 0.421215  [352000/481450]\n",
      "loss: 0.224266  [355200/481450]\n",
      "loss: 0.380492  [358400/481450]\n",
      "loss: 0.266345  [361600/481450]\n",
      "loss: 0.137042  [364800/481450]\n",
      "loss: 0.069569  [368000/481450]\n",
      "loss: 0.172430  [371200/481450]\n",
      "loss: 0.376881  [374400/481450]\n",
      "loss: 0.200449  [377600/481450]\n",
      "loss: 0.127419  [380800/481450]\n",
      "loss: 0.214221  [384000/481450]\n",
      "loss: 0.298241  [387200/481450]\n",
      "loss: 0.256527  [390400/481450]\n",
      "loss: 0.289698  [393600/481450]\n",
      "loss: 0.256656  [396800/481450]\n",
      "loss: 0.125811  [400000/481450]\n",
      "loss: 0.344146  [403200/481450]\n",
      "loss: 0.264411  [406400/481450]\n",
      "loss: 0.111594  [409600/481450]\n",
      "loss: 0.339793  [412800/481450]\n",
      "loss: 0.313894  [416000/481450]\n",
      "loss: 0.351357  [419200/481450]\n",
      "loss: 0.154606  [422400/481450]\n",
      "loss: 0.172164  [425600/481450]\n",
      "loss: 0.310428  [428800/481450]\n",
      "loss: 0.242984  [432000/481450]\n",
      "loss: 0.295371  [435200/481450]\n",
      "loss: 0.220380  [438400/481450]\n",
      "loss: 0.063348  [441600/481450]\n",
      "loss: 0.378446  [444800/481450]\n",
      "loss: 0.096279  [448000/481450]\n",
      "loss: 0.333188  [451200/481450]\n",
      "loss: 0.117707  [454400/481450]\n",
      "loss: 0.266249  [457600/481450]\n",
      "loss: 0.281549  [460800/481450]\n",
      "loss: 0.240997  [464000/481450]\n",
      "loss: 0.118190  [467200/481450]\n",
      "loss: 0.434632  [470400/481450]\n",
      "loss: 0.185624  [473600/481450]\n",
      "loss: 0.226908  [476800/481450]\n",
      "loss: 0.233514  [480000/481450]\n",
      "Train Accuracy: 89.6020%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.380936, F1-score: 87.63% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.165175  [    0/481450]\n",
      "loss: 0.175454  [ 3200/481450]\n",
      "loss: 0.282082  [ 6400/481450]\n",
      "loss: 0.195234  [ 9600/481450]\n",
      "loss: 0.408138  [12800/481450]\n",
      "loss: 0.188621  [16000/481450]\n",
      "loss: 0.279488  [19200/481450]\n",
      "loss: 0.086398  [22400/481450]\n",
      "loss: 0.272009  [25600/481450]\n",
      "loss: 0.453657  [28800/481450]\n",
      "loss: 0.169909  [32000/481450]\n",
      "loss: 0.098750  [35200/481450]\n",
      "loss: 0.204722  [38400/481450]\n",
      "loss: 0.200050  [41600/481450]\n",
      "loss: 0.420614  [44800/481450]\n",
      "loss: 0.425523  [48000/481450]\n",
      "loss: 0.218702  [51200/481450]\n",
      "loss: 0.290083  [54400/481450]\n",
      "loss: 0.232086  [57600/481450]\n",
      "loss: 0.335111  [60800/481450]\n",
      "loss: 0.264934  [64000/481450]\n",
      "loss: 0.301827  [67200/481450]\n",
      "loss: 0.157646  [70400/481450]\n",
      "loss: 0.217383  [73600/481450]\n",
      "loss: 0.384668  [76800/481450]\n",
      "loss: 0.284402  [80000/481450]\n",
      "loss: 0.272775  [83200/481450]\n",
      "loss: 0.093055  [86400/481450]\n",
      "loss: 0.302544  [89600/481450]\n",
      "loss: 0.289063  [92800/481450]\n",
      "loss: 0.234639  [96000/481450]\n",
      "loss: 0.118038  [99200/481450]\n",
      "loss: 0.117713  [102400/481450]\n",
      "loss: 0.241494  [105600/481450]\n",
      "loss: 0.125053  [108800/481450]\n",
      "loss: 0.308631  [112000/481450]\n",
      "loss: 0.292158  [115200/481450]\n",
      "loss: 0.338713  [118400/481450]\n",
      "loss: 0.262800  [121600/481450]\n",
      "loss: 0.169634  [124800/481450]\n",
      "loss: 0.269389  [128000/481450]\n",
      "loss: 0.353626  [131200/481450]\n",
      "loss: 0.205216  [134400/481450]\n",
      "loss: 0.509228  [137600/481450]\n",
      "loss: 0.095887  [140800/481450]\n",
      "loss: 0.260509  [144000/481450]\n",
      "loss: 0.289373  [147200/481450]\n",
      "loss: 0.253555  [150400/481450]\n",
      "loss: 0.494695  [153600/481450]\n",
      "loss: 0.261078  [156800/481450]\n",
      "loss: 0.138596  [160000/481450]\n",
      "loss: 0.463010  [163200/481450]\n",
      "loss: 0.507024  [166400/481450]\n",
      "loss: 0.116555  [169600/481450]\n",
      "loss: 0.270805  [172800/481450]\n",
      "loss: 0.150321  [176000/481450]\n",
      "loss: 0.470526  [179200/481450]\n",
      "loss: 0.332328  [182400/481450]\n",
      "loss: 0.116193  [185600/481450]\n",
      "loss: 0.201467  [188800/481450]\n",
      "loss: 0.120236  [192000/481450]\n",
      "loss: 0.255297  [195200/481450]\n",
      "loss: 0.344842  [198400/481450]\n",
      "loss: 0.323884  [201600/481450]\n",
      "loss: 0.272129  [204800/481450]\n",
      "loss: 0.336533  [208000/481450]\n",
      "loss: 0.316542  [211200/481450]\n",
      "loss: 0.369841  [214400/481450]\n",
      "loss: 0.371835  [217600/481450]\n",
      "loss: 0.197272  [220800/481450]\n",
      "loss: 0.389578  [224000/481450]\n",
      "loss: 0.215666  [227200/481450]\n",
      "loss: 0.324998  [230400/481450]\n",
      "loss: 0.330402  [233600/481450]\n",
      "loss: 0.265573  [236800/481450]\n",
      "loss: 0.178209  [240000/481450]\n",
      "loss: 0.383489  [243200/481450]\n",
      "loss: 0.146314  [246400/481450]\n",
      "loss: 0.450937  [249600/481450]\n",
      "loss: 0.197264  [252800/481450]\n",
      "loss: 0.421966  [256000/481450]\n",
      "loss: 0.139525  [259200/481450]\n",
      "loss: 0.196153  [262400/481450]\n",
      "loss: 0.315393  [265600/481450]\n",
      "loss: 0.286269  [268800/481450]\n",
      "loss: 0.160445  [272000/481450]\n",
      "loss: 0.478288  [275200/481450]\n",
      "loss: 0.253110  [278400/481450]\n",
      "loss: 0.237558  [281600/481450]\n",
      "loss: 0.098786  [284800/481450]\n",
      "loss: 0.158539  [288000/481450]\n",
      "loss: 0.348301  [291200/481450]\n",
      "loss: 0.187118  [294400/481450]\n",
      "loss: 0.420518  [297600/481450]\n",
      "loss: 0.204340  [300800/481450]\n",
      "loss: 0.134211  [304000/481450]\n",
      "loss: 0.180127  [307200/481450]\n",
      "loss: 0.134075  [310400/481450]\n",
      "loss: 0.178533  [313600/481450]\n",
      "loss: 0.317536  [316800/481450]\n",
      "loss: 0.327284  [320000/481450]\n",
      "loss: 0.217482  [323200/481450]\n",
      "loss: 0.158427  [326400/481450]\n",
      "loss: 0.018920  [329600/481450]\n",
      "loss: 0.234722  [332800/481450]\n",
      "loss: 0.197694  [336000/481450]\n",
      "loss: 0.434455  [339200/481450]\n",
      "loss: 0.171726  [342400/481450]\n",
      "loss: 0.225739  [345600/481450]\n",
      "loss: 0.091437  [348800/481450]\n",
      "loss: 0.362833  [352000/481450]\n",
      "loss: 0.382136  [355200/481450]\n",
      "loss: 0.271682  [358400/481450]\n",
      "loss: 0.207462  [361600/481450]\n",
      "loss: 0.348455  [364800/481450]\n",
      "loss: 0.133533  [368000/481450]\n",
      "loss: 0.144600  [371200/481450]\n",
      "loss: 0.391528  [374400/481450]\n",
      "loss: 0.210015  [377600/481450]\n",
      "loss: 0.186419  [380800/481450]\n",
      "loss: 0.351200  [384000/481450]\n",
      "loss: 0.194635  [387200/481450]\n",
      "loss: 0.187145  [390400/481450]\n",
      "loss: 0.409266  [393600/481450]\n",
      "loss: 0.181246  [396800/481450]\n",
      "loss: 0.274283  [400000/481450]\n",
      "loss: 0.101350  [403200/481450]\n",
      "loss: 0.093160  [406400/481450]\n",
      "loss: 0.280120  [409600/481450]\n",
      "loss: 0.345425  [412800/481450]\n",
      "loss: 0.094326  [416000/481450]\n",
      "loss: 0.432977  [419200/481450]\n",
      "loss: 0.442019  [422400/481450]\n",
      "loss: 0.411713  [425600/481450]\n",
      "loss: 0.157816  [428800/481450]\n",
      "loss: 0.657749  [432000/481450]\n",
      "loss: 0.202235  [435200/481450]\n",
      "loss: 0.232460  [438400/481450]\n",
      "loss: 0.877298  [441600/481450]\n",
      "loss: 0.167369  [444800/481450]\n",
      "loss: 0.283377  [448000/481450]\n",
      "loss: 0.284739  [451200/481450]\n",
      "loss: 0.299178  [454400/481450]\n",
      "loss: 0.499513  [457600/481450]\n",
      "loss: 0.190908  [460800/481450]\n",
      "loss: 0.296977  [464000/481450]\n",
      "loss: 0.289671  [467200/481450]\n",
      "loss: 0.217582  [470400/481450]\n",
      "loss: 0.265348  [473600/481450]\n",
      "loss: 0.041964  [476800/481450]\n",
      "loss: 0.331888  [480000/481450]\n",
      "Train Accuracy: 89.9389%\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.389606, F1-score: 87.64% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.467270  [    0/481450]\n",
      "loss: 0.206787  [ 3200/481450]\n",
      "loss: 0.268609  [ 6400/481450]\n",
      "loss: 0.086149  [ 9600/481450]\n",
      "loss: 0.228577  [12800/481450]\n",
      "loss: 0.217503  [16000/481450]\n",
      "loss: 0.552004  [19200/481450]\n",
      "loss: 0.224667  [22400/481450]\n",
      "loss: 0.381599  [25600/481450]\n",
      "loss: 0.172407  [28800/481450]\n",
      "loss: 0.256106  [32000/481450]\n",
      "loss: 0.286923  [35200/481450]\n",
      "loss: 0.212463  [38400/481450]\n",
      "loss: 0.360455  [41600/481450]\n",
      "loss: 0.131434  [44800/481450]\n",
      "loss: 0.105330  [48000/481450]\n",
      "loss: 0.321716  [51200/481450]\n",
      "loss: 0.039738  [54400/481450]\n",
      "loss: 0.331720  [57600/481450]\n",
      "loss: 0.249298  [60800/481450]\n",
      "loss: 0.203581  [64000/481450]\n",
      "loss: 0.074788  [67200/481450]\n",
      "loss: 0.185648  [70400/481450]\n",
      "loss: 0.100047  [73600/481450]\n",
      "loss: 0.153946  [76800/481450]\n",
      "loss: 0.284732  [80000/481450]\n",
      "loss: 0.448362  [83200/481450]\n",
      "loss: 0.117558  [86400/481450]\n",
      "loss: 0.281500  [89600/481450]\n",
      "loss: 0.377223  [92800/481450]\n",
      "loss: 0.324045  [96000/481450]\n",
      "loss: 0.210296  [99200/481450]\n",
      "loss: 0.202501  [102400/481450]\n",
      "loss: 0.182247  [105600/481450]\n",
      "loss: 0.188947  [108800/481450]\n",
      "loss: 0.131664  [112000/481450]\n",
      "loss: 0.306544  [115200/481450]\n",
      "loss: 0.128745  [118400/481450]\n",
      "loss: 0.156927  [121600/481450]\n",
      "loss: 0.108270  [124800/481450]\n",
      "loss: 0.466362  [128000/481450]\n",
      "loss: 0.303714  [131200/481450]\n",
      "loss: 0.248242  [134400/481450]\n",
      "loss: 0.281121  [137600/481450]\n",
      "loss: 0.206211  [140800/481450]\n",
      "loss: 0.129635  [144000/481450]\n",
      "loss: 0.506326  [147200/481450]\n",
      "loss: 0.238332  [150400/481450]\n",
      "loss: 0.332484  [153600/481450]\n",
      "loss: 0.216413  [156800/481450]\n",
      "loss: 0.453149  [160000/481450]\n",
      "loss: 0.222671  [163200/481450]\n",
      "loss: 0.149816  [166400/481450]\n",
      "loss: 0.267552  [169600/481450]\n",
      "loss: 0.332409  [172800/481450]\n",
      "loss: 0.114713  [176000/481450]\n",
      "loss: 0.140568  [179200/481450]\n",
      "loss: 0.162069  [182400/481450]\n",
      "loss: 0.341645  [185600/481450]\n",
      "loss: 0.215810  [188800/481450]\n",
      "loss: 0.204254  [192000/481450]\n",
      "loss: 0.140396  [195200/481450]\n",
      "loss: 0.066714  [198400/481450]\n",
      "loss: 0.441577  [201600/481450]\n",
      "loss: 0.513105  [204800/481450]\n",
      "loss: 0.202084  [208000/481450]\n",
      "loss: 0.334030  [211200/481450]\n",
      "loss: 0.493391  [214400/481450]\n",
      "loss: 0.222617  [217600/481450]\n",
      "loss: 0.165186  [220800/481450]\n",
      "loss: 0.107737  [224000/481450]\n",
      "loss: 0.234159  [227200/481450]\n",
      "loss: 0.237424  [230400/481450]\n",
      "loss: 0.272728  [233600/481450]\n",
      "loss: 0.568969  [236800/481450]\n",
      "loss: 0.395324  [240000/481450]\n",
      "loss: 0.105715  [243200/481450]\n",
      "loss: 0.399972  [246400/481450]\n",
      "loss: 0.395868  [249600/481450]\n",
      "loss: 0.150931  [252800/481450]\n",
      "loss: 0.123578  [256000/481450]\n",
      "loss: 0.197518  [259200/481450]\n",
      "loss: 0.100554  [262400/481450]\n",
      "loss: 0.190302  [265600/481450]\n",
      "loss: 0.435443  [268800/481450]\n",
      "loss: 0.450886  [272000/481450]\n",
      "loss: 0.177437  [275200/481450]\n",
      "loss: 0.250864  [278400/481450]\n",
      "loss: 0.526734  [281600/481450]\n",
      "loss: 0.161217  [284800/481450]\n",
      "loss: 0.268500  [288000/481450]\n",
      "loss: 0.266494  [291200/481450]\n",
      "loss: 0.302402  [294400/481450]\n",
      "loss: 0.016849  [297600/481450]\n",
      "loss: 0.157743  [300800/481450]\n",
      "loss: 0.203174  [304000/481450]\n",
      "loss: 0.173784  [307200/481450]\n",
      "loss: 0.109912  [310400/481450]\n",
      "loss: 0.111663  [313600/481450]\n",
      "loss: 0.165105  [316800/481450]\n",
      "loss: 0.359730  [320000/481450]\n",
      "loss: 0.189370  [323200/481450]\n",
      "loss: 0.258590  [326400/481450]\n",
      "loss: 0.141983  [329600/481450]\n",
      "loss: 0.286516  [332800/481450]\n",
      "loss: 0.280610  [336000/481450]\n",
      "loss: 0.196340  [339200/481450]\n",
      "loss: 0.049127  [342400/481450]\n",
      "loss: 0.493430  [345600/481450]\n",
      "loss: 0.303408  [348800/481450]\n",
      "loss: 0.210660  [352000/481450]\n",
      "loss: 0.055392  [355200/481450]\n",
      "loss: 0.223329  [358400/481450]\n",
      "loss: 0.238545  [361600/481450]\n",
      "loss: 0.325078  [364800/481450]\n",
      "loss: 0.316645  [368000/481450]\n",
      "loss: 0.154787  [371200/481450]\n",
      "loss: 0.220181  [374400/481450]\n",
      "loss: 0.277855  [377600/481450]\n",
      "loss: 0.168794  [380800/481450]\n",
      "loss: 0.330669  [384000/481450]\n",
      "loss: 0.300542  [387200/481450]\n",
      "loss: 0.116279  [390400/481450]\n",
      "loss: 0.189766  [393600/481450]\n",
      "loss: 0.059226  [396800/481450]\n",
      "loss: 0.264649  [400000/481450]\n",
      "loss: 0.573949  [403200/481450]\n",
      "loss: 0.447318  [406400/481450]\n",
      "loss: 0.406994  [409600/481450]\n",
      "loss: 0.386239  [412800/481450]\n",
      "loss: 0.406576  [416000/481450]\n",
      "loss: 0.195115  [419200/481450]\n",
      "loss: 0.185446  [422400/481450]\n",
      "loss: 0.319583  [425600/481450]\n",
      "loss: 0.302160  [428800/481450]\n",
      "loss: 0.200815  [432000/481450]\n",
      "loss: 0.335735  [435200/481450]\n",
      "loss: 0.381306  [438400/481450]\n",
      "loss: 0.070855  [441600/481450]\n",
      "loss: 0.141866  [444800/481450]\n",
      "loss: 0.076681  [448000/481450]\n",
      "loss: 0.191862  [451200/481450]\n",
      "loss: 0.085649  [454400/481450]\n",
      "loss: 0.180389  [457600/481450]\n",
      "loss: 0.177802  [460800/481450]\n",
      "loss: 0.056313  [464000/481450]\n",
      "loss: 0.328187  [467200/481450]\n",
      "loss: 0.175798  [470400/481450]\n",
      "loss: 0.739047  [473600/481450]\n",
      "loss: 0.132406  [476800/481450]\n",
      "loss: 0.165781  [480000/481450]\n",
      "Train Accuracy: 90.2210%\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.425283, F1-score: 86.98% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.248545  [    0/481450]\n",
      "loss: 0.231028  [ 3200/481450]\n",
      "loss: 0.196304  [ 6400/481450]\n",
      "loss: 0.445425  [ 9600/481450]\n",
      "loss: 0.106207  [12800/481450]\n",
      "loss: 0.071025  [16000/481450]\n",
      "loss: 0.297566  [19200/481450]\n",
      "loss: 0.352504  [22400/481450]\n",
      "loss: 0.307445  [25600/481450]\n",
      "loss: 0.376050  [28800/481450]\n",
      "loss: 0.067362  [32000/481450]\n",
      "loss: 0.180539  [35200/481450]\n",
      "loss: 0.175673  [38400/481450]\n",
      "loss: 0.177924  [41600/481450]\n",
      "loss: 0.286758  [44800/481450]\n",
      "loss: 0.400830  [48000/481450]\n",
      "loss: 0.279140  [51200/481450]\n",
      "loss: 0.104754  [54400/481450]\n",
      "loss: 0.228002  [57600/481450]\n",
      "loss: 0.275591  [60800/481450]\n",
      "loss: 0.220797  [64000/481450]\n",
      "loss: 0.214238  [67200/481450]\n",
      "loss: 0.128405  [70400/481450]\n",
      "loss: 0.085470  [73600/481450]\n",
      "loss: 0.148036  [76800/481450]\n",
      "loss: 0.193812  [80000/481450]\n",
      "loss: 0.098738  [83200/481450]\n",
      "loss: 0.321994  [86400/481450]\n",
      "loss: 0.175727  [89600/481450]\n",
      "loss: 0.164933  [92800/481450]\n",
      "loss: 0.373308  [96000/481450]\n",
      "loss: 0.233867  [99200/481450]\n",
      "loss: 0.316320  [102400/481450]\n",
      "loss: 0.272105  [105600/481450]\n",
      "loss: 0.117634  [108800/481450]\n",
      "loss: 0.142716  [112000/481450]\n",
      "loss: 0.254660  [115200/481450]\n",
      "loss: 0.249604  [118400/481450]\n",
      "loss: 0.230639  [121600/481450]\n",
      "loss: 0.088810  [124800/481450]\n",
      "loss: 0.208512  [128000/481450]\n",
      "loss: 0.147630  [131200/481450]\n",
      "loss: 0.185436  [134400/481450]\n",
      "loss: 0.077860  [137600/481450]\n",
      "loss: 0.164956  [140800/481450]\n",
      "loss: 0.544862  [144000/481450]\n",
      "loss: 0.346723  [147200/481450]\n",
      "loss: 0.348450  [150400/481450]\n",
      "loss: 0.254710  [153600/481450]\n",
      "loss: 0.159683  [156800/481450]\n",
      "loss: 0.217013  [160000/481450]\n",
      "loss: 0.342629  [163200/481450]\n",
      "loss: 0.140070  [166400/481450]\n",
      "loss: 0.230265  [169600/481450]\n",
      "loss: 0.274705  [172800/481450]\n",
      "loss: 0.293368  [176000/481450]\n",
      "loss: 0.335848  [179200/481450]\n",
      "loss: 0.363376  [182400/481450]\n",
      "loss: 0.293481  [185600/481450]\n",
      "loss: 0.187251  [188800/481450]\n",
      "loss: 0.105650  [192000/481450]\n",
      "loss: 0.044380  [195200/481450]\n",
      "loss: 0.311443  [198400/481450]\n",
      "loss: 0.447510  [201600/481450]\n",
      "loss: 0.046398  [204800/481450]\n",
      "loss: 0.384162  [208000/481450]\n",
      "loss: 0.076660  [211200/481450]\n",
      "loss: 0.459799  [214400/481450]\n",
      "loss: 0.315045  [217600/481450]\n",
      "loss: 0.451330  [220800/481450]\n",
      "loss: 0.215314  [224000/481450]\n",
      "loss: 0.149643  [227200/481450]\n",
      "loss: 0.255350  [230400/481450]\n",
      "loss: 0.248368  [233600/481450]\n",
      "loss: 0.320195  [236800/481450]\n",
      "loss: 0.262575  [240000/481450]\n",
      "loss: 0.272250  [243200/481450]\n",
      "loss: 0.160553  [246400/481450]\n",
      "loss: 0.307744  [249600/481450]\n",
      "loss: 0.305294  [252800/481450]\n",
      "loss: 0.551225  [256000/481450]\n",
      "loss: 0.174319  [259200/481450]\n",
      "loss: 0.140613  [262400/481450]\n",
      "loss: 0.381002  [265600/481450]\n",
      "loss: 0.130485  [268800/481450]\n",
      "loss: 0.306406  [272000/481450]\n",
      "loss: 0.141585  [275200/481450]\n",
      "loss: 0.192782  [278400/481450]\n",
      "loss: 0.204774  [281600/481450]\n",
      "loss: 0.150797  [284800/481450]\n",
      "loss: 0.347242  [288000/481450]\n",
      "loss: 0.233200  [291200/481450]\n",
      "loss: 0.051855  [294400/481450]\n",
      "loss: 0.081638  [297600/481450]\n",
      "loss: 0.302992  [300800/481450]\n",
      "loss: 0.345083  [304000/481450]\n",
      "loss: 0.266710  [307200/481450]\n",
      "loss: 0.206055  [310400/481450]\n",
      "loss: 0.178908  [313600/481450]\n",
      "loss: 0.240527  [316800/481450]\n",
      "loss: 0.250154  [320000/481450]\n",
      "loss: 0.182200  [323200/481450]\n",
      "loss: 0.418993  [326400/481450]\n",
      "loss: 0.295808  [329600/481450]\n",
      "loss: 0.440278  [332800/481450]\n",
      "loss: 0.338842  [336000/481450]\n",
      "loss: 0.322605  [339200/481450]\n",
      "loss: 0.320389  [342400/481450]\n",
      "loss: 0.105449  [345600/481450]\n",
      "loss: 0.445673  [348800/481450]\n",
      "loss: 0.219013  [352000/481450]\n",
      "loss: 0.237711  [355200/481450]\n",
      "loss: 0.436889  [358400/481450]\n",
      "loss: 0.400641  [361600/481450]\n",
      "loss: 0.355657  [364800/481450]\n",
      "loss: 0.227942  [368000/481450]\n",
      "loss: 0.199091  [371200/481450]\n",
      "loss: 0.210312  [374400/481450]\n",
      "loss: 0.120237  [377600/481450]\n",
      "loss: 0.145010  [380800/481450]\n",
      "loss: 0.057918  [384000/481450]\n",
      "loss: 0.231164  [387200/481450]\n",
      "loss: 0.173796  [390400/481450]\n",
      "loss: 0.144694  [393600/481450]\n",
      "loss: 0.400764  [396800/481450]\n",
      "loss: 0.361596  [400000/481450]\n",
      "loss: 0.181446  [403200/481450]\n",
      "loss: 0.249600  [406400/481450]\n",
      "loss: 0.218075  [409600/481450]\n",
      "loss: 0.229519  [412800/481450]\n",
      "loss: 0.157763  [416000/481450]\n",
      "loss: 0.311765  [419200/481450]\n",
      "loss: 0.276248  [422400/481450]\n",
      "loss: 0.387663  [425600/481450]\n",
      "loss: 0.193662  [428800/481450]\n",
      "loss: 0.176847  [432000/481450]\n",
      "loss: 0.047164  [435200/481450]\n",
      "loss: 0.103383  [438400/481450]\n",
      "loss: 0.360632  [441600/481450]\n",
      "loss: 0.149462  [444800/481450]\n",
      "loss: 0.266984  [448000/481450]\n",
      "loss: 0.258446  [451200/481450]\n",
      "loss: 0.284854  [454400/481450]\n",
      "loss: 0.995902  [457600/481450]\n",
      "loss: 0.353681  [460800/481450]\n",
      "loss: 0.352913  [464000/481450]\n",
      "loss: 0.281944  [467200/481450]\n",
      "loss: 0.176364  [470400/481450]\n",
      "loss: 0.328649  [473600/481450]\n",
      "loss: 0.056505  [476800/481450]\n",
      "loss: 0.080090  [480000/481450]\n",
      "Train Accuracy: 90.4368%\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.442893, F1-score: 86.29% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.096055  [    0/481450]\n",
      "loss: 0.333723  [ 3200/481450]\n",
      "loss: 0.189231  [ 6400/481450]\n",
      "loss: 0.059920  [ 9600/481450]\n",
      "loss: 0.305105  [12800/481450]\n",
      "loss: 0.259268  [16000/481450]\n",
      "loss: 0.148986  [19200/481450]\n",
      "loss: 0.504801  [22400/481450]\n",
      "loss: 0.445106  [25600/481450]\n",
      "loss: 0.270903  [28800/481450]\n",
      "loss: 0.221218  [32000/481450]\n",
      "loss: 0.122407  [35200/481450]\n",
      "loss: 0.160384  [38400/481450]\n",
      "loss: 0.279717  [41600/481450]\n",
      "loss: 0.151988  [44800/481450]\n",
      "loss: 0.164314  [48000/481450]\n",
      "loss: 0.240521  [51200/481450]\n",
      "loss: 0.168368  [54400/481450]\n",
      "loss: 0.160867  [57600/481450]\n",
      "loss: 0.128184  [60800/481450]\n",
      "loss: 0.268707  [64000/481450]\n",
      "loss: 0.353413  [67200/481450]\n",
      "loss: 0.152325  [70400/481450]\n",
      "loss: 0.099749  [73600/481450]\n",
      "loss: 0.148044  [76800/481450]\n",
      "loss: 0.063345  [80000/481450]\n",
      "loss: 0.095811  [83200/481450]\n",
      "loss: 0.154690  [86400/481450]\n",
      "loss: 0.308930  [89600/481450]\n",
      "loss: 0.461914  [92800/481450]\n",
      "loss: 0.503137  [96000/481450]\n",
      "loss: 0.179434  [99200/481450]\n",
      "loss: 0.140934  [102400/481450]\n",
      "loss: 0.279618  [105600/481450]\n",
      "loss: 0.141218  [108800/481450]\n",
      "loss: 0.150405  [112000/481450]\n",
      "loss: 0.344716  [115200/481450]\n",
      "loss: 0.407346  [118400/481450]\n",
      "loss: 0.324852  [121600/481450]\n",
      "loss: 0.262944  [124800/481450]\n",
      "loss: 0.130235  [128000/481450]\n",
      "loss: 0.171511  [131200/481450]\n",
      "loss: 0.181296  [134400/481450]\n",
      "loss: 0.224182  [137600/481450]\n",
      "loss: 0.092126  [140800/481450]\n",
      "loss: 0.105837  [144000/481450]\n",
      "loss: 0.301709  [147200/481450]\n",
      "loss: 0.211653  [150400/481450]\n",
      "loss: 0.260043  [153600/481450]\n",
      "loss: 0.238739  [156800/481450]\n",
      "loss: 0.172493  [160000/481450]\n",
      "loss: 0.121896  [163200/481450]\n",
      "loss: 0.303498  [166400/481450]\n",
      "loss: 0.349665  [169600/481450]\n",
      "loss: 0.258104  [172800/481450]\n",
      "loss: 0.134926  [176000/481450]\n",
      "loss: 0.235974  [179200/481450]\n",
      "loss: 0.151817  [182400/481450]\n",
      "loss: 0.191131  [185600/481450]\n",
      "loss: 0.187594  [188800/481450]\n",
      "loss: 0.231910  [192000/481450]\n",
      "loss: 0.398653  [195200/481450]\n",
      "loss: 0.445932  [198400/481450]\n",
      "loss: 0.230771  [201600/481450]\n",
      "loss: 0.276893  [204800/481450]\n",
      "loss: 0.408717  [208000/481450]\n",
      "loss: 0.118619  [211200/481450]\n",
      "loss: 0.251712  [214400/481450]\n",
      "loss: 0.217491  [217600/481450]\n",
      "loss: 0.292872  [220800/481450]\n",
      "loss: 0.437216  [224000/481450]\n",
      "loss: 0.040560  [227200/481450]\n",
      "loss: 0.336084  [230400/481450]\n",
      "loss: 0.185264  [233600/481450]\n",
      "loss: 0.263068  [236800/481450]\n",
      "loss: 0.236912  [240000/481450]\n",
      "loss: 0.084911  [243200/481450]\n",
      "loss: 0.108507  [246400/481450]\n",
      "loss: 0.348507  [249600/481450]\n",
      "loss: 0.410420  [252800/481450]\n",
      "loss: 0.359842  [256000/481450]\n",
      "loss: 0.131990  [259200/481450]\n",
      "loss: 0.222922  [262400/481450]\n",
      "loss: 0.294812  [265600/481450]\n",
      "loss: 0.229585  [268800/481450]\n",
      "loss: 0.284574  [272000/481450]\n",
      "loss: 0.308686  [275200/481450]\n",
      "loss: 0.243436  [278400/481450]\n",
      "loss: 0.178850  [281600/481450]\n",
      "loss: 0.197131  [284800/481450]\n",
      "loss: 0.308724  [288000/481450]\n",
      "loss: 0.417538  [291200/481450]\n",
      "loss: 0.144516  [294400/481450]\n",
      "loss: 0.225146  [297600/481450]\n",
      "loss: 0.252091  [300800/481450]\n",
      "loss: 0.148897  [304000/481450]\n",
      "loss: 0.427232  [307200/481450]\n",
      "loss: 0.192244  [310400/481450]\n",
      "loss: 0.217705  [313600/481450]\n",
      "loss: 0.243601  [316800/481450]\n",
      "loss: 0.136897  [320000/481450]\n",
      "loss: 0.406861  [323200/481450]\n",
      "loss: 0.382841  [326400/481450]\n",
      "loss: 0.460604  [329600/481450]\n",
      "loss: 0.294327  [332800/481450]\n",
      "loss: 0.321976  [336000/481450]\n",
      "loss: 0.304473  [339200/481450]\n",
      "loss: 0.236365  [342400/481450]\n",
      "loss: 0.302405  [345600/481450]\n",
      "loss: 0.269208  [348800/481450]\n",
      "loss: 0.287872  [352000/481450]\n",
      "loss: 0.342497  [355200/481450]\n",
      "loss: 0.331691  [358400/481450]\n",
      "loss: 0.469218  [361600/481450]\n",
      "loss: 0.259878  [364800/481450]\n",
      "loss: 0.297518  [368000/481450]\n",
      "loss: 0.404549  [371200/481450]\n",
      "loss: 0.294256  [374400/481450]\n",
      "loss: 0.147323  [377600/481450]\n",
      "loss: 0.534343  [380800/481450]\n",
      "loss: 0.223399  [384000/481450]\n",
      "loss: 0.266685  [387200/481450]\n",
      "loss: 0.210482  [390400/481450]\n",
      "loss: 0.273555  [393600/481450]\n",
      "loss: 0.057674  [396800/481450]\n",
      "loss: 0.300027  [400000/481450]\n",
      "loss: 0.265530  [403200/481450]\n",
      "loss: 0.245966  [406400/481450]\n",
      "loss: 0.369854  [409600/481450]\n",
      "loss: 0.404574  [412800/481450]\n",
      "loss: 0.140393  [416000/481450]\n",
      "loss: 0.240659  [419200/481450]\n",
      "loss: 0.121959  [422400/481450]\n",
      "loss: 0.289997  [425600/481450]\n",
      "loss: 0.148130  [428800/481450]\n",
      "loss: 0.444204  [432000/481450]\n",
      "loss: 0.076078  [435200/481450]\n",
      "loss: 0.217587  [438400/481450]\n",
      "loss: 0.136963  [441600/481450]\n",
      "loss: 0.465975  [444800/481450]\n",
      "loss: 0.380594  [448000/481450]\n",
      "loss: 0.197788  [451200/481450]\n",
      "loss: 0.287872  [454400/481450]\n",
      "loss: 0.244875  [457600/481450]\n",
      "loss: 0.371422  [460800/481450]\n",
      "loss: 0.137845  [464000/481450]\n",
      "loss: 0.227294  [467200/481450]\n",
      "loss: 0.177602  [470400/481450]\n",
      "loss: 0.335653  [473600/481450]\n",
      "loss: 0.139384  [476800/481450]\n",
      "loss: 0.247053  [480000/481450]\n",
      "Train Accuracy: 90.6329%\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.453547, F1-score: 86.24% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.181961  [    0/481450]\n",
      "loss: 0.125061  [ 3200/481450]\n",
      "loss: 0.314669  [ 6400/481450]\n",
      "loss: 0.087636  [ 9600/481450]\n",
      "loss: 0.193767  [12800/481450]\n",
      "loss: 0.238245  [16000/481450]\n",
      "loss: 0.475562  [19200/481450]\n",
      "loss: 0.150108  [22400/481450]\n",
      "loss: 0.144418  [25600/481450]\n",
      "loss: 0.413482  [28800/481450]\n",
      "loss: 0.158305  [32000/481450]\n",
      "loss: 0.192836  [35200/481450]\n",
      "loss: 0.221444  [38400/481450]\n",
      "loss: 0.094931  [41600/481450]\n",
      "loss: 0.169851  [44800/481450]\n",
      "loss: 0.160565  [48000/481450]\n",
      "loss: 0.091330  [51200/481450]\n",
      "loss: 0.401783  [54400/481450]\n",
      "loss: 0.278994  [57600/481450]\n",
      "loss: 0.212596  [60800/481450]\n",
      "loss: 0.203706  [64000/481450]\n",
      "loss: 0.142958  [67200/481450]\n",
      "loss: 0.300566  [70400/481450]\n",
      "loss: 0.144636  [73600/481450]\n",
      "loss: 0.263641  [76800/481450]\n",
      "loss: 0.220436  [80000/481450]\n",
      "loss: 0.339744  [83200/481450]\n",
      "loss: 0.132472  [86400/481450]\n",
      "loss: 0.308717  [89600/481450]\n",
      "loss: 0.497381  [92800/481450]\n",
      "loss: 0.244211  [96000/481450]\n",
      "loss: 0.190929  [99200/481450]\n",
      "loss: 0.361813  [102400/481450]\n",
      "loss: 0.102763  [105600/481450]\n",
      "loss: 0.237441  [108800/481450]\n",
      "loss: 0.209750  [112000/481450]\n",
      "loss: 0.132097  [115200/481450]\n",
      "loss: 0.188340  [118400/481450]\n",
      "loss: 0.147214  [121600/481450]\n",
      "loss: 0.319461  [124800/481450]\n",
      "loss: 0.332038  [128000/481450]\n",
      "loss: 0.133478  [131200/481450]\n",
      "loss: 0.192962  [134400/481450]\n",
      "loss: 0.154992  [137600/481450]\n",
      "loss: 0.572581  [140800/481450]\n",
      "loss: 0.303101  [144000/481450]\n",
      "loss: 0.138232  [147200/481450]\n",
      "loss: 0.507378  [150400/481450]\n",
      "loss: 0.149652  [153600/481450]\n",
      "loss: 0.264842  [156800/481450]\n",
      "loss: 0.167471  [160000/481450]\n",
      "loss: 0.165939  [163200/481450]\n",
      "loss: 0.186987  [166400/481450]\n",
      "loss: 0.231245  [169600/481450]\n",
      "loss: 0.320933  [172800/481450]\n",
      "loss: 0.324361  [176000/481450]\n",
      "loss: 0.164034  [179200/481450]\n",
      "loss: 0.264351  [182400/481450]\n",
      "loss: 0.120383  [185600/481450]\n",
      "loss: 0.202944  [188800/481450]\n",
      "loss: 0.077735  [192000/481450]\n",
      "loss: 0.323546  [195200/481450]\n",
      "loss: 0.185542  [198400/481450]\n",
      "loss: 0.286028  [201600/481450]\n",
      "loss: 0.198445  [204800/481450]\n",
      "loss: 0.211722  [208000/481450]\n",
      "loss: 0.303194  [211200/481450]\n",
      "loss: 0.147463  [214400/481450]\n",
      "loss: 0.484384  [217600/481450]\n",
      "loss: 0.293900  [220800/481450]\n",
      "loss: 0.170569  [224000/481450]\n",
      "loss: 0.212681  [227200/481450]\n",
      "loss: 0.449954  [230400/481450]\n",
      "loss: 0.241057  [233600/481450]\n",
      "loss: 0.267072  [236800/481450]\n",
      "loss: 0.176105  [240000/481450]\n",
      "loss: 0.405601  [243200/481450]\n",
      "loss: 0.148999  [246400/481450]\n",
      "loss: 0.290561  [249600/481450]\n",
      "loss: 0.179142  [252800/481450]\n",
      "loss: 0.282141  [256000/481450]\n",
      "loss: 0.165088  [259200/481450]\n",
      "loss: 0.363610  [262400/481450]\n",
      "loss: 0.384093  [265600/481450]\n",
      "loss: 0.080190  [268800/481450]\n",
      "loss: 0.091545  [272000/481450]\n",
      "loss: 0.146310  [275200/481450]\n",
      "loss: 0.391107  [278400/481450]\n",
      "loss: 0.302519  [281600/481450]\n",
      "loss: 0.250153  [284800/481450]\n",
      "loss: 0.103813  [288000/481450]\n",
      "loss: 0.102991  [291200/481450]\n",
      "loss: 0.201228  [294400/481450]\n",
      "loss: 0.377618  [297600/481450]\n",
      "loss: 0.172699  [300800/481450]\n",
      "loss: 0.050962  [304000/481450]\n",
      "loss: 0.084844  [307200/481450]\n",
      "loss: 0.304075  [310400/481450]\n",
      "loss: 0.198671  [313600/481450]\n",
      "loss: 0.264765  [316800/481450]\n",
      "loss: 0.283011  [320000/481450]\n",
      "loss: 0.303786  [323200/481450]\n",
      "loss: 0.107132  [326400/481450]\n",
      "loss: 0.324146  [329600/481450]\n",
      "loss: 0.214526  [332800/481450]\n",
      "loss: 0.489169  [336000/481450]\n",
      "loss: 0.107781  [339200/481450]\n",
      "loss: 0.216362  [342400/481450]\n",
      "loss: 0.321796  [345600/481450]\n",
      "loss: 0.375909  [348800/481450]\n",
      "loss: 0.198877  [352000/481450]\n",
      "loss: 0.102606  [355200/481450]\n",
      "loss: 0.223948  [358400/481450]\n",
      "loss: 0.383111  [361600/481450]\n",
      "loss: 0.317314  [364800/481450]\n",
      "loss: 0.288825  [368000/481450]\n",
      "loss: 0.342002  [371200/481450]\n",
      "loss: 0.150025  [374400/481450]\n",
      "loss: 0.272432  [377600/481450]\n",
      "loss: 0.579989  [380800/481450]\n",
      "loss: 0.161162  [384000/481450]\n",
      "loss: 0.268211  [387200/481450]\n",
      "loss: 0.173381  [390400/481450]\n",
      "loss: 0.492772  [393600/481450]\n",
      "loss: 0.210016  [396800/481450]\n",
      "loss: 0.218434  [400000/481450]\n",
      "loss: 0.290758  [403200/481450]\n",
      "loss: 0.448516  [406400/481450]\n",
      "loss: 0.270362  [409600/481450]\n",
      "loss: 0.300528  [412800/481450]\n",
      "loss: 0.477665  [416000/481450]\n",
      "loss: 0.378731  [419200/481450]\n",
      "loss: 0.263492  [422400/481450]\n",
      "loss: 0.226801  [425600/481450]\n",
      "loss: 0.236044  [428800/481450]\n",
      "loss: 0.405182  [432000/481450]\n",
      "loss: 0.251373  [435200/481450]\n",
      "loss: 0.186471  [438400/481450]\n",
      "loss: 0.204948  [441600/481450]\n",
      "loss: 0.174010  [444800/481450]\n",
      "loss: 0.307962  [448000/481450]\n",
      "loss: 0.350460  [451200/481450]\n",
      "loss: 0.087961  [454400/481450]\n",
      "loss: 0.295711  [457600/481450]\n",
      "loss: 0.118516  [460800/481450]\n",
      "loss: 0.223437  [464000/481450]\n",
      "loss: 0.202519  [467200/481450]\n",
      "loss: 0.260059  [470400/481450]\n",
      "loss: 0.104673  [473600/481450]\n",
      "loss: 0.170780  [476800/481450]\n",
      "loss: 0.252793  [480000/481450]\n",
      "Train Accuracy: 90.7243%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.475540, F1-score: 85.77% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.431122  [    0/481450]\n",
      "loss: 0.129951  [ 3200/481450]\n",
      "loss: 0.255083  [ 6400/481450]\n",
      "loss: 0.191011  [ 9600/481450]\n",
      "loss: 0.284978  [12800/481450]\n",
      "loss: 0.249740  [16000/481450]\n",
      "loss: 0.171454  [19200/481450]\n",
      "loss: 0.128404  [22400/481450]\n",
      "loss: 0.097558  [25600/481450]\n",
      "loss: 0.174314  [28800/481450]\n",
      "loss: 0.191561  [32000/481450]\n",
      "loss: 0.298865  [35200/481450]\n",
      "loss: 0.187166  [38400/481450]\n",
      "loss: 0.143016  [41600/481450]\n",
      "loss: 0.254260  [44800/481450]\n",
      "loss: 0.380921  [48000/481450]\n",
      "loss: 0.268325  [51200/481450]\n",
      "loss: 0.199166  [54400/481450]\n",
      "loss: 0.219308  [57600/481450]\n",
      "loss: 0.407407  [60800/481450]\n",
      "loss: 0.267970  [64000/481450]\n",
      "loss: 0.486611  [67200/481450]\n",
      "loss: 0.241564  [70400/481450]\n",
      "loss: 0.084424  [73600/481450]\n",
      "loss: 0.396795  [76800/481450]\n",
      "loss: 0.268645  [80000/481450]\n",
      "loss: 0.112060  [83200/481450]\n",
      "loss: 0.267920  [86400/481450]\n",
      "loss: 0.367685  [89600/481450]\n",
      "loss: 0.226399  [92800/481450]\n",
      "loss: 0.107798  [96000/481450]\n",
      "loss: 0.157261  [99200/481450]\n",
      "loss: 0.164141  [102400/481450]\n",
      "loss: 0.140397  [105600/481450]\n",
      "loss: 0.045199  [108800/481450]\n",
      "loss: 0.196073  [112000/481450]\n",
      "loss: 0.286209  [115200/481450]\n",
      "loss: 0.174332  [118400/481450]\n",
      "loss: 0.133773  [121600/481450]\n",
      "loss: 0.361335  [124800/481450]\n",
      "loss: 0.183270  [128000/481450]\n",
      "loss: 0.271077  [131200/481450]\n",
      "loss: 0.190498  [134400/481450]\n",
      "loss: 0.172538  [137600/481450]\n",
      "loss: 0.324689  [140800/481450]\n",
      "loss: 0.322751  [144000/481450]\n",
      "loss: 0.300288  [147200/481450]\n",
      "loss: 0.229734  [150400/481450]\n",
      "loss: 0.292481  [153600/481450]\n",
      "loss: 0.135658  [156800/481450]\n",
      "loss: 0.236614  [160000/481450]\n",
      "loss: 0.355984  [163200/481450]\n",
      "loss: 0.139114  [166400/481450]\n",
      "loss: 0.331783  [169600/481450]\n",
      "loss: 0.131311  [172800/481450]\n",
      "loss: 0.185177  [176000/481450]\n",
      "loss: 0.358406  [179200/481450]\n",
      "loss: 0.462715  [182400/481450]\n",
      "loss: 0.164910  [185600/481450]\n",
      "loss: 0.160775  [188800/481450]\n",
      "loss: 0.144379  [192000/481450]\n",
      "loss: 0.427565  [195200/481450]\n",
      "loss: 0.181540  [198400/481450]\n",
      "loss: 0.379190  [201600/481450]\n",
      "loss: 0.151644  [204800/481450]\n",
      "loss: 0.145017  [208000/481450]\n",
      "loss: 0.418698  [211200/481450]\n",
      "loss: 0.409694  [214400/481450]\n",
      "loss: 0.199797  [217600/481450]\n",
      "loss: 0.278664  [220800/481450]\n",
      "loss: 0.217253  [224000/481450]\n",
      "loss: 0.226288  [227200/481450]\n",
      "loss: 0.178956  [230400/481450]\n",
      "loss: 0.121103  [233600/481450]\n",
      "loss: 0.107959  [236800/481450]\n",
      "loss: 0.476109  [240000/481450]\n",
      "loss: 0.252613  [243200/481450]\n",
      "loss: 0.178898  [246400/481450]\n",
      "loss: 0.115793  [249600/481450]\n",
      "loss: 0.237454  [252800/481450]\n",
      "loss: 0.206884  [256000/481450]\n",
      "loss: 0.394265  [259200/481450]\n",
      "loss: 0.108546  [262400/481450]\n",
      "loss: 0.157229  [265600/481450]\n",
      "loss: 0.233951  [268800/481450]\n",
      "loss: 0.080090  [272000/481450]\n",
      "loss: 0.128598  [275200/481450]\n",
      "loss: 0.146242  [278400/481450]\n",
      "loss: 0.362224  [281600/481450]\n",
      "loss: 0.213202  [284800/481450]\n",
      "loss: 0.311191  [288000/481450]\n",
      "loss: 0.149066  [291200/481450]\n",
      "loss: 0.111656  [294400/481450]\n",
      "loss: 0.300159  [297600/481450]\n",
      "loss: 0.066866  [300800/481450]\n",
      "loss: 0.517465  [304000/481450]\n",
      "loss: 0.245268  [307200/481450]\n",
      "loss: 0.194965  [310400/481450]\n",
      "loss: 0.314921  [313600/481450]\n",
      "loss: 0.150386  [316800/481450]\n",
      "loss: 0.141547  [320000/481450]\n",
      "loss: 0.306218  [323200/481450]\n",
      "loss: 0.265780  [326400/481450]\n",
      "loss: 0.218397  [329600/481450]\n",
      "loss: 0.405047  [332800/481450]\n",
      "loss: 0.175050  [336000/481450]\n",
      "loss: 0.227941  [339200/481450]\n",
      "loss: 0.128181  [342400/481450]\n",
      "loss: 0.288664  [345600/481450]\n",
      "loss: 0.538815  [348800/481450]\n",
      "loss: 0.138801  [352000/481450]\n",
      "loss: 0.378668  [355200/481450]\n",
      "loss: 0.090549  [358400/481450]\n",
      "loss: 0.054777  [361600/481450]\n",
      "loss: 0.348987  [364800/481450]\n",
      "loss: 0.216729  [368000/481450]\n",
      "loss: 0.517554  [371200/481450]\n",
      "loss: 0.468757  [374400/481450]\n",
      "loss: 0.164988  [377600/481450]\n",
      "loss: 0.201021  [380800/481450]\n",
      "loss: 0.281258  [384000/481450]\n",
      "loss: 0.039156  [387200/481450]\n",
      "loss: 0.077834  [390400/481450]\n",
      "loss: 0.200039  [393600/481450]\n",
      "loss: 0.185101  [396800/481450]\n",
      "loss: 0.252111  [400000/481450]\n",
      "loss: 0.308766  [403200/481450]\n",
      "loss: 0.212786  [406400/481450]\n",
      "loss: 0.080296  [409600/481450]\n",
      "loss: 0.439691  [412800/481450]\n",
      "loss: 0.495081  [416000/481450]\n",
      "loss: 0.255596  [419200/481450]\n",
      "loss: 0.090755  [422400/481450]\n",
      "loss: 0.279143  [425600/481450]\n",
      "loss: 0.116168  [428800/481450]\n",
      "loss: 0.150605  [432000/481450]\n",
      "loss: 0.135489  [435200/481450]\n",
      "loss: 0.467447  [438400/481450]\n",
      "loss: 0.229150  [441600/481450]\n",
      "loss: 0.252140  [444800/481450]\n",
      "loss: 0.181049  [448000/481450]\n",
      "loss: 0.362939  [451200/481450]\n",
      "loss: 0.179619  [454400/481450]\n",
      "loss: 0.103930  [457600/481450]\n",
      "loss: 0.150794  [460800/481450]\n",
      "loss: 0.209565  [464000/481450]\n",
      "loss: 0.229122  [467200/481450]\n",
      "loss: 0.288946  [470400/481450]\n",
      "loss: 0.175322  [473600/481450]\n",
      "loss: 0.358753  [476800/481450]\n",
      "loss: 0.140723  [480000/481450]\n",
      "Train Accuracy: 90.7860%\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.509982, F1-score: 85.10% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.090455  [    0/481450]\n",
      "loss: 0.189872  [ 3200/481450]\n",
      "loss: 0.280569  [ 6400/481450]\n",
      "loss: 0.259225  [ 9600/481450]\n",
      "loss: 0.278204  [12800/481450]\n",
      "loss: 0.219545  [16000/481450]\n",
      "loss: 0.230771  [19200/481450]\n",
      "loss: 0.341031  [22400/481450]\n",
      "loss: 0.433400  [25600/481450]\n",
      "loss: 0.152032  [28800/481450]\n",
      "loss: 0.409549  [32000/481450]\n",
      "loss: 0.131366  [35200/481450]\n",
      "loss: 0.284332  [38400/481450]\n",
      "loss: 0.247491  [41600/481450]\n",
      "loss: 0.191715  [44800/481450]\n",
      "loss: 0.519078  [48000/481450]\n",
      "loss: 0.216035  [51200/481450]\n",
      "loss: 0.211679  [54400/481450]\n",
      "loss: 0.408104  [57600/481450]\n",
      "loss: 0.241328  [60800/481450]\n",
      "loss: 0.139193  [64000/481450]\n",
      "loss: 0.204046  [67200/481450]\n",
      "loss: 0.213820  [70400/481450]\n",
      "loss: 0.164178  [73600/481450]\n",
      "loss: 0.195175  [76800/481450]\n",
      "loss: 0.228620  [80000/481450]\n",
      "loss: 0.292078  [83200/481450]\n",
      "loss: 0.163012  [86400/481450]\n",
      "loss: 0.289706  [89600/481450]\n",
      "loss: 0.284199  [92800/481450]\n",
      "loss: 0.240373  [96000/481450]\n",
      "loss: 0.208954  [99200/481450]\n",
      "loss: 0.161412  [102400/481450]\n",
      "loss: 0.193257  [105600/481450]\n",
      "loss: 0.131568  [108800/481450]\n",
      "loss: 0.223349  [112000/481450]\n",
      "loss: 0.203458  [115200/481450]\n",
      "loss: 0.471500  [118400/481450]\n",
      "loss: 0.181177  [121600/481450]\n",
      "loss: 0.214315  [124800/481450]\n",
      "loss: 0.306234  [128000/481450]\n",
      "loss: 0.416924  [131200/481450]\n",
      "loss: 0.178909  [134400/481450]\n",
      "loss: 0.216537  [137600/481450]\n",
      "loss: 0.103330  [140800/481450]\n",
      "loss: 0.167543  [144000/481450]\n",
      "loss: 0.513839  [147200/481450]\n",
      "loss: 0.315792  [150400/481450]\n",
      "loss: 0.228637  [153600/481450]\n",
      "loss: 0.230161  [156800/481450]\n",
      "loss: 0.319841  [160000/481450]\n",
      "loss: 0.108580  [163200/481450]\n",
      "loss: 0.315574  [166400/481450]\n",
      "loss: 0.152007  [169600/481450]\n",
      "loss: 0.261104  [172800/481450]\n",
      "loss: 0.108067  [176000/481450]\n",
      "loss: 0.084604  [179200/481450]\n",
      "loss: 0.298805  [182400/481450]\n",
      "loss: 0.372194  [185600/481450]\n",
      "loss: 0.098363  [188800/481450]\n",
      "loss: 0.437505  [192000/481450]\n",
      "loss: 0.148038  [195200/481450]\n",
      "loss: 0.220137  [198400/481450]\n",
      "loss: 0.305130  [201600/481450]\n",
      "loss: 0.394063  [204800/481450]\n",
      "loss: 0.121976  [208000/481450]\n",
      "loss: 0.186004  [211200/481450]\n",
      "loss: 0.133316  [214400/481450]\n",
      "loss: 0.311604  [217600/481450]\n",
      "loss: 0.218383  [220800/481450]\n",
      "loss: 0.079221  [224000/481450]\n",
      "loss: 0.337694  [227200/481450]\n",
      "loss: 0.157383  [230400/481450]\n",
      "loss: 0.269778  [233600/481450]\n",
      "loss: 0.409447  [236800/481450]\n",
      "loss: 0.322961  [240000/481450]\n",
      "loss: 0.242260  [243200/481450]\n",
      "loss: 0.295681  [246400/481450]\n",
      "loss: 0.377975  [249600/481450]\n",
      "loss: 0.123753  [252800/481450]\n",
      "loss: 0.282027  [256000/481450]\n",
      "loss: 0.255899  [259200/481450]\n",
      "loss: 0.159985  [262400/481450]\n",
      "loss: 0.224752  [265600/481450]\n",
      "loss: 0.265196  [268800/481450]\n",
      "loss: 0.198652  [272000/481450]\n",
      "loss: 0.150852  [275200/481450]\n",
      "loss: 0.423639  [278400/481450]\n",
      "loss: 0.579506  [281600/481450]\n",
      "loss: 0.366401  [284800/481450]\n",
      "loss: 0.188427  [288000/481450]\n",
      "loss: 0.280347  [291200/481450]\n",
      "loss: 0.206561  [294400/481450]\n",
      "loss: 0.352908  [297600/481450]\n",
      "loss: 0.131712  [300800/481450]\n",
      "loss: 0.094793  [304000/481450]\n",
      "loss: 0.283166  [307200/481450]\n",
      "loss: 0.140710  [310400/481450]\n",
      "loss: 0.284492  [313600/481450]\n",
      "loss: 0.177739  [316800/481450]\n",
      "loss: 0.157086  [320000/481450]\n",
      "loss: 0.432134  [323200/481450]\n",
      "loss: 0.114240  [326400/481450]\n",
      "loss: 0.143954  [329600/481450]\n",
      "loss: 0.073707  [332800/481450]\n",
      "loss: 0.315792  [336000/481450]\n",
      "loss: 0.434888  [339200/481450]\n",
      "loss: 0.257721  [342400/481450]\n",
      "loss: 0.091375  [345600/481450]\n",
      "loss: 0.417923  [348800/481450]\n",
      "loss: 0.187171  [352000/481450]\n",
      "loss: 0.303521  [355200/481450]\n",
      "loss: 0.119684  [358400/481450]\n",
      "loss: 0.019011  [361600/481450]\n",
      "loss: 0.296946  [364800/481450]\n",
      "loss: 0.305923  [368000/481450]\n",
      "loss: 0.248275  [371200/481450]\n",
      "loss: 0.165365  [374400/481450]\n",
      "loss: 0.272109  [377600/481450]\n",
      "loss: 0.288329  [380800/481450]\n",
      "loss: 0.240110  [384000/481450]\n",
      "loss: 0.395842  [387200/481450]\n",
      "loss: 0.268944  [390400/481450]\n",
      "loss: 0.119800  [393600/481450]\n",
      "loss: 0.190081  [396800/481450]\n",
      "loss: 0.199943  [400000/481450]\n",
      "loss: 0.279441  [403200/481450]\n",
      "loss: 0.177339  [406400/481450]\n",
      "loss: 0.221575  [409600/481450]\n",
      "loss: 0.272662  [412800/481450]\n",
      "loss: 0.176212  [416000/481450]\n",
      "loss: 0.248841  [419200/481450]\n",
      "loss: 0.127138  [422400/481450]\n",
      "loss: 0.133511  [425600/481450]\n",
      "loss: 0.327829  [428800/481450]\n",
      "loss: 0.189357  [432000/481450]\n",
      "loss: 0.358629  [435200/481450]\n",
      "loss: 0.193759  [438400/481450]\n",
      "loss: 0.142348  [441600/481450]\n",
      "loss: 0.267217  [444800/481450]\n",
      "loss: 0.505079  [448000/481450]\n",
      "loss: 0.108657  [451200/481450]\n",
      "loss: 0.385196  [454400/481450]\n",
      "loss: 0.257460  [457600/481450]\n",
      "loss: 0.208115  [460800/481450]\n",
      "loss: 0.087497  [464000/481450]\n",
      "loss: 0.176157  [467200/481450]\n",
      "loss: 0.176609  [470400/481450]\n",
      "loss: 0.099670  [473600/481450]\n",
      "loss: 0.053146  [476800/481450]\n",
      "loss: 0.147974  [480000/481450]\n",
      "Train Accuracy: 90.8464%\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.498507, F1-score: 85.40% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.127808  [    0/481450]\n",
      "loss: 0.180158  [ 3200/481450]\n",
      "loss: 0.299631  [ 6400/481450]\n",
      "loss: 0.246491  [ 9600/481450]\n",
      "loss: 0.070153  [12800/481450]\n",
      "loss: 0.105169  [16000/481450]\n",
      "loss: 0.184365  [19200/481450]\n",
      "loss: 0.196132  [22400/481450]\n",
      "loss: 0.285872  [25600/481450]\n",
      "loss: 0.276514  [28800/481450]\n",
      "loss: 0.291608  [32000/481450]\n",
      "loss: 0.161472  [35200/481450]\n",
      "loss: 0.147979  [38400/481450]\n",
      "loss: 0.474630  [41600/481450]\n",
      "loss: 0.360272  [44800/481450]\n",
      "loss: 0.304911  [48000/481450]\n",
      "loss: 0.165114  [51200/481450]\n",
      "loss: 0.250044  [54400/481450]\n",
      "loss: 0.306664  [57600/481450]\n",
      "loss: 0.136765  [60800/481450]\n",
      "loss: 0.414514  [64000/481450]\n",
      "loss: 0.226783  [67200/481450]\n",
      "loss: 0.042474  [70400/481450]\n",
      "loss: 0.160643  [73600/481450]\n",
      "loss: 0.178359  [76800/481450]\n",
      "loss: 0.352608  [80000/481450]\n",
      "loss: 0.146803  [83200/481450]\n",
      "loss: 0.327190  [86400/481450]\n",
      "loss: 0.270533  [89600/481450]\n",
      "loss: 0.363183  [92800/481450]\n",
      "loss: 0.386853  [96000/481450]\n",
      "loss: 0.330039  [99200/481450]\n",
      "loss: 0.094956  [102400/481450]\n",
      "loss: 0.304546  [105600/481450]\n",
      "loss: 0.335178  [108800/481450]\n",
      "loss: 0.337608  [112000/481450]\n",
      "loss: 0.273806  [115200/481450]\n",
      "loss: 0.286675  [118400/481450]\n",
      "loss: 0.336824  [121600/481450]\n",
      "loss: 0.093566  [124800/481450]\n",
      "loss: 0.321773  [128000/481450]\n",
      "loss: 0.254424  [131200/481450]\n",
      "loss: 0.208459  [134400/481450]\n",
      "loss: 0.145831  [137600/481450]\n",
      "loss: 0.157669  [140800/481450]\n",
      "loss: 0.248647  [144000/481450]\n",
      "loss: 0.256891  [147200/481450]\n",
      "loss: 0.327636  [150400/481450]\n",
      "loss: 0.378715  [153600/481450]\n",
      "loss: 0.192002  [156800/481450]\n",
      "loss: 0.167521  [160000/481450]\n",
      "loss: 0.168777  [163200/481450]\n",
      "loss: 0.196944  [166400/481450]\n",
      "loss: 0.233717  [169600/481450]\n",
      "loss: 0.263051  [172800/481450]\n",
      "loss: 0.141069  [176000/481450]\n",
      "loss: 0.133164  [179200/481450]\n",
      "loss: 0.301588  [182400/481450]\n",
      "loss: 0.228249  [185600/481450]\n",
      "loss: 0.184588  [188800/481450]\n",
      "loss: 0.289042  [192000/481450]\n",
      "loss: 0.137101  [195200/481450]\n",
      "loss: 0.163703  [198400/481450]\n",
      "loss: 0.223980  [201600/481450]\n",
      "loss: 0.090162  [204800/481450]\n",
      "loss: 0.164130  [208000/481450]\n",
      "loss: 0.301001  [211200/481450]\n",
      "loss: 0.421455  [214400/481450]\n",
      "loss: 0.124356  [217600/481450]\n",
      "loss: 0.082510  [220800/481450]\n",
      "loss: 0.417139  [224000/481450]\n",
      "loss: 0.261677  [227200/481450]\n",
      "loss: 0.393243  [230400/481450]\n",
      "loss: 0.283016  [233600/481450]\n",
      "loss: 0.122478  [236800/481450]\n",
      "loss: 0.126396  [240000/481450]\n",
      "loss: 0.132465  [243200/481450]\n",
      "loss: 0.230460  [246400/481450]\n",
      "loss: 0.171059  [249600/481450]\n",
      "loss: 0.342450  [252800/481450]\n",
      "loss: 0.497418  [256000/481450]\n",
      "loss: 0.162979  [259200/481450]\n",
      "loss: 0.224851  [262400/481450]\n",
      "loss: 0.098376  [265600/481450]\n",
      "loss: 0.099526  [268800/481450]\n",
      "loss: 0.260637  [272000/481450]\n",
      "loss: 0.096840  [275200/481450]\n",
      "loss: 0.198877  [278400/481450]\n",
      "loss: 0.070164  [281600/481450]\n",
      "loss: 0.121037  [284800/481450]\n",
      "loss: 0.320141  [288000/481450]\n",
      "loss: 0.213956  [291200/481450]\n",
      "loss: 0.088274  [294400/481450]\n",
      "loss: 0.270659  [297600/481450]\n",
      "loss: 0.236245  [300800/481450]\n",
      "loss: 0.111986  [304000/481450]\n",
      "loss: 0.177546  [307200/481450]\n",
      "loss: 0.234642  [310400/481450]\n",
      "loss: 0.302818  [313600/481450]\n",
      "loss: 0.171501  [316800/481450]\n",
      "loss: 0.178160  [320000/481450]\n",
      "loss: 0.300162  [323200/481450]\n",
      "loss: 0.168289  [326400/481450]\n",
      "loss: 0.147930  [329600/481450]\n",
      "loss: 0.107491  [332800/481450]\n",
      "loss: 0.261625  [336000/481450]\n",
      "loss: 0.177677  [339200/481450]\n",
      "loss: 0.193744  [342400/481450]\n",
      "loss: 0.180326  [345600/481450]\n",
      "loss: 0.158779  [348800/481450]\n",
      "loss: 0.108882  [352000/481450]\n",
      "loss: 0.384153  [355200/481450]\n",
      "loss: 0.396519  [358400/481450]\n",
      "loss: 0.232138  [361600/481450]\n",
      "loss: 0.184213  [364800/481450]\n",
      "loss: 0.166142  [368000/481450]\n",
      "loss: 0.243352  [371200/481450]\n",
      "loss: 0.194640  [374400/481450]\n",
      "loss: 0.125742  [377600/481450]\n",
      "loss: 0.163508  [380800/481450]\n",
      "loss: 0.110214  [384000/481450]\n",
      "loss: 0.054866  [387200/481450]\n",
      "loss: 0.239939  [390400/481450]\n",
      "loss: 0.226572  [393600/481450]\n",
      "loss: 0.096944  [396800/481450]\n",
      "loss: 0.261898  [400000/481450]\n",
      "loss: 0.150720  [403200/481450]\n",
      "loss: 0.213161  [406400/481450]\n",
      "loss: 0.109518  [409600/481450]\n",
      "loss: 0.224653  [412800/481450]\n",
      "loss: 0.231021  [416000/481450]\n",
      "loss: 0.168599  [419200/481450]\n",
      "loss: 0.037748  [422400/481450]\n",
      "loss: 0.213019  [425600/481450]\n",
      "loss: 0.359669  [428800/481450]\n",
      "loss: 0.173099  [432000/481450]\n",
      "loss: 0.433764  [435200/481450]\n",
      "loss: 0.093395  [438400/481450]\n",
      "loss: 0.174430  [441600/481450]\n",
      "loss: 0.204935  [444800/481450]\n",
      "loss: 0.358645  [448000/481450]\n",
      "loss: 0.404160  [451200/481450]\n",
      "loss: 0.117284  [454400/481450]\n",
      "loss: 0.223689  [457600/481450]\n",
      "loss: 0.338955  [460800/481450]\n",
      "loss: 0.150577  [464000/481450]\n",
      "loss: 0.234516  [467200/481450]\n",
      "loss: 0.359268  [470400/481450]\n",
      "loss: 0.112493  [473600/481450]\n",
      "loss: 0.151261  [476800/481450]\n",
      "loss: 0.049158  [480000/481450]\n",
      "Train Accuracy: 90.9401%\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.497553, F1-score: 85.50% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.191421  [    0/481450]\n",
      "loss: 0.087901  [ 3200/481450]\n",
      "loss: 0.112699  [ 6400/481450]\n",
      "loss: 0.465857  [ 9600/481450]\n",
      "loss: 0.170562  [12800/481450]\n",
      "loss: 0.190875  [16000/481450]\n",
      "loss: 0.080613  [19200/481450]\n",
      "loss: 0.259269  [22400/481450]\n",
      "loss: 0.254862  [25600/481450]\n",
      "loss: 0.077621  [28800/481450]\n",
      "loss: 0.083015  [32000/481450]\n",
      "loss: 0.162521  [35200/481450]\n",
      "loss: 0.172388  [38400/481450]\n",
      "loss: 0.281690  [41600/481450]\n",
      "loss: 0.207768  [44800/481450]\n",
      "loss: 0.477435  [48000/481450]\n",
      "loss: 0.174560  [51200/481450]\n",
      "loss: 0.425006  [54400/481450]\n",
      "loss: 0.188653  [57600/481450]\n",
      "loss: 0.200188  [60800/481450]\n",
      "loss: 0.288538  [64000/481450]\n",
      "loss: 0.415472  [67200/481450]\n",
      "loss: 0.129460  [70400/481450]\n",
      "loss: 0.264744  [73600/481450]\n",
      "loss: 0.301231  [76800/481450]\n",
      "loss: 0.254437  [80000/481450]\n",
      "loss: 0.300800  [83200/481450]\n",
      "loss: 0.638075  [86400/481450]\n",
      "loss: 0.270483  [89600/481450]\n",
      "loss: 0.092768  [92800/481450]\n",
      "loss: 0.441047  [96000/481450]\n",
      "loss: 0.258075  [99200/481450]\n",
      "loss: 0.300899  [102400/481450]\n",
      "loss: 0.437134  [105600/481450]\n",
      "loss: 0.200702  [108800/481450]\n",
      "loss: 0.051837  [112000/481450]\n",
      "loss: 0.258496  [115200/481450]\n",
      "loss: 0.292255  [118400/481450]\n",
      "loss: 0.124476  [121600/481450]\n",
      "loss: 0.283486  [124800/481450]\n",
      "loss: 0.254036  [128000/481450]\n",
      "loss: 0.103376  [131200/481450]\n",
      "loss: 0.276122  [134400/481450]\n",
      "loss: 0.265232  [137600/481450]\n",
      "loss: 0.078546  [140800/481450]\n",
      "loss: 0.146709  [144000/481450]\n",
      "loss: 0.093207  [147200/481450]\n",
      "loss: 0.238412  [150400/481450]\n",
      "loss: 0.231123  [153600/481450]\n",
      "loss: 0.086679  [156800/481450]\n",
      "loss: 0.159541  [160000/481450]\n",
      "loss: 0.111781  [163200/481450]\n",
      "loss: 0.253186  [166400/481450]\n",
      "loss: 0.375893  [169600/481450]\n",
      "loss: 0.313777  [172800/481450]\n",
      "loss: 0.336130  [176000/481450]\n",
      "loss: 0.147925  [179200/481450]\n",
      "loss: 0.125083  [182400/481450]\n",
      "loss: 0.201098  [185600/481450]\n",
      "loss: 0.207958  [188800/481450]\n",
      "loss: 0.292738  [192000/481450]\n",
      "loss: 0.327012  [195200/481450]\n",
      "loss: 0.155477  [198400/481450]\n",
      "loss: 0.181606  [201600/481450]\n",
      "loss: 0.161492  [204800/481450]\n",
      "loss: 0.224425  [208000/481450]\n",
      "loss: 0.096211  [211200/481450]\n",
      "loss: 0.140384  [214400/481450]\n",
      "loss: 0.211370  [217600/481450]\n",
      "loss: 0.209141  [220800/481450]\n",
      "loss: 0.072304  [224000/481450]\n",
      "loss: 0.441165  [227200/481450]\n",
      "loss: 0.180432  [230400/481450]\n",
      "loss: 0.245109  [233600/481450]\n",
      "loss: 0.155150  [236800/481450]\n",
      "loss: 0.222019  [240000/481450]\n",
      "loss: 0.159114  [243200/481450]\n",
      "loss: 0.130481  [246400/481450]\n",
      "loss: 0.265720  [249600/481450]\n",
      "loss: 0.156307  [252800/481450]\n",
      "loss: 0.316740  [256000/481450]\n",
      "loss: 0.497509  [259200/481450]\n",
      "loss: 0.159531  [262400/481450]\n",
      "loss: 0.258895  [265600/481450]\n",
      "loss: 0.267755  [268800/481450]\n",
      "loss: 0.211421  [272000/481450]\n",
      "loss: 0.256432  [275200/481450]\n",
      "loss: 0.216529  [278400/481450]\n",
      "loss: 0.085305  [281600/481450]\n",
      "loss: 0.098755  [284800/481450]\n",
      "loss: 0.122826  [288000/481450]\n",
      "loss: 0.229688  [291200/481450]\n",
      "loss: 0.123662  [294400/481450]\n",
      "loss: 0.291302  [297600/481450]\n",
      "loss: 0.214514  [300800/481450]\n",
      "loss: 0.159912  [304000/481450]\n",
      "loss: 0.102835  [307200/481450]\n",
      "loss: 0.153063  [310400/481450]\n",
      "loss: 0.220158  [313600/481450]\n",
      "loss: 0.163047  [316800/481450]\n",
      "loss: 0.270473  [320000/481450]\n",
      "loss: 0.365227  [323200/481450]\n",
      "loss: 0.080639  [326400/481450]\n",
      "loss: 0.337912  [329600/481450]\n",
      "loss: 0.089348  [332800/481450]\n",
      "loss: 0.182964  [336000/481450]\n",
      "loss: 0.156059  [339200/481450]\n",
      "loss: 0.470723  [342400/481450]\n",
      "loss: 0.178431  [345600/481450]\n",
      "loss: 0.106688  [348800/481450]\n",
      "loss: 0.116877  [352000/481450]\n",
      "loss: 0.259404  [355200/481450]\n",
      "loss: 0.112889  [358400/481450]\n",
      "loss: 0.149385  [361600/481450]\n",
      "loss: 0.231161  [364800/481450]\n",
      "loss: 0.264832  [368000/481450]\n",
      "loss: 0.356353  [371200/481450]\n",
      "loss: 0.110579  [374400/481450]\n",
      "loss: 0.288545  [377600/481450]\n",
      "loss: 0.171275  [380800/481450]\n",
      "loss: 0.074181  [384000/481450]\n",
      "loss: 0.282789  [387200/481450]\n",
      "loss: 0.069789  [390400/481450]\n",
      "loss: 0.094188  [393600/481450]\n",
      "loss: 0.278761  [396800/481450]\n",
      "loss: 0.254231  [400000/481450]\n",
      "loss: 0.191345  [403200/481450]\n",
      "loss: 0.120957  [406400/481450]\n",
      "loss: 0.285036  [409600/481450]\n",
      "loss: 0.139871  [412800/481450]\n",
      "loss: 0.088092  [416000/481450]\n",
      "loss: 0.222357  [419200/481450]\n",
      "loss: 0.291855  [422400/481450]\n",
      "loss: 0.295615  [425600/481450]\n",
      "loss: 0.271876  [428800/481450]\n",
      "loss: 0.399356  [432000/481450]\n",
      "loss: 0.024845  [435200/481450]\n",
      "loss: 0.286641  [438400/481450]\n",
      "loss: 0.221110  [441600/481450]\n",
      "loss: 0.297817  [444800/481450]\n",
      "loss: 0.114757  [448000/481450]\n",
      "loss: 0.230993  [451200/481450]\n",
      "loss: 0.135712  [454400/481450]\n",
      "loss: 0.478003  [457600/481450]\n",
      "loss: 0.408470  [460800/481450]\n",
      "loss: 0.197358  [464000/481450]\n",
      "loss: 0.281412  [467200/481450]\n",
      "loss: 0.373143  [470400/481450]\n",
      "loss: 0.247008  [473600/481450]\n",
      "loss: 0.147054  [476800/481450]\n",
      "loss: 0.208755  [480000/481450]\n",
      "Train Accuracy: 90.9559%\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.550858, F1-score: 85.56% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.118758  [    0/481450]\n",
      "loss: 0.145953  [ 3200/481450]\n",
      "loss: 0.422773  [ 6400/481450]\n",
      "loss: 0.369682  [ 9600/481450]\n",
      "loss: 0.187395  [12800/481450]\n",
      "loss: 0.223566  [16000/481450]\n",
      "loss: 0.173948  [19200/481450]\n",
      "loss: 0.090307  [22400/481450]\n",
      "loss: 0.173116  [25600/481450]\n",
      "loss: 0.356773  [28800/481450]\n",
      "loss: 0.150396  [32000/481450]\n",
      "loss: 0.253677  [35200/481450]\n",
      "loss: 0.163523  [38400/481450]\n",
      "loss: 0.123907  [41600/481450]\n",
      "loss: 0.245102  [44800/481450]\n",
      "loss: 0.133768  [48000/481450]\n",
      "loss: 0.318494  [51200/481450]\n",
      "loss: 0.048253  [54400/481450]\n",
      "loss: 0.435742  [57600/481450]\n",
      "loss: 0.146609  [60800/481450]\n",
      "loss: 0.097354  [64000/481450]\n",
      "loss: 0.208227  [67200/481450]\n",
      "loss: 0.154489  [70400/481450]\n",
      "loss: 0.143555  [73600/481450]\n",
      "loss: 0.417838  [76800/481450]\n",
      "loss: 0.234780  [80000/481450]\n",
      "loss: 0.223175  [83200/481450]\n",
      "loss: 0.287431  [86400/481450]\n",
      "loss: 0.282880  [89600/481450]\n",
      "loss: 0.256049  [92800/481450]\n",
      "loss: 0.174271  [96000/481450]\n",
      "loss: 0.206995  [99200/481450]\n",
      "loss: 0.309260  [102400/481450]\n",
      "loss: 0.039647  [105600/481450]\n",
      "loss: 0.091894  [108800/481450]\n",
      "loss: 0.154407  [112000/481450]\n",
      "loss: 0.239734  [115200/481450]\n",
      "loss: 0.229601  [118400/481450]\n",
      "loss: 0.284213  [121600/481450]\n",
      "loss: 0.122983  [124800/481450]\n",
      "loss: 0.350216  [128000/481450]\n",
      "loss: 0.351942  [131200/481450]\n",
      "loss: 0.385536  [134400/481450]\n",
      "loss: 0.550936  [137600/481450]\n",
      "loss: 0.303495  [140800/481450]\n",
      "loss: 0.327728  [144000/481450]\n",
      "loss: 0.315704  [147200/481450]\n",
      "loss: 0.322417  [150400/481450]\n",
      "loss: 0.015520  [153600/481450]\n",
      "loss: 0.271302  [156800/481450]\n",
      "loss: 0.316007  [160000/481450]\n",
      "loss: 0.333869  [163200/481450]\n",
      "loss: 0.290349  [166400/481450]\n",
      "loss: 0.113618  [169600/481450]\n",
      "loss: 0.418879  [172800/481450]\n",
      "loss: 0.185002  [176000/481450]\n",
      "loss: 0.283924  [179200/481450]\n",
      "loss: 0.337939  [182400/481450]\n",
      "loss: 0.369213  [185600/481450]\n",
      "loss: 0.149790  [188800/481450]\n",
      "loss: 0.133770  [192000/481450]\n",
      "loss: 0.366970  [195200/481450]\n",
      "loss: 0.191359  [198400/481450]\n",
      "loss: 0.162306  [201600/481450]\n",
      "loss: 0.123088  [204800/481450]\n",
      "loss: 0.169131  [208000/481450]\n",
      "loss: 0.084495  [211200/481450]\n",
      "loss: 0.138538  [214400/481450]\n",
      "loss: 0.264092  [217600/481450]\n",
      "loss: 0.316690  [220800/481450]\n",
      "loss: 0.200757  [224000/481450]\n",
      "loss: 0.253265  [227200/481450]\n",
      "loss: 0.141366  [230400/481450]\n",
      "loss: 0.434029  [233600/481450]\n",
      "loss: 0.155388  [236800/481450]\n",
      "loss: 0.304276  [240000/481450]\n",
      "loss: 0.324115  [243200/481450]\n",
      "loss: 0.347838  [246400/481450]\n",
      "loss: 0.063269  [249600/481450]\n",
      "loss: 0.333715  [252800/481450]\n",
      "loss: 0.112526  [256000/481450]\n",
      "loss: 0.265306  [259200/481450]\n",
      "loss: 0.349189  [262400/481450]\n",
      "loss: 0.173545  [265600/481450]\n",
      "loss: 0.292773  [268800/481450]\n",
      "loss: 0.470410  [272000/481450]\n",
      "loss: 0.275280  [275200/481450]\n",
      "loss: 0.200195  [278400/481450]\n",
      "loss: 0.099947  [281600/481450]\n",
      "loss: 0.112956  [284800/481450]\n",
      "loss: 0.244200  [288000/481450]\n",
      "loss: 0.314553  [291200/481450]\n",
      "loss: 0.177524  [294400/481450]\n",
      "loss: 0.201515  [297600/481450]\n",
      "loss: 0.273944  [300800/481450]\n",
      "loss: 0.299381  [304000/481450]\n",
      "loss: 0.049647  [307200/481450]\n",
      "loss: 0.199435  [310400/481450]\n",
      "loss: 0.285068  [313600/481450]\n",
      "loss: 0.537255  [316800/481450]\n",
      "loss: 0.359786  [320000/481450]\n",
      "loss: 0.112363  [323200/481450]\n",
      "loss: 0.181903  [326400/481450]\n",
      "loss: 0.105772  [329600/481450]\n",
      "loss: 0.138824  [332800/481450]\n",
      "loss: 0.133532  [336000/481450]\n",
      "loss: 0.248552  [339200/481450]\n",
      "loss: 0.065998  [342400/481450]\n",
      "loss: 0.262351  [345600/481450]\n",
      "loss: 0.295228  [348800/481450]\n",
      "loss: 0.147966  [352000/481450]\n",
      "loss: 0.305508  [355200/481450]\n",
      "loss: 0.238227  [358400/481450]\n",
      "loss: 0.157147  [361600/481450]\n",
      "loss: 0.246621  [364800/481450]\n",
      "loss: 0.081387  [368000/481450]\n",
      "loss: 0.069703  [371200/481450]\n",
      "loss: 0.203423  [374400/481450]\n",
      "loss: 0.204097  [377600/481450]\n",
      "loss: 0.319538  [380800/481450]\n",
      "loss: 0.223468  [384000/481450]\n",
      "loss: 0.150784  [387200/481450]\n",
      "loss: 0.456918  [390400/481450]\n",
      "loss: 0.523254  [393600/481450]\n",
      "loss: 0.049155  [396800/481450]\n",
      "loss: 0.315689  [400000/481450]\n",
      "loss: 0.177485  [403200/481450]\n",
      "loss: 0.200514  [406400/481450]\n",
      "loss: 0.025003  [409600/481450]\n",
      "loss: 0.164655  [412800/481450]\n",
      "loss: 0.479362  [416000/481450]\n",
      "loss: 0.128235  [419200/481450]\n",
      "loss: 0.171345  [422400/481450]\n",
      "loss: 0.279253  [425600/481450]\n",
      "loss: 0.148646  [428800/481450]\n",
      "loss: 0.223750  [432000/481450]\n",
      "loss: 0.115274  [435200/481450]\n",
      "loss: 0.126110  [438400/481450]\n",
      "loss: 0.151927  [441600/481450]\n",
      "loss: 0.209824  [444800/481450]\n",
      "loss: 0.304484  [448000/481450]\n",
      "loss: 0.191218  [451200/481450]\n",
      "loss: 0.181746  [454400/481450]\n",
      "loss: 0.114464  [457600/481450]\n",
      "loss: 0.242944  [460800/481450]\n",
      "loss: 0.345402  [464000/481450]\n",
      "loss: 0.190297  [467200/481450]\n",
      "loss: 0.252577  [470400/481450]\n",
      "loss: 0.238951  [473600/481450]\n",
      "loss: 0.248713  [476800/481450]\n",
      "loss: 0.120813  [480000/481450]\n",
      "Train Accuracy: 91.0259%\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.592966, F1-score: 85.32% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.168084  [    0/481450]\n",
      "loss: 0.181060  [ 3200/481450]\n",
      "loss: 0.233560  [ 6400/481450]\n",
      "loss: 0.236654  [ 9600/481450]\n",
      "loss: 0.307989  [12800/481450]\n",
      "loss: 0.132279  [16000/481450]\n",
      "loss: 0.092332  [19200/481450]\n",
      "loss: 0.364082  [22400/481450]\n",
      "loss: 0.241129  [25600/481450]\n",
      "loss: 0.278858  [28800/481450]\n",
      "loss: 0.116607  [32000/481450]\n",
      "loss: 0.314535  [35200/481450]\n",
      "loss: 0.211626  [38400/481450]\n",
      "loss: 0.232022  [41600/481450]\n",
      "loss: 0.102616  [44800/481450]\n",
      "loss: 0.201091  [48000/481450]\n",
      "loss: 0.246883  [51200/481450]\n",
      "loss: 0.098735  [54400/481450]\n",
      "loss: 0.188519  [57600/481450]\n",
      "loss: 0.118360  [60800/481450]\n",
      "loss: 0.173522  [64000/481450]\n",
      "loss: 0.176280  [67200/481450]\n",
      "loss: 0.183430  [70400/481450]\n",
      "loss: 0.112972  [73600/481450]\n",
      "loss: 0.368135  [76800/481450]\n",
      "loss: 0.037277  [80000/481450]\n",
      "loss: 0.077695  [83200/481450]\n",
      "loss: 0.076133  [86400/481450]\n",
      "loss: 0.167040  [89600/481450]\n",
      "loss: 0.176870  [92800/481450]\n",
      "loss: 0.188706  [96000/481450]\n",
      "loss: 0.150368  [99200/481450]\n",
      "loss: 0.145898  [102400/481450]\n",
      "loss: 0.329352  [105600/481450]\n",
      "loss: 0.090334  [108800/481450]\n",
      "loss: 0.192741  [112000/481450]\n",
      "loss: 0.213699  [115200/481450]\n",
      "loss: 0.206084  [118400/481450]\n",
      "loss: 0.263792  [121600/481450]\n",
      "loss: 0.085383  [124800/481450]\n",
      "loss: 0.176540  [128000/481450]\n",
      "loss: 0.160440  [131200/481450]\n",
      "loss: 0.368703  [134400/481450]\n",
      "loss: 0.099433  [137600/481450]\n",
      "loss: 0.204781  [140800/481450]\n",
      "loss: 0.180000  [144000/481450]\n",
      "loss: 0.268639  [147200/481450]\n",
      "loss: 0.147949  [150400/481450]\n",
      "loss: 0.432653  [153600/481450]\n",
      "loss: 0.203065  [156800/481450]\n",
      "loss: 0.172473  [160000/481450]\n",
      "loss: 0.158540  [163200/481450]\n",
      "loss: 0.407232  [166400/481450]\n",
      "loss: 0.129399  [169600/481450]\n",
      "loss: 0.161049  [172800/481450]\n",
      "loss: 0.110244  [176000/481450]\n",
      "loss: 0.264292  [179200/481450]\n",
      "loss: 0.161768  [182400/481450]\n",
      "loss: 0.346112  [185600/481450]\n",
      "loss: 0.264690  [188800/481450]\n",
      "loss: 0.172806  [192000/481450]\n",
      "loss: 0.176368  [195200/481450]\n",
      "loss: 0.197402  [198400/481450]\n",
      "loss: 0.331500  [201600/481450]\n",
      "loss: 0.093014  [204800/481450]\n",
      "loss: 0.240001  [208000/481450]\n",
      "loss: 0.245702  [211200/481450]\n",
      "loss: 0.136569  [214400/481450]\n",
      "loss: 0.322257  [217600/481450]\n",
      "loss: 0.233818  [220800/481450]\n",
      "loss: 0.427379  [224000/481450]\n",
      "loss: 0.143052  [227200/481450]\n",
      "loss: 0.109527  [230400/481450]\n",
      "loss: 0.141372  [233600/481450]\n",
      "loss: 0.126813  [236800/481450]\n",
      "loss: 0.283675  [240000/481450]\n",
      "loss: 0.069476  [243200/481450]\n",
      "loss: 0.324435  [246400/481450]\n",
      "loss: 0.231125  [249600/481450]\n",
      "loss: 0.218495  [252800/481450]\n",
      "loss: 0.195834  [256000/481450]\n",
      "loss: 0.501428  [259200/481450]\n",
      "loss: 0.175097  [262400/481450]\n",
      "loss: 0.252906  [265600/481450]\n",
      "loss: 0.047518  [268800/481450]\n",
      "loss: 0.043166  [272000/481450]\n",
      "loss: 0.319703  [275200/481450]\n",
      "loss: 0.288205  [278400/481450]\n",
      "loss: 0.123668  [281600/481450]\n",
      "loss: 0.065829  [284800/481450]\n",
      "loss: 0.281941  [288000/481450]\n",
      "loss: 0.141660  [291200/481450]\n",
      "loss: 0.111120  [294400/481450]\n",
      "loss: 0.184042  [297600/481450]\n",
      "loss: 0.346402  [300800/481450]\n",
      "loss: 0.237786  [304000/481450]\n",
      "loss: 0.231808  [307200/481450]\n",
      "loss: 0.053335  [310400/481450]\n",
      "loss: 0.273318  [313600/481450]\n",
      "loss: 0.207693  [316800/481450]\n",
      "loss: 0.217237  [320000/481450]\n",
      "loss: 0.465887  [323200/481450]\n",
      "loss: 0.136745  [326400/481450]\n",
      "loss: 0.206573  [329600/481450]\n",
      "loss: 0.137225  [332800/481450]\n",
      "loss: 0.251571  [336000/481450]\n",
      "loss: 0.103014  [339200/481450]\n",
      "loss: 0.064637  [342400/481450]\n",
      "loss: 0.075239  [345600/481450]\n",
      "loss: 0.203278  [348800/481450]\n",
      "loss: 0.115871  [352000/481450]\n",
      "loss: 0.201603  [355200/481450]\n",
      "loss: 0.361304  [358400/481450]\n",
      "loss: 0.204613  [361600/481450]\n",
      "loss: 0.175656  [364800/481450]\n",
      "loss: 0.121278  [368000/481450]\n",
      "loss: 0.141404  [371200/481450]\n",
      "loss: 0.205463  [374400/481450]\n",
      "loss: 0.235482  [377600/481450]\n",
      "loss: 0.224550  [380800/481450]\n",
      "loss: 0.329445  [384000/481450]\n",
      "loss: 0.228965  [387200/481450]\n",
      "loss: 0.318196  [390400/481450]\n",
      "loss: 0.616948  [393600/481450]\n",
      "loss: 0.198114  [396800/481450]\n",
      "loss: 0.223589  [400000/481450]\n",
      "loss: 0.126992  [403200/481450]\n",
      "loss: 0.174135  [406400/481450]\n",
      "loss: 0.229918  [409600/481450]\n",
      "loss: 0.287918  [412800/481450]\n",
      "loss: 0.377142  [416000/481450]\n",
      "loss: 0.108063  [419200/481450]\n",
      "loss: 0.149148  [422400/481450]\n",
      "loss: 0.206158  [425600/481450]\n",
      "loss: 0.341612  [428800/481450]\n",
      "loss: 0.245166  [432000/481450]\n",
      "loss: 0.395434  [435200/481450]\n",
      "loss: 0.306108  [438400/481450]\n",
      "loss: 0.209658  [441600/481450]\n",
      "loss: 0.117018  [444800/481450]\n",
      "loss: 0.239951  [448000/481450]\n",
      "loss: 0.253344  [451200/481450]\n",
      "loss: 0.146608  [454400/481450]\n",
      "loss: 0.063959  [457600/481450]\n",
      "loss: 0.269506  [460800/481450]\n",
      "loss: 0.169880  [464000/481450]\n",
      "loss: 0.127906  [467200/481450]\n",
      "loss: 0.227124  [470400/481450]\n",
      "loss: 0.199560  [473600/481450]\n",
      "loss: 0.239785  [476800/481450]\n",
      "loss: 0.175828  [480000/481450]\n",
      "Train Accuracy: 91.0763%\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.590213, F1-score: 85.20% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431a6d93-0220-4085-bb3f-8965b97240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e34344-c96e-42c9-b318-96c5e291cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299554  [    0/481450]\n",
      "loss: 2.301553  [ 3200/481450]\n",
      "loss: 2.295126  [ 6400/481450]\n",
      "loss: 2.291452  [ 9600/481450]\n",
      "loss: 2.284972  [12800/481450]\n",
      "loss: 2.269739  [16000/481450]\n",
      "loss: 2.189251  [19200/481450]\n",
      "loss: 2.225468  [22400/481450]\n",
      "loss: 2.036146  [25600/481450]\n",
      "loss: 2.160871  [28800/481450]\n",
      "loss: 1.978571  [32000/481450]\n",
      "loss: 1.906189  [35200/481450]\n",
      "loss: 1.914405  [38400/481450]\n",
      "loss: 1.971960  [41600/481450]\n",
      "loss: 1.897991  [44800/481450]\n",
      "loss: 1.906381  [48000/481450]\n",
      "loss: 1.966027  [51200/481450]\n",
      "loss: 1.875139  [54400/481450]\n",
      "loss: 1.954501  [57600/481450]\n",
      "loss: 1.790950  [60800/481450]\n",
      "loss: 1.728896  [64000/481450]\n",
      "loss: 1.609065  [67200/481450]\n",
      "loss: 1.730175  [70400/481450]\n",
      "loss: 1.890268  [73600/481450]\n",
      "loss: 1.607302  [76800/481450]\n",
      "loss: 1.749880  [80000/481450]\n",
      "loss: 1.549158  [83200/481450]\n",
      "loss: 1.623417  [86400/481450]\n",
      "loss: 1.741142  [89600/481450]\n",
      "loss: 1.583601  [92800/481450]\n",
      "loss: 1.636041  [96000/481450]\n",
      "loss: 1.515680  [99200/481450]\n",
      "loss: 1.551420  [102400/481450]\n",
      "loss: 1.397777  [105600/481450]\n",
      "loss: 1.732817  [108800/481450]\n",
      "loss: 1.500405  [112000/481450]\n",
      "loss: 1.610054  [115200/481450]\n",
      "loss: 1.385825  [118400/481450]\n",
      "loss: 1.503126  [121600/481450]\n",
      "loss: 1.468029  [124800/481450]\n",
      "loss: 1.701162  [128000/481450]\n",
      "loss: 1.466733  [131200/481450]\n",
      "loss: 1.447070  [134400/481450]\n",
      "loss: 1.540469  [137600/481450]\n",
      "loss: 1.366319  [140800/481450]\n",
      "loss: 1.447986  [144000/481450]\n",
      "loss: 1.383605  [147200/481450]\n",
      "loss: 1.522320  [150400/481450]\n",
      "loss: 1.281763  [153600/481450]\n",
      "loss: 1.624419  [156800/481450]\n",
      "loss: 1.485643  [160000/481450]\n",
      "loss: 1.399402  [163200/481450]\n",
      "loss: 1.382413  [166400/481450]\n",
      "loss: 1.142830  [169600/481450]\n",
      "loss: 1.302864  [172800/481450]\n",
      "loss: 1.136420  [176000/481450]\n",
      "loss: 1.144632  [179200/481450]\n",
      "loss: 1.411052  [182400/481450]\n",
      "loss: 1.150003  [185600/481450]\n",
      "loss: 1.210703  [188800/481450]\n",
      "loss: 1.209647  [192000/481450]\n",
      "loss: 1.184294  [195200/481450]\n",
      "loss: 1.256051  [198400/481450]\n",
      "loss: 1.118176  [201600/481450]\n",
      "loss: 0.835797  [204800/481450]\n",
      "loss: 0.945242  [208000/481450]\n",
      "loss: 1.040646  [211200/481450]\n",
      "loss: 0.854339  [214400/481450]\n",
      "loss: 1.246210  [217600/481450]\n",
      "loss: 1.007830  [220800/481450]\n",
      "loss: 0.850981  [224000/481450]\n",
      "loss: 0.853289  [227200/481450]\n",
      "loss: 0.866751  [230400/481450]\n",
      "loss: 1.062877  [233600/481450]\n",
      "loss: 1.012681  [236800/481450]\n",
      "loss: 0.911072  [240000/481450]\n",
      "loss: 1.202080  [243200/481450]\n",
      "loss: 0.919028  [246400/481450]\n",
      "loss: 0.967026  [249600/481450]\n",
      "loss: 0.895167  [252800/481450]\n",
      "loss: 0.757264  [256000/481450]\n",
      "loss: 1.023457  [259200/481450]\n",
      "loss: 0.859670  [262400/481450]\n",
      "loss: 0.994559  [265600/481450]\n",
      "loss: 0.997858  [268800/481450]\n",
      "loss: 1.078647  [272000/481450]\n",
      "loss: 0.928273  [275200/481450]\n",
      "loss: 0.874435  [278400/481450]\n",
      "loss: 0.869427  [281600/481450]\n",
      "loss: 0.742351  [284800/481450]\n",
      "loss: 1.025938  [288000/481450]\n",
      "loss: 0.956714  [291200/481450]\n",
      "loss: 0.928800  [294400/481450]\n",
      "loss: 0.886214  [297600/481450]\n",
      "loss: 0.800780  [300800/481450]\n",
      "loss: 0.763357  [304000/481450]\n",
      "loss: 0.851758  [307200/481450]\n",
      "loss: 0.589397  [310400/481450]\n",
      "loss: 0.755204  [313600/481450]\n",
      "loss: 0.607739  [316800/481450]\n",
      "loss: 0.753753  [320000/481450]\n",
      "loss: 0.631726  [323200/481450]\n",
      "loss: 0.518193  [326400/481450]\n",
      "loss: 0.787731  [329600/481450]\n",
      "loss: 0.732393  [332800/481450]\n",
      "loss: 0.584266  [336000/481450]\n",
      "loss: 0.475160  [339200/481450]\n",
      "loss: 0.489937  [342400/481450]\n",
      "loss: 0.582605  [345600/481450]\n",
      "loss: 0.684758  [348800/481450]\n",
      "loss: 0.744645  [352000/481450]\n",
      "loss: 0.657788  [355200/481450]\n",
      "loss: 0.685893  [358400/481450]\n",
      "loss: 0.611006  [361600/481450]\n",
      "loss: 0.691291  [364800/481450]\n",
      "loss: 0.478617  [368000/481450]\n",
      "loss: 0.719696  [371200/481450]\n",
      "loss: 0.608254  [374400/481450]\n",
      "loss: 0.481778  [377600/481450]\n",
      "loss: 0.710812  [380800/481450]\n",
      "loss: 0.624868  [384000/481450]\n",
      "loss: 0.570412  [387200/481450]\n",
      "loss: 0.635719  [390400/481450]\n",
      "loss: 0.877773  [393600/481450]\n",
      "loss: 0.755940  [396800/481450]\n",
      "loss: 0.776570  [400000/481450]\n",
      "loss: 0.742357  [403200/481450]\n",
      "loss: 0.608174  [406400/481450]\n",
      "loss: 0.623187  [409600/481450]\n",
      "loss: 0.924247  [412800/481450]\n",
      "loss: 0.572516  [416000/481450]\n",
      "loss: 0.757366  [419200/481450]\n",
      "loss: 0.621336  [422400/481450]\n",
      "loss: 0.666014  [425600/481450]\n",
      "loss: 0.542630  [428800/481450]\n",
      "loss: 0.556336  [432000/481450]\n",
      "loss: 0.635425  [435200/481450]\n",
      "loss: 0.455155  [438400/481450]\n",
      "loss: 0.369195  [441600/481450]\n",
      "loss: 0.381284  [444800/481450]\n",
      "loss: 0.516927  [448000/481450]\n",
      "loss: 0.434112  [451200/481450]\n",
      "loss: 0.505051  [454400/481450]\n",
      "loss: 0.503181  [457600/481450]\n",
      "loss: 0.426077  [460800/481450]\n",
      "loss: 0.477459  [464000/481450]\n",
      "loss: 0.301170  [467200/481450]\n",
      "loss: 0.599495  [470400/481450]\n",
      "loss: 0.638821  [473600/481450]\n",
      "loss: 0.743693  [476800/481450]\n",
      "loss: 0.553819  [480000/481450]\n",
      "Train Accuracy: 59.0832%\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.464275, F1-score: 84.84% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.549526  [    0/481450]\n",
      "loss: 0.416353  [ 3200/481450]\n",
      "loss: 0.535704  [ 6400/481450]\n",
      "loss: 0.648393  [ 9600/481450]\n",
      "loss: 0.603772  [12800/481450]\n",
      "loss: 0.473544  [16000/481450]\n",
      "loss: 0.559136  [19200/481450]\n",
      "loss: 0.624310  [22400/481450]\n",
      "loss: 0.554427  [25600/481450]\n",
      "loss: 0.522819  [28800/481450]\n",
      "loss: 0.494496  [32000/481450]\n",
      "loss: 0.319717  [35200/481450]\n",
      "loss: 0.500331  [38400/481450]\n",
      "loss: 0.405252  [41600/481450]\n",
      "loss: 0.516679  [44800/481450]\n",
      "loss: 0.423506  [48000/481450]\n",
      "loss: 0.451188  [51200/481450]\n",
      "loss: 0.485627  [54400/481450]\n",
      "loss: 0.615074  [57600/481450]\n",
      "loss: 0.651293  [60800/481450]\n",
      "loss: 0.376539  [64000/481450]\n",
      "loss: 0.499610  [67200/481450]\n",
      "loss: 0.681735  [70400/481450]\n",
      "loss: 0.407024  [73600/481450]\n",
      "loss: 0.442168  [76800/481450]\n",
      "loss: 0.416999  [80000/481450]\n",
      "loss: 0.403378  [83200/481450]\n",
      "loss: 0.531816  [86400/481450]\n",
      "loss: 0.470179  [89600/481450]\n",
      "loss: 0.425767  [92800/481450]\n",
      "loss: 0.401102  [96000/481450]\n",
      "loss: 0.321074  [99200/481450]\n",
      "loss: 0.516974  [102400/481450]\n",
      "loss: 0.315020  [105600/481450]\n",
      "loss: 0.169802  [108800/481450]\n",
      "loss: 0.466422  [112000/481450]\n",
      "loss: 0.402740  [115200/481450]\n",
      "loss: 0.536289  [118400/481450]\n",
      "loss: 0.480476  [121600/481450]\n",
      "loss: 0.655534  [124800/481450]\n",
      "loss: 0.445576  [128000/481450]\n",
      "loss: 0.642903  [131200/481450]\n",
      "loss: 0.349741  [134400/481450]\n",
      "loss: 0.452668  [137600/481450]\n",
      "loss: 0.376170  [140800/481450]\n",
      "loss: 0.526224  [144000/481450]\n",
      "loss: 0.949526  [147200/481450]\n",
      "loss: 0.336913  [150400/481450]\n",
      "loss: 0.344020  [153600/481450]\n",
      "loss: 0.370687  [156800/481450]\n",
      "loss: 0.373828  [160000/481450]\n",
      "loss: 0.245490  [163200/481450]\n",
      "loss: 0.384817  [166400/481450]\n",
      "loss: 0.550883  [169600/481450]\n",
      "loss: 0.443630  [172800/481450]\n",
      "loss: 0.241597  [176000/481450]\n",
      "loss: 0.442614  [179200/481450]\n",
      "loss: 0.513900  [182400/481450]\n",
      "loss: 0.260038  [185600/481450]\n",
      "loss: 0.766483  [188800/481450]\n",
      "loss: 0.365958  [192000/481450]\n",
      "loss: 0.681617  [195200/481450]\n",
      "loss: 0.239555  [198400/481450]\n",
      "loss: 0.641453  [201600/481450]\n",
      "loss: 0.417118  [204800/481450]\n",
      "loss: 0.436869  [208000/481450]\n",
      "loss: 0.359115  [211200/481450]\n",
      "loss: 0.357038  [214400/481450]\n",
      "loss: 0.516348  [217600/481450]\n",
      "loss: 0.167713  [220800/481450]\n",
      "loss: 0.492942  [224000/481450]\n",
      "loss: 0.156663  [227200/481450]\n",
      "loss: 0.235717  [230400/481450]\n",
      "loss: 0.740446  [233600/481450]\n",
      "loss: 0.570598  [236800/481450]\n",
      "loss: 0.497533  [240000/481450]\n",
      "loss: 0.476158  [243200/481450]\n",
      "loss: 0.178865  [246400/481450]\n",
      "loss: 0.450120  [249600/481450]\n",
      "loss: 0.375664  [252800/481450]\n",
      "loss: 0.248106  [256000/481450]\n",
      "loss: 0.226232  [259200/481450]\n",
      "loss: 0.428131  [262400/481450]\n",
      "loss: 0.438990  [265600/481450]\n",
      "loss: 0.353311  [268800/481450]\n",
      "loss: 0.525331  [272000/481450]\n",
      "loss: 0.590552  [275200/481450]\n",
      "loss: 0.250151  [278400/481450]\n",
      "loss: 0.405133  [281600/481450]\n",
      "loss: 0.322668  [284800/481450]\n",
      "loss: 0.424601  [288000/481450]\n",
      "loss: 0.255873  [291200/481450]\n",
      "loss: 0.300956  [294400/481450]\n",
      "loss: 0.277466  [297600/481450]\n",
      "loss: 0.511586  [300800/481450]\n",
      "loss: 0.487045  [304000/481450]\n",
      "loss: 0.417903  [307200/481450]\n",
      "loss: 0.359831  [310400/481450]\n",
      "loss: 0.343692  [313600/481450]\n",
      "loss: 0.477147  [316800/481450]\n",
      "loss: 0.388046  [320000/481450]\n",
      "loss: 0.340059  [323200/481450]\n",
      "loss: 0.105179  [326400/481450]\n",
      "loss: 0.415727  [329600/481450]\n",
      "loss: 0.375352  [332800/481450]\n",
      "loss: 0.481901  [336000/481450]\n",
      "loss: 0.824188  [339200/481450]\n",
      "loss: 0.286492  [342400/481450]\n",
      "loss: 0.135430  [345600/481450]\n",
      "loss: 0.519584  [348800/481450]\n",
      "loss: 0.255646  [352000/481450]\n",
      "loss: 0.258521  [355200/481450]\n",
      "loss: 0.543415  [358400/481450]\n",
      "loss: 0.238088  [361600/481450]\n",
      "loss: 0.302256  [364800/481450]\n",
      "loss: 0.283171  [368000/481450]\n",
      "loss: 0.348099  [371200/481450]\n",
      "loss: 0.275121  [374400/481450]\n",
      "loss: 0.437690  [377600/481450]\n",
      "loss: 0.418821  [380800/481450]\n",
      "loss: 0.532590  [384000/481450]\n",
      "loss: 0.371987  [387200/481450]\n",
      "loss: 0.385879  [390400/481450]\n",
      "loss: 0.350186  [393600/481450]\n",
      "loss: 0.302868  [396800/481450]\n",
      "loss: 0.348871  [400000/481450]\n",
      "loss: 0.311688  [403200/481450]\n",
      "loss: 0.332887  [406400/481450]\n",
      "loss: 0.551198  [409600/481450]\n",
      "loss: 0.514694  [412800/481450]\n",
      "loss: 0.288080  [416000/481450]\n",
      "loss: 0.277868  [419200/481450]\n",
      "loss: 0.226365  [422400/481450]\n",
      "loss: 0.864810  [425600/481450]\n",
      "loss: 0.285065  [428800/481450]\n",
      "loss: 0.441291  [432000/481450]\n",
      "loss: 0.432169  [435200/481450]\n",
      "loss: 0.301856  [438400/481450]\n",
      "loss: 0.312341  [441600/481450]\n",
      "loss: 0.517047  [444800/481450]\n",
      "loss: 0.585974  [448000/481450]\n",
      "loss: 0.286127  [451200/481450]\n",
      "loss: 0.650979  [454400/481450]\n",
      "loss: 0.408150  [457600/481450]\n",
      "loss: 0.247523  [460800/481450]\n",
      "loss: 0.237400  [464000/481450]\n",
      "loss: 0.448388  [467200/481450]\n",
      "loss: 0.358032  [470400/481450]\n",
      "loss: 0.221832  [473600/481450]\n",
      "loss: 0.276747  [476800/481450]\n",
      "loss: 0.518246  [480000/481450]\n",
      "Train Accuracy: 84.7251%\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.473067, F1-score: 86.13% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.351789  [    0/481450]\n",
      "loss: 0.686640  [ 3200/481450]\n",
      "loss: 0.281664  [ 6400/481450]\n",
      "loss: 0.525843  [ 9600/481450]\n",
      "loss: 0.557119  [12800/481450]\n",
      "loss: 0.402680  [16000/481450]\n",
      "loss: 0.343929  [19200/481450]\n",
      "loss: 0.243546  [22400/481450]\n",
      "loss: 0.282876  [25600/481450]\n",
      "loss: 0.361540  [28800/481450]\n",
      "loss: 0.304771  [32000/481450]\n",
      "loss: 0.402314  [35200/481450]\n",
      "loss: 0.424836  [38400/481450]\n",
      "loss: 0.174665  [41600/481450]\n",
      "loss: 0.179429  [44800/481450]\n",
      "loss: 0.267281  [48000/481450]\n",
      "loss: 0.437208  [51200/481450]\n",
      "loss: 0.138819  [54400/481450]\n",
      "loss: 0.381551  [57600/481450]\n",
      "loss: 0.272940  [60800/481450]\n",
      "loss: 0.415267  [64000/481450]\n",
      "loss: 0.240828  [67200/481450]\n",
      "loss: 0.390515  [70400/481450]\n",
      "loss: 0.236121  [73600/481450]\n",
      "loss: 0.330558  [76800/481450]\n",
      "loss: 0.238007  [80000/481450]\n",
      "loss: 0.154570  [83200/481450]\n",
      "loss: 0.366637  [86400/481450]\n",
      "loss: 0.416084  [89600/481450]\n",
      "loss: 0.396860  [92800/481450]\n",
      "loss: 0.567622  [96000/481450]\n",
      "loss: 0.246696  [99200/481450]\n",
      "loss: 0.315719  [102400/481450]\n",
      "loss: 0.540346  [105600/481450]\n",
      "loss: 0.196841  [108800/481450]\n",
      "loss: 0.214863  [112000/481450]\n",
      "loss: 0.355505  [115200/481450]\n",
      "loss: 0.209295  [118400/481450]\n",
      "loss: 0.366912  [121600/481450]\n",
      "loss: 0.402583  [124800/481450]\n",
      "loss: 0.595722  [128000/481450]\n",
      "loss: 0.438945  [131200/481450]\n",
      "loss: 0.317188  [134400/481450]\n",
      "loss: 0.269349  [137600/481450]\n",
      "loss: 0.535460  [140800/481450]\n",
      "loss: 0.377842  [144000/481450]\n",
      "loss: 0.307539  [147200/481450]\n",
      "loss: 0.223152  [150400/481450]\n",
      "loss: 0.432109  [153600/481450]\n",
      "loss: 0.581440  [156800/481450]\n",
      "loss: 0.487696  [160000/481450]\n",
      "loss: 0.631791  [163200/481450]\n",
      "loss: 0.483119  [166400/481450]\n",
      "loss: 0.204325  [169600/481450]\n",
      "loss: 0.425894  [172800/481450]\n",
      "loss: 0.501989  [176000/481450]\n",
      "loss: 0.461927  [179200/481450]\n",
      "loss: 0.307172  [182400/481450]\n",
      "loss: 0.190406  [185600/481450]\n",
      "loss: 0.318869  [188800/481450]\n",
      "loss: 0.348161  [192000/481450]\n",
      "loss: 0.142369  [195200/481450]\n",
      "loss: 0.470005  [198400/481450]\n",
      "loss: 0.339096  [201600/481450]\n",
      "loss: 0.425520  [204800/481450]\n",
      "loss: 0.409748  [208000/481450]\n",
      "loss: 0.340291  [211200/481450]\n",
      "loss: 0.258866  [214400/481450]\n",
      "loss: 0.244442  [217600/481450]\n",
      "loss: 0.172447  [220800/481450]\n",
      "loss: 0.282636  [224000/481450]\n",
      "loss: 0.356751  [227200/481450]\n",
      "loss: 0.394865  [230400/481450]\n",
      "loss: 0.397348  [233600/481450]\n",
      "loss: 0.238319  [236800/481450]\n",
      "loss: 0.500887  [240000/481450]\n",
      "loss: 0.277645  [243200/481450]\n",
      "loss: 0.247285  [246400/481450]\n",
      "loss: 0.328062  [249600/481450]\n",
      "loss: 0.350513  [252800/481450]\n",
      "loss: 0.332516  [256000/481450]\n",
      "loss: 0.165806  [259200/481450]\n",
      "loss: 0.186483  [262400/481450]\n",
      "loss: 0.115978  [265600/481450]\n",
      "loss: 0.307728  [268800/481450]\n",
      "loss: 0.436030  [272000/481450]\n",
      "loss: 0.449383  [275200/481450]\n",
      "loss: 0.608085  [278400/481450]\n",
      "loss: 0.370096  [281600/481450]\n",
      "loss: 0.348158  [284800/481450]\n",
      "loss: 0.352549  [288000/481450]\n",
      "loss: 0.618697  [291200/481450]\n",
      "loss: 0.206013  [294400/481450]\n",
      "loss: 0.501522  [297600/481450]\n",
      "loss: 0.242486  [300800/481450]\n",
      "loss: 0.462760  [304000/481450]\n",
      "loss: 0.259296  [307200/481450]\n",
      "loss: 0.365518  [310400/481450]\n",
      "loss: 0.499341  [313600/481450]\n",
      "loss: 0.385196  [316800/481450]\n",
      "loss: 0.376804  [320000/481450]\n",
      "loss: 0.466071  [323200/481450]\n",
      "loss: 0.392930  [326400/481450]\n",
      "loss: 0.388628  [329600/481450]\n",
      "loss: 0.222910  [332800/481450]\n",
      "loss: 0.395989  [336000/481450]\n",
      "loss: 0.218767  [339200/481450]\n",
      "loss: 0.120300  [342400/481450]\n",
      "loss: 0.205707  [345600/481450]\n",
      "loss: 0.282373  [348800/481450]\n",
      "loss: 0.525168  [352000/481450]\n",
      "loss: 0.199083  [355200/481450]\n",
      "loss: 0.208832  [358400/481450]\n",
      "loss: 0.301241  [361600/481450]\n",
      "loss: 0.351062  [364800/481450]\n",
      "loss: 0.281362  [368000/481450]\n",
      "loss: 0.412280  [371200/481450]\n",
      "loss: 0.335903  [374400/481450]\n",
      "loss: 0.263967  [377600/481450]\n",
      "loss: 0.323144  [380800/481450]\n",
      "loss: 0.335594  [384000/481450]\n",
      "loss: 0.556340  [387200/481450]\n",
      "loss: 0.322638  [390400/481450]\n",
      "loss: 0.267255  [393600/481450]\n",
      "loss: 0.522413  [396800/481450]\n",
      "loss: 0.246169  [400000/481450]\n",
      "loss: 0.260074  [403200/481450]\n",
      "loss: 0.210819  [406400/481450]\n",
      "loss: 0.516754  [409600/481450]\n",
      "loss: 0.278641  [412800/481450]\n",
      "loss: 0.581824  [416000/481450]\n",
      "loss: 0.438277  [419200/481450]\n",
      "loss: 0.415456  [422400/481450]\n",
      "loss: 0.258090  [425600/481450]\n",
      "loss: 0.470903  [428800/481450]\n",
      "loss: 0.214939  [432000/481450]\n",
      "loss: 0.338060  [435200/481450]\n",
      "loss: 0.201817  [438400/481450]\n",
      "loss: 0.278200  [441600/481450]\n",
      "loss: 0.586352  [444800/481450]\n",
      "loss: 0.292289  [448000/481450]\n",
      "loss: 0.530274  [451200/481450]\n",
      "loss: 0.236794  [454400/481450]\n",
      "loss: 0.381521  [457600/481450]\n",
      "loss: 0.314065  [460800/481450]\n",
      "loss: 0.255252  [464000/481450]\n",
      "loss: 0.064051  [467200/481450]\n",
      "loss: 0.292549  [470400/481450]\n",
      "loss: 0.262337  [473600/481450]\n",
      "loss: 0.279872  [476800/481450]\n",
      "loss: 0.116122  [480000/481450]\n",
      "Train Accuracy: 86.4673%\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.533732, F1-score: 85.33% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.412361  [    0/481450]\n",
      "loss: 0.319233  [ 3200/481450]\n",
      "loss: 0.331643  [ 6400/481450]\n",
      "loss: 0.239849  [ 9600/481450]\n",
      "loss: 0.327761  [12800/481450]\n",
      "loss: 0.539056  [16000/481450]\n",
      "loss: 0.282825  [19200/481450]\n",
      "loss: 0.447964  [22400/481450]\n",
      "loss: 0.266734  [25600/481450]\n",
      "loss: 0.128777  [28800/481450]\n",
      "loss: 0.431133  [32000/481450]\n",
      "loss: 0.380159  [35200/481450]\n",
      "loss: 0.298074  [38400/481450]\n",
      "loss: 0.076756  [41600/481450]\n",
      "loss: 0.217031  [44800/481450]\n",
      "loss: 0.389751  [48000/481450]\n",
      "loss: 0.437929  [51200/481450]\n",
      "loss: 0.265320  [54400/481450]\n",
      "loss: 0.244029  [57600/481450]\n",
      "loss: 0.318427  [60800/481450]\n",
      "loss: 0.414190  [64000/481450]\n",
      "loss: 0.209449  [67200/481450]\n",
      "loss: 0.380812  [70400/481450]\n",
      "loss: 0.187011  [73600/481450]\n",
      "loss: 0.434738  [76800/481450]\n",
      "loss: 0.392288  [80000/481450]\n",
      "loss: 0.084515  [83200/481450]\n",
      "loss: 0.164389  [86400/481450]\n",
      "loss: 0.233617  [89600/481450]\n",
      "loss: 0.661681  [92800/481450]\n",
      "loss: 0.206789  [96000/481450]\n",
      "loss: 0.305148  [99200/481450]\n",
      "loss: 0.402926  [102400/481450]\n",
      "loss: 0.328955  [105600/481450]\n",
      "loss: 0.196848  [108800/481450]\n",
      "loss: 1.008110  [112000/481450]\n",
      "loss: 0.218677  [115200/481450]\n",
      "loss: 0.198432  [118400/481450]\n",
      "loss: 0.665024  [121600/481450]\n",
      "loss: 0.264659  [124800/481450]\n",
      "loss: 0.296352  [128000/481450]\n",
      "loss: 0.253449  [131200/481450]\n",
      "loss: 0.297972  [134400/481450]\n",
      "loss: 0.182091  [137600/481450]\n",
      "loss: 0.588609  [140800/481450]\n",
      "loss: 0.217221  [144000/481450]\n",
      "loss: 0.229864  [147200/481450]\n",
      "loss: 0.414931  [150400/481450]\n",
      "loss: 0.487171  [153600/481450]\n",
      "loss: 0.437958  [156800/481450]\n",
      "loss: 0.473172  [160000/481450]\n",
      "loss: 0.288959  [163200/481450]\n",
      "loss: 0.369139  [166400/481450]\n",
      "loss: 0.437655  [169600/481450]\n",
      "loss: 0.198255  [172800/481450]\n",
      "loss: 0.303798  [176000/481450]\n",
      "loss: 0.304964  [179200/481450]\n",
      "loss: 0.399350  [182400/481450]\n",
      "loss: 0.068174  [185600/481450]\n",
      "loss: 0.361435  [188800/481450]\n",
      "loss: 0.324644  [192000/481450]\n",
      "loss: 0.230597  [195200/481450]\n",
      "loss: 0.482790  [198400/481450]\n",
      "loss: 0.378021  [201600/481450]\n",
      "loss: 0.320287  [204800/481450]\n",
      "loss: 0.246981  [208000/481450]\n",
      "loss: 0.289639  [211200/481450]\n",
      "loss: 0.354521  [214400/481450]\n",
      "loss: 0.133941  [217600/481450]\n",
      "loss: 0.205415  [220800/481450]\n",
      "loss: 0.486960  [224000/481450]\n",
      "loss: 0.243947  [227200/481450]\n",
      "loss: 0.114435  [230400/481450]\n",
      "loss: 0.319032  [233600/481450]\n",
      "loss: 0.382885  [236800/481450]\n",
      "loss: 0.251669  [240000/481450]\n",
      "loss: 0.272059  [243200/481450]\n",
      "loss: 0.374974  [246400/481450]\n",
      "loss: 0.279956  [249600/481450]\n",
      "loss: 0.391384  [252800/481450]\n",
      "loss: 0.301112  [256000/481450]\n",
      "loss: 0.392802  [259200/481450]\n",
      "loss: 0.367357  [262400/481450]\n",
      "loss: 0.136921  [265600/481450]\n",
      "loss: 0.407524  [268800/481450]\n",
      "loss: 0.438395  [272000/481450]\n",
      "loss: 0.302526  [275200/481450]\n",
      "loss: 0.523394  [278400/481450]\n",
      "loss: 0.412882  [281600/481450]\n",
      "loss: 0.154258  [284800/481450]\n",
      "loss: 0.168436  [288000/481450]\n",
      "loss: 0.274394  [291200/481450]\n",
      "loss: 0.192785  [294400/481450]\n",
      "loss: 0.242757  [297600/481450]\n",
      "loss: 0.468281  [300800/481450]\n",
      "loss: 0.322693  [304000/481450]\n",
      "loss: 0.254569  [307200/481450]\n",
      "loss: 0.250592  [310400/481450]\n",
      "loss: 0.214923  [313600/481450]\n",
      "loss: 0.371705  [316800/481450]\n",
      "loss: 0.513155  [320000/481450]\n",
      "loss: 0.317516  [323200/481450]\n",
      "loss: 0.300799  [326400/481450]\n",
      "loss: 0.399451  [329600/481450]\n",
      "loss: 0.339916  [332800/481450]\n",
      "loss: 0.197415  [336000/481450]\n",
      "loss: 0.212631  [339200/481450]\n",
      "loss: 0.428565  [342400/481450]\n",
      "loss: 0.418912  [345600/481450]\n",
      "loss: 0.348797  [348800/481450]\n",
      "loss: 0.305253  [352000/481450]\n",
      "loss: 0.380463  [355200/481450]\n",
      "loss: 0.244235  [358400/481450]\n",
      "loss: 0.309690  [361600/481450]\n",
      "loss: 0.395419  [364800/481450]\n",
      "loss: 0.469685  [368000/481450]\n",
      "loss: 0.330315  [371200/481450]\n",
      "loss: 0.351808  [374400/481450]\n",
      "loss: 0.315100  [377600/481450]\n",
      "loss: 0.207414  [380800/481450]\n",
      "loss: 0.274279  [384000/481450]\n",
      "loss: 0.283009  [387200/481450]\n",
      "loss: 0.158108  [390400/481450]\n",
      "loss: 0.295292  [393600/481450]\n",
      "loss: 0.344053  [396800/481450]\n",
      "loss: 0.143843  [400000/481450]\n",
      "loss: 0.317153  [403200/481450]\n",
      "loss: 0.210927  [406400/481450]\n",
      "loss: 0.343953  [409600/481450]\n",
      "loss: 0.502374  [412800/481450]\n",
      "loss: 0.232793  [416000/481450]\n",
      "loss: 0.322040  [419200/481450]\n",
      "loss: 0.402516  [422400/481450]\n",
      "loss: 0.347256  [425600/481450]\n",
      "loss: 0.355773  [428800/481450]\n",
      "loss: 0.120612  [432000/481450]\n",
      "loss: 0.322601  [435200/481450]\n",
      "loss: 0.226720  [438400/481450]\n",
      "loss: 0.266239  [441600/481450]\n",
      "loss: 0.441005  [444800/481450]\n",
      "loss: 0.265355  [448000/481450]\n",
      "loss: 0.279925  [451200/481450]\n",
      "loss: 0.438584  [454400/481450]\n",
      "loss: 0.214853  [457600/481450]\n",
      "loss: 0.202593  [460800/481450]\n",
      "loss: 0.189513  [464000/481450]\n",
      "loss: 0.436237  [467200/481450]\n",
      "loss: 0.392567  [470400/481450]\n",
      "loss: 0.272770  [473600/481450]\n",
      "loss: 0.338188  [476800/481450]\n",
      "loss: 0.320040  [480000/481450]\n",
      "Train Accuracy: 87.2421%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.586245, F1-score: 85.65% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.178394  [    0/481450]\n",
      "loss: 0.243608  [ 3200/481450]\n",
      "loss: 0.630504  [ 6400/481450]\n",
      "loss: 0.499693  [ 9600/481450]\n",
      "loss: 0.254738  [12800/481450]\n",
      "loss: 0.404805  [16000/481450]\n",
      "loss: 0.196862  [19200/481450]\n",
      "loss: 0.327363  [22400/481450]\n",
      "loss: 0.209479  [25600/481450]\n",
      "loss: 0.308986  [28800/481450]\n",
      "loss: 0.435077  [32000/481450]\n",
      "loss: 0.152942  [35200/481450]\n",
      "loss: 0.422749  [38400/481450]\n",
      "loss: 0.289341  [41600/481450]\n",
      "loss: 0.184281  [44800/481450]\n",
      "loss: 0.256719  [48000/481450]\n",
      "loss: 0.247143  [51200/481450]\n",
      "loss: 0.297428  [54400/481450]\n",
      "loss: 0.329392  [57600/481450]\n",
      "loss: 0.269235  [60800/481450]\n",
      "loss: 0.173227  [64000/481450]\n",
      "loss: 0.462555  [67200/481450]\n",
      "loss: 0.547615  [70400/481450]\n",
      "loss: 0.214285  [73600/481450]\n",
      "loss: 0.397174  [76800/481450]\n",
      "loss: 0.487726  [80000/481450]\n",
      "loss: 0.311390  [83200/481450]\n",
      "loss: 0.301337  [86400/481450]\n",
      "loss: 0.350634  [89600/481450]\n",
      "loss: 0.301935  [92800/481450]\n",
      "loss: 0.357325  [96000/481450]\n",
      "loss: 0.292316  [99200/481450]\n",
      "loss: 0.270204  [102400/481450]\n",
      "loss: 0.322985  [105600/481450]\n",
      "loss: 0.167226  [108800/481450]\n",
      "loss: 0.270543  [112000/481450]\n",
      "loss: 0.415671  [115200/481450]\n",
      "loss: 0.187194  [118400/481450]\n",
      "loss: 0.160646  [121600/481450]\n",
      "loss: 0.119289  [124800/481450]\n",
      "loss: 0.246529  [128000/481450]\n",
      "loss: 0.225352  [131200/481450]\n",
      "loss: 0.175739  [134400/481450]\n",
      "loss: 0.414775  [137600/481450]\n",
      "loss: 0.483140  [140800/481450]\n",
      "loss: 0.183878  [144000/481450]\n",
      "loss: 0.203944  [147200/481450]\n",
      "loss: 0.369495  [150400/481450]\n",
      "loss: 0.285884  [153600/481450]\n",
      "loss: 0.190469  [156800/481450]\n",
      "loss: 0.247806  [160000/481450]\n",
      "loss: 0.296282  [163200/481450]\n",
      "loss: 0.390376  [166400/481450]\n",
      "loss: 0.213144  [169600/481450]\n",
      "loss: 0.148683  [172800/481450]\n",
      "loss: 0.217738  [176000/481450]\n",
      "loss: 0.439940  [179200/481450]\n",
      "loss: 0.171094  [182400/481450]\n",
      "loss: 0.439751  [185600/481450]\n",
      "loss: 0.315518  [188800/481450]\n",
      "loss: 0.478858  [192000/481450]\n",
      "loss: 0.429466  [195200/481450]\n",
      "loss: 0.418770  [198400/481450]\n",
      "loss: 0.224847  [201600/481450]\n",
      "loss: 0.337052  [204800/481450]\n",
      "loss: 0.235941  [208000/481450]\n",
      "loss: 0.295891  [211200/481450]\n",
      "loss: 0.367865  [214400/481450]\n",
      "loss: 0.474662  [217600/481450]\n",
      "loss: 0.383526  [220800/481450]\n",
      "loss: 0.252423  [224000/481450]\n",
      "loss: 0.579020  [227200/481450]\n",
      "loss: 0.183122  [230400/481450]\n",
      "loss: 0.273622  [233600/481450]\n",
      "loss: 0.254866  [236800/481450]\n",
      "loss: 0.270453  [240000/481450]\n",
      "loss: 0.107818  [243200/481450]\n",
      "loss: 0.372863  [246400/481450]\n",
      "loss: 0.148264  [249600/481450]\n",
      "loss: 0.165611  [252800/481450]\n",
      "loss: 0.434451  [256000/481450]\n",
      "loss: 0.473391  [259200/481450]\n",
      "loss: 0.306402  [262400/481450]\n",
      "loss: 0.164058  [265600/481450]\n",
      "loss: 0.367781  [268800/481450]\n",
      "loss: 0.151797  [272000/481450]\n",
      "loss: 0.167650  [275200/481450]\n",
      "loss: 0.319968  [278400/481450]\n",
      "loss: 0.358222  [281600/481450]\n",
      "loss: 0.181128  [284800/481450]\n",
      "loss: 0.324973  [288000/481450]\n",
      "loss: 0.308883  [291200/481450]\n",
      "loss: 0.461680  [294400/481450]\n",
      "loss: 0.170876  [297600/481450]\n",
      "loss: 0.236146  [300800/481450]\n",
      "loss: 0.106013  [304000/481450]\n",
      "loss: 0.269641  [307200/481450]\n",
      "loss: 0.251486  [310400/481450]\n",
      "loss: 0.369259  [313600/481450]\n",
      "loss: 0.272234  [316800/481450]\n",
      "loss: 0.223195  [320000/481450]\n",
      "loss: 0.394757  [323200/481450]\n",
      "loss: 0.251859  [326400/481450]\n",
      "loss: 0.158514  [329600/481450]\n",
      "loss: 0.365049  [332800/481450]\n",
      "loss: 0.496092  [336000/481450]\n",
      "loss: 0.161339  [339200/481450]\n",
      "loss: 0.334439  [342400/481450]\n",
      "loss: 0.412118  [345600/481450]\n",
      "loss: 0.332049  [348800/481450]\n",
      "loss: 0.192776  [352000/481450]\n",
      "loss: 0.221900  [355200/481450]\n",
      "loss: 0.411608  [358400/481450]\n",
      "loss: 0.222703  [361600/481450]\n",
      "loss: 0.431602  [364800/481450]\n",
      "loss: 0.325050  [368000/481450]\n",
      "loss: 0.207377  [371200/481450]\n",
      "loss: 0.209531  [374400/481450]\n",
      "loss: 0.244761  [377600/481450]\n",
      "loss: 0.301123  [380800/481450]\n",
      "loss: 0.459766  [384000/481450]\n",
      "loss: 0.439647  [387200/481450]\n",
      "loss: 0.325192  [390400/481450]\n",
      "loss: 0.262642  [393600/481450]\n",
      "loss: 0.251528  [396800/481450]\n",
      "loss: 0.339169  [400000/481450]\n",
      "loss: 0.298051  [403200/481450]\n",
      "loss: 0.387075  [406400/481450]\n",
      "loss: 0.394090  [409600/481450]\n",
      "loss: 0.218602  [412800/481450]\n",
      "loss: 0.260270  [416000/481450]\n",
      "loss: 0.355421  [419200/481450]\n",
      "loss: 0.283206  [422400/481450]\n",
      "loss: 0.180363  [425600/481450]\n",
      "loss: 0.154005  [428800/481450]\n",
      "loss: 0.193350  [432000/481450]\n",
      "loss: 0.297108  [435200/481450]\n",
      "loss: 0.177438  [438400/481450]\n",
      "loss: 0.263316  [441600/481450]\n",
      "loss: 0.301336  [444800/481450]\n",
      "loss: 0.300960  [448000/481450]\n",
      "loss: 0.288530  [451200/481450]\n",
      "loss: 0.397805  [454400/481450]\n",
      "loss: 0.537997  [457600/481450]\n",
      "loss: 0.207263  [460800/481450]\n",
      "loss: 0.199630  [464000/481450]\n",
      "loss: 0.352835  [467200/481450]\n",
      "loss: 0.554976  [470400/481450]\n",
      "loss: 0.324958  [473600/481450]\n",
      "loss: 0.300120  [476800/481450]\n",
      "loss: 0.281273  [480000/481450]\n",
      "Train Accuracy: 87.9142%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.635155, F1-score: 85.81% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.163958  [    0/481450]\n",
      "loss: 0.241551  [ 3200/481450]\n",
      "loss: 0.209360  [ 6400/481450]\n",
      "loss: 0.613982  [ 9600/481450]\n",
      "loss: 0.421081  [12800/481450]\n",
      "loss: 0.365781  [16000/481450]\n",
      "loss: 0.254621  [19200/481450]\n",
      "loss: 0.262691  [22400/481450]\n",
      "loss: 0.318541  [25600/481450]\n",
      "loss: 0.306475  [28800/481450]\n",
      "loss: 0.164327  [32000/481450]\n",
      "loss: 0.413298  [35200/481450]\n",
      "loss: 0.210271  [38400/481450]\n",
      "loss: 0.323233  [41600/481450]\n",
      "loss: 0.178471  [44800/481450]\n",
      "loss: 0.314318  [48000/481450]\n",
      "loss: 0.135324  [51200/481450]\n",
      "loss: 0.270715  [54400/481450]\n",
      "loss: 0.384935  [57600/481450]\n",
      "loss: 0.364239  [60800/481450]\n",
      "loss: 0.274544  [64000/481450]\n",
      "loss: 0.169558  [67200/481450]\n",
      "loss: 0.309209  [70400/481450]\n",
      "loss: 0.266555  [73600/481450]\n",
      "loss: 0.262690  [76800/481450]\n",
      "loss: 0.416164  [80000/481450]\n",
      "loss: 0.382281  [83200/481450]\n",
      "loss: 0.341710  [86400/481450]\n",
      "loss: 0.439978  [89600/481450]\n",
      "loss: 0.240832  [92800/481450]\n",
      "loss: 0.357654  [96000/481450]\n",
      "loss: 0.258044  [99200/481450]\n",
      "loss: 0.253571  [102400/481450]\n",
      "loss: 0.351884  [105600/481450]\n",
      "loss: 0.120302  [108800/481450]\n",
      "loss: 0.292260  [112000/481450]\n",
      "loss: 0.257964  [115200/481450]\n",
      "loss: 0.298189  [118400/481450]\n",
      "loss: 0.248263  [121600/481450]\n",
      "loss: 0.127451  [124800/481450]\n",
      "loss: 0.276016  [128000/481450]\n",
      "loss: 0.345472  [131200/481450]\n",
      "loss: 0.243501  [134400/481450]\n",
      "loss: 0.176802  [137600/481450]\n",
      "loss: 0.224535  [140800/481450]\n",
      "loss: 0.211259  [144000/481450]\n",
      "loss: 0.441977  [147200/481450]\n",
      "loss: 0.267726  [150400/481450]\n",
      "loss: 0.316580  [153600/481450]\n",
      "loss: 0.305090  [156800/481450]\n",
      "loss: 0.176642  [160000/481450]\n",
      "loss: 0.340189  [163200/481450]\n",
      "loss: 0.292423  [166400/481450]\n",
      "loss: 0.241955  [169600/481450]\n",
      "loss: 0.269003  [172800/481450]\n",
      "loss: 0.598814  [176000/481450]\n",
      "loss: 0.419934  [179200/481450]\n",
      "loss: 0.273355  [182400/481450]\n",
      "loss: 0.336352  [185600/481450]\n",
      "loss: 0.181591  [188800/481450]\n",
      "loss: 0.444195  [192000/481450]\n",
      "loss: 0.145761  [195200/481450]\n",
      "loss: 0.341925  [198400/481450]\n",
      "loss: 0.258374  [201600/481450]\n",
      "loss: 0.477507  [204800/481450]\n",
      "loss: 0.329519  [208000/481450]\n",
      "loss: 0.157282  [211200/481450]\n",
      "loss: 0.213295  [214400/481450]\n",
      "loss: 0.269324  [217600/481450]\n",
      "loss: 0.251852  [220800/481450]\n",
      "loss: 0.298223  [224000/481450]\n",
      "loss: 0.299868  [227200/481450]\n",
      "loss: 0.111739  [230400/481450]\n",
      "loss: 0.432366  [233600/481450]\n",
      "loss: 0.245783  [236800/481450]\n",
      "loss: 0.182156  [240000/481450]\n",
      "loss: 0.294796  [243200/481450]\n",
      "loss: 0.222678  [246400/481450]\n",
      "loss: 0.182914  [249600/481450]\n",
      "loss: 0.280075  [252800/481450]\n",
      "loss: 0.315041  [256000/481450]\n",
      "loss: 0.217819  [259200/481450]\n",
      "loss: 0.382491  [262400/481450]\n",
      "loss: 0.170348  [265600/481450]\n",
      "loss: 0.171442  [268800/481450]\n",
      "loss: 0.203904  [272000/481450]\n",
      "loss: 0.234326  [275200/481450]\n",
      "loss: 0.197333  [278400/481450]\n",
      "loss: 0.164355  [281600/481450]\n",
      "loss: 0.510247  [284800/481450]\n",
      "loss: 0.249895  [288000/481450]\n",
      "loss: 0.315056  [291200/481450]\n",
      "loss: 0.289243  [294400/481450]\n",
      "loss: 0.162359  [297600/481450]\n",
      "loss: 0.376574  [300800/481450]\n",
      "loss: 0.172664  [304000/481450]\n",
      "loss: 0.319506  [307200/481450]\n",
      "loss: 0.425055  [310400/481450]\n",
      "loss: 0.232925  [313600/481450]\n",
      "loss: 0.124397  [316800/481450]\n",
      "loss: 0.367727  [320000/481450]\n",
      "loss: 0.417274  [323200/481450]\n",
      "loss: 0.281511  [326400/481450]\n",
      "loss: 0.309181  [329600/481450]\n",
      "loss: 0.400862  [332800/481450]\n",
      "loss: 0.201302  [336000/481450]\n",
      "loss: 0.108841  [339200/481450]\n",
      "loss: 0.236147  [342400/481450]\n",
      "loss: 0.184363  [345600/481450]\n",
      "loss: 0.075777  [348800/481450]\n",
      "loss: 0.412698  [352000/481450]\n",
      "loss: 0.305338  [355200/481450]\n",
      "loss: 0.432518  [358400/481450]\n",
      "loss: 0.422855  [361600/481450]\n",
      "loss: 0.288145  [364800/481450]\n",
      "loss: 0.235794  [368000/481450]\n",
      "loss: 0.179807  [371200/481450]\n",
      "loss: 0.208002  [374400/481450]\n",
      "loss: 0.295735  [377600/481450]\n",
      "loss: 0.339111  [380800/481450]\n",
      "loss: 0.148544  [384000/481450]\n",
      "loss: 0.374479  [387200/481450]\n",
      "loss: 0.353498  [390400/481450]\n",
      "loss: 0.282240  [393600/481450]\n",
      "loss: 0.519486  [396800/481450]\n",
      "loss: 0.095019  [400000/481450]\n",
      "loss: 0.238815  [403200/481450]\n",
      "loss: 0.471025  [406400/481450]\n",
      "loss: 0.335152  [409600/481450]\n",
      "loss: 0.176773  [412800/481450]\n",
      "loss: 0.148049  [416000/481450]\n",
      "loss: 0.230200  [419200/481450]\n",
      "loss: 0.162677  [422400/481450]\n",
      "loss: 0.372290  [425600/481450]\n",
      "loss: 0.549952  [428800/481450]\n",
      "loss: 0.202219  [432000/481450]\n",
      "loss: 0.050406  [435200/481450]\n",
      "loss: 0.145021  [438400/481450]\n",
      "loss: 0.062185  [441600/481450]\n",
      "loss: 0.282617  [444800/481450]\n",
      "loss: 0.257213  [448000/481450]\n",
      "loss: 0.097251  [451200/481450]\n",
      "loss: 0.127396  [454400/481450]\n",
      "loss: 0.420515  [457600/481450]\n",
      "loss: 0.375790  [460800/481450]\n",
      "loss: 0.369029  [464000/481450]\n",
      "loss: 0.156610  [467200/481450]\n",
      "loss: 0.383036  [470400/481450]\n",
      "loss: 0.170908  [473600/481450]\n",
      "loss: 0.572815  [476800/481450]\n",
      "loss: 0.116359  [480000/481450]\n",
      "Train Accuracy: 88.6470%\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.613336, F1-score: 86.64% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.220444  [    0/481450]\n",
      "loss: 0.289053  [ 3200/481450]\n",
      "loss: 0.259646  [ 6400/481450]\n",
      "loss: 0.134867  [ 9600/481450]\n",
      "loss: 0.230007  [12800/481450]\n",
      "loss: 0.279796  [16000/481450]\n",
      "loss: 0.242750  [19200/481450]\n",
      "loss: 0.215956  [22400/481450]\n",
      "loss: 0.424596  [25600/481450]\n",
      "loss: 0.367061  [28800/481450]\n",
      "loss: 0.228487  [32000/481450]\n",
      "loss: 0.130405  [35200/481450]\n",
      "loss: 0.276026  [38400/481450]\n",
      "loss: 0.112380  [41600/481450]\n",
      "loss: 0.209376  [44800/481450]\n",
      "loss: 0.196497  [48000/481450]\n",
      "loss: 0.277256  [51200/481450]\n",
      "loss: 0.182717  [54400/481450]\n",
      "loss: 0.300346  [57600/481450]\n",
      "loss: 0.201388  [60800/481450]\n",
      "loss: 0.144873  [64000/481450]\n",
      "loss: 0.221623  [67200/481450]\n",
      "loss: 0.297305  [70400/481450]\n",
      "loss: 0.219182  [73600/481450]\n",
      "loss: 0.476310  [76800/481450]\n",
      "loss: 0.394110  [80000/481450]\n",
      "loss: 0.326546  [83200/481450]\n",
      "loss: 0.232502  [86400/481450]\n",
      "loss: 0.176611  [89600/481450]\n",
      "loss: 0.311457  [92800/481450]\n",
      "loss: 0.257724  [96000/481450]\n",
      "loss: 0.322820  [99200/481450]\n",
      "loss: 0.177791  [102400/481450]\n",
      "loss: 0.171301  [105600/481450]\n",
      "loss: 0.242656  [108800/481450]\n",
      "loss: 0.246710  [112000/481450]\n",
      "loss: 0.245033  [115200/481450]\n",
      "loss: 0.404658  [118400/481450]\n",
      "loss: 0.260136  [121600/481450]\n",
      "loss: 0.146588  [124800/481450]\n",
      "loss: 0.354746  [128000/481450]\n",
      "loss: 0.198485  [131200/481450]\n",
      "loss: 0.168262  [134400/481450]\n",
      "loss: 0.151465  [137600/481450]\n",
      "loss: 0.263647  [140800/481450]\n",
      "loss: 0.250525  [144000/481450]\n",
      "loss: 0.167964  [147200/481450]\n",
      "loss: 0.206809  [150400/481450]\n",
      "loss: 0.315171  [153600/481450]\n",
      "loss: 0.400280  [156800/481450]\n",
      "loss: 0.325113  [160000/481450]\n",
      "loss: 0.487832  [163200/481450]\n",
      "loss: 0.231584  [166400/481450]\n",
      "loss: 0.319052  [169600/481450]\n",
      "loss: 0.137253  [172800/481450]\n",
      "loss: 0.129715  [176000/481450]\n",
      "loss: 0.385837  [179200/481450]\n",
      "loss: 0.474060  [182400/481450]\n",
      "loss: 0.432629  [185600/481450]\n",
      "loss: 0.293322  [188800/481450]\n",
      "loss: 0.048519  [192000/481450]\n",
      "loss: 0.417530  [195200/481450]\n",
      "loss: 0.202065  [198400/481450]\n",
      "loss: 0.144589  [201600/481450]\n",
      "loss: 0.451817  [204800/481450]\n",
      "loss: 0.212245  [208000/481450]\n",
      "loss: 0.373333  [211200/481450]\n",
      "loss: 0.208701  [214400/481450]\n",
      "loss: 0.232845  [217600/481450]\n",
      "loss: 0.149803  [220800/481450]\n",
      "loss: 0.471735  [224000/481450]\n",
      "loss: 0.258937  [227200/481450]\n",
      "loss: 0.249211  [230400/481450]\n",
      "loss: 0.323600  [233600/481450]\n",
      "loss: 0.197465  [236800/481450]\n",
      "loss: 0.349676  [240000/481450]\n",
      "loss: 0.255994  [243200/481450]\n",
      "loss: 0.157419  [246400/481450]\n",
      "loss: 0.319555  [249600/481450]\n",
      "loss: 0.151746  [252800/481450]\n",
      "loss: 0.215309  [256000/481450]\n",
      "loss: 0.369524  [259200/481450]\n",
      "loss: 0.286346  [262400/481450]\n",
      "loss: 0.221966  [265600/481450]\n",
      "loss: 0.213440  [268800/481450]\n",
      "loss: 0.189423  [272000/481450]\n",
      "loss: 0.353000  [275200/481450]\n",
      "loss: 0.216320  [278400/481450]\n",
      "loss: 0.197685  [281600/481450]\n",
      "loss: 0.188256  [284800/481450]\n",
      "loss: 0.201115  [288000/481450]\n",
      "loss: 0.339864  [291200/481450]\n",
      "loss: 0.321527  [294400/481450]\n",
      "loss: 0.232117  [297600/481450]\n",
      "loss: 0.137887  [300800/481450]\n",
      "loss: 0.406792  [304000/481450]\n",
      "loss: 0.161061  [307200/481450]\n",
      "loss: 0.242953  [310400/481450]\n",
      "loss: 0.115170  [313600/481450]\n",
      "loss: 0.169224  [316800/481450]\n",
      "loss: 0.234509  [320000/481450]\n",
      "loss: 0.154176  [323200/481450]\n",
      "loss: 0.323217  [326400/481450]\n",
      "loss: 0.156376  [329600/481450]\n",
      "loss: 0.225339  [332800/481450]\n",
      "loss: 0.208974  [336000/481450]\n",
      "loss: 0.186463  [339200/481450]\n",
      "loss: 0.352096  [342400/481450]\n",
      "loss: 0.213020  [345600/481450]\n",
      "loss: 0.279875  [348800/481450]\n",
      "loss: 0.307819  [352000/481450]\n",
      "loss: 0.183865  [355200/481450]\n",
      "loss: 0.251331  [358400/481450]\n",
      "loss: 0.205938  [361600/481450]\n",
      "loss: 0.231691  [364800/481450]\n",
      "loss: 0.256609  [368000/481450]\n",
      "loss: 0.356200  [371200/481450]\n",
      "loss: 0.359607  [374400/481450]\n",
      "loss: 0.372746  [377600/481450]\n",
      "loss: 0.382169  [380800/481450]\n",
      "loss: 0.443744  [384000/481450]\n",
      "loss: 0.287900  [387200/481450]\n",
      "loss: 0.190325  [390400/481450]\n",
      "loss: 0.213738  [393600/481450]\n",
      "loss: 0.216314  [396800/481450]\n",
      "loss: 0.356790  [400000/481450]\n",
      "loss: 0.371436  [403200/481450]\n",
      "loss: 0.333963  [406400/481450]\n",
      "loss: 0.289091  [409600/481450]\n",
      "loss: 0.272351  [412800/481450]\n",
      "loss: 0.308824  [416000/481450]\n",
      "loss: 0.457845  [419200/481450]\n",
      "loss: 0.228278  [422400/481450]\n",
      "loss: 0.264632  [425600/481450]\n",
      "loss: 0.348465  [428800/481450]\n",
      "loss: 0.194189  [432000/481450]\n",
      "loss: 0.243913  [435200/481450]\n",
      "loss: 0.376750  [438400/481450]\n",
      "loss: 0.188653  [441600/481450]\n",
      "loss: 0.218074  [444800/481450]\n",
      "loss: 0.213679  [448000/481450]\n",
      "loss: 0.497489  [451200/481450]\n",
      "loss: 0.263427  [454400/481450]\n",
      "loss: 0.171013  [457600/481450]\n",
      "loss: 0.229704  [460800/481450]\n",
      "loss: 0.350191  [464000/481450]\n",
      "loss: 0.276650  [467200/481450]\n",
      "loss: 0.132618  [470400/481450]\n",
      "loss: 0.282002  [473600/481450]\n",
      "loss: 0.426634  [476800/481450]\n",
      "loss: 0.306912  [480000/481450]\n",
      "Train Accuracy: 89.1326%\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.610479, F1-score: 86.92% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.163418  [    0/481450]\n",
      "loss: 0.409668  [ 3200/481450]\n",
      "loss: 0.106288  [ 6400/481450]\n",
      "loss: 0.236960  [ 9600/481450]\n",
      "loss: 0.244920  [12800/481450]\n",
      "loss: 0.155754  [16000/481450]\n",
      "loss: 0.164348  [19200/481450]\n",
      "loss: 0.393429  [22400/481450]\n",
      "loss: 0.279325  [25600/481450]\n",
      "loss: 0.308085  [28800/481450]\n",
      "loss: 0.337805  [32000/481450]\n",
      "loss: 0.340259  [35200/481450]\n",
      "loss: 0.230415  [38400/481450]\n",
      "loss: 0.273043  [41600/481450]\n",
      "loss: 0.215701  [44800/481450]\n",
      "loss: 0.251371  [48000/481450]\n",
      "loss: 0.031759  [51200/481450]\n",
      "loss: 0.414418  [54400/481450]\n",
      "loss: 0.495043  [57600/481450]\n",
      "loss: 0.285004  [60800/481450]\n",
      "loss: 0.303924  [64000/481450]\n",
      "loss: 0.249891  [67200/481450]\n",
      "loss: 0.358534  [70400/481450]\n",
      "loss: 0.276158  [73600/481450]\n",
      "loss: 0.504901  [76800/481450]\n",
      "loss: 0.423582  [80000/481450]\n",
      "loss: 0.287961  [83200/481450]\n",
      "loss: 0.506026  [86400/481450]\n",
      "loss: 0.179753  [89600/481450]\n",
      "loss: 0.095644  [92800/481450]\n",
      "loss: 0.130307  [96000/481450]\n",
      "loss: 0.191524  [99200/481450]\n",
      "loss: 0.162176  [102400/481450]\n",
      "loss: 0.109027  [105600/481450]\n",
      "loss: 0.188209  [108800/481450]\n",
      "loss: 0.407791  [112000/481450]\n",
      "loss: 0.136650  [115200/481450]\n",
      "loss: 0.192103  [118400/481450]\n",
      "loss: 0.198280  [121600/481450]\n",
      "loss: 0.175415  [124800/481450]\n",
      "loss: 0.148606  [128000/481450]\n",
      "loss: 0.328513  [131200/481450]\n",
      "loss: 0.247116  [134400/481450]\n",
      "loss: 0.145430  [137600/481450]\n",
      "loss: 0.216767  [140800/481450]\n",
      "loss: 0.436652  [144000/481450]\n",
      "loss: 0.414333  [147200/481450]\n",
      "loss: 0.445738  [150400/481450]\n",
      "loss: 0.479576  [153600/481450]\n",
      "loss: 0.249372  [156800/481450]\n",
      "loss: 0.196167  [160000/481450]\n",
      "loss: 0.198872  [163200/481450]\n",
      "loss: 0.328940  [166400/481450]\n",
      "loss: 0.157400  [169600/481450]\n",
      "loss: 0.305477  [172800/481450]\n",
      "loss: 0.226357  [176000/481450]\n",
      "loss: 0.245209  [179200/481450]\n",
      "loss: 0.221344  [182400/481450]\n",
      "loss: 0.297654  [185600/481450]\n",
      "loss: 0.263138  [188800/481450]\n",
      "loss: 0.108446  [192000/481450]\n",
      "loss: 0.227657  [195200/481450]\n",
      "loss: 0.285385  [198400/481450]\n",
      "loss: 0.262286  [201600/481450]\n",
      "loss: 0.206241  [204800/481450]\n",
      "loss: 0.184938  [208000/481450]\n",
      "loss: 0.232654  [211200/481450]\n",
      "loss: 0.220053  [214400/481450]\n",
      "loss: 0.164904  [217600/481450]\n",
      "loss: 0.558800  [220800/481450]\n",
      "loss: 0.294200  [224000/481450]\n",
      "loss: 0.235574  [227200/481450]\n",
      "loss: 0.356057  [230400/481450]\n",
      "loss: 0.181396  [233600/481450]\n",
      "loss: 0.162275  [236800/481450]\n",
      "loss: 0.346032  [240000/481450]\n",
      "loss: 0.302238  [243200/481450]\n",
      "loss: 0.113670  [246400/481450]\n",
      "loss: 0.220216  [249600/481450]\n",
      "loss: 0.165357  [252800/481450]\n",
      "loss: 0.147639  [256000/481450]\n",
      "loss: 0.216230  [259200/481450]\n",
      "loss: 0.253183  [262400/481450]\n",
      "loss: 0.138742  [265600/481450]\n",
      "loss: 0.312923  [268800/481450]\n",
      "loss: 0.367410  [272000/481450]\n",
      "loss: 0.330963  [275200/481450]\n",
      "loss: 0.099767  [278400/481450]\n",
      "loss: 0.175135  [281600/481450]\n",
      "loss: 0.210794  [284800/481450]\n",
      "loss: 0.332982  [288000/481450]\n",
      "loss: 0.268682  [291200/481450]\n",
      "loss: 0.543036  [294400/481450]\n",
      "loss: 0.246724  [297600/481450]\n",
      "loss: 0.442585  [300800/481450]\n",
      "loss: 0.349704  [304000/481450]\n",
      "loss: 0.324714  [307200/481450]\n",
      "loss: 0.402819  [310400/481450]\n",
      "loss: 0.124257  [313600/481450]\n",
      "loss: 0.135968  [316800/481450]\n",
      "loss: 0.376111  [320000/481450]\n",
      "loss: 0.234757  [323200/481450]\n",
      "loss: 0.214822  [326400/481450]\n",
      "loss: 0.370228  [329600/481450]\n",
      "loss: 0.148902  [332800/481450]\n",
      "loss: 0.120350  [336000/481450]\n",
      "loss: 0.233123  [339200/481450]\n",
      "loss: 0.249464  [342400/481450]\n",
      "loss: 0.358244  [345600/481450]\n",
      "loss: 0.161169  [348800/481450]\n",
      "loss: 0.071872  [352000/481450]\n",
      "loss: 0.191931  [355200/481450]\n",
      "loss: 0.084938  [358400/481450]\n",
      "loss: 0.243729  [361600/481450]\n",
      "loss: 0.409398  [364800/481450]\n",
      "loss: 0.305831  [368000/481450]\n",
      "loss: 0.151559  [371200/481450]\n",
      "loss: 0.344516  [374400/481450]\n",
      "loss: 0.116429  [377600/481450]\n",
      "loss: 0.244994  [380800/481450]\n",
      "loss: 0.390018  [384000/481450]\n",
      "loss: 0.189757  [387200/481450]\n",
      "loss: 0.247216  [390400/481450]\n",
      "loss: 0.363218  [393600/481450]\n",
      "loss: 0.382198  [396800/481450]\n",
      "loss: 0.305798  [400000/481450]\n",
      "loss: 0.184324  [403200/481450]\n",
      "loss: 0.183678  [406400/481450]\n",
      "loss: 0.284777  [409600/481450]\n",
      "loss: 0.084568  [412800/481450]\n",
      "loss: 0.323459  [416000/481450]\n",
      "loss: 0.366593  [419200/481450]\n",
      "loss: 0.250881  [422400/481450]\n",
      "loss: 0.377387  [425600/481450]\n",
      "loss: 0.210862  [428800/481450]\n",
      "loss: 0.324407  [432000/481450]\n",
      "loss: 0.295547  [435200/481450]\n",
      "loss: 0.360335  [438400/481450]\n",
      "loss: 0.364878  [441600/481450]\n",
      "loss: 0.277978  [444800/481450]\n",
      "loss: 0.254600  [448000/481450]\n",
      "loss: 0.233654  [451200/481450]\n",
      "loss: 0.246136  [454400/481450]\n",
      "loss: 0.390171  [457600/481450]\n",
      "loss: 0.167913  [460800/481450]\n",
      "loss: 0.112404  [464000/481450]\n",
      "loss: 0.173169  [467200/481450]\n",
      "loss: 0.320908  [470400/481450]\n",
      "loss: 0.094651  [473600/481450]\n",
      "loss: 0.226813  [476800/481450]\n",
      "loss: 0.309700  [480000/481450]\n",
      "Train Accuracy: 89.5626%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.556944, F1-score: 87.38% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.197585  [    0/481450]\n",
      "loss: 0.164822  [ 3200/481450]\n",
      "loss: 0.145368  [ 6400/481450]\n",
      "loss: 0.319550  [ 9600/481450]\n",
      "loss: 0.309729  [12800/481450]\n",
      "loss: 0.095946  [16000/481450]\n",
      "loss: 0.258557  [19200/481450]\n",
      "loss: 0.226458  [22400/481450]\n",
      "loss: 0.241718  [25600/481450]\n",
      "loss: 0.108890  [28800/481450]\n",
      "loss: 0.395568  [32000/481450]\n",
      "loss: 0.327897  [35200/481450]\n",
      "loss: 0.444376  [38400/481450]\n",
      "loss: 0.303829  [41600/481450]\n",
      "loss: 0.288875  [44800/481450]\n",
      "loss: 0.587372  [48000/481450]\n",
      "loss: 0.181860  [51200/481450]\n",
      "loss: 0.480441  [54400/481450]\n",
      "loss: 0.357508  [57600/481450]\n",
      "loss: 0.296356  [60800/481450]\n",
      "loss: 0.228428  [64000/481450]\n",
      "loss: 0.236203  [67200/481450]\n",
      "loss: 0.275061  [70400/481450]\n",
      "loss: 0.233368  [73600/481450]\n",
      "loss: 0.192078  [76800/481450]\n",
      "loss: 0.102775  [80000/481450]\n",
      "loss: 0.310503  [83200/481450]\n",
      "loss: 0.169340  [86400/481450]\n",
      "loss: 0.139034  [89600/481450]\n",
      "loss: 0.258526  [92800/481450]\n",
      "loss: 0.287040  [96000/481450]\n",
      "loss: 0.078704  [99200/481450]\n",
      "loss: 0.457634  [102400/481450]\n",
      "loss: 0.261500  [105600/481450]\n",
      "loss: 0.238631  [108800/481450]\n",
      "loss: 0.190045  [112000/481450]\n",
      "loss: 0.237086  [115200/481450]\n",
      "loss: 0.153657  [118400/481450]\n",
      "loss: 0.207370  [121600/481450]\n",
      "loss: 0.448912  [124800/481450]\n",
      "loss: 0.214893  [128000/481450]\n",
      "loss: 0.124815  [131200/481450]\n",
      "loss: 0.334936  [134400/481450]\n",
      "loss: 0.225809  [137600/481450]\n",
      "loss: 0.263755  [140800/481450]\n",
      "loss: 0.200381  [144000/481450]\n",
      "loss: 0.301836  [147200/481450]\n",
      "loss: 0.302806  [150400/481450]\n",
      "loss: 0.167355  [153600/481450]\n",
      "loss: 0.052365  [156800/481450]\n",
      "loss: 0.513650  [160000/481450]\n",
      "loss: 0.175837  [163200/481450]\n",
      "loss: 0.189652  [166400/481450]\n",
      "loss: 0.226411  [169600/481450]\n",
      "loss: 0.038727  [172800/481450]\n",
      "loss: 0.187302  [176000/481450]\n",
      "loss: 0.215494  [179200/481450]\n",
      "loss: 0.312296  [182400/481450]\n",
      "loss: 0.150993  [185600/481450]\n",
      "loss: 0.229323  [188800/481450]\n",
      "loss: 0.222827  [192000/481450]\n",
      "loss: 0.251350  [195200/481450]\n",
      "loss: 0.251169  [198400/481450]\n",
      "loss: 0.171146  [201600/481450]\n",
      "loss: 0.208157  [204800/481450]\n",
      "loss: 0.282019  [208000/481450]\n",
      "loss: 0.286475  [211200/481450]\n",
      "loss: 0.571339  [214400/481450]\n",
      "loss: 0.106712  [217600/481450]\n",
      "loss: 0.219126  [220800/481450]\n",
      "loss: 0.217142  [224000/481450]\n",
      "loss: 0.167260  [227200/481450]\n",
      "loss: 0.205208  [230400/481450]\n",
      "loss: 0.279507  [233600/481450]\n",
      "loss: 0.190866  [236800/481450]\n",
      "loss: 0.208226  [240000/481450]\n",
      "loss: 0.537489  [243200/481450]\n",
      "loss: 0.296116  [246400/481450]\n",
      "loss: 0.134204  [249600/481450]\n",
      "loss: 0.355031  [252800/481450]\n",
      "loss: 0.281411  [256000/481450]\n",
      "loss: 0.211899  [259200/481450]\n",
      "loss: 0.469706  [262400/481450]\n",
      "loss: 0.265527  [265600/481450]\n",
      "loss: 0.258079  [268800/481450]\n",
      "loss: 0.158802  [272000/481450]\n",
      "loss: 0.182673  [275200/481450]\n",
      "loss: 0.278433  [278400/481450]\n",
      "loss: 0.220150  [281600/481450]\n",
      "loss: 0.291329  [284800/481450]\n",
      "loss: 0.146519  [288000/481450]\n",
      "loss: 0.212506  [291200/481450]\n",
      "loss: 0.118357  [294400/481450]\n",
      "loss: 0.124961  [297600/481450]\n",
      "loss: 0.089053  [300800/481450]\n",
      "loss: 0.173206  [304000/481450]\n",
      "loss: 0.355215  [307200/481450]\n",
      "loss: 0.210974  [310400/481450]\n",
      "loss: 0.164231  [313600/481450]\n",
      "loss: 0.270042  [316800/481450]\n",
      "loss: 0.260921  [320000/481450]\n",
      "loss: 0.168027  [323200/481450]\n",
      "loss: 0.287316  [326400/481450]\n",
      "loss: 0.400658  [329600/481450]\n",
      "loss: 0.067343  [332800/481450]\n",
      "loss: 0.382057  [336000/481450]\n",
      "loss: 0.112551  [339200/481450]\n",
      "loss: 0.290631  [342400/481450]\n",
      "loss: 0.167764  [345600/481450]\n",
      "loss: 0.129842  [348800/481450]\n",
      "loss: 0.157478  [352000/481450]\n",
      "loss: 0.238345  [355200/481450]\n",
      "loss: 0.651394  [358400/481450]\n",
      "loss: 0.273987  [361600/481450]\n",
      "loss: 0.369599  [364800/481450]\n",
      "loss: 0.274759  [368000/481450]\n",
      "loss: 0.170653  [371200/481450]\n",
      "loss: 0.258794  [374400/481450]\n",
      "loss: 0.209757  [377600/481450]\n",
      "loss: 0.432535  [380800/481450]\n",
      "loss: 0.379607  [384000/481450]\n",
      "loss: 0.292744  [387200/481450]\n",
      "loss: 0.314859  [390400/481450]\n",
      "loss: 0.232070  [393600/481450]\n",
      "loss: 0.274749  [396800/481450]\n",
      "loss: 0.524370  [400000/481450]\n",
      "loss: 0.228284  [403200/481450]\n",
      "loss: 0.185811  [406400/481450]\n",
      "loss: 0.200235  [409600/481450]\n",
      "loss: 0.248883  [412800/481450]\n",
      "loss: 0.256854  [416000/481450]\n",
      "loss: 0.479977  [419200/481450]\n",
      "loss: 0.309526  [422400/481450]\n",
      "loss: 0.155730  [425600/481450]\n",
      "loss: 0.392760  [428800/481450]\n",
      "loss: 0.237248  [432000/481450]\n",
      "loss: 0.322044  [435200/481450]\n",
      "loss: 0.171243  [438400/481450]\n",
      "loss: 0.364217  [441600/481450]\n",
      "loss: 0.392235  [444800/481450]\n",
      "loss: 0.105702  [448000/481450]\n",
      "loss: 0.380282  [451200/481450]\n",
      "loss: 0.373746  [454400/481450]\n",
      "loss: 0.294384  [457600/481450]\n",
      "loss: 0.267228  [460800/481450]\n",
      "loss: 0.591015  [464000/481450]\n",
      "loss: 0.281554  [467200/481450]\n",
      "loss: 0.131638  [470400/481450]\n",
      "loss: 0.362603  [473600/481450]\n",
      "loss: 0.118607  [476800/481450]\n",
      "loss: 0.354557  [480000/481450]\n",
      "Train Accuracy: 89.9015%\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.569279, F1-score: 87.12% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.205557  [    0/481450]\n",
      "loss: 0.110355  [ 3200/481450]\n",
      "loss: 0.070342  [ 6400/481450]\n",
      "loss: 0.219523  [ 9600/481450]\n",
      "loss: 0.137510  [12800/481450]\n",
      "loss: 0.325950  [16000/481450]\n",
      "loss: 0.212999  [19200/481450]\n",
      "loss: 0.190419  [22400/481450]\n",
      "loss: 0.175115  [25600/481450]\n",
      "loss: 0.270821  [28800/481450]\n",
      "loss: 0.341030  [32000/481450]\n",
      "loss: 0.114483  [35200/481450]\n",
      "loss: 0.157352  [38400/481450]\n",
      "loss: 0.094957  [41600/481450]\n",
      "loss: 0.185875  [44800/481450]\n",
      "loss: 0.187703  [48000/481450]\n",
      "loss: 0.182071  [51200/481450]\n",
      "loss: 0.102275  [54400/481450]\n",
      "loss: 0.188211  [57600/481450]\n",
      "loss: 0.152966  [60800/481450]\n",
      "loss: 0.438318  [64000/481450]\n",
      "loss: 0.104840  [67200/481450]\n",
      "loss: 0.288033  [70400/481450]\n",
      "loss: 0.224763  [73600/481450]\n",
      "loss: 0.171478  [76800/481450]\n",
      "loss: 0.228393  [80000/481450]\n",
      "loss: 0.247134  [83200/481450]\n",
      "loss: 0.292695  [86400/481450]\n",
      "loss: 0.240213  [89600/481450]\n",
      "loss: 0.293323  [92800/481450]\n",
      "loss: 0.110180  [96000/481450]\n",
      "loss: 0.257337  [99200/481450]\n",
      "loss: 0.150742  [102400/481450]\n",
      "loss: 0.272430  [105600/481450]\n",
      "loss: 0.189163  [108800/481450]\n",
      "loss: 0.162052  [112000/481450]\n",
      "loss: 0.166313  [115200/481450]\n",
      "loss: 0.480282  [118400/481450]\n",
      "loss: 0.072120  [121600/481450]\n",
      "loss: 0.287086  [124800/481450]\n",
      "loss: 0.284023  [128000/481450]\n",
      "loss: 0.331375  [131200/481450]\n",
      "loss: 0.218243  [134400/481450]\n",
      "loss: 0.155307  [137600/481450]\n",
      "loss: 0.139227  [140800/481450]\n",
      "loss: 0.251409  [144000/481450]\n",
      "loss: 0.177304  [147200/481450]\n",
      "loss: 0.104233  [150400/481450]\n",
      "loss: 0.202805  [153600/481450]\n",
      "loss: 0.349547  [156800/481450]\n",
      "loss: 0.245625  [160000/481450]\n",
      "loss: 0.266874  [163200/481450]\n",
      "loss: 0.465881  [166400/481450]\n",
      "loss: 0.054148  [169600/481450]\n",
      "loss: 0.396283  [172800/481450]\n",
      "loss: 0.126495  [176000/481450]\n",
      "loss: 0.266025  [179200/481450]\n",
      "loss: 0.109913  [182400/481450]\n",
      "loss: 0.197429  [185600/481450]\n",
      "loss: 0.146638  [188800/481450]\n",
      "loss: 0.276930  [192000/481450]\n",
      "loss: 0.119150  [195200/481450]\n",
      "loss: 0.237294  [198400/481450]\n",
      "loss: 0.120249  [201600/481450]\n",
      "loss: 0.248940  [204800/481450]\n",
      "loss: 0.335262  [208000/481450]\n",
      "loss: 0.301915  [211200/481450]\n",
      "loss: 0.222511  [214400/481450]\n",
      "loss: 0.219547  [217600/481450]\n",
      "loss: 0.092764  [220800/481450]\n",
      "loss: 0.361931  [224000/481450]\n",
      "loss: 0.240489  [227200/481450]\n",
      "loss: 0.280400  [230400/481450]\n",
      "loss: 0.051721  [233600/481450]\n",
      "loss: 0.109789  [236800/481450]\n",
      "loss: 0.215840  [240000/481450]\n",
      "loss: 0.445071  [243200/481450]\n",
      "loss: 0.167400  [246400/481450]\n",
      "loss: 0.297973  [249600/481450]\n",
      "loss: 0.195431  [252800/481450]\n",
      "loss: 0.148267  [256000/481450]\n",
      "loss: 0.313015  [259200/481450]\n",
      "loss: 0.201364  [262400/481450]\n",
      "loss: 0.522908  [265600/481450]\n",
      "loss: 0.317854  [268800/481450]\n",
      "loss: 0.179089  [272000/481450]\n",
      "loss: 0.333861  [275200/481450]\n",
      "loss: 0.292821  [278400/481450]\n",
      "loss: 0.193605  [281600/481450]\n",
      "loss: 0.169446  [284800/481450]\n",
      "loss: 0.099828  [288000/481450]\n",
      "loss: 0.343343  [291200/481450]\n",
      "loss: 0.202049  [294400/481450]\n",
      "loss: 0.181489  [297600/481450]\n",
      "loss: 0.246908  [300800/481450]\n",
      "loss: 0.091180  [304000/481450]\n",
      "loss: 0.181950  [307200/481450]\n",
      "loss: 0.209134  [310400/481450]\n",
      "loss: 0.489296  [313600/481450]\n",
      "loss: 0.157635  [316800/481450]\n",
      "loss: 0.242718  [320000/481450]\n",
      "loss: 0.338784  [323200/481450]\n",
      "loss: 0.368874  [326400/481450]\n",
      "loss: 0.085229  [329600/481450]\n",
      "loss: 0.240074  [332800/481450]\n",
      "loss: 0.426083  [336000/481450]\n",
      "loss: 0.052948  [339200/481450]\n",
      "loss: 0.278027  [342400/481450]\n",
      "loss: 0.409150  [345600/481450]\n",
      "loss: 0.239397  [348800/481450]\n",
      "loss: 0.087777  [352000/481450]\n",
      "loss: 0.312829  [355200/481450]\n",
      "loss: 0.123948  [358400/481450]\n",
      "loss: 0.257843  [361600/481450]\n",
      "loss: 0.184184  [364800/481450]\n",
      "loss: 0.430111  [368000/481450]\n",
      "loss: 0.231497  [371200/481450]\n",
      "loss: 0.157471  [374400/481450]\n",
      "loss: 0.101531  [377600/481450]\n",
      "loss: 0.183683  [380800/481450]\n",
      "loss: 0.104929  [384000/481450]\n",
      "loss: 0.137127  [387200/481450]\n",
      "loss: 0.440731  [390400/481450]\n",
      "loss: 0.386334  [393600/481450]\n",
      "loss: 0.221240  [396800/481450]\n",
      "loss: 0.222151  [400000/481450]\n",
      "loss: 0.209433  [403200/481450]\n",
      "loss: 0.457444  [406400/481450]\n",
      "loss: 0.467519  [409600/481450]\n",
      "loss: 0.161347  [412800/481450]\n",
      "loss: 0.256661  [416000/481450]\n",
      "loss: 0.309161  [419200/481450]\n",
      "loss: 0.220258  [422400/481450]\n",
      "loss: 0.156512  [425600/481450]\n",
      "loss: 0.349608  [428800/481450]\n",
      "loss: 0.244074  [432000/481450]\n",
      "loss: 0.180236  [435200/481450]\n",
      "loss: 0.168390  [438400/481450]\n",
      "loss: 0.133692  [441600/481450]\n",
      "loss: 0.269893  [444800/481450]\n",
      "loss: 0.146349  [448000/481450]\n",
      "loss: 0.224820  [451200/481450]\n",
      "loss: 0.254962  [454400/481450]\n",
      "loss: 0.087154  [457600/481450]\n",
      "loss: 0.345950  [460800/481450]\n",
      "loss: 0.174462  [464000/481450]\n",
      "loss: 0.264098  [467200/481450]\n",
      "loss: 0.168620  [470400/481450]\n",
      "loss: 0.389196  [473600/481450]\n",
      "loss: 0.339820  [476800/481450]\n",
      "loss: 0.445982  [480000/481450]\n",
      "Train Accuracy: 90.1822%\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.578018, F1-score: 87.12% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.445651  [    0/481450]\n",
      "loss: 0.246544  [ 3200/481450]\n",
      "loss: 0.349853  [ 6400/481450]\n",
      "loss: 0.184639  [ 9600/481450]\n",
      "loss: 0.157060  [12800/481450]\n",
      "loss: 0.314502  [16000/481450]\n",
      "loss: 0.276381  [19200/481450]\n",
      "loss: 0.202409  [22400/481450]\n",
      "loss: 0.287625  [25600/481450]\n",
      "loss: 0.074178  [28800/481450]\n",
      "loss: 0.285092  [32000/481450]\n",
      "loss: 0.328331  [35200/481450]\n",
      "loss: 0.208847  [38400/481450]\n",
      "loss: 0.247427  [41600/481450]\n",
      "loss: 0.232533  [44800/481450]\n",
      "loss: 0.106905  [48000/481450]\n",
      "loss: 0.313032  [51200/481450]\n",
      "loss: 0.129184  [54400/481450]\n",
      "loss: 0.278587  [57600/481450]\n",
      "loss: 0.320810  [60800/481450]\n",
      "loss: 0.200003  [64000/481450]\n",
      "loss: 0.254002  [67200/481450]\n",
      "loss: 0.247070  [70400/481450]\n",
      "loss: 0.203489  [73600/481450]\n",
      "loss: 0.418885  [76800/481450]\n",
      "loss: 0.377932  [80000/481450]\n",
      "loss: 0.239055  [83200/481450]\n",
      "loss: 0.097362  [86400/481450]\n",
      "loss: 0.374863  [89600/481450]\n",
      "loss: 0.203513  [92800/481450]\n",
      "loss: 0.237921  [96000/481450]\n",
      "loss: 0.223438  [99200/481450]\n",
      "loss: 0.253854  [102400/481450]\n",
      "loss: 0.400292  [105600/481450]\n",
      "loss: 0.108621  [108800/481450]\n",
      "loss: 0.252372  [112000/481450]\n",
      "loss: 0.312405  [115200/481450]\n",
      "loss: 0.145576  [118400/481450]\n",
      "loss: 0.379632  [121600/481450]\n",
      "loss: 0.335814  [124800/481450]\n",
      "loss: 0.079499  [128000/481450]\n",
      "loss: 0.379630  [131200/481450]\n",
      "loss: 0.268490  [134400/481450]\n",
      "loss: 0.336169  [137600/481450]\n",
      "loss: 0.179529  [140800/481450]\n",
      "loss: 0.144727  [144000/481450]\n",
      "loss: 0.355158  [147200/481450]\n",
      "loss: 0.241725  [150400/481450]\n",
      "loss: 0.378603  [153600/481450]\n",
      "loss: 0.264538  [156800/481450]\n",
      "loss: 0.241612  [160000/481450]\n",
      "loss: 0.193326  [163200/481450]\n",
      "loss: 0.226138  [166400/481450]\n",
      "loss: 0.176262  [169600/481450]\n",
      "loss: 0.331831  [172800/481450]\n",
      "loss: 0.028066  [176000/481450]\n",
      "loss: 0.294804  [179200/481450]\n",
      "loss: 0.079224  [182400/481450]\n",
      "loss: 0.225520  [185600/481450]\n",
      "loss: 0.226789  [188800/481450]\n",
      "loss: 0.172361  [192000/481450]\n",
      "loss: 0.162827  [195200/481450]\n",
      "loss: 0.118317  [198400/481450]\n",
      "loss: 0.175034  [201600/481450]\n",
      "loss: 0.108832  [204800/481450]\n",
      "loss: 0.213343  [208000/481450]\n",
      "loss: 0.667275  [211200/481450]\n",
      "loss: 0.333770  [214400/481450]\n",
      "loss: 0.209064  [217600/481450]\n",
      "loss: 0.333601  [220800/481450]\n",
      "loss: 0.365968  [224000/481450]\n",
      "loss: 0.233999  [227200/481450]\n",
      "loss: 0.189734  [230400/481450]\n",
      "loss: 0.149627  [233600/481450]\n",
      "loss: 0.270602  [236800/481450]\n",
      "loss: 0.344694  [240000/481450]\n",
      "loss: 0.244893  [243200/481450]\n",
      "loss: 0.420560  [246400/481450]\n",
      "loss: 0.292429  [249600/481450]\n",
      "loss: 0.398601  [252800/481450]\n",
      "loss: 0.073938  [256000/481450]\n",
      "loss: 0.119958  [259200/481450]\n",
      "loss: 0.136313  [262400/481450]\n",
      "loss: 0.201794  [265600/481450]\n",
      "loss: 0.048481  [268800/481450]\n",
      "loss: 0.319077  [272000/481450]\n",
      "loss: 0.477450  [275200/481450]\n",
      "loss: 0.273812  [278400/481450]\n",
      "loss: 0.362672  [281600/481450]\n",
      "loss: 0.179172  [284800/481450]\n",
      "loss: 0.131670  [288000/481450]\n",
      "loss: 0.102533  [291200/481450]\n",
      "loss: 0.164790  [294400/481450]\n",
      "loss: 0.050877  [297600/481450]\n",
      "loss: 0.219575  [300800/481450]\n",
      "loss: 0.199554  [304000/481450]\n",
      "loss: 0.377056  [307200/481450]\n",
      "loss: 0.242266  [310400/481450]\n",
      "loss: 0.300106  [313600/481450]\n",
      "loss: 0.184962  [316800/481450]\n",
      "loss: 0.337241  [320000/481450]\n",
      "loss: 0.146261  [323200/481450]\n",
      "loss: 0.194463  [326400/481450]\n",
      "loss: 0.341974  [329600/481450]\n",
      "loss: 0.194761  [332800/481450]\n",
      "loss: 0.200004  [336000/481450]\n",
      "loss: 0.256745  [339200/481450]\n",
      "loss: 0.159177  [342400/481450]\n",
      "loss: 0.243273  [345600/481450]\n",
      "loss: 0.282835  [348800/481450]\n",
      "loss: 0.435141  [352000/481450]\n",
      "loss: 0.129339  [355200/481450]\n",
      "loss: 0.357236  [358400/481450]\n",
      "loss: 0.514968  [361600/481450]\n",
      "loss: 0.198964  [364800/481450]\n",
      "loss: 0.139382  [368000/481450]\n",
      "loss: 0.320687  [371200/481450]\n",
      "loss: 0.170493  [374400/481450]\n",
      "loss: 0.337931  [377600/481450]\n",
      "loss: 0.214170  [380800/481450]\n",
      "loss: 0.112206  [384000/481450]\n",
      "loss: 0.132951  [387200/481450]\n",
      "loss: 0.247962  [390400/481450]\n",
      "loss: 0.132792  [393600/481450]\n",
      "loss: 0.235393  [396800/481450]\n",
      "loss: 0.476104  [400000/481450]\n",
      "loss: 0.396408  [403200/481450]\n",
      "loss: 0.284688  [406400/481450]\n",
      "loss: 0.347543  [409600/481450]\n",
      "loss: 0.242780  [412800/481450]\n",
      "loss: 0.097419  [416000/481450]\n",
      "loss: 0.114968  [419200/481450]\n",
      "loss: 0.161505  [422400/481450]\n",
      "loss: 0.280244  [425600/481450]\n",
      "loss: 0.170407  [428800/481450]\n",
      "loss: 0.138897  [432000/481450]\n",
      "loss: 0.232563  [435200/481450]\n",
      "loss: 0.128222  [438400/481450]\n",
      "loss: 0.501169  [441600/481450]\n",
      "loss: 0.248743  [444800/481450]\n",
      "loss: 0.200161  [448000/481450]\n",
      "loss: 0.193820  [451200/481450]\n",
      "loss: 0.333330  [454400/481450]\n",
      "loss: 0.046604  [457600/481450]\n",
      "loss: 0.268688  [460800/481450]\n",
      "loss: 0.120398  [464000/481450]\n",
      "loss: 0.282372  [467200/481450]\n",
      "loss: 0.136000  [470400/481450]\n",
      "loss: 0.107257  [473600/481450]\n",
      "loss: 0.242309  [476800/481450]\n",
      "loss: 0.130671  [480000/481450]\n",
      "Train Accuracy: 90.4360%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.551584, F1-score: 87.62% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.245588  [    0/481450]\n",
      "loss: 0.327295  [ 3200/481450]\n",
      "loss: 0.289880  [ 6400/481450]\n",
      "loss: 0.284917  [ 9600/481450]\n",
      "loss: 0.269638  [12800/481450]\n",
      "loss: 0.229182  [16000/481450]\n",
      "loss: 0.226634  [19200/481450]\n",
      "loss: 0.308655  [22400/481450]\n",
      "loss: 0.201886  [25600/481450]\n",
      "loss: 0.114902  [28800/481450]\n",
      "loss: 0.351612  [32000/481450]\n",
      "loss: 0.185355  [35200/481450]\n",
      "loss: 0.581807  [38400/481450]\n",
      "loss: 0.188020  [41600/481450]\n",
      "loss: 0.158410  [44800/481450]\n",
      "loss: 0.120183  [48000/481450]\n",
      "loss: 0.235182  [51200/481450]\n",
      "loss: 0.338509  [54400/481450]\n",
      "loss: 0.404992  [57600/481450]\n",
      "loss: 0.196351  [60800/481450]\n",
      "loss: 0.328204  [64000/481450]\n",
      "loss: 0.447378  [67200/481450]\n",
      "loss: 0.230250  [70400/481450]\n",
      "loss: 0.239008  [73600/481450]\n",
      "loss: 0.110419  [76800/481450]\n",
      "loss: 0.204177  [80000/481450]\n",
      "loss: 0.139158  [83200/481450]\n",
      "loss: 0.231914  [86400/481450]\n",
      "loss: 0.101522  [89600/481450]\n",
      "loss: 0.181905  [92800/481450]\n",
      "loss: 0.197721  [96000/481450]\n",
      "loss: 0.308833  [99200/481450]\n",
      "loss: 0.137558  [102400/481450]\n",
      "loss: 0.480292  [105600/481450]\n",
      "loss: 0.230065  [108800/481450]\n",
      "loss: 0.163667  [112000/481450]\n",
      "loss: 0.182350  [115200/481450]\n",
      "loss: 0.207311  [118400/481450]\n",
      "loss: 0.138488  [121600/481450]\n",
      "loss: 0.297713  [124800/481450]\n",
      "loss: 0.237855  [128000/481450]\n",
      "loss: 0.401664  [131200/481450]\n",
      "loss: 0.201031  [134400/481450]\n",
      "loss: 0.083215  [137600/481450]\n",
      "loss: 0.238209  [140800/481450]\n",
      "loss: 0.259712  [144000/481450]\n",
      "loss: 0.465399  [147200/481450]\n",
      "loss: 0.213845  [150400/481450]\n",
      "loss: 0.656021  [153600/481450]\n",
      "loss: 0.232692  [156800/481450]\n",
      "loss: 0.201173  [160000/481450]\n",
      "loss: 0.168971  [163200/481450]\n",
      "loss: 0.188810  [166400/481450]\n",
      "loss: 0.312274  [169600/481450]\n",
      "loss: 0.185847  [172800/481450]\n",
      "loss: 0.231938  [176000/481450]\n",
      "loss: 0.350594  [179200/481450]\n",
      "loss: 0.251913  [182400/481450]\n",
      "loss: 0.315285  [185600/481450]\n",
      "loss: 0.364252  [188800/481450]\n",
      "loss: 0.349814  [192000/481450]\n",
      "loss: 0.385495  [195200/481450]\n",
      "loss: 0.367618  [198400/481450]\n",
      "loss: 0.168701  [201600/481450]\n",
      "loss: 0.257010  [204800/481450]\n",
      "loss: 0.237149  [208000/481450]\n",
      "loss: 0.399208  [211200/481450]\n",
      "loss: 0.209469  [214400/481450]\n",
      "loss: 0.158614  [217600/481450]\n",
      "loss: 0.383666  [220800/481450]\n",
      "loss: 0.086061  [224000/481450]\n",
      "loss: 0.287933  [227200/481450]\n",
      "loss: 0.125305  [230400/481450]\n",
      "loss: 0.387984  [233600/481450]\n",
      "loss: 0.216595  [236800/481450]\n",
      "loss: 0.267892  [240000/481450]\n",
      "loss: 0.135224  [243200/481450]\n",
      "loss: 0.078158  [246400/481450]\n",
      "loss: 0.283008  [249600/481450]\n",
      "loss: 0.079868  [252800/481450]\n",
      "loss: 0.285990  [256000/481450]\n",
      "loss: 0.286544  [259200/481450]\n",
      "loss: 0.170135  [262400/481450]\n",
      "loss: 0.303551  [265600/481450]\n",
      "loss: 0.145635  [268800/481450]\n",
      "loss: 0.143922  [272000/481450]\n",
      "loss: 0.139074  [275200/481450]\n",
      "loss: 0.245117  [278400/481450]\n",
      "loss: 0.239138  [281600/481450]\n",
      "loss: 0.026879  [284800/481450]\n",
      "loss: 0.187458  [288000/481450]\n",
      "loss: 0.310385  [291200/481450]\n",
      "loss: 0.187100  [294400/481450]\n",
      "loss: 0.216236  [297600/481450]\n",
      "loss: 0.199193  [300800/481450]\n",
      "loss: 0.128292  [304000/481450]\n",
      "loss: 0.147480  [307200/481450]\n",
      "loss: 0.178992  [310400/481450]\n",
      "loss: 0.397444  [313600/481450]\n",
      "loss: 0.160396  [316800/481450]\n",
      "loss: 0.129023  [320000/481450]\n",
      "loss: 0.269042  [323200/481450]\n",
      "loss: 0.201341  [326400/481450]\n",
      "loss: 0.160679  [329600/481450]\n",
      "loss: 0.207642  [332800/481450]\n",
      "loss: 0.215778  [336000/481450]\n",
      "loss: 0.187289  [339200/481450]\n",
      "loss: 0.236147  [342400/481450]\n",
      "loss: 0.418510  [345600/481450]\n",
      "loss: 0.077726  [348800/481450]\n",
      "loss: 0.083871  [352000/481450]\n",
      "loss: 0.222284  [355200/481450]\n",
      "loss: 0.125429  [358400/481450]\n",
      "loss: 0.276978  [361600/481450]\n",
      "loss: 0.030972  [364800/481450]\n",
      "loss: 0.404467  [368000/481450]\n",
      "loss: 0.100471  [371200/481450]\n",
      "loss: 0.153267  [374400/481450]\n",
      "loss: 0.509557  [377600/481450]\n",
      "loss: 0.274845  [380800/481450]\n",
      "loss: 0.065279  [384000/481450]\n",
      "loss: 0.123560  [387200/481450]\n",
      "loss: 0.454803  [390400/481450]\n",
      "loss: 0.408510  [393600/481450]\n",
      "loss: 0.333842  [396800/481450]\n",
      "loss: 0.237099  [400000/481450]\n",
      "loss: 0.269971  [403200/481450]\n",
      "loss: 0.194162  [406400/481450]\n",
      "loss: 0.164165  [409600/481450]\n",
      "loss: 0.354553  [412800/481450]\n",
      "loss: 0.211737  [416000/481450]\n",
      "loss: 0.360825  [419200/481450]\n",
      "loss: 0.097629  [422400/481450]\n",
      "loss: 0.229586  [425600/481450]\n",
      "loss: 0.200249  [428800/481450]\n",
      "loss: 0.273434  [432000/481450]\n",
      "loss: 0.377199  [435200/481450]\n",
      "loss: 0.169521  [438400/481450]\n",
      "loss: 0.183292  [441600/481450]\n",
      "loss: 0.221317  [444800/481450]\n",
      "loss: 0.125294  [448000/481450]\n",
      "loss: 0.330585  [451200/481450]\n",
      "loss: 0.074831  [454400/481450]\n",
      "loss: 0.131298  [457600/481450]\n",
      "loss: 0.188934  [460800/481450]\n",
      "loss: 0.124021  [464000/481450]\n",
      "loss: 0.177085  [467200/481450]\n",
      "loss: 0.180353  [470400/481450]\n",
      "loss: 0.511938  [473600/481450]\n",
      "loss: 0.132885  [476800/481450]\n",
      "loss: 0.167889  [480000/481450]\n",
      "Train Accuracy: 90.6393%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.614149, F1-score: 87.38% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.260609  [    0/481450]\n",
      "loss: 0.221552  [ 3200/481450]\n",
      "loss: 0.120837  [ 6400/481450]\n",
      "loss: 0.099391  [ 9600/481450]\n",
      "loss: 0.389780  [12800/481450]\n",
      "loss: 0.129264  [16000/481450]\n",
      "loss: 0.198680  [19200/481450]\n",
      "loss: 0.155483  [22400/481450]\n",
      "loss: 0.433370  [25600/481450]\n",
      "loss: 0.087714  [28800/481450]\n",
      "loss: 0.534451  [32000/481450]\n",
      "loss: 0.231920  [35200/481450]\n",
      "loss: 0.287293  [38400/481450]\n",
      "loss: 0.185998  [41600/481450]\n",
      "loss: 0.091606  [44800/481450]\n",
      "loss: 0.243607  [48000/481450]\n",
      "loss: 0.234371  [51200/481450]\n",
      "loss: 0.142300  [54400/481450]\n",
      "loss: 0.215546  [57600/481450]\n",
      "loss: 0.169531  [60800/481450]\n",
      "loss: 0.243455  [64000/481450]\n",
      "loss: 0.238149  [67200/481450]\n",
      "loss: 0.167015  [70400/481450]\n",
      "loss: 0.243356  [73600/481450]\n",
      "loss: 0.179799  [76800/481450]\n",
      "loss: 0.495126  [80000/481450]\n",
      "loss: 0.179699  [83200/481450]\n",
      "loss: 0.255597  [86400/481450]\n",
      "loss: 0.191950  [89600/481450]\n",
      "loss: 0.495985  [92800/481450]\n",
      "loss: 0.330503  [96000/481450]\n",
      "loss: 0.191794  [99200/481450]\n",
      "loss: 0.157222  [102400/481450]\n",
      "loss: 0.227640  [105600/481450]\n",
      "loss: 0.128888  [108800/481450]\n",
      "loss: 0.131015  [112000/481450]\n",
      "loss: 0.253659  [115200/481450]\n",
      "loss: 0.102315  [118400/481450]\n",
      "loss: 0.158487  [121600/481450]\n",
      "loss: 0.103053  [124800/481450]\n",
      "loss: 0.171940  [128000/481450]\n",
      "loss: 0.239602  [131200/481450]\n",
      "loss: 0.399250  [134400/481450]\n",
      "loss: 0.170699  [137600/481450]\n",
      "loss: 0.198728  [140800/481450]\n",
      "loss: 0.225017  [144000/481450]\n",
      "loss: 0.219681  [147200/481450]\n",
      "loss: 0.187094  [150400/481450]\n",
      "loss: 0.394734  [153600/481450]\n",
      "loss: 0.379810  [156800/481450]\n",
      "loss: 0.290895  [160000/481450]\n",
      "loss: 0.320606  [163200/481450]\n",
      "loss: 0.217177  [166400/481450]\n",
      "loss: 0.163876  [169600/481450]\n",
      "loss: 0.258280  [172800/481450]\n",
      "loss: 0.121355  [176000/481450]\n",
      "loss: 0.092592  [179200/481450]\n",
      "loss: 0.287203  [182400/481450]\n",
      "loss: 0.159357  [185600/481450]\n",
      "loss: 0.171502  [188800/481450]\n",
      "loss: 0.312144  [192000/481450]\n",
      "loss: 0.299492  [195200/481450]\n",
      "loss: 0.165973  [198400/481450]\n",
      "loss: 0.100606  [201600/481450]\n",
      "loss: 0.176117  [204800/481450]\n",
      "loss: 0.225705  [208000/481450]\n",
      "loss: 0.255144  [211200/481450]\n",
      "loss: 0.211173  [214400/481450]\n",
      "loss: 0.221534  [217600/481450]\n",
      "loss: 0.179632  [220800/481450]\n",
      "loss: 0.228525  [224000/481450]\n",
      "loss: 0.261412  [227200/481450]\n",
      "loss: 0.073082  [230400/481450]\n",
      "loss: 0.418232  [233600/481450]\n",
      "loss: 0.227359  [236800/481450]\n",
      "loss: 0.171944  [240000/481450]\n",
      "loss: 0.255247  [243200/481450]\n",
      "loss: 0.570700  [246400/481450]\n",
      "loss: 0.420639  [249600/481450]\n",
      "loss: 0.164206  [252800/481450]\n",
      "loss: 0.234805  [256000/481450]\n",
      "loss: 0.130698  [259200/481450]\n",
      "loss: 0.177415  [262400/481450]\n",
      "loss: 0.279410  [265600/481450]\n",
      "loss: 0.257629  [268800/481450]\n",
      "loss: 0.179839  [272000/481450]\n",
      "loss: 0.085578  [275200/481450]\n",
      "loss: 0.368416  [278400/481450]\n",
      "loss: 0.176747  [281600/481450]\n",
      "loss: 0.171625  [284800/481450]\n",
      "loss: 0.315026  [288000/481450]\n",
      "loss: 0.300333  [291200/481450]\n",
      "loss: 0.524067  [294400/481450]\n",
      "loss: 0.600917  [297600/481450]\n",
      "loss: 0.094568  [300800/481450]\n",
      "loss: 0.125529  [304000/481450]\n",
      "loss: 0.267729  [307200/481450]\n",
      "loss: 0.082214  [310400/481450]\n",
      "loss: 0.171095  [313600/481450]\n",
      "loss: 0.082579  [316800/481450]\n",
      "loss: 0.134558  [320000/481450]\n",
      "loss: 0.171271  [323200/481450]\n",
      "loss: 0.499653  [326400/481450]\n",
      "loss: 0.186957  [329600/481450]\n",
      "loss: 0.126405  [332800/481450]\n",
      "loss: 0.223820  [336000/481450]\n",
      "loss: 0.202338  [339200/481450]\n",
      "loss: 0.395595  [342400/481450]\n",
      "loss: 0.281583  [345600/481450]\n",
      "loss: 0.303005  [348800/481450]\n",
      "loss: 0.119716  [352000/481450]\n",
      "loss: 0.085201  [355200/481450]\n",
      "loss: 0.250463  [358400/481450]\n",
      "loss: 0.356939  [361600/481450]\n",
      "loss: 0.159677  [364800/481450]\n",
      "loss: 0.373644  [368000/481450]\n",
      "loss: 0.291393  [371200/481450]\n",
      "loss: 0.151721  [374400/481450]\n",
      "loss: 0.086303  [377600/481450]\n",
      "loss: 0.248691  [380800/481450]\n",
      "loss: 0.264958  [384000/481450]\n",
      "loss: 0.226190  [387200/481450]\n",
      "loss: 0.264764  [390400/481450]\n",
      "loss: 0.098846  [393600/481450]\n",
      "loss: 0.172962  [396800/481450]\n",
      "loss: 0.185785  [400000/481450]\n",
      "loss: 0.366184  [403200/481450]\n",
      "loss: 0.114989  [406400/481450]\n",
      "loss: 0.259280  [409600/481450]\n",
      "loss: 0.166932  [412800/481450]\n",
      "loss: 0.133206  [416000/481450]\n",
      "loss: 0.186656  [419200/481450]\n",
      "loss: 0.148368  [422400/481450]\n",
      "loss: 0.159689  [425600/481450]\n",
      "loss: 0.166025  [428800/481450]\n",
      "loss: 0.218800  [432000/481450]\n",
      "loss: 0.247328  [435200/481450]\n",
      "loss: 0.176031  [438400/481450]\n",
      "loss: 0.268161  [441600/481450]\n",
      "loss: 0.353304  [444800/481450]\n",
      "loss: 0.215095  [448000/481450]\n",
      "loss: 0.331181  [451200/481450]\n",
      "loss: 0.211136  [454400/481450]\n",
      "loss: 0.293856  [457600/481450]\n",
      "loss: 0.125810  [460800/481450]\n",
      "loss: 0.113946  [464000/481450]\n",
      "loss: 0.070586  [467200/481450]\n",
      "loss: 0.162544  [470400/481450]\n",
      "loss: 0.126224  [473600/481450]\n",
      "loss: 0.264267  [476800/481450]\n",
      "loss: 0.317864  [480000/481450]\n",
      "Train Accuracy: 90.8067%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.575540, F1-score: 87.64% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.226288  [    0/481450]\n",
      "loss: 0.113808  [ 3200/481450]\n",
      "loss: 0.166338  [ 6400/481450]\n",
      "loss: 0.385344  [ 9600/481450]\n",
      "loss: 0.403088  [12800/481450]\n",
      "loss: 0.214286  [16000/481450]\n",
      "loss: 0.024687  [19200/481450]\n",
      "loss: 0.136596  [22400/481450]\n",
      "loss: 0.035125  [25600/481450]\n",
      "loss: 0.156097  [28800/481450]\n",
      "loss: 0.154795  [32000/481450]\n",
      "loss: 0.213253  [35200/481450]\n",
      "loss: 0.278605  [38400/481450]\n",
      "loss: 0.282860  [41600/481450]\n",
      "loss: 0.103105  [44800/481450]\n",
      "loss: 0.162486  [48000/481450]\n",
      "loss: 0.202502  [51200/481450]\n",
      "loss: 0.232077  [54400/481450]\n",
      "loss: 0.170727  [57600/481450]\n",
      "loss: 0.126441  [60800/481450]\n",
      "loss: 0.095000  [64000/481450]\n",
      "loss: 0.138377  [67200/481450]\n",
      "loss: 0.370073  [70400/481450]\n",
      "loss: 0.115866  [73600/481450]\n",
      "loss: 0.302511  [76800/481450]\n",
      "loss: 0.244377  [80000/481450]\n",
      "loss: 0.041020  [83200/481450]\n",
      "loss: 0.181969  [86400/481450]\n",
      "loss: 0.475134  [89600/481450]\n",
      "loss: 0.247410  [92800/481450]\n",
      "loss: 0.220094  [96000/481450]\n",
      "loss: 0.115227  [99200/481450]\n",
      "loss: 0.252002  [102400/481450]\n",
      "loss: 0.230991  [105600/481450]\n",
      "loss: 0.257687  [108800/481450]\n",
      "loss: 0.193033  [112000/481450]\n",
      "loss: 0.149338  [115200/481450]\n",
      "loss: 0.103574  [118400/481450]\n",
      "loss: 0.339017  [121600/481450]\n",
      "loss: 0.239401  [124800/481450]\n",
      "loss: 0.192783  [128000/481450]\n",
      "loss: 0.297844  [131200/481450]\n",
      "loss: 0.357746  [134400/481450]\n",
      "loss: 0.084912  [137600/481450]\n",
      "loss: 0.166874  [140800/481450]\n",
      "loss: 0.272015  [144000/481450]\n",
      "loss: 0.282834  [147200/481450]\n",
      "loss: 0.167340  [150400/481450]\n",
      "loss: 0.421295  [153600/481450]\n",
      "loss: 0.222127  [156800/481450]\n",
      "loss: 0.226748  [160000/481450]\n",
      "loss: 0.175694  [163200/481450]\n",
      "loss: 0.170802  [166400/481450]\n",
      "loss: 0.272258  [169600/481450]\n",
      "loss: 0.396016  [172800/481450]\n",
      "loss: 0.078339  [176000/481450]\n",
      "loss: 0.422716  [179200/481450]\n",
      "loss: 0.266503  [182400/481450]\n",
      "loss: 0.363478  [185600/481450]\n",
      "loss: 0.189349  [188800/481450]\n",
      "loss: 0.398906  [192000/481450]\n",
      "loss: 0.098168  [195200/481450]\n",
      "loss: 0.101043  [198400/481450]\n",
      "loss: 0.245554  [201600/481450]\n",
      "loss: 0.167178  [204800/481450]\n",
      "loss: 0.103014  [208000/481450]\n",
      "loss: 0.215648  [211200/481450]\n",
      "loss: 0.400582  [214400/481450]\n",
      "loss: 0.242830  [217600/481450]\n",
      "loss: 0.278644  [220800/481450]\n",
      "loss: 0.234626  [224000/481450]\n",
      "loss: 0.160336  [227200/481450]\n",
      "loss: 0.183248  [230400/481450]\n",
      "loss: 0.141843  [233600/481450]\n",
      "loss: 0.192512  [236800/481450]\n",
      "loss: 0.128434  [240000/481450]\n",
      "loss: 0.200871  [243200/481450]\n",
      "loss: 0.137773  [246400/481450]\n",
      "loss: 0.368776  [249600/481450]\n",
      "loss: 0.297995  [252800/481450]\n",
      "loss: 0.336684  [256000/481450]\n",
      "loss: 0.126661  [259200/481450]\n",
      "loss: 0.256382  [262400/481450]\n",
      "loss: 0.117977  [265600/481450]\n",
      "loss: 0.144587  [268800/481450]\n",
      "loss: 0.161159  [272000/481450]\n",
      "loss: 0.159511  [275200/481450]\n",
      "loss: 0.141982  [278400/481450]\n",
      "loss: 0.210719  [281600/481450]\n",
      "loss: 0.445980  [284800/481450]\n",
      "loss: 0.674570  [288000/481450]\n",
      "loss: 0.139475  [291200/481450]\n",
      "loss: 0.571691  [294400/481450]\n",
      "loss: 0.100747  [297600/481450]\n",
      "loss: 0.102015  [300800/481450]\n",
      "loss: 0.161805  [304000/481450]\n",
      "loss: 0.280738  [307200/481450]\n",
      "loss: 0.500113  [310400/481450]\n",
      "loss: 0.381278  [313600/481450]\n",
      "loss: 0.241540  [316800/481450]\n",
      "loss: 0.276327  [320000/481450]\n",
      "loss: 0.146980  [323200/481450]\n",
      "loss: 0.077403  [326400/481450]\n",
      "loss: 0.209210  [329600/481450]\n",
      "loss: 0.201147  [332800/481450]\n",
      "loss: 0.370601  [336000/481450]\n",
      "loss: 0.177734  [339200/481450]\n",
      "loss: 0.195194  [342400/481450]\n",
      "loss: 0.354539  [345600/481450]\n",
      "loss: 0.284766  [348800/481450]\n",
      "loss: 0.143188  [352000/481450]\n",
      "loss: 0.246349  [355200/481450]\n",
      "loss: 0.460844  [358400/481450]\n",
      "loss: 0.346226  [361600/481450]\n",
      "loss: 0.606258  [364800/481450]\n",
      "loss: 0.011541  [368000/481450]\n",
      "loss: 0.250479  [371200/481450]\n",
      "loss: 0.278783  [374400/481450]\n",
      "loss: 0.414711  [377600/481450]\n",
      "loss: 0.361730  [380800/481450]\n",
      "loss: 0.199758  [384000/481450]\n",
      "loss: 0.373177  [387200/481450]\n",
      "loss: 0.212370  [390400/481450]\n",
      "loss: 0.230289  [393600/481450]\n",
      "loss: 0.248130  [396800/481450]\n",
      "loss: 0.096506  [400000/481450]\n",
      "loss: 0.115553  [403200/481450]\n",
      "loss: 0.161360  [406400/481450]\n",
      "loss: 0.314800  [409600/481450]\n",
      "loss: 0.085525  [412800/481450]\n",
      "loss: 0.214984  [416000/481450]\n",
      "loss: 0.352295  [419200/481450]\n",
      "loss: 0.275417  [422400/481450]\n",
      "loss: 0.153237  [425600/481450]\n",
      "loss: 0.343582  [428800/481450]\n",
      "loss: 0.207747  [432000/481450]\n",
      "loss: 0.267053  [435200/481450]\n",
      "loss: 0.303332  [438400/481450]\n",
      "loss: 0.266291  [441600/481450]\n",
      "loss: 0.124982  [444800/481450]\n",
      "loss: 0.201454  [448000/481450]\n",
      "loss: 0.394665  [451200/481450]\n",
      "loss: 0.282602  [454400/481450]\n",
      "loss: 0.181405  [457600/481450]\n",
      "loss: 0.238125  [460800/481450]\n",
      "loss: 0.337486  [464000/481450]\n",
      "loss: 0.075931  [467200/481450]\n",
      "loss: 0.109316  [470400/481450]\n",
      "loss: 0.307816  [473600/481450]\n",
      "loss: 0.404669  [476800/481450]\n",
      "loss: 0.056322  [480000/481450]\n",
      "Train Accuracy: 90.9989%\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.570937, F1-score: 87.81% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.248403  [    0/481450]\n",
      "loss: 0.093107  [ 3200/481450]\n",
      "loss: 0.165574  [ 6400/481450]\n",
      "loss: 0.241910  [ 9600/481450]\n",
      "loss: 0.304427  [12800/481450]\n",
      "loss: 0.167970  [16000/481450]\n",
      "loss: 0.396180  [19200/481450]\n",
      "loss: 0.309053  [22400/481450]\n",
      "loss: 0.217900  [25600/481450]\n",
      "loss: 0.258284  [28800/481450]\n",
      "loss: 0.254540  [32000/481450]\n",
      "loss: 0.070104  [35200/481450]\n",
      "loss: 0.232924  [38400/481450]\n",
      "loss: 0.168844  [41600/481450]\n",
      "loss: 0.297101  [44800/481450]\n",
      "loss: 0.076624  [48000/481450]\n",
      "loss: 0.082470  [51200/481450]\n",
      "loss: 0.169431  [54400/481450]\n",
      "loss: 0.097821  [57600/481450]\n",
      "loss: 0.123894  [60800/481450]\n",
      "loss: 0.193655  [64000/481450]\n",
      "loss: 0.126334  [67200/481450]\n",
      "loss: 0.109465  [70400/481450]\n",
      "loss: 0.319877  [73600/481450]\n",
      "loss: 0.175305  [76800/481450]\n",
      "loss: 0.199511  [80000/481450]\n",
      "loss: 0.222197  [83200/481450]\n",
      "loss: 0.171271  [86400/481450]\n",
      "loss: 0.186278  [89600/481450]\n",
      "loss: 0.188766  [92800/481450]\n",
      "loss: 0.253780  [96000/481450]\n",
      "loss: 0.123499  [99200/481450]\n",
      "loss: 0.209448  [102400/481450]\n",
      "loss: 0.213344  [105600/481450]\n",
      "loss: 0.249636  [108800/481450]\n",
      "loss: 0.224511  [112000/481450]\n",
      "loss: 0.228821  [115200/481450]\n",
      "loss: 0.270065  [118400/481450]\n",
      "loss: 0.341796  [121600/481450]\n",
      "loss: 0.377398  [124800/481450]\n",
      "loss: 0.206114  [128000/481450]\n",
      "loss: 0.433491  [131200/481450]\n",
      "loss: 0.194468  [134400/481450]\n",
      "loss: 0.149563  [137600/481450]\n",
      "loss: 0.166804  [140800/481450]\n",
      "loss: 0.335419  [144000/481450]\n",
      "loss: 0.198792  [147200/481450]\n",
      "loss: 0.200812  [150400/481450]\n",
      "loss: 0.103392  [153600/481450]\n",
      "loss: 0.102871  [156800/481450]\n",
      "loss: 0.153366  [160000/481450]\n",
      "loss: 0.139104  [163200/481450]\n",
      "loss: 0.351437  [166400/481450]\n",
      "loss: 0.114824  [169600/481450]\n",
      "loss: 0.094608  [172800/481450]\n",
      "loss: 0.251673  [176000/481450]\n",
      "loss: 0.233741  [179200/481450]\n",
      "loss: 0.305458  [182400/481450]\n",
      "loss: 0.293596  [185600/481450]\n",
      "loss: 0.187413  [188800/481450]\n",
      "loss: 0.215542  [192000/481450]\n",
      "loss: 0.119244  [195200/481450]\n",
      "loss: 0.329289  [198400/481450]\n",
      "loss: 0.079044  [201600/481450]\n",
      "loss: 0.266145  [204800/481450]\n",
      "loss: 0.218870  [208000/481450]\n",
      "loss: 0.102529  [211200/481450]\n",
      "loss: 0.057768  [214400/481450]\n",
      "loss: 0.329604  [217600/481450]\n",
      "loss: 0.195510  [220800/481450]\n",
      "loss: 0.254487  [224000/481450]\n",
      "loss: 0.214722  [227200/481450]\n",
      "loss: 0.070112  [230400/481450]\n",
      "loss: 0.165043  [233600/481450]\n",
      "loss: 0.213616  [236800/481450]\n",
      "loss: 0.116309  [240000/481450]\n",
      "loss: 0.252291  [243200/481450]\n",
      "loss: 0.086689  [246400/481450]\n",
      "loss: 0.160341  [249600/481450]\n",
      "loss: 0.172142  [252800/481450]\n",
      "loss: 0.192604  [256000/481450]\n",
      "loss: 0.395374  [259200/481450]\n",
      "loss: 0.155911  [262400/481450]\n",
      "loss: 0.210186  [265600/481450]\n",
      "loss: 0.180701  [268800/481450]\n",
      "loss: 0.223208  [272000/481450]\n",
      "loss: 0.136259  [275200/481450]\n",
      "loss: 0.088919  [278400/481450]\n",
      "loss: 0.238866  [281600/481450]\n",
      "loss: 0.066076  [284800/481450]\n",
      "loss: 0.204272  [288000/481450]\n",
      "loss: 0.169297  [291200/481450]\n",
      "loss: 0.256367  [294400/481450]\n",
      "loss: 0.378403  [297600/481450]\n",
      "loss: 0.367499  [300800/481450]\n",
      "loss: 0.259594  [304000/481450]\n",
      "loss: 0.191764  [307200/481450]\n",
      "loss: 0.404111  [310400/481450]\n",
      "loss: 0.104050  [313600/481450]\n",
      "loss: 0.281172  [316800/481450]\n",
      "loss: 0.152799  [320000/481450]\n",
      "loss: 0.322930  [323200/481450]\n",
      "loss: 0.343277  [326400/481450]\n",
      "loss: 0.399678  [329600/481450]\n",
      "loss: 0.262731  [332800/481450]\n",
      "loss: 0.163354  [336000/481450]\n",
      "loss: 0.200779  [339200/481450]\n",
      "loss: 0.138507  [342400/481450]\n",
      "loss: 0.191426  [345600/481450]\n",
      "loss: 0.053843  [348800/481450]\n",
      "loss: 0.208356  [352000/481450]\n",
      "loss: 0.530637  [355200/481450]\n",
      "loss: 0.088217  [358400/481450]\n",
      "loss: 0.274951  [361600/481450]\n",
      "loss: 0.236433  [364800/481450]\n",
      "loss: 0.184660  [368000/481450]\n",
      "loss: 0.347002  [371200/481450]\n",
      "loss: 0.128194  [374400/481450]\n",
      "loss: 0.056049  [377600/481450]\n",
      "loss: 0.192183  [380800/481450]\n",
      "loss: 0.102749  [384000/481450]\n",
      "loss: 0.158340  [387200/481450]\n",
      "loss: 0.629777  [390400/481450]\n",
      "loss: 0.372435  [393600/481450]\n",
      "loss: 0.255706  [396800/481450]\n",
      "loss: 0.118533  [400000/481450]\n",
      "loss: 0.169526  [403200/481450]\n",
      "loss: 0.212985  [406400/481450]\n",
      "loss: 0.274736  [409600/481450]\n",
      "loss: 0.196660  [412800/481450]\n",
      "loss: 0.290235  [416000/481450]\n",
      "loss: 0.094413  [419200/481450]\n",
      "loss: 0.088119  [422400/481450]\n",
      "loss: 0.414903  [425600/481450]\n",
      "loss: 0.276895  [428800/481450]\n",
      "loss: 0.418991  [432000/481450]\n",
      "loss: 0.159710  [435200/481450]\n",
      "loss: 0.098446  [438400/481450]\n",
      "loss: 0.232078  [441600/481450]\n",
      "loss: 0.232401  [444800/481450]\n",
      "loss: 0.267436  [448000/481450]\n",
      "loss: 0.083771  [451200/481450]\n",
      "loss: 0.164081  [454400/481450]\n",
      "loss: 0.161738  [457600/481450]\n",
      "loss: 0.369505  [460800/481450]\n",
      "loss: 0.055162  [464000/481450]\n",
      "loss: 0.546317  [467200/481450]\n",
      "loss: 0.390229  [470400/481450]\n",
      "loss: 0.310589  [473600/481450]\n",
      "loss: 0.122921  [476800/481450]\n",
      "loss: 0.119871  [480000/481450]\n",
      "Train Accuracy: 91.1096%\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.599008, F1-score: 87.10% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.288887  [    0/481450]\n",
      "loss: 0.212876  [ 3200/481450]\n",
      "loss: 0.112996  [ 6400/481450]\n",
      "loss: 0.132295  [ 9600/481450]\n",
      "loss: 0.339119  [12800/481450]\n",
      "loss: 0.203048  [16000/481450]\n",
      "loss: 0.373742  [19200/481450]\n",
      "loss: 0.133693  [22400/481450]\n",
      "loss: 0.099137  [25600/481450]\n",
      "loss: 0.235881  [28800/481450]\n",
      "loss: 0.163123  [32000/481450]\n",
      "loss: 0.095579  [35200/481450]\n",
      "loss: 0.142459  [38400/481450]\n",
      "loss: 0.240348  [41600/481450]\n",
      "loss: 0.227900  [44800/481450]\n",
      "loss: 0.192226  [48000/481450]\n",
      "loss: 0.050986  [51200/481450]\n",
      "loss: 0.399874  [54400/481450]\n",
      "loss: 0.111409  [57600/481450]\n",
      "loss: 0.390055  [60800/481450]\n",
      "loss: 0.155101  [64000/481450]\n",
      "loss: 0.083494  [67200/481450]\n",
      "loss: 0.338101  [70400/481450]\n",
      "loss: 0.251650  [73600/481450]\n",
      "loss: 0.153422  [76800/481450]\n",
      "loss: 0.222603  [80000/481450]\n",
      "loss: 0.284610  [83200/481450]\n",
      "loss: 0.152985  [86400/481450]\n",
      "loss: 0.258936  [89600/481450]\n",
      "loss: 0.248434  [92800/481450]\n",
      "loss: 0.084150  [96000/481450]\n",
      "loss: 0.166419  [99200/481450]\n",
      "loss: 0.188487  [102400/481450]\n",
      "loss: 0.134890  [105600/481450]\n",
      "loss: 0.370615  [108800/481450]\n",
      "loss: 0.482718  [112000/481450]\n",
      "loss: 0.184896  [115200/481450]\n",
      "loss: 0.133193  [118400/481450]\n",
      "loss: 0.221426  [121600/481450]\n",
      "loss: 0.164863  [124800/481450]\n",
      "loss: 0.084758  [128000/481450]\n",
      "loss: 0.283890  [131200/481450]\n",
      "loss: 0.259347  [134400/481450]\n",
      "loss: 0.191400  [137600/481450]\n",
      "loss: 0.169427  [140800/481450]\n",
      "loss: 0.062065  [144000/481450]\n",
      "loss: 0.468188  [147200/481450]\n",
      "loss: 0.120215  [150400/481450]\n",
      "loss: 0.052930  [153600/481450]\n",
      "loss: 0.231723  [156800/481450]\n",
      "loss: 0.198533  [160000/481450]\n",
      "loss: 0.184477  [163200/481450]\n",
      "loss: 0.222421  [166400/481450]\n",
      "loss: 0.175296  [169600/481450]\n",
      "loss: 0.162246  [172800/481450]\n",
      "loss: 0.144813  [176000/481450]\n",
      "loss: 0.554164  [179200/481450]\n",
      "loss: 0.283071  [182400/481450]\n",
      "loss: 0.210659  [185600/481450]\n",
      "loss: 0.334353  [188800/481450]\n",
      "loss: 0.102199  [192000/481450]\n",
      "loss: 0.163151  [195200/481450]\n",
      "loss: 0.265092  [198400/481450]\n",
      "loss: 0.272501  [201600/481450]\n",
      "loss: 0.221805  [204800/481450]\n",
      "loss: 0.112887  [208000/481450]\n",
      "loss: 0.226432  [211200/481450]\n",
      "loss: 0.113110  [214400/481450]\n",
      "loss: 0.269383  [217600/481450]\n",
      "loss: 0.311553  [220800/481450]\n",
      "loss: 0.303955  [224000/481450]\n",
      "loss: 0.333363  [227200/481450]\n",
      "loss: 0.090534  [230400/481450]\n",
      "loss: 0.144811  [233600/481450]\n",
      "loss: 0.431817  [236800/481450]\n",
      "loss: 0.236854  [240000/481450]\n",
      "loss: 0.562418  [243200/481450]\n",
      "loss: 0.117955  [246400/481450]\n",
      "loss: 0.118980  [249600/481450]\n",
      "loss: 0.153586  [252800/481450]\n",
      "loss: 0.288655  [256000/481450]\n",
      "loss: 0.342439  [259200/481450]\n",
      "loss: 0.244328  [262400/481450]\n",
      "loss: 0.299915  [265600/481450]\n",
      "loss: 0.230142  [268800/481450]\n",
      "loss: 0.142090  [272000/481450]\n",
      "loss: 0.273476  [275200/481450]\n",
      "loss: 0.190431  [278400/481450]\n",
      "loss: 0.327473  [281600/481450]\n",
      "loss: 0.082332  [284800/481450]\n",
      "loss: 0.280543  [288000/481450]\n",
      "loss: 0.219414  [291200/481450]\n",
      "loss: 0.231961  [294400/481450]\n",
      "loss: 0.056334  [297600/481450]\n",
      "loss: 0.229317  [300800/481450]\n",
      "loss: 0.196548  [304000/481450]\n",
      "loss: 0.162613  [307200/481450]\n",
      "loss: 0.433725  [310400/481450]\n",
      "loss: 0.077288  [313600/481450]\n",
      "loss: 0.202727  [316800/481450]\n",
      "loss: 0.189266  [320000/481450]\n",
      "loss: 0.263356  [323200/481450]\n",
      "loss: 0.181593  [326400/481450]\n",
      "loss: 0.243560  [329600/481450]\n",
      "loss: 0.232724  [332800/481450]\n",
      "loss: 0.283279  [336000/481450]\n",
      "loss: 0.181581  [339200/481450]\n",
      "loss: 0.236217  [342400/481450]\n",
      "loss: 0.059533  [345600/481450]\n",
      "loss: 0.047280  [348800/481450]\n",
      "loss: 0.112525  [352000/481450]\n",
      "loss: 0.244327  [355200/481450]\n",
      "loss: 0.189013  [358400/481450]\n",
      "loss: 0.098996  [361600/481450]\n",
      "loss: 0.108175  [364800/481450]\n",
      "loss: 0.660521  [368000/481450]\n",
      "loss: 0.120320  [371200/481450]\n",
      "loss: 0.222968  [374400/481450]\n",
      "loss: 0.212797  [377600/481450]\n",
      "loss: 0.176672  [380800/481450]\n",
      "loss: 0.107373  [384000/481450]\n",
      "loss: 0.496203  [387200/481450]\n",
      "loss: 0.192299  [390400/481450]\n",
      "loss: 0.179756  [393600/481450]\n",
      "loss: 0.263069  [396800/481450]\n",
      "loss: 0.422092  [400000/481450]\n",
      "loss: 0.181031  [403200/481450]\n",
      "loss: 0.160251  [406400/481450]\n",
      "loss: 0.051026  [409600/481450]\n",
      "loss: 0.205304  [412800/481450]\n",
      "loss: 0.276363  [416000/481450]\n",
      "loss: 0.134493  [419200/481450]\n",
      "loss: 0.308828  [422400/481450]\n",
      "loss: 0.215369  [425600/481450]\n",
      "loss: 0.201724  [428800/481450]\n",
      "loss: 0.101934  [432000/481450]\n",
      "loss: 0.175745  [435200/481450]\n",
      "loss: 0.168015  [438400/481450]\n",
      "loss: 0.151592  [441600/481450]\n",
      "loss: 0.210872  [444800/481450]\n",
      "loss: 0.130767  [448000/481450]\n",
      "loss: 0.236819  [451200/481450]\n",
      "loss: 0.446942  [454400/481450]\n",
      "loss: 0.137213  [457600/481450]\n",
      "loss: 0.297997  [460800/481450]\n",
      "loss: 0.470000  [464000/481450]\n",
      "loss: 0.141743  [467200/481450]\n",
      "loss: 0.263902  [470400/481450]\n",
      "loss: 0.257401  [473600/481450]\n",
      "loss: 0.216132  [476800/481450]\n",
      "loss: 0.210583  [480000/481450]\n",
      "Train Accuracy: 91.2346%\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.561929, F1-score: 87.64% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.272069  [    0/481450]\n",
      "loss: 0.134475  [ 3200/481450]\n",
      "loss: 0.187908  [ 6400/481450]\n",
      "loss: 0.168490  [ 9600/481450]\n",
      "loss: 0.200842  [12800/481450]\n",
      "loss: 0.363314  [16000/481450]\n",
      "loss: 0.026997  [19200/481450]\n",
      "loss: 0.161962  [22400/481450]\n",
      "loss: 0.088063  [25600/481450]\n",
      "loss: 0.149417  [28800/481450]\n",
      "loss: 0.233182  [32000/481450]\n",
      "loss: 0.119272  [35200/481450]\n",
      "loss: 0.262730  [38400/481450]\n",
      "loss: 0.130136  [41600/481450]\n",
      "loss: 0.234282  [44800/481450]\n",
      "loss: 0.178482  [48000/481450]\n",
      "loss: 0.189869  [51200/481450]\n",
      "loss: 0.251356  [54400/481450]\n",
      "loss: 0.175586  [57600/481450]\n",
      "loss: 0.251712  [60800/481450]\n",
      "loss: 0.200714  [64000/481450]\n",
      "loss: 0.159582  [67200/481450]\n",
      "loss: 0.158779  [70400/481450]\n",
      "loss: 0.231292  [73600/481450]\n",
      "loss: 0.323986  [76800/481450]\n",
      "loss: 0.132202  [80000/481450]\n",
      "loss: 0.183444  [83200/481450]\n",
      "loss: 0.282849  [86400/481450]\n",
      "loss: 0.298630  [89600/481450]\n",
      "loss: 0.078314  [92800/481450]\n",
      "loss: 0.136789  [96000/481450]\n",
      "loss: 0.252815  [99200/481450]\n",
      "loss: 0.123930  [102400/481450]\n",
      "loss: 0.205580  [105600/481450]\n",
      "loss: 0.093641  [108800/481450]\n",
      "loss: 0.092396  [112000/481450]\n",
      "loss: 0.458373  [115200/481450]\n",
      "loss: 0.205577  [118400/481450]\n",
      "loss: 0.241949  [121600/481450]\n",
      "loss: 0.360110  [124800/481450]\n",
      "loss: 0.238884  [128000/481450]\n",
      "loss: 0.122919  [131200/481450]\n",
      "loss: 0.256433  [134400/481450]\n",
      "loss: 0.058417  [137600/481450]\n",
      "loss: 0.166545  [140800/481450]\n",
      "loss: 0.196595  [144000/481450]\n",
      "loss: 0.158515  [147200/481450]\n",
      "loss: 0.198302  [150400/481450]\n",
      "loss: 0.155482  [153600/481450]\n",
      "loss: 0.124707  [156800/481450]\n",
      "loss: 0.108060  [160000/481450]\n",
      "loss: 0.211222  [163200/481450]\n",
      "loss: 0.187114  [166400/481450]\n",
      "loss: 0.249730  [169600/481450]\n",
      "loss: 0.119101  [172800/481450]\n",
      "loss: 0.134474  [176000/481450]\n",
      "loss: 0.224241  [179200/481450]\n",
      "loss: 0.041748  [182400/481450]\n",
      "loss: 0.167781  [185600/481450]\n",
      "loss: 0.333522  [188800/481450]\n",
      "loss: 0.070523  [192000/481450]\n",
      "loss: 0.164141  [195200/481450]\n",
      "loss: 0.042782  [198400/481450]\n",
      "loss: 0.201883  [201600/481450]\n",
      "loss: 0.111952  [204800/481450]\n",
      "loss: 0.165058  [208000/481450]\n",
      "loss: 0.272116  [211200/481450]\n",
      "loss: 0.587530  [214400/481450]\n",
      "loss: 0.192677  [217600/481450]\n",
      "loss: 0.258003  [220800/481450]\n",
      "loss: 0.140143  [224000/481450]\n",
      "loss: 0.318532  [227200/481450]\n",
      "loss: 0.023669  [230400/481450]\n",
      "loss: 0.109691  [233600/481450]\n",
      "loss: 0.168803  [236800/481450]\n",
      "loss: 0.105356  [240000/481450]\n",
      "loss: 0.215755  [243200/481450]\n",
      "loss: 0.129949  [246400/481450]\n",
      "loss: 0.093844  [249600/481450]\n",
      "loss: 0.073169  [252800/481450]\n",
      "loss: 0.349912  [256000/481450]\n",
      "loss: 0.368201  [259200/481450]\n",
      "loss: 0.157496  [262400/481450]\n",
      "loss: 0.257968  [265600/481450]\n",
      "loss: 0.331768  [268800/481450]\n",
      "loss: 0.133660  [272000/481450]\n",
      "loss: 0.161209  [275200/481450]\n",
      "loss: 0.362411  [278400/481450]\n",
      "loss: 0.178954  [281600/481450]\n",
      "loss: 0.187362  [284800/481450]\n",
      "loss: 0.304333  [288000/481450]\n",
      "loss: 0.185566  [291200/481450]\n",
      "loss: 0.346347  [294400/481450]\n",
      "loss: 0.212022  [297600/481450]\n",
      "loss: 0.179648  [300800/481450]\n",
      "loss: 0.195016  [304000/481450]\n",
      "loss: 0.297446  [307200/481450]\n",
      "loss: 0.242549  [310400/481450]\n",
      "loss: 0.152866  [313600/481450]\n",
      "loss: 0.154359  [316800/481450]\n",
      "loss: 0.324088  [320000/481450]\n",
      "loss: 0.065824  [323200/481450]\n",
      "loss: 0.247351  [326400/481450]\n",
      "loss: 0.097965  [329600/481450]\n",
      "loss: 0.181905  [332800/481450]\n",
      "loss: 0.168673  [336000/481450]\n",
      "loss: 0.379500  [339200/481450]\n",
      "loss: 0.051041  [342400/481450]\n",
      "loss: 0.206871  [345600/481450]\n",
      "loss: 0.050497  [348800/481450]\n",
      "loss: 0.069729  [352000/481450]\n",
      "loss: 0.155625  [355200/481450]\n",
      "loss: 0.108535  [358400/481450]\n",
      "loss: 0.044512  [361600/481450]\n",
      "loss: 0.141638  [364800/481450]\n",
      "loss: 0.258594  [368000/481450]\n",
      "loss: 0.324773  [371200/481450]\n",
      "loss: 0.155599  [374400/481450]\n",
      "loss: 0.226131  [377600/481450]\n",
      "loss: 0.181156  [380800/481450]\n",
      "loss: 0.357957  [384000/481450]\n",
      "loss: 0.121760  [387200/481450]\n",
      "loss: 0.463833  [390400/481450]\n",
      "loss: 0.121057  [393600/481450]\n",
      "loss: 0.200355  [396800/481450]\n",
      "loss: 0.137723  [400000/481450]\n",
      "loss: 0.101145  [403200/481450]\n",
      "loss: 0.119105  [406400/481450]\n",
      "loss: 0.150901  [409600/481450]\n",
      "loss: 0.165322  [412800/481450]\n",
      "loss: 0.073397  [416000/481450]\n",
      "loss: 0.163801  [419200/481450]\n",
      "loss: 0.103845  [422400/481450]\n",
      "loss: 0.159111  [425600/481450]\n",
      "loss: 0.186914  [428800/481450]\n",
      "loss: 0.174117  [432000/481450]\n",
      "loss: 0.403812  [435200/481450]\n",
      "loss: 0.157076  [438400/481450]\n",
      "loss: 0.064733  [441600/481450]\n",
      "loss: 0.201379  [444800/481450]\n",
      "loss: 0.164916  [448000/481450]\n",
      "loss: 0.057837  [451200/481450]\n",
      "loss: 0.227474  [454400/481450]\n",
      "loss: 0.414810  [457600/481450]\n",
      "loss: 0.366047  [460800/481450]\n",
      "loss: 0.139026  [464000/481450]\n",
      "loss: 0.209004  [467200/481450]\n",
      "loss: 0.164422  [470400/481450]\n",
      "loss: 0.192733  [473600/481450]\n",
      "loss: 0.144014  [476800/481450]\n",
      "loss: 0.036428  [480000/481450]\n",
      "Train Accuracy: 91.3470%\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.576606, F1-score: 87.33% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.292872  [    0/481450]\n",
      "loss: 0.188458  [ 3200/481450]\n",
      "loss: 0.242187  [ 6400/481450]\n",
      "loss: 0.042271  [ 9600/481450]\n",
      "loss: 0.194654  [12800/481450]\n",
      "loss: 0.168633  [16000/481450]\n",
      "loss: 0.216931  [19200/481450]\n",
      "loss: 0.240222  [22400/481450]\n",
      "loss: 0.133691  [25600/481450]\n",
      "loss: 0.120070  [28800/481450]\n",
      "loss: 0.283265  [32000/481450]\n",
      "loss: 0.236512  [35200/481450]\n",
      "loss: 0.364750  [38400/481450]\n",
      "loss: 0.202559  [41600/481450]\n",
      "loss: 0.172482  [44800/481450]\n",
      "loss: 0.197384  [48000/481450]\n",
      "loss: 0.094371  [51200/481450]\n",
      "loss: 0.344622  [54400/481450]\n",
      "loss: 0.212549  [57600/481450]\n",
      "loss: 0.139651  [60800/481450]\n",
      "loss: 0.111542  [64000/481450]\n",
      "loss: 0.067198  [67200/481450]\n",
      "loss: 0.230564  [70400/481450]\n",
      "loss: 0.243065  [73600/481450]\n",
      "loss: 0.264099  [76800/481450]\n",
      "loss: 0.364179  [80000/481450]\n",
      "loss: 0.163239  [83200/481450]\n",
      "loss: 0.159155  [86400/481450]\n",
      "loss: 0.294187  [89600/481450]\n",
      "loss: 0.177650  [92800/481450]\n",
      "loss: 0.328891  [96000/481450]\n",
      "loss: 0.234197  [99200/481450]\n",
      "loss: 0.289181  [102400/481450]\n",
      "loss: 0.145335  [105600/481450]\n",
      "loss: 0.032239  [108800/481450]\n",
      "loss: 0.132429  [112000/481450]\n",
      "loss: 0.317044  [115200/481450]\n",
      "loss: 0.328926  [118400/481450]\n",
      "loss: 0.216999  [121600/481450]\n",
      "loss: 0.219195  [124800/481450]\n",
      "loss: 0.216120  [128000/481450]\n",
      "loss: 0.137552  [131200/481450]\n",
      "loss: 0.206496  [134400/481450]\n",
      "loss: 0.244038  [137600/481450]\n",
      "loss: 0.113986  [140800/481450]\n",
      "loss: 0.170619  [144000/481450]\n",
      "loss: 0.134669  [147200/481450]\n",
      "loss: 0.343703  [150400/481450]\n",
      "loss: 0.204078  [153600/481450]\n",
      "loss: 0.390122  [156800/481450]\n",
      "loss: 0.310382  [160000/481450]\n",
      "loss: 0.294106  [163200/481450]\n",
      "loss: 0.380948  [166400/481450]\n",
      "loss: 0.153985  [169600/481450]\n",
      "loss: 0.096072  [172800/481450]\n",
      "loss: 0.156803  [176000/481450]\n",
      "loss: 0.253003  [179200/481450]\n",
      "loss: 0.224030  [182400/481450]\n",
      "loss: 0.284764  [185600/481450]\n",
      "loss: 0.102384  [188800/481450]\n",
      "loss: 0.086227  [192000/481450]\n",
      "loss: 0.164047  [195200/481450]\n",
      "loss: 0.576647  [198400/481450]\n",
      "loss: 0.173092  [201600/481450]\n",
      "loss: 0.289385  [204800/481450]\n",
      "loss: 0.106735  [208000/481450]\n",
      "loss: 0.175761  [211200/481450]\n",
      "loss: 0.107064  [214400/481450]\n",
      "loss: 0.329669  [217600/481450]\n",
      "loss: 0.162745  [220800/481450]\n",
      "loss: 0.285267  [224000/481450]\n",
      "loss: 0.048873  [227200/481450]\n",
      "loss: 0.210659  [230400/481450]\n",
      "loss: 0.071252  [233600/481450]\n",
      "loss: 0.283440  [236800/481450]\n",
      "loss: 0.203284  [240000/481450]\n",
      "loss: 0.178348  [243200/481450]\n",
      "loss: 0.119960  [246400/481450]\n",
      "loss: 0.148647  [249600/481450]\n",
      "loss: 0.315336  [252800/481450]\n",
      "loss: 0.176933  [256000/481450]\n",
      "loss: 0.147852  [259200/481450]\n",
      "loss: 0.200982  [262400/481450]\n",
      "loss: 0.013782  [265600/481450]\n",
      "loss: 0.260683  [268800/481450]\n",
      "loss: 0.078921  [272000/481450]\n",
      "loss: 0.064678  [275200/481450]\n",
      "loss: 0.132338  [278400/481450]\n",
      "loss: 0.247117  [281600/481450]\n",
      "loss: 0.074957  [284800/481450]\n",
      "loss: 0.213603  [288000/481450]\n",
      "loss: 0.140958  [291200/481450]\n",
      "loss: 0.253653  [294400/481450]\n",
      "loss: 0.108652  [297600/481450]\n",
      "loss: 0.148219  [300800/481450]\n",
      "loss: 0.286188  [304000/481450]\n",
      "loss: 0.225552  [307200/481450]\n",
      "loss: 0.132659  [310400/481450]\n",
      "loss: 0.272556  [313600/481450]\n",
      "loss: 0.118043  [316800/481450]\n",
      "loss: 0.230074  [320000/481450]\n",
      "loss: 0.183082  [323200/481450]\n",
      "loss: 0.127618  [326400/481450]\n",
      "loss: 0.149680  [329600/481450]\n",
      "loss: 0.013885  [332800/481450]\n",
      "loss: 0.201832  [336000/481450]\n",
      "loss: 0.241626  [339200/481450]\n",
      "loss: 0.132213  [342400/481450]\n",
      "loss: 0.038276  [345600/481450]\n",
      "loss: 0.286076  [348800/481450]\n",
      "loss: 0.298235  [352000/481450]\n",
      "loss: 0.118128  [355200/481450]\n",
      "loss: 0.146093  [358400/481450]\n",
      "loss: 0.174643  [361600/481450]\n",
      "loss: 0.106268  [364800/481450]\n",
      "loss: 0.129367  [368000/481450]\n",
      "loss: 0.305458  [371200/481450]\n",
      "loss: 0.204401  [374400/481450]\n",
      "loss: 0.082327  [377600/481450]\n",
      "loss: 0.190037  [380800/481450]\n",
      "loss: 0.077016  [384000/481450]\n",
      "loss: 0.212242  [387200/481450]\n",
      "loss: 0.107582  [390400/481450]\n",
      "loss: 0.361734  [393600/481450]\n",
      "loss: 0.095038  [396800/481450]\n",
      "loss: 0.337314  [400000/481450]\n",
      "loss: 0.069217  [403200/481450]\n",
      "loss: 0.233272  [406400/481450]\n",
      "loss: 0.247606  [409600/481450]\n",
      "loss: 0.140772  [412800/481450]\n",
      "loss: 0.299943  [416000/481450]\n",
      "loss: 0.107210  [419200/481450]\n",
      "loss: 0.110035  [422400/481450]\n",
      "loss: 0.311249  [425600/481450]\n",
      "loss: 0.156236  [428800/481450]\n",
      "loss: 0.129388  [432000/481450]\n",
      "loss: 0.377772  [435200/481450]\n",
      "loss: 0.105507  [438400/481450]\n",
      "loss: 0.197601  [441600/481450]\n",
      "loss: 0.393539  [444800/481450]\n",
      "loss: 0.202473  [448000/481450]\n",
      "loss: 0.260624  [451200/481450]\n",
      "loss: 0.389614  [454400/481450]\n",
      "loss: 0.314666  [457600/481450]\n",
      "loss: 0.100060  [460800/481450]\n",
      "loss: 0.105093  [464000/481450]\n",
      "loss: 0.078157  [467200/481450]\n",
      "loss: 0.143483  [470400/481450]\n",
      "loss: 0.091785  [473600/481450]\n",
      "loss: 0.193001  [476800/481450]\n",
      "loss: 0.193959  [480000/481450]\n",
      "Train Accuracy: 91.4244%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.601537, F1-score: 87.30% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.293852  [    0/481450]\n",
      "loss: 0.140145  [ 3200/481450]\n",
      "loss: 0.338805  [ 6400/481450]\n",
      "loss: 0.346190  [ 9600/481450]\n",
      "loss: 0.200652  [12800/481450]\n",
      "loss: 0.133985  [16000/481450]\n",
      "loss: 0.182211  [19200/481450]\n",
      "loss: 0.297239  [22400/481450]\n",
      "loss: 0.233935  [25600/481450]\n",
      "loss: 0.100926  [28800/481450]\n",
      "loss: 0.296997  [32000/481450]\n",
      "loss: 0.224021  [35200/481450]\n",
      "loss: 0.223484  [38400/481450]\n",
      "loss: 0.280052  [41600/481450]\n",
      "loss: 0.158745  [44800/481450]\n",
      "loss: 0.179346  [48000/481450]\n",
      "loss: 0.378757  [51200/481450]\n",
      "loss: 0.372829  [54400/481450]\n",
      "loss: 0.135202  [57600/481450]\n",
      "loss: 0.164658  [60800/481450]\n",
      "loss: 0.120805  [64000/481450]\n",
      "loss: 0.231280  [67200/481450]\n",
      "loss: 0.194715  [70400/481450]\n",
      "loss: 0.153476  [73600/481450]\n",
      "loss: 0.083133  [76800/481450]\n",
      "loss: 0.155785  [80000/481450]\n",
      "loss: 0.223236  [83200/481450]\n",
      "loss: 0.254190  [86400/481450]\n",
      "loss: 0.035368  [89600/481450]\n",
      "loss: 0.182514  [92800/481450]\n",
      "loss: 0.078676  [96000/481450]\n",
      "loss: 0.147350  [99200/481450]\n",
      "loss: 0.080668  [102400/481450]\n",
      "loss: 0.238862  [105600/481450]\n",
      "loss: 0.496540  [108800/481450]\n",
      "loss: 0.103488  [112000/481450]\n",
      "loss: 0.257350  [115200/481450]\n",
      "loss: 0.129760  [118400/481450]\n",
      "loss: 0.136764  [121600/481450]\n",
      "loss: 0.334034  [124800/481450]\n",
      "loss: 0.306695  [128000/481450]\n",
      "loss: 0.308579  [131200/481450]\n",
      "loss: 0.123456  [134400/481450]\n",
      "loss: 0.291586  [137600/481450]\n",
      "loss: 0.195290  [140800/481450]\n",
      "loss: 0.166599  [144000/481450]\n",
      "loss: 0.065715  [147200/481450]\n",
      "loss: 0.154556  [150400/481450]\n",
      "loss: 0.174486  [153600/481450]\n",
      "loss: 0.309006  [156800/481450]\n",
      "loss: 0.226020  [160000/481450]\n",
      "loss: 0.099349  [163200/481450]\n",
      "loss: 0.145871  [166400/481450]\n",
      "loss: 0.169761  [169600/481450]\n",
      "loss: 0.208148  [172800/481450]\n",
      "loss: 0.192269  [176000/481450]\n",
      "loss: 0.525598  [179200/481450]\n",
      "loss: 0.103518  [182400/481450]\n",
      "loss: 0.268483  [185600/481450]\n",
      "loss: 0.477904  [188800/481450]\n",
      "loss: 0.312029  [192000/481450]\n",
      "loss: 0.198815  [195200/481450]\n",
      "loss: 0.130438  [198400/481450]\n",
      "loss: 0.284000  [201600/481450]\n",
      "loss: 0.189185  [204800/481450]\n",
      "loss: 0.479137  [208000/481450]\n",
      "loss: 0.104480  [211200/481450]\n",
      "loss: 0.217888  [214400/481450]\n",
      "loss: 0.068480  [217600/481450]\n",
      "loss: 0.255133  [220800/481450]\n",
      "loss: 0.082090  [224000/481450]\n",
      "loss: 0.250062  [227200/481450]\n",
      "loss: 0.107683  [230400/481450]\n",
      "loss: 0.241433  [233600/481450]\n",
      "loss: 0.206217  [236800/481450]\n",
      "loss: 0.240499  [240000/481450]\n",
      "loss: 0.204232  [243200/481450]\n",
      "loss: 0.122913  [246400/481450]\n",
      "loss: 0.234735  [249600/481450]\n",
      "loss: 0.212584  [252800/481450]\n",
      "loss: 0.105122  [256000/481450]\n",
      "loss: 0.191109  [259200/481450]\n",
      "loss: 0.165366  [262400/481450]\n",
      "loss: 0.083238  [265600/481450]\n",
      "loss: 0.197832  [268800/481450]\n",
      "loss: 0.155268  [272000/481450]\n",
      "loss: 0.091083  [275200/481450]\n",
      "loss: 0.333946  [278400/481450]\n",
      "loss: 0.100532  [281600/481450]\n",
      "loss: 0.409436  [284800/481450]\n",
      "loss: 0.253065  [288000/481450]\n",
      "loss: 0.087352  [291200/481450]\n",
      "loss: 0.208431  [294400/481450]\n",
      "loss: 0.288316  [297600/481450]\n",
      "loss: 0.158058  [300800/481450]\n",
      "loss: 0.157977  [304000/481450]\n",
      "loss: 0.193853  [307200/481450]\n",
      "loss: 0.123617  [310400/481450]\n",
      "loss: 0.079272  [313600/481450]\n",
      "loss: 0.142205  [316800/481450]\n",
      "loss: 0.205977  [320000/481450]\n",
      "loss: 0.330177  [323200/481450]\n",
      "loss: 0.232332  [326400/481450]\n",
      "loss: 0.209838  [329600/481450]\n",
      "loss: 0.077678  [332800/481450]\n",
      "loss: 0.140148  [336000/481450]\n",
      "loss: 0.212220  [339200/481450]\n",
      "loss: 0.244509  [342400/481450]\n",
      "loss: 0.151635  [345600/481450]\n",
      "loss: 0.151774  [348800/481450]\n",
      "loss: 0.240836  [352000/481450]\n",
      "loss: 0.385093  [355200/481450]\n",
      "loss: 0.072556  [358400/481450]\n",
      "loss: 0.175327  [361600/481450]\n",
      "loss: 0.171345  [364800/481450]\n",
      "loss: 0.257373  [368000/481450]\n",
      "loss: 0.043542  [371200/481450]\n",
      "loss: 0.270181  [374400/481450]\n",
      "loss: 0.191341  [377600/481450]\n",
      "loss: 0.223152  [380800/481450]\n",
      "loss: 0.074661  [384000/481450]\n",
      "loss: 0.096822  [387200/481450]\n",
      "loss: 0.191838  [390400/481450]\n",
      "loss: 0.148737  [393600/481450]\n",
      "loss: 0.088559  [396800/481450]\n",
      "loss: 0.212745  [400000/481450]\n",
      "loss: 0.323554  [403200/481450]\n",
      "loss: 0.407631  [406400/481450]\n",
      "loss: 0.061950  [409600/481450]\n",
      "loss: 0.034184  [412800/481450]\n",
      "loss: 0.102531  [416000/481450]\n",
      "loss: 0.300922  [419200/481450]\n",
      "loss: 0.213182  [422400/481450]\n",
      "loss: 0.069836  [425600/481450]\n",
      "loss: 0.099199  [428800/481450]\n",
      "loss: 0.235002  [432000/481450]\n",
      "loss: 0.207293  [435200/481450]\n",
      "loss: 0.250413  [438400/481450]\n",
      "loss: 0.444315  [441600/481450]\n",
      "loss: 0.164359  [444800/481450]\n",
      "loss: 0.353796  [448000/481450]\n",
      "loss: 0.110196  [451200/481450]\n",
      "loss: 0.143503  [454400/481450]\n",
      "loss: 0.105295  [457600/481450]\n",
      "loss: 0.292640  [460800/481450]\n",
      "loss: 0.254417  [464000/481450]\n",
      "loss: 0.160041  [467200/481450]\n",
      "loss: 0.197808  [470400/481450]\n",
      "loss: 0.182284  [473600/481450]\n",
      "loss: 0.359994  [476800/481450]\n",
      "loss: 0.310075  [480000/481450]\n",
      "Train Accuracy: 91.5225%\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.690509, F1-score: 86.63% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.019843  [    0/481450]\n",
      "loss: 0.091699  [ 3200/481450]\n",
      "loss: 0.334064  [ 6400/481450]\n",
      "loss: 0.151981  [ 9600/481450]\n",
      "loss: 0.119755  [12800/481450]\n",
      "loss: 0.130635  [16000/481450]\n",
      "loss: 0.352371  [19200/481450]\n",
      "loss: 0.150827  [22400/481450]\n",
      "loss: 0.138937  [25600/481450]\n",
      "loss: 0.228565  [28800/481450]\n",
      "loss: 0.305333  [32000/481450]\n",
      "loss: 0.188663  [35200/481450]\n",
      "loss: 0.073065  [38400/481450]\n",
      "loss: 0.036045  [41600/481450]\n",
      "loss: 0.044202  [44800/481450]\n",
      "loss: 0.058292  [48000/481450]\n",
      "loss: 0.198426  [51200/481450]\n",
      "loss: 0.054154  [54400/481450]\n",
      "loss: 0.290930  [57600/481450]\n",
      "loss: 0.316506  [60800/481450]\n",
      "loss: 0.180975  [64000/481450]\n",
      "loss: 0.213443  [67200/481450]\n",
      "loss: 0.151495  [70400/481450]\n",
      "loss: 0.113272  [73600/481450]\n",
      "loss: 0.234522  [76800/481450]\n",
      "loss: 0.186086  [80000/481450]\n",
      "loss: 0.469977  [83200/481450]\n",
      "loss: 0.235719  [86400/481450]\n",
      "loss: 0.209004  [89600/481450]\n",
      "loss: 0.305321  [92800/481450]\n",
      "loss: 0.316962  [96000/481450]\n",
      "loss: 0.389251  [99200/481450]\n",
      "loss: 0.220825  [102400/481450]\n",
      "loss: 0.234050  [105600/481450]\n",
      "loss: 0.226694  [108800/481450]\n",
      "loss: 0.378516  [112000/481450]\n",
      "loss: 0.235387  [115200/481450]\n",
      "loss: 0.295964  [118400/481450]\n",
      "loss: 0.152503  [121600/481450]\n",
      "loss: 0.156231  [124800/481450]\n",
      "loss: 0.470513  [128000/481450]\n",
      "loss: 0.245164  [131200/481450]\n",
      "loss: 0.105744  [134400/481450]\n",
      "loss: 0.175230  [137600/481450]\n",
      "loss: 0.231375  [140800/481450]\n",
      "loss: 0.259438  [144000/481450]\n",
      "loss: 0.277102  [147200/481450]\n",
      "loss: 0.357066  [150400/481450]\n",
      "loss: 0.244166  [153600/481450]\n",
      "loss: 0.192282  [156800/481450]\n",
      "loss: 0.130993  [160000/481450]\n",
      "loss: 0.177858  [163200/481450]\n",
      "loss: 0.345773  [166400/481450]\n",
      "loss: 0.292023  [169600/481450]\n",
      "loss: 0.267983  [172800/481450]\n",
      "loss: 0.110088  [176000/481450]\n",
      "loss: 0.173080  [179200/481450]\n",
      "loss: 0.130211  [182400/481450]\n",
      "loss: 0.071846  [185600/481450]\n",
      "loss: 0.396844  [188800/481450]\n",
      "loss: 0.171384  [192000/481450]\n",
      "loss: 0.191743  [195200/481450]\n",
      "loss: 0.219783  [198400/481450]\n",
      "loss: 0.114111  [201600/481450]\n",
      "loss: 0.167881  [204800/481450]\n",
      "loss: 0.135037  [208000/481450]\n",
      "loss: 0.193742  [211200/481450]\n",
      "loss: 0.196251  [214400/481450]\n",
      "loss: 0.398506  [217600/481450]\n",
      "loss: 0.267323  [220800/481450]\n",
      "loss: 0.164218  [224000/481450]\n",
      "loss: 0.057629  [227200/481450]\n",
      "loss: 0.065881  [230400/481450]\n",
      "loss: 0.118746  [233600/481450]\n",
      "loss: 0.114694  [236800/481450]\n",
      "loss: 0.274695  [240000/481450]\n",
      "loss: 0.088339  [243200/481450]\n",
      "loss: 0.186252  [246400/481450]\n",
      "loss: 0.340437  [249600/481450]\n",
      "loss: 0.166172  [252800/481450]\n",
      "loss: 0.020099  [256000/481450]\n",
      "loss: 0.107344  [259200/481450]\n",
      "loss: 0.139782  [262400/481450]\n",
      "loss: 0.265492  [265600/481450]\n",
      "loss: 0.304966  [268800/481450]\n",
      "loss: 0.159357  [272000/481450]\n",
      "loss: 0.301496  [275200/481450]\n",
      "loss: 0.213824  [278400/481450]\n",
      "loss: 0.092622  [281600/481450]\n",
      "loss: 0.164826  [284800/481450]\n",
      "loss: 0.254954  [288000/481450]\n",
      "loss: 0.110353  [291200/481450]\n",
      "loss: 0.217034  [294400/481450]\n",
      "loss: 0.118823  [297600/481450]\n",
      "loss: 0.329982  [300800/481450]\n",
      "loss: 0.162979  [304000/481450]\n",
      "loss: 0.159083  [307200/481450]\n",
      "loss: 0.254024  [310400/481450]\n",
      "loss: 0.296924  [313600/481450]\n",
      "loss: 0.135600  [316800/481450]\n",
      "loss: 0.069635  [320000/481450]\n",
      "loss: 0.157145  [323200/481450]\n",
      "loss: 0.281387  [326400/481450]\n",
      "loss: 0.121641  [329600/481450]\n",
      "loss: 0.265109  [332800/481450]\n",
      "loss: 0.522636  [336000/481450]\n",
      "loss: 0.174307  [339200/481450]\n",
      "loss: 0.409139  [342400/481450]\n",
      "loss: 0.191284  [345600/481450]\n",
      "loss: 0.058593  [348800/481450]\n",
      "loss: 0.107798  [352000/481450]\n",
      "loss: 0.194370  [355200/481450]\n",
      "loss: 0.200887  [358400/481450]\n",
      "loss: 0.178726  [361600/481450]\n",
      "loss: 0.095830  [364800/481450]\n",
      "loss: 0.262110  [368000/481450]\n",
      "loss: 0.387504  [371200/481450]\n",
      "loss: 0.122650  [374400/481450]\n",
      "loss: 0.219505  [377600/481450]\n",
      "loss: 0.155869  [380800/481450]\n",
      "loss: 0.147170  [384000/481450]\n",
      "loss: 0.311747  [387200/481450]\n",
      "loss: 0.227913  [390400/481450]\n",
      "loss: 0.216800  [393600/481450]\n",
      "loss: 0.243844  [396800/481450]\n",
      "loss: 0.160015  [400000/481450]\n",
      "loss: 0.339557  [403200/481450]\n",
      "loss: 0.233783  [406400/481450]\n",
      "loss: 0.216685  [409600/481450]\n",
      "loss: 0.100560  [412800/481450]\n",
      "loss: 0.132891  [416000/481450]\n",
      "loss: 0.173283  [419200/481450]\n",
      "loss: 0.121102  [422400/481450]\n",
      "loss: 0.298985  [425600/481450]\n",
      "loss: 0.073788  [428800/481450]\n",
      "loss: 0.337812  [432000/481450]\n",
      "loss: 0.146777  [435200/481450]\n",
      "loss: 0.176287  [438400/481450]\n",
      "loss: 0.087019  [441600/481450]\n",
      "loss: 0.465211  [444800/481450]\n",
      "loss: 0.169889  [448000/481450]\n",
      "loss: 0.029444  [451200/481450]\n",
      "loss: 0.156445  [454400/481450]\n",
      "loss: 0.043826  [457600/481450]\n",
      "loss: 0.016972  [460800/481450]\n",
      "loss: 0.140068  [464000/481450]\n",
      "loss: 0.155015  [467200/481450]\n",
      "loss: 0.100514  [470400/481450]\n",
      "loss: 0.236709  [473600/481450]\n",
      "loss: 0.244277  [476800/481450]\n",
      "loss: 0.188534  [480000/481450]\n",
      "Train Accuracy: 91.5929%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.630268, F1-score: 87.33% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Smooth and plot training loss for each fold\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, losses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mall_losses\u001b[49m):\n\u001b[0;32m     17\u001b[0m     smoothed_loss \u001b[38;5;241m=\u001b[39m smooth_loss(losses, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Adjust window_size as needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(smoothed_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
