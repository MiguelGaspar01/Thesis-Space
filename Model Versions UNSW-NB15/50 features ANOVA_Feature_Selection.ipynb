{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb2e4ac-d59e-4e27-9f18-e9f0aa674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y_binary = train_labels_encoded[\"label\"]\n",
    "test_data_y_binary = test_labels_encoded[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b95a23d-e0ba-48a9-8e8f-a76411be9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = test_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c14778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175341, 56)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['encoder__proto_arp', 'encoder__proto_ospf', 'encoder__proto_tcp', 'encoder__proto_udp', 'encoder__proto_unas', 'encoder__service_dns', 'encoder__service_ftp-data', 'encoder__state_CON', 'encoder__state_FIN', 'encoder__state_INT', 'remainder__dur', 'remainder__spkts', 'remainder__dpkts', 'remainder__sbytes', 'remainder__dbytes', 'remainder__rate', 'remainder__sttl', 'remainder__dttl', 'remainder__sload', 'remainder__dload', 'remainder__sloss', 'remainder__dloss', 'remainder__sinpkt', 'remainder__dinpkt', 'remainder__sjit', 'remainder__djit', 'remainder__swin', 'remainder__stcpb', 'remainder__dtcpb', 'remainder__dwin', 'remainder__tcprtt', 'remainder__synack', 'remainder__ackdat', 'remainder__dmean', 'remainder__ct_state_ttl', 'remainder__ct_dst_ltm', 'remainder__ct_src_dport_ltm', 'remainder__ct_dst_sport_ltm', 'remainder__ct_srv_dst', 'remainder__is_sm_ips_ports']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the top 40 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=45)\n",
    "X_selected = selector.fit_transform(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = train_data_X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_data_y.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_over_train = []\n",
    "loss_over_test = []\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    mean_loss = epoch_loss / len(dataloader) \n",
    "    loss_over_train.append(mean_loss)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    mean_loss = test_loss / num_batches  # Divide by the number of batches\n",
    "    loss_over_test.append(mean_loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9449d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "Epoch 1: Train Acc=66.52%, Val Acc=73.18%, F1=67.95%, Macro F1=29.75%\n",
      "Epoch 2: Train Acc=74.72%, Val Acc=75.57%, F1=72.52%, Macro F1=35.35%\n",
      "Epoch 3: Train Acc=75.93%, Val Acc=77.09%, F1=74.26%, Macro F1=37.69%\n",
      "Epoch 4: Train Acc=77.10%, Val Acc=76.99%, F1=74.40%, Macro F1=37.91%\n",
      "Epoch 5: Train Acc=77.28%, Val Acc=77.13%, F1=74.43%, Macro F1=37.92%\n",
      "Epoch 6: Train Acc=77.54%, Val Acc=77.94%, F1=75.02%, Macro F1=38.50%\n",
      "Epoch 7: Train Acc=77.79%, Val Acc=77.84%, F1=75.01%, Macro F1=38.63%\n",
      "Epoch 8: Train Acc=78.00%, Val Acc=78.17%, F1=75.20%, Macro F1=38.73%\n",
      "Epoch 9: Train Acc=78.21%, Val Acc=78.40%, F1=75.33%, Macro F1=38.83%\n",
      "Epoch 10: Train Acc=78.24%, Val Acc=78.31%, F1=75.41%, Macro F1=39.04%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "epochs = 200  # Number of training epochs per fold\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define loss function and learning parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Combine datasets (Only Train Data)\n",
    "dataset = train_dataset  # Use only training dataset\n",
    "\n",
    "# Store results across folds\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_losses = []\n",
    "\n",
    "# K-Fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\\n\" + \"-\" * 30)\n",
    "    print(\"initializing model...\")\n",
    "    model = KAN([45, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best\n",
    "    # Create data subsets for the current fold\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Re-initialize model and optimizer for each fold\n",
    "     # Replace with your model class\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training function\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        return correct / total * 100, epoch_loss / len(dataloader)\n",
    "\n",
    "    # Validation function\n",
    "    def val_loop(dataloader, model, loss_fn):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "                all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracy = correct / len(dataloader.dataset) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "        mean_loss = val_loss / len(dataloader)\n",
    "\n",
    "        return accuracy, f1, f1_macro, mean_loss\n",
    "\n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_acc, f1, f1_macro, val_loss = val_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%, F1={f1:.2f}%, Macro F1={f1_macro:.2f}%\")\n",
    "\n",
    "    # Store fold results\n",
    "    fold_accuracies.append(val_acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_f1_macro_scores.append(f1_macro)\n",
    "    fold_losses.append(val_loss)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Avg Validation Accuracy: {sum(fold_accuracies) / k_folds:.2f}%\")\n",
    "print(f\"Avg Weighted F1-score: {sum(fold_f1_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Macro F1-score: {sum(fold_f1_macro_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Validation Loss: {sum(fold_losses) / k_folds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398cd152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.260435  [    0/175341]\n",
      "loss: 2.178968  [ 1600/175341]\n",
      "loss: 2.091363  [ 3200/175341]\n",
      "loss: 1.759666  [ 4800/175341]\n",
      "loss: 2.150178  [ 6400/175341]\n",
      "loss: 1.818884  [ 8000/175341]\n",
      "loss: 1.956686  [ 9600/175341]\n",
      "loss: 1.439500  [11200/175341]\n",
      "loss: 1.694268  [12800/175341]\n",
      "loss: 1.211658  [14400/175341]\n",
      "loss: 1.299304  [16000/175341]\n",
      "loss: 1.116312  [17600/175341]\n",
      "loss: 1.090212  [19200/175341]\n",
      "loss: 1.289851  [20800/175341]\n",
      "loss: 1.368616  [22400/175341]\n",
      "loss: 0.811075  [24000/175341]\n",
      "loss: 0.847782  [25600/175341]\n",
      "loss: 0.809049  [27200/175341]\n",
      "loss: 0.681475  [28800/175341]\n",
      "loss: 1.096894  [30400/175341]\n",
      "loss: 1.002019  [32000/175341]\n",
      "loss: 1.114056  [33600/175341]\n",
      "loss: 0.635750  [35200/175341]\n",
      "loss: 0.955600  [36800/175341]\n",
      "loss: 0.825421  [38400/175341]\n",
      "loss: 0.653893  [40000/175341]\n",
      "loss: 0.807320  [41600/175341]\n",
      "loss: 0.788785  [43200/175341]\n",
      "loss: 0.911538  [44800/175341]\n",
      "loss: 0.951580  [46400/175341]\n",
      "loss: 0.479551  [48000/175341]\n",
      "loss: 0.874507  [49600/175341]\n",
      "loss: 1.082126  [51200/175341]\n",
      "loss: 0.850718  [52800/175341]\n",
      "loss: 0.860953  [54400/175341]\n",
      "loss: 0.709920  [56000/175341]\n",
      "loss: 0.582725  [57600/175341]\n",
      "loss: 0.619450  [59200/175341]\n",
      "loss: 0.445798  [60800/175341]\n",
      "loss: 0.892685  [62400/175341]\n",
      "loss: 1.159057  [64000/175341]\n",
      "loss: 1.058356  [65600/175341]\n",
      "loss: 0.517562  [67200/175341]\n",
      "loss: 0.752924  [68800/175341]\n",
      "loss: 1.302872  [70400/175341]\n",
      "loss: 0.434889  [72000/175341]\n",
      "loss: 0.679462  [73600/175341]\n",
      "loss: 0.862947  [75200/175341]\n",
      "loss: 0.653610  [76800/175341]\n",
      "loss: 0.798905  [78400/175341]\n",
      "loss: 0.881251  [80000/175341]\n",
      "loss: 0.544090  [81600/175341]\n",
      "loss: 0.631564  [83200/175341]\n",
      "loss: 0.928196  [84800/175341]\n",
      "loss: 0.441612  [86400/175341]\n",
      "loss: 0.912364  [88000/175341]\n",
      "loss: 0.958065  [89600/175341]\n",
      "loss: 0.644614  [91200/175341]\n",
      "loss: 0.851593  [92800/175341]\n",
      "loss: 0.554810  [94400/175341]\n",
      "loss: 0.943238  [96000/175341]\n",
      "loss: 0.680692  [97600/175341]\n",
      "loss: 0.533448  [99200/175341]\n",
      "loss: 1.058082  [100800/175341]\n",
      "loss: 0.779170  [102400/175341]\n",
      "loss: 0.738956  [104000/175341]\n",
      "loss: 1.199663  [105600/175341]\n",
      "loss: 0.766741  [107200/175341]\n",
      "loss: 0.629659  [108800/175341]\n",
      "loss: 0.615182  [110400/175341]\n",
      "loss: 1.012738  [112000/175341]\n",
      "loss: 0.466325  [113600/175341]\n",
      "loss: 0.763039  [115200/175341]\n",
      "loss: 0.836740  [116800/175341]\n",
      "loss: 0.714050  [118400/175341]\n",
      "loss: 0.535774  [120000/175341]\n",
      "loss: 0.976282  [121600/175341]\n",
      "loss: 1.064292  [123200/175341]\n",
      "loss: 0.226624  [124800/175341]\n",
      "loss: 0.497684  [126400/175341]\n",
      "loss: 0.820052  [128000/175341]\n",
      "loss: 0.549488  [129600/175341]\n",
      "loss: 1.150566  [131200/175341]\n",
      "loss: 0.383703  [132800/175341]\n",
      "loss: 0.523202  [134400/175341]\n",
      "loss: 0.809549  [136000/175341]\n",
      "loss: 0.930346  [137600/175341]\n",
      "loss: 1.009032  [139200/175341]\n",
      "loss: 0.458623  [140800/175341]\n",
      "loss: 0.465446  [142400/175341]\n",
      "loss: 0.711373  [144000/175341]\n",
      "loss: 0.504184  [145600/175341]\n",
      "loss: 0.548046  [147200/175341]\n",
      "loss: 0.450272  [148800/175341]\n",
      "loss: 0.775995  [150400/175341]\n",
      "loss: 0.607252  [152000/175341]\n",
      "loss: 0.520809  [153600/175341]\n",
      "loss: 0.792475  [155200/175341]\n",
      "loss: 0.628938  [156800/175341]\n",
      "loss: 0.400355  [158400/175341]\n",
      "loss: 1.224654  [160000/175341]\n",
      "loss: 0.409976  [161600/175341]\n",
      "loss: 0.841460  [163200/175341]\n",
      "loss: 0.771836  [164800/175341]\n",
      "loss: 0.850555  [166400/175341]\n",
      "loss: 0.802739  [168000/175341]\n",
      "loss: 1.008603  [169600/175341]\n",
      "loss: 0.638848  [171200/175341]\n",
      "loss: 0.829978  [172800/175341]\n",
      "loss: 0.740784  [174400/175341]\n",
      "Train Accuracy: 70.5825%\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.712270, F1-score: 69.82%, Macro_F1-Score:  30.54%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.698796  [    0/175341]\n",
      "loss: 0.796067  [ 1600/175341]\n",
      "loss: 0.372216  [ 3200/175341]\n",
      "loss: 0.323333  [ 4800/175341]\n",
      "loss: 0.901284  [ 6400/175341]\n",
      "loss: 0.411641  [ 8000/175341]\n",
      "loss: 0.659507  [ 9600/175341]\n",
      "loss: 1.063277  [11200/175341]\n",
      "loss: 0.569575  [12800/175341]\n",
      "loss: 0.519908  [14400/175341]\n",
      "loss: 1.530105  [16000/175341]\n",
      "loss: 1.132342  [17600/175341]\n",
      "loss: 0.552334  [19200/175341]\n",
      "loss: 0.561811  [20800/175341]\n",
      "loss: 0.747198  [22400/175341]\n",
      "loss: 0.824228  [24000/175341]\n",
      "loss: 0.528535  [25600/175341]\n",
      "loss: 0.514317  [27200/175341]\n",
      "loss: 1.082862  [28800/175341]\n",
      "loss: 0.800389  [30400/175341]\n",
      "loss: 0.605506  [32000/175341]\n",
      "loss: 1.091300  [33600/175341]\n",
      "loss: 0.947693  [35200/175341]\n",
      "loss: 0.595582  [36800/175341]\n",
      "loss: 0.722366  [38400/175341]\n",
      "loss: 0.327584  [40000/175341]\n",
      "loss: 0.257604  [41600/175341]\n",
      "loss: 0.550908  [43200/175341]\n",
      "loss: 1.040637  [44800/175341]\n",
      "loss: 0.447757  [46400/175341]\n",
      "loss: 1.046841  [48000/175341]\n",
      "loss: 0.501860  [49600/175341]\n",
      "loss: 0.498579  [51200/175341]\n",
      "loss: 1.046824  [52800/175341]\n",
      "loss: 1.485288  [54400/175341]\n",
      "loss: 0.991312  [56000/175341]\n",
      "loss: 0.649235  [57600/175341]\n",
      "loss: 0.446069  [59200/175341]\n",
      "loss: 0.870075  [60800/175341]\n",
      "loss: 0.915609  [62400/175341]\n",
      "loss: 0.428242  [64000/175341]\n",
      "loss: 0.893359  [65600/175341]\n",
      "loss: 0.344645  [67200/175341]\n",
      "loss: 0.497033  [68800/175341]\n",
      "loss: 0.448051  [70400/175341]\n",
      "loss: 0.890963  [72000/175341]\n",
      "loss: 0.620716  [73600/175341]\n",
      "loss: 0.962063  [75200/175341]\n",
      "loss: 0.703357  [76800/175341]\n",
      "loss: 0.533429  [78400/175341]\n",
      "loss: 0.565264  [80000/175341]\n",
      "loss: 0.273412  [81600/175341]\n",
      "loss: 0.684869  [83200/175341]\n",
      "loss: 0.511320  [84800/175341]\n",
      "loss: 0.839228  [86400/175341]\n",
      "loss: 0.551595  [88000/175341]\n",
      "loss: 1.006779  [89600/175341]\n",
      "loss: 0.556155  [91200/175341]\n",
      "loss: 0.725453  [92800/175341]\n",
      "loss: 0.818291  [94400/175341]\n",
      "loss: 0.919694  [96000/175341]\n",
      "loss: 0.560111  [97600/175341]\n",
      "loss: 0.582182  [99200/175341]\n",
      "loss: 0.467627  [100800/175341]\n",
      "loss: 0.711579  [102400/175341]\n",
      "loss: 0.535869  [104000/175341]\n",
      "loss: 0.985255  [105600/175341]\n",
      "loss: 1.080809  [107200/175341]\n",
      "loss: 0.785012  [108800/175341]\n",
      "loss: 0.598300  [110400/175341]\n",
      "loss: 0.467317  [112000/175341]\n",
      "loss: 0.867014  [113600/175341]\n",
      "loss: 0.879234  [115200/175341]\n",
      "loss: 0.321741  [116800/175341]\n",
      "loss: 1.005238  [118400/175341]\n",
      "loss: 0.821104  [120000/175341]\n",
      "loss: 1.132864  [121600/175341]\n",
      "loss: 0.502539  [123200/175341]\n",
      "loss: 0.216731  [124800/175341]\n",
      "loss: 0.863703  [126400/175341]\n",
      "loss: 0.784162  [128000/175341]\n",
      "loss: 0.277725  [129600/175341]\n",
      "loss: 0.656141  [131200/175341]\n",
      "loss: 0.595710  [132800/175341]\n",
      "loss: 1.079365  [134400/175341]\n",
      "loss: 1.094277  [136000/175341]\n",
      "loss: 0.783175  [137600/175341]\n",
      "loss: 0.788679  [139200/175341]\n",
      "loss: 0.517549  [140800/175341]\n",
      "loss: 0.620772  [142400/175341]\n",
      "loss: 0.272362  [144000/175341]\n",
      "loss: 0.516885  [145600/175341]\n",
      "loss: 0.348845  [147200/175341]\n",
      "loss: 0.800889  [148800/175341]\n",
      "loss: 0.365128  [150400/175341]\n",
      "loss: 0.939442  [152000/175341]\n",
      "loss: 0.738949  [153600/175341]\n",
      "loss: 1.072061  [155200/175341]\n",
      "loss: 0.355872  [156800/175341]\n",
      "loss: 1.519530  [158400/175341]\n",
      "loss: 0.554566  [160000/175341]\n",
      "loss: 0.822467  [161600/175341]\n",
      "loss: 0.864055  [163200/175341]\n",
      "loss: 0.566050  [164800/175341]\n",
      "loss: 0.314730  [166400/175341]\n",
      "loss: 0.617277  [168000/175341]\n",
      "loss: 0.828777  [169600/175341]\n",
      "loss: 0.390905  [171200/175341]\n",
      "loss: 0.277392  [172800/175341]\n",
      "loss: 0.655212  [174400/175341]\n",
      "Train Accuracy: 76.3398%\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.679106, F1-score: 70.25%, Macro_F1-Score:  32.74%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.730572  [    0/175341]\n",
      "loss: 0.682257  [ 1600/175341]\n",
      "loss: 0.471496  [ 3200/175341]\n",
      "loss: 0.814763  [ 4800/175341]\n",
      "loss: 0.251420  [ 6400/175341]\n",
      "loss: 0.639719  [ 8000/175341]\n",
      "loss: 0.986776  [ 9600/175341]\n",
      "loss: 0.673529  [11200/175341]\n",
      "loss: 0.784391  [12800/175341]\n",
      "loss: 0.543975  [14400/175341]\n",
      "loss: 0.395347  [16000/175341]\n",
      "loss: 0.349667  [17600/175341]\n",
      "loss: 0.624560  [19200/175341]\n",
      "loss: 0.600989  [20800/175341]\n",
      "loss: 0.672443  [22400/175341]\n",
      "loss: 0.580489  [24000/175341]\n",
      "loss: 0.750828  [25600/175341]\n",
      "loss: 0.696807  [27200/175341]\n",
      "loss: 0.652015  [28800/175341]\n",
      "loss: 0.620920  [30400/175341]\n",
      "loss: 0.796538  [32000/175341]\n",
      "loss: 0.518307  [33600/175341]\n",
      "loss: 1.223594  [35200/175341]\n",
      "loss: 0.386568  [36800/175341]\n",
      "loss: 0.208566  [38400/175341]\n",
      "loss: 0.558831  [40000/175341]\n",
      "loss: 1.223478  [41600/175341]\n",
      "loss: 0.521042  [43200/175341]\n",
      "loss: 0.796032  [44800/175341]\n",
      "loss: 0.702335  [46400/175341]\n",
      "loss: 0.489447  [48000/175341]\n",
      "loss: 0.284546  [49600/175341]\n",
      "loss: 0.457081  [51200/175341]\n",
      "loss: 0.548636  [52800/175341]\n",
      "loss: 0.805087  [54400/175341]\n",
      "loss: 0.386654  [56000/175341]\n",
      "loss: 0.679985  [57600/175341]\n",
      "loss: 0.343918  [59200/175341]\n",
      "loss: 0.271437  [60800/175341]\n",
      "loss: 0.493665  [62400/175341]\n",
      "loss: 0.480580  [64000/175341]\n",
      "loss: 0.203430  [65600/175341]\n",
      "loss: 0.522999  [67200/175341]\n",
      "loss: 0.149791  [68800/175341]\n",
      "loss: 1.024941  [70400/175341]\n",
      "loss: 0.737083  [72000/175341]\n",
      "loss: 0.440752  [73600/175341]\n",
      "loss: 0.294652  [75200/175341]\n",
      "loss: 0.447095  [76800/175341]\n",
      "loss: 0.422899  [78400/175341]\n",
      "loss: 0.867693  [80000/175341]\n",
      "loss: 0.755203  [81600/175341]\n",
      "loss: 0.532308  [83200/175341]\n",
      "loss: 1.009452  [84800/175341]\n",
      "loss: 0.559291  [86400/175341]\n",
      "loss: 0.247125  [88000/175341]\n",
      "loss: 0.427462  [89600/175341]\n",
      "loss: 1.106858  [91200/175341]\n",
      "loss: 0.792696  [92800/175341]\n",
      "loss: 0.692726  [94400/175341]\n",
      "loss: 0.630331  [96000/175341]\n",
      "loss: 0.513404  [97600/175341]\n",
      "loss: 0.878812  [99200/175341]\n",
      "loss: 0.516949  [100800/175341]\n",
      "loss: 0.968784  [102400/175341]\n",
      "loss: 0.739459  [104000/175341]\n",
      "loss: 0.826678  [105600/175341]\n",
      "loss: 0.768084  [107200/175341]\n",
      "loss: 0.811226  [108800/175341]\n",
      "loss: 0.596334  [110400/175341]\n",
      "loss: 0.891852  [112000/175341]\n",
      "loss: 0.729625  [113600/175341]\n",
      "loss: 0.874390  [115200/175341]\n",
      "loss: 0.415909  [116800/175341]\n",
      "loss: 0.529601  [118400/175341]\n",
      "loss: 0.442899  [120000/175341]\n",
      "loss: 0.547304  [121600/175341]\n",
      "loss: 0.609614  [123200/175341]\n",
      "loss: 0.465008  [124800/175341]\n",
      "loss: 0.492498  [126400/175341]\n",
      "loss: 0.463292  [128000/175341]\n",
      "loss: 0.535091  [129600/175341]\n",
      "loss: 0.791921  [131200/175341]\n",
      "loss: 0.978283  [132800/175341]\n",
      "loss: 0.427985  [134400/175341]\n",
      "loss: 0.721466  [136000/175341]\n",
      "loss: 0.440411  [137600/175341]\n",
      "loss: 0.224987  [139200/175341]\n",
      "loss: 0.554704  [140800/175341]\n",
      "loss: 0.532785  [142400/175341]\n",
      "loss: 0.764533  [144000/175341]\n",
      "loss: 0.708224  [145600/175341]\n",
      "loss: 0.402538  [147200/175341]\n",
      "loss: 0.763568  [148800/175341]\n",
      "loss: 0.980272  [150400/175341]\n",
      "loss: 0.745069  [152000/175341]\n",
      "loss: 0.388006  [153600/175341]\n",
      "loss: 0.872928  [155200/175341]\n",
      "loss: 0.446339  [156800/175341]\n",
      "loss: 0.489958  [158400/175341]\n",
      "loss: 0.475910  [160000/175341]\n",
      "loss: 0.502070  [161600/175341]\n",
      "loss: 0.709030  [163200/175341]\n",
      "loss: 0.171752  [164800/175341]\n",
      "loss: 0.517799  [166400/175341]\n",
      "loss: 0.377460  [168000/175341]\n",
      "loss: 0.647453  [169600/175341]\n",
      "loss: 0.328704  [171200/175341]\n",
      "loss: 0.615348  [172800/175341]\n",
      "loss: 0.561549  [174400/175341]\n",
      "Train Accuracy: 77.2660%\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.655109, F1-score: 70.41%, Macro_F1-Score:  32.93%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.468976  [    0/175341]\n",
      "loss: 0.427100  [ 1600/175341]\n",
      "loss: 0.329528  [ 3200/175341]\n",
      "loss: 0.761211  [ 4800/175341]\n",
      "loss: 0.618573  [ 6400/175341]\n",
      "loss: 0.623770  [ 8000/175341]\n",
      "loss: 0.350490  [ 9600/175341]\n",
      "loss: 0.963814  [11200/175341]\n",
      "loss: 0.610496  [12800/175341]\n",
      "loss: 0.530913  [14400/175341]\n",
      "loss: 0.804769  [16000/175341]\n",
      "loss: 0.879596  [17600/175341]\n",
      "loss: 0.917367  [19200/175341]\n",
      "loss: 0.516947  [20800/175341]\n",
      "loss: 0.271927  [22400/175341]\n",
      "loss: 0.687133  [24000/175341]\n",
      "loss: 0.591002  [25600/175341]\n",
      "loss: 0.698677  [27200/175341]\n",
      "loss: 0.675826  [28800/175341]\n",
      "loss: 0.466406  [30400/175341]\n",
      "loss: 0.533906  [32000/175341]\n",
      "loss: 0.472533  [33600/175341]\n",
      "loss: 0.450741  [35200/175341]\n",
      "loss: 0.873196  [36800/175341]\n",
      "loss: 0.469592  [38400/175341]\n",
      "loss: 0.630887  [40000/175341]\n",
      "loss: 0.651654  [41600/175341]\n",
      "loss: 0.632464  [43200/175341]\n",
      "loss: 0.368740  [44800/175341]\n",
      "loss: 0.403986  [46400/175341]\n",
      "loss: 0.422229  [48000/175341]\n",
      "loss: 0.315969  [49600/175341]\n",
      "loss: 0.525651  [51200/175341]\n",
      "loss: 0.592883  [52800/175341]\n",
      "loss: 0.776516  [54400/175341]\n",
      "loss: 0.381368  [56000/175341]\n",
      "loss: 0.895411  [57600/175341]\n",
      "loss: 0.584711  [59200/175341]\n",
      "loss: 0.450937  [60800/175341]\n",
      "loss: 0.523140  [62400/175341]\n",
      "loss: 0.701446  [64000/175341]\n",
      "loss: 0.463391  [65600/175341]\n",
      "loss: 0.428084  [67200/175341]\n",
      "loss: 0.500312  [68800/175341]\n",
      "loss: 0.302830  [70400/175341]\n",
      "loss: 0.955167  [72000/175341]\n",
      "loss: 0.314582  [73600/175341]\n",
      "loss: 0.963579  [75200/175341]\n",
      "loss: 0.565695  [76800/175341]\n",
      "loss: 1.225027  [78400/175341]\n",
      "loss: 0.822090  [80000/175341]\n",
      "loss: 0.600530  [81600/175341]\n",
      "loss: 0.987511  [83200/175341]\n",
      "loss: 0.375053  [84800/175341]\n",
      "loss: 0.721301  [86400/175341]\n",
      "loss: 0.868006  [88000/175341]\n",
      "loss: 1.305549  [89600/175341]\n",
      "loss: 0.751340  [91200/175341]\n",
      "loss: 0.509591  [92800/175341]\n",
      "loss: 0.839261  [94400/175341]\n",
      "loss: 0.630565  [96000/175341]\n",
      "loss: 0.920778  [97600/175341]\n",
      "loss: 0.154341  [99200/175341]\n",
      "loss: 1.115902  [100800/175341]\n",
      "loss: 0.108773  [102400/175341]\n",
      "loss: 0.514617  [104000/175341]\n",
      "loss: 0.234215  [105600/175341]\n",
      "loss: 1.259122  [107200/175341]\n",
      "loss: 0.404174  [108800/175341]\n",
      "loss: 0.355053  [110400/175341]\n",
      "loss: 0.761980  [112000/175341]\n",
      "loss: 0.398532  [113600/175341]\n",
      "loss: 0.548505  [115200/175341]\n",
      "loss: 0.462818  [116800/175341]\n",
      "loss: 0.768007  [118400/175341]\n",
      "loss: 0.295548  [120000/175341]\n",
      "loss: 0.868019  [121600/175341]\n",
      "loss: 0.897672  [123200/175341]\n",
      "loss: 0.553720  [124800/175341]\n",
      "loss: 0.914847  [126400/175341]\n",
      "loss: 0.764882  [128000/175341]\n",
      "loss: 0.720732  [129600/175341]\n",
      "loss: 0.541477  [131200/175341]\n",
      "loss: 0.828145  [132800/175341]\n",
      "loss: 0.552084  [134400/175341]\n",
      "loss: 0.510198  [136000/175341]\n",
      "loss: 0.428776  [137600/175341]\n",
      "loss: 0.932941  [139200/175341]\n",
      "loss: 0.338604  [140800/175341]\n",
      "loss: 1.196510  [142400/175341]\n",
      "loss: 0.841979  [144000/175341]\n",
      "loss: 0.872997  [145600/175341]\n",
      "loss: 1.032902  [147200/175341]\n",
      "loss: 0.718442  [148800/175341]\n",
      "loss: 0.234676  [150400/175341]\n",
      "loss: 0.405313  [152000/175341]\n",
      "loss: 0.593405  [153600/175341]\n",
      "loss: 0.734212  [155200/175341]\n",
      "loss: 0.348529  [156800/175341]\n",
      "loss: 0.493997  [158400/175341]\n",
      "loss: 0.437716  [160000/175341]\n",
      "loss: 0.298262  [161600/175341]\n",
      "loss: 0.689010  [163200/175341]\n",
      "loss: 0.702081  [164800/175341]\n",
      "loss: 0.498898  [166400/175341]\n",
      "loss: 0.416990  [168000/175341]\n",
      "loss: 0.752354  [169600/175341]\n",
      "loss: 0.621527  [171200/175341]\n",
      "loss: 0.443640  [172800/175341]\n",
      "loss: 0.335231  [174400/175341]\n",
      "Train Accuracy: 77.9105%\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.647213, F1-score: 70.59%, Macro_F1-Score:  33.97%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.701742  [    0/175341]\n",
      "loss: 0.466446  [ 1600/175341]\n",
      "loss: 0.554692  [ 3200/175341]\n",
      "loss: 0.825656  [ 4800/175341]\n",
      "loss: 0.556810  [ 6400/175341]\n",
      "loss: 0.399096  [ 8000/175341]\n",
      "loss: 0.590724  [ 9600/175341]\n",
      "loss: 0.488721  [11200/175341]\n",
      "loss: 0.837571  [12800/175341]\n",
      "loss: 0.398028  [14400/175341]\n",
      "loss: 0.347726  [16000/175341]\n",
      "loss: 1.009267  [17600/175341]\n",
      "loss: 0.611419  [19200/175341]\n",
      "loss: 0.732381  [20800/175341]\n",
      "loss: 0.704396  [22400/175341]\n",
      "loss: 0.484845  [24000/175341]\n",
      "loss: 0.480906  [25600/175341]\n",
      "loss: 0.567721  [27200/175341]\n",
      "loss: 0.240594  [28800/175341]\n",
      "loss: 0.517904  [30400/175341]\n",
      "loss: 0.312551  [32000/175341]\n",
      "loss: 0.528182  [33600/175341]\n",
      "loss: 0.590161  [35200/175341]\n",
      "loss: 0.542846  [36800/175341]\n",
      "loss: 0.042391  [38400/175341]\n",
      "loss: 0.762234  [40000/175341]\n",
      "loss: 0.372451  [41600/175341]\n",
      "loss: 0.554675  [43200/175341]\n",
      "loss: 0.845423  [44800/175341]\n",
      "loss: 0.977132  [46400/175341]\n",
      "loss: 0.681140  [48000/175341]\n",
      "loss: 0.848667  [49600/175341]\n",
      "loss: 0.436685  [51200/175341]\n",
      "loss: 0.602754  [52800/175341]\n",
      "loss: 0.437566  [54400/175341]\n",
      "loss: 0.330433  [56000/175341]\n",
      "loss: 0.796692  [57600/175341]\n",
      "loss: 0.265040  [59200/175341]\n",
      "loss: 0.430110  [60800/175341]\n",
      "loss: 0.642490  [62400/175341]\n",
      "loss: 0.395742  [64000/175341]\n",
      "loss: 0.440005  [65600/175341]\n",
      "loss: 1.234747  [67200/175341]\n",
      "loss: 0.475883  [68800/175341]\n",
      "loss: 0.236746  [70400/175341]\n",
      "loss: 0.677096  [72000/175341]\n",
      "loss: 1.141158  [73600/175341]\n",
      "loss: 0.582115  [75200/175341]\n",
      "loss: 0.628663  [76800/175341]\n",
      "loss: 0.588550  [78400/175341]\n",
      "loss: 0.370271  [80000/175341]\n",
      "loss: 0.483887  [81600/175341]\n",
      "loss: 0.398118  [83200/175341]\n",
      "loss: 0.706764  [84800/175341]\n",
      "loss: 0.695310  [86400/175341]\n",
      "loss: 0.343484  [88000/175341]\n",
      "loss: 0.378727  [89600/175341]\n",
      "loss: 0.556108  [91200/175341]\n",
      "loss: 0.667479  [92800/175341]\n",
      "loss: 0.816543  [94400/175341]\n",
      "loss: 0.471627  [96000/175341]\n",
      "loss: 0.625416  [97600/175341]\n",
      "loss: 0.253583  [99200/175341]\n",
      "loss: 0.894716  [100800/175341]\n",
      "loss: 0.638600  [102400/175341]\n",
      "loss: 0.565919  [104000/175341]\n",
      "loss: 0.570881  [105600/175341]\n",
      "loss: 0.622756  [107200/175341]\n",
      "loss: 0.725803  [108800/175341]\n",
      "loss: 0.314780  [110400/175341]\n",
      "loss: 0.460428  [112000/175341]\n",
      "loss: 0.609070  [113600/175341]\n",
      "loss: 0.368426  [115200/175341]\n",
      "loss: 0.659591  [116800/175341]\n",
      "loss: 0.689190  [118400/175341]\n",
      "loss: 0.578191  [120000/175341]\n",
      "loss: 0.716912  [121600/175341]\n",
      "loss: 0.389649  [123200/175341]\n",
      "loss: 0.411877  [124800/175341]\n",
      "loss: 0.471274  [126400/175341]\n",
      "loss: 0.630224  [128000/175341]\n",
      "loss: 0.656895  [129600/175341]\n",
      "loss: 0.512210  [131200/175341]\n",
      "loss: 0.360932  [132800/175341]\n",
      "loss: 0.454564  [134400/175341]\n",
      "loss: 0.562986  [136000/175341]\n",
      "loss: 0.819969  [137600/175341]\n",
      "loss: 0.343731  [139200/175341]\n",
      "loss: 0.232546  [140800/175341]\n",
      "loss: 0.471528  [142400/175341]\n",
      "loss: 0.613212  [144000/175341]\n",
      "loss: 0.642234  [145600/175341]\n",
      "loss: 0.485984  [147200/175341]\n",
      "loss: 0.914216  [148800/175341]\n",
      "loss: 0.409949  [150400/175341]\n",
      "loss: 0.426851  [152000/175341]\n",
      "loss: 0.828737  [153600/175341]\n",
      "loss: 0.641587  [155200/175341]\n",
      "loss: 0.197626  [156800/175341]\n",
      "loss: 0.390453  [158400/175341]\n",
      "loss: 0.450509  [160000/175341]\n",
      "loss: 0.356398  [161600/175341]\n",
      "loss: 0.502493  [163200/175341]\n",
      "loss: 0.595099  [164800/175341]\n",
      "loss: 0.529733  [166400/175341]\n",
      "loss: 0.655664  [168000/175341]\n",
      "loss: 0.564117  [169600/175341]\n",
      "loss: 1.020984  [171200/175341]\n",
      "loss: 0.246682  [172800/175341]\n",
      "loss: 0.300696  [174400/175341]\n",
      "Train Accuracy: 78.3833%\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.631348, F1-score: 71.35%, Macro_F1-Score:  34.25%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.363524  [    0/175341]\n",
      "loss: 1.085340  [ 1600/175341]\n",
      "loss: 0.574574  [ 3200/175341]\n",
      "loss: 0.252087  [ 4800/175341]\n",
      "loss: 0.410643  [ 6400/175341]\n",
      "loss: 0.191707  [ 8000/175341]\n",
      "loss: 0.733032  [ 9600/175341]\n",
      "loss: 1.143760  [11200/175341]\n",
      "loss: 0.426516  [12800/175341]\n",
      "loss: 0.995589  [14400/175341]\n",
      "loss: 0.492782  [16000/175341]\n",
      "loss: 0.723208  [17600/175341]\n",
      "loss: 0.489369  [19200/175341]\n",
      "loss: 0.770131  [20800/175341]\n",
      "loss: 0.571012  [22400/175341]\n",
      "loss: 0.554883  [24000/175341]\n",
      "loss: 0.407827  [25600/175341]\n",
      "loss: 0.539683  [27200/175341]\n",
      "loss: 0.641654  [28800/175341]\n",
      "loss: 0.355112  [30400/175341]\n",
      "loss: 0.904542  [32000/175341]\n",
      "loss: 0.833871  [33600/175341]\n",
      "loss: 0.767514  [35200/175341]\n",
      "loss: 0.307179  [36800/175341]\n",
      "loss: 0.487058  [38400/175341]\n",
      "loss: 0.366592  [40000/175341]\n",
      "loss: 0.475312  [41600/175341]\n",
      "loss: 0.776891  [43200/175341]\n",
      "loss: 0.905264  [44800/175341]\n",
      "loss: 0.208068  [46400/175341]\n",
      "loss: 0.538386  [48000/175341]\n",
      "loss: 0.264008  [49600/175341]\n",
      "loss: 0.128200  [51200/175341]\n",
      "loss: 0.174933  [52800/175341]\n",
      "loss: 0.584828  [54400/175341]\n",
      "loss: 0.794698  [56000/175341]\n",
      "loss: 0.531858  [57600/175341]\n",
      "loss: 0.889542  [59200/175341]\n",
      "loss: 0.260213  [60800/175341]\n",
      "loss: 0.396399  [62400/175341]\n",
      "loss: 0.810815  [64000/175341]\n",
      "loss: 0.459470  [65600/175341]\n",
      "loss: 0.413637  [67200/175341]\n",
      "loss: 0.400404  [68800/175341]\n",
      "loss: 0.642574  [70400/175341]\n",
      "loss: 0.366953  [72000/175341]\n",
      "loss: 0.594975  [73600/175341]\n",
      "loss: 0.669908  [75200/175341]\n",
      "loss: 0.265478  [76800/175341]\n",
      "loss: 0.533671  [78400/175341]\n",
      "loss: 0.362142  [80000/175341]\n",
      "loss: 0.594867  [81600/175341]\n",
      "loss: 0.507098  [83200/175341]\n",
      "loss: 0.825920  [84800/175341]\n",
      "loss: 0.528349  [86400/175341]\n",
      "loss: 0.516422  [88000/175341]\n",
      "loss: 0.126453  [89600/175341]\n",
      "loss: 0.607896  [91200/175341]\n",
      "loss: 0.689948  [92800/175341]\n",
      "loss: 0.381360  [94400/175341]\n",
      "loss: 1.241184  [96000/175341]\n",
      "loss: 0.612245  [97600/175341]\n",
      "loss: 0.215130  [99200/175341]\n",
      "loss: 0.402484  [100800/175341]\n",
      "loss: 0.224030  [102400/175341]\n",
      "loss: 0.189944  [104000/175341]\n",
      "loss: 0.742902  [105600/175341]\n",
      "loss: 0.536350  [107200/175341]\n",
      "loss: 0.641266  [108800/175341]\n",
      "loss: 0.505874  [110400/175341]\n",
      "loss: 0.563866  [112000/175341]\n",
      "loss: 0.475610  [113600/175341]\n",
      "loss: 0.671633  [115200/175341]\n",
      "loss: 0.136462  [116800/175341]\n",
      "loss: 0.214528  [118400/175341]\n",
      "loss: 0.511439  [120000/175341]\n",
      "loss: 0.325645  [121600/175341]\n",
      "loss: 0.585425  [123200/175341]\n",
      "loss: 0.481876  [124800/175341]\n",
      "loss: 0.697159  [126400/175341]\n",
      "loss: 0.361782  [128000/175341]\n",
      "loss: 0.606655  [129600/175341]\n",
      "loss: 0.513874  [131200/175341]\n",
      "loss: 0.303094  [132800/175341]\n",
      "loss: 0.417860  [134400/175341]\n",
      "loss: 0.316593  [136000/175341]\n",
      "loss: 0.222564  [137600/175341]\n",
      "loss: 0.920095  [139200/175341]\n",
      "loss: 0.719960  [140800/175341]\n",
      "loss: 1.080302  [142400/175341]\n",
      "loss: 1.039282  [144000/175341]\n",
      "loss: 0.340918  [145600/175341]\n",
      "loss: 0.438790  [147200/175341]\n",
      "loss: 0.242656  [148800/175341]\n",
      "loss: 0.638612  [150400/175341]\n",
      "loss: 0.578511  [152000/175341]\n",
      "loss: 0.673034  [153600/175341]\n",
      "loss: 0.536601  [155200/175341]\n",
      "loss: 0.498206  [156800/175341]\n",
      "loss: 0.708811  [158400/175341]\n",
      "loss: 0.717446  [160000/175341]\n",
      "loss: 0.668516  [161600/175341]\n",
      "loss: 0.834922  [163200/175341]\n",
      "loss: 0.544325  [164800/175341]\n",
      "loss: 0.918060  [166400/175341]\n",
      "loss: 0.286008  [168000/175341]\n",
      "loss: 0.595330  [169600/175341]\n",
      "loss: 0.411805  [171200/175341]\n",
      "loss: 0.359985  [172800/175341]\n",
      "loss: 0.369345  [174400/175341]\n",
      "Train Accuracy: 78.6393%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.623923, F1-score: 72.70%, Macro_F1-Score:  34.35%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.244373  [    0/175341]\n",
      "loss: 0.612323  [ 1600/175341]\n",
      "loss: 0.703950  [ 3200/175341]\n",
      "loss: 0.977733  [ 4800/175341]\n",
      "loss: 0.649729  [ 6400/175341]\n",
      "loss: 0.415919  [ 8000/175341]\n",
      "loss: 0.586589  [ 9600/175341]\n",
      "loss: 0.525996  [11200/175341]\n",
      "loss: 0.334190  [12800/175341]\n",
      "loss: 0.451479  [14400/175341]\n",
      "loss: 0.631435  [16000/175341]\n",
      "loss: 0.237468  [17600/175341]\n",
      "loss: 0.574018  [19200/175341]\n",
      "loss: 0.799229  [20800/175341]\n",
      "loss: 0.335210  [22400/175341]\n",
      "loss: 0.358905  [24000/175341]\n",
      "loss: 1.058402  [25600/175341]\n",
      "loss: 1.118879  [27200/175341]\n",
      "loss: 0.378653  [28800/175341]\n",
      "loss: 0.625400  [30400/175341]\n",
      "loss: 0.412667  [32000/175341]\n",
      "loss: 0.751200  [33600/175341]\n",
      "loss: 1.168911  [35200/175341]\n",
      "loss: 0.221385  [36800/175341]\n",
      "loss: 0.219396  [38400/175341]\n",
      "loss: 1.079068  [40000/175341]\n",
      "loss: 0.608958  [41600/175341]\n",
      "loss: 0.665799  [43200/175341]\n",
      "loss: 0.583785  [44800/175341]\n",
      "loss: 0.534645  [46400/175341]\n",
      "loss: 0.500685  [48000/175341]\n",
      "loss: 0.417893  [49600/175341]\n",
      "loss: 0.711697  [51200/175341]\n",
      "loss: 0.416589  [52800/175341]\n",
      "loss: 0.407524  [54400/175341]\n",
      "loss: 0.558476  [56000/175341]\n",
      "loss: 0.482634  [57600/175341]\n",
      "loss: 0.398456  [59200/175341]\n",
      "loss: 0.463653  [60800/175341]\n",
      "loss: 0.695665  [62400/175341]\n",
      "loss: 0.698284  [64000/175341]\n",
      "loss: 0.471899  [65600/175341]\n",
      "loss: 0.636182  [67200/175341]\n",
      "loss: 0.426373  [68800/175341]\n",
      "loss: 0.355181  [70400/175341]\n",
      "loss: 0.758869  [72000/175341]\n",
      "loss: 0.553598  [73600/175341]\n",
      "loss: 0.505359  [75200/175341]\n",
      "loss: 0.750444  [76800/175341]\n",
      "loss: 0.305251  [78400/175341]\n",
      "loss: 0.553867  [80000/175341]\n",
      "loss: 1.067933  [81600/175341]\n",
      "loss: 0.406215  [83200/175341]\n",
      "loss: 0.288266  [84800/175341]\n",
      "loss: 0.401273  [86400/175341]\n",
      "loss: 0.571138  [88000/175341]\n",
      "loss: 0.457755  [89600/175341]\n",
      "loss: 0.485516  [91200/175341]\n",
      "loss: 0.374503  [92800/175341]\n",
      "loss: 0.444060  [94400/175341]\n",
      "loss: 0.273096  [96000/175341]\n",
      "loss: 0.761709  [97600/175341]\n",
      "loss: 0.355081  [99200/175341]\n",
      "loss: 0.656913  [100800/175341]\n",
      "loss: 0.779483  [102400/175341]\n",
      "loss: 0.352202  [104000/175341]\n",
      "loss: 0.843163  [105600/175341]\n",
      "loss: 0.772627  [107200/175341]\n",
      "loss: 0.323860  [108800/175341]\n",
      "loss: 0.307054  [110400/175341]\n",
      "loss: 0.181369  [112000/175341]\n",
      "loss: 0.357797  [113600/175341]\n",
      "loss: 0.354424  [115200/175341]\n",
      "loss: 0.274979  [116800/175341]\n",
      "loss: 0.450196  [118400/175341]\n",
      "loss: 0.437730  [120000/175341]\n",
      "loss: 0.668182  [121600/175341]\n",
      "loss: 1.119792  [123200/175341]\n",
      "loss: 0.607258  [124800/175341]\n",
      "loss: 1.229358  [126400/175341]\n",
      "loss: 0.511617  [128000/175341]\n",
      "loss: 0.430834  [129600/175341]\n",
      "loss: 0.564406  [131200/175341]\n",
      "loss: 0.573361  [132800/175341]\n",
      "loss: 0.422869  [134400/175341]\n",
      "loss: 0.330230  [136000/175341]\n",
      "loss: 0.531516  [137600/175341]\n",
      "loss: 0.460546  [139200/175341]\n",
      "loss: 0.196535  [140800/175341]\n",
      "loss: 0.723674  [142400/175341]\n",
      "loss: 0.548249  [144000/175341]\n",
      "loss: 0.424230  [145600/175341]\n",
      "loss: 0.361100  [147200/175341]\n",
      "loss: 0.402178  [148800/175341]\n",
      "loss: 0.739483  [150400/175341]\n",
      "loss: 0.706187  [152000/175341]\n",
      "loss: 0.611212  [153600/175341]\n",
      "loss: 0.510813  [155200/175341]\n",
      "loss: 0.436414  [156800/175341]\n",
      "loss: 0.732209  [158400/175341]\n",
      "loss: 0.617525  [160000/175341]\n",
      "loss: 0.279539  [161600/175341]\n",
      "loss: 0.740561  [163200/175341]\n",
      "loss: 0.609986  [164800/175341]\n",
      "loss: 0.509653  [166400/175341]\n",
      "loss: 0.704115  [168000/175341]\n",
      "loss: 1.028315  [169600/175341]\n",
      "loss: 0.140179  [171200/175341]\n",
      "loss: 0.533431  [172800/175341]\n",
      "loss: 0.403374  [174400/175341]\n",
      "Train Accuracy: 78.8047%\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.619199, F1-score: 72.43%, Macro_F1-Score:  36.94%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.302836  [    0/175341]\n",
      "loss: 0.640549  [ 1600/175341]\n",
      "loss: 0.610258  [ 3200/175341]\n",
      "loss: 0.244483  [ 4800/175341]\n",
      "loss: 0.659346  [ 6400/175341]\n",
      "loss: 0.576934  [ 8000/175341]\n",
      "loss: 0.432253  [ 9600/175341]\n",
      "loss: 0.309230  [11200/175341]\n",
      "loss: 0.513205  [12800/175341]\n",
      "loss: 0.425331  [14400/175341]\n",
      "loss: 0.308024  [16000/175341]\n",
      "loss: 0.574860  [17600/175341]\n",
      "loss: 0.676896  [19200/175341]\n",
      "loss: 0.776751  [20800/175341]\n",
      "loss: 0.785014  [22400/175341]\n",
      "loss: 0.909867  [24000/175341]\n",
      "loss: 0.714524  [25600/175341]\n",
      "loss: 0.675469  [27200/175341]\n",
      "loss: 0.684189  [28800/175341]\n",
      "loss: 0.274785  [30400/175341]\n",
      "loss: 0.150712  [32000/175341]\n",
      "loss: 0.509098  [33600/175341]\n",
      "loss: 0.634235  [35200/175341]\n",
      "loss: 0.380923  [36800/175341]\n",
      "loss: 0.334145  [38400/175341]\n",
      "loss: 0.520213  [40000/175341]\n",
      "loss: 0.509433  [41600/175341]\n",
      "loss: 0.620167  [43200/175341]\n",
      "loss: 0.608883  [44800/175341]\n",
      "loss: 0.278095  [46400/175341]\n",
      "loss: 0.378551  [48000/175341]\n",
      "loss: 0.543819  [49600/175341]\n",
      "loss: 0.481977  [51200/175341]\n",
      "loss: 0.721468  [52800/175341]\n",
      "loss: 0.889552  [54400/175341]\n",
      "loss: 0.894045  [56000/175341]\n",
      "loss: 0.594793  [57600/175341]\n",
      "loss: 0.376039  [59200/175341]\n",
      "loss: 0.434305  [60800/175341]\n",
      "loss: 0.296900  [62400/175341]\n",
      "loss: 0.721008  [64000/175341]\n",
      "loss: 0.424309  [65600/175341]\n",
      "loss: 0.240586  [67200/175341]\n",
      "loss: 0.608708  [68800/175341]\n",
      "loss: 0.328255  [70400/175341]\n",
      "loss: 0.448014  [72000/175341]\n",
      "loss: 0.496704  [73600/175341]\n",
      "loss: 0.189137  [75200/175341]\n",
      "loss: 0.556081  [76800/175341]\n",
      "loss: 0.656515  [78400/175341]\n",
      "loss: 0.310784  [80000/175341]\n",
      "loss: 0.499548  [81600/175341]\n",
      "loss: 0.721737  [83200/175341]\n",
      "loss: 0.711214  [84800/175341]\n",
      "loss: 0.504182  [86400/175341]\n",
      "loss: 0.429685  [88000/175341]\n",
      "loss: 0.573038  [89600/175341]\n",
      "loss: 0.679002  [91200/175341]\n",
      "loss: 0.247893  [92800/175341]\n",
      "loss: 0.411819  [94400/175341]\n",
      "loss: 0.479317  [96000/175341]\n",
      "loss: 0.518026  [97600/175341]\n",
      "loss: 0.413934  [99200/175341]\n",
      "loss: 0.476819  [100800/175341]\n",
      "loss: 0.715202  [102400/175341]\n",
      "loss: 0.483336  [104000/175341]\n",
      "loss: 0.557430  [105600/175341]\n",
      "loss: 0.302105  [107200/175341]\n",
      "loss: 0.392483  [108800/175341]\n",
      "loss: 0.451616  [110400/175341]\n",
      "loss: 0.387761  [112000/175341]\n",
      "loss: 0.700109  [113600/175341]\n",
      "loss: 0.447675  [115200/175341]\n",
      "loss: 0.700557  [116800/175341]\n",
      "loss: 0.559570  [118400/175341]\n",
      "loss: 0.648365  [120000/175341]\n",
      "loss: 0.607266  [121600/175341]\n",
      "loss: 0.650473  [123200/175341]\n",
      "loss: 0.345851  [124800/175341]\n",
      "loss: 0.768955  [126400/175341]\n",
      "loss: 0.500095  [128000/175341]\n",
      "loss: 0.468106  [129600/175341]\n",
      "loss: 0.122120  [131200/175341]\n",
      "loss: 0.529825  [132800/175341]\n",
      "loss: 0.329078  [134400/175341]\n",
      "loss: 0.486731  [136000/175341]\n",
      "loss: 0.563426  [137600/175341]\n",
      "loss: 0.357739  [139200/175341]\n",
      "loss: 0.617531  [140800/175341]\n",
      "loss: 0.474778  [142400/175341]\n",
      "loss: 0.679373  [144000/175341]\n",
      "loss: 0.621688  [145600/175341]\n",
      "loss: 0.619809  [147200/175341]\n",
      "loss: 1.063906  [148800/175341]\n",
      "loss: 0.936397  [150400/175341]\n",
      "loss: 0.749270  [152000/175341]\n",
      "loss: 0.145674  [153600/175341]\n",
      "loss: 0.410046  [155200/175341]\n",
      "loss: 0.624748  [156800/175341]\n",
      "loss: 0.187641  [158400/175341]\n",
      "loss: 0.298712  [160000/175341]\n",
      "loss: 0.371945  [161600/175341]\n",
      "loss: 0.453723  [163200/175341]\n",
      "loss: 0.417489  [164800/175341]\n",
      "loss: 0.219742  [166400/175341]\n",
      "loss: 0.422261  [168000/175341]\n",
      "loss: 0.590487  [169600/175341]\n",
      "loss: 0.543331  [171200/175341]\n",
      "loss: 0.358213  [172800/175341]\n",
      "loss: 0.211723  [174400/175341]\n",
      "Train Accuracy: 78.9707%\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.603198, F1-score: 73.51%, Macro_F1-Score:  37.66%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.331087  [    0/175341]\n",
      "loss: 0.321000  [ 1600/175341]\n",
      "loss: 0.769530  [ 3200/175341]\n",
      "loss: 0.620610  [ 4800/175341]\n",
      "loss: 0.875741  [ 6400/175341]\n",
      "loss: 0.341315  [ 8000/175341]\n",
      "loss: 0.505101  [ 9600/175341]\n",
      "loss: 0.412714  [11200/175341]\n",
      "loss: 0.237123  [12800/175341]\n",
      "loss: 0.659112  [14400/175341]\n",
      "loss: 0.257913  [16000/175341]\n",
      "loss: 0.556258  [17600/175341]\n",
      "loss: 0.212246  [19200/175341]\n",
      "loss: 1.000739  [20800/175341]\n",
      "loss: 0.629511  [22400/175341]\n",
      "loss: 1.200174  [24000/175341]\n",
      "loss: 0.372989  [25600/175341]\n",
      "loss: 0.569427  [27200/175341]\n",
      "loss: 0.422127  [28800/175341]\n",
      "loss: 0.939778  [30400/175341]\n",
      "loss: 0.379554  [32000/175341]\n",
      "loss: 0.269415  [33600/175341]\n",
      "loss: 0.629495  [35200/175341]\n",
      "loss: 0.274632  [36800/175341]\n",
      "loss: 0.426825  [38400/175341]\n",
      "loss: 0.270671  [40000/175341]\n",
      "loss: 0.497892  [41600/175341]\n",
      "loss: 0.291906  [43200/175341]\n",
      "loss: 0.718465  [44800/175341]\n",
      "loss: 0.470351  [46400/175341]\n",
      "loss: 0.500132  [48000/175341]\n",
      "loss: 0.517712  [49600/175341]\n",
      "loss: 0.269800  [51200/175341]\n",
      "loss: 0.363360  [52800/175341]\n",
      "loss: 0.659086  [54400/175341]\n",
      "loss: 0.525178  [56000/175341]\n",
      "loss: 0.357350  [57600/175341]\n",
      "loss: 0.597783  [59200/175341]\n",
      "loss: 0.335516  [60800/175341]\n",
      "loss: 0.510436  [62400/175341]\n",
      "loss: 0.791244  [64000/175341]\n",
      "loss: 0.278392  [65600/175341]\n",
      "loss: 0.964896  [67200/175341]\n",
      "loss: 0.649531  [68800/175341]\n",
      "loss: 0.497236  [70400/175341]\n",
      "loss: 0.641657  [72000/175341]\n",
      "loss: 0.340126  [73600/175341]\n",
      "loss: 0.341186  [75200/175341]\n",
      "loss: 0.845421  [76800/175341]\n",
      "loss: 0.709415  [78400/175341]\n",
      "loss: 0.366594  [80000/175341]\n",
      "loss: 0.258217  [81600/175341]\n",
      "loss: 0.703281  [83200/175341]\n",
      "loss: 0.555589  [84800/175341]\n",
      "loss: 0.555182  [86400/175341]\n",
      "loss: 0.448118  [88000/175341]\n",
      "loss: 0.283830  [89600/175341]\n",
      "loss: 0.654293  [91200/175341]\n",
      "loss: 0.600319  [92800/175341]\n",
      "loss: 0.410229  [94400/175341]\n",
      "loss: 0.486690  [96000/175341]\n",
      "loss: 0.938096  [97600/175341]\n",
      "loss: 0.433782  [99200/175341]\n",
      "loss: 0.535438  [100800/175341]\n",
      "loss: 0.289911  [102400/175341]\n",
      "loss: 0.460040  [104000/175341]\n",
      "loss: 0.413045  [105600/175341]\n",
      "loss: 0.303298  [107200/175341]\n",
      "loss: 0.491911  [108800/175341]\n",
      "loss: 0.413054  [110400/175341]\n",
      "loss: 0.833152  [112000/175341]\n",
      "loss: 0.612128  [113600/175341]\n",
      "loss: 0.384834  [115200/175341]\n",
      "loss: 0.215011  [116800/175341]\n",
      "loss: 0.460224  [118400/175341]\n",
      "loss: 0.282379  [120000/175341]\n",
      "loss: 0.324524  [121600/175341]\n",
      "loss: 0.315443  [123200/175341]\n",
      "loss: 0.838394  [124800/175341]\n",
      "loss: 0.284546  [126400/175341]\n",
      "loss: 0.604350  [128000/175341]\n",
      "loss: 0.210118  [129600/175341]\n",
      "loss: 0.267226  [131200/175341]\n",
      "loss: 0.227187  [132800/175341]\n",
      "loss: 0.484329  [134400/175341]\n",
      "loss: 0.527922  [136000/175341]\n",
      "loss: 0.445609  [137600/175341]\n",
      "loss: 0.665935  [139200/175341]\n",
      "loss: 0.383168  [140800/175341]\n",
      "loss: 0.350042  [142400/175341]\n",
      "loss: 1.258743  [144000/175341]\n",
      "loss: 0.335405  [145600/175341]\n",
      "loss: 0.176755  [147200/175341]\n",
      "loss: 1.290168  [148800/175341]\n",
      "loss: 0.481038  [150400/175341]\n",
      "loss: 0.199464  [152000/175341]\n",
      "loss: 0.714755  [153600/175341]\n",
      "loss: 0.355098  [155200/175341]\n",
      "loss: 0.454568  [156800/175341]\n",
      "loss: 0.932018  [158400/175341]\n",
      "loss: 0.420905  [160000/175341]\n",
      "loss: 0.695882  [161600/175341]\n",
      "loss: 1.020284  [163200/175341]\n",
      "loss: 0.727836  [164800/175341]\n",
      "loss: 0.712513  [166400/175341]\n",
      "loss: 0.228911  [168000/175341]\n",
      "loss: 0.295197  [169600/175341]\n",
      "loss: 0.401533  [171200/175341]\n",
      "loss: 0.293085  [172800/175341]\n",
      "loss: 0.629642  [174400/175341]\n",
      "Train Accuracy: 79.1794%\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.626098, F1-score: 72.08%, Macro_F1-Score:  37.85%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.702366  [    0/175341]\n",
      "loss: 0.834213  [ 1600/175341]\n",
      "loss: 0.638406  [ 3200/175341]\n",
      "loss: 0.321492  [ 4800/175341]\n",
      "loss: 0.262141  [ 6400/175341]\n",
      "loss: 0.664983  [ 8000/175341]\n",
      "loss: 0.330061  [ 9600/175341]\n",
      "loss: 0.871585  [11200/175341]\n",
      "loss: 0.752660  [12800/175341]\n",
      "loss: 0.516485  [14400/175341]\n",
      "loss: 0.290076  [16000/175341]\n",
      "loss: 0.540607  [17600/175341]\n",
      "loss: 0.394776  [19200/175341]\n",
      "loss: 0.634635  [20800/175341]\n",
      "loss: 0.522068  [22400/175341]\n",
      "loss: 0.356260  [24000/175341]\n",
      "loss: 0.204642  [25600/175341]\n",
      "loss: 0.409080  [27200/175341]\n",
      "loss: 0.245777  [28800/175341]\n",
      "loss: 0.832177  [30400/175341]\n",
      "loss: 0.311772  [32000/175341]\n",
      "loss: 0.430569  [33600/175341]\n",
      "loss: 0.511399  [35200/175341]\n",
      "loss: 0.572437  [36800/175341]\n",
      "loss: 0.768561  [38400/175341]\n",
      "loss: 0.614667  [40000/175341]\n",
      "loss: 1.011888  [41600/175341]\n",
      "loss: 0.922108  [43200/175341]\n",
      "loss: 0.213910  [44800/175341]\n",
      "loss: 0.743246  [46400/175341]\n",
      "loss: 1.121718  [48000/175341]\n",
      "loss: 0.619978  [49600/175341]\n",
      "loss: 0.866901  [51200/175341]\n",
      "loss: 0.385221  [52800/175341]\n",
      "loss: 0.415843  [54400/175341]\n",
      "loss: 0.934660  [56000/175341]\n",
      "loss: 0.786156  [57600/175341]\n",
      "loss: 0.610976  [59200/175341]\n",
      "loss: 0.431224  [60800/175341]\n",
      "loss: 0.552474  [62400/175341]\n",
      "loss: 0.229188  [64000/175341]\n",
      "loss: 0.736655  [65600/175341]\n",
      "loss: 0.603973  [67200/175341]\n",
      "loss: 0.805244  [68800/175341]\n",
      "loss: 0.121385  [70400/175341]\n",
      "loss: 0.744529  [72000/175341]\n",
      "loss: 0.831638  [73600/175341]\n",
      "loss: 0.512879  [75200/175341]\n",
      "loss: 0.346604  [76800/175341]\n",
      "loss: 0.360069  [78400/175341]\n",
      "loss: 0.481206  [80000/175341]\n",
      "loss: 0.513484  [81600/175341]\n",
      "loss: 0.436320  [83200/175341]\n",
      "loss: 0.484098  [84800/175341]\n",
      "loss: 0.353340  [86400/175341]\n",
      "loss: 0.597354  [88000/175341]\n",
      "loss: 0.298795  [89600/175341]\n",
      "loss: 0.508346  [91200/175341]\n",
      "loss: 0.661472  [92800/175341]\n",
      "loss: 0.222881  [94400/175341]\n",
      "loss: 0.615925  [96000/175341]\n",
      "loss: 1.210426  [97600/175341]\n",
      "loss: 0.221287  [99200/175341]\n",
      "loss: 0.477835  [100800/175341]\n",
      "loss: 0.449927  [102400/175341]\n",
      "loss: 0.562704  [104000/175341]\n",
      "loss: 0.755292  [105600/175341]\n",
      "loss: 0.480897  [107200/175341]\n",
      "loss: 0.466239  [108800/175341]\n",
      "loss: 0.545425  [110400/175341]\n",
      "loss: 0.635524  [112000/175341]\n",
      "loss: 0.582499  [113600/175341]\n",
      "loss: 0.616661  [115200/175341]\n",
      "loss: 0.478026  [116800/175341]\n",
      "loss: 0.457258  [118400/175341]\n",
      "loss: 0.340183  [120000/175341]\n",
      "loss: 0.926358  [121600/175341]\n",
      "loss: 0.569474  [123200/175341]\n",
      "loss: 0.483993  [124800/175341]\n",
      "loss: 0.446573  [126400/175341]\n",
      "loss: 0.390869  [128000/175341]\n",
      "loss: 0.527308  [129600/175341]\n",
      "loss: 0.297683  [131200/175341]\n",
      "loss: 0.348599  [132800/175341]\n",
      "loss: 0.403332  [134400/175341]\n",
      "loss: 0.625127  [136000/175341]\n",
      "loss: 0.187544  [137600/175341]\n",
      "loss: 0.598120  [139200/175341]\n",
      "loss: 0.794571  [140800/175341]\n",
      "loss: 0.108133  [142400/175341]\n",
      "loss: 0.754138  [144000/175341]\n",
      "loss: 0.800996  [145600/175341]\n",
      "loss: 0.350348  [147200/175341]\n",
      "loss: 0.414124  [148800/175341]\n",
      "loss: 0.250420  [150400/175341]\n",
      "loss: 0.449480  [152000/175341]\n",
      "loss: 0.375512  [153600/175341]\n",
      "loss: 0.442966  [155200/175341]\n",
      "loss: 0.643032  [156800/175341]\n",
      "loss: 0.580109  [158400/175341]\n",
      "loss: 0.652808  [160000/175341]\n",
      "loss: 0.840977  [161600/175341]\n",
      "loss: 0.514204  [163200/175341]\n",
      "loss: 0.807835  [164800/175341]\n",
      "loss: 0.483897  [166400/175341]\n",
      "loss: 1.455665  [168000/175341]\n",
      "loss: 0.591351  [169600/175341]\n",
      "loss: 0.203030  [171200/175341]\n",
      "loss: 0.579443  [172800/175341]\n",
      "loss: 0.311609  [174400/175341]\n",
      "Train Accuracy: 79.3100%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.613630, F1-score: 73.43%, Macro_F1-Score:  37.85%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.570873  [    0/175341]\n",
      "loss: 0.283659  [ 1600/175341]\n",
      "loss: 0.214194  [ 3200/175341]\n",
      "loss: 0.425149  [ 4800/175341]\n",
      "loss: 0.752437  [ 6400/175341]\n",
      "loss: 0.367447  [ 8000/175341]\n",
      "loss: 0.587105  [ 9600/175341]\n",
      "loss: 1.061609  [11200/175341]\n",
      "loss: 0.648933  [12800/175341]\n",
      "loss: 0.147982  [14400/175341]\n",
      "loss: 0.340029  [16000/175341]\n",
      "loss: 0.703602  [17600/175341]\n",
      "loss: 0.472184  [19200/175341]\n",
      "loss: 0.565138  [20800/175341]\n",
      "loss: 0.549507  [22400/175341]\n",
      "loss: 0.484141  [24000/175341]\n",
      "loss: 0.140128  [25600/175341]\n",
      "loss: 0.688791  [27200/175341]\n",
      "loss: 0.255617  [28800/175341]\n",
      "loss: 0.226329  [30400/175341]\n",
      "loss: 0.916919  [32000/175341]\n",
      "loss: 0.601035  [33600/175341]\n",
      "loss: 0.444093  [35200/175341]\n",
      "loss: 0.401905  [36800/175341]\n",
      "loss: 0.674528  [38400/175341]\n",
      "loss: 0.696833  [40000/175341]\n",
      "loss: 0.673869  [41600/175341]\n",
      "loss: 0.465699  [43200/175341]\n",
      "loss: 0.211851  [44800/175341]\n",
      "loss: 0.693173  [46400/175341]\n",
      "loss: 0.616286  [48000/175341]\n",
      "loss: 0.272980  [49600/175341]\n",
      "loss: 0.351441  [51200/175341]\n",
      "loss: 0.466249  [52800/175341]\n",
      "loss: 0.451705  [54400/175341]\n",
      "loss: 0.293245  [56000/175341]\n",
      "loss: 0.626465  [57600/175341]\n",
      "loss: 0.192362  [59200/175341]\n",
      "loss: 1.038351  [60800/175341]\n",
      "loss: 0.349850  [62400/175341]\n",
      "loss: 0.677938  [64000/175341]\n",
      "loss: 0.334511  [65600/175341]\n",
      "loss: 0.485241  [67200/175341]\n",
      "loss: 0.595657  [68800/175341]\n",
      "loss: 0.310901  [70400/175341]\n",
      "loss: 0.912392  [72000/175341]\n",
      "loss: 0.576714  [73600/175341]\n",
      "loss: 0.539241  [75200/175341]\n",
      "loss: 0.346525  [76800/175341]\n",
      "loss: 0.509711  [78400/175341]\n",
      "loss: 0.358752  [80000/175341]\n",
      "loss: 0.372159  [81600/175341]\n",
      "loss: 0.703521  [83200/175341]\n",
      "loss: 0.305531  [84800/175341]\n",
      "loss: 0.455810  [86400/175341]\n",
      "loss: 0.305274  [88000/175341]\n",
      "loss: 0.453029  [89600/175341]\n",
      "loss: 0.553222  [91200/175341]\n",
      "loss: 0.218397  [92800/175341]\n",
      "loss: 0.702214  [94400/175341]\n",
      "loss: 0.390073  [96000/175341]\n",
      "loss: 0.833544  [97600/175341]\n",
      "loss: 0.573328  [99200/175341]\n",
      "loss: 0.789938  [100800/175341]\n",
      "loss: 0.788960  [102400/175341]\n",
      "loss: 0.821326  [104000/175341]\n",
      "loss: 0.390995  [105600/175341]\n",
      "loss: 0.333983  [107200/175341]\n",
      "loss: 0.269988  [108800/175341]\n",
      "loss: 0.625697  [110400/175341]\n",
      "loss: 0.763984  [112000/175341]\n",
      "loss: 0.449462  [113600/175341]\n",
      "loss: 0.952889  [115200/175341]\n",
      "loss: 0.592797  [116800/175341]\n",
      "loss: 0.216698  [118400/175341]\n",
      "loss: 0.359075  [120000/175341]\n",
      "loss: 0.800640  [121600/175341]\n",
      "loss: 0.348850  [123200/175341]\n",
      "loss: 0.519455  [124800/175341]\n",
      "loss: 0.461442  [126400/175341]\n",
      "loss: 0.523838  [128000/175341]\n",
      "loss: 0.439756  [129600/175341]\n",
      "loss: 0.360536  [131200/175341]\n",
      "loss: 0.727676  [132800/175341]\n",
      "loss: 0.608808  [134400/175341]\n",
      "loss: 0.538642  [136000/175341]\n",
      "loss: 0.566777  [137600/175341]\n",
      "loss: 0.499253  [139200/175341]\n",
      "loss: 0.277935  [140800/175341]\n",
      "loss: 0.575744  [142400/175341]\n",
      "loss: 0.298112  [144000/175341]\n",
      "loss: 0.597252  [145600/175341]\n",
      "loss: 0.225045  [147200/175341]\n",
      "loss: 0.506385  [148800/175341]\n",
      "loss: 0.836494  [150400/175341]\n",
      "loss: 0.572494  [152000/175341]\n",
      "loss: 0.482411  [153600/175341]\n",
      "loss: 0.658394  [155200/175341]\n",
      "loss: 0.275104  [156800/175341]\n",
      "loss: 0.658505  [158400/175341]\n",
      "loss: 0.597537  [160000/175341]\n",
      "loss: 0.384620  [161600/175341]\n",
      "loss: 0.869988  [163200/175341]\n",
      "loss: 0.546608  [164800/175341]\n",
      "loss: 0.930025  [166400/175341]\n",
      "loss: 0.408329  [168000/175341]\n",
      "loss: 0.473799  [169600/175341]\n",
      "loss: 0.714279  [171200/175341]\n",
      "loss: 0.472220  [172800/175341]\n",
      "loss: 0.906664  [174400/175341]\n",
      "Train Accuracy: 79.4115%\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.596902, F1-score: 73.24%, Macro_F1-Score:  38.10%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.859912  [    0/175341]\n",
      "loss: 0.516040  [ 1600/175341]\n",
      "loss: 0.200347  [ 3200/175341]\n",
      "loss: 0.095488  [ 4800/175341]\n",
      "loss: 0.446866  [ 6400/175341]\n",
      "loss: 0.439168  [ 8000/175341]\n",
      "loss: 0.484334  [ 9600/175341]\n",
      "loss: 0.367916  [11200/175341]\n",
      "loss: 0.824760  [12800/175341]\n",
      "loss: 0.574414  [14400/175341]\n",
      "loss: 0.333486  [16000/175341]\n",
      "loss: 0.515892  [17600/175341]\n",
      "loss: 0.543392  [19200/175341]\n",
      "loss: 0.882688  [20800/175341]\n",
      "loss: 0.721309  [22400/175341]\n",
      "loss: 0.656598  [24000/175341]\n",
      "loss: 0.297028  [25600/175341]\n",
      "loss: 0.670501  [27200/175341]\n",
      "loss: 0.517014  [28800/175341]\n",
      "loss: 0.117418  [30400/175341]\n",
      "loss: 0.308228  [32000/175341]\n",
      "loss: 0.537656  [33600/175341]\n",
      "loss: 0.704536  [35200/175341]\n",
      "loss: 0.548495  [36800/175341]\n",
      "loss: 0.480599  [38400/175341]\n",
      "loss: 0.949331  [40000/175341]\n",
      "loss: 0.818699  [41600/175341]\n",
      "loss: 0.639781  [43200/175341]\n",
      "loss: 0.415172  [44800/175341]\n",
      "loss: 0.826549  [46400/175341]\n",
      "loss: 0.309676  [48000/175341]\n",
      "loss: 0.582373  [49600/175341]\n",
      "loss: 0.633412  [51200/175341]\n",
      "loss: 0.166990  [52800/175341]\n",
      "loss: 0.207131  [54400/175341]\n",
      "loss: 0.834887  [56000/175341]\n",
      "loss: 0.788528  [57600/175341]\n",
      "loss: 0.593190  [59200/175341]\n",
      "loss: 0.410853  [60800/175341]\n",
      "loss: 0.607486  [62400/175341]\n",
      "loss: 0.238885  [64000/175341]\n",
      "loss: 0.901632  [65600/175341]\n",
      "loss: 0.170092  [67200/175341]\n",
      "loss: 0.223507  [68800/175341]\n",
      "loss: 0.402417  [70400/175341]\n",
      "loss: 0.460942  [72000/175341]\n",
      "loss: 0.906168  [73600/175341]\n",
      "loss: 1.096613  [75200/175341]\n",
      "loss: 0.450830  [76800/175341]\n",
      "loss: 0.263455  [78400/175341]\n",
      "loss: 0.801764  [80000/175341]\n",
      "loss: 0.773818  [81600/175341]\n",
      "loss: 0.432061  [83200/175341]\n",
      "loss: 0.366104  [84800/175341]\n",
      "loss: 0.224305  [86400/175341]\n",
      "loss: 0.771462  [88000/175341]\n",
      "loss: 0.427045  [89600/175341]\n",
      "loss: 0.571601  [91200/175341]\n",
      "loss: 0.307215  [92800/175341]\n",
      "loss: 0.877736  [94400/175341]\n",
      "loss: 0.617617  [96000/175341]\n",
      "loss: 0.335735  [97600/175341]\n",
      "loss: 0.689475  [99200/175341]\n",
      "loss: 0.740848  [100800/175341]\n",
      "loss: 0.639000  [102400/175341]\n",
      "loss: 0.452282  [104000/175341]\n",
      "loss: 0.808932  [105600/175341]\n",
      "loss: 0.494963  [107200/175341]\n",
      "loss: 0.415882  [108800/175341]\n",
      "loss: 0.629248  [110400/175341]\n",
      "loss: 0.297247  [112000/175341]\n",
      "loss: 0.734649  [113600/175341]\n",
      "loss: 0.945796  [115200/175341]\n",
      "loss: 0.593550  [116800/175341]\n",
      "loss: 0.349917  [118400/175341]\n",
      "loss: 0.403238  [120000/175341]\n",
      "loss: 0.145441  [121600/175341]\n",
      "loss: 0.482686  [123200/175341]\n",
      "loss: 0.618342  [124800/175341]\n",
      "loss: 0.322433  [126400/175341]\n",
      "loss: 0.969269  [128000/175341]\n",
      "loss: 0.900696  [129600/175341]\n",
      "loss: 0.174889  [131200/175341]\n",
      "loss: 0.481963  [132800/175341]\n",
      "loss: 0.216739  [134400/175341]\n",
      "loss: 0.431062  [136000/175341]\n",
      "loss: 0.583528  [137600/175341]\n",
      "loss: 0.558688  [139200/175341]\n",
      "loss: 0.400352  [140800/175341]\n",
      "loss: 0.534927  [142400/175341]\n",
      "loss: 0.401672  [144000/175341]\n",
      "loss: 0.476549  [145600/175341]\n",
      "loss: 0.974802  [147200/175341]\n",
      "loss: 0.742841  [148800/175341]\n",
      "loss: 0.485629  [150400/175341]\n",
      "loss: 0.170920  [152000/175341]\n",
      "loss: 0.236419  [153600/175341]\n",
      "loss: 1.326810  [155200/175341]\n",
      "loss: 0.357019  [156800/175341]\n",
      "loss: 0.454105  [158400/175341]\n",
      "loss: 0.383133  [160000/175341]\n",
      "loss: 0.695498  [161600/175341]\n",
      "loss: 0.685237  [163200/175341]\n",
      "loss: 0.259033  [164800/175341]\n",
      "loss: 0.454810  [166400/175341]\n",
      "loss: 0.395055  [168000/175341]\n",
      "loss: 0.353505  [169600/175341]\n",
      "loss: 0.431501  [171200/175341]\n",
      "loss: 0.296598  [172800/175341]\n",
      "loss: 0.427938  [174400/175341]\n",
      "Train Accuracy: 79.4395%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.592210, F1-score: 73.77%, Macro_F1-Score:  38.29%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.466610  [    0/175341]\n",
      "loss: 0.488005  [ 1600/175341]\n",
      "loss: 0.865657  [ 3200/175341]\n",
      "loss: 0.341329  [ 4800/175341]\n",
      "loss: 0.295258  [ 6400/175341]\n",
      "loss: 0.602248  [ 8000/175341]\n",
      "loss: 0.292027  [ 9600/175341]\n",
      "loss: 0.762522  [11200/175341]\n",
      "loss: 0.206428  [12800/175341]\n",
      "loss: 0.948854  [14400/175341]\n",
      "loss: 0.882012  [16000/175341]\n",
      "loss: 0.396713  [17600/175341]\n",
      "loss: 0.683260  [19200/175341]\n",
      "loss: 0.648157  [20800/175341]\n",
      "loss: 0.218885  [22400/175341]\n",
      "loss: 0.407814  [24000/175341]\n",
      "loss: 0.501657  [25600/175341]\n",
      "loss: 0.618074  [27200/175341]\n",
      "loss: 0.304678  [28800/175341]\n",
      "loss: 0.406723  [30400/175341]\n",
      "loss: 0.536190  [32000/175341]\n",
      "loss: 1.075003  [33600/175341]\n",
      "loss: 0.271406  [35200/175341]\n",
      "loss: 0.531361  [36800/175341]\n",
      "loss: 0.390963  [38400/175341]\n",
      "loss: 0.558107  [40000/175341]\n",
      "loss: 0.976072  [41600/175341]\n",
      "loss: 0.615240  [43200/175341]\n",
      "loss: 0.377678  [44800/175341]\n",
      "loss: 0.843601  [46400/175341]\n",
      "loss: 0.371513  [48000/175341]\n",
      "loss: 0.250292  [49600/175341]\n",
      "loss: 0.507968  [51200/175341]\n",
      "loss: 0.422270  [52800/175341]\n",
      "loss: 0.360504  [54400/175341]\n",
      "loss: 0.417601  [56000/175341]\n",
      "loss: 0.441081  [57600/175341]\n",
      "loss: 0.587925  [59200/175341]\n",
      "loss: 0.335801  [60800/175341]\n",
      "loss: 0.221887  [62400/175341]\n",
      "loss: 0.247851  [64000/175341]\n",
      "loss: 0.467589  [65600/175341]\n",
      "loss: 0.757955  [67200/175341]\n",
      "loss: 0.538548  [68800/175341]\n",
      "loss: 0.457428  [70400/175341]\n",
      "loss: 0.383262  [72000/175341]\n",
      "loss: 0.283244  [73600/175341]\n",
      "loss: 0.382286  [75200/175341]\n",
      "loss: 0.839499  [76800/175341]\n",
      "loss: 0.421669  [78400/175341]\n",
      "loss: 0.889490  [80000/175341]\n",
      "loss: 0.686312  [81600/175341]\n",
      "loss: 0.418936  [83200/175341]\n",
      "loss: 0.572684  [84800/175341]\n",
      "loss: 0.243320  [86400/175341]\n",
      "loss: 0.248722  [88000/175341]\n",
      "loss: 0.585335  [89600/175341]\n",
      "loss: 0.569251  [91200/175341]\n",
      "loss: 0.410546  [92800/175341]\n",
      "loss: 0.703328  [94400/175341]\n",
      "loss: 0.428232  [96000/175341]\n",
      "loss: 0.719679  [97600/175341]\n",
      "loss: 0.992249  [99200/175341]\n",
      "loss: 0.497241  [100800/175341]\n",
      "loss: 0.544669  [102400/175341]\n",
      "loss: 0.536551  [104000/175341]\n",
      "loss: 0.277421  [105600/175341]\n",
      "loss: 0.361964  [107200/175341]\n",
      "loss: 1.033728  [108800/175341]\n",
      "loss: 0.925958  [110400/175341]\n",
      "loss: 0.283273  [112000/175341]\n",
      "loss: 0.276406  [113600/175341]\n",
      "loss: 0.603115  [115200/175341]\n",
      "loss: 0.682530  [116800/175341]\n",
      "loss: 0.370866  [118400/175341]\n",
      "loss: 0.758070  [120000/175341]\n",
      "loss: 0.315806  [121600/175341]\n",
      "loss: 0.833900  [123200/175341]\n",
      "loss: 0.225643  [124800/175341]\n",
      "loss: 0.979082  [126400/175341]\n",
      "loss: 0.545434  [128000/175341]\n",
      "loss: 0.250498  [129600/175341]\n",
      "loss: 0.511194  [131200/175341]\n",
      "loss: 0.349394  [132800/175341]\n",
      "loss: 0.507661  [134400/175341]\n",
      "loss: 0.338193  [136000/175341]\n",
      "loss: 0.369073  [137600/175341]\n",
      "loss: 0.570360  [139200/175341]\n",
      "loss: 0.506648  [140800/175341]\n",
      "loss: 0.663759  [142400/175341]\n",
      "loss: 0.381763  [144000/175341]\n",
      "loss: 0.184196  [145600/175341]\n",
      "loss: 0.579768  [147200/175341]\n",
      "loss: 0.467013  [148800/175341]\n",
      "loss: 0.076763  [150400/175341]\n",
      "loss: 0.523710  [152000/175341]\n",
      "loss: 0.287846  [153600/175341]\n",
      "loss: 0.373701  [155200/175341]\n",
      "loss: 0.469033  [156800/175341]\n",
      "loss: 0.816949  [158400/175341]\n",
      "loss: 0.558223  [160000/175341]\n",
      "loss: 0.439594  [161600/175341]\n",
      "loss: 0.404570  [163200/175341]\n",
      "loss: 0.808451  [164800/175341]\n",
      "loss: 0.356034  [166400/175341]\n",
      "loss: 0.365856  [168000/175341]\n",
      "loss: 1.010291  [169600/175341]\n",
      "loss: 0.386362  [171200/175341]\n",
      "loss: 0.428441  [172800/175341]\n",
      "loss: 0.319203  [174400/175341]\n",
      "Train Accuracy: 79.5804%\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.603572, F1-score: 72.99%, Macro_F1-Score:  38.29%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.008710  [    0/175341]\n",
      "loss: 0.877863  [ 1600/175341]\n",
      "loss: 0.254265  [ 3200/175341]\n",
      "loss: 0.750682  [ 4800/175341]\n",
      "loss: 0.935924  [ 6400/175341]\n",
      "loss: 0.452424  [ 8000/175341]\n",
      "loss: 0.551016  [ 9600/175341]\n",
      "loss: 1.009633  [11200/175341]\n",
      "loss: 0.406150  [12800/175341]\n",
      "loss: 0.375364  [14400/175341]\n",
      "loss: 0.905121  [16000/175341]\n",
      "loss: 0.393781  [17600/175341]\n",
      "loss: 0.736356  [19200/175341]\n",
      "loss: 0.549353  [20800/175341]\n",
      "loss: 0.888381  [22400/175341]\n",
      "loss: 0.576908  [24000/175341]\n",
      "loss: 0.587558  [25600/175341]\n",
      "loss: 0.347753  [27200/175341]\n",
      "loss: 0.583901  [28800/175341]\n",
      "loss: 0.384699  [30400/175341]\n",
      "loss: 0.657324  [32000/175341]\n",
      "loss: 0.261940  [33600/175341]\n",
      "loss: 0.384634  [35200/175341]\n",
      "loss: 0.606678  [36800/175341]\n",
      "loss: 0.734655  [38400/175341]\n",
      "loss: 0.551734  [40000/175341]\n",
      "loss: 0.759686  [41600/175341]\n",
      "loss: 0.503493  [43200/175341]\n",
      "loss: 0.550406  [44800/175341]\n",
      "loss: 0.439368  [46400/175341]\n",
      "loss: 0.381309  [48000/175341]\n",
      "loss: 0.160371  [49600/175341]\n",
      "loss: 0.578879  [51200/175341]\n",
      "loss: 0.438752  [52800/175341]\n",
      "loss: 0.727849  [54400/175341]\n",
      "loss: 0.281915  [56000/175341]\n",
      "loss: 0.510004  [57600/175341]\n",
      "loss: 0.233482  [59200/175341]\n",
      "loss: 0.223992  [60800/175341]\n",
      "loss: 0.393759  [62400/175341]\n",
      "loss: 0.422419  [64000/175341]\n",
      "loss: 0.402018  [65600/175341]\n",
      "loss: 0.338647  [67200/175341]\n",
      "loss: 0.308993  [68800/175341]\n",
      "loss: 0.256446  [70400/175341]\n",
      "loss: 0.421673  [72000/175341]\n",
      "loss: 0.787751  [73600/175341]\n",
      "loss: 0.067563  [75200/175341]\n",
      "loss: 0.432900  [76800/175341]\n",
      "loss: 0.415121  [78400/175341]\n",
      "loss: 0.454278  [80000/175341]\n",
      "loss: 0.274843  [81600/175341]\n",
      "loss: 0.293875  [83200/175341]\n",
      "loss: 0.937075  [84800/175341]\n",
      "loss: 0.512175  [86400/175341]\n",
      "loss: 0.655980  [88000/175341]\n",
      "loss: 0.593162  [89600/175341]\n",
      "loss: 0.964849  [91200/175341]\n",
      "loss: 0.414987  [92800/175341]\n",
      "loss: 0.314712  [94400/175341]\n",
      "loss: 0.680196  [96000/175341]\n",
      "loss: 0.387630  [97600/175341]\n",
      "loss: 0.522864  [99200/175341]\n",
      "loss: 0.473826  [100800/175341]\n",
      "loss: 0.574438  [102400/175341]\n",
      "loss: 0.316029  [104000/175341]\n",
      "loss: 0.458777  [105600/175341]\n",
      "loss: 0.695234  [107200/175341]\n",
      "loss: 0.573974  [108800/175341]\n",
      "loss: 0.386845  [110400/175341]\n",
      "loss: 0.367957  [112000/175341]\n",
      "loss: 0.240046  [113600/175341]\n",
      "loss: 0.647465  [115200/175341]\n",
      "loss: 0.354797  [116800/175341]\n",
      "loss: 0.189577  [118400/175341]\n",
      "loss: 0.425321  [120000/175341]\n",
      "loss: 0.535060  [121600/175341]\n",
      "loss: 0.334722  [123200/175341]\n",
      "loss: 0.890461  [124800/175341]\n",
      "loss: 0.378602  [126400/175341]\n",
      "loss: 0.621670  [128000/175341]\n",
      "loss: 0.326885  [129600/175341]\n",
      "loss: 0.423709  [131200/175341]\n",
      "loss: 0.684483  [132800/175341]\n",
      "loss: 1.130694  [134400/175341]\n",
      "loss: 0.581764  [136000/175341]\n",
      "loss: 0.448839  [137600/175341]\n",
      "loss: 0.165688  [139200/175341]\n",
      "loss: 0.311858  [140800/175341]\n",
      "loss: 0.572553  [142400/175341]\n",
      "loss: 0.368439  [144000/175341]\n",
      "loss: 0.544584  [145600/175341]\n",
      "loss: 0.587739  [147200/175341]\n",
      "loss: 0.714726  [148800/175341]\n",
      "loss: 0.579155  [150400/175341]\n",
      "loss: 0.240715  [152000/175341]\n",
      "loss: 0.371906  [153600/175341]\n",
      "loss: 0.217375  [155200/175341]\n",
      "loss: 0.898901  [156800/175341]\n",
      "loss: 0.285879  [158400/175341]\n",
      "loss: 0.860803  [160000/175341]\n",
      "loss: 0.696691  [161600/175341]\n",
      "loss: 0.539516  [163200/175341]\n",
      "loss: 0.431997  [164800/175341]\n",
      "loss: 0.465373  [166400/175341]\n",
      "loss: 0.350133  [168000/175341]\n",
      "loss: 0.457237  [169600/175341]\n",
      "loss: 0.414084  [171200/175341]\n",
      "loss: 1.074104  [172800/175341]\n",
      "loss: 0.675123  [174400/175341]\n",
      "Train Accuracy: 79.6636%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.588437, F1-score: 73.89%, Macro_F1-Score:  38.61%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.803249  [    0/175341]\n",
      "loss: 0.263752  [ 1600/175341]\n",
      "loss: 0.710891  [ 3200/175341]\n",
      "loss: 0.429078  [ 4800/175341]\n",
      "loss: 0.477609  [ 6400/175341]\n",
      "loss: 0.351853  [ 8000/175341]\n",
      "loss: 0.865054  [ 9600/175341]\n",
      "loss: 0.772622  [11200/175341]\n",
      "loss: 0.700436  [12800/175341]\n",
      "loss: 0.543338  [14400/175341]\n",
      "loss: 0.886206  [16000/175341]\n",
      "loss: 0.634477  [17600/175341]\n",
      "loss: 0.651570  [19200/175341]\n",
      "loss: 0.310929  [20800/175341]\n",
      "loss: 0.510929  [22400/175341]\n",
      "loss: 0.944678  [24000/175341]\n",
      "loss: 0.384602  [25600/175341]\n",
      "loss: 0.164596  [27200/175341]\n",
      "loss: 0.230888  [28800/175341]\n",
      "loss: 0.612209  [30400/175341]\n",
      "loss: 0.639088  [32000/175341]\n",
      "loss: 0.551986  [33600/175341]\n",
      "loss: 0.589383  [35200/175341]\n",
      "loss: 0.501613  [36800/175341]\n",
      "loss: 0.581621  [38400/175341]\n",
      "loss: 0.713307  [40000/175341]\n",
      "loss: 0.562728  [41600/175341]\n",
      "loss: 0.437920  [43200/175341]\n",
      "loss: 0.752007  [44800/175341]\n",
      "loss: 0.835585  [46400/175341]\n",
      "loss: 0.721229  [48000/175341]\n",
      "loss: 0.633957  [49600/175341]\n",
      "loss: 0.491196  [51200/175341]\n",
      "loss: 0.225308  [52800/175341]\n",
      "loss: 0.706600  [54400/175341]\n",
      "loss: 0.316014  [56000/175341]\n",
      "loss: 0.186453  [57600/175341]\n",
      "loss: 0.641919  [59200/175341]\n",
      "loss: 1.390179  [60800/175341]\n",
      "loss: 0.507810  [62400/175341]\n",
      "loss: 0.324466  [64000/175341]\n",
      "loss: 0.467174  [65600/175341]\n",
      "loss: 0.551999  [67200/175341]\n",
      "loss: 0.532689  [68800/175341]\n",
      "loss: 0.566011  [70400/175341]\n",
      "loss: 0.272108  [72000/175341]\n",
      "loss: 0.733877  [73600/175341]\n",
      "loss: 0.559479  [75200/175341]\n",
      "loss: 0.191991  [76800/175341]\n",
      "loss: 0.604029  [78400/175341]\n",
      "loss: 0.518739  [80000/175341]\n",
      "loss: 0.837000  [81600/175341]\n",
      "loss: 0.607144  [83200/175341]\n",
      "loss: 0.419201  [84800/175341]\n",
      "loss: 0.555723  [86400/175341]\n",
      "loss: 0.508381  [88000/175341]\n",
      "loss: 0.395194  [89600/175341]\n",
      "loss: 0.933117  [91200/175341]\n",
      "loss: 0.327145  [92800/175341]\n",
      "loss: 0.608882  [94400/175341]\n",
      "loss: 0.464097  [96000/175341]\n",
      "loss: 0.575303  [97600/175341]\n",
      "loss: 0.551879  [99200/175341]\n",
      "loss: 0.465432  [100800/175341]\n",
      "loss: 0.316276  [102400/175341]\n",
      "loss: 0.728910  [104000/175341]\n",
      "loss: 0.497240  [105600/175341]\n",
      "loss: 0.540547  [107200/175341]\n",
      "loss: 1.003197  [108800/175341]\n",
      "loss: 0.645824  [110400/175341]\n",
      "loss: 0.382197  [112000/175341]\n",
      "loss: 0.243021  [113600/175341]\n",
      "loss: 0.282903  [115200/175341]\n",
      "loss: 0.452988  [116800/175341]\n",
      "loss: 1.151111  [118400/175341]\n",
      "loss: 0.573054  [120000/175341]\n",
      "loss: 0.548007  [121600/175341]\n",
      "loss: 0.286433  [123200/175341]\n",
      "loss: 0.689562  [124800/175341]\n",
      "loss: 0.417019  [126400/175341]\n",
      "loss: 0.341805  [128000/175341]\n",
      "loss: 0.299244  [129600/175341]\n",
      "loss: 0.602203  [131200/175341]\n",
      "loss: 0.796396  [132800/175341]\n",
      "loss: 0.825429  [134400/175341]\n",
      "loss: 0.470741  [136000/175341]\n",
      "loss: 0.191863  [137600/175341]\n",
      "loss: 0.380448  [139200/175341]\n",
      "loss: 0.695355  [140800/175341]\n",
      "loss: 0.250050  [142400/175341]\n",
      "loss: 0.172322  [144000/175341]\n",
      "loss: 0.192137  [145600/175341]\n",
      "loss: 0.496164  [147200/175341]\n",
      "loss: 0.596337  [148800/175341]\n",
      "loss: 0.798442  [150400/175341]\n",
      "loss: 0.657699  [152000/175341]\n",
      "loss: 0.384844  [153600/175341]\n",
      "loss: 0.287265  [155200/175341]\n",
      "loss: 0.209875  [156800/175341]\n",
      "loss: 0.212302  [158400/175341]\n",
      "loss: 0.225292  [160000/175341]\n",
      "loss: 0.905549  [161600/175341]\n",
      "loss: 0.831042  [163200/175341]\n",
      "loss: 0.637573  [164800/175341]\n",
      "loss: 0.511082  [166400/175341]\n",
      "loss: 0.719011  [168000/175341]\n",
      "loss: 0.749340  [169600/175341]\n",
      "loss: 0.542032  [171200/175341]\n",
      "loss: 0.395161  [172800/175341]\n",
      "loss: 0.252344  [174400/175341]\n",
      "Train Accuracy: 79.6916%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.595064, F1-score: 73.47%, Macro_F1-Score:  38.56%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.700594  [    0/175341]\n",
      "loss: 0.337439  [ 1600/175341]\n",
      "loss: 0.275002  [ 3200/175341]\n",
      "loss: 0.513140  [ 4800/175341]\n",
      "loss: 0.266927  [ 6400/175341]\n",
      "loss: 0.315910  [ 8000/175341]\n",
      "loss: 0.591143  [ 9600/175341]\n",
      "loss: 0.561000  [11200/175341]\n",
      "loss: 0.426161  [12800/175341]\n",
      "loss: 0.253149  [14400/175341]\n",
      "loss: 0.497494  [16000/175341]\n",
      "loss: 0.223120  [17600/175341]\n",
      "loss: 0.283193  [19200/175341]\n",
      "loss: 0.545846  [20800/175341]\n",
      "loss: 0.278351  [22400/175341]\n",
      "loss: 0.372615  [24000/175341]\n",
      "loss: 0.486487  [25600/175341]\n",
      "loss: 0.360043  [27200/175341]\n",
      "loss: 0.565663  [28800/175341]\n",
      "loss: 0.460875  [30400/175341]\n",
      "loss: 0.348237  [32000/175341]\n",
      "loss: 0.114080  [33600/175341]\n",
      "loss: 0.588241  [35200/175341]\n",
      "loss: 0.857589  [36800/175341]\n",
      "loss: 0.631924  [38400/175341]\n",
      "loss: 0.675154  [40000/175341]\n",
      "loss: 0.511344  [41600/175341]\n",
      "loss: 0.675306  [43200/175341]\n",
      "loss: 0.744882  [44800/175341]\n",
      "loss: 0.720425  [46400/175341]\n",
      "loss: 0.837718  [48000/175341]\n",
      "loss: 0.423458  [49600/175341]\n",
      "loss: 0.454170  [51200/175341]\n",
      "loss: 0.472695  [52800/175341]\n",
      "loss: 0.825467  [54400/175341]\n",
      "loss: 0.629561  [56000/175341]\n",
      "loss: 0.387852  [57600/175341]\n",
      "loss: 0.548817  [59200/175341]\n",
      "loss: 0.859137  [60800/175341]\n",
      "loss: 0.424738  [62400/175341]\n",
      "loss: 0.288137  [64000/175341]\n",
      "loss: 0.514067  [65600/175341]\n",
      "loss: 0.577095  [67200/175341]\n",
      "loss: 0.608120  [68800/175341]\n",
      "loss: 0.464137  [70400/175341]\n",
      "loss: 0.299674  [72000/175341]\n",
      "loss: 0.403492  [73600/175341]\n",
      "loss: 0.464825  [75200/175341]\n",
      "loss: 0.479649  [76800/175341]\n",
      "loss: 0.690236  [78400/175341]\n",
      "loss: 0.387104  [80000/175341]\n",
      "loss: 0.331400  [81600/175341]\n",
      "loss: 0.246553  [83200/175341]\n",
      "loss: 0.537329  [84800/175341]\n",
      "loss: 0.478590  [86400/175341]\n",
      "loss: 0.319973  [88000/175341]\n",
      "loss: 0.193011  [89600/175341]\n",
      "loss: 0.758265  [91200/175341]\n",
      "loss: 0.906048  [92800/175341]\n",
      "loss: 0.393681  [94400/175341]\n",
      "loss: 0.715789  [96000/175341]\n",
      "loss: 0.662773  [97600/175341]\n",
      "loss: 0.755576  [99200/175341]\n",
      "loss: 0.407585  [100800/175341]\n",
      "loss: 0.787171  [102400/175341]\n",
      "loss: 0.525549  [104000/175341]\n",
      "loss: 0.866097  [105600/175341]\n",
      "loss: 0.213658  [107200/175341]\n",
      "loss: 1.142243  [108800/175341]\n",
      "loss: 0.697337  [110400/175341]\n",
      "loss: 0.859215  [112000/175341]\n",
      "loss: 0.271537  [113600/175341]\n",
      "loss: 0.642009  [115200/175341]\n",
      "loss: 0.584898  [116800/175341]\n",
      "loss: 0.307954  [118400/175341]\n",
      "loss: 0.892439  [120000/175341]\n",
      "loss: 0.965405  [121600/175341]\n",
      "loss: 0.619328  [123200/175341]\n",
      "loss: 0.172277  [124800/175341]\n",
      "loss: 0.882317  [126400/175341]\n",
      "loss: 0.405097  [128000/175341]\n",
      "loss: 0.158760  [129600/175341]\n",
      "loss: 0.444113  [131200/175341]\n",
      "loss: 0.638416  [132800/175341]\n",
      "loss: 0.249296  [134400/175341]\n",
      "loss: 0.481273  [136000/175341]\n",
      "loss: 0.287162  [137600/175341]\n",
      "loss: 0.789258  [139200/175341]\n",
      "loss: 0.396178  [140800/175341]\n",
      "loss: 0.326262  [142400/175341]\n",
      "loss: 0.526323  [144000/175341]\n",
      "loss: 0.812682  [145600/175341]\n",
      "loss: 0.346027  [147200/175341]\n",
      "loss: 0.931305  [148800/175341]\n",
      "loss: 0.628584  [150400/175341]\n",
      "loss: 0.482229  [152000/175341]\n",
      "loss: 0.409929  [153600/175341]\n",
      "loss: 0.675025  [155200/175341]\n",
      "loss: 0.934290  [156800/175341]\n",
      "loss: 0.406211  [158400/175341]\n",
      "loss: 0.316950  [160000/175341]\n",
      "loss: 0.883787  [161600/175341]\n",
      "loss: 0.118033  [163200/175341]\n",
      "loss: 0.279557  [164800/175341]\n",
      "loss: 0.245298  [166400/175341]\n",
      "loss: 0.266464  [168000/175341]\n",
      "loss: 0.571457  [169600/175341]\n",
      "loss: 0.491225  [171200/175341]\n",
      "loss: 0.237737  [172800/175341]\n",
      "loss: 0.581574  [174400/175341]\n",
      "Train Accuracy: 79.7805%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.574843, F1-score: 74.44%, Macro_F1-Score:  38.96%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.305298  [    0/175341]\n",
      "loss: 0.594631  [ 1600/175341]\n",
      "loss: 0.189526  [ 3200/175341]\n",
      "loss: 0.669262  [ 4800/175341]\n",
      "loss: 0.435138  [ 6400/175341]\n",
      "loss: 0.368021  [ 8000/175341]\n",
      "loss: 0.602381  [ 9600/175341]\n",
      "loss: 0.780609  [11200/175341]\n",
      "loss: 0.609295  [12800/175341]\n",
      "loss: 0.344655  [14400/175341]\n",
      "loss: 0.372692  [16000/175341]\n",
      "loss: 0.609451  [17600/175341]\n",
      "loss: 0.268403  [19200/175341]\n",
      "loss: 0.499707  [20800/175341]\n",
      "loss: 0.452211  [22400/175341]\n",
      "loss: 0.479592  [24000/175341]\n",
      "loss: 0.397838  [25600/175341]\n",
      "loss: 0.778500  [27200/175341]\n",
      "loss: 0.295427  [28800/175341]\n",
      "loss: 0.579510  [30400/175341]\n",
      "loss: 0.287455  [32000/175341]\n",
      "loss: 0.379764  [33600/175341]\n",
      "loss: 0.244637  [35200/175341]\n",
      "loss: 0.559543  [36800/175341]\n",
      "loss: 0.459176  [38400/175341]\n",
      "loss: 0.875895  [40000/175341]\n",
      "loss: 0.299493  [41600/175341]\n",
      "loss: 0.384456  [43200/175341]\n",
      "loss: 0.425829  [44800/175341]\n",
      "loss: 0.689473  [46400/175341]\n",
      "loss: 0.098580  [48000/175341]\n",
      "loss: 0.200448  [49600/175341]\n",
      "loss: 0.596822  [51200/175341]\n",
      "loss: 0.455652  [52800/175341]\n",
      "loss: 0.931250  [54400/175341]\n",
      "loss: 1.005152  [56000/175341]\n",
      "loss: 0.484304  [57600/175341]\n",
      "loss: 0.610708  [59200/175341]\n",
      "loss: 0.368041  [60800/175341]\n",
      "loss: 0.817801  [62400/175341]\n",
      "loss: 0.584092  [64000/175341]\n",
      "loss: 0.321032  [65600/175341]\n",
      "loss: 0.327732  [67200/175341]\n",
      "loss: 0.511292  [68800/175341]\n",
      "loss: 0.281436  [70400/175341]\n",
      "loss: 0.419483  [72000/175341]\n",
      "loss: 0.482462  [73600/175341]\n",
      "loss: 0.627870  [75200/175341]\n",
      "loss: 0.510361  [76800/175341]\n",
      "loss: 0.823276  [78400/175341]\n",
      "loss: 0.746317  [80000/175341]\n",
      "loss: 0.640446  [81600/175341]\n",
      "loss: 0.196677  [83200/175341]\n",
      "loss: 0.612255  [84800/175341]\n",
      "loss: 0.721322  [86400/175341]\n",
      "loss: 0.686450  [88000/175341]\n",
      "loss: 0.739322  [89600/175341]\n",
      "loss: 0.381685  [91200/175341]\n",
      "loss: 0.674773  [92800/175341]\n",
      "loss: 0.338529  [94400/175341]\n",
      "loss: 0.523039  [96000/175341]\n",
      "loss: 0.432457  [97600/175341]\n",
      "loss: 0.491402  [99200/175341]\n",
      "loss: 0.420098  [100800/175341]\n",
      "loss: 0.808431  [102400/175341]\n",
      "loss: 0.311642  [104000/175341]\n",
      "loss: 0.185047  [105600/175341]\n",
      "loss: 0.774480  [107200/175341]\n",
      "loss: 0.710783  [108800/175341]\n",
      "loss: 0.386191  [110400/175341]\n",
      "loss: 0.382789  [112000/175341]\n",
      "loss: 0.641154  [113600/175341]\n",
      "loss: 0.526591  [115200/175341]\n",
      "loss: 0.769496  [116800/175341]\n",
      "loss: 0.216034  [118400/175341]\n",
      "loss: 0.181735  [120000/175341]\n",
      "loss: 0.393984  [121600/175341]\n",
      "loss: 0.412484  [123200/175341]\n",
      "loss: 0.378427  [124800/175341]\n",
      "loss: 0.442596  [126400/175341]\n",
      "loss: 0.217082  [128000/175341]\n",
      "loss: 0.490300  [129600/175341]\n",
      "loss: 0.347085  [131200/175341]\n",
      "loss: 0.279071  [132800/175341]\n",
      "loss: 0.519598  [134400/175341]\n",
      "loss: 0.972132  [136000/175341]\n",
      "loss: 0.934650  [137600/175341]\n",
      "loss: 0.462111  [139200/175341]\n",
      "loss: 0.624135  [140800/175341]\n",
      "loss: 0.493478  [142400/175341]\n",
      "loss: 0.351088  [144000/175341]\n",
      "loss: 0.760533  [145600/175341]\n",
      "loss: 0.418720  [147200/175341]\n",
      "loss: 0.581401  [148800/175341]\n",
      "loss: 0.447168  [150400/175341]\n",
      "loss: 0.579956  [152000/175341]\n",
      "loss: 0.395110  [153600/175341]\n",
      "loss: 0.733235  [155200/175341]\n",
      "loss: 0.538173  [156800/175341]\n",
      "loss: 0.348783  [158400/175341]\n",
      "loss: 0.266279  [160000/175341]\n",
      "loss: 0.696171  [161600/175341]\n",
      "loss: 0.318132  [163200/175341]\n",
      "loss: 0.182676  [164800/175341]\n",
      "loss: 0.691168  [166400/175341]\n",
      "loss: 0.415053  [168000/175341]\n",
      "loss: 0.374979  [169600/175341]\n",
      "loss: 0.360053  [171200/175341]\n",
      "loss: 0.383399  [172800/175341]\n",
      "loss: 0.589979  [174400/175341]\n",
      "Train Accuracy: 79.8016%\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.599893, F1-score: 73.32%, Macro_F1-Score:  38.48%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.506966  [    0/175341]\n",
      "loss: 0.797318  [ 1600/175341]\n",
      "loss: 0.548277  [ 3200/175341]\n",
      "loss: 0.419808  [ 4800/175341]\n",
      "loss: 0.163730  [ 6400/175341]\n",
      "loss: 0.466865  [ 8000/175341]\n",
      "loss: 0.900488  [ 9600/175341]\n",
      "loss: 0.754570  [11200/175341]\n",
      "loss: 0.559829  [12800/175341]\n",
      "loss: 0.293306  [14400/175341]\n",
      "loss: 0.560015  [16000/175341]\n",
      "loss: 0.807833  [17600/175341]\n",
      "loss: 1.051166  [19200/175341]\n",
      "loss: 0.566379  [20800/175341]\n",
      "loss: 0.213194  [22400/175341]\n",
      "loss: 0.518007  [24000/175341]\n",
      "loss: 0.432310  [25600/175341]\n",
      "loss: 0.480362  [27200/175341]\n",
      "loss: 0.790493  [28800/175341]\n",
      "loss: 0.554987  [30400/175341]\n",
      "loss: 0.300754  [32000/175341]\n",
      "loss: 0.558372  [33600/175341]\n",
      "loss: 0.857270  [35200/175341]\n",
      "loss: 0.142224  [36800/175341]\n",
      "loss: 0.648599  [38400/175341]\n",
      "loss: 0.499261  [40000/175341]\n",
      "loss: 0.474548  [41600/175341]\n",
      "loss: 0.355132  [43200/175341]\n",
      "loss: 0.595266  [44800/175341]\n",
      "loss: 0.317458  [46400/175341]\n",
      "loss: 0.720070  [48000/175341]\n",
      "loss: 0.464623  [49600/175341]\n",
      "loss: 0.306124  [51200/175341]\n",
      "loss: 0.467305  [52800/175341]\n",
      "loss: 0.378818  [54400/175341]\n",
      "loss: 0.281858  [56000/175341]\n",
      "loss: 0.266174  [57600/175341]\n",
      "loss: 0.388576  [59200/175341]\n",
      "loss: 0.418940  [60800/175341]\n",
      "loss: 0.301852  [62400/175341]\n",
      "loss: 0.439633  [64000/175341]\n",
      "loss: 0.533235  [65600/175341]\n",
      "loss: 0.303489  [67200/175341]\n",
      "loss: 0.168726  [68800/175341]\n",
      "loss: 0.340308  [70400/175341]\n",
      "loss: 0.972422  [72000/175341]\n",
      "loss: 0.398024  [73600/175341]\n",
      "loss: 0.551567  [75200/175341]\n",
      "loss: 0.503831  [76800/175341]\n",
      "loss: 0.542587  [78400/175341]\n",
      "loss: 0.693901  [80000/175341]\n",
      "loss: 0.383413  [81600/175341]\n",
      "loss: 0.409351  [83200/175341]\n",
      "loss: 0.557814  [84800/175341]\n",
      "loss: 0.511772  [86400/175341]\n",
      "loss: 0.535626  [88000/175341]\n",
      "loss: 0.640832  [89600/175341]\n",
      "loss: 0.199966  [91200/175341]\n",
      "loss: 0.335315  [92800/175341]\n",
      "loss: 0.396237  [94400/175341]\n",
      "loss: 0.609602  [96000/175341]\n",
      "loss: 0.555062  [97600/175341]\n",
      "loss: 0.820207  [99200/175341]\n",
      "loss: 0.146691  [100800/175341]\n",
      "loss: 0.563428  [102400/175341]\n",
      "loss: 0.182616  [104000/175341]\n",
      "loss: 0.357349  [105600/175341]\n",
      "loss: 0.275230  [107200/175341]\n",
      "loss: 0.705028  [108800/175341]\n",
      "loss: 0.679751  [110400/175341]\n",
      "loss: 0.576450  [112000/175341]\n",
      "loss: 0.418169  [113600/175341]\n",
      "loss: 0.297812  [115200/175341]\n",
      "loss: 0.367348  [116800/175341]\n",
      "loss: 0.784297  [118400/175341]\n",
      "loss: 0.411910  [120000/175341]\n",
      "loss: 0.659958  [121600/175341]\n",
      "loss: 0.793851  [123200/175341]\n",
      "loss: 0.433315  [124800/175341]\n",
      "loss: 0.671669  [126400/175341]\n",
      "loss: 0.328805  [128000/175341]\n",
      "loss: 0.368015  [129600/175341]\n",
      "loss: 0.565383  [131200/175341]\n",
      "loss: 0.821509  [132800/175341]\n",
      "loss: 0.186789  [134400/175341]\n",
      "loss: 0.962037  [136000/175341]\n",
      "loss: 0.129052  [137600/175341]\n",
      "loss: 0.686949  [139200/175341]\n",
      "loss: 0.388355  [140800/175341]\n",
      "loss: 0.464934  [142400/175341]\n",
      "loss: 0.422624  [144000/175341]\n",
      "loss: 0.426057  [145600/175341]\n",
      "loss: 0.638642  [147200/175341]\n",
      "loss: 0.375870  [148800/175341]\n",
      "loss: 0.350730  [150400/175341]\n",
      "loss: 0.373163  [152000/175341]\n",
      "loss: 0.332285  [153600/175341]\n",
      "loss: 0.193036  [155200/175341]\n",
      "loss: 0.455146  [156800/175341]\n",
      "loss: 0.396335  [158400/175341]\n",
      "loss: 0.173841  [160000/175341]\n",
      "loss: 0.809786  [161600/175341]\n",
      "loss: 0.571975  [163200/175341]\n",
      "loss: 0.496161  [164800/175341]\n",
      "loss: 0.287016  [166400/175341]\n",
      "loss: 0.714035  [168000/175341]\n",
      "loss: 0.284571  [169600/175341]\n",
      "loss: 0.869422  [171200/175341]\n",
      "loss: 0.427344  [172800/175341]\n",
      "loss: 0.608606  [174400/175341]\n",
      "Train Accuracy: 79.9094%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.567926, F1-score: 75.50%, Macro_F1-Score:  39.21%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.471599  [    0/175341]\n",
      "loss: 0.404887  [ 1600/175341]\n",
      "loss: 0.845113  [ 3200/175341]\n",
      "loss: 1.044430  [ 4800/175341]\n",
      "loss: 0.435952  [ 6400/175341]\n",
      "loss: 0.412011  [ 8000/175341]\n",
      "loss: 0.795859  [ 9600/175341]\n",
      "loss: 0.591966  [11200/175341]\n",
      "loss: 0.299263  [12800/175341]\n",
      "loss: 0.159060  [14400/175341]\n",
      "loss: 0.402076  [16000/175341]\n",
      "loss: 0.931393  [17600/175341]\n",
      "loss: 0.596481  [19200/175341]\n",
      "loss: 0.801545  [20800/175341]\n",
      "loss: 0.268013  [22400/175341]\n",
      "loss: 0.745569  [24000/175341]\n",
      "loss: 0.583109  [25600/175341]\n",
      "loss: 0.582456  [27200/175341]\n",
      "loss: 1.210776  [28800/175341]\n",
      "loss: 0.182816  [30400/175341]\n",
      "loss: 0.370331  [32000/175341]\n",
      "loss: 0.801097  [33600/175341]\n",
      "loss: 0.445741  [35200/175341]\n",
      "loss: 0.534261  [36800/175341]\n",
      "loss: 0.348640  [38400/175341]\n",
      "loss: 0.289462  [40000/175341]\n",
      "loss: 0.316527  [41600/175341]\n",
      "loss: 0.424789  [43200/175341]\n",
      "loss: 0.177457  [44800/175341]\n",
      "loss: 0.480020  [46400/175341]\n",
      "loss: 0.587678  [48000/175341]\n",
      "loss: 0.340090  [49600/175341]\n",
      "loss: 0.544142  [51200/175341]\n",
      "loss: 0.415286  [52800/175341]\n",
      "loss: 0.343331  [54400/175341]\n",
      "loss: 0.762428  [56000/175341]\n",
      "loss: 0.204359  [57600/175341]\n",
      "loss: 0.885394  [59200/175341]\n",
      "loss: 0.852206  [60800/175341]\n",
      "loss: 0.731406  [62400/175341]\n",
      "loss: 0.272833  [64000/175341]\n",
      "loss: 0.646445  [65600/175341]\n",
      "loss: 0.601408  [67200/175341]\n",
      "loss: 0.658174  [68800/175341]\n",
      "loss: 0.529410  [70400/175341]\n",
      "loss: 0.734154  [72000/175341]\n",
      "loss: 0.516417  [73600/175341]\n",
      "loss: 0.152427  [75200/175341]\n",
      "loss: 0.775454  [76800/175341]\n",
      "loss: 0.275915  [78400/175341]\n",
      "loss: 0.844424  [80000/175341]\n",
      "loss: 0.467062  [81600/175341]\n",
      "loss: 0.913387  [83200/175341]\n",
      "loss: 0.637797  [84800/175341]\n",
      "loss: 0.623882  [86400/175341]\n",
      "loss: 0.550760  [88000/175341]\n",
      "loss: 0.766100  [89600/175341]\n",
      "loss: 0.590786  [91200/175341]\n",
      "loss: 0.283467  [92800/175341]\n",
      "loss: 0.364585  [94400/175341]\n",
      "loss: 0.510806  [96000/175341]\n",
      "loss: 0.741603  [97600/175341]\n",
      "loss: 0.279942  [99200/175341]\n",
      "loss: 0.356888  [100800/175341]\n",
      "loss: 0.356113  [102400/175341]\n",
      "loss: 0.475645  [104000/175341]\n",
      "loss: 0.355352  [105600/175341]\n",
      "loss: 0.743442  [107200/175341]\n",
      "loss: 0.545153  [108800/175341]\n",
      "loss: 0.466937  [110400/175341]\n",
      "loss: 0.257703  [112000/175341]\n",
      "loss: 1.107678  [113600/175341]\n",
      "loss: 0.227417  [115200/175341]\n",
      "loss: 0.349206  [116800/175341]\n",
      "loss: 0.943016  [118400/175341]\n",
      "loss: 0.544351  [120000/175341]\n",
      "loss: 0.456245  [121600/175341]\n",
      "loss: 0.555797  [123200/175341]\n",
      "loss: 0.407147  [124800/175341]\n",
      "loss: 0.293187  [126400/175341]\n",
      "loss: 0.425710  [128000/175341]\n",
      "loss: 0.340161  [129600/175341]\n",
      "loss: 0.450401  [131200/175341]\n",
      "loss: 0.492004  [132800/175341]\n",
      "loss: 0.616427  [134400/175341]\n",
      "loss: 0.496509  [136000/175341]\n",
      "loss: 0.163586  [137600/175341]\n",
      "loss: 0.274560  [139200/175341]\n",
      "loss: 0.661071  [140800/175341]\n",
      "loss: 0.904831  [142400/175341]\n",
      "loss: 0.353240  [144000/175341]\n",
      "loss: 0.261291  [145600/175341]\n",
      "loss: 0.542895  [147200/175341]\n",
      "loss: 0.380996  [148800/175341]\n",
      "loss: 0.373540  [150400/175341]\n",
      "loss: 1.148364  [152000/175341]\n",
      "loss: 0.270027  [153600/175341]\n",
      "loss: 0.506790  [155200/175341]\n",
      "loss: 0.767417  [156800/175341]\n",
      "loss: 0.578097  [158400/175341]\n",
      "loss: 0.437048  [160000/175341]\n",
      "loss: 0.589959  [161600/175341]\n",
      "loss: 0.658653  [163200/175341]\n",
      "loss: 0.822693  [164800/175341]\n",
      "loss: 0.513401  [166400/175341]\n",
      "loss: 0.513865  [168000/175341]\n",
      "loss: 0.481997  [169600/175341]\n",
      "loss: 0.710036  [171200/175341]\n",
      "loss: 0.840431  [172800/175341]\n",
      "loss: 0.119852  [174400/175341]\n",
      "Train Accuracy: 79.9374%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.578658, F1-score: 74.34%, Macro_F1-Score:  38.83%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.959317  [    0/175341]\n",
      "loss: 0.602620  [ 1600/175341]\n",
      "loss: 0.627320  [ 3200/175341]\n",
      "loss: 0.952905  [ 4800/175341]\n",
      "loss: 0.482782  [ 6400/175341]\n",
      "loss: 0.618201  [ 8000/175341]\n",
      "loss: 0.698017  [ 9600/175341]\n",
      "loss: 0.380403  [11200/175341]\n",
      "loss: 0.499121  [12800/175341]\n",
      "loss: 0.371279  [14400/175341]\n",
      "loss: 0.234343  [16000/175341]\n",
      "loss: 0.212549  [17600/175341]\n",
      "loss: 0.094871  [19200/175341]\n",
      "loss: 0.331738  [20800/175341]\n",
      "loss: 0.385351  [22400/175341]\n",
      "loss: 0.818901  [24000/175341]\n",
      "loss: 0.515027  [25600/175341]\n",
      "loss: 0.609969  [27200/175341]\n",
      "loss: 0.713010  [28800/175341]\n",
      "loss: 0.740568  [30400/175341]\n",
      "loss: 0.269440  [32000/175341]\n",
      "loss: 0.279672  [33600/175341]\n",
      "loss: 0.534142  [35200/175341]\n",
      "loss: 0.675527  [36800/175341]\n",
      "loss: 0.298988  [38400/175341]\n",
      "loss: 0.661622  [40000/175341]\n",
      "loss: 0.279719  [41600/175341]\n",
      "loss: 0.272430  [43200/175341]\n",
      "loss: 0.489783  [44800/175341]\n",
      "loss: 0.790331  [46400/175341]\n",
      "loss: 1.001439  [48000/175341]\n",
      "loss: 0.380025  [49600/175341]\n",
      "loss: 0.235712  [51200/175341]\n",
      "loss: 0.638825  [52800/175341]\n",
      "loss: 0.821133  [54400/175341]\n",
      "loss: 0.293897  [56000/175341]\n",
      "loss: 0.334760  [57600/175341]\n",
      "loss: 0.349518  [59200/175341]\n",
      "loss: 1.077593  [60800/175341]\n",
      "loss: 0.355910  [62400/175341]\n",
      "loss: 0.287232  [64000/175341]\n",
      "loss: 0.991175  [65600/175341]\n",
      "loss: 0.468096  [67200/175341]\n",
      "loss: 0.846490  [68800/175341]\n",
      "loss: 0.983615  [70400/175341]\n",
      "loss: 0.432681  [72000/175341]\n",
      "loss: 0.145305  [73600/175341]\n",
      "loss: 0.153455  [75200/175341]\n",
      "loss: 0.800775  [76800/175341]\n",
      "loss: 0.285797  [78400/175341]\n",
      "loss: 0.475896  [80000/175341]\n",
      "loss: 0.797439  [81600/175341]\n",
      "loss: 0.634093  [83200/175341]\n",
      "loss: 0.481125  [84800/175341]\n",
      "loss: 0.411795  [86400/175341]\n",
      "loss: 0.509283  [88000/175341]\n",
      "loss: 0.192661  [89600/175341]\n",
      "loss: 0.451943  [91200/175341]\n",
      "loss: 0.905617  [92800/175341]\n",
      "loss: 0.253313  [94400/175341]\n",
      "loss: 0.950996  [96000/175341]\n",
      "loss: 0.374979  [97600/175341]\n",
      "loss: 0.816584  [99200/175341]\n",
      "loss: 0.370874  [100800/175341]\n",
      "loss: 0.137628  [102400/175341]\n",
      "loss: 0.491086  [104000/175341]\n",
      "loss: 0.813297  [105600/175341]\n",
      "loss: 0.466036  [107200/175341]\n",
      "loss: 0.623750  [108800/175341]\n",
      "loss: 0.534603  [110400/175341]\n",
      "loss: 0.415312  [112000/175341]\n",
      "loss: 0.782534  [113600/175341]\n",
      "loss: 0.518522  [115200/175341]\n",
      "loss: 0.663130  [116800/175341]\n",
      "loss: 0.448025  [118400/175341]\n",
      "loss: 0.680937  [120000/175341]\n",
      "loss: 0.784209  [121600/175341]\n",
      "loss: 0.503680  [123200/175341]\n",
      "loss: 0.351722  [124800/175341]\n",
      "loss: 0.495060  [126400/175341]\n",
      "loss: 0.463636  [128000/175341]\n",
      "loss: 0.428145  [129600/175341]\n",
      "loss: 0.233634  [131200/175341]\n",
      "loss: 0.652793  [132800/175341]\n",
      "loss: 0.672310  [134400/175341]\n",
      "loss: 0.784349  [136000/175341]\n",
      "loss: 0.351845  [137600/175341]\n",
      "loss: 0.637467  [139200/175341]\n",
      "loss: 0.297449  [140800/175341]\n",
      "loss: 0.214372  [142400/175341]\n",
      "loss: 0.385643  [144000/175341]\n",
      "loss: 0.592816  [145600/175341]\n",
      "loss: 0.501271  [147200/175341]\n",
      "loss: 0.960238  [148800/175341]\n",
      "loss: 0.637556  [150400/175341]\n",
      "loss: 0.288541  [152000/175341]\n",
      "loss: 0.476806  [153600/175341]\n",
      "loss: 0.736374  [155200/175341]\n",
      "loss: 0.520701  [156800/175341]\n",
      "loss: 0.170252  [158400/175341]\n",
      "loss: 0.332162  [160000/175341]\n",
      "loss: 0.157040  [161600/175341]\n",
      "loss: 0.216599  [163200/175341]\n",
      "loss: 0.513304  [164800/175341]\n",
      "loss: 0.511501  [166400/175341]\n",
      "loss: 0.753162  [168000/175341]\n",
      "loss: 0.468580  [169600/175341]\n",
      "loss: 0.310150  [171200/175341]\n",
      "loss: 0.377903  [172800/175341]\n",
      "loss: 0.714172  [174400/175341]\n",
      "Train Accuracy: 80.0104%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.580882, F1-score: 74.32%, Macro_F1-Score:  39.18%  \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.607635  [    0/175341]\n",
      "loss: 0.344944  [ 1600/175341]\n",
      "loss: 0.368370  [ 3200/175341]\n",
      "loss: 0.241290  [ 4800/175341]\n",
      "loss: 0.304458  [ 6400/175341]\n",
      "loss: 0.406953  [ 8000/175341]\n",
      "loss: 0.348219  [ 9600/175341]\n",
      "loss: 0.874833  [11200/175341]\n",
      "loss: 1.095136  [12800/175341]\n",
      "loss: 0.940091  [14400/175341]\n",
      "loss: 0.754979  [16000/175341]\n",
      "loss: 0.732095  [17600/175341]\n",
      "loss: 0.442357  [19200/175341]\n",
      "loss: 0.308819  [20800/175341]\n",
      "loss: 0.363961  [22400/175341]\n",
      "loss: 0.752496  [24000/175341]\n",
      "loss: 0.406712  [25600/175341]\n",
      "loss: 0.811779  [27200/175341]\n",
      "loss: 0.646538  [28800/175341]\n",
      "loss: 0.488998  [30400/175341]\n",
      "loss: 0.443968  [32000/175341]\n",
      "loss: 0.233630  [33600/175341]\n",
      "loss: 0.439604  [35200/175341]\n",
      "loss: 0.381251  [36800/175341]\n",
      "loss: 0.687324  [38400/175341]\n",
      "loss: 0.493578  [40000/175341]\n",
      "loss: 0.601924  [41600/175341]\n",
      "loss: 0.631136  [43200/175341]\n",
      "loss: 0.217615  [44800/175341]\n",
      "loss: 0.334186  [46400/175341]\n",
      "loss: 0.416382  [48000/175341]\n",
      "loss: 0.685658  [49600/175341]\n",
      "loss: 0.216806  [51200/175341]\n",
      "loss: 0.413029  [52800/175341]\n",
      "loss: 0.376772  [54400/175341]\n",
      "loss: 0.816510  [56000/175341]\n",
      "loss: 0.674039  [57600/175341]\n",
      "loss: 0.725550  [59200/175341]\n",
      "loss: 0.428410  [60800/175341]\n",
      "loss: 0.739094  [62400/175341]\n",
      "loss: 0.353657  [64000/175341]\n",
      "loss: 0.227912  [65600/175341]\n",
      "loss: 0.629490  [67200/175341]\n",
      "loss: 0.763195  [68800/175341]\n",
      "loss: 0.621312  [70400/175341]\n",
      "loss: 0.320817  [72000/175341]\n",
      "loss: 0.283871  [73600/175341]\n",
      "loss: 0.929399  [75200/175341]\n",
      "loss: 0.202973  [76800/175341]\n",
      "loss: 0.505183  [78400/175341]\n",
      "loss: 0.600411  [80000/175341]\n",
      "loss: 0.377964  [81600/175341]\n",
      "loss: 0.274996  [83200/175341]\n",
      "loss: 1.057306  [84800/175341]\n",
      "loss: 0.554825  [86400/175341]\n",
      "loss: 0.654207  [88000/175341]\n",
      "loss: 0.503673  [89600/175341]\n",
      "loss: 0.265098  [91200/175341]\n",
      "loss: 0.334580  [92800/175341]\n",
      "loss: 0.314374  [94400/175341]\n",
      "loss: 0.232481  [96000/175341]\n",
      "loss: 0.620092  [97600/175341]\n",
      "loss: 0.559502  [99200/175341]\n",
      "loss: 0.470831  [100800/175341]\n",
      "loss: 0.527207  [102400/175341]\n",
      "loss: 0.783741  [104000/175341]\n",
      "loss: 0.254713  [105600/175341]\n",
      "loss: 0.483859  [107200/175341]\n",
      "loss: 0.745672  [108800/175341]\n",
      "loss: 0.373919  [110400/175341]\n",
      "loss: 0.646009  [112000/175341]\n",
      "loss: 0.425507  [113600/175341]\n",
      "loss: 0.427870  [115200/175341]\n",
      "loss: 0.813794  [116800/175341]\n",
      "loss: 0.284070  [118400/175341]\n",
      "loss: 1.136101  [120000/175341]\n",
      "loss: 0.112969  [121600/175341]\n",
      "loss: 0.354080  [123200/175341]\n",
      "loss: 0.623563  [124800/175341]\n",
      "loss: 0.607714  [126400/175341]\n",
      "loss: 0.212673  [128000/175341]\n",
      "loss: 0.552857  [129600/175341]\n",
      "loss: 0.827587  [131200/175341]\n",
      "loss: 0.626045  [132800/175341]\n",
      "loss: 0.429124  [134400/175341]\n",
      "loss: 0.473069  [136000/175341]\n",
      "loss: 0.580715  [137600/175341]\n",
      "loss: 0.422999  [139200/175341]\n",
      "loss: 0.236460  [140800/175341]\n",
      "loss: 0.118160  [142400/175341]\n",
      "loss: 0.608162  [144000/175341]\n",
      "loss: 0.841258  [145600/175341]\n",
      "loss: 0.410708  [147200/175341]\n",
      "loss: 0.778260  [148800/175341]\n",
      "loss: 0.277671  [150400/175341]\n",
      "loss: 0.702327  [152000/175341]\n",
      "loss: 0.415904  [153600/175341]\n",
      "loss: 0.832561  [155200/175341]\n",
      "loss: 0.298925  [156800/175341]\n",
      "loss: 0.350660  [158400/175341]\n",
      "loss: 0.803895  [160000/175341]\n",
      "loss: 0.809867  [161600/175341]\n",
      "loss: 0.539669  [163200/175341]\n",
      "loss: 0.666261  [164800/175341]\n",
      "loss: 0.822194  [166400/175341]\n",
      "loss: 0.485597  [168000/175341]\n",
      "loss: 0.548796  [169600/175341]\n",
      "loss: 0.290742  [171200/175341]\n",
      "loss: 0.886614  [172800/175341]\n",
      "loss: 0.210796  [174400/175341]\n",
      "Train Accuracy: 79.9984%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.581113, F1-score: 73.93%, Macro_F1-Score:  38.99%  \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.812220  [    0/175341]\n",
      "loss: 0.819975  [ 1600/175341]\n",
      "loss: 0.311004  [ 3200/175341]\n",
      "loss: 0.809919  [ 4800/175341]\n",
      "loss: 0.437741  [ 6400/175341]\n",
      "loss: 0.513322  [ 8000/175341]\n",
      "loss: 0.484563  [ 9600/175341]\n",
      "loss: 1.030102  [11200/175341]\n",
      "loss: 0.339171  [12800/175341]\n",
      "loss: 0.352134  [14400/175341]\n",
      "loss: 0.660683  [16000/175341]\n",
      "loss: 0.303435  [17600/175341]\n",
      "loss: 0.418092  [19200/175341]\n",
      "loss: 0.553233  [20800/175341]\n",
      "loss: 0.525312  [22400/175341]\n",
      "loss: 0.783249  [24000/175341]\n",
      "loss: 0.728810  [25600/175341]\n",
      "loss: 0.419628  [27200/175341]\n",
      "loss: 0.636801  [28800/175341]\n",
      "loss: 0.768148  [30400/175341]\n",
      "loss: 0.622598  [32000/175341]\n",
      "loss: 0.767823  [33600/175341]\n",
      "loss: 0.999019  [35200/175341]\n",
      "loss: 0.473983  [36800/175341]\n",
      "loss: 0.598264  [38400/175341]\n",
      "loss: 0.543521  [40000/175341]\n",
      "loss: 0.374225  [41600/175341]\n",
      "loss: 0.890125  [43200/175341]\n",
      "loss: 0.677001  [44800/175341]\n",
      "loss: 0.633535  [46400/175341]\n",
      "loss: 0.388993  [48000/175341]\n",
      "loss: 0.644094  [49600/175341]\n",
      "loss: 0.631238  [51200/175341]\n",
      "loss: 0.289192  [52800/175341]\n",
      "loss: 0.326690  [54400/175341]\n",
      "loss: 0.738401  [56000/175341]\n",
      "loss: 0.770062  [57600/175341]\n",
      "loss: 0.625824  [59200/175341]\n",
      "loss: 0.492253  [60800/175341]\n",
      "loss: 0.814917  [62400/175341]\n",
      "loss: 0.813330  [64000/175341]\n",
      "loss: 0.735067  [65600/175341]\n",
      "loss: 0.392682  [67200/175341]\n",
      "loss: 0.401780  [68800/175341]\n",
      "loss: 0.698593  [70400/175341]\n",
      "loss: 0.223673  [72000/175341]\n",
      "loss: 0.320789  [73600/175341]\n",
      "loss: 0.475135  [75200/175341]\n",
      "loss: 0.605477  [76800/175341]\n",
      "loss: 0.833730  [78400/175341]\n",
      "loss: 0.595506  [80000/175341]\n",
      "loss: 0.531436  [81600/175341]\n",
      "loss: 0.141831  [83200/175341]\n",
      "loss: 0.422780  [84800/175341]\n",
      "loss: 0.621475  [86400/175341]\n",
      "loss: 0.277986  [88000/175341]\n",
      "loss: 0.536730  [89600/175341]\n",
      "loss: 0.612580  [91200/175341]\n",
      "loss: 0.473958  [92800/175341]\n",
      "loss: 0.395710  [94400/175341]\n",
      "loss: 0.562294  [96000/175341]\n",
      "loss: 0.550228  [97600/175341]\n",
      "loss: 0.576321  [99200/175341]\n",
      "loss: 0.382218  [100800/175341]\n",
      "loss: 0.615294  [102400/175341]\n",
      "loss: 0.533045  [104000/175341]\n",
      "loss: 0.592246  [105600/175341]\n",
      "loss: 0.555270  [107200/175341]\n",
      "loss: 0.215487  [108800/175341]\n",
      "loss: 0.616823  [110400/175341]\n",
      "loss: 0.158985  [112000/175341]\n",
      "loss: 0.424543  [113600/175341]\n",
      "loss: 0.110192  [115200/175341]\n",
      "loss: 0.443871  [116800/175341]\n",
      "loss: 0.393618  [118400/175341]\n",
      "loss: 0.624143  [120000/175341]\n",
      "loss: 0.641950  [121600/175341]\n",
      "loss: 0.657218  [123200/175341]\n",
      "loss: 0.614257  [124800/175341]\n",
      "loss: 0.568057  [126400/175341]\n",
      "loss: 0.710694  [128000/175341]\n",
      "loss: 0.756009  [129600/175341]\n",
      "loss: 0.244105  [131200/175341]\n",
      "loss: 0.791071  [132800/175341]\n",
      "loss: 0.914658  [134400/175341]\n",
      "loss: 0.420869  [136000/175341]\n",
      "loss: 0.282596  [137600/175341]\n",
      "loss: 0.198028  [139200/175341]\n",
      "loss: 0.612079  [140800/175341]\n",
      "loss: 0.264901  [142400/175341]\n",
      "loss: 0.497359  [144000/175341]\n",
      "loss: 0.282373  [145600/175341]\n",
      "loss: 0.321324  [147200/175341]\n",
      "loss: 0.854828  [148800/175341]\n",
      "loss: 0.568381  [150400/175341]\n",
      "loss: 0.537854  [152000/175341]\n",
      "loss: 0.529080  [153600/175341]\n",
      "loss: 0.200764  [155200/175341]\n",
      "loss: 0.549883  [156800/175341]\n",
      "loss: 0.340952  [158400/175341]\n",
      "loss: 0.205930  [160000/175341]\n",
      "loss: 0.360032  [161600/175341]\n",
      "loss: 0.795232  [163200/175341]\n",
      "loss: 0.581460  [164800/175341]\n",
      "loss: 0.604207  [166400/175341]\n",
      "loss: 0.686448  [168000/175341]\n",
      "loss: 0.431644  [169600/175341]\n",
      "loss: 0.550074  [171200/175341]\n",
      "loss: 0.242354  [172800/175341]\n",
      "loss: 0.811232  [174400/175341]\n",
      "Train Accuracy: 80.0440%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.584056, F1-score: 74.04%, Macro_F1-Score:  39.25%  \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.424667  [    0/175341]\n",
      "loss: 0.209508  [ 1600/175341]\n",
      "loss: 0.276305  [ 3200/175341]\n",
      "loss: 0.515496  [ 4800/175341]\n",
      "loss: 0.569146  [ 6400/175341]\n",
      "loss: 0.386817  [ 8000/175341]\n",
      "loss: 0.413581  [ 9600/175341]\n",
      "loss: 0.462716  [11200/175341]\n",
      "loss: 0.619035  [12800/175341]\n",
      "loss: 0.370041  [14400/175341]\n",
      "loss: 0.613927  [16000/175341]\n",
      "loss: 0.986430  [17600/175341]\n",
      "loss: 0.241048  [19200/175341]\n",
      "loss: 0.244893  [20800/175341]\n",
      "loss: 0.317975  [22400/175341]\n",
      "loss: 0.342617  [24000/175341]\n",
      "loss: 0.569356  [25600/175341]\n",
      "loss: 0.542276  [27200/175341]\n",
      "loss: 0.327147  [28800/175341]\n",
      "loss: 0.342101  [30400/175341]\n",
      "loss: 1.185560  [32000/175341]\n",
      "loss: 0.448855  [33600/175341]\n",
      "loss: 0.696625  [35200/175341]\n",
      "loss: 0.629971  [36800/175341]\n",
      "loss: 0.430793  [38400/175341]\n",
      "loss: 0.373438  [40000/175341]\n",
      "loss: 0.703697  [41600/175341]\n",
      "loss: 0.694275  [43200/175341]\n",
      "loss: 0.639465  [44800/175341]\n",
      "loss: 0.435006  [46400/175341]\n",
      "loss: 0.711833  [48000/175341]\n",
      "loss: 0.659089  [49600/175341]\n",
      "loss: 0.428240  [51200/175341]\n",
      "loss: 0.619741  [52800/175341]\n",
      "loss: 0.685109  [54400/175341]\n",
      "loss: 0.749443  [56000/175341]\n",
      "loss: 0.521379  [57600/175341]\n",
      "loss: 0.800332  [59200/175341]\n",
      "loss: 0.568310  [60800/175341]\n",
      "loss: 0.294279  [62400/175341]\n",
      "loss: 0.862005  [64000/175341]\n",
      "loss: 0.736169  [65600/175341]\n",
      "loss: 0.436009  [67200/175341]\n",
      "loss: 0.605406  [68800/175341]\n",
      "loss: 0.476437  [70400/175341]\n",
      "loss: 0.345826  [72000/175341]\n",
      "loss: 0.419226  [73600/175341]\n",
      "loss: 0.612990  [75200/175341]\n",
      "loss: 0.577526  [76800/175341]\n",
      "loss: 0.276479  [78400/175341]\n",
      "loss: 0.662425  [80000/175341]\n",
      "loss: 0.704051  [81600/175341]\n",
      "loss: 0.320339  [83200/175341]\n",
      "loss: 0.309749  [84800/175341]\n",
      "loss: 0.349793  [86400/175341]\n",
      "loss: 0.472904  [88000/175341]\n",
      "loss: 0.255329  [89600/175341]\n",
      "loss: 0.547822  [91200/175341]\n",
      "loss: 0.833655  [92800/175341]\n",
      "loss: 0.571551  [94400/175341]\n",
      "loss: 0.191909  [96000/175341]\n",
      "loss: 0.873535  [97600/175341]\n",
      "loss: 0.982331  [99200/175341]\n",
      "loss: 0.289772  [100800/175341]\n",
      "loss: 0.150055  [102400/175341]\n",
      "loss: 0.237703  [104000/175341]\n",
      "loss: 0.424289  [105600/175341]\n",
      "loss: 0.754386  [107200/175341]\n",
      "loss: 0.393514  [108800/175341]\n",
      "loss: 0.347974  [110400/175341]\n",
      "loss: 0.368047  [112000/175341]\n",
      "loss: 0.642859  [113600/175341]\n",
      "loss: 0.335517  [115200/175341]\n",
      "loss: 0.253579  [116800/175341]\n",
      "loss: 0.559566  [118400/175341]\n",
      "loss: 0.613337  [120000/175341]\n",
      "loss: 0.284199  [121600/175341]\n",
      "loss: 0.821845  [123200/175341]\n",
      "loss: 0.346639  [124800/175341]\n",
      "loss: 0.375709  [126400/175341]\n",
      "loss: 0.679441  [128000/175341]\n",
      "loss: 0.829971  [129600/175341]\n",
      "loss: 0.478970  [131200/175341]\n",
      "loss: 0.412897  [132800/175341]\n",
      "loss: 0.732852  [134400/175341]\n",
      "loss: 0.352354  [136000/175341]\n",
      "loss: 0.351762  [137600/175341]\n",
      "loss: 0.166318  [139200/175341]\n",
      "loss: 0.404644  [140800/175341]\n",
      "loss: 0.309232  [142400/175341]\n",
      "loss: 0.431710  [144000/175341]\n",
      "loss: 0.156220  [145600/175341]\n",
      "loss: 0.196687  [147200/175341]\n",
      "loss: 0.438064  [148800/175341]\n",
      "loss: 0.239120  [150400/175341]\n",
      "loss: 0.496502  [152000/175341]\n",
      "loss: 0.465817  [153600/175341]\n",
      "loss: 0.399184  [155200/175341]\n",
      "loss: 0.301491  [156800/175341]\n",
      "loss: 0.734021  [158400/175341]\n",
      "loss: 0.292051  [160000/175341]\n",
      "loss: 0.874521  [161600/175341]\n",
      "loss: 0.390459  [163200/175341]\n",
      "loss: 0.251091  [164800/175341]\n",
      "loss: 0.533543  [166400/175341]\n",
      "loss: 0.200714  [168000/175341]\n",
      "loss: 0.638780  [169600/175341]\n",
      "loss: 0.494819  [171200/175341]\n",
      "loss: 0.349430  [172800/175341]\n",
      "loss: 0.448594  [174400/175341]\n",
      "Train Accuracy: 80.0817%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.575803, F1-score: 74.13%, Macro_F1-Score:  38.88%  \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.451878  [    0/175341]\n",
      "loss: 0.533046  [ 1600/175341]\n",
      "loss: 0.108730  [ 3200/175341]\n",
      "loss: 0.310824  [ 4800/175341]\n",
      "loss: 0.894628  [ 6400/175341]\n",
      "loss: 0.708561  [ 8000/175341]\n",
      "loss: 0.520338  [ 9600/175341]\n",
      "loss: 0.353689  [11200/175341]\n",
      "loss: 0.605711  [12800/175341]\n",
      "loss: 0.368734  [14400/175341]\n",
      "loss: 0.748411  [16000/175341]\n",
      "loss: 0.491735  [17600/175341]\n",
      "loss: 0.646262  [19200/175341]\n",
      "loss: 0.738455  [20800/175341]\n",
      "loss: 0.345398  [22400/175341]\n",
      "loss: 0.463733  [24000/175341]\n",
      "loss: 0.404077  [25600/175341]\n",
      "loss: 0.617199  [27200/175341]\n",
      "loss: 0.528697  [28800/175341]\n",
      "loss: 0.493311  [30400/175341]\n",
      "loss: 0.143256  [32000/175341]\n",
      "loss: 0.231808  [33600/175341]\n",
      "loss: 0.623723  [35200/175341]\n",
      "loss: 0.177596  [36800/175341]\n",
      "loss: 1.032805  [38400/175341]\n",
      "loss: 0.311275  [40000/175341]\n",
      "loss: 0.371916  [41600/175341]\n",
      "loss: 0.329430  [43200/175341]\n",
      "loss: 0.252977  [44800/175341]\n",
      "loss: 0.447119  [46400/175341]\n",
      "loss: 0.539822  [48000/175341]\n",
      "loss: 0.587942  [49600/175341]\n",
      "loss: 0.523611  [51200/175341]\n",
      "loss: 0.244355  [52800/175341]\n",
      "loss: 0.453999  [54400/175341]\n",
      "loss: 0.995196  [56000/175341]\n",
      "loss: 0.457216  [57600/175341]\n",
      "loss: 0.534980  [59200/175341]\n",
      "loss: 0.271381  [60800/175341]\n",
      "loss: 0.616613  [62400/175341]\n",
      "loss: 0.356402  [64000/175341]\n",
      "loss: 0.313748  [65600/175341]\n",
      "loss: 0.396528  [67200/175341]\n",
      "loss: 0.964072  [68800/175341]\n",
      "loss: 0.277863  [70400/175341]\n",
      "loss: 0.295235  [72000/175341]\n",
      "loss: 0.707767  [73600/175341]\n",
      "loss: 0.304361  [75200/175341]\n",
      "loss: 0.489528  [76800/175341]\n",
      "loss: 0.447493  [78400/175341]\n",
      "loss: 0.500693  [80000/175341]\n",
      "loss: 0.265712  [81600/175341]\n",
      "loss: 0.189867  [83200/175341]\n",
      "loss: 0.405818  [84800/175341]\n",
      "loss: 0.353961  [86400/175341]\n",
      "loss: 0.987035  [88000/175341]\n",
      "loss: 0.685066  [89600/175341]\n",
      "loss: 0.456606  [91200/175341]\n",
      "loss: 0.686630  [92800/175341]\n",
      "loss: 0.701805  [94400/175341]\n",
      "loss: 0.665158  [96000/175341]\n",
      "loss: 0.337092  [97600/175341]\n",
      "loss: 0.846506  [99200/175341]\n",
      "loss: 0.441195  [100800/175341]\n",
      "loss: 0.457278  [102400/175341]\n",
      "loss: 0.511619  [104000/175341]\n",
      "loss: 0.590184  [105600/175341]\n",
      "loss: 0.906705  [107200/175341]\n",
      "loss: 0.591765  [108800/175341]\n",
      "loss: 0.377714  [110400/175341]\n",
      "loss: 0.593537  [112000/175341]\n",
      "loss: 0.907322  [113600/175341]\n",
      "loss: 0.659119  [115200/175341]\n",
      "loss: 0.343587  [116800/175341]\n",
      "loss: 0.337999  [118400/175341]\n",
      "loss: 0.691859  [120000/175341]\n",
      "loss: 0.481919  [121600/175341]\n",
      "loss: 0.966710  [123200/175341]\n",
      "loss: 0.674574  [124800/175341]\n",
      "loss: 0.649646  [126400/175341]\n",
      "loss: 0.708798  [128000/175341]\n",
      "loss: 0.657788  [129600/175341]\n",
      "loss: 0.567704  [131200/175341]\n",
      "loss: 0.289849  [132800/175341]\n",
      "loss: 0.576421  [134400/175341]\n",
      "loss: 0.254788  [136000/175341]\n",
      "loss: 0.819500  [137600/175341]\n",
      "loss: 0.704292  [139200/175341]\n",
      "loss: 0.203093  [140800/175341]\n",
      "loss: 0.431909  [142400/175341]\n",
      "loss: 0.241630  [144000/175341]\n",
      "loss: 0.264065  [145600/175341]\n",
      "loss: 0.426439  [147200/175341]\n",
      "loss: 0.553665  [148800/175341]\n",
      "loss: 0.534337  [150400/175341]\n",
      "loss: 0.573000  [152000/175341]\n",
      "loss: 0.217559  [153600/175341]\n",
      "loss: 0.684410  [155200/175341]\n",
      "loss: 0.599932  [156800/175341]\n",
      "loss: 0.552461  [158400/175341]\n",
      "loss: 0.546994  [160000/175341]\n",
      "loss: 0.458397  [161600/175341]\n",
      "loss: 0.776615  [163200/175341]\n",
      "loss: 0.418017  [164800/175341]\n",
      "loss: 0.947561  [166400/175341]\n",
      "loss: 0.599296  [168000/175341]\n",
      "loss: 0.568552  [169600/175341]\n",
      "loss: 0.614362  [171200/175341]\n",
      "loss: 0.651092  [172800/175341]\n",
      "loss: 0.478733  [174400/175341]\n",
      "Train Accuracy: 80.1102%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.574974, F1-score: 74.06%, Macro_F1-Score:  39.62%  \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.464830  [    0/175341]\n",
      "loss: 0.539979  [ 1600/175341]\n",
      "loss: 0.408697  [ 3200/175341]\n",
      "loss: 0.596190  [ 4800/175341]\n",
      "loss: 0.954419  [ 6400/175341]\n",
      "loss: 0.453543  [ 8000/175341]\n",
      "loss: 0.430415  [ 9600/175341]\n",
      "loss: 0.373725  [11200/175341]\n",
      "loss: 0.425012  [12800/175341]\n",
      "loss: 0.642482  [14400/175341]\n",
      "loss: 0.584901  [16000/175341]\n",
      "loss: 0.718439  [17600/175341]\n",
      "loss: 0.265779  [19200/175341]\n",
      "loss: 0.564516  [20800/175341]\n",
      "loss: 0.477083  [22400/175341]\n",
      "loss: 0.771929  [24000/175341]\n",
      "loss: 0.656623  [25600/175341]\n",
      "loss: 0.792924  [27200/175341]\n",
      "loss: 0.439375  [28800/175341]\n",
      "loss: 0.421404  [30400/175341]\n",
      "loss: 0.363260  [32000/175341]\n",
      "loss: 0.377154  [33600/175341]\n",
      "loss: 0.475941  [35200/175341]\n",
      "loss: 0.251511  [36800/175341]\n",
      "loss: 0.497799  [38400/175341]\n",
      "loss: 0.653742  [40000/175341]\n",
      "loss: 0.409311  [41600/175341]\n",
      "loss: 0.542756  [43200/175341]\n",
      "loss: 0.296726  [44800/175341]\n",
      "loss: 0.537293  [46400/175341]\n",
      "loss: 0.457652  [48000/175341]\n",
      "loss: 0.771171  [49600/175341]\n",
      "loss: 0.185909  [51200/175341]\n",
      "loss: 0.656167  [52800/175341]\n",
      "loss: 0.453824  [54400/175341]\n",
      "loss: 0.338997  [56000/175341]\n",
      "loss: 0.553138  [57600/175341]\n",
      "loss: 0.657280  [59200/175341]\n",
      "loss: 0.762506  [60800/175341]\n",
      "loss: 0.549317  [62400/175341]\n",
      "loss: 0.482554  [64000/175341]\n",
      "loss: 0.235514  [65600/175341]\n",
      "loss: 0.633509  [67200/175341]\n",
      "loss: 0.221543  [68800/175341]\n",
      "loss: 0.555807  [70400/175341]\n",
      "loss: 0.488943  [72000/175341]\n",
      "loss: 0.521752  [73600/175341]\n",
      "loss: 0.441136  [75200/175341]\n",
      "loss: 0.300897  [76800/175341]\n",
      "loss: 0.239440  [78400/175341]\n",
      "loss: 0.152640  [80000/175341]\n",
      "loss: 0.652686  [81600/175341]\n",
      "loss: 0.791323  [83200/175341]\n",
      "loss: 0.437089  [84800/175341]\n",
      "loss: 0.205880  [86400/175341]\n",
      "loss: 0.608429  [88000/175341]\n",
      "loss: 0.434262  [89600/175341]\n",
      "loss: 0.423730  [91200/175341]\n",
      "loss: 0.333817  [92800/175341]\n",
      "loss: 0.652063  [94400/175341]\n",
      "loss: 0.438674  [96000/175341]\n",
      "loss: 0.444510  [97600/175341]\n",
      "loss: 0.218165  [99200/175341]\n",
      "loss: 0.485645  [100800/175341]\n",
      "loss: 0.223556  [102400/175341]\n",
      "loss: 0.489791  [104000/175341]\n",
      "loss: 0.623943  [105600/175341]\n",
      "loss: 0.923064  [107200/175341]\n",
      "loss: 0.914013  [108800/175341]\n",
      "loss: 0.617822  [110400/175341]\n",
      "loss: 0.396667  [112000/175341]\n",
      "loss: 0.574320  [113600/175341]\n",
      "loss: 1.077285  [115200/175341]\n",
      "loss: 0.451605  [116800/175341]\n",
      "loss: 0.706832  [118400/175341]\n",
      "loss: 0.411510  [120000/175341]\n",
      "loss: 0.494996  [121600/175341]\n",
      "loss: 0.309276  [123200/175341]\n",
      "loss: 0.277822  [124800/175341]\n",
      "loss: 0.587715  [126400/175341]\n",
      "loss: 0.638289  [128000/175341]\n",
      "loss: 0.215817  [129600/175341]\n",
      "loss: 0.321934  [131200/175341]\n",
      "loss: 0.756790  [132800/175341]\n",
      "loss: 0.750575  [134400/175341]\n",
      "loss: 0.250637  [136000/175341]\n",
      "loss: 0.438819  [137600/175341]\n",
      "loss: 0.294282  [139200/175341]\n",
      "loss: 0.451215  [140800/175341]\n",
      "loss: 0.500827  [142400/175341]\n",
      "loss: 0.401026  [144000/175341]\n",
      "loss: 1.044423  [145600/175341]\n",
      "loss: 0.386939  [147200/175341]\n",
      "loss: 0.692852  [148800/175341]\n",
      "loss: 0.643992  [150400/175341]\n",
      "loss: 0.556223  [152000/175341]\n",
      "loss: 0.566302  [153600/175341]\n",
      "loss: 1.037570  [155200/175341]\n",
      "loss: 0.602789  [156800/175341]\n",
      "loss: 0.372189  [158400/175341]\n",
      "loss: 0.909341  [160000/175341]\n",
      "loss: 0.528542  [161600/175341]\n",
      "loss: 0.324245  [163200/175341]\n",
      "loss: 0.398891  [164800/175341]\n",
      "loss: 0.341676  [166400/175341]\n",
      "loss: 0.198406  [168000/175341]\n",
      "loss: 0.169208  [169600/175341]\n",
      "loss: 0.383280  [171200/175341]\n",
      "loss: 0.357747  [172800/175341]\n",
      "loss: 0.472994  [174400/175341]\n",
      "Train Accuracy: 80.1832%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.551243, F1-score: 75.80%, Macro_F1-Score:  40.00%  \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.969029  [    0/175341]\n",
      "loss: 0.786610  [ 1600/175341]\n",
      "loss: 0.268686  [ 3200/175341]\n",
      "loss: 0.430461  [ 4800/175341]\n",
      "loss: 0.308432  [ 6400/175341]\n",
      "loss: 0.490665  [ 8000/175341]\n",
      "loss: 0.324287  [ 9600/175341]\n",
      "loss: 0.473575  [11200/175341]\n",
      "loss: 0.743259  [12800/175341]\n",
      "loss: 0.685163  [14400/175341]\n",
      "loss: 0.211847  [16000/175341]\n",
      "loss: 0.230936  [17600/175341]\n",
      "loss: 0.302825  [19200/175341]\n",
      "loss: 0.559833  [20800/175341]\n",
      "loss: 0.378856  [22400/175341]\n",
      "loss: 0.205629  [24000/175341]\n",
      "loss: 0.687162  [25600/175341]\n",
      "loss: 0.453316  [27200/175341]\n",
      "loss: 0.351114  [28800/175341]\n",
      "loss: 0.330721  [30400/175341]\n",
      "loss: 0.375226  [32000/175341]\n",
      "loss: 0.555776  [33600/175341]\n",
      "loss: 0.445847  [35200/175341]\n",
      "loss: 0.248924  [36800/175341]\n",
      "loss: 0.379532  [38400/175341]\n",
      "loss: 0.429687  [40000/175341]\n",
      "loss: 0.353836  [41600/175341]\n",
      "loss: 0.504644  [43200/175341]\n",
      "loss: 0.394813  [44800/175341]\n",
      "loss: 0.626996  [46400/175341]\n",
      "loss: 0.528221  [48000/175341]\n",
      "loss: 0.849654  [49600/175341]\n",
      "loss: 0.351701  [51200/175341]\n",
      "loss: 0.419447  [52800/175341]\n",
      "loss: 0.903628  [54400/175341]\n",
      "loss: 0.762737  [56000/175341]\n",
      "loss: 0.568824  [57600/175341]\n",
      "loss: 0.528112  [59200/175341]\n",
      "loss: 0.461995  [60800/175341]\n",
      "loss: 0.317646  [62400/175341]\n",
      "loss: 0.242963  [64000/175341]\n",
      "loss: 0.685535  [65600/175341]\n",
      "loss: 0.277363  [67200/175341]\n",
      "loss: 0.426494  [68800/175341]\n",
      "loss: 0.246707  [70400/175341]\n",
      "loss: 0.466914  [72000/175341]\n",
      "loss: 0.518940  [73600/175341]\n",
      "loss: 0.463750  [75200/175341]\n",
      "loss: 0.638240  [76800/175341]\n",
      "loss: 0.939289  [78400/175341]\n",
      "loss: 0.522105  [80000/175341]\n",
      "loss: 0.249632  [81600/175341]\n",
      "loss: 0.163857  [83200/175341]\n",
      "loss: 0.656588  [84800/175341]\n",
      "loss: 0.439238  [86400/175341]\n",
      "loss: 0.339227  [88000/175341]\n",
      "loss: 0.508073  [89600/175341]\n",
      "loss: 0.291615  [91200/175341]\n",
      "loss: 0.625327  [92800/175341]\n",
      "loss: 0.636985  [94400/175341]\n",
      "loss: 0.651461  [96000/175341]\n",
      "loss: 0.587779  [97600/175341]\n",
      "loss: 0.322128  [99200/175341]\n",
      "loss: 0.638853  [100800/175341]\n",
      "loss: 0.575414  [102400/175341]\n",
      "loss: 0.314450  [104000/175341]\n",
      "loss: 0.530151  [105600/175341]\n",
      "loss: 0.181371  [107200/175341]\n",
      "loss: 0.406606  [108800/175341]\n",
      "loss: 0.833576  [110400/175341]\n",
      "loss: 0.308491  [112000/175341]\n",
      "loss: 0.327984  [113600/175341]\n",
      "loss: 0.303215  [115200/175341]\n",
      "loss: 1.134557  [116800/175341]\n",
      "loss: 0.655723  [118400/175341]\n",
      "loss: 0.271254  [120000/175341]\n",
      "loss: 0.504277  [121600/175341]\n",
      "loss: 0.684420  [123200/175341]\n",
      "loss: 0.688935  [124800/175341]\n",
      "loss: 0.336424  [126400/175341]\n",
      "loss: 0.456624  [128000/175341]\n",
      "loss: 0.332720  [129600/175341]\n",
      "loss: 0.831586  [131200/175341]\n",
      "loss: 0.387874  [132800/175341]\n",
      "loss: 0.641567  [134400/175341]\n",
      "loss: 0.580970  [136000/175341]\n",
      "loss: 0.455470  [137600/175341]\n",
      "loss: 0.766471  [139200/175341]\n",
      "loss: 0.477202  [140800/175341]\n",
      "loss: 0.750475  [142400/175341]\n",
      "loss: 0.549279  [144000/175341]\n",
      "loss: 0.343613  [145600/175341]\n",
      "loss: 0.636097  [147200/175341]\n",
      "loss: 0.227231  [148800/175341]\n",
      "loss: 0.411525  [150400/175341]\n",
      "loss: 0.367549  [152000/175341]\n",
      "loss: 0.514774  [153600/175341]\n",
      "loss: 0.372369  [155200/175341]\n",
      "loss: 0.313071  [156800/175341]\n",
      "loss: 0.416722  [158400/175341]\n",
      "loss: 0.881419  [160000/175341]\n",
      "loss: 0.273489  [161600/175341]\n",
      "loss: 0.734879  [163200/175341]\n",
      "loss: 0.498935  [164800/175341]\n",
      "loss: 0.665078  [166400/175341]\n",
      "loss: 0.209400  [168000/175341]\n",
      "loss: 0.931259  [169600/175341]\n",
      "loss: 0.538682  [171200/175341]\n",
      "loss: 0.534628  [172800/175341]\n",
      "loss: 0.417014  [174400/175341]\n",
      "Train Accuracy: 80.1935%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.554289, F1-score: 75.52%, Macro_F1-Score:  39.73%  \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.156554  [    0/175341]\n",
      "loss: 0.546604  [ 1600/175341]\n",
      "loss: 0.404154  [ 3200/175341]\n",
      "loss: 0.552649  [ 4800/175341]\n",
      "loss: 0.288647  [ 6400/175341]\n",
      "loss: 0.632296  [ 8000/175341]\n",
      "loss: 0.926796  [ 9600/175341]\n",
      "loss: 0.985699  [11200/175341]\n",
      "loss: 0.461591  [12800/175341]\n",
      "loss: 0.719318  [14400/175341]\n",
      "loss: 0.641090  [16000/175341]\n",
      "loss: 0.647483  [17600/175341]\n",
      "loss: 0.226672  [19200/175341]\n",
      "loss: 1.003680  [20800/175341]\n",
      "loss: 0.700338  [22400/175341]\n",
      "loss: 0.383852  [24000/175341]\n",
      "loss: 0.676401  [25600/175341]\n",
      "loss: 0.582499  [27200/175341]\n",
      "loss: 0.387773  [28800/175341]\n",
      "loss: 0.546712  [30400/175341]\n",
      "loss: 0.694775  [32000/175341]\n",
      "loss: 0.904428  [33600/175341]\n",
      "loss: 0.141239  [35200/175341]\n",
      "loss: 0.375111  [36800/175341]\n",
      "loss: 0.550635  [38400/175341]\n",
      "loss: 0.385366  [40000/175341]\n",
      "loss: 0.405981  [41600/175341]\n",
      "loss: 0.401240  [43200/175341]\n",
      "loss: 0.544340  [44800/175341]\n",
      "loss: 0.573977  [46400/175341]\n",
      "loss: 0.713035  [48000/175341]\n",
      "loss: 0.781547  [49600/175341]\n",
      "loss: 0.267175  [51200/175341]\n",
      "loss: 0.240737  [52800/175341]\n",
      "loss: 0.554882  [54400/175341]\n",
      "loss: 0.332020  [56000/175341]\n",
      "loss: 0.479712  [57600/175341]\n",
      "loss: 0.654372  [59200/175341]\n",
      "loss: 0.283661  [60800/175341]\n",
      "loss: 0.622747  [62400/175341]\n",
      "loss: 0.677323  [64000/175341]\n",
      "loss: 0.568126  [65600/175341]\n",
      "loss: 0.251808  [67200/175341]\n",
      "loss: 0.121542  [68800/175341]\n",
      "loss: 0.256472  [70400/175341]\n",
      "loss: 0.287328  [72000/175341]\n",
      "loss: 0.232631  [73600/175341]\n",
      "loss: 0.490544  [75200/175341]\n",
      "loss: 0.444081  [76800/175341]\n",
      "loss: 0.610188  [78400/175341]\n",
      "loss: 0.682352  [80000/175341]\n",
      "loss: 0.652973  [81600/175341]\n",
      "loss: 0.703064  [83200/175341]\n",
      "loss: 0.188272  [84800/175341]\n",
      "loss: 0.480090  [86400/175341]\n",
      "loss: 0.349925  [88000/175341]\n",
      "loss: 0.707670  [89600/175341]\n",
      "loss: 0.336160  [91200/175341]\n",
      "loss: 0.169855  [92800/175341]\n",
      "loss: 0.346341  [94400/175341]\n",
      "loss: 0.286586  [96000/175341]\n",
      "loss: 0.180462  [97600/175341]\n",
      "loss: 0.577806  [99200/175341]\n",
      "loss: 0.893829  [100800/175341]\n",
      "loss: 0.452496  [102400/175341]\n",
      "loss: 0.493509  [104000/175341]\n",
      "loss: 0.612169  [105600/175341]\n",
      "loss: 0.318508  [107200/175341]\n",
      "loss: 0.509191  [108800/175341]\n",
      "loss: 0.257005  [110400/175341]\n",
      "loss: 0.082110  [112000/175341]\n",
      "loss: 0.898175  [113600/175341]\n",
      "loss: 0.319772  [115200/175341]\n",
      "loss: 0.827317  [116800/175341]\n",
      "loss: 0.972508  [118400/175341]\n",
      "loss: 0.169635  [120000/175341]\n",
      "loss: 0.760071  [121600/175341]\n",
      "loss: 0.960562  [123200/175341]\n",
      "loss: 0.539104  [124800/175341]\n",
      "loss: 0.455097  [126400/175341]\n",
      "loss: 0.683013  [128000/175341]\n",
      "loss: 0.520473  [129600/175341]\n",
      "loss: 0.384819  [131200/175341]\n",
      "loss: 0.196976  [132800/175341]\n",
      "loss: 0.569589  [134400/175341]\n",
      "loss: 0.413127  [136000/175341]\n",
      "loss: 0.332486  [137600/175341]\n",
      "loss: 0.365998  [139200/175341]\n",
      "loss: 0.611244  [140800/175341]\n",
      "loss: 0.609857  [142400/175341]\n",
      "loss: 0.780791  [144000/175341]\n",
      "loss: 0.573158  [145600/175341]\n",
      "loss: 0.396288  [147200/175341]\n",
      "loss: 0.597600  [148800/175341]\n",
      "loss: 0.706786  [150400/175341]\n",
      "loss: 0.841566  [152000/175341]\n",
      "loss: 0.574703  [153600/175341]\n",
      "loss: 0.379169  [155200/175341]\n",
      "loss: 0.365910  [156800/175341]\n",
      "loss: 0.358896  [158400/175341]\n",
      "loss: 0.550785  [160000/175341]\n",
      "loss: 0.523610  [161600/175341]\n",
      "loss: 0.479554  [163200/175341]\n",
      "loss: 0.392447  [164800/175341]\n",
      "loss: 0.755804  [166400/175341]\n",
      "loss: 0.367088  [168000/175341]\n",
      "loss: 0.201179  [169600/175341]\n",
      "loss: 0.510106  [171200/175341]\n",
      "loss: 0.717778  [172800/175341]\n",
      "loss: 0.374718  [174400/175341]\n",
      "Train Accuracy: 80.2128%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.573087, F1-score: 74.49%, Macro_F1-Score:  39.20%  \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.387665  [    0/175341]\n",
      "loss: 0.701276  [ 1600/175341]\n",
      "loss: 0.877545  [ 3200/175341]\n",
      "loss: 0.613228  [ 4800/175341]\n",
      "loss: 0.786909  [ 6400/175341]\n",
      "loss: 0.434762  [ 8000/175341]\n",
      "loss: 0.624256  [ 9600/175341]\n",
      "loss: 0.557869  [11200/175341]\n",
      "loss: 0.396154  [12800/175341]\n",
      "loss: 1.040193  [14400/175341]\n",
      "loss: 0.711543  [16000/175341]\n",
      "loss: 0.387547  [17600/175341]\n",
      "loss: 0.692061  [19200/175341]\n",
      "loss: 0.829780  [20800/175341]\n",
      "loss: 0.804342  [22400/175341]\n",
      "loss: 0.389022  [24000/175341]\n",
      "loss: 0.676693  [25600/175341]\n",
      "loss: 0.295067  [27200/175341]\n",
      "loss: 0.435281  [28800/175341]\n",
      "loss: 0.295040  [30400/175341]\n",
      "loss: 0.406969  [32000/175341]\n",
      "loss: 0.652772  [33600/175341]\n",
      "loss: 0.920559  [35200/175341]\n",
      "loss: 0.652087  [36800/175341]\n",
      "loss: 0.728380  [38400/175341]\n",
      "loss: 0.168254  [40000/175341]\n",
      "loss: 0.565649  [41600/175341]\n",
      "loss: 1.199606  [43200/175341]\n",
      "loss: 0.253031  [44800/175341]\n",
      "loss: 0.497396  [46400/175341]\n",
      "loss: 0.424890  [48000/175341]\n",
      "loss: 0.879669  [49600/175341]\n",
      "loss: 0.959381  [51200/175341]\n",
      "loss: 0.420643  [52800/175341]\n",
      "loss: 0.435834  [54400/175341]\n",
      "loss: 0.647026  [56000/175341]\n",
      "loss: 0.434317  [57600/175341]\n",
      "loss: 0.550841  [59200/175341]\n",
      "loss: 0.915845  [60800/175341]\n",
      "loss: 0.569339  [62400/175341]\n",
      "loss: 0.354673  [64000/175341]\n",
      "loss: 0.557917  [65600/175341]\n",
      "loss: 0.397328  [67200/175341]\n",
      "loss: 0.198860  [68800/175341]\n",
      "loss: 0.371829  [70400/175341]\n",
      "loss: 0.707613  [72000/175341]\n",
      "loss: 0.300810  [73600/175341]\n",
      "loss: 0.789352  [75200/175341]\n",
      "loss: 0.873155  [76800/175341]\n",
      "loss: 0.531275  [78400/175341]\n",
      "loss: 0.566945  [80000/175341]\n",
      "loss: 0.749692  [81600/175341]\n",
      "loss: 0.399357  [83200/175341]\n",
      "loss: 0.251446  [84800/175341]\n",
      "loss: 0.166042  [86400/175341]\n",
      "loss: 0.380819  [88000/175341]\n",
      "loss: 0.599546  [89600/175341]\n",
      "loss: 0.567226  [91200/175341]\n",
      "loss: 0.543221  [92800/175341]\n",
      "loss: 0.592158  [94400/175341]\n",
      "loss: 0.760159  [96000/175341]\n",
      "loss: 0.464098  [97600/175341]\n",
      "loss: 0.730872  [99200/175341]\n",
      "loss: 0.428637  [100800/175341]\n",
      "loss: 0.372214  [102400/175341]\n",
      "loss: 0.322393  [104000/175341]\n",
      "loss: 0.624293  [105600/175341]\n",
      "loss: 0.309808  [107200/175341]\n",
      "loss: 0.706110  [108800/175341]\n",
      "loss: 0.584623  [110400/175341]\n",
      "loss: 0.652487  [112000/175341]\n",
      "loss: 0.802783  [113600/175341]\n",
      "loss: 0.466816  [115200/175341]\n",
      "loss: 0.911741  [116800/175341]\n",
      "loss: 0.603701  [118400/175341]\n",
      "loss: 0.637119  [120000/175341]\n",
      "loss: 0.408253  [121600/175341]\n",
      "loss: 0.437373  [123200/175341]\n",
      "loss: 0.464111  [124800/175341]\n",
      "loss: 0.437875  [126400/175341]\n",
      "loss: 0.130430  [128000/175341]\n",
      "loss: 0.759579  [129600/175341]\n",
      "loss: 0.368550  [131200/175341]\n",
      "loss: 0.472461  [132800/175341]\n",
      "loss: 0.545326  [134400/175341]\n",
      "loss: 0.173712  [136000/175341]\n",
      "loss: 0.308352  [137600/175341]\n",
      "loss: 0.566640  [139200/175341]\n",
      "loss: 0.348681  [140800/175341]\n",
      "loss: 0.213794  [142400/175341]\n",
      "loss: 0.567573  [144000/175341]\n",
      "loss: 0.227306  [145600/175341]\n",
      "loss: 0.534965  [147200/175341]\n",
      "loss: 0.860128  [148800/175341]\n",
      "loss: 0.460993  [150400/175341]\n",
      "loss: 0.545759  [152000/175341]\n",
      "loss: 0.780449  [153600/175341]\n",
      "loss: 1.241494  [155200/175341]\n",
      "loss: 0.716615  [156800/175341]\n",
      "loss: 0.277332  [158400/175341]\n",
      "loss: 0.676719  [160000/175341]\n",
      "loss: 0.787166  [161600/175341]\n",
      "loss: 0.902767  [163200/175341]\n",
      "loss: 0.866476  [164800/175341]\n",
      "loss: 0.513927  [166400/175341]\n",
      "loss: 0.554163  [168000/175341]\n",
      "loss: 0.362082  [169600/175341]\n",
      "loss: 0.574412  [171200/175341]\n",
      "loss: 0.456126  [172800/175341]\n",
      "loss: 0.402264  [174400/175341]\n",
      "Train Accuracy: 80.2220%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.563665, F1-score: 74.84%, Macro_F1-Score:  39.60%  \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.492106  [    0/175341]\n",
      "loss: 0.383685  [ 1600/175341]\n",
      "loss: 0.664748  [ 3200/175341]\n",
      "loss: 0.452551  [ 4800/175341]\n",
      "loss: 0.212913  [ 6400/175341]\n",
      "loss: 0.314122  [ 8000/175341]\n",
      "loss: 0.583288  [ 9600/175341]\n",
      "loss: 0.506134  [11200/175341]\n",
      "loss: 0.654489  [12800/175341]\n",
      "loss: 0.805369  [14400/175341]\n",
      "loss: 0.316335  [16000/175341]\n",
      "loss: 0.497380  [17600/175341]\n",
      "loss: 0.461148  [19200/175341]\n",
      "loss: 0.191967  [20800/175341]\n",
      "loss: 0.683697  [22400/175341]\n",
      "loss: 0.385862  [24000/175341]\n",
      "loss: 0.407593  [25600/175341]\n",
      "loss: 0.464977  [27200/175341]\n",
      "loss: 0.259877  [28800/175341]\n",
      "loss: 0.574357  [30400/175341]\n",
      "loss: 0.263917  [32000/175341]\n",
      "loss: 0.515100  [33600/175341]\n",
      "loss: 0.874226  [35200/175341]\n",
      "loss: 0.471110  [36800/175341]\n",
      "loss: 0.310445  [38400/175341]\n",
      "loss: 0.138650  [40000/175341]\n",
      "loss: 0.435203  [41600/175341]\n",
      "loss: 0.880981  [43200/175341]\n",
      "loss: 0.905410  [44800/175341]\n",
      "loss: 0.551200  [46400/175341]\n",
      "loss: 0.683080  [48000/175341]\n",
      "loss: 0.292786  [49600/175341]\n",
      "loss: 0.391577  [51200/175341]\n",
      "loss: 0.583109  [52800/175341]\n",
      "loss: 0.405226  [54400/175341]\n",
      "loss: 0.386225  [56000/175341]\n",
      "loss: 0.368390  [57600/175341]\n",
      "loss: 0.425526  [59200/175341]\n",
      "loss: 0.506819  [60800/175341]\n",
      "loss: 0.377808  [62400/175341]\n",
      "loss: 0.617832  [64000/175341]\n",
      "loss: 0.384336  [65600/175341]\n",
      "loss: 0.411803  [67200/175341]\n",
      "loss: 0.726477  [68800/175341]\n",
      "loss: 0.345448  [70400/175341]\n",
      "loss: 0.359747  [72000/175341]\n",
      "loss: 0.659556  [73600/175341]\n",
      "loss: 0.389119  [75200/175341]\n",
      "loss: 0.426316  [76800/175341]\n",
      "loss: 0.393163  [78400/175341]\n",
      "loss: 0.291973  [80000/175341]\n",
      "loss: 0.275921  [81600/175341]\n",
      "loss: 0.637073  [83200/175341]\n",
      "loss: 0.542295  [84800/175341]\n",
      "loss: 0.553917  [86400/175341]\n",
      "loss: 0.222146  [88000/175341]\n",
      "loss: 0.490543  [89600/175341]\n",
      "loss: 0.133752  [91200/175341]\n",
      "loss: 0.520493  [92800/175341]\n",
      "loss: 0.311130  [94400/175341]\n",
      "loss: 0.746939  [96000/175341]\n",
      "loss: 0.533518  [97600/175341]\n",
      "loss: 0.676584  [99200/175341]\n",
      "loss: 0.619255  [100800/175341]\n",
      "loss: 0.478774  [102400/175341]\n",
      "loss: 0.582279  [104000/175341]\n",
      "loss: 0.356597  [105600/175341]\n",
      "loss: 0.753327  [107200/175341]\n",
      "loss: 0.737647  [108800/175341]\n",
      "loss: 0.488082  [110400/175341]\n",
      "loss: 0.513775  [112000/175341]\n",
      "loss: 0.441640  [113600/175341]\n",
      "loss: 0.415418  [115200/175341]\n",
      "loss: 0.295343  [116800/175341]\n",
      "loss: 0.641342  [118400/175341]\n",
      "loss: 0.487650  [120000/175341]\n",
      "loss: 0.312797  [121600/175341]\n",
      "loss: 0.250981  [123200/175341]\n",
      "loss: 0.578166  [124800/175341]\n",
      "loss: 0.606909  [126400/175341]\n",
      "loss: 0.835447  [128000/175341]\n",
      "loss: 0.393312  [129600/175341]\n",
      "loss: 0.256957  [131200/175341]\n",
      "loss: 0.332846  [132800/175341]\n",
      "loss: 0.353509  [134400/175341]\n",
      "loss: 0.415516  [136000/175341]\n",
      "loss: 0.385433  [137600/175341]\n",
      "loss: 0.361016  [139200/175341]\n",
      "loss: 0.315860  [140800/175341]\n",
      "loss: 0.176332  [142400/175341]\n",
      "loss: 0.734192  [144000/175341]\n",
      "loss: 0.415415  [145600/175341]\n",
      "loss: 0.099394  [147200/175341]\n",
      "loss: 0.181133  [148800/175341]\n",
      "loss: 0.584781  [150400/175341]\n",
      "loss: 0.106152  [152000/175341]\n",
      "loss: 0.497828  [153600/175341]\n",
      "loss: 0.243663  [155200/175341]\n",
      "loss: 0.199298  [156800/175341]\n",
      "loss: 0.543275  [158400/175341]\n",
      "loss: 0.623105  [160000/175341]\n",
      "loss: 0.139936  [161600/175341]\n",
      "loss: 0.543111  [163200/175341]\n",
      "loss: 0.546178  [164800/175341]\n",
      "loss: 0.347942  [166400/175341]\n",
      "loss: 0.709251  [168000/175341]\n",
      "loss: 0.522823  [169600/175341]\n",
      "loss: 0.373694  [171200/175341]\n",
      "loss: 0.341608  [172800/175341]\n",
      "loss: 0.282272  [174400/175341]\n",
      "Train Accuracy: 80.2408%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.558860, F1-score: 75.42%, Macro_F1-Score:  39.52%  \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.311006  [    0/175341]\n",
      "loss: 0.233565  [ 1600/175341]\n",
      "loss: 0.145824  [ 3200/175341]\n",
      "loss: 0.341769  [ 4800/175341]\n",
      "loss: 0.491283  [ 6400/175341]\n",
      "loss: 0.336996  [ 8000/175341]\n",
      "loss: 0.884810  [ 9600/175341]\n",
      "loss: 0.335162  [11200/175341]\n",
      "loss: 0.467100  [12800/175341]\n",
      "loss: 0.676948  [14400/175341]\n",
      "loss: 0.298404  [16000/175341]\n",
      "loss: 0.293929  [17600/175341]\n",
      "loss: 0.250318  [19200/175341]\n",
      "loss: 0.505783  [20800/175341]\n",
      "loss: 0.785153  [22400/175341]\n",
      "loss: 0.646345  [24000/175341]\n",
      "loss: 0.423218  [25600/175341]\n",
      "loss: 0.304518  [27200/175341]\n",
      "loss: 0.327430  [28800/175341]\n",
      "loss: 0.510060  [30400/175341]\n",
      "loss: 0.347815  [32000/175341]\n",
      "loss: 0.497271  [33600/175341]\n",
      "loss: 0.567927  [35200/175341]\n",
      "loss: 0.769327  [36800/175341]\n",
      "loss: 0.534874  [38400/175341]\n",
      "loss: 0.298379  [40000/175341]\n",
      "loss: 0.434841  [41600/175341]\n",
      "loss: 0.235610  [43200/175341]\n",
      "loss: 0.485136  [44800/175341]\n",
      "loss: 0.812575  [46400/175341]\n",
      "loss: 1.004910  [48000/175341]\n",
      "loss: 0.815933  [49600/175341]\n",
      "loss: 0.518841  [51200/175341]\n",
      "loss: 0.272026  [52800/175341]\n",
      "loss: 0.677931  [54400/175341]\n",
      "loss: 0.395452  [56000/175341]\n",
      "loss: 0.712471  [57600/175341]\n",
      "loss: 0.318185  [59200/175341]\n",
      "loss: 0.165079  [60800/175341]\n",
      "loss: 0.831381  [62400/175341]\n",
      "loss: 0.524794  [64000/175341]\n",
      "loss: 0.252740  [65600/175341]\n",
      "loss: 0.326695  [67200/175341]\n",
      "loss: 0.307341  [68800/175341]\n",
      "loss: 0.224181  [70400/175341]\n",
      "loss: 0.648903  [72000/175341]\n",
      "loss: 0.672527  [73600/175341]\n",
      "loss: 0.464542  [75200/175341]\n",
      "loss: 0.761758  [76800/175341]\n",
      "loss: 0.704675  [78400/175341]\n",
      "loss: 0.760084  [80000/175341]\n",
      "loss: 0.270590  [81600/175341]\n",
      "loss: 0.793505  [83200/175341]\n",
      "loss: 0.346553  [84800/175341]\n",
      "loss: 0.698739  [86400/175341]\n",
      "loss: 0.621364  [88000/175341]\n",
      "loss: 0.464711  [89600/175341]\n",
      "loss: 0.547600  [91200/175341]\n",
      "loss: 0.527396  [92800/175341]\n",
      "loss: 1.011270  [94400/175341]\n",
      "loss: 0.427398  [96000/175341]\n",
      "loss: 0.762405  [97600/175341]\n",
      "loss: 0.465882  [99200/175341]\n",
      "loss: 0.654238  [100800/175341]\n",
      "loss: 0.471483  [102400/175341]\n",
      "loss: 0.579975  [104000/175341]\n",
      "loss: 0.283344  [105600/175341]\n",
      "loss: 0.604704  [107200/175341]\n",
      "loss: 0.443563  [108800/175341]\n",
      "loss: 0.277756  [110400/175341]\n",
      "loss: 0.453596  [112000/175341]\n",
      "loss: 0.974779  [113600/175341]\n",
      "loss: 0.434477  [115200/175341]\n",
      "loss: 0.308715  [116800/175341]\n",
      "loss: 0.718928  [118400/175341]\n",
      "loss: 0.325509  [120000/175341]\n",
      "loss: 0.379111  [121600/175341]\n",
      "loss: 0.395605  [123200/175341]\n",
      "loss: 0.734078  [124800/175341]\n",
      "loss: 0.763195  [126400/175341]\n",
      "loss: 0.619464  [128000/175341]\n",
      "loss: 0.276617  [129600/175341]\n",
      "loss: 0.389134  [131200/175341]\n",
      "loss: 0.337449  [132800/175341]\n",
      "loss: 1.282942  [134400/175341]\n",
      "loss: 0.313357  [136000/175341]\n",
      "loss: 0.339839  [137600/175341]\n",
      "loss: 0.560786  [139200/175341]\n",
      "loss: 0.342807  [140800/175341]\n",
      "loss: 0.390091  [142400/175341]\n",
      "loss: 0.692880  [144000/175341]\n",
      "loss: 0.352500  [145600/175341]\n",
      "loss: 0.615511  [147200/175341]\n",
      "loss: 0.115777  [148800/175341]\n",
      "loss: 0.532206  [150400/175341]\n",
      "loss: 0.500960  [152000/175341]\n",
      "loss: 0.779277  [153600/175341]\n",
      "loss: 0.339208  [155200/175341]\n",
      "loss: 0.510431  [156800/175341]\n",
      "loss: 0.245371  [158400/175341]\n",
      "loss: 0.258353  [160000/175341]\n",
      "loss: 0.437090  [161600/175341]\n",
      "loss: 0.444371  [163200/175341]\n",
      "loss: 0.301415  [164800/175341]\n",
      "loss: 0.607446  [166400/175341]\n",
      "loss: 0.401114  [168000/175341]\n",
      "loss: 0.338102  [169600/175341]\n",
      "loss: 0.463552  [171200/175341]\n",
      "loss: 0.347493  [172800/175341]\n",
      "loss: 0.561096  [174400/175341]\n",
      "Train Accuracy: 80.2465%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.573506, F1-score: 74.11%, Macro_F1-Score:  39.48%  \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.553923  [    0/175341]\n",
      "loss: 0.529567  [ 1600/175341]\n",
      "loss: 0.658542  [ 3200/175341]\n",
      "loss: 0.272727  [ 4800/175341]\n",
      "loss: 0.522861  [ 6400/175341]\n",
      "loss: 0.200396  [ 8000/175341]\n",
      "loss: 0.639857  [ 9600/175341]\n",
      "loss: 0.497734  [11200/175341]\n",
      "loss: 0.278659  [12800/175341]\n",
      "loss: 0.417001  [14400/175341]\n",
      "loss: 0.290633  [16000/175341]\n",
      "loss: 0.392316  [17600/175341]\n",
      "loss: 0.672274  [19200/175341]\n",
      "loss: 0.570269  [20800/175341]\n",
      "loss: 0.232006  [22400/175341]\n",
      "loss: 0.650510  [24000/175341]\n",
      "loss: 0.848653  [25600/175341]\n",
      "loss: 0.701822  [27200/175341]\n",
      "loss: 0.786575  [28800/175341]\n",
      "loss: 0.625285  [30400/175341]\n",
      "loss: 0.562905  [32000/175341]\n",
      "loss: 0.534234  [33600/175341]\n",
      "loss: 0.364933  [35200/175341]\n",
      "loss: 0.622999  [36800/175341]\n",
      "loss: 0.458124  [38400/175341]\n",
      "loss: 0.169281  [40000/175341]\n",
      "loss: 1.128700  [41600/175341]\n",
      "loss: 0.295926  [43200/175341]\n",
      "loss: 0.314847  [44800/175341]\n",
      "loss: 0.867371  [46400/175341]\n",
      "loss: 0.494282  [48000/175341]\n",
      "loss: 0.133318  [49600/175341]\n",
      "loss: 0.431875  [51200/175341]\n",
      "loss: 0.515096  [52800/175341]\n",
      "loss: 0.274183  [54400/175341]\n",
      "loss: 0.598091  [56000/175341]\n",
      "loss: 0.907267  [57600/175341]\n",
      "loss: 0.231828  [59200/175341]\n",
      "loss: 0.187954  [60800/175341]\n",
      "loss: 0.427517  [62400/175341]\n",
      "loss: 0.667583  [64000/175341]\n",
      "loss: 0.475729  [65600/175341]\n",
      "loss: 0.440553  [67200/175341]\n",
      "loss: 0.691950  [68800/175341]\n",
      "loss: 0.296341  [70400/175341]\n",
      "loss: 0.697501  [72000/175341]\n",
      "loss: 0.343960  [73600/175341]\n",
      "loss: 1.377069  [75200/175341]\n",
      "loss: 0.071472  [76800/175341]\n",
      "loss: 0.905178  [78400/175341]\n",
      "loss: 0.527790  [80000/175341]\n",
      "loss: 0.462724  [81600/175341]\n",
      "loss: 0.646836  [83200/175341]\n",
      "loss: 0.167063  [84800/175341]\n",
      "loss: 0.715665  [86400/175341]\n",
      "loss: 0.464050  [88000/175341]\n",
      "loss: 0.454642  [89600/175341]\n",
      "loss: 0.041735  [91200/175341]\n",
      "loss: 0.407935  [92800/175341]\n",
      "loss: 0.773899  [94400/175341]\n",
      "loss: 0.357171  [96000/175341]\n",
      "loss: 0.437874  [97600/175341]\n",
      "loss: 0.701122  [99200/175341]\n",
      "loss: 0.428432  [100800/175341]\n",
      "loss: 0.263162  [102400/175341]\n",
      "loss: 0.499546  [104000/175341]\n",
      "loss: 0.917247  [105600/175341]\n",
      "loss: 0.556136  [107200/175341]\n",
      "loss: 0.536015  [108800/175341]\n",
      "loss: 0.159112  [110400/175341]\n",
      "loss: 0.266293  [112000/175341]\n",
      "loss: 0.482797  [113600/175341]\n",
      "loss: 0.600030  [115200/175341]\n",
      "loss: 0.844773  [116800/175341]\n",
      "loss: 0.280132  [118400/175341]\n",
      "loss: 0.693281  [120000/175341]\n",
      "loss: 0.521792  [121600/175341]\n",
      "loss: 0.702280  [123200/175341]\n",
      "loss: 0.620628  [124800/175341]\n",
      "loss: 0.602841  [126400/175341]\n",
      "loss: 0.901013  [128000/175341]\n",
      "loss: 0.345160  [129600/175341]\n",
      "loss: 0.157451  [131200/175341]\n",
      "loss: 0.531212  [132800/175341]\n",
      "loss: 0.283518  [134400/175341]\n",
      "loss: 0.371162  [136000/175341]\n",
      "loss: 0.499188  [137600/175341]\n",
      "loss: 0.622358  [139200/175341]\n",
      "loss: 0.166083  [140800/175341]\n",
      "loss: 0.514287  [142400/175341]\n",
      "loss: 0.504369  [144000/175341]\n",
      "loss: 0.327982  [145600/175341]\n",
      "loss: 0.452850  [147200/175341]\n",
      "loss: 0.656065  [148800/175341]\n",
      "loss: 0.751691  [150400/175341]\n",
      "loss: 0.337184  [152000/175341]\n",
      "loss: 1.077349  [153600/175341]\n",
      "loss: 0.697212  [155200/175341]\n",
      "loss: 0.508675  [156800/175341]\n",
      "loss: 0.541034  [158400/175341]\n",
      "loss: 0.825812  [160000/175341]\n",
      "loss: 0.578874  [161600/175341]\n",
      "loss: 0.490810  [163200/175341]\n",
      "loss: 0.718896  [164800/175341]\n",
      "loss: 0.759783  [166400/175341]\n",
      "loss: 0.238254  [168000/175341]\n",
      "loss: 0.372787  [169600/175341]\n",
      "loss: 0.293420  [171200/175341]\n",
      "loss: 0.467123  [172800/175341]\n",
      "loss: 0.469961  [174400/175341]\n",
      "Train Accuracy: 80.2876%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.561853, F1-score: 74.67%, Macro_F1-Score:  40.17%  \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.580909  [    0/175341]\n",
      "loss: 0.376420  [ 1600/175341]\n",
      "loss: 0.251549  [ 3200/175341]\n",
      "loss: 0.253069  [ 4800/175341]\n",
      "loss: 0.324466  [ 6400/175341]\n",
      "loss: 0.580766  [ 8000/175341]\n",
      "loss: 0.481604  [ 9600/175341]\n",
      "loss: 0.610286  [11200/175341]\n",
      "loss: 0.448404  [12800/175341]\n",
      "loss: 0.578748  [14400/175341]\n",
      "loss: 0.582715  [16000/175341]\n",
      "loss: 0.089015  [17600/175341]\n",
      "loss: 0.557245  [19200/175341]\n",
      "loss: 0.283317  [20800/175341]\n",
      "loss: 0.401607  [22400/175341]\n",
      "loss: 0.711763  [24000/175341]\n",
      "loss: 0.253039  [25600/175341]\n",
      "loss: 0.809219  [27200/175341]\n",
      "loss: 0.214757  [28800/175341]\n",
      "loss: 0.309488  [30400/175341]\n",
      "loss: 0.862735  [32000/175341]\n",
      "loss: 0.808086  [33600/175341]\n",
      "loss: 0.416300  [35200/175341]\n",
      "loss: 0.512507  [36800/175341]\n",
      "loss: 0.295173  [38400/175341]\n",
      "loss: 0.480124  [40000/175341]\n",
      "loss: 0.396025  [41600/175341]\n",
      "loss: 0.536092  [43200/175341]\n",
      "loss: 0.221337  [44800/175341]\n",
      "loss: 0.444233  [46400/175341]\n",
      "loss: 0.577342  [48000/175341]\n",
      "loss: 0.144862  [49600/175341]\n",
      "loss: 0.542604  [51200/175341]\n",
      "loss: 0.679777  [52800/175341]\n",
      "loss: 0.273368  [54400/175341]\n",
      "loss: 0.833525  [56000/175341]\n",
      "loss: 0.342352  [57600/175341]\n",
      "loss: 0.505922  [59200/175341]\n",
      "loss: 0.763388  [60800/175341]\n",
      "loss: 0.359115  [62400/175341]\n",
      "loss: 0.766353  [64000/175341]\n",
      "loss: 0.496431  [65600/175341]\n",
      "loss: 0.304920  [67200/175341]\n",
      "loss: 0.406143  [68800/175341]\n",
      "loss: 0.189230  [70400/175341]\n",
      "loss: 0.623911  [72000/175341]\n",
      "loss: 0.645667  [73600/175341]\n",
      "loss: 0.541894  [75200/175341]\n",
      "loss: 0.980858  [76800/175341]\n",
      "loss: 0.166376  [78400/175341]\n",
      "loss: 0.342487  [80000/175341]\n",
      "loss: 0.184537  [81600/175341]\n",
      "loss: 0.257074  [83200/175341]\n",
      "loss: 0.499334  [84800/175341]\n",
      "loss: 0.398773  [86400/175341]\n",
      "loss: 0.241794  [88000/175341]\n",
      "loss: 0.472614  [89600/175341]\n",
      "loss: 0.537340  [91200/175341]\n",
      "loss: 0.430222  [92800/175341]\n",
      "loss: 0.473883  [94400/175341]\n",
      "loss: 0.851140  [96000/175341]\n",
      "loss: 0.183385  [97600/175341]\n",
      "loss: 0.176231  [99200/175341]\n",
      "loss: 0.138064  [100800/175341]\n",
      "loss: 0.105404  [102400/175341]\n",
      "loss: 0.336675  [104000/175341]\n",
      "loss: 0.806539  [105600/175341]\n",
      "loss: 0.354658  [107200/175341]\n",
      "loss: 0.991867  [108800/175341]\n",
      "loss: 0.254874  [110400/175341]\n",
      "loss: 0.511787  [112000/175341]\n",
      "loss: 0.422049  [113600/175341]\n",
      "loss: 0.706790  [115200/175341]\n",
      "loss: 0.269917  [116800/175341]\n",
      "loss: 0.327120  [118400/175341]\n",
      "loss: 0.559841  [120000/175341]\n",
      "loss: 0.639874  [121600/175341]\n",
      "loss: 0.421309  [123200/175341]\n",
      "loss: 0.390609  [124800/175341]\n",
      "loss: 0.495290  [126400/175341]\n",
      "loss: 0.742647  [128000/175341]\n",
      "loss: 0.622647  [129600/175341]\n",
      "loss: 0.444891  [131200/175341]\n",
      "loss: 0.622376  [132800/175341]\n",
      "loss: 0.368941  [134400/175341]\n",
      "loss: 0.825089  [136000/175341]\n",
      "loss: 0.442856  [137600/175341]\n",
      "loss: 0.495459  [139200/175341]\n",
      "loss: 0.200119  [140800/175341]\n",
      "loss: 0.447021  [142400/175341]\n",
      "loss: 0.663740  [144000/175341]\n",
      "loss: 0.207835  [145600/175341]\n",
      "loss: 0.794108  [147200/175341]\n",
      "loss: 0.445391  [148800/175341]\n",
      "loss: 0.418125  [150400/175341]\n",
      "loss: 0.567983  [152000/175341]\n",
      "loss: 0.193029  [153600/175341]\n",
      "loss: 0.668020  [155200/175341]\n",
      "loss: 0.307624  [156800/175341]\n",
      "loss: 0.484662  [158400/175341]\n",
      "loss: 0.363743  [160000/175341]\n",
      "loss: 0.396699  [161600/175341]\n",
      "loss: 0.216732  [163200/175341]\n",
      "loss: 0.164077  [164800/175341]\n",
      "loss: 0.589004  [166400/175341]\n",
      "loss: 0.445489  [168000/175341]\n",
      "loss: 0.741892  [169600/175341]\n",
      "loss: 0.735605  [171200/175341]\n",
      "loss: 0.547794  [172800/175341]\n",
      "loss: 0.509560  [174400/175341]\n",
      "Train Accuracy: 80.3457%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.561095, F1-score: 74.86%, Macro_F1-Score:  39.19%  \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.301443  [    0/175341]\n",
      "loss: 0.301202  [ 1600/175341]\n",
      "loss: 0.711369  [ 3200/175341]\n",
      "loss: 0.546298  [ 4800/175341]\n",
      "loss: 0.346242  [ 6400/175341]\n",
      "loss: 0.528977  [ 8000/175341]\n",
      "loss: 0.470702  [ 9600/175341]\n",
      "loss: 0.394378  [11200/175341]\n",
      "loss: 0.474313  [12800/175341]\n",
      "loss: 0.659419  [14400/175341]\n",
      "loss: 0.187826  [16000/175341]\n",
      "loss: 0.351459  [17600/175341]\n",
      "loss: 0.427302  [19200/175341]\n",
      "loss: 0.714011  [20800/175341]\n",
      "loss: 0.433109  [22400/175341]\n",
      "loss: 0.242830  [24000/175341]\n",
      "loss: 0.287754  [25600/175341]\n",
      "loss: 0.362053  [27200/175341]\n",
      "loss: 0.736876  [28800/175341]\n",
      "loss: 0.383777  [30400/175341]\n",
      "loss: 0.540963  [32000/175341]\n",
      "loss: 0.732309  [33600/175341]\n",
      "loss: 0.404150  [35200/175341]\n",
      "loss: 0.743395  [36800/175341]\n",
      "loss: 0.477087  [38400/175341]\n",
      "loss: 0.389261  [40000/175341]\n",
      "loss: 0.359313  [41600/175341]\n",
      "loss: 0.703176  [43200/175341]\n",
      "loss: 0.352554  [44800/175341]\n",
      "loss: 0.595582  [46400/175341]\n",
      "loss: 0.535531  [48000/175341]\n",
      "loss: 0.376907  [49600/175341]\n",
      "loss: 0.426715  [51200/175341]\n",
      "loss: 0.711157  [52800/175341]\n",
      "loss: 0.537716  [54400/175341]\n",
      "loss: 0.362798  [56000/175341]\n",
      "loss: 0.467749  [57600/175341]\n",
      "loss: 0.579267  [59200/175341]\n",
      "loss: 0.202642  [60800/175341]\n",
      "loss: 0.530132  [62400/175341]\n",
      "loss: 0.744807  [64000/175341]\n",
      "loss: 0.259539  [65600/175341]\n",
      "loss: 0.231957  [67200/175341]\n",
      "loss: 0.595253  [68800/175341]\n",
      "loss: 0.292367  [70400/175341]\n",
      "loss: 0.211032  [72000/175341]\n",
      "loss: 0.477322  [73600/175341]\n",
      "loss: 0.274909  [75200/175341]\n",
      "loss: 0.291780  [76800/175341]\n",
      "loss: 0.419098  [78400/175341]\n",
      "loss: 0.705529  [80000/175341]\n",
      "loss: 0.650199  [81600/175341]\n",
      "loss: 0.326542  [83200/175341]\n",
      "loss: 0.391845  [84800/175341]\n",
      "loss: 0.685990  [86400/175341]\n",
      "loss: 0.742220  [88000/175341]\n",
      "loss: 0.415041  [89600/175341]\n",
      "loss: 0.706627  [91200/175341]\n",
      "loss: 0.347073  [92800/175341]\n",
      "loss: 0.310348  [94400/175341]\n",
      "loss: 0.325776  [96000/175341]\n",
      "loss: 0.345765  [97600/175341]\n",
      "loss: 0.149886  [99200/175341]\n",
      "loss: 0.782502  [100800/175341]\n",
      "loss: 0.414360  [102400/175341]\n",
      "loss: 0.179507  [104000/175341]\n",
      "loss: 0.418263  [105600/175341]\n",
      "loss: 0.281539  [107200/175341]\n",
      "loss: 0.331689  [108800/175341]\n",
      "loss: 0.212597  [110400/175341]\n",
      "loss: 0.548558  [112000/175341]\n",
      "loss: 0.405829  [113600/175341]\n",
      "loss: 0.797350  [115200/175341]\n",
      "loss: 0.677165  [116800/175341]\n",
      "loss: 0.432819  [118400/175341]\n",
      "loss: 0.316848  [120000/175341]\n",
      "loss: 0.330828  [121600/175341]\n",
      "loss: 0.982787  [123200/175341]\n",
      "loss: 0.436325  [124800/175341]\n",
      "loss: 0.230203  [126400/175341]\n",
      "loss: 0.253458  [128000/175341]\n",
      "loss: 0.533395  [129600/175341]\n",
      "loss: 0.539649  [131200/175341]\n",
      "loss: 0.194660  [132800/175341]\n",
      "loss: 0.363886  [134400/175341]\n",
      "loss: 0.470247  [136000/175341]\n",
      "loss: 0.225944  [137600/175341]\n",
      "loss: 0.496746  [139200/175341]\n",
      "loss: 0.380930  [140800/175341]\n",
      "loss: 0.265406  [142400/175341]\n",
      "loss: 0.471634  [144000/175341]\n",
      "loss: 0.447287  [145600/175341]\n",
      "loss: 0.511650  [147200/175341]\n",
      "loss: 0.904891  [148800/175341]\n",
      "loss: 0.949351  [150400/175341]\n",
      "loss: 0.236417  [152000/175341]\n",
      "loss: 0.653749  [153600/175341]\n",
      "loss: 0.750533  [155200/175341]\n",
      "loss: 0.182784  [156800/175341]\n",
      "loss: 0.689398  [158400/175341]\n",
      "loss: 0.531080  [160000/175341]\n",
      "loss: 0.373667  [161600/175341]\n",
      "loss: 0.634508  [163200/175341]\n",
      "loss: 0.438105  [164800/175341]\n",
      "loss: 0.504676  [166400/175341]\n",
      "loss: 0.352436  [168000/175341]\n",
      "loss: 0.573985  [169600/175341]\n",
      "loss: 0.632168  [171200/175341]\n",
      "loss: 0.529365  [172800/175341]\n",
      "loss: 0.423824  [174400/175341]\n",
      "Train Accuracy: 80.3452%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.587320, F1-score: 74.76%, Macro_F1-Score:  41.20%  \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.307372  [    0/175341]\n",
      "loss: 0.512929  [ 1600/175341]\n",
      "loss: 0.588460  [ 3200/175341]\n",
      "loss: 0.735963  [ 4800/175341]\n",
      "loss: 0.365812  [ 6400/175341]\n",
      "loss: 0.890627  [ 8000/175341]\n",
      "loss: 0.612907  [ 9600/175341]\n",
      "loss: 0.662078  [11200/175341]\n",
      "loss: 0.408002  [12800/175341]\n",
      "loss: 0.490554  [14400/175341]\n",
      "loss: 0.393598  [16000/175341]\n",
      "loss: 0.283844  [17600/175341]\n",
      "loss: 0.130044  [19200/175341]\n",
      "loss: 1.149084  [20800/175341]\n",
      "loss: 0.664226  [22400/175341]\n",
      "loss: 0.836020  [24000/175341]\n",
      "loss: 0.287231  [25600/175341]\n",
      "loss: 0.757277  [27200/175341]\n",
      "loss: 0.501426  [28800/175341]\n",
      "loss: 0.737330  [30400/175341]\n",
      "loss: 0.429633  [32000/175341]\n",
      "loss: 0.314408  [33600/175341]\n",
      "loss: 0.520758  [35200/175341]\n",
      "loss: 0.319791  [36800/175341]\n",
      "loss: 0.279842  [38400/175341]\n",
      "loss: 0.337207  [40000/175341]\n",
      "loss: 0.764075  [41600/175341]\n",
      "loss: 0.096152  [43200/175341]\n",
      "loss: 0.489826  [44800/175341]\n",
      "loss: 0.609220  [46400/175341]\n",
      "loss: 0.232373  [48000/175341]\n",
      "loss: 0.687134  [49600/175341]\n",
      "loss: 0.108534  [51200/175341]\n",
      "loss: 0.378040  [52800/175341]\n",
      "loss: 0.454035  [54400/175341]\n",
      "loss: 0.407521  [56000/175341]\n",
      "loss: 0.326212  [57600/175341]\n",
      "loss: 0.539128  [59200/175341]\n",
      "loss: 1.102125  [60800/175341]\n",
      "loss: 0.481664  [62400/175341]\n",
      "loss: 0.328854  [64000/175341]\n",
      "loss: 0.475286  [65600/175341]\n",
      "loss: 0.858994  [67200/175341]\n",
      "loss: 0.259231  [68800/175341]\n",
      "loss: 0.526293  [70400/175341]\n",
      "loss: 0.454048  [72000/175341]\n",
      "loss: 0.798989  [73600/175341]\n",
      "loss: 0.260138  [75200/175341]\n",
      "loss: 0.693249  [76800/175341]\n",
      "loss: 0.238014  [78400/175341]\n",
      "loss: 0.341099  [80000/175341]\n",
      "loss: 0.386946  [81600/175341]\n",
      "loss: 0.415553  [83200/175341]\n",
      "loss: 0.421187  [84800/175341]\n",
      "loss: 0.198756  [86400/175341]\n",
      "loss: 0.712425  [88000/175341]\n",
      "loss: 0.820949  [89600/175341]\n",
      "loss: 0.231590  [91200/175341]\n",
      "loss: 0.976297  [92800/175341]\n",
      "loss: 0.705448  [94400/175341]\n",
      "loss: 0.650857  [96000/175341]\n",
      "loss: 0.174880  [97600/175341]\n",
      "loss: 0.531809  [99200/175341]\n",
      "loss: 0.705166  [100800/175341]\n",
      "loss: 0.188432  [102400/175341]\n",
      "loss: 0.396856  [104000/175341]\n",
      "loss: 0.307806  [105600/175341]\n",
      "loss: 0.518101  [107200/175341]\n",
      "loss: 0.566019  [108800/175341]\n",
      "loss: 0.488172  [110400/175341]\n",
      "loss: 0.336710  [112000/175341]\n",
      "loss: 0.786103  [113600/175341]\n",
      "loss: 0.631480  [115200/175341]\n",
      "loss: 0.678437  [116800/175341]\n",
      "loss: 0.604703  [118400/175341]\n",
      "loss: 0.388612  [120000/175341]\n",
      "loss: 0.346141  [121600/175341]\n",
      "loss: 0.341978  [123200/175341]\n",
      "loss: 0.705749  [124800/175341]\n",
      "loss: 0.856513  [126400/175341]\n",
      "loss: 0.171158  [128000/175341]\n",
      "loss: 0.177188  [129600/175341]\n",
      "loss: 0.356059  [131200/175341]\n",
      "loss: 0.508034  [132800/175341]\n",
      "loss: 0.495396  [134400/175341]\n",
      "loss: 0.697922  [136000/175341]\n",
      "loss: 0.373752  [137600/175341]\n",
      "loss: 0.621644  [139200/175341]\n",
      "loss: 0.704267  [140800/175341]\n",
      "loss: 0.207139  [142400/175341]\n",
      "loss: 0.316116  [144000/175341]\n",
      "loss: 0.539706  [145600/175341]\n",
      "loss: 0.587748  [147200/175341]\n",
      "loss: 0.387407  [148800/175341]\n",
      "loss: 0.649983  [150400/175341]\n",
      "loss: 0.899834  [152000/175341]\n",
      "loss: 0.377039  [153600/175341]\n",
      "loss: 0.705096  [155200/175341]\n",
      "loss: 0.584385  [156800/175341]\n",
      "loss: 0.310287  [158400/175341]\n",
      "loss: 0.913094  [160000/175341]\n",
      "loss: 0.147082  [161600/175341]\n",
      "loss: 0.479370  [163200/175341]\n",
      "loss: 0.313616  [164800/175341]\n",
      "loss: 0.654606  [166400/175341]\n",
      "loss: 0.637806  [168000/175341]\n",
      "loss: 0.787430  [169600/175341]\n",
      "loss: 0.538716  [171200/175341]\n",
      "loss: 0.340905  [172800/175341]\n",
      "loss: 0.391480  [174400/175341]\n",
      "Train Accuracy: 80.3161%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.559908, F1-score: 75.09%, Macro_F1-Score:  39.63%  \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.384749  [    0/175341]\n",
      "loss: 0.257008  [ 1600/175341]\n",
      "loss: 0.447099  [ 3200/175341]\n",
      "loss: 0.402726  [ 4800/175341]\n",
      "loss: 0.161841  [ 6400/175341]\n",
      "loss: 0.339872  [ 8000/175341]\n",
      "loss: 0.649132  [ 9600/175341]\n",
      "loss: 0.183896  [11200/175341]\n",
      "loss: 0.375753  [12800/175341]\n",
      "loss: 0.454961  [14400/175341]\n",
      "loss: 0.422534  [16000/175341]\n",
      "loss: 0.300492  [17600/175341]\n",
      "loss: 0.753235  [19200/175341]\n",
      "loss: 0.618297  [20800/175341]\n",
      "loss: 0.752206  [22400/175341]\n",
      "loss: 0.293697  [24000/175341]\n",
      "loss: 0.737227  [25600/175341]\n",
      "loss: 0.444269  [27200/175341]\n",
      "loss: 0.291616  [28800/175341]\n",
      "loss: 0.701993  [30400/175341]\n",
      "loss: 0.689349  [32000/175341]\n",
      "loss: 0.399396  [33600/175341]\n",
      "loss: 0.530183  [35200/175341]\n",
      "loss: 0.403824  [36800/175341]\n",
      "loss: 0.486406  [38400/175341]\n",
      "loss: 0.507048  [40000/175341]\n",
      "loss: 0.667144  [41600/175341]\n",
      "loss: 0.430997  [43200/175341]\n",
      "loss: 0.508308  [44800/175341]\n",
      "loss: 0.569017  [46400/175341]\n",
      "loss: 0.629774  [48000/175341]\n",
      "loss: 0.555998  [49600/175341]\n",
      "loss: 0.384349  [51200/175341]\n",
      "loss: 0.719089  [52800/175341]\n",
      "loss: 0.300207  [54400/175341]\n",
      "loss: 0.480908  [56000/175341]\n",
      "loss: 0.411587  [57600/175341]\n",
      "loss: 0.298034  [59200/175341]\n",
      "loss: 0.215520  [60800/175341]\n",
      "loss: 0.472220  [62400/175341]\n",
      "loss: 0.288050  [64000/175341]\n",
      "loss: 0.421464  [65600/175341]\n",
      "loss: 0.355926  [67200/175341]\n",
      "loss: 0.360138  [68800/175341]\n",
      "loss: 0.564397  [70400/175341]\n",
      "loss: 0.927135  [72000/175341]\n",
      "loss: 0.664190  [73600/175341]\n",
      "loss: 0.221408  [75200/175341]\n",
      "loss: 0.593767  [76800/175341]\n",
      "loss: 0.523744  [78400/175341]\n",
      "loss: 0.239749  [80000/175341]\n",
      "loss: 0.306288  [81600/175341]\n",
      "loss: 0.659291  [83200/175341]\n",
      "loss: 0.658997  [84800/175341]\n",
      "loss: 1.266093  [86400/175341]\n",
      "loss: 0.952808  [88000/175341]\n",
      "loss: 0.613706  [89600/175341]\n",
      "loss: 0.821605  [91200/175341]\n",
      "loss: 0.750463  [92800/175341]\n",
      "loss: 0.759368  [94400/175341]\n",
      "loss: 0.637091  [96000/175341]\n",
      "loss: 0.395822  [97600/175341]\n",
      "loss: 0.371249  [99200/175341]\n",
      "loss: 0.603439  [100800/175341]\n",
      "loss: 0.413013  [102400/175341]\n",
      "loss: 0.420409  [104000/175341]\n",
      "loss: 0.358570  [105600/175341]\n",
      "loss: 0.702189  [107200/175341]\n",
      "loss: 0.694730  [108800/175341]\n",
      "loss: 0.427099  [110400/175341]\n",
      "loss: 0.386622  [112000/175341]\n",
      "loss: 0.357162  [113600/175341]\n",
      "loss: 0.343590  [115200/175341]\n",
      "loss: 0.388955  [116800/175341]\n",
      "loss: 0.325858  [118400/175341]\n",
      "loss: 0.667716  [120000/175341]\n",
      "loss: 0.545376  [121600/175341]\n",
      "loss: 0.137641  [123200/175341]\n",
      "loss: 0.189867  [124800/175341]\n",
      "loss: 0.360813  [126400/175341]\n",
      "loss: 0.361117  [128000/175341]\n",
      "loss: 0.316430  [129600/175341]\n",
      "loss: 0.472752  [131200/175341]\n",
      "loss: 0.607928  [132800/175341]\n",
      "loss: 0.710263  [134400/175341]\n",
      "loss: 0.411179  [136000/175341]\n",
      "loss: 0.517322  [137600/175341]\n",
      "loss: 0.466458  [139200/175341]\n",
      "loss: 0.298388  [140800/175341]\n",
      "loss: 0.149915  [142400/175341]\n",
      "loss: 0.511513  [144000/175341]\n",
      "loss: 0.390980  [145600/175341]\n",
      "loss: 0.881043  [147200/175341]\n",
      "loss: 0.551772  [148800/175341]\n",
      "loss: 0.679163  [150400/175341]\n",
      "loss: 0.396218  [152000/175341]\n",
      "loss: 0.339877  [153600/175341]\n",
      "loss: 0.535328  [155200/175341]\n",
      "loss: 0.309931  [156800/175341]\n",
      "loss: 0.911860  [158400/175341]\n",
      "loss: 0.364488  [160000/175341]\n",
      "loss: 0.507114  [161600/175341]\n",
      "loss: 0.489075  [163200/175341]\n",
      "loss: 0.332173  [164800/175341]\n",
      "loss: 0.224032  [166400/175341]\n",
      "loss: 0.568902  [168000/175341]\n",
      "loss: 0.608862  [169600/175341]\n",
      "loss: 0.534737  [171200/175341]\n",
      "loss: 0.803458  [172800/175341]\n",
      "loss: 0.499014  [174400/175341]\n",
      "Train Accuracy: 80.3611%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.570760, F1-score: 74.32%, Macro_F1-Score:  39.00%  \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.597904  [    0/175341]\n",
      "loss: 0.510307  [ 1600/175341]\n",
      "loss: 0.920345  [ 3200/175341]\n",
      "loss: 0.470654  [ 4800/175341]\n",
      "loss: 0.405968  [ 6400/175341]\n",
      "loss: 0.143929  [ 8000/175341]\n",
      "loss: 0.445421  [ 9600/175341]\n",
      "loss: 0.553300  [11200/175341]\n",
      "loss: 0.580775  [12800/175341]\n",
      "loss: 0.419498  [14400/175341]\n",
      "loss: 0.670959  [16000/175341]\n",
      "loss: 0.463721  [17600/175341]\n",
      "loss: 0.247931  [19200/175341]\n",
      "loss: 0.417473  [20800/175341]\n",
      "loss: 0.450311  [22400/175341]\n",
      "loss: 0.764697  [24000/175341]\n",
      "loss: 0.419642  [25600/175341]\n",
      "loss: 0.633419  [27200/175341]\n",
      "loss: 0.986636  [28800/175341]\n",
      "loss: 0.928993  [30400/175341]\n",
      "loss: 0.320910  [32000/175341]\n",
      "loss: 0.300048  [33600/175341]\n",
      "loss: 0.156971  [35200/175341]\n",
      "loss: 0.174046  [36800/175341]\n",
      "loss: 1.041145  [38400/175341]\n",
      "loss: 0.450467  [40000/175341]\n",
      "loss: 0.437627  [41600/175341]\n",
      "loss: 0.229994  [43200/175341]\n",
      "loss: 0.276980  [44800/175341]\n",
      "loss: 0.388291  [46400/175341]\n",
      "loss: 0.545717  [48000/175341]\n",
      "loss: 0.756885  [49600/175341]\n",
      "loss: 0.559907  [51200/175341]\n",
      "loss: 0.978727  [52800/175341]\n",
      "loss: 0.461643  [54400/175341]\n",
      "loss: 0.345007  [56000/175341]\n",
      "loss: 0.182215  [57600/175341]\n",
      "loss: 0.680092  [59200/175341]\n",
      "loss: 0.229479  [60800/175341]\n",
      "loss: 0.872676  [62400/175341]\n",
      "loss: 0.374053  [64000/175341]\n",
      "loss: 0.338136  [65600/175341]\n",
      "loss: 0.620711  [67200/175341]\n",
      "loss: 0.241192  [68800/175341]\n",
      "loss: 0.520734  [70400/175341]\n",
      "loss: 0.459150  [72000/175341]\n",
      "loss: 0.252535  [73600/175341]\n",
      "loss: 0.726164  [75200/175341]\n",
      "loss: 0.553186  [76800/175341]\n",
      "loss: 0.738179  [78400/175341]\n",
      "loss: 0.510792  [80000/175341]\n",
      "loss: 0.540666  [81600/175341]\n",
      "loss: 0.726976  [83200/175341]\n",
      "loss: 0.537011  [84800/175341]\n",
      "loss: 0.303082  [86400/175341]\n",
      "loss: 0.408541  [88000/175341]\n",
      "loss: 0.590619  [89600/175341]\n",
      "loss: 1.062633  [91200/175341]\n",
      "loss: 0.497745  [92800/175341]\n",
      "loss: 0.208982  [94400/175341]\n",
      "loss: 0.349023  [96000/175341]\n",
      "loss: 0.963119  [97600/175341]\n",
      "loss: 0.313249  [99200/175341]\n",
      "loss: 0.646682  [100800/175341]\n",
      "loss: 0.414605  [102400/175341]\n",
      "loss: 0.208457  [104000/175341]\n",
      "loss: 0.242296  [105600/175341]\n",
      "loss: 0.299725  [107200/175341]\n",
      "loss: 0.412393  [108800/175341]\n",
      "loss: 0.271293  [110400/175341]\n",
      "loss: 0.513257  [112000/175341]\n",
      "loss: 0.522061  [113600/175341]\n",
      "loss: 0.336598  [115200/175341]\n",
      "loss: 0.273555  [116800/175341]\n",
      "loss: 0.465285  [118400/175341]\n",
      "loss: 0.402231  [120000/175341]\n",
      "loss: 0.481271  [121600/175341]\n",
      "loss: 0.411669  [123200/175341]\n",
      "loss: 0.294306  [124800/175341]\n",
      "loss: 0.544961  [126400/175341]\n",
      "loss: 0.124672  [128000/175341]\n",
      "loss: 0.764924  [129600/175341]\n",
      "loss: 0.453543  [131200/175341]\n",
      "loss: 0.660272  [132800/175341]\n",
      "loss: 0.405650  [134400/175341]\n",
      "loss: 0.789546  [136000/175341]\n",
      "loss: 0.294833  [137600/175341]\n",
      "loss: 0.513277  [139200/175341]\n",
      "loss: 0.347555  [140800/175341]\n",
      "loss: 0.782694  [142400/175341]\n",
      "loss: 0.796188  [144000/175341]\n",
      "loss: 0.226588  [145600/175341]\n",
      "loss: 0.670996  [147200/175341]\n",
      "loss: 0.706429  [148800/175341]\n",
      "loss: 0.993618  [150400/175341]\n",
      "loss: 0.735244  [152000/175341]\n",
      "loss: 0.812875  [153600/175341]\n",
      "loss: 0.444501  [155200/175341]\n",
      "loss: 0.516947  [156800/175341]\n",
      "loss: 0.215614  [158400/175341]\n",
      "loss: 1.136559  [160000/175341]\n",
      "loss: 0.252566  [161600/175341]\n",
      "loss: 0.420629  [163200/175341]\n",
      "loss: 0.459163  [164800/175341]\n",
      "loss: 0.673144  [166400/175341]\n",
      "loss: 0.510335  [168000/175341]\n",
      "loss: 0.540392  [169600/175341]\n",
      "loss: 0.269906  [171200/175341]\n",
      "loss: 0.752955  [172800/175341]\n",
      "loss: 0.458185  [174400/175341]\n",
      "Train Accuracy: 80.3674%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.566406, F1-score: 74.49%, Macro_F1-Score:  39.98%  \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.346725  [    0/175341]\n",
      "loss: 0.308201  [ 1600/175341]\n",
      "loss: 0.267740  [ 3200/175341]\n",
      "loss: 0.075818  [ 4800/175341]\n",
      "loss: 0.369460  [ 6400/175341]\n",
      "loss: 0.188043  [ 8000/175341]\n",
      "loss: 0.545312  [ 9600/175341]\n",
      "loss: 0.471912  [11200/175341]\n",
      "loss: 0.572471  [12800/175341]\n",
      "loss: 0.183308  [14400/175341]\n",
      "loss: 0.404770  [16000/175341]\n",
      "loss: 0.578907  [17600/175341]\n",
      "loss: 0.874583  [19200/175341]\n",
      "loss: 0.457867  [20800/175341]\n",
      "loss: 0.229434  [22400/175341]\n",
      "loss: 0.428255  [24000/175341]\n",
      "loss: 0.391641  [25600/175341]\n",
      "loss: 0.309820  [27200/175341]\n",
      "loss: 0.505953  [28800/175341]\n",
      "loss: 0.504249  [30400/175341]\n",
      "loss: 0.685067  [32000/175341]\n",
      "loss: 0.685268  [33600/175341]\n",
      "loss: 0.347586  [35200/175341]\n",
      "loss: 0.317274  [36800/175341]\n",
      "loss: 0.558060  [38400/175341]\n",
      "loss: 0.729279  [40000/175341]\n",
      "loss: 0.688893  [41600/175341]\n",
      "loss: 0.802182  [43200/175341]\n",
      "loss: 0.689197  [44800/175341]\n",
      "loss: 0.307791  [46400/175341]\n",
      "loss: 0.287539  [48000/175341]\n",
      "loss: 0.144936  [49600/175341]\n",
      "loss: 0.597227  [51200/175341]\n",
      "loss: 0.221492  [52800/175341]\n",
      "loss: 0.473003  [54400/175341]\n",
      "loss: 0.311102  [56000/175341]\n",
      "loss: 0.631458  [57600/175341]\n",
      "loss: 0.453850  [59200/175341]\n",
      "loss: 0.475511  [60800/175341]\n",
      "loss: 0.443054  [62400/175341]\n",
      "loss: 0.188774  [64000/175341]\n",
      "loss: 0.513725  [65600/175341]\n",
      "loss: 0.280795  [67200/175341]\n",
      "loss: 0.579947  [68800/175341]\n",
      "loss: 0.271174  [70400/175341]\n",
      "loss: 0.993098  [72000/175341]\n",
      "loss: 0.415853  [73600/175341]\n",
      "loss: 0.558750  [75200/175341]\n",
      "loss: 0.508617  [76800/175341]\n",
      "loss: 0.339269  [78400/175341]\n",
      "loss: 0.227033  [80000/175341]\n",
      "loss: 0.266814  [81600/175341]\n",
      "loss: 0.505774  [83200/175341]\n",
      "loss: 0.283192  [84800/175341]\n",
      "loss: 0.396138  [86400/175341]\n",
      "loss: 1.490496  [88000/175341]\n",
      "loss: 0.488032  [89600/175341]\n",
      "loss: 0.490410  [91200/175341]\n",
      "loss: 0.490027  [92800/175341]\n",
      "loss: 0.497219  [94400/175341]\n",
      "loss: 0.448562  [96000/175341]\n",
      "loss: 0.560823  [97600/175341]\n",
      "loss: 0.309864  [99200/175341]\n",
      "loss: 0.635573  [100800/175341]\n",
      "loss: 0.090019  [102400/175341]\n",
      "loss: 0.536525  [104000/175341]\n",
      "loss: 0.594171  [105600/175341]\n",
      "loss: 0.778799  [107200/175341]\n",
      "loss: 0.688921  [108800/175341]\n",
      "loss: 0.662434  [110400/175341]\n",
      "loss: 0.297267  [112000/175341]\n",
      "loss: 0.452052  [113600/175341]\n",
      "loss: 0.568978  [115200/175341]\n",
      "loss: 0.784980  [116800/175341]\n",
      "loss: 0.695521  [118400/175341]\n",
      "loss: 0.377445  [120000/175341]\n",
      "loss: 0.614548  [121600/175341]\n",
      "loss: 0.178025  [123200/175341]\n",
      "loss: 0.469371  [124800/175341]\n",
      "loss: 0.613716  [126400/175341]\n",
      "loss: 0.543700  [128000/175341]\n",
      "loss: 0.355978  [129600/175341]\n",
      "loss: 0.686015  [131200/175341]\n",
      "loss: 0.526109  [132800/175341]\n",
      "loss: 0.629470  [134400/175341]\n",
      "loss: 0.615796  [136000/175341]\n",
      "loss: 0.463225  [137600/175341]\n",
      "loss: 0.884710  [139200/175341]\n",
      "loss: 0.481866  [140800/175341]\n",
      "loss: 0.408668  [142400/175341]\n",
      "loss: 0.530254  [144000/175341]\n",
      "loss: 0.720196  [145600/175341]\n",
      "loss: 0.306546  [147200/175341]\n",
      "loss: 0.427911  [148800/175341]\n",
      "loss: 0.906093  [150400/175341]\n",
      "loss: 0.898927  [152000/175341]\n",
      "loss: 0.427801  [153600/175341]\n",
      "loss: 1.017629  [155200/175341]\n",
      "loss: 0.726466  [156800/175341]\n",
      "loss: 0.291259  [158400/175341]\n",
      "loss: 0.148635  [160000/175341]\n",
      "loss: 0.348700  [161600/175341]\n",
      "loss: 0.672072  [163200/175341]\n",
      "loss: 0.464097  [164800/175341]\n",
      "loss: 0.801897  [166400/175341]\n",
      "loss: 0.716392  [168000/175341]\n",
      "loss: 0.657641  [169600/175341]\n",
      "loss: 0.447026  [171200/175341]\n",
      "loss: 0.559442  [172800/175341]\n",
      "loss: 0.393542  [174400/175341]\n",
      "Train Accuracy: 80.3908%\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.601464, F1-score: 73.08%, Macro_F1-Score:  39.76%  \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.112683  [    0/175341]\n",
      "loss: 0.317344  [ 1600/175341]\n",
      "loss: 0.359062  [ 3200/175341]\n",
      "loss: 0.399273  [ 4800/175341]\n",
      "loss: 0.382312  [ 6400/175341]\n",
      "loss: 0.372512  [ 8000/175341]\n",
      "loss: 0.108832  [ 9600/175341]\n",
      "loss: 0.355975  [11200/175341]\n",
      "loss: 0.459281  [12800/175341]\n",
      "loss: 0.825767  [14400/175341]\n",
      "loss: 0.362395  [16000/175341]\n",
      "loss: 0.502689  [17600/175341]\n",
      "loss: 0.640980  [19200/175341]\n",
      "loss: 0.787924  [20800/175341]\n",
      "loss: 0.280689  [22400/175341]\n",
      "loss: 0.483977  [24000/175341]\n",
      "loss: 0.617128  [25600/175341]\n",
      "loss: 0.562986  [27200/175341]\n",
      "loss: 0.529309  [28800/175341]\n",
      "loss: 0.896702  [30400/175341]\n",
      "loss: 0.172937  [32000/175341]\n",
      "loss: 0.319938  [33600/175341]\n",
      "loss: 0.473316  [35200/175341]\n",
      "loss: 0.408243  [36800/175341]\n",
      "loss: 0.531548  [38400/175341]\n",
      "loss: 0.350240  [40000/175341]\n",
      "loss: 0.265700  [41600/175341]\n",
      "loss: 0.385978  [43200/175341]\n",
      "loss: 0.731678  [44800/175341]\n",
      "loss: 0.259221  [46400/175341]\n",
      "loss: 0.265142  [48000/175341]\n",
      "loss: 0.408462  [49600/175341]\n",
      "loss: 0.341511  [51200/175341]\n",
      "loss: 0.326401  [52800/175341]\n",
      "loss: 0.394111  [54400/175341]\n",
      "loss: 0.388289  [56000/175341]\n",
      "loss: 0.411392  [57600/175341]\n",
      "loss: 0.299371  [59200/175341]\n",
      "loss: 0.264935  [60800/175341]\n",
      "loss: 0.732905  [62400/175341]\n",
      "loss: 0.407170  [64000/175341]\n",
      "loss: 0.603430  [65600/175341]\n",
      "loss: 0.412520  [67200/175341]\n",
      "loss: 0.346146  [68800/175341]\n",
      "loss: 0.895094  [70400/175341]\n",
      "loss: 0.100217  [72000/175341]\n",
      "loss: 0.487027  [73600/175341]\n",
      "loss: 0.338524  [75200/175341]\n",
      "loss: 0.490751  [76800/175341]\n",
      "loss: 0.530263  [78400/175341]\n",
      "loss: 0.564401  [80000/175341]\n",
      "loss: 0.478519  [81600/175341]\n",
      "loss: 0.371639  [83200/175341]\n",
      "loss: 0.905715  [84800/175341]\n",
      "loss: 0.413875  [86400/175341]\n",
      "loss: 0.454980  [88000/175341]\n",
      "loss: 0.351473  [89600/175341]\n",
      "loss: 0.819014  [91200/175341]\n",
      "loss: 0.685293  [92800/175341]\n",
      "loss: 0.170921  [94400/175341]\n",
      "loss: 0.215954  [96000/175341]\n",
      "loss: 0.399169  [97600/175341]\n",
      "loss: 0.469945  [99200/175341]\n",
      "loss: 0.211470  [100800/175341]\n",
      "loss: 0.411239  [102400/175341]\n",
      "loss: 0.762372  [104000/175341]\n",
      "loss: 0.907624  [105600/175341]\n",
      "loss: 0.755399  [107200/175341]\n",
      "loss: 0.454633  [108800/175341]\n",
      "loss: 0.753014  [110400/175341]\n",
      "loss: 1.134345  [112000/175341]\n",
      "loss: 0.439767  [113600/175341]\n",
      "loss: 0.335464  [115200/175341]\n",
      "loss: 0.799217  [116800/175341]\n",
      "loss: 0.436117  [118400/175341]\n",
      "loss: 0.632660  [120000/175341]\n",
      "loss: 0.022222  [121600/175341]\n",
      "loss: 0.291218  [123200/175341]\n",
      "loss: 0.380018  [124800/175341]\n",
      "loss: 0.707927  [126400/175341]\n",
      "loss: 0.612950  [128000/175341]\n",
      "loss: 0.729922  [129600/175341]\n",
      "loss: 0.601912  [131200/175341]\n",
      "loss: 0.491181  [132800/175341]\n",
      "loss: 0.480643  [134400/175341]\n",
      "loss: 0.446032  [136000/175341]\n",
      "loss: 0.490772  [137600/175341]\n",
      "loss: 0.654342  [139200/175341]\n",
      "loss: 0.615270  [140800/175341]\n",
      "loss: 0.564572  [142400/175341]\n",
      "loss: 0.468675  [144000/175341]\n",
      "loss: 0.538135  [145600/175341]\n",
      "loss: 0.545241  [147200/175341]\n",
      "loss: 0.683826  [148800/175341]\n",
      "loss: 0.590642  [150400/175341]\n",
      "loss: 0.425945  [152000/175341]\n",
      "loss: 0.607805  [153600/175341]\n",
      "loss: 0.217806  [155200/175341]\n",
      "loss: 0.698903  [156800/175341]\n",
      "loss: 0.446249  [158400/175341]\n",
      "loss: 0.504640  [160000/175341]\n",
      "loss: 0.311684  [161600/175341]\n",
      "loss: 1.089379  [163200/175341]\n",
      "loss: 0.234447  [164800/175341]\n",
      "loss: 0.504023  [166400/175341]\n",
      "loss: 0.526125  [168000/175341]\n",
      "loss: 0.192348  [169600/175341]\n",
      "loss: 0.526853  [171200/175341]\n",
      "loss: 0.397636  [172800/175341]\n",
      "loss: 0.678851  [174400/175341]\n",
      "Train Accuracy: 80.3971%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.559610, F1-score: 74.93%, Macro_F1-Score:  39.99%  \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.375558  [    0/175341]\n",
      "loss: 0.183005  [ 1600/175341]\n",
      "loss: 0.598239  [ 3200/175341]\n",
      "loss: 0.232689  [ 4800/175341]\n",
      "loss: 0.744404  [ 6400/175341]\n",
      "loss: 0.183794  [ 8000/175341]\n",
      "loss: 0.718515  [ 9600/175341]\n",
      "loss: 0.509388  [11200/175341]\n",
      "loss: 0.259756  [12800/175341]\n",
      "loss: 0.339252  [14400/175341]\n",
      "loss: 0.295156  [16000/175341]\n",
      "loss: 0.816661  [17600/175341]\n",
      "loss: 0.879235  [19200/175341]\n",
      "loss: 0.379397  [20800/175341]\n",
      "loss: 0.365569  [22400/175341]\n",
      "loss: 0.905127  [24000/175341]\n",
      "loss: 0.242261  [25600/175341]\n",
      "loss: 0.287529  [27200/175341]\n",
      "loss: 0.517797  [28800/175341]\n",
      "loss: 0.715334  [30400/175341]\n",
      "loss: 0.583114  [32000/175341]\n",
      "loss: 0.405144  [33600/175341]\n",
      "loss: 0.690523  [35200/175341]\n",
      "loss: 0.568450  [36800/175341]\n",
      "loss: 0.910053  [38400/175341]\n",
      "loss: 0.336829  [40000/175341]\n",
      "loss: 0.527134  [41600/175341]\n",
      "loss: 0.287458  [43200/175341]\n",
      "loss: 0.528755  [44800/175341]\n",
      "loss: 0.371442  [46400/175341]\n",
      "loss: 0.684290  [48000/175341]\n",
      "loss: 0.447798  [49600/175341]\n",
      "loss: 0.746896  [51200/175341]\n",
      "loss: 0.672174  [52800/175341]\n",
      "loss: 0.760252  [54400/175341]\n",
      "loss: 0.366416  [56000/175341]\n",
      "loss: 0.284577  [57600/175341]\n",
      "loss: 0.145088  [59200/175341]\n",
      "loss: 0.685052  [60800/175341]\n",
      "loss: 0.532721  [62400/175341]\n",
      "loss: 0.408915  [64000/175341]\n",
      "loss: 0.435181  [65600/175341]\n",
      "loss: 0.320148  [67200/175341]\n",
      "loss: 0.352991  [68800/175341]\n",
      "loss: 0.395636  [70400/175341]\n",
      "loss: 0.498599  [72000/175341]\n",
      "loss: 0.492556  [73600/175341]\n",
      "loss: 0.838242  [75200/175341]\n",
      "loss: 0.361014  [76800/175341]\n",
      "loss: 0.441417  [78400/175341]\n",
      "loss: 0.453637  [80000/175341]\n",
      "loss: 0.595964  [81600/175341]\n",
      "loss: 0.557123  [83200/175341]\n",
      "loss: 0.835879  [84800/175341]\n",
      "loss: 0.167144  [86400/175341]\n",
      "loss: 0.325631  [88000/175341]\n",
      "loss: 0.336304  [89600/175341]\n",
      "loss: 0.415039  [91200/175341]\n",
      "loss: 0.357733  [92800/175341]\n",
      "loss: 0.257974  [94400/175341]\n",
      "loss: 0.394108  [96000/175341]\n",
      "loss: 0.274311  [97600/175341]\n",
      "loss: 0.531352  [99200/175341]\n",
      "loss: 0.689034  [100800/175341]\n",
      "loss: 0.359008  [102400/175341]\n",
      "loss: 0.363643  [104000/175341]\n",
      "loss: 0.140619  [105600/175341]\n",
      "loss: 0.731182  [107200/175341]\n",
      "loss: 0.664556  [108800/175341]\n",
      "loss: 0.423067  [110400/175341]\n",
      "loss: 0.352153  [112000/175341]\n",
      "loss: 0.139975  [113600/175341]\n",
      "loss: 0.450330  [115200/175341]\n",
      "loss: 0.623452  [116800/175341]\n",
      "loss: 0.281909  [118400/175341]\n",
      "loss: 0.859591  [120000/175341]\n",
      "loss: 0.348149  [121600/175341]\n",
      "loss: 0.532645  [123200/175341]\n",
      "loss: 0.505007  [124800/175341]\n",
      "loss: 0.506959  [126400/175341]\n",
      "loss: 0.422188  [128000/175341]\n",
      "loss: 1.048827  [129600/175341]\n",
      "loss: 0.569344  [131200/175341]\n",
      "loss: 0.329025  [132800/175341]\n",
      "loss: 0.890655  [134400/175341]\n",
      "loss: 0.732094  [136000/175341]\n",
      "loss: 0.419707  [137600/175341]\n",
      "loss: 0.549772  [139200/175341]\n",
      "loss: 0.476028  [140800/175341]\n",
      "loss: 0.498473  [142400/175341]\n",
      "loss: 0.157740  [144000/175341]\n",
      "loss: 0.418937  [145600/175341]\n",
      "loss: 0.590713  [147200/175341]\n",
      "loss: 0.409504  [148800/175341]\n",
      "loss: 0.629103  [150400/175341]\n",
      "loss: 1.010661  [152000/175341]\n",
      "loss: 0.357431  [153600/175341]\n",
      "loss: 0.617902  [155200/175341]\n",
      "loss: 0.757258  [156800/175341]\n",
      "loss: 0.689160  [158400/175341]\n",
      "loss: 0.736110  [160000/175341]\n",
      "loss: 0.393796  [161600/175341]\n",
      "loss: 0.442377  [163200/175341]\n",
      "loss: 1.117959  [164800/175341]\n",
      "loss: 0.682668  [166400/175341]\n",
      "loss: 0.354707  [168000/175341]\n",
      "loss: 0.391068  [169600/175341]\n",
      "loss: 0.333462  [171200/175341]\n",
      "loss: 0.566386  [172800/175341]\n",
      "loss: 0.399456  [174400/175341]\n",
      "Train Accuracy: 80.4364%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.565225, F1-score: 74.37%, Macro_F1-Score:  39.31%  \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.963150  [    0/175341]\n",
      "loss: 0.343591  [ 1600/175341]\n",
      "loss: 0.321008  [ 3200/175341]\n",
      "loss: 0.207683  [ 4800/175341]\n",
      "loss: 0.475942  [ 6400/175341]\n",
      "loss: 0.542222  [ 8000/175341]\n",
      "loss: 0.336985  [ 9600/175341]\n",
      "loss: 0.135225  [11200/175341]\n",
      "loss: 0.262143  [12800/175341]\n",
      "loss: 0.371974  [14400/175341]\n",
      "loss: 0.129704  [16000/175341]\n",
      "loss: 0.792928  [17600/175341]\n",
      "loss: 0.570247  [19200/175341]\n",
      "loss: 0.499523  [20800/175341]\n",
      "loss: 0.484470  [22400/175341]\n",
      "loss: 0.413276  [24000/175341]\n",
      "loss: 0.610898  [25600/175341]\n",
      "loss: 0.443864  [27200/175341]\n",
      "loss: 0.460076  [28800/175341]\n",
      "loss: 0.798141  [30400/175341]\n",
      "loss: 0.142996  [32000/175341]\n",
      "loss: 0.551836  [33600/175341]\n",
      "loss: 0.205827  [35200/175341]\n",
      "loss: 0.259318  [36800/175341]\n",
      "loss: 0.343596  [38400/175341]\n",
      "loss: 0.299826  [40000/175341]\n",
      "loss: 0.295345  [41600/175341]\n",
      "loss: 0.649981  [43200/175341]\n",
      "loss: 0.258749  [44800/175341]\n",
      "loss: 0.503565  [46400/175341]\n",
      "loss: 0.197112  [48000/175341]\n",
      "loss: 0.568736  [49600/175341]\n",
      "loss: 0.488634  [51200/175341]\n",
      "loss: 0.274011  [52800/175341]\n",
      "loss: 0.435699  [54400/175341]\n",
      "loss: 0.431881  [56000/175341]\n",
      "loss: 0.420080  [57600/175341]\n",
      "loss: 0.746493  [59200/175341]\n",
      "loss: 0.277955  [60800/175341]\n",
      "loss: 0.411447  [62400/175341]\n",
      "loss: 0.556643  [64000/175341]\n",
      "loss: 0.438939  [65600/175341]\n",
      "loss: 0.332498  [67200/175341]\n",
      "loss: 0.334227  [68800/175341]\n",
      "loss: 0.338810  [70400/175341]\n",
      "loss: 0.247926  [72000/175341]\n",
      "loss: 0.604655  [73600/175341]\n",
      "loss: 0.761780  [75200/175341]\n",
      "loss: 0.241632  [76800/175341]\n",
      "loss: 0.873888  [78400/175341]\n",
      "loss: 0.338130  [80000/175341]\n",
      "loss: 0.350050  [81600/175341]\n",
      "loss: 0.230649  [83200/175341]\n",
      "loss: 0.706598  [84800/175341]\n",
      "loss: 1.015408  [86400/175341]\n",
      "loss: 0.155258  [88000/175341]\n",
      "loss: 0.282144  [89600/175341]\n",
      "loss: 0.316565  [91200/175341]\n",
      "loss: 0.590863  [92800/175341]\n",
      "loss: 0.298984  [94400/175341]\n",
      "loss: 0.420497  [96000/175341]\n",
      "loss: 0.471808  [97600/175341]\n",
      "loss: 0.307674  [99200/175341]\n",
      "loss: 0.655816  [100800/175341]\n",
      "loss: 0.681186  [102400/175341]\n",
      "loss: 0.363787  [104000/175341]\n",
      "loss: 0.298766  [105600/175341]\n",
      "loss: 0.160087  [107200/175341]\n",
      "loss: 0.384842  [108800/175341]\n",
      "loss: 0.450144  [110400/175341]\n",
      "loss: 0.276613  [112000/175341]\n",
      "loss: 0.459473  [113600/175341]\n",
      "loss: 0.880589  [115200/175341]\n",
      "loss: 0.504567  [116800/175341]\n",
      "loss: 0.769691  [118400/175341]\n",
      "loss: 0.560273  [120000/175341]\n",
      "loss: 0.369486  [121600/175341]\n",
      "loss: 0.565610  [123200/175341]\n",
      "loss: 0.170263  [124800/175341]\n",
      "loss: 0.475525  [126400/175341]\n",
      "loss: 0.821063  [128000/175341]\n",
      "loss: 0.407739  [129600/175341]\n",
      "loss: 0.144455  [131200/175341]\n",
      "loss: 0.263793  [132800/175341]\n",
      "loss: 0.677788  [134400/175341]\n",
      "loss: 0.752254  [136000/175341]\n",
      "loss: 0.390935  [137600/175341]\n",
      "loss: 0.163380  [139200/175341]\n",
      "loss: 0.988508  [140800/175341]\n",
      "loss: 0.197328  [142400/175341]\n",
      "loss: 0.330694  [144000/175341]\n",
      "loss: 0.601859  [145600/175341]\n",
      "loss: 0.236643  [147200/175341]\n",
      "loss: 0.373359  [148800/175341]\n",
      "loss: 0.496537  [150400/175341]\n",
      "loss: 0.331764  [152000/175341]\n",
      "loss: 0.567226  [153600/175341]\n",
      "loss: 0.465251  [155200/175341]\n",
      "loss: 0.402734  [156800/175341]\n",
      "loss: 0.374610  [158400/175341]\n",
      "loss: 0.094712  [160000/175341]\n",
      "loss: 0.294780  [161600/175341]\n",
      "loss: 0.490429  [163200/175341]\n",
      "loss: 0.814450  [164800/175341]\n",
      "loss: 0.412039  [166400/175341]\n",
      "loss: 0.480690  [168000/175341]\n",
      "loss: 0.441694  [169600/175341]\n",
      "loss: 0.257571  [171200/175341]\n",
      "loss: 0.511503  [172800/175341]\n",
      "loss: 0.594752  [174400/175341]\n",
      "Train Accuracy: 80.3959%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.561953, F1-score: 75.15%, Macro_F1-Score:  40.28%  \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.276226  [    0/175341]\n",
      "loss: 0.505929  [ 1600/175341]\n",
      "loss: 0.476491  [ 3200/175341]\n",
      "loss: 0.171300  [ 4800/175341]\n",
      "loss: 0.335382  [ 6400/175341]\n",
      "loss: 0.683975  [ 8000/175341]\n",
      "loss: 0.122690  [ 9600/175341]\n",
      "loss: 0.961465  [11200/175341]\n",
      "loss: 0.532648  [12800/175341]\n",
      "loss: 0.314594  [14400/175341]\n",
      "loss: 0.457669  [16000/175341]\n",
      "loss: 0.296148  [17600/175341]\n",
      "loss: 0.564220  [19200/175341]\n",
      "loss: 0.418407  [20800/175341]\n",
      "loss: 0.323207  [22400/175341]\n",
      "loss: 0.382135  [24000/175341]\n",
      "loss: 0.841325  [25600/175341]\n",
      "loss: 0.415961  [27200/175341]\n",
      "loss: 0.361825  [28800/175341]\n",
      "loss: 0.398471  [30400/175341]\n",
      "loss: 0.477744  [32000/175341]\n",
      "loss: 0.553513  [33600/175341]\n",
      "loss: 0.880866  [35200/175341]\n",
      "loss: 0.452430  [36800/175341]\n",
      "loss: 0.314453  [38400/175341]\n",
      "loss: 0.340097  [40000/175341]\n",
      "loss: 0.648830  [41600/175341]\n",
      "loss: 0.148284  [43200/175341]\n",
      "loss: 0.231317  [44800/175341]\n",
      "loss: 0.441299  [46400/175341]\n",
      "loss: 0.786614  [48000/175341]\n",
      "loss: 0.291223  [49600/175341]\n",
      "loss: 0.596459  [51200/175341]\n",
      "loss: 0.588228  [52800/175341]\n",
      "loss: 0.385349  [54400/175341]\n",
      "loss: 0.177477  [56000/175341]\n",
      "loss: 0.706382  [57600/175341]\n",
      "loss: 0.369089  [59200/175341]\n",
      "loss: 0.351872  [60800/175341]\n",
      "loss: 0.458561  [62400/175341]\n",
      "loss: 0.544173  [64000/175341]\n",
      "loss: 0.558273  [65600/175341]\n",
      "loss: 0.438785  [67200/175341]\n",
      "loss: 0.200436  [68800/175341]\n",
      "loss: 0.775532  [70400/175341]\n",
      "loss: 0.489484  [72000/175341]\n",
      "loss: 0.409319  [73600/175341]\n",
      "loss: 0.705451  [75200/175341]\n",
      "loss: 0.467402  [76800/175341]\n",
      "loss: 0.328733  [78400/175341]\n",
      "loss: 0.443185  [80000/175341]\n",
      "loss: 0.167807  [81600/175341]\n",
      "loss: 0.428353  [83200/175341]\n",
      "loss: 0.585212  [84800/175341]\n",
      "loss: 0.780010  [86400/175341]\n",
      "loss: 0.665475  [88000/175341]\n",
      "loss: 0.695605  [89600/175341]\n",
      "loss: 0.442389  [91200/175341]\n",
      "loss: 0.547548  [92800/175341]\n",
      "loss: 0.496266  [94400/175341]\n",
      "loss: 0.561281  [96000/175341]\n",
      "loss: 0.608663  [97600/175341]\n",
      "loss: 0.776195  [99200/175341]\n",
      "loss: 0.402255  [100800/175341]\n",
      "loss: 0.304575  [102400/175341]\n",
      "loss: 0.529016  [104000/175341]\n",
      "loss: 0.673129  [105600/175341]\n",
      "loss: 0.683646  [107200/175341]\n",
      "loss: 0.335700  [108800/175341]\n",
      "loss: 0.436985  [110400/175341]\n",
      "loss: 0.501495  [112000/175341]\n",
      "loss: 0.321889  [113600/175341]\n",
      "loss: 0.585264  [115200/175341]\n",
      "loss: 0.679583  [116800/175341]\n",
      "loss: 0.432505  [118400/175341]\n",
      "loss: 0.227359  [120000/175341]\n",
      "loss: 0.725258  [121600/175341]\n",
      "loss: 0.475959  [123200/175341]\n",
      "loss: 0.600970  [124800/175341]\n",
      "loss: 0.362798  [126400/175341]\n",
      "loss: 0.675348  [128000/175341]\n",
      "loss: 0.531026  [129600/175341]\n",
      "loss: 0.357417  [131200/175341]\n",
      "loss: 0.323108  [132800/175341]\n",
      "loss: 0.532020  [134400/175341]\n",
      "loss: 0.678270  [136000/175341]\n",
      "loss: 0.521950  [137600/175341]\n",
      "loss: 0.839503  [139200/175341]\n",
      "loss: 0.467907  [140800/175341]\n",
      "loss: 0.541405  [142400/175341]\n",
      "loss: 0.181235  [144000/175341]\n",
      "loss: 0.721621  [145600/175341]\n",
      "loss: 0.245862  [147200/175341]\n",
      "loss: 0.433176  [148800/175341]\n",
      "loss: 0.533816  [150400/175341]\n",
      "loss: 0.201969  [152000/175341]\n",
      "loss: 0.595238  [153600/175341]\n",
      "loss: 0.549803  [155200/175341]\n",
      "loss: 0.401744  [156800/175341]\n",
      "loss: 0.523408  [158400/175341]\n",
      "loss: 0.418928  [160000/175341]\n",
      "loss: 0.380022  [161600/175341]\n",
      "loss: 0.158773  [163200/175341]\n",
      "loss: 0.513779  [164800/175341]\n",
      "loss: 0.342158  [166400/175341]\n",
      "loss: 0.398129  [168000/175341]\n",
      "loss: 0.350131  [169600/175341]\n",
      "loss: 0.516948  [171200/175341]\n",
      "loss: 0.179712  [172800/175341]\n",
      "loss: 0.459934  [174400/175341]\n",
      "Train Accuracy: 80.4552%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.547102, F1-score: 76.15%, Macro_F1-Score:  40.22%  \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.723746  [    0/175341]\n",
      "loss: 0.326966  [ 1600/175341]\n",
      "loss: 0.975799  [ 3200/175341]\n",
      "loss: 0.378396  [ 4800/175341]\n",
      "loss: 0.085897  [ 6400/175341]\n",
      "loss: 0.619483  [ 8000/175341]\n",
      "loss: 0.734205  [ 9600/175341]\n",
      "loss: 0.738279  [11200/175341]\n",
      "loss: 0.210328  [12800/175341]\n",
      "loss: 0.364131  [14400/175341]\n",
      "loss: 0.752832  [16000/175341]\n",
      "loss: 0.425108  [17600/175341]\n",
      "loss: 0.504068  [19200/175341]\n",
      "loss: 0.840801  [20800/175341]\n",
      "loss: 0.558329  [22400/175341]\n",
      "loss: 0.271667  [24000/175341]\n",
      "loss: 0.361445  [25600/175341]\n",
      "loss: 0.376002  [27200/175341]\n",
      "loss: 0.898731  [28800/175341]\n",
      "loss: 0.656486  [30400/175341]\n",
      "loss: 0.511110  [32000/175341]\n",
      "loss: 0.562903  [33600/175341]\n",
      "loss: 0.558639  [35200/175341]\n",
      "loss: 1.182375  [36800/175341]\n",
      "loss: 0.230326  [38400/175341]\n",
      "loss: 0.313277  [40000/175341]\n",
      "loss: 0.171958  [41600/175341]\n",
      "loss: 0.953412  [43200/175341]\n",
      "loss: 0.185326  [44800/175341]\n",
      "loss: 0.193121  [46400/175341]\n",
      "loss: 0.585974  [48000/175341]\n",
      "loss: 0.370375  [49600/175341]\n",
      "loss: 0.466833  [51200/175341]\n",
      "loss: 0.639140  [52800/175341]\n",
      "loss: 0.757111  [54400/175341]\n",
      "loss: 0.876978  [56000/175341]\n",
      "loss: 0.471356  [57600/175341]\n",
      "loss: 0.239991  [59200/175341]\n",
      "loss: 0.517998  [60800/175341]\n",
      "loss: 0.381549  [62400/175341]\n",
      "loss: 0.578677  [64000/175341]\n",
      "loss: 0.357028  [65600/175341]\n",
      "loss: 0.265131  [67200/175341]\n",
      "loss: 0.365579  [68800/175341]\n",
      "loss: 0.166466  [70400/175341]\n",
      "loss: 0.378104  [72000/175341]\n",
      "loss: 0.410994  [73600/175341]\n",
      "loss: 0.315746  [75200/175341]\n",
      "loss: 0.227378  [76800/175341]\n",
      "loss: 0.872612  [78400/175341]\n",
      "loss: 0.991083  [80000/175341]\n",
      "loss: 0.383407  [81600/175341]\n",
      "loss: 0.201811  [83200/175341]\n",
      "loss: 0.888829  [84800/175341]\n",
      "loss: 0.383919  [86400/175341]\n",
      "loss: 0.136750  [88000/175341]\n",
      "loss: 0.443458  [89600/175341]\n",
      "loss: 0.854905  [91200/175341]\n",
      "loss: 0.600556  [92800/175341]\n",
      "loss: 0.297884  [94400/175341]\n",
      "loss: 0.728462  [96000/175341]\n",
      "loss: 0.320417  [97600/175341]\n",
      "loss: 0.849832  [99200/175341]\n",
      "loss: 0.448544  [100800/175341]\n",
      "loss: 0.470290  [102400/175341]\n",
      "loss: 0.753219  [104000/175341]\n",
      "loss: 0.178744  [105600/175341]\n",
      "loss: 0.376311  [107200/175341]\n",
      "loss: 0.369298  [108800/175341]\n",
      "loss: 0.638395  [110400/175341]\n",
      "loss: 0.517033  [112000/175341]\n",
      "loss: 0.499227  [113600/175341]\n",
      "loss: 0.102813  [115200/175341]\n",
      "loss: 0.398185  [116800/175341]\n",
      "loss: 0.298610  [118400/175341]\n",
      "loss: 0.319057  [120000/175341]\n",
      "loss: 0.488649  [121600/175341]\n",
      "loss: 0.326872  [123200/175341]\n",
      "loss: 0.413370  [124800/175341]\n",
      "loss: 0.329388  [126400/175341]\n",
      "loss: 0.606197  [128000/175341]\n",
      "loss: 0.374484  [129600/175341]\n",
      "loss: 1.094433  [131200/175341]\n",
      "loss: 0.994273  [132800/175341]\n",
      "loss: 0.374703  [134400/175341]\n",
      "loss: 0.475669  [136000/175341]\n",
      "loss: 0.245208  [137600/175341]\n",
      "loss: 0.678953  [139200/175341]\n",
      "loss: 0.679275  [140800/175341]\n",
      "loss: 0.587051  [142400/175341]\n",
      "loss: 1.174582  [144000/175341]\n",
      "loss: 0.133922  [145600/175341]\n",
      "loss: 0.591252  [147200/175341]\n",
      "loss: 0.534964  [148800/175341]\n",
      "loss: 0.856097  [150400/175341]\n",
      "loss: 0.456265  [152000/175341]\n",
      "loss: 0.998734  [153600/175341]\n",
      "loss: 0.588433  [155200/175341]\n",
      "loss: 0.292288  [156800/175341]\n",
      "loss: 0.417838  [158400/175341]\n",
      "loss: 0.517769  [160000/175341]\n",
      "loss: 0.325463  [161600/175341]\n",
      "loss: 0.708345  [163200/175341]\n",
      "loss: 0.398889  [164800/175341]\n",
      "loss: 0.275432  [166400/175341]\n",
      "loss: 0.131661  [168000/175341]\n",
      "loss: 0.236504  [169600/175341]\n",
      "loss: 0.596938  [171200/175341]\n",
      "loss: 0.434264  [172800/175341]\n",
      "loss: 0.774097  [174400/175341]\n",
      "Train Accuracy: 80.4632%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.557187, F1-score: 76.05%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.791817  [    0/175341]\n",
      "loss: 0.532818  [ 1600/175341]\n",
      "loss: 0.477072  [ 3200/175341]\n",
      "loss: 0.458805  [ 4800/175341]\n",
      "loss: 0.363113  [ 6400/175341]\n",
      "loss: 0.643178  [ 8000/175341]\n",
      "loss: 0.445947  [ 9600/175341]\n",
      "loss: 0.913029  [11200/175341]\n",
      "loss: 0.507874  [12800/175341]\n",
      "loss: 0.551012  [14400/175341]\n",
      "loss: 0.332043  [16000/175341]\n",
      "loss: 0.413266  [17600/175341]\n",
      "loss: 0.216082  [19200/175341]\n",
      "loss: 0.337382  [20800/175341]\n",
      "loss: 0.557409  [22400/175341]\n",
      "loss: 0.632918  [24000/175341]\n",
      "loss: 0.651676  [25600/175341]\n",
      "loss: 1.016473  [27200/175341]\n",
      "loss: 0.779792  [28800/175341]\n",
      "loss: 0.367072  [30400/175341]\n",
      "loss: 0.743549  [32000/175341]\n",
      "loss: 0.152063  [33600/175341]\n",
      "loss: 0.593381  [35200/175341]\n",
      "loss: 0.308891  [36800/175341]\n",
      "loss: 0.764691  [38400/175341]\n",
      "loss: 0.560232  [40000/175341]\n",
      "loss: 0.371287  [41600/175341]\n",
      "loss: 0.833435  [43200/175341]\n",
      "loss: 0.724713  [44800/175341]\n",
      "loss: 0.340419  [46400/175341]\n",
      "loss: 0.398355  [48000/175341]\n",
      "loss: 0.174968  [49600/175341]\n",
      "loss: 0.625596  [51200/175341]\n",
      "loss: 0.580582  [52800/175341]\n",
      "loss: 0.432499  [54400/175341]\n",
      "loss: 0.431358  [56000/175341]\n",
      "loss: 0.424056  [57600/175341]\n",
      "loss: 0.267405  [59200/175341]\n",
      "loss: 0.605051  [60800/175341]\n",
      "loss: 1.011427  [62400/175341]\n",
      "loss: 0.569367  [64000/175341]\n",
      "loss: 0.542726  [65600/175341]\n",
      "loss: 0.366829  [67200/175341]\n",
      "loss: 0.411434  [68800/175341]\n",
      "loss: 0.796336  [70400/175341]\n",
      "loss: 0.879918  [72000/175341]\n",
      "loss: 1.238930  [73600/175341]\n",
      "loss: 0.699037  [75200/175341]\n",
      "loss: 0.406018  [76800/175341]\n",
      "loss: 0.486334  [78400/175341]\n",
      "loss: 0.361425  [80000/175341]\n",
      "loss: 0.365581  [81600/175341]\n",
      "loss: 0.284022  [83200/175341]\n",
      "loss: 0.335501  [84800/175341]\n",
      "loss: 0.693454  [86400/175341]\n",
      "loss: 0.493587  [88000/175341]\n",
      "loss: 0.858541  [89600/175341]\n",
      "loss: 0.489439  [91200/175341]\n",
      "loss: 0.353775  [92800/175341]\n",
      "loss: 0.483105  [94400/175341]\n",
      "loss: 0.628948  [96000/175341]\n",
      "loss: 0.649221  [97600/175341]\n",
      "loss: 0.800313  [99200/175341]\n",
      "loss: 0.437483  [100800/175341]\n",
      "loss: 0.216762  [102400/175341]\n",
      "loss: 0.484643  [104000/175341]\n",
      "loss: 0.336763  [105600/175341]\n",
      "loss: 0.259359  [107200/175341]\n",
      "loss: 0.470664  [108800/175341]\n",
      "loss: 0.358623  [110400/175341]\n",
      "loss: 0.557342  [112000/175341]\n",
      "loss: 0.386503  [113600/175341]\n",
      "loss: 0.563139  [115200/175341]\n",
      "loss: 0.562111  [116800/175341]\n",
      "loss: 0.799109  [118400/175341]\n",
      "loss: 0.167121  [120000/175341]\n",
      "loss: 0.527937  [121600/175341]\n",
      "loss: 0.587659  [123200/175341]\n",
      "loss: 0.700162  [124800/175341]\n",
      "loss: 0.310146  [126400/175341]\n",
      "loss: 0.450608  [128000/175341]\n",
      "loss: 0.069799  [129600/175341]\n",
      "loss: 0.610922  [131200/175341]\n",
      "loss: 0.801624  [132800/175341]\n",
      "loss: 0.654537  [134400/175341]\n",
      "loss: 0.450829  [136000/175341]\n",
      "loss: 0.600964  [137600/175341]\n",
      "loss: 0.295585  [139200/175341]\n",
      "loss: 0.580355  [140800/175341]\n",
      "loss: 0.354968  [142400/175341]\n",
      "loss: 0.300094  [144000/175341]\n",
      "loss: 0.746020  [145600/175341]\n",
      "loss: 0.709510  [147200/175341]\n",
      "loss: 0.423003  [148800/175341]\n",
      "loss: 0.430898  [150400/175341]\n",
      "loss: 0.704800  [152000/175341]\n",
      "loss: 0.581325  [153600/175341]\n",
      "loss: 0.644674  [155200/175341]\n",
      "loss: 0.628623  [156800/175341]\n",
      "loss: 0.598377  [158400/175341]\n",
      "loss: 0.546375  [160000/175341]\n",
      "loss: 0.212917  [161600/175341]\n",
      "loss: 0.331046  [163200/175341]\n",
      "loss: 0.162757  [164800/175341]\n",
      "loss: 0.322621  [166400/175341]\n",
      "loss: 0.246158  [168000/175341]\n",
      "loss: 0.468489  [169600/175341]\n",
      "loss: 0.544430  [171200/175341]\n",
      "loss: 0.244996  [172800/175341]\n",
      "loss: 0.520373  [174400/175341]\n",
      "Train Accuracy: 80.4701%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.566704, F1-score: 74.67%, Macro_F1-Score:  40.53%  \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.366082  [    0/175341]\n",
      "loss: 0.613695  [ 1600/175341]\n",
      "loss: 0.557060  [ 3200/175341]\n",
      "loss: 0.431046  [ 4800/175341]\n",
      "loss: 0.254514  [ 6400/175341]\n",
      "loss: 0.348566  [ 8000/175341]\n",
      "loss: 0.516685  [ 9600/175341]\n",
      "loss: 0.240565  [11200/175341]\n",
      "loss: 0.400967  [12800/175341]\n",
      "loss: 0.318060  [14400/175341]\n",
      "loss: 0.335212  [16000/175341]\n",
      "loss: 0.686727  [17600/175341]\n",
      "loss: 0.186500  [19200/175341]\n",
      "loss: 0.596957  [20800/175341]\n",
      "loss: 0.473291  [22400/175341]\n",
      "loss: 0.583340  [24000/175341]\n",
      "loss: 0.600815  [25600/175341]\n",
      "loss: 0.653098  [27200/175341]\n",
      "loss: 0.173845  [28800/175341]\n",
      "loss: 0.311778  [30400/175341]\n",
      "loss: 0.354121  [32000/175341]\n",
      "loss: 0.580907  [33600/175341]\n",
      "loss: 0.571219  [35200/175341]\n",
      "loss: 0.594179  [36800/175341]\n",
      "loss: 0.376829  [38400/175341]\n",
      "loss: 0.161972  [40000/175341]\n",
      "loss: 0.723656  [41600/175341]\n",
      "loss: 0.631648  [43200/175341]\n",
      "loss: 0.303081  [44800/175341]\n",
      "loss: 0.522729  [46400/175341]\n",
      "loss: 0.215448  [48000/175341]\n",
      "loss: 0.592360  [49600/175341]\n",
      "loss: 0.503667  [51200/175341]\n",
      "loss: 0.464546  [52800/175341]\n",
      "loss: 1.011463  [54400/175341]\n",
      "loss: 0.305029  [56000/175341]\n",
      "loss: 0.258942  [57600/175341]\n",
      "loss: 0.423465  [59200/175341]\n",
      "loss: 0.939578  [60800/175341]\n",
      "loss: 0.491318  [62400/175341]\n",
      "loss: 0.644074  [64000/175341]\n",
      "loss: 0.476352  [65600/175341]\n",
      "loss: 0.389989  [67200/175341]\n",
      "loss: 0.275985  [68800/175341]\n",
      "loss: 0.718247  [70400/175341]\n",
      "loss: 0.747840  [72000/175341]\n",
      "loss: 0.449260  [73600/175341]\n",
      "loss: 0.343192  [75200/175341]\n",
      "loss: 0.295249  [76800/175341]\n",
      "loss: 0.282290  [78400/175341]\n",
      "loss: 0.665908  [80000/175341]\n",
      "loss: 0.249864  [81600/175341]\n",
      "loss: 0.484678  [83200/175341]\n",
      "loss: 0.442483  [84800/175341]\n",
      "loss: 0.321268  [86400/175341]\n",
      "loss: 0.589412  [88000/175341]\n",
      "loss: 1.196477  [89600/175341]\n",
      "loss: 0.326159  [91200/175341]\n",
      "loss: 0.399923  [92800/175341]\n",
      "loss: 0.500918  [94400/175341]\n",
      "loss: 0.539130  [96000/175341]\n",
      "loss: 0.727442  [97600/175341]\n",
      "loss: 0.373303  [99200/175341]\n",
      "loss: 0.778553  [100800/175341]\n",
      "loss: 0.571022  [102400/175341]\n",
      "loss: 0.549799  [104000/175341]\n",
      "loss: 0.435864  [105600/175341]\n",
      "loss: 0.573586  [107200/175341]\n",
      "loss: 0.412225  [108800/175341]\n",
      "loss: 0.343539  [110400/175341]\n",
      "loss: 0.480951  [112000/175341]\n",
      "loss: 0.939839  [113600/175341]\n",
      "loss: 0.648775  [115200/175341]\n",
      "loss: 0.317387  [116800/175341]\n",
      "loss: 0.669148  [118400/175341]\n",
      "loss: 0.522707  [120000/175341]\n",
      "loss: 0.356382  [121600/175341]\n",
      "loss: 0.478532  [123200/175341]\n",
      "loss: 0.845355  [124800/175341]\n",
      "loss: 0.837446  [126400/175341]\n",
      "loss: 0.479506  [128000/175341]\n",
      "loss: 0.405152  [129600/175341]\n",
      "loss: 0.312197  [131200/175341]\n",
      "loss: 0.180790  [132800/175341]\n",
      "loss: 0.408197  [134400/175341]\n",
      "loss: 1.087589  [136000/175341]\n",
      "loss: 0.775871  [137600/175341]\n",
      "loss: 0.312769  [139200/175341]\n",
      "loss: 0.634159  [140800/175341]\n",
      "loss: 0.771235  [142400/175341]\n",
      "loss: 0.612095  [144000/175341]\n",
      "loss: 0.196135  [145600/175341]\n",
      "loss: 0.516405  [147200/175341]\n",
      "loss: 0.300558  [148800/175341]\n",
      "loss: 0.313958  [150400/175341]\n",
      "loss: 0.614892  [152000/175341]\n",
      "loss: 0.072855  [153600/175341]\n",
      "loss: 0.480647  [155200/175341]\n",
      "loss: 0.223629  [156800/175341]\n",
      "loss: 0.798767  [158400/175341]\n",
      "loss: 0.882225  [160000/175341]\n",
      "loss: 0.782828  [161600/175341]\n",
      "loss: 0.321871  [163200/175341]\n",
      "loss: 0.542795  [164800/175341]\n",
      "loss: 0.270472  [166400/175341]\n",
      "loss: 0.329854  [168000/175341]\n",
      "loss: 0.632602  [169600/175341]\n",
      "loss: 0.441010  [171200/175341]\n",
      "loss: 0.537377  [172800/175341]\n",
      "loss: 0.657981  [174400/175341]\n",
      "Train Accuracy: 80.5037%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.564928, F1-score: 74.98%, Macro_F1-Score:  39.31%  \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.702352  [    0/175341]\n",
      "loss: 0.796894  [ 1600/175341]\n",
      "loss: 0.555436  [ 3200/175341]\n",
      "loss: 0.911415  [ 4800/175341]\n",
      "loss: 0.841246  [ 6400/175341]\n",
      "loss: 0.523845  [ 8000/175341]\n",
      "loss: 0.806573  [ 9600/175341]\n",
      "loss: 0.542108  [11200/175341]\n",
      "loss: 0.258989  [12800/175341]\n",
      "loss: 0.493390  [14400/175341]\n",
      "loss: 0.310385  [16000/175341]\n",
      "loss: 0.452111  [17600/175341]\n",
      "loss: 0.170225  [19200/175341]\n",
      "loss: 0.670844  [20800/175341]\n",
      "loss: 0.166010  [22400/175341]\n",
      "loss: 0.543937  [24000/175341]\n",
      "loss: 0.198626  [25600/175341]\n",
      "loss: 0.685593  [27200/175341]\n",
      "loss: 0.545119  [28800/175341]\n",
      "loss: 0.752225  [30400/175341]\n",
      "loss: 0.642969  [32000/175341]\n",
      "loss: 0.403061  [33600/175341]\n",
      "loss: 0.464022  [35200/175341]\n",
      "loss: 0.385739  [36800/175341]\n",
      "loss: 0.813685  [38400/175341]\n",
      "loss: 0.273067  [40000/175341]\n",
      "loss: 0.434849  [41600/175341]\n",
      "loss: 0.790726  [43200/175341]\n",
      "loss: 0.469400  [44800/175341]\n",
      "loss: 0.637246  [46400/175341]\n",
      "loss: 0.543274  [48000/175341]\n",
      "loss: 0.547181  [49600/175341]\n",
      "loss: 0.679511  [51200/175341]\n",
      "loss: 0.390387  [52800/175341]\n",
      "loss: 0.583754  [54400/175341]\n",
      "loss: 0.577311  [56000/175341]\n",
      "loss: 0.385241  [57600/175341]\n",
      "loss: 0.567447  [59200/175341]\n",
      "loss: 0.405592  [60800/175341]\n",
      "loss: 0.421194  [62400/175341]\n",
      "loss: 0.193394  [64000/175341]\n",
      "loss: 0.904571  [65600/175341]\n",
      "loss: 0.591088  [67200/175341]\n",
      "loss: 0.274404  [68800/175341]\n",
      "loss: 0.632165  [70400/175341]\n",
      "loss: 0.556699  [72000/175341]\n",
      "loss: 0.848920  [73600/175341]\n",
      "loss: 0.264311  [75200/175341]\n",
      "loss: 0.494592  [76800/175341]\n",
      "loss: 0.489334  [78400/175341]\n",
      "loss: 0.691306  [80000/175341]\n",
      "loss: 0.459887  [81600/175341]\n",
      "loss: 0.133951  [83200/175341]\n",
      "loss: 0.665625  [84800/175341]\n",
      "loss: 0.311000  [86400/175341]\n",
      "loss: 0.671943  [88000/175341]\n",
      "loss: 0.272304  [89600/175341]\n",
      "loss: 0.234307  [91200/175341]\n",
      "loss: 0.629874  [92800/175341]\n",
      "loss: 0.645520  [94400/175341]\n",
      "loss: 0.474391  [96000/175341]\n",
      "loss: 0.194922  [97600/175341]\n",
      "loss: 0.763164  [99200/175341]\n",
      "loss: 0.321567  [100800/175341]\n",
      "loss: 0.623417  [102400/175341]\n",
      "loss: 0.525500  [104000/175341]\n",
      "loss: 0.724125  [105600/175341]\n",
      "loss: 0.677927  [107200/175341]\n",
      "loss: 0.726183  [108800/175341]\n",
      "loss: 0.286540  [110400/175341]\n",
      "loss: 0.670687  [112000/175341]\n",
      "loss: 0.261188  [113600/175341]\n",
      "loss: 0.718348  [115200/175341]\n",
      "loss: 0.153063  [116800/175341]\n",
      "loss: 0.415456  [118400/175341]\n",
      "loss: 0.537969  [120000/175341]\n",
      "loss: 0.577284  [121600/175341]\n",
      "loss: 0.715397  [123200/175341]\n",
      "loss: 0.325368  [124800/175341]\n",
      "loss: 0.470007  [126400/175341]\n",
      "loss: 0.722254  [128000/175341]\n",
      "loss: 0.161656  [129600/175341]\n",
      "loss: 1.028187  [131200/175341]\n",
      "loss: 0.622890  [132800/175341]\n",
      "loss: 0.497786  [134400/175341]\n",
      "loss: 0.438819  [136000/175341]\n",
      "loss: 0.427754  [137600/175341]\n",
      "loss: 0.440980  [139200/175341]\n",
      "loss: 0.556702  [140800/175341]\n",
      "loss: 0.191809  [142400/175341]\n",
      "loss: 0.511791  [144000/175341]\n",
      "loss: 0.341180  [145600/175341]\n",
      "loss: 0.458852  [147200/175341]\n",
      "loss: 0.296674  [148800/175341]\n",
      "loss: 0.744132  [150400/175341]\n",
      "loss: 0.484403  [152000/175341]\n",
      "loss: 0.226327  [153600/175341]\n",
      "loss: 0.218857  [155200/175341]\n",
      "loss: 0.596815  [156800/175341]\n",
      "loss: 0.960986  [158400/175341]\n",
      "loss: 0.308062  [160000/175341]\n",
      "loss: 1.241376  [161600/175341]\n",
      "loss: 0.359355  [163200/175341]\n",
      "loss: 0.536130  [164800/175341]\n",
      "loss: 0.678563  [166400/175341]\n",
      "loss: 0.626888  [168000/175341]\n",
      "loss: 0.646694  [169600/175341]\n",
      "loss: 0.241070  [171200/175341]\n",
      "loss: 0.271600  [172800/175341]\n",
      "loss: 0.500952  [174400/175341]\n",
      "Train Accuracy: 80.4330%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.563540, F1-score: 75.24%, Macro_F1-Score:  40.51%  \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.512305  [    0/175341]\n",
      "loss: 0.507340  [ 1600/175341]\n",
      "loss: 0.385924  [ 3200/175341]\n",
      "loss: 0.383162  [ 4800/175341]\n",
      "loss: 0.471963  [ 6400/175341]\n",
      "loss: 0.381652  [ 8000/175341]\n",
      "loss: 0.703971  [ 9600/175341]\n",
      "loss: 0.500699  [11200/175341]\n",
      "loss: 0.358957  [12800/175341]\n",
      "loss: 0.422886  [14400/175341]\n",
      "loss: 0.121874  [16000/175341]\n",
      "loss: 0.104310  [17600/175341]\n",
      "loss: 0.313710  [19200/175341]\n",
      "loss: 0.366199  [20800/175341]\n",
      "loss: 0.534277  [22400/175341]\n",
      "loss: 0.379420  [24000/175341]\n",
      "loss: 0.575436  [25600/175341]\n",
      "loss: 0.180879  [27200/175341]\n",
      "loss: 0.278529  [28800/175341]\n",
      "loss: 0.266151  [30400/175341]\n",
      "loss: 0.726364  [32000/175341]\n",
      "loss: 0.432016  [33600/175341]\n",
      "loss: 0.248110  [35200/175341]\n",
      "loss: 0.404750  [36800/175341]\n",
      "loss: 0.712617  [38400/175341]\n",
      "loss: 0.063880  [40000/175341]\n",
      "loss: 0.344901  [41600/175341]\n",
      "loss: 0.548178  [43200/175341]\n",
      "loss: 0.371223  [44800/175341]\n",
      "loss: 0.396101  [46400/175341]\n",
      "loss: 0.638229  [48000/175341]\n",
      "loss: 0.220172  [49600/175341]\n",
      "loss: 0.452155  [51200/175341]\n",
      "loss: 0.385756  [52800/175341]\n",
      "loss: 0.481455  [54400/175341]\n",
      "loss: 0.581798  [56000/175341]\n",
      "loss: 0.584108  [57600/175341]\n",
      "loss: 0.776293  [59200/175341]\n",
      "loss: 0.317638  [60800/175341]\n",
      "loss: 0.197362  [62400/175341]\n",
      "loss: 0.391171  [64000/175341]\n",
      "loss: 0.173116  [65600/175341]\n",
      "loss: 0.845064  [67200/175341]\n",
      "loss: 0.426679  [68800/175341]\n",
      "loss: 1.044705  [70400/175341]\n",
      "loss: 0.324482  [72000/175341]\n",
      "loss: 0.285971  [73600/175341]\n",
      "loss: 0.300261  [75200/175341]\n",
      "loss: 0.701471  [76800/175341]\n",
      "loss: 0.185788  [78400/175341]\n",
      "loss: 0.422431  [80000/175341]\n",
      "loss: 0.179488  [81600/175341]\n",
      "loss: 0.554143  [83200/175341]\n",
      "loss: 0.633090  [84800/175341]\n",
      "loss: 0.642965  [86400/175341]\n",
      "loss: 0.369644  [88000/175341]\n",
      "loss: 0.693158  [89600/175341]\n",
      "loss: 0.491141  [91200/175341]\n",
      "loss: 0.174639  [92800/175341]\n",
      "loss: 0.259005  [94400/175341]\n",
      "loss: 0.599411  [96000/175341]\n",
      "loss: 0.330072  [97600/175341]\n",
      "loss: 0.391697  [99200/175341]\n",
      "loss: 0.448387  [100800/175341]\n",
      "loss: 0.579509  [102400/175341]\n",
      "loss: 0.602468  [104000/175341]\n",
      "loss: 0.434331  [105600/175341]\n",
      "loss: 0.664357  [107200/175341]\n",
      "loss: 0.193479  [108800/175341]\n",
      "loss: 0.780157  [110400/175341]\n",
      "loss: 0.580040  [112000/175341]\n",
      "loss: 0.530322  [113600/175341]\n",
      "loss: 0.600948  [115200/175341]\n",
      "loss: 0.692181  [116800/175341]\n",
      "loss: 0.338438  [118400/175341]\n",
      "loss: 0.400251  [120000/175341]\n",
      "loss: 0.737401  [121600/175341]\n",
      "loss: 0.533039  [123200/175341]\n",
      "loss: 0.549927  [124800/175341]\n",
      "loss: 0.336127  [126400/175341]\n",
      "loss: 0.471566  [128000/175341]\n",
      "loss: 0.466153  [129600/175341]\n",
      "loss: 0.751814  [131200/175341]\n",
      "loss: 0.397777  [132800/175341]\n",
      "loss: 0.725632  [134400/175341]\n",
      "loss: 0.223110  [136000/175341]\n",
      "loss: 0.292139  [137600/175341]\n",
      "loss: 0.294136  [139200/175341]\n",
      "loss: 1.177582  [140800/175341]\n",
      "loss: 0.570092  [142400/175341]\n",
      "loss: 0.612494  [144000/175341]\n",
      "loss: 0.327635  [145600/175341]\n",
      "loss: 0.266364  [147200/175341]\n",
      "loss: 0.826038  [148800/175341]\n",
      "loss: 1.102864  [150400/175341]\n",
      "loss: 0.230658  [152000/175341]\n",
      "loss: 0.484872  [153600/175341]\n",
      "loss: 0.834446  [155200/175341]\n",
      "loss: 0.675156  [156800/175341]\n",
      "loss: 0.393872  [158400/175341]\n",
      "loss: 0.294396  [160000/175341]\n",
      "loss: 0.495608  [161600/175341]\n",
      "loss: 0.160362  [163200/175341]\n",
      "loss: 0.387870  [164800/175341]\n",
      "loss: 0.439681  [166400/175341]\n",
      "loss: 0.398730  [168000/175341]\n",
      "loss: 0.366792  [169600/175341]\n",
      "loss: 0.679773  [171200/175341]\n",
      "loss: 0.431938  [172800/175341]\n",
      "loss: 0.701268  [174400/175341]\n",
      "Train Accuracy: 80.5277%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.564105, F1-score: 74.69%, Macro_F1-Score:  40.34%  \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.556076  [    0/175341]\n",
      "loss: 0.409258  [ 1600/175341]\n",
      "loss: 0.253973  [ 3200/175341]\n",
      "loss: 0.537154  [ 4800/175341]\n",
      "loss: 0.303234  [ 6400/175341]\n",
      "loss: 0.228992  [ 8000/175341]\n",
      "loss: 0.589913  [ 9600/175341]\n",
      "loss: 0.437811  [11200/175341]\n",
      "loss: 0.604692  [12800/175341]\n",
      "loss: 0.522945  [14400/175341]\n",
      "loss: 0.320639  [16000/175341]\n",
      "loss: 0.504634  [17600/175341]\n",
      "loss: 0.615653  [19200/175341]\n",
      "loss: 0.291063  [20800/175341]\n",
      "loss: 0.131577  [22400/175341]\n",
      "loss: 0.347161  [24000/175341]\n",
      "loss: 0.601853  [25600/175341]\n",
      "loss: 0.477001  [27200/175341]\n",
      "loss: 0.589843  [28800/175341]\n",
      "loss: 0.509566  [30400/175341]\n",
      "loss: 0.361614  [32000/175341]\n",
      "loss: 0.293268  [33600/175341]\n",
      "loss: 0.314925  [35200/175341]\n",
      "loss: 0.397080  [36800/175341]\n",
      "loss: 0.530275  [38400/175341]\n",
      "loss: 0.767351  [40000/175341]\n",
      "loss: 0.486784  [41600/175341]\n",
      "loss: 0.441551  [43200/175341]\n",
      "loss: 0.927776  [44800/175341]\n",
      "loss: 1.020088  [46400/175341]\n",
      "loss: 0.295547  [48000/175341]\n",
      "loss: 0.337854  [49600/175341]\n",
      "loss: 0.637007  [51200/175341]\n",
      "loss: 0.294861  [52800/175341]\n",
      "loss: 0.380427  [54400/175341]\n",
      "loss: 0.629862  [56000/175341]\n",
      "loss: 0.131249  [57600/175341]\n",
      "loss: 0.639232  [59200/175341]\n",
      "loss: 0.406318  [60800/175341]\n",
      "loss: 0.442970  [62400/175341]\n",
      "loss: 0.390135  [64000/175341]\n",
      "loss: 0.244390  [65600/175341]\n",
      "loss: 0.520965  [67200/175341]\n",
      "loss: 0.472191  [68800/175341]\n",
      "loss: 0.268424  [70400/175341]\n",
      "loss: 0.276250  [72000/175341]\n",
      "loss: 0.667566  [73600/175341]\n",
      "loss: 0.091827  [75200/175341]\n",
      "loss: 0.621658  [76800/175341]\n",
      "loss: 0.363440  [78400/175341]\n",
      "loss: 0.489535  [80000/175341]\n",
      "loss: 0.245432  [81600/175341]\n",
      "loss: 0.351744  [83200/175341]\n",
      "loss: 0.283172  [84800/175341]\n",
      "loss: 0.469002  [86400/175341]\n",
      "loss: 0.696233  [88000/175341]\n",
      "loss: 0.514259  [89600/175341]\n",
      "loss: 0.319311  [91200/175341]\n",
      "loss: 0.470243  [92800/175341]\n",
      "loss: 0.349790  [94400/175341]\n",
      "loss: 0.580618  [96000/175341]\n",
      "loss: 0.341493  [97600/175341]\n",
      "loss: 0.811630  [99200/175341]\n",
      "loss: 0.356608  [100800/175341]\n",
      "loss: 0.599138  [102400/175341]\n",
      "loss: 0.514287  [104000/175341]\n",
      "loss: 0.552932  [105600/175341]\n",
      "loss: 0.599378  [107200/175341]\n",
      "loss: 0.765195  [108800/175341]\n",
      "loss: 0.246632  [110400/175341]\n",
      "loss: 0.557321  [112000/175341]\n",
      "loss: 0.671083  [113600/175341]\n",
      "loss: 1.116833  [115200/175341]\n",
      "loss: 0.786461  [116800/175341]\n",
      "loss: 0.353370  [118400/175341]\n",
      "loss: 0.288259  [120000/175341]\n",
      "loss: 0.642137  [121600/175341]\n",
      "loss: 0.565084  [123200/175341]\n",
      "loss: 0.583365  [124800/175341]\n",
      "loss: 1.144382  [126400/175341]\n",
      "loss: 0.450670  [128000/175341]\n",
      "loss: 0.628635  [129600/175341]\n",
      "loss: 0.376846  [131200/175341]\n",
      "loss: 0.564321  [132800/175341]\n",
      "loss: 0.921346  [134400/175341]\n",
      "loss: 0.313950  [136000/175341]\n",
      "loss: 0.599954  [137600/175341]\n",
      "loss: 0.528872  [139200/175341]\n",
      "loss: 0.186095  [140800/175341]\n",
      "loss: 1.014234  [142400/175341]\n",
      "loss: 0.557104  [144000/175341]\n",
      "loss: 0.620300  [145600/175341]\n",
      "loss: 0.675724  [147200/175341]\n",
      "loss: 0.603586  [148800/175341]\n",
      "loss: 0.446578  [150400/175341]\n",
      "loss: 0.889842  [152000/175341]\n",
      "loss: 0.387641  [153600/175341]\n",
      "loss: 0.387593  [155200/175341]\n",
      "loss: 0.409874  [156800/175341]\n",
      "loss: 0.769683  [158400/175341]\n",
      "loss: 0.328896  [160000/175341]\n",
      "loss: 0.569234  [161600/175341]\n",
      "loss: 0.267589  [163200/175341]\n",
      "loss: 0.765487  [164800/175341]\n",
      "loss: 0.594822  [166400/175341]\n",
      "loss: 0.247441  [168000/175341]\n",
      "loss: 0.577402  [169600/175341]\n",
      "loss: 0.219706  [171200/175341]\n",
      "loss: 0.447861  [172800/175341]\n",
      "loss: 0.293086  [174400/175341]\n",
      "Train Accuracy: 80.5054%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.563523, F1-score: 75.55%, Macro_F1-Score:  41.69%  \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.541657  [    0/175341]\n",
      "loss: 0.226890  [ 1600/175341]\n",
      "loss: 0.684235  [ 3200/175341]\n",
      "loss: 0.405876  [ 4800/175341]\n",
      "loss: 0.560445  [ 6400/175341]\n",
      "loss: 0.377512  [ 8000/175341]\n",
      "loss: 0.283675  [ 9600/175341]\n",
      "loss: 0.546169  [11200/175341]\n",
      "loss: 0.173159  [12800/175341]\n",
      "loss: 1.139982  [14400/175341]\n",
      "loss: 0.895914  [16000/175341]\n",
      "loss: 0.592170  [17600/175341]\n",
      "loss: 0.482653  [19200/175341]\n",
      "loss: 0.332137  [20800/175341]\n",
      "loss: 0.530567  [22400/175341]\n",
      "loss: 0.600024  [24000/175341]\n",
      "loss: 0.944585  [25600/175341]\n",
      "loss: 0.448084  [27200/175341]\n",
      "loss: 0.349318  [28800/175341]\n",
      "loss: 0.313305  [30400/175341]\n",
      "loss: 0.410513  [32000/175341]\n",
      "loss: 0.112248  [33600/175341]\n",
      "loss: 0.258549  [35200/175341]\n",
      "loss: 0.460985  [36800/175341]\n",
      "loss: 0.267168  [38400/175341]\n",
      "loss: 0.602893  [40000/175341]\n",
      "loss: 1.049855  [41600/175341]\n",
      "loss: 0.249407  [43200/175341]\n",
      "loss: 0.697603  [44800/175341]\n",
      "loss: 0.677564  [46400/175341]\n",
      "loss: 0.483165  [48000/175341]\n",
      "loss: 0.847294  [49600/175341]\n",
      "loss: 0.513799  [51200/175341]\n",
      "loss: 0.582753  [52800/175341]\n",
      "loss: 0.551441  [54400/175341]\n",
      "loss: 0.469960  [56000/175341]\n",
      "loss: 0.577790  [57600/175341]\n",
      "loss: 0.521780  [59200/175341]\n",
      "loss: 0.463099  [60800/175341]\n",
      "loss: 0.639610  [62400/175341]\n",
      "loss: 0.932157  [64000/175341]\n",
      "loss: 0.411917  [65600/175341]\n",
      "loss: 0.401295  [67200/175341]\n",
      "loss: 0.433987  [68800/175341]\n",
      "loss: 0.358570  [70400/175341]\n",
      "loss: 0.608106  [72000/175341]\n",
      "loss: 0.754884  [73600/175341]\n",
      "loss: 0.044222  [75200/175341]\n",
      "loss: 0.192921  [76800/175341]\n",
      "loss: 0.483756  [78400/175341]\n",
      "loss: 0.430565  [80000/175341]\n",
      "loss: 0.556713  [81600/175341]\n",
      "loss: 0.684874  [83200/175341]\n",
      "loss: 0.367815  [84800/175341]\n",
      "loss: 0.264628  [86400/175341]\n",
      "loss: 0.518304  [88000/175341]\n",
      "loss: 0.802585  [89600/175341]\n",
      "loss: 0.451346  [91200/175341]\n",
      "loss: 0.545157  [92800/175341]\n",
      "loss: 0.486247  [94400/175341]\n",
      "loss: 0.502890  [96000/175341]\n",
      "loss: 0.385522  [97600/175341]\n",
      "loss: 0.355302  [99200/175341]\n",
      "loss: 0.461018  [100800/175341]\n",
      "loss: 0.528097  [102400/175341]\n",
      "loss: 0.233612  [104000/175341]\n",
      "loss: 0.444983  [105600/175341]\n",
      "loss: 1.149853  [107200/175341]\n",
      "loss: 0.115682  [108800/175341]\n",
      "loss: 0.559498  [110400/175341]\n",
      "loss: 0.519952  [112000/175341]\n",
      "loss: 0.463271  [113600/175341]\n",
      "loss: 0.519671  [115200/175341]\n",
      "loss: 0.770185  [116800/175341]\n",
      "loss: 0.519123  [118400/175341]\n",
      "loss: 0.397182  [120000/175341]\n",
      "loss: 0.399281  [121600/175341]\n",
      "loss: 0.545341  [123200/175341]\n",
      "loss: 0.446965  [124800/175341]\n",
      "loss: 0.982423  [126400/175341]\n",
      "loss: 0.204516  [128000/175341]\n",
      "loss: 0.750431  [129600/175341]\n",
      "loss: 0.213147  [131200/175341]\n",
      "loss: 0.701184  [132800/175341]\n",
      "loss: 0.617070  [134400/175341]\n",
      "loss: 0.424835  [136000/175341]\n",
      "loss: 0.640171  [137600/175341]\n",
      "loss: 0.811603  [139200/175341]\n",
      "loss: 0.611389  [140800/175341]\n",
      "loss: 0.781941  [142400/175341]\n",
      "loss: 0.677722  [144000/175341]\n",
      "loss: 0.406634  [145600/175341]\n",
      "loss: 0.330309  [147200/175341]\n",
      "loss: 0.468559  [148800/175341]\n",
      "loss: 0.735054  [150400/175341]\n",
      "loss: 0.409826  [152000/175341]\n",
      "loss: 0.610933  [153600/175341]\n",
      "loss: 0.474748  [155200/175341]\n",
      "loss: 0.644027  [156800/175341]\n",
      "loss: 0.341909  [158400/175341]\n",
      "loss: 0.421484  [160000/175341]\n",
      "loss: 0.239415  [161600/175341]\n",
      "loss: 0.609837  [163200/175341]\n",
      "loss: 0.935956  [164800/175341]\n",
      "loss: 0.843809  [166400/175341]\n",
      "loss: 0.587716  [168000/175341]\n",
      "loss: 0.490234  [169600/175341]\n",
      "loss: 1.180875  [171200/175341]\n",
      "loss: 1.202413  [172800/175341]\n",
      "loss: 0.297461  [174400/175341]\n",
      "Train Accuracy: 80.5556%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.569821, F1-score: 74.99%, Macro_F1-Score:  39.63%  \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.684621  [    0/175341]\n",
      "loss: 0.319961  [ 1600/175341]\n",
      "loss: 0.410838  [ 3200/175341]\n",
      "loss: 0.235357  [ 4800/175341]\n",
      "loss: 0.550757  [ 6400/175341]\n",
      "loss: 0.616185  [ 8000/175341]\n",
      "loss: 0.530248  [ 9600/175341]\n",
      "loss: 0.274685  [11200/175341]\n",
      "loss: 0.569235  [12800/175341]\n",
      "loss: 0.424481  [14400/175341]\n",
      "loss: 0.782451  [16000/175341]\n",
      "loss: 0.131222  [17600/175341]\n",
      "loss: 0.136641  [19200/175341]\n",
      "loss: 0.355312  [20800/175341]\n",
      "loss: 0.966241  [22400/175341]\n",
      "loss: 0.741246  [24000/175341]\n",
      "loss: 0.587168  [25600/175341]\n",
      "loss: 0.324910  [27200/175341]\n",
      "loss: 0.220633  [28800/175341]\n",
      "loss: 0.255480  [30400/175341]\n",
      "loss: 0.666957  [32000/175341]\n",
      "loss: 0.800180  [33600/175341]\n",
      "loss: 0.346817  [35200/175341]\n",
      "loss: 0.833698  [36800/175341]\n",
      "loss: 0.376308  [38400/175341]\n",
      "loss: 0.817773  [40000/175341]\n",
      "loss: 0.404052  [41600/175341]\n",
      "loss: 0.157336  [43200/175341]\n",
      "loss: 0.245477  [44800/175341]\n",
      "loss: 0.397804  [46400/175341]\n",
      "loss: 0.658075  [48000/175341]\n",
      "loss: 0.268929  [49600/175341]\n",
      "loss: 0.579618  [51200/175341]\n",
      "loss: 0.368950  [52800/175341]\n",
      "loss: 0.314081  [54400/175341]\n",
      "loss: 0.382099  [56000/175341]\n",
      "loss: 0.308846  [57600/175341]\n",
      "loss: 0.281902  [59200/175341]\n",
      "loss: 0.392320  [60800/175341]\n",
      "loss: 0.380319  [62400/175341]\n",
      "loss: 0.463764  [64000/175341]\n",
      "loss: 0.692508  [65600/175341]\n",
      "loss: 0.834275  [67200/175341]\n",
      "loss: 0.362649  [68800/175341]\n",
      "loss: 0.314673  [70400/175341]\n",
      "loss: 0.687275  [72000/175341]\n",
      "loss: 0.567670  [73600/175341]\n",
      "loss: 0.197577  [75200/175341]\n",
      "loss: 0.607089  [76800/175341]\n",
      "loss: 1.233214  [78400/175341]\n",
      "loss: 0.272092  [80000/175341]\n",
      "loss: 0.664989  [81600/175341]\n",
      "loss: 0.289333  [83200/175341]\n",
      "loss: 0.694423  [84800/175341]\n",
      "loss: 0.845670  [86400/175341]\n",
      "loss: 0.271413  [88000/175341]\n",
      "loss: 0.603794  [89600/175341]\n",
      "loss: 0.473518  [91200/175341]\n",
      "loss: 0.632828  [92800/175341]\n",
      "loss: 0.246689  [94400/175341]\n",
      "loss: 0.472648  [96000/175341]\n",
      "loss: 0.496859  [97600/175341]\n",
      "loss: 0.691491  [99200/175341]\n",
      "loss: 0.562404  [100800/175341]\n",
      "loss: 0.377179  [102400/175341]\n",
      "loss: 0.619359  [104000/175341]\n",
      "loss: 0.941806  [105600/175341]\n",
      "loss: 0.544481  [107200/175341]\n",
      "loss: 0.404461  [108800/175341]\n",
      "loss: 0.665756  [110400/175341]\n",
      "loss: 0.647290  [112000/175341]\n",
      "loss: 0.437081  [113600/175341]\n",
      "loss: 0.560535  [115200/175341]\n",
      "loss: 0.291424  [116800/175341]\n",
      "loss: 0.283945  [118400/175341]\n",
      "loss: 0.730473  [120000/175341]\n",
      "loss: 0.180590  [121600/175341]\n",
      "loss: 0.152506  [123200/175341]\n",
      "loss: 0.465738  [124800/175341]\n",
      "loss: 0.332260  [126400/175341]\n",
      "loss: 0.786751  [128000/175341]\n",
      "loss: 0.316118  [129600/175341]\n",
      "loss: 0.710854  [131200/175341]\n",
      "loss: 0.484227  [132800/175341]\n",
      "loss: 0.255336  [134400/175341]\n",
      "loss: 0.799753  [136000/175341]\n",
      "loss: 0.386503  [137600/175341]\n",
      "loss: 0.439680  [139200/175341]\n",
      "loss: 0.456783  [140800/175341]\n",
      "loss: 0.365039  [142400/175341]\n",
      "loss: 0.369141  [144000/175341]\n",
      "loss: 0.326283  [145600/175341]\n",
      "loss: 0.206388  [147200/175341]\n",
      "loss: 0.486760  [148800/175341]\n",
      "loss: 0.919681  [150400/175341]\n",
      "loss: 0.204463  [152000/175341]\n",
      "loss: 0.295070  [153600/175341]\n",
      "loss: 0.318042  [155200/175341]\n",
      "loss: 0.434751  [156800/175341]\n",
      "loss: 0.562548  [158400/175341]\n",
      "loss: 0.149139  [160000/175341]\n",
      "loss: 0.559846  [161600/175341]\n",
      "loss: 0.405062  [163200/175341]\n",
      "loss: 1.007771  [164800/175341]\n",
      "loss: 0.166344  [166400/175341]\n",
      "loss: 0.385403  [168000/175341]\n",
      "loss: 0.510608  [169600/175341]\n",
      "loss: 0.505038  [171200/175341]\n",
      "loss: 0.716697  [172800/175341]\n",
      "loss: 0.486555  [174400/175341]\n",
      "Train Accuracy: 80.5488%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.582656, F1-score: 74.24%, Macro_F1-Score:  40.29%  \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.927630  [    0/175341]\n",
      "loss: 0.341147  [ 1600/175341]\n",
      "loss: 0.578605  [ 3200/175341]\n",
      "loss: 0.475450  [ 4800/175341]\n",
      "loss: 0.341189  [ 6400/175341]\n",
      "loss: 0.491907  [ 8000/175341]\n",
      "loss: 0.834006  [ 9600/175341]\n",
      "loss: 0.202720  [11200/175341]\n",
      "loss: 0.839661  [12800/175341]\n",
      "loss: 0.732605  [14400/175341]\n",
      "loss: 0.678843  [16000/175341]\n",
      "loss: 0.446565  [17600/175341]\n",
      "loss: 0.426129  [19200/175341]\n",
      "loss: 0.481407  [20800/175341]\n",
      "loss: 0.663599  [22400/175341]\n",
      "loss: 0.488287  [24000/175341]\n",
      "loss: 0.394883  [25600/175341]\n",
      "loss: 0.649911  [27200/175341]\n",
      "loss: 0.158120  [28800/175341]\n",
      "loss: 0.287616  [30400/175341]\n",
      "loss: 0.489955  [32000/175341]\n",
      "loss: 0.713164  [33600/175341]\n",
      "loss: 0.834674  [35200/175341]\n",
      "loss: 0.422230  [36800/175341]\n",
      "loss: 0.818592  [38400/175341]\n",
      "loss: 0.257216  [40000/175341]\n",
      "loss: 0.473924  [41600/175341]\n",
      "loss: 0.331370  [43200/175341]\n",
      "loss: 0.350814  [44800/175341]\n",
      "loss: 0.518866  [46400/175341]\n",
      "loss: 0.307765  [48000/175341]\n",
      "loss: 0.354314  [49600/175341]\n",
      "loss: 0.325980  [51200/175341]\n",
      "loss: 0.668002  [52800/175341]\n",
      "loss: 0.579095  [54400/175341]\n",
      "loss: 0.526598  [56000/175341]\n",
      "loss: 0.508812  [57600/175341]\n",
      "loss: 0.375485  [59200/175341]\n",
      "loss: 0.576382  [60800/175341]\n",
      "loss: 0.451143  [62400/175341]\n",
      "loss: 0.582119  [64000/175341]\n",
      "loss: 0.651092  [65600/175341]\n",
      "loss: 0.560679  [67200/175341]\n",
      "loss: 0.391577  [68800/175341]\n",
      "loss: 0.247479  [70400/175341]\n",
      "loss: 0.338602  [72000/175341]\n",
      "loss: 0.573318  [73600/175341]\n",
      "loss: 0.697992  [75200/175341]\n",
      "loss: 0.511548  [76800/175341]\n",
      "loss: 0.333240  [78400/175341]\n",
      "loss: 0.318098  [80000/175341]\n",
      "loss: 0.447261  [81600/175341]\n",
      "loss: 0.413340  [83200/175341]\n",
      "loss: 0.401935  [84800/175341]\n",
      "loss: 0.800252  [86400/175341]\n",
      "loss: 0.423848  [88000/175341]\n",
      "loss: 0.762662  [89600/175341]\n",
      "loss: 0.317055  [91200/175341]\n",
      "loss: 0.558031  [92800/175341]\n",
      "loss: 0.376966  [94400/175341]\n",
      "loss: 0.529315  [96000/175341]\n",
      "loss: 0.605676  [97600/175341]\n",
      "loss: 0.680362  [99200/175341]\n",
      "loss: 0.283990  [100800/175341]\n",
      "loss: 0.186756  [102400/175341]\n",
      "loss: 0.346138  [104000/175341]\n",
      "loss: 0.398346  [105600/175341]\n",
      "loss: 0.342180  [107200/175341]\n",
      "loss: 0.324939  [108800/175341]\n",
      "loss: 0.339654  [110400/175341]\n",
      "loss: 0.442139  [112000/175341]\n",
      "loss: 0.478482  [113600/175341]\n",
      "loss: 0.426499  [115200/175341]\n",
      "loss: 0.501072  [116800/175341]\n",
      "loss: 0.328064  [118400/175341]\n",
      "loss: 0.456436  [120000/175341]\n",
      "loss: 0.752382  [121600/175341]\n",
      "loss: 0.993120  [123200/175341]\n",
      "loss: 0.718281  [124800/175341]\n",
      "loss: 0.761344  [126400/175341]\n",
      "loss: 0.488889  [128000/175341]\n",
      "loss: 0.571448  [129600/175341]\n",
      "loss: 0.738479  [131200/175341]\n",
      "loss: 0.383631  [132800/175341]\n",
      "loss: 0.283364  [134400/175341]\n",
      "loss: 0.519506  [136000/175341]\n",
      "loss: 0.439490  [137600/175341]\n",
      "loss: 0.590445  [139200/175341]\n",
      "loss: 0.527878  [140800/175341]\n",
      "loss: 0.600214  [142400/175341]\n",
      "loss: 0.279464  [144000/175341]\n",
      "loss: 0.200429  [145600/175341]\n",
      "loss: 0.422266  [147200/175341]\n",
      "loss: 0.734748  [148800/175341]\n",
      "loss: 0.435024  [150400/175341]\n",
      "loss: 0.544874  [152000/175341]\n",
      "loss: 0.331169  [153600/175341]\n",
      "loss: 0.286218  [155200/175341]\n",
      "loss: 0.418107  [156800/175341]\n",
      "loss: 0.578755  [158400/175341]\n",
      "loss: 0.562827  [160000/175341]\n",
      "loss: 0.391713  [161600/175341]\n",
      "loss: 0.390958  [163200/175341]\n",
      "loss: 0.464966  [164800/175341]\n",
      "loss: 0.493863  [166400/175341]\n",
      "loss: 0.328758  [168000/175341]\n",
      "loss: 0.534743  [169600/175341]\n",
      "loss: 0.522314  [171200/175341]\n",
      "loss: 0.625696  [172800/175341]\n",
      "loss: 0.097381  [174400/175341]\n",
      "Train Accuracy: 80.5721%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.567767, F1-score: 74.44%, Macro_F1-Score:  39.91%  \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.526512  [    0/175341]\n",
      "loss: 0.173714  [ 1600/175341]\n",
      "loss: 0.422260  [ 3200/175341]\n",
      "loss: 0.305999  [ 4800/175341]\n",
      "loss: 0.574946  [ 6400/175341]\n",
      "loss: 0.551899  [ 8000/175341]\n",
      "loss: 0.360662  [ 9600/175341]\n",
      "loss: 0.594837  [11200/175341]\n",
      "loss: 0.390613  [12800/175341]\n",
      "loss: 0.246082  [14400/175341]\n",
      "loss: 0.290588  [16000/175341]\n",
      "loss: 0.582720  [17600/175341]\n",
      "loss: 0.629169  [19200/175341]\n",
      "loss: 0.585467  [20800/175341]\n",
      "loss: 0.357043  [22400/175341]\n",
      "loss: 0.634079  [24000/175341]\n",
      "loss: 0.382680  [25600/175341]\n",
      "loss: 0.432383  [27200/175341]\n",
      "loss: 0.101497  [28800/175341]\n",
      "loss: 0.656004  [30400/175341]\n",
      "loss: 0.480379  [32000/175341]\n",
      "loss: 0.392704  [33600/175341]\n",
      "loss: 0.776707  [35200/175341]\n",
      "loss: 0.842292  [36800/175341]\n",
      "loss: 0.578640  [38400/175341]\n",
      "loss: 0.354945  [40000/175341]\n",
      "loss: 0.773294  [41600/175341]\n",
      "loss: 1.077149  [43200/175341]\n",
      "loss: 0.427244  [44800/175341]\n",
      "loss: 0.790487  [46400/175341]\n",
      "loss: 0.746744  [48000/175341]\n",
      "loss: 0.712186  [49600/175341]\n",
      "loss: 0.388125  [51200/175341]\n",
      "loss: 0.859822  [52800/175341]\n",
      "loss: 0.361672  [54400/175341]\n",
      "loss: 0.476232  [56000/175341]\n",
      "loss: 0.568664  [57600/175341]\n",
      "loss: 0.463436  [59200/175341]\n",
      "loss: 0.401778  [60800/175341]\n",
      "loss: 0.532054  [62400/175341]\n",
      "loss: 0.681892  [64000/175341]\n",
      "loss: 0.420187  [65600/175341]\n",
      "loss: 0.437190  [67200/175341]\n",
      "loss: 0.905786  [68800/175341]\n",
      "loss: 0.342844  [70400/175341]\n",
      "loss: 0.885774  [72000/175341]\n",
      "loss: 0.491044  [73600/175341]\n",
      "loss: 0.453753  [75200/175341]\n",
      "loss: 0.362281  [76800/175341]\n",
      "loss: 0.823044  [78400/175341]\n",
      "loss: 0.816597  [80000/175341]\n",
      "loss: 0.190411  [81600/175341]\n",
      "loss: 0.462450  [83200/175341]\n",
      "loss: 0.330821  [84800/175341]\n",
      "loss: 0.248148  [86400/175341]\n",
      "loss: 0.629240  [88000/175341]\n",
      "loss: 0.505017  [89600/175341]\n",
      "loss: 0.381976  [91200/175341]\n",
      "loss: 0.288050  [92800/175341]\n",
      "loss: 0.226415  [94400/175341]\n",
      "loss: 0.400484  [96000/175341]\n",
      "loss: 0.515786  [97600/175341]\n",
      "loss: 0.422493  [99200/175341]\n",
      "loss: 0.519621  [100800/175341]\n",
      "loss: 0.494489  [102400/175341]\n",
      "loss: 0.407277  [104000/175341]\n",
      "loss: 0.125541  [105600/175341]\n",
      "loss: 0.077571  [107200/175341]\n",
      "loss: 0.547405  [108800/175341]\n",
      "loss: 0.342148  [110400/175341]\n",
      "loss: 0.393872  [112000/175341]\n",
      "loss: 0.595266  [113600/175341]\n",
      "loss: 0.379799  [115200/175341]\n",
      "loss: 0.563041  [116800/175341]\n",
      "loss: 1.020429  [118400/175341]\n",
      "loss: 0.797772  [120000/175341]\n",
      "loss: 0.529078  [121600/175341]\n",
      "loss: 0.928813  [123200/175341]\n",
      "loss: 0.863453  [124800/175341]\n",
      "loss: 0.521002  [126400/175341]\n",
      "loss: 0.247459  [128000/175341]\n",
      "loss: 0.348580  [129600/175341]\n",
      "loss: 0.691013  [131200/175341]\n",
      "loss: 0.869914  [132800/175341]\n",
      "loss: 0.280883  [134400/175341]\n",
      "loss: 0.505758  [136000/175341]\n",
      "loss: 0.539993  [137600/175341]\n",
      "loss: 0.631253  [139200/175341]\n",
      "loss: 0.571098  [140800/175341]\n",
      "loss: 0.372588  [142400/175341]\n",
      "loss: 0.558308  [144000/175341]\n",
      "loss: 0.559561  [145600/175341]\n",
      "loss: 0.487205  [147200/175341]\n",
      "loss: 0.472121  [148800/175341]\n",
      "loss: 0.171985  [150400/175341]\n",
      "loss: 0.493385  [152000/175341]\n",
      "loss: 0.354756  [153600/175341]\n",
      "loss: 0.191854  [155200/175341]\n",
      "loss: 0.325676  [156800/175341]\n",
      "loss: 0.280174  [158400/175341]\n",
      "loss: 0.206269  [160000/175341]\n",
      "loss: 0.754265  [161600/175341]\n",
      "loss: 0.409663  [163200/175341]\n",
      "loss: 0.330990  [164800/175341]\n",
      "loss: 0.348358  [166400/175341]\n",
      "loss: 0.733882  [168000/175341]\n",
      "loss: 0.988166  [169600/175341]\n",
      "loss: 0.745934  [171200/175341]\n",
      "loss: 0.194469  [172800/175341]\n",
      "loss: 0.465376  [174400/175341]\n",
      "Train Accuracy: 80.5607%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.570284, F1-score: 74.53%, Macro_F1-Score:  39.47%  \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.505176  [    0/175341]\n",
      "loss: 0.235253  [ 1600/175341]\n",
      "loss: 0.479582  [ 3200/175341]\n",
      "loss: 0.194360  [ 4800/175341]\n",
      "loss: 0.247744  [ 6400/175341]\n",
      "loss: 0.915592  [ 8000/175341]\n",
      "loss: 0.780105  [ 9600/175341]\n",
      "loss: 0.938256  [11200/175341]\n",
      "loss: 0.212799  [12800/175341]\n",
      "loss: 0.383889  [14400/175341]\n",
      "loss: 0.849823  [16000/175341]\n",
      "loss: 0.577816  [17600/175341]\n",
      "loss: 1.270106  [19200/175341]\n",
      "loss: 0.949884  [20800/175341]\n",
      "loss: 0.602253  [22400/175341]\n",
      "loss: 0.396301  [24000/175341]\n",
      "loss: 0.416934  [25600/175341]\n",
      "loss: 0.102713  [27200/175341]\n",
      "loss: 0.369940  [28800/175341]\n",
      "loss: 0.334425  [30400/175341]\n",
      "loss: 0.166419  [32000/175341]\n",
      "loss: 0.219420  [33600/175341]\n",
      "loss: 0.474672  [35200/175341]\n",
      "loss: 0.934719  [36800/175341]\n",
      "loss: 0.548818  [38400/175341]\n",
      "loss: 0.503863  [40000/175341]\n",
      "loss: 0.636028  [41600/175341]\n",
      "loss: 0.228097  [43200/175341]\n",
      "loss: 0.919635  [44800/175341]\n",
      "loss: 0.155095  [46400/175341]\n",
      "loss: 0.672689  [48000/175341]\n",
      "loss: 0.557806  [49600/175341]\n",
      "loss: 1.007099  [51200/175341]\n",
      "loss: 0.325523  [52800/175341]\n",
      "loss: 0.466688  [54400/175341]\n",
      "loss: 0.308203  [56000/175341]\n",
      "loss: 0.396390  [57600/175341]\n",
      "loss: 0.372951  [59200/175341]\n",
      "loss: 0.486443  [60800/175341]\n",
      "loss: 0.500449  [62400/175341]\n",
      "loss: 0.369224  [64000/175341]\n",
      "loss: 0.324189  [65600/175341]\n",
      "loss: 0.612446  [67200/175341]\n",
      "loss: 0.562656  [68800/175341]\n",
      "loss: 0.377850  [70400/175341]\n",
      "loss: 1.210098  [72000/175341]\n",
      "loss: 0.557348  [73600/175341]\n",
      "loss: 0.863522  [75200/175341]\n",
      "loss: 0.355520  [76800/175341]\n",
      "loss: 0.125032  [78400/175341]\n",
      "loss: 0.365300  [80000/175341]\n",
      "loss: 0.290573  [81600/175341]\n",
      "loss: 0.188937  [83200/175341]\n",
      "loss: 0.740339  [84800/175341]\n",
      "loss: 0.471987  [86400/175341]\n",
      "loss: 0.796442  [88000/175341]\n",
      "loss: 0.197661  [89600/175341]\n",
      "loss: 0.773563  [91200/175341]\n",
      "loss: 0.693644  [92800/175341]\n",
      "loss: 0.340571  [94400/175341]\n",
      "loss: 0.398949  [96000/175341]\n",
      "loss: 0.341159  [97600/175341]\n",
      "loss: 0.493705  [99200/175341]\n",
      "loss: 0.419334  [100800/175341]\n",
      "loss: 0.416147  [102400/175341]\n",
      "loss: 0.413129  [104000/175341]\n",
      "loss: 0.338993  [105600/175341]\n",
      "loss: 0.518650  [107200/175341]\n",
      "loss: 0.308196  [108800/175341]\n",
      "loss: 0.765913  [110400/175341]\n",
      "loss: 0.663694  [112000/175341]\n",
      "loss: 0.345522  [113600/175341]\n",
      "loss: 0.188444  [115200/175341]\n",
      "loss: 0.640029  [116800/175341]\n",
      "loss: 0.333074  [118400/175341]\n",
      "loss: 0.364500  [120000/175341]\n",
      "loss: 0.364921  [121600/175341]\n",
      "loss: 0.506433  [123200/175341]\n",
      "loss: 0.355262  [124800/175341]\n",
      "loss: 0.271502  [126400/175341]\n",
      "loss: 0.443787  [128000/175341]\n",
      "loss: 0.502904  [129600/175341]\n",
      "loss: 0.277280  [131200/175341]\n",
      "loss: 0.728129  [132800/175341]\n",
      "loss: 0.222734  [134400/175341]\n",
      "loss: 0.497294  [136000/175341]\n",
      "loss: 0.563041  [137600/175341]\n",
      "loss: 0.511752  [139200/175341]\n",
      "loss: 0.528648  [140800/175341]\n",
      "loss: 0.390625  [142400/175341]\n",
      "loss: 0.694270  [144000/175341]\n",
      "loss: 0.650729  [145600/175341]\n",
      "loss: 0.236288  [147200/175341]\n",
      "loss: 0.425776  [148800/175341]\n",
      "loss: 0.421682  [150400/175341]\n",
      "loss: 0.601285  [152000/175341]\n",
      "loss: 0.436274  [153600/175341]\n",
      "loss: 0.620538  [155200/175341]\n",
      "loss: 0.720291  [156800/175341]\n",
      "loss: 0.522245  [158400/175341]\n",
      "loss: 0.665096  [160000/175341]\n",
      "loss: 0.674409  [161600/175341]\n",
      "loss: 0.775917  [163200/175341]\n",
      "loss: 0.600913  [164800/175341]\n",
      "loss: 0.731778  [166400/175341]\n",
      "loss: 0.642679  [168000/175341]\n",
      "loss: 0.779831  [169600/175341]\n",
      "loss: 0.427963  [171200/175341]\n",
      "loss: 0.346209  [172800/175341]\n",
      "loss: 0.153876  [174400/175341]\n",
      "Train Accuracy: 80.5830%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.561115, F1-score: 75.00%, Macro_F1-Score:  40.67%  \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.677242  [    0/175341]\n",
      "loss: 0.414313  [ 1600/175341]\n",
      "loss: 0.388758  [ 3200/175341]\n",
      "loss: 0.510170  [ 4800/175341]\n",
      "loss: 0.563477  [ 6400/175341]\n",
      "loss: 0.700936  [ 8000/175341]\n",
      "loss: 0.348590  [ 9600/175341]\n",
      "loss: 0.555336  [11200/175341]\n",
      "loss: 0.404394  [12800/175341]\n",
      "loss: 0.851561  [14400/175341]\n",
      "loss: 0.271721  [16000/175341]\n",
      "loss: 0.210461  [17600/175341]\n",
      "loss: 0.412763  [19200/175341]\n",
      "loss: 0.429981  [20800/175341]\n",
      "loss: 0.480129  [22400/175341]\n",
      "loss: 0.467513  [24000/175341]\n",
      "loss: 0.539743  [25600/175341]\n",
      "loss: 0.493333  [27200/175341]\n",
      "loss: 0.357535  [28800/175341]\n",
      "loss: 0.356494  [30400/175341]\n",
      "loss: 0.254196  [32000/175341]\n",
      "loss: 0.338664  [33600/175341]\n",
      "loss: 0.358455  [35200/175341]\n",
      "loss: 0.293329  [36800/175341]\n",
      "loss: 0.375932  [38400/175341]\n",
      "loss: 0.549662  [40000/175341]\n",
      "loss: 0.799838  [41600/175341]\n",
      "loss: 0.278823  [43200/175341]\n",
      "loss: 0.519835  [44800/175341]\n",
      "loss: 0.394773  [46400/175341]\n",
      "loss: 0.347855  [48000/175341]\n",
      "loss: 0.253828  [49600/175341]\n",
      "loss: 0.175998  [51200/175341]\n",
      "loss: 0.552811  [52800/175341]\n",
      "loss: 0.255057  [54400/175341]\n",
      "loss: 0.242483  [56000/175341]\n",
      "loss: 0.857153  [57600/175341]\n",
      "loss: 0.458768  [59200/175341]\n",
      "loss: 0.256095  [60800/175341]\n",
      "loss: 0.190981  [62400/175341]\n",
      "loss: 0.703808  [64000/175341]\n",
      "loss: 0.351040  [65600/175341]\n",
      "loss: 0.922189  [67200/175341]\n",
      "loss: 0.214002  [68800/175341]\n",
      "loss: 0.557100  [70400/175341]\n",
      "loss: 0.608629  [72000/175341]\n",
      "loss: 0.171477  [73600/175341]\n",
      "loss: 0.883350  [75200/175341]\n",
      "loss: 0.277659  [76800/175341]\n",
      "loss: 0.547802  [78400/175341]\n",
      "loss: 0.555799  [80000/175341]\n",
      "loss: 0.723849  [81600/175341]\n",
      "loss: 0.726257  [83200/175341]\n",
      "loss: 0.522745  [84800/175341]\n",
      "loss: 0.914046  [86400/175341]\n",
      "loss: 0.800337  [88000/175341]\n",
      "loss: 0.429734  [89600/175341]\n",
      "loss: 0.259412  [91200/175341]\n",
      "loss: 0.560689  [92800/175341]\n",
      "loss: 0.646305  [94400/175341]\n",
      "loss: 0.568733  [96000/175341]\n",
      "loss: 0.596784  [97600/175341]\n",
      "loss: 0.390284  [99200/175341]\n",
      "loss: 0.826435  [100800/175341]\n",
      "loss: 0.687948  [102400/175341]\n",
      "loss: 0.225152  [104000/175341]\n",
      "loss: 0.469003  [105600/175341]\n",
      "loss: 0.242810  [107200/175341]\n",
      "loss: 0.306450  [108800/175341]\n",
      "loss: 0.645957  [110400/175341]\n",
      "loss: 0.482742  [112000/175341]\n",
      "loss: 0.654016  [113600/175341]\n",
      "loss: 0.228912  [115200/175341]\n",
      "loss: 0.252644  [116800/175341]\n",
      "loss: 0.660264  [118400/175341]\n",
      "loss: 0.543368  [120000/175341]\n",
      "loss: 0.638292  [121600/175341]\n",
      "loss: 0.865350  [123200/175341]\n",
      "loss: 0.324325  [124800/175341]\n",
      "loss: 0.975351  [126400/175341]\n",
      "loss: 0.821280  [128000/175341]\n",
      "loss: 0.255624  [129600/175341]\n",
      "loss: 0.309071  [131200/175341]\n",
      "loss: 0.614252  [132800/175341]\n",
      "loss: 0.307858  [134400/175341]\n",
      "loss: 0.217242  [136000/175341]\n",
      "loss: 0.603133  [137600/175341]\n",
      "loss: 0.620708  [139200/175341]\n",
      "loss: 0.326071  [140800/175341]\n",
      "loss: 0.847470  [142400/175341]\n",
      "loss: 0.253501  [144000/175341]\n",
      "loss: 0.248015  [145600/175341]\n",
      "loss: 0.450506  [147200/175341]\n",
      "loss: 0.440293  [148800/175341]\n",
      "loss: 0.846264  [150400/175341]\n",
      "loss: 0.390399  [152000/175341]\n",
      "loss: 0.584389  [153600/175341]\n",
      "loss: 0.406646  [155200/175341]\n",
      "loss: 0.802511  [156800/175341]\n",
      "loss: 0.536254  [158400/175341]\n",
      "loss: 0.506220  [160000/175341]\n",
      "loss: 0.340053  [161600/175341]\n",
      "loss: 0.765418  [163200/175341]\n",
      "loss: 0.614016  [164800/175341]\n",
      "loss: 0.658619  [166400/175341]\n",
      "loss: 0.331491  [168000/175341]\n",
      "loss: 0.281463  [169600/175341]\n",
      "loss: 0.511873  [171200/175341]\n",
      "loss: 0.614533  [172800/175341]\n",
      "loss: 0.497596  [174400/175341]\n",
      "Train Accuracy: 80.6326%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.572766, F1-score: 73.92%, Macro_F1-Score:  40.67%  \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.458765  [    0/175341]\n",
      "loss: 0.375558  [ 1600/175341]\n",
      "loss: 0.418573  [ 3200/175341]\n",
      "loss: 0.336262  [ 4800/175341]\n",
      "loss: 0.228760  [ 6400/175341]\n",
      "loss: 0.404201  [ 8000/175341]\n",
      "loss: 0.318879  [ 9600/175341]\n",
      "loss: 0.523504  [11200/175341]\n",
      "loss: 0.559184  [12800/175341]\n",
      "loss: 0.714741  [14400/175341]\n",
      "loss: 0.365703  [16000/175341]\n",
      "loss: 0.622225  [17600/175341]\n",
      "loss: 0.689364  [19200/175341]\n",
      "loss: 0.521510  [20800/175341]\n",
      "loss: 0.278720  [22400/175341]\n",
      "loss: 0.331754  [24000/175341]\n",
      "loss: 0.312422  [25600/175341]\n",
      "loss: 0.412641  [27200/175341]\n",
      "loss: 0.462380  [28800/175341]\n",
      "loss: 0.333703  [30400/175341]\n",
      "loss: 0.272273  [32000/175341]\n",
      "loss: 0.653753  [33600/175341]\n",
      "loss: 0.674746  [35200/175341]\n",
      "loss: 0.451089  [36800/175341]\n",
      "loss: 0.576432  [38400/175341]\n",
      "loss: 0.162868  [40000/175341]\n",
      "loss: 0.357630  [41600/175341]\n",
      "loss: 0.279572  [43200/175341]\n",
      "loss: 0.777343  [44800/175341]\n",
      "loss: 0.199071  [46400/175341]\n",
      "loss: 0.463428  [48000/175341]\n",
      "loss: 0.504818  [49600/175341]\n",
      "loss: 0.690947  [51200/175341]\n",
      "loss: 0.410616  [52800/175341]\n",
      "loss: 0.518127  [54400/175341]\n",
      "loss: 0.343486  [56000/175341]\n",
      "loss: 0.521620  [57600/175341]\n",
      "loss: 0.179562  [59200/175341]\n",
      "loss: 0.496988  [60800/175341]\n",
      "loss: 0.623556  [62400/175341]\n",
      "loss: 0.424177  [64000/175341]\n",
      "loss: 0.310988  [65600/175341]\n",
      "loss: 0.497352  [67200/175341]\n",
      "loss: 0.429467  [68800/175341]\n",
      "loss: 0.154357  [70400/175341]\n",
      "loss: 0.645662  [72000/175341]\n",
      "loss: 0.313602  [73600/175341]\n",
      "loss: 0.763159  [75200/175341]\n",
      "loss: 0.382256  [76800/175341]\n",
      "loss: 0.281890  [78400/175341]\n",
      "loss: 0.222717  [80000/175341]\n",
      "loss: 0.470962  [81600/175341]\n",
      "loss: 0.351833  [83200/175341]\n",
      "loss: 0.456328  [84800/175341]\n",
      "loss: 0.445812  [86400/175341]\n",
      "loss: 0.381521  [88000/175341]\n",
      "loss: 0.492137  [89600/175341]\n",
      "loss: 0.273441  [91200/175341]\n",
      "loss: 0.671132  [92800/175341]\n",
      "loss: 0.295110  [94400/175341]\n",
      "loss: 0.467833  [96000/175341]\n",
      "loss: 0.644386  [97600/175341]\n",
      "loss: 0.415581  [99200/175341]\n",
      "loss: 0.508356  [100800/175341]\n",
      "loss: 0.983648  [102400/175341]\n",
      "loss: 0.345705  [104000/175341]\n",
      "loss: 0.510521  [105600/175341]\n",
      "loss: 0.581900  [107200/175341]\n",
      "loss: 0.674301  [108800/175341]\n",
      "loss: 0.596526  [110400/175341]\n",
      "loss: 0.361180  [112000/175341]\n",
      "loss: 0.426232  [113600/175341]\n",
      "loss: 0.523666  [115200/175341]\n",
      "loss: 0.707478  [116800/175341]\n",
      "loss: 0.295407  [118400/175341]\n",
      "loss: 0.311540  [120000/175341]\n",
      "loss: 0.610270  [121600/175341]\n",
      "loss: 0.676419  [123200/175341]\n",
      "loss: 0.620117  [124800/175341]\n",
      "loss: 0.417911  [126400/175341]\n",
      "loss: 0.323686  [128000/175341]\n",
      "loss: 0.623086  [129600/175341]\n",
      "loss: 0.310347  [131200/175341]\n",
      "loss: 0.097317  [132800/175341]\n",
      "loss: 0.718230  [134400/175341]\n",
      "loss: 0.335514  [136000/175341]\n",
      "loss: 0.245664  [137600/175341]\n",
      "loss: 0.402935  [139200/175341]\n",
      "loss: 0.260745  [140800/175341]\n",
      "loss: 0.821431  [142400/175341]\n",
      "loss: 0.672350  [144000/175341]\n",
      "loss: 0.556142  [145600/175341]\n",
      "loss: 0.717106  [147200/175341]\n",
      "loss: 0.383812  [148800/175341]\n",
      "loss: 0.149586  [150400/175341]\n",
      "loss: 0.615957  [152000/175341]\n",
      "loss: 0.612565  [153600/175341]\n",
      "loss: 0.556100  [155200/175341]\n",
      "loss: 0.165172  [156800/175341]\n",
      "loss: 0.365125  [158400/175341]\n",
      "loss: 1.032505  [160000/175341]\n",
      "loss: 0.725276  [161600/175341]\n",
      "loss: 0.313636  [163200/175341]\n",
      "loss: 0.390907  [164800/175341]\n",
      "loss: 0.729756  [166400/175341]\n",
      "loss: 0.613966  [168000/175341]\n",
      "loss: 0.669792  [169600/175341]\n",
      "loss: 0.734304  [171200/175341]\n",
      "loss: 0.551912  [172800/175341]\n",
      "loss: 0.414215  [174400/175341]\n",
      "Train Accuracy: 80.5493%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.582812, F1-score: 74.02%, Macro_F1-Score:  39.95%  \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.267680  [    0/175341]\n",
      "loss: 0.420497  [ 1600/175341]\n",
      "loss: 0.427221  [ 3200/175341]\n",
      "loss: 0.431601  [ 4800/175341]\n",
      "loss: 0.968348  [ 6400/175341]\n",
      "loss: 0.392386  [ 8000/175341]\n",
      "loss: 0.385066  [ 9600/175341]\n",
      "loss: 0.458781  [11200/175341]\n",
      "loss: 0.416747  [12800/175341]\n",
      "loss: 0.209941  [14400/175341]\n",
      "loss: 0.463966  [16000/175341]\n",
      "loss: 0.778997  [17600/175341]\n",
      "loss: 0.350785  [19200/175341]\n",
      "loss: 0.108060  [20800/175341]\n",
      "loss: 0.440037  [22400/175341]\n",
      "loss: 0.512770  [24000/175341]\n",
      "loss: 0.981067  [25600/175341]\n",
      "loss: 0.399595  [27200/175341]\n",
      "loss: 0.342206  [28800/175341]\n",
      "loss: 0.476641  [30400/175341]\n",
      "loss: 0.573068  [32000/175341]\n",
      "loss: 0.313428  [33600/175341]\n",
      "loss: 0.665573  [35200/175341]\n",
      "loss: 0.676183  [36800/175341]\n",
      "loss: 0.761540  [38400/175341]\n",
      "loss: 0.882443  [40000/175341]\n",
      "loss: 0.397525  [41600/175341]\n",
      "loss: 0.587105  [43200/175341]\n",
      "loss: 1.084919  [44800/175341]\n",
      "loss: 0.570262  [46400/175341]\n",
      "loss: 0.423241  [48000/175341]\n",
      "loss: 0.565898  [49600/175341]\n",
      "loss: 0.174439  [51200/175341]\n",
      "loss: 0.308693  [52800/175341]\n",
      "loss: 0.924075  [54400/175341]\n",
      "loss: 0.381950  [56000/175341]\n",
      "loss: 0.512191  [57600/175341]\n",
      "loss: 0.363002  [59200/175341]\n",
      "loss: 0.543700  [60800/175341]\n",
      "loss: 0.767089  [62400/175341]\n",
      "loss: 0.585945  [64000/175341]\n",
      "loss: 0.775912  [65600/175341]\n",
      "loss: 0.360836  [67200/175341]\n",
      "loss: 0.250153  [68800/175341]\n",
      "loss: 0.460495  [70400/175341]\n",
      "loss: 0.514838  [72000/175341]\n",
      "loss: 0.323651  [73600/175341]\n",
      "loss: 0.281946  [75200/175341]\n",
      "loss: 0.707618  [76800/175341]\n",
      "loss: 0.213692  [78400/175341]\n",
      "loss: 0.696687  [80000/175341]\n",
      "loss: 0.123347  [81600/175341]\n",
      "loss: 0.529577  [83200/175341]\n",
      "loss: 0.428029  [84800/175341]\n",
      "loss: 0.855949  [86400/175341]\n",
      "loss: 0.392826  [88000/175341]\n",
      "loss: 0.445692  [89600/175341]\n",
      "loss: 0.667656  [91200/175341]\n",
      "loss: 0.241778  [92800/175341]\n",
      "loss: 0.657237  [94400/175341]\n",
      "loss: 0.454196  [96000/175341]\n",
      "loss: 0.550804  [97600/175341]\n",
      "loss: 0.298798  [99200/175341]\n",
      "loss: 0.544086  [100800/175341]\n",
      "loss: 0.369624  [102400/175341]\n",
      "loss: 0.448422  [104000/175341]\n",
      "loss: 0.287661  [105600/175341]\n",
      "loss: 0.640796  [107200/175341]\n",
      "loss: 0.992528  [108800/175341]\n",
      "loss: 0.509080  [110400/175341]\n",
      "loss: 0.347925  [112000/175341]\n",
      "loss: 0.339108  [113600/175341]\n",
      "loss: 0.624058  [115200/175341]\n",
      "loss: 0.279899  [116800/175341]\n",
      "loss: 0.364335  [118400/175341]\n",
      "loss: 0.544355  [120000/175341]\n",
      "loss: 0.246196  [121600/175341]\n",
      "loss: 0.412813  [123200/175341]\n",
      "loss: 0.465648  [124800/175341]\n",
      "loss: 0.514748  [126400/175341]\n",
      "loss: 0.115603  [128000/175341]\n",
      "loss: 0.894692  [129600/175341]\n",
      "loss: 0.719073  [131200/175341]\n",
      "loss: 0.583632  [132800/175341]\n",
      "loss: 0.550669  [134400/175341]\n",
      "loss: 0.234344  [136000/175341]\n",
      "loss: 0.181917  [137600/175341]\n",
      "loss: 0.486450  [139200/175341]\n",
      "loss: 0.994134  [140800/175341]\n",
      "loss: 0.330994  [142400/175341]\n",
      "loss: 0.202878  [144000/175341]\n",
      "loss: 0.384777  [145600/175341]\n",
      "loss: 0.450613  [147200/175341]\n",
      "loss: 0.566412  [148800/175341]\n",
      "loss: 0.436969  [150400/175341]\n",
      "loss: 0.434346  [152000/175341]\n",
      "loss: 0.650415  [153600/175341]\n",
      "loss: 0.923331  [155200/175341]\n",
      "loss: 0.523007  [156800/175341]\n",
      "loss: 0.469209  [158400/175341]\n",
      "loss: 0.634559  [160000/175341]\n",
      "loss: 0.290730  [161600/175341]\n",
      "loss: 0.414075  [163200/175341]\n",
      "loss: 0.386528  [164800/175341]\n",
      "loss: 0.414883  [166400/175341]\n",
      "loss: 0.352590  [168000/175341]\n",
      "loss: 0.489363  [169600/175341]\n",
      "loss: 0.161106  [171200/175341]\n",
      "loss: 1.021004  [172800/175341]\n",
      "loss: 0.818809  [174400/175341]\n",
      "Train Accuracy: 80.5790%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.565831, F1-score: 74.61%, Macro_F1-Score:  40.21%  \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.703464  [    0/175341]\n",
      "loss: 0.738207  [ 1600/175341]\n",
      "loss: 0.401968  [ 3200/175341]\n",
      "loss: 0.183030  [ 4800/175341]\n",
      "loss: 0.689847  [ 6400/175341]\n",
      "loss: 0.601059  [ 8000/175341]\n",
      "loss: 0.437998  [ 9600/175341]\n",
      "loss: 0.349870  [11200/175341]\n",
      "loss: 0.531671  [12800/175341]\n",
      "loss: 0.531404  [14400/175341]\n",
      "loss: 0.610858  [16000/175341]\n",
      "loss: 0.377448  [17600/175341]\n",
      "loss: 0.485552  [19200/175341]\n",
      "loss: 0.792592  [20800/175341]\n",
      "loss: 0.252696  [22400/175341]\n",
      "loss: 0.403857  [24000/175341]\n",
      "loss: 0.288536  [25600/175341]\n",
      "loss: 0.207220  [27200/175341]\n",
      "loss: 0.419019  [28800/175341]\n",
      "loss: 0.261346  [30400/175341]\n",
      "loss: 0.563606  [32000/175341]\n",
      "loss: 0.481890  [33600/175341]\n",
      "loss: 0.725393  [35200/175341]\n",
      "loss: 0.656731  [36800/175341]\n",
      "loss: 0.653803  [38400/175341]\n",
      "loss: 0.205630  [40000/175341]\n",
      "loss: 0.384294  [41600/175341]\n",
      "loss: 0.431446  [43200/175341]\n",
      "loss: 0.345306  [44800/175341]\n",
      "loss: 0.352545  [46400/175341]\n",
      "loss: 0.342024  [48000/175341]\n",
      "loss: 0.717901  [49600/175341]\n",
      "loss: 0.575801  [51200/175341]\n",
      "loss: 0.393958  [52800/175341]\n",
      "loss: 1.048929  [54400/175341]\n",
      "loss: 0.199398  [56000/175341]\n",
      "loss: 0.683432  [57600/175341]\n",
      "loss: 0.268747  [59200/175341]\n",
      "loss: 0.457346  [60800/175341]\n",
      "loss: 0.245834  [62400/175341]\n",
      "loss: 0.322488  [64000/175341]\n",
      "loss: 0.457193  [65600/175341]\n",
      "loss: 0.402891  [67200/175341]\n",
      "loss: 0.540300  [68800/175341]\n",
      "loss: 0.405744  [70400/175341]\n",
      "loss: 0.617548  [72000/175341]\n",
      "loss: 0.255943  [73600/175341]\n",
      "loss: 0.344349  [75200/175341]\n",
      "loss: 0.362696  [76800/175341]\n",
      "loss: 0.375189  [78400/175341]\n",
      "loss: 0.276076  [80000/175341]\n",
      "loss: 0.181871  [81600/175341]\n",
      "loss: 0.364406  [83200/175341]\n",
      "loss: 0.693729  [84800/175341]\n",
      "loss: 0.215139  [86400/175341]\n",
      "loss: 0.475851  [88000/175341]\n",
      "loss: 0.228647  [89600/175341]\n",
      "loss: 0.468847  [91200/175341]\n",
      "loss: 0.616533  [92800/175341]\n",
      "loss: 0.116225  [94400/175341]\n",
      "loss: 0.868697  [96000/175341]\n",
      "loss: 0.656415  [97600/175341]\n",
      "loss: 0.580926  [99200/175341]\n",
      "loss: 0.793822  [100800/175341]\n",
      "loss: 0.233500  [102400/175341]\n",
      "loss: 0.298679  [104000/175341]\n",
      "loss: 0.880633  [105600/175341]\n",
      "loss: 0.641425  [107200/175341]\n",
      "loss: 0.269073  [108800/175341]\n",
      "loss: 0.395622  [110400/175341]\n",
      "loss: 0.345709  [112000/175341]\n",
      "loss: 0.280239  [113600/175341]\n",
      "loss: 0.498521  [115200/175341]\n",
      "loss: 0.256556  [116800/175341]\n",
      "loss: 0.498447  [118400/175341]\n",
      "loss: 0.493864  [120000/175341]\n",
      "loss: 0.309352  [121600/175341]\n",
      "loss: 0.540932  [123200/175341]\n",
      "loss: 0.780752  [124800/175341]\n",
      "loss: 0.526683  [126400/175341]\n",
      "loss: 0.761572  [128000/175341]\n",
      "loss: 0.566208  [129600/175341]\n",
      "loss: 0.846028  [131200/175341]\n",
      "loss: 0.255914  [132800/175341]\n",
      "loss: 0.559744  [134400/175341]\n",
      "loss: 0.563862  [136000/175341]\n",
      "loss: 1.045586  [137600/175341]\n",
      "loss: 0.454350  [139200/175341]\n",
      "loss: 0.855339  [140800/175341]\n",
      "loss: 0.207450  [142400/175341]\n",
      "loss: 0.405749  [144000/175341]\n",
      "loss: 0.413424  [145600/175341]\n",
      "loss: 0.734272  [147200/175341]\n",
      "loss: 0.383268  [148800/175341]\n",
      "loss: 0.518074  [150400/175341]\n",
      "loss: 0.508426  [152000/175341]\n",
      "loss: 0.245909  [153600/175341]\n",
      "loss: 0.215236  [155200/175341]\n",
      "loss: 0.540337  [156800/175341]\n",
      "loss: 0.290821  [158400/175341]\n",
      "loss: 0.314919  [160000/175341]\n",
      "loss: 0.289075  [161600/175341]\n",
      "loss: 0.458563  [163200/175341]\n",
      "loss: 0.336601  [164800/175341]\n",
      "loss: 0.358366  [166400/175341]\n",
      "loss: 0.575855  [168000/175341]\n",
      "loss: 0.291918  [169600/175341]\n",
      "loss: 0.323305  [171200/175341]\n",
      "loss: 0.701794  [172800/175341]\n",
      "loss: 0.276587  [174400/175341]\n",
      "Train Accuracy: 80.5944%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.561562, F1-score: 74.85%, Macro_F1-Score:  41.23%  \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.336347  [    0/175341]\n",
      "loss: 0.598162  [ 1600/175341]\n",
      "loss: 0.425400  [ 3200/175341]\n",
      "loss: 0.662438  [ 4800/175341]\n",
      "loss: 0.548794  [ 6400/175341]\n",
      "loss: 0.464092  [ 8000/175341]\n",
      "loss: 0.271963  [ 9600/175341]\n",
      "loss: 0.599834  [11200/175341]\n",
      "loss: 0.613298  [12800/175341]\n",
      "loss: 0.430932  [14400/175341]\n",
      "loss: 0.518498  [16000/175341]\n",
      "loss: 0.969090  [17600/175341]\n",
      "loss: 0.553141  [19200/175341]\n",
      "loss: 0.213839  [20800/175341]\n",
      "loss: 0.508660  [22400/175341]\n",
      "loss: 0.691869  [24000/175341]\n",
      "loss: 0.108622  [25600/175341]\n",
      "loss: 0.552479  [27200/175341]\n",
      "loss: 0.307258  [28800/175341]\n",
      "loss: 0.110787  [30400/175341]\n",
      "loss: 0.574341  [32000/175341]\n",
      "loss: 0.253142  [33600/175341]\n",
      "loss: 0.800895  [35200/175341]\n",
      "loss: 1.182613  [36800/175341]\n",
      "loss: 0.085722  [38400/175341]\n",
      "loss: 0.496852  [40000/175341]\n",
      "loss: 0.399935  [41600/175341]\n",
      "loss: 0.689172  [43200/175341]\n",
      "loss: 0.404865  [44800/175341]\n",
      "loss: 0.316631  [46400/175341]\n",
      "loss: 0.585321  [48000/175341]\n",
      "loss: 0.544053  [49600/175341]\n",
      "loss: 0.345742  [51200/175341]\n",
      "loss: 0.349889  [52800/175341]\n",
      "loss: 0.455159  [54400/175341]\n",
      "loss: 0.113416  [56000/175341]\n",
      "loss: 0.113263  [57600/175341]\n",
      "loss: 0.241435  [59200/175341]\n",
      "loss: 0.301329  [60800/175341]\n",
      "loss: 0.521821  [62400/175341]\n",
      "loss: 0.283514  [64000/175341]\n",
      "loss: 0.501295  [65600/175341]\n",
      "loss: 0.174872  [67200/175341]\n",
      "loss: 0.321210  [68800/175341]\n",
      "loss: 0.970667  [70400/175341]\n",
      "loss: 0.722299  [72000/175341]\n",
      "loss: 0.431091  [73600/175341]\n",
      "loss: 0.715987  [75200/175341]\n",
      "loss: 0.241483  [76800/175341]\n",
      "loss: 0.202735  [78400/175341]\n",
      "loss: 0.389262  [80000/175341]\n",
      "loss: 0.265674  [81600/175341]\n",
      "loss: 0.445250  [83200/175341]\n",
      "loss: 0.422061  [84800/175341]\n",
      "loss: 0.395409  [86400/175341]\n",
      "loss: 0.807221  [88000/175341]\n",
      "loss: 0.499449  [89600/175341]\n",
      "loss: 0.359903  [91200/175341]\n",
      "loss: 0.095453  [92800/175341]\n",
      "loss: 0.401283  [94400/175341]\n",
      "loss: 0.799990  [96000/175341]\n",
      "loss: 0.662100  [97600/175341]\n",
      "loss: 0.460795  [99200/175341]\n",
      "loss: 0.251500  [100800/175341]\n",
      "loss: 0.305620  [102400/175341]\n",
      "loss: 0.415935  [104000/175341]\n",
      "loss: 0.506508  [105600/175341]\n",
      "loss: 0.425891  [107200/175341]\n",
      "loss: 0.418422  [108800/175341]\n",
      "loss: 0.569063  [110400/175341]\n",
      "loss: 0.574193  [112000/175341]\n",
      "loss: 0.859986  [113600/175341]\n",
      "loss: 0.801626  [115200/175341]\n",
      "loss: 0.947243  [116800/175341]\n",
      "loss: 0.572921  [118400/175341]\n",
      "loss: 0.457187  [120000/175341]\n",
      "loss: 0.386830  [121600/175341]\n",
      "loss: 0.601217  [123200/175341]\n",
      "loss: 0.327527  [124800/175341]\n",
      "loss: 0.782339  [126400/175341]\n",
      "loss: 1.084350  [128000/175341]\n",
      "loss: 0.421144  [129600/175341]\n",
      "loss: 0.388765  [131200/175341]\n",
      "loss: 0.255169  [132800/175341]\n",
      "loss: 0.489427  [134400/175341]\n",
      "loss: 0.094610  [136000/175341]\n",
      "loss: 0.470791  [137600/175341]\n",
      "loss: 0.102035  [139200/175341]\n",
      "loss: 0.359351  [140800/175341]\n",
      "loss: 0.517615  [142400/175341]\n",
      "loss: 0.246244  [144000/175341]\n",
      "loss: 0.450510  [145600/175341]\n",
      "loss: 0.349151  [147200/175341]\n",
      "loss: 0.147462  [148800/175341]\n",
      "loss: 0.210464  [150400/175341]\n",
      "loss: 0.475573  [152000/175341]\n",
      "loss: 0.297312  [153600/175341]\n",
      "loss: 0.554186  [155200/175341]\n",
      "loss: 0.388685  [156800/175341]\n",
      "loss: 0.231620  [158400/175341]\n",
      "loss: 0.232860  [160000/175341]\n",
      "loss: 0.659602  [161600/175341]\n",
      "loss: 0.207534  [163200/175341]\n",
      "loss: 0.301017  [164800/175341]\n",
      "loss: 0.662172  [166400/175341]\n",
      "loss: 0.607643  [168000/175341]\n",
      "loss: 0.436245  [169600/175341]\n",
      "loss: 0.730994  [171200/175341]\n",
      "loss: 0.530171  [172800/175341]\n",
      "loss: 0.265979  [174400/175341]\n",
      "Train Accuracy: 80.6246%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.553884, F1-score: 74.98%, Macro_F1-Score:  40.76%  \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.407899  [    0/175341]\n",
      "loss: 0.503179  [ 1600/175341]\n",
      "loss: 0.545716  [ 3200/175341]\n",
      "loss: 0.412169  [ 4800/175341]\n",
      "loss: 0.724706  [ 6400/175341]\n",
      "loss: 0.247539  [ 8000/175341]\n",
      "loss: 0.692102  [ 9600/175341]\n",
      "loss: 0.439688  [11200/175341]\n",
      "loss: 0.297200  [12800/175341]\n",
      "loss: 0.347198  [14400/175341]\n",
      "loss: 0.537449  [16000/175341]\n",
      "loss: 0.258165  [17600/175341]\n",
      "loss: 0.695294  [19200/175341]\n",
      "loss: 0.639550  [20800/175341]\n",
      "loss: 0.554009  [22400/175341]\n",
      "loss: 0.933092  [24000/175341]\n",
      "loss: 0.427662  [25600/175341]\n",
      "loss: 0.667617  [27200/175341]\n",
      "loss: 0.768650  [28800/175341]\n",
      "loss: 0.239763  [30400/175341]\n",
      "loss: 0.197819  [32000/175341]\n",
      "loss: 0.564255  [33600/175341]\n",
      "loss: 0.610892  [35200/175341]\n",
      "loss: 0.601981  [36800/175341]\n",
      "loss: 0.801923  [38400/175341]\n",
      "loss: 0.258422  [40000/175341]\n",
      "loss: 0.335205  [41600/175341]\n",
      "loss: 0.515958  [43200/175341]\n",
      "loss: 0.400937  [44800/175341]\n",
      "loss: 0.368092  [46400/175341]\n",
      "loss: 0.477613  [48000/175341]\n",
      "loss: 0.486564  [49600/175341]\n",
      "loss: 0.453694  [51200/175341]\n",
      "loss: 0.580751  [52800/175341]\n",
      "loss: 0.512886  [54400/175341]\n",
      "loss: 0.638637  [56000/175341]\n",
      "loss: 0.784075  [57600/175341]\n",
      "loss: 0.175383  [59200/175341]\n",
      "loss: 0.416353  [60800/175341]\n",
      "loss: 0.426982  [62400/175341]\n",
      "loss: 0.491299  [64000/175341]\n",
      "loss: 0.287857  [65600/175341]\n",
      "loss: 0.526085  [67200/175341]\n",
      "loss: 0.526168  [68800/175341]\n",
      "loss: 0.423705  [70400/175341]\n",
      "loss: 0.371886  [72000/175341]\n",
      "loss: 0.421776  [73600/175341]\n",
      "loss: 0.749833  [75200/175341]\n",
      "loss: 0.519653  [76800/175341]\n",
      "loss: 0.817361  [78400/175341]\n",
      "loss: 0.312671  [80000/175341]\n",
      "loss: 0.119398  [81600/175341]\n",
      "loss: 0.184824  [83200/175341]\n",
      "loss: 0.428118  [84800/175341]\n",
      "loss: 0.236162  [86400/175341]\n",
      "loss: 0.340421  [88000/175341]\n",
      "loss: 0.418817  [89600/175341]\n",
      "loss: 0.779087  [91200/175341]\n",
      "loss: 0.249741  [92800/175341]\n",
      "loss: 0.546985  [94400/175341]\n",
      "loss: 0.438099  [96000/175341]\n",
      "loss: 0.593428  [97600/175341]\n",
      "loss: 0.194873  [99200/175341]\n",
      "loss: 0.571601  [100800/175341]\n",
      "loss: 0.617857  [102400/175341]\n",
      "loss: 0.744452  [104000/175341]\n",
      "loss: 0.241151  [105600/175341]\n",
      "loss: 0.547348  [107200/175341]\n",
      "loss: 0.354836  [108800/175341]\n",
      "loss: 0.241910  [110400/175341]\n",
      "loss: 0.320184  [112000/175341]\n",
      "loss: 0.314321  [113600/175341]\n",
      "loss: 0.774184  [115200/175341]\n",
      "loss: 0.301812  [116800/175341]\n",
      "loss: 0.223920  [118400/175341]\n",
      "loss: 0.215196  [120000/175341]\n",
      "loss: 0.633609  [121600/175341]\n",
      "loss: 0.802475  [123200/175341]\n",
      "loss: 0.242265  [124800/175341]\n",
      "loss: 0.858079  [126400/175341]\n",
      "loss: 0.301820  [128000/175341]\n",
      "loss: 0.369736  [129600/175341]\n",
      "loss: 0.303421  [131200/175341]\n",
      "loss: 0.571524  [132800/175341]\n",
      "loss: 0.435298  [134400/175341]\n",
      "loss: 0.588741  [136000/175341]\n",
      "loss: 0.265372  [137600/175341]\n",
      "loss: 0.328665  [139200/175341]\n",
      "loss: 0.347979  [140800/175341]\n",
      "loss: 0.457461  [142400/175341]\n",
      "loss: 0.360284  [144000/175341]\n",
      "loss: 0.351879  [145600/175341]\n",
      "loss: 0.276798  [147200/175341]\n",
      "loss: 0.142409  [148800/175341]\n",
      "loss: 0.433889  [150400/175341]\n",
      "loss: 0.290434  [152000/175341]\n",
      "loss: 0.660952  [153600/175341]\n",
      "loss: 0.461792  [155200/175341]\n",
      "loss: 0.663412  [156800/175341]\n",
      "loss: 0.264490  [158400/175341]\n",
      "loss: 0.477442  [160000/175341]\n",
      "loss: 0.573013  [161600/175341]\n",
      "loss: 0.131669  [163200/175341]\n",
      "loss: 0.740365  [164800/175341]\n",
      "loss: 0.655448  [166400/175341]\n",
      "loss: 1.121289  [168000/175341]\n",
      "loss: 0.212179  [169600/175341]\n",
      "loss: 0.211479  [171200/175341]\n",
      "loss: 0.861853  [172800/175341]\n",
      "loss: 0.421337  [174400/175341]\n",
      "Train Accuracy: 80.6104%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.565865, F1-score: 74.61%, Macro_F1-Score:  41.41%  \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.861553  [    0/175341]\n",
      "loss: 0.251124  [ 1600/175341]\n",
      "loss: 0.061370  [ 3200/175341]\n",
      "loss: 0.175807  [ 4800/175341]\n",
      "loss: 0.564806  [ 6400/175341]\n",
      "loss: 1.037911  [ 8000/175341]\n",
      "loss: 0.482619  [ 9600/175341]\n",
      "loss: 0.844178  [11200/175341]\n",
      "loss: 0.311649  [12800/175341]\n",
      "loss: 0.295278  [14400/175341]\n",
      "loss: 0.093725  [16000/175341]\n",
      "loss: 0.821801  [17600/175341]\n",
      "loss: 0.638742  [19200/175341]\n",
      "loss: 0.491955  [20800/175341]\n",
      "loss: 0.695758  [22400/175341]\n",
      "loss: 0.633656  [24000/175341]\n",
      "loss: 0.243259  [25600/175341]\n",
      "loss: 0.383319  [27200/175341]\n",
      "loss: 0.316360  [28800/175341]\n",
      "loss: 0.647745  [30400/175341]\n",
      "loss: 0.575905  [32000/175341]\n",
      "loss: 0.736850  [33600/175341]\n",
      "loss: 0.383610  [35200/175341]\n",
      "loss: 0.439672  [36800/175341]\n",
      "loss: 1.105868  [38400/175341]\n",
      "loss: 0.611146  [40000/175341]\n",
      "loss: 0.593227  [41600/175341]\n",
      "loss: 0.230134  [43200/175341]\n",
      "loss: 0.378965  [44800/175341]\n",
      "loss: 0.393812  [46400/175341]\n",
      "loss: 1.019573  [48000/175341]\n",
      "loss: 0.399458  [49600/175341]\n",
      "loss: 0.436965  [51200/175341]\n",
      "loss: 0.278343  [52800/175341]\n",
      "loss: 0.535883  [54400/175341]\n",
      "loss: 0.923378  [56000/175341]\n",
      "loss: 0.322026  [57600/175341]\n",
      "loss: 0.333981  [59200/175341]\n",
      "loss: 0.304417  [60800/175341]\n",
      "loss: 0.253600  [62400/175341]\n",
      "loss: 0.400692  [64000/175341]\n",
      "loss: 0.270704  [65600/175341]\n",
      "loss: 0.648871  [67200/175341]\n",
      "loss: 0.952290  [68800/175341]\n",
      "loss: 0.252212  [70400/175341]\n",
      "loss: 0.348932  [72000/175341]\n",
      "loss: 0.672808  [73600/175341]\n",
      "loss: 0.962463  [75200/175341]\n",
      "loss: 0.506550  [76800/175341]\n",
      "loss: 0.287924  [78400/175341]\n",
      "loss: 0.854520  [80000/175341]\n",
      "loss: 0.406572  [81600/175341]\n",
      "loss: 0.241916  [83200/175341]\n",
      "loss: 0.347601  [84800/175341]\n",
      "loss: 0.544647  [86400/175341]\n",
      "loss: 0.631431  [88000/175341]\n",
      "loss: 0.150403  [89600/175341]\n",
      "loss: 0.743150  [91200/175341]\n",
      "loss: 0.189348  [92800/175341]\n",
      "loss: 1.121222  [94400/175341]\n",
      "loss: 0.554305  [96000/175341]\n",
      "loss: 0.764559  [97600/175341]\n",
      "loss: 0.320452  [99200/175341]\n",
      "loss: 0.384591  [100800/175341]\n",
      "loss: 0.358667  [102400/175341]\n",
      "loss: 0.393997  [104000/175341]\n",
      "loss: 0.402717  [105600/175341]\n",
      "loss: 0.615462  [107200/175341]\n",
      "loss: 0.180812  [108800/175341]\n",
      "loss: 0.814956  [110400/175341]\n",
      "loss: 0.434158  [112000/175341]\n",
      "loss: 0.483496  [113600/175341]\n",
      "loss: 0.220336  [115200/175341]\n",
      "loss: 0.473625  [116800/175341]\n",
      "loss: 0.257716  [118400/175341]\n",
      "loss: 0.573237  [120000/175341]\n",
      "loss: 0.267653  [121600/175341]\n",
      "loss: 0.215960  [123200/175341]\n",
      "loss: 0.623642  [124800/175341]\n",
      "loss: 0.431825  [126400/175341]\n",
      "loss: 0.186549  [128000/175341]\n",
      "loss: 0.819147  [129600/175341]\n",
      "loss: 0.409525  [131200/175341]\n",
      "loss: 0.325798  [132800/175341]\n",
      "loss: 0.721752  [134400/175341]\n",
      "loss: 0.490114  [136000/175341]\n",
      "loss: 0.311074  [137600/175341]\n",
      "loss: 0.581785  [139200/175341]\n",
      "loss: 0.243650  [140800/175341]\n",
      "loss: 0.194800  [142400/175341]\n",
      "loss: 0.404754  [144000/175341]\n",
      "loss: 0.260516  [145600/175341]\n",
      "loss: 0.516393  [147200/175341]\n",
      "loss: 0.377942  [148800/175341]\n",
      "loss: 0.173813  [150400/175341]\n",
      "loss: 0.409117  [152000/175341]\n",
      "loss: 0.782049  [153600/175341]\n",
      "loss: 0.565895  [155200/175341]\n",
      "loss: 0.538098  [156800/175341]\n",
      "loss: 0.371345  [158400/175341]\n",
      "loss: 1.213519  [160000/175341]\n",
      "loss: 0.642757  [161600/175341]\n",
      "loss: 0.246637  [163200/175341]\n",
      "loss: 0.593004  [164800/175341]\n",
      "loss: 0.585164  [166400/175341]\n",
      "loss: 0.482649  [168000/175341]\n",
      "loss: 0.470150  [169600/175341]\n",
      "loss: 0.308664  [171200/175341]\n",
      "loss: 0.599026  [172800/175341]\n",
      "loss: 0.716194  [174400/175341]\n",
      "Train Accuracy: 80.6583%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.542593, F1-score: 76.10%, Macro_F1-Score:  41.80%  \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.770761  [    0/175341]\n",
      "loss: 0.347257  [ 1600/175341]\n",
      "loss: 0.549205  [ 3200/175341]\n",
      "loss: 0.401240  [ 4800/175341]\n",
      "loss: 0.474022  [ 6400/175341]\n",
      "loss: 0.580992  [ 8000/175341]\n",
      "loss: 0.460571  [ 9600/175341]\n",
      "loss: 0.574574  [11200/175341]\n",
      "loss: 0.525100  [12800/175341]\n",
      "loss: 0.522529  [14400/175341]\n",
      "loss: 0.225840  [16000/175341]\n",
      "loss: 0.415321  [17600/175341]\n",
      "loss: 0.292542  [19200/175341]\n",
      "loss: 0.309544  [20800/175341]\n",
      "loss: 0.447773  [22400/175341]\n",
      "loss: 0.287355  [24000/175341]\n",
      "loss: 0.404826  [25600/175341]\n",
      "loss: 0.485711  [27200/175341]\n",
      "loss: 0.531549  [28800/175341]\n",
      "loss: 0.238293  [30400/175341]\n",
      "loss: 0.561112  [32000/175341]\n",
      "loss: 0.495586  [33600/175341]\n",
      "loss: 0.677406  [35200/175341]\n",
      "loss: 0.597863  [36800/175341]\n",
      "loss: 0.267781  [38400/175341]\n",
      "loss: 0.090567  [40000/175341]\n",
      "loss: 0.291613  [41600/175341]\n",
      "loss: 0.379629  [43200/175341]\n",
      "loss: 0.171652  [44800/175341]\n",
      "loss: 0.423437  [46400/175341]\n",
      "loss: 0.569074  [48000/175341]\n",
      "loss: 0.571452  [49600/175341]\n",
      "loss: 0.410154  [51200/175341]\n",
      "loss: 0.736350  [52800/175341]\n",
      "loss: 0.674138  [54400/175341]\n",
      "loss: 0.374681  [56000/175341]\n",
      "loss: 0.341995  [57600/175341]\n",
      "loss: 0.451029  [59200/175341]\n",
      "loss: 0.124654  [60800/175341]\n",
      "loss: 0.179179  [62400/175341]\n",
      "loss: 0.300845  [64000/175341]\n",
      "loss: 0.276938  [65600/175341]\n",
      "loss: 0.516894  [67200/175341]\n",
      "loss: 0.531406  [68800/175341]\n",
      "loss: 0.536178  [70400/175341]\n",
      "loss: 0.209641  [72000/175341]\n",
      "loss: 0.496179  [73600/175341]\n",
      "loss: 0.951868  [75200/175341]\n",
      "loss: 0.758081  [76800/175341]\n",
      "loss: 0.339938  [78400/175341]\n",
      "loss: 0.333144  [80000/175341]\n",
      "loss: 0.406110  [81600/175341]\n",
      "loss: 0.492718  [83200/175341]\n",
      "loss: 0.163195  [84800/175341]\n",
      "loss: 0.599588  [86400/175341]\n",
      "loss: 0.369880  [88000/175341]\n",
      "loss: 0.324843  [89600/175341]\n",
      "loss: 0.381757  [91200/175341]\n",
      "loss: 0.678885  [92800/175341]\n",
      "loss: 0.068375  [94400/175341]\n",
      "loss: 0.475885  [96000/175341]\n",
      "loss: 0.664594  [97600/175341]\n",
      "loss: 0.494513  [99200/175341]\n",
      "loss: 0.329320  [100800/175341]\n",
      "loss: 0.505638  [102400/175341]\n",
      "loss: 0.454370  [104000/175341]\n",
      "loss: 0.382334  [105600/175341]\n",
      "loss: 0.400166  [107200/175341]\n",
      "loss: 0.569564  [108800/175341]\n",
      "loss: 0.401062  [110400/175341]\n",
      "loss: 0.675677  [112000/175341]\n",
      "loss: 0.530489  [113600/175341]\n",
      "loss: 0.374533  [115200/175341]\n",
      "loss: 0.274794  [116800/175341]\n",
      "loss: 0.885262  [118400/175341]\n",
      "loss: 0.782485  [120000/175341]\n",
      "loss: 0.248163  [121600/175341]\n",
      "loss: 0.310735  [123200/175341]\n",
      "loss: 1.023402  [124800/175341]\n",
      "loss: 0.416997  [126400/175341]\n",
      "loss: 0.656482  [128000/175341]\n",
      "loss: 0.259104  [129600/175341]\n",
      "loss: 1.060969  [131200/175341]\n",
      "loss: 0.235096  [132800/175341]\n",
      "loss: 0.698675  [134400/175341]\n",
      "loss: 0.525909  [136000/175341]\n",
      "loss: 0.130482  [137600/175341]\n",
      "loss: 0.750274  [139200/175341]\n",
      "loss: 0.408726  [140800/175341]\n",
      "loss: 0.731090  [142400/175341]\n",
      "loss: 0.461088  [144000/175341]\n",
      "loss: 0.266564  [145600/175341]\n",
      "loss: 0.354361  [147200/175341]\n",
      "loss: 0.433092  [148800/175341]\n",
      "loss: 0.310946  [150400/175341]\n",
      "loss: 0.397653  [152000/175341]\n",
      "loss: 0.258909  [153600/175341]\n",
      "loss: 0.198424  [155200/175341]\n",
      "loss: 0.264481  [156800/175341]\n",
      "loss: 0.638869  [158400/175341]\n",
      "loss: 1.027015  [160000/175341]\n",
      "loss: 0.271605  [161600/175341]\n",
      "loss: 0.262221  [163200/175341]\n",
      "loss: 0.555372  [164800/175341]\n",
      "loss: 0.439837  [166400/175341]\n",
      "loss: 0.843803  [168000/175341]\n",
      "loss: 0.246710  [169600/175341]\n",
      "loss: 0.408220  [171200/175341]\n",
      "loss: 0.313694  [172800/175341]\n",
      "loss: 0.590798  [174400/175341]\n",
      "Train Accuracy: 80.6554%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.574420, F1-score: 74.38%, Macro_F1-Score:  40.79%  \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.444644  [    0/175341]\n",
      "loss: 0.288032  [ 1600/175341]\n",
      "loss: 0.392009  [ 3200/175341]\n",
      "loss: 0.322607  [ 4800/175341]\n",
      "loss: 0.289770  [ 6400/175341]\n",
      "loss: 0.485041  [ 8000/175341]\n",
      "loss: 0.457672  [ 9600/175341]\n",
      "loss: 0.404766  [11200/175341]\n",
      "loss: 0.164088  [12800/175341]\n",
      "loss: 0.669880  [14400/175341]\n",
      "loss: 0.411892  [16000/175341]\n",
      "loss: 0.496481  [17600/175341]\n",
      "loss: 0.444974  [19200/175341]\n",
      "loss: 0.587211  [20800/175341]\n",
      "loss: 0.495869  [22400/175341]\n",
      "loss: 0.396807  [24000/175341]\n",
      "loss: 0.114522  [25600/175341]\n",
      "loss: 0.436482  [27200/175341]\n",
      "loss: 0.188347  [28800/175341]\n",
      "loss: 0.327252  [30400/175341]\n",
      "loss: 0.355081  [32000/175341]\n",
      "loss: 0.348487  [33600/175341]\n",
      "loss: 0.338495  [35200/175341]\n",
      "loss: 0.314243  [36800/175341]\n",
      "loss: 0.475985  [38400/175341]\n",
      "loss: 0.379967  [40000/175341]\n",
      "loss: 0.514740  [41600/175341]\n",
      "loss: 0.347084  [43200/175341]\n",
      "loss: 0.688718  [44800/175341]\n",
      "loss: 0.194659  [46400/175341]\n",
      "loss: 0.415859  [48000/175341]\n",
      "loss: 0.491837  [49600/175341]\n",
      "loss: 0.338220  [51200/175341]\n",
      "loss: 0.254186  [52800/175341]\n",
      "loss: 0.469624  [54400/175341]\n",
      "loss: 0.704758  [56000/175341]\n",
      "loss: 0.674660  [57600/175341]\n",
      "loss: 0.519131  [59200/175341]\n",
      "loss: 0.310320  [60800/175341]\n",
      "loss: 0.319461  [62400/175341]\n",
      "loss: 0.403806  [64000/175341]\n",
      "loss: 0.091878  [65600/175341]\n",
      "loss: 0.471317  [67200/175341]\n",
      "loss: 0.734330  [68800/175341]\n",
      "loss: 0.297976  [70400/175341]\n",
      "loss: 0.677997  [72000/175341]\n",
      "loss: 0.292008  [73600/175341]\n",
      "loss: 0.481767  [75200/175341]\n",
      "loss: 0.389272  [76800/175341]\n",
      "loss: 0.301214  [78400/175341]\n",
      "loss: 0.349216  [80000/175341]\n",
      "loss: 0.912504  [81600/175341]\n",
      "loss: 0.253554  [83200/175341]\n",
      "loss: 0.401705  [84800/175341]\n",
      "loss: 0.329007  [86400/175341]\n",
      "loss: 0.315731  [88000/175341]\n",
      "loss: 0.988937  [89600/175341]\n",
      "loss: 0.393146  [91200/175341]\n",
      "loss: 0.185026  [92800/175341]\n",
      "loss: 0.385713  [94400/175341]\n",
      "loss: 0.745829  [96000/175341]\n",
      "loss: 0.625708  [97600/175341]\n",
      "loss: 0.406985  [99200/175341]\n",
      "loss: 0.308316  [100800/175341]\n",
      "loss: 0.428145  [102400/175341]\n",
      "loss: 0.402404  [104000/175341]\n",
      "loss: 0.288391  [105600/175341]\n",
      "loss: 0.342541  [107200/175341]\n",
      "loss: 0.515194  [108800/175341]\n",
      "loss: 0.282878  [110400/175341]\n",
      "loss: 0.355812  [112000/175341]\n",
      "loss: 0.705827  [113600/175341]\n",
      "loss: 0.312886  [115200/175341]\n",
      "loss: 0.475793  [116800/175341]\n",
      "loss: 0.481062  [118400/175341]\n",
      "loss: 1.005665  [120000/175341]\n",
      "loss: 0.332015  [121600/175341]\n",
      "loss: 0.591585  [123200/175341]\n",
      "loss: 0.241458  [124800/175341]\n",
      "loss: 0.558295  [126400/175341]\n",
      "loss: 0.869661  [128000/175341]\n",
      "loss: 0.362784  [129600/175341]\n",
      "loss: 0.414354  [131200/175341]\n",
      "loss: 0.150227  [132800/175341]\n",
      "loss: 0.534876  [134400/175341]\n",
      "loss: 0.437848  [136000/175341]\n",
      "loss: 0.710229  [137600/175341]\n",
      "loss: 0.533268  [139200/175341]\n",
      "loss: 0.075082  [140800/175341]\n",
      "loss: 0.925092  [142400/175341]\n",
      "loss: 0.504915  [144000/175341]\n",
      "loss: 0.379168  [145600/175341]\n",
      "loss: 0.138334  [147200/175341]\n",
      "loss: 0.189878  [148800/175341]\n",
      "loss: 0.505732  [150400/175341]\n",
      "loss: 0.338671  [152000/175341]\n",
      "loss: 0.384823  [153600/175341]\n",
      "loss: 0.269985  [155200/175341]\n",
      "loss: 0.339691  [156800/175341]\n",
      "loss: 0.404387  [158400/175341]\n",
      "loss: 0.817689  [160000/175341]\n",
      "loss: 0.526613  [161600/175341]\n",
      "loss: 0.335910  [163200/175341]\n",
      "loss: 0.492016  [164800/175341]\n",
      "loss: 0.358198  [166400/175341]\n",
      "loss: 0.458361  [168000/175341]\n",
      "loss: 0.552189  [169600/175341]\n",
      "loss: 0.465059  [171200/175341]\n",
      "loss: 0.674236  [172800/175341]\n",
      "loss: 0.491707  [174400/175341]\n",
      "Train Accuracy: 80.7016%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.556918, F1-score: 75.19%, Macro_F1-Score:  40.79%  \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.645585  [    0/175341]\n",
      "loss: 0.692176  [ 1600/175341]\n",
      "loss: 0.350910  [ 3200/175341]\n",
      "loss: 0.205194  [ 4800/175341]\n",
      "loss: 0.546572  [ 6400/175341]\n",
      "loss: 0.565911  [ 8000/175341]\n",
      "loss: 0.712875  [ 9600/175341]\n",
      "loss: 0.356084  [11200/175341]\n",
      "loss: 0.610563  [12800/175341]\n",
      "loss: 0.778959  [14400/175341]\n",
      "loss: 0.881136  [16000/175341]\n",
      "loss: 0.661133  [17600/175341]\n",
      "loss: 0.453870  [19200/175341]\n",
      "loss: 0.476542  [20800/175341]\n",
      "loss: 0.581620  [22400/175341]\n",
      "loss: 0.487135  [24000/175341]\n",
      "loss: 0.600418  [25600/175341]\n",
      "loss: 0.452788  [27200/175341]\n",
      "loss: 0.412443  [28800/175341]\n",
      "loss: 0.468726  [30400/175341]\n",
      "loss: 0.568226  [32000/175341]\n",
      "loss: 0.342952  [33600/175341]\n",
      "loss: 0.306850  [35200/175341]\n",
      "loss: 0.802443  [36800/175341]\n",
      "loss: 0.115613  [38400/175341]\n",
      "loss: 0.679741  [40000/175341]\n",
      "loss: 0.291631  [41600/175341]\n",
      "loss: 0.584670  [43200/175341]\n",
      "loss: 0.337350  [44800/175341]\n",
      "loss: 0.363723  [46400/175341]\n",
      "loss: 0.510210  [48000/175341]\n",
      "loss: 0.468199  [49600/175341]\n",
      "loss: 0.469391  [51200/175341]\n",
      "loss: 0.482899  [52800/175341]\n",
      "loss: 0.785254  [54400/175341]\n",
      "loss: 0.643826  [56000/175341]\n",
      "loss: 0.365374  [57600/175341]\n",
      "loss: 0.714538  [59200/175341]\n",
      "loss: 0.525700  [60800/175341]\n",
      "loss: 0.491010  [62400/175341]\n",
      "loss: 0.573136  [64000/175341]\n",
      "loss: 0.915826  [65600/175341]\n",
      "loss: 0.409053  [67200/175341]\n",
      "loss: 0.570795  [68800/175341]\n",
      "loss: 0.680405  [70400/175341]\n",
      "loss: 0.392956  [72000/175341]\n",
      "loss: 0.571483  [73600/175341]\n",
      "loss: 0.396677  [75200/175341]\n",
      "loss: 0.664032  [76800/175341]\n",
      "loss: 0.676224  [78400/175341]\n",
      "loss: 0.449765  [80000/175341]\n",
      "loss: 0.932908  [81600/175341]\n",
      "loss: 0.815255  [83200/175341]\n",
      "loss: 0.406377  [84800/175341]\n",
      "loss: 0.701462  [86400/175341]\n",
      "loss: 0.872836  [88000/175341]\n",
      "loss: 0.681585  [89600/175341]\n",
      "loss: 0.853437  [91200/175341]\n",
      "loss: 0.518526  [92800/175341]\n",
      "loss: 0.205301  [94400/175341]\n",
      "loss: 0.595880  [96000/175341]\n",
      "loss: 0.364070  [97600/175341]\n",
      "loss: 0.199923  [99200/175341]\n",
      "loss: 0.941436  [100800/175341]\n",
      "loss: 0.660682  [102400/175341]\n",
      "loss: 0.435108  [104000/175341]\n",
      "loss: 0.570084  [105600/175341]\n",
      "loss: 0.154895  [107200/175341]\n",
      "loss: 0.631660  [108800/175341]\n",
      "loss: 0.566888  [110400/175341]\n",
      "loss: 0.413336  [112000/175341]\n",
      "loss: 0.439876  [113600/175341]\n",
      "loss: 0.805259  [115200/175341]\n",
      "loss: 0.572832  [116800/175341]\n",
      "loss: 0.172629  [118400/175341]\n",
      "loss: 0.575774  [120000/175341]\n",
      "loss: 0.441373  [121600/175341]\n",
      "loss: 0.735444  [123200/175341]\n",
      "loss: 0.534526  [124800/175341]\n",
      "loss: 0.364327  [126400/175341]\n",
      "loss: 0.413514  [128000/175341]\n",
      "loss: 0.471992  [129600/175341]\n",
      "loss: 0.784094  [131200/175341]\n",
      "loss: 0.245819  [132800/175341]\n",
      "loss: 0.380560  [134400/175341]\n",
      "loss: 0.338873  [136000/175341]\n",
      "loss: 0.714167  [137600/175341]\n",
      "loss: 0.489335  [139200/175341]\n",
      "loss: 0.330990  [140800/175341]\n",
      "loss: 0.300250  [142400/175341]\n",
      "loss: 0.699726  [144000/175341]\n",
      "loss: 0.403881  [145600/175341]\n",
      "loss: 0.637411  [147200/175341]\n",
      "loss: 0.357212  [148800/175341]\n",
      "loss: 0.796694  [150400/175341]\n",
      "loss: 0.403279  [152000/175341]\n",
      "loss: 0.348674  [153600/175341]\n",
      "loss: 0.315557  [155200/175341]\n",
      "loss: 0.870499  [156800/175341]\n",
      "loss: 0.456982  [158400/175341]\n",
      "loss: 0.588969  [160000/175341]\n",
      "loss: 0.237111  [161600/175341]\n",
      "loss: 0.248671  [163200/175341]\n",
      "loss: 0.513877  [164800/175341]\n",
      "loss: 0.398292  [166400/175341]\n",
      "loss: 0.661830  [168000/175341]\n",
      "loss: 0.746502  [169600/175341]\n",
      "loss: 0.694038  [171200/175341]\n",
      "loss: 0.193618  [172800/175341]\n",
      "loss: 0.745920  [174400/175341]\n",
      "Train Accuracy: 80.7216%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.571095, F1-score: 74.26%, Macro_F1-Score:  40.84%  \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.624817  [    0/175341]\n",
      "loss: 0.413253  [ 1600/175341]\n",
      "loss: 0.459251  [ 3200/175341]\n",
      "loss: 0.553371  [ 4800/175341]\n",
      "loss: 0.461293  [ 6400/175341]\n",
      "loss: 0.244298  [ 8000/175341]\n",
      "loss: 0.339949  [ 9600/175341]\n",
      "loss: 0.718786  [11200/175341]\n",
      "loss: 0.764142  [12800/175341]\n",
      "loss: 0.630265  [14400/175341]\n",
      "loss: 0.317804  [16000/175341]\n",
      "loss: 0.233664  [17600/175341]\n",
      "loss: 0.323063  [19200/175341]\n",
      "loss: 0.848183  [20800/175341]\n",
      "loss: 0.688497  [22400/175341]\n",
      "loss: 0.163199  [24000/175341]\n",
      "loss: 0.598086  [25600/175341]\n",
      "loss: 0.176368  [27200/175341]\n",
      "loss: 0.096210  [28800/175341]\n",
      "loss: 0.307598  [30400/175341]\n",
      "loss: 0.488411  [32000/175341]\n",
      "loss: 0.723476  [33600/175341]\n",
      "loss: 0.706661  [35200/175341]\n",
      "loss: 1.080216  [36800/175341]\n",
      "loss: 0.459360  [38400/175341]\n",
      "loss: 0.271306  [40000/175341]\n",
      "loss: 0.283057  [41600/175341]\n",
      "loss: 0.514756  [43200/175341]\n",
      "loss: 0.184934  [44800/175341]\n",
      "loss: 0.891753  [46400/175341]\n",
      "loss: 0.478381  [48000/175341]\n",
      "loss: 1.056679  [49600/175341]\n",
      "loss: 0.619710  [51200/175341]\n",
      "loss: 0.410770  [52800/175341]\n",
      "loss: 0.195722  [54400/175341]\n",
      "loss: 0.090425  [56000/175341]\n",
      "loss: 0.382615  [57600/175341]\n",
      "loss: 0.470171  [59200/175341]\n",
      "loss: 0.366490  [60800/175341]\n",
      "loss: 0.338893  [62400/175341]\n",
      "loss: 0.430186  [64000/175341]\n",
      "loss: 0.617145  [65600/175341]\n",
      "loss: 0.290406  [67200/175341]\n",
      "loss: 0.441459  [68800/175341]\n",
      "loss: 0.317707  [70400/175341]\n",
      "loss: 0.539691  [72000/175341]\n",
      "loss: 0.306729  [73600/175341]\n",
      "loss: 0.221132  [75200/175341]\n",
      "loss: 0.173333  [76800/175341]\n",
      "loss: 0.249928  [78400/175341]\n",
      "loss: 0.677389  [80000/175341]\n",
      "loss: 0.423445  [81600/175341]\n",
      "loss: 0.306095  [83200/175341]\n",
      "loss: 0.675467  [84800/175341]\n",
      "loss: 0.626242  [86400/175341]\n",
      "loss: 0.238337  [88000/175341]\n",
      "loss: 0.325755  [89600/175341]\n",
      "loss: 0.243473  [91200/175341]\n",
      "loss: 0.369782  [92800/175341]\n",
      "loss: 0.369505  [94400/175341]\n",
      "loss: 0.725870  [96000/175341]\n",
      "loss: 0.887506  [97600/175341]\n",
      "loss: 0.350597  [99200/175341]\n",
      "loss: 0.527594  [100800/175341]\n",
      "loss: 1.126894  [102400/175341]\n",
      "loss: 0.840473  [104000/175341]\n",
      "loss: 0.499689  [105600/175341]\n",
      "loss: 0.651689  [107200/175341]\n",
      "loss: 0.403257  [108800/175341]\n",
      "loss: 0.369530  [110400/175341]\n",
      "loss: 0.339114  [112000/175341]\n",
      "loss: 0.451359  [113600/175341]\n",
      "loss: 0.573928  [115200/175341]\n",
      "loss: 0.212656  [116800/175341]\n",
      "loss: 0.324285  [118400/175341]\n",
      "loss: 0.600524  [120000/175341]\n",
      "loss: 0.406256  [121600/175341]\n",
      "loss: 0.775057  [123200/175341]\n",
      "loss: 0.831669  [124800/175341]\n",
      "loss: 0.977564  [126400/175341]\n",
      "loss: 0.651243  [128000/175341]\n",
      "loss: 0.813526  [129600/175341]\n",
      "loss: 0.527477  [131200/175341]\n",
      "loss: 0.269277  [132800/175341]\n",
      "loss: 0.375457  [134400/175341]\n",
      "loss: 0.366771  [136000/175341]\n",
      "loss: 0.403577  [137600/175341]\n",
      "loss: 0.117654  [139200/175341]\n",
      "loss: 0.411213  [140800/175341]\n",
      "loss: 0.523220  [142400/175341]\n",
      "loss: 0.489158  [144000/175341]\n",
      "loss: 0.294088  [145600/175341]\n",
      "loss: 0.441434  [147200/175341]\n",
      "loss: 0.378271  [148800/175341]\n",
      "loss: 0.484984  [150400/175341]\n",
      "loss: 0.191913  [152000/175341]\n",
      "loss: 0.589417  [153600/175341]\n",
      "loss: 0.619810  [155200/175341]\n",
      "loss: 0.446074  [156800/175341]\n",
      "loss: 0.610729  [158400/175341]\n",
      "loss: 0.552254  [160000/175341]\n",
      "loss: 0.625441  [161600/175341]\n",
      "loss: 0.409411  [163200/175341]\n",
      "loss: 0.704746  [164800/175341]\n",
      "loss: 0.368353  [166400/175341]\n",
      "loss: 0.294029  [168000/175341]\n",
      "loss: 0.787171  [169600/175341]\n",
      "loss: 0.342919  [171200/175341]\n",
      "loss: 0.377689  [172800/175341]\n",
      "loss: 0.535252  [174400/175341]\n",
      "Train Accuracy: 80.7107%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.553879, F1-score: 74.96%, Macro_F1-Score:  40.53%  \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.560363  [    0/175341]\n",
      "loss: 0.519698  [ 1600/175341]\n",
      "loss: 0.275778  [ 3200/175341]\n",
      "loss: 0.456817  [ 4800/175341]\n",
      "loss: 0.452113  [ 6400/175341]\n",
      "loss: 0.602037  [ 8000/175341]\n",
      "loss: 0.796612  [ 9600/175341]\n",
      "loss: 0.254431  [11200/175341]\n",
      "loss: 0.602269  [12800/175341]\n",
      "loss: 0.571301  [14400/175341]\n",
      "loss: 0.259049  [16000/175341]\n",
      "loss: 0.333316  [17600/175341]\n",
      "loss: 0.767036  [19200/175341]\n",
      "loss: 0.473642  [20800/175341]\n",
      "loss: 0.403695  [22400/175341]\n",
      "loss: 0.594610  [24000/175341]\n",
      "loss: 0.467357  [25600/175341]\n",
      "loss: 0.321782  [27200/175341]\n",
      "loss: 0.532847  [28800/175341]\n",
      "loss: 0.420911  [30400/175341]\n",
      "loss: 0.538002  [32000/175341]\n",
      "loss: 0.231861  [33600/175341]\n",
      "loss: 0.295290  [35200/175341]\n",
      "loss: 0.369728  [36800/175341]\n",
      "loss: 0.183766  [38400/175341]\n",
      "loss: 0.448127  [40000/175341]\n",
      "loss: 0.736673  [41600/175341]\n",
      "loss: 0.550662  [43200/175341]\n",
      "loss: 0.377426  [44800/175341]\n",
      "loss: 0.392106  [46400/175341]\n",
      "loss: 0.306973  [48000/175341]\n",
      "loss: 0.356026  [49600/175341]\n",
      "loss: 0.907007  [51200/175341]\n",
      "loss: 0.610094  [52800/175341]\n",
      "loss: 0.576604  [54400/175341]\n",
      "loss: 0.400519  [56000/175341]\n",
      "loss: 0.449165  [57600/175341]\n",
      "loss: 0.473184  [59200/175341]\n",
      "loss: 0.205308  [60800/175341]\n",
      "loss: 0.446535  [62400/175341]\n",
      "loss: 0.460465  [64000/175341]\n",
      "loss: 0.760720  [65600/175341]\n",
      "loss: 0.575348  [67200/175341]\n",
      "loss: 0.103744  [68800/175341]\n",
      "loss: 0.221642  [70400/175341]\n",
      "loss: 0.715161  [72000/175341]\n",
      "loss: 0.713591  [73600/175341]\n",
      "loss: 0.375212  [75200/175341]\n",
      "loss: 0.537635  [76800/175341]\n",
      "loss: 0.625928  [78400/175341]\n",
      "loss: 0.977869  [80000/175341]\n",
      "loss: 0.477217  [81600/175341]\n",
      "loss: 0.301124  [83200/175341]\n",
      "loss: 0.494296  [84800/175341]\n",
      "loss: 0.725112  [86400/175341]\n",
      "loss: 0.413419  [88000/175341]\n",
      "loss: 0.430344  [89600/175341]\n",
      "loss: 0.537921  [91200/175341]\n",
      "loss: 0.196512  [92800/175341]\n",
      "loss: 0.207374  [94400/175341]\n",
      "loss: 0.898841  [96000/175341]\n",
      "loss: 0.564887  [97600/175341]\n",
      "loss: 0.739168  [99200/175341]\n",
      "loss: 0.686871  [100800/175341]\n",
      "loss: 0.176050  [102400/175341]\n",
      "loss: 0.544822  [104000/175341]\n",
      "loss: 0.092914  [105600/175341]\n",
      "loss: 0.368933  [107200/175341]\n",
      "loss: 0.971497  [108800/175341]\n",
      "loss: 0.705551  [110400/175341]\n",
      "loss: 0.461808  [112000/175341]\n",
      "loss: 0.679456  [113600/175341]\n",
      "loss: 0.527643  [115200/175341]\n",
      "loss: 0.568701  [116800/175341]\n",
      "loss: 0.137792  [118400/175341]\n",
      "loss: 0.424208  [120000/175341]\n",
      "loss: 0.619312  [121600/175341]\n",
      "loss: 0.468890  [123200/175341]\n",
      "loss: 0.598058  [124800/175341]\n",
      "loss: 0.563948  [126400/175341]\n",
      "loss: 0.593716  [128000/175341]\n",
      "loss: 0.621171  [129600/175341]\n",
      "loss: 0.506094  [131200/175341]\n",
      "loss: 0.597316  [132800/175341]\n",
      "loss: 0.557667  [134400/175341]\n",
      "loss: 0.635576  [136000/175341]\n",
      "loss: 0.536246  [137600/175341]\n",
      "loss: 0.465047  [139200/175341]\n",
      "loss: 0.301805  [140800/175341]\n",
      "loss: 0.063037  [142400/175341]\n",
      "loss: 0.319318  [144000/175341]\n",
      "loss: 0.430168  [145600/175341]\n",
      "loss: 0.255206  [147200/175341]\n",
      "loss: 0.355064  [148800/175341]\n",
      "loss: 0.340626  [150400/175341]\n",
      "loss: 0.473941  [152000/175341]\n",
      "loss: 0.342181  [153600/175341]\n",
      "loss: 0.673457  [155200/175341]\n",
      "loss: 0.294585  [156800/175341]\n",
      "loss: 0.473067  [158400/175341]\n",
      "loss: 0.629085  [160000/175341]\n",
      "loss: 0.385113  [161600/175341]\n",
      "loss: 0.385323  [163200/175341]\n",
      "loss: 0.587566  [164800/175341]\n",
      "loss: 0.455272  [166400/175341]\n",
      "loss: 0.371238  [168000/175341]\n",
      "loss: 0.593144  [169600/175341]\n",
      "loss: 0.627857  [171200/175341]\n",
      "loss: 1.135184  [172800/175341]\n",
      "loss: 0.492825  [174400/175341]\n",
      "Train Accuracy: 80.6976%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.556186, F1-score: 75.01%, Macro_F1-Score:  40.98%  \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.402961  [    0/175341]\n",
      "loss: 0.690700  [ 1600/175341]\n",
      "loss: 0.058412  [ 3200/175341]\n",
      "loss: 0.428270  [ 4800/175341]\n",
      "loss: 0.300768  [ 6400/175341]\n",
      "loss: 0.477344  [ 8000/175341]\n",
      "loss: 0.268919  [ 9600/175341]\n",
      "loss: 0.822749  [11200/175341]\n",
      "loss: 0.608783  [12800/175341]\n",
      "loss: 0.428929  [14400/175341]\n",
      "loss: 0.853916  [16000/175341]\n",
      "loss: 0.490167  [17600/175341]\n",
      "loss: 0.559378  [19200/175341]\n",
      "loss: 0.596226  [20800/175341]\n",
      "loss: 0.309928  [22400/175341]\n",
      "loss: 0.173773  [24000/175341]\n",
      "loss: 0.326305  [25600/175341]\n",
      "loss: 0.428754  [27200/175341]\n",
      "loss: 0.314128  [28800/175341]\n",
      "loss: 0.966326  [30400/175341]\n",
      "loss: 0.567477  [32000/175341]\n",
      "loss: 0.738410  [33600/175341]\n",
      "loss: 0.371844  [35200/175341]\n",
      "loss: 0.902840  [36800/175341]\n",
      "loss: 0.773816  [38400/175341]\n",
      "loss: 0.677347  [40000/175341]\n",
      "loss: 0.254961  [41600/175341]\n",
      "loss: 0.440971  [43200/175341]\n",
      "loss: 0.249788  [44800/175341]\n",
      "loss: 0.294424  [46400/175341]\n",
      "loss: 0.554477  [48000/175341]\n",
      "loss: 0.146594  [49600/175341]\n",
      "loss: 0.257316  [51200/175341]\n",
      "loss: 0.804269  [52800/175341]\n",
      "loss: 0.256882  [54400/175341]\n",
      "loss: 0.407440  [56000/175341]\n",
      "loss: 0.706507  [57600/175341]\n",
      "loss: 0.479886  [59200/175341]\n",
      "loss: 0.622843  [60800/175341]\n",
      "loss: 0.228960  [62400/175341]\n",
      "loss: 0.841789  [64000/175341]\n",
      "loss: 0.395897  [65600/175341]\n",
      "loss: 0.546690  [67200/175341]\n",
      "loss: 0.317555  [68800/175341]\n",
      "loss: 0.225704  [70400/175341]\n",
      "loss: 0.453383  [72000/175341]\n",
      "loss: 0.409205  [73600/175341]\n",
      "loss: 0.500258  [75200/175341]\n",
      "loss: 0.465307  [76800/175341]\n",
      "loss: 0.674366  [78400/175341]\n",
      "loss: 1.237160  [80000/175341]\n",
      "loss: 0.677314  [81600/175341]\n",
      "loss: 0.408289  [83200/175341]\n",
      "loss: 0.507566  [84800/175341]\n",
      "loss: 0.502422  [86400/175341]\n",
      "loss: 0.773567  [88000/175341]\n",
      "loss: 0.346022  [89600/175341]\n",
      "loss: 0.682284  [91200/175341]\n",
      "loss: 0.461855  [92800/175341]\n",
      "loss: 0.461198  [94400/175341]\n",
      "loss: 0.730239  [96000/175341]\n",
      "loss: 0.636491  [97600/175341]\n",
      "loss: 0.390243  [99200/175341]\n",
      "loss: 0.487953  [100800/175341]\n",
      "loss: 0.634156  [102400/175341]\n",
      "loss: 0.359084  [104000/175341]\n",
      "loss: 0.628837  [105600/175341]\n",
      "loss: 0.215254  [107200/175341]\n",
      "loss: 0.037009  [108800/175341]\n",
      "loss: 0.372042  [110400/175341]\n",
      "loss: 0.658611  [112000/175341]\n",
      "loss: 0.563972  [113600/175341]\n",
      "loss: 0.536952  [115200/175341]\n",
      "loss: 0.315699  [116800/175341]\n",
      "loss: 0.322222  [118400/175341]\n",
      "loss: 0.468060  [120000/175341]\n",
      "loss: 0.312624  [121600/175341]\n",
      "loss: 0.765295  [123200/175341]\n",
      "loss: 0.741386  [124800/175341]\n",
      "loss: 0.299884  [126400/175341]\n",
      "loss: 0.562873  [128000/175341]\n",
      "loss: 0.462777  [129600/175341]\n",
      "loss: 0.768585  [131200/175341]\n",
      "loss: 0.079835  [132800/175341]\n",
      "loss: 0.472589  [134400/175341]\n",
      "loss: 0.674251  [136000/175341]\n",
      "loss: 0.701743  [137600/175341]\n",
      "loss: 0.375583  [139200/175341]\n",
      "loss: 0.786304  [140800/175341]\n",
      "loss: 0.302656  [142400/175341]\n",
      "loss: 0.447959  [144000/175341]\n",
      "loss: 0.117353  [145600/175341]\n",
      "loss: 0.221784  [147200/175341]\n",
      "loss: 0.526167  [148800/175341]\n",
      "loss: 0.540152  [150400/175341]\n",
      "loss: 0.642068  [152000/175341]\n",
      "loss: 0.656661  [153600/175341]\n",
      "loss: 0.316893  [155200/175341]\n",
      "loss: 0.718863  [156800/175341]\n",
      "loss: 0.474828  [158400/175341]\n",
      "loss: 0.364870  [160000/175341]\n",
      "loss: 0.368345  [161600/175341]\n",
      "loss: 0.754927  [163200/175341]\n",
      "loss: 0.737766  [164800/175341]\n",
      "loss: 0.256533  [166400/175341]\n",
      "loss: 0.169855  [168000/175341]\n",
      "loss: 0.747805  [169600/175341]\n",
      "loss: 0.169824  [171200/175341]\n",
      "loss: 0.117214  [172800/175341]\n",
      "loss: 0.577941  [174400/175341]\n",
      "Train Accuracy: 80.7820%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.552273, F1-score: 75.12%, Macro_F1-Score:  40.45%  \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.599322  [    0/175341]\n",
      "loss: 0.676034  [ 1600/175341]\n",
      "loss: 0.190693  [ 3200/175341]\n",
      "loss: 0.472691  [ 4800/175341]\n",
      "loss: 0.418440  [ 6400/175341]\n",
      "loss: 0.611302  [ 8000/175341]\n",
      "loss: 0.804249  [ 9600/175341]\n",
      "loss: 0.375481  [11200/175341]\n",
      "loss: 0.509074  [12800/175341]\n",
      "loss: 0.374782  [14400/175341]\n",
      "loss: 0.298882  [16000/175341]\n",
      "loss: 0.518098  [17600/175341]\n",
      "loss: 0.908238  [19200/175341]\n",
      "loss: 0.646867  [20800/175341]\n",
      "loss: 0.762678  [22400/175341]\n",
      "loss: 0.155162  [24000/175341]\n",
      "loss: 0.265638  [25600/175341]\n",
      "loss: 0.859745  [27200/175341]\n",
      "loss: 0.590283  [28800/175341]\n",
      "loss: 0.763336  [30400/175341]\n",
      "loss: 0.396936  [32000/175341]\n",
      "loss: 0.434051  [33600/175341]\n",
      "loss: 0.255427  [35200/175341]\n",
      "loss: 0.334813  [36800/175341]\n",
      "loss: 0.542817  [38400/175341]\n",
      "loss: 0.482617  [40000/175341]\n",
      "loss: 0.679369  [41600/175341]\n",
      "loss: 0.655389  [43200/175341]\n",
      "loss: 0.490366  [44800/175341]\n",
      "loss: 0.410592  [46400/175341]\n",
      "loss: 0.484347  [48000/175341]\n",
      "loss: 0.213675  [49600/175341]\n",
      "loss: 0.425266  [51200/175341]\n",
      "loss: 0.563977  [52800/175341]\n",
      "loss: 0.383940  [54400/175341]\n",
      "loss: 0.747131  [56000/175341]\n",
      "loss: 0.323029  [57600/175341]\n",
      "loss: 0.381047  [59200/175341]\n",
      "loss: 0.398311  [60800/175341]\n",
      "loss: 0.401740  [62400/175341]\n",
      "loss: 0.258131  [64000/175341]\n",
      "loss: 0.628171  [65600/175341]\n",
      "loss: 0.353946  [67200/175341]\n",
      "loss: 0.345468  [68800/175341]\n",
      "loss: 0.529251  [70400/175341]\n",
      "loss: 0.288074  [72000/175341]\n",
      "loss: 0.543101  [73600/175341]\n",
      "loss: 0.414884  [75200/175341]\n",
      "loss: 0.784041  [76800/175341]\n",
      "loss: 0.342187  [78400/175341]\n",
      "loss: 0.282821  [80000/175341]\n",
      "loss: 0.226919  [81600/175341]\n",
      "loss: 0.279570  [83200/175341]\n",
      "loss: 0.195384  [84800/175341]\n",
      "loss: 0.659537  [86400/175341]\n",
      "loss: 0.632425  [88000/175341]\n",
      "loss: 0.648670  [89600/175341]\n",
      "loss: 0.064901  [91200/175341]\n",
      "loss: 0.543540  [92800/175341]\n",
      "loss: 0.755577  [94400/175341]\n",
      "loss: 0.322265  [96000/175341]\n",
      "loss: 0.196623  [97600/175341]\n",
      "loss: 0.639998  [99200/175341]\n",
      "loss: 0.658975  [100800/175341]\n",
      "loss: 0.991335  [102400/175341]\n",
      "loss: 0.436903  [104000/175341]\n",
      "loss: 0.365178  [105600/175341]\n",
      "loss: 0.261216  [107200/175341]\n",
      "loss: 0.924095  [108800/175341]\n",
      "loss: 0.801813  [110400/175341]\n",
      "loss: 0.223517  [112000/175341]\n",
      "loss: 0.776482  [113600/175341]\n",
      "loss: 0.328185  [115200/175341]\n",
      "loss: 0.333153  [116800/175341]\n",
      "loss: 0.322026  [118400/175341]\n",
      "loss: 0.772453  [120000/175341]\n",
      "loss: 0.661344  [121600/175341]\n",
      "loss: 0.567506  [123200/175341]\n",
      "loss: 0.606703  [124800/175341]\n",
      "loss: 0.352152  [126400/175341]\n",
      "loss: 0.226938  [128000/175341]\n",
      "loss: 0.408966  [129600/175341]\n",
      "loss: 0.208255  [131200/175341]\n",
      "loss: 0.665451  [132800/175341]\n",
      "loss: 0.964768  [134400/175341]\n",
      "loss: 0.499198  [136000/175341]\n",
      "loss: 0.213604  [137600/175341]\n",
      "loss: 0.453895  [139200/175341]\n",
      "loss: 0.312923  [140800/175341]\n",
      "loss: 0.426017  [142400/175341]\n",
      "loss: 0.768701  [144000/175341]\n",
      "loss: 0.451765  [145600/175341]\n",
      "loss: 0.360518  [147200/175341]\n",
      "loss: 0.651862  [148800/175341]\n",
      "loss: 0.262765  [150400/175341]\n",
      "loss: 0.267030  [152000/175341]\n",
      "loss: 0.244885  [153600/175341]\n",
      "loss: 0.356881  [155200/175341]\n",
      "loss: 0.175697  [156800/175341]\n",
      "loss: 0.364506  [158400/175341]\n",
      "loss: 0.502581  [160000/175341]\n",
      "loss: 0.577312  [161600/175341]\n",
      "loss: 0.239141  [163200/175341]\n",
      "loss: 0.499978  [164800/175341]\n",
      "loss: 0.436490  [166400/175341]\n",
      "loss: 0.333011  [168000/175341]\n",
      "loss: 0.558476  [169600/175341]\n",
      "loss: 0.158225  [171200/175341]\n",
      "loss: 0.512350  [172800/175341]\n",
      "loss: 0.537610  [174400/175341]\n",
      "Train Accuracy: 80.7193%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.543136, F1-score: 76.45%, Macro_F1-Score:  41.47%  \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.252251  [    0/175341]\n",
      "loss: 0.773297  [ 1600/175341]\n",
      "loss: 0.569059  [ 3200/175341]\n",
      "loss: 0.438113  [ 4800/175341]\n",
      "loss: 0.295793  [ 6400/175341]\n",
      "loss: 0.336720  [ 8000/175341]\n",
      "loss: 0.751749  [ 9600/175341]\n",
      "loss: 0.126381  [11200/175341]\n",
      "loss: 0.340092  [12800/175341]\n",
      "loss: 0.350581  [14400/175341]\n",
      "loss: 0.221298  [16000/175341]\n",
      "loss: 1.260698  [17600/175341]\n",
      "loss: 0.368019  [19200/175341]\n",
      "loss: 0.573169  [20800/175341]\n",
      "loss: 0.361419  [22400/175341]\n",
      "loss: 0.154444  [24000/175341]\n",
      "loss: 0.317864  [25600/175341]\n",
      "loss: 0.237186  [27200/175341]\n",
      "loss: 0.662857  [28800/175341]\n",
      "loss: 0.568443  [30400/175341]\n",
      "loss: 0.494635  [32000/175341]\n",
      "loss: 0.423261  [33600/175341]\n",
      "loss: 0.463938  [35200/175341]\n",
      "loss: 0.567028  [36800/175341]\n",
      "loss: 0.607519  [38400/175341]\n",
      "loss: 0.275652  [40000/175341]\n",
      "loss: 0.512502  [41600/175341]\n",
      "loss: 0.614144  [43200/175341]\n",
      "loss: 0.479321  [44800/175341]\n",
      "loss: 0.324359  [46400/175341]\n",
      "loss: 0.510163  [48000/175341]\n",
      "loss: 0.198030  [49600/175341]\n",
      "loss: 0.257744  [51200/175341]\n",
      "loss: 0.325497  [52800/175341]\n",
      "loss: 0.435604  [54400/175341]\n",
      "loss: 0.692650  [56000/175341]\n",
      "loss: 0.678760  [57600/175341]\n",
      "loss: 0.398457  [59200/175341]\n",
      "loss: 0.850433  [60800/175341]\n",
      "loss: 0.814978  [62400/175341]\n",
      "loss: 0.316079  [64000/175341]\n",
      "loss: 0.457692  [65600/175341]\n",
      "loss: 0.338298  [67200/175341]\n",
      "loss: 0.550784  [68800/175341]\n",
      "loss: 0.456484  [70400/175341]\n",
      "loss: 0.647920  [72000/175341]\n",
      "loss: 0.135960  [73600/175341]\n",
      "loss: 0.618998  [75200/175341]\n",
      "loss: 0.688622  [76800/175341]\n",
      "loss: 0.271897  [78400/175341]\n",
      "loss: 0.155501  [80000/175341]\n",
      "loss: 0.287850  [81600/175341]\n",
      "loss: 0.495481  [83200/175341]\n",
      "loss: 0.565206  [84800/175341]\n",
      "loss: 0.724218  [86400/175341]\n",
      "loss: 0.232167  [88000/175341]\n",
      "loss: 0.143767  [89600/175341]\n",
      "loss: 0.355130  [91200/175341]\n",
      "loss: 0.837836  [92800/175341]\n",
      "loss: 0.579203  [94400/175341]\n",
      "loss: 0.396295  [96000/175341]\n",
      "loss: 0.482540  [97600/175341]\n",
      "loss: 0.462333  [99200/175341]\n",
      "loss: 0.366823  [100800/175341]\n",
      "loss: 0.378200  [102400/175341]\n",
      "loss: 0.284660  [104000/175341]\n",
      "loss: 0.292479  [105600/175341]\n",
      "loss: 0.240331  [107200/175341]\n",
      "loss: 0.550372  [108800/175341]\n",
      "loss: 0.567089  [110400/175341]\n",
      "loss: 0.872225  [112000/175341]\n",
      "loss: 0.201677  [113600/175341]\n",
      "loss: 0.618954  [115200/175341]\n",
      "loss: 0.606369  [116800/175341]\n",
      "loss: 0.264306  [118400/175341]\n",
      "loss: 0.545455  [120000/175341]\n",
      "loss: 1.038482  [121600/175341]\n",
      "loss: 0.495219  [123200/175341]\n",
      "loss: 0.526019  [124800/175341]\n",
      "loss: 0.420045  [126400/175341]\n",
      "loss: 0.698939  [128000/175341]\n",
      "loss: 1.121850  [129600/175341]\n",
      "loss: 0.262885  [131200/175341]\n",
      "loss: 0.336393  [132800/175341]\n",
      "loss: 0.315832  [134400/175341]\n",
      "loss: 0.319747  [136000/175341]\n",
      "loss: 0.385247  [137600/175341]\n",
      "loss: 0.312437  [139200/175341]\n",
      "loss: 0.371112  [140800/175341]\n",
      "loss: 0.369617  [142400/175341]\n",
      "loss: 0.408305  [144000/175341]\n",
      "loss: 0.280510  [145600/175341]\n",
      "loss: 0.376263  [147200/175341]\n",
      "loss: 0.626381  [148800/175341]\n",
      "loss: 0.400284  [150400/175341]\n",
      "loss: 0.402579  [152000/175341]\n",
      "loss: 0.374091  [153600/175341]\n",
      "loss: 0.352763  [155200/175341]\n",
      "loss: 0.294616  [156800/175341]\n",
      "loss: 0.363424  [158400/175341]\n",
      "loss: 0.475664  [160000/175341]\n",
      "loss: 0.461482  [161600/175341]\n",
      "loss: 0.304144  [163200/175341]\n",
      "loss: 0.402261  [164800/175341]\n",
      "loss: 0.530038  [166400/175341]\n",
      "loss: 0.452260  [168000/175341]\n",
      "loss: 0.431181  [169600/175341]\n",
      "loss: 0.692456  [171200/175341]\n",
      "loss: 0.253797  [172800/175341]\n",
      "loss: 0.407061  [174400/175341]\n",
      "Train Accuracy: 80.6970%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.555558, F1-score: 75.24%, Macro_F1-Score:  41.70%  \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.440552  [    0/175341]\n",
      "loss: 0.409126  [ 1600/175341]\n",
      "loss: 0.466579  [ 3200/175341]\n",
      "loss: 0.220595  [ 4800/175341]\n",
      "loss: 0.709936  [ 6400/175341]\n",
      "loss: 0.503405  [ 8000/175341]\n",
      "loss: 0.590695  [ 9600/175341]\n",
      "loss: 0.727111  [11200/175341]\n",
      "loss: 0.341911  [12800/175341]\n",
      "loss: 0.502677  [14400/175341]\n",
      "loss: 0.780671  [16000/175341]\n",
      "loss: 0.449843  [17600/175341]\n",
      "loss: 0.634503  [19200/175341]\n",
      "loss: 0.265213  [20800/175341]\n",
      "loss: 0.814610  [22400/175341]\n",
      "loss: 0.243402  [24000/175341]\n",
      "loss: 0.061054  [25600/175341]\n",
      "loss: 0.821800  [27200/175341]\n",
      "loss: 0.508448  [28800/175341]\n",
      "loss: 0.688318  [30400/175341]\n",
      "loss: 0.995599  [32000/175341]\n",
      "loss: 1.028769  [33600/175341]\n",
      "loss: 0.397197  [35200/175341]\n",
      "loss: 0.521726  [36800/175341]\n",
      "loss: 0.440704  [38400/175341]\n",
      "loss: 0.536874  [40000/175341]\n",
      "loss: 0.555368  [41600/175341]\n",
      "loss: 0.632933  [43200/175341]\n",
      "loss: 0.754167  [44800/175341]\n",
      "loss: 0.439283  [46400/175341]\n",
      "loss: 0.584817  [48000/175341]\n",
      "loss: 0.482007  [49600/175341]\n",
      "loss: 0.476953  [51200/175341]\n",
      "loss: 0.216543  [52800/175341]\n",
      "loss: 0.494946  [54400/175341]\n",
      "loss: 0.593048  [56000/175341]\n",
      "loss: 0.545297  [57600/175341]\n",
      "loss: 0.387985  [59200/175341]\n",
      "loss: 0.200277  [60800/175341]\n",
      "loss: 0.237959  [62400/175341]\n",
      "loss: 0.667775  [64000/175341]\n",
      "loss: 0.655084  [65600/175341]\n",
      "loss: 0.678038  [67200/175341]\n",
      "loss: 0.590757  [68800/175341]\n",
      "loss: 0.400682  [70400/175341]\n",
      "loss: 0.416819  [72000/175341]\n",
      "loss: 0.553536  [73600/175341]\n",
      "loss: 0.306531  [75200/175341]\n",
      "loss: 0.599379  [76800/175341]\n",
      "loss: 0.408273  [78400/175341]\n",
      "loss: 0.740968  [80000/175341]\n",
      "loss: 0.533314  [81600/175341]\n",
      "loss: 0.264593  [83200/175341]\n",
      "loss: 0.717615  [84800/175341]\n",
      "loss: 0.376267  [86400/175341]\n",
      "loss: 0.165641  [88000/175341]\n",
      "loss: 0.436251  [89600/175341]\n",
      "loss: 0.387885  [91200/175341]\n",
      "loss: 0.583923  [92800/175341]\n",
      "loss: 0.417973  [94400/175341]\n",
      "loss: 0.112909  [96000/175341]\n",
      "loss: 0.316872  [97600/175341]\n",
      "loss: 0.545982  [99200/175341]\n",
      "loss: 0.305964  [100800/175341]\n",
      "loss: 0.573504  [102400/175341]\n",
      "loss: 0.463475  [104000/175341]\n",
      "loss: 0.144971  [105600/175341]\n",
      "loss: 0.264398  [107200/175341]\n",
      "loss: 0.344175  [108800/175341]\n",
      "loss: 0.702671  [110400/175341]\n",
      "loss: 0.161266  [112000/175341]\n",
      "loss: 0.751201  [113600/175341]\n",
      "loss: 0.424690  [115200/175341]\n",
      "loss: 0.695518  [116800/175341]\n",
      "loss: 0.604308  [118400/175341]\n",
      "loss: 0.279943  [120000/175341]\n",
      "loss: 0.401240  [121600/175341]\n",
      "loss: 0.359270  [123200/175341]\n",
      "loss: 0.592211  [124800/175341]\n",
      "loss: 0.472944  [126400/175341]\n",
      "loss: 0.378135  [128000/175341]\n",
      "loss: 0.909195  [129600/175341]\n",
      "loss: 1.240426  [131200/175341]\n",
      "loss: 0.382273  [132800/175341]\n",
      "loss: 1.892073  [134400/175341]\n",
      "loss: 0.318707  [136000/175341]\n",
      "loss: 0.430295  [137600/175341]\n",
      "loss: 0.507789  [139200/175341]\n",
      "loss: 0.550279  [140800/175341]\n",
      "loss: 0.494503  [142400/175341]\n",
      "loss: 0.957966  [144000/175341]\n",
      "loss: 0.649703  [145600/175341]\n",
      "loss: 0.384484  [147200/175341]\n",
      "loss: 0.261017  [148800/175341]\n",
      "loss: 0.884009  [150400/175341]\n",
      "loss: 0.658887  [152000/175341]\n",
      "loss: 0.464289  [153600/175341]\n",
      "loss: 0.737185  [155200/175341]\n",
      "loss: 0.611742  [156800/175341]\n",
      "loss: 0.200986  [158400/175341]\n",
      "loss: 0.330863  [160000/175341]\n",
      "loss: 0.296312  [161600/175341]\n",
      "loss: 0.568940  [163200/175341]\n",
      "loss: 0.292133  [164800/175341]\n",
      "loss: 0.185172  [166400/175341]\n",
      "loss: 0.296921  [168000/175341]\n",
      "loss: 0.597683  [169600/175341]\n",
      "loss: 0.222033  [171200/175341]\n",
      "loss: 0.469388  [172800/175341]\n",
      "loss: 0.697573  [174400/175341]\n",
      "Train Accuracy: 80.7638%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.562177, F1-score: 74.84%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.090964  [    0/175341]\n",
      "loss: 0.510384  [ 1600/175341]\n",
      "loss: 0.414602  [ 3200/175341]\n",
      "loss: 0.347779  [ 4800/175341]\n",
      "loss: 0.510853  [ 6400/175341]\n",
      "loss: 0.639927  [ 8000/175341]\n",
      "loss: 0.439766  [ 9600/175341]\n",
      "loss: 0.427754  [11200/175341]\n",
      "loss: 0.336612  [12800/175341]\n",
      "loss: 0.521990  [14400/175341]\n",
      "loss: 0.105709  [16000/175341]\n",
      "loss: 0.234854  [17600/175341]\n",
      "loss: 0.657604  [19200/175341]\n",
      "loss: 0.750030  [20800/175341]\n",
      "loss: 0.202988  [22400/175341]\n",
      "loss: 0.761152  [24000/175341]\n",
      "loss: 0.456826  [25600/175341]\n",
      "loss: 0.272028  [27200/175341]\n",
      "loss: 0.889824  [28800/175341]\n",
      "loss: 0.489385  [30400/175341]\n",
      "loss: 0.639767  [32000/175341]\n",
      "loss: 0.550054  [33600/175341]\n",
      "loss: 0.492734  [35200/175341]\n",
      "loss: 0.479603  [36800/175341]\n",
      "loss: 1.064651  [38400/175341]\n",
      "loss: 0.217992  [40000/175341]\n",
      "loss: 0.370097  [41600/175341]\n",
      "loss: 0.563321  [43200/175341]\n",
      "loss: 0.474106  [44800/175341]\n",
      "loss: 0.298598  [46400/175341]\n",
      "loss: 0.645539  [48000/175341]\n",
      "loss: 0.623627  [49600/175341]\n",
      "loss: 0.432741  [51200/175341]\n",
      "loss: 0.623365  [52800/175341]\n",
      "loss: 0.492537  [54400/175341]\n",
      "loss: 0.530831  [56000/175341]\n",
      "loss: 0.474470  [57600/175341]\n",
      "loss: 0.157765  [59200/175341]\n",
      "loss: 0.468072  [60800/175341]\n",
      "loss: 0.403367  [62400/175341]\n",
      "loss: 0.293513  [64000/175341]\n",
      "loss: 0.038551  [65600/175341]\n",
      "loss: 0.733756  [67200/175341]\n",
      "loss: 0.676214  [68800/175341]\n",
      "loss: 0.646652  [70400/175341]\n",
      "loss: 0.308511  [72000/175341]\n",
      "loss: 0.304059  [73600/175341]\n",
      "loss: 0.620697  [75200/175341]\n",
      "loss: 0.570509  [76800/175341]\n",
      "loss: 0.258206  [78400/175341]\n",
      "loss: 0.379198  [80000/175341]\n",
      "loss: 0.425945  [81600/175341]\n",
      "loss: 0.430269  [83200/175341]\n",
      "loss: 0.232952  [84800/175341]\n",
      "loss: 0.174935  [86400/175341]\n",
      "loss: 0.993664  [88000/175341]\n",
      "loss: 0.591182  [89600/175341]\n",
      "loss: 0.223647  [91200/175341]\n",
      "loss: 0.657814  [92800/175341]\n",
      "loss: 0.221961  [94400/175341]\n",
      "loss: 0.440212  [96000/175341]\n",
      "loss: 0.624542  [97600/175341]\n",
      "loss: 0.708616  [99200/175341]\n",
      "loss: 0.810575  [100800/175341]\n",
      "loss: 0.357478  [102400/175341]\n",
      "loss: 0.366842  [104000/175341]\n",
      "loss: 0.618338  [105600/175341]\n",
      "loss: 0.577331  [107200/175341]\n",
      "loss: 0.399702  [108800/175341]\n",
      "loss: 0.673280  [110400/175341]\n",
      "loss: 0.509102  [112000/175341]\n",
      "loss: 0.446934  [113600/175341]\n",
      "loss: 0.793146  [115200/175341]\n",
      "loss: 0.396397  [116800/175341]\n",
      "loss: 0.320803  [118400/175341]\n",
      "loss: 0.516752  [120000/175341]\n",
      "loss: 0.655103  [121600/175341]\n",
      "loss: 0.293651  [123200/175341]\n",
      "loss: 0.385348  [124800/175341]\n",
      "loss: 0.742202  [126400/175341]\n",
      "loss: 0.246325  [128000/175341]\n",
      "loss: 0.572683  [129600/175341]\n",
      "loss: 0.316873  [131200/175341]\n",
      "loss: 0.366037  [132800/175341]\n",
      "loss: 0.437957  [134400/175341]\n",
      "loss: 0.337852  [136000/175341]\n",
      "loss: 0.359278  [137600/175341]\n",
      "loss: 0.448004  [139200/175341]\n",
      "loss: 0.690008  [140800/175341]\n",
      "loss: 0.732591  [142400/175341]\n",
      "loss: 0.634462  [144000/175341]\n",
      "loss: 0.681143  [145600/175341]\n",
      "loss: 0.558587  [147200/175341]\n",
      "loss: 0.727591  [148800/175341]\n",
      "loss: 0.279377  [150400/175341]\n",
      "loss: 0.834242  [152000/175341]\n",
      "loss: 0.612674  [153600/175341]\n",
      "loss: 0.592865  [155200/175341]\n",
      "loss: 0.744251  [156800/175341]\n",
      "loss: 0.279323  [158400/175341]\n",
      "loss: 0.624641  [160000/175341]\n",
      "loss: 0.331199  [161600/175341]\n",
      "loss: 0.686350  [163200/175341]\n",
      "loss: 0.808610  [164800/175341]\n",
      "loss: 0.480317  [166400/175341]\n",
      "loss: 0.386732  [168000/175341]\n",
      "loss: 0.337105  [169600/175341]\n",
      "loss: 0.367216  [171200/175341]\n",
      "loss: 0.303946  [172800/175341]\n",
      "loss: 0.455453  [174400/175341]\n",
      "Train Accuracy: 80.7472%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.552203, F1-score: 75.03%, Macro_F1-Score:  41.14%  \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.522938  [    0/175341]\n",
      "loss: 0.640611  [ 1600/175341]\n",
      "loss: 0.764063  [ 3200/175341]\n",
      "loss: 0.267233  [ 4800/175341]\n",
      "loss: 0.173311  [ 6400/175341]\n",
      "loss: 0.353100  [ 8000/175341]\n",
      "loss: 0.818659  [ 9600/175341]\n",
      "loss: 0.381355  [11200/175341]\n",
      "loss: 0.276810  [12800/175341]\n",
      "loss: 0.354678  [14400/175341]\n",
      "loss: 0.486215  [16000/175341]\n",
      "loss: 0.270964  [17600/175341]\n",
      "loss: 0.671201  [19200/175341]\n",
      "loss: 0.499369  [20800/175341]\n",
      "loss: 0.731913  [22400/175341]\n",
      "loss: 0.387295  [24000/175341]\n",
      "loss: 0.482317  [25600/175341]\n",
      "loss: 0.383922  [27200/175341]\n",
      "loss: 0.835911  [28800/175341]\n",
      "loss: 0.328940  [30400/175341]\n",
      "loss: 0.558351  [32000/175341]\n",
      "loss: 0.180463  [33600/175341]\n",
      "loss: 0.699163  [35200/175341]\n",
      "loss: 0.363994  [36800/175341]\n",
      "loss: 0.589269  [38400/175341]\n",
      "loss: 0.437107  [40000/175341]\n",
      "loss: 0.515781  [41600/175341]\n",
      "loss: 0.471223  [43200/175341]\n",
      "loss: 0.250944  [44800/175341]\n",
      "loss: 0.529696  [46400/175341]\n",
      "loss: 0.453796  [48000/175341]\n",
      "loss: 0.408734  [49600/175341]\n",
      "loss: 0.148889  [51200/175341]\n",
      "loss: 0.320620  [52800/175341]\n",
      "loss: 0.340075  [54400/175341]\n",
      "loss: 0.952606  [56000/175341]\n",
      "loss: 0.568329  [57600/175341]\n",
      "loss: 0.409534  [59200/175341]\n",
      "loss: 0.513034  [60800/175341]\n",
      "loss: 0.327043  [62400/175341]\n",
      "loss: 0.396219  [64000/175341]\n",
      "loss: 0.323110  [65600/175341]\n",
      "loss: 0.357625  [67200/175341]\n",
      "loss: 0.193834  [68800/175341]\n",
      "loss: 0.523704  [70400/175341]\n",
      "loss: 0.275786  [72000/175341]\n",
      "loss: 0.408303  [73600/175341]\n",
      "loss: 0.929018  [75200/175341]\n",
      "loss: 0.331418  [76800/175341]\n",
      "loss: 0.259080  [78400/175341]\n",
      "loss: 0.343162  [80000/175341]\n",
      "loss: 0.615108  [81600/175341]\n",
      "loss: 0.443768  [83200/175341]\n",
      "loss: 0.212723  [84800/175341]\n",
      "loss: 0.536212  [86400/175341]\n",
      "loss: 0.846505  [88000/175341]\n",
      "loss: 0.520871  [89600/175341]\n",
      "loss: 0.709985  [91200/175341]\n",
      "loss: 0.651755  [92800/175341]\n",
      "loss: 0.191669  [94400/175341]\n",
      "loss: 0.468143  [96000/175341]\n",
      "loss: 0.532229  [97600/175341]\n",
      "loss: 0.703325  [99200/175341]\n",
      "loss: 0.393208  [100800/175341]\n",
      "loss: 0.604046  [102400/175341]\n",
      "loss: 0.370365  [104000/175341]\n",
      "loss: 0.442342  [105600/175341]\n",
      "loss: 0.357634  [107200/175341]\n",
      "loss: 0.294253  [108800/175341]\n",
      "loss: 0.470192  [110400/175341]\n",
      "loss: 0.933707  [112000/175341]\n",
      "loss: 0.189440  [113600/175341]\n",
      "loss: 0.547511  [115200/175341]\n",
      "loss: 0.458017  [116800/175341]\n",
      "loss: 0.483255  [118400/175341]\n",
      "loss: 0.669923  [120000/175341]\n",
      "loss: 0.660863  [121600/175341]\n",
      "loss: 0.401691  [123200/175341]\n",
      "loss: 0.323104  [124800/175341]\n",
      "loss: 0.874125  [126400/175341]\n",
      "loss: 0.678978  [128000/175341]\n",
      "loss: 1.180792  [129600/175341]\n",
      "loss: 0.527728  [131200/175341]\n",
      "loss: 0.655074  [132800/175341]\n",
      "loss: 0.419220  [134400/175341]\n",
      "loss: 0.820257  [136000/175341]\n",
      "loss: 0.278322  [137600/175341]\n",
      "loss: 0.344947  [139200/175341]\n",
      "loss: 0.595297  [140800/175341]\n",
      "loss: 0.473172  [142400/175341]\n",
      "loss: 0.462223  [144000/175341]\n",
      "loss: 0.376067  [145600/175341]\n",
      "loss: 0.603392  [147200/175341]\n",
      "loss: 0.453235  [148800/175341]\n",
      "loss: 0.268010  [150400/175341]\n",
      "loss: 0.207622  [152000/175341]\n",
      "loss: 0.869654  [153600/175341]\n",
      "loss: 0.531005  [155200/175341]\n",
      "loss: 0.357410  [156800/175341]\n",
      "loss: 0.643261  [158400/175341]\n",
      "loss: 0.134406  [160000/175341]\n",
      "loss: 0.451318  [161600/175341]\n",
      "loss: 0.434203  [163200/175341]\n",
      "loss: 0.556314  [164800/175341]\n",
      "loss: 0.470455  [166400/175341]\n",
      "loss: 0.615060  [168000/175341]\n",
      "loss: 0.734703  [169600/175341]\n",
      "loss: 0.269581  [171200/175341]\n",
      "loss: 0.198723  [172800/175341]\n",
      "loss: 0.486830  [174400/175341]\n",
      "Train Accuracy: 80.7700%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.574857, F1-score: 74.54%, Macro_F1-Score:  40.66%  \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.256508  [    0/175341]\n",
      "loss: 0.880988  [ 1600/175341]\n",
      "loss: 0.400163  [ 3200/175341]\n",
      "loss: 0.840549  [ 4800/175341]\n",
      "loss: 0.498657  [ 6400/175341]\n",
      "loss: 0.762844  [ 8000/175341]\n",
      "loss: 0.533441  [ 9600/175341]\n",
      "loss: 0.347647  [11200/175341]\n",
      "loss: 0.362880  [12800/175341]\n",
      "loss: 0.302500  [14400/175341]\n",
      "loss: 0.379056  [16000/175341]\n",
      "loss: 0.374843  [17600/175341]\n",
      "loss: 0.903593  [19200/175341]\n",
      "loss: 0.283842  [20800/175341]\n",
      "loss: 0.853417  [22400/175341]\n",
      "loss: 0.891281  [24000/175341]\n",
      "loss: 0.544688  [25600/175341]\n",
      "loss: 0.772044  [27200/175341]\n",
      "loss: 0.496985  [28800/175341]\n",
      "loss: 0.381566  [30400/175341]\n",
      "loss: 0.374436  [32000/175341]\n",
      "loss: 0.860126  [33600/175341]\n",
      "loss: 0.322495  [35200/175341]\n",
      "loss: 0.558843  [36800/175341]\n",
      "loss: 0.816438  [38400/175341]\n",
      "loss: 0.411375  [40000/175341]\n",
      "loss: 0.111719  [41600/175341]\n",
      "loss: 0.585435  [43200/175341]\n",
      "loss: 0.651954  [44800/175341]\n",
      "loss: 0.413212  [46400/175341]\n",
      "loss: 0.519003  [48000/175341]\n",
      "loss: 0.738135  [49600/175341]\n",
      "loss: 0.234034  [51200/175341]\n",
      "loss: 0.205841  [52800/175341]\n",
      "loss: 0.956389  [54400/175341]\n",
      "loss: 0.506742  [56000/175341]\n",
      "loss: 0.584486  [57600/175341]\n",
      "loss: 0.335695  [59200/175341]\n",
      "loss: 0.534569  [60800/175341]\n",
      "loss: 0.256385  [62400/175341]\n",
      "loss: 0.300363  [64000/175341]\n",
      "loss: 0.419765  [65600/175341]\n",
      "loss: 0.456768  [67200/175341]\n",
      "loss: 0.521278  [68800/175341]\n",
      "loss: 0.233497  [70400/175341]\n",
      "loss: 0.385596  [72000/175341]\n",
      "loss: 0.802539  [73600/175341]\n",
      "loss: 0.656998  [75200/175341]\n",
      "loss: 0.259373  [76800/175341]\n",
      "loss: 0.334921  [78400/175341]\n",
      "loss: 0.716769  [80000/175341]\n",
      "loss: 0.214476  [81600/175341]\n",
      "loss: 0.106381  [83200/175341]\n",
      "loss: 0.747536  [84800/175341]\n",
      "loss: 0.206396  [86400/175341]\n",
      "loss: 0.405655  [88000/175341]\n",
      "loss: 0.404764  [89600/175341]\n",
      "loss: 0.354013  [91200/175341]\n",
      "loss: 0.272046  [92800/175341]\n",
      "loss: 0.637068  [94400/175341]\n",
      "loss: 0.793587  [96000/175341]\n",
      "loss: 0.296046  [97600/175341]\n",
      "loss: 0.309108  [99200/175341]\n",
      "loss: 0.400862  [100800/175341]\n",
      "loss: 0.253177  [102400/175341]\n",
      "loss: 0.339785  [104000/175341]\n",
      "loss: 0.456546  [105600/175341]\n",
      "loss: 0.484625  [107200/175341]\n",
      "loss: 0.591442  [108800/175341]\n",
      "loss: 0.669912  [110400/175341]\n",
      "loss: 0.342701  [112000/175341]\n",
      "loss: 0.418030  [113600/175341]\n",
      "loss: 0.258078  [115200/175341]\n",
      "loss: 0.758724  [116800/175341]\n",
      "loss: 0.426951  [118400/175341]\n",
      "loss: 0.716614  [120000/175341]\n",
      "loss: 0.696082  [121600/175341]\n",
      "loss: 0.445822  [123200/175341]\n",
      "loss: 0.301403  [124800/175341]\n",
      "loss: 1.057081  [126400/175341]\n",
      "loss: 0.402195  [128000/175341]\n",
      "loss: 0.214435  [129600/175341]\n",
      "loss: 0.499155  [131200/175341]\n",
      "loss: 0.442833  [132800/175341]\n",
      "loss: 0.417801  [134400/175341]\n",
      "loss: 0.448904  [136000/175341]\n",
      "loss: 0.583063  [137600/175341]\n",
      "loss: 0.672240  [139200/175341]\n",
      "loss: 0.483653  [140800/175341]\n",
      "loss: 0.770124  [142400/175341]\n",
      "loss: 0.443659  [144000/175341]\n",
      "loss: 0.453875  [145600/175341]\n",
      "loss: 0.505881  [147200/175341]\n",
      "loss: 0.898420  [148800/175341]\n",
      "loss: 0.248520  [150400/175341]\n",
      "loss: 0.643863  [152000/175341]\n",
      "loss: 0.568561  [153600/175341]\n",
      "loss: 0.691518  [155200/175341]\n",
      "loss: 0.715849  [156800/175341]\n",
      "loss: 0.549704  [158400/175341]\n",
      "loss: 0.575506  [160000/175341]\n",
      "loss: 0.427627  [161600/175341]\n",
      "loss: 0.717987  [163200/175341]\n",
      "loss: 0.351508  [164800/175341]\n",
      "loss: 0.536430  [166400/175341]\n",
      "loss: 0.418307  [168000/175341]\n",
      "loss: 0.607924  [169600/175341]\n",
      "loss: 0.462464  [171200/175341]\n",
      "loss: 0.491713  [172800/175341]\n",
      "loss: 0.415980  [174400/175341]\n",
      "Train Accuracy: 80.7467%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.551733, F1-score: 76.03%, Macro_F1-Score:  42.06%  \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.508579  [    0/175341]\n",
      "loss: 0.431024  [ 1600/175341]\n",
      "loss: 0.498053  [ 3200/175341]\n",
      "loss: 0.255568  [ 4800/175341]\n",
      "loss: 0.327586  [ 6400/175341]\n",
      "loss: 0.178204  [ 8000/175341]\n",
      "loss: 0.429427  [ 9600/175341]\n",
      "loss: 0.438855  [11200/175341]\n",
      "loss: 0.330631  [12800/175341]\n",
      "loss: 0.750207  [14400/175341]\n",
      "loss: 0.428647  [16000/175341]\n",
      "loss: 0.605728  [17600/175341]\n",
      "loss: 0.695772  [19200/175341]\n",
      "loss: 0.847518  [20800/175341]\n",
      "loss: 0.554523  [22400/175341]\n",
      "loss: 0.343237  [24000/175341]\n",
      "loss: 0.364187  [25600/175341]\n",
      "loss: 0.368186  [27200/175341]\n",
      "loss: 0.284122  [28800/175341]\n",
      "loss: 0.236487  [30400/175341]\n",
      "loss: 0.212619  [32000/175341]\n",
      "loss: 0.669464  [33600/175341]\n",
      "loss: 0.107368  [35200/175341]\n",
      "loss: 0.444219  [36800/175341]\n",
      "loss: 0.591282  [38400/175341]\n",
      "loss: 0.749462  [40000/175341]\n",
      "loss: 0.482165  [41600/175341]\n",
      "loss: 0.271725  [43200/175341]\n",
      "loss: 0.638347  [44800/175341]\n",
      "loss: 0.763317  [46400/175341]\n",
      "loss: 0.467611  [48000/175341]\n",
      "loss: 0.834195  [49600/175341]\n",
      "loss: 0.275207  [51200/175341]\n",
      "loss: 0.177705  [52800/175341]\n",
      "loss: 0.607861  [54400/175341]\n",
      "loss: 1.152174  [56000/175341]\n",
      "loss: 0.428445  [57600/175341]\n",
      "loss: 0.295863  [59200/175341]\n",
      "loss: 0.288985  [60800/175341]\n",
      "loss: 0.536598  [62400/175341]\n",
      "loss: 0.234371  [64000/175341]\n",
      "loss: 0.304284  [65600/175341]\n",
      "loss: 0.341225  [67200/175341]\n",
      "loss: 0.259239  [68800/175341]\n",
      "loss: 0.425134  [70400/175341]\n",
      "loss: 0.505754  [72000/175341]\n",
      "loss: 0.512213  [73600/175341]\n",
      "loss: 0.812079  [75200/175341]\n",
      "loss: 0.512883  [76800/175341]\n",
      "loss: 0.414483  [78400/175341]\n",
      "loss: 0.606888  [80000/175341]\n",
      "loss: 0.578486  [81600/175341]\n",
      "loss: 0.250171  [83200/175341]\n",
      "loss: 0.636504  [84800/175341]\n",
      "loss: 0.319603  [86400/175341]\n",
      "loss: 0.615474  [88000/175341]\n",
      "loss: 0.820323  [89600/175341]\n",
      "loss: 0.287316  [91200/175341]\n",
      "loss: 0.367710  [92800/175341]\n",
      "loss: 0.377015  [94400/175341]\n",
      "loss: 0.251713  [96000/175341]\n",
      "loss: 0.589205  [97600/175341]\n",
      "loss: 0.544973  [99200/175341]\n",
      "loss: 0.324203  [100800/175341]\n",
      "loss: 0.515254  [102400/175341]\n",
      "loss: 0.371953  [104000/175341]\n",
      "loss: 0.489480  [105600/175341]\n",
      "loss: 0.177874  [107200/175341]\n",
      "loss: 0.203440  [108800/175341]\n",
      "loss: 0.185022  [110400/175341]\n",
      "loss: 0.671135  [112000/175341]\n",
      "loss: 0.295849  [113600/175341]\n",
      "loss: 0.581579  [115200/175341]\n",
      "loss: 0.581295  [116800/175341]\n",
      "loss: 0.547675  [118400/175341]\n",
      "loss: 0.593145  [120000/175341]\n",
      "loss: 0.316508  [121600/175341]\n",
      "loss: 0.694053  [123200/175341]\n",
      "loss: 0.162572  [124800/175341]\n",
      "loss: 0.388507  [126400/175341]\n",
      "loss: 0.911606  [128000/175341]\n",
      "loss: 0.082586  [129600/175341]\n",
      "loss: 0.719355  [131200/175341]\n",
      "loss: 0.413387  [132800/175341]\n",
      "loss: 0.224364  [134400/175341]\n",
      "loss: 0.208208  [136000/175341]\n",
      "loss: 0.461364  [137600/175341]\n",
      "loss: 0.126728  [139200/175341]\n",
      "loss: 0.645641  [140800/175341]\n",
      "loss: 0.425375  [142400/175341]\n",
      "loss: 0.409257  [144000/175341]\n",
      "loss: 0.722746  [145600/175341]\n",
      "loss: 0.507015  [147200/175341]\n",
      "loss: 0.417165  [148800/175341]\n",
      "loss: 0.257144  [150400/175341]\n",
      "loss: 0.407830  [152000/175341]\n",
      "loss: 0.519086  [153600/175341]\n",
      "loss: 0.182029  [155200/175341]\n",
      "loss: 0.315495  [156800/175341]\n",
      "loss: 0.613059  [158400/175341]\n",
      "loss: 0.479525  [160000/175341]\n",
      "loss: 0.480564  [161600/175341]\n",
      "loss: 0.774089  [163200/175341]\n",
      "loss: 0.722939  [164800/175341]\n",
      "loss: 0.511277  [166400/175341]\n",
      "loss: 0.678080  [168000/175341]\n",
      "loss: 0.302235  [169600/175341]\n",
      "loss: 0.768587  [171200/175341]\n",
      "loss: 0.303802  [172800/175341]\n",
      "loss: 0.379614  [174400/175341]\n",
      "Train Accuracy: 80.8402%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.540414, F1-score: 76.96%, Macro_F1-Score:  43.40%  \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.475275  [    0/175341]\n",
      "loss: 0.595573  [ 1600/175341]\n",
      "loss: 0.699362  [ 3200/175341]\n",
      "loss: 0.633812  [ 4800/175341]\n",
      "loss: 0.481156  [ 6400/175341]\n",
      "loss: 0.574614  [ 8000/175341]\n",
      "loss: 0.664305  [ 9600/175341]\n",
      "loss: 0.614771  [11200/175341]\n",
      "loss: 0.302991  [12800/175341]\n",
      "loss: 0.403390  [14400/175341]\n",
      "loss: 0.469894  [16000/175341]\n",
      "loss: 0.537032  [17600/175341]\n",
      "loss: 0.535441  [19200/175341]\n",
      "loss: 0.190366  [20800/175341]\n",
      "loss: 0.343999  [22400/175341]\n",
      "loss: 0.730773  [24000/175341]\n",
      "loss: 0.247579  [25600/175341]\n",
      "loss: 0.377959  [27200/175341]\n",
      "loss: 0.208328  [28800/175341]\n",
      "loss: 0.528965  [30400/175341]\n",
      "loss: 0.329452  [32000/175341]\n",
      "loss: 0.429439  [33600/175341]\n",
      "loss: 0.218111  [35200/175341]\n",
      "loss: 0.683829  [36800/175341]\n",
      "loss: 0.285081  [38400/175341]\n",
      "loss: 0.536905  [40000/175341]\n",
      "loss: 0.612599  [41600/175341]\n",
      "loss: 0.234544  [43200/175341]\n",
      "loss: 0.281629  [44800/175341]\n",
      "loss: 0.347369  [46400/175341]\n",
      "loss: 0.506856  [48000/175341]\n",
      "loss: 0.665705  [49600/175341]\n",
      "loss: 0.748279  [51200/175341]\n",
      "loss: 0.171946  [52800/175341]\n",
      "loss: 0.418441  [54400/175341]\n",
      "loss: 0.429028  [56000/175341]\n",
      "loss: 0.384184  [57600/175341]\n",
      "loss: 0.516888  [59200/175341]\n",
      "loss: 0.769511  [60800/175341]\n",
      "loss: 0.428679  [62400/175341]\n",
      "loss: 0.857300  [64000/175341]\n",
      "loss: 0.289412  [65600/175341]\n",
      "loss: 0.809374  [67200/175341]\n",
      "loss: 0.424609  [68800/175341]\n",
      "loss: 0.812188  [70400/175341]\n",
      "loss: 0.269995  [72000/175341]\n",
      "loss: 0.272720  [73600/175341]\n",
      "loss: 0.711339  [75200/175341]\n",
      "loss: 0.458093  [76800/175341]\n",
      "loss: 0.554914  [78400/175341]\n",
      "loss: 0.513123  [80000/175341]\n",
      "loss: 0.632022  [81600/175341]\n",
      "loss: 0.814626  [83200/175341]\n",
      "loss: 0.348363  [84800/175341]\n",
      "loss: 0.241608  [86400/175341]\n",
      "loss: 0.460621  [88000/175341]\n",
      "loss: 0.892687  [89600/175341]\n",
      "loss: 0.468488  [91200/175341]\n",
      "loss: 0.276113  [92800/175341]\n",
      "loss: 0.545441  [94400/175341]\n",
      "loss: 0.584139  [96000/175341]\n",
      "loss: 0.452389  [97600/175341]\n",
      "loss: 0.440457  [99200/175341]\n",
      "loss: 0.547712  [100800/175341]\n",
      "loss: 0.578631  [102400/175341]\n",
      "loss: 0.179339  [104000/175341]\n",
      "loss: 1.138118  [105600/175341]\n",
      "loss: 0.817123  [107200/175341]\n",
      "loss: 0.624888  [108800/175341]\n",
      "loss: 0.648873  [110400/175341]\n",
      "loss: 0.285628  [112000/175341]\n",
      "loss: 0.373085  [113600/175341]\n",
      "loss: 0.511674  [115200/175341]\n",
      "loss: 0.520270  [116800/175341]\n",
      "loss: 0.621307  [118400/175341]\n",
      "loss: 0.277699  [120000/175341]\n",
      "loss: 0.621290  [121600/175341]\n",
      "loss: 0.671405  [123200/175341]\n",
      "loss: 0.364474  [124800/175341]\n",
      "loss: 0.696573  [126400/175341]\n",
      "loss: 0.662932  [128000/175341]\n",
      "loss: 0.314726  [129600/175341]\n",
      "loss: 0.793598  [131200/175341]\n",
      "loss: 0.604081  [132800/175341]\n",
      "loss: 0.510524  [134400/175341]\n",
      "loss: 0.250587  [136000/175341]\n",
      "loss: 0.119070  [137600/175341]\n",
      "loss: 0.670538  [139200/175341]\n",
      "loss: 0.538100  [140800/175341]\n",
      "loss: 0.320310  [142400/175341]\n",
      "loss: 0.466952  [144000/175341]\n",
      "loss: 0.244277  [145600/175341]\n",
      "loss: 0.537208  [147200/175341]\n",
      "loss: 0.133400  [148800/175341]\n",
      "loss: 0.952459  [150400/175341]\n",
      "loss: 0.507775  [152000/175341]\n",
      "loss: 0.466580  [153600/175341]\n",
      "loss: 0.450589  [155200/175341]\n",
      "loss: 0.482445  [156800/175341]\n",
      "loss: 0.872724  [158400/175341]\n",
      "loss: 0.272002  [160000/175341]\n",
      "loss: 0.261838  [161600/175341]\n",
      "loss: 0.400387  [163200/175341]\n",
      "loss: 0.923899  [164800/175341]\n",
      "loss: 0.464372  [166400/175341]\n",
      "loss: 0.208202  [168000/175341]\n",
      "loss: 0.627101  [169600/175341]\n",
      "loss: 0.279127  [171200/175341]\n",
      "loss: 0.338450  [172800/175341]\n",
      "loss: 0.505462  [174400/175341]\n",
      "Train Accuracy: 80.8043%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.545152, F1-score: 75.35%, Macro_F1-Score:  41.05%  \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.277248  [    0/175341]\n",
      "loss: 0.218097  [ 1600/175341]\n",
      "loss: 0.450698  [ 3200/175341]\n",
      "loss: 0.370729  [ 4800/175341]\n",
      "loss: 0.508779  [ 6400/175341]\n",
      "loss: 0.668431  [ 8000/175341]\n",
      "loss: 0.506081  [ 9600/175341]\n",
      "loss: 0.386728  [11200/175341]\n",
      "loss: 0.215438  [12800/175341]\n",
      "loss: 0.701850  [14400/175341]\n",
      "loss: 0.667319  [16000/175341]\n",
      "loss: 0.928151  [17600/175341]\n",
      "loss: 0.701254  [19200/175341]\n",
      "loss: 1.118099  [20800/175341]\n",
      "loss: 0.375102  [22400/175341]\n",
      "loss: 0.624188  [24000/175341]\n",
      "loss: 0.373001  [25600/175341]\n",
      "loss: 0.377520  [27200/175341]\n",
      "loss: 0.407384  [28800/175341]\n",
      "loss: 0.400158  [30400/175341]\n",
      "loss: 0.293246  [32000/175341]\n",
      "loss: 0.612332  [33600/175341]\n",
      "loss: 0.901108  [35200/175341]\n",
      "loss: 0.498652  [36800/175341]\n",
      "loss: 0.476224  [38400/175341]\n",
      "loss: 0.547860  [40000/175341]\n",
      "loss: 0.198419  [41600/175341]\n",
      "loss: 0.548570  [43200/175341]\n",
      "loss: 0.366816  [44800/175341]\n",
      "loss: 0.551869  [46400/175341]\n",
      "loss: 0.343850  [48000/175341]\n",
      "loss: 0.498266  [49600/175341]\n",
      "loss: 0.476068  [51200/175341]\n",
      "loss: 0.685941  [52800/175341]\n",
      "loss: 0.458453  [54400/175341]\n",
      "loss: 0.400875  [56000/175341]\n",
      "loss: 0.503753  [57600/175341]\n",
      "loss: 0.514903  [59200/175341]\n",
      "loss: 0.615123  [60800/175341]\n",
      "loss: 0.459195  [62400/175341]\n",
      "loss: 0.345400  [64000/175341]\n",
      "loss: 0.705760  [65600/175341]\n",
      "loss: 0.599811  [67200/175341]\n",
      "loss: 0.487064  [68800/175341]\n",
      "loss: 0.487917  [70400/175341]\n",
      "loss: 0.311653  [72000/175341]\n",
      "loss: 0.531327  [73600/175341]\n",
      "loss: 0.345136  [75200/175341]\n",
      "loss: 0.391685  [76800/175341]\n",
      "loss: 0.862707  [78400/175341]\n",
      "loss: 0.242573  [80000/175341]\n",
      "loss: 0.951696  [81600/175341]\n",
      "loss: 1.047491  [83200/175341]\n",
      "loss: 0.796721  [84800/175341]\n",
      "loss: 0.555047  [86400/175341]\n",
      "loss: 0.693939  [88000/175341]\n",
      "loss: 0.548624  [89600/175341]\n",
      "loss: 0.426742  [91200/175341]\n",
      "loss: 0.686792  [92800/175341]\n",
      "loss: 0.382066  [94400/175341]\n",
      "loss: 0.300157  [96000/175341]\n",
      "loss: 0.447760  [97600/175341]\n",
      "loss: 0.780773  [99200/175341]\n",
      "loss: 0.916750  [100800/175341]\n",
      "loss: 0.442746  [102400/175341]\n",
      "loss: 0.530183  [104000/175341]\n",
      "loss: 0.601954  [105600/175341]\n",
      "loss: 0.480074  [107200/175341]\n",
      "loss: 0.425307  [108800/175341]\n",
      "loss: 0.458656  [110400/175341]\n",
      "loss: 0.250425  [112000/175341]\n",
      "loss: 0.641233  [113600/175341]\n",
      "loss: 0.863401  [115200/175341]\n",
      "loss: 0.335625  [116800/175341]\n",
      "loss: 0.698272  [118400/175341]\n",
      "loss: 0.860455  [120000/175341]\n",
      "loss: 0.603972  [121600/175341]\n",
      "loss: 0.288314  [123200/175341]\n",
      "loss: 0.630321  [124800/175341]\n",
      "loss: 0.454956  [126400/175341]\n",
      "loss: 0.636156  [128000/175341]\n",
      "loss: 0.368631  [129600/175341]\n",
      "loss: 0.427074  [131200/175341]\n",
      "loss: 0.335583  [132800/175341]\n",
      "loss: 0.422932  [134400/175341]\n",
      "loss: 0.306601  [136000/175341]\n",
      "loss: 0.216607  [137600/175341]\n",
      "loss: 0.971772  [139200/175341]\n",
      "loss: 0.400903  [140800/175341]\n",
      "loss: 0.299710  [142400/175341]\n",
      "loss: 0.741269  [144000/175341]\n",
      "loss: 0.575814  [145600/175341]\n",
      "loss: 0.510777  [147200/175341]\n",
      "loss: 0.225453  [148800/175341]\n",
      "loss: 0.294557  [150400/175341]\n",
      "loss: 0.355189  [152000/175341]\n",
      "loss: 0.594091  [153600/175341]\n",
      "loss: 0.289466  [155200/175341]\n",
      "loss: 0.247591  [156800/175341]\n",
      "loss: 1.080500  [158400/175341]\n",
      "loss: 0.438789  [160000/175341]\n",
      "loss: 0.493987  [161600/175341]\n",
      "loss: 0.456895  [163200/175341]\n",
      "loss: 0.272569  [164800/175341]\n",
      "loss: 0.325086  [166400/175341]\n",
      "loss: 0.368812  [168000/175341]\n",
      "loss: 0.273373  [169600/175341]\n",
      "loss: 0.822696  [171200/175341]\n",
      "loss: 0.724597  [172800/175341]\n",
      "loss: 0.532544  [174400/175341]\n",
      "Train Accuracy: 80.7997%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.550732, F1-score: 75.01%, Macro_F1-Score:  41.15%  \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.264668  [    0/175341]\n",
      "loss: 0.398652  [ 1600/175341]\n",
      "loss: 0.311437  [ 3200/175341]\n",
      "loss: 0.400392  [ 4800/175341]\n",
      "loss: 0.163167  [ 6400/175341]\n",
      "loss: 0.316631  [ 8000/175341]\n",
      "loss: 0.299999  [ 9600/175341]\n",
      "loss: 0.172255  [11200/175341]\n",
      "loss: 0.537148  [12800/175341]\n",
      "loss: 0.346112  [14400/175341]\n",
      "loss: 0.610566  [16000/175341]\n",
      "loss: 0.990061  [17600/175341]\n",
      "loss: 0.345969  [19200/175341]\n",
      "loss: 0.376749  [20800/175341]\n",
      "loss: 0.902727  [22400/175341]\n",
      "loss: 0.313461  [24000/175341]\n",
      "loss: 0.781066  [25600/175341]\n",
      "loss: 0.545697  [27200/175341]\n",
      "loss: 0.359638  [28800/175341]\n",
      "loss: 0.404260  [30400/175341]\n",
      "loss: 0.636214  [32000/175341]\n",
      "loss: 1.207283  [33600/175341]\n",
      "loss: 0.374138  [35200/175341]\n",
      "loss: 0.135655  [36800/175341]\n",
      "loss: 0.238347  [38400/175341]\n",
      "loss: 0.812306  [40000/175341]\n",
      "loss: 0.412665  [41600/175341]\n",
      "loss: 0.376131  [43200/175341]\n",
      "loss: 0.688625  [44800/175341]\n",
      "loss: 0.358841  [46400/175341]\n",
      "loss: 0.753386  [48000/175341]\n",
      "loss: 0.358254  [49600/175341]\n",
      "loss: 0.655263  [51200/175341]\n",
      "loss: 0.291647  [52800/175341]\n",
      "loss: 0.402020  [54400/175341]\n",
      "loss: 0.346948  [56000/175341]\n",
      "loss: 0.189976  [57600/175341]\n",
      "loss: 0.238883  [59200/175341]\n",
      "loss: 0.525126  [60800/175341]\n",
      "loss: 0.338755  [62400/175341]\n",
      "loss: 0.390553  [64000/175341]\n",
      "loss: 0.716611  [65600/175341]\n",
      "loss: 0.761195  [67200/175341]\n",
      "loss: 0.179902  [68800/175341]\n",
      "loss: 0.237350  [70400/175341]\n",
      "loss: 0.646583  [72000/175341]\n",
      "loss: 0.375250  [73600/175341]\n",
      "loss: 0.303143  [75200/175341]\n",
      "loss: 0.722198  [76800/175341]\n",
      "loss: 0.760486  [78400/175341]\n",
      "loss: 0.562738  [80000/175341]\n",
      "loss: 0.387409  [81600/175341]\n",
      "loss: 0.548307  [83200/175341]\n",
      "loss: 0.290135  [84800/175341]\n",
      "loss: 0.468205  [86400/175341]\n",
      "loss: 0.787790  [88000/175341]\n",
      "loss: 0.843556  [89600/175341]\n",
      "loss: 0.370703  [91200/175341]\n",
      "loss: 0.362249  [92800/175341]\n",
      "loss: 0.184261  [94400/175341]\n",
      "loss: 0.431701  [96000/175341]\n",
      "loss: 0.237368  [97600/175341]\n",
      "loss: 0.558868  [99200/175341]\n",
      "loss: 0.712389  [100800/175341]\n",
      "loss: 0.302965  [102400/175341]\n",
      "loss: 0.402792  [104000/175341]\n",
      "loss: 0.327217  [105600/175341]\n",
      "loss: 0.635955  [107200/175341]\n",
      "loss: 0.976471  [108800/175341]\n",
      "loss: 0.350548  [110400/175341]\n",
      "loss: 0.426394  [112000/175341]\n",
      "loss: 0.530579  [113600/175341]\n",
      "loss: 0.285721  [115200/175341]\n",
      "loss: 0.734255  [116800/175341]\n",
      "loss: 0.270895  [118400/175341]\n",
      "loss: 0.841471  [120000/175341]\n",
      "loss: 0.976799  [121600/175341]\n",
      "loss: 0.704379  [123200/175341]\n",
      "loss: 0.741895  [124800/175341]\n",
      "loss: 0.221899  [126400/175341]\n",
      "loss: 1.027677  [128000/175341]\n",
      "loss: 0.221996  [129600/175341]\n",
      "loss: 0.790164  [131200/175341]\n",
      "loss: 0.334023  [132800/175341]\n",
      "loss: 0.387613  [134400/175341]\n",
      "loss: 0.582935  [136000/175341]\n",
      "loss: 0.615392  [137600/175341]\n",
      "loss: 0.392423  [139200/175341]\n",
      "loss: 0.599683  [140800/175341]\n",
      "loss: 0.247488  [142400/175341]\n",
      "loss: 0.200570  [144000/175341]\n",
      "loss: 0.159143  [145600/175341]\n",
      "loss: 0.327225  [147200/175341]\n",
      "loss: 0.806218  [148800/175341]\n",
      "loss: 0.156872  [150400/175341]\n",
      "loss: 0.172384  [152000/175341]\n",
      "loss: 0.260133  [153600/175341]\n",
      "loss: 0.240461  [155200/175341]\n",
      "loss: 0.426106  [156800/175341]\n",
      "loss: 0.410756  [158400/175341]\n",
      "loss: 0.435693  [160000/175341]\n",
      "loss: 0.527903  [161600/175341]\n",
      "loss: 0.447460  [163200/175341]\n",
      "loss: 0.736072  [164800/175341]\n",
      "loss: 0.403400  [166400/175341]\n",
      "loss: 0.771446  [168000/175341]\n",
      "loss: 0.729866  [169600/175341]\n",
      "loss: 0.273710  [171200/175341]\n",
      "loss: 0.393657  [172800/175341]\n",
      "loss: 0.427750  [174400/175341]\n",
      "Train Accuracy: 80.8248%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.564298, F1-score: 74.38%, Macro_F1-Score:  41.02%  \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.683660  [    0/175341]\n",
      "loss: 0.420112  [ 1600/175341]\n",
      "loss: 0.805959  [ 3200/175341]\n",
      "loss: 0.462474  [ 4800/175341]\n",
      "loss: 0.734858  [ 6400/175341]\n",
      "loss: 0.650363  [ 8000/175341]\n",
      "loss: 0.437088  [ 9600/175341]\n",
      "loss: 0.719153  [11200/175341]\n",
      "loss: 0.370007  [12800/175341]\n",
      "loss: 0.575733  [14400/175341]\n",
      "loss: 0.351385  [16000/175341]\n",
      "loss: 0.696063  [17600/175341]\n",
      "loss: 0.322693  [19200/175341]\n",
      "loss: 0.213508  [20800/175341]\n",
      "loss: 0.245431  [22400/175341]\n",
      "loss: 0.643814  [24000/175341]\n",
      "loss: 0.471500  [25600/175341]\n",
      "loss: 0.500015  [27200/175341]\n",
      "loss: 0.366355  [28800/175341]\n",
      "loss: 0.309001  [30400/175341]\n",
      "loss: 0.443408  [32000/175341]\n",
      "loss: 0.422146  [33600/175341]\n",
      "loss: 0.387521  [35200/175341]\n",
      "loss: 0.320357  [36800/175341]\n",
      "loss: 0.269937  [38400/175341]\n",
      "loss: 0.373366  [40000/175341]\n",
      "loss: 0.473549  [41600/175341]\n",
      "loss: 0.421442  [43200/175341]\n",
      "loss: 0.754058  [44800/175341]\n",
      "loss: 0.548379  [46400/175341]\n",
      "loss: 0.552598  [48000/175341]\n",
      "loss: 0.451178  [49600/175341]\n",
      "loss: 1.224154  [51200/175341]\n",
      "loss: 0.509856  [52800/175341]\n",
      "loss: 0.357379  [54400/175341]\n",
      "loss: 0.269042  [56000/175341]\n",
      "loss: 0.411989  [57600/175341]\n",
      "loss: 0.612781  [59200/175341]\n",
      "loss: 0.552679  [60800/175341]\n",
      "loss: 0.390963  [62400/175341]\n",
      "loss: 0.650818  [64000/175341]\n",
      "loss: 0.168820  [65600/175341]\n",
      "loss: 0.343317  [67200/175341]\n",
      "loss: 0.425815  [68800/175341]\n",
      "loss: 0.405045  [70400/175341]\n",
      "loss: 0.691147  [72000/175341]\n",
      "loss: 0.453102  [73600/175341]\n",
      "loss: 0.451358  [75200/175341]\n",
      "loss: 0.121057  [76800/175341]\n",
      "loss: 0.408529  [78400/175341]\n",
      "loss: 0.261519  [80000/175341]\n",
      "loss: 0.459239  [81600/175341]\n",
      "loss: 0.554174  [83200/175341]\n",
      "loss: 0.560565  [84800/175341]\n",
      "loss: 0.537305  [86400/175341]\n",
      "loss: 0.490181  [88000/175341]\n",
      "loss: 0.835352  [89600/175341]\n",
      "loss: 0.387638  [91200/175341]\n",
      "loss: 0.344150  [92800/175341]\n",
      "loss: 0.479968  [94400/175341]\n",
      "loss: 0.475958  [96000/175341]\n",
      "loss: 0.824499  [97600/175341]\n",
      "loss: 0.506581  [99200/175341]\n",
      "loss: 0.291572  [100800/175341]\n",
      "loss: 0.385146  [102400/175341]\n",
      "loss: 0.769364  [104000/175341]\n",
      "loss: 0.392124  [105600/175341]\n",
      "loss: 0.583534  [107200/175341]\n",
      "loss: 0.391267  [108800/175341]\n",
      "loss: 0.429582  [110400/175341]\n",
      "loss: 0.186265  [112000/175341]\n",
      "loss: 0.342369  [113600/175341]\n",
      "loss: 0.164553  [115200/175341]\n",
      "loss: 0.639000  [116800/175341]\n",
      "loss: 0.918646  [118400/175341]\n",
      "loss: 0.291496  [120000/175341]\n",
      "loss: 0.412531  [121600/175341]\n",
      "loss: 0.556275  [123200/175341]\n",
      "loss: 0.316898  [124800/175341]\n",
      "loss: 0.601475  [126400/175341]\n",
      "loss: 0.411828  [128000/175341]\n",
      "loss: 0.269734  [129600/175341]\n",
      "loss: 0.821164  [131200/175341]\n",
      "loss: 0.406641  [132800/175341]\n",
      "loss: 0.143198  [134400/175341]\n",
      "loss: 0.374583  [136000/175341]\n",
      "loss: 0.366000  [137600/175341]\n",
      "loss: 0.810979  [139200/175341]\n",
      "loss: 0.121137  [140800/175341]\n",
      "loss: 0.469866  [142400/175341]\n",
      "loss: 0.381109  [144000/175341]\n",
      "loss: 0.427792  [145600/175341]\n",
      "loss: 0.619443  [147200/175341]\n",
      "loss: 0.497317  [148800/175341]\n",
      "loss: 0.404791  [150400/175341]\n",
      "loss: 0.503116  [152000/175341]\n",
      "loss: 0.578143  [153600/175341]\n",
      "loss: 0.425629  [155200/175341]\n",
      "loss: 0.204377  [156800/175341]\n",
      "loss: 0.275117  [158400/175341]\n",
      "loss: 0.557801  [160000/175341]\n",
      "loss: 0.333245  [161600/175341]\n",
      "loss: 0.712591  [163200/175341]\n",
      "loss: 0.249071  [164800/175341]\n",
      "loss: 0.643533  [166400/175341]\n",
      "loss: 0.372923  [168000/175341]\n",
      "loss: 0.415965  [169600/175341]\n",
      "loss: 0.333904  [171200/175341]\n",
      "loss: 0.132305  [172800/175341]\n",
      "loss: 0.378813  [174400/175341]\n",
      "Train Accuracy: 80.7968%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.558985, F1-score: 74.63%, Macro_F1-Score:  40.88%  \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.331793  [    0/175341]\n",
      "loss: 0.478131  [ 1600/175341]\n",
      "loss: 0.138930  [ 3200/175341]\n",
      "loss: 0.655161  [ 4800/175341]\n",
      "loss: 0.635002  [ 6400/175341]\n",
      "loss: 0.379092  [ 8000/175341]\n",
      "loss: 0.720932  [ 9600/175341]\n",
      "loss: 0.764076  [11200/175341]\n",
      "loss: 0.648513  [12800/175341]\n",
      "loss: 0.175855  [14400/175341]\n",
      "loss: 0.411084  [16000/175341]\n",
      "loss: 0.747471  [17600/175341]\n",
      "loss: 0.590585  [19200/175341]\n",
      "loss: 0.340496  [20800/175341]\n",
      "loss: 0.328999  [22400/175341]\n",
      "loss: 0.907427  [24000/175341]\n",
      "loss: 0.557170  [25600/175341]\n",
      "loss: 0.186830  [27200/175341]\n",
      "loss: 0.130614  [28800/175341]\n",
      "loss: 0.145305  [30400/175341]\n",
      "loss: 0.507930  [32000/175341]\n",
      "loss: 0.408783  [33600/175341]\n",
      "loss: 0.378758  [35200/175341]\n",
      "loss: 0.429970  [36800/175341]\n",
      "loss: 0.560742  [38400/175341]\n",
      "loss: 0.855729  [40000/175341]\n",
      "loss: 0.526043  [41600/175341]\n",
      "loss: 0.115695  [43200/175341]\n",
      "loss: 0.511863  [44800/175341]\n",
      "loss: 0.334226  [46400/175341]\n",
      "loss: 0.297328  [48000/175341]\n",
      "loss: 0.756051  [49600/175341]\n",
      "loss: 0.265036  [51200/175341]\n",
      "loss: 0.230653  [52800/175341]\n",
      "loss: 0.529676  [54400/175341]\n",
      "loss: 0.417463  [56000/175341]\n",
      "loss: 0.436319  [57600/175341]\n",
      "loss: 0.151637  [59200/175341]\n",
      "loss: 0.415472  [60800/175341]\n",
      "loss: 0.592396  [62400/175341]\n",
      "loss: 0.295672  [64000/175341]\n",
      "loss: 0.281126  [65600/175341]\n",
      "loss: 0.836189  [67200/175341]\n",
      "loss: 0.246836  [68800/175341]\n",
      "loss: 0.266077  [70400/175341]\n",
      "loss: 0.091637  [72000/175341]\n",
      "loss: 0.357369  [73600/175341]\n",
      "loss: 0.300268  [75200/175341]\n",
      "loss: 0.616885  [76800/175341]\n",
      "loss: 0.363797  [78400/175341]\n",
      "loss: 0.274704  [80000/175341]\n",
      "loss: 0.843182  [81600/175341]\n",
      "loss: 0.574565  [83200/175341]\n",
      "loss: 0.548034  [84800/175341]\n",
      "loss: 0.186619  [86400/175341]\n",
      "loss: 0.370233  [88000/175341]\n",
      "loss: 0.302790  [89600/175341]\n",
      "loss: 0.392851  [91200/175341]\n",
      "loss: 0.418804  [92800/175341]\n",
      "loss: 0.736095  [94400/175341]\n",
      "loss: 0.217983  [96000/175341]\n",
      "loss: 0.638389  [97600/175341]\n",
      "loss: 0.446081  [99200/175341]\n",
      "loss: 0.358107  [100800/175341]\n",
      "loss: 0.273243  [102400/175341]\n",
      "loss: 0.622261  [104000/175341]\n",
      "loss: 0.655677  [105600/175341]\n",
      "loss: 0.535456  [107200/175341]\n",
      "loss: 0.435922  [108800/175341]\n",
      "loss: 0.448163  [110400/175341]\n",
      "loss: 0.607658  [112000/175341]\n",
      "loss: 0.608227  [113600/175341]\n",
      "loss: 0.666076  [115200/175341]\n",
      "loss: 0.468051  [116800/175341]\n",
      "loss: 0.502829  [118400/175341]\n",
      "loss: 0.722736  [120000/175341]\n",
      "loss: 0.309453  [121600/175341]\n",
      "loss: 0.267931  [123200/175341]\n",
      "loss: 0.311193  [124800/175341]\n",
      "loss: 0.423441  [126400/175341]\n",
      "loss: 0.394280  [128000/175341]\n",
      "loss: 0.461617  [129600/175341]\n",
      "loss: 0.414791  [131200/175341]\n",
      "loss: 0.599719  [132800/175341]\n",
      "loss: 0.250135  [134400/175341]\n",
      "loss: 0.448630  [136000/175341]\n",
      "loss: 0.605881  [137600/175341]\n",
      "loss: 0.157879  [139200/175341]\n",
      "loss: 0.502693  [140800/175341]\n",
      "loss: 0.670030  [142400/175341]\n",
      "loss: 0.709373  [144000/175341]\n",
      "loss: 0.765172  [145600/175341]\n",
      "loss: 0.498956  [147200/175341]\n",
      "loss: 0.409098  [148800/175341]\n",
      "loss: 0.292135  [150400/175341]\n",
      "loss: 0.258059  [152000/175341]\n",
      "loss: 0.619346  [153600/175341]\n",
      "loss: 0.484626  [155200/175341]\n",
      "loss: 0.409323  [156800/175341]\n",
      "loss: 0.285059  [158400/175341]\n",
      "loss: 0.809887  [160000/175341]\n",
      "loss: 0.446841  [161600/175341]\n",
      "loss: 0.533359  [163200/175341]\n",
      "loss: 0.711977  [164800/175341]\n",
      "loss: 0.797788  [166400/175341]\n",
      "loss: 0.296624  [168000/175341]\n",
      "loss: 0.417499  [169600/175341]\n",
      "loss: 0.310402  [171200/175341]\n",
      "loss: 0.538169  [172800/175341]\n",
      "loss: 0.243242  [174400/175341]\n",
      "Train Accuracy: 80.7957%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.546176, F1-score: 75.92%, Macro_F1-Score:  41.84%  \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.171576  [    0/175341]\n",
      "loss: 0.387502  [ 1600/175341]\n",
      "loss: 0.883097  [ 3200/175341]\n",
      "loss: 0.426906  [ 4800/175341]\n",
      "loss: 0.476436  [ 6400/175341]\n",
      "loss: 0.196203  [ 8000/175341]\n",
      "loss: 0.223420  [ 9600/175341]\n",
      "loss: 0.735194  [11200/175341]\n",
      "loss: 0.739198  [12800/175341]\n",
      "loss: 0.415012  [14400/175341]\n",
      "loss: 0.389225  [16000/175341]\n",
      "loss: 0.701707  [17600/175341]\n",
      "loss: 0.393560  [19200/175341]\n",
      "loss: 0.445455  [20800/175341]\n",
      "loss: 0.674144  [22400/175341]\n",
      "loss: 0.671224  [24000/175341]\n",
      "loss: 0.067869  [25600/175341]\n",
      "loss: 0.141066  [27200/175341]\n",
      "loss: 0.717907  [28800/175341]\n",
      "loss: 0.345077  [30400/175341]\n",
      "loss: 0.562056  [32000/175341]\n",
      "loss: 0.406863  [33600/175341]\n",
      "loss: 0.264368  [35200/175341]\n",
      "loss: 0.488922  [36800/175341]\n",
      "loss: 0.232480  [38400/175341]\n",
      "loss: 0.454137  [40000/175341]\n",
      "loss: 0.395069  [41600/175341]\n",
      "loss: 0.471624  [43200/175341]\n",
      "loss: 0.261859  [44800/175341]\n",
      "loss: 0.283627  [46400/175341]\n",
      "loss: 0.760486  [48000/175341]\n",
      "loss: 0.613373  [49600/175341]\n",
      "loss: 0.138383  [51200/175341]\n",
      "loss: 0.544475  [52800/175341]\n",
      "loss: 0.594374  [54400/175341]\n",
      "loss: 0.654228  [56000/175341]\n",
      "loss: 0.506105  [57600/175341]\n",
      "loss: 0.605029  [59200/175341]\n",
      "loss: 0.771223  [60800/175341]\n",
      "loss: 0.739843  [62400/175341]\n",
      "loss: 1.008117  [64000/175341]\n",
      "loss: 0.594530  [65600/175341]\n",
      "loss: 0.123433  [67200/175341]\n",
      "loss: 0.606312  [68800/175341]\n",
      "loss: 0.535374  [70400/175341]\n",
      "loss: 0.580669  [72000/175341]\n",
      "loss: 0.575788  [73600/175341]\n",
      "loss: 0.794450  [75200/175341]\n",
      "loss: 0.486113  [76800/175341]\n",
      "loss: 0.884222  [78400/175341]\n",
      "loss: 0.197288  [80000/175341]\n",
      "loss: 0.347757  [81600/175341]\n",
      "loss: 0.282186  [83200/175341]\n",
      "loss: 0.624343  [84800/175341]\n",
      "loss: 0.403070  [86400/175341]\n",
      "loss: 0.368882  [88000/175341]\n",
      "loss: 0.655674  [89600/175341]\n",
      "loss: 0.395898  [91200/175341]\n",
      "loss: 0.414532  [92800/175341]\n",
      "loss: 0.634103  [94400/175341]\n",
      "loss: 0.190400  [96000/175341]\n",
      "loss: 0.438996  [97600/175341]\n",
      "loss: 0.497894  [99200/175341]\n",
      "loss: 0.223137  [100800/175341]\n",
      "loss: 0.422237  [102400/175341]\n",
      "loss: 0.490111  [104000/175341]\n",
      "loss: 0.339625  [105600/175341]\n",
      "loss: 0.445671  [107200/175341]\n",
      "loss: 0.492358  [108800/175341]\n",
      "loss: 0.505846  [110400/175341]\n",
      "loss: 0.788380  [112000/175341]\n",
      "loss: 1.060467  [113600/175341]\n",
      "loss: 0.335059  [115200/175341]\n",
      "loss: 0.400877  [116800/175341]\n",
      "loss: 0.603473  [118400/175341]\n",
      "loss: 0.589279  [120000/175341]\n",
      "loss: 0.485670  [121600/175341]\n",
      "loss: 0.735708  [123200/175341]\n",
      "loss: 0.554452  [124800/175341]\n",
      "loss: 1.270169  [126400/175341]\n",
      "loss: 0.320507  [128000/175341]\n",
      "loss: 0.289442  [129600/175341]\n",
      "loss: 0.558183  [131200/175341]\n",
      "loss: 0.623326  [132800/175341]\n",
      "loss: 0.352379  [134400/175341]\n",
      "loss: 0.285564  [136000/175341]\n",
      "loss: 0.565307  [137600/175341]\n",
      "loss: 0.658103  [139200/175341]\n",
      "loss: 0.381418  [140800/175341]\n",
      "loss: 0.655155  [142400/175341]\n",
      "loss: 0.808995  [144000/175341]\n",
      "loss: 0.345627  [145600/175341]\n",
      "loss: 0.829012  [147200/175341]\n",
      "loss: 0.352678  [148800/175341]\n",
      "loss: 0.371625  [150400/175341]\n",
      "loss: 0.253176  [152000/175341]\n",
      "loss: 0.484330  [153600/175341]\n",
      "loss: 0.618877  [155200/175341]\n",
      "loss: 0.822555  [156800/175341]\n",
      "loss: 0.567091  [158400/175341]\n",
      "loss: 0.496714  [160000/175341]\n",
      "loss: 0.407248  [161600/175341]\n",
      "loss: 0.550949  [163200/175341]\n",
      "loss: 0.580199  [164800/175341]\n",
      "loss: 0.666937  [166400/175341]\n",
      "loss: 0.608367  [168000/175341]\n",
      "loss: 0.620194  [169600/175341]\n",
      "loss: 0.987789  [171200/175341]\n",
      "loss: 0.483846  [172800/175341]\n",
      "loss: 0.766703  [174400/175341]\n",
      "Train Accuracy: 80.8493%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.544795, F1-score: 75.48%, Macro_F1-Score:  41.46%  \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.617638  [    0/175341]\n",
      "loss: 0.392130  [ 1600/175341]\n",
      "loss: 0.477282  [ 3200/175341]\n",
      "loss: 0.655060  [ 4800/175341]\n",
      "loss: 0.767035  [ 6400/175341]\n",
      "loss: 0.380235  [ 8000/175341]\n",
      "loss: 0.871085  [ 9600/175341]\n",
      "loss: 0.251214  [11200/175341]\n",
      "loss: 0.441605  [12800/175341]\n",
      "loss: 0.381712  [14400/175341]\n",
      "loss: 0.264334  [16000/175341]\n",
      "loss: 0.849226  [17600/175341]\n",
      "loss: 0.321385  [19200/175341]\n",
      "loss: 0.621965  [20800/175341]\n",
      "loss: 0.137813  [22400/175341]\n",
      "loss: 0.236071  [24000/175341]\n",
      "loss: 0.322137  [25600/175341]\n",
      "loss: 0.742763  [27200/175341]\n",
      "loss: 0.427706  [28800/175341]\n",
      "loss: 0.201136  [30400/175341]\n",
      "loss: 0.925559  [32000/175341]\n",
      "loss: 0.205440  [33600/175341]\n",
      "loss: 0.223264  [35200/175341]\n",
      "loss: 0.377486  [36800/175341]\n",
      "loss: 0.503710  [38400/175341]\n",
      "loss: 0.480642  [40000/175341]\n",
      "loss: 0.289458  [41600/175341]\n",
      "loss: 0.322153  [43200/175341]\n",
      "loss: 0.852927  [44800/175341]\n",
      "loss: 0.536307  [46400/175341]\n",
      "loss: 0.455231  [48000/175341]\n",
      "loss: 0.403242  [49600/175341]\n",
      "loss: 0.529990  [51200/175341]\n",
      "loss: 0.790566  [52800/175341]\n",
      "loss: 0.471058  [54400/175341]\n",
      "loss: 0.294936  [56000/175341]\n",
      "loss: 0.258877  [57600/175341]\n",
      "loss: 0.319247  [59200/175341]\n",
      "loss: 0.763395  [60800/175341]\n",
      "loss: 0.476984  [62400/175341]\n",
      "loss: 0.760156  [64000/175341]\n",
      "loss: 0.200489  [65600/175341]\n",
      "loss: 0.584510  [67200/175341]\n",
      "loss: 0.421535  [68800/175341]\n",
      "loss: 0.739570  [70400/175341]\n",
      "loss: 0.168557  [72000/175341]\n",
      "loss: 0.580713  [73600/175341]\n",
      "loss: 0.674285  [75200/175341]\n",
      "loss: 0.666616  [76800/175341]\n",
      "loss: 0.376033  [78400/175341]\n",
      "loss: 0.301341  [80000/175341]\n",
      "loss: 0.767930  [81600/175341]\n",
      "loss: 0.511021  [83200/175341]\n",
      "loss: 0.439961  [84800/175341]\n",
      "loss: 0.792293  [86400/175341]\n",
      "loss: 0.748379  [88000/175341]\n",
      "loss: 0.402820  [89600/175341]\n",
      "loss: 0.460755  [91200/175341]\n",
      "loss: 0.330431  [92800/175341]\n",
      "loss: 0.235091  [94400/175341]\n",
      "loss: 0.777333  [96000/175341]\n",
      "loss: 0.693791  [97600/175341]\n",
      "loss: 0.508052  [99200/175341]\n",
      "loss: 0.448008  [100800/175341]\n",
      "loss: 0.678037  [102400/175341]\n",
      "loss: 0.409857  [104000/175341]\n",
      "loss: 0.468387  [105600/175341]\n",
      "loss: 0.692783  [107200/175341]\n",
      "loss: 0.429259  [108800/175341]\n",
      "loss: 0.975343  [110400/175341]\n",
      "loss: 0.456553  [112000/175341]\n",
      "loss: 0.389894  [113600/175341]\n",
      "loss: 0.591060  [115200/175341]\n",
      "loss: 0.286129  [116800/175341]\n",
      "loss: 0.476755  [118400/175341]\n",
      "loss: 1.335565  [120000/175341]\n",
      "loss: 0.545520  [121600/175341]\n",
      "loss: 0.615098  [123200/175341]\n",
      "loss: 0.403226  [124800/175341]\n",
      "loss: 0.229041  [126400/175341]\n",
      "loss: 0.563941  [128000/175341]\n",
      "loss: 0.259136  [129600/175341]\n",
      "loss: 0.697430  [131200/175341]\n",
      "loss: 0.580449  [132800/175341]\n",
      "loss: 0.449842  [134400/175341]\n",
      "loss: 0.319158  [136000/175341]\n",
      "loss: 0.262243  [137600/175341]\n",
      "loss: 0.579438  [139200/175341]\n",
      "loss: 0.268611  [140800/175341]\n",
      "loss: 0.650320  [142400/175341]\n",
      "loss: 0.723954  [144000/175341]\n",
      "loss: 0.188053  [145600/175341]\n",
      "loss: 0.644309  [147200/175341]\n",
      "loss: 0.219868  [148800/175341]\n",
      "loss: 0.439901  [150400/175341]\n",
      "loss: 0.649070  [152000/175341]\n",
      "loss: 0.444898  [153600/175341]\n",
      "loss: 0.626452  [155200/175341]\n",
      "loss: 0.370343  [156800/175341]\n",
      "loss: 0.409234  [158400/175341]\n",
      "loss: 0.224803  [160000/175341]\n",
      "loss: 0.269264  [161600/175341]\n",
      "loss: 0.632998  [163200/175341]\n",
      "loss: 0.683417  [164800/175341]\n",
      "loss: 0.207572  [166400/175341]\n",
      "loss: 0.316148  [168000/175341]\n",
      "loss: 0.622907  [169600/175341]\n",
      "loss: 0.283562  [171200/175341]\n",
      "loss: 0.800470  [172800/175341]\n",
      "loss: 0.431469  [174400/175341]\n",
      "Train Accuracy: 80.8276%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.554604, F1-score: 75.23%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.748582  [    0/175341]\n",
      "loss: 0.497917  [ 1600/175341]\n",
      "loss: 0.803801  [ 3200/175341]\n",
      "loss: 0.498243  [ 4800/175341]\n",
      "loss: 0.074207  [ 6400/175341]\n",
      "loss: 0.517154  [ 8000/175341]\n",
      "loss: 0.343906  [ 9600/175341]\n",
      "loss: 0.770005  [11200/175341]\n",
      "loss: 0.484760  [12800/175341]\n",
      "loss: 0.372210  [14400/175341]\n",
      "loss: 0.736378  [16000/175341]\n",
      "loss: 0.617766  [17600/175341]\n",
      "loss: 0.742106  [19200/175341]\n",
      "loss: 0.418336  [20800/175341]\n",
      "loss: 0.492123  [22400/175341]\n",
      "loss: 0.648848  [24000/175341]\n",
      "loss: 0.511689  [25600/175341]\n",
      "loss: 0.595621  [27200/175341]\n",
      "loss: 0.396567  [28800/175341]\n",
      "loss: 0.582854  [30400/175341]\n",
      "loss: 0.702080  [32000/175341]\n",
      "loss: 0.591323  [33600/175341]\n",
      "loss: 0.346601  [35200/175341]\n",
      "loss: 0.740941  [36800/175341]\n",
      "loss: 0.843873  [38400/175341]\n",
      "loss: 0.184647  [40000/175341]\n",
      "loss: 0.207997  [41600/175341]\n",
      "loss: 0.450691  [43200/175341]\n",
      "loss: 0.433484  [44800/175341]\n",
      "loss: 0.310111  [46400/175341]\n",
      "loss: 0.283219  [48000/175341]\n",
      "loss: 0.367898  [49600/175341]\n",
      "loss: 0.217377  [51200/175341]\n",
      "loss: 0.383394  [52800/175341]\n",
      "loss: 0.532146  [54400/175341]\n",
      "loss: 0.676304  [56000/175341]\n",
      "loss: 0.631877  [57600/175341]\n",
      "loss: 0.559814  [59200/175341]\n",
      "loss: 0.558392  [60800/175341]\n",
      "loss: 0.226123  [62400/175341]\n",
      "loss: 0.235682  [64000/175341]\n",
      "loss: 0.908573  [65600/175341]\n",
      "loss: 0.245208  [67200/175341]\n",
      "loss: 0.888274  [68800/175341]\n",
      "loss: 0.335461  [70400/175341]\n",
      "loss: 0.412294  [72000/175341]\n",
      "loss: 0.518632  [73600/175341]\n",
      "loss: 0.380164  [75200/175341]\n",
      "loss: 0.235367  [76800/175341]\n",
      "loss: 0.498010  [78400/175341]\n",
      "loss: 0.632449  [80000/175341]\n",
      "loss: 0.382436  [81600/175341]\n",
      "loss: 0.391796  [83200/175341]\n",
      "loss: 0.710599  [84800/175341]\n",
      "loss: 0.566533  [86400/175341]\n",
      "loss: 0.283739  [88000/175341]\n",
      "loss: 0.690194  [89600/175341]\n",
      "loss: 0.390941  [91200/175341]\n",
      "loss: 0.660605  [92800/175341]\n",
      "loss: 0.335808  [94400/175341]\n",
      "loss: 0.488810  [96000/175341]\n",
      "loss: 0.489957  [97600/175341]\n",
      "loss: 0.547127  [99200/175341]\n",
      "loss: 0.450385  [100800/175341]\n",
      "loss: 0.650251  [102400/175341]\n",
      "loss: 0.616855  [104000/175341]\n",
      "loss: 0.464222  [105600/175341]\n",
      "loss: 0.275768  [107200/175341]\n",
      "loss: 0.451594  [108800/175341]\n",
      "loss: 0.267503  [110400/175341]\n",
      "loss: 0.544972  [112000/175341]\n",
      "loss: 0.606128  [113600/175341]\n",
      "loss: 0.892464  [115200/175341]\n",
      "loss: 1.080316  [116800/175341]\n",
      "loss: 0.651057  [118400/175341]\n",
      "loss: 0.651169  [120000/175341]\n",
      "loss: 0.375519  [121600/175341]\n",
      "loss: 0.579707  [123200/175341]\n",
      "loss: 0.182331  [124800/175341]\n",
      "loss: 0.326117  [126400/175341]\n",
      "loss: 0.518794  [128000/175341]\n",
      "loss: 0.572655  [129600/175341]\n",
      "loss: 0.141798  [131200/175341]\n",
      "loss: 0.759626  [132800/175341]\n",
      "loss: 0.768179  [134400/175341]\n",
      "loss: 0.465864  [136000/175341]\n",
      "loss: 0.440376  [137600/175341]\n",
      "loss: 0.924232  [139200/175341]\n",
      "loss: 0.680974  [140800/175341]\n",
      "loss: 0.481372  [142400/175341]\n",
      "loss: 0.285600  [144000/175341]\n",
      "loss: 0.403657  [145600/175341]\n",
      "loss: 0.483546  [147200/175341]\n",
      "loss: 0.800849  [148800/175341]\n",
      "loss: 0.292221  [150400/175341]\n",
      "loss: 0.679730  [152000/175341]\n",
      "loss: 0.486133  [153600/175341]\n",
      "loss: 0.225222  [155200/175341]\n",
      "loss: 0.213682  [156800/175341]\n",
      "loss: 0.519042  [158400/175341]\n",
      "loss: 0.321868  [160000/175341]\n",
      "loss: 0.896042  [161600/175341]\n",
      "loss: 0.253974  [163200/175341]\n",
      "loss: 0.486327  [164800/175341]\n",
      "loss: 0.262701  [166400/175341]\n",
      "loss: 0.370481  [168000/175341]\n",
      "loss: 0.337849  [169600/175341]\n",
      "loss: 0.375817  [171200/175341]\n",
      "loss: 0.552085  [172800/175341]\n",
      "loss: 0.557637  [174400/175341]\n",
      "Train Accuracy: 80.8282%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.550200, F1-score: 75.98%, Macro_F1-Score:  42.19%  \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.371752  [    0/175341]\n",
      "loss: 0.616150  [ 1600/175341]\n",
      "loss: 0.771367  [ 3200/175341]\n",
      "loss: 0.358296  [ 4800/175341]\n",
      "loss: 0.399358  [ 6400/175341]\n",
      "loss: 0.932805  [ 8000/175341]\n",
      "loss: 0.526402  [ 9600/175341]\n",
      "loss: 0.389872  [11200/175341]\n",
      "loss: 0.615233  [12800/175341]\n",
      "loss: 0.598350  [14400/175341]\n",
      "loss: 0.452647  [16000/175341]\n",
      "loss: 0.436412  [17600/175341]\n",
      "loss: 0.157468  [19200/175341]\n",
      "loss: 0.654273  [20800/175341]\n",
      "loss: 0.357663  [22400/175341]\n",
      "loss: 0.733238  [24000/175341]\n",
      "loss: 0.534563  [25600/175341]\n",
      "loss: 0.469531  [27200/175341]\n",
      "loss: 1.276439  [28800/175341]\n",
      "loss: 0.430216  [30400/175341]\n",
      "loss: 0.477897  [32000/175341]\n",
      "loss: 0.818380  [33600/175341]\n",
      "loss: 0.716650  [35200/175341]\n",
      "loss: 0.755117  [36800/175341]\n",
      "loss: 0.236551  [38400/175341]\n",
      "loss: 0.738695  [40000/175341]\n",
      "loss: 0.496094  [41600/175341]\n",
      "loss: 0.508448  [43200/175341]\n",
      "loss: 0.392524  [44800/175341]\n",
      "loss: 0.562490  [46400/175341]\n",
      "loss: 0.232014  [48000/175341]\n",
      "loss: 0.331740  [49600/175341]\n",
      "loss: 0.161988  [51200/175341]\n",
      "loss: 0.167801  [52800/175341]\n",
      "loss: 0.530506  [54400/175341]\n",
      "loss: 0.190950  [56000/175341]\n",
      "loss: 0.335804  [57600/175341]\n",
      "loss: 0.495498  [59200/175341]\n",
      "loss: 0.416465  [60800/175341]\n",
      "loss: 0.568761  [62400/175341]\n",
      "loss: 0.726024  [64000/175341]\n",
      "loss: 0.713255  [65600/175341]\n",
      "loss: 0.453737  [67200/175341]\n",
      "loss: 0.554371  [68800/175341]\n",
      "loss: 0.306005  [70400/175341]\n",
      "loss: 0.116428  [72000/175341]\n",
      "loss: 0.547324  [73600/175341]\n",
      "loss: 0.562992  [75200/175341]\n",
      "loss: 0.425545  [76800/175341]\n",
      "loss: 0.422510  [78400/175341]\n",
      "loss: 0.454445  [80000/175341]\n",
      "loss: 0.345555  [81600/175341]\n",
      "loss: 0.705126  [83200/175341]\n",
      "loss: 0.461422  [84800/175341]\n",
      "loss: 0.943473  [86400/175341]\n",
      "loss: 0.157584  [88000/175341]\n",
      "loss: 0.504833  [89600/175341]\n",
      "loss: 0.233255  [91200/175341]\n",
      "loss: 0.361608  [92800/175341]\n",
      "loss: 0.378696  [94400/175341]\n",
      "loss: 0.335635  [96000/175341]\n",
      "loss: 0.441884  [97600/175341]\n",
      "loss: 0.542217  [99200/175341]\n",
      "loss: 0.658567  [100800/175341]\n",
      "loss: 0.299229  [102400/175341]\n",
      "loss: 0.447800  [104000/175341]\n",
      "loss: 0.443015  [105600/175341]\n",
      "loss: 1.225373  [107200/175341]\n",
      "loss: 0.693320  [108800/175341]\n",
      "loss: 0.357181  [110400/175341]\n",
      "loss: 0.524909  [112000/175341]\n",
      "loss: 0.336432  [113600/175341]\n",
      "loss: 0.633543  [115200/175341]\n",
      "loss: 0.570463  [116800/175341]\n",
      "loss: 0.339699  [118400/175341]\n",
      "loss: 0.454256  [120000/175341]\n",
      "loss: 0.463925  [121600/175341]\n",
      "loss: 0.438828  [123200/175341]\n",
      "loss: 0.451282  [124800/175341]\n",
      "loss: 0.298784  [126400/175341]\n",
      "loss: 0.223786  [128000/175341]\n",
      "loss: 0.179789  [129600/175341]\n",
      "loss: 0.416093  [131200/175341]\n",
      "loss: 0.298101  [132800/175341]\n",
      "loss: 0.624202  [134400/175341]\n",
      "loss: 0.470645  [136000/175341]\n",
      "loss: 0.249321  [137600/175341]\n",
      "loss: 0.292662  [139200/175341]\n",
      "loss: 0.315289  [140800/175341]\n",
      "loss: 0.272469  [142400/175341]\n",
      "loss: 0.147653  [144000/175341]\n",
      "loss: 0.343923  [145600/175341]\n",
      "loss: 0.918091  [147200/175341]\n",
      "loss: 0.785377  [148800/175341]\n",
      "loss: 0.491813  [150400/175341]\n",
      "loss: 0.510144  [152000/175341]\n",
      "loss: 0.318816  [153600/175341]\n",
      "loss: 0.266228  [155200/175341]\n",
      "loss: 0.898926  [156800/175341]\n",
      "loss: 0.424174  [158400/175341]\n",
      "loss: 0.732213  [160000/175341]\n",
      "loss: 0.287397  [161600/175341]\n",
      "loss: 0.662976  [163200/175341]\n",
      "loss: 0.229026  [164800/175341]\n",
      "loss: 0.537960  [166400/175341]\n",
      "loss: 0.477579  [168000/175341]\n",
      "loss: 0.270734  [169600/175341]\n",
      "loss: 0.639843  [171200/175341]\n",
      "loss: 0.313756  [172800/175341]\n",
      "loss: 0.371162  [174400/175341]\n",
      "Train Accuracy: 80.8716%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.542170, F1-score: 75.95%, Macro_F1-Score:  42.10%  \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.313337  [    0/175341]\n",
      "loss: 0.433275  [ 1600/175341]\n",
      "loss: 0.619385  [ 3200/175341]\n",
      "loss: 0.269010  [ 4800/175341]\n",
      "loss: 0.918597  [ 6400/175341]\n",
      "loss: 0.284230  [ 8000/175341]\n",
      "loss: 0.183968  [ 9600/175341]\n",
      "loss: 0.570914  [11200/175341]\n",
      "loss: 0.488045  [12800/175341]\n",
      "loss: 0.220172  [14400/175341]\n",
      "loss: 0.428818  [16000/175341]\n",
      "loss: 0.493051  [17600/175341]\n",
      "loss: 0.621035  [19200/175341]\n",
      "loss: 0.761262  [20800/175341]\n",
      "loss: 0.345477  [22400/175341]\n",
      "loss: 0.575779  [24000/175341]\n",
      "loss: 0.223159  [25600/175341]\n",
      "loss: 0.596631  [27200/175341]\n",
      "loss: 0.563937  [28800/175341]\n",
      "loss: 0.209499  [30400/175341]\n",
      "loss: 0.330742  [32000/175341]\n",
      "loss: 0.266908  [33600/175341]\n",
      "loss: 0.504024  [35200/175341]\n",
      "loss: 0.632774  [36800/175341]\n",
      "loss: 0.199202  [38400/175341]\n",
      "loss: 0.592011  [40000/175341]\n",
      "loss: 0.274031  [41600/175341]\n",
      "loss: 0.273891  [43200/175341]\n",
      "loss: 0.358281  [44800/175341]\n",
      "loss: 0.825457  [46400/175341]\n",
      "loss: 0.800453  [48000/175341]\n",
      "loss: 0.392361  [49600/175341]\n",
      "loss: 0.440480  [51200/175341]\n",
      "loss: 0.768264  [52800/175341]\n",
      "loss: 0.321026  [54400/175341]\n",
      "loss: 1.043064  [56000/175341]\n",
      "loss: 0.461746  [57600/175341]\n",
      "loss: 0.216297  [59200/175341]\n",
      "loss: 0.884547  [60800/175341]\n",
      "loss: 0.247967  [62400/175341]\n",
      "loss: 0.668677  [64000/175341]\n",
      "loss: 0.618411  [65600/175341]\n",
      "loss: 0.582812  [67200/175341]\n",
      "loss: 0.778569  [68800/175341]\n",
      "loss: 0.423668  [70400/175341]\n",
      "loss: 0.455190  [72000/175341]\n",
      "loss: 0.376765  [73600/175341]\n",
      "loss: 0.617704  [75200/175341]\n",
      "loss: 0.398495  [76800/175341]\n",
      "loss: 0.571496  [78400/175341]\n",
      "loss: 0.623921  [80000/175341]\n",
      "loss: 0.323799  [81600/175341]\n",
      "loss: 0.235180  [83200/175341]\n",
      "loss: 0.198814  [84800/175341]\n",
      "loss: 0.277044  [86400/175341]\n",
      "loss: 0.717749  [88000/175341]\n",
      "loss: 0.507468  [89600/175341]\n",
      "loss: 0.850570  [91200/175341]\n",
      "loss: 0.642321  [92800/175341]\n",
      "loss: 0.541516  [94400/175341]\n",
      "loss: 0.329143  [96000/175341]\n",
      "loss: 0.806895  [97600/175341]\n",
      "loss: 0.508689  [99200/175341]\n",
      "loss: 0.662032  [100800/175341]\n",
      "loss: 0.655338  [102400/175341]\n",
      "loss: 0.535222  [104000/175341]\n",
      "loss: 0.879040  [105600/175341]\n",
      "loss: 0.502859  [107200/175341]\n",
      "loss: 0.440928  [108800/175341]\n",
      "loss: 0.410410  [110400/175341]\n",
      "loss: 0.386623  [112000/175341]\n",
      "loss: 0.450304  [113600/175341]\n",
      "loss: 0.422521  [115200/175341]\n",
      "loss: 0.735550  [116800/175341]\n",
      "loss: 0.585082  [118400/175341]\n",
      "loss: 0.566382  [120000/175341]\n",
      "loss: 0.482680  [121600/175341]\n",
      "loss: 0.329107  [123200/175341]\n",
      "loss: 0.328294  [124800/175341]\n",
      "loss: 0.563297  [126400/175341]\n",
      "loss: 0.630272  [128000/175341]\n",
      "loss: 0.836011  [129600/175341]\n",
      "loss: 0.598550  [131200/175341]\n",
      "loss: 0.426546  [132800/175341]\n",
      "loss: 0.157195  [134400/175341]\n",
      "loss: 0.460409  [136000/175341]\n",
      "loss: 0.792867  [137600/175341]\n",
      "loss: 0.508952  [139200/175341]\n",
      "loss: 0.522973  [140800/175341]\n",
      "loss: 0.545049  [142400/175341]\n",
      "loss: 0.169316  [144000/175341]\n",
      "loss: 0.429357  [145600/175341]\n",
      "loss: 0.725967  [147200/175341]\n",
      "loss: 0.783457  [148800/175341]\n",
      "loss: 0.353650  [150400/175341]\n",
      "loss: 0.858104  [152000/175341]\n",
      "loss: 0.670119  [153600/175341]\n",
      "loss: 1.013451  [155200/175341]\n",
      "loss: 0.431386  [156800/175341]\n",
      "loss: 0.584951  [158400/175341]\n",
      "loss: 0.704899  [160000/175341]\n",
      "loss: 0.634787  [161600/175341]\n",
      "loss: 0.681137  [163200/175341]\n",
      "loss: 0.678819  [164800/175341]\n",
      "loss: 0.377170  [166400/175341]\n",
      "loss: 0.551487  [168000/175341]\n",
      "loss: 0.312011  [169600/175341]\n",
      "loss: 0.514072  [171200/175341]\n",
      "loss: 0.928262  [172800/175341]\n",
      "loss: 0.451655  [174400/175341]\n",
      "Train Accuracy: 80.8698%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.546929, F1-score: 75.81%, Macro_F1-Score:  41.94%  \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.221007  [    0/175341]\n",
      "loss: 0.205854  [ 1600/175341]\n",
      "loss: 0.535397  [ 3200/175341]\n",
      "loss: 0.151609  [ 4800/175341]\n",
      "loss: 0.264733  [ 6400/175341]\n",
      "loss: 0.574750  [ 8000/175341]\n",
      "loss: 0.254918  [ 9600/175341]\n",
      "loss: 0.195634  [11200/175341]\n",
      "loss: 0.430497  [12800/175341]\n",
      "loss: 0.328227  [14400/175341]\n",
      "loss: 0.116916  [16000/175341]\n",
      "loss: 0.228172  [17600/175341]\n",
      "loss: 0.137596  [19200/175341]\n",
      "loss: 0.567570  [20800/175341]\n",
      "loss: 0.598875  [22400/175341]\n",
      "loss: 0.248954  [24000/175341]\n",
      "loss: 0.307092  [25600/175341]\n",
      "loss: 0.787998  [27200/175341]\n",
      "loss: 1.032074  [28800/175341]\n",
      "loss: 0.772956  [30400/175341]\n",
      "loss: 0.439956  [32000/175341]\n",
      "loss: 0.551376  [33600/175341]\n",
      "loss: 0.536947  [35200/175341]\n",
      "loss: 0.298966  [36800/175341]\n",
      "loss: 0.625143  [38400/175341]\n",
      "loss: 0.486768  [40000/175341]\n",
      "loss: 0.396167  [41600/175341]\n",
      "loss: 0.421985  [43200/175341]\n",
      "loss: 0.215743  [44800/175341]\n",
      "loss: 0.696319  [46400/175341]\n",
      "loss: 0.285013  [48000/175341]\n",
      "loss: 0.473197  [49600/175341]\n",
      "loss: 0.628765  [51200/175341]\n",
      "loss: 0.339250  [52800/175341]\n",
      "loss: 0.427735  [54400/175341]\n",
      "loss: 0.482997  [56000/175341]\n",
      "loss: 0.306910  [57600/175341]\n",
      "loss: 0.346518  [59200/175341]\n",
      "loss: 0.608935  [60800/175341]\n",
      "loss: 0.597483  [62400/175341]\n",
      "loss: 0.755689  [64000/175341]\n",
      "loss: 0.765778  [65600/175341]\n",
      "loss: 0.712659  [67200/175341]\n",
      "loss: 0.606643  [68800/175341]\n",
      "loss: 0.511465  [70400/175341]\n",
      "loss: 0.274078  [72000/175341]\n",
      "loss: 0.611955  [73600/175341]\n",
      "loss: 0.591875  [75200/175341]\n",
      "loss: 0.797800  [76800/175341]\n",
      "loss: 0.460908  [78400/175341]\n",
      "loss: 0.413545  [80000/175341]\n",
      "loss: 0.202227  [81600/175341]\n",
      "loss: 0.437127  [83200/175341]\n",
      "loss: 0.441347  [84800/175341]\n",
      "loss: 0.500171  [86400/175341]\n",
      "loss: 0.474286  [88000/175341]\n",
      "loss: 0.284673  [89600/175341]\n",
      "loss: 0.492481  [91200/175341]\n",
      "loss: 0.955613  [92800/175341]\n",
      "loss: 1.223104  [94400/175341]\n",
      "loss: 0.330780  [96000/175341]\n",
      "loss: 0.281491  [97600/175341]\n",
      "loss: 0.261628  [99200/175341]\n",
      "loss: 0.698310  [100800/175341]\n",
      "loss: 0.466215  [102400/175341]\n",
      "loss: 0.804560  [104000/175341]\n",
      "loss: 0.260562  [105600/175341]\n",
      "loss: 0.189054  [107200/175341]\n",
      "loss: 0.310777  [108800/175341]\n",
      "loss: 0.415547  [110400/175341]\n",
      "loss: 0.616715  [112000/175341]\n",
      "loss: 0.512563  [113600/175341]\n",
      "loss: 0.501546  [115200/175341]\n",
      "loss: 0.521669  [116800/175341]\n",
      "loss: 0.498124  [118400/175341]\n",
      "loss: 0.584449  [120000/175341]\n",
      "loss: 0.314862  [121600/175341]\n",
      "loss: 0.384082  [123200/175341]\n",
      "loss: 0.295206  [124800/175341]\n",
      "loss: 0.763687  [126400/175341]\n",
      "loss: 0.487906  [128000/175341]\n",
      "loss: 0.078423  [129600/175341]\n",
      "loss: 0.246334  [131200/175341]\n",
      "loss: 0.525748  [132800/175341]\n",
      "loss: 0.364232  [134400/175341]\n",
      "loss: 0.343774  [136000/175341]\n",
      "loss: 0.424366  [137600/175341]\n",
      "loss: 0.347959  [139200/175341]\n",
      "loss: 0.454743  [140800/175341]\n",
      "loss: 0.334703  [142400/175341]\n",
      "loss: 0.566661  [144000/175341]\n",
      "loss: 0.726794  [145600/175341]\n",
      "loss: 0.480985  [147200/175341]\n",
      "loss: 0.793796  [148800/175341]\n",
      "loss: 0.793805  [150400/175341]\n",
      "loss: 0.436061  [152000/175341]\n",
      "loss: 0.502762  [153600/175341]\n",
      "loss: 0.494652  [155200/175341]\n",
      "loss: 0.414468  [156800/175341]\n",
      "loss: 0.538577  [158400/175341]\n",
      "loss: 0.306091  [160000/175341]\n",
      "loss: 0.565240  [161600/175341]\n",
      "loss: 0.201401  [163200/175341]\n",
      "loss: 0.631481  [164800/175341]\n",
      "loss: 0.380122  [166400/175341]\n",
      "loss: 0.588936  [168000/175341]\n",
      "loss: 1.008777  [169600/175341]\n",
      "loss: 0.354835  [171200/175341]\n",
      "loss: 0.477369  [172800/175341]\n",
      "loss: 0.554255  [174400/175341]\n",
      "Train Accuracy: 80.8681%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.548889, F1-score: 75.58%, Macro_F1-Score:  41.27%  \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.493841  [    0/175341]\n",
      "loss: 0.483013  [ 1600/175341]\n",
      "loss: 0.448895  [ 3200/175341]\n",
      "loss: 0.339259  [ 4800/175341]\n",
      "loss: 0.322637  [ 6400/175341]\n",
      "loss: 0.457072  [ 8000/175341]\n",
      "loss: 0.467348  [ 9600/175341]\n",
      "loss: 0.491723  [11200/175341]\n",
      "loss: 0.583134  [12800/175341]\n",
      "loss: 0.271228  [14400/175341]\n",
      "loss: 0.623396  [16000/175341]\n",
      "loss: 0.501844  [17600/175341]\n",
      "loss: 0.717154  [19200/175341]\n",
      "loss: 0.468092  [20800/175341]\n",
      "loss: 0.541440  [22400/175341]\n",
      "loss: 0.961539  [24000/175341]\n",
      "loss: 0.442387  [25600/175341]\n",
      "loss: 0.237415  [27200/175341]\n",
      "loss: 0.204119  [28800/175341]\n",
      "loss: 0.599000  [30400/175341]\n",
      "loss: 0.183439  [32000/175341]\n",
      "loss: 0.226713  [33600/175341]\n",
      "loss: 0.274848  [35200/175341]\n",
      "loss: 0.684065  [36800/175341]\n",
      "loss: 1.137691  [38400/175341]\n",
      "loss: 0.298102  [40000/175341]\n",
      "loss: 0.453436  [41600/175341]\n",
      "loss: 0.293303  [43200/175341]\n",
      "loss: 0.141612  [44800/175341]\n",
      "loss: 1.219470  [46400/175341]\n",
      "loss: 0.300167  [48000/175341]\n",
      "loss: 0.703505  [49600/175341]\n",
      "loss: 0.465248  [51200/175341]\n",
      "loss: 0.441206  [52800/175341]\n",
      "loss: 0.544344  [54400/175341]\n",
      "loss: 0.426149  [56000/175341]\n",
      "loss: 0.506239  [57600/175341]\n",
      "loss: 0.986923  [59200/175341]\n",
      "loss: 0.595312  [60800/175341]\n",
      "loss: 0.350511  [62400/175341]\n",
      "loss: 0.332716  [64000/175341]\n",
      "loss: 0.689114  [65600/175341]\n",
      "loss: 0.423075  [67200/175341]\n",
      "loss: 0.939334  [68800/175341]\n",
      "loss: 0.353495  [70400/175341]\n",
      "loss: 0.421862  [72000/175341]\n",
      "loss: 0.661973  [73600/175341]\n",
      "loss: 0.528183  [75200/175341]\n",
      "loss: 0.436403  [76800/175341]\n",
      "loss: 0.776473  [78400/175341]\n",
      "loss: 0.282513  [80000/175341]\n",
      "loss: 0.764720  [81600/175341]\n",
      "loss: 0.544131  [83200/175341]\n",
      "loss: 0.272516  [84800/175341]\n",
      "loss: 0.640099  [86400/175341]\n",
      "loss: 0.518182  [88000/175341]\n",
      "loss: 0.298600  [89600/175341]\n",
      "loss: 0.292337  [91200/175341]\n",
      "loss: 0.490064  [92800/175341]\n",
      "loss: 0.894074  [94400/175341]\n",
      "loss: 0.737876  [96000/175341]\n",
      "loss: 0.374886  [97600/175341]\n",
      "loss: 0.644170  [99200/175341]\n",
      "loss: 0.727726  [100800/175341]\n",
      "loss: 0.565404  [102400/175341]\n",
      "loss: 0.668279  [104000/175341]\n",
      "loss: 0.521720  [105600/175341]\n",
      "loss: 0.248445  [107200/175341]\n",
      "loss: 0.361551  [108800/175341]\n",
      "loss: 0.614234  [110400/175341]\n",
      "loss: 0.350037  [112000/175341]\n",
      "loss: 0.166957  [113600/175341]\n",
      "loss: 0.446633  [115200/175341]\n",
      "loss: 0.559464  [116800/175341]\n",
      "loss: 0.502535  [118400/175341]\n",
      "loss: 0.458155  [120000/175341]\n",
      "loss: 0.644330  [121600/175341]\n",
      "loss: 0.520274  [123200/175341]\n",
      "loss: 0.369592  [124800/175341]\n",
      "loss: 0.457462  [126400/175341]\n",
      "loss: 0.502158  [128000/175341]\n",
      "loss: 0.346750  [129600/175341]\n",
      "loss: 0.257875  [131200/175341]\n",
      "loss: 0.310728  [132800/175341]\n",
      "loss: 0.824661  [134400/175341]\n",
      "loss: 0.488770  [136000/175341]\n",
      "loss: 0.621472  [137600/175341]\n",
      "loss: 0.340366  [139200/175341]\n",
      "loss: 0.477420  [140800/175341]\n",
      "loss: 0.591565  [142400/175341]\n",
      "loss: 0.526140  [144000/175341]\n",
      "loss: 0.460881  [145600/175341]\n",
      "loss: 0.373244  [147200/175341]\n",
      "loss: 0.474493  [148800/175341]\n",
      "loss: 0.328061  [150400/175341]\n",
      "loss: 0.523669  [152000/175341]\n",
      "loss: 0.826520  [153600/175341]\n",
      "loss: 0.569629  [155200/175341]\n",
      "loss: 0.511332  [156800/175341]\n",
      "loss: 0.538191  [158400/175341]\n",
      "loss: 0.456891  [160000/175341]\n",
      "loss: 0.437803  [161600/175341]\n",
      "loss: 0.142796  [163200/175341]\n",
      "loss: 0.317211  [164800/175341]\n",
      "loss: 0.548933  [166400/175341]\n",
      "loss: 0.846203  [168000/175341]\n",
      "loss: 0.363173  [169600/175341]\n",
      "loss: 0.340427  [171200/175341]\n",
      "loss: 0.298054  [172800/175341]\n",
      "loss: 0.376150  [174400/175341]\n",
      "Train Accuracy: 80.8681%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.564394, F1-score: 74.95%, Macro_F1-Score:  42.38%  \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.432614  [    0/175341]\n",
      "loss: 0.546003  [ 1600/175341]\n",
      "loss: 0.605447  [ 3200/175341]\n",
      "loss: 0.708057  [ 4800/175341]\n",
      "loss: 0.583273  [ 6400/175341]\n",
      "loss: 0.680772  [ 8000/175341]\n",
      "loss: 0.613208  [ 9600/175341]\n",
      "loss: 0.189559  [11200/175341]\n",
      "loss: 0.286610  [12800/175341]\n",
      "loss: 0.505759  [14400/175341]\n",
      "loss: 0.587411  [16000/175341]\n",
      "loss: 0.506402  [17600/175341]\n",
      "loss: 0.584749  [19200/175341]\n",
      "loss: 0.494440  [20800/175341]\n",
      "loss: 0.372972  [22400/175341]\n",
      "loss: 0.684101  [24000/175341]\n",
      "loss: 0.254199  [25600/175341]\n",
      "loss: 0.341000  [27200/175341]\n",
      "loss: 0.723182  [28800/175341]\n",
      "loss: 0.136190  [30400/175341]\n",
      "loss: 0.601671  [32000/175341]\n",
      "loss: 0.294107  [33600/175341]\n",
      "loss: 0.225364  [35200/175341]\n",
      "loss: 0.257352  [36800/175341]\n",
      "loss: 0.316915  [38400/175341]\n",
      "loss: 0.654603  [40000/175341]\n",
      "loss: 0.772945  [41600/175341]\n",
      "loss: 0.481374  [43200/175341]\n",
      "loss: 0.135955  [44800/175341]\n",
      "loss: 0.419192  [46400/175341]\n",
      "loss: 0.581766  [48000/175341]\n",
      "loss: 0.595614  [49600/175341]\n",
      "loss: 0.544787  [51200/175341]\n",
      "loss: 0.524821  [52800/175341]\n",
      "loss: 0.369690  [54400/175341]\n",
      "loss: 0.349574  [56000/175341]\n",
      "loss: 0.682932  [57600/175341]\n",
      "loss: 0.349381  [59200/175341]\n",
      "loss: 0.487496  [60800/175341]\n",
      "loss: 0.122395  [62400/175341]\n",
      "loss: 0.464231  [64000/175341]\n",
      "loss: 0.473233  [65600/175341]\n",
      "loss: 0.722503  [67200/175341]\n",
      "loss: 0.289034  [68800/175341]\n",
      "loss: 0.710289  [70400/175341]\n",
      "loss: 0.506990  [72000/175341]\n",
      "loss: 0.379069  [73600/175341]\n",
      "loss: 0.628290  [75200/175341]\n",
      "loss: 1.099270  [76800/175341]\n",
      "loss: 0.491447  [78400/175341]\n",
      "loss: 0.696562  [80000/175341]\n",
      "loss: 0.656883  [81600/175341]\n",
      "loss: 0.436021  [83200/175341]\n",
      "loss: 0.457986  [84800/175341]\n",
      "loss: 0.232651  [86400/175341]\n",
      "loss: 0.768894  [88000/175341]\n",
      "loss: 0.291406  [89600/175341]\n",
      "loss: 0.602220  [91200/175341]\n",
      "loss: 1.142376  [92800/175341]\n",
      "loss: 0.274386  [94400/175341]\n",
      "loss: 0.477204  [96000/175341]\n",
      "loss: 0.457403  [97600/175341]\n",
      "loss: 0.452014  [99200/175341]\n",
      "loss: 0.337146  [100800/175341]\n",
      "loss: 0.667884  [102400/175341]\n",
      "loss: 0.732162  [104000/175341]\n",
      "loss: 0.445928  [105600/175341]\n",
      "loss: 0.618821  [107200/175341]\n",
      "loss: 0.385681  [108800/175341]\n",
      "loss: 0.781133  [110400/175341]\n",
      "loss: 0.308989  [112000/175341]\n",
      "loss: 0.652078  [113600/175341]\n",
      "loss: 0.538465  [115200/175341]\n",
      "loss: 0.214462  [116800/175341]\n",
      "loss: 0.537345  [118400/175341]\n",
      "loss: 0.648896  [120000/175341]\n",
      "loss: 0.415222  [121600/175341]\n",
      "loss: 0.388593  [123200/175341]\n",
      "loss: 0.888334  [124800/175341]\n",
      "loss: 0.423154  [126400/175341]\n",
      "loss: 0.211808  [128000/175341]\n",
      "loss: 0.496414  [129600/175341]\n",
      "loss: 0.178087  [131200/175341]\n",
      "loss: 0.366467  [132800/175341]\n",
      "loss: 0.604812  [134400/175341]\n",
      "loss: 0.455149  [136000/175341]\n",
      "loss: 0.596567  [137600/175341]\n",
      "loss: 0.465545  [139200/175341]\n",
      "loss: 0.394705  [140800/175341]\n",
      "loss: 0.727129  [142400/175341]\n",
      "loss: 0.252503  [144000/175341]\n",
      "loss: 0.348290  [145600/175341]\n",
      "loss: 0.439067  [147200/175341]\n",
      "loss: 0.402407  [148800/175341]\n",
      "loss: 0.588665  [150400/175341]\n",
      "loss: 0.311892  [152000/175341]\n",
      "loss: 0.611050  [153600/175341]\n",
      "loss: 0.802088  [155200/175341]\n",
      "loss: 0.398702  [156800/175341]\n",
      "loss: 0.612229  [158400/175341]\n",
      "loss: 0.356626  [160000/175341]\n",
      "loss: 0.231159  [161600/175341]\n",
      "loss: 0.292983  [163200/175341]\n",
      "loss: 0.286823  [164800/175341]\n",
      "loss: 0.544786  [166400/175341]\n",
      "loss: 0.324846  [168000/175341]\n",
      "loss: 0.449812  [169600/175341]\n",
      "loss: 0.614259  [171200/175341]\n",
      "loss: 0.255771  [172800/175341]\n",
      "loss: 0.455282  [174400/175341]\n",
      "Train Accuracy: 80.8556%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.549885, F1-score: 75.46%, Macro_F1-Score:  41.01%  \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.703710  [    0/175341]\n",
      "loss: 0.657648  [ 1600/175341]\n",
      "loss: 0.843174  [ 3200/175341]\n",
      "loss: 0.440199  [ 4800/175341]\n",
      "loss: 0.415387  [ 6400/175341]\n",
      "loss: 0.210132  [ 8000/175341]\n",
      "loss: 0.699474  [ 9600/175341]\n",
      "loss: 0.506544  [11200/175341]\n",
      "loss: 0.922780  [12800/175341]\n",
      "loss: 0.707063  [14400/175341]\n",
      "loss: 0.437365  [16000/175341]\n",
      "loss: 0.431987  [17600/175341]\n",
      "loss: 0.299875  [19200/175341]\n",
      "loss: 0.477345  [20800/175341]\n",
      "loss: 0.543470  [22400/175341]\n",
      "loss: 0.826724  [24000/175341]\n",
      "loss: 0.407596  [25600/175341]\n",
      "loss: 0.275337  [27200/175341]\n",
      "loss: 0.321699  [28800/175341]\n",
      "loss: 0.363153  [30400/175341]\n",
      "loss: 0.370097  [32000/175341]\n",
      "loss: 0.574048  [33600/175341]\n",
      "loss: 0.130908  [35200/175341]\n",
      "loss: 0.088477  [36800/175341]\n",
      "loss: 0.155260  [38400/175341]\n",
      "loss: 0.397200  [40000/175341]\n",
      "loss: 0.333339  [41600/175341]\n",
      "loss: 0.432132  [43200/175341]\n",
      "loss: 0.789474  [44800/175341]\n",
      "loss: 0.494356  [46400/175341]\n",
      "loss: 0.347265  [48000/175341]\n",
      "loss: 0.386027  [49600/175341]\n",
      "loss: 0.350524  [51200/175341]\n",
      "loss: 0.680395  [52800/175341]\n",
      "loss: 0.289308  [54400/175341]\n",
      "loss: 0.748072  [56000/175341]\n",
      "loss: 0.827762  [57600/175341]\n",
      "loss: 0.346772  [59200/175341]\n",
      "loss: 0.378659  [60800/175341]\n",
      "loss: 0.210224  [62400/175341]\n",
      "loss: 0.802689  [64000/175341]\n",
      "loss: 0.309031  [65600/175341]\n",
      "loss: 0.939456  [67200/175341]\n",
      "loss: 0.660196  [68800/175341]\n",
      "loss: 0.547465  [70400/175341]\n",
      "loss: 0.448674  [72000/175341]\n",
      "loss: 0.427072  [73600/175341]\n",
      "loss: 0.519997  [75200/175341]\n",
      "loss: 0.143057  [76800/175341]\n",
      "loss: 0.521417  [78400/175341]\n",
      "loss: 0.507352  [80000/175341]\n",
      "loss: 0.350502  [81600/175341]\n",
      "loss: 0.223432  [83200/175341]\n",
      "loss: 0.571160  [84800/175341]\n",
      "loss: 0.495204  [86400/175341]\n",
      "loss: 0.723963  [88000/175341]\n",
      "loss: 0.659344  [89600/175341]\n",
      "loss: 0.409974  [91200/175341]\n",
      "loss: 0.625621  [92800/175341]\n",
      "loss: 0.658847  [94400/175341]\n",
      "loss: 0.403004  [96000/175341]\n",
      "loss: 0.281484  [97600/175341]\n",
      "loss: 0.380172  [99200/175341]\n",
      "loss: 0.433801  [100800/175341]\n",
      "loss: 0.277967  [102400/175341]\n",
      "loss: 0.895799  [104000/175341]\n",
      "loss: 0.819336  [105600/175341]\n",
      "loss: 0.849730  [107200/175341]\n",
      "loss: 0.338931  [108800/175341]\n",
      "loss: 0.249968  [110400/175341]\n",
      "loss: 0.222982  [112000/175341]\n",
      "loss: 0.453875  [113600/175341]\n",
      "loss: 0.400054  [115200/175341]\n",
      "loss: 0.879496  [116800/175341]\n",
      "loss: 0.156901  [118400/175341]\n",
      "loss: 0.245620  [120000/175341]\n",
      "loss: 0.472437  [121600/175341]\n",
      "loss: 0.332752  [123200/175341]\n",
      "loss: 0.198471  [124800/175341]\n",
      "loss: 0.748766  [126400/175341]\n",
      "loss: 0.487488  [128000/175341]\n",
      "loss: 0.666841  [129600/175341]\n",
      "loss: 0.254751  [131200/175341]\n",
      "loss: 0.278313  [132800/175341]\n",
      "loss: 0.416468  [134400/175341]\n",
      "loss: 0.380832  [136000/175341]\n",
      "loss: 0.829707  [137600/175341]\n",
      "loss: 0.356747  [139200/175341]\n",
      "loss: 0.540542  [140800/175341]\n",
      "loss: 0.391462  [142400/175341]\n",
      "loss: 0.502937  [144000/175341]\n",
      "loss: 0.292227  [145600/175341]\n",
      "loss: 0.625169  [147200/175341]\n",
      "loss: 0.545320  [148800/175341]\n",
      "loss: 0.273688  [150400/175341]\n",
      "loss: 0.382436  [152000/175341]\n",
      "loss: 0.505194  [153600/175341]\n",
      "loss: 0.550960  [155200/175341]\n",
      "loss: 0.499261  [156800/175341]\n",
      "loss: 0.152015  [158400/175341]\n",
      "loss: 0.359649  [160000/175341]\n",
      "loss: 0.320189  [161600/175341]\n",
      "loss: 0.519971  [163200/175341]\n",
      "loss: 0.195464  [164800/175341]\n",
      "loss: 0.282985  [166400/175341]\n",
      "loss: 0.413972  [168000/175341]\n",
      "loss: 0.243270  [169600/175341]\n",
      "loss: 0.479833  [171200/175341]\n",
      "loss: 0.543813  [172800/175341]\n",
      "loss: 0.309370  [174400/175341]\n",
      "Train Accuracy: 80.9189%\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.531495, F1-score: 76.95%, Macro_F1-Score:  41.80%  \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.449129  [    0/175341]\n",
      "loss: 0.396305  [ 1600/175341]\n",
      "loss: 0.549699  [ 3200/175341]\n",
      "loss: 0.259693  [ 4800/175341]\n",
      "loss: 0.773359  [ 6400/175341]\n",
      "loss: 0.237363  [ 8000/175341]\n",
      "loss: 0.125027  [ 9600/175341]\n",
      "loss: 0.523455  [11200/175341]\n",
      "loss: 0.468928  [12800/175341]\n",
      "loss: 0.724033  [14400/175341]\n",
      "loss: 0.648120  [16000/175341]\n",
      "loss: 0.706821  [17600/175341]\n",
      "loss: 0.302415  [19200/175341]\n",
      "loss: 0.105615  [20800/175341]\n",
      "loss: 0.162630  [22400/175341]\n",
      "loss: 0.129183  [24000/175341]\n",
      "loss: 0.618125  [25600/175341]\n",
      "loss: 0.694847  [27200/175341]\n",
      "loss: 0.598308  [28800/175341]\n",
      "loss: 0.817599  [30400/175341]\n",
      "loss: 0.320260  [32000/175341]\n",
      "loss: 0.653301  [33600/175341]\n",
      "loss: 0.431798  [35200/175341]\n",
      "loss: 0.408755  [36800/175341]\n",
      "loss: 0.484965  [38400/175341]\n",
      "loss: 0.415685  [40000/175341]\n",
      "loss: 1.007528  [41600/175341]\n",
      "loss: 0.315177  [43200/175341]\n",
      "loss: 0.215413  [44800/175341]\n",
      "loss: 0.672730  [46400/175341]\n",
      "loss: 0.562060  [48000/175341]\n",
      "loss: 0.410417  [49600/175341]\n",
      "loss: 0.258441  [51200/175341]\n",
      "loss: 0.747712  [52800/175341]\n",
      "loss: 0.515967  [54400/175341]\n",
      "loss: 0.672837  [56000/175341]\n",
      "loss: 0.448418  [57600/175341]\n",
      "loss: 0.155889  [59200/175341]\n",
      "loss: 0.317719  [60800/175341]\n",
      "loss: 0.214854  [62400/175341]\n",
      "loss: 0.588670  [64000/175341]\n",
      "loss: 0.297900  [65600/175341]\n",
      "loss: 0.196618  [67200/175341]\n",
      "loss: 0.360234  [68800/175341]\n",
      "loss: 0.622913  [70400/175341]\n",
      "loss: 0.727293  [72000/175341]\n",
      "loss: 0.542212  [73600/175341]\n",
      "loss: 0.195129  [75200/175341]\n",
      "loss: 0.621960  [76800/175341]\n",
      "loss: 0.282267  [78400/175341]\n",
      "loss: 0.368432  [80000/175341]\n",
      "loss: 0.451600  [81600/175341]\n",
      "loss: 0.565521  [83200/175341]\n",
      "loss: 0.934984  [84800/175341]\n",
      "loss: 0.262224  [86400/175341]\n",
      "loss: 0.557352  [88000/175341]\n",
      "loss: 0.884275  [89600/175341]\n",
      "loss: 0.748213  [91200/175341]\n",
      "loss: 0.543411  [92800/175341]\n",
      "loss: 0.895073  [94400/175341]\n",
      "loss: 0.501513  [96000/175341]\n",
      "loss: 0.339247  [97600/175341]\n",
      "loss: 0.282075  [99200/175341]\n",
      "loss: 0.256985  [100800/175341]\n",
      "loss: 0.629257  [102400/175341]\n",
      "loss: 0.526982  [104000/175341]\n",
      "loss: 1.210410  [105600/175341]\n",
      "loss: 0.150461  [107200/175341]\n",
      "loss: 0.381676  [108800/175341]\n",
      "loss: 0.421854  [110400/175341]\n",
      "loss: 0.367437  [112000/175341]\n",
      "loss: 0.845308  [113600/175341]\n",
      "loss: 0.654760  [115200/175341]\n",
      "loss: 0.402479  [116800/175341]\n",
      "loss: 0.615812  [118400/175341]\n",
      "loss: 0.589895  [120000/175341]\n",
      "loss: 0.643484  [121600/175341]\n",
      "loss: 0.554247  [123200/175341]\n",
      "loss: 0.393750  [124800/175341]\n",
      "loss: 0.394029  [126400/175341]\n",
      "loss: 0.539617  [128000/175341]\n",
      "loss: 0.489864  [129600/175341]\n",
      "loss: 0.877047  [131200/175341]\n",
      "loss: 0.951071  [132800/175341]\n",
      "loss: 0.420947  [134400/175341]\n",
      "loss: 0.610758  [136000/175341]\n",
      "loss: 0.586419  [137600/175341]\n",
      "loss: 0.309943  [139200/175341]\n",
      "loss: 0.786846  [140800/175341]\n",
      "loss: 0.369005  [142400/175341]\n",
      "loss: 0.309916  [144000/175341]\n",
      "loss: 0.264219  [145600/175341]\n",
      "loss: 0.517273  [147200/175341]\n",
      "loss: 0.515857  [148800/175341]\n",
      "loss: 0.356563  [150400/175341]\n",
      "loss: 1.184862  [152000/175341]\n",
      "loss: 0.914776  [153600/175341]\n",
      "loss: 0.347073  [155200/175341]\n",
      "loss: 0.482619  [156800/175341]\n",
      "loss: 0.125135  [158400/175341]\n",
      "loss: 0.748800  [160000/175341]\n",
      "loss: 0.287796  [161600/175341]\n",
      "loss: 0.465577  [163200/175341]\n",
      "loss: 0.572610  [164800/175341]\n",
      "loss: 0.491639  [166400/175341]\n",
      "loss: 0.449953  [168000/175341]\n",
      "loss: 0.375963  [169600/175341]\n",
      "loss: 0.202947  [171200/175341]\n",
      "loss: 0.670447  [172800/175341]\n",
      "loss: 0.796508  [174400/175341]\n",
      "Train Accuracy: 80.9554%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.557820, F1-score: 74.87%, Macro_F1-Score:  41.85%  \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.480436  [    0/175341]\n",
      "loss: 0.317828  [ 1600/175341]\n",
      "loss: 0.539366  [ 3200/175341]\n",
      "loss: 0.759867  [ 4800/175341]\n",
      "loss: 0.596898  [ 6400/175341]\n",
      "loss: 0.717215  [ 8000/175341]\n",
      "loss: 0.553311  [ 9600/175341]\n",
      "loss: 0.427700  [11200/175341]\n",
      "loss: 0.498977  [12800/175341]\n",
      "loss: 0.619147  [14400/175341]\n",
      "loss: 0.670053  [16000/175341]\n",
      "loss: 0.520856  [17600/175341]\n",
      "loss: 0.274432  [19200/175341]\n",
      "loss: 0.533901  [20800/175341]\n",
      "loss: 0.178514  [22400/175341]\n",
      "loss: 0.209137  [24000/175341]\n",
      "loss: 0.635647  [25600/175341]\n",
      "loss: 1.115544  [27200/175341]\n",
      "loss: 0.375611  [28800/175341]\n",
      "loss: 0.557710  [30400/175341]\n",
      "loss: 0.595750  [32000/175341]\n",
      "loss: 0.346605  [33600/175341]\n",
      "loss: 0.517668  [35200/175341]\n",
      "loss: 0.142628  [36800/175341]\n",
      "loss: 0.667455  [38400/175341]\n",
      "loss: 0.397710  [40000/175341]\n",
      "loss: 0.729212  [41600/175341]\n",
      "loss: 0.598211  [43200/175341]\n",
      "loss: 0.245619  [44800/175341]\n",
      "loss: 0.230468  [46400/175341]\n",
      "loss: 0.783454  [48000/175341]\n",
      "loss: 0.360416  [49600/175341]\n",
      "loss: 0.446300  [51200/175341]\n",
      "loss: 0.584842  [52800/175341]\n",
      "loss: 0.161820  [54400/175341]\n",
      "loss: 0.708736  [56000/175341]\n",
      "loss: 0.610019  [57600/175341]\n",
      "loss: 0.371938  [59200/175341]\n",
      "loss: 0.551091  [60800/175341]\n",
      "loss: 0.434110  [62400/175341]\n",
      "loss: 0.435564  [64000/175341]\n",
      "loss: 0.472952  [65600/175341]\n",
      "loss: 0.736856  [67200/175341]\n",
      "loss: 0.232838  [68800/175341]\n",
      "loss: 0.248480  [70400/175341]\n",
      "loss: 0.606747  [72000/175341]\n",
      "loss: 0.845016  [73600/175341]\n",
      "loss: 0.257597  [75200/175341]\n",
      "loss: 0.304797  [76800/175341]\n",
      "loss: 0.521134  [78400/175341]\n",
      "loss: 0.366106  [80000/175341]\n",
      "loss: 0.560736  [81600/175341]\n",
      "loss: 0.247663  [83200/175341]\n",
      "loss: 0.219446  [84800/175341]\n",
      "loss: 0.517827  [86400/175341]\n",
      "loss: 0.788286  [88000/175341]\n",
      "loss: 0.173464  [89600/175341]\n",
      "loss: 0.222046  [91200/175341]\n",
      "loss: 0.362989  [92800/175341]\n",
      "loss: 0.374052  [94400/175341]\n",
      "loss: 0.073045  [96000/175341]\n",
      "loss: 0.266429  [97600/175341]\n",
      "loss: 0.687589  [99200/175341]\n",
      "loss: 0.360265  [100800/175341]\n",
      "loss: 0.292111  [102400/175341]\n",
      "loss: 0.491904  [104000/175341]\n",
      "loss: 0.516731  [105600/175341]\n",
      "loss: 0.569625  [107200/175341]\n",
      "loss: 0.306496  [108800/175341]\n",
      "loss: 0.476390  [110400/175341]\n",
      "loss: 0.441966  [112000/175341]\n",
      "loss: 0.238899  [113600/175341]\n",
      "loss: 0.494359  [115200/175341]\n",
      "loss: 0.360972  [116800/175341]\n",
      "loss: 0.282880  [118400/175341]\n",
      "loss: 0.881471  [120000/175341]\n",
      "loss: 0.588509  [121600/175341]\n",
      "loss: 0.631730  [123200/175341]\n",
      "loss: 0.614474  [124800/175341]\n",
      "loss: 0.326350  [126400/175341]\n",
      "loss: 0.519010  [128000/175341]\n",
      "loss: 0.578819  [129600/175341]\n",
      "loss: 0.445793  [131200/175341]\n",
      "loss: 0.363954  [132800/175341]\n",
      "loss: 0.650810  [134400/175341]\n",
      "loss: 0.334269  [136000/175341]\n",
      "loss: 0.254616  [137600/175341]\n",
      "loss: 0.321213  [139200/175341]\n",
      "loss: 0.556444  [140800/175341]\n",
      "loss: 0.592544  [142400/175341]\n",
      "loss: 0.625890  [144000/175341]\n",
      "loss: 0.344770  [145600/175341]\n",
      "loss: 0.607006  [147200/175341]\n",
      "loss: 0.583754  [148800/175341]\n",
      "loss: 0.764451  [150400/175341]\n",
      "loss: 0.767780  [152000/175341]\n",
      "loss: 0.374116  [153600/175341]\n",
      "loss: 0.521475  [155200/175341]\n",
      "loss: 0.534586  [156800/175341]\n",
      "loss: 0.296355  [158400/175341]\n",
      "loss: 0.644782  [160000/175341]\n",
      "loss: 0.611041  [161600/175341]\n",
      "loss: 0.319570  [163200/175341]\n",
      "loss: 0.585605  [164800/175341]\n",
      "loss: 0.362104  [166400/175341]\n",
      "loss: 0.116282  [168000/175341]\n",
      "loss: 0.442267  [169600/175341]\n",
      "loss: 0.299915  [171200/175341]\n",
      "loss: 0.295783  [172800/175341]\n",
      "loss: 0.857687  [174400/175341]\n",
      "Train Accuracy: 80.8864%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.542685, F1-score: 75.44%, Macro_F1-Score:  41.69%  \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.927050  [    0/175341]\n",
      "loss: 0.630210  [ 1600/175341]\n",
      "loss: 0.255859  [ 3200/175341]\n",
      "loss: 0.715739  [ 4800/175341]\n",
      "loss: 0.294179  [ 6400/175341]\n",
      "loss: 0.504793  [ 8000/175341]\n",
      "loss: 0.525900  [ 9600/175341]\n",
      "loss: 0.454188  [11200/175341]\n",
      "loss: 0.635619  [12800/175341]\n",
      "loss: 0.545519  [14400/175341]\n",
      "loss: 0.394599  [16000/175341]\n",
      "loss: 0.362425  [17600/175341]\n",
      "loss: 0.431911  [19200/175341]\n",
      "loss: 0.448092  [20800/175341]\n",
      "loss: 0.338364  [22400/175341]\n",
      "loss: 0.212084  [24000/175341]\n",
      "loss: 0.375850  [25600/175341]\n",
      "loss: 0.489125  [27200/175341]\n",
      "loss: 0.489291  [28800/175341]\n",
      "loss: 0.452336  [30400/175341]\n",
      "loss: 0.286055  [32000/175341]\n",
      "loss: 0.289138  [33600/175341]\n",
      "loss: 0.264677  [35200/175341]\n",
      "loss: 1.135546  [36800/175341]\n",
      "loss: 0.580066  [38400/175341]\n",
      "loss: 0.983025  [40000/175341]\n",
      "loss: 0.490645  [41600/175341]\n",
      "loss: 0.949846  [43200/175341]\n",
      "loss: 0.468834  [44800/175341]\n",
      "loss: 0.309924  [46400/175341]\n",
      "loss: 0.213083  [48000/175341]\n",
      "loss: 0.624877  [49600/175341]\n",
      "loss: 0.337097  [51200/175341]\n",
      "loss: 0.889933  [52800/175341]\n",
      "loss: 0.747034  [54400/175341]\n",
      "loss: 0.351373  [56000/175341]\n",
      "loss: 0.402188  [57600/175341]\n",
      "loss: 0.597320  [59200/175341]\n",
      "loss: 0.474267  [60800/175341]\n",
      "loss: 0.444844  [62400/175341]\n",
      "loss: 0.899079  [64000/175341]\n",
      "loss: 0.354282  [65600/175341]\n",
      "loss: 0.555228  [67200/175341]\n",
      "loss: 0.899710  [68800/175341]\n",
      "loss: 0.419963  [70400/175341]\n",
      "loss: 0.266107  [72000/175341]\n",
      "loss: 0.563182  [73600/175341]\n",
      "loss: 0.378951  [75200/175341]\n",
      "loss: 0.545238  [76800/175341]\n",
      "loss: 0.944222  [78400/175341]\n",
      "loss: 0.214762  [80000/175341]\n",
      "loss: 0.365207  [81600/175341]\n",
      "loss: 0.392481  [83200/175341]\n",
      "loss: 0.569317  [84800/175341]\n",
      "loss: 0.824782  [86400/175341]\n",
      "loss: 0.395931  [88000/175341]\n",
      "loss: 0.351643  [89600/175341]\n",
      "loss: 0.579619  [91200/175341]\n",
      "loss: 0.193138  [92800/175341]\n",
      "loss: 0.286980  [94400/175341]\n",
      "loss: 0.314036  [96000/175341]\n",
      "loss: 0.528698  [97600/175341]\n",
      "loss: 0.334987  [99200/175341]\n",
      "loss: 0.480976  [100800/175341]\n",
      "loss: 0.429344  [102400/175341]\n",
      "loss: 0.569063  [104000/175341]\n",
      "loss: 0.360667  [105600/175341]\n",
      "loss: 0.262181  [107200/175341]\n",
      "loss: 0.269324  [108800/175341]\n",
      "loss: 0.442762  [110400/175341]\n",
      "loss: 0.318537  [112000/175341]\n",
      "loss: 0.648107  [113600/175341]\n",
      "loss: 0.235525  [115200/175341]\n",
      "loss: 0.357650  [116800/175341]\n",
      "loss: 0.592353  [118400/175341]\n",
      "loss: 0.895388  [120000/175341]\n",
      "loss: 0.326706  [121600/175341]\n",
      "loss: 0.314306  [123200/175341]\n",
      "loss: 0.739333  [124800/175341]\n",
      "loss: 0.610545  [126400/175341]\n",
      "loss: 0.502842  [128000/175341]\n",
      "loss: 0.651375  [129600/175341]\n",
      "loss: 0.419005  [131200/175341]\n",
      "loss: 0.568324  [132800/175341]\n",
      "loss: 0.285126  [134400/175341]\n",
      "loss: 0.441322  [136000/175341]\n",
      "loss: 0.480325  [137600/175341]\n",
      "loss: 0.622063  [139200/175341]\n",
      "loss: 0.455308  [140800/175341]\n",
      "loss: 0.537294  [142400/175341]\n",
      "loss: 0.335303  [144000/175341]\n",
      "loss: 0.744096  [145600/175341]\n",
      "loss: 0.285612  [147200/175341]\n",
      "loss: 0.266642  [148800/175341]\n",
      "loss: 0.501549  [150400/175341]\n",
      "loss: 0.420077  [152000/175341]\n",
      "loss: 0.407413  [153600/175341]\n",
      "loss: 0.217330  [155200/175341]\n",
      "loss: 0.742996  [156800/175341]\n",
      "loss: 0.406120  [158400/175341]\n",
      "loss: 0.781470  [160000/175341]\n",
      "loss: 0.576762  [161600/175341]\n",
      "loss: 0.614439  [163200/175341]\n",
      "loss: 0.347630  [164800/175341]\n",
      "loss: 0.145073  [166400/175341]\n",
      "loss: 0.439357  [168000/175341]\n",
      "loss: 0.508980  [169600/175341]\n",
      "loss: 0.594654  [171200/175341]\n",
      "loss: 0.271617  [172800/175341]\n",
      "loss: 0.588210  [174400/175341]\n",
      "Train Accuracy: 80.9446%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.536656, F1-score: 76.85%, Macro_F1-Score:  42.77%  \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.148642  [    0/175341]\n",
      "loss: 0.356868  [ 1600/175341]\n",
      "loss: 0.355609  [ 3200/175341]\n",
      "loss: 0.331423  [ 4800/175341]\n",
      "loss: 0.599057  [ 6400/175341]\n",
      "loss: 0.752341  [ 8000/175341]\n",
      "loss: 0.666456  [ 9600/175341]\n",
      "loss: 0.550245  [11200/175341]\n",
      "loss: 0.189238  [12800/175341]\n",
      "loss: 0.574098  [14400/175341]\n",
      "loss: 0.325488  [16000/175341]\n",
      "loss: 0.783249  [17600/175341]\n",
      "loss: 0.137172  [19200/175341]\n",
      "loss: 0.253568  [20800/175341]\n",
      "loss: 0.435333  [22400/175341]\n",
      "loss: 0.639340  [24000/175341]\n",
      "loss: 0.481022  [25600/175341]\n",
      "loss: 0.801983  [27200/175341]\n",
      "loss: 0.136602  [28800/175341]\n",
      "loss: 0.646258  [30400/175341]\n",
      "loss: 0.144168  [32000/175341]\n",
      "loss: 0.185188  [33600/175341]\n",
      "loss: 0.570152  [35200/175341]\n",
      "loss: 1.144553  [36800/175341]\n",
      "loss: 0.784675  [38400/175341]\n",
      "loss: 0.290549  [40000/175341]\n",
      "loss: 0.280327  [41600/175341]\n",
      "loss: 0.186880  [43200/175341]\n",
      "loss: 0.347995  [44800/175341]\n",
      "loss: 0.159235  [46400/175341]\n",
      "loss: 0.743548  [48000/175341]\n",
      "loss: 0.989085  [49600/175341]\n",
      "loss: 0.539439  [51200/175341]\n",
      "loss: 0.486733  [52800/175341]\n",
      "loss: 0.459205  [54400/175341]\n",
      "loss: 0.814585  [56000/175341]\n",
      "loss: 0.288995  [57600/175341]\n",
      "loss: 0.369524  [59200/175341]\n",
      "loss: 0.607932  [60800/175341]\n",
      "loss: 0.327798  [62400/175341]\n",
      "loss: 0.733955  [64000/175341]\n",
      "loss: 0.669818  [65600/175341]\n",
      "loss: 0.487950  [67200/175341]\n",
      "loss: 0.263973  [68800/175341]\n",
      "loss: 0.291328  [70400/175341]\n",
      "loss: 0.461392  [72000/175341]\n",
      "loss: 0.285473  [73600/175341]\n",
      "loss: 0.681640  [75200/175341]\n",
      "loss: 0.627582  [76800/175341]\n",
      "loss: 0.396820  [78400/175341]\n",
      "loss: 0.602395  [80000/175341]\n",
      "loss: 0.244034  [81600/175341]\n",
      "loss: 0.531468  [83200/175341]\n",
      "loss: 0.576905  [84800/175341]\n",
      "loss: 0.744162  [86400/175341]\n",
      "loss: 0.389651  [88000/175341]\n",
      "loss: 1.206476  [89600/175341]\n",
      "loss: 0.740090  [91200/175341]\n",
      "loss: 0.401954  [92800/175341]\n",
      "loss: 0.522405  [94400/175341]\n",
      "loss: 0.684002  [96000/175341]\n",
      "loss: 0.569990  [97600/175341]\n",
      "loss: 0.092817  [99200/175341]\n",
      "loss: 0.589854  [100800/175341]\n",
      "loss: 0.541851  [102400/175341]\n",
      "loss: 0.486514  [104000/175341]\n",
      "loss: 0.752170  [105600/175341]\n",
      "loss: 0.406487  [107200/175341]\n",
      "loss: 1.000156  [108800/175341]\n",
      "loss: 0.563879  [110400/175341]\n",
      "loss: 0.371131  [112000/175341]\n",
      "loss: 0.305375  [113600/175341]\n",
      "loss: 0.653521  [115200/175341]\n",
      "loss: 0.486260  [116800/175341]\n",
      "loss: 0.326053  [118400/175341]\n",
      "loss: 0.232677  [120000/175341]\n",
      "loss: 0.271092  [121600/175341]\n",
      "loss: 0.718348  [123200/175341]\n",
      "loss: 0.687335  [124800/175341]\n",
      "loss: 1.012693  [126400/175341]\n",
      "loss: 0.241809  [128000/175341]\n",
      "loss: 0.332704  [129600/175341]\n",
      "loss: 0.598510  [131200/175341]\n",
      "loss: 0.546858  [132800/175341]\n",
      "loss: 0.422035  [134400/175341]\n",
      "loss: 0.734022  [136000/175341]\n",
      "loss: 0.556855  [137600/175341]\n",
      "loss: 0.729763  [139200/175341]\n",
      "loss: 0.408384  [140800/175341]\n",
      "loss: 0.444710  [142400/175341]\n",
      "loss: 0.219759  [144000/175341]\n",
      "loss: 0.448517  [145600/175341]\n",
      "loss: 0.540093  [147200/175341]\n",
      "loss: 0.219996  [148800/175341]\n",
      "loss: 0.518618  [150400/175341]\n",
      "loss: 0.391411  [152000/175341]\n",
      "loss: 0.959952  [153600/175341]\n",
      "loss: 0.259625  [155200/175341]\n",
      "loss: 0.586503  [156800/175341]\n",
      "loss: 0.338915  [158400/175341]\n",
      "loss: 0.389041  [160000/175341]\n",
      "loss: 0.363145  [161600/175341]\n",
      "loss: 0.663624  [163200/175341]\n",
      "loss: 0.234972  [164800/175341]\n",
      "loss: 0.512888  [166400/175341]\n",
      "loss: 0.332417  [168000/175341]\n",
      "loss: 0.778589  [169600/175341]\n",
      "loss: 0.404831  [171200/175341]\n",
      "loss: 0.256241  [172800/175341]\n",
      "loss: 0.297574  [174400/175341]\n",
      "Train Accuracy: 80.9274%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.548158, F1-score: 75.26%, Macro_F1-Score:  40.85%  \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.270218  [    0/175341]\n",
      "loss: 0.869795  [ 1600/175341]\n",
      "loss: 0.243926  [ 3200/175341]\n",
      "loss: 0.450642  [ 4800/175341]\n",
      "loss: 0.600854  [ 6400/175341]\n",
      "loss: 0.446576  [ 8000/175341]\n",
      "loss: 0.510516  [ 9600/175341]\n",
      "loss: 0.632507  [11200/175341]\n",
      "loss: 0.428083  [12800/175341]\n",
      "loss: 0.406906  [14400/175341]\n",
      "loss: 0.548253  [16000/175341]\n",
      "loss: 0.631290  [17600/175341]\n",
      "loss: 0.816320  [19200/175341]\n",
      "loss: 0.354313  [20800/175341]\n",
      "loss: 0.253906  [22400/175341]\n",
      "loss: 0.649335  [24000/175341]\n",
      "loss: 0.442539  [25600/175341]\n",
      "loss: 0.699401  [27200/175341]\n",
      "loss: 0.514490  [28800/175341]\n",
      "loss: 0.789506  [30400/175341]\n",
      "loss: 0.310380  [32000/175341]\n",
      "loss: 0.242222  [33600/175341]\n",
      "loss: 0.579295  [35200/175341]\n",
      "loss: 0.394149  [36800/175341]\n",
      "loss: 0.219937  [38400/175341]\n",
      "loss: 0.071142  [40000/175341]\n",
      "loss: 0.628271  [41600/175341]\n",
      "loss: 0.656389  [43200/175341]\n",
      "loss: 0.699964  [44800/175341]\n",
      "loss: 0.126073  [46400/175341]\n",
      "loss: 0.826280  [48000/175341]\n",
      "loss: 0.497424  [49600/175341]\n",
      "loss: 0.271047  [51200/175341]\n",
      "loss: 0.552673  [52800/175341]\n",
      "loss: 1.063773  [54400/175341]\n",
      "loss: 0.338816  [56000/175341]\n",
      "loss: 0.661441  [57600/175341]\n",
      "loss: 0.590107  [59200/175341]\n",
      "loss: 0.579895  [60800/175341]\n",
      "loss: 0.300392  [62400/175341]\n",
      "loss: 0.470397  [64000/175341]\n",
      "loss: 0.221628  [65600/175341]\n",
      "loss: 0.432866  [67200/175341]\n",
      "loss: 0.516373  [68800/175341]\n",
      "loss: 0.526401  [70400/175341]\n",
      "loss: 0.602918  [72000/175341]\n",
      "loss: 0.216017  [73600/175341]\n",
      "loss: 0.423863  [75200/175341]\n",
      "loss: 0.916995  [76800/175341]\n",
      "loss: 0.584157  [78400/175341]\n",
      "loss: 0.282204  [80000/175341]\n",
      "loss: 0.668886  [81600/175341]\n",
      "loss: 0.373972  [83200/175341]\n",
      "loss: 0.258410  [84800/175341]\n",
      "loss: 0.397980  [86400/175341]\n",
      "loss: 0.692083  [88000/175341]\n",
      "loss: 0.679651  [89600/175341]\n",
      "loss: 0.129060  [91200/175341]\n",
      "loss: 0.498550  [92800/175341]\n",
      "loss: 0.451998  [94400/175341]\n",
      "loss: 0.388777  [96000/175341]\n",
      "loss: 0.517083  [97600/175341]\n",
      "loss: 0.162645  [99200/175341]\n",
      "loss: 0.789328  [100800/175341]\n",
      "loss: 0.411107  [102400/175341]\n",
      "loss: 0.895463  [104000/175341]\n",
      "loss: 0.535344  [105600/175341]\n",
      "loss: 0.746326  [107200/175341]\n",
      "loss: 0.446195  [108800/175341]\n",
      "loss: 0.376000  [110400/175341]\n",
      "loss: 0.792520  [112000/175341]\n",
      "loss: 0.396811  [113600/175341]\n",
      "loss: 0.451757  [115200/175341]\n",
      "loss: 0.366708  [116800/175341]\n",
      "loss: 0.813258  [118400/175341]\n",
      "loss: 0.540056  [120000/175341]\n",
      "loss: 0.442074  [121600/175341]\n",
      "loss: 0.323395  [123200/175341]\n",
      "loss: 0.544215  [124800/175341]\n",
      "loss: 0.311675  [126400/175341]\n",
      "loss: 0.362178  [128000/175341]\n",
      "loss: 0.436293  [129600/175341]\n",
      "loss: 0.359659  [131200/175341]\n",
      "loss: 0.752039  [132800/175341]\n",
      "loss: 0.350950  [134400/175341]\n",
      "loss: 0.646115  [136000/175341]\n",
      "loss: 0.292129  [137600/175341]\n",
      "loss: 0.200087  [139200/175341]\n",
      "loss: 0.568843  [140800/175341]\n",
      "loss: 0.829928  [142400/175341]\n",
      "loss: 0.268110  [144000/175341]\n",
      "loss: 0.392047  [145600/175341]\n",
      "loss: 0.477086  [147200/175341]\n",
      "loss: 0.336164  [148800/175341]\n",
      "loss: 0.288999  [150400/175341]\n",
      "loss: 0.312474  [152000/175341]\n",
      "loss: 0.294681  [153600/175341]\n",
      "loss: 0.400342  [155200/175341]\n",
      "loss: 0.542723  [156800/175341]\n",
      "loss: 0.110787  [158400/175341]\n",
      "loss: 0.196590  [160000/175341]\n",
      "loss: 0.519788  [161600/175341]\n",
      "loss: 0.476026  [163200/175341]\n",
      "loss: 0.598437  [164800/175341]\n",
      "loss: 0.675620  [166400/175341]\n",
      "loss: 0.379777  [168000/175341]\n",
      "loss: 0.408818  [169600/175341]\n",
      "loss: 0.947529  [171200/175341]\n",
      "loss: 0.458790  [172800/175341]\n",
      "loss: 1.038868  [174400/175341]\n",
      "Train Accuracy: 80.9611%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.556124, F1-score: 75.52%, Macro_F1-Score:  42.20%  \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.510926  [    0/175341]\n",
      "loss: 0.306182  [ 1600/175341]\n",
      "loss: 0.374260  [ 3200/175341]\n",
      "loss: 0.478150  [ 4800/175341]\n",
      "loss: 0.459939  [ 6400/175341]\n",
      "loss: 0.701131  [ 8000/175341]\n",
      "loss: 0.275298  [ 9600/175341]\n",
      "loss: 0.259955  [11200/175341]\n",
      "loss: 0.545720  [12800/175341]\n",
      "loss: 0.391316  [14400/175341]\n",
      "loss: 0.286563  [16000/175341]\n",
      "loss: 0.754100  [17600/175341]\n",
      "loss: 0.338947  [19200/175341]\n",
      "loss: 0.535726  [20800/175341]\n",
      "loss: 0.439696  [22400/175341]\n",
      "loss: 0.262134  [24000/175341]\n",
      "loss: 0.549436  [25600/175341]\n",
      "loss: 0.389794  [27200/175341]\n",
      "loss: 0.274377  [28800/175341]\n",
      "loss: 0.625753  [30400/175341]\n",
      "loss: 0.322964  [32000/175341]\n",
      "loss: 0.735523  [33600/175341]\n",
      "loss: 0.321415  [35200/175341]\n",
      "loss: 0.235674  [36800/175341]\n",
      "loss: 0.279084  [38400/175341]\n",
      "loss: 0.640072  [40000/175341]\n",
      "loss: 1.114017  [41600/175341]\n",
      "loss: 0.439755  [43200/175341]\n",
      "loss: 0.775722  [44800/175341]\n",
      "loss: 0.412783  [46400/175341]\n",
      "loss: 0.528159  [48000/175341]\n",
      "loss: 0.632124  [49600/175341]\n",
      "loss: 0.441494  [51200/175341]\n",
      "loss: 0.470074  [52800/175341]\n",
      "loss: 0.460419  [54400/175341]\n",
      "loss: 0.657911  [56000/175341]\n",
      "loss: 0.108507  [57600/175341]\n",
      "loss: 0.697091  [59200/175341]\n",
      "loss: 0.552757  [60800/175341]\n",
      "loss: 0.397635  [62400/175341]\n",
      "loss: 0.725450  [64000/175341]\n",
      "loss: 0.664352  [65600/175341]\n",
      "loss: 0.458285  [67200/175341]\n",
      "loss: 0.862720  [68800/175341]\n",
      "loss: 0.358535  [70400/175341]\n",
      "loss: 0.783442  [72000/175341]\n",
      "loss: 0.453067  [73600/175341]\n",
      "loss: 0.377555  [75200/175341]\n",
      "loss: 0.676106  [76800/175341]\n",
      "loss: 0.776651  [78400/175341]\n",
      "loss: 0.596874  [80000/175341]\n",
      "loss: 0.238404  [81600/175341]\n",
      "loss: 0.747484  [83200/175341]\n",
      "loss: 0.382663  [84800/175341]\n",
      "loss: 0.627161  [86400/175341]\n",
      "loss: 0.250914  [88000/175341]\n",
      "loss: 0.690424  [89600/175341]\n",
      "loss: 0.997402  [91200/175341]\n",
      "loss: 0.418401  [92800/175341]\n",
      "loss: 0.324503  [94400/175341]\n",
      "loss: 0.287714  [96000/175341]\n",
      "loss: 0.301445  [97600/175341]\n",
      "loss: 0.572202  [99200/175341]\n",
      "loss: 0.356374  [100800/175341]\n",
      "loss: 0.677826  [102400/175341]\n",
      "loss: 0.744379  [104000/175341]\n",
      "loss: 0.419388  [105600/175341]\n",
      "loss: 0.759471  [107200/175341]\n",
      "loss: 0.659101  [108800/175341]\n",
      "loss: 0.675455  [110400/175341]\n",
      "loss: 0.320373  [112000/175341]\n",
      "loss: 0.281156  [113600/175341]\n",
      "loss: 0.316576  [115200/175341]\n",
      "loss: 0.405999  [116800/175341]\n",
      "loss: 0.827958  [118400/175341]\n",
      "loss: 0.540363  [120000/175341]\n",
      "loss: 0.476532  [121600/175341]\n",
      "loss: 0.356819  [123200/175341]\n",
      "loss: 0.926721  [124800/175341]\n",
      "loss: 0.495113  [126400/175341]\n",
      "loss: 0.185446  [128000/175341]\n",
      "loss: 0.577518  [129600/175341]\n",
      "loss: 0.253687  [131200/175341]\n",
      "loss: 0.493924  [132800/175341]\n",
      "loss: 0.677042  [134400/175341]\n",
      "loss: 0.400221  [136000/175341]\n",
      "loss: 0.395839  [137600/175341]\n",
      "loss: 0.241522  [139200/175341]\n",
      "loss: 0.625999  [140800/175341]\n",
      "loss: 0.591422  [142400/175341]\n",
      "loss: 0.365288  [144000/175341]\n",
      "loss: 0.778497  [145600/175341]\n",
      "loss: 0.477619  [147200/175341]\n",
      "loss: 0.264297  [148800/175341]\n",
      "loss: 0.262428  [150400/175341]\n",
      "loss: 0.326436  [152000/175341]\n",
      "loss: 0.692270  [153600/175341]\n",
      "loss: 0.637923  [155200/175341]\n",
      "loss: 0.398054  [156800/175341]\n",
      "loss: 0.662503  [158400/175341]\n",
      "loss: 0.383411  [160000/175341]\n",
      "loss: 0.315705  [161600/175341]\n",
      "loss: 0.339038  [163200/175341]\n",
      "loss: 0.811521  [164800/175341]\n",
      "loss: 0.568939  [166400/175341]\n",
      "loss: 0.405666  [168000/175341]\n",
      "loss: 0.274518  [169600/175341]\n",
      "loss: 0.312456  [171200/175341]\n",
      "loss: 0.555961  [172800/175341]\n",
      "loss: 0.748555  [174400/175341]\n",
      "Train Accuracy: 80.9292%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.542833, F1-score: 76.20%, Macro_F1-Score:  42.18%  \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.160509  [    0/175341]\n",
      "loss: 0.628867  [ 1600/175341]\n",
      "loss: 0.954527  [ 3200/175341]\n",
      "loss: 0.330878  [ 4800/175341]\n",
      "loss: 0.289607  [ 6400/175341]\n",
      "loss: 0.190556  [ 8000/175341]\n",
      "loss: 0.432383  [ 9600/175341]\n",
      "loss: 0.507053  [11200/175341]\n",
      "loss: 0.459077  [12800/175341]\n",
      "loss: 0.874005  [14400/175341]\n",
      "loss: 0.807990  [16000/175341]\n",
      "loss: 0.598728  [17600/175341]\n",
      "loss: 0.344266  [19200/175341]\n",
      "loss: 0.546935  [20800/175341]\n",
      "loss: 0.172928  [22400/175341]\n",
      "loss: 0.417113  [24000/175341]\n",
      "loss: 0.520318  [25600/175341]\n",
      "loss: 0.321791  [27200/175341]\n",
      "loss: 0.553958  [28800/175341]\n",
      "loss: 0.651098  [30400/175341]\n",
      "loss: 0.143769  [32000/175341]\n",
      "loss: 0.400580  [33600/175341]\n",
      "loss: 0.204511  [35200/175341]\n",
      "loss: 0.559511  [36800/175341]\n",
      "loss: 0.313954  [38400/175341]\n",
      "loss: 0.296071  [40000/175341]\n",
      "loss: 0.366354  [41600/175341]\n",
      "loss: 0.594443  [43200/175341]\n",
      "loss: 0.264933  [44800/175341]\n",
      "loss: 0.678547  [46400/175341]\n",
      "loss: 0.299222  [48000/175341]\n",
      "loss: 0.528779  [49600/175341]\n",
      "loss: 0.360587  [51200/175341]\n",
      "loss: 0.333074  [52800/175341]\n",
      "loss: 0.456062  [54400/175341]\n",
      "loss: 0.713630  [56000/175341]\n",
      "loss: 0.590751  [57600/175341]\n",
      "loss: 0.449284  [59200/175341]\n",
      "loss: 0.451373  [60800/175341]\n",
      "loss: 0.642818  [62400/175341]\n",
      "loss: 0.561372  [64000/175341]\n",
      "loss: 0.226838  [65600/175341]\n",
      "loss: 0.228957  [67200/175341]\n",
      "loss: 0.145474  [68800/175341]\n",
      "loss: 0.645519  [70400/175341]\n",
      "loss: 0.282600  [72000/175341]\n",
      "loss: 0.569114  [73600/175341]\n",
      "loss: 0.657469  [75200/175341]\n",
      "loss: 0.592442  [76800/175341]\n",
      "loss: 0.549364  [78400/175341]\n",
      "loss: 0.355591  [80000/175341]\n",
      "loss: 0.099839  [81600/175341]\n",
      "loss: 0.453552  [83200/175341]\n",
      "loss: 0.224622  [84800/175341]\n",
      "loss: 0.387674  [86400/175341]\n",
      "loss: 0.451587  [88000/175341]\n",
      "loss: 0.604334  [89600/175341]\n",
      "loss: 0.947066  [91200/175341]\n",
      "loss: 0.208637  [92800/175341]\n",
      "loss: 0.564637  [94400/175341]\n",
      "loss: 0.491761  [96000/175341]\n",
      "loss: 0.558799  [97600/175341]\n",
      "loss: 0.543702  [99200/175341]\n",
      "loss: 0.532344  [100800/175341]\n",
      "loss: 0.784080  [102400/175341]\n",
      "loss: 0.270501  [104000/175341]\n",
      "loss: 0.459967  [105600/175341]\n",
      "loss: 0.503591  [107200/175341]\n",
      "loss: 0.456424  [108800/175341]\n",
      "loss: 0.249442  [110400/175341]\n",
      "loss: 0.398442  [112000/175341]\n",
      "loss: 0.497300  [113600/175341]\n",
      "loss: 0.612060  [115200/175341]\n",
      "loss: 0.447191  [116800/175341]\n",
      "loss: 0.718242  [118400/175341]\n",
      "loss: 0.625177  [120000/175341]\n",
      "loss: 0.294976  [121600/175341]\n",
      "loss: 0.606871  [123200/175341]\n",
      "loss: 0.537310  [124800/175341]\n",
      "loss: 0.222860  [126400/175341]\n",
      "loss: 0.765170  [128000/175341]\n",
      "loss: 0.929407  [129600/175341]\n",
      "loss: 0.288958  [131200/175341]\n",
      "loss: 0.334178  [132800/175341]\n",
      "loss: 0.442815  [134400/175341]\n",
      "loss: 0.214910  [136000/175341]\n",
      "loss: 0.300020  [137600/175341]\n",
      "loss: 0.606657  [139200/175341]\n",
      "loss: 0.537904  [140800/175341]\n",
      "loss: 0.584070  [142400/175341]\n",
      "loss: 0.210671  [144000/175341]\n",
      "loss: 0.387069  [145600/175341]\n",
      "loss: 0.686929  [147200/175341]\n",
      "loss: 0.319928  [148800/175341]\n",
      "loss: 0.311121  [150400/175341]\n",
      "loss: 0.399705  [152000/175341]\n",
      "loss: 0.240319  [153600/175341]\n",
      "loss: 0.450931  [155200/175341]\n",
      "loss: 0.303537  [156800/175341]\n",
      "loss: 0.388017  [158400/175341]\n",
      "loss: 0.585396  [160000/175341]\n",
      "loss: 0.244109  [161600/175341]\n",
      "loss: 0.455936  [163200/175341]\n",
      "loss: 0.315580  [164800/175341]\n",
      "loss: 0.738537  [166400/175341]\n",
      "loss: 0.422399  [168000/175341]\n",
      "loss: 0.620776  [169600/175341]\n",
      "loss: 0.625457  [171200/175341]\n",
      "loss: 0.268183  [172800/175341]\n",
      "loss: 0.779847  [174400/175341]\n",
      "Train Accuracy: 80.9303%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.549800, F1-score: 76.05%, Macro_F1-Score:  42.10%  \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.784074  [    0/175341]\n",
      "loss: 0.613161  [ 1600/175341]\n",
      "loss: 0.738438  [ 3200/175341]\n",
      "loss: 0.248190  [ 4800/175341]\n",
      "loss: 0.500487  [ 6400/175341]\n",
      "loss: 0.477704  [ 8000/175341]\n",
      "loss: 0.299884  [ 9600/175341]\n",
      "loss: 0.382268  [11200/175341]\n",
      "loss: 0.799644  [12800/175341]\n",
      "loss: 0.419774  [14400/175341]\n",
      "loss: 0.851835  [16000/175341]\n",
      "loss: 0.303951  [17600/175341]\n",
      "loss: 0.191788  [19200/175341]\n",
      "loss: 0.387383  [20800/175341]\n",
      "loss: 0.104284  [22400/175341]\n",
      "loss: 0.229073  [24000/175341]\n",
      "loss: 0.725324  [25600/175341]\n",
      "loss: 0.233851  [27200/175341]\n",
      "loss: 0.375634  [28800/175341]\n",
      "loss: 0.524584  [30400/175341]\n",
      "loss: 0.369676  [32000/175341]\n",
      "loss: 0.257535  [33600/175341]\n",
      "loss: 0.363881  [35200/175341]\n",
      "loss: 0.793760  [36800/175341]\n",
      "loss: 0.262045  [38400/175341]\n",
      "loss: 0.274553  [40000/175341]\n",
      "loss: 0.382852  [41600/175341]\n",
      "loss: 0.651400  [43200/175341]\n",
      "loss: 1.258938  [44800/175341]\n",
      "loss: 1.096205  [46400/175341]\n",
      "loss: 0.724922  [48000/175341]\n",
      "loss: 0.690113  [49600/175341]\n",
      "loss: 0.539005  [51200/175341]\n",
      "loss: 0.417233  [52800/175341]\n",
      "loss: 0.501636  [54400/175341]\n",
      "loss: 0.193444  [56000/175341]\n",
      "loss: 0.126070  [57600/175341]\n",
      "loss: 0.529760  [59200/175341]\n",
      "loss: 0.653513  [60800/175341]\n",
      "loss: 0.541701  [62400/175341]\n",
      "loss: 0.275455  [64000/175341]\n",
      "loss: 0.179347  [65600/175341]\n",
      "loss: 0.759861  [67200/175341]\n",
      "loss: 0.966983  [68800/175341]\n",
      "loss: 0.728838  [70400/175341]\n",
      "loss: 0.290441  [72000/175341]\n",
      "loss: 0.639074  [73600/175341]\n",
      "loss: 0.412224  [75200/175341]\n",
      "loss: 0.435438  [76800/175341]\n",
      "loss: 0.660397  [78400/175341]\n",
      "loss: 0.478946  [80000/175341]\n",
      "loss: 0.700931  [81600/175341]\n",
      "loss: 0.723395  [83200/175341]\n",
      "loss: 0.201027  [84800/175341]\n",
      "loss: 0.345521  [86400/175341]\n",
      "loss: 0.494368  [88000/175341]\n",
      "loss: 0.652813  [89600/175341]\n",
      "loss: 0.247104  [91200/175341]\n",
      "loss: 0.921477  [92800/175341]\n",
      "loss: 0.270693  [94400/175341]\n",
      "loss: 0.348704  [96000/175341]\n",
      "loss: 0.418474  [97600/175341]\n",
      "loss: 0.368359  [99200/175341]\n",
      "loss: 0.750576  [100800/175341]\n",
      "loss: 0.236987  [102400/175341]\n",
      "loss: 0.623888  [104000/175341]\n",
      "loss: 0.513607  [105600/175341]\n",
      "loss: 0.317800  [107200/175341]\n",
      "loss: 0.365679  [108800/175341]\n",
      "loss: 0.362994  [110400/175341]\n",
      "loss: 0.112028  [112000/175341]\n",
      "loss: 0.388029  [113600/175341]\n",
      "loss: 0.898159  [115200/175341]\n",
      "loss: 0.148127  [116800/175341]\n",
      "loss: 0.500521  [118400/175341]\n",
      "loss: 0.529072  [120000/175341]\n",
      "loss: 0.540202  [121600/175341]\n",
      "loss: 0.351559  [123200/175341]\n",
      "loss: 0.928190  [124800/175341]\n",
      "loss: 0.223025  [126400/175341]\n",
      "loss: 0.656691  [128000/175341]\n",
      "loss: 0.542326  [129600/175341]\n",
      "loss: 0.693731  [131200/175341]\n",
      "loss: 0.180760  [132800/175341]\n",
      "loss: 0.580635  [134400/175341]\n",
      "loss: 0.391296  [136000/175341]\n",
      "loss: 0.422853  [137600/175341]\n",
      "loss: 0.293471  [139200/175341]\n",
      "loss: 0.575003  [140800/175341]\n",
      "loss: 0.734404  [142400/175341]\n",
      "loss: 0.670600  [144000/175341]\n",
      "loss: 0.759374  [145600/175341]\n",
      "loss: 0.534379  [147200/175341]\n",
      "loss: 0.462383  [148800/175341]\n",
      "loss: 0.283453  [150400/175341]\n",
      "loss: 0.500846  [152000/175341]\n",
      "loss: 0.408460  [153600/175341]\n",
      "loss: 0.514494  [155200/175341]\n",
      "loss: 0.608173  [156800/175341]\n",
      "loss: 0.625141  [158400/175341]\n",
      "loss: 0.772440  [160000/175341]\n",
      "loss: 0.460124  [161600/175341]\n",
      "loss: 0.202255  [163200/175341]\n",
      "loss: 0.553503  [164800/175341]\n",
      "loss: 0.658262  [166400/175341]\n",
      "loss: 0.729201  [168000/175341]\n",
      "loss: 0.513164  [169600/175341]\n",
      "loss: 0.446377  [171200/175341]\n",
      "loss: 0.652720  [172800/175341]\n",
      "loss: 0.339966  [174400/175341]\n",
      "Train Accuracy: 80.9902%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.541269, F1-score: 76.55%, Macro_F1-Score:  42.71%  \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.293069  [    0/175341]\n",
      "loss: 0.279930  [ 1600/175341]\n",
      "loss: 0.399614  [ 3200/175341]\n",
      "loss: 0.246212  [ 4800/175341]\n",
      "loss: 0.486883  [ 6400/175341]\n",
      "loss: 0.229470  [ 8000/175341]\n",
      "loss: 0.596793  [ 9600/175341]\n",
      "loss: 0.439715  [11200/175341]\n",
      "loss: 0.587367  [12800/175341]\n",
      "loss: 0.542654  [14400/175341]\n",
      "loss: 0.464142  [16000/175341]\n",
      "loss: 0.858703  [17600/175341]\n",
      "loss: 0.496555  [19200/175341]\n",
      "loss: 0.387089  [20800/175341]\n",
      "loss: 0.419582  [22400/175341]\n",
      "loss: 0.489466  [24000/175341]\n",
      "loss: 0.567203  [25600/175341]\n",
      "loss: 0.359921  [27200/175341]\n",
      "loss: 0.613132  [28800/175341]\n",
      "loss: 0.368991  [30400/175341]\n",
      "loss: 0.889845  [32000/175341]\n",
      "loss: 0.464402  [33600/175341]\n",
      "loss: 0.350610  [35200/175341]\n",
      "loss: 0.274078  [36800/175341]\n",
      "loss: 0.522006  [38400/175341]\n",
      "loss: 0.301598  [40000/175341]\n",
      "loss: 0.459512  [41600/175341]\n",
      "loss: 0.469096  [43200/175341]\n",
      "loss: 0.263381  [44800/175341]\n",
      "loss: 0.543690  [46400/175341]\n",
      "loss: 0.324788  [48000/175341]\n",
      "loss: 0.747131  [49600/175341]\n",
      "loss: 0.342671  [51200/175341]\n",
      "loss: 0.529280  [52800/175341]\n",
      "loss: 0.569524  [54400/175341]\n",
      "loss: 0.326716  [56000/175341]\n",
      "loss: 0.541870  [57600/175341]\n",
      "loss: 0.561322  [59200/175341]\n",
      "loss: 0.453876  [60800/175341]\n",
      "loss: 0.320414  [62400/175341]\n",
      "loss: 0.451030  [64000/175341]\n",
      "loss: 0.330767  [65600/175341]\n",
      "loss: 0.365143  [67200/175341]\n",
      "loss: 0.686119  [68800/175341]\n",
      "loss: 0.363384  [70400/175341]\n",
      "loss: 0.489928  [72000/175341]\n",
      "loss: 0.360562  [73600/175341]\n",
      "loss: 0.198430  [75200/175341]\n",
      "loss: 0.659953  [76800/175341]\n",
      "loss: 0.579394  [78400/175341]\n",
      "loss: 0.850736  [80000/175341]\n",
      "loss: 0.222362  [81600/175341]\n",
      "loss: 0.594675  [83200/175341]\n",
      "loss: 0.577170  [84800/175341]\n",
      "loss: 0.494373  [86400/175341]\n",
      "loss: 0.316357  [88000/175341]\n",
      "loss: 0.389685  [89600/175341]\n",
      "loss: 0.425231  [91200/175341]\n",
      "loss: 0.630715  [92800/175341]\n",
      "loss: 0.356051  [94400/175341]\n",
      "loss: 0.343025  [96000/175341]\n",
      "loss: 0.142340  [97600/175341]\n",
      "loss: 0.759400  [99200/175341]\n",
      "loss: 0.523994  [100800/175341]\n",
      "loss: 0.849373  [102400/175341]\n",
      "loss: 0.294930  [104000/175341]\n",
      "loss: 0.375812  [105600/175341]\n",
      "loss: 0.212173  [107200/175341]\n",
      "loss: 0.374198  [108800/175341]\n",
      "loss: 0.448465  [110400/175341]\n",
      "loss: 0.388236  [112000/175341]\n",
      "loss: 0.656095  [113600/175341]\n",
      "loss: 0.183137  [115200/175341]\n",
      "loss: 0.555478  [116800/175341]\n",
      "loss: 0.829552  [118400/175341]\n",
      "loss: 0.515343  [120000/175341]\n",
      "loss: 0.478430  [121600/175341]\n",
      "loss: 0.361244  [123200/175341]\n",
      "loss: 0.581970  [124800/175341]\n",
      "loss: 0.442809  [126400/175341]\n",
      "loss: 0.638383  [128000/175341]\n",
      "loss: 0.832559  [129600/175341]\n",
      "loss: 0.155177  [131200/175341]\n",
      "loss: 0.480865  [132800/175341]\n",
      "loss: 0.156344  [134400/175341]\n",
      "loss: 0.518953  [136000/175341]\n",
      "loss: 0.652425  [137600/175341]\n",
      "loss: 0.467151  [139200/175341]\n",
      "loss: 0.810979  [140800/175341]\n",
      "loss: 0.660481  [142400/175341]\n",
      "loss: 0.491833  [144000/175341]\n",
      "loss: 0.427204  [145600/175341]\n",
      "loss: 0.495004  [147200/175341]\n",
      "loss: 0.406213  [148800/175341]\n",
      "loss: 0.440922  [150400/175341]\n",
      "loss: 0.322221  [152000/175341]\n",
      "loss: 0.397452  [153600/175341]\n",
      "loss: 0.623548  [155200/175341]\n",
      "loss: 0.422862  [156800/175341]\n",
      "loss: 0.536238  [158400/175341]\n",
      "loss: 0.511506  [160000/175341]\n",
      "loss: 0.309776  [161600/175341]\n",
      "loss: 0.323468  [163200/175341]\n",
      "loss: 0.390822  [164800/175341]\n",
      "loss: 0.385828  [166400/175341]\n",
      "loss: 0.419081  [168000/175341]\n",
      "loss: 0.459027  [169600/175341]\n",
      "loss: 0.423157  [171200/175341]\n",
      "loss: 0.286821  [172800/175341]\n",
      "loss: 0.239210  [174400/175341]\n",
      "Train Accuracy: 80.9428%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.550637, F1-score: 75.49%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.398623  [    0/175341]\n",
      "loss: 0.670104  [ 1600/175341]\n",
      "loss: 0.741733  [ 3200/175341]\n",
      "loss: 0.556980  [ 4800/175341]\n",
      "loss: 0.507722  [ 6400/175341]\n",
      "loss: 1.345122  [ 8000/175341]\n",
      "loss: 0.593848  [ 9600/175341]\n",
      "loss: 0.458476  [11200/175341]\n",
      "loss: 0.311053  [12800/175341]\n",
      "loss: 0.591106  [14400/175341]\n",
      "loss: 0.392944  [16000/175341]\n",
      "loss: 0.296617  [17600/175341]\n",
      "loss: 0.689899  [19200/175341]\n",
      "loss: 0.966339  [20800/175341]\n",
      "loss: 0.142007  [22400/175341]\n",
      "loss: 0.511877  [24000/175341]\n",
      "loss: 0.352912  [25600/175341]\n",
      "loss: 0.486323  [27200/175341]\n",
      "loss: 0.398450  [28800/175341]\n",
      "loss: 0.317039  [30400/175341]\n",
      "loss: 0.198042  [32000/175341]\n",
      "loss: 0.369557  [33600/175341]\n",
      "loss: 0.308666  [35200/175341]\n",
      "loss: 0.134641  [36800/175341]\n",
      "loss: 0.295856  [38400/175341]\n",
      "loss: 0.244506  [40000/175341]\n",
      "loss: 0.849239  [41600/175341]\n",
      "loss: 0.531652  [43200/175341]\n",
      "loss: 0.657083  [44800/175341]\n",
      "loss: 0.483215  [46400/175341]\n",
      "loss: 0.326458  [48000/175341]\n",
      "loss: 0.405114  [49600/175341]\n",
      "loss: 0.350698  [51200/175341]\n",
      "loss: 0.579589  [52800/175341]\n",
      "loss: 0.742761  [54400/175341]\n",
      "loss: 0.487316  [56000/175341]\n",
      "loss: 0.438708  [57600/175341]\n",
      "loss: 0.542849  [59200/175341]\n",
      "loss: 0.239284  [60800/175341]\n",
      "loss: 0.495917  [62400/175341]\n",
      "loss: 0.565547  [64000/175341]\n",
      "loss: 0.385292  [65600/175341]\n",
      "loss: 0.922210  [67200/175341]\n",
      "loss: 0.329502  [68800/175341]\n",
      "loss: 0.791199  [70400/175341]\n",
      "loss: 0.594891  [72000/175341]\n",
      "loss: 0.535417  [73600/175341]\n",
      "loss: 0.543074  [75200/175341]\n",
      "loss: 0.395332  [76800/175341]\n",
      "loss: 0.639391  [78400/175341]\n",
      "loss: 0.307533  [80000/175341]\n",
      "loss: 0.503512  [81600/175341]\n",
      "loss: 0.379790  [83200/175341]\n",
      "loss: 0.319003  [84800/175341]\n",
      "loss: 0.819013  [86400/175341]\n",
      "loss: 0.206630  [88000/175341]\n",
      "loss: 0.264718  [89600/175341]\n",
      "loss: 0.650074  [91200/175341]\n",
      "loss: 0.381562  [92800/175341]\n",
      "loss: 0.194433  [94400/175341]\n",
      "loss: 0.152320  [96000/175341]\n",
      "loss: 0.437477  [97600/175341]\n",
      "loss: 0.257621  [99200/175341]\n",
      "loss: 0.298125  [100800/175341]\n",
      "loss: 0.333726  [102400/175341]\n",
      "loss: 0.182091  [104000/175341]\n",
      "loss: 0.488975  [105600/175341]\n",
      "loss: 0.331262  [107200/175341]\n",
      "loss: 0.218598  [108800/175341]\n",
      "loss: 0.463096  [110400/175341]\n",
      "loss: 0.249663  [112000/175341]\n",
      "loss: 0.496604  [113600/175341]\n",
      "loss: 1.100347  [115200/175341]\n",
      "loss: 0.435167  [116800/175341]\n",
      "loss: 0.280957  [118400/175341]\n",
      "loss: 0.794100  [120000/175341]\n",
      "loss: 0.799997  [121600/175341]\n",
      "loss: 0.666812  [123200/175341]\n",
      "loss: 0.436932  [124800/175341]\n",
      "loss: 0.473461  [126400/175341]\n",
      "loss: 0.303253  [128000/175341]\n",
      "loss: 0.687493  [129600/175341]\n",
      "loss: 0.165335  [131200/175341]\n",
      "loss: 0.511231  [132800/175341]\n",
      "loss: 0.474659  [134400/175341]\n",
      "loss: 0.489815  [136000/175341]\n",
      "loss: 0.830589  [137600/175341]\n",
      "loss: 0.479232  [139200/175341]\n",
      "loss: 0.408115  [140800/175341]\n",
      "loss: 0.534315  [142400/175341]\n",
      "loss: 0.533713  [144000/175341]\n",
      "loss: 0.882195  [145600/175341]\n",
      "loss: 0.304445  [147200/175341]\n",
      "loss: 0.183087  [148800/175341]\n",
      "loss: 0.359569  [150400/175341]\n",
      "loss: 0.726661  [152000/175341]\n",
      "loss: 0.549279  [153600/175341]\n",
      "loss: 0.391121  [155200/175341]\n",
      "loss: 0.076791  [156800/175341]\n",
      "loss: 0.491951  [158400/175341]\n",
      "loss: 0.314196  [160000/175341]\n",
      "loss: 0.667425  [161600/175341]\n",
      "loss: 0.508731  [163200/175341]\n",
      "loss: 1.224553  [164800/175341]\n",
      "loss: 0.157021  [166400/175341]\n",
      "loss: 0.284085  [168000/175341]\n",
      "loss: 0.272092  [169600/175341]\n",
      "loss: 0.768544  [171200/175341]\n",
      "loss: 0.637554  [172800/175341]\n",
      "loss: 0.805771  [174400/175341]\n",
      "Train Accuracy: 80.9411%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.567893, F1-score: 75.05%, Macro_F1-Score:  41.62%  \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.714909  [    0/175341]\n",
      "loss: 0.151972  [ 1600/175341]\n",
      "loss: 0.235629  [ 3200/175341]\n",
      "loss: 0.813903  [ 4800/175341]\n",
      "loss: 0.452262  [ 6400/175341]\n",
      "loss: 0.593783  [ 8000/175341]\n",
      "loss: 0.414465  [ 9600/175341]\n",
      "loss: 0.079236  [11200/175341]\n",
      "loss: 0.546492  [12800/175341]\n",
      "loss: 0.387681  [14400/175341]\n",
      "loss: 0.821175  [16000/175341]\n",
      "loss: 0.696981  [17600/175341]\n",
      "loss: 0.516262  [19200/175341]\n",
      "loss: 0.324287  [20800/175341]\n",
      "loss: 0.485862  [22400/175341]\n",
      "loss: 0.394786  [24000/175341]\n",
      "loss: 0.432340  [25600/175341]\n",
      "loss: 0.924980  [27200/175341]\n",
      "loss: 0.476331  [28800/175341]\n",
      "loss: 0.791597  [30400/175341]\n",
      "loss: 0.341382  [32000/175341]\n",
      "loss: 0.604207  [33600/175341]\n",
      "loss: 0.228961  [35200/175341]\n",
      "loss: 0.464670  [36800/175341]\n",
      "loss: 0.364603  [38400/175341]\n",
      "loss: 0.282325  [40000/175341]\n",
      "loss: 0.707951  [41600/175341]\n",
      "loss: 0.814351  [43200/175341]\n",
      "loss: 0.337400  [44800/175341]\n",
      "loss: 0.688702  [46400/175341]\n",
      "loss: 0.138254  [48000/175341]\n",
      "loss: 0.227897  [49600/175341]\n",
      "loss: 0.470823  [51200/175341]\n",
      "loss: 0.333264  [52800/175341]\n",
      "loss: 0.286997  [54400/175341]\n",
      "loss: 0.375235  [56000/175341]\n",
      "loss: 0.463344  [57600/175341]\n",
      "loss: 0.329546  [59200/175341]\n",
      "loss: 0.545079  [60800/175341]\n",
      "loss: 0.311181  [62400/175341]\n",
      "loss: 0.460069  [64000/175341]\n",
      "loss: 0.235099  [65600/175341]\n",
      "loss: 0.358326  [67200/175341]\n",
      "loss: 0.687051  [68800/175341]\n",
      "loss: 0.291630  [70400/175341]\n",
      "loss: 0.457686  [72000/175341]\n",
      "loss: 0.407862  [73600/175341]\n",
      "loss: 0.116260  [75200/175341]\n",
      "loss: 0.452158  [76800/175341]\n",
      "loss: 0.440597  [78400/175341]\n",
      "loss: 0.703426  [80000/175341]\n",
      "loss: 0.566691  [81600/175341]\n",
      "loss: 0.532371  [83200/175341]\n",
      "loss: 0.371601  [84800/175341]\n",
      "loss: 0.708839  [86400/175341]\n",
      "loss: 0.640020  [88000/175341]\n",
      "loss: 0.508681  [89600/175341]\n",
      "loss: 0.486981  [91200/175341]\n",
      "loss: 0.676951  [92800/175341]\n",
      "loss: 0.794744  [94400/175341]\n",
      "loss: 0.673819  [96000/175341]\n",
      "loss: 0.179884  [97600/175341]\n",
      "loss: 0.848671  [99200/175341]\n",
      "loss: 0.199368  [100800/175341]\n",
      "loss: 0.420872  [102400/175341]\n",
      "loss: 0.329802  [104000/175341]\n",
      "loss: 0.382032  [105600/175341]\n",
      "loss: 0.174234  [107200/175341]\n",
      "loss: 0.456386  [108800/175341]\n",
      "loss: 0.725179  [110400/175341]\n",
      "loss: 0.452615  [112000/175341]\n",
      "loss: 0.321195  [113600/175341]\n",
      "loss: 0.878755  [115200/175341]\n",
      "loss: 0.585746  [116800/175341]\n",
      "loss: 0.429443  [118400/175341]\n",
      "loss: 0.430823  [120000/175341]\n",
      "loss: 0.625325  [121600/175341]\n",
      "loss: 0.367814  [123200/175341]\n",
      "loss: 0.491362  [124800/175341]\n",
      "loss: 0.262092  [126400/175341]\n",
      "loss: 0.808869  [128000/175341]\n",
      "loss: 0.270074  [129600/175341]\n",
      "loss: 0.514866  [131200/175341]\n",
      "loss: 0.559331  [132800/175341]\n",
      "loss: 0.341174  [134400/175341]\n",
      "loss: 0.099994  [136000/175341]\n",
      "loss: 0.269147  [137600/175341]\n",
      "loss: 0.235019  [139200/175341]\n",
      "loss: 0.335197  [140800/175341]\n",
      "loss: 0.592997  [142400/175341]\n",
      "loss: 0.378250  [144000/175341]\n",
      "loss: 0.516382  [145600/175341]\n",
      "loss: 0.398898  [147200/175341]\n",
      "loss: 0.385556  [148800/175341]\n",
      "loss: 0.592219  [150400/175341]\n",
      "loss: 0.347092  [152000/175341]\n",
      "loss: 0.303871  [153600/175341]\n",
      "loss: 0.303073  [155200/175341]\n",
      "loss: 0.309615  [156800/175341]\n",
      "loss: 0.812064  [158400/175341]\n",
      "loss: 0.412992  [160000/175341]\n",
      "loss: 0.649999  [161600/175341]\n",
      "loss: 0.381896  [163200/175341]\n",
      "loss: 0.701273  [164800/175341]\n",
      "loss: 0.335671  [166400/175341]\n",
      "loss: 0.573789  [168000/175341]\n",
      "loss: 0.340312  [169600/175341]\n",
      "loss: 0.424594  [171200/175341]\n",
      "loss: 0.378841  [172800/175341]\n",
      "loss: 0.351736  [174400/175341]\n",
      "Train Accuracy: 80.9942%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.545220, F1-score: 75.48%, Macro_F1-Score:  41.38%  \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.133972  [    0/175341]\n",
      "loss: 0.555665  [ 1600/175341]\n",
      "loss: 0.129785  [ 3200/175341]\n",
      "loss: 0.423099  [ 4800/175341]\n",
      "loss: 0.237889  [ 6400/175341]\n",
      "loss: 0.312571  [ 8000/175341]\n",
      "loss: 0.451012  [ 9600/175341]\n",
      "loss: 0.522142  [11200/175341]\n",
      "loss: 0.463320  [12800/175341]\n",
      "loss: 0.457959  [14400/175341]\n",
      "loss: 0.332511  [16000/175341]\n",
      "loss: 0.335679  [17600/175341]\n",
      "loss: 0.552460  [19200/175341]\n",
      "loss: 0.250661  [20800/175341]\n",
      "loss: 0.387548  [22400/175341]\n",
      "loss: 0.261879  [24000/175341]\n",
      "loss: 0.669754  [25600/175341]\n",
      "loss: 0.476723  [27200/175341]\n",
      "loss: 0.578144  [28800/175341]\n",
      "loss: 0.199245  [30400/175341]\n",
      "loss: 0.744112  [32000/175341]\n",
      "loss: 0.419030  [33600/175341]\n",
      "loss: 0.491891  [35200/175341]\n",
      "loss: 0.850730  [36800/175341]\n",
      "loss: 0.624626  [38400/175341]\n",
      "loss: 0.605623  [40000/175341]\n",
      "loss: 0.389638  [41600/175341]\n",
      "loss: 0.269490  [43200/175341]\n",
      "loss: 0.090490  [44800/175341]\n",
      "loss: 0.434060  [46400/175341]\n",
      "loss: 0.412284  [48000/175341]\n",
      "loss: 0.624097  [49600/175341]\n",
      "loss: 0.388164  [51200/175341]\n",
      "loss: 0.474991  [52800/175341]\n",
      "loss: 0.432280  [54400/175341]\n",
      "loss: 0.593345  [56000/175341]\n",
      "loss: 0.369852  [57600/175341]\n",
      "loss: 0.333210  [59200/175341]\n",
      "loss: 0.813615  [60800/175341]\n",
      "loss: 0.348587  [62400/175341]\n",
      "loss: 0.354098  [64000/175341]\n",
      "loss: 0.422029  [65600/175341]\n",
      "loss: 0.406474  [67200/175341]\n",
      "loss: 0.490342  [68800/175341]\n",
      "loss: 0.461620  [70400/175341]\n",
      "loss: 0.458418  [72000/175341]\n",
      "loss: 0.468325  [73600/175341]\n",
      "loss: 0.842273  [75200/175341]\n",
      "loss: 0.594203  [76800/175341]\n",
      "loss: 0.513287  [78400/175341]\n",
      "loss: 0.377860  [80000/175341]\n",
      "loss: 0.312440  [81600/175341]\n",
      "loss: 0.595814  [83200/175341]\n",
      "loss: 0.718245  [84800/175341]\n",
      "loss: 0.513484  [86400/175341]\n",
      "loss: 0.477713  [88000/175341]\n",
      "loss: 0.469985  [89600/175341]\n",
      "loss: 0.350687  [91200/175341]\n",
      "loss: 0.511114  [92800/175341]\n",
      "loss: 0.235384  [94400/175341]\n",
      "loss: 0.227167  [96000/175341]\n",
      "loss: 0.844034  [97600/175341]\n",
      "loss: 0.559606  [99200/175341]\n",
      "loss: 0.331511  [100800/175341]\n",
      "loss: 0.625737  [102400/175341]\n",
      "loss: 0.718914  [104000/175341]\n",
      "loss: 0.209076  [105600/175341]\n",
      "loss: 0.666120  [107200/175341]\n",
      "loss: 0.217475  [108800/175341]\n",
      "loss: 0.508916  [110400/175341]\n",
      "loss: 0.568600  [112000/175341]\n",
      "loss: 0.366954  [113600/175341]\n",
      "loss: 0.445357  [115200/175341]\n",
      "loss: 0.845402  [116800/175341]\n",
      "loss: 0.436038  [118400/175341]\n",
      "loss: 0.343644  [120000/175341]\n",
      "loss: 0.344644  [121600/175341]\n",
      "loss: 0.542455  [123200/175341]\n",
      "loss: 0.471859  [124800/175341]\n",
      "loss: 0.894493  [126400/175341]\n",
      "loss: 0.353144  [128000/175341]\n",
      "loss: 0.302365  [129600/175341]\n",
      "loss: 0.394792  [131200/175341]\n",
      "loss: 0.413775  [132800/175341]\n",
      "loss: 0.387455  [134400/175341]\n",
      "loss: 0.479364  [136000/175341]\n",
      "loss: 0.268731  [137600/175341]\n",
      "loss: 0.442402  [139200/175341]\n",
      "loss: 0.515702  [140800/175341]\n",
      "loss: 0.596636  [142400/175341]\n",
      "loss: 0.445302  [144000/175341]\n",
      "loss: 0.172544  [145600/175341]\n",
      "loss: 0.493845  [147200/175341]\n",
      "loss: 0.223173  [148800/175341]\n",
      "loss: 0.178397  [150400/175341]\n",
      "loss: 0.385962  [152000/175341]\n",
      "loss: 0.515203  [153600/175341]\n",
      "loss: 0.484768  [155200/175341]\n",
      "loss: 0.619384  [156800/175341]\n",
      "loss: 0.582951  [158400/175341]\n",
      "loss: 0.203034  [160000/175341]\n",
      "loss: 0.424117  [161600/175341]\n",
      "loss: 0.370519  [163200/175341]\n",
      "loss: 0.411679  [164800/175341]\n",
      "loss: 0.472069  [166400/175341]\n",
      "loss: 0.738196  [168000/175341]\n",
      "loss: 0.544422  [169600/175341]\n",
      "loss: 0.824249  [171200/175341]\n",
      "loss: 0.298810  [172800/175341]\n",
      "loss: 0.418962  [174400/175341]\n",
      "Train Accuracy: 81.0062%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.553277, F1-score: 75.72%, Macro_F1-Score:  42.05%  \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.375159  [    0/175341]\n",
      "loss: 0.305875  [ 1600/175341]\n",
      "loss: 0.400573  [ 3200/175341]\n",
      "loss: 0.229153  [ 4800/175341]\n",
      "loss: 0.316929  [ 6400/175341]\n",
      "loss: 0.411580  [ 8000/175341]\n",
      "loss: 0.292187  [ 9600/175341]\n",
      "loss: 0.542723  [11200/175341]\n",
      "loss: 0.351380  [12800/175341]\n",
      "loss: 0.430175  [14400/175341]\n",
      "loss: 0.462763  [16000/175341]\n",
      "loss: 0.458878  [17600/175341]\n",
      "loss: 0.181301  [19200/175341]\n",
      "loss: 0.547123  [20800/175341]\n",
      "loss: 0.121400  [22400/175341]\n",
      "loss: 0.520989  [24000/175341]\n",
      "loss: 0.245084  [25600/175341]\n",
      "loss: 0.165105  [27200/175341]\n",
      "loss: 0.580335  [28800/175341]\n",
      "loss: 0.149769  [30400/175341]\n",
      "loss: 0.593501  [32000/175341]\n",
      "loss: 0.356305  [33600/175341]\n",
      "loss: 0.585715  [35200/175341]\n",
      "loss: 0.457937  [36800/175341]\n",
      "loss: 0.435202  [38400/175341]\n",
      "loss: 0.283247  [40000/175341]\n",
      "loss: 0.641538  [41600/175341]\n",
      "loss: 0.213214  [43200/175341]\n",
      "loss: 0.561062  [44800/175341]\n",
      "loss: 0.343245  [46400/175341]\n",
      "loss: 0.671581  [48000/175341]\n",
      "loss: 0.429540  [49600/175341]\n",
      "loss: 0.411587  [51200/175341]\n",
      "loss: 0.205425  [52800/175341]\n",
      "loss: 0.438011  [54400/175341]\n",
      "loss: 0.601067  [56000/175341]\n",
      "loss: 0.381535  [57600/175341]\n",
      "loss: 0.585348  [59200/175341]\n",
      "loss: 0.363648  [60800/175341]\n",
      "loss: 0.371421  [62400/175341]\n",
      "loss: 0.442965  [64000/175341]\n",
      "loss: 0.342880  [65600/175341]\n",
      "loss: 0.591266  [67200/175341]\n",
      "loss: 0.293504  [68800/175341]\n",
      "loss: 0.553822  [70400/175341]\n",
      "loss: 0.553100  [72000/175341]\n",
      "loss: 0.812674  [73600/175341]\n",
      "loss: 0.565282  [75200/175341]\n",
      "loss: 0.298253  [76800/175341]\n",
      "loss: 0.327734  [78400/175341]\n",
      "loss: 0.249089  [80000/175341]\n",
      "loss: 0.374686  [81600/175341]\n",
      "loss: 0.375361  [83200/175341]\n",
      "loss: 0.277544  [84800/175341]\n",
      "loss: 0.613471  [86400/175341]\n",
      "loss: 0.203955  [88000/175341]\n",
      "loss: 0.312737  [89600/175341]\n",
      "loss: 0.518588  [91200/175341]\n",
      "loss: 0.869591  [92800/175341]\n",
      "loss: 0.884201  [94400/175341]\n",
      "loss: 0.443086  [96000/175341]\n",
      "loss: 0.615929  [97600/175341]\n",
      "loss: 0.525363  [99200/175341]\n",
      "loss: 0.485209  [100800/175341]\n",
      "loss: 0.400836  [102400/175341]\n",
      "loss: 0.413433  [104000/175341]\n",
      "loss: 0.094760  [105600/175341]\n",
      "loss: 0.696858  [107200/175341]\n",
      "loss: 0.840816  [108800/175341]\n",
      "loss: 0.611529  [110400/175341]\n",
      "loss: 0.500381  [112000/175341]\n",
      "loss: 0.313095  [113600/175341]\n",
      "loss: 0.522934  [115200/175341]\n",
      "loss: 0.637983  [116800/175341]\n",
      "loss: 0.696108  [118400/175341]\n",
      "loss: 0.328100  [120000/175341]\n",
      "loss: 0.258444  [121600/175341]\n",
      "loss: 0.380271  [123200/175341]\n",
      "loss: 0.502749  [124800/175341]\n",
      "loss: 0.618992  [126400/175341]\n",
      "loss: 0.592638  [128000/175341]\n",
      "loss: 0.451039  [129600/175341]\n",
      "loss: 0.407890  [131200/175341]\n",
      "loss: 0.373313  [132800/175341]\n",
      "loss: 1.004577  [134400/175341]\n",
      "loss: 0.795430  [136000/175341]\n",
      "loss: 0.109409  [137600/175341]\n",
      "loss: 0.892430  [139200/175341]\n",
      "loss: 0.224275  [140800/175341]\n",
      "loss: 0.580515  [142400/175341]\n",
      "loss: 0.502398  [144000/175341]\n",
      "loss: 0.591900  [145600/175341]\n",
      "loss: 0.652630  [147200/175341]\n",
      "loss: 0.182174  [148800/175341]\n",
      "loss: 0.441093  [150400/175341]\n",
      "loss: 0.409853  [152000/175341]\n",
      "loss: 0.739671  [153600/175341]\n",
      "loss: 0.551945  [155200/175341]\n",
      "loss: 0.490537  [156800/175341]\n",
      "loss: 0.475760  [158400/175341]\n",
      "loss: 0.753801  [160000/175341]\n",
      "loss: 0.406576  [161600/175341]\n",
      "loss: 0.791165  [163200/175341]\n",
      "loss: 0.264462  [164800/175341]\n",
      "loss: 0.384947  [166400/175341]\n",
      "loss: 0.137937  [168000/175341]\n",
      "loss: 0.440845  [169600/175341]\n",
      "loss: 0.534587  [171200/175341]\n",
      "loss: 0.535712  [172800/175341]\n",
      "loss: 0.391880  [174400/175341]\n",
      "Train Accuracy: 80.9554%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.564133, F1-score: 74.59%, Macro_F1-Score:  40.43%  \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.285628  [    0/175341]\n",
      "loss: 0.307699  [ 1600/175341]\n",
      "loss: 0.351649  [ 3200/175341]\n",
      "loss: 0.546998  [ 4800/175341]\n",
      "loss: 0.339342  [ 6400/175341]\n",
      "loss: 0.530726  [ 8000/175341]\n",
      "loss: 0.569543  [ 9600/175341]\n",
      "loss: 0.461470  [11200/175341]\n",
      "loss: 0.376814  [12800/175341]\n",
      "loss: 0.371349  [14400/175341]\n",
      "loss: 0.261368  [16000/175341]\n",
      "loss: 0.417991  [17600/175341]\n",
      "loss: 0.434245  [19200/175341]\n",
      "loss: 0.727512  [20800/175341]\n",
      "loss: 1.067856  [22400/175341]\n",
      "loss: 0.454014  [24000/175341]\n",
      "loss: 0.336239  [25600/175341]\n",
      "loss: 0.595271  [27200/175341]\n",
      "loss: 0.630661  [28800/175341]\n",
      "loss: 0.839373  [30400/175341]\n",
      "loss: 0.278967  [32000/175341]\n",
      "loss: 0.336792  [33600/175341]\n",
      "loss: 0.529378  [35200/175341]\n",
      "loss: 0.684799  [36800/175341]\n",
      "loss: 0.300747  [38400/175341]\n",
      "loss: 0.173542  [40000/175341]\n",
      "loss: 0.521271  [41600/175341]\n",
      "loss: 0.521559  [43200/175341]\n",
      "loss: 0.288180  [44800/175341]\n",
      "loss: 0.342173  [46400/175341]\n",
      "loss: 0.573630  [48000/175341]\n",
      "loss: 0.192523  [49600/175341]\n",
      "loss: 1.238724  [51200/175341]\n",
      "loss: 0.659968  [52800/175341]\n",
      "loss: 0.596220  [54400/175341]\n",
      "loss: 0.452292  [56000/175341]\n",
      "loss: 0.592352  [57600/175341]\n",
      "loss: 0.266742  [59200/175341]\n",
      "loss: 0.573333  [60800/175341]\n",
      "loss: 0.474122  [62400/175341]\n",
      "loss: 0.248857  [64000/175341]\n",
      "loss: 0.765123  [65600/175341]\n",
      "loss: 0.369063  [67200/175341]\n",
      "loss: 0.582226  [68800/175341]\n",
      "loss: 0.446432  [70400/175341]\n",
      "loss: 0.681788  [72000/175341]\n",
      "loss: 0.524021  [73600/175341]\n",
      "loss: 0.222015  [75200/175341]\n",
      "loss: 0.797085  [76800/175341]\n",
      "loss: 0.422824  [78400/175341]\n",
      "loss: 0.541279  [80000/175341]\n",
      "loss: 0.531610  [81600/175341]\n",
      "loss: 0.555694  [83200/175341]\n",
      "loss: 0.245975  [84800/175341]\n",
      "loss: 0.066812  [86400/175341]\n",
      "loss: 0.527218  [88000/175341]\n",
      "loss: 0.508563  [89600/175341]\n",
      "loss: 0.226314  [91200/175341]\n",
      "loss: 0.423709  [92800/175341]\n",
      "loss: 0.362381  [94400/175341]\n",
      "loss: 0.499995  [96000/175341]\n",
      "loss: 0.427166  [97600/175341]\n",
      "loss: 0.339799  [99200/175341]\n",
      "loss: 0.813772  [100800/175341]\n",
      "loss: 0.647865  [102400/175341]\n",
      "loss: 0.346463  [104000/175341]\n",
      "loss: 0.314556  [105600/175341]\n",
      "loss: 0.445097  [107200/175341]\n",
      "loss: 0.509007  [108800/175341]\n",
      "loss: 0.420056  [110400/175341]\n",
      "loss: 0.364961  [112000/175341]\n",
      "loss: 0.381172  [113600/175341]\n",
      "loss: 0.340826  [115200/175341]\n",
      "loss: 0.075443  [116800/175341]\n",
      "loss: 0.368651  [118400/175341]\n",
      "loss: 0.461991  [120000/175341]\n",
      "loss: 0.712193  [121600/175341]\n",
      "loss: 0.225778  [123200/175341]\n",
      "loss: 0.286190  [124800/175341]\n",
      "loss: 0.246668  [126400/175341]\n",
      "loss: 0.685284  [128000/175341]\n",
      "loss: 0.610153  [129600/175341]\n",
      "loss: 0.453579  [131200/175341]\n",
      "loss: 0.327465  [132800/175341]\n",
      "loss: 0.876148  [134400/175341]\n",
      "loss: 0.408816  [136000/175341]\n",
      "loss: 0.507175  [137600/175341]\n",
      "loss: 0.865402  [139200/175341]\n",
      "loss: 0.598519  [140800/175341]\n",
      "loss: 1.097107  [142400/175341]\n",
      "loss: 0.406149  [144000/175341]\n",
      "loss: 0.245820  [145600/175341]\n",
      "loss: 0.307979  [147200/175341]\n",
      "loss: 0.556699  [148800/175341]\n",
      "loss: 0.495795  [150400/175341]\n",
      "loss: 0.289221  [152000/175341]\n",
      "loss: 0.654387  [153600/175341]\n",
      "loss: 0.678950  [155200/175341]\n",
      "loss: 0.707803  [156800/175341]\n",
      "loss: 0.220614  [158400/175341]\n",
      "loss: 0.510054  [160000/175341]\n",
      "loss: 0.300049  [161600/175341]\n",
      "loss: 0.574251  [163200/175341]\n",
      "loss: 0.151735  [164800/175341]\n",
      "loss: 0.622160  [166400/175341]\n",
      "loss: 0.424442  [168000/175341]\n",
      "loss: 0.275696  [169600/175341]\n",
      "loss: 0.414140  [171200/175341]\n",
      "loss: 0.572463  [172800/175341]\n",
      "loss: 0.406239  [174400/175341]\n",
      "Train Accuracy: 80.9890%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.556564, F1-score: 75.21%, Macro_F1-Score:  41.78%  \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.740362  [    0/175341]\n",
      "loss: 0.493920  [ 1600/175341]\n",
      "loss: 0.662826  [ 3200/175341]\n",
      "loss: 0.491650  [ 4800/175341]\n",
      "loss: 0.635736  [ 6400/175341]\n",
      "loss: 0.242552  [ 8000/175341]\n",
      "loss: 0.881949  [ 9600/175341]\n",
      "loss: 0.469887  [11200/175341]\n",
      "loss: 0.446519  [12800/175341]\n",
      "loss: 0.791982  [14400/175341]\n",
      "loss: 0.574241  [16000/175341]\n",
      "loss: 0.437830  [17600/175341]\n",
      "loss: 0.411531  [19200/175341]\n",
      "loss: 0.453296  [20800/175341]\n",
      "loss: 0.597896  [22400/175341]\n",
      "loss: 0.415421  [24000/175341]\n",
      "loss: 0.249323  [25600/175341]\n",
      "loss: 0.433669  [27200/175341]\n",
      "loss: 0.170633  [28800/175341]\n",
      "loss: 0.680307  [30400/175341]\n",
      "loss: 0.323485  [32000/175341]\n",
      "loss: 0.593341  [33600/175341]\n",
      "loss: 0.510493  [35200/175341]\n",
      "loss: 0.317250  [36800/175341]\n",
      "loss: 0.458231  [38400/175341]\n",
      "loss: 0.358561  [40000/175341]\n",
      "loss: 0.440093  [41600/175341]\n",
      "loss: 0.592757  [43200/175341]\n",
      "loss: 0.671168  [44800/175341]\n",
      "loss: 0.394644  [46400/175341]\n",
      "loss: 0.689200  [48000/175341]\n",
      "loss: 0.395621  [49600/175341]\n",
      "loss: 0.265651  [51200/175341]\n",
      "loss: 0.201070  [52800/175341]\n",
      "loss: 0.411780  [54400/175341]\n",
      "loss: 0.453550  [56000/175341]\n",
      "loss: 0.208371  [57600/175341]\n",
      "loss: 0.805216  [59200/175341]\n",
      "loss: 0.180557  [60800/175341]\n",
      "loss: 0.318756  [62400/175341]\n",
      "loss: 0.408688  [64000/175341]\n",
      "loss: 0.384206  [65600/175341]\n",
      "loss: 0.434877  [67200/175341]\n",
      "loss: 0.965004  [68800/175341]\n",
      "loss: 0.201458  [70400/175341]\n",
      "loss: 0.393861  [72000/175341]\n",
      "loss: 0.512148  [73600/175341]\n",
      "loss: 0.440213  [75200/175341]\n",
      "loss: 0.518138  [76800/175341]\n",
      "loss: 0.595636  [78400/175341]\n",
      "loss: 0.594273  [80000/175341]\n",
      "loss: 1.058209  [81600/175341]\n",
      "loss: 0.346325  [83200/175341]\n",
      "loss: 0.232044  [84800/175341]\n",
      "loss: 0.279432  [86400/175341]\n",
      "loss: 0.340088  [88000/175341]\n",
      "loss: 0.676899  [89600/175341]\n",
      "loss: 0.204929  [91200/175341]\n",
      "loss: 0.489592  [92800/175341]\n",
      "loss: 0.124464  [94400/175341]\n",
      "loss: 0.386598  [96000/175341]\n",
      "loss: 0.591484  [97600/175341]\n",
      "loss: 0.590822  [99200/175341]\n",
      "loss: 0.287045  [100800/175341]\n",
      "loss: 0.998737  [102400/175341]\n",
      "loss: 0.476425  [104000/175341]\n",
      "loss: 0.282194  [105600/175341]\n",
      "loss: 0.552989  [107200/175341]\n",
      "loss: 0.379888  [108800/175341]\n",
      "loss: 0.722990  [110400/175341]\n",
      "loss: 0.604251  [112000/175341]\n",
      "loss: 0.268984  [113600/175341]\n",
      "loss: 0.526984  [115200/175341]\n",
      "loss: 0.517199  [116800/175341]\n",
      "loss: 0.483636  [118400/175341]\n",
      "loss: 1.040957  [120000/175341]\n",
      "loss: 0.524403  [121600/175341]\n",
      "loss: 0.494083  [123200/175341]\n",
      "loss: 0.890700  [124800/175341]\n",
      "loss: 0.278064  [126400/175341]\n",
      "loss: 0.584762  [128000/175341]\n",
      "loss: 0.328321  [129600/175341]\n",
      "loss: 0.374179  [131200/175341]\n",
      "loss: 0.335687  [132800/175341]\n",
      "loss: 0.376278  [134400/175341]\n",
      "loss: 0.453444  [136000/175341]\n",
      "loss: 0.366444  [137600/175341]\n",
      "loss: 0.245669  [139200/175341]\n",
      "loss: 0.402527  [140800/175341]\n",
      "loss: 0.660217  [142400/175341]\n",
      "loss: 0.282917  [144000/175341]\n",
      "loss: 0.535581  [145600/175341]\n",
      "loss: 0.334495  [147200/175341]\n",
      "loss: 0.363435  [148800/175341]\n",
      "loss: 0.320381  [150400/175341]\n",
      "loss: 0.391877  [152000/175341]\n",
      "loss: 0.442719  [153600/175341]\n",
      "loss: 0.545432  [155200/175341]\n",
      "loss: 0.401135  [156800/175341]\n",
      "loss: 0.541085  [158400/175341]\n",
      "loss: 0.640871  [160000/175341]\n",
      "loss: 0.271488  [161600/175341]\n",
      "loss: 0.906575  [163200/175341]\n",
      "loss: 0.622750  [164800/175341]\n",
      "loss: 1.074704  [166400/175341]\n",
      "loss: 0.283525  [168000/175341]\n",
      "loss: 0.497496  [169600/175341]\n",
      "loss: 0.452001  [171200/175341]\n",
      "loss: 0.350353  [172800/175341]\n",
      "loss: 0.355967  [174400/175341]\n",
      "Train Accuracy: 81.0301%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.543446, F1-score: 75.58%, Macro_F1-Score:  41.40%  \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.660489  [    0/175341]\n",
      "loss: 0.243091  [ 1600/175341]\n",
      "loss: 0.476373  [ 3200/175341]\n",
      "loss: 0.288866  [ 4800/175341]\n",
      "loss: 0.491480  [ 6400/175341]\n",
      "loss: 0.470894  [ 8000/175341]\n",
      "loss: 0.806393  [ 9600/175341]\n",
      "loss: 0.522565  [11200/175341]\n",
      "loss: 0.456075  [12800/175341]\n",
      "loss: 0.434066  [14400/175341]\n",
      "loss: 0.342782  [16000/175341]\n",
      "loss: 0.403449  [17600/175341]\n",
      "loss: 0.533287  [19200/175341]\n",
      "loss: 0.462490  [20800/175341]\n",
      "loss: 0.362942  [22400/175341]\n",
      "loss: 0.296791  [24000/175341]\n",
      "loss: 0.783764  [25600/175341]\n",
      "loss: 0.422039  [27200/175341]\n",
      "loss: 0.286558  [28800/175341]\n",
      "loss: 0.646901  [30400/175341]\n",
      "loss: 0.246893  [32000/175341]\n",
      "loss: 0.636207  [33600/175341]\n",
      "loss: 0.697556  [35200/175341]\n",
      "loss: 0.328394  [36800/175341]\n",
      "loss: 0.693624  [38400/175341]\n",
      "loss: 0.489615  [40000/175341]\n",
      "loss: 0.257379  [41600/175341]\n",
      "loss: 0.607889  [43200/175341]\n",
      "loss: 0.367031  [44800/175341]\n",
      "loss: 0.449231  [46400/175341]\n",
      "loss: 0.155177  [48000/175341]\n",
      "loss: 0.235480  [49600/175341]\n",
      "loss: 0.474940  [51200/175341]\n",
      "loss: 0.361174  [52800/175341]\n",
      "loss: 0.404895  [54400/175341]\n",
      "loss: 0.597683  [56000/175341]\n",
      "loss: 0.588785  [57600/175341]\n",
      "loss: 0.503936  [59200/175341]\n",
      "loss: 0.054239  [60800/175341]\n",
      "loss: 0.692997  [62400/175341]\n",
      "loss: 0.202115  [64000/175341]\n",
      "loss: 0.611040  [65600/175341]\n",
      "loss: 1.028396  [67200/175341]\n",
      "loss: 0.632206  [68800/175341]\n",
      "loss: 0.364606  [70400/175341]\n",
      "loss: 0.209808  [72000/175341]\n",
      "loss: 0.757778  [73600/175341]\n",
      "loss: 0.891667  [75200/175341]\n",
      "loss: 0.545510  [76800/175341]\n",
      "loss: 0.370506  [78400/175341]\n",
      "loss: 0.744060  [80000/175341]\n",
      "loss: 0.291085  [81600/175341]\n",
      "loss: 0.631634  [83200/175341]\n",
      "loss: 0.488239  [84800/175341]\n",
      "loss: 0.394190  [86400/175341]\n",
      "loss: 0.235440  [88000/175341]\n",
      "loss: 0.310565  [89600/175341]\n",
      "loss: 0.622552  [91200/175341]\n",
      "loss: 0.139880  [92800/175341]\n",
      "loss: 0.630472  [94400/175341]\n",
      "loss: 0.745459  [96000/175341]\n",
      "loss: 0.312833  [97600/175341]\n",
      "loss: 0.463702  [99200/175341]\n",
      "loss: 0.529637  [100800/175341]\n",
      "loss: 0.456063  [102400/175341]\n",
      "loss: 0.269539  [104000/175341]\n",
      "loss: 0.433101  [105600/175341]\n",
      "loss: 0.639167  [107200/175341]\n",
      "loss: 0.058364  [108800/175341]\n",
      "loss: 0.461587  [110400/175341]\n",
      "loss: 0.622106  [112000/175341]\n",
      "loss: 0.426232  [113600/175341]\n",
      "loss: 0.640484  [115200/175341]\n",
      "loss: 0.220295  [116800/175341]\n",
      "loss: 0.371847  [118400/175341]\n",
      "loss: 0.293670  [120000/175341]\n",
      "loss: 0.605864  [121600/175341]\n",
      "loss: 0.473613  [123200/175341]\n",
      "loss: 0.438368  [124800/175341]\n",
      "loss: 0.460075  [126400/175341]\n",
      "loss: 0.520959  [128000/175341]\n",
      "loss: 0.542702  [129600/175341]\n",
      "loss: 0.795349  [131200/175341]\n",
      "loss: 0.391006  [132800/175341]\n",
      "loss: 0.192136  [134400/175341]\n",
      "loss: 0.202992  [136000/175341]\n",
      "loss: 0.453108  [137600/175341]\n",
      "loss: 0.222585  [139200/175341]\n",
      "loss: 0.663696  [140800/175341]\n",
      "loss: 0.602752  [142400/175341]\n",
      "loss: 0.593002  [144000/175341]\n",
      "loss: 0.376985  [145600/175341]\n",
      "loss: 0.571425  [147200/175341]\n",
      "loss: 0.000242  [148800/175341]\n",
      "loss: 0.691780  [150400/175341]\n",
      "loss: 0.600045  [152000/175341]\n",
      "loss: 0.285251  [153600/175341]\n",
      "loss: 0.057739  [155200/175341]\n",
      "loss: 0.600622  [156800/175341]\n",
      "loss: 0.457553  [158400/175341]\n",
      "loss: 0.549592  [160000/175341]\n",
      "loss: 0.086007  [161600/175341]\n",
      "loss: 0.679719  [163200/175341]\n",
      "loss: 0.722374  [164800/175341]\n",
      "loss: 0.473018  [166400/175341]\n",
      "loss: 0.255384  [168000/175341]\n",
      "loss: 0.427786  [169600/175341]\n",
      "loss: 0.164688  [171200/175341]\n",
      "loss: 0.368598  [172800/175341]\n",
      "loss: 0.267772  [174400/175341]\n",
      "Train Accuracy: 81.0067%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.549227, F1-score: 76.07%, Macro_F1-Score:  42.69%  \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.500084  [    0/175341]\n",
      "loss: 0.670968  [ 1600/175341]\n",
      "loss: 0.434295  [ 3200/175341]\n",
      "loss: 0.630721  [ 4800/175341]\n",
      "loss: 0.436370  [ 6400/175341]\n",
      "loss: 0.482969  [ 8000/175341]\n",
      "loss: 0.357326  [ 9600/175341]\n",
      "loss: 0.986510  [11200/175341]\n",
      "loss: 0.315163  [12800/175341]\n",
      "loss: 0.484943  [14400/175341]\n",
      "loss: 0.675669  [16000/175341]\n",
      "loss: 0.449612  [17600/175341]\n",
      "loss: 0.626653  [19200/175341]\n",
      "loss: 0.467093  [20800/175341]\n",
      "loss: 0.245126  [22400/175341]\n",
      "loss: 0.486630  [24000/175341]\n",
      "loss: 0.440365  [25600/175341]\n",
      "loss: 0.230227  [27200/175341]\n",
      "loss: 0.423586  [28800/175341]\n",
      "loss: 1.087112  [30400/175341]\n",
      "loss: 0.556317  [32000/175341]\n",
      "loss: 0.354843  [33600/175341]\n",
      "loss: 0.360732  [35200/175341]\n",
      "loss: 0.302249  [36800/175341]\n",
      "loss: 0.365627  [38400/175341]\n",
      "loss: 0.218936  [40000/175341]\n",
      "loss: 0.263795  [41600/175341]\n",
      "loss: 0.632230  [43200/175341]\n",
      "loss: 0.564554  [44800/175341]\n",
      "loss: 0.647508  [46400/175341]\n",
      "loss: 0.461062  [48000/175341]\n",
      "loss: 0.201484  [49600/175341]\n",
      "loss: 0.317987  [51200/175341]\n",
      "loss: 0.622371  [52800/175341]\n",
      "loss: 0.555348  [54400/175341]\n",
      "loss: 0.477589  [56000/175341]\n",
      "loss: 0.510014  [57600/175341]\n",
      "loss: 0.427396  [59200/175341]\n",
      "loss: 0.419096  [60800/175341]\n",
      "loss: 0.324709  [62400/175341]\n",
      "loss: 0.990646  [64000/175341]\n",
      "loss: 0.333096  [65600/175341]\n",
      "loss: 0.458751  [67200/175341]\n",
      "loss: 0.452807  [68800/175341]\n",
      "loss: 0.392640  [70400/175341]\n",
      "loss: 0.735564  [72000/175341]\n",
      "loss: 0.359522  [73600/175341]\n",
      "loss: 0.139133  [75200/175341]\n",
      "loss: 0.365604  [76800/175341]\n",
      "loss: 0.559295  [78400/175341]\n",
      "loss: 0.311606  [80000/175341]\n",
      "loss: 0.616864  [81600/175341]\n",
      "loss: 0.332308  [83200/175341]\n",
      "loss: 0.338393  [84800/175341]\n",
      "loss: 0.491040  [86400/175341]\n",
      "loss: 0.718101  [88000/175341]\n",
      "loss: 0.249964  [89600/175341]\n",
      "loss: 1.119840  [91200/175341]\n",
      "loss: 0.680533  [92800/175341]\n",
      "loss: 0.654108  [94400/175341]\n",
      "loss: 0.323530  [96000/175341]\n",
      "loss: 0.727528  [97600/175341]\n",
      "loss: 0.514197  [99200/175341]\n",
      "loss: 0.513801  [100800/175341]\n",
      "loss: 0.558077  [102400/175341]\n",
      "loss: 0.633900  [104000/175341]\n",
      "loss: 0.667684  [105600/175341]\n",
      "loss: 0.108668  [107200/175341]\n",
      "loss: 0.266896  [108800/175341]\n",
      "loss: 0.331038  [110400/175341]\n",
      "loss: 0.911539  [112000/175341]\n",
      "loss: 0.453948  [113600/175341]\n",
      "loss: 0.771819  [115200/175341]\n",
      "loss: 0.233082  [116800/175341]\n",
      "loss: 0.268341  [118400/175341]\n",
      "loss: 0.409501  [120000/175341]\n",
      "loss: 0.664868  [121600/175341]\n",
      "loss: 0.632595  [123200/175341]\n",
      "loss: 0.442806  [124800/175341]\n",
      "loss: 0.510097  [126400/175341]\n",
      "loss: 0.475695  [128000/175341]\n",
      "loss: 0.292219  [129600/175341]\n",
      "loss: 0.247487  [131200/175341]\n",
      "loss: 0.265887  [132800/175341]\n",
      "loss: 0.568132  [134400/175341]\n",
      "loss: 0.389464  [136000/175341]\n",
      "loss: 0.505666  [137600/175341]\n",
      "loss: 0.119116  [139200/175341]\n",
      "loss: 0.509866  [140800/175341]\n",
      "loss: 0.504384  [142400/175341]\n",
      "loss: 0.847627  [144000/175341]\n",
      "loss: 0.488337  [145600/175341]\n",
      "loss: 0.400818  [147200/175341]\n",
      "loss: 0.672888  [148800/175341]\n",
      "loss: 0.420051  [150400/175341]\n",
      "loss: 0.286003  [152000/175341]\n",
      "loss: 0.637080  [153600/175341]\n",
      "loss: 0.339539  [155200/175341]\n",
      "loss: 0.584783  [156800/175341]\n",
      "loss: 0.311413  [158400/175341]\n",
      "loss: 0.538808  [160000/175341]\n",
      "loss: 0.786789  [161600/175341]\n",
      "loss: 0.478421  [163200/175341]\n",
      "loss: 0.305351  [164800/175341]\n",
      "loss: 0.530036  [166400/175341]\n",
      "loss: 0.101592  [168000/175341]\n",
      "loss: 0.622234  [169600/175341]\n",
      "loss: 0.773015  [171200/175341]\n",
      "loss: 0.257767  [172800/175341]\n",
      "loss: 0.296223  [174400/175341]\n",
      "Train Accuracy: 81.0421%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.568006, F1-score: 74.94%, Macro_F1-Score:  41.84%  \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.503760  [    0/175341]\n",
      "loss: 0.228285  [ 1600/175341]\n",
      "loss: 0.291254  [ 3200/175341]\n",
      "loss: 0.529687  [ 4800/175341]\n",
      "loss: 0.817695  [ 6400/175341]\n",
      "loss: 0.879549  [ 8000/175341]\n",
      "loss: 0.722052  [ 9600/175341]\n",
      "loss: 0.747352  [11200/175341]\n",
      "loss: 0.381197  [12800/175341]\n",
      "loss: 0.346680  [14400/175341]\n",
      "loss: 0.492510  [16000/175341]\n",
      "loss: 0.293025  [17600/175341]\n",
      "loss: 0.830967  [19200/175341]\n",
      "loss: 0.749243  [20800/175341]\n",
      "loss: 0.044303  [22400/175341]\n",
      "loss: 0.494964  [24000/175341]\n",
      "loss: 0.250812  [25600/175341]\n",
      "loss: 0.407823  [27200/175341]\n",
      "loss: 0.460065  [28800/175341]\n",
      "loss: 0.275944  [30400/175341]\n",
      "loss: 0.738846  [32000/175341]\n",
      "loss: 0.511631  [33600/175341]\n",
      "loss: 0.648735  [35200/175341]\n",
      "loss: 0.543327  [36800/175341]\n",
      "loss: 0.599743  [38400/175341]\n",
      "loss: 0.428774  [40000/175341]\n",
      "loss: 0.387064  [41600/175341]\n",
      "loss: 0.221752  [43200/175341]\n",
      "loss: 0.500316  [44800/175341]\n",
      "loss: 0.361835  [46400/175341]\n",
      "loss: 0.445814  [48000/175341]\n",
      "loss: 0.625427  [49600/175341]\n",
      "loss: 0.306719  [51200/175341]\n",
      "loss: 0.252304  [52800/175341]\n",
      "loss: 0.381617  [54400/175341]\n",
      "loss: 0.359050  [56000/175341]\n",
      "loss: 0.460713  [57600/175341]\n",
      "loss: 0.486151  [59200/175341]\n",
      "loss: 0.179163  [60800/175341]\n",
      "loss: 0.152200  [62400/175341]\n",
      "loss: 0.344150  [64000/175341]\n",
      "loss: 0.656439  [65600/175341]\n",
      "loss: 0.635924  [67200/175341]\n",
      "loss: 0.280688  [68800/175341]\n",
      "loss: 0.553660  [70400/175341]\n",
      "loss: 0.630861  [72000/175341]\n",
      "loss: 0.792375  [73600/175341]\n",
      "loss: 0.398971  [75200/175341]\n",
      "loss: 0.593808  [76800/175341]\n",
      "loss: 0.727992  [78400/175341]\n",
      "loss: 0.484215  [80000/175341]\n",
      "loss: 0.352267  [81600/175341]\n",
      "loss: 0.217276  [83200/175341]\n",
      "loss: 0.257219  [84800/175341]\n",
      "loss: 0.090481  [86400/175341]\n",
      "loss: 0.593968  [88000/175341]\n",
      "loss: 0.229966  [89600/175341]\n",
      "loss: 0.100789  [91200/175341]\n",
      "loss: 0.443959  [92800/175341]\n",
      "loss: 0.634104  [94400/175341]\n",
      "loss: 0.638863  [96000/175341]\n",
      "loss: 0.479877  [97600/175341]\n",
      "loss: 0.735974  [99200/175341]\n",
      "loss: 0.113497  [100800/175341]\n",
      "loss: 0.259938  [102400/175341]\n",
      "loss: 0.622822  [104000/175341]\n",
      "loss: 0.671102  [105600/175341]\n",
      "loss: 0.609834  [107200/175341]\n",
      "loss: 0.266847  [108800/175341]\n",
      "loss: 0.548753  [110400/175341]\n",
      "loss: 0.580943  [112000/175341]\n",
      "loss: 0.364405  [113600/175341]\n",
      "loss: 0.289182  [115200/175341]\n",
      "loss: 0.262863  [116800/175341]\n",
      "loss: 0.521131  [118400/175341]\n",
      "loss: 0.136322  [120000/175341]\n",
      "loss: 0.614690  [121600/175341]\n",
      "loss: 0.511168  [123200/175341]\n",
      "loss: 0.529468  [124800/175341]\n",
      "loss: 0.484554  [126400/175341]\n",
      "loss: 0.185437  [128000/175341]\n",
      "loss: 0.411602  [129600/175341]\n",
      "loss: 0.283482  [131200/175341]\n",
      "loss: 0.554522  [132800/175341]\n",
      "loss: 0.614450  [134400/175341]\n",
      "loss: 0.580642  [136000/175341]\n",
      "loss: 0.403343  [137600/175341]\n",
      "loss: 0.441274  [139200/175341]\n",
      "loss: 0.208688  [140800/175341]\n",
      "loss: 0.314047  [142400/175341]\n",
      "loss: 0.913357  [144000/175341]\n",
      "loss: 0.745141  [145600/175341]\n",
      "loss: 1.179203  [147200/175341]\n",
      "loss: 0.373173  [148800/175341]\n",
      "loss: 0.809910  [150400/175341]\n",
      "loss: 0.329493  [152000/175341]\n",
      "loss: 0.634395  [153600/175341]\n",
      "loss: 0.810583  [155200/175341]\n",
      "loss: 0.453490  [156800/175341]\n",
      "loss: 0.544808  [158400/175341]\n",
      "loss: 0.475007  [160000/175341]\n",
      "loss: 0.759676  [161600/175341]\n",
      "loss: 0.331878  [163200/175341]\n",
      "loss: 0.502090  [164800/175341]\n",
      "loss: 0.292744  [166400/175341]\n",
      "loss: 0.579431  [168000/175341]\n",
      "loss: 0.569761  [169600/175341]\n",
      "loss: 0.339885  [171200/175341]\n",
      "loss: 0.469972  [172800/175341]\n",
      "loss: 0.823243  [174400/175341]\n",
      "Train Accuracy: 81.0278%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.551698, F1-score: 74.92%, Macro_F1-Score:  41.14%  \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.524893  [    0/175341]\n",
      "loss: 0.281911  [ 1600/175341]\n",
      "loss: 0.979642  [ 3200/175341]\n",
      "loss: 0.413150  [ 4800/175341]\n",
      "loss: 0.450624  [ 6400/175341]\n",
      "loss: 0.654698  [ 8000/175341]\n",
      "loss: 0.426297  [ 9600/175341]\n",
      "loss: 0.892961  [11200/175341]\n",
      "loss: 0.318890  [12800/175341]\n",
      "loss: 0.252216  [14400/175341]\n",
      "loss: 0.884268  [16000/175341]\n",
      "loss: 0.518481  [17600/175341]\n",
      "loss: 0.574480  [19200/175341]\n",
      "loss: 0.693464  [20800/175341]\n",
      "loss: 1.050354  [22400/175341]\n",
      "loss: 0.613416  [24000/175341]\n",
      "loss: 0.490604  [25600/175341]\n",
      "loss: 0.239369  [27200/175341]\n",
      "loss: 0.453748  [28800/175341]\n",
      "loss: 0.395214  [30400/175341]\n",
      "loss: 0.847113  [32000/175341]\n",
      "loss: 0.745769  [33600/175341]\n",
      "loss: 0.352379  [35200/175341]\n",
      "loss: 0.263494  [36800/175341]\n",
      "loss: 0.438943  [38400/175341]\n",
      "loss: 0.156747  [40000/175341]\n",
      "loss: 0.334175  [41600/175341]\n",
      "loss: 0.221842  [43200/175341]\n",
      "loss: 0.279887  [44800/175341]\n",
      "loss: 0.412050  [46400/175341]\n",
      "loss: 0.467999  [48000/175341]\n",
      "loss: 0.835568  [49600/175341]\n",
      "loss: 0.247731  [51200/175341]\n",
      "loss: 0.324447  [52800/175341]\n",
      "loss: 0.313431  [54400/175341]\n",
      "loss: 0.341209  [56000/175341]\n",
      "loss: 0.659110  [57600/175341]\n",
      "loss: 0.301552  [59200/175341]\n",
      "loss: 0.647360  [60800/175341]\n",
      "loss: 0.810180  [62400/175341]\n",
      "loss: 0.690655  [64000/175341]\n",
      "loss: 0.758588  [65600/175341]\n",
      "loss: 0.558454  [67200/175341]\n",
      "loss: 0.545395  [68800/175341]\n",
      "loss: 0.364850  [70400/175341]\n",
      "loss: 0.214039  [72000/175341]\n",
      "loss: 0.482900  [73600/175341]\n",
      "loss: 0.470530  [75200/175341]\n",
      "loss: 0.434891  [76800/175341]\n",
      "loss: 0.176420  [78400/175341]\n",
      "loss: 0.281433  [80000/175341]\n",
      "loss: 0.398837  [81600/175341]\n",
      "loss: 0.600762  [83200/175341]\n",
      "loss: 0.683277  [84800/175341]\n",
      "loss: 0.367234  [86400/175341]\n",
      "loss: 0.605250  [88000/175341]\n",
      "loss: 0.441918  [89600/175341]\n",
      "loss: 0.506942  [91200/175341]\n",
      "loss: 0.736690  [92800/175341]\n",
      "loss: 0.074054  [94400/175341]\n",
      "loss: 0.242921  [96000/175341]\n",
      "loss: 0.339860  [97600/175341]\n",
      "loss: 0.652735  [99200/175341]\n",
      "loss: 0.215249  [100800/175341]\n",
      "loss: 0.466487  [102400/175341]\n",
      "loss: 0.959461  [104000/175341]\n",
      "loss: 0.314042  [105600/175341]\n",
      "loss: 0.446604  [107200/175341]\n",
      "loss: 0.360886  [108800/175341]\n",
      "loss: 0.224749  [110400/175341]\n",
      "loss: 0.205724  [112000/175341]\n",
      "loss: 0.432128  [113600/175341]\n",
      "loss: 0.540073  [115200/175341]\n",
      "loss: 0.507170  [116800/175341]\n",
      "loss: 0.529014  [118400/175341]\n",
      "loss: 0.448551  [120000/175341]\n",
      "loss: 0.166798  [121600/175341]\n",
      "loss: 0.581143  [123200/175341]\n",
      "loss: 0.503632  [124800/175341]\n",
      "loss: 0.334086  [126400/175341]\n",
      "loss: 0.216686  [128000/175341]\n",
      "loss: 0.636639  [129600/175341]\n",
      "loss: 0.473958  [131200/175341]\n",
      "loss: 0.210786  [132800/175341]\n",
      "loss: 0.388295  [134400/175341]\n",
      "loss: 1.026130  [136000/175341]\n",
      "loss: 0.433034  [137600/175341]\n",
      "loss: 0.245114  [139200/175341]\n",
      "loss: 0.293326  [140800/175341]\n",
      "loss: 0.724904  [142400/175341]\n",
      "loss: 0.470139  [144000/175341]\n",
      "loss: 0.210172  [145600/175341]\n",
      "loss: 0.453538  [147200/175341]\n",
      "loss: 0.368671  [148800/175341]\n",
      "loss: 0.304135  [150400/175341]\n",
      "loss: 0.558013  [152000/175341]\n",
      "loss: 0.448122  [153600/175341]\n",
      "loss: 0.561672  [155200/175341]\n",
      "loss: 0.699278  [156800/175341]\n",
      "loss: 0.801781  [158400/175341]\n",
      "loss: 0.737235  [160000/175341]\n",
      "loss: 0.416721  [161600/175341]\n",
      "loss: 0.558461  [163200/175341]\n",
      "loss: 0.562238  [164800/175341]\n",
      "loss: 0.214468  [166400/175341]\n",
      "loss: 0.639399  [168000/175341]\n",
      "loss: 0.233119  [169600/175341]\n",
      "loss: 0.607403  [171200/175341]\n",
      "loss: 0.481423  [172800/175341]\n",
      "loss: 0.816057  [174400/175341]\n",
      "Train Accuracy: 81.0518%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.560082, F1-score: 75.19%, Macro_F1-Score:  42.27%  \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.605675  [    0/175341]\n",
      "loss: 0.373404  [ 1600/175341]\n",
      "loss: 0.443693  [ 3200/175341]\n",
      "loss: 0.467269  [ 4800/175341]\n",
      "loss: 0.637366  [ 6400/175341]\n",
      "loss: 0.550146  [ 8000/175341]\n",
      "loss: 0.483827  [ 9600/175341]\n",
      "loss: 0.853981  [11200/175341]\n",
      "loss: 0.516262  [12800/175341]\n",
      "loss: 0.121348  [14400/175341]\n",
      "loss: 0.758878  [16000/175341]\n",
      "loss: 0.318650  [17600/175341]\n",
      "loss: 0.572705  [19200/175341]\n",
      "loss: 0.417488  [20800/175341]\n",
      "loss: 0.826555  [22400/175341]\n",
      "loss: 0.994451  [24000/175341]\n",
      "loss: 0.588900  [25600/175341]\n",
      "loss: 0.276409  [27200/175341]\n",
      "loss: 0.425611  [28800/175341]\n",
      "loss: 0.520953  [30400/175341]\n",
      "loss: 0.553766  [32000/175341]\n",
      "loss: 1.152428  [33600/175341]\n",
      "loss: 0.770550  [35200/175341]\n",
      "loss: 0.268356  [36800/175341]\n",
      "loss: 0.661846  [38400/175341]\n",
      "loss: 0.253359  [40000/175341]\n",
      "loss: 0.546966  [41600/175341]\n",
      "loss: 0.415969  [43200/175341]\n",
      "loss: 0.810466  [44800/175341]\n",
      "loss: 0.428467  [46400/175341]\n",
      "loss: 0.740211  [48000/175341]\n",
      "loss: 0.757619  [49600/175341]\n",
      "loss: 0.335729  [51200/175341]\n",
      "loss: 0.552553  [52800/175341]\n",
      "loss: 0.609285  [54400/175341]\n",
      "loss: 0.329885  [56000/175341]\n",
      "loss: 0.542097  [57600/175341]\n",
      "loss: 0.476855  [59200/175341]\n",
      "loss: 0.277082  [60800/175341]\n",
      "loss: 0.272435  [62400/175341]\n",
      "loss: 0.455256  [64000/175341]\n",
      "loss: 0.420418  [65600/175341]\n",
      "loss: 0.568021  [67200/175341]\n",
      "loss: 0.585259  [68800/175341]\n",
      "loss: 0.242311  [70400/175341]\n",
      "loss: 0.462549  [72000/175341]\n",
      "loss: 0.675009  [73600/175341]\n",
      "loss: 0.188877  [75200/175341]\n",
      "loss: 0.854955  [76800/175341]\n",
      "loss: 0.813750  [78400/175341]\n",
      "loss: 0.877621  [80000/175341]\n",
      "loss: 0.444619  [81600/175341]\n",
      "loss: 0.540202  [83200/175341]\n",
      "loss: 0.753466  [84800/175341]\n",
      "loss: 0.399008  [86400/175341]\n",
      "loss: 0.957441  [88000/175341]\n",
      "loss: 0.702711  [89600/175341]\n",
      "loss: 0.708503  [91200/175341]\n",
      "loss: 0.367976  [92800/175341]\n",
      "loss: 0.311387  [94400/175341]\n",
      "loss: 0.608965  [96000/175341]\n",
      "loss: 0.433227  [97600/175341]\n",
      "loss: 0.831710  [99200/175341]\n",
      "loss: 0.329388  [100800/175341]\n",
      "loss: 0.764015  [102400/175341]\n",
      "loss: 0.404052  [104000/175341]\n",
      "loss: 0.434496  [105600/175341]\n",
      "loss: 0.484488  [107200/175341]\n",
      "loss: 0.572884  [108800/175341]\n",
      "loss: 0.824064  [110400/175341]\n",
      "loss: 0.292425  [112000/175341]\n",
      "loss: 0.439168  [113600/175341]\n",
      "loss: 0.352438  [115200/175341]\n",
      "loss: 0.834420  [116800/175341]\n",
      "loss: 0.597939  [118400/175341]\n",
      "loss: 0.354007  [120000/175341]\n",
      "loss: 0.597411  [121600/175341]\n",
      "loss: 0.413878  [123200/175341]\n",
      "loss: 0.800253  [124800/175341]\n",
      "loss: 0.422511  [126400/175341]\n",
      "loss: 0.906053  [128000/175341]\n",
      "loss: 0.238943  [129600/175341]\n",
      "loss: 0.798096  [131200/175341]\n",
      "loss: 0.433908  [132800/175341]\n",
      "loss: 0.169725  [134400/175341]\n",
      "loss: 0.699233  [136000/175341]\n",
      "loss: 0.439402  [137600/175341]\n",
      "loss: 0.407434  [139200/175341]\n",
      "loss: 0.520264  [140800/175341]\n",
      "loss: 0.749384  [142400/175341]\n",
      "loss: 0.209720  [144000/175341]\n",
      "loss: 0.346724  [145600/175341]\n",
      "loss: 0.430910  [147200/175341]\n",
      "loss: 0.917607  [148800/175341]\n",
      "loss: 0.495447  [150400/175341]\n",
      "loss: 0.466010  [152000/175341]\n",
      "loss: 0.495116  [153600/175341]\n",
      "loss: 0.634741  [155200/175341]\n",
      "loss: 0.527059  [156800/175341]\n",
      "loss: 0.333690  [158400/175341]\n",
      "loss: 0.667958  [160000/175341]\n",
      "loss: 0.731177  [161600/175341]\n",
      "loss: 0.434498  [163200/175341]\n",
      "loss: 0.370670  [164800/175341]\n",
      "loss: 0.481257  [166400/175341]\n",
      "loss: 0.564657  [168000/175341]\n",
      "loss: 0.281347  [169600/175341]\n",
      "loss: 0.489107  [171200/175341]\n",
      "loss: 0.549394  [172800/175341]\n",
      "loss: 0.381025  [174400/175341]\n",
      "Train Accuracy: 81.0039%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.559566, F1-score: 75.04%, Macro_F1-Score:  41.74%  \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.211789  [    0/175341]\n",
      "loss: 1.192723  [ 1600/175341]\n",
      "loss: 0.413738  [ 3200/175341]\n",
      "loss: 0.651430  [ 4800/175341]\n",
      "loss: 0.338484  [ 6400/175341]\n",
      "loss: 0.441996  [ 8000/175341]\n",
      "loss: 0.401473  [ 9600/175341]\n",
      "loss: 0.562627  [11200/175341]\n",
      "loss: 0.598174  [12800/175341]\n",
      "loss: 0.690547  [14400/175341]\n",
      "loss: 0.382246  [16000/175341]\n",
      "loss: 0.501093  [17600/175341]\n",
      "loss: 0.383577  [19200/175341]\n",
      "loss: 0.494093  [20800/175341]\n",
      "loss: 0.135106  [22400/175341]\n",
      "loss: 0.498625  [24000/175341]\n",
      "loss: 0.336614  [25600/175341]\n",
      "loss: 0.619868  [27200/175341]\n",
      "loss: 0.501737  [28800/175341]\n",
      "loss: 0.674566  [30400/175341]\n",
      "loss: 0.680490  [32000/175341]\n",
      "loss: 0.628459  [33600/175341]\n",
      "loss: 0.570470  [35200/175341]\n",
      "loss: 0.420707  [36800/175341]\n",
      "loss: 0.516890  [38400/175341]\n",
      "loss: 0.411407  [40000/175341]\n",
      "loss: 0.756651  [41600/175341]\n",
      "loss: 0.416050  [43200/175341]\n",
      "loss: 0.350828  [44800/175341]\n",
      "loss: 0.401212  [46400/175341]\n",
      "loss: 0.768376  [48000/175341]\n",
      "loss: 0.216636  [49600/175341]\n",
      "loss: 0.799665  [51200/175341]\n",
      "loss: 0.337495  [52800/175341]\n",
      "loss: 0.924292  [54400/175341]\n",
      "loss: 0.504357  [56000/175341]\n",
      "loss: 0.740342  [57600/175341]\n",
      "loss: 0.160167  [59200/175341]\n",
      "loss: 0.424668  [60800/175341]\n",
      "loss: 0.771103  [62400/175341]\n",
      "loss: 0.753360  [64000/175341]\n",
      "loss: 0.200996  [65600/175341]\n",
      "loss: 0.620848  [67200/175341]\n",
      "loss: 0.468982  [68800/175341]\n",
      "loss: 0.566245  [70400/175341]\n",
      "loss: 0.254516  [72000/175341]\n",
      "loss: 0.279344  [73600/175341]\n",
      "loss: 0.409650  [75200/175341]\n",
      "loss: 1.000713  [76800/175341]\n",
      "loss: 0.314291  [78400/175341]\n",
      "loss: 0.530148  [80000/175341]\n",
      "loss: 0.287548  [81600/175341]\n",
      "loss: 0.419777  [83200/175341]\n",
      "loss: 0.571716  [84800/175341]\n",
      "loss: 0.273241  [86400/175341]\n",
      "loss: 0.426863  [88000/175341]\n",
      "loss: 0.589997  [89600/175341]\n",
      "loss: 0.650790  [91200/175341]\n",
      "loss: 0.190634  [92800/175341]\n",
      "loss: 0.361439  [94400/175341]\n",
      "loss: 0.363602  [96000/175341]\n",
      "loss: 0.494861  [97600/175341]\n",
      "loss: 0.511194  [99200/175341]\n",
      "loss: 0.556241  [100800/175341]\n",
      "loss: 0.541018  [102400/175341]\n",
      "loss: 0.276313  [104000/175341]\n",
      "loss: 0.633444  [105600/175341]\n",
      "loss: 0.516795  [107200/175341]\n",
      "loss: 0.366639  [108800/175341]\n",
      "loss: 0.641282  [110400/175341]\n",
      "loss: 0.992582  [112000/175341]\n",
      "loss: 0.311493  [113600/175341]\n",
      "loss: 0.369796  [115200/175341]\n",
      "loss: 0.747961  [116800/175341]\n",
      "loss: 0.282923  [118400/175341]\n",
      "loss: 0.529195  [120000/175341]\n",
      "loss: 0.295609  [121600/175341]\n",
      "loss: 0.414907  [123200/175341]\n",
      "loss: 0.639501  [124800/175341]\n",
      "loss: 0.348385  [126400/175341]\n",
      "loss: 0.569593  [128000/175341]\n",
      "loss: 0.299072  [129600/175341]\n",
      "loss: 0.237997  [131200/175341]\n",
      "loss: 0.484495  [132800/175341]\n",
      "loss: 0.356824  [134400/175341]\n",
      "loss: 0.677889  [136000/175341]\n",
      "loss: 0.496133  [137600/175341]\n",
      "loss: 0.752231  [139200/175341]\n",
      "loss: 0.860106  [140800/175341]\n",
      "loss: 0.266480  [142400/175341]\n",
      "loss: 0.604097  [144000/175341]\n",
      "loss: 0.179764  [145600/175341]\n",
      "loss: 0.600183  [147200/175341]\n",
      "loss: 0.415601  [148800/175341]\n",
      "loss: 0.619708  [150400/175341]\n",
      "loss: 0.331746  [152000/175341]\n",
      "loss: 0.429493  [153600/175341]\n",
      "loss: 0.271769  [155200/175341]\n",
      "loss: 0.581565  [156800/175341]\n",
      "loss: 0.944199  [158400/175341]\n",
      "loss: 0.497696  [160000/175341]\n",
      "loss: 0.402146  [161600/175341]\n",
      "loss: 0.489124  [163200/175341]\n",
      "loss: 0.522187  [164800/175341]\n",
      "loss: 0.452148  [166400/175341]\n",
      "loss: 0.224500  [168000/175341]\n",
      "loss: 0.540794  [169600/175341]\n",
      "loss: 0.783004  [171200/175341]\n",
      "loss: 0.148945  [172800/175341]\n",
      "loss: 0.430997  [174400/175341]\n",
      "Train Accuracy: 80.9965%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.546964, F1-score: 75.50%, Macro_F1-Score:  41.33%  \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.653785  [    0/175341]\n",
      "loss: 0.562962  [ 1600/175341]\n",
      "loss: 0.425761  [ 3200/175341]\n",
      "loss: 0.636346  [ 4800/175341]\n",
      "loss: 0.925623  [ 6400/175341]\n",
      "loss: 0.489639  [ 8000/175341]\n",
      "loss: 0.594405  [ 9600/175341]\n",
      "loss: 0.289037  [11200/175341]\n",
      "loss: 0.331423  [12800/175341]\n",
      "loss: 0.508536  [14400/175341]\n",
      "loss: 0.497297  [16000/175341]\n",
      "loss: 1.154069  [17600/175341]\n",
      "loss: 0.209502  [19200/175341]\n",
      "loss: 0.826820  [20800/175341]\n",
      "loss: 0.227924  [22400/175341]\n",
      "loss: 0.541058  [24000/175341]\n",
      "loss: 0.751991  [25600/175341]\n",
      "loss: 0.427833  [27200/175341]\n",
      "loss: 0.473911  [28800/175341]\n",
      "loss: 1.407309  [30400/175341]\n",
      "loss: 0.441577  [32000/175341]\n",
      "loss: 0.744153  [33600/175341]\n",
      "loss: 0.328219  [35200/175341]\n",
      "loss: 0.135249  [36800/175341]\n",
      "loss: 0.398551  [38400/175341]\n",
      "loss: 0.571956  [40000/175341]\n",
      "loss: 1.424076  [41600/175341]\n",
      "loss: 0.283343  [43200/175341]\n",
      "loss: 0.448295  [44800/175341]\n",
      "loss: 0.443834  [46400/175341]\n",
      "loss: 1.028490  [48000/175341]\n",
      "loss: 0.401556  [49600/175341]\n",
      "loss: 0.323565  [51200/175341]\n",
      "loss: 0.574289  [52800/175341]\n",
      "loss: 1.009931  [54400/175341]\n",
      "loss: 0.602338  [56000/175341]\n",
      "loss: 0.522013  [57600/175341]\n",
      "loss: 0.459819  [59200/175341]\n",
      "loss: 0.869306  [60800/175341]\n",
      "loss: 0.377237  [62400/175341]\n",
      "loss: 0.319289  [64000/175341]\n",
      "loss: 0.505646  [65600/175341]\n",
      "loss: 0.179941  [67200/175341]\n",
      "loss: 0.204287  [68800/175341]\n",
      "loss: 0.366378  [70400/175341]\n",
      "loss: 0.686731  [72000/175341]\n",
      "loss: 0.297470  [73600/175341]\n",
      "loss: 0.311627  [75200/175341]\n",
      "loss: 0.387915  [76800/175341]\n",
      "loss: 0.254892  [78400/175341]\n",
      "loss: 0.519600  [80000/175341]\n",
      "loss: 0.254636  [81600/175341]\n",
      "loss: 0.174677  [83200/175341]\n",
      "loss: 0.388574  [84800/175341]\n",
      "loss: 0.224938  [86400/175341]\n",
      "loss: 0.322311  [88000/175341]\n",
      "loss: 0.421463  [89600/175341]\n",
      "loss: 0.427086  [91200/175341]\n",
      "loss: 0.481424  [92800/175341]\n",
      "loss: 0.673894  [94400/175341]\n",
      "loss: 0.564128  [96000/175341]\n",
      "loss: 0.400585  [97600/175341]\n",
      "loss: 0.600555  [99200/175341]\n",
      "loss: 0.379532  [100800/175341]\n",
      "loss: 0.321857  [102400/175341]\n",
      "loss: 0.499137  [104000/175341]\n",
      "loss: 0.113061  [105600/175341]\n",
      "loss: 0.292692  [107200/175341]\n",
      "loss: 0.131097  [108800/175341]\n",
      "loss: 0.353682  [110400/175341]\n",
      "loss: 0.952395  [112000/175341]\n",
      "loss: 0.370052  [113600/175341]\n",
      "loss: 0.494311  [115200/175341]\n",
      "loss: 0.709911  [116800/175341]\n",
      "loss: 0.764269  [118400/175341]\n",
      "loss: 0.149147  [120000/175341]\n",
      "loss: 0.485036  [121600/175341]\n",
      "loss: 0.416122  [123200/175341]\n",
      "loss: 0.964894  [124800/175341]\n",
      "loss: 0.811232  [126400/175341]\n",
      "loss: 0.788817  [128000/175341]\n",
      "loss: 0.456061  [129600/175341]\n",
      "loss: 0.298810  [131200/175341]\n",
      "loss: 0.550826  [132800/175341]\n",
      "loss: 0.266844  [134400/175341]\n",
      "loss: 0.336025  [136000/175341]\n",
      "loss: 0.448410  [137600/175341]\n",
      "loss: 0.233923  [139200/175341]\n",
      "loss: 0.373056  [140800/175341]\n",
      "loss: 0.977468  [142400/175341]\n",
      "loss: 0.428322  [144000/175341]\n",
      "loss: 0.364931  [145600/175341]\n",
      "loss: 0.295907  [147200/175341]\n",
      "loss: 0.537395  [148800/175341]\n",
      "loss: 0.710903  [150400/175341]\n",
      "loss: 0.584064  [152000/175341]\n",
      "loss: 0.444604  [153600/175341]\n",
      "loss: 0.529625  [155200/175341]\n",
      "loss: 0.861093  [156800/175341]\n",
      "loss: 0.479620  [158400/175341]\n",
      "loss: 0.425920  [160000/175341]\n",
      "loss: 0.543816  [161600/175341]\n",
      "loss: 0.695415  [163200/175341]\n",
      "loss: 0.317042  [164800/175341]\n",
      "loss: 0.743986  [166400/175341]\n",
      "loss: 0.160366  [168000/175341]\n",
      "loss: 0.468129  [169600/175341]\n",
      "loss: 0.215570  [171200/175341]\n",
      "loss: 0.512351  [172800/175341]\n",
      "loss: 0.498653  [174400/175341]\n",
      "Train Accuracy: 81.0090%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.543916, F1-score: 75.59%, Macro_F1-Score:  41.19%  \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.426643  [    0/175341]\n",
      "loss: 0.388790  [ 1600/175341]\n",
      "loss: 0.545996  [ 3200/175341]\n",
      "loss: 0.531490  [ 4800/175341]\n",
      "loss: 0.307780  [ 6400/175341]\n",
      "loss: 0.338539  [ 8000/175341]\n",
      "loss: 0.091274  [ 9600/175341]\n",
      "loss: 0.675275  [11200/175341]\n",
      "loss: 0.100392  [12800/175341]\n",
      "loss: 0.321231  [14400/175341]\n",
      "loss: 0.248784  [16000/175341]\n",
      "loss: 0.243034  [17600/175341]\n",
      "loss: 0.560215  [19200/175341]\n",
      "loss: 0.161475  [20800/175341]\n",
      "loss: 0.412297  [22400/175341]\n",
      "loss: 0.657555  [24000/175341]\n",
      "loss: 0.412553  [25600/175341]\n",
      "loss: 0.264720  [27200/175341]\n",
      "loss: 0.676760  [28800/175341]\n",
      "loss: 0.211237  [30400/175341]\n",
      "loss: 0.255143  [32000/175341]\n",
      "loss: 0.715760  [33600/175341]\n",
      "loss: 0.338210  [35200/175341]\n",
      "loss: 0.336800  [36800/175341]\n",
      "loss: 0.864456  [38400/175341]\n",
      "loss: 0.411575  [40000/175341]\n",
      "loss: 0.565916  [41600/175341]\n",
      "loss: 0.617356  [43200/175341]\n",
      "loss: 0.194550  [44800/175341]\n",
      "loss: 0.422095  [46400/175341]\n",
      "loss: 0.736195  [48000/175341]\n",
      "loss: 0.203952  [49600/175341]\n",
      "loss: 0.235270  [51200/175341]\n",
      "loss: 0.875676  [52800/175341]\n",
      "loss: 0.185381  [54400/175341]\n",
      "loss: 0.497124  [56000/175341]\n",
      "loss: 0.340952  [57600/175341]\n",
      "loss: 0.499303  [59200/175341]\n",
      "loss: 0.664899  [60800/175341]\n",
      "loss: 0.203808  [62400/175341]\n",
      "loss: 0.428272  [64000/175341]\n",
      "loss: 0.633568  [65600/175341]\n",
      "loss: 0.458659  [67200/175341]\n",
      "loss: 0.584716  [68800/175341]\n",
      "loss: 0.457267  [70400/175341]\n",
      "loss: 0.288545  [72000/175341]\n",
      "loss: 0.538315  [73600/175341]\n",
      "loss: 0.474065  [75200/175341]\n",
      "loss: 0.411954  [76800/175341]\n",
      "loss: 0.697129  [78400/175341]\n",
      "loss: 0.412500  [80000/175341]\n",
      "loss: 0.629917  [81600/175341]\n",
      "loss: 0.432428  [83200/175341]\n",
      "loss: 0.426322  [84800/175341]\n",
      "loss: 0.643028  [86400/175341]\n",
      "loss: 0.493727  [88000/175341]\n",
      "loss: 0.258975  [89600/175341]\n",
      "loss: 0.614847  [91200/175341]\n",
      "loss: 0.331304  [92800/175341]\n",
      "loss: 0.676518  [94400/175341]\n",
      "loss: 0.231467  [96000/175341]\n",
      "loss: 0.624189  [97600/175341]\n",
      "loss: 0.415583  [99200/175341]\n",
      "loss: 0.318727  [100800/175341]\n",
      "loss: 0.858768  [102400/175341]\n",
      "loss: 0.263674  [104000/175341]\n",
      "loss: 0.707496  [105600/175341]\n",
      "loss: 0.453653  [107200/175341]\n",
      "loss: 0.695262  [108800/175341]\n",
      "loss: 0.663048  [110400/175341]\n",
      "loss: 1.047398  [112000/175341]\n",
      "loss: 0.401831  [113600/175341]\n",
      "loss: 0.495824  [115200/175341]\n",
      "loss: 0.263688  [116800/175341]\n",
      "loss: 0.157642  [118400/175341]\n",
      "loss: 0.669117  [120000/175341]\n",
      "loss: 0.677101  [121600/175341]\n",
      "loss: 0.576405  [123200/175341]\n",
      "loss: 0.460622  [124800/175341]\n",
      "loss: 0.591165  [126400/175341]\n",
      "loss: 0.580071  [128000/175341]\n",
      "loss: 0.699024  [129600/175341]\n",
      "loss: 0.346975  [131200/175341]\n",
      "loss: 0.379238  [132800/175341]\n",
      "loss: 0.483719  [134400/175341]\n",
      "loss: 0.553920  [136000/175341]\n",
      "loss: 0.532719  [137600/175341]\n",
      "loss: 0.453325  [139200/175341]\n",
      "loss: 0.459084  [140800/175341]\n",
      "loss: 0.176017  [142400/175341]\n",
      "loss: 0.718149  [144000/175341]\n",
      "loss: 0.528538  [145600/175341]\n",
      "loss: 0.547748  [147200/175341]\n",
      "loss: 0.613507  [148800/175341]\n",
      "loss: 0.169631  [150400/175341]\n",
      "loss: 0.709684  [152000/175341]\n",
      "loss: 0.471410  [153600/175341]\n",
      "loss: 0.055094  [155200/175341]\n",
      "loss: 0.321289  [156800/175341]\n",
      "loss: 0.398886  [158400/175341]\n",
      "loss: 0.099988  [160000/175341]\n",
      "loss: 0.270171  [161600/175341]\n",
      "loss: 0.496393  [163200/175341]\n",
      "loss: 0.733048  [164800/175341]\n",
      "loss: 0.649830  [166400/175341]\n",
      "loss: 0.207935  [168000/175341]\n",
      "loss: 0.646705  [169600/175341]\n",
      "loss: 0.280383  [171200/175341]\n",
      "loss: 0.763947  [172800/175341]\n",
      "loss: 0.602843  [174400/175341]\n",
      "Train Accuracy: 81.0478%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.557290, F1-score: 74.75%, Macro_F1-Score:  41.44%  \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.595216  [    0/175341]\n",
      "loss: 0.533387  [ 1600/175341]\n",
      "loss: 0.225971  [ 3200/175341]\n",
      "loss: 0.388569  [ 4800/175341]\n",
      "loss: 0.557086  [ 6400/175341]\n",
      "loss: 0.814206  [ 8000/175341]\n",
      "loss: 0.530165  [ 9600/175341]\n",
      "loss: 0.536673  [11200/175341]\n",
      "loss: 0.346049  [12800/175341]\n",
      "loss: 0.659138  [14400/175341]\n",
      "loss: 0.365068  [16000/175341]\n",
      "loss: 0.068764  [17600/175341]\n",
      "loss: 0.420373  [19200/175341]\n",
      "loss: 0.381414  [20800/175341]\n",
      "loss: 0.365600  [22400/175341]\n",
      "loss: 0.092863  [24000/175341]\n",
      "loss: 0.609701  [25600/175341]\n",
      "loss: 0.775493  [27200/175341]\n",
      "loss: 0.719039  [28800/175341]\n",
      "loss: 0.750031  [30400/175341]\n",
      "loss: 0.466743  [32000/175341]\n",
      "loss: 0.359398  [33600/175341]\n",
      "loss: 0.382021  [35200/175341]\n",
      "loss: 0.225838  [36800/175341]\n",
      "loss: 0.627205  [38400/175341]\n",
      "loss: 0.707883  [40000/175341]\n",
      "loss: 0.410828  [41600/175341]\n",
      "loss: 0.316104  [43200/175341]\n",
      "loss: 0.336221  [44800/175341]\n",
      "loss: 0.378275  [46400/175341]\n",
      "loss: 0.376824  [48000/175341]\n",
      "loss: 0.511941  [49600/175341]\n",
      "loss: 0.342716  [51200/175341]\n",
      "loss: 0.480974  [52800/175341]\n",
      "loss: 0.541409  [54400/175341]\n",
      "loss: 0.521636  [56000/175341]\n",
      "loss: 0.585554  [57600/175341]\n",
      "loss: 1.036867  [59200/175341]\n",
      "loss: 0.384811  [60800/175341]\n",
      "loss: 0.482834  [62400/175341]\n",
      "loss: 0.388610  [64000/175341]\n",
      "loss: 0.401522  [65600/175341]\n",
      "loss: 0.667647  [67200/175341]\n",
      "loss: 0.534798  [68800/175341]\n",
      "loss: 0.758734  [70400/175341]\n",
      "loss: 0.539460  [72000/175341]\n",
      "loss: 0.424035  [73600/175341]\n",
      "loss: 0.297628  [75200/175341]\n",
      "loss: 0.335681  [76800/175341]\n",
      "loss: 0.532556  [78400/175341]\n",
      "loss: 0.392164  [80000/175341]\n",
      "loss: 0.380414  [81600/175341]\n",
      "loss: 0.350595  [83200/175341]\n",
      "loss: 0.556615  [84800/175341]\n",
      "loss: 0.768338  [86400/175341]\n",
      "loss: 0.949899  [88000/175341]\n",
      "loss: 0.531201  [89600/175341]\n",
      "loss: 0.578137  [91200/175341]\n",
      "loss: 0.819258  [92800/175341]\n",
      "loss: 0.484939  [94400/175341]\n",
      "loss: 0.428773  [96000/175341]\n",
      "loss: 0.499187  [97600/175341]\n",
      "loss: 0.608265  [99200/175341]\n",
      "loss: 0.308841  [100800/175341]\n",
      "loss: 0.481183  [102400/175341]\n",
      "loss: 0.467190  [104000/175341]\n",
      "loss: 1.197510  [105600/175341]\n",
      "loss: 0.197144  [107200/175341]\n",
      "loss: 0.543726  [108800/175341]\n",
      "loss: 0.413401  [110400/175341]\n",
      "loss: 0.266146  [112000/175341]\n",
      "loss: 0.566490  [113600/175341]\n",
      "loss: 0.553829  [115200/175341]\n",
      "loss: 0.759481  [116800/175341]\n",
      "loss: 0.463140  [118400/175341]\n",
      "loss: 0.532468  [120000/175341]\n",
      "loss: 0.305960  [121600/175341]\n",
      "loss: 0.177535  [123200/175341]\n",
      "loss: 0.482722  [124800/175341]\n",
      "loss: 0.193797  [126400/175341]\n",
      "loss: 0.377166  [128000/175341]\n",
      "loss: 0.185497  [129600/175341]\n",
      "loss: 0.583715  [131200/175341]\n",
      "loss: 0.190089  [132800/175341]\n",
      "loss: 0.641764  [134400/175341]\n",
      "loss: 0.268447  [136000/175341]\n",
      "loss: 0.096138  [137600/175341]\n",
      "loss: 0.436002  [139200/175341]\n",
      "loss: 0.836720  [140800/175341]\n",
      "loss: 0.324721  [142400/175341]\n",
      "loss: 0.551258  [144000/175341]\n",
      "loss: 0.488926  [145600/175341]\n",
      "loss: 0.339302  [147200/175341]\n",
      "loss: 1.086904  [148800/175341]\n",
      "loss: 1.037886  [150400/175341]\n",
      "loss: 0.445908  [152000/175341]\n",
      "loss: 0.357085  [153600/175341]\n",
      "loss: 0.296410  [155200/175341]\n",
      "loss: 0.385652  [156800/175341]\n",
      "loss: 0.590186  [158400/175341]\n",
      "loss: 0.517440  [160000/175341]\n",
      "loss: 0.378208  [161600/175341]\n",
      "loss: 0.692595  [163200/175341]\n",
      "loss: 0.343285  [164800/175341]\n",
      "loss: 0.388609  [166400/175341]\n",
      "loss: 0.651371  [168000/175341]\n",
      "loss: 0.449814  [169600/175341]\n",
      "loss: 0.167286  [171200/175341]\n",
      "loss: 0.166566  [172800/175341]\n",
      "loss: 0.307237  [174400/175341]\n",
      "Train Accuracy: 81.0495%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.550891, F1-score: 75.27%, Macro_F1-Score:  41.39%  \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.376165  [    0/175341]\n",
      "loss: 0.401678  [ 1600/175341]\n",
      "loss: 0.785892  [ 3200/175341]\n",
      "loss: 0.656228  [ 4800/175341]\n",
      "loss: 0.325307  [ 6400/175341]\n",
      "loss: 0.632910  [ 8000/175341]\n",
      "loss: 0.400955  [ 9600/175341]\n",
      "loss: 0.424144  [11200/175341]\n",
      "loss: 0.247133  [12800/175341]\n",
      "loss: 0.580576  [14400/175341]\n",
      "loss: 0.758034  [16000/175341]\n",
      "loss: 0.360226  [17600/175341]\n",
      "loss: 0.516778  [19200/175341]\n",
      "loss: 0.453510  [20800/175341]\n",
      "loss: 0.625465  [22400/175341]\n",
      "loss: 0.303711  [24000/175341]\n",
      "loss: 1.015808  [25600/175341]\n",
      "loss: 0.664068  [27200/175341]\n",
      "loss: 0.473592  [28800/175341]\n",
      "loss: 0.624666  [30400/175341]\n",
      "loss: 0.727676  [32000/175341]\n",
      "loss: 0.285153  [33600/175341]\n",
      "loss: 0.604836  [35200/175341]\n",
      "loss: 0.172833  [36800/175341]\n",
      "loss: 0.177998  [38400/175341]\n",
      "loss: 0.263178  [40000/175341]\n",
      "loss: 0.139963  [41600/175341]\n",
      "loss: 0.630297  [43200/175341]\n",
      "loss: 0.276353  [44800/175341]\n",
      "loss: 0.623864  [46400/175341]\n",
      "loss: 0.451541  [48000/175341]\n",
      "loss: 0.804122  [49600/175341]\n",
      "loss: 0.498115  [51200/175341]\n",
      "loss: 0.407447  [52800/175341]\n",
      "loss: 0.357879  [54400/175341]\n",
      "loss: 0.695642  [56000/175341]\n",
      "loss: 0.530362  [57600/175341]\n",
      "loss: 0.314886  [59200/175341]\n",
      "loss: 0.825445  [60800/175341]\n",
      "loss: 0.351200  [62400/175341]\n",
      "loss: 0.309154  [64000/175341]\n",
      "loss: 0.154183  [65600/175341]\n",
      "loss: 0.088939  [67200/175341]\n",
      "loss: 0.484772  [68800/175341]\n",
      "loss: 0.648733  [70400/175341]\n",
      "loss: 0.349417  [72000/175341]\n",
      "loss: 0.448676  [73600/175341]\n",
      "loss: 0.386564  [75200/175341]\n",
      "loss: 0.345157  [76800/175341]\n",
      "loss: 0.779464  [78400/175341]\n",
      "loss: 0.292132  [80000/175341]\n",
      "loss: 0.558730  [81600/175341]\n",
      "loss: 0.306058  [83200/175341]\n",
      "loss: 0.576746  [84800/175341]\n",
      "loss: 0.450795  [86400/175341]\n",
      "loss: 0.254426  [88000/175341]\n",
      "loss: 0.387385  [89600/175341]\n",
      "loss: 0.272558  [91200/175341]\n",
      "loss: 0.179438  [92800/175341]\n",
      "loss: 0.332180  [94400/175341]\n",
      "loss: 0.174486  [96000/175341]\n",
      "loss: 0.106911  [97600/175341]\n",
      "loss: 0.541368  [99200/175341]\n",
      "loss: 0.431377  [100800/175341]\n",
      "loss: 0.542774  [102400/175341]\n",
      "loss: 0.131849  [104000/175341]\n",
      "loss: 0.568411  [105600/175341]\n",
      "loss: 0.473014  [107200/175341]\n",
      "loss: 0.181652  [108800/175341]\n",
      "loss: 0.678558  [110400/175341]\n",
      "loss: 0.127295  [112000/175341]\n",
      "loss: 0.394926  [113600/175341]\n",
      "loss: 0.398588  [115200/175341]\n",
      "loss: 0.335558  [116800/175341]\n",
      "loss: 0.152719  [118400/175341]\n",
      "loss: 0.256500  [120000/175341]\n",
      "loss: 0.546258  [121600/175341]\n",
      "loss: 0.293509  [123200/175341]\n",
      "loss: 0.261177  [124800/175341]\n",
      "loss: 0.358883  [126400/175341]\n",
      "loss: 0.434328  [128000/175341]\n",
      "loss: 0.150100  [129600/175341]\n",
      "loss: 0.720745  [131200/175341]\n",
      "loss: 0.333503  [132800/175341]\n",
      "loss: 0.691709  [134400/175341]\n",
      "loss: 0.498655  [136000/175341]\n",
      "loss: 0.166528  [137600/175341]\n",
      "loss: 0.571452  [139200/175341]\n",
      "loss: 0.844608  [140800/175341]\n",
      "loss: 0.675867  [142400/175341]\n",
      "loss: 0.253775  [144000/175341]\n",
      "loss: 0.622740  [145600/175341]\n",
      "loss: 0.614395  [147200/175341]\n",
      "loss: 0.713764  [148800/175341]\n",
      "loss: 0.690603  [150400/175341]\n",
      "loss: 0.687916  [152000/175341]\n",
      "loss: 0.623454  [153600/175341]\n",
      "loss: 0.571512  [155200/175341]\n",
      "loss: 0.422934  [156800/175341]\n",
      "loss: 1.006635  [158400/175341]\n",
      "loss: 0.208354  [160000/175341]\n",
      "loss: 0.473018  [161600/175341]\n",
      "loss: 0.310335  [163200/175341]\n",
      "loss: 0.431984  [164800/175341]\n",
      "loss: 0.678609  [166400/175341]\n",
      "loss: 0.474222  [168000/175341]\n",
      "loss: 0.854349  [169600/175341]\n",
      "loss: 0.399773  [171200/175341]\n",
      "loss: 0.640621  [172800/175341]\n",
      "loss: 0.663670  [174400/175341]\n",
      "Train Accuracy: 81.1003%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.551357, F1-score: 75.60%, Macro_F1-Score:  42.34%  \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.390210  [    0/175341]\n",
      "loss: 0.460425  [ 1600/175341]\n",
      "loss: 0.383346  [ 3200/175341]\n",
      "loss: 0.191612  [ 4800/175341]\n",
      "loss: 0.260455  [ 6400/175341]\n",
      "loss: 0.553430  [ 8000/175341]\n",
      "loss: 0.539906  [ 9600/175341]\n",
      "loss: 0.169501  [11200/175341]\n",
      "loss: 0.233839  [12800/175341]\n",
      "loss: 1.181820  [14400/175341]\n",
      "loss: 0.503280  [16000/175341]\n",
      "loss: 0.474314  [17600/175341]\n",
      "loss: 0.267072  [19200/175341]\n",
      "loss: 0.584524  [20800/175341]\n",
      "loss: 0.421303  [22400/175341]\n",
      "loss: 0.744437  [24000/175341]\n",
      "loss: 0.921693  [25600/175341]\n",
      "loss: 0.555942  [27200/175341]\n",
      "loss: 0.430456  [28800/175341]\n",
      "loss: 0.525941  [30400/175341]\n",
      "loss: 0.819357  [32000/175341]\n",
      "loss: 0.378374  [33600/175341]\n",
      "loss: 0.209766  [35200/175341]\n",
      "loss: 0.651122  [36800/175341]\n",
      "loss: 0.789108  [38400/175341]\n",
      "loss: 0.891785  [40000/175341]\n",
      "loss: 0.459229  [41600/175341]\n",
      "loss: 0.235794  [43200/175341]\n",
      "loss: 0.352425  [44800/175341]\n",
      "loss: 0.416847  [46400/175341]\n",
      "loss: 0.458938  [48000/175341]\n",
      "loss: 0.203421  [49600/175341]\n",
      "loss: 0.605876  [51200/175341]\n",
      "loss: 0.376374  [52800/175341]\n",
      "loss: 0.675873  [54400/175341]\n",
      "loss: 0.404973  [56000/175341]\n",
      "loss: 0.579033  [57600/175341]\n",
      "loss: 0.368707  [59200/175341]\n",
      "loss: 0.275937  [60800/175341]\n",
      "loss: 0.418393  [62400/175341]\n",
      "loss: 0.518539  [64000/175341]\n",
      "loss: 0.435148  [65600/175341]\n",
      "loss: 0.203532  [67200/175341]\n",
      "loss: 0.411780  [68800/175341]\n",
      "loss: 0.433194  [70400/175341]\n",
      "loss: 0.352565  [72000/175341]\n",
      "loss: 0.489981  [73600/175341]\n",
      "loss: 0.481309  [75200/175341]\n",
      "loss: 0.658989  [76800/175341]\n",
      "loss: 0.289314  [78400/175341]\n",
      "loss: 0.480709  [80000/175341]\n",
      "loss: 0.811664  [81600/175341]\n",
      "loss: 0.292414  [83200/175341]\n",
      "loss: 0.669747  [84800/175341]\n",
      "loss: 0.632761  [86400/175341]\n",
      "loss: 0.387828  [88000/175341]\n",
      "loss: 0.365169  [89600/175341]\n",
      "loss: 0.613295  [91200/175341]\n",
      "loss: 0.466084  [92800/175341]\n",
      "loss: 0.267897  [94400/175341]\n",
      "loss: 0.315060  [96000/175341]\n",
      "loss: 0.512198  [97600/175341]\n",
      "loss: 0.603908  [99200/175341]\n",
      "loss: 0.222014  [100800/175341]\n",
      "loss: 0.451218  [102400/175341]\n",
      "loss: 0.459766  [104000/175341]\n",
      "loss: 0.444487  [105600/175341]\n",
      "loss: 0.843897  [107200/175341]\n",
      "loss: 0.452277  [108800/175341]\n",
      "loss: 0.527737  [110400/175341]\n",
      "loss: 0.605670  [112000/175341]\n",
      "loss: 0.529922  [113600/175341]\n",
      "loss: 0.411606  [115200/175341]\n",
      "loss: 0.395135  [116800/175341]\n",
      "loss: 0.483768  [118400/175341]\n",
      "loss: 0.290096  [120000/175341]\n",
      "loss: 0.492402  [121600/175341]\n",
      "loss: 0.573128  [123200/175341]\n",
      "loss: 0.570771  [124800/175341]\n",
      "loss: 0.235703  [126400/175341]\n",
      "loss: 0.497313  [128000/175341]\n",
      "loss: 0.321986  [129600/175341]\n",
      "loss: 0.245208  [131200/175341]\n",
      "loss: 0.545457  [132800/175341]\n",
      "loss: 0.236062  [134400/175341]\n",
      "loss: 0.329373  [136000/175341]\n",
      "loss: 0.381198  [137600/175341]\n",
      "loss: 0.401659  [139200/175341]\n",
      "loss: 0.233139  [140800/175341]\n",
      "loss: 0.463374  [142400/175341]\n",
      "loss: 0.593635  [144000/175341]\n",
      "loss: 0.716781  [145600/175341]\n",
      "loss: 0.150701  [147200/175341]\n",
      "loss: 0.302888  [148800/175341]\n",
      "loss: 1.026856  [150400/175341]\n",
      "loss: 0.601234  [152000/175341]\n",
      "loss: 0.641944  [153600/175341]\n",
      "loss: 0.365616  [155200/175341]\n",
      "loss: 0.563839  [156800/175341]\n",
      "loss: 0.325770  [158400/175341]\n",
      "loss: 0.642751  [160000/175341]\n",
      "loss: 0.301260  [161600/175341]\n",
      "loss: 0.305584  [163200/175341]\n",
      "loss: 0.624311  [164800/175341]\n",
      "loss: 0.189786  [166400/175341]\n",
      "loss: 0.717521  [168000/175341]\n",
      "loss: 0.429383  [169600/175341]\n",
      "loss: 0.191708  [171200/175341]\n",
      "loss: 0.361931  [172800/175341]\n",
      "loss: 0.636656  [174400/175341]\n",
      "Train Accuracy: 81.0609%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.543979, F1-score: 75.55%, Macro_F1-Score:  41.02%  \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.726906  [    0/175341]\n",
      "loss: 0.655725  [ 1600/175341]\n",
      "loss: 0.294425  [ 3200/175341]\n",
      "loss: 0.304988  [ 4800/175341]\n",
      "loss: 0.309097  [ 6400/175341]\n",
      "loss: 0.404328  [ 8000/175341]\n",
      "loss: 0.480772  [ 9600/175341]\n",
      "loss: 0.353674  [11200/175341]\n",
      "loss: 0.515365  [12800/175341]\n",
      "loss: 0.493351  [14400/175341]\n",
      "loss: 0.346437  [16000/175341]\n",
      "loss: 0.675790  [17600/175341]\n",
      "loss: 0.237993  [19200/175341]\n",
      "loss: 0.518094  [20800/175341]\n",
      "loss: 0.678465  [22400/175341]\n",
      "loss: 0.084808  [24000/175341]\n",
      "loss: 0.423159  [25600/175341]\n",
      "loss: 0.500400  [27200/175341]\n",
      "loss: 0.453861  [28800/175341]\n",
      "loss: 0.543607  [30400/175341]\n",
      "loss: 0.766377  [32000/175341]\n",
      "loss: 0.286224  [33600/175341]\n",
      "loss: 0.437358  [35200/175341]\n",
      "loss: 0.581659  [36800/175341]\n",
      "loss: 0.362309  [38400/175341]\n",
      "loss: 0.624402  [40000/175341]\n",
      "loss: 0.293777  [41600/175341]\n",
      "loss: 0.367280  [43200/175341]\n",
      "loss: 0.099097  [44800/175341]\n",
      "loss: 0.486964  [46400/175341]\n",
      "loss: 0.650001  [48000/175341]\n",
      "loss: 0.700731  [49600/175341]\n",
      "loss: 0.244005  [51200/175341]\n",
      "loss: 0.636888  [52800/175341]\n",
      "loss: 0.243853  [54400/175341]\n",
      "loss: 0.237474  [56000/175341]\n",
      "loss: 0.339522  [57600/175341]\n",
      "loss: 0.137393  [59200/175341]\n",
      "loss: 0.641600  [60800/175341]\n",
      "loss: 0.475594  [62400/175341]\n",
      "loss: 0.398237  [64000/175341]\n",
      "loss: 0.733248  [65600/175341]\n",
      "loss: 0.574986  [67200/175341]\n",
      "loss: 0.361279  [68800/175341]\n",
      "loss: 0.890727  [70400/175341]\n",
      "loss: 0.235197  [72000/175341]\n",
      "loss: 0.295994  [73600/175341]\n",
      "loss: 0.228674  [75200/175341]\n",
      "loss: 0.091301  [76800/175341]\n",
      "loss: 0.449180  [78400/175341]\n",
      "loss: 0.411046  [80000/175341]\n",
      "loss: 0.235454  [81600/175341]\n",
      "loss: 0.117179  [83200/175341]\n",
      "loss: 0.119631  [84800/175341]\n",
      "loss: 0.239832  [86400/175341]\n",
      "loss: 0.395126  [88000/175341]\n",
      "loss: 0.573441  [89600/175341]\n",
      "loss: 0.188416  [91200/175341]\n",
      "loss: 0.191570  [92800/175341]\n",
      "loss: 0.459475  [94400/175341]\n",
      "loss: 0.258772  [96000/175341]\n",
      "loss: 0.303945  [97600/175341]\n",
      "loss: 0.540510  [99200/175341]\n",
      "loss: 0.414356  [100800/175341]\n",
      "loss: 0.548268  [102400/175341]\n",
      "loss: 0.374607  [104000/175341]\n",
      "loss: 0.564054  [105600/175341]\n",
      "loss: 0.597123  [107200/175341]\n",
      "loss: 0.132268  [108800/175341]\n",
      "loss: 0.315775  [110400/175341]\n",
      "loss: 0.367789  [112000/175341]\n",
      "loss: 0.768105  [113600/175341]\n",
      "loss: 0.151353  [115200/175341]\n",
      "loss: 0.723330  [116800/175341]\n",
      "loss: 0.878313  [118400/175341]\n",
      "loss: 0.342120  [120000/175341]\n",
      "loss: 0.673809  [121600/175341]\n",
      "loss: 0.630270  [123200/175341]\n",
      "loss: 0.321679  [124800/175341]\n",
      "loss: 0.378390  [126400/175341]\n",
      "loss: 0.307465  [128000/175341]\n",
      "loss: 0.664386  [129600/175341]\n",
      "loss: 0.436094  [131200/175341]\n",
      "loss: 0.187921  [132800/175341]\n",
      "loss: 0.477875  [134400/175341]\n",
      "loss: 0.279776  [136000/175341]\n",
      "loss: 0.167185  [137600/175341]\n",
      "loss: 0.485511  [139200/175341]\n",
      "loss: 0.549536  [140800/175341]\n",
      "loss: 0.305291  [142400/175341]\n",
      "loss: 0.714874  [144000/175341]\n",
      "loss: 0.449256  [145600/175341]\n",
      "loss: 0.938572  [147200/175341]\n",
      "loss: 0.605656  [148800/175341]\n",
      "loss: 0.222021  [150400/175341]\n",
      "loss: 0.674090  [152000/175341]\n",
      "loss: 0.358111  [153600/175341]\n",
      "loss: 0.247803  [155200/175341]\n",
      "loss: 0.428490  [156800/175341]\n",
      "loss: 0.431155  [158400/175341]\n",
      "loss: 0.551273  [160000/175341]\n",
      "loss: 0.494386  [161600/175341]\n",
      "loss: 0.355927  [163200/175341]\n",
      "loss: 0.253816  [164800/175341]\n",
      "loss: 0.398268  [166400/175341]\n",
      "loss: 0.265086  [168000/175341]\n",
      "loss: 0.493399  [169600/175341]\n",
      "loss: 0.254465  [171200/175341]\n",
      "loss: 0.372757  [172800/175341]\n",
      "loss: 0.894047  [174400/175341]\n",
      "Train Accuracy: 81.0649%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.543656, F1-score: 76.03%, Macro_F1-Score:  41.64%  \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.451096  [    0/175341]\n",
      "loss: 0.370955  [ 1600/175341]\n",
      "loss: 0.231909  [ 3200/175341]\n",
      "loss: 0.244909  [ 4800/175341]\n",
      "loss: 0.690646  [ 6400/175341]\n",
      "loss: 0.316454  [ 8000/175341]\n",
      "loss: 0.287762  [ 9600/175341]\n",
      "loss: 0.457870  [11200/175341]\n",
      "loss: 0.260117  [12800/175341]\n",
      "loss: 0.144196  [14400/175341]\n",
      "loss: 0.623751  [16000/175341]\n",
      "loss: 0.389574  [17600/175341]\n",
      "loss: 0.652232  [19200/175341]\n",
      "loss: 0.355421  [20800/175341]\n",
      "loss: 0.390208  [22400/175341]\n",
      "loss: 0.207662  [24000/175341]\n",
      "loss: 0.144803  [25600/175341]\n",
      "loss: 0.337425  [27200/175341]\n",
      "loss: 0.575816  [28800/175341]\n",
      "loss: 0.424908  [30400/175341]\n",
      "loss: 0.456331  [32000/175341]\n",
      "loss: 0.299535  [33600/175341]\n",
      "loss: 0.372634  [35200/175341]\n",
      "loss: 0.630969  [36800/175341]\n",
      "loss: 0.543299  [38400/175341]\n",
      "loss: 0.419768  [40000/175341]\n",
      "loss: 0.907756  [41600/175341]\n",
      "loss: 0.469339  [43200/175341]\n",
      "loss: 0.677935  [44800/175341]\n",
      "loss: 1.084082  [46400/175341]\n",
      "loss: 0.407116  [48000/175341]\n",
      "loss: 0.486315  [49600/175341]\n",
      "loss: 0.202723  [51200/175341]\n",
      "loss: 0.519281  [52800/175341]\n",
      "loss: 0.211332  [54400/175341]\n",
      "loss: 0.393016  [56000/175341]\n",
      "loss: 0.437520  [57600/175341]\n",
      "loss: 0.544256  [59200/175341]\n",
      "loss: 0.732242  [60800/175341]\n",
      "loss: 0.409580  [62400/175341]\n",
      "loss: 0.558493  [64000/175341]\n",
      "loss: 0.617120  [65600/175341]\n",
      "loss: 0.460751  [67200/175341]\n",
      "loss: 0.598884  [68800/175341]\n",
      "loss: 0.354878  [70400/175341]\n",
      "loss: 0.406071  [72000/175341]\n",
      "loss: 0.310504  [73600/175341]\n",
      "loss: 0.533633  [75200/175341]\n",
      "loss: 0.587544  [76800/175341]\n",
      "loss: 0.258606  [78400/175341]\n",
      "loss: 0.661698  [80000/175341]\n",
      "loss: 0.389679  [81600/175341]\n",
      "loss: 0.122775  [83200/175341]\n",
      "loss: 0.336085  [84800/175341]\n",
      "loss: 0.548652  [86400/175341]\n",
      "loss: 0.332121  [88000/175341]\n",
      "loss: 1.153522  [89600/175341]\n",
      "loss: 0.377095  [91200/175341]\n",
      "loss: 0.451815  [92800/175341]\n",
      "loss: 0.638245  [94400/175341]\n",
      "loss: 0.666932  [96000/175341]\n",
      "loss: 0.320731  [97600/175341]\n",
      "loss: 0.181780  [99200/175341]\n",
      "loss: 0.249414  [100800/175341]\n",
      "loss: 0.184653  [102400/175341]\n",
      "loss: 0.367641  [104000/175341]\n",
      "loss: 0.184681  [105600/175341]\n",
      "loss: 0.752770  [107200/175341]\n",
      "loss: 0.512611  [108800/175341]\n",
      "loss: 0.483898  [110400/175341]\n",
      "loss: 0.180170  [112000/175341]\n",
      "loss: 0.672175  [113600/175341]\n",
      "loss: 0.411022  [115200/175341]\n",
      "loss: 0.446815  [116800/175341]\n",
      "loss: 0.642369  [118400/175341]\n",
      "loss: 0.264225  [120000/175341]\n",
      "loss: 0.207017  [121600/175341]\n",
      "loss: 0.392333  [123200/175341]\n",
      "loss: 0.511839  [124800/175341]\n",
      "loss: 0.735991  [126400/175341]\n",
      "loss: 0.503453  [128000/175341]\n",
      "loss: 0.277879  [129600/175341]\n",
      "loss: 0.631073  [131200/175341]\n",
      "loss: 0.490982  [132800/175341]\n",
      "loss: 0.500967  [134400/175341]\n",
      "loss: 0.576506  [136000/175341]\n",
      "loss: 0.253451  [137600/175341]\n",
      "loss: 0.657273  [139200/175341]\n",
      "loss: 0.667988  [140800/175341]\n",
      "loss: 0.330003  [142400/175341]\n",
      "loss: 0.240445  [144000/175341]\n",
      "loss: 0.100322  [145600/175341]\n",
      "loss: 0.699158  [147200/175341]\n",
      "loss: 0.143108  [148800/175341]\n",
      "loss: 0.589970  [150400/175341]\n",
      "loss: 0.224213  [152000/175341]\n",
      "loss: 0.229561  [153600/175341]\n",
      "loss: 0.915902  [155200/175341]\n",
      "loss: 0.368260  [156800/175341]\n",
      "loss: 0.627619  [158400/175341]\n",
      "loss: 0.139412  [160000/175341]\n",
      "loss: 0.406760  [161600/175341]\n",
      "loss: 0.369934  [163200/175341]\n",
      "loss: 0.182820  [164800/175341]\n",
      "loss: 0.409339  [166400/175341]\n",
      "loss: 0.413233  [168000/175341]\n",
      "loss: 0.401893  [169600/175341]\n",
      "loss: 0.767668  [171200/175341]\n",
      "loss: 0.517785  [172800/175341]\n",
      "loss: 0.481607  [174400/175341]\n",
      "Train Accuracy: 81.0963%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.550724, F1-score: 74.80%, Macro_F1-Score:  41.05%  \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.531039  [    0/175341]\n",
      "loss: 0.270345  [ 1600/175341]\n",
      "loss: 0.392217  [ 3200/175341]\n",
      "loss: 0.440634  [ 4800/175341]\n",
      "loss: 0.503957  [ 6400/175341]\n",
      "loss: 0.511140  [ 8000/175341]\n",
      "loss: 0.257292  [ 9600/175341]\n",
      "loss: 0.674570  [11200/175341]\n",
      "loss: 0.209459  [12800/175341]\n",
      "loss: 0.148029  [14400/175341]\n",
      "loss: 0.600098  [16000/175341]\n",
      "loss: 0.317194  [17600/175341]\n",
      "loss: 0.587673  [19200/175341]\n",
      "loss: 0.291173  [20800/175341]\n",
      "loss: 0.280765  [22400/175341]\n",
      "loss: 0.702060  [24000/175341]\n",
      "loss: 0.724806  [25600/175341]\n",
      "loss: 0.409577  [27200/175341]\n",
      "loss: 0.426320  [28800/175341]\n",
      "loss: 0.480182  [30400/175341]\n",
      "loss: 0.420344  [32000/175341]\n",
      "loss: 0.688515  [33600/175341]\n",
      "loss: 0.521833  [35200/175341]\n",
      "loss: 0.268553  [36800/175341]\n",
      "loss: 0.420791  [38400/175341]\n",
      "loss: 0.329647  [40000/175341]\n",
      "loss: 0.351570  [41600/175341]\n",
      "loss: 0.492539  [43200/175341]\n",
      "loss: 0.250207  [44800/175341]\n",
      "loss: 0.283314  [46400/175341]\n",
      "loss: 0.127413  [48000/175341]\n",
      "loss: 0.515847  [49600/175341]\n",
      "loss: 0.324048  [51200/175341]\n",
      "loss: 0.707310  [52800/175341]\n",
      "loss: 0.526036  [54400/175341]\n",
      "loss: 0.519877  [56000/175341]\n",
      "loss: 0.385192  [57600/175341]\n",
      "loss: 0.571054  [59200/175341]\n",
      "loss: 0.431922  [60800/175341]\n",
      "loss: 0.365076  [62400/175341]\n",
      "loss: 0.610475  [64000/175341]\n",
      "loss: 0.315559  [65600/175341]\n",
      "loss: 0.616460  [67200/175341]\n",
      "loss: 0.316449  [68800/175341]\n",
      "loss: 0.488613  [70400/175341]\n",
      "loss: 0.707843  [72000/175341]\n",
      "loss: 0.275654  [73600/175341]\n",
      "loss: 0.664464  [75200/175341]\n",
      "loss: 0.602346  [76800/175341]\n",
      "loss: 0.332902  [78400/175341]\n",
      "loss: 0.343783  [80000/175341]\n",
      "loss: 0.636003  [81600/175341]\n",
      "loss: 0.496486  [83200/175341]\n",
      "loss: 0.785193  [84800/175341]\n",
      "loss: 0.708805  [86400/175341]\n",
      "loss: 0.126458  [88000/175341]\n",
      "loss: 0.209853  [89600/175341]\n",
      "loss: 0.361042  [91200/175341]\n",
      "loss: 1.135011  [92800/175341]\n",
      "loss: 0.530244  [94400/175341]\n",
      "loss: 0.491684  [96000/175341]\n",
      "loss: 0.726720  [97600/175341]\n",
      "loss: 0.490522  [99200/175341]\n",
      "loss: 0.349248  [100800/175341]\n",
      "loss: 0.516187  [102400/175341]\n",
      "loss: 0.495635  [104000/175341]\n",
      "loss: 0.440743  [105600/175341]\n",
      "loss: 0.581885  [107200/175341]\n",
      "loss: 0.507871  [108800/175341]\n",
      "loss: 0.671324  [110400/175341]\n",
      "loss: 0.409717  [112000/175341]\n",
      "loss: 0.487032  [113600/175341]\n",
      "loss: 0.477600  [115200/175341]\n",
      "loss: 0.654559  [116800/175341]\n",
      "loss: 0.161952  [118400/175341]\n",
      "loss: 0.194627  [120000/175341]\n",
      "loss: 0.731518  [121600/175341]\n",
      "loss: 0.763433  [123200/175341]\n",
      "loss: 0.302665  [124800/175341]\n",
      "loss: 0.742036  [126400/175341]\n",
      "loss: 0.152895  [128000/175341]\n",
      "loss: 0.416679  [129600/175341]\n",
      "loss: 1.193261  [131200/175341]\n",
      "loss: 0.431433  [132800/175341]\n",
      "loss: 0.229362  [134400/175341]\n",
      "loss: 0.459549  [136000/175341]\n",
      "loss: 0.287139  [137600/175341]\n",
      "loss: 0.329585  [139200/175341]\n",
      "loss: 0.375565  [140800/175341]\n",
      "loss: 0.559177  [142400/175341]\n",
      "loss: 0.318051  [144000/175341]\n",
      "loss: 0.378890  [145600/175341]\n",
      "loss: 0.339070  [147200/175341]\n",
      "loss: 0.319927  [148800/175341]\n",
      "loss: 0.398319  [150400/175341]\n",
      "loss: 0.433219  [152000/175341]\n",
      "loss: 0.256577  [153600/175341]\n",
      "loss: 0.218438  [155200/175341]\n",
      "loss: 0.237365  [156800/175341]\n",
      "loss: 0.417727  [158400/175341]\n",
      "loss: 0.567551  [160000/175341]\n",
      "loss: 0.646847  [161600/175341]\n",
      "loss: 0.276850  [163200/175341]\n",
      "loss: 0.352000  [164800/175341]\n",
      "loss: 0.645619  [166400/175341]\n",
      "loss: 0.210262  [168000/175341]\n",
      "loss: 0.432885  [169600/175341]\n",
      "loss: 0.366753  [171200/175341]\n",
      "loss: 0.328633  [172800/175341]\n",
      "loss: 0.593397  [174400/175341]\n",
      "Train Accuracy: 81.0769%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.566840, F1-score: 74.66%, Macro_F1-Score:  41.60%  \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.465364  [    0/175341]\n",
      "loss: 0.429636  [ 1600/175341]\n",
      "loss: 0.475544  [ 3200/175341]\n",
      "loss: 0.778107  [ 4800/175341]\n",
      "loss: 0.226967  [ 6400/175341]\n",
      "loss: 0.498311  [ 8000/175341]\n",
      "loss: 0.595946  [ 9600/175341]\n",
      "loss: 0.421194  [11200/175341]\n",
      "loss: 0.519261  [12800/175341]\n",
      "loss: 0.790195  [14400/175341]\n",
      "loss: 0.268469  [16000/175341]\n",
      "loss: 0.525010  [17600/175341]\n",
      "loss: 0.440063  [19200/175341]\n",
      "loss: 0.439996  [20800/175341]\n",
      "loss: 0.376429  [22400/175341]\n",
      "loss: 0.422418  [24000/175341]\n",
      "loss: 0.290954  [25600/175341]\n",
      "loss: 0.305136  [27200/175341]\n",
      "loss: 1.112897  [28800/175341]\n",
      "loss: 0.675871  [30400/175341]\n",
      "loss: 0.486552  [32000/175341]\n",
      "loss: 0.700812  [33600/175341]\n",
      "loss: 0.724408  [35200/175341]\n",
      "loss: 0.885798  [36800/175341]\n",
      "loss: 0.372160  [38400/175341]\n",
      "loss: 0.511589  [40000/175341]\n",
      "loss: 0.064732  [41600/175341]\n",
      "loss: 0.220656  [43200/175341]\n",
      "loss: 0.409552  [44800/175341]\n",
      "loss: 0.203214  [46400/175341]\n",
      "loss: 0.404411  [48000/175341]\n",
      "loss: 0.772746  [49600/175341]\n",
      "loss: 0.722790  [51200/175341]\n",
      "loss: 0.645476  [52800/175341]\n",
      "loss: 0.567125  [54400/175341]\n",
      "loss: 0.194112  [56000/175341]\n",
      "loss: 0.326571  [57600/175341]\n",
      "loss: 0.525366  [59200/175341]\n",
      "loss: 0.713452  [60800/175341]\n",
      "loss: 0.631624  [62400/175341]\n",
      "loss: 0.418523  [64000/175341]\n",
      "loss: 0.107143  [65600/175341]\n",
      "loss: 0.497442  [67200/175341]\n",
      "loss: 0.471193  [68800/175341]\n",
      "loss: 0.512878  [70400/175341]\n",
      "loss: 0.123411  [72000/175341]\n",
      "loss: 0.176840  [73600/175341]\n",
      "loss: 0.193196  [75200/175341]\n",
      "loss: 0.520190  [76800/175341]\n",
      "loss: 0.447371  [78400/175341]\n",
      "loss: 0.642100  [80000/175341]\n",
      "loss: 0.656314  [81600/175341]\n",
      "loss: 0.658705  [83200/175341]\n",
      "loss: 0.937135  [84800/175341]\n",
      "loss: 0.078510  [86400/175341]\n",
      "loss: 0.422299  [88000/175341]\n",
      "loss: 0.394293  [89600/175341]\n",
      "loss: 0.318009  [91200/175341]\n",
      "loss: 0.459047  [92800/175341]\n",
      "loss: 0.555757  [94400/175341]\n",
      "loss: 0.561718  [96000/175341]\n",
      "loss: 0.424066  [97600/175341]\n",
      "loss: 0.379116  [99200/175341]\n",
      "loss: 0.277028  [100800/175341]\n",
      "loss: 0.447880  [102400/175341]\n",
      "loss: 0.639756  [104000/175341]\n",
      "loss: 0.350392  [105600/175341]\n",
      "loss: 0.381766  [107200/175341]\n",
      "loss: 0.398442  [108800/175341]\n",
      "loss: 0.476966  [110400/175341]\n",
      "loss: 0.685577  [112000/175341]\n",
      "loss: 0.376506  [113600/175341]\n",
      "loss: 0.288615  [115200/175341]\n",
      "loss: 0.329555  [116800/175341]\n",
      "loss: 0.907786  [118400/175341]\n",
      "loss: 0.701851  [120000/175341]\n",
      "loss: 0.482604  [121600/175341]\n",
      "loss: 0.834985  [123200/175341]\n",
      "loss: 0.389634  [124800/175341]\n",
      "loss: 1.041478  [126400/175341]\n",
      "loss: 0.461928  [128000/175341]\n",
      "loss: 0.573167  [129600/175341]\n",
      "loss: 0.298398  [131200/175341]\n",
      "loss: 0.546066  [132800/175341]\n",
      "loss: 0.521782  [134400/175341]\n",
      "loss: 0.459534  [136000/175341]\n",
      "loss: 0.260580  [137600/175341]\n",
      "loss: 0.559091  [139200/175341]\n",
      "loss: 0.369318  [140800/175341]\n",
      "loss: 0.391862  [142400/175341]\n",
      "loss: 0.360243  [144000/175341]\n",
      "loss: 0.882762  [145600/175341]\n",
      "loss: 0.647447  [147200/175341]\n",
      "loss: 0.376435  [148800/175341]\n",
      "loss: 0.512071  [150400/175341]\n",
      "loss: 0.747543  [152000/175341]\n",
      "loss: 0.358727  [153600/175341]\n",
      "loss: 0.785572  [155200/175341]\n",
      "loss: 0.667924  [156800/175341]\n",
      "loss: 0.328852  [158400/175341]\n",
      "loss: 0.341570  [160000/175341]\n",
      "loss: 0.331124  [161600/175341]\n",
      "loss: 0.402016  [163200/175341]\n",
      "loss: 0.228855  [164800/175341]\n",
      "loss: 0.354627  [166400/175341]\n",
      "loss: 0.504591  [168000/175341]\n",
      "loss: 0.185702  [169600/175341]\n",
      "loss: 0.156734  [171200/175341]\n",
      "loss: 0.282301  [172800/175341]\n",
      "loss: 0.284019  [174400/175341]\n",
      "Train Accuracy: 81.0888%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.545514, F1-score: 75.74%, Macro_F1-Score:  41.55%  \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.634441  [    0/175341]\n",
      "loss: 0.533740  [ 1600/175341]\n",
      "loss: 0.652461  [ 3200/175341]\n",
      "loss: 0.284581  [ 4800/175341]\n",
      "loss: 0.741191  [ 6400/175341]\n",
      "loss: 0.857193  [ 8000/175341]\n",
      "loss: 0.385195  [ 9600/175341]\n",
      "loss: 0.633809  [11200/175341]\n",
      "loss: 0.146789  [12800/175341]\n",
      "loss: 0.562992  [14400/175341]\n",
      "loss: 0.178394  [16000/175341]\n",
      "loss: 0.548432  [17600/175341]\n",
      "loss: 0.484122  [19200/175341]\n",
      "loss: 0.633999  [20800/175341]\n",
      "loss: 0.632233  [22400/175341]\n",
      "loss: 0.556093  [24000/175341]\n",
      "loss: 0.323149  [25600/175341]\n",
      "loss: 0.122363  [27200/175341]\n",
      "loss: 0.389864  [28800/175341]\n",
      "loss: 0.463391  [30400/175341]\n",
      "loss: 0.784407  [32000/175341]\n",
      "loss: 0.166005  [33600/175341]\n",
      "loss: 0.165009  [35200/175341]\n",
      "loss: 0.436951  [36800/175341]\n",
      "loss: 0.431937  [38400/175341]\n",
      "loss: 0.368691  [40000/175341]\n",
      "loss: 0.217594  [41600/175341]\n",
      "loss: 0.113887  [43200/175341]\n",
      "loss: 0.482688  [44800/175341]\n",
      "loss: 0.235742  [46400/175341]\n",
      "loss: 0.638465  [48000/175341]\n",
      "loss: 0.434271  [49600/175341]\n",
      "loss: 0.400324  [51200/175341]\n",
      "loss: 0.327736  [52800/175341]\n",
      "loss: 0.721547  [54400/175341]\n",
      "loss: 0.395251  [56000/175341]\n",
      "loss: 0.551227  [57600/175341]\n",
      "loss: 0.325100  [59200/175341]\n",
      "loss: 0.352458  [60800/175341]\n",
      "loss: 0.696486  [62400/175341]\n",
      "loss: 0.552978  [64000/175341]\n",
      "loss: 0.725619  [65600/175341]\n",
      "loss: 0.482749  [67200/175341]\n",
      "loss: 0.233924  [68800/175341]\n",
      "loss: 1.003176  [70400/175341]\n",
      "loss: 0.275125  [72000/175341]\n",
      "loss: 0.496048  [73600/175341]\n",
      "loss: 0.668290  [75200/175341]\n",
      "loss: 0.616996  [76800/175341]\n",
      "loss: 0.207066  [78400/175341]\n",
      "loss: 0.722963  [80000/175341]\n",
      "loss: 0.280625  [81600/175341]\n",
      "loss: 0.132245  [83200/175341]\n",
      "loss: 0.216277  [84800/175341]\n",
      "loss: 0.315430  [86400/175341]\n",
      "loss: 0.437683  [88000/175341]\n",
      "loss: 0.355634  [89600/175341]\n",
      "loss: 0.743088  [91200/175341]\n",
      "loss: 0.578767  [92800/175341]\n",
      "loss: 0.243688  [94400/175341]\n",
      "loss: 0.203357  [96000/175341]\n",
      "loss: 0.628480  [97600/175341]\n",
      "loss: 0.316228  [99200/175341]\n",
      "loss: 0.453370  [100800/175341]\n",
      "loss: 0.519679  [102400/175341]\n",
      "loss: 0.400524  [104000/175341]\n",
      "loss: 0.776109  [105600/175341]\n",
      "loss: 0.321427  [107200/175341]\n",
      "loss: 0.533250  [108800/175341]\n",
      "loss: 0.385623  [110400/175341]\n",
      "loss: 0.384381  [112000/175341]\n",
      "loss: 0.731532  [113600/175341]\n",
      "loss: 0.503015  [115200/175341]\n",
      "loss: 0.543555  [116800/175341]\n",
      "loss: 0.150732  [118400/175341]\n",
      "loss: 0.408631  [120000/175341]\n",
      "loss: 0.526319  [121600/175341]\n",
      "loss: 0.452474  [123200/175341]\n",
      "loss: 0.340360  [124800/175341]\n",
      "loss: 0.556503  [126400/175341]\n",
      "loss: 0.882566  [128000/175341]\n",
      "loss: 0.976083  [129600/175341]\n",
      "loss: 0.291536  [131200/175341]\n",
      "loss: 0.317020  [132800/175341]\n",
      "loss: 0.424730  [134400/175341]\n",
      "loss: 0.504986  [136000/175341]\n",
      "loss: 0.141012  [137600/175341]\n",
      "loss: 1.077896  [139200/175341]\n",
      "loss: 0.233869  [140800/175341]\n",
      "loss: 0.240485  [142400/175341]\n",
      "loss: 0.540660  [144000/175341]\n",
      "loss: 0.465741  [145600/175341]\n",
      "loss: 0.157292  [147200/175341]\n",
      "loss: 0.727054  [148800/175341]\n",
      "loss: 0.562519  [150400/175341]\n",
      "loss: 0.224405  [152000/175341]\n",
      "loss: 0.706573  [153600/175341]\n",
      "loss: 0.773170  [155200/175341]\n",
      "loss: 0.208846  [156800/175341]\n",
      "loss: 0.443097  [158400/175341]\n",
      "loss: 0.541210  [160000/175341]\n",
      "loss: 0.490446  [161600/175341]\n",
      "loss: 0.267757  [163200/175341]\n",
      "loss: 0.353900  [164800/175341]\n",
      "loss: 0.305272  [166400/175341]\n",
      "loss: 0.408769  [168000/175341]\n",
      "loss: 0.292980  [169600/175341]\n",
      "loss: 0.249095  [171200/175341]\n",
      "loss: 0.605021  [172800/175341]\n",
      "loss: 0.213150  [174400/175341]\n",
      "Train Accuracy: 81.1054%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.562173, F1-score: 74.66%, Macro_F1-Score:  41.68%  \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.355990  [    0/175341]\n",
      "loss: 0.193649  [ 1600/175341]\n",
      "loss: 0.378648  [ 3200/175341]\n",
      "loss: 0.539562  [ 4800/175341]\n",
      "loss: 0.544474  [ 6400/175341]\n",
      "loss: 0.368187  [ 8000/175341]\n",
      "loss: 0.543301  [ 9600/175341]\n",
      "loss: 0.246657  [11200/175341]\n",
      "loss: 0.388662  [12800/175341]\n",
      "loss: 0.698776  [14400/175341]\n",
      "loss: 0.701862  [16000/175341]\n",
      "loss: 0.408187  [17600/175341]\n",
      "loss: 0.460009  [19200/175341]\n",
      "loss: 0.740339  [20800/175341]\n",
      "loss: 0.281944  [22400/175341]\n",
      "loss: 0.149211  [24000/175341]\n",
      "loss: 0.440031  [25600/175341]\n",
      "loss: 0.550096  [27200/175341]\n",
      "loss: 0.749845  [28800/175341]\n",
      "loss: 0.606241  [30400/175341]\n",
      "loss: 0.689441  [32000/175341]\n",
      "loss: 0.510528  [33600/175341]\n",
      "loss: 0.175934  [35200/175341]\n",
      "loss: 0.436469  [36800/175341]\n",
      "loss: 0.459110  [38400/175341]\n",
      "loss: 0.688694  [40000/175341]\n",
      "loss: 0.342278  [41600/175341]\n",
      "loss: 0.609930  [43200/175341]\n",
      "loss: 0.933813  [44800/175341]\n",
      "loss: 1.443606  [46400/175341]\n",
      "loss: 0.640931  [48000/175341]\n",
      "loss: 0.575727  [49600/175341]\n",
      "loss: 0.560474  [51200/175341]\n",
      "loss: 0.484781  [52800/175341]\n",
      "loss: 0.333659  [54400/175341]\n",
      "loss: 0.367018  [56000/175341]\n",
      "loss: 0.601914  [57600/175341]\n",
      "loss: 0.511866  [59200/175341]\n",
      "loss: 0.826789  [60800/175341]\n",
      "loss: 0.510642  [62400/175341]\n",
      "loss: 0.393700  [64000/175341]\n",
      "loss: 0.559675  [65600/175341]\n",
      "loss: 0.503266  [67200/175341]\n",
      "loss: 0.482415  [68800/175341]\n",
      "loss: 0.181434  [70400/175341]\n",
      "loss: 0.329571  [72000/175341]\n",
      "loss: 0.885039  [73600/175341]\n",
      "loss: 0.682169  [75200/175341]\n",
      "loss: 0.249607  [76800/175341]\n",
      "loss: 0.480137  [78400/175341]\n",
      "loss: 0.429688  [80000/175341]\n",
      "loss: 0.658531  [81600/175341]\n",
      "loss: 0.369367  [83200/175341]\n",
      "loss: 0.694070  [84800/175341]\n",
      "loss: 0.439292  [86400/175341]\n",
      "loss: 0.642902  [88000/175341]\n",
      "loss: 0.355537  [89600/175341]\n",
      "loss: 0.197328  [91200/175341]\n",
      "loss: 0.241184  [92800/175341]\n",
      "loss: 0.455424  [94400/175341]\n",
      "loss: 0.322756  [96000/175341]\n",
      "loss: 0.119028  [97600/175341]\n",
      "loss: 0.449434  [99200/175341]\n",
      "loss: 0.273201  [100800/175341]\n",
      "loss: 0.404882  [102400/175341]\n",
      "loss: 0.207083  [104000/175341]\n",
      "loss: 0.632151  [105600/175341]\n",
      "loss: 0.754699  [107200/175341]\n",
      "loss: 0.312978  [108800/175341]\n",
      "loss: 0.270623  [110400/175341]\n",
      "loss: 0.474915  [112000/175341]\n",
      "loss: 0.410699  [113600/175341]\n",
      "loss: 0.649092  [115200/175341]\n",
      "loss: 0.325206  [116800/175341]\n",
      "loss: 0.480367  [118400/175341]\n",
      "loss: 0.382981  [120000/175341]\n",
      "loss: 0.216563  [121600/175341]\n",
      "loss: 0.665262  [123200/175341]\n",
      "loss: 0.566124  [124800/175341]\n",
      "loss: 0.415090  [126400/175341]\n",
      "loss: 0.434467  [128000/175341]\n",
      "loss: 0.366068  [129600/175341]\n",
      "loss: 0.288479  [131200/175341]\n",
      "loss: 0.503014  [132800/175341]\n",
      "loss: 0.692829  [134400/175341]\n",
      "loss: 0.213952  [136000/175341]\n",
      "loss: 0.333136  [137600/175341]\n",
      "loss: 0.690511  [139200/175341]\n",
      "loss: 0.578183  [140800/175341]\n",
      "loss: 0.741243  [142400/175341]\n",
      "loss: 0.491085  [144000/175341]\n",
      "loss: 0.256014  [145600/175341]\n",
      "loss: 1.047461  [147200/175341]\n",
      "loss: 0.454131  [148800/175341]\n",
      "loss: 0.801456  [150400/175341]\n",
      "loss: 0.743448  [152000/175341]\n",
      "loss: 0.779612  [153600/175341]\n",
      "loss: 0.521370  [155200/175341]\n",
      "loss: 1.091269  [156800/175341]\n",
      "loss: 0.372687  [158400/175341]\n",
      "loss: 0.365610  [160000/175341]\n",
      "loss: 0.395191  [161600/175341]\n",
      "loss: 0.419792  [163200/175341]\n",
      "loss: 0.283659  [164800/175341]\n",
      "loss: 0.579034  [166400/175341]\n",
      "loss: 0.498385  [168000/175341]\n",
      "loss: 0.629590  [169600/175341]\n",
      "loss: 0.965923  [171200/175341]\n",
      "loss: 0.385621  [172800/175341]\n",
      "loss: 0.369929  [174400/175341]\n",
      "Train Accuracy: 81.0860%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.531324, F1-score: 76.24%, Macro_F1-Score:  41.87%  \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.793895  [    0/175341]\n",
      "loss: 0.277765  [ 1600/175341]\n",
      "loss: 0.429065  [ 3200/175341]\n",
      "loss: 0.779868  [ 4800/175341]\n",
      "loss: 0.531800  [ 6400/175341]\n",
      "loss: 0.497183  [ 8000/175341]\n",
      "loss: 0.752950  [ 9600/175341]\n",
      "loss: 0.424735  [11200/175341]\n",
      "loss: 0.974232  [12800/175341]\n",
      "loss: 0.710496  [14400/175341]\n",
      "loss: 0.454113  [16000/175341]\n",
      "loss: 0.281724  [17600/175341]\n",
      "loss: 0.394762  [19200/175341]\n",
      "loss: 0.340880  [20800/175341]\n",
      "loss: 0.433613  [22400/175341]\n",
      "loss: 0.350997  [24000/175341]\n",
      "loss: 0.747582  [25600/175341]\n",
      "loss: 0.221942  [27200/175341]\n",
      "loss: 0.349273  [28800/175341]\n",
      "loss: 0.210600  [30400/175341]\n",
      "loss: 0.725597  [32000/175341]\n",
      "loss: 0.731365  [33600/175341]\n",
      "loss: 0.372117  [35200/175341]\n",
      "loss: 0.208280  [36800/175341]\n",
      "loss: 0.367056  [38400/175341]\n",
      "loss: 0.728252  [40000/175341]\n",
      "loss: 0.353288  [41600/175341]\n",
      "loss: 0.490841  [43200/175341]\n",
      "loss: 0.441581  [44800/175341]\n",
      "loss: 0.567618  [46400/175341]\n",
      "loss: 0.541021  [48000/175341]\n",
      "loss: 0.310832  [49600/175341]\n",
      "loss: 0.591274  [51200/175341]\n",
      "loss: 0.697887  [52800/175341]\n",
      "loss: 0.138733  [54400/175341]\n",
      "loss: 0.162616  [56000/175341]\n",
      "loss: 0.511622  [57600/175341]\n",
      "loss: 0.669509  [59200/175341]\n",
      "loss: 0.246964  [60800/175341]\n",
      "loss: 0.502380  [62400/175341]\n",
      "loss: 0.156801  [64000/175341]\n",
      "loss: 0.457749  [65600/175341]\n",
      "loss: 0.472409  [67200/175341]\n",
      "loss: 0.382028  [68800/175341]\n",
      "loss: 0.388035  [70400/175341]\n",
      "loss: 0.621530  [72000/175341]\n",
      "loss: 0.382142  [73600/175341]\n",
      "loss: 0.325212  [75200/175341]\n",
      "loss: 0.294177  [76800/175341]\n",
      "loss: 0.487091  [78400/175341]\n",
      "loss: 0.567352  [80000/175341]\n",
      "loss: 0.538296  [81600/175341]\n",
      "loss: 0.700472  [83200/175341]\n",
      "loss: 0.151801  [84800/175341]\n",
      "loss: 0.370595  [86400/175341]\n",
      "loss: 0.435533  [88000/175341]\n",
      "loss: 0.616412  [89600/175341]\n",
      "loss: 0.336419  [91200/175341]\n",
      "loss: 0.510635  [92800/175341]\n",
      "loss: 0.671152  [94400/175341]\n",
      "loss: 0.456634  [96000/175341]\n",
      "loss: 0.150946  [97600/175341]\n",
      "loss: 0.228860  [99200/175341]\n",
      "loss: 0.263605  [100800/175341]\n",
      "loss: 0.655070  [102400/175341]\n",
      "loss: 0.408458  [104000/175341]\n",
      "loss: 0.434982  [105600/175341]\n",
      "loss: 0.296083  [107200/175341]\n",
      "loss: 0.752939  [108800/175341]\n",
      "loss: 0.649904  [110400/175341]\n",
      "loss: 0.541330  [112000/175341]\n",
      "loss: 0.382848  [113600/175341]\n",
      "loss: 0.435927  [115200/175341]\n",
      "loss: 0.444069  [116800/175341]\n",
      "loss: 0.496188  [118400/175341]\n",
      "loss: 0.861082  [120000/175341]\n",
      "loss: 0.258190  [121600/175341]\n",
      "loss: 0.289104  [123200/175341]\n",
      "loss: 0.191444  [124800/175341]\n",
      "loss: 0.333690  [126400/175341]\n",
      "loss: 0.319296  [128000/175341]\n",
      "loss: 0.728863  [129600/175341]\n",
      "loss: 0.327746  [131200/175341]\n",
      "loss: 0.964856  [132800/175341]\n",
      "loss: 0.599427  [134400/175341]\n",
      "loss: 0.448910  [136000/175341]\n",
      "loss: 0.756012  [137600/175341]\n",
      "loss: 0.648631  [139200/175341]\n",
      "loss: 0.517450  [140800/175341]\n",
      "loss: 0.466815  [142400/175341]\n",
      "loss: 0.455936  [144000/175341]\n",
      "loss: 0.351535  [145600/175341]\n",
      "loss: 0.306322  [147200/175341]\n",
      "loss: 0.148226  [148800/175341]\n",
      "loss: 0.305615  [150400/175341]\n",
      "loss: 0.441698  [152000/175341]\n",
      "loss: 0.554789  [153600/175341]\n",
      "loss: 0.423392  [155200/175341]\n",
      "loss: 0.481725  [156800/175341]\n",
      "loss: 0.350259  [158400/175341]\n",
      "loss: 0.608657  [160000/175341]\n",
      "loss: 0.645602  [161600/175341]\n",
      "loss: 0.196040  [163200/175341]\n",
      "loss: 0.245696  [164800/175341]\n",
      "loss: 0.569845  [166400/175341]\n",
      "loss: 0.619589  [168000/175341]\n",
      "loss: 0.382022  [169600/175341]\n",
      "loss: 0.212951  [171200/175341]\n",
      "loss: 0.571395  [172800/175341]\n",
      "loss: 0.391259  [174400/175341]\n",
      "Train Accuracy: 81.0951%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.554133, F1-score: 75.50%, Macro_F1-Score:  42.58%  \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.666499  [    0/175341]\n",
      "loss: 0.495158  [ 1600/175341]\n",
      "loss: 0.752455  [ 3200/175341]\n",
      "loss: 0.505591  [ 4800/175341]\n",
      "loss: 0.824458  [ 6400/175341]\n",
      "loss: 0.777455  [ 8000/175341]\n",
      "loss: 0.787474  [ 9600/175341]\n",
      "loss: 0.478295  [11200/175341]\n",
      "loss: 0.599163  [12800/175341]\n",
      "loss: 0.322325  [14400/175341]\n",
      "loss: 0.352364  [16000/175341]\n",
      "loss: 0.379415  [17600/175341]\n",
      "loss: 0.408505  [19200/175341]\n",
      "loss: 0.657030  [20800/175341]\n",
      "loss: 0.613190  [22400/175341]\n",
      "loss: 0.243155  [24000/175341]\n",
      "loss: 0.403957  [25600/175341]\n",
      "loss: 0.544090  [27200/175341]\n",
      "loss: 0.674075  [28800/175341]\n",
      "loss: 0.444250  [30400/175341]\n",
      "loss: 0.369758  [32000/175341]\n",
      "loss: 0.451064  [33600/175341]\n",
      "loss: 0.505125  [35200/175341]\n",
      "loss: 0.682683  [36800/175341]\n",
      "loss: 0.226231  [38400/175341]\n",
      "loss: 0.664252  [40000/175341]\n",
      "loss: 0.588315  [41600/175341]\n",
      "loss: 0.477818  [43200/175341]\n",
      "loss: 0.416909  [44800/175341]\n",
      "loss: 0.283148  [46400/175341]\n",
      "loss: 0.279020  [48000/175341]\n",
      "loss: 0.334486  [49600/175341]\n",
      "loss: 0.682806  [51200/175341]\n",
      "loss: 0.263719  [52800/175341]\n",
      "loss: 0.325573  [54400/175341]\n",
      "loss: 0.635649  [56000/175341]\n",
      "loss: 0.359852  [57600/175341]\n",
      "loss: 0.588159  [59200/175341]\n",
      "loss: 0.299493  [60800/175341]\n",
      "loss: 0.511377  [62400/175341]\n",
      "loss: 0.541941  [64000/175341]\n",
      "loss: 0.378695  [65600/175341]\n",
      "loss: 0.165187  [67200/175341]\n",
      "loss: 0.605466  [68800/175341]\n",
      "loss: 0.658036  [70400/175341]\n",
      "loss: 0.373816  [72000/175341]\n",
      "loss: 0.761588  [73600/175341]\n",
      "loss: 0.213739  [75200/175341]\n",
      "loss: 0.311923  [76800/175341]\n",
      "loss: 0.603301  [78400/175341]\n",
      "loss: 0.276374  [80000/175341]\n",
      "loss: 0.331264  [81600/175341]\n",
      "loss: 0.202004  [83200/175341]\n",
      "loss: 0.663214  [84800/175341]\n",
      "loss: 0.127181  [86400/175341]\n",
      "loss: 0.874601  [88000/175341]\n",
      "loss: 0.332420  [89600/175341]\n",
      "loss: 0.567339  [91200/175341]\n",
      "loss: 0.173141  [92800/175341]\n",
      "loss: 0.520010  [94400/175341]\n",
      "loss: 0.530758  [96000/175341]\n",
      "loss: 0.341534  [97600/175341]\n",
      "loss: 0.602030  [99200/175341]\n",
      "loss: 0.162102  [100800/175341]\n",
      "loss: 0.467353  [102400/175341]\n",
      "loss: 0.589338  [104000/175341]\n",
      "loss: 0.780995  [105600/175341]\n",
      "loss: 0.561109  [107200/175341]\n",
      "loss: 0.333057  [108800/175341]\n",
      "loss: 0.350323  [110400/175341]\n",
      "loss: 0.907283  [112000/175341]\n",
      "loss: 0.173440  [113600/175341]\n",
      "loss: 0.242970  [115200/175341]\n",
      "loss: 0.919116  [116800/175341]\n",
      "loss: 1.082190  [118400/175341]\n",
      "loss: 0.415923  [120000/175341]\n",
      "loss: 0.304904  [121600/175341]\n",
      "loss: 0.506944  [123200/175341]\n",
      "loss: 0.727147  [124800/175341]\n",
      "loss: 0.349000  [126400/175341]\n",
      "loss: 0.465287  [128000/175341]\n",
      "loss: 0.624300  [129600/175341]\n",
      "loss: 0.272024  [131200/175341]\n",
      "loss: 0.415663  [132800/175341]\n",
      "loss: 0.499579  [134400/175341]\n",
      "loss: 0.241792  [136000/175341]\n",
      "loss: 0.348771  [137600/175341]\n",
      "loss: 0.545831  [139200/175341]\n",
      "loss: 0.522025  [140800/175341]\n",
      "loss: 0.487697  [142400/175341]\n",
      "loss: 0.574775  [144000/175341]\n",
      "loss: 0.456720  [145600/175341]\n",
      "loss: 0.186399  [147200/175341]\n",
      "loss: 0.555015  [148800/175341]\n",
      "loss: 0.398620  [150400/175341]\n",
      "loss: 0.689675  [152000/175341]\n",
      "loss: 0.500145  [153600/175341]\n",
      "loss: 0.572878  [155200/175341]\n",
      "loss: 0.837135  [156800/175341]\n",
      "loss: 0.371362  [158400/175341]\n",
      "loss: 0.498221  [160000/175341]\n",
      "loss: 0.664563  [161600/175341]\n",
      "loss: 0.098219  [163200/175341]\n",
      "loss: 0.487773  [164800/175341]\n",
      "loss: 0.549173  [166400/175341]\n",
      "loss: 0.240052  [168000/175341]\n",
      "loss: 0.591697  [169600/175341]\n",
      "loss: 0.435101  [171200/175341]\n",
      "loss: 0.913558  [172800/175341]\n",
      "loss: 0.727863  [174400/175341]\n",
      "Train Accuracy: 81.1042%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.562856, F1-score: 74.60%, Macro_F1-Score:  41.32%  \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.517041  [    0/175341]\n",
      "loss: 0.419335  [ 1600/175341]\n",
      "loss: 0.326127  [ 3200/175341]\n",
      "loss: 0.357754  [ 4800/175341]\n",
      "loss: 0.474827  [ 6400/175341]\n",
      "loss: 0.319337  [ 8000/175341]\n",
      "loss: 0.342915  [ 9600/175341]\n",
      "loss: 0.634278  [11200/175341]\n",
      "loss: 0.624180  [12800/175341]\n",
      "loss: 0.200063  [14400/175341]\n",
      "loss: 0.720645  [16000/175341]\n",
      "loss: 0.372556  [17600/175341]\n",
      "loss: 0.453873  [19200/175341]\n",
      "loss: 0.635108  [20800/175341]\n",
      "loss: 0.278680  [22400/175341]\n",
      "loss: 0.525751  [24000/175341]\n",
      "loss: 0.535435  [25600/175341]\n",
      "loss: 0.436336  [27200/175341]\n",
      "loss: 0.387658  [28800/175341]\n",
      "loss: 0.353249  [30400/175341]\n",
      "loss: 0.305949  [32000/175341]\n",
      "loss: 0.297961  [33600/175341]\n",
      "loss: 0.540400  [35200/175341]\n",
      "loss: 0.359606  [36800/175341]\n",
      "loss: 0.430248  [38400/175341]\n",
      "loss: 0.463166  [40000/175341]\n",
      "loss: 0.694476  [41600/175341]\n",
      "loss: 0.501682  [43200/175341]\n",
      "loss: 0.546754  [44800/175341]\n",
      "loss: 0.556509  [46400/175341]\n",
      "loss: 0.421706  [48000/175341]\n",
      "loss: 0.331911  [49600/175341]\n",
      "loss: 0.072514  [51200/175341]\n",
      "loss: 0.247233  [52800/175341]\n",
      "loss: 0.204853  [54400/175341]\n",
      "loss: 0.165964  [56000/175341]\n",
      "loss: 0.727642  [57600/175341]\n",
      "loss: 0.669131  [59200/175341]\n",
      "loss: 0.631192  [60800/175341]\n",
      "loss: 0.384290  [62400/175341]\n",
      "loss: 0.786968  [64000/175341]\n",
      "loss: 0.292826  [65600/175341]\n",
      "loss: 0.307626  [67200/175341]\n",
      "loss: 0.347354  [68800/175341]\n",
      "loss: 0.362115  [70400/175341]\n",
      "loss: 0.442232  [72000/175341]\n",
      "loss: 0.449533  [73600/175341]\n",
      "loss: 0.681530  [75200/175341]\n",
      "loss: 0.791184  [76800/175341]\n",
      "loss: 0.519794  [78400/175341]\n",
      "loss: 0.615803  [80000/175341]\n",
      "loss: 0.320365  [81600/175341]\n",
      "loss: 0.584021  [83200/175341]\n",
      "loss: 0.679184  [84800/175341]\n",
      "loss: 0.341058  [86400/175341]\n",
      "loss: 0.150156  [88000/175341]\n",
      "loss: 0.359920  [89600/175341]\n",
      "loss: 0.501649  [91200/175341]\n",
      "loss: 0.565812  [92800/175341]\n",
      "loss: 0.616941  [94400/175341]\n",
      "loss: 0.385867  [96000/175341]\n",
      "loss: 0.267232  [97600/175341]\n",
      "loss: 0.179744  [99200/175341]\n",
      "loss: 0.682126  [100800/175341]\n",
      "loss: 0.303631  [102400/175341]\n",
      "loss: 0.197484  [104000/175341]\n",
      "loss: 0.349711  [105600/175341]\n",
      "loss: 0.553614  [107200/175341]\n",
      "loss: 0.170000  [108800/175341]\n",
      "loss: 0.383147  [110400/175341]\n",
      "loss: 0.365462  [112000/175341]\n",
      "loss: 0.619723  [113600/175341]\n",
      "loss: 0.413243  [115200/175341]\n",
      "loss: 0.162153  [116800/175341]\n",
      "loss: 0.612895  [118400/175341]\n",
      "loss: 0.476145  [120000/175341]\n",
      "loss: 0.319535  [121600/175341]\n",
      "loss: 0.450641  [123200/175341]\n",
      "loss: 0.177209  [124800/175341]\n",
      "loss: 0.699275  [126400/175341]\n",
      "loss: 0.598615  [128000/175341]\n",
      "loss: 0.473787  [129600/175341]\n",
      "loss: 0.637879  [131200/175341]\n",
      "loss: 0.429194  [132800/175341]\n",
      "loss: 0.447321  [134400/175341]\n",
      "loss: 0.363174  [136000/175341]\n",
      "loss: 0.585056  [137600/175341]\n",
      "loss: 0.323739  [139200/175341]\n",
      "loss: 0.255905  [140800/175341]\n",
      "loss: 0.340475  [142400/175341]\n",
      "loss: 0.334237  [144000/175341]\n",
      "loss: 0.354015  [145600/175341]\n",
      "loss: 0.280025  [147200/175341]\n",
      "loss: 0.541508  [148800/175341]\n",
      "loss: 0.216918  [150400/175341]\n",
      "loss: 0.438009  [152000/175341]\n",
      "loss: 0.302436  [153600/175341]\n",
      "loss: 0.143134  [155200/175341]\n",
      "loss: 0.262283  [156800/175341]\n",
      "loss: 0.633199  [158400/175341]\n",
      "loss: 0.297392  [160000/175341]\n",
      "loss: 0.898942  [161600/175341]\n",
      "loss: 0.483407  [163200/175341]\n",
      "loss: 0.440419  [164800/175341]\n",
      "loss: 0.219192  [166400/175341]\n",
      "loss: 0.566003  [168000/175341]\n",
      "loss: 0.583231  [169600/175341]\n",
      "loss: 0.599613  [171200/175341]\n",
      "loss: 0.245222  [172800/175341]\n",
      "loss: 0.579995  [174400/175341]\n",
      "Train Accuracy: 81.1054%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.539657, F1-score: 76.12%, Macro_F1-Score:  41.75%  \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.676316  [    0/175341]\n",
      "loss: 0.829998  [ 1600/175341]\n",
      "loss: 0.343489  [ 3200/175341]\n",
      "loss: 0.393071  [ 4800/175341]\n",
      "loss: 0.608269  [ 6400/175341]\n",
      "loss: 0.422459  [ 8000/175341]\n",
      "loss: 0.322101  [ 9600/175341]\n",
      "loss: 0.305780  [11200/175341]\n",
      "loss: 0.465809  [12800/175341]\n",
      "loss: 0.420711  [14400/175341]\n",
      "loss: 0.743233  [16000/175341]\n",
      "loss: 0.639757  [17600/175341]\n",
      "loss: 0.795567  [19200/175341]\n",
      "loss: 0.381438  [20800/175341]\n",
      "loss: 0.442520  [22400/175341]\n",
      "loss: 0.805230  [24000/175341]\n",
      "loss: 0.252360  [25600/175341]\n",
      "loss: 0.713800  [27200/175341]\n",
      "loss: 0.443926  [28800/175341]\n",
      "loss: 0.475127  [30400/175341]\n",
      "loss: 0.445999  [32000/175341]\n",
      "loss: 0.455245  [33600/175341]\n",
      "loss: 0.253450  [35200/175341]\n",
      "loss: 0.540394  [36800/175341]\n",
      "loss: 0.371263  [38400/175341]\n",
      "loss: 0.762297  [40000/175341]\n",
      "loss: 0.426739  [41600/175341]\n",
      "loss: 0.704904  [43200/175341]\n",
      "loss: 0.345370  [44800/175341]\n",
      "loss: 0.420871  [46400/175341]\n",
      "loss: 0.323952  [48000/175341]\n",
      "loss: 0.627297  [49600/175341]\n",
      "loss: 0.570754  [51200/175341]\n",
      "loss: 0.501079  [52800/175341]\n",
      "loss: 0.318702  [54400/175341]\n",
      "loss: 0.468930  [56000/175341]\n",
      "loss: 0.410738  [57600/175341]\n",
      "loss: 0.354072  [59200/175341]\n",
      "loss: 0.519256  [60800/175341]\n",
      "loss: 0.848815  [62400/175341]\n",
      "loss: 0.357369  [64000/175341]\n",
      "loss: 0.349570  [65600/175341]\n",
      "loss: 0.196429  [67200/175341]\n",
      "loss: 0.337846  [68800/175341]\n",
      "loss: 0.387694  [70400/175341]\n",
      "loss: 0.434864  [72000/175341]\n",
      "loss: 0.355813  [73600/175341]\n",
      "loss: 0.384832  [75200/175341]\n",
      "loss: 0.690098  [76800/175341]\n",
      "loss: 0.872733  [78400/175341]\n",
      "loss: 0.708183  [80000/175341]\n",
      "loss: 0.278274  [81600/175341]\n",
      "loss: 0.711244  [83200/175341]\n",
      "loss: 0.239118  [84800/175341]\n",
      "loss: 0.335486  [86400/175341]\n",
      "loss: 0.632917  [88000/175341]\n",
      "loss: 0.808788  [89600/175341]\n",
      "loss: 0.422966  [91200/175341]\n",
      "loss: 0.588332  [92800/175341]\n",
      "loss: 0.528616  [94400/175341]\n",
      "loss: 0.635075  [96000/175341]\n",
      "loss: 0.166120  [97600/175341]\n",
      "loss: 0.335062  [99200/175341]\n",
      "loss: 0.368373  [100800/175341]\n",
      "loss: 0.310186  [102400/175341]\n",
      "loss: 0.093365  [104000/175341]\n",
      "loss: 0.330868  [105600/175341]\n",
      "loss: 0.891999  [107200/175341]\n",
      "loss: 0.192421  [108800/175341]\n",
      "loss: 0.112836  [110400/175341]\n",
      "loss: 0.492076  [112000/175341]\n",
      "loss: 0.543653  [113600/175341]\n",
      "loss: 0.357047  [115200/175341]\n",
      "loss: 0.374338  [116800/175341]\n",
      "loss: 0.483651  [118400/175341]\n",
      "loss: 0.520547  [120000/175341]\n",
      "loss: 0.433099  [121600/175341]\n",
      "loss: 0.531131  [123200/175341]\n",
      "loss: 0.599322  [124800/175341]\n",
      "loss: 0.799180  [126400/175341]\n",
      "loss: 0.367209  [128000/175341]\n",
      "loss: 0.344697  [129600/175341]\n",
      "loss: 0.570831  [131200/175341]\n",
      "loss: 0.589185  [132800/175341]\n",
      "loss: 0.831723  [134400/175341]\n",
      "loss: 0.791597  [136000/175341]\n",
      "loss: 0.608658  [137600/175341]\n",
      "loss: 0.659636  [139200/175341]\n",
      "loss: 0.431436  [140800/175341]\n",
      "loss: 0.517345  [142400/175341]\n",
      "loss: 0.340426  [144000/175341]\n",
      "loss: 0.587411  [145600/175341]\n",
      "loss: 0.410921  [147200/175341]\n",
      "loss: 0.218979  [148800/175341]\n",
      "loss: 0.402459  [150400/175341]\n",
      "loss: 0.806971  [152000/175341]\n",
      "loss: 0.331767  [153600/175341]\n",
      "loss: 0.478711  [155200/175341]\n",
      "loss: 0.374592  [156800/175341]\n",
      "loss: 0.232316  [158400/175341]\n",
      "loss: 0.287155  [160000/175341]\n",
      "loss: 0.605236  [161600/175341]\n",
      "loss: 0.591878  [163200/175341]\n",
      "loss: 0.172904  [164800/175341]\n",
      "loss: 0.526221  [166400/175341]\n",
      "loss: 0.561235  [168000/175341]\n",
      "loss: 0.453318  [169600/175341]\n",
      "loss: 0.662347  [171200/175341]\n",
      "loss: 0.147291  [172800/175341]\n",
      "loss: 0.155055  [174400/175341]\n",
      "Train Accuracy: 81.1082%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.545259, F1-score: 76.23%, Macro_F1-Score:  43.29%  \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.259295  [    0/175341]\n",
      "loss: 0.235972  [ 1600/175341]\n",
      "loss: 0.529390  [ 3200/175341]\n",
      "loss: 0.527330  [ 4800/175341]\n",
      "loss: 0.485965  [ 6400/175341]\n",
      "loss: 0.195114  [ 8000/175341]\n",
      "loss: 0.572977  [ 9600/175341]\n",
      "loss: 0.513572  [11200/175341]\n",
      "loss: 0.205690  [12800/175341]\n",
      "loss: 0.463711  [14400/175341]\n",
      "loss: 0.206466  [16000/175341]\n",
      "loss: 0.221560  [17600/175341]\n",
      "loss: 0.231452  [19200/175341]\n",
      "loss: 0.499641  [20800/175341]\n",
      "loss: 0.358325  [22400/175341]\n",
      "loss: 0.679956  [24000/175341]\n",
      "loss: 0.285844  [25600/175341]\n",
      "loss: 0.686376  [27200/175341]\n",
      "loss: 0.444798  [28800/175341]\n",
      "loss: 0.326421  [30400/175341]\n",
      "loss: 0.677528  [32000/175341]\n",
      "loss: 0.323596  [33600/175341]\n",
      "loss: 0.538077  [35200/175341]\n",
      "loss: 0.306050  [36800/175341]\n",
      "loss: 0.428995  [38400/175341]\n",
      "loss: 0.375164  [40000/175341]\n",
      "loss: 0.618491  [41600/175341]\n",
      "loss: 0.370491  [43200/175341]\n",
      "loss: 0.597876  [44800/175341]\n",
      "loss: 0.411633  [46400/175341]\n",
      "loss: 0.388118  [48000/175341]\n",
      "loss: 0.840158  [49600/175341]\n",
      "loss: 0.216582  [51200/175341]\n",
      "loss: 0.456920  [52800/175341]\n",
      "loss: 0.367574  [54400/175341]\n",
      "loss: 0.693951  [56000/175341]\n",
      "loss: 0.363173  [57600/175341]\n",
      "loss: 0.400825  [59200/175341]\n",
      "loss: 0.226173  [60800/175341]\n",
      "loss: 0.483033  [62400/175341]\n",
      "loss: 0.342287  [64000/175341]\n",
      "loss: 0.252295  [65600/175341]\n",
      "loss: 0.351741  [67200/175341]\n",
      "loss: 0.548397  [68800/175341]\n",
      "loss: 0.353492  [70400/175341]\n",
      "loss: 0.453215  [72000/175341]\n",
      "loss: 0.625920  [73600/175341]\n",
      "loss: 0.370462  [75200/175341]\n",
      "loss: 0.322008  [76800/175341]\n",
      "loss: 0.665351  [78400/175341]\n",
      "loss: 0.336055  [80000/175341]\n",
      "loss: 0.570719  [81600/175341]\n",
      "loss: 0.684857  [83200/175341]\n",
      "loss: 0.264551  [84800/175341]\n",
      "loss: 0.421083  [86400/175341]\n",
      "loss: 0.548802  [88000/175341]\n",
      "loss: 0.386350  [89600/175341]\n",
      "loss: 0.349435  [91200/175341]\n",
      "loss: 0.357884  [92800/175341]\n",
      "loss: 0.433817  [94400/175341]\n",
      "loss: 0.449021  [96000/175341]\n",
      "loss: 0.557160  [97600/175341]\n",
      "loss: 0.528943  [99200/175341]\n",
      "loss: 0.284595  [100800/175341]\n",
      "loss: 0.308606  [102400/175341]\n",
      "loss: 0.270337  [104000/175341]\n",
      "loss: 0.548534  [105600/175341]\n",
      "loss: 0.662887  [107200/175341]\n",
      "loss: 0.398972  [108800/175341]\n",
      "loss: 0.399630  [110400/175341]\n",
      "loss: 0.356835  [112000/175341]\n",
      "loss: 0.278275  [113600/175341]\n",
      "loss: 0.554919  [115200/175341]\n",
      "loss: 0.427121  [116800/175341]\n",
      "loss: 0.232292  [118400/175341]\n",
      "loss: 0.396995  [120000/175341]\n",
      "loss: 0.275768  [121600/175341]\n",
      "loss: 0.443515  [123200/175341]\n",
      "loss: 0.294904  [124800/175341]\n",
      "loss: 0.486753  [126400/175341]\n",
      "loss: 0.294341  [128000/175341]\n",
      "loss: 0.396043  [129600/175341]\n",
      "loss: 0.678934  [131200/175341]\n",
      "loss: 0.095647  [132800/175341]\n",
      "loss: 0.357240  [134400/175341]\n",
      "loss: 0.976162  [136000/175341]\n",
      "loss: 0.737395  [137600/175341]\n",
      "loss: 0.083249  [139200/175341]\n",
      "loss: 0.786439  [140800/175341]\n",
      "loss: 0.597203  [142400/175341]\n",
      "loss: 0.349229  [144000/175341]\n",
      "loss: 0.184203  [145600/175341]\n",
      "loss: 0.415442  [147200/175341]\n",
      "loss: 0.376856  [148800/175341]\n",
      "loss: 0.446337  [150400/175341]\n",
      "loss: 0.752469  [152000/175341]\n",
      "loss: 0.510345  [153600/175341]\n",
      "loss: 0.553176  [155200/175341]\n",
      "loss: 0.602758  [156800/175341]\n",
      "loss: 0.540289  [158400/175341]\n",
      "loss: 0.659000  [160000/175341]\n",
      "loss: 0.357069  [161600/175341]\n",
      "loss: 0.688215  [163200/175341]\n",
      "loss: 0.363067  [164800/175341]\n",
      "loss: 0.679127  [166400/175341]\n",
      "loss: 0.327934  [168000/175341]\n",
      "loss: 0.200851  [169600/175341]\n",
      "loss: 0.292609  [171200/175341]\n",
      "loss: 0.355527  [172800/175341]\n",
      "loss: 0.319484  [174400/175341]\n",
      "Train Accuracy: 81.1328%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.546271, F1-score: 75.79%, Macro_F1-Score:  42.24%  \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.271421  [    0/175341]\n",
      "loss: 0.485046  [ 1600/175341]\n",
      "loss: 0.233125  [ 3200/175341]\n",
      "loss: 0.147855  [ 4800/175341]\n",
      "loss: 0.700690  [ 6400/175341]\n",
      "loss: 0.766205  [ 8000/175341]\n",
      "loss: 0.210708  [ 9600/175341]\n",
      "loss: 0.231336  [11200/175341]\n",
      "loss: 0.244802  [12800/175341]\n",
      "loss: 0.418809  [14400/175341]\n",
      "loss: 0.395475  [16000/175341]\n",
      "loss: 0.395254  [17600/175341]\n",
      "loss: 0.501456  [19200/175341]\n",
      "loss: 0.497051  [20800/175341]\n",
      "loss: 0.805225  [22400/175341]\n",
      "loss: 0.508865  [24000/175341]\n",
      "loss: 0.676181  [25600/175341]\n",
      "loss: 0.227312  [27200/175341]\n",
      "loss: 0.365592  [28800/175341]\n",
      "loss: 0.695130  [30400/175341]\n",
      "loss: 0.596340  [32000/175341]\n",
      "loss: 0.551914  [33600/175341]\n",
      "loss: 0.331529  [35200/175341]\n",
      "loss: 0.799150  [36800/175341]\n",
      "loss: 0.441141  [38400/175341]\n",
      "loss: 0.685750  [40000/175341]\n",
      "loss: 0.498854  [41600/175341]\n",
      "loss: 0.642546  [43200/175341]\n",
      "loss: 0.857272  [44800/175341]\n",
      "loss: 0.608798  [46400/175341]\n",
      "loss: 0.474951  [48000/175341]\n",
      "loss: 0.463552  [49600/175341]\n",
      "loss: 0.407473  [51200/175341]\n",
      "loss: 0.319317  [52800/175341]\n",
      "loss: 0.437532  [54400/175341]\n",
      "loss: 1.056499  [56000/175341]\n",
      "loss: 0.697341  [57600/175341]\n",
      "loss: 0.541874  [59200/175341]\n",
      "loss: 0.807011  [60800/175341]\n",
      "loss: 0.509587  [62400/175341]\n",
      "loss: 0.362307  [64000/175341]\n",
      "loss: 0.460167  [65600/175341]\n",
      "loss: 0.382769  [67200/175341]\n",
      "loss: 0.448005  [68800/175341]\n",
      "loss: 0.358092  [70400/175341]\n",
      "loss: 0.510810  [72000/175341]\n",
      "loss: 0.495967  [73600/175341]\n",
      "loss: 0.435737  [75200/175341]\n",
      "loss: 0.232088  [76800/175341]\n",
      "loss: 0.326610  [78400/175341]\n",
      "loss: 0.632321  [80000/175341]\n",
      "loss: 0.325271  [81600/175341]\n",
      "loss: 0.562264  [83200/175341]\n",
      "loss: 0.388921  [84800/175341]\n",
      "loss: 0.390452  [86400/175341]\n",
      "loss: 0.624471  [88000/175341]\n",
      "loss: 0.323158  [89600/175341]\n",
      "loss: 0.209602  [91200/175341]\n",
      "loss: 0.905166  [92800/175341]\n",
      "loss: 0.399948  [94400/175341]\n",
      "loss: 0.918538  [96000/175341]\n",
      "loss: 0.609256  [97600/175341]\n",
      "loss: 0.574865  [99200/175341]\n",
      "loss: 0.432493  [100800/175341]\n",
      "loss: 0.559366  [102400/175341]\n",
      "loss: 0.451500  [104000/175341]\n",
      "loss: 0.312860  [105600/175341]\n",
      "loss: 0.706589  [107200/175341]\n",
      "loss: 0.450807  [108800/175341]\n",
      "loss: 0.414598  [110400/175341]\n",
      "loss: 0.718365  [112000/175341]\n",
      "loss: 0.652994  [113600/175341]\n",
      "loss: 0.441335  [115200/175341]\n",
      "loss: 0.651648  [116800/175341]\n",
      "loss: 0.732224  [118400/175341]\n",
      "loss: 0.565112  [120000/175341]\n",
      "loss: 0.387013  [121600/175341]\n",
      "loss: 0.278225  [123200/175341]\n",
      "loss: 0.596914  [124800/175341]\n",
      "loss: 0.518767  [126400/175341]\n",
      "loss: 0.514787  [128000/175341]\n",
      "loss: 0.469542  [129600/175341]\n",
      "loss: 0.284271  [131200/175341]\n",
      "loss: 0.529222  [132800/175341]\n",
      "loss: 0.406027  [134400/175341]\n",
      "loss: 0.335178  [136000/175341]\n",
      "loss: 0.334852  [137600/175341]\n",
      "loss: 0.473214  [139200/175341]\n",
      "loss: 0.482301  [140800/175341]\n",
      "loss: 0.886334  [142400/175341]\n",
      "loss: 0.311562  [144000/175341]\n",
      "loss: 0.450622  [145600/175341]\n",
      "loss: 0.360820  [147200/175341]\n",
      "loss: 0.636574  [148800/175341]\n",
      "loss: 0.547346  [150400/175341]\n",
      "loss: 0.489327  [152000/175341]\n",
      "loss: 0.492524  [153600/175341]\n",
      "loss: 0.405504  [155200/175341]\n",
      "loss: 0.300410  [156800/175341]\n",
      "loss: 0.607581  [158400/175341]\n",
      "loss: 0.156663  [160000/175341]\n",
      "loss: 0.303693  [161600/175341]\n",
      "loss: 0.402441  [163200/175341]\n",
      "loss: 0.440056  [164800/175341]\n",
      "loss: 0.509517  [166400/175341]\n",
      "loss: 0.316977  [168000/175341]\n",
      "loss: 0.442839  [169600/175341]\n",
      "loss: 0.735523  [171200/175341]\n",
      "loss: 0.587435  [172800/175341]\n",
      "loss: 0.592824  [174400/175341]\n",
      "Train Accuracy: 81.1077%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.554493, F1-score: 75.54%, Macro_F1-Score:  42.60%  \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.202992  [    0/175341]\n",
      "loss: 0.244942  [ 1600/175341]\n",
      "loss: 0.323363  [ 3200/175341]\n",
      "loss: 0.831372  [ 4800/175341]\n",
      "loss: 0.517290  [ 6400/175341]\n",
      "loss: 0.343540  [ 8000/175341]\n",
      "loss: 0.422295  [ 9600/175341]\n",
      "loss: 0.286316  [11200/175341]\n",
      "loss: 0.656781  [12800/175341]\n",
      "loss: 0.251766  [14400/175341]\n",
      "loss: 0.381637  [16000/175341]\n",
      "loss: 0.389260  [17600/175341]\n",
      "loss: 0.194198  [19200/175341]\n",
      "loss: 0.521979  [20800/175341]\n",
      "loss: 0.489232  [22400/175341]\n",
      "loss: 0.507688  [24000/175341]\n",
      "loss: 0.395805  [25600/175341]\n",
      "loss: 0.241050  [27200/175341]\n",
      "loss: 0.556708  [28800/175341]\n",
      "loss: 0.344140  [30400/175341]\n",
      "loss: 0.347266  [32000/175341]\n",
      "loss: 0.348239  [33600/175341]\n",
      "loss: 0.412758  [35200/175341]\n",
      "loss: 0.540130  [36800/175341]\n",
      "loss: 0.340291  [38400/175341]\n",
      "loss: 0.370372  [40000/175341]\n",
      "loss: 0.747502  [41600/175341]\n",
      "loss: 0.351141  [43200/175341]\n",
      "loss: 0.571653  [44800/175341]\n",
      "loss: 0.504189  [46400/175341]\n",
      "loss: 0.563726  [48000/175341]\n",
      "loss: 0.944587  [49600/175341]\n",
      "loss: 0.633742  [51200/175341]\n",
      "loss: 0.574521  [52800/175341]\n",
      "loss: 0.626218  [54400/175341]\n",
      "loss: 0.216378  [56000/175341]\n",
      "loss: 1.089553  [57600/175341]\n",
      "loss: 0.460685  [59200/175341]\n",
      "loss: 0.266105  [60800/175341]\n",
      "loss: 0.754071  [62400/175341]\n",
      "loss: 0.705608  [64000/175341]\n",
      "loss: 0.295078  [65600/175341]\n",
      "loss: 0.146185  [67200/175341]\n",
      "loss: 0.705266  [68800/175341]\n",
      "loss: 0.483337  [70400/175341]\n",
      "loss: 0.239054  [72000/175341]\n",
      "loss: 0.565384  [73600/175341]\n",
      "loss: 0.563113  [75200/175341]\n",
      "loss: 0.243953  [76800/175341]\n",
      "loss: 0.619764  [78400/175341]\n",
      "loss: 0.236996  [80000/175341]\n",
      "loss: 0.307570  [81600/175341]\n",
      "loss: 0.163753  [83200/175341]\n",
      "loss: 0.746013  [84800/175341]\n",
      "loss: 0.382754  [86400/175341]\n",
      "loss: 0.559884  [88000/175341]\n",
      "loss: 0.588703  [89600/175341]\n",
      "loss: 0.729487  [91200/175341]\n",
      "loss: 0.419908  [92800/175341]\n",
      "loss: 0.273732  [94400/175341]\n",
      "loss: 0.501289  [96000/175341]\n",
      "loss: 0.539985  [97600/175341]\n",
      "loss: 0.633726  [99200/175341]\n",
      "loss: 0.433917  [100800/175341]\n",
      "loss: 0.467854  [102400/175341]\n",
      "loss: 0.196610  [104000/175341]\n",
      "loss: 0.565383  [105600/175341]\n",
      "loss: 0.295904  [107200/175341]\n",
      "loss: 0.360407  [108800/175341]\n",
      "loss: 0.625492  [110400/175341]\n",
      "loss: 0.362076  [112000/175341]\n",
      "loss: 0.294540  [113600/175341]\n",
      "loss: 0.154549  [115200/175341]\n",
      "loss: 1.247658  [116800/175341]\n",
      "loss: 0.403542  [118400/175341]\n",
      "loss: 0.381152  [120000/175341]\n",
      "loss: 0.319854  [121600/175341]\n",
      "loss: 0.304729  [123200/175341]\n",
      "loss: 0.697376  [124800/175341]\n",
      "loss: 0.504980  [126400/175341]\n",
      "loss: 0.384228  [128000/175341]\n",
      "loss: 0.173468  [129600/175341]\n",
      "loss: 0.822783  [131200/175341]\n",
      "loss: 0.441698  [132800/175341]\n",
      "loss: 0.357246  [134400/175341]\n",
      "loss: 0.461525  [136000/175341]\n",
      "loss: 0.401770  [137600/175341]\n",
      "loss: 0.475692  [139200/175341]\n",
      "loss: 1.096330  [140800/175341]\n",
      "loss: 0.544007  [142400/175341]\n",
      "loss: 0.996771  [144000/175341]\n",
      "loss: 0.381434  [145600/175341]\n",
      "loss: 0.181283  [147200/175341]\n",
      "loss: 0.237027  [148800/175341]\n",
      "loss: 0.394483  [150400/175341]\n",
      "loss: 0.657253  [152000/175341]\n",
      "loss: 0.944505  [153600/175341]\n",
      "loss: 0.356920  [155200/175341]\n",
      "loss: 0.563845  [156800/175341]\n",
      "loss: 0.374982  [158400/175341]\n",
      "loss: 0.293837  [160000/175341]\n",
      "loss: 0.215834  [161600/175341]\n",
      "loss: 0.311731  [163200/175341]\n",
      "loss: 0.402276  [164800/175341]\n",
      "loss: 0.330939  [166400/175341]\n",
      "loss: 0.568624  [168000/175341]\n",
      "loss: 0.480415  [169600/175341]\n",
      "loss: 0.291343  [171200/175341]\n",
      "loss: 0.377500  [172800/175341]\n",
      "loss: 0.326902  [174400/175341]\n",
      "Train Accuracy: 81.1681%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.542591, F1-score: 75.62%, Macro_F1-Score:  41.75%  \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.729100  [    0/175341]\n",
      "loss: 0.514983  [ 1600/175341]\n",
      "loss: 0.349708  [ 3200/175341]\n",
      "loss: 0.490541  [ 4800/175341]\n",
      "loss: 0.418064  [ 6400/175341]\n",
      "loss: 0.464049  [ 8000/175341]\n",
      "loss: 0.378801  [ 9600/175341]\n",
      "loss: 0.203477  [11200/175341]\n",
      "loss: 0.598036  [12800/175341]\n",
      "loss: 0.581928  [14400/175341]\n",
      "loss: 0.283035  [16000/175341]\n",
      "loss: 0.375796  [17600/175341]\n",
      "loss: 0.841259  [19200/175341]\n",
      "loss: 0.219119  [20800/175341]\n",
      "loss: 0.453533  [22400/175341]\n",
      "loss: 0.251695  [24000/175341]\n",
      "loss: 0.438715  [25600/175341]\n",
      "loss: 0.930508  [27200/175341]\n",
      "loss: 0.276474  [28800/175341]\n",
      "loss: 0.347865  [30400/175341]\n",
      "loss: 0.545729  [32000/175341]\n",
      "loss: 0.571676  [33600/175341]\n",
      "loss: 0.558196  [35200/175341]\n",
      "loss: 0.609733  [36800/175341]\n",
      "loss: 0.280049  [38400/175341]\n",
      "loss: 0.380809  [40000/175341]\n",
      "loss: 0.377240  [41600/175341]\n",
      "loss: 0.503351  [43200/175341]\n",
      "loss: 0.747663  [44800/175341]\n",
      "loss: 0.472168  [46400/175341]\n",
      "loss: 0.554329  [48000/175341]\n",
      "loss: 0.130530  [49600/175341]\n",
      "loss: 0.458043  [51200/175341]\n",
      "loss: 0.379654  [52800/175341]\n",
      "loss: 0.493538  [54400/175341]\n",
      "loss: 0.654163  [56000/175341]\n",
      "loss: 0.854645  [57600/175341]\n",
      "loss: 0.746670  [59200/175341]\n",
      "loss: 0.721104  [60800/175341]\n",
      "loss: 0.202225  [62400/175341]\n",
      "loss: 0.294206  [64000/175341]\n",
      "loss: 0.352783  [65600/175341]\n",
      "loss: 0.287373  [67200/175341]\n",
      "loss: 0.166162  [68800/175341]\n",
      "loss: 0.430508  [70400/175341]\n",
      "loss: 0.588675  [72000/175341]\n",
      "loss: 0.404492  [73600/175341]\n",
      "loss: 0.597164  [75200/175341]\n",
      "loss: 0.398223  [76800/175341]\n",
      "loss: 0.725465  [78400/175341]\n",
      "loss: 0.286200  [80000/175341]\n",
      "loss: 0.354740  [81600/175341]\n",
      "loss: 0.518686  [83200/175341]\n",
      "loss: 0.486967  [84800/175341]\n",
      "loss: 0.511101  [86400/175341]\n",
      "loss: 0.642355  [88000/175341]\n",
      "loss: 0.481849  [89600/175341]\n",
      "loss: 0.637419  [91200/175341]\n",
      "loss: 0.699220  [92800/175341]\n",
      "loss: 0.414575  [94400/175341]\n",
      "loss: 0.679224  [96000/175341]\n",
      "loss: 0.274080  [97600/175341]\n",
      "loss: 0.157266  [99200/175341]\n",
      "loss: 0.432226  [100800/175341]\n",
      "loss: 0.563279  [102400/175341]\n",
      "loss: 0.652215  [104000/175341]\n",
      "loss: 0.793369  [105600/175341]\n",
      "loss: 0.814210  [107200/175341]\n",
      "loss: 0.549470  [108800/175341]\n",
      "loss: 0.287682  [110400/175341]\n",
      "loss: 0.652183  [112000/175341]\n",
      "loss: 0.224387  [113600/175341]\n",
      "loss: 0.745358  [115200/175341]\n",
      "loss: 0.818784  [116800/175341]\n",
      "loss: 0.529493  [118400/175341]\n",
      "loss: 0.888546  [120000/175341]\n",
      "loss: 0.776248  [121600/175341]\n",
      "loss: 0.858985  [123200/175341]\n",
      "loss: 0.374554  [124800/175341]\n",
      "loss: 0.347029  [126400/175341]\n",
      "loss: 1.034830  [128000/175341]\n",
      "loss: 0.450652  [129600/175341]\n",
      "loss: 0.208890  [131200/175341]\n",
      "loss: 0.243673  [132800/175341]\n",
      "loss: 0.195466  [134400/175341]\n",
      "loss: 0.893422  [136000/175341]\n",
      "loss: 0.436809  [137600/175341]\n",
      "loss: 0.558178  [139200/175341]\n",
      "loss: 0.245946  [140800/175341]\n",
      "loss: 0.140967  [142400/175341]\n",
      "loss: 0.283940  [144000/175341]\n",
      "loss: 0.427203  [145600/175341]\n",
      "loss: 1.056002  [147200/175341]\n",
      "loss: 0.211597  [148800/175341]\n",
      "loss: 0.449941  [150400/175341]\n",
      "loss: 0.708688  [152000/175341]\n",
      "loss: 0.495547  [153600/175341]\n",
      "loss: 0.489799  [155200/175341]\n",
      "loss: 0.526406  [156800/175341]\n",
      "loss: 0.559861  [158400/175341]\n",
      "loss: 0.163360  [160000/175341]\n",
      "loss: 0.654982  [161600/175341]\n",
      "loss: 0.096761  [163200/175341]\n",
      "loss: 0.413249  [164800/175341]\n",
      "loss: 0.653303  [166400/175341]\n",
      "loss: 0.520602  [168000/175341]\n",
      "loss: 0.304491  [169600/175341]\n",
      "loss: 0.592055  [171200/175341]\n",
      "loss: 0.340482  [172800/175341]\n",
      "loss: 0.107746  [174400/175341]\n",
      "Train Accuracy: 81.1544%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.573936, F1-score: 74.54%, Macro_F1-Score:  41.25%  \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.569736  [    0/175341]\n",
      "loss: 0.345660  [ 1600/175341]\n",
      "loss: 0.434610  [ 3200/175341]\n",
      "loss: 0.466300  [ 4800/175341]\n",
      "loss: 0.649543  [ 6400/175341]\n",
      "loss: 0.596385  [ 8000/175341]\n",
      "loss: 0.399131  [ 9600/175341]\n",
      "loss: 0.291644  [11200/175341]\n",
      "loss: 0.464099  [12800/175341]\n",
      "loss: 0.431817  [14400/175341]\n",
      "loss: 0.408861  [16000/175341]\n",
      "loss: 0.372250  [17600/175341]\n",
      "loss: 0.150920  [19200/175341]\n",
      "loss: 0.833887  [20800/175341]\n",
      "loss: 0.338300  [22400/175341]\n",
      "loss: 0.644010  [24000/175341]\n",
      "loss: 0.728621  [25600/175341]\n",
      "loss: 0.345743  [27200/175341]\n",
      "loss: 0.150054  [28800/175341]\n",
      "loss: 0.429521  [30400/175341]\n",
      "loss: 0.436839  [32000/175341]\n",
      "loss: 0.565091  [33600/175341]\n",
      "loss: 0.313851  [35200/175341]\n",
      "loss: 0.297550  [36800/175341]\n",
      "loss: 0.198851  [38400/175341]\n",
      "loss: 0.716868  [40000/175341]\n",
      "loss: 0.406073  [41600/175341]\n",
      "loss: 0.880847  [43200/175341]\n",
      "loss: 0.393035  [44800/175341]\n",
      "loss: 0.520199  [46400/175341]\n",
      "loss: 0.580463  [48000/175341]\n",
      "loss: 0.320052  [49600/175341]\n",
      "loss: 0.598065  [51200/175341]\n",
      "loss: 0.812051  [52800/175341]\n",
      "loss: 0.396357  [54400/175341]\n",
      "loss: 0.428464  [56000/175341]\n",
      "loss: 0.298311  [57600/175341]\n",
      "loss: 0.217144  [59200/175341]\n",
      "loss: 0.362999  [60800/175341]\n",
      "loss: 0.348863  [62400/175341]\n",
      "loss: 0.596027  [64000/175341]\n",
      "loss: 0.451910  [65600/175341]\n",
      "loss: 0.639847  [67200/175341]\n",
      "loss: 0.419196  [68800/175341]\n",
      "loss: 0.483421  [70400/175341]\n",
      "loss: 0.244359  [72000/175341]\n",
      "loss: 1.022279  [73600/175341]\n",
      "loss: 0.438534  [75200/175341]\n",
      "loss: 0.358261  [76800/175341]\n",
      "loss: 0.666954  [78400/175341]\n",
      "loss: 0.076173  [80000/175341]\n",
      "loss: 0.348144  [81600/175341]\n",
      "loss: 0.252427  [83200/175341]\n",
      "loss: 0.283826  [84800/175341]\n",
      "loss: 0.370415  [86400/175341]\n",
      "loss: 0.400778  [88000/175341]\n",
      "loss: 0.654848  [89600/175341]\n",
      "loss: 0.407112  [91200/175341]\n",
      "loss: 0.499891  [92800/175341]\n",
      "loss: 0.399265  [94400/175341]\n",
      "loss: 0.384250  [96000/175341]\n",
      "loss: 0.706081  [97600/175341]\n",
      "loss: 0.394476  [99200/175341]\n",
      "loss: 0.836400  [100800/175341]\n",
      "loss: 0.471875  [102400/175341]\n",
      "loss: 0.346781  [104000/175341]\n",
      "loss: 0.639947  [105600/175341]\n",
      "loss: 0.484595  [107200/175341]\n",
      "loss: 0.501604  [108800/175341]\n",
      "loss: 0.805886  [110400/175341]\n",
      "loss: 0.523078  [112000/175341]\n",
      "loss: 0.648704  [113600/175341]\n",
      "loss: 0.574744  [115200/175341]\n",
      "loss: 0.262261  [116800/175341]\n",
      "loss: 0.551687  [118400/175341]\n",
      "loss: 0.389522  [120000/175341]\n",
      "loss: 0.413695  [121600/175341]\n",
      "loss: 0.800367  [123200/175341]\n",
      "loss: 0.514720  [124800/175341]\n",
      "loss: 0.467189  [126400/175341]\n",
      "loss: 0.317823  [128000/175341]\n",
      "loss: 0.470905  [129600/175341]\n",
      "loss: 0.227582  [131200/175341]\n",
      "loss: 0.258589  [132800/175341]\n",
      "loss: 0.408079  [134400/175341]\n",
      "loss: 0.425348  [136000/175341]\n",
      "loss: 0.774414  [137600/175341]\n",
      "loss: 0.359253  [139200/175341]\n",
      "loss: 0.673564  [140800/175341]\n",
      "loss: 0.723589  [142400/175341]\n",
      "loss: 0.407971  [144000/175341]\n",
      "loss: 0.547006  [145600/175341]\n",
      "loss: 0.493604  [147200/175341]\n",
      "loss: 0.476419  [148800/175341]\n",
      "loss: 0.894684  [150400/175341]\n",
      "loss: 0.413013  [152000/175341]\n",
      "loss: 0.629680  [153600/175341]\n",
      "loss: 0.495578  [155200/175341]\n",
      "loss: 0.447038  [156800/175341]\n",
      "loss: 0.515955  [158400/175341]\n",
      "loss: 0.582922  [160000/175341]\n",
      "loss: 0.412748  [161600/175341]\n",
      "loss: 0.452464  [163200/175341]\n",
      "loss: 0.680189  [164800/175341]\n",
      "loss: 0.194348  [166400/175341]\n",
      "loss: 0.396465  [168000/175341]\n",
      "loss: 0.210817  [169600/175341]\n",
      "loss: 0.402601  [171200/175341]\n",
      "loss: 0.478266  [172800/175341]\n",
      "loss: 0.390114  [174400/175341]\n",
      "Train Accuracy: 81.1179%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.543324, F1-score: 75.75%, Macro_F1-Score:  42.33%  \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.098170  [    0/175341]\n",
      "loss: 0.393262  [ 1600/175341]\n",
      "loss: 0.309903  [ 3200/175341]\n",
      "loss: 0.367238  [ 4800/175341]\n",
      "loss: 0.454427  [ 6400/175341]\n",
      "loss: 0.625524  [ 8000/175341]\n",
      "loss: 0.685388  [ 9600/175341]\n",
      "loss: 0.112040  [11200/175341]\n",
      "loss: 0.646641  [12800/175341]\n",
      "loss: 0.498778  [14400/175341]\n",
      "loss: 0.677584  [16000/175341]\n",
      "loss: 0.446241  [17600/175341]\n",
      "loss: 0.447073  [19200/175341]\n",
      "loss: 0.601509  [20800/175341]\n",
      "loss: 0.095347  [22400/175341]\n",
      "loss: 0.823352  [24000/175341]\n",
      "loss: 0.225302  [25600/175341]\n",
      "loss: 0.658943  [27200/175341]\n",
      "loss: 0.379786  [28800/175341]\n",
      "loss: 0.431598  [30400/175341]\n",
      "loss: 0.476312  [32000/175341]\n",
      "loss: 0.573865  [33600/175341]\n",
      "loss: 0.684280  [35200/175341]\n",
      "loss: 0.331687  [36800/175341]\n",
      "loss: 0.524420  [38400/175341]\n",
      "loss: 1.124363  [40000/175341]\n",
      "loss: 0.455463  [41600/175341]\n",
      "loss: 0.425282  [43200/175341]\n",
      "loss: 0.459576  [44800/175341]\n",
      "loss: 0.896474  [46400/175341]\n",
      "loss: 0.592849  [48000/175341]\n",
      "loss: 0.660828  [49600/175341]\n",
      "loss: 0.747699  [51200/175341]\n",
      "loss: 0.357331  [52800/175341]\n",
      "loss: 0.481886  [54400/175341]\n",
      "loss: 0.424528  [56000/175341]\n",
      "loss: 0.543064  [57600/175341]\n",
      "loss: 0.236659  [59200/175341]\n",
      "loss: 0.419699  [60800/175341]\n",
      "loss: 0.522046  [62400/175341]\n",
      "loss: 0.593906  [64000/175341]\n",
      "loss: 0.409065  [65600/175341]\n",
      "loss: 1.075948  [67200/175341]\n",
      "loss: 0.683246  [68800/175341]\n",
      "loss: 0.451217  [70400/175341]\n",
      "loss: 0.408311  [72000/175341]\n",
      "loss: 0.329430  [73600/175341]\n",
      "loss: 0.660983  [75200/175341]\n",
      "loss: 0.659885  [76800/175341]\n",
      "loss: 0.247524  [78400/175341]\n",
      "loss: 0.421825  [80000/175341]\n",
      "loss: 0.667154  [81600/175341]\n",
      "loss: 0.374073  [83200/175341]\n",
      "loss: 0.839964  [84800/175341]\n",
      "loss: 0.319628  [86400/175341]\n",
      "loss: 0.281467  [88000/175341]\n",
      "loss: 0.445965  [89600/175341]\n",
      "loss: 0.319513  [91200/175341]\n",
      "loss: 0.253410  [92800/175341]\n",
      "loss: 0.056745  [94400/175341]\n",
      "loss: 0.490334  [96000/175341]\n",
      "loss: 0.540006  [97600/175341]\n",
      "loss: 0.190379  [99200/175341]\n",
      "loss: 0.725759  [100800/175341]\n",
      "loss: 0.371184  [102400/175341]\n",
      "loss: 0.311490  [104000/175341]\n",
      "loss: 0.197481  [105600/175341]\n",
      "loss: 0.457532  [107200/175341]\n",
      "loss: 0.322682  [108800/175341]\n",
      "loss: 0.464070  [110400/175341]\n",
      "loss: 0.612507  [112000/175341]\n",
      "loss: 0.475829  [113600/175341]\n",
      "loss: 0.590450  [115200/175341]\n",
      "loss: 0.281207  [116800/175341]\n",
      "loss: 0.206605  [118400/175341]\n",
      "loss: 0.839810  [120000/175341]\n",
      "loss: 0.353171  [121600/175341]\n",
      "loss: 0.540465  [123200/175341]\n",
      "loss: 0.323245  [124800/175341]\n",
      "loss: 0.564008  [126400/175341]\n",
      "loss: 0.217130  [128000/175341]\n",
      "loss: 0.316859  [129600/175341]\n",
      "loss: 0.177755  [131200/175341]\n",
      "loss: 0.224122  [132800/175341]\n",
      "loss: 0.662405  [134400/175341]\n",
      "loss: 0.224555  [136000/175341]\n",
      "loss: 0.741140  [137600/175341]\n",
      "loss: 1.322530  [139200/175341]\n",
      "loss: 0.457665  [140800/175341]\n",
      "loss: 0.418309  [142400/175341]\n",
      "loss: 0.437082  [144000/175341]\n",
      "loss: 0.420542  [145600/175341]\n",
      "loss: 0.655262  [147200/175341]\n",
      "loss: 0.389794  [148800/175341]\n",
      "loss: 0.654129  [150400/175341]\n",
      "loss: 0.398371  [152000/175341]\n",
      "loss: 0.251780  [153600/175341]\n",
      "loss: 1.223778  [155200/175341]\n",
      "loss: 0.468755  [156800/175341]\n",
      "loss: 0.278573  [158400/175341]\n",
      "loss: 0.805493  [160000/175341]\n",
      "loss: 0.507074  [161600/175341]\n",
      "loss: 0.200288  [163200/175341]\n",
      "loss: 0.390714  [164800/175341]\n",
      "loss: 0.469992  [166400/175341]\n",
      "loss: 0.341313  [168000/175341]\n",
      "loss: 0.598715  [169600/175341]\n",
      "loss: 0.443163  [171200/175341]\n",
      "loss: 0.158830  [172800/175341]\n",
      "loss: 0.294388  [174400/175341]\n",
      "Train Accuracy: 81.1413%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.542806, F1-score: 75.53%, Macro_F1-Score:  41.93%  \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.565181  [    0/175341]\n",
      "loss: 0.381808  [ 1600/175341]\n",
      "loss: 0.499891  [ 3200/175341]\n",
      "loss: 0.600699  [ 4800/175341]\n",
      "loss: 0.278090  [ 6400/175341]\n",
      "loss: 0.449118  [ 8000/175341]\n",
      "loss: 0.172759  [ 9600/175341]\n",
      "loss: 0.315615  [11200/175341]\n",
      "loss: 0.440410  [12800/175341]\n",
      "loss: 0.255428  [14400/175341]\n",
      "loss: 0.825215  [16000/175341]\n",
      "loss: 0.602010  [17600/175341]\n",
      "loss: 0.530368  [19200/175341]\n",
      "loss: 0.534819  [20800/175341]\n",
      "loss: 0.345958  [22400/175341]\n",
      "loss: 0.216369  [24000/175341]\n",
      "loss: 0.325560  [25600/175341]\n",
      "loss: 0.658625  [27200/175341]\n",
      "loss: 0.709020  [28800/175341]\n",
      "loss: 0.427613  [30400/175341]\n",
      "loss: 0.366949  [32000/175341]\n",
      "loss: 0.098320  [33600/175341]\n",
      "loss: 0.556777  [35200/175341]\n",
      "loss: 0.564590  [36800/175341]\n",
      "loss: 0.827314  [38400/175341]\n",
      "loss: 0.245663  [40000/175341]\n",
      "loss: 0.405400  [41600/175341]\n",
      "loss: 0.457475  [43200/175341]\n",
      "loss: 0.480911  [44800/175341]\n",
      "loss: 0.556080  [46400/175341]\n",
      "loss: 0.387411  [48000/175341]\n",
      "loss: 0.637791  [49600/175341]\n",
      "loss: 0.435028  [51200/175341]\n",
      "loss: 0.917257  [52800/175341]\n",
      "loss: 0.679759  [54400/175341]\n",
      "loss: 0.429358  [56000/175341]\n",
      "loss: 0.377122  [57600/175341]\n",
      "loss: 0.130976  [59200/175341]\n",
      "loss: 0.382157  [60800/175341]\n",
      "loss: 0.179750  [62400/175341]\n",
      "loss: 0.192091  [64000/175341]\n",
      "loss: 1.022672  [65600/175341]\n",
      "loss: 0.541454  [67200/175341]\n",
      "loss: 0.302252  [68800/175341]\n",
      "loss: 0.297728  [70400/175341]\n",
      "loss: 0.478851  [72000/175341]\n",
      "loss: 0.530080  [73600/175341]\n",
      "loss: 0.784802  [75200/175341]\n",
      "loss: 0.390068  [76800/175341]\n",
      "loss: 0.532981  [78400/175341]\n",
      "loss: 0.418043  [80000/175341]\n",
      "loss: 0.542711  [81600/175341]\n",
      "loss: 0.693532  [83200/175341]\n",
      "loss: 0.520848  [84800/175341]\n",
      "loss: 0.456733  [86400/175341]\n",
      "loss: 0.657108  [88000/175341]\n",
      "loss: 0.638891  [89600/175341]\n",
      "loss: 0.578373  [91200/175341]\n",
      "loss: 0.726585  [92800/175341]\n",
      "loss: 0.560993  [94400/175341]\n",
      "loss: 0.526308  [96000/175341]\n",
      "loss: 0.345938  [97600/175341]\n",
      "loss: 0.411646  [99200/175341]\n",
      "loss: 0.994887  [100800/175341]\n",
      "loss: 0.511119  [102400/175341]\n",
      "loss: 0.242189  [104000/175341]\n",
      "loss: 0.771327  [105600/175341]\n",
      "loss: 0.604035  [107200/175341]\n",
      "loss: 0.453327  [108800/175341]\n",
      "loss: 0.702123  [110400/175341]\n",
      "loss: 0.603155  [112000/175341]\n",
      "loss: 0.287170  [113600/175341]\n",
      "loss: 0.206536  [115200/175341]\n",
      "loss: 0.564934  [116800/175341]\n",
      "loss: 0.561800  [118400/175341]\n",
      "loss: 0.446275  [120000/175341]\n",
      "loss: 0.609490  [121600/175341]\n",
      "loss: 0.532442  [123200/175341]\n",
      "loss: 0.844373  [124800/175341]\n",
      "loss: 0.462884  [126400/175341]\n",
      "loss: 0.297986  [128000/175341]\n",
      "loss: 0.122850  [129600/175341]\n",
      "loss: 0.197063  [131200/175341]\n",
      "loss: 0.290670  [132800/175341]\n",
      "loss: 0.256278  [134400/175341]\n",
      "loss: 0.693997  [136000/175341]\n",
      "loss: 0.354180  [137600/175341]\n",
      "loss: 0.746159  [139200/175341]\n",
      "loss: 0.160944  [140800/175341]\n",
      "loss: 0.449931  [142400/175341]\n",
      "loss: 0.468663  [144000/175341]\n",
      "loss: 0.378309  [145600/175341]\n",
      "loss: 0.220393  [147200/175341]\n",
      "loss: 0.103959  [148800/175341]\n",
      "loss: 0.459561  [150400/175341]\n",
      "loss: 0.728037  [152000/175341]\n",
      "loss: 0.443600  [153600/175341]\n",
      "loss: 0.773732  [155200/175341]\n",
      "loss: 0.483337  [156800/175341]\n",
      "loss: 0.482470  [158400/175341]\n",
      "loss: 0.167655  [160000/175341]\n",
      "loss: 0.453111  [161600/175341]\n",
      "loss: 0.445186  [163200/175341]\n",
      "loss: 1.104333  [164800/175341]\n",
      "loss: 0.492516  [166400/175341]\n",
      "loss: 0.680448  [168000/175341]\n",
      "loss: 0.316241  [169600/175341]\n",
      "loss: 0.437872  [171200/175341]\n",
      "loss: 0.551514  [172800/175341]\n",
      "loss: 0.451154  [174400/175341]\n",
      "Train Accuracy: 81.1750%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.546760, F1-score: 76.61%, Macro_F1-Score:  43.50%  \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.873138  [    0/175341]\n",
      "loss: 0.941948  [ 1600/175341]\n",
      "loss: 0.486692  [ 3200/175341]\n",
      "loss: 0.678916  [ 4800/175341]\n",
      "loss: 0.570914  [ 6400/175341]\n",
      "loss: 0.736907  [ 8000/175341]\n",
      "loss: 0.519646  [ 9600/175341]\n",
      "loss: 0.491601  [11200/175341]\n",
      "loss: 0.523857  [12800/175341]\n",
      "loss: 0.502858  [14400/175341]\n",
      "loss: 0.241036  [16000/175341]\n",
      "loss: 0.528990  [17600/175341]\n",
      "loss: 0.642967  [19200/175341]\n",
      "loss: 0.451254  [20800/175341]\n",
      "loss: 0.894628  [22400/175341]\n",
      "loss: 0.271145  [24000/175341]\n",
      "loss: 0.423071  [25600/175341]\n",
      "loss: 1.077576  [27200/175341]\n",
      "loss: 0.729064  [28800/175341]\n",
      "loss: 1.152503  [30400/175341]\n",
      "loss: 0.344145  [32000/175341]\n",
      "loss: 0.525708  [33600/175341]\n",
      "loss: 0.398269  [35200/175341]\n",
      "loss: 0.154394  [36800/175341]\n",
      "loss: 0.450438  [38400/175341]\n",
      "loss: 0.408687  [40000/175341]\n",
      "loss: 0.338282  [41600/175341]\n",
      "loss: 0.501810  [43200/175341]\n",
      "loss: 0.298825  [44800/175341]\n",
      "loss: 0.356710  [46400/175341]\n",
      "loss: 0.584872  [48000/175341]\n",
      "loss: 0.165298  [49600/175341]\n",
      "loss: 0.544993  [51200/175341]\n",
      "loss: 0.963075  [52800/175341]\n",
      "loss: 0.629017  [54400/175341]\n",
      "loss: 0.668202  [56000/175341]\n",
      "loss: 0.443277  [57600/175341]\n",
      "loss: 0.435258  [59200/175341]\n",
      "loss: 0.373254  [60800/175341]\n",
      "loss: 0.388296  [62400/175341]\n",
      "loss: 0.250647  [64000/175341]\n",
      "loss: 0.377696  [65600/175341]\n",
      "loss: 0.336514  [67200/175341]\n",
      "loss: 0.606084  [68800/175341]\n",
      "loss: 0.248535  [70400/175341]\n",
      "loss: 0.560865  [72000/175341]\n",
      "loss: 0.488814  [73600/175341]\n",
      "loss: 0.803348  [75200/175341]\n",
      "loss: 0.431572  [76800/175341]\n",
      "loss: 0.429591  [78400/175341]\n",
      "loss: 0.222636  [80000/175341]\n",
      "loss: 0.329783  [81600/175341]\n",
      "loss: 0.446330  [83200/175341]\n",
      "loss: 0.550534  [84800/175341]\n",
      "loss: 0.420725  [86400/175341]\n",
      "loss: 0.620748  [88000/175341]\n",
      "loss: 0.486286  [89600/175341]\n",
      "loss: 0.337318  [91200/175341]\n",
      "loss: 0.146454  [92800/175341]\n",
      "loss: 0.785918  [94400/175341]\n",
      "loss: 0.581004  [96000/175341]\n",
      "loss: 0.352620  [97600/175341]\n",
      "loss: 0.246937  [99200/175341]\n",
      "loss: 0.884388  [100800/175341]\n",
      "loss: 0.403543  [102400/175341]\n",
      "loss: 0.492054  [104000/175341]\n",
      "loss: 0.496618  [105600/175341]\n",
      "loss: 0.716341  [107200/175341]\n",
      "loss: 0.562942  [108800/175341]\n",
      "loss: 0.577372  [110400/175341]\n",
      "loss: 0.158324  [112000/175341]\n",
      "loss: 0.787555  [113600/175341]\n",
      "loss: 0.237588  [115200/175341]\n",
      "loss: 0.264968  [116800/175341]\n",
      "loss: 0.651229  [118400/175341]\n",
      "loss: 0.424753  [120000/175341]\n",
      "loss: 0.616670  [121600/175341]\n",
      "loss: 0.377064  [123200/175341]\n",
      "loss: 0.709757  [124800/175341]\n",
      "loss: 0.493509  [126400/175341]\n",
      "loss: 0.456437  [128000/175341]\n",
      "loss: 0.607654  [129600/175341]\n",
      "loss: 0.445148  [131200/175341]\n",
      "loss: 0.499351  [132800/175341]\n",
      "loss: 0.462214  [134400/175341]\n",
      "loss: 0.446521  [136000/175341]\n",
      "loss: 0.808867  [137600/175341]\n",
      "loss: 0.411247  [139200/175341]\n",
      "loss: 0.463755  [140800/175341]\n",
      "loss: 0.530378  [142400/175341]\n",
      "loss: 0.556320  [144000/175341]\n",
      "loss: 0.570359  [145600/175341]\n",
      "loss: 0.375190  [147200/175341]\n",
      "loss: 0.484774  [148800/175341]\n",
      "loss: 0.292367  [150400/175341]\n",
      "loss: 0.256833  [152000/175341]\n",
      "loss: 0.270358  [153600/175341]\n",
      "loss: 0.543532  [155200/175341]\n",
      "loss: 0.470081  [156800/175341]\n",
      "loss: 0.353510  [158400/175341]\n",
      "loss: 0.774178  [160000/175341]\n",
      "loss: 0.614953  [161600/175341]\n",
      "loss: 0.449597  [163200/175341]\n",
      "loss: 0.358908  [164800/175341]\n",
      "loss: 0.200141  [166400/175341]\n",
      "loss: 0.604546  [168000/175341]\n",
      "loss: 0.659088  [169600/175341]\n",
      "loss: 0.211520  [171200/175341]\n",
      "loss: 0.396884  [172800/175341]\n",
      "loss: 0.451525  [174400/175341]\n",
      "Train Accuracy: 81.1847%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.544573, F1-score: 76.14%, Macro_F1-Score:  42.04%  \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.403980  [    0/175341]\n",
      "loss: 0.424734  [ 1600/175341]\n",
      "loss: 0.462644  [ 3200/175341]\n",
      "loss: 0.500378  [ 4800/175341]\n",
      "loss: 0.345741  [ 6400/175341]\n",
      "loss: 0.463283  [ 8000/175341]\n",
      "loss: 0.371242  [ 9600/175341]\n",
      "loss: 0.215742  [11200/175341]\n",
      "loss: 0.639013  [12800/175341]\n",
      "loss: 0.202947  [14400/175341]\n",
      "loss: 0.519571  [16000/175341]\n",
      "loss: 0.477596  [17600/175341]\n",
      "loss: 0.426696  [19200/175341]\n",
      "loss: 0.808689  [20800/175341]\n",
      "loss: 0.460595  [22400/175341]\n",
      "loss: 0.326195  [24000/175341]\n",
      "loss: 0.413218  [25600/175341]\n",
      "loss: 0.313072  [27200/175341]\n",
      "loss: 0.372336  [28800/175341]\n",
      "loss: 0.528687  [30400/175341]\n",
      "loss: 0.504374  [32000/175341]\n",
      "loss: 0.436269  [33600/175341]\n",
      "loss: 0.482105  [35200/175341]\n",
      "loss: 0.311327  [36800/175341]\n",
      "loss: 0.544111  [38400/175341]\n",
      "loss: 0.811972  [40000/175341]\n",
      "loss: 0.529855  [41600/175341]\n",
      "loss: 0.390103  [43200/175341]\n",
      "loss: 0.418675  [44800/175341]\n",
      "loss: 0.418023  [46400/175341]\n",
      "loss: 0.432393  [48000/175341]\n",
      "loss: 0.517289  [49600/175341]\n",
      "loss: 0.142557  [51200/175341]\n",
      "loss: 0.487041  [52800/175341]\n",
      "loss: 0.651740  [54400/175341]\n",
      "loss: 0.521101  [56000/175341]\n",
      "loss: 0.339125  [57600/175341]\n",
      "loss: 0.713863  [59200/175341]\n",
      "loss: 0.402015  [60800/175341]\n",
      "loss: 0.550243  [62400/175341]\n",
      "loss: 0.575211  [64000/175341]\n",
      "loss: 0.175089  [65600/175341]\n",
      "loss: 0.640615  [67200/175341]\n",
      "loss: 0.490383  [68800/175341]\n",
      "loss: 0.440629  [70400/175341]\n",
      "loss: 0.863412  [72000/175341]\n",
      "loss: 0.653067  [73600/175341]\n",
      "loss: 0.479385  [75200/175341]\n",
      "loss: 0.559921  [76800/175341]\n",
      "loss: 0.112929  [78400/175341]\n",
      "loss: 0.159409  [80000/175341]\n",
      "loss: 0.359518  [81600/175341]\n",
      "loss: 0.564682  [83200/175341]\n",
      "loss: 0.796745  [84800/175341]\n",
      "loss: 1.170629  [86400/175341]\n",
      "loss: 0.361974  [88000/175341]\n",
      "loss: 0.472694  [89600/175341]\n",
      "loss: 0.149513  [91200/175341]\n",
      "loss: 0.422482  [92800/175341]\n",
      "loss: 0.548517  [94400/175341]\n",
      "loss: 0.399799  [96000/175341]\n",
      "loss: 0.576726  [97600/175341]\n",
      "loss: 0.136486  [99200/175341]\n",
      "loss: 0.461148  [100800/175341]\n",
      "loss: 0.213052  [102400/175341]\n",
      "loss: 0.511232  [104000/175341]\n",
      "loss: 0.500746  [105600/175341]\n",
      "loss: 0.293366  [107200/175341]\n",
      "loss: 0.080742  [108800/175341]\n",
      "loss: 0.436105  [110400/175341]\n",
      "loss: 0.328276  [112000/175341]\n",
      "loss: 0.291903  [113600/175341]\n",
      "loss: 0.258307  [115200/175341]\n",
      "loss: 0.240325  [116800/175341]\n",
      "loss: 0.438921  [118400/175341]\n",
      "loss: 0.652621  [120000/175341]\n",
      "loss: 0.607472  [121600/175341]\n",
      "loss: 0.585972  [123200/175341]\n",
      "loss: 0.314546  [124800/175341]\n",
      "loss: 0.273275  [126400/175341]\n",
      "loss: 0.463129  [128000/175341]\n",
      "loss: 0.456333  [129600/175341]\n",
      "loss: 0.326229  [131200/175341]\n",
      "loss: 0.624048  [132800/175341]\n",
      "loss: 0.650037  [134400/175341]\n",
      "loss: 0.248828  [136000/175341]\n",
      "loss: 0.492260  [137600/175341]\n",
      "loss: 0.411289  [139200/175341]\n",
      "loss: 0.854516  [140800/175341]\n",
      "loss: 0.420649  [142400/175341]\n",
      "loss: 0.382368  [144000/175341]\n",
      "loss: 0.873468  [145600/175341]\n",
      "loss: 0.646155  [147200/175341]\n",
      "loss: 0.144816  [148800/175341]\n",
      "loss: 0.368460  [150400/175341]\n",
      "loss: 0.991055  [152000/175341]\n",
      "loss: 0.674954  [153600/175341]\n",
      "loss: 0.350656  [155200/175341]\n",
      "loss: 0.872631  [156800/175341]\n",
      "loss: 0.619682  [158400/175341]\n",
      "loss: 0.363749  [160000/175341]\n",
      "loss: 0.637260  [161600/175341]\n",
      "loss: 0.389002  [163200/175341]\n",
      "loss: 0.731660  [164800/175341]\n",
      "loss: 0.253931  [166400/175341]\n",
      "loss: 0.574682  [168000/175341]\n",
      "loss: 0.145184  [169600/175341]\n",
      "loss: 0.651143  [171200/175341]\n",
      "loss: 0.228186  [172800/175341]\n",
      "loss: 0.595215  [174400/175341]\n",
      "Train Accuracy: 81.1887%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.547656, F1-score: 75.80%, Macro_F1-Score:  42.04%  \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.297891  [    0/175341]\n",
      "loss: 0.841919  [ 1600/175341]\n",
      "loss: 0.446069  [ 3200/175341]\n",
      "loss: 0.252577  [ 4800/175341]\n",
      "loss: 0.498786  [ 6400/175341]\n",
      "loss: 0.429320  [ 8000/175341]\n",
      "loss: 0.439511  [ 9600/175341]\n",
      "loss: 0.488133  [11200/175341]\n",
      "loss: 0.285925  [12800/175341]\n",
      "loss: 0.158805  [14400/175341]\n",
      "loss: 0.671478  [16000/175341]\n",
      "loss: 0.564181  [17600/175341]\n",
      "loss: 1.009910  [19200/175341]\n",
      "loss: 0.556278  [20800/175341]\n",
      "loss: 0.506845  [22400/175341]\n",
      "loss: 0.407213  [24000/175341]\n",
      "loss: 0.402840  [25600/175341]\n",
      "loss: 0.859175  [27200/175341]\n",
      "loss: 0.900419  [28800/175341]\n",
      "loss: 0.342440  [30400/175341]\n",
      "loss: 0.262337  [32000/175341]\n",
      "loss: 0.436166  [33600/175341]\n",
      "loss: 0.723248  [35200/175341]\n",
      "loss: 0.415805  [36800/175341]\n",
      "loss: 0.177741  [38400/175341]\n",
      "loss: 0.452072  [40000/175341]\n",
      "loss: 0.396945  [41600/175341]\n",
      "loss: 0.627428  [43200/175341]\n",
      "loss: 0.335238  [44800/175341]\n",
      "loss: 0.589188  [46400/175341]\n",
      "loss: 0.255828  [48000/175341]\n",
      "loss: 0.850983  [49600/175341]\n",
      "loss: 0.217480  [51200/175341]\n",
      "loss: 0.208108  [52800/175341]\n",
      "loss: 0.457311  [54400/175341]\n",
      "loss: 0.955350  [56000/175341]\n",
      "loss: 0.431589  [57600/175341]\n",
      "loss: 0.277463  [59200/175341]\n",
      "loss: 0.580455  [60800/175341]\n",
      "loss: 0.670719  [62400/175341]\n",
      "loss: 0.381024  [64000/175341]\n",
      "loss: 0.252618  [65600/175341]\n",
      "loss: 0.271586  [67200/175341]\n",
      "loss: 0.613150  [68800/175341]\n",
      "loss: 0.686941  [70400/175341]\n",
      "loss: 0.390646  [72000/175341]\n",
      "loss: 0.746656  [73600/175341]\n",
      "loss: 0.244523  [75200/175341]\n",
      "loss: 0.683648  [76800/175341]\n",
      "loss: 0.296488  [78400/175341]\n",
      "loss: 0.685883  [80000/175341]\n",
      "loss: 0.402066  [81600/175341]\n",
      "loss: 0.622496  [83200/175341]\n",
      "loss: 0.159925  [84800/175341]\n",
      "loss: 0.501682  [86400/175341]\n",
      "loss: 0.418341  [88000/175341]\n",
      "loss: 0.844274  [89600/175341]\n",
      "loss: 0.642653  [91200/175341]\n",
      "loss: 0.344500  [92800/175341]\n",
      "loss: 0.241356  [94400/175341]\n",
      "loss: 0.768647  [96000/175341]\n",
      "loss: 0.304118  [97600/175341]\n",
      "loss: 0.513559  [99200/175341]\n",
      "loss: 0.593944  [100800/175341]\n",
      "loss: 0.422745  [102400/175341]\n",
      "loss: 0.879012  [104000/175341]\n",
      "loss: 0.470640  [105600/175341]\n",
      "loss: 0.605529  [107200/175341]\n",
      "loss: 0.686412  [108800/175341]\n",
      "loss: 0.688290  [110400/175341]\n",
      "loss: 0.579799  [112000/175341]\n",
      "loss: 1.409802  [113600/175341]\n",
      "loss: 0.261462  [115200/175341]\n",
      "loss: 0.413956  [116800/175341]\n",
      "loss: 0.280320  [118400/175341]\n",
      "loss: 0.399307  [120000/175341]\n",
      "loss: 0.167746  [121600/175341]\n",
      "loss: 0.491562  [123200/175341]\n",
      "loss: 0.320757  [124800/175341]\n",
      "loss: 0.517508  [126400/175341]\n",
      "loss: 0.676301  [128000/175341]\n",
      "loss: 0.542220  [129600/175341]\n",
      "loss: 0.300113  [131200/175341]\n",
      "loss: 0.770911  [132800/175341]\n",
      "loss: 0.487694  [134400/175341]\n",
      "loss: 0.124525  [136000/175341]\n",
      "loss: 0.953970  [137600/175341]\n",
      "loss: 0.190540  [139200/175341]\n",
      "loss: 0.438485  [140800/175341]\n",
      "loss: 0.379909  [142400/175341]\n",
      "loss: 0.477614  [144000/175341]\n",
      "loss: 0.701685  [145600/175341]\n",
      "loss: 0.481912  [147200/175341]\n",
      "loss: 0.331687  [148800/175341]\n",
      "loss: 0.336318  [150400/175341]\n",
      "loss: 0.435335  [152000/175341]\n",
      "loss: 0.782958  [153600/175341]\n",
      "loss: 0.505864  [155200/175341]\n",
      "loss: 0.239237  [156800/175341]\n",
      "loss: 1.061728  [158400/175341]\n",
      "loss: 0.217781  [160000/175341]\n",
      "loss: 0.309806  [161600/175341]\n",
      "loss: 0.237362  [163200/175341]\n",
      "loss: 0.493742  [164800/175341]\n",
      "loss: 0.370067  [166400/175341]\n",
      "loss: 0.549268  [168000/175341]\n",
      "loss: 0.145830  [169600/175341]\n",
      "loss: 0.536937  [171200/175341]\n",
      "loss: 0.110942  [172800/175341]\n",
      "loss: 0.773723  [174400/175341]\n",
      "Train Accuracy: 81.2155%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.547561, F1-score: 75.91%, Macro_F1-Score:  42.65%  \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.168365  [    0/175341]\n",
      "loss: 0.483903  [ 1600/175341]\n",
      "loss: 0.280972  [ 3200/175341]\n",
      "loss: 0.794086  [ 4800/175341]\n",
      "loss: 0.449313  [ 6400/175341]\n",
      "loss: 0.560727  [ 8000/175341]\n",
      "loss: 0.319170  [ 9600/175341]\n",
      "loss: 0.084420  [11200/175341]\n",
      "loss: 0.206698  [12800/175341]\n",
      "loss: 0.405868  [14400/175341]\n",
      "loss: 0.370968  [16000/175341]\n",
      "loss: 0.513820  [17600/175341]\n",
      "loss: 0.314057  [19200/175341]\n",
      "loss: 0.407625  [20800/175341]\n",
      "loss: 0.622994  [22400/175341]\n",
      "loss: 0.437711  [24000/175341]\n",
      "loss: 0.181845  [25600/175341]\n",
      "loss: 0.379413  [27200/175341]\n",
      "loss: 0.265593  [28800/175341]\n",
      "loss: 0.759553  [30400/175341]\n",
      "loss: 0.411622  [32000/175341]\n",
      "loss: 0.315152  [33600/175341]\n",
      "loss: 0.488876  [35200/175341]\n",
      "loss: 0.609481  [36800/175341]\n",
      "loss: 0.406442  [38400/175341]\n",
      "loss: 0.401617  [40000/175341]\n",
      "loss: 0.663323  [41600/175341]\n",
      "loss: 0.584687  [43200/175341]\n",
      "loss: 0.214566  [44800/175341]\n",
      "loss: 0.198791  [46400/175341]\n",
      "loss: 0.597731  [48000/175341]\n",
      "loss: 0.201801  [49600/175341]\n",
      "loss: 0.377420  [51200/175341]\n",
      "loss: 0.500649  [52800/175341]\n",
      "loss: 0.679019  [54400/175341]\n",
      "loss: 0.365626  [56000/175341]\n",
      "loss: 0.579903  [57600/175341]\n",
      "loss: 0.509345  [59200/175341]\n",
      "loss: 0.354829  [60800/175341]\n",
      "loss: 0.213671  [62400/175341]\n",
      "loss: 0.871606  [64000/175341]\n",
      "loss: 0.271461  [65600/175341]\n",
      "loss: 0.642166  [67200/175341]\n",
      "loss: 0.584294  [68800/175341]\n",
      "loss: 0.863880  [70400/175341]\n",
      "loss: 0.334508  [72000/175341]\n",
      "loss: 0.820148  [73600/175341]\n",
      "loss: 0.248679  [75200/175341]\n",
      "loss: 0.084865  [76800/175341]\n",
      "loss: 0.125894  [78400/175341]\n",
      "loss: 0.277588  [80000/175341]\n",
      "loss: 0.368565  [81600/175341]\n",
      "loss: 0.544123  [83200/175341]\n",
      "loss: 0.527191  [84800/175341]\n",
      "loss: 0.612697  [86400/175341]\n",
      "loss: 0.126639  [88000/175341]\n",
      "loss: 0.381532  [89600/175341]\n",
      "loss: 0.335110  [91200/175341]\n",
      "loss: 0.495405  [92800/175341]\n",
      "loss: 0.472763  [94400/175341]\n",
      "loss: 0.698228  [96000/175341]\n",
      "loss: 0.796406  [97600/175341]\n",
      "loss: 0.699156  [99200/175341]\n",
      "loss: 0.340359  [100800/175341]\n",
      "loss: 0.512750  [102400/175341]\n",
      "loss: 0.468638  [104000/175341]\n",
      "loss: 0.325369  [105600/175341]\n",
      "loss: 0.607091  [107200/175341]\n",
      "loss: 0.489576  [108800/175341]\n",
      "loss: 0.358582  [110400/175341]\n",
      "loss: 0.371963  [112000/175341]\n",
      "loss: 0.372381  [113600/175341]\n",
      "loss: 0.364244  [115200/175341]\n",
      "loss: 0.639023  [116800/175341]\n",
      "loss: 0.300133  [118400/175341]\n",
      "loss: 0.557624  [120000/175341]\n",
      "loss: 0.552953  [121600/175341]\n",
      "loss: 0.611856  [123200/175341]\n",
      "loss: 0.121978  [124800/175341]\n",
      "loss: 0.250683  [126400/175341]\n",
      "loss: 0.295770  [128000/175341]\n",
      "loss: 0.370301  [129600/175341]\n",
      "loss: 0.675562  [131200/175341]\n",
      "loss: 0.537057  [132800/175341]\n",
      "loss: 0.486224  [134400/175341]\n",
      "loss: 0.298144  [136000/175341]\n",
      "loss: 0.447771  [137600/175341]\n",
      "loss: 0.481934  [139200/175341]\n",
      "loss: 0.590529  [140800/175341]\n",
      "loss: 0.640039  [142400/175341]\n",
      "loss: 0.470606  [144000/175341]\n",
      "loss: 0.496443  [145600/175341]\n",
      "loss: 0.301612  [147200/175341]\n",
      "loss: 0.833869  [148800/175341]\n",
      "loss: 0.177740  [150400/175341]\n",
      "loss: 0.654819  [152000/175341]\n",
      "loss: 0.730125  [153600/175341]\n",
      "loss: 0.482508  [155200/175341]\n",
      "loss: 0.582877  [156800/175341]\n",
      "loss: 0.431694  [158400/175341]\n",
      "loss: 0.803156  [160000/175341]\n",
      "loss: 0.516258  [161600/175341]\n",
      "loss: 0.432437  [163200/175341]\n",
      "loss: 0.191322  [164800/175341]\n",
      "loss: 0.882373  [166400/175341]\n",
      "loss: 0.645315  [168000/175341]\n",
      "loss: 0.571472  [169600/175341]\n",
      "loss: 0.358872  [171200/175341]\n",
      "loss: 0.403350  [172800/175341]\n",
      "loss: 0.242576  [174400/175341]\n",
      "Train Accuracy: 81.2177%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.532977, F1-score: 76.03%, Macro_F1-Score:  41.44%  \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.648536  [    0/175341]\n",
      "loss: 0.582651  [ 1600/175341]\n",
      "loss: 0.492751  [ 3200/175341]\n",
      "loss: 0.312277  [ 4800/175341]\n",
      "loss: 0.250261  [ 6400/175341]\n",
      "loss: 0.290393  [ 8000/175341]\n",
      "loss: 0.209819  [ 9600/175341]\n",
      "loss: 0.532728  [11200/175341]\n",
      "loss: 0.810758  [12800/175341]\n",
      "loss: 0.405679  [14400/175341]\n",
      "loss: 0.557790  [16000/175341]\n",
      "loss: 0.373430  [17600/175341]\n",
      "loss: 0.653977  [19200/175341]\n",
      "loss: 0.582735  [20800/175341]\n",
      "loss: 0.426505  [22400/175341]\n",
      "loss: 0.515212  [24000/175341]\n",
      "loss: 0.563617  [25600/175341]\n",
      "loss: 0.345477  [27200/175341]\n",
      "loss: 0.775925  [28800/175341]\n",
      "loss: 0.417816  [30400/175341]\n",
      "loss: 0.167241  [32000/175341]\n",
      "loss: 0.094731  [33600/175341]\n",
      "loss: 0.385345  [35200/175341]\n",
      "loss: 0.307855  [36800/175341]\n",
      "loss: 0.602500  [38400/175341]\n",
      "loss: 0.515149  [40000/175341]\n",
      "loss: 0.616895  [41600/175341]\n",
      "loss: 0.198794  [43200/175341]\n",
      "loss: 0.656108  [44800/175341]\n",
      "loss: 0.402673  [46400/175341]\n",
      "loss: 0.042273  [48000/175341]\n",
      "loss: 0.572524  [49600/175341]\n",
      "loss: 0.543150  [51200/175341]\n",
      "loss: 0.560915  [52800/175341]\n",
      "loss: 0.646789  [54400/175341]\n",
      "loss: 0.369034  [56000/175341]\n",
      "loss: 0.340938  [57600/175341]\n",
      "loss: 0.639550  [59200/175341]\n",
      "loss: 0.295950  [60800/175341]\n",
      "loss: 0.233022  [62400/175341]\n",
      "loss: 0.653681  [64000/175341]\n",
      "loss: 0.287827  [65600/175341]\n",
      "loss: 0.948342  [67200/175341]\n",
      "loss: 0.290073  [68800/175341]\n",
      "loss: 0.754477  [70400/175341]\n",
      "loss: 0.484551  [72000/175341]\n",
      "loss: 0.807620  [73600/175341]\n",
      "loss: 0.492803  [75200/175341]\n",
      "loss: 0.525954  [76800/175341]\n",
      "loss: 0.492861  [78400/175341]\n",
      "loss: 0.225240  [80000/175341]\n",
      "loss: 0.538820  [81600/175341]\n",
      "loss: 0.951002  [83200/175341]\n",
      "loss: 0.041680  [84800/175341]\n",
      "loss: 0.973280  [86400/175341]\n",
      "loss: 0.378082  [88000/175341]\n",
      "loss: 0.447096  [89600/175341]\n",
      "loss: 0.823327  [91200/175341]\n",
      "loss: 0.755226  [92800/175341]\n",
      "loss: 0.368131  [94400/175341]\n",
      "loss: 1.033585  [96000/175341]\n",
      "loss: 0.479952  [97600/175341]\n",
      "loss: 0.194221  [99200/175341]\n",
      "loss: 0.204689  [100800/175341]\n",
      "loss: 0.458519  [102400/175341]\n",
      "loss: 0.361427  [104000/175341]\n",
      "loss: 0.582063  [105600/175341]\n",
      "loss: 0.491722  [107200/175341]\n",
      "loss: 0.908215  [108800/175341]\n",
      "loss: 0.611336  [110400/175341]\n",
      "loss: 0.359783  [112000/175341]\n",
      "loss: 0.315613  [113600/175341]\n",
      "loss: 0.299801  [115200/175341]\n",
      "loss: 0.451755  [116800/175341]\n",
      "loss: 0.281215  [118400/175341]\n",
      "loss: 0.384050  [120000/175341]\n",
      "loss: 0.403154  [121600/175341]\n",
      "loss: 0.748221  [123200/175341]\n",
      "loss: 0.694682  [124800/175341]\n",
      "loss: 0.650221  [126400/175341]\n",
      "loss: 0.688745  [128000/175341]\n",
      "loss: 0.178331  [129600/175341]\n",
      "loss: 0.360204  [131200/175341]\n",
      "loss: 0.280483  [132800/175341]\n",
      "loss: 0.456576  [134400/175341]\n",
      "loss: 0.403439  [136000/175341]\n",
      "loss: 0.408996  [137600/175341]\n",
      "loss: 0.467632  [139200/175341]\n",
      "loss: 0.737781  [140800/175341]\n",
      "loss: 0.654422  [142400/175341]\n",
      "loss: 0.462659  [144000/175341]\n",
      "loss: 0.678480  [145600/175341]\n",
      "loss: 0.510443  [147200/175341]\n",
      "loss: 0.372169  [148800/175341]\n",
      "loss: 0.278333  [150400/175341]\n",
      "loss: 0.200424  [152000/175341]\n",
      "loss: 0.473546  [153600/175341]\n",
      "loss: 0.445560  [155200/175341]\n",
      "loss: 0.589551  [156800/175341]\n",
      "loss: 0.446650  [158400/175341]\n",
      "loss: 0.257130  [160000/175341]\n",
      "loss: 0.152764  [161600/175341]\n",
      "loss: 0.830823  [163200/175341]\n",
      "loss: 0.354227  [164800/175341]\n",
      "loss: 0.415538  [166400/175341]\n",
      "loss: 0.359727  [168000/175341]\n",
      "loss: 0.308566  [169600/175341]\n",
      "loss: 0.369186  [171200/175341]\n",
      "loss: 0.363216  [172800/175341]\n",
      "loss: 0.499798  [174400/175341]\n",
      "Train Accuracy: 81.1573%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.539252, F1-score: 75.84%, Macro_F1-Score:  42.38%  \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.577773  [    0/175341]\n",
      "loss: 0.515742  [ 1600/175341]\n",
      "loss: 0.703146  [ 3200/175341]\n",
      "loss: 0.371137  [ 4800/175341]\n",
      "loss: 0.544165  [ 6400/175341]\n",
      "loss: 0.297511  [ 8000/175341]\n",
      "loss: 0.781720  [ 9600/175341]\n",
      "loss: 0.396297  [11200/175341]\n",
      "loss: 0.556175  [12800/175341]\n",
      "loss: 0.494483  [14400/175341]\n",
      "loss: 0.284206  [16000/175341]\n",
      "loss: 0.606341  [17600/175341]\n",
      "loss: 0.209243  [19200/175341]\n",
      "loss: 0.542526  [20800/175341]\n",
      "loss: 0.418883  [22400/175341]\n",
      "loss: 0.153714  [24000/175341]\n",
      "loss: 0.664130  [25600/175341]\n",
      "loss: 0.305963  [27200/175341]\n",
      "loss: 0.730804  [28800/175341]\n",
      "loss: 0.247778  [30400/175341]\n",
      "loss: 0.466756  [32000/175341]\n",
      "loss: 0.358799  [33600/175341]\n",
      "loss: 0.305047  [35200/175341]\n",
      "loss: 0.793135  [36800/175341]\n",
      "loss: 0.452077  [38400/175341]\n",
      "loss: 0.141191  [40000/175341]\n",
      "loss: 0.490593  [41600/175341]\n",
      "loss: 0.434524  [43200/175341]\n",
      "loss: 0.187036  [44800/175341]\n",
      "loss: 0.683842  [46400/175341]\n",
      "loss: 0.623219  [48000/175341]\n",
      "loss: 0.514837  [49600/175341]\n",
      "loss: 0.211703  [51200/175341]\n",
      "loss: 1.112734  [52800/175341]\n",
      "loss: 0.297817  [54400/175341]\n",
      "loss: 0.724709  [56000/175341]\n",
      "loss: 0.422333  [57600/175341]\n",
      "loss: 0.619867  [59200/175341]\n",
      "loss: 0.850759  [60800/175341]\n",
      "loss: 0.492092  [62400/175341]\n",
      "loss: 0.416876  [64000/175341]\n",
      "loss: 0.491179  [65600/175341]\n",
      "loss: 0.348076  [67200/175341]\n",
      "loss: 0.361666  [68800/175341]\n",
      "loss: 0.187586  [70400/175341]\n",
      "loss: 0.336229  [72000/175341]\n",
      "loss: 0.462247  [73600/175341]\n",
      "loss: 0.600094  [75200/175341]\n",
      "loss: 0.530538  [76800/175341]\n",
      "loss: 0.457576  [78400/175341]\n",
      "loss: 0.811691  [80000/175341]\n",
      "loss: 0.367895  [81600/175341]\n",
      "loss: 0.251202  [83200/175341]\n",
      "loss: 0.692278  [84800/175341]\n",
      "loss: 0.668656  [86400/175341]\n",
      "loss: 0.144874  [88000/175341]\n",
      "loss: 0.505177  [89600/175341]\n",
      "loss: 0.435135  [91200/175341]\n",
      "loss: 0.473643  [92800/175341]\n",
      "loss: 0.691694  [94400/175341]\n",
      "loss: 0.503592  [96000/175341]\n",
      "loss: 0.401893  [97600/175341]\n",
      "loss: 0.298532  [99200/175341]\n",
      "loss: 0.602956  [100800/175341]\n",
      "loss: 0.694473  [102400/175341]\n",
      "loss: 0.236551  [104000/175341]\n",
      "loss: 0.631970  [105600/175341]\n",
      "loss: 0.853027  [107200/175341]\n",
      "loss: 0.503558  [108800/175341]\n",
      "loss: 0.619801  [110400/175341]\n",
      "loss: 0.723854  [112000/175341]\n",
      "loss: 0.650614  [113600/175341]\n",
      "loss: 0.597706  [115200/175341]\n",
      "loss: 0.523568  [116800/175341]\n",
      "loss: 0.358235  [118400/175341]\n",
      "loss: 0.412430  [120000/175341]\n",
      "loss: 0.805238  [121600/175341]\n",
      "loss: 0.287466  [123200/175341]\n",
      "loss: 0.345092  [124800/175341]\n",
      "loss: 0.597407  [126400/175341]\n",
      "loss: 0.471275  [128000/175341]\n",
      "loss: 0.475518  [129600/175341]\n",
      "loss: 0.631310  [131200/175341]\n",
      "loss: 0.425272  [132800/175341]\n",
      "loss: 0.531135  [134400/175341]\n",
      "loss: 0.245511  [136000/175341]\n",
      "loss: 0.428139  [137600/175341]\n",
      "loss: 0.743096  [139200/175341]\n",
      "loss: 0.722502  [140800/175341]\n",
      "loss: 0.475940  [142400/175341]\n",
      "loss: 0.583301  [144000/175341]\n",
      "loss: 0.320125  [145600/175341]\n",
      "loss: 0.632422  [147200/175341]\n",
      "loss: 0.488883  [148800/175341]\n",
      "loss: 0.588841  [150400/175341]\n",
      "loss: 0.189054  [152000/175341]\n",
      "loss: 0.888031  [153600/175341]\n",
      "loss: 0.850601  [155200/175341]\n",
      "loss: 0.181158  [156800/175341]\n",
      "loss: 0.228978  [158400/175341]\n",
      "loss: 0.389020  [160000/175341]\n",
      "loss: 0.183176  [161600/175341]\n",
      "loss: 0.376724  [163200/175341]\n",
      "loss: 0.706311  [164800/175341]\n",
      "loss: 0.427646  [166400/175341]\n",
      "loss: 0.680934  [168000/175341]\n",
      "loss: 0.305774  [169600/175341]\n",
      "loss: 0.519185  [171200/175341]\n",
      "loss: 0.448012  [172800/175341]\n",
      "loss: 0.312781  [174400/175341]\n",
      "Train Accuracy: 81.2360%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.529063, F1-score: 76.26%, Macro_F1-Score:  42.06%  \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.419092  [    0/175341]\n",
      "loss: 0.088102  [ 1600/175341]\n",
      "loss: 0.375141  [ 3200/175341]\n",
      "loss: 0.178493  [ 4800/175341]\n",
      "loss: 0.418530  [ 6400/175341]\n",
      "loss: 0.562378  [ 8000/175341]\n",
      "loss: 0.367790  [ 9600/175341]\n",
      "loss: 0.324369  [11200/175341]\n",
      "loss: 0.891983  [12800/175341]\n",
      "loss: 0.108593  [14400/175341]\n",
      "loss: 0.813818  [16000/175341]\n",
      "loss: 0.321336  [17600/175341]\n",
      "loss: 0.462699  [19200/175341]\n",
      "loss: 0.259321  [20800/175341]\n",
      "loss: 0.622433  [22400/175341]\n",
      "loss: 0.815267  [24000/175341]\n",
      "loss: 0.508503  [25600/175341]\n",
      "loss: 0.231725  [27200/175341]\n",
      "loss: 0.293325  [28800/175341]\n",
      "loss: 0.732499  [30400/175341]\n",
      "loss: 0.621554  [32000/175341]\n",
      "loss: 0.621097  [33600/175341]\n",
      "loss: 0.618387  [35200/175341]\n",
      "loss: 0.515040  [36800/175341]\n",
      "loss: 0.415383  [38400/175341]\n",
      "loss: 0.072986  [40000/175341]\n",
      "loss: 0.645211  [41600/175341]\n",
      "loss: 0.154107  [43200/175341]\n",
      "loss: 0.273232  [44800/175341]\n",
      "loss: 0.392165  [46400/175341]\n",
      "loss: 0.286425  [48000/175341]\n",
      "loss: 0.579715  [49600/175341]\n",
      "loss: 0.179925  [51200/175341]\n",
      "loss: 0.177227  [52800/175341]\n",
      "loss: 0.466817  [54400/175341]\n",
      "loss: 0.331536  [56000/175341]\n",
      "loss: 0.084019  [57600/175341]\n",
      "loss: 0.965785  [59200/175341]\n",
      "loss: 0.661553  [60800/175341]\n",
      "loss: 0.874056  [62400/175341]\n",
      "loss: 0.202378  [64000/175341]\n",
      "loss: 0.532590  [65600/175341]\n",
      "loss: 0.868239  [67200/175341]\n",
      "loss: 0.342469  [68800/175341]\n",
      "loss: 0.522711  [70400/175341]\n",
      "loss: 0.445171  [72000/175341]\n",
      "loss: 0.468024  [73600/175341]\n",
      "loss: 0.769581  [75200/175341]\n",
      "loss: 0.321780  [76800/175341]\n",
      "loss: 0.670298  [78400/175341]\n",
      "loss: 0.699331  [80000/175341]\n",
      "loss: 0.393582  [81600/175341]\n",
      "loss: 0.291152  [83200/175341]\n",
      "loss: 0.424467  [84800/175341]\n",
      "loss: 0.526378  [86400/175341]\n",
      "loss: 0.392780  [88000/175341]\n",
      "loss: 0.179774  [89600/175341]\n",
      "loss: 0.169571  [91200/175341]\n",
      "loss: 0.433134  [92800/175341]\n",
      "loss: 0.524512  [94400/175341]\n",
      "loss: 0.184646  [96000/175341]\n",
      "loss: 0.232840  [97600/175341]\n",
      "loss: 0.432838  [99200/175341]\n",
      "loss: 0.430140  [100800/175341]\n",
      "loss: 0.354678  [102400/175341]\n",
      "loss: 0.265365  [104000/175341]\n",
      "loss: 0.108463  [105600/175341]\n",
      "loss: 0.579362  [107200/175341]\n",
      "loss: 0.606076  [108800/175341]\n",
      "loss: 0.599495  [110400/175341]\n",
      "loss: 0.373559  [112000/175341]\n",
      "loss: 0.663812  [113600/175341]\n",
      "loss: 0.703196  [115200/175341]\n",
      "loss: 0.694135  [116800/175341]\n",
      "loss: 0.503640  [118400/175341]\n",
      "loss: 0.327309  [120000/175341]\n",
      "loss: 0.059278  [121600/175341]\n",
      "loss: 0.330606  [123200/175341]\n",
      "loss: 0.287066  [124800/175341]\n",
      "loss: 0.589903  [126400/175341]\n",
      "loss: 0.333288  [128000/175341]\n",
      "loss: 0.360014  [129600/175341]\n",
      "loss: 0.599108  [131200/175341]\n",
      "loss: 0.646952  [132800/175341]\n",
      "loss: 0.437837  [134400/175341]\n",
      "loss: 0.608294  [136000/175341]\n",
      "loss: 0.655916  [137600/175341]\n",
      "loss: 0.984231  [139200/175341]\n",
      "loss: 0.793208  [140800/175341]\n",
      "loss: 0.907938  [142400/175341]\n",
      "loss: 0.564553  [144000/175341]\n",
      "loss: 0.320762  [145600/175341]\n",
      "loss: 0.925548  [147200/175341]\n",
      "loss: 0.519718  [148800/175341]\n",
      "loss: 0.385439  [150400/175341]\n",
      "loss: 0.558701  [152000/175341]\n",
      "loss: 0.587708  [153600/175341]\n",
      "loss: 0.917453  [155200/175341]\n",
      "loss: 0.331523  [156800/175341]\n",
      "loss: 0.658489  [158400/175341]\n",
      "loss: 0.503511  [160000/175341]\n",
      "loss: 0.450251  [161600/175341]\n",
      "loss: 0.317055  [163200/175341]\n",
      "loss: 0.374090  [164800/175341]\n",
      "loss: 0.406659  [166400/175341]\n",
      "loss: 0.434289  [168000/175341]\n",
      "loss: 0.125198  [169600/175341]\n",
      "loss: 0.662844  [171200/175341]\n",
      "loss: 0.314752  [172800/175341]\n",
      "loss: 0.426477  [174400/175341]\n",
      "Train Accuracy: 81.1869%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.558690, F1-score: 75.21%, Macro_F1-Score:  41.55%  \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.517823  [    0/175341]\n",
      "loss: 0.377682  [ 1600/175341]\n",
      "loss: 0.552222  [ 3200/175341]\n",
      "loss: 0.438252  [ 4800/175341]\n",
      "loss: 0.405144  [ 6400/175341]\n",
      "loss: 0.286682  [ 8000/175341]\n",
      "loss: 0.337180  [ 9600/175341]\n",
      "loss: 0.071098  [11200/175341]\n",
      "loss: 0.448961  [12800/175341]\n",
      "loss: 0.759578  [14400/175341]\n",
      "loss: 0.397466  [16000/175341]\n",
      "loss: 0.361476  [17600/175341]\n",
      "loss: 0.317309  [19200/175341]\n",
      "loss: 0.404720  [20800/175341]\n",
      "loss: 0.555889  [22400/175341]\n",
      "loss: 0.468519  [24000/175341]\n",
      "loss: 0.103542  [25600/175341]\n",
      "loss: 0.256327  [27200/175341]\n",
      "loss: 0.406046  [28800/175341]\n",
      "loss: 0.404160  [30400/175341]\n",
      "loss: 0.171150  [32000/175341]\n",
      "loss: 0.321230  [33600/175341]\n",
      "loss: 0.202106  [35200/175341]\n",
      "loss: 0.508368  [36800/175341]\n",
      "loss: 0.447249  [38400/175341]\n",
      "loss: 0.325147  [40000/175341]\n",
      "loss: 0.533489  [41600/175341]\n",
      "loss: 0.459300  [43200/175341]\n",
      "loss: 0.405956  [44800/175341]\n",
      "loss: 1.020110  [46400/175341]\n",
      "loss: 0.526402  [48000/175341]\n",
      "loss: 0.867590  [49600/175341]\n",
      "loss: 0.715972  [51200/175341]\n",
      "loss: 0.881030  [52800/175341]\n",
      "loss: 0.234062  [54400/175341]\n",
      "loss: 0.343703  [56000/175341]\n",
      "loss: 0.777226  [57600/175341]\n",
      "loss: 0.330096  [59200/175341]\n",
      "loss: 0.347280  [60800/175341]\n",
      "loss: 0.616913  [62400/175341]\n",
      "loss: 0.073971  [64000/175341]\n",
      "loss: 0.405320  [65600/175341]\n",
      "loss: 0.677342  [67200/175341]\n",
      "loss: 0.791500  [68800/175341]\n",
      "loss: 0.360670  [70400/175341]\n",
      "loss: 0.467758  [72000/175341]\n",
      "loss: 0.386113  [73600/175341]\n",
      "loss: 0.433020  [75200/175341]\n",
      "loss: 0.890905  [76800/175341]\n",
      "loss: 0.556391  [78400/175341]\n",
      "loss: 0.593505  [80000/175341]\n",
      "loss: 0.361863  [81600/175341]\n",
      "loss: 0.681027  [83200/175341]\n",
      "loss: 0.325294  [84800/175341]\n",
      "loss: 0.504315  [86400/175341]\n",
      "loss: 0.818530  [88000/175341]\n",
      "loss: 0.253917  [89600/175341]\n",
      "loss: 0.626307  [91200/175341]\n",
      "loss: 0.153463  [92800/175341]\n",
      "loss: 0.647793  [94400/175341]\n",
      "loss: 0.461051  [96000/175341]\n",
      "loss: 0.370254  [97600/175341]\n",
      "loss: 0.378928  [99200/175341]\n",
      "loss: 0.524048  [100800/175341]\n",
      "loss: 0.452741  [102400/175341]\n",
      "loss: 0.528541  [104000/175341]\n",
      "loss: 0.322214  [105600/175341]\n",
      "loss: 0.308399  [107200/175341]\n",
      "loss: 0.516053  [108800/175341]\n",
      "loss: 0.428104  [110400/175341]\n",
      "loss: 0.727563  [112000/175341]\n",
      "loss: 0.445720  [113600/175341]\n",
      "loss: 0.433489  [115200/175341]\n",
      "loss: 0.294094  [116800/175341]\n",
      "loss: 0.660529  [118400/175341]\n",
      "loss: 0.561734  [120000/175341]\n",
      "loss: 0.531266  [121600/175341]\n",
      "loss: 0.489581  [123200/175341]\n",
      "loss: 0.262144  [124800/175341]\n",
      "loss: 0.241451  [126400/175341]\n",
      "loss: 0.228258  [128000/175341]\n",
      "loss: 0.473774  [129600/175341]\n",
      "loss: 0.482018  [131200/175341]\n",
      "loss: 0.663225  [132800/175341]\n",
      "loss: 0.388955  [134400/175341]\n",
      "loss: 0.425764  [136000/175341]\n",
      "loss: 0.409880  [137600/175341]\n",
      "loss: 0.495714  [139200/175341]\n",
      "loss: 0.125747  [140800/175341]\n",
      "loss: 0.380506  [142400/175341]\n",
      "loss: 0.560897  [144000/175341]\n",
      "loss: 0.278461  [145600/175341]\n",
      "loss: 0.395977  [147200/175341]\n",
      "loss: 0.675934  [148800/175341]\n",
      "loss: 0.158330  [150400/175341]\n",
      "loss: 0.793163  [152000/175341]\n",
      "loss: 0.200098  [153600/175341]\n",
      "loss: 0.331534  [155200/175341]\n",
      "loss: 0.716568  [156800/175341]\n",
      "loss: 0.671057  [158400/175341]\n",
      "loss: 0.387244  [160000/175341]\n",
      "loss: 0.441766  [161600/175341]\n",
      "loss: 0.434989  [163200/175341]\n",
      "loss: 0.579560  [164800/175341]\n",
      "loss: 0.289886  [166400/175341]\n",
      "loss: 0.755645  [168000/175341]\n",
      "loss: 0.242442  [169600/175341]\n",
      "loss: 0.181392  [171200/175341]\n",
      "loss: 0.629392  [172800/175341]\n",
      "loss: 0.348980  [174400/175341]\n",
      "Train Accuracy: 81.2075%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.543343, F1-score: 76.11%, Macro_F1-Score:  42.57%  \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.492599  [    0/175341]\n",
      "loss: 0.369687  [ 1600/175341]\n",
      "loss: 0.347648  [ 3200/175341]\n",
      "loss: 0.362740  [ 4800/175341]\n",
      "loss: 0.381908  [ 6400/175341]\n",
      "loss: 0.106516  [ 8000/175341]\n",
      "loss: 0.537816  [ 9600/175341]\n",
      "loss: 0.685006  [11200/175341]\n",
      "loss: 0.620659  [12800/175341]\n",
      "loss: 0.707277  [14400/175341]\n",
      "loss: 0.354128  [16000/175341]\n",
      "loss: 0.537718  [17600/175341]\n",
      "loss: 0.191191  [19200/175341]\n",
      "loss: 0.515017  [20800/175341]\n",
      "loss: 0.648552  [22400/175341]\n",
      "loss: 0.387614  [24000/175341]\n",
      "loss: 0.736044  [25600/175341]\n",
      "loss: 0.323665  [27200/175341]\n",
      "loss: 0.612925  [28800/175341]\n",
      "loss: 0.713877  [30400/175341]\n",
      "loss: 0.494017  [32000/175341]\n",
      "loss: 0.470217  [33600/175341]\n",
      "loss: 0.570918  [35200/175341]\n",
      "loss: 0.375103  [36800/175341]\n",
      "loss: 0.253929  [38400/175341]\n",
      "loss: 0.345811  [40000/175341]\n",
      "loss: 0.102915  [41600/175341]\n",
      "loss: 0.404555  [43200/175341]\n",
      "loss: 0.256178  [44800/175341]\n",
      "loss: 0.803479  [46400/175341]\n",
      "loss: 0.598480  [48000/175341]\n",
      "loss: 0.280717  [49600/175341]\n",
      "loss: 0.499593  [51200/175341]\n",
      "loss: 0.700194  [52800/175341]\n",
      "loss: 0.589441  [54400/175341]\n",
      "loss: 0.902252  [56000/175341]\n",
      "loss: 0.675796  [57600/175341]\n",
      "loss: 0.134154  [59200/175341]\n",
      "loss: 0.354781  [60800/175341]\n",
      "loss: 0.295168  [62400/175341]\n",
      "loss: 0.466316  [64000/175341]\n",
      "loss: 0.493794  [65600/175341]\n",
      "loss: 0.137237  [67200/175341]\n",
      "loss: 0.337191  [68800/175341]\n",
      "loss: 0.259890  [70400/175341]\n",
      "loss: 0.469775  [72000/175341]\n",
      "loss: 1.186085  [73600/175341]\n",
      "loss: 0.688690  [75200/175341]\n",
      "loss: 0.488655  [76800/175341]\n",
      "loss: 0.216539  [78400/175341]\n",
      "loss: 0.453985  [80000/175341]\n",
      "loss: 0.273179  [81600/175341]\n",
      "loss: 0.489618  [83200/175341]\n",
      "loss: 0.278532  [84800/175341]\n",
      "loss: 0.276118  [86400/175341]\n",
      "loss: 0.676649  [88000/175341]\n",
      "loss: 0.675810  [89600/175341]\n",
      "loss: 0.229575  [91200/175341]\n",
      "loss: 0.173255  [92800/175341]\n",
      "loss: 0.521113  [94400/175341]\n",
      "loss: 0.387572  [96000/175341]\n",
      "loss: 0.821264  [97600/175341]\n",
      "loss: 1.010393  [99200/175341]\n",
      "loss: 0.141329  [100800/175341]\n",
      "loss: 0.570671  [102400/175341]\n",
      "loss: 0.148726  [104000/175341]\n",
      "loss: 0.480784  [105600/175341]\n",
      "loss: 0.682081  [107200/175341]\n",
      "loss: 0.405894  [108800/175341]\n",
      "loss: 0.417777  [110400/175341]\n",
      "loss: 0.712521  [112000/175341]\n",
      "loss: 0.619303  [113600/175341]\n",
      "loss: 0.432972  [115200/175341]\n",
      "loss: 0.663012  [116800/175341]\n",
      "loss: 0.846214  [118400/175341]\n",
      "loss: 0.637457  [120000/175341]\n",
      "loss: 0.610367  [121600/175341]\n",
      "loss: 0.290081  [123200/175341]\n",
      "loss: 0.239608  [124800/175341]\n",
      "loss: 0.655745  [126400/175341]\n",
      "loss: 0.396309  [128000/175341]\n",
      "loss: 0.684551  [129600/175341]\n",
      "loss: 0.622728  [131200/175341]\n",
      "loss: 0.403177  [132800/175341]\n",
      "loss: 0.595835  [134400/175341]\n",
      "loss: 1.188810  [136000/175341]\n",
      "loss: 0.337426  [137600/175341]\n",
      "loss: 0.249867  [139200/175341]\n",
      "loss: 0.774821  [140800/175341]\n",
      "loss: 0.329365  [142400/175341]\n",
      "loss: 0.120741  [144000/175341]\n",
      "loss: 0.133019  [145600/175341]\n",
      "loss: 0.501663  [147200/175341]\n",
      "loss: 0.275766  [148800/175341]\n",
      "loss: 0.502994  [150400/175341]\n",
      "loss: 0.183900  [152000/175341]\n",
      "loss: 0.664378  [153600/175341]\n",
      "loss: 0.665245  [155200/175341]\n",
      "loss: 0.626251  [156800/175341]\n",
      "loss: 0.459118  [158400/175341]\n",
      "loss: 0.422672  [160000/175341]\n",
      "loss: 0.785549  [161600/175341]\n",
      "loss: 0.740410  [163200/175341]\n",
      "loss: 0.798283  [164800/175341]\n",
      "loss: 0.484813  [166400/175341]\n",
      "loss: 0.104280  [168000/175341]\n",
      "loss: 0.664930  [169600/175341]\n",
      "loss: 0.597759  [171200/175341]\n",
      "loss: 0.488884  [172800/175341]\n",
      "loss: 0.913127  [174400/175341]\n",
      "Train Accuracy: 81.1778%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.536344, F1-score: 76.31%, Macro_F1-Score:  42.52%  \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.913325  [    0/175341]\n",
      "loss: 0.605702  [ 1600/175341]\n",
      "loss: 0.116027  [ 3200/175341]\n",
      "loss: 0.334101  [ 4800/175341]\n",
      "loss: 0.412694  [ 6400/175341]\n",
      "loss: 0.078952  [ 8000/175341]\n",
      "loss: 0.890661  [ 9600/175341]\n",
      "loss: 0.453424  [11200/175341]\n",
      "loss: 0.422948  [12800/175341]\n",
      "loss: 0.592434  [14400/175341]\n",
      "loss: 0.270600  [16000/175341]\n",
      "loss: 0.431544  [17600/175341]\n",
      "loss: 0.242155  [19200/175341]\n",
      "loss: 0.367918  [20800/175341]\n",
      "loss: 0.610312  [22400/175341]\n",
      "loss: 0.332611  [24000/175341]\n",
      "loss: 0.396829  [25600/175341]\n",
      "loss: 0.279358  [27200/175341]\n",
      "loss: 0.513956  [28800/175341]\n",
      "loss: 0.524025  [30400/175341]\n",
      "loss: 0.991356  [32000/175341]\n",
      "loss: 0.148611  [33600/175341]\n",
      "loss: 0.657576  [35200/175341]\n",
      "loss: 0.160158  [36800/175341]\n",
      "loss: 0.104742  [38400/175341]\n",
      "loss: 0.368645  [40000/175341]\n",
      "loss: 0.625939  [41600/175341]\n",
      "loss: 0.606094  [43200/175341]\n",
      "loss: 0.782285  [44800/175341]\n",
      "loss: 0.525556  [46400/175341]\n",
      "loss: 0.629959  [48000/175341]\n",
      "loss: 0.606469  [49600/175341]\n",
      "loss: 0.245524  [51200/175341]\n",
      "loss: 0.343172  [52800/175341]\n",
      "loss: 0.405856  [54400/175341]\n",
      "loss: 0.388953  [56000/175341]\n",
      "loss: 0.484144  [57600/175341]\n",
      "loss: 0.294836  [59200/175341]\n",
      "loss: 0.720849  [60800/175341]\n",
      "loss: 0.411512  [62400/175341]\n",
      "loss: 0.339340  [64000/175341]\n",
      "loss: 0.433772  [65600/175341]\n",
      "loss: 0.402063  [67200/175341]\n",
      "loss: 0.637338  [68800/175341]\n",
      "loss: 0.496474  [70400/175341]\n",
      "loss: 0.644910  [72000/175341]\n",
      "loss: 0.368699  [73600/175341]\n",
      "loss: 0.380655  [75200/175341]\n",
      "loss: 0.777586  [76800/175341]\n",
      "loss: 0.642088  [78400/175341]\n",
      "loss: 0.524528  [80000/175341]\n",
      "loss: 0.433199  [81600/175341]\n",
      "loss: 0.637625  [83200/175341]\n",
      "loss: 0.367866  [84800/175341]\n",
      "loss: 0.514952  [86400/175341]\n",
      "loss: 0.478894  [88000/175341]\n",
      "loss: 0.617899  [89600/175341]\n",
      "loss: 0.247697  [91200/175341]\n",
      "loss: 0.647150  [92800/175341]\n",
      "loss: 0.429173  [94400/175341]\n",
      "loss: 0.710541  [96000/175341]\n",
      "loss: 0.556431  [97600/175341]\n",
      "loss: 0.350673  [99200/175341]\n",
      "loss: 0.401440  [100800/175341]\n",
      "loss: 0.395313  [102400/175341]\n",
      "loss: 0.422546  [104000/175341]\n",
      "loss: 0.355870  [105600/175341]\n",
      "loss: 0.534145  [107200/175341]\n",
      "loss: 0.168963  [108800/175341]\n",
      "loss: 0.555071  [110400/175341]\n",
      "loss: 0.322323  [112000/175341]\n",
      "loss: 0.553927  [113600/175341]\n",
      "loss: 0.890827  [115200/175341]\n",
      "loss: 0.593821  [116800/175341]\n",
      "loss: 0.458106  [118400/175341]\n",
      "loss: 0.502418  [120000/175341]\n",
      "loss: 0.624387  [121600/175341]\n",
      "loss: 0.789558  [123200/175341]\n",
      "loss: 0.213437  [124800/175341]\n",
      "loss: 0.341751  [126400/175341]\n",
      "loss: 0.672424  [128000/175341]\n",
      "loss: 0.334249  [129600/175341]\n",
      "loss: 0.513318  [131200/175341]\n",
      "loss: 0.378900  [132800/175341]\n",
      "loss: 0.255182  [134400/175341]\n",
      "loss: 0.418016  [136000/175341]\n",
      "loss: 0.311156  [137600/175341]\n",
      "loss: 0.571913  [139200/175341]\n",
      "loss: 0.309477  [140800/175341]\n",
      "loss: 0.237539  [142400/175341]\n",
      "loss: 0.126257  [144000/175341]\n",
      "loss: 0.552531  [145600/175341]\n",
      "loss: 0.505988  [147200/175341]\n",
      "loss: 0.804075  [148800/175341]\n",
      "loss: 0.503297  [150400/175341]\n",
      "loss: 0.674824  [152000/175341]\n",
      "loss: 0.550028  [153600/175341]\n",
      "loss: 0.929804  [155200/175341]\n",
      "loss: 0.333344  [156800/175341]\n",
      "loss: 0.535321  [158400/175341]\n",
      "loss: 0.504370  [160000/175341]\n",
      "loss: 0.593938  [161600/175341]\n",
      "loss: 0.725980  [163200/175341]\n",
      "loss: 0.344006  [164800/175341]\n",
      "loss: 0.631432  [166400/175341]\n",
      "loss: 0.438352  [168000/175341]\n",
      "loss: 0.694215  [169600/175341]\n",
      "loss: 0.306136  [171200/175341]\n",
      "loss: 0.830465  [172800/175341]\n",
      "loss: 0.462943  [174400/175341]\n",
      "Train Accuracy: 81.1904%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.558510, F1-score: 75.53%, Macro_F1-Score:  42.78%  \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.356272  [    0/175341]\n",
      "loss: 0.083996  [ 1600/175341]\n",
      "loss: 0.211538  [ 3200/175341]\n",
      "loss: 0.288859  [ 4800/175341]\n",
      "loss: 0.469313  [ 6400/175341]\n",
      "loss: 0.395438  [ 8000/175341]\n",
      "loss: 0.462780  [ 9600/175341]\n",
      "loss: 0.645683  [11200/175341]\n",
      "loss: 0.294456  [12800/175341]\n",
      "loss: 0.571161  [14400/175341]\n",
      "loss: 0.407555  [16000/175341]\n",
      "loss: 0.487161  [17600/175341]\n",
      "loss: 0.393128  [19200/175341]\n",
      "loss: 0.341158  [20800/175341]\n",
      "loss: 0.357897  [22400/175341]\n",
      "loss: 0.101296  [24000/175341]\n",
      "loss: 0.237428  [25600/175341]\n",
      "loss: 0.606991  [27200/175341]\n",
      "loss: 0.426366  [28800/175341]\n",
      "loss: 0.731852  [30400/175341]\n",
      "loss: 0.529507  [32000/175341]\n",
      "loss: 0.248529  [33600/175341]\n",
      "loss: 0.713992  [35200/175341]\n",
      "loss: 0.412243  [36800/175341]\n",
      "loss: 0.497918  [38400/175341]\n",
      "loss: 0.543000  [40000/175341]\n",
      "loss: 0.510251  [41600/175341]\n",
      "loss: 0.452390  [43200/175341]\n",
      "loss: 0.590060  [44800/175341]\n",
      "loss: 0.275535  [46400/175341]\n",
      "loss: 0.188599  [48000/175341]\n",
      "loss: 0.354801  [49600/175341]\n",
      "loss: 0.299864  [51200/175341]\n",
      "loss: 0.409551  [52800/175341]\n",
      "loss: 0.421896  [54400/175341]\n",
      "loss: 0.811379  [56000/175341]\n",
      "loss: 0.293341  [57600/175341]\n",
      "loss: 0.525649  [59200/175341]\n",
      "loss: 0.936637  [60800/175341]\n",
      "loss: 0.797718  [62400/175341]\n",
      "loss: 0.442567  [64000/175341]\n",
      "loss: 0.610018  [65600/175341]\n",
      "loss: 0.699655  [67200/175341]\n",
      "loss: 0.434482  [68800/175341]\n",
      "loss: 0.663891  [70400/175341]\n",
      "loss: 0.556302  [72000/175341]\n",
      "loss: 0.342855  [73600/175341]\n",
      "loss: 0.458870  [75200/175341]\n",
      "loss: 0.308707  [76800/175341]\n",
      "loss: 0.462218  [78400/175341]\n",
      "loss: 0.722630  [80000/175341]\n",
      "loss: 0.741408  [81600/175341]\n",
      "loss: 0.581499  [83200/175341]\n",
      "loss: 0.780889  [84800/175341]\n",
      "loss: 0.603008  [86400/175341]\n",
      "loss: 0.723506  [88000/175341]\n",
      "loss: 0.421623  [89600/175341]\n",
      "loss: 0.481494  [91200/175341]\n",
      "loss: 0.484220  [92800/175341]\n",
      "loss: 0.746513  [94400/175341]\n",
      "loss: 0.531534  [96000/175341]\n",
      "loss: 0.422163  [97600/175341]\n",
      "loss: 0.374722  [99200/175341]\n",
      "loss: 0.731439  [100800/175341]\n",
      "loss: 0.221595  [102400/175341]\n",
      "loss: 0.249421  [104000/175341]\n",
      "loss: 0.240546  [105600/175341]\n",
      "loss: 0.278896  [107200/175341]\n",
      "loss: 0.485269  [108800/175341]\n",
      "loss: 0.348253  [110400/175341]\n",
      "loss: 0.322261  [112000/175341]\n",
      "loss: 0.403581  [113600/175341]\n",
      "loss: 0.985296  [115200/175341]\n",
      "loss: 0.471494  [116800/175341]\n",
      "loss: 0.183115  [118400/175341]\n",
      "loss: 0.684715  [120000/175341]\n",
      "loss: 0.272586  [121600/175341]\n",
      "loss: 0.265586  [123200/175341]\n",
      "loss: 0.661072  [124800/175341]\n",
      "loss: 0.237658  [126400/175341]\n",
      "loss: 0.321919  [128000/175341]\n",
      "loss: 0.651214  [129600/175341]\n",
      "loss: 0.671235  [131200/175341]\n",
      "loss: 0.801167  [132800/175341]\n",
      "loss: 0.853556  [134400/175341]\n",
      "loss: 0.531283  [136000/175341]\n",
      "loss: 0.516708  [137600/175341]\n",
      "loss: 0.349887  [139200/175341]\n",
      "loss: 0.197309  [140800/175341]\n",
      "loss: 0.365423  [142400/175341]\n",
      "loss: 0.451803  [144000/175341]\n",
      "loss: 0.266235  [145600/175341]\n",
      "loss: 0.631324  [147200/175341]\n",
      "loss: 0.296263  [148800/175341]\n",
      "loss: 0.188469  [150400/175341]\n",
      "loss: 0.636503  [152000/175341]\n",
      "loss: 0.693498  [153600/175341]\n",
      "loss: 0.517739  [155200/175341]\n",
      "loss: 0.807901  [156800/175341]\n",
      "loss: 0.535519  [158400/175341]\n",
      "loss: 0.347570  [160000/175341]\n",
      "loss: 0.806418  [161600/175341]\n",
      "loss: 0.975608  [163200/175341]\n",
      "loss: 0.406055  [164800/175341]\n",
      "loss: 0.281627  [166400/175341]\n",
      "loss: 0.254800  [168000/175341]\n",
      "loss: 0.607730  [169600/175341]\n",
      "loss: 0.141122  [171200/175341]\n",
      "loss: 0.579257  [172800/175341]\n",
      "loss: 0.474778  [174400/175341]\n",
      "Train Accuracy: 81.1647%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.536114, F1-score: 75.94%, Macro_F1-Score:  42.27%  \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.397197  [    0/175341]\n",
      "loss: 0.223195  [ 1600/175341]\n",
      "loss: 0.641948  [ 3200/175341]\n",
      "loss: 0.778637  [ 4800/175341]\n",
      "loss: 0.685885  [ 6400/175341]\n",
      "loss: 0.542578  [ 8000/175341]\n",
      "loss: 0.614361  [ 9600/175341]\n",
      "loss: 0.340648  [11200/175341]\n",
      "loss: 0.569052  [12800/175341]\n",
      "loss: 0.363497  [14400/175341]\n",
      "loss: 0.413686  [16000/175341]\n",
      "loss: 0.768229  [17600/175341]\n",
      "loss: 0.238817  [19200/175341]\n",
      "loss: 0.290087  [20800/175341]\n",
      "loss: 0.625494  [22400/175341]\n",
      "loss: 0.533704  [24000/175341]\n",
      "loss: 0.520732  [25600/175341]\n",
      "loss: 0.686745  [27200/175341]\n",
      "loss: 0.666976  [28800/175341]\n",
      "loss: 0.514199  [30400/175341]\n",
      "loss: 0.428420  [32000/175341]\n",
      "loss: 1.346440  [33600/175341]\n",
      "loss: 0.470911  [35200/175341]\n",
      "loss: 0.794312  [36800/175341]\n",
      "loss: 0.291014  [38400/175341]\n",
      "loss: 0.258559  [40000/175341]\n",
      "loss: 0.639774  [41600/175341]\n",
      "loss: 0.377023  [43200/175341]\n",
      "loss: 0.746216  [44800/175341]\n",
      "loss: 0.700197  [46400/175341]\n",
      "loss: 0.243744  [48000/175341]\n",
      "loss: 0.433607  [49600/175341]\n",
      "loss: 0.436855  [51200/175341]\n",
      "loss: 0.558337  [52800/175341]\n",
      "loss: 0.594385  [54400/175341]\n",
      "loss: 0.638355  [56000/175341]\n",
      "loss: 0.497973  [57600/175341]\n",
      "loss: 0.433515  [59200/175341]\n",
      "loss: 0.528979  [60800/175341]\n",
      "loss: 0.274694  [62400/175341]\n",
      "loss: 0.579667  [64000/175341]\n",
      "loss: 0.338264  [65600/175341]\n",
      "loss: 1.290977  [67200/175341]\n",
      "loss: 0.391808  [68800/175341]\n",
      "loss: 0.570567  [70400/175341]\n",
      "loss: 0.679450  [72000/175341]\n",
      "loss: 0.311015  [73600/175341]\n",
      "loss: 0.555745  [75200/175341]\n",
      "loss: 0.264971  [76800/175341]\n",
      "loss: 0.817588  [78400/175341]\n",
      "loss: 0.490997  [80000/175341]\n",
      "loss: 0.694079  [81600/175341]\n",
      "loss: 0.708949  [83200/175341]\n",
      "loss: 0.133745  [84800/175341]\n",
      "loss: 0.337660  [86400/175341]\n",
      "loss: 0.340407  [88000/175341]\n",
      "loss: 0.476639  [89600/175341]\n",
      "loss: 0.250088  [91200/175341]\n",
      "loss: 0.250555  [92800/175341]\n",
      "loss: 0.260542  [94400/175341]\n",
      "loss: 0.403032  [96000/175341]\n",
      "loss: 0.439615  [97600/175341]\n",
      "loss: 0.253850  [99200/175341]\n",
      "loss: 0.264309  [100800/175341]\n",
      "loss: 0.704851  [102400/175341]\n",
      "loss: 0.225445  [104000/175341]\n",
      "loss: 0.270627  [105600/175341]\n",
      "loss: 0.618975  [107200/175341]\n",
      "loss: 0.543857  [108800/175341]\n",
      "loss: 0.666259  [110400/175341]\n",
      "loss: 0.416138  [112000/175341]\n",
      "loss: 0.741319  [113600/175341]\n",
      "loss: 0.816945  [115200/175341]\n",
      "loss: 0.415969  [116800/175341]\n",
      "loss: 0.520418  [118400/175341]\n",
      "loss: 0.897240  [120000/175341]\n",
      "loss: 0.201078  [121600/175341]\n",
      "loss: 0.550467  [123200/175341]\n",
      "loss: 0.602829  [124800/175341]\n",
      "loss: 0.510541  [126400/175341]\n",
      "loss: 0.751263  [128000/175341]\n",
      "loss: 0.391032  [129600/175341]\n",
      "loss: 0.335393  [131200/175341]\n",
      "loss: 1.143518  [132800/175341]\n",
      "loss: 0.708225  [134400/175341]\n",
      "loss: 0.768093  [136000/175341]\n",
      "loss: 0.506199  [137600/175341]\n",
      "loss: 0.428967  [139200/175341]\n",
      "loss: 0.529117  [140800/175341]\n",
      "loss: 0.442096  [142400/175341]\n",
      "loss: 0.190996  [144000/175341]\n",
      "loss: 0.262371  [145600/175341]\n",
      "loss: 0.647667  [147200/175341]\n",
      "loss: 0.947095  [148800/175341]\n",
      "loss: 0.486303  [150400/175341]\n",
      "loss: 0.753860  [152000/175341]\n",
      "loss: 0.383596  [153600/175341]\n",
      "loss: 0.403351  [155200/175341]\n",
      "loss: 0.545290  [156800/175341]\n",
      "loss: 0.512590  [158400/175341]\n",
      "loss: 0.526995  [160000/175341]\n",
      "loss: 0.480315  [161600/175341]\n",
      "loss: 0.309749  [163200/175341]\n",
      "loss: 0.775180  [164800/175341]\n",
      "loss: 0.235365  [166400/175341]\n",
      "loss: 0.632587  [168000/175341]\n",
      "loss: 0.505914  [169600/175341]\n",
      "loss: 0.293846  [171200/175341]\n",
      "loss: 0.324666  [172800/175341]\n",
      "loss: 0.181828  [174400/175341]\n",
      "Train Accuracy: 81.2388%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.544192, F1-score: 75.95%, Macro_F1-Score:  42.16%  \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.647322  [    0/175341]\n",
      "loss: 0.201300  [ 1600/175341]\n",
      "loss: 0.503907  [ 3200/175341]\n",
      "loss: 0.958521  [ 4800/175341]\n",
      "loss: 0.486868  [ 6400/175341]\n",
      "loss: 0.678947  [ 8000/175341]\n",
      "loss: 0.831081  [ 9600/175341]\n",
      "loss: 0.317317  [11200/175341]\n",
      "loss: 0.361207  [12800/175341]\n",
      "loss: 0.195657  [14400/175341]\n",
      "loss: 0.567292  [16000/175341]\n",
      "loss: 0.317668  [17600/175341]\n",
      "loss: 0.376099  [19200/175341]\n",
      "loss: 0.730170  [20800/175341]\n",
      "loss: 0.531811  [22400/175341]\n",
      "loss: 0.743658  [24000/175341]\n",
      "loss: 0.413045  [25600/175341]\n",
      "loss: 0.391675  [27200/175341]\n",
      "loss: 0.917511  [28800/175341]\n",
      "loss: 0.339420  [30400/175341]\n",
      "loss: 0.677667  [32000/175341]\n",
      "loss: 0.271426  [33600/175341]\n",
      "loss: 0.320836  [35200/175341]\n",
      "loss: 0.066386  [36800/175341]\n",
      "loss: 0.506081  [38400/175341]\n",
      "loss: 0.651808  [40000/175341]\n",
      "loss: 0.867926  [41600/175341]\n",
      "loss: 0.349818  [43200/175341]\n",
      "loss: 0.620975  [44800/175341]\n",
      "loss: 0.512232  [46400/175341]\n",
      "loss: 0.392694  [48000/175341]\n",
      "loss: 0.341026  [49600/175341]\n",
      "loss: 0.480558  [51200/175341]\n",
      "loss: 0.518439  [52800/175341]\n",
      "loss: 0.558656  [54400/175341]\n",
      "loss: 0.596062  [56000/175341]\n",
      "loss: 0.308986  [57600/175341]\n",
      "loss: 0.362291  [59200/175341]\n",
      "loss: 0.707381  [60800/175341]\n",
      "loss: 0.678054  [62400/175341]\n",
      "loss: 0.670476  [64000/175341]\n",
      "loss: 0.202278  [65600/175341]\n",
      "loss: 0.349747  [67200/175341]\n",
      "loss: 0.205295  [68800/175341]\n",
      "loss: 0.227508  [70400/175341]\n",
      "loss: 0.445810  [72000/175341]\n",
      "loss: 0.616247  [73600/175341]\n",
      "loss: 0.764434  [75200/175341]\n",
      "loss: 0.476473  [76800/175341]\n",
      "loss: 0.312673  [78400/175341]\n",
      "loss: 0.605931  [80000/175341]\n",
      "loss: 0.674683  [81600/175341]\n",
      "loss: 0.406865  [83200/175341]\n",
      "loss: 0.395005  [84800/175341]\n",
      "loss: 0.503573  [86400/175341]\n",
      "loss: 0.175589  [88000/175341]\n",
      "loss: 0.237032  [89600/175341]\n",
      "loss: 0.641586  [91200/175341]\n",
      "loss: 0.619209  [92800/175341]\n",
      "loss: 0.199767  [94400/175341]\n",
      "loss: 0.324219  [96000/175341]\n",
      "loss: 0.342563  [97600/175341]\n",
      "loss: 0.559613  [99200/175341]\n",
      "loss: 0.578296  [100800/175341]\n",
      "loss: 0.137603  [102400/175341]\n",
      "loss: 0.396583  [104000/175341]\n",
      "loss: 0.478870  [105600/175341]\n",
      "loss: 0.235796  [107200/175341]\n",
      "loss: 0.282417  [108800/175341]\n",
      "loss: 0.818602  [110400/175341]\n",
      "loss: 0.304240  [112000/175341]\n",
      "loss: 0.587377  [113600/175341]\n",
      "loss: 0.388786  [115200/175341]\n",
      "loss: 0.441897  [116800/175341]\n",
      "loss: 0.359162  [118400/175341]\n",
      "loss: 0.525470  [120000/175341]\n",
      "loss: 0.713807  [121600/175341]\n",
      "loss: 0.513152  [123200/175341]\n",
      "loss: 0.207838  [124800/175341]\n",
      "loss: 0.202187  [126400/175341]\n",
      "loss: 0.867465  [128000/175341]\n",
      "loss: 0.620301  [129600/175341]\n",
      "loss: 0.383246  [131200/175341]\n",
      "loss: 0.539401  [132800/175341]\n",
      "loss: 0.783653  [134400/175341]\n",
      "loss: 0.447836  [136000/175341]\n",
      "loss: 0.838914  [137600/175341]\n",
      "loss: 0.638847  [139200/175341]\n",
      "loss: 0.080881  [140800/175341]\n",
      "loss: 0.728537  [142400/175341]\n",
      "loss: 0.236581  [144000/175341]\n",
      "loss: 0.585606  [145600/175341]\n",
      "loss: 0.723028  [147200/175341]\n",
      "loss: 0.421122  [148800/175341]\n",
      "loss: 0.634866  [150400/175341]\n",
      "loss: 0.431953  [152000/175341]\n",
      "loss: 0.448485  [153600/175341]\n",
      "loss: 0.205300  [155200/175341]\n",
      "loss: 0.587962  [156800/175341]\n",
      "loss: 0.302673  [158400/175341]\n",
      "loss: 0.205649  [160000/175341]\n",
      "loss: 0.363928  [161600/175341]\n",
      "loss: 0.378930  [163200/175341]\n",
      "loss: 0.752946  [164800/175341]\n",
      "loss: 0.277757  [166400/175341]\n",
      "loss: 0.423077  [168000/175341]\n",
      "loss: 0.509203  [169600/175341]\n",
      "loss: 0.300958  [171200/175341]\n",
      "loss: 0.631678  [172800/175341]\n",
      "loss: 0.370618  [174400/175341]\n",
      "Train Accuracy: 81.2371%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.559084, F1-score: 75.10%, Macro_F1-Score:  42.63%  \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.389818  [    0/175341]\n",
      "loss: 0.646397  [ 1600/175341]\n",
      "loss: 0.593127  [ 3200/175341]\n",
      "loss: 0.354085  [ 4800/175341]\n",
      "loss: 0.355965  [ 6400/175341]\n",
      "loss: 0.434368  [ 8000/175341]\n",
      "loss: 0.412177  [ 9600/175341]\n",
      "loss: 0.653820  [11200/175341]\n",
      "loss: 0.600250  [12800/175341]\n",
      "loss: 0.624035  [14400/175341]\n",
      "loss: 0.805660  [16000/175341]\n",
      "loss: 0.392980  [17600/175341]\n",
      "loss: 0.463780  [19200/175341]\n",
      "loss: 0.831847  [20800/175341]\n",
      "loss: 0.365907  [22400/175341]\n",
      "loss: 0.722644  [24000/175341]\n",
      "loss: 0.473906  [25600/175341]\n",
      "loss: 0.558571  [27200/175341]\n",
      "loss: 0.554225  [28800/175341]\n",
      "loss: 0.317833  [30400/175341]\n",
      "loss: 1.111360  [32000/175341]\n",
      "loss: 0.149376  [33600/175341]\n",
      "loss: 0.237594  [35200/175341]\n",
      "loss: 0.452201  [36800/175341]\n",
      "loss: 0.146210  [38400/175341]\n",
      "loss: 0.680626  [40000/175341]\n",
      "loss: 0.689853  [41600/175341]\n",
      "loss: 0.709734  [43200/175341]\n",
      "loss: 0.218971  [44800/175341]\n",
      "loss: 0.474848  [46400/175341]\n",
      "loss: 0.458443  [48000/175341]\n",
      "loss: 0.428921  [49600/175341]\n",
      "loss: 0.252861  [51200/175341]\n",
      "loss: 0.639114  [52800/175341]\n",
      "loss: 0.395954  [54400/175341]\n",
      "loss: 0.398572  [56000/175341]\n",
      "loss: 0.839816  [57600/175341]\n",
      "loss: 0.176784  [59200/175341]\n",
      "loss: 0.920749  [60800/175341]\n",
      "loss: 0.255337  [62400/175341]\n",
      "loss: 0.659124  [64000/175341]\n",
      "loss: 0.403380  [65600/175341]\n",
      "loss: 0.384023  [67200/175341]\n",
      "loss: 0.667782  [68800/175341]\n",
      "loss: 0.458721  [70400/175341]\n",
      "loss: 0.854999  [72000/175341]\n",
      "loss: 0.167898  [73600/175341]\n",
      "loss: 0.504335  [75200/175341]\n",
      "loss: 0.404534  [76800/175341]\n",
      "loss: 1.011895  [78400/175341]\n",
      "loss: 0.324344  [80000/175341]\n",
      "loss: 0.891728  [81600/175341]\n",
      "loss: 0.574101  [83200/175341]\n",
      "loss: 0.486392  [84800/175341]\n",
      "loss: 0.529615  [86400/175341]\n",
      "loss: 0.350676  [88000/175341]\n",
      "loss: 0.635221  [89600/175341]\n",
      "loss: 0.345599  [91200/175341]\n",
      "loss: 0.287214  [92800/175341]\n",
      "loss: 0.388605  [94400/175341]\n",
      "loss: 0.546855  [96000/175341]\n",
      "loss: 0.291312  [97600/175341]\n",
      "loss: 0.553062  [99200/175341]\n",
      "loss: 0.151273  [100800/175341]\n",
      "loss: 0.525594  [102400/175341]\n",
      "loss: 0.588490  [104000/175341]\n",
      "loss: 0.319424  [105600/175341]\n",
      "loss: 0.603712  [107200/175341]\n",
      "loss: 0.367975  [108800/175341]\n",
      "loss: 0.261048  [110400/175341]\n",
      "loss: 0.506894  [112000/175341]\n",
      "loss: 0.090473  [113600/175341]\n",
      "loss: 0.791367  [115200/175341]\n",
      "loss: 0.342761  [116800/175341]\n",
      "loss: 0.235924  [118400/175341]\n",
      "loss: 0.483639  [120000/175341]\n",
      "loss: 0.613599  [121600/175341]\n",
      "loss: 0.518779  [123200/175341]\n",
      "loss: 0.713538  [124800/175341]\n",
      "loss: 0.572263  [126400/175341]\n",
      "loss: 0.734717  [128000/175341]\n",
      "loss: 0.174163  [129600/175341]\n",
      "loss: 0.145990  [131200/175341]\n",
      "loss: 0.615824  [132800/175341]\n",
      "loss: 0.173264  [134400/175341]\n",
      "loss: 0.533553  [136000/175341]\n",
      "loss: 0.212848  [137600/175341]\n",
      "loss: 0.465710  [139200/175341]\n",
      "loss: 0.439488  [140800/175341]\n",
      "loss: 0.457671  [142400/175341]\n",
      "loss: 0.294038  [144000/175341]\n",
      "loss: 0.675474  [145600/175341]\n",
      "loss: 0.380070  [147200/175341]\n",
      "loss: 0.279453  [148800/175341]\n",
      "loss: 0.637244  [150400/175341]\n",
      "loss: 0.356089  [152000/175341]\n",
      "loss: 0.685612  [153600/175341]\n",
      "loss: 0.655602  [155200/175341]\n",
      "loss: 1.171630  [156800/175341]\n",
      "loss: 0.612439  [158400/175341]\n",
      "loss: 0.503605  [160000/175341]\n",
      "loss: 0.617060  [161600/175341]\n",
      "loss: 0.403143  [163200/175341]\n",
      "loss: 0.829210  [164800/175341]\n",
      "loss: 0.526169  [166400/175341]\n",
      "loss: 0.143598  [168000/175341]\n",
      "loss: 0.417020  [169600/175341]\n",
      "loss: 0.714366  [171200/175341]\n",
      "loss: 0.545628  [172800/175341]\n",
      "loss: 0.968153  [174400/175341]\n",
      "Train Accuracy: 81.2468%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.540102, F1-score: 76.09%, Macro_F1-Score:  41.80%  \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.319668  [    0/175341]\n",
      "loss: 0.773738  [ 1600/175341]\n",
      "loss: 0.270605  [ 3200/175341]\n",
      "loss: 0.291394  [ 4800/175341]\n",
      "loss: 0.404097  [ 6400/175341]\n",
      "loss: 0.204285  [ 8000/175341]\n",
      "loss: 0.204909  [ 9600/175341]\n",
      "loss: 0.273877  [11200/175341]\n",
      "loss: 0.224426  [12800/175341]\n",
      "loss: 0.963810  [14400/175341]\n",
      "loss: 0.578990  [16000/175341]\n",
      "loss: 0.661525  [17600/175341]\n",
      "loss: 0.412097  [19200/175341]\n",
      "loss: 0.817835  [20800/175341]\n",
      "loss: 0.402802  [22400/175341]\n",
      "loss: 0.563623  [24000/175341]\n",
      "loss: 0.433903  [25600/175341]\n",
      "loss: 0.735535  [27200/175341]\n",
      "loss: 0.430045  [28800/175341]\n",
      "loss: 0.293546  [30400/175341]\n",
      "loss: 0.219076  [32000/175341]\n",
      "loss: 0.257922  [33600/175341]\n",
      "loss: 0.370421  [35200/175341]\n",
      "loss: 0.732327  [36800/175341]\n",
      "loss: 0.413977  [38400/175341]\n",
      "loss: 0.444954  [40000/175341]\n",
      "loss: 0.213947  [41600/175341]\n",
      "loss: 0.241495  [43200/175341]\n",
      "loss: 0.414785  [44800/175341]\n",
      "loss: 0.290346  [46400/175341]\n",
      "loss: 0.638900  [48000/175341]\n",
      "loss: 0.643736  [49600/175341]\n",
      "loss: 0.552647  [51200/175341]\n",
      "loss: 0.172814  [52800/175341]\n",
      "loss: 0.463701  [54400/175341]\n",
      "loss: 0.683997  [56000/175341]\n",
      "loss: 0.179218  [57600/175341]\n",
      "loss: 0.258833  [59200/175341]\n",
      "loss: 0.742212  [60800/175341]\n",
      "loss: 0.506531  [62400/175341]\n",
      "loss: 0.473005  [64000/175341]\n",
      "loss: 0.726825  [65600/175341]\n",
      "loss: 0.186037  [67200/175341]\n",
      "loss: 0.531227  [68800/175341]\n",
      "loss: 0.420582  [70400/175341]\n",
      "loss: 0.468418  [72000/175341]\n",
      "loss: 0.470796  [73600/175341]\n",
      "loss: 0.587404  [75200/175341]\n",
      "loss: 0.555859  [76800/175341]\n",
      "loss: 0.108558  [78400/175341]\n",
      "loss: 0.171213  [80000/175341]\n",
      "loss: 0.399219  [81600/175341]\n",
      "loss: 0.624328  [83200/175341]\n",
      "loss: 0.236751  [84800/175341]\n",
      "loss: 0.201380  [86400/175341]\n",
      "loss: 0.170178  [88000/175341]\n",
      "loss: 0.514732  [89600/175341]\n",
      "loss: 0.581469  [91200/175341]\n",
      "loss: 0.367444  [92800/175341]\n",
      "loss: 0.200388  [94400/175341]\n",
      "loss: 0.387175  [96000/175341]\n",
      "loss: 0.198432  [97600/175341]\n",
      "loss: 0.389503  [99200/175341]\n",
      "loss: 1.107192  [100800/175341]\n",
      "loss: 0.329742  [102400/175341]\n",
      "loss: 0.387571  [104000/175341]\n",
      "loss: 0.465872  [105600/175341]\n",
      "loss: 0.824934  [107200/175341]\n",
      "loss: 0.455257  [108800/175341]\n",
      "loss: 0.298373  [110400/175341]\n",
      "loss: 0.630316  [112000/175341]\n",
      "loss: 0.589689  [113600/175341]\n",
      "loss: 0.647432  [115200/175341]\n",
      "loss: 0.272311  [116800/175341]\n",
      "loss: 0.267500  [118400/175341]\n",
      "loss: 0.431253  [120000/175341]\n",
      "loss: 0.434982  [121600/175341]\n",
      "loss: 0.310691  [123200/175341]\n",
      "loss: 0.332269  [124800/175341]\n",
      "loss: 0.549285  [126400/175341]\n",
      "loss: 0.510223  [128000/175341]\n",
      "loss: 0.966525  [129600/175341]\n",
      "loss: 0.342079  [131200/175341]\n",
      "loss: 0.163015  [132800/175341]\n",
      "loss: 0.364926  [134400/175341]\n",
      "loss: 0.834334  [136000/175341]\n",
      "loss: 0.662804  [137600/175341]\n",
      "loss: 0.333524  [139200/175341]\n",
      "loss: 0.329441  [140800/175341]\n",
      "loss: 0.832577  [142400/175341]\n",
      "loss: 0.315933  [144000/175341]\n",
      "loss: 0.625538  [145600/175341]\n",
      "loss: 0.432234  [147200/175341]\n",
      "loss: 0.270562  [148800/175341]\n",
      "loss: 0.942447  [150400/175341]\n",
      "loss: 0.466503  [152000/175341]\n",
      "loss: 0.509091  [153600/175341]\n",
      "loss: 0.481779  [155200/175341]\n",
      "loss: 0.489980  [156800/175341]\n",
      "loss: 0.347885  [158400/175341]\n",
      "loss: 0.553895  [160000/175341]\n",
      "loss: 0.317637  [161600/175341]\n",
      "loss: 0.384982  [163200/175341]\n",
      "loss: 0.657543  [164800/175341]\n",
      "loss: 0.545762  [166400/175341]\n",
      "loss: 0.751603  [168000/175341]\n",
      "loss: 0.232382  [169600/175341]\n",
      "loss: 0.257928  [171200/175341]\n",
      "loss: 0.351957  [172800/175341]\n",
      "loss: 0.635263  [174400/175341]\n",
      "Train Accuracy: 81.2212%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.567897, F1-score: 74.94%, Macro_F1-Score:  41.37%  \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.611018  [    0/175341]\n",
      "loss: 0.558562  [ 1600/175341]\n",
      "loss: 0.564075  [ 3200/175341]\n",
      "loss: 1.036480  [ 4800/175341]\n",
      "loss: 0.462332  [ 6400/175341]\n",
      "loss: 0.425584  [ 8000/175341]\n",
      "loss: 0.563969  [ 9600/175341]\n",
      "loss: 0.714440  [11200/175341]\n",
      "loss: 0.816757  [12800/175341]\n",
      "loss: 0.311218  [14400/175341]\n",
      "loss: 0.395386  [16000/175341]\n",
      "loss: 0.616745  [17600/175341]\n",
      "loss: 1.129745  [19200/175341]\n",
      "loss: 0.579110  [20800/175341]\n",
      "loss: 0.270017  [22400/175341]\n",
      "loss: 0.376112  [24000/175341]\n",
      "loss: 0.256903  [25600/175341]\n",
      "loss: 0.670943  [27200/175341]\n",
      "loss: 0.498453  [28800/175341]\n",
      "loss: 0.676742  [30400/175341]\n",
      "loss: 0.375245  [32000/175341]\n",
      "loss: 0.388720  [33600/175341]\n",
      "loss: 0.759800  [35200/175341]\n",
      "loss: 0.716819  [36800/175341]\n",
      "loss: 0.293979  [38400/175341]\n",
      "loss: 0.492708  [40000/175341]\n",
      "loss: 0.289847  [41600/175341]\n",
      "loss: 0.250981  [43200/175341]\n",
      "loss: 0.658672  [44800/175341]\n",
      "loss: 0.794088  [46400/175341]\n",
      "loss: 0.138246  [48000/175341]\n",
      "loss: 0.877646  [49600/175341]\n",
      "loss: 0.255285  [51200/175341]\n",
      "loss: 0.659842  [52800/175341]\n",
      "loss: 0.350359  [54400/175341]\n",
      "loss: 0.177618  [56000/175341]\n",
      "loss: 0.510416  [57600/175341]\n",
      "loss: 0.391551  [59200/175341]\n",
      "loss: 0.846362  [60800/175341]\n",
      "loss: 0.282383  [62400/175341]\n",
      "loss: 0.535204  [64000/175341]\n",
      "loss: 0.443985  [65600/175341]\n",
      "loss: 0.470180  [67200/175341]\n",
      "loss: 0.448590  [68800/175341]\n",
      "loss: 0.138764  [70400/175341]\n",
      "loss: 0.559795  [72000/175341]\n",
      "loss: 0.552184  [73600/175341]\n",
      "loss: 0.421888  [75200/175341]\n",
      "loss: 0.419973  [76800/175341]\n",
      "loss: 0.416342  [78400/175341]\n",
      "loss: 0.241798  [80000/175341]\n",
      "loss: 0.391636  [81600/175341]\n",
      "loss: 0.681363  [83200/175341]\n",
      "loss: 0.521265  [84800/175341]\n",
      "loss: 0.834936  [86400/175341]\n",
      "loss: 0.570391  [88000/175341]\n",
      "loss: 0.535173  [89600/175341]\n",
      "loss: 0.543531  [91200/175341]\n",
      "loss: 0.804322  [92800/175341]\n",
      "loss: 0.595814  [94400/175341]\n",
      "loss: 0.674978  [96000/175341]\n",
      "loss: 0.714307  [97600/175341]\n",
      "loss: 0.267411  [99200/175341]\n",
      "loss: 0.411735  [100800/175341]\n",
      "loss: 0.366512  [102400/175341]\n",
      "loss: 0.412951  [104000/175341]\n",
      "loss: 0.405857  [105600/175341]\n",
      "loss: 0.419416  [107200/175341]\n",
      "loss: 0.521842  [108800/175341]\n",
      "loss: 0.320820  [110400/175341]\n",
      "loss: 0.490752  [112000/175341]\n",
      "loss: 0.924553  [113600/175341]\n",
      "loss: 0.530747  [115200/175341]\n",
      "loss: 0.199173  [116800/175341]\n",
      "loss: 0.376255  [118400/175341]\n",
      "loss: 0.421805  [120000/175341]\n",
      "loss: 0.574007  [121600/175341]\n",
      "loss: 0.552857  [123200/175341]\n",
      "loss: 0.279806  [124800/175341]\n",
      "loss: 0.927865  [126400/175341]\n",
      "loss: 0.730681  [128000/175341]\n",
      "loss: 0.153640  [129600/175341]\n",
      "loss: 0.408228  [131200/175341]\n",
      "loss: 0.188000  [132800/175341]\n",
      "loss: 0.623671  [134400/175341]\n",
      "loss: 0.666744  [136000/175341]\n",
      "loss: 0.836525  [137600/175341]\n",
      "loss: 0.485392  [139200/175341]\n",
      "loss: 0.711336  [140800/175341]\n",
      "loss: 0.152335  [142400/175341]\n",
      "loss: 0.118995  [144000/175341]\n",
      "loss: 0.541718  [145600/175341]\n",
      "loss: 0.262948  [147200/175341]\n",
      "loss: 0.347769  [148800/175341]\n",
      "loss: 0.200089  [150400/175341]\n",
      "loss: 0.398520  [152000/175341]\n",
      "loss: 0.207586  [153600/175341]\n",
      "loss: 0.618283  [155200/175341]\n",
      "loss: 0.408746  [156800/175341]\n",
      "loss: 0.482109  [158400/175341]\n",
      "loss: 0.394667  [160000/175341]\n",
      "loss: 0.394056  [161600/175341]\n",
      "loss: 1.045769  [163200/175341]\n",
      "loss: 0.612358  [164800/175341]\n",
      "loss: 0.488990  [166400/175341]\n",
      "loss: 0.541340  [168000/175341]\n",
      "loss: 0.722647  [169600/175341]\n",
      "loss: 0.325511  [171200/175341]\n",
      "loss: 0.261612  [172800/175341]\n",
      "loss: 0.246623  [174400/175341]\n",
      "Train Accuracy: 81.2502%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.546628, F1-score: 75.88%, Macro_F1-Score:  42.75%  \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.612895  [    0/175341]\n",
      "loss: 0.443883  [ 1600/175341]\n",
      "loss: 0.203492  [ 3200/175341]\n",
      "loss: 0.775463  [ 4800/175341]\n",
      "loss: 0.427497  [ 6400/175341]\n",
      "loss: 0.182908  [ 8000/175341]\n",
      "loss: 0.273010  [ 9600/175341]\n",
      "loss: 0.167614  [11200/175341]\n",
      "loss: 0.166429  [12800/175341]\n",
      "loss: 0.744509  [14400/175341]\n",
      "loss: 0.287324  [16000/175341]\n",
      "loss: 0.327647  [17600/175341]\n",
      "loss: 0.749767  [19200/175341]\n",
      "loss: 0.564214  [20800/175341]\n",
      "loss: 0.508433  [22400/175341]\n",
      "loss: 0.144809  [24000/175341]\n",
      "loss: 0.313231  [25600/175341]\n",
      "loss: 0.358989  [27200/175341]\n",
      "loss: 0.692228  [28800/175341]\n",
      "loss: 0.778581  [30400/175341]\n",
      "loss: 0.796548  [32000/175341]\n",
      "loss: 0.692766  [33600/175341]\n",
      "loss: 0.434201  [35200/175341]\n",
      "loss: 0.041661  [36800/175341]\n",
      "loss: 0.262866  [38400/175341]\n",
      "loss: 0.646967  [40000/175341]\n",
      "loss: 0.488229  [41600/175341]\n",
      "loss: 0.255303  [43200/175341]\n",
      "loss: 0.627035  [44800/175341]\n",
      "loss: 0.389502  [46400/175341]\n",
      "loss: 0.292807  [48000/175341]\n",
      "loss: 0.589893  [49600/175341]\n",
      "loss: 0.790858  [51200/175341]\n",
      "loss: 0.147051  [52800/175341]\n",
      "loss: 0.511041  [54400/175341]\n",
      "loss: 0.518284  [56000/175341]\n",
      "loss: 0.586314  [57600/175341]\n",
      "loss: 0.531579  [59200/175341]\n",
      "loss: 0.417725  [60800/175341]\n",
      "loss: 0.390352  [62400/175341]\n",
      "loss: 0.201181  [64000/175341]\n",
      "loss: 1.074951  [65600/175341]\n",
      "loss: 0.498665  [67200/175341]\n",
      "loss: 0.768625  [68800/175341]\n",
      "loss: 0.463102  [70400/175341]\n",
      "loss: 0.469509  [72000/175341]\n",
      "loss: 0.314045  [73600/175341]\n",
      "loss: 0.165360  [75200/175341]\n",
      "loss: 0.459376  [76800/175341]\n",
      "loss: 0.616351  [78400/175341]\n",
      "loss: 0.239131  [80000/175341]\n",
      "loss: 0.220841  [81600/175341]\n",
      "loss: 0.235789  [83200/175341]\n",
      "loss: 0.562008  [84800/175341]\n",
      "loss: 0.433907  [86400/175341]\n",
      "loss: 0.571050  [88000/175341]\n",
      "loss: 0.352277  [89600/175341]\n",
      "loss: 0.439745  [91200/175341]\n",
      "loss: 0.692400  [92800/175341]\n",
      "loss: 0.274501  [94400/175341]\n",
      "loss: 0.180745  [96000/175341]\n",
      "loss: 0.791607  [97600/175341]\n",
      "loss: 0.156296  [99200/175341]\n",
      "loss: 0.397588  [100800/175341]\n",
      "loss: 0.365530  [102400/175341]\n",
      "loss: 0.767021  [104000/175341]\n",
      "loss: 0.243297  [105600/175341]\n",
      "loss: 0.361944  [107200/175341]\n",
      "loss: 1.328554  [108800/175341]\n",
      "loss: 0.215417  [110400/175341]\n",
      "loss: 0.545083  [112000/175341]\n",
      "loss: 0.347548  [113600/175341]\n",
      "loss: 0.252330  [115200/175341]\n",
      "loss: 0.667530  [116800/175341]\n",
      "loss: 0.728826  [118400/175341]\n",
      "loss: 0.369189  [120000/175341]\n",
      "loss: 0.519090  [121600/175341]\n",
      "loss: 0.165856  [123200/175341]\n",
      "loss: 0.578475  [124800/175341]\n",
      "loss: 0.652105  [126400/175341]\n",
      "loss: 0.509870  [128000/175341]\n",
      "loss: 0.665948  [129600/175341]\n",
      "loss: 0.167704  [131200/175341]\n",
      "loss: 0.451728  [132800/175341]\n",
      "loss: 0.338176  [134400/175341]\n",
      "loss: 0.502917  [136000/175341]\n",
      "loss: 0.727726  [137600/175341]\n",
      "loss: 0.794590  [139200/175341]\n",
      "loss: 0.287814  [140800/175341]\n",
      "loss: 0.478169  [142400/175341]\n",
      "loss: 0.653473  [144000/175341]\n",
      "loss: 0.735634  [145600/175341]\n",
      "loss: 0.446258  [147200/175341]\n",
      "loss: 0.339873  [148800/175341]\n",
      "loss: 0.730346  [150400/175341]\n",
      "loss: 0.224188  [152000/175341]\n",
      "loss: 0.154744  [153600/175341]\n",
      "loss: 0.652941  [155200/175341]\n",
      "loss: 0.215460  [156800/175341]\n",
      "loss: 0.468143  [158400/175341]\n",
      "loss: 0.469304  [160000/175341]\n",
      "loss: 0.593785  [161600/175341]\n",
      "loss: 0.402162  [163200/175341]\n",
      "loss: 0.252081  [164800/175341]\n",
      "loss: 0.387953  [166400/175341]\n",
      "loss: 0.505320  [168000/175341]\n",
      "loss: 0.190563  [169600/175341]\n",
      "loss: 0.474153  [171200/175341]\n",
      "loss: 0.220582  [172800/175341]\n",
      "loss: 0.387370  [174400/175341]\n",
      "Train Accuracy: 81.2457%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.549395, F1-score: 76.23%, Macro_F1-Score:  42.64%  \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.381239  [    0/175341]\n",
      "loss: 0.512406  [ 1600/175341]\n",
      "loss: 0.345549  [ 3200/175341]\n",
      "loss: 0.661323  [ 4800/175341]\n",
      "loss: 0.261887  [ 6400/175341]\n",
      "loss: 0.403488  [ 8000/175341]\n",
      "loss: 0.415398  [ 9600/175341]\n",
      "loss: 0.458471  [11200/175341]\n",
      "loss: 0.544469  [12800/175341]\n",
      "loss: 0.222266  [14400/175341]\n",
      "loss: 0.309190  [16000/175341]\n",
      "loss: 0.354969  [17600/175341]\n",
      "loss: 0.400393  [19200/175341]\n",
      "loss: 0.526818  [20800/175341]\n",
      "loss: 0.430655  [22400/175341]\n",
      "loss: 0.561573  [24000/175341]\n",
      "loss: 0.440994  [25600/175341]\n",
      "loss: 0.137148  [27200/175341]\n",
      "loss: 0.376678  [28800/175341]\n",
      "loss: 0.540906  [30400/175341]\n",
      "loss: 0.361619  [32000/175341]\n",
      "loss: 0.216560  [33600/175341]\n",
      "loss: 0.530745  [35200/175341]\n",
      "loss: 0.221887  [36800/175341]\n",
      "loss: 0.613165  [38400/175341]\n",
      "loss: 0.478712  [40000/175341]\n",
      "loss: 0.963086  [41600/175341]\n",
      "loss: 0.322437  [43200/175341]\n",
      "loss: 0.711783  [44800/175341]\n",
      "loss: 0.563405  [46400/175341]\n",
      "loss: 0.497269  [48000/175341]\n",
      "loss: 0.474476  [49600/175341]\n",
      "loss: 0.571411  [51200/175341]\n",
      "loss: 0.660426  [52800/175341]\n",
      "loss: 0.279859  [54400/175341]\n",
      "loss: 0.231361  [56000/175341]\n",
      "loss: 0.630542  [57600/175341]\n",
      "loss: 0.384640  [59200/175341]\n",
      "loss: 0.440495  [60800/175341]\n",
      "loss: 0.242595  [62400/175341]\n",
      "loss: 0.686849  [64000/175341]\n",
      "loss: 0.312588  [65600/175341]\n",
      "loss: 0.618601  [67200/175341]\n",
      "loss: 0.342593  [68800/175341]\n",
      "loss: 0.207413  [70400/175341]\n",
      "loss: 0.265016  [72000/175341]\n",
      "loss: 0.157650  [73600/175341]\n",
      "loss: 0.224344  [75200/175341]\n",
      "loss: 0.606999  [76800/175341]\n",
      "loss: 0.524691  [78400/175341]\n",
      "loss: 0.684862  [80000/175341]\n",
      "loss: 0.350424  [81600/175341]\n",
      "loss: 0.512905  [83200/175341]\n",
      "loss: 0.387855  [84800/175341]\n",
      "loss: 0.463053  [86400/175341]\n",
      "loss: 0.281864  [88000/175341]\n",
      "loss: 0.216399  [89600/175341]\n",
      "loss: 0.987677  [91200/175341]\n",
      "loss: 0.342700  [92800/175341]\n",
      "loss: 0.371954  [94400/175341]\n",
      "loss: 0.680368  [96000/175341]\n",
      "loss: 0.426244  [97600/175341]\n",
      "loss: 0.255509  [99200/175341]\n",
      "loss: 0.139262  [100800/175341]\n",
      "loss: 0.448470  [102400/175341]\n",
      "loss: 0.302317  [104000/175341]\n",
      "loss: 0.475599  [105600/175341]\n",
      "loss: 0.406193  [107200/175341]\n",
      "loss: 0.193270  [108800/175341]\n",
      "loss: 0.525677  [110400/175341]\n",
      "loss: 0.353089  [112000/175341]\n",
      "loss: 0.352037  [113600/175341]\n",
      "loss: 0.237650  [115200/175341]\n",
      "loss: 0.447389  [116800/175341]\n",
      "loss: 0.502478  [118400/175341]\n",
      "loss: 0.405970  [120000/175341]\n",
      "loss: 0.091889  [121600/175341]\n",
      "loss: 0.405605  [123200/175341]\n",
      "loss: 0.610043  [124800/175341]\n",
      "loss: 0.796902  [126400/175341]\n",
      "loss: 0.491224  [128000/175341]\n",
      "loss: 0.181107  [129600/175341]\n",
      "loss: 0.396072  [131200/175341]\n",
      "loss: 0.288176  [132800/175341]\n",
      "loss: 0.188400  [134400/175341]\n",
      "loss: 0.659927  [136000/175341]\n",
      "loss: 0.259752  [137600/175341]\n",
      "loss: 0.271560  [139200/175341]\n",
      "loss: 0.778240  [140800/175341]\n",
      "loss: 0.323828  [142400/175341]\n",
      "loss: 0.516356  [144000/175341]\n",
      "loss: 0.540873  [145600/175341]\n",
      "loss: 0.664035  [147200/175341]\n",
      "loss: 0.284538  [148800/175341]\n",
      "loss: 0.426403  [150400/175341]\n",
      "loss: 0.679417  [152000/175341]\n",
      "loss: 0.300463  [153600/175341]\n",
      "loss: 0.292093  [155200/175341]\n",
      "loss: 0.260981  [156800/175341]\n",
      "loss: 0.563327  [158400/175341]\n",
      "loss: 0.256290  [160000/175341]\n",
      "loss: 0.298004  [161600/175341]\n",
      "loss: 0.704852  [163200/175341]\n",
      "loss: 0.578428  [164800/175341]\n",
      "loss: 1.339984  [166400/175341]\n",
      "loss: 0.172851  [168000/175341]\n",
      "loss: 0.335758  [169600/175341]\n",
      "loss: 0.231253  [171200/175341]\n",
      "loss: 0.564964  [172800/175341]\n",
      "loss: 0.247154  [174400/175341]\n",
      "Train Accuracy: 81.2309%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.539562, F1-score: 76.08%, Macro_F1-Score:  42.56%  \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.153841  [    0/175341]\n",
      "loss: 0.519116  [ 1600/175341]\n",
      "loss: 0.381670  [ 3200/175341]\n",
      "loss: 0.271349  [ 4800/175341]\n",
      "loss: 0.625454  [ 6400/175341]\n",
      "loss: 0.421609  [ 8000/175341]\n",
      "loss: 0.750572  [ 9600/175341]\n",
      "loss: 0.520211  [11200/175341]\n",
      "loss: 0.488633  [12800/175341]\n",
      "loss: 0.613200  [14400/175341]\n",
      "loss: 0.289519  [16000/175341]\n",
      "loss: 0.832906  [17600/175341]\n",
      "loss: 0.812892  [19200/175341]\n",
      "loss: 0.354936  [20800/175341]\n",
      "loss: 0.377333  [22400/175341]\n",
      "loss: 0.271641  [24000/175341]\n",
      "loss: 0.253394  [25600/175341]\n",
      "loss: 0.315579  [27200/175341]\n",
      "loss: 0.194149  [28800/175341]\n",
      "loss: 0.420824  [30400/175341]\n",
      "loss: 0.583513  [32000/175341]\n",
      "loss: 0.701746  [33600/175341]\n",
      "loss: 1.118995  [35200/175341]\n",
      "loss: 0.339661  [36800/175341]\n",
      "loss: 0.493027  [38400/175341]\n",
      "loss: 0.382126  [40000/175341]\n",
      "loss: 0.379578  [41600/175341]\n",
      "loss: 0.278487  [43200/175341]\n",
      "loss: 0.961031  [44800/175341]\n",
      "loss: 0.814347  [46400/175341]\n",
      "loss: 0.283768  [48000/175341]\n",
      "loss: 0.279983  [49600/175341]\n",
      "loss: 0.338074  [51200/175341]\n",
      "loss: 0.567560  [52800/175341]\n",
      "loss: 0.322517  [54400/175341]\n",
      "loss: 0.281972  [56000/175341]\n",
      "loss: 1.008075  [57600/175341]\n",
      "loss: 0.288074  [59200/175341]\n",
      "loss: 0.757149  [60800/175341]\n",
      "loss: 0.583899  [62400/175341]\n",
      "loss: 0.455304  [64000/175341]\n",
      "loss: 0.534510  [65600/175341]\n",
      "loss: 0.610311  [67200/175341]\n",
      "loss: 0.343144  [68800/175341]\n",
      "loss: 0.665692  [70400/175341]\n",
      "loss: 0.682207  [72000/175341]\n",
      "loss: 0.350536  [73600/175341]\n",
      "loss: 0.480881  [75200/175341]\n",
      "loss: 0.244830  [76800/175341]\n",
      "loss: 0.535380  [78400/175341]\n",
      "loss: 0.177925  [80000/175341]\n",
      "loss: 0.099466  [81600/175341]\n",
      "loss: 0.595693  [83200/175341]\n",
      "loss: 0.612220  [84800/175341]\n",
      "loss: 0.443453  [86400/175341]\n",
      "loss: 0.643940  [88000/175341]\n",
      "loss: 0.453210  [89600/175341]\n",
      "loss: 0.882364  [91200/175341]\n",
      "loss: 0.508005  [92800/175341]\n",
      "loss: 0.131768  [94400/175341]\n",
      "loss: 0.524028  [96000/175341]\n",
      "loss: 0.546376  [97600/175341]\n",
      "loss: 0.604948  [99200/175341]\n",
      "loss: 0.203545  [100800/175341]\n",
      "loss: 0.411968  [102400/175341]\n",
      "loss: 0.084411  [104000/175341]\n",
      "loss: 0.621676  [105600/175341]\n",
      "loss: 0.318340  [107200/175341]\n",
      "loss: 0.448088  [108800/175341]\n",
      "loss: 0.446391  [110400/175341]\n",
      "loss: 0.564309  [112000/175341]\n",
      "loss: 0.460701  [113600/175341]\n",
      "loss: 0.237246  [115200/175341]\n",
      "loss: 0.466085  [116800/175341]\n",
      "loss: 0.093337  [118400/175341]\n",
      "loss: 0.231862  [120000/175341]\n",
      "loss: 0.531401  [121600/175341]\n",
      "loss: 0.691368  [123200/175341]\n",
      "loss: 0.673845  [124800/175341]\n",
      "loss: 0.373409  [126400/175341]\n",
      "loss: 0.730226  [128000/175341]\n",
      "loss: 0.435667  [129600/175341]\n",
      "loss: 0.459295  [131200/175341]\n",
      "loss: 0.305322  [132800/175341]\n",
      "loss: 0.566135  [134400/175341]\n",
      "loss: 0.422840  [136000/175341]\n",
      "loss: 0.455901  [137600/175341]\n",
      "loss: 0.130963  [139200/175341]\n",
      "loss: 0.766781  [140800/175341]\n",
      "loss: 0.725532  [142400/175341]\n",
      "loss: 0.468742  [144000/175341]\n",
      "loss: 0.296964  [145600/175341]\n",
      "loss: 0.431739  [147200/175341]\n",
      "loss: 0.449182  [148800/175341]\n",
      "loss: 0.527489  [150400/175341]\n",
      "loss: 0.639098  [152000/175341]\n",
      "loss: 0.880018  [153600/175341]\n",
      "loss: 0.621359  [155200/175341]\n",
      "loss: 0.148048  [156800/175341]\n",
      "loss: 0.809898  [158400/175341]\n",
      "loss: 0.482386  [160000/175341]\n",
      "loss: 0.261707  [161600/175341]\n",
      "loss: 0.339236  [163200/175341]\n",
      "loss: 0.353495  [164800/175341]\n",
      "loss: 0.539148  [166400/175341]\n",
      "loss: 0.192282  [168000/175341]\n",
      "loss: 0.548755  [169600/175341]\n",
      "loss: 0.382755  [171200/175341]\n",
      "loss: 0.242246  [172800/175341]\n",
      "loss: 0.273530  [174400/175341]\n",
      "Train Accuracy: 81.2594%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.546683, F1-score: 76.47%, Macro_F1-Score:  43.09%  \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.136027  [    0/175341]\n",
      "loss: 0.769603  [ 1600/175341]\n",
      "loss: 0.292203  [ 3200/175341]\n",
      "loss: 0.487998  [ 4800/175341]\n",
      "loss: 0.410504  [ 6400/175341]\n",
      "loss: 0.681148  [ 8000/175341]\n",
      "loss: 0.154880  [ 9600/175341]\n",
      "loss: 0.422845  [11200/175341]\n",
      "loss: 0.402445  [12800/175341]\n",
      "loss: 0.663623  [14400/175341]\n",
      "loss: 0.153226  [16000/175341]\n",
      "loss: 0.611823  [17600/175341]\n",
      "loss: 0.535384  [19200/175341]\n",
      "loss: 0.345852  [20800/175341]\n",
      "loss: 0.310450  [22400/175341]\n",
      "loss: 0.218474  [24000/175341]\n",
      "loss: 0.438015  [25600/175341]\n",
      "loss: 0.578210  [27200/175341]\n",
      "loss: 0.211248  [28800/175341]\n",
      "loss: 0.307378  [30400/175341]\n",
      "loss: 0.522478  [32000/175341]\n",
      "loss: 0.305197  [33600/175341]\n",
      "loss: 0.192079  [35200/175341]\n",
      "loss: 0.427681  [36800/175341]\n",
      "loss: 0.533874  [38400/175341]\n",
      "loss: 0.348623  [40000/175341]\n",
      "loss: 0.208836  [41600/175341]\n",
      "loss: 0.587382  [43200/175341]\n",
      "loss: 0.350828  [44800/175341]\n",
      "loss: 0.404763  [46400/175341]\n",
      "loss: 0.803274  [48000/175341]\n",
      "loss: 0.588432  [49600/175341]\n",
      "loss: 0.207831  [51200/175341]\n",
      "loss: 0.292635  [52800/175341]\n",
      "loss: 0.385951  [54400/175341]\n",
      "loss: 0.265335  [56000/175341]\n",
      "loss: 0.371725  [57600/175341]\n",
      "loss: 0.637246  [59200/175341]\n",
      "loss: 0.620702  [60800/175341]\n",
      "loss: 0.447500  [62400/175341]\n",
      "loss: 0.302681  [64000/175341]\n",
      "loss: 0.317963  [65600/175341]\n",
      "loss: 0.371277  [67200/175341]\n",
      "loss: 0.343307  [68800/175341]\n",
      "loss: 0.710791  [70400/175341]\n",
      "loss: 0.686161  [72000/175341]\n",
      "loss: 0.134728  [73600/175341]\n",
      "loss: 0.342807  [75200/175341]\n",
      "loss: 0.709221  [76800/175341]\n",
      "loss: 0.440198  [78400/175341]\n",
      "loss: 0.590012  [80000/175341]\n",
      "loss: 0.427343  [81600/175341]\n",
      "loss: 0.360552  [83200/175341]\n",
      "loss: 0.232657  [84800/175341]\n",
      "loss: 0.485040  [86400/175341]\n",
      "loss: 0.265930  [88000/175341]\n",
      "loss: 0.563791  [89600/175341]\n",
      "loss: 0.640407  [91200/175341]\n",
      "loss: 0.224165  [92800/175341]\n",
      "loss: 0.939230  [94400/175341]\n",
      "loss: 0.480447  [96000/175341]\n",
      "loss: 0.553352  [97600/175341]\n",
      "loss: 0.258181  [99200/175341]\n",
      "loss: 0.175103  [100800/175341]\n",
      "loss: 0.825885  [102400/175341]\n",
      "loss: 0.359664  [104000/175341]\n",
      "loss: 0.392346  [105600/175341]\n",
      "loss: 0.372334  [107200/175341]\n",
      "loss: 0.385189  [108800/175341]\n",
      "loss: 0.314756  [110400/175341]\n",
      "loss: 0.448811  [112000/175341]\n",
      "loss: 0.595518  [113600/175341]\n",
      "loss: 0.615431  [115200/175341]\n",
      "loss: 0.508276  [116800/175341]\n",
      "loss: 0.178369  [118400/175341]\n",
      "loss: 0.549342  [120000/175341]\n",
      "loss: 0.451978  [121600/175341]\n",
      "loss: 0.608538  [123200/175341]\n",
      "loss: 0.818297  [124800/175341]\n",
      "loss: 0.145313  [126400/175341]\n",
      "loss: 0.325826  [128000/175341]\n",
      "loss: 0.873686  [129600/175341]\n",
      "loss: 0.173649  [131200/175341]\n",
      "loss: 0.094675  [132800/175341]\n",
      "loss: 0.349000  [134400/175341]\n",
      "loss: 0.478590  [136000/175341]\n",
      "loss: 0.251495  [137600/175341]\n",
      "loss: 0.753573  [139200/175341]\n",
      "loss: 0.739798  [140800/175341]\n",
      "loss: 0.388376  [142400/175341]\n",
      "loss: 0.607957  [144000/175341]\n",
      "loss: 0.328624  [145600/175341]\n",
      "loss: 1.169622  [147200/175341]\n",
      "loss: 0.422841  [148800/175341]\n",
      "loss: 0.613160  [150400/175341]\n",
      "loss: 0.192940  [152000/175341]\n",
      "loss: 0.367885  [153600/175341]\n",
      "loss: 0.228738  [155200/175341]\n",
      "loss: 0.487959  [156800/175341]\n",
      "loss: 0.116812  [158400/175341]\n",
      "loss: 0.226233  [160000/175341]\n",
      "loss: 0.733864  [161600/175341]\n",
      "loss: 0.181711  [163200/175341]\n",
      "loss: 0.590760  [164800/175341]\n",
      "loss: 0.476437  [166400/175341]\n",
      "loss: 0.561127  [168000/175341]\n",
      "loss: 0.219003  [169600/175341]\n",
      "loss: 0.223156  [171200/175341]\n",
      "loss: 0.740655  [172800/175341]\n",
      "loss: 0.571622  [174400/175341]\n",
      "Train Accuracy: 81.2662%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.533527, F1-score: 76.81%, Macro_F1-Score:  42.45%  \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.411651  [    0/175341]\n",
      "loss: 0.334101  [ 1600/175341]\n",
      "loss: 0.401808  [ 3200/175341]\n",
      "loss: 0.431745  [ 4800/175341]\n",
      "loss: 0.262901  [ 6400/175341]\n",
      "loss: 0.601351  [ 8000/175341]\n",
      "loss: 0.291628  [ 9600/175341]\n",
      "loss: 0.979380  [11200/175341]\n",
      "loss: 0.114068  [12800/175341]\n",
      "loss: 0.537871  [14400/175341]\n",
      "loss: 0.092120  [16000/175341]\n",
      "loss: 0.720223  [17600/175341]\n",
      "loss: 0.719633  [19200/175341]\n",
      "loss: 0.433328  [20800/175341]\n",
      "loss: 0.413127  [22400/175341]\n",
      "loss: 0.356033  [24000/175341]\n",
      "loss: 0.655217  [25600/175341]\n",
      "loss: 0.278638  [27200/175341]\n",
      "loss: 0.519907  [28800/175341]\n",
      "loss: 0.413413  [30400/175341]\n",
      "loss: 0.490236  [32000/175341]\n",
      "loss: 0.424695  [33600/175341]\n",
      "loss: 0.427266  [35200/175341]\n",
      "loss: 0.710211  [36800/175341]\n",
      "loss: 0.695860  [38400/175341]\n",
      "loss: 0.381507  [40000/175341]\n",
      "loss: 0.595727  [41600/175341]\n",
      "loss: 0.686370  [43200/175341]\n",
      "loss: 0.359623  [44800/175341]\n",
      "loss: 0.759476  [46400/175341]\n",
      "loss: 1.148730  [48000/175341]\n",
      "loss: 0.467298  [49600/175341]\n",
      "loss: 0.746949  [51200/175341]\n",
      "loss: 0.592173  [52800/175341]\n",
      "loss: 0.696810  [54400/175341]\n",
      "loss: 0.384413  [56000/175341]\n",
      "loss: 0.863925  [57600/175341]\n",
      "loss: 0.968192  [59200/175341]\n",
      "loss: 0.490116  [60800/175341]\n",
      "loss: 0.584590  [62400/175341]\n",
      "loss: 0.538235  [64000/175341]\n",
      "loss: 0.268401  [65600/175341]\n",
      "loss: 0.476929  [67200/175341]\n",
      "loss: 0.428772  [68800/175341]\n",
      "loss: 0.284592  [70400/175341]\n",
      "loss: 0.553045  [72000/175341]\n",
      "loss: 0.231117  [73600/175341]\n",
      "loss: 0.436953  [75200/175341]\n",
      "loss: 0.446071  [76800/175341]\n",
      "loss: 0.355000  [78400/175341]\n",
      "loss: 0.477327  [80000/175341]\n",
      "loss: 0.612116  [81600/175341]\n",
      "loss: 0.520866  [83200/175341]\n",
      "loss: 0.828190  [84800/175341]\n",
      "loss: 0.290040  [86400/175341]\n",
      "loss: 0.501161  [88000/175341]\n",
      "loss: 0.466122  [89600/175341]\n",
      "loss: 0.603724  [91200/175341]\n",
      "loss: 0.472241  [92800/175341]\n",
      "loss: 0.959942  [94400/175341]\n",
      "loss: 0.266889  [96000/175341]\n",
      "loss: 0.269177  [97600/175341]\n",
      "loss: 0.527730  [99200/175341]\n",
      "loss: 0.425247  [100800/175341]\n",
      "loss: 0.356664  [102400/175341]\n",
      "loss: 1.022631  [104000/175341]\n",
      "loss: 0.535971  [105600/175341]\n",
      "loss: 0.386810  [107200/175341]\n",
      "loss: 0.593132  [108800/175341]\n",
      "loss: 0.588387  [110400/175341]\n",
      "loss: 0.438515  [112000/175341]\n",
      "loss: 0.561763  [113600/175341]\n",
      "loss: 0.294247  [115200/175341]\n",
      "loss: 0.565093  [116800/175341]\n",
      "loss: 0.465933  [118400/175341]\n",
      "loss: 0.660526  [120000/175341]\n",
      "loss: 0.304753  [121600/175341]\n",
      "loss: 0.655601  [123200/175341]\n",
      "loss: 0.362945  [124800/175341]\n",
      "loss: 0.795053  [126400/175341]\n",
      "loss: 0.395578  [128000/175341]\n",
      "loss: 0.198000  [129600/175341]\n",
      "loss: 0.639257  [131200/175341]\n",
      "loss: 0.227693  [132800/175341]\n",
      "loss: 0.553384  [134400/175341]\n",
      "loss: 0.705516  [136000/175341]\n",
      "loss: 0.461638  [137600/175341]\n",
      "loss: 0.545266  [139200/175341]\n",
      "loss: 0.381211  [140800/175341]\n",
      "loss: 0.322959  [142400/175341]\n",
      "loss: 0.566806  [144000/175341]\n",
      "loss: 0.181561  [145600/175341]\n",
      "loss: 0.590122  [147200/175341]\n",
      "loss: 0.178962  [148800/175341]\n",
      "loss: 0.708373  [150400/175341]\n",
      "loss: 0.845547  [152000/175341]\n",
      "loss: 0.457438  [153600/175341]\n",
      "loss: 0.588220  [155200/175341]\n",
      "loss: 0.213144  [156800/175341]\n",
      "loss: 1.085411  [158400/175341]\n",
      "loss: 0.872443  [160000/175341]\n",
      "loss: 0.587282  [161600/175341]\n",
      "loss: 0.289110  [163200/175341]\n",
      "loss: 0.744976  [164800/175341]\n",
      "loss: 0.312662  [166400/175341]\n",
      "loss: 0.218483  [168000/175341]\n",
      "loss: 0.464872  [169600/175341]\n",
      "loss: 0.296879  [171200/175341]\n",
      "loss: 0.604912  [172800/175341]\n",
      "loss: 0.506718  [174400/175341]\n",
      "Train Accuracy: 81.2240%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.539271, F1-score: 76.81%, Macro_F1-Score:  43.05%  \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.161148  [    0/175341]\n",
      "loss: 0.832964  [ 1600/175341]\n",
      "loss: 0.108938  [ 3200/175341]\n",
      "loss: 0.325898  [ 4800/175341]\n",
      "loss: 0.689946  [ 6400/175341]\n",
      "loss: 1.085160  [ 8000/175341]\n",
      "loss: 0.166880  [ 9600/175341]\n",
      "loss: 0.607594  [11200/175341]\n",
      "loss: 0.324843  [12800/175341]\n",
      "loss: 0.345590  [14400/175341]\n",
      "loss: 0.741980  [16000/175341]\n",
      "loss: 1.077313  [17600/175341]\n",
      "loss: 0.181771  [19200/175341]\n",
      "loss: 0.247050  [20800/175341]\n",
      "loss: 0.526232  [22400/175341]\n",
      "loss: 0.262292  [24000/175341]\n",
      "loss: 0.479512  [25600/175341]\n",
      "loss: 0.258459  [27200/175341]\n",
      "loss: 0.299271  [28800/175341]\n",
      "loss: 0.826128  [30400/175341]\n",
      "loss: 0.644751  [32000/175341]\n",
      "loss: 0.469472  [33600/175341]\n",
      "loss: 0.678353  [35200/175341]\n",
      "loss: 0.442369  [36800/175341]\n",
      "loss: 0.416769  [38400/175341]\n",
      "loss: 0.380519  [40000/175341]\n",
      "loss: 0.668188  [41600/175341]\n",
      "loss: 0.277545  [43200/175341]\n",
      "loss: 0.483692  [44800/175341]\n",
      "loss: 0.390233  [46400/175341]\n",
      "loss: 0.269222  [48000/175341]\n",
      "loss: 0.807249  [49600/175341]\n",
      "loss: 0.631047  [51200/175341]\n",
      "loss: 0.191932  [52800/175341]\n",
      "loss: 0.185576  [54400/175341]\n",
      "loss: 0.487368  [56000/175341]\n",
      "loss: 0.557710  [57600/175341]\n",
      "loss: 0.194411  [59200/175341]\n",
      "loss: 0.516792  [60800/175341]\n",
      "loss: 0.349451  [62400/175341]\n",
      "loss: 0.522235  [64000/175341]\n",
      "loss: 0.642577  [65600/175341]\n",
      "loss: 0.523997  [67200/175341]\n",
      "loss: 0.454290  [68800/175341]\n",
      "loss: 0.523414  [70400/175341]\n",
      "loss: 0.181685  [72000/175341]\n",
      "loss: 0.659331  [73600/175341]\n",
      "loss: 0.671543  [75200/175341]\n",
      "loss: 0.727532  [76800/175341]\n",
      "loss: 0.218147  [78400/175341]\n",
      "loss: 0.161866  [80000/175341]\n",
      "loss: 0.268452  [81600/175341]\n",
      "loss: 0.315922  [83200/175341]\n",
      "loss: 0.325333  [84800/175341]\n",
      "loss: 0.408748  [86400/175341]\n",
      "loss: 0.532551  [88000/175341]\n",
      "loss: 0.330791  [89600/175341]\n",
      "loss: 0.255335  [91200/175341]\n",
      "loss: 0.146801  [92800/175341]\n",
      "loss: 0.572532  [94400/175341]\n",
      "loss: 0.355118  [96000/175341]\n",
      "loss: 0.394519  [97600/175341]\n",
      "loss: 0.198955  [99200/175341]\n",
      "loss: 0.875527  [100800/175341]\n",
      "loss: 0.821136  [102400/175341]\n",
      "loss: 0.470148  [104000/175341]\n",
      "loss: 0.254741  [105600/175341]\n",
      "loss: 0.379395  [107200/175341]\n",
      "loss: 0.554104  [108800/175341]\n",
      "loss: 0.431346  [110400/175341]\n",
      "loss: 0.355288  [112000/175341]\n",
      "loss: 0.521022  [113600/175341]\n",
      "loss: 0.161842  [115200/175341]\n",
      "loss: 0.442436  [116800/175341]\n",
      "loss: 0.536602  [118400/175341]\n",
      "loss: 0.578085  [120000/175341]\n",
      "loss: 0.681537  [121600/175341]\n",
      "loss: 0.567920  [123200/175341]\n",
      "loss: 0.232444  [124800/175341]\n",
      "loss: 0.577029  [126400/175341]\n",
      "loss: 0.695616  [128000/175341]\n",
      "loss: 0.232232  [129600/175341]\n",
      "loss: 0.541901  [131200/175341]\n",
      "loss: 0.593964  [132800/175341]\n",
      "loss: 0.444219  [134400/175341]\n",
      "loss: 0.841934  [136000/175341]\n",
      "loss: 0.376653  [137600/175341]\n",
      "loss: 0.370001  [139200/175341]\n",
      "loss: 0.666675  [140800/175341]\n",
      "loss: 0.208150  [142400/175341]\n",
      "loss: 0.306729  [144000/175341]\n",
      "loss: 0.281909  [145600/175341]\n",
      "loss: 0.444284  [147200/175341]\n",
      "loss: 0.740476  [148800/175341]\n",
      "loss: 0.518079  [150400/175341]\n",
      "loss: 0.606101  [152000/175341]\n",
      "loss: 0.311131  [153600/175341]\n",
      "loss: 0.421419  [155200/175341]\n",
      "loss: 0.551063  [156800/175341]\n",
      "loss: 0.349636  [158400/175341]\n",
      "loss: 0.456446  [160000/175341]\n",
      "loss: 0.528883  [161600/175341]\n",
      "loss: 0.698245  [163200/175341]\n",
      "loss: 0.361047  [164800/175341]\n",
      "loss: 0.651602  [166400/175341]\n",
      "loss: 0.542222  [168000/175341]\n",
      "loss: 0.656979  [169600/175341]\n",
      "loss: 0.510611  [171200/175341]\n",
      "loss: 0.338520  [172800/175341]\n",
      "loss: 0.433742  [174400/175341]\n",
      "Train Accuracy: 81.2907%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.545938, F1-score: 76.28%, Macro_F1-Score:  42.41%  \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.375677  [    0/175341]\n",
      "loss: 0.625189  [ 1600/175341]\n",
      "loss: 0.248652  [ 3200/175341]\n",
      "loss: 0.408189  [ 4800/175341]\n",
      "loss: 0.438906  [ 6400/175341]\n",
      "loss: 0.263829  [ 8000/175341]\n",
      "loss: 0.541879  [ 9600/175341]\n",
      "loss: 0.275521  [11200/175341]\n",
      "loss: 0.533398  [12800/175341]\n",
      "loss: 0.289451  [14400/175341]\n",
      "loss: 0.357388  [16000/175341]\n",
      "loss: 0.821612  [17600/175341]\n",
      "loss: 0.223606  [19200/175341]\n",
      "loss: 0.380182  [20800/175341]\n",
      "loss: 0.402439  [22400/175341]\n",
      "loss: 0.529023  [24000/175341]\n",
      "loss: 0.099254  [25600/175341]\n",
      "loss: 0.432582  [27200/175341]\n",
      "loss: 0.470063  [28800/175341]\n",
      "loss: 0.378802  [30400/175341]\n",
      "loss: 0.274615  [32000/175341]\n",
      "loss: 0.484880  [33600/175341]\n",
      "loss: 0.182465  [35200/175341]\n",
      "loss: 0.184630  [36800/175341]\n",
      "loss: 0.697311  [38400/175341]\n",
      "loss: 0.265271  [40000/175341]\n",
      "loss: 0.331159  [41600/175341]\n",
      "loss: 0.212562  [43200/175341]\n",
      "loss: 0.309626  [44800/175341]\n",
      "loss: 0.493051  [46400/175341]\n",
      "loss: 0.491359  [48000/175341]\n",
      "loss: 0.464193  [49600/175341]\n",
      "loss: 0.737114  [51200/175341]\n",
      "loss: 0.263222  [52800/175341]\n",
      "loss: 0.413589  [54400/175341]\n",
      "loss: 0.549978  [56000/175341]\n",
      "loss: 0.533239  [57600/175341]\n",
      "loss: 0.372256  [59200/175341]\n",
      "loss: 1.312701  [60800/175341]\n",
      "loss: 0.650809  [62400/175341]\n",
      "loss: 0.254961  [64000/175341]\n",
      "loss: 0.345327  [65600/175341]\n",
      "loss: 0.217273  [67200/175341]\n",
      "loss: 0.500480  [68800/175341]\n",
      "loss: 0.242804  [70400/175341]\n",
      "loss: 0.699114  [72000/175341]\n",
      "loss: 0.555352  [73600/175341]\n",
      "loss: 0.743369  [75200/175341]\n",
      "loss: 0.624196  [76800/175341]\n",
      "loss: 0.509559  [78400/175341]\n",
      "loss: 0.549585  [80000/175341]\n",
      "loss: 0.551275  [81600/175341]\n",
      "loss: 0.723266  [83200/175341]\n",
      "loss: 0.273821  [84800/175341]\n",
      "loss: 0.184434  [86400/175341]\n",
      "loss: 0.606258  [88000/175341]\n",
      "loss: 0.831118  [89600/175341]\n",
      "loss: 0.576402  [91200/175341]\n",
      "loss: 0.702309  [92800/175341]\n",
      "loss: 0.710134  [94400/175341]\n",
      "loss: 0.529723  [96000/175341]\n",
      "loss: 0.402879  [97600/175341]\n",
      "loss: 0.487682  [99200/175341]\n",
      "loss: 0.800828  [100800/175341]\n",
      "loss: 0.548621  [102400/175341]\n",
      "loss: 0.715636  [104000/175341]\n",
      "loss: 0.446766  [105600/175341]\n",
      "loss: 0.131558  [107200/175341]\n",
      "loss: 0.325164  [108800/175341]\n",
      "loss: 0.540766  [110400/175341]\n",
      "loss: 0.534366  [112000/175341]\n",
      "loss: 0.283892  [113600/175341]\n",
      "loss: 0.316598  [115200/175341]\n",
      "loss: 0.259425  [116800/175341]\n",
      "loss: 0.405483  [118400/175341]\n",
      "loss: 0.399508  [120000/175341]\n",
      "loss: 0.073278  [121600/175341]\n",
      "loss: 0.634634  [123200/175341]\n",
      "loss: 0.420300  [124800/175341]\n",
      "loss: 0.447739  [126400/175341]\n",
      "loss: 0.582715  [128000/175341]\n",
      "loss: 0.701267  [129600/175341]\n",
      "loss: 0.469565  [131200/175341]\n",
      "loss: 0.434369  [132800/175341]\n",
      "loss: 0.476382  [134400/175341]\n",
      "loss: 0.770052  [136000/175341]\n",
      "loss: 0.523623  [137600/175341]\n",
      "loss: 0.635394  [139200/175341]\n",
      "loss: 0.298721  [140800/175341]\n",
      "loss: 0.231986  [142400/175341]\n",
      "loss: 0.682635  [144000/175341]\n",
      "loss: 0.220598  [145600/175341]\n",
      "loss: 0.885192  [147200/175341]\n",
      "loss: 0.485532  [148800/175341]\n",
      "loss: 0.295306  [150400/175341]\n",
      "loss: 0.735715  [152000/175341]\n",
      "loss: 0.492638  [153600/175341]\n",
      "loss: 0.675393  [155200/175341]\n",
      "loss: 0.383202  [156800/175341]\n",
      "loss: 0.509979  [158400/175341]\n",
      "loss: 1.086745  [160000/175341]\n",
      "loss: 0.350206  [161600/175341]\n",
      "loss: 0.126393  [163200/175341]\n",
      "loss: 0.609133  [164800/175341]\n",
      "loss: 0.587193  [166400/175341]\n",
      "loss: 0.449579  [168000/175341]\n",
      "loss: 0.570201  [169600/175341]\n",
      "loss: 0.218742  [171200/175341]\n",
      "loss: 0.889634  [172800/175341]\n",
      "loss: 0.611756  [174400/175341]\n",
      "Train Accuracy: 81.2702%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.538595, F1-score: 76.02%, Macro_F1-Score:  42.32%  \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.165046  [    0/175341]\n",
      "loss: 0.222905  [ 1600/175341]\n",
      "loss: 0.733192  [ 3200/175341]\n",
      "loss: 0.500998  [ 4800/175341]\n",
      "loss: 0.365532  [ 6400/175341]\n",
      "loss: 0.283688  [ 8000/175341]\n",
      "loss: 1.019104  [ 9600/175341]\n",
      "loss: 0.959229  [11200/175341]\n",
      "loss: 0.190084  [12800/175341]\n",
      "loss: 0.120356  [14400/175341]\n",
      "loss: 0.366269  [16000/175341]\n",
      "loss: 0.378702  [17600/175341]\n",
      "loss: 0.583181  [19200/175341]\n",
      "loss: 0.985943  [20800/175341]\n",
      "loss: 0.250769  [22400/175341]\n",
      "loss: 0.753071  [24000/175341]\n",
      "loss: 0.425041  [25600/175341]\n",
      "loss: 0.228394  [27200/175341]\n",
      "loss: 0.254399  [28800/175341]\n",
      "loss: 0.603355  [30400/175341]\n",
      "loss: 0.614349  [32000/175341]\n",
      "loss: 0.187013  [33600/175341]\n",
      "loss: 0.268563  [35200/175341]\n",
      "loss: 0.813645  [36800/175341]\n",
      "loss: 0.733879  [38400/175341]\n",
      "loss: 0.323318  [40000/175341]\n",
      "loss: 0.318164  [41600/175341]\n",
      "loss: 0.552569  [43200/175341]\n",
      "loss: 0.408402  [44800/175341]\n",
      "loss: 0.276597  [46400/175341]\n",
      "loss: 0.445181  [48000/175341]\n",
      "loss: 0.595404  [49600/175341]\n",
      "loss: 0.376539  [51200/175341]\n",
      "loss: 0.352673  [52800/175341]\n",
      "loss: 0.801179  [54400/175341]\n",
      "loss: 0.541740  [56000/175341]\n",
      "loss: 0.677754  [57600/175341]\n",
      "loss: 0.445411  [59200/175341]\n",
      "loss: 0.113478  [60800/175341]\n",
      "loss: 0.630936  [62400/175341]\n",
      "loss: 0.734036  [64000/175341]\n",
      "loss: 0.768448  [65600/175341]\n",
      "loss: 0.824289  [67200/175341]\n",
      "loss: 0.393992  [68800/175341]\n",
      "loss: 0.591478  [70400/175341]\n",
      "loss: 0.726116  [72000/175341]\n",
      "loss: 0.363764  [73600/175341]\n",
      "loss: 0.342631  [75200/175341]\n",
      "loss: 0.246842  [76800/175341]\n",
      "loss: 0.324087  [78400/175341]\n",
      "loss: 0.519047  [80000/175341]\n",
      "loss: 0.377794  [81600/175341]\n",
      "loss: 0.311238  [83200/175341]\n",
      "loss: 0.534184  [84800/175341]\n",
      "loss: 0.334126  [86400/175341]\n",
      "loss: 0.496787  [88000/175341]\n",
      "loss: 0.468430  [89600/175341]\n",
      "loss: 0.739488  [91200/175341]\n",
      "loss: 0.636796  [92800/175341]\n",
      "loss: 0.820800  [94400/175341]\n",
      "loss: 0.695760  [96000/175341]\n",
      "loss: 0.530040  [97600/175341]\n",
      "loss: 0.695822  [99200/175341]\n",
      "loss: 0.295890  [100800/175341]\n",
      "loss: 0.507438  [102400/175341]\n",
      "loss: 0.442261  [104000/175341]\n",
      "loss: 0.555883  [105600/175341]\n",
      "loss: 0.651520  [107200/175341]\n",
      "loss: 0.290640  [108800/175341]\n",
      "loss: 0.503842  [110400/175341]\n",
      "loss: 0.237457  [112000/175341]\n",
      "loss: 0.634793  [113600/175341]\n",
      "loss: 0.261085  [115200/175341]\n",
      "loss: 0.672623  [116800/175341]\n",
      "loss: 0.228071  [118400/175341]\n",
      "loss: 0.498356  [120000/175341]\n",
      "loss: 0.521017  [121600/175341]\n",
      "loss: 0.351929  [123200/175341]\n",
      "loss: 0.031404  [124800/175341]\n",
      "loss: 0.447859  [126400/175341]\n",
      "loss: 0.430413  [128000/175341]\n",
      "loss: 0.333864  [129600/175341]\n",
      "loss: 0.910138  [131200/175341]\n",
      "loss: 0.412296  [132800/175341]\n",
      "loss: 0.327529  [134400/175341]\n",
      "loss: 0.651892  [136000/175341]\n",
      "loss: 0.366821  [137600/175341]\n",
      "loss: 0.344678  [139200/175341]\n",
      "loss: 0.593909  [140800/175341]\n",
      "loss: 0.784212  [142400/175341]\n",
      "loss: 0.656682  [144000/175341]\n",
      "loss: 0.443722  [145600/175341]\n",
      "loss: 0.294779  [147200/175341]\n",
      "loss: 0.357070  [148800/175341]\n",
      "loss: 0.496002  [150400/175341]\n",
      "loss: 0.110635  [152000/175341]\n",
      "loss: 0.343555  [153600/175341]\n",
      "loss: 0.515384  [155200/175341]\n",
      "loss: 0.685159  [156800/175341]\n",
      "loss: 0.777355  [158400/175341]\n",
      "loss: 0.460625  [160000/175341]\n",
      "loss: 0.593286  [161600/175341]\n",
      "loss: 0.586155  [163200/175341]\n",
      "loss: 0.331398  [164800/175341]\n",
      "loss: 0.626196  [166400/175341]\n",
      "loss: 0.497679  [168000/175341]\n",
      "loss: 0.509889  [169600/175341]\n",
      "loss: 0.227075  [171200/175341]\n",
      "loss: 0.231928  [172800/175341]\n",
      "loss: 0.552921  [174400/175341]\n",
      "Train Accuracy: 81.3016%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.536334, F1-score: 76.20%, Macro_F1-Score:  42.06%  \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.424466  [    0/175341]\n",
      "loss: 0.532463  [ 1600/175341]\n",
      "loss: 0.174944  [ 3200/175341]\n",
      "loss: 0.696403  [ 4800/175341]\n",
      "loss: 0.495597  [ 6400/175341]\n",
      "loss: 0.718321  [ 8000/175341]\n",
      "loss: 0.687483  [ 9600/175341]\n",
      "loss: 0.542049  [11200/175341]\n",
      "loss: 0.159920  [12800/175341]\n",
      "loss: 0.338258  [14400/175341]\n",
      "loss: 0.268809  [16000/175341]\n",
      "loss: 0.281562  [17600/175341]\n",
      "loss: 0.211288  [19200/175341]\n",
      "loss: 0.236776  [20800/175341]\n",
      "loss: 0.457357  [22400/175341]\n",
      "loss: 0.356797  [24000/175341]\n",
      "loss: 0.590754  [25600/175341]\n",
      "loss: 0.720549  [27200/175341]\n",
      "loss: 0.327508  [28800/175341]\n",
      "loss: 0.235316  [30400/175341]\n",
      "loss: 0.154720  [32000/175341]\n",
      "loss: 0.283489  [33600/175341]\n",
      "loss: 0.243515  [35200/175341]\n",
      "loss: 0.289890  [36800/175341]\n",
      "loss: 0.555801  [38400/175341]\n",
      "loss: 0.966451  [40000/175341]\n",
      "loss: 0.456167  [41600/175341]\n",
      "loss: 0.307503  [43200/175341]\n",
      "loss: 0.365997  [44800/175341]\n",
      "loss: 0.355037  [46400/175341]\n",
      "loss: 0.688268  [48000/175341]\n",
      "loss: 0.443742  [49600/175341]\n",
      "loss: 0.193776  [51200/175341]\n",
      "loss: 0.497931  [52800/175341]\n",
      "loss: 0.674968  [54400/175341]\n",
      "loss: 0.613371  [56000/175341]\n",
      "loss: 0.395517  [57600/175341]\n",
      "loss: 0.466302  [59200/175341]\n",
      "loss: 0.294641  [60800/175341]\n",
      "loss: 0.456695  [62400/175341]\n",
      "loss: 0.410591  [64000/175341]\n",
      "loss: 0.349433  [65600/175341]\n",
      "loss: 0.347725  [67200/175341]\n",
      "loss: 0.481796  [68800/175341]\n",
      "loss: 0.214974  [70400/175341]\n",
      "loss: 0.408332  [72000/175341]\n",
      "loss: 0.352453  [73600/175341]\n",
      "loss: 0.530329  [75200/175341]\n",
      "loss: 0.466043  [76800/175341]\n",
      "loss: 0.167957  [78400/175341]\n",
      "loss: 0.638592  [80000/175341]\n",
      "loss: 0.362679  [81600/175341]\n",
      "loss: 0.254929  [83200/175341]\n",
      "loss: 0.292571  [84800/175341]\n",
      "loss: 0.304564  [86400/175341]\n",
      "loss: 0.279420  [88000/175341]\n",
      "loss: 0.585096  [89600/175341]\n",
      "loss: 0.258717  [91200/175341]\n",
      "loss: 0.593677  [92800/175341]\n",
      "loss: 0.338951  [94400/175341]\n",
      "loss: 0.291941  [96000/175341]\n",
      "loss: 0.318619  [97600/175341]\n",
      "loss: 0.312100  [99200/175341]\n",
      "loss: 0.689255  [100800/175341]\n",
      "loss: 0.229670  [102400/175341]\n",
      "loss: 0.585646  [104000/175341]\n",
      "loss: 0.548774  [105600/175341]\n",
      "loss: 0.408197  [107200/175341]\n",
      "loss: 0.306172  [108800/175341]\n",
      "loss: 0.236657  [110400/175341]\n",
      "loss: 0.541214  [112000/175341]\n",
      "loss: 0.340862  [113600/175341]\n",
      "loss: 0.793951  [115200/175341]\n",
      "loss: 0.560348  [116800/175341]\n",
      "loss: 0.772150  [118400/175341]\n",
      "loss: 0.512246  [120000/175341]\n",
      "loss: 0.168415  [121600/175341]\n",
      "loss: 0.533667  [123200/175341]\n",
      "loss: 0.424856  [124800/175341]\n",
      "loss: 0.734901  [126400/175341]\n",
      "loss: 0.197359  [128000/175341]\n",
      "loss: 0.470499  [129600/175341]\n",
      "loss: 0.542858  [131200/175341]\n",
      "loss: 0.216729  [132800/175341]\n",
      "loss: 0.539198  [134400/175341]\n",
      "loss: 0.512659  [136000/175341]\n",
      "loss: 0.544774  [137600/175341]\n",
      "loss: 0.305436  [139200/175341]\n",
      "loss: 0.517891  [140800/175341]\n",
      "loss: 0.388494  [142400/175341]\n",
      "loss: 0.296220  [144000/175341]\n",
      "loss: 0.594049  [145600/175341]\n",
      "loss: 0.408356  [147200/175341]\n",
      "loss: 0.437624  [148800/175341]\n",
      "loss: 0.291244  [150400/175341]\n",
      "loss: 0.448839  [152000/175341]\n",
      "loss: 0.780653  [153600/175341]\n",
      "loss: 0.600068  [155200/175341]\n",
      "loss: 0.567048  [156800/175341]\n",
      "loss: 0.869715  [158400/175341]\n",
      "loss: 0.545094  [160000/175341]\n",
      "loss: 0.416732  [161600/175341]\n",
      "loss: 0.952058  [163200/175341]\n",
      "loss: 0.556827  [164800/175341]\n",
      "loss: 0.990259  [166400/175341]\n",
      "loss: 0.052821  [168000/175341]\n",
      "loss: 0.491393  [169600/175341]\n",
      "loss: 0.269638  [171200/175341]\n",
      "loss: 0.762392  [172800/175341]\n",
      "loss: 0.347476  [174400/175341]\n",
      "Train Accuracy: 81.2862%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.533031, F1-score: 76.98%, Macro_F1-Score:  43.33%  \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.657311  [    0/175341]\n",
      "loss: 0.505078  [ 1600/175341]\n",
      "loss: 0.335652  [ 3200/175341]\n",
      "loss: 0.483629  [ 4800/175341]\n",
      "loss: 0.796926  [ 6400/175341]\n",
      "loss: 0.748407  [ 8000/175341]\n",
      "loss: 0.240465  [ 9600/175341]\n",
      "loss: 0.550132  [11200/175341]\n",
      "loss: 0.408518  [12800/175341]\n",
      "loss: 0.725630  [14400/175341]\n",
      "loss: 0.520114  [16000/175341]\n",
      "loss: 0.634442  [17600/175341]\n",
      "loss: 0.604144  [19200/175341]\n",
      "loss: 0.479894  [20800/175341]\n",
      "loss: 0.580891  [22400/175341]\n",
      "loss: 0.245294  [24000/175341]\n",
      "loss: 0.595640  [25600/175341]\n",
      "loss: 0.428217  [27200/175341]\n",
      "loss: 0.317002  [28800/175341]\n",
      "loss: 0.426568  [30400/175341]\n",
      "loss: 0.832851  [32000/175341]\n",
      "loss: 0.453324  [33600/175341]\n",
      "loss: 0.483968  [35200/175341]\n",
      "loss: 0.445595  [36800/175341]\n",
      "loss: 0.575257  [38400/175341]\n",
      "loss: 0.165571  [40000/175341]\n",
      "loss: 1.091923  [41600/175341]\n",
      "loss: 0.626601  [43200/175341]\n",
      "loss: 0.319777  [44800/175341]\n",
      "loss: 0.807462  [46400/175341]\n",
      "loss: 0.689241  [48000/175341]\n",
      "loss: 0.396239  [49600/175341]\n",
      "loss: 0.469581  [51200/175341]\n",
      "loss: 0.730821  [52800/175341]\n",
      "loss: 0.306656  [54400/175341]\n",
      "loss: 0.699514  [56000/175341]\n",
      "loss: 0.276326  [57600/175341]\n",
      "loss: 0.410918  [59200/175341]\n",
      "loss: 0.422535  [60800/175341]\n",
      "loss: 0.534613  [62400/175341]\n",
      "loss: 0.815156  [64000/175341]\n",
      "loss: 0.573599  [65600/175341]\n",
      "loss: 0.362470  [67200/175341]\n",
      "loss: 0.452824  [68800/175341]\n",
      "loss: 0.422777  [70400/175341]\n",
      "loss: 0.423301  [72000/175341]\n",
      "loss: 0.281684  [73600/175341]\n",
      "loss: 0.437929  [75200/175341]\n",
      "loss: 1.053793  [76800/175341]\n",
      "loss: 0.447686  [78400/175341]\n",
      "loss: 0.204283  [80000/175341]\n",
      "loss: 0.708501  [81600/175341]\n",
      "loss: 1.051786  [83200/175341]\n",
      "loss: 0.263256  [84800/175341]\n",
      "loss: 0.615116  [86400/175341]\n",
      "loss: 0.603786  [88000/175341]\n",
      "loss: 0.132575  [89600/175341]\n",
      "loss: 0.603793  [91200/175341]\n",
      "loss: 0.361094  [92800/175341]\n",
      "loss: 0.687961  [94400/175341]\n",
      "loss: 0.628242  [96000/175341]\n",
      "loss: 0.529893  [97600/175341]\n",
      "loss: 0.256809  [99200/175341]\n",
      "loss: 0.927247  [100800/175341]\n",
      "loss: 0.389484  [102400/175341]\n",
      "loss: 0.268300  [104000/175341]\n",
      "loss: 0.618635  [105600/175341]\n",
      "loss: 0.637605  [107200/175341]\n",
      "loss: 0.893365  [108800/175341]\n",
      "loss: 0.458669  [110400/175341]\n",
      "loss: 0.940279  [112000/175341]\n",
      "loss: 0.278017  [113600/175341]\n",
      "loss: 0.470039  [115200/175341]\n",
      "loss: 0.552204  [116800/175341]\n",
      "loss: 0.175056  [118400/175341]\n",
      "loss: 0.787056  [120000/175341]\n",
      "loss: 0.588567  [121600/175341]\n",
      "loss: 0.460760  [123200/175341]\n",
      "loss: 0.436362  [124800/175341]\n",
      "loss: 0.837374  [126400/175341]\n",
      "loss: 0.529684  [128000/175341]\n",
      "loss: 0.639274  [129600/175341]\n",
      "loss: 0.458703  [131200/175341]\n",
      "loss: 0.660126  [132800/175341]\n",
      "loss: 0.798513  [134400/175341]\n",
      "loss: 0.894488  [136000/175341]\n",
      "loss: 0.278127  [137600/175341]\n",
      "loss: 0.376682  [139200/175341]\n",
      "loss: 0.317179  [140800/175341]\n",
      "loss: 0.427237  [142400/175341]\n",
      "loss: 0.199211  [144000/175341]\n",
      "loss: 0.362596  [145600/175341]\n",
      "loss: 0.819955  [147200/175341]\n",
      "loss: 0.532117  [148800/175341]\n",
      "loss: 0.333509  [150400/175341]\n",
      "loss: 0.223118  [152000/175341]\n",
      "loss: 0.572778  [153600/175341]\n",
      "loss: 0.546206  [155200/175341]\n",
      "loss: 0.653402  [156800/175341]\n",
      "loss: 0.403259  [158400/175341]\n",
      "loss: 0.319663  [160000/175341]\n",
      "loss: 0.521447  [161600/175341]\n",
      "loss: 0.215133  [163200/175341]\n",
      "loss: 0.413906  [164800/175341]\n",
      "loss: 0.562673  [166400/175341]\n",
      "loss: 0.354940  [168000/175341]\n",
      "loss: 0.938104  [169600/175341]\n",
      "loss: 0.368747  [171200/175341]\n",
      "loss: 0.580027  [172800/175341]\n",
      "loss: 0.446292  [174400/175341]\n",
      "Train Accuracy: 81.2565%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.559618, F1-score: 75.48%, Macro_F1-Score:  42.60%  \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.488762  [    0/175341]\n",
      "loss: 0.307682  [ 1600/175341]\n",
      "loss: 0.441814  [ 3200/175341]\n",
      "loss: 0.595750  [ 4800/175341]\n",
      "loss: 1.211066  [ 6400/175341]\n",
      "loss: 0.393890  [ 8000/175341]\n",
      "loss: 0.341488  [ 9600/175341]\n",
      "loss: 0.411399  [11200/175341]\n",
      "loss: 0.543350  [12800/175341]\n",
      "loss: 0.802208  [14400/175341]\n",
      "loss: 0.387615  [16000/175341]\n",
      "loss: 0.631958  [17600/175341]\n",
      "loss: 0.476442  [19200/175341]\n",
      "loss: 0.681969  [20800/175341]\n",
      "loss: 0.387934  [22400/175341]\n",
      "loss: 0.726861  [24000/175341]\n",
      "loss: 0.747994  [25600/175341]\n",
      "loss: 0.195893  [27200/175341]\n",
      "loss: 0.790837  [28800/175341]\n",
      "loss: 0.468671  [30400/175341]\n",
      "loss: 0.737481  [32000/175341]\n",
      "loss: 0.454558  [33600/175341]\n",
      "loss: 0.755670  [35200/175341]\n",
      "loss: 0.718646  [36800/175341]\n",
      "loss: 0.518842  [38400/175341]\n",
      "loss: 0.567390  [40000/175341]\n",
      "loss: 0.274659  [41600/175341]\n",
      "loss: 0.660869  [43200/175341]\n",
      "loss: 0.183123  [44800/175341]\n",
      "loss: 0.232217  [46400/175341]\n",
      "loss: 0.693539  [48000/175341]\n",
      "loss: 1.037451  [49600/175341]\n",
      "loss: 0.507922  [51200/175341]\n",
      "loss: 0.157024  [52800/175341]\n",
      "loss: 0.718398  [54400/175341]\n",
      "loss: 0.318890  [56000/175341]\n",
      "loss: 0.128775  [57600/175341]\n",
      "loss: 0.321031  [59200/175341]\n",
      "loss: 0.245920  [60800/175341]\n",
      "loss: 0.521910  [62400/175341]\n",
      "loss: 0.403565  [64000/175341]\n",
      "loss: 0.551281  [65600/175341]\n",
      "loss: 0.674678  [67200/175341]\n",
      "loss: 0.206436  [68800/175341]\n",
      "loss: 0.600362  [70400/175341]\n",
      "loss: 0.739085  [72000/175341]\n",
      "loss: 0.583666  [73600/175341]\n",
      "loss: 0.782690  [75200/175341]\n",
      "loss: 0.589105  [76800/175341]\n",
      "loss: 0.602797  [78400/175341]\n",
      "loss: 0.455074  [80000/175341]\n",
      "loss: 0.421759  [81600/175341]\n",
      "loss: 0.361900  [83200/175341]\n",
      "loss: 0.633821  [84800/175341]\n",
      "loss: 0.419069  [86400/175341]\n",
      "loss: 0.463429  [88000/175341]\n",
      "loss: 0.437746  [89600/175341]\n",
      "loss: 0.715371  [91200/175341]\n",
      "loss: 0.323067  [92800/175341]\n",
      "loss: 0.224179  [94400/175341]\n",
      "loss: 0.278946  [96000/175341]\n",
      "loss: 0.521998  [97600/175341]\n",
      "loss: 0.932884  [99200/175341]\n",
      "loss: 0.830009  [100800/175341]\n",
      "loss: 1.388516  [102400/175341]\n",
      "loss: 0.270293  [104000/175341]\n",
      "loss: 0.632911  [105600/175341]\n",
      "loss: 0.596774  [107200/175341]\n",
      "loss: 0.726512  [108800/175341]\n",
      "loss: 0.350923  [110400/175341]\n",
      "loss: 0.372065  [112000/175341]\n",
      "loss: 0.542320  [113600/175341]\n",
      "loss: 0.160492  [115200/175341]\n",
      "loss: 0.192425  [116800/175341]\n",
      "loss: 0.672083  [118400/175341]\n",
      "loss: 0.581834  [120000/175341]\n",
      "loss: 0.829962  [121600/175341]\n",
      "loss: 0.913692  [123200/175341]\n",
      "loss: 0.740073  [124800/175341]\n",
      "loss: 0.594710  [126400/175341]\n",
      "loss: 0.296123  [128000/175341]\n",
      "loss: 0.503892  [129600/175341]\n",
      "loss: 0.509914  [131200/175341]\n",
      "loss: 0.576857  [132800/175341]\n",
      "loss: 0.402912  [134400/175341]\n",
      "loss: 0.710355  [136000/175341]\n",
      "loss: 0.297337  [137600/175341]\n",
      "loss: 0.591451  [139200/175341]\n",
      "loss: 0.523924  [140800/175341]\n",
      "loss: 0.308917  [142400/175341]\n",
      "loss: 0.361005  [144000/175341]\n",
      "loss: 0.395083  [145600/175341]\n",
      "loss: 0.425826  [147200/175341]\n",
      "loss: 0.761239  [148800/175341]\n",
      "loss: 0.464948  [150400/175341]\n",
      "loss: 0.205724  [152000/175341]\n",
      "loss: 0.623915  [153600/175341]\n",
      "loss: 0.537783  [155200/175341]\n",
      "loss: 0.363097  [156800/175341]\n",
      "loss: 0.536915  [158400/175341]\n",
      "loss: 0.829081  [160000/175341]\n",
      "loss: 0.636829  [161600/175341]\n",
      "loss: 0.569315  [163200/175341]\n",
      "loss: 0.314279  [164800/175341]\n",
      "loss: 0.628754  [166400/175341]\n",
      "loss: 0.538810  [168000/175341]\n",
      "loss: 0.724358  [169600/175341]\n",
      "loss: 0.635320  [171200/175341]\n",
      "loss: 0.319659  [172800/175341]\n",
      "loss: 0.572093  [174400/175341]\n",
      "Train Accuracy: 81.3101%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.544058, F1-score: 75.55%, Macro_F1-Score:  42.14%  \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.471946  [    0/175341]\n",
      "loss: 0.107399  [ 1600/175341]\n",
      "loss: 0.237647  [ 3200/175341]\n",
      "loss: 0.232978  [ 4800/175341]\n",
      "loss: 0.362837  [ 6400/175341]\n",
      "loss: 0.467124  [ 8000/175341]\n",
      "loss: 0.602717  [ 9600/175341]\n",
      "loss: 0.527067  [11200/175341]\n",
      "loss: 0.535554  [12800/175341]\n",
      "loss: 0.403196  [14400/175341]\n",
      "loss: 0.360170  [16000/175341]\n",
      "loss: 0.670131  [17600/175341]\n",
      "loss: 0.497918  [19200/175341]\n",
      "loss: 0.479884  [20800/175341]\n",
      "loss: 0.712946  [22400/175341]\n",
      "loss: 0.660396  [24000/175341]\n",
      "loss: 0.812806  [25600/175341]\n",
      "loss: 0.396613  [27200/175341]\n",
      "loss: 0.425023  [28800/175341]\n",
      "loss: 0.549694  [30400/175341]\n",
      "loss: 0.296198  [32000/175341]\n",
      "loss: 0.460869  [33600/175341]\n",
      "loss: 0.386117  [35200/175341]\n",
      "loss: 0.288585  [36800/175341]\n",
      "loss: 0.300194  [38400/175341]\n",
      "loss: 0.415287  [40000/175341]\n",
      "loss: 0.540981  [41600/175341]\n",
      "loss: 0.561440  [43200/175341]\n",
      "loss: 0.670798  [44800/175341]\n",
      "loss: 0.748105  [46400/175341]\n",
      "loss: 0.406856  [48000/175341]\n",
      "loss: 0.368929  [49600/175341]\n",
      "loss: 0.626026  [51200/175341]\n",
      "loss: 0.185861  [52800/175341]\n",
      "loss: 0.807129  [54400/175341]\n",
      "loss: 0.187122  [56000/175341]\n",
      "loss: 0.292219  [57600/175341]\n",
      "loss: 0.601996  [59200/175341]\n",
      "loss: 0.453758  [60800/175341]\n",
      "loss: 1.004333  [62400/175341]\n",
      "loss: 0.741917  [64000/175341]\n",
      "loss: 0.529430  [65600/175341]\n",
      "loss: 0.295490  [67200/175341]\n",
      "loss: 0.593161  [68800/175341]\n",
      "loss: 0.169118  [70400/175341]\n",
      "loss: 0.560382  [72000/175341]\n",
      "loss: 0.273075  [73600/175341]\n",
      "loss: 0.670871  [75200/175341]\n",
      "loss: 0.656252  [76800/175341]\n",
      "loss: 0.512302  [78400/175341]\n",
      "loss: 0.424722  [80000/175341]\n",
      "loss: 0.195329  [81600/175341]\n",
      "loss: 1.495814  [83200/175341]\n",
      "loss: 0.234605  [84800/175341]\n",
      "loss: 0.457465  [86400/175341]\n",
      "loss: 0.390988  [88000/175341]\n",
      "loss: 0.767558  [89600/175341]\n",
      "loss: 0.274828  [91200/175341]\n",
      "loss: 0.255401  [92800/175341]\n",
      "loss: 0.964530  [94400/175341]\n",
      "loss: 0.353388  [96000/175341]\n",
      "loss: 0.162265  [97600/175341]\n",
      "loss: 0.530039  [99200/175341]\n",
      "loss: 0.325734  [100800/175341]\n",
      "loss: 0.306696  [102400/175341]\n",
      "loss: 0.448736  [104000/175341]\n",
      "loss: 0.673711  [105600/175341]\n",
      "loss: 0.281647  [107200/175341]\n",
      "loss: 0.665297  [108800/175341]\n",
      "loss: 0.622202  [110400/175341]\n",
      "loss: 0.404045  [112000/175341]\n",
      "loss: 0.926220  [113600/175341]\n",
      "loss: 0.160682  [115200/175341]\n",
      "loss: 0.230552  [116800/175341]\n",
      "loss: 0.518520  [118400/175341]\n",
      "loss: 0.574932  [120000/175341]\n",
      "loss: 0.281127  [121600/175341]\n",
      "loss: 0.490994  [123200/175341]\n",
      "loss: 0.296235  [124800/175341]\n",
      "loss: 0.367340  [126400/175341]\n",
      "loss: 0.526788  [128000/175341]\n",
      "loss: 0.571660  [129600/175341]\n",
      "loss: 0.139377  [131200/175341]\n",
      "loss: 0.682882  [132800/175341]\n",
      "loss: 0.484372  [134400/175341]\n",
      "loss: 1.176197  [136000/175341]\n",
      "loss: 0.750508  [137600/175341]\n",
      "loss: 0.206329  [139200/175341]\n",
      "loss: 0.516748  [140800/175341]\n",
      "loss: 0.339594  [142400/175341]\n",
      "loss: 0.408057  [144000/175341]\n",
      "loss: 0.824433  [145600/175341]\n",
      "loss: 0.461042  [147200/175341]\n",
      "loss: 0.470008  [148800/175341]\n",
      "loss: 0.278571  [150400/175341]\n",
      "loss: 0.351484  [152000/175341]\n",
      "loss: 0.650446  [153600/175341]\n",
      "loss: 0.438535  [155200/175341]\n",
      "loss: 0.393161  [156800/175341]\n",
      "loss: 0.840971  [158400/175341]\n",
      "loss: 0.399515  [160000/175341]\n",
      "loss: 0.376671  [161600/175341]\n",
      "loss: 0.505886  [163200/175341]\n",
      "loss: 0.489629  [164800/175341]\n",
      "loss: 0.354629  [166400/175341]\n",
      "loss: 0.306088  [168000/175341]\n",
      "loss: 0.344253  [169600/175341]\n",
      "loss: 0.376899  [171200/175341]\n",
      "loss: 0.400016  [172800/175341]\n",
      "loss: 0.781431  [174400/175341]\n",
      "Train Accuracy: 81.2799%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.539930, F1-score: 75.57%, Macro_F1-Score:  41.55%  \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.454209  [    0/175341]\n",
      "loss: 0.519199  [ 1600/175341]\n",
      "loss: 1.229402  [ 3200/175341]\n",
      "loss: 0.438737  [ 4800/175341]\n",
      "loss: 0.403130  [ 6400/175341]\n",
      "loss: 0.248558  [ 8000/175341]\n",
      "loss: 0.733061  [ 9600/175341]\n",
      "loss: 0.207233  [11200/175341]\n",
      "loss: 0.345030  [12800/175341]\n",
      "loss: 0.237827  [14400/175341]\n",
      "loss: 0.064871  [16000/175341]\n",
      "loss: 0.434238  [17600/175341]\n",
      "loss: 0.801855  [19200/175341]\n",
      "loss: 0.338419  [20800/175341]\n",
      "loss: 0.744461  [22400/175341]\n",
      "loss: 0.134376  [24000/175341]\n",
      "loss: 0.498082  [25600/175341]\n",
      "loss: 0.468579  [27200/175341]\n",
      "loss: 0.489434  [28800/175341]\n",
      "loss: 0.290787  [30400/175341]\n",
      "loss: 0.291351  [32000/175341]\n",
      "loss: 0.348499  [33600/175341]\n",
      "loss: 1.145458  [35200/175341]\n",
      "loss: 0.618973  [36800/175341]\n",
      "loss: 0.330229  [38400/175341]\n",
      "loss: 0.533792  [40000/175341]\n",
      "loss: 0.670229  [41600/175341]\n",
      "loss: 0.337158  [43200/175341]\n",
      "loss: 0.905372  [44800/175341]\n",
      "loss: 0.550204  [46400/175341]\n",
      "loss: 0.517108  [48000/175341]\n",
      "loss: 0.464873  [49600/175341]\n",
      "loss: 0.415938  [51200/175341]\n",
      "loss: 0.309647  [52800/175341]\n",
      "loss: 0.100855  [54400/175341]\n",
      "loss: 0.631158  [56000/175341]\n",
      "loss: 0.445207  [57600/175341]\n",
      "loss: 0.474256  [59200/175341]\n",
      "loss: 0.478952  [60800/175341]\n",
      "loss: 0.389711  [62400/175341]\n",
      "loss: 0.474912  [64000/175341]\n",
      "loss: 0.521449  [65600/175341]\n",
      "loss: 0.199384  [67200/175341]\n",
      "loss: 0.530917  [68800/175341]\n",
      "loss: 0.403504  [70400/175341]\n",
      "loss: 0.160498  [72000/175341]\n",
      "loss: 0.369135  [73600/175341]\n",
      "loss: 0.676479  [75200/175341]\n",
      "loss: 0.389118  [76800/175341]\n",
      "loss: 0.162049  [78400/175341]\n",
      "loss: 0.340949  [80000/175341]\n",
      "loss: 0.339966  [81600/175341]\n",
      "loss: 0.331894  [83200/175341]\n",
      "loss: 0.310198  [84800/175341]\n",
      "loss: 0.166134  [86400/175341]\n",
      "loss: 0.657441  [88000/175341]\n",
      "loss: 0.339465  [89600/175341]\n",
      "loss: 0.668220  [91200/175341]\n",
      "loss: 0.911247  [92800/175341]\n",
      "loss: 0.726520  [94400/175341]\n",
      "loss: 0.280737  [96000/175341]\n",
      "loss: 0.119672  [97600/175341]\n",
      "loss: 0.473064  [99200/175341]\n",
      "loss: 0.184767  [100800/175341]\n",
      "loss: 0.266070  [102400/175341]\n",
      "loss: 0.448947  [104000/175341]\n",
      "loss: 0.224176  [105600/175341]\n",
      "loss: 0.679440  [107200/175341]\n",
      "loss: 0.287314  [108800/175341]\n",
      "loss: 0.472186  [110400/175341]\n",
      "loss: 0.626506  [112000/175341]\n",
      "loss: 0.808230  [113600/175341]\n",
      "loss: 0.523265  [115200/175341]\n",
      "loss: 0.126110  [116800/175341]\n",
      "loss: 0.497284  [118400/175341]\n",
      "loss: 0.359349  [120000/175341]\n",
      "loss: 0.830046  [121600/175341]\n",
      "loss: 0.219878  [123200/175341]\n",
      "loss: 0.147677  [124800/175341]\n",
      "loss: 0.589164  [126400/175341]\n",
      "loss: 0.467627  [128000/175341]\n",
      "loss: 0.638783  [129600/175341]\n",
      "loss: 0.432480  [131200/175341]\n",
      "loss: 0.376599  [132800/175341]\n",
      "loss: 0.523161  [134400/175341]\n",
      "loss: 0.476363  [136000/175341]\n",
      "loss: 0.244073  [137600/175341]\n",
      "loss: 0.594841  [139200/175341]\n",
      "loss: 0.515627  [140800/175341]\n",
      "loss: 0.230694  [142400/175341]\n",
      "loss: 0.439347  [144000/175341]\n",
      "loss: 0.352862  [145600/175341]\n",
      "loss: 0.287493  [147200/175341]\n",
      "loss: 0.756869  [148800/175341]\n",
      "loss: 0.391337  [150400/175341]\n",
      "loss: 0.368906  [152000/175341]\n",
      "loss: 0.739614  [153600/175341]\n",
      "loss: 0.299969  [155200/175341]\n",
      "loss: 0.359990  [156800/175341]\n",
      "loss: 0.321676  [158400/175341]\n",
      "loss: 0.590471  [160000/175341]\n",
      "loss: 0.663762  [161600/175341]\n",
      "loss: 0.174312  [163200/175341]\n",
      "loss: 0.414388  [164800/175341]\n",
      "loss: 0.121153  [166400/175341]\n",
      "loss: 0.687196  [168000/175341]\n",
      "loss: 0.408713  [169600/175341]\n",
      "loss: 0.373439  [171200/175341]\n",
      "loss: 0.229856  [172800/175341]\n",
      "loss: 0.338040  [174400/175341]\n",
      "Train Accuracy: 81.3210%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.538547, F1-score: 76.14%, Macro_F1-Score:  41.94%  \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.343211  [    0/175341]\n",
      "loss: 0.611338  [ 1600/175341]\n",
      "loss: 0.050735  [ 3200/175341]\n",
      "loss: 0.658755  [ 4800/175341]\n",
      "loss: 0.226174  [ 6400/175341]\n",
      "loss: 0.295853  [ 8000/175341]\n",
      "loss: 0.382562  [ 9600/175341]\n",
      "loss: 0.615163  [11200/175341]\n",
      "loss: 0.512336  [12800/175341]\n",
      "loss: 0.380931  [14400/175341]\n",
      "loss: 0.378903  [16000/175341]\n",
      "loss: 0.389412  [17600/175341]\n",
      "loss: 0.266978  [19200/175341]\n",
      "loss: 0.220538  [20800/175341]\n",
      "loss: 0.389098  [22400/175341]\n",
      "loss: 0.529225  [24000/175341]\n",
      "loss: 0.229435  [25600/175341]\n",
      "loss: 0.213572  [27200/175341]\n",
      "loss: 0.431480  [28800/175341]\n",
      "loss: 0.176249  [30400/175341]\n",
      "loss: 1.114348  [32000/175341]\n",
      "loss: 0.315196  [33600/175341]\n",
      "loss: 0.359623  [35200/175341]\n",
      "loss: 0.248283  [36800/175341]\n",
      "loss: 0.164035  [38400/175341]\n",
      "loss: 0.310853  [40000/175341]\n",
      "loss: 0.241550  [41600/175341]\n",
      "loss: 0.619768  [43200/175341]\n",
      "loss: 0.404148  [44800/175341]\n",
      "loss: 0.893578  [46400/175341]\n",
      "loss: 0.459195  [48000/175341]\n",
      "loss: 0.224874  [49600/175341]\n",
      "loss: 0.266602  [51200/175341]\n",
      "loss: 0.585753  [52800/175341]\n",
      "loss: 0.223375  [54400/175341]\n",
      "loss: 0.244533  [56000/175341]\n",
      "loss: 0.284387  [57600/175341]\n",
      "loss: 0.102806  [59200/175341]\n",
      "loss: 0.516795  [60800/175341]\n",
      "loss: 0.351702  [62400/175341]\n",
      "loss: 0.438961  [64000/175341]\n",
      "loss: 0.459013  [65600/175341]\n",
      "loss: 0.377874  [67200/175341]\n",
      "loss: 0.572536  [68800/175341]\n",
      "loss: 0.571940  [70400/175341]\n",
      "loss: 0.552577  [72000/175341]\n",
      "loss: 0.704569  [73600/175341]\n",
      "loss: 0.434546  [75200/175341]\n",
      "loss: 0.476296  [76800/175341]\n",
      "loss: 0.272191  [78400/175341]\n",
      "loss: 0.866379  [80000/175341]\n",
      "loss: 0.235590  [81600/175341]\n",
      "loss: 0.246642  [83200/175341]\n",
      "loss: 0.176736  [84800/175341]\n",
      "loss: 0.393274  [86400/175341]\n",
      "loss: 0.378928  [88000/175341]\n",
      "loss: 0.322909  [89600/175341]\n",
      "loss: 0.511725  [91200/175341]\n",
      "loss: 0.584768  [92800/175341]\n",
      "loss: 0.291808  [94400/175341]\n",
      "loss: 0.579624  [96000/175341]\n",
      "loss: 0.459536  [97600/175341]\n",
      "loss: 0.455564  [99200/175341]\n",
      "loss: 0.352283  [100800/175341]\n",
      "loss: 0.304755  [102400/175341]\n",
      "loss: 0.626961  [104000/175341]\n",
      "loss: 0.484973  [105600/175341]\n",
      "loss: 0.281292  [107200/175341]\n",
      "loss: 0.752717  [108800/175341]\n",
      "loss: 0.482873  [110400/175341]\n",
      "loss: 0.199678  [112000/175341]\n",
      "loss: 0.535307  [113600/175341]\n",
      "loss: 0.363825  [115200/175341]\n",
      "loss: 0.472879  [116800/175341]\n",
      "loss: 0.064479  [118400/175341]\n",
      "loss: 0.226318  [120000/175341]\n",
      "loss: 0.404883  [121600/175341]\n",
      "loss: 0.338274  [123200/175341]\n",
      "loss: 0.649072  [124800/175341]\n",
      "loss: 0.613634  [126400/175341]\n",
      "loss: 0.794688  [128000/175341]\n",
      "loss: 0.690212  [129600/175341]\n",
      "loss: 0.756394  [131200/175341]\n",
      "loss: 0.656623  [132800/175341]\n",
      "loss: 0.435483  [134400/175341]\n",
      "loss: 0.508786  [136000/175341]\n",
      "loss: 0.277289  [137600/175341]\n",
      "loss: 0.260655  [139200/175341]\n",
      "loss: 0.300907  [140800/175341]\n",
      "loss: 0.552922  [142400/175341]\n",
      "loss: 0.428958  [144000/175341]\n",
      "loss: 0.330527  [145600/175341]\n",
      "loss: 0.261972  [147200/175341]\n",
      "loss: 0.510992  [148800/175341]\n",
      "loss: 0.499765  [150400/175341]\n",
      "loss: 0.212039  [152000/175341]\n",
      "loss: 0.411370  [153600/175341]\n",
      "loss: 0.465724  [155200/175341]\n",
      "loss: 0.169392  [156800/175341]\n",
      "loss: 0.373252  [158400/175341]\n",
      "loss: 0.682349  [160000/175341]\n",
      "loss: 0.261470  [161600/175341]\n",
      "loss: 0.151038  [163200/175341]\n",
      "loss: 0.210917  [164800/175341]\n",
      "loss: 0.581016  [166400/175341]\n",
      "loss: 0.265412  [168000/175341]\n",
      "loss: 0.439038  [169600/175341]\n",
      "loss: 0.175284  [171200/175341]\n",
      "loss: 0.643824  [172800/175341]\n",
      "loss: 0.221887  [174400/175341]\n",
      "Train Accuracy: 81.3096%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.553633, F1-score: 75.27%, Macro_F1-Score:  42.44%  \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.779691  [    0/175341]\n",
      "loss: 0.956066  [ 1600/175341]\n",
      "loss: 0.312673  [ 3200/175341]\n",
      "loss: 0.728501  [ 4800/175341]\n",
      "loss: 0.646330  [ 6400/175341]\n",
      "loss: 0.759072  [ 8000/175341]\n",
      "loss: 0.537189  [ 9600/175341]\n",
      "loss: 0.543075  [11200/175341]\n",
      "loss: 0.370388  [12800/175341]\n",
      "loss: 0.178933  [14400/175341]\n",
      "loss: 0.246811  [16000/175341]\n",
      "loss: 0.175185  [17600/175341]\n",
      "loss: 0.395801  [19200/175341]\n",
      "loss: 0.368334  [20800/175341]\n",
      "loss: 0.625728  [22400/175341]\n",
      "loss: 0.154834  [24000/175341]\n",
      "loss: 0.642392  [25600/175341]\n",
      "loss: 0.183820  [27200/175341]\n",
      "loss: 0.327513  [28800/175341]\n",
      "loss: 0.318269  [30400/175341]\n",
      "loss: 0.119222  [32000/175341]\n",
      "loss: 0.177793  [33600/175341]\n",
      "loss: 0.236522  [35200/175341]\n",
      "loss: 0.356613  [36800/175341]\n",
      "loss: 0.424067  [38400/175341]\n",
      "loss: 0.487713  [40000/175341]\n",
      "loss: 0.232977  [41600/175341]\n",
      "loss: 0.394281  [43200/175341]\n",
      "loss: 0.318013  [44800/175341]\n",
      "loss: 0.457187  [46400/175341]\n",
      "loss: 0.269725  [48000/175341]\n",
      "loss: 0.526285  [49600/175341]\n",
      "loss: 0.398162  [51200/175341]\n",
      "loss: 0.447462  [52800/175341]\n",
      "loss: 0.460285  [54400/175341]\n",
      "loss: 0.555324  [56000/175341]\n",
      "loss: 0.517023  [57600/175341]\n",
      "loss: 0.321943  [59200/175341]\n",
      "loss: 0.431867  [60800/175341]\n",
      "loss: 0.309140  [62400/175341]\n",
      "loss: 0.473937  [64000/175341]\n",
      "loss: 0.218543  [65600/175341]\n",
      "loss: 0.348482  [67200/175341]\n",
      "loss: 0.333796  [68800/175341]\n",
      "loss: 0.624875  [70400/175341]\n",
      "loss: 0.253502  [72000/175341]\n",
      "loss: 0.360932  [73600/175341]\n",
      "loss: 0.622397  [75200/175341]\n",
      "loss: 0.702179  [76800/175341]\n",
      "loss: 0.576024  [78400/175341]\n",
      "loss: 0.583570  [80000/175341]\n",
      "loss: 0.350984  [81600/175341]\n",
      "loss: 0.488497  [83200/175341]\n",
      "loss: 0.791172  [84800/175341]\n",
      "loss: 0.136677  [86400/175341]\n",
      "loss: 0.407194  [88000/175341]\n",
      "loss: 0.447201  [89600/175341]\n",
      "loss: 0.282507  [91200/175341]\n",
      "loss: 0.586933  [92800/175341]\n",
      "loss: 0.962209  [94400/175341]\n",
      "loss: 0.663276  [96000/175341]\n",
      "loss: 0.780551  [97600/175341]\n",
      "loss: 0.264791  [99200/175341]\n",
      "loss: 0.536611  [100800/175341]\n",
      "loss: 0.710525  [102400/175341]\n",
      "loss: 0.446887  [104000/175341]\n",
      "loss: 0.193898  [105600/175341]\n",
      "loss: 0.506054  [107200/175341]\n",
      "loss: 0.737932  [108800/175341]\n",
      "loss: 0.408198  [110400/175341]\n",
      "loss: 0.707016  [112000/175341]\n",
      "loss: 0.903543  [113600/175341]\n",
      "loss: 0.252899  [115200/175341]\n",
      "loss: 0.609564  [116800/175341]\n",
      "loss: 0.528757  [118400/175341]\n",
      "loss: 1.136642  [120000/175341]\n",
      "loss: 0.535477  [121600/175341]\n",
      "loss: 0.344167  [123200/175341]\n",
      "loss: 0.436217  [124800/175341]\n",
      "loss: 0.938484  [126400/175341]\n",
      "loss: 0.513838  [128000/175341]\n",
      "loss: 0.507219  [129600/175341]\n",
      "loss: 0.477419  [131200/175341]\n",
      "loss: 0.247131  [132800/175341]\n",
      "loss: 0.583085  [134400/175341]\n",
      "loss: 0.417774  [136000/175341]\n",
      "loss: 0.928701  [137600/175341]\n",
      "loss: 0.964945  [139200/175341]\n",
      "loss: 0.539916  [140800/175341]\n",
      "loss: 0.336577  [142400/175341]\n",
      "loss: 0.310237  [144000/175341]\n",
      "loss: 0.401144  [145600/175341]\n",
      "loss: 0.415129  [147200/175341]\n",
      "loss: 0.951197  [148800/175341]\n",
      "loss: 0.465469  [150400/175341]\n",
      "loss: 0.324415  [152000/175341]\n",
      "loss: 0.562577  [153600/175341]\n",
      "loss: 0.403569  [155200/175341]\n",
      "loss: 0.111791  [156800/175341]\n",
      "loss: 0.373967  [158400/175341]\n",
      "loss: 0.511889  [160000/175341]\n",
      "loss: 0.247615  [161600/175341]\n",
      "loss: 0.453002  [163200/175341]\n",
      "loss: 0.386711  [164800/175341]\n",
      "loss: 0.335831  [166400/175341]\n",
      "loss: 0.632661  [168000/175341]\n",
      "loss: 0.641564  [169600/175341]\n",
      "loss: 0.469599  [171200/175341]\n",
      "loss: 0.872257  [172800/175341]\n",
      "loss: 0.268239  [174400/175341]\n",
      "Train Accuracy: 81.2925%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.550997, F1-score: 75.25%, Macro_F1-Score:  41.14%  \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.122403  [    0/175341]\n",
      "loss: 0.260131  [ 1600/175341]\n",
      "loss: 0.241757  [ 3200/175341]\n",
      "loss: 0.436978  [ 4800/175341]\n",
      "loss: 0.472507  [ 6400/175341]\n",
      "loss: 0.298614  [ 8000/175341]\n",
      "loss: 0.658553  [ 9600/175341]\n",
      "loss: 0.545303  [11200/175341]\n",
      "loss: 0.585323  [12800/175341]\n",
      "loss: 0.355990  [14400/175341]\n",
      "loss: 0.341926  [16000/175341]\n",
      "loss: 0.548958  [17600/175341]\n",
      "loss: 0.366223  [19200/175341]\n",
      "loss: 0.315236  [20800/175341]\n",
      "loss: 0.816105  [22400/175341]\n",
      "loss: 0.673093  [24000/175341]\n",
      "loss: 0.366524  [25600/175341]\n",
      "loss: 0.238338  [27200/175341]\n",
      "loss: 0.479462  [28800/175341]\n",
      "loss: 0.330731  [30400/175341]\n",
      "loss: 0.498728  [32000/175341]\n",
      "loss: 0.168773  [33600/175341]\n",
      "loss: 0.502103  [35200/175341]\n",
      "loss: 0.789431  [36800/175341]\n",
      "loss: 0.609255  [38400/175341]\n",
      "loss: 0.453165  [40000/175341]\n",
      "loss: 0.205068  [41600/175341]\n",
      "loss: 0.507397  [43200/175341]\n",
      "loss: 0.788211  [44800/175341]\n",
      "loss: 0.724331  [46400/175341]\n",
      "loss: 0.509577  [48000/175341]\n",
      "loss: 0.160958  [49600/175341]\n",
      "loss: 0.413248  [51200/175341]\n",
      "loss: 0.582818  [52800/175341]\n",
      "loss: 0.430422  [54400/175341]\n",
      "loss: 0.170161  [56000/175341]\n",
      "loss: 0.163469  [57600/175341]\n",
      "loss: 0.255658  [59200/175341]\n",
      "loss: 0.339565  [60800/175341]\n",
      "loss: 0.640088  [62400/175341]\n",
      "loss: 0.377812  [64000/175341]\n",
      "loss: 0.702414  [65600/175341]\n",
      "loss: 0.090686  [67200/175341]\n",
      "loss: 0.728171  [68800/175341]\n",
      "loss: 0.388062  [70400/175341]\n",
      "loss: 0.472689  [72000/175341]\n",
      "loss: 0.774387  [73600/175341]\n",
      "loss: 0.286640  [75200/175341]\n",
      "loss: 0.346038  [76800/175341]\n",
      "loss: 0.376214  [78400/175341]\n",
      "loss: 0.517586  [80000/175341]\n",
      "loss: 0.487853  [81600/175341]\n",
      "loss: 0.273510  [83200/175341]\n",
      "loss: 0.591176  [84800/175341]\n",
      "loss: 0.638935  [86400/175341]\n",
      "loss: 0.352713  [88000/175341]\n",
      "loss: 0.273480  [89600/175341]\n",
      "loss: 0.323874  [91200/175341]\n",
      "loss: 1.126074  [92800/175341]\n",
      "loss: 0.395569  [94400/175341]\n",
      "loss: 0.536212  [96000/175341]\n",
      "loss: 0.429171  [97600/175341]\n",
      "loss: 0.375469  [99200/175341]\n",
      "loss: 0.438980  [100800/175341]\n",
      "loss: 0.567890  [102400/175341]\n",
      "loss: 0.309216  [104000/175341]\n",
      "loss: 0.333180  [105600/175341]\n",
      "loss: 0.280043  [107200/175341]\n",
      "loss: 0.430963  [108800/175341]\n",
      "loss: 1.090749  [110400/175341]\n",
      "loss: 0.250112  [112000/175341]\n",
      "loss: 0.344492  [113600/175341]\n",
      "loss: 0.461755  [115200/175341]\n",
      "loss: 0.597035  [116800/175341]\n",
      "loss: 0.558325  [118400/175341]\n",
      "loss: 0.631679  [120000/175341]\n",
      "loss: 0.464055  [121600/175341]\n",
      "loss: 0.128086  [123200/175341]\n",
      "loss: 0.577568  [124800/175341]\n",
      "loss: 0.685181  [126400/175341]\n",
      "loss: 0.487618  [128000/175341]\n",
      "loss: 0.512757  [129600/175341]\n",
      "loss: 0.580286  [131200/175341]\n",
      "loss: 0.481935  [132800/175341]\n",
      "loss: 0.676952  [134400/175341]\n",
      "loss: 0.486880  [136000/175341]\n",
      "loss: 0.484047  [137600/175341]\n",
      "loss: 0.349781  [139200/175341]\n",
      "loss: 0.418685  [140800/175341]\n",
      "loss: 0.207343  [142400/175341]\n",
      "loss: 0.845632  [144000/175341]\n",
      "loss: 0.350918  [145600/175341]\n",
      "loss: 0.491914  [147200/175341]\n",
      "loss: 0.425366  [148800/175341]\n",
      "loss: 0.599006  [150400/175341]\n",
      "loss: 0.231724  [152000/175341]\n",
      "loss: 0.465735  [153600/175341]\n",
      "loss: 0.410531  [155200/175341]\n",
      "loss: 0.414653  [156800/175341]\n",
      "loss: 0.375114  [158400/175341]\n",
      "loss: 0.519748  [160000/175341]\n",
      "loss: 0.579927  [161600/175341]\n",
      "loss: 0.386982  [163200/175341]\n",
      "loss: 0.813815  [164800/175341]\n",
      "loss: 0.891194  [166400/175341]\n",
      "loss: 0.667557  [168000/175341]\n",
      "loss: 0.552873  [169600/175341]\n",
      "loss: 0.448177  [171200/175341]\n",
      "loss: 0.616997  [172800/175341]\n",
      "loss: 0.372682  [174400/175341]\n",
      "Train Accuracy: 81.3096%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.558132, F1-score: 75.06%, Macro_F1-Score:  41.72%  \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.533363  [    0/175341]\n",
      "loss: 0.478439  [ 1600/175341]\n",
      "loss: 0.289473  [ 3200/175341]\n",
      "loss: 0.258284  [ 4800/175341]\n",
      "loss: 0.321259  [ 6400/175341]\n",
      "loss: 0.524721  [ 8000/175341]\n",
      "loss: 0.451185  [ 9600/175341]\n",
      "loss: 0.512791  [11200/175341]\n",
      "loss: 0.399262  [12800/175341]\n",
      "loss: 0.532488  [14400/175341]\n",
      "loss: 0.368635  [16000/175341]\n",
      "loss: 0.444066  [17600/175341]\n",
      "loss: 0.779916  [19200/175341]\n",
      "loss: 0.491498  [20800/175341]\n",
      "loss: 0.390585  [22400/175341]\n",
      "loss: 0.283305  [24000/175341]\n",
      "loss: 0.186982  [25600/175341]\n",
      "loss: 0.563367  [27200/175341]\n",
      "loss: 0.271493  [28800/175341]\n",
      "loss: 0.354804  [30400/175341]\n",
      "loss: 0.864633  [32000/175341]\n",
      "loss: 0.548083  [33600/175341]\n",
      "loss: 0.186953  [35200/175341]\n",
      "loss: 0.369816  [36800/175341]\n",
      "loss: 0.681337  [38400/175341]\n",
      "loss: 0.453752  [40000/175341]\n",
      "loss: 0.031245  [41600/175341]\n",
      "loss: 0.932397  [43200/175341]\n",
      "loss: 0.653358  [44800/175341]\n",
      "loss: 0.510676  [46400/175341]\n",
      "loss: 0.475615  [48000/175341]\n",
      "loss: 0.519876  [49600/175341]\n",
      "loss: 0.928663  [51200/175341]\n",
      "loss: 0.099961  [52800/175341]\n",
      "loss: 0.385738  [54400/175341]\n",
      "loss: 0.704525  [56000/175341]\n",
      "loss: 0.314052  [57600/175341]\n",
      "loss: 0.621492  [59200/175341]\n",
      "loss: 0.952684  [60800/175341]\n",
      "loss: 0.919684  [62400/175341]\n",
      "loss: 0.453282  [64000/175341]\n",
      "loss: 0.329005  [65600/175341]\n",
      "loss: 0.551896  [67200/175341]\n",
      "loss: 0.473510  [68800/175341]\n",
      "loss: 0.266011  [70400/175341]\n",
      "loss: 0.457024  [72000/175341]\n",
      "loss: 0.485080  [73600/175341]\n",
      "loss: 0.485152  [75200/175341]\n",
      "loss: 0.338135  [76800/175341]\n",
      "loss: 0.434320  [78400/175341]\n",
      "loss: 0.292736  [80000/175341]\n",
      "loss: 0.309805  [81600/175341]\n",
      "loss: 0.484500  [83200/175341]\n",
      "loss: 0.942247  [84800/175341]\n",
      "loss: 0.596709  [86400/175341]\n",
      "loss: 0.597593  [88000/175341]\n",
      "loss: 0.709349  [89600/175341]\n",
      "loss: 0.357761  [91200/175341]\n",
      "loss: 0.421973  [92800/175341]\n",
      "loss: 0.541758  [94400/175341]\n",
      "loss: 0.419055  [96000/175341]\n",
      "loss: 0.496672  [97600/175341]\n",
      "loss: 0.464283  [99200/175341]\n",
      "loss: 0.283152  [100800/175341]\n",
      "loss: 0.239528  [102400/175341]\n",
      "loss: 0.505729  [104000/175341]\n",
      "loss: 0.605401  [105600/175341]\n",
      "loss: 0.676171  [107200/175341]\n",
      "loss: 0.492267  [108800/175341]\n",
      "loss: 0.764109  [110400/175341]\n",
      "loss: 0.451028  [112000/175341]\n",
      "loss: 0.632462  [113600/175341]\n",
      "loss: 0.639116  [115200/175341]\n",
      "loss: 0.584485  [116800/175341]\n",
      "loss: 0.590582  [118400/175341]\n",
      "loss: 0.261567  [120000/175341]\n",
      "loss: 0.555162  [121600/175341]\n",
      "loss: 1.084712  [123200/175341]\n",
      "loss: 0.410323  [124800/175341]\n",
      "loss: 0.261203  [126400/175341]\n",
      "loss: 0.180983  [128000/175341]\n",
      "loss: 0.411344  [129600/175341]\n",
      "loss: 0.596213  [131200/175341]\n",
      "loss: 0.705027  [132800/175341]\n",
      "loss: 0.249567  [134400/175341]\n",
      "loss: 0.797591  [136000/175341]\n",
      "loss: 0.347814  [137600/175341]\n",
      "loss: 0.215944  [139200/175341]\n",
      "loss: 0.461265  [140800/175341]\n",
      "loss: 0.192981  [142400/175341]\n",
      "loss: 0.422032  [144000/175341]\n",
      "loss: 0.413892  [145600/175341]\n",
      "loss: 0.317276  [147200/175341]\n",
      "loss: 0.467164  [148800/175341]\n",
      "loss: 0.586814  [150400/175341]\n",
      "loss: 0.930792  [152000/175341]\n",
      "loss: 0.821187  [153600/175341]\n",
      "loss: 0.876822  [155200/175341]\n",
      "loss: 0.460451  [156800/175341]\n",
      "loss: 0.929839  [158400/175341]\n",
      "loss: 0.483437  [160000/175341]\n",
      "loss: 0.452333  [161600/175341]\n",
      "loss: 0.699198  [163200/175341]\n",
      "loss: 0.344118  [164800/175341]\n",
      "loss: 0.152345  [166400/175341]\n",
      "loss: 0.879036  [168000/175341]\n",
      "loss: 0.374476  [169600/175341]\n",
      "loss: 0.266352  [171200/175341]\n",
      "loss: 0.396686  [172800/175341]\n",
      "loss: 0.538296  [174400/175341]\n",
      "Train Accuracy: 81.2970%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.555780, F1-score: 75.94%, Macro_F1-Score:  42.46%  \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.703549  [    0/175341]\n",
      "loss: 0.763266  [ 1600/175341]\n",
      "loss: 0.180350  [ 3200/175341]\n",
      "loss: 0.532945  [ 4800/175341]\n",
      "loss: 0.613741  [ 6400/175341]\n",
      "loss: 0.548804  [ 8000/175341]\n",
      "loss: 0.338789  [ 9600/175341]\n",
      "loss: 0.399465  [11200/175341]\n",
      "loss: 0.483332  [12800/175341]\n",
      "loss: 0.515979  [14400/175341]\n",
      "loss: 0.645806  [16000/175341]\n",
      "loss: 0.302735  [17600/175341]\n",
      "loss: 0.223297  [19200/175341]\n",
      "loss: 0.584730  [20800/175341]\n",
      "loss: 0.143996  [22400/175341]\n",
      "loss: 0.340443  [24000/175341]\n",
      "loss: 0.379074  [25600/175341]\n",
      "loss: 0.237809  [27200/175341]\n",
      "loss: 0.326734  [28800/175341]\n",
      "loss: 0.230959  [30400/175341]\n",
      "loss: 0.430433  [32000/175341]\n",
      "loss: 0.128951  [33600/175341]\n",
      "loss: 0.457920  [35200/175341]\n",
      "loss: 0.196892  [36800/175341]\n",
      "loss: 0.267023  [38400/175341]\n",
      "loss: 0.680074  [40000/175341]\n",
      "loss: 0.341454  [41600/175341]\n",
      "loss: 0.291255  [43200/175341]\n",
      "loss: 0.192547  [44800/175341]\n",
      "loss: 0.398211  [46400/175341]\n",
      "loss: 0.306459  [48000/175341]\n",
      "loss: 0.231419  [49600/175341]\n",
      "loss: 0.652835  [51200/175341]\n",
      "loss: 0.612911  [52800/175341]\n",
      "loss: 0.325437  [54400/175341]\n",
      "loss: 0.713288  [56000/175341]\n",
      "loss: 0.372220  [57600/175341]\n",
      "loss: 0.481637  [59200/175341]\n",
      "loss: 0.150464  [60800/175341]\n",
      "loss: 0.702824  [62400/175341]\n",
      "loss: 0.408742  [64000/175341]\n",
      "loss: 0.796354  [65600/175341]\n",
      "loss: 0.276456  [67200/175341]\n",
      "loss: 0.172745  [68800/175341]\n",
      "loss: 0.953900  [70400/175341]\n",
      "loss: 0.650492  [72000/175341]\n",
      "loss: 0.371022  [73600/175341]\n",
      "loss: 0.456499  [75200/175341]\n",
      "loss: 0.613606  [76800/175341]\n",
      "loss: 0.602751  [78400/175341]\n",
      "loss: 0.753531  [80000/175341]\n",
      "loss: 0.293107  [81600/175341]\n",
      "loss: 0.431554  [83200/175341]\n",
      "loss: 0.233661  [84800/175341]\n",
      "loss: 0.412367  [86400/175341]\n",
      "loss: 0.575747  [88000/175341]\n",
      "loss: 0.305706  [89600/175341]\n",
      "loss: 0.453417  [91200/175341]\n",
      "loss: 0.231235  [92800/175341]\n",
      "loss: 0.617513  [94400/175341]\n",
      "loss: 0.302695  [96000/175341]\n",
      "loss: 0.262577  [97600/175341]\n",
      "loss: 0.347053  [99200/175341]\n",
      "loss: 0.773541  [100800/175341]\n",
      "loss: 0.934052  [102400/175341]\n",
      "loss: 0.303894  [104000/175341]\n",
      "loss: 0.352702  [105600/175341]\n",
      "loss: 0.298154  [107200/175341]\n",
      "loss: 0.512460  [108800/175341]\n",
      "loss: 0.156244  [110400/175341]\n",
      "loss: 0.738287  [112000/175341]\n",
      "loss: 0.535153  [113600/175341]\n",
      "loss: 0.399634  [115200/175341]\n",
      "loss: 0.335551  [116800/175341]\n",
      "loss: 0.399619  [118400/175341]\n",
      "loss: 0.250485  [120000/175341]\n",
      "loss: 0.516239  [121600/175341]\n",
      "loss: 0.544470  [123200/175341]\n",
      "loss: 0.381655  [124800/175341]\n",
      "loss: 0.638822  [126400/175341]\n",
      "loss: 0.199825  [128000/175341]\n",
      "loss: 0.061682  [129600/175341]\n",
      "loss: 0.362810  [131200/175341]\n",
      "loss: 0.316289  [132800/175341]\n",
      "loss: 0.672964  [134400/175341]\n",
      "loss: 0.572378  [136000/175341]\n",
      "loss: 0.638340  [137600/175341]\n",
      "loss: 0.480770  [139200/175341]\n",
      "loss: 1.100493  [140800/175341]\n",
      "loss: 0.440586  [142400/175341]\n",
      "loss: 0.575916  [144000/175341]\n",
      "loss: 0.376244  [145600/175341]\n",
      "loss: 0.216026  [147200/175341]\n",
      "loss: 0.401967  [148800/175341]\n",
      "loss: 0.591106  [150400/175341]\n",
      "loss: 0.305163  [152000/175341]\n",
      "loss: 0.814950  [153600/175341]\n",
      "loss: 0.354878  [155200/175341]\n",
      "loss: 1.024715  [156800/175341]\n",
      "loss: 0.591204  [158400/175341]\n",
      "loss: 0.238834  [160000/175341]\n",
      "loss: 0.413504  [161600/175341]\n",
      "loss: 0.129301  [163200/175341]\n",
      "loss: 0.582559  [164800/175341]\n",
      "loss: 0.628879  [166400/175341]\n",
      "loss: 0.444091  [168000/175341]\n",
      "loss: 0.317781  [169600/175341]\n",
      "loss: 0.285566  [171200/175341]\n",
      "loss: 0.716589  [172800/175341]\n",
      "loss: 0.583697  [174400/175341]\n",
      "Train Accuracy: 81.2788%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.531567, F1-score: 76.66%, Macro_F1-Score:  42.70%  \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.490232  [    0/175341]\n",
      "loss: 0.585981  [ 1600/175341]\n",
      "loss: 0.254525  [ 3200/175341]\n",
      "loss: 0.138623  [ 4800/175341]\n",
      "loss: 0.304417  [ 6400/175341]\n",
      "loss: 0.107430  [ 8000/175341]\n",
      "loss: 0.554524  [ 9600/175341]\n",
      "loss: 0.159308  [11200/175341]\n",
      "loss: 0.355628  [12800/175341]\n",
      "loss: 0.503793  [14400/175341]\n",
      "loss: 0.674328  [16000/175341]\n",
      "loss: 0.607606  [17600/175341]\n",
      "loss: 0.862623  [19200/175341]\n",
      "loss: 0.177294  [20800/175341]\n",
      "loss: 0.525154  [22400/175341]\n",
      "loss: 0.806339  [24000/175341]\n",
      "loss: 0.431624  [25600/175341]\n",
      "loss: 0.640183  [27200/175341]\n",
      "loss: 0.386582  [28800/175341]\n",
      "loss: 0.425562  [30400/175341]\n",
      "loss: 0.810345  [32000/175341]\n",
      "loss: 0.385072  [33600/175341]\n",
      "loss: 0.582596  [35200/175341]\n",
      "loss: 0.478578  [36800/175341]\n",
      "loss: 0.173541  [38400/175341]\n",
      "loss: 0.281379  [40000/175341]\n",
      "loss: 0.625533  [41600/175341]\n",
      "loss: 0.390441  [43200/175341]\n",
      "loss: 0.612571  [44800/175341]\n",
      "loss: 0.761254  [46400/175341]\n",
      "loss: 0.300895  [48000/175341]\n",
      "loss: 0.450418  [49600/175341]\n",
      "loss: 0.152359  [51200/175341]\n",
      "loss: 0.360486  [52800/175341]\n",
      "loss: 0.455107  [54400/175341]\n",
      "loss: 0.640515  [56000/175341]\n",
      "loss: 0.402667  [57600/175341]\n",
      "loss: 0.563179  [59200/175341]\n",
      "loss: 0.641994  [60800/175341]\n",
      "loss: 0.525420  [62400/175341]\n",
      "loss: 0.299887  [64000/175341]\n",
      "loss: 0.618861  [65600/175341]\n",
      "loss: 0.589282  [67200/175341]\n",
      "loss: 0.560522  [68800/175341]\n",
      "loss: 0.575493  [70400/175341]\n",
      "loss: 0.424913  [72000/175341]\n",
      "loss: 0.417702  [73600/175341]\n",
      "loss: 0.163827  [75200/175341]\n",
      "loss: 0.341434  [76800/175341]\n",
      "loss: 0.286239  [78400/175341]\n",
      "loss: 0.619836  [80000/175341]\n",
      "loss: 0.449111  [81600/175341]\n",
      "loss: 0.938304  [83200/175341]\n",
      "loss: 0.743741  [84800/175341]\n",
      "loss: 0.227952  [86400/175341]\n",
      "loss: 0.217452  [88000/175341]\n",
      "loss: 0.552419  [89600/175341]\n",
      "loss: 0.716721  [91200/175341]\n",
      "loss: 0.519155  [92800/175341]\n",
      "loss: 0.369337  [94400/175341]\n",
      "loss: 0.213060  [96000/175341]\n",
      "loss: 0.831824  [97600/175341]\n",
      "loss: 0.371370  [99200/175341]\n",
      "loss: 0.386928  [100800/175341]\n",
      "loss: 0.263802  [102400/175341]\n",
      "loss: 0.340957  [104000/175341]\n",
      "loss: 0.505656  [105600/175341]\n",
      "loss: 0.415671  [107200/175341]\n",
      "loss: 0.317199  [108800/175341]\n",
      "loss: 0.312872  [110400/175341]\n",
      "loss: 0.433866  [112000/175341]\n",
      "loss: 0.646325  [113600/175341]\n",
      "loss: 0.704783  [115200/175341]\n",
      "loss: 0.979675  [116800/175341]\n",
      "loss: 0.409349  [118400/175341]\n",
      "loss: 0.137814  [120000/175341]\n",
      "loss: 0.675374  [121600/175341]\n",
      "loss: 0.521086  [123200/175341]\n",
      "loss: 0.743566  [124800/175341]\n",
      "loss: 0.497377  [126400/175341]\n",
      "loss: 0.289616  [128000/175341]\n",
      "loss: 0.504864  [129600/175341]\n",
      "loss: 0.707996  [131200/175341]\n",
      "loss: 0.326542  [132800/175341]\n",
      "loss: 0.477115  [134400/175341]\n",
      "loss: 0.415675  [136000/175341]\n",
      "loss: 0.505098  [137600/175341]\n",
      "loss: 0.229363  [139200/175341]\n",
      "loss: 0.619678  [140800/175341]\n",
      "loss: 0.226307  [142400/175341]\n",
      "loss: 0.618318  [144000/175341]\n",
      "loss: 0.414803  [145600/175341]\n",
      "loss: 0.523766  [147200/175341]\n",
      "loss: 0.495248  [148800/175341]\n",
      "loss: 0.582934  [150400/175341]\n",
      "loss: 0.386736  [152000/175341]\n",
      "loss: 0.325019  [153600/175341]\n",
      "loss: 0.167839  [155200/175341]\n",
      "loss: 0.487881  [156800/175341]\n",
      "loss: 0.301465  [158400/175341]\n",
      "loss: 0.310864  [160000/175341]\n",
      "loss: 0.407566  [161600/175341]\n",
      "loss: 0.613132  [163200/175341]\n",
      "loss: 0.329734  [164800/175341]\n",
      "loss: 0.618381  [166400/175341]\n",
      "loss: 0.385325  [168000/175341]\n",
      "loss: 0.330096  [169600/175341]\n",
      "loss: 0.879565  [171200/175341]\n",
      "loss: 0.219396  [172800/175341]\n",
      "loss: 0.197568  [174400/175341]\n",
      "Train Accuracy: 81.3141%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.542583, F1-score: 76.30%, Macro_F1-Score:  43.13%  \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.591717  [    0/175341]\n",
      "loss: 0.179854  [ 1600/175341]\n",
      "loss: 0.638620  [ 3200/175341]\n",
      "loss: 0.591239  [ 4800/175341]\n",
      "loss: 0.141361  [ 6400/175341]\n",
      "loss: 0.398270  [ 8000/175341]\n",
      "loss: 0.665155  [ 9600/175341]\n",
      "loss: 0.444401  [11200/175341]\n",
      "loss: 0.315166  [12800/175341]\n",
      "loss: 0.603886  [14400/175341]\n",
      "loss: 0.377617  [16000/175341]\n",
      "loss: 0.605519  [17600/175341]\n",
      "loss: 0.441772  [19200/175341]\n",
      "loss: 0.416836  [20800/175341]\n",
      "loss: 0.388857  [22400/175341]\n",
      "loss: 0.175500  [24000/175341]\n",
      "loss: 0.157466  [25600/175341]\n",
      "loss: 0.522992  [27200/175341]\n",
      "loss: 0.592023  [28800/175341]\n",
      "loss: 0.436309  [30400/175341]\n",
      "loss: 0.919350  [32000/175341]\n",
      "loss: 0.202694  [33600/175341]\n",
      "loss: 0.222928  [35200/175341]\n",
      "loss: 0.379254  [36800/175341]\n",
      "loss: 0.265304  [38400/175341]\n",
      "loss: 0.371125  [40000/175341]\n",
      "loss: 0.415142  [41600/175341]\n",
      "loss: 0.588137  [43200/175341]\n",
      "loss: 0.662332  [44800/175341]\n",
      "loss: 0.719921  [46400/175341]\n",
      "loss: 0.217205  [48000/175341]\n",
      "loss: 0.268129  [49600/175341]\n",
      "loss: 0.196246  [51200/175341]\n",
      "loss: 0.489172  [52800/175341]\n",
      "loss: 0.340350  [54400/175341]\n",
      "loss: 0.521349  [56000/175341]\n",
      "loss: 0.997002  [57600/175341]\n",
      "loss: 0.575344  [59200/175341]\n",
      "loss: 0.342443  [60800/175341]\n",
      "loss: 0.792083  [62400/175341]\n",
      "loss: 0.158243  [64000/175341]\n",
      "loss: 0.391130  [65600/175341]\n",
      "loss: 0.541576  [67200/175341]\n",
      "loss: 0.264755  [68800/175341]\n",
      "loss: 0.388074  [70400/175341]\n",
      "loss: 0.356127  [72000/175341]\n",
      "loss: 0.511373  [73600/175341]\n",
      "loss: 0.524104  [75200/175341]\n",
      "loss: 0.337853  [76800/175341]\n",
      "loss: 0.205731  [78400/175341]\n",
      "loss: 0.390147  [80000/175341]\n",
      "loss: 0.308263  [81600/175341]\n",
      "loss: 0.358163  [83200/175341]\n",
      "loss: 0.847767  [84800/175341]\n",
      "loss: 0.474876  [86400/175341]\n",
      "loss: 1.111979  [88000/175341]\n",
      "loss: 0.385809  [89600/175341]\n",
      "loss: 0.309605  [91200/175341]\n",
      "loss: 0.597460  [92800/175341]\n",
      "loss: 0.191083  [94400/175341]\n",
      "loss: 0.556641  [96000/175341]\n",
      "loss: 0.952142  [97600/175341]\n",
      "loss: 0.560282  [99200/175341]\n",
      "loss: 0.526040  [100800/175341]\n",
      "loss: 0.549136  [102400/175341]\n",
      "loss: 0.327055  [104000/175341]\n",
      "loss: 0.437379  [105600/175341]\n",
      "loss: 1.049559  [107200/175341]\n",
      "loss: 0.606161  [108800/175341]\n",
      "loss: 0.222971  [110400/175341]\n",
      "loss: 0.758901  [112000/175341]\n",
      "loss: 0.380728  [113600/175341]\n",
      "loss: 0.517872  [115200/175341]\n",
      "loss: 0.528696  [116800/175341]\n",
      "loss: 0.227392  [118400/175341]\n",
      "loss: 0.284972  [120000/175341]\n",
      "loss: 0.956081  [121600/175341]\n",
      "loss: 0.317177  [123200/175341]\n",
      "loss: 0.477204  [124800/175341]\n",
      "loss: 0.351627  [126400/175341]\n",
      "loss: 0.374207  [128000/175341]\n",
      "loss: 0.243993  [129600/175341]\n",
      "loss: 0.314817  [131200/175341]\n",
      "loss: 0.356235  [132800/175341]\n",
      "loss: 0.364782  [134400/175341]\n",
      "loss: 0.445177  [136000/175341]\n",
      "loss: 0.666964  [137600/175341]\n",
      "loss: 0.277953  [139200/175341]\n",
      "loss: 0.218812  [140800/175341]\n",
      "loss: 0.685284  [142400/175341]\n",
      "loss: 0.424991  [144000/175341]\n",
      "loss: 0.467639  [145600/175341]\n",
      "loss: 0.854000  [147200/175341]\n",
      "loss: 0.408295  [148800/175341]\n",
      "loss: 0.397742  [150400/175341]\n",
      "loss: 0.649169  [152000/175341]\n",
      "loss: 0.660342  [153600/175341]\n",
      "loss: 0.430683  [155200/175341]\n",
      "loss: 0.567528  [156800/175341]\n",
      "loss: 0.836641  [158400/175341]\n",
      "loss: 0.260082  [160000/175341]\n",
      "loss: 0.756234  [161600/175341]\n",
      "loss: 0.879678  [163200/175341]\n",
      "loss: 0.207132  [164800/175341]\n",
      "loss: 0.442609  [166400/175341]\n",
      "loss: 0.403209  [168000/175341]\n",
      "loss: 0.236954  [169600/175341]\n",
      "loss: 0.573837  [171200/175341]\n",
      "loss: 0.508447  [172800/175341]\n",
      "loss: 0.300564  [174400/175341]\n",
      "Train Accuracy: 81.2839%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.536792, F1-score: 75.96%, Macro_F1-Score:  42.54%  \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.632867  [    0/175341]\n",
      "loss: 0.346380  [ 1600/175341]\n",
      "loss: 0.332744  [ 3200/175341]\n",
      "loss: 0.656864  [ 4800/175341]\n",
      "loss: 0.404675  [ 6400/175341]\n",
      "loss: 0.329638  [ 8000/175341]\n",
      "loss: 0.835056  [ 9600/175341]\n",
      "loss: 0.870133  [11200/175341]\n",
      "loss: 0.370074  [12800/175341]\n",
      "loss: 0.835061  [14400/175341]\n",
      "loss: 0.173768  [16000/175341]\n",
      "loss: 0.083932  [17600/175341]\n",
      "loss: 1.261532  [19200/175341]\n",
      "loss: 0.429050  [20800/175341]\n",
      "loss: 0.352765  [22400/175341]\n",
      "loss: 0.273074  [24000/175341]\n",
      "loss: 0.177785  [25600/175341]\n",
      "loss: 0.643949  [27200/175341]\n",
      "loss: 0.250862  [28800/175341]\n",
      "loss: 0.472563  [30400/175341]\n",
      "loss: 0.987181  [32000/175341]\n",
      "loss: 0.783412  [33600/175341]\n",
      "loss: 0.787334  [35200/175341]\n",
      "loss: 0.535741  [36800/175341]\n",
      "loss: 0.431453  [38400/175341]\n",
      "loss: 0.564586  [40000/175341]\n",
      "loss: 0.346287  [41600/175341]\n",
      "loss: 0.715513  [43200/175341]\n",
      "loss: 0.170698  [44800/175341]\n",
      "loss: 0.359504  [46400/175341]\n",
      "loss: 0.675552  [48000/175341]\n",
      "loss: 0.329192  [49600/175341]\n",
      "loss: 0.614540  [51200/175341]\n",
      "loss: 0.379893  [52800/175341]\n",
      "loss: 0.858636  [54400/175341]\n",
      "loss: 0.295913  [56000/175341]\n",
      "loss: 0.297167  [57600/175341]\n",
      "loss: 0.372899  [59200/175341]\n",
      "loss: 0.189566  [60800/175341]\n",
      "loss: 0.608947  [62400/175341]\n",
      "loss: 0.281033  [64000/175341]\n",
      "loss: 0.293261  [65600/175341]\n",
      "loss: 0.711974  [67200/175341]\n",
      "loss: 0.563227  [68800/175341]\n",
      "loss: 0.463408  [70400/175341]\n",
      "loss: 0.353240  [72000/175341]\n",
      "loss: 0.775026  [73600/175341]\n",
      "loss: 0.494242  [75200/175341]\n",
      "loss: 0.303265  [76800/175341]\n",
      "loss: 0.559945  [78400/175341]\n",
      "loss: 0.513487  [80000/175341]\n",
      "loss: 0.316588  [81600/175341]\n",
      "loss: 0.125694  [83200/175341]\n",
      "loss: 0.835626  [84800/175341]\n",
      "loss: 0.537341  [86400/175341]\n",
      "loss: 0.401513  [88000/175341]\n",
      "loss: 0.445467  [89600/175341]\n",
      "loss: 0.307863  [91200/175341]\n",
      "loss: 0.692500  [92800/175341]\n",
      "loss: 0.317233  [94400/175341]\n",
      "loss: 0.525226  [96000/175341]\n",
      "loss: 0.372897  [97600/175341]\n",
      "loss: 0.493138  [99200/175341]\n",
      "loss: 0.308573  [100800/175341]\n",
      "loss: 0.635006  [102400/175341]\n",
      "loss: 0.226804  [104000/175341]\n",
      "loss: 0.264246  [105600/175341]\n",
      "loss: 0.460580  [107200/175341]\n",
      "loss: 0.462843  [108800/175341]\n",
      "loss: 0.336852  [110400/175341]\n",
      "loss: 0.294025  [112000/175341]\n",
      "loss: 0.443201  [113600/175341]\n",
      "loss: 0.635978  [115200/175341]\n",
      "loss: 0.266113  [116800/175341]\n",
      "loss: 0.406840  [118400/175341]\n",
      "loss: 0.395401  [120000/175341]\n",
      "loss: 0.435511  [121600/175341]\n",
      "loss: 0.511511  [123200/175341]\n",
      "loss: 0.335835  [124800/175341]\n",
      "loss: 0.495178  [126400/175341]\n",
      "loss: 0.401546  [128000/175341]\n",
      "loss: 0.340224  [129600/175341]\n",
      "loss: 0.537150  [131200/175341]\n",
      "loss: 1.009981  [132800/175341]\n",
      "loss: 0.423806  [134400/175341]\n",
      "loss: 0.607005  [136000/175341]\n",
      "loss: 0.607231  [137600/175341]\n",
      "loss: 0.390923  [139200/175341]\n",
      "loss: 0.912954  [140800/175341]\n",
      "loss: 0.392056  [142400/175341]\n",
      "loss: 0.577427  [144000/175341]\n",
      "loss: 0.676543  [145600/175341]\n",
      "loss: 0.184324  [147200/175341]\n",
      "loss: 0.317150  [148800/175341]\n",
      "loss: 0.281491  [150400/175341]\n",
      "loss: 0.215576  [152000/175341]\n",
      "loss: 0.281513  [153600/175341]\n",
      "loss: 0.248041  [155200/175341]\n",
      "loss: 0.363125  [156800/175341]\n",
      "loss: 0.483593  [158400/175341]\n",
      "loss: 0.593038  [160000/175341]\n",
      "loss: 0.441149  [161600/175341]\n",
      "loss: 0.471004  [163200/175341]\n",
      "loss: 0.347677  [164800/175341]\n",
      "loss: 0.690546  [166400/175341]\n",
      "loss: 0.691211  [168000/175341]\n",
      "loss: 0.580054  [169600/175341]\n",
      "loss: 0.926702  [171200/175341]\n",
      "loss: 0.690588  [172800/175341]\n",
      "loss: 0.862951  [174400/175341]\n",
      "Train Accuracy: 81.2822%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.540702, F1-score: 76.04%, Macro_F1-Score:  41.77%  \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.241379  [    0/175341]\n",
      "loss: 0.505816  [ 1600/175341]\n",
      "loss: 0.575118  [ 3200/175341]\n",
      "loss: 0.772769  [ 4800/175341]\n",
      "loss: 0.318889  [ 6400/175341]\n",
      "loss: 0.361223  [ 8000/175341]\n",
      "loss: 0.290675  [ 9600/175341]\n",
      "loss: 0.679935  [11200/175341]\n",
      "loss: 0.374436  [12800/175341]\n",
      "loss: 0.319422  [14400/175341]\n",
      "loss: 0.309979  [16000/175341]\n",
      "loss: 0.335540  [17600/175341]\n",
      "loss: 0.701325  [19200/175341]\n",
      "loss: 0.319190  [20800/175341]\n",
      "loss: 0.391115  [22400/175341]\n",
      "loss: 0.336032  [24000/175341]\n",
      "loss: 0.213415  [25600/175341]\n",
      "loss: 0.430394  [27200/175341]\n",
      "loss: 0.413708  [28800/175341]\n",
      "loss: 0.713031  [30400/175341]\n",
      "loss: 0.371249  [32000/175341]\n",
      "loss: 0.434189  [33600/175341]\n",
      "loss: 0.461144  [35200/175341]\n",
      "loss: 0.328648  [36800/175341]\n",
      "loss: 0.501474  [38400/175341]\n",
      "loss: 0.282271  [40000/175341]\n",
      "loss: 0.637086  [41600/175341]\n",
      "loss: 0.448208  [43200/175341]\n",
      "loss: 0.662882  [44800/175341]\n",
      "loss: 0.279745  [46400/175341]\n",
      "loss: 0.260026  [48000/175341]\n",
      "loss: 0.333787  [49600/175341]\n",
      "loss: 0.878690  [51200/175341]\n",
      "loss: 0.229823  [52800/175341]\n",
      "loss: 0.491144  [54400/175341]\n",
      "loss: 0.543532  [56000/175341]\n",
      "loss: 0.308971  [57600/175341]\n",
      "loss: 0.230016  [59200/175341]\n",
      "loss: 0.757212  [60800/175341]\n",
      "loss: 0.537350  [62400/175341]\n",
      "loss: 0.425656  [64000/175341]\n",
      "loss: 0.277977  [65600/175341]\n",
      "loss: 0.409153  [67200/175341]\n",
      "loss: 0.487068  [68800/175341]\n",
      "loss: 0.565006  [70400/175341]\n",
      "loss: 0.409538  [72000/175341]\n",
      "loss: 0.540546  [73600/175341]\n",
      "loss: 0.276926  [75200/175341]\n",
      "loss: 0.555417  [76800/175341]\n",
      "loss: 0.413066  [78400/175341]\n",
      "loss: 0.750709  [80000/175341]\n",
      "loss: 0.467426  [81600/175341]\n",
      "loss: 0.453675  [83200/175341]\n",
      "loss: 0.317776  [84800/175341]\n",
      "loss: 0.191794  [86400/175341]\n",
      "loss: 0.484575  [88000/175341]\n",
      "loss: 0.646633  [89600/175341]\n",
      "loss: 0.696566  [91200/175341]\n",
      "loss: 0.321164  [92800/175341]\n",
      "loss: 0.417242  [94400/175341]\n",
      "loss: 0.485049  [96000/175341]\n",
      "loss: 0.134200  [97600/175341]\n",
      "loss: 0.697070  [99200/175341]\n",
      "loss: 0.443693  [100800/175341]\n",
      "loss: 0.686400  [102400/175341]\n",
      "loss: 0.593472  [104000/175341]\n",
      "loss: 0.198395  [105600/175341]\n",
      "loss: 0.165905  [107200/175341]\n",
      "loss: 0.656479  [108800/175341]\n",
      "loss: 0.440527  [110400/175341]\n",
      "loss: 0.659192  [112000/175341]\n",
      "loss: 0.480451  [113600/175341]\n",
      "loss: 0.753651  [115200/175341]\n",
      "loss: 0.973158  [116800/175341]\n",
      "loss: 0.551199  [118400/175341]\n",
      "loss: 0.505113  [120000/175341]\n",
      "loss: 0.419935  [121600/175341]\n",
      "loss: 0.348936  [123200/175341]\n",
      "loss: 0.443949  [124800/175341]\n",
      "loss: 0.529476  [126400/175341]\n",
      "loss: 0.475084  [128000/175341]\n",
      "loss: 0.458510  [129600/175341]\n",
      "loss: 0.457112  [131200/175341]\n",
      "loss: 0.180964  [132800/175341]\n",
      "loss: 0.293411  [134400/175341]\n",
      "loss: 0.514682  [136000/175341]\n",
      "loss: 0.244057  [137600/175341]\n",
      "loss: 0.385546  [139200/175341]\n",
      "loss: 0.734856  [140800/175341]\n",
      "loss: 0.225994  [142400/175341]\n",
      "loss: 0.601736  [144000/175341]\n",
      "loss: 0.404008  [145600/175341]\n",
      "loss: 0.312582  [147200/175341]\n",
      "loss: 0.395169  [148800/175341]\n",
      "loss: 0.214176  [150400/175341]\n",
      "loss: 0.241382  [152000/175341]\n",
      "loss: 0.965090  [153600/175341]\n",
      "loss: 0.463346  [155200/175341]\n",
      "loss: 0.612693  [156800/175341]\n",
      "loss: 0.412860  [158400/175341]\n",
      "loss: 0.393230  [160000/175341]\n",
      "loss: 0.401647  [161600/175341]\n",
      "loss: 0.319102  [163200/175341]\n",
      "loss: 0.359140  [164800/175341]\n",
      "loss: 0.494865  [166400/175341]\n",
      "loss: 0.230018  [168000/175341]\n",
      "loss: 0.366253  [169600/175341]\n",
      "loss: 0.342469  [171200/175341]\n",
      "loss: 0.375962  [172800/175341]\n",
      "loss: 0.654422  [174400/175341]\n",
      "Train Accuracy: 81.2987%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.541150, F1-score: 76.00%, Macro_F1-Score:  42.42%  \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.788780  [    0/175341]\n",
      "loss: 0.522049  [ 1600/175341]\n",
      "loss: 0.359512  [ 3200/175341]\n",
      "loss: 0.574055  [ 4800/175341]\n",
      "loss: 0.494654  [ 6400/175341]\n",
      "loss: 0.412499  [ 8000/175341]\n",
      "loss: 0.638461  [ 9600/175341]\n",
      "loss: 0.303460  [11200/175341]\n",
      "loss: 0.335550  [12800/175341]\n",
      "loss: 0.281048  [14400/175341]\n",
      "loss: 0.678413  [16000/175341]\n",
      "loss: 0.455049  [17600/175341]\n",
      "loss: 0.313963  [19200/175341]\n",
      "loss: 0.217972  [20800/175341]\n",
      "loss: 0.180821  [22400/175341]\n",
      "loss: 0.765609  [24000/175341]\n",
      "loss: 0.643609  [25600/175341]\n",
      "loss: 0.737323  [27200/175341]\n",
      "loss: 0.309991  [28800/175341]\n",
      "loss: 0.444686  [30400/175341]\n",
      "loss: 0.568076  [32000/175341]\n",
      "loss: 0.417912  [33600/175341]\n",
      "loss: 0.360345  [35200/175341]\n",
      "loss: 0.438002  [36800/175341]\n",
      "loss: 0.316683  [38400/175341]\n",
      "loss: 0.309132  [40000/175341]\n",
      "loss: 0.781027  [41600/175341]\n",
      "loss: 0.214482  [43200/175341]\n",
      "loss: 0.261228  [44800/175341]\n",
      "loss: 0.466293  [46400/175341]\n",
      "loss: 0.483866  [48000/175341]\n",
      "loss: 0.292498  [49600/175341]\n",
      "loss: 0.371832  [51200/175341]\n",
      "loss: 0.339392  [52800/175341]\n",
      "loss: 0.476354  [54400/175341]\n",
      "loss: 0.521160  [56000/175341]\n",
      "loss: 0.499605  [57600/175341]\n",
      "loss: 0.978474  [59200/175341]\n",
      "loss: 0.751097  [60800/175341]\n",
      "loss: 0.635127  [62400/175341]\n",
      "loss: 0.264089  [64000/175341]\n",
      "loss: 0.693641  [65600/175341]\n",
      "loss: 0.580841  [67200/175341]\n",
      "loss: 0.366645  [68800/175341]\n",
      "loss: 0.641279  [70400/175341]\n",
      "loss: 0.479453  [72000/175341]\n",
      "loss: 0.595964  [73600/175341]\n",
      "loss: 0.481817  [75200/175341]\n",
      "loss: 0.247943  [76800/175341]\n",
      "loss: 0.676402  [78400/175341]\n",
      "loss: 0.444372  [80000/175341]\n",
      "loss: 0.364460  [81600/175341]\n",
      "loss: 0.602807  [83200/175341]\n",
      "loss: 0.242258  [84800/175341]\n",
      "loss: 0.798649  [86400/175341]\n",
      "loss: 0.758798  [88000/175341]\n",
      "loss: 0.554085  [89600/175341]\n",
      "loss: 0.384881  [91200/175341]\n",
      "loss: 0.288739  [92800/175341]\n",
      "loss: 0.819799  [94400/175341]\n",
      "loss: 0.509015  [96000/175341]\n",
      "loss: 0.953192  [97600/175341]\n",
      "loss: 0.688973  [99200/175341]\n",
      "loss: 0.530534  [100800/175341]\n",
      "loss: 0.453439  [102400/175341]\n",
      "loss: 0.391911  [104000/175341]\n",
      "loss: 0.543602  [105600/175341]\n",
      "loss: 0.599138  [107200/175341]\n",
      "loss: 0.599653  [108800/175341]\n",
      "loss: 0.557718  [110400/175341]\n",
      "loss: 0.341900  [112000/175341]\n",
      "loss: 0.762748  [113600/175341]\n",
      "loss: 0.227718  [115200/175341]\n",
      "loss: 0.630265  [116800/175341]\n",
      "loss: 0.146586  [118400/175341]\n",
      "loss: 0.444604  [120000/175341]\n",
      "loss: 0.466740  [121600/175341]\n",
      "loss: 0.520153  [123200/175341]\n",
      "loss: 0.550136  [124800/175341]\n",
      "loss: 0.698069  [126400/175341]\n",
      "loss: 0.687053  [128000/175341]\n",
      "loss: 0.773897  [129600/175341]\n",
      "loss: 0.572671  [131200/175341]\n",
      "loss: 0.225808  [132800/175341]\n",
      "loss: 0.912982  [134400/175341]\n",
      "loss: 0.352283  [136000/175341]\n",
      "loss: 0.321298  [137600/175341]\n",
      "loss: 0.267538  [139200/175341]\n",
      "loss: 0.802535  [140800/175341]\n",
      "loss: 0.271433  [142400/175341]\n",
      "loss: 0.349970  [144000/175341]\n",
      "loss: 0.370414  [145600/175341]\n",
      "loss: 0.470423  [147200/175341]\n",
      "loss: 0.585009  [148800/175341]\n",
      "loss: 0.368804  [150400/175341]\n",
      "loss: 0.426322  [152000/175341]\n",
      "loss: 0.750701  [153600/175341]\n",
      "loss: 0.886584  [155200/175341]\n",
      "loss: 0.384275  [156800/175341]\n",
      "loss: 0.257417  [158400/175341]\n",
      "loss: 1.018788  [160000/175341]\n",
      "loss: 0.348822  [161600/175341]\n",
      "loss: 0.393052  [163200/175341]\n",
      "loss: 0.470420  [164800/175341]\n",
      "loss: 0.501826  [166400/175341]\n",
      "loss: 0.270376  [168000/175341]\n",
      "loss: 0.655325  [169600/175341]\n",
      "loss: 0.092540  [171200/175341]\n",
      "loss: 0.686734  [172800/175341]\n",
      "loss: 0.209508  [174400/175341]\n",
      "Train Accuracy: 81.3187%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.536512, F1-score: 76.69%, Macro_F1-Score:  43.41%  \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.621583  [    0/175341]\n",
      "loss: 0.421093  [ 1600/175341]\n",
      "loss: 0.245139  [ 3200/175341]\n",
      "loss: 0.105526  [ 4800/175341]\n",
      "loss: 0.635185  [ 6400/175341]\n",
      "loss: 0.610060  [ 8000/175341]\n",
      "loss: 0.294592  [ 9600/175341]\n",
      "loss: 0.425290  [11200/175341]\n",
      "loss: 0.569190  [12800/175341]\n",
      "loss: 0.495684  [14400/175341]\n",
      "loss: 0.958733  [16000/175341]\n",
      "loss: 0.580160  [17600/175341]\n",
      "loss: 0.485604  [19200/175341]\n",
      "loss: 0.626383  [20800/175341]\n",
      "loss: 0.152175  [22400/175341]\n",
      "loss: 0.147258  [24000/175341]\n",
      "loss: 0.468685  [25600/175341]\n",
      "loss: 0.300492  [27200/175341]\n",
      "loss: 0.499806  [28800/175341]\n",
      "loss: 0.540717  [30400/175341]\n",
      "loss: 0.340136  [32000/175341]\n",
      "loss: 0.247357  [33600/175341]\n",
      "loss: 0.239557  [35200/175341]\n",
      "loss: 0.263290  [36800/175341]\n",
      "loss: 0.661879  [38400/175341]\n",
      "loss: 0.469039  [40000/175341]\n",
      "loss: 0.317406  [41600/175341]\n",
      "loss: 0.253931  [43200/175341]\n",
      "loss: 0.473233  [44800/175341]\n",
      "loss: 0.691840  [46400/175341]\n",
      "loss: 0.461679  [48000/175341]\n",
      "loss: 0.174062  [49600/175341]\n",
      "loss: 0.254215  [51200/175341]\n",
      "loss: 0.452607  [52800/175341]\n",
      "loss: 0.249491  [54400/175341]\n",
      "loss: 0.692815  [56000/175341]\n",
      "loss: 0.599660  [57600/175341]\n",
      "loss: 0.843583  [59200/175341]\n",
      "loss: 0.441015  [60800/175341]\n",
      "loss: 0.381398  [62400/175341]\n",
      "loss: 0.538592  [64000/175341]\n",
      "loss: 0.436459  [65600/175341]\n",
      "loss: 0.188468  [67200/175341]\n",
      "loss: 0.565930  [68800/175341]\n",
      "loss: 0.293472  [70400/175341]\n",
      "loss: 0.342055  [72000/175341]\n",
      "loss: 0.487506  [73600/175341]\n",
      "loss: 0.770605  [75200/175341]\n",
      "loss: 0.150869  [76800/175341]\n",
      "loss: 0.242039  [78400/175341]\n",
      "loss: 0.205525  [80000/175341]\n",
      "loss: 1.534594  [81600/175341]\n",
      "loss: 0.452730  [83200/175341]\n",
      "loss: 0.318022  [84800/175341]\n",
      "loss: 0.223750  [86400/175341]\n",
      "loss: 0.413495  [88000/175341]\n",
      "loss: 0.364445  [89600/175341]\n",
      "loss: 0.459666  [91200/175341]\n",
      "loss: 0.406131  [92800/175341]\n",
      "loss: 0.373957  [94400/175341]\n",
      "loss: 0.772408  [96000/175341]\n",
      "loss: 0.316067  [97600/175341]\n",
      "loss: 0.536021  [99200/175341]\n",
      "loss: 0.245266  [100800/175341]\n",
      "loss: 0.153057  [102400/175341]\n",
      "loss: 0.466790  [104000/175341]\n",
      "loss: 0.511092  [105600/175341]\n",
      "loss: 0.465526  [107200/175341]\n",
      "loss: 0.280777  [108800/175341]\n",
      "loss: 0.073294  [110400/175341]\n",
      "loss: 0.583004  [112000/175341]\n",
      "loss: 0.696085  [113600/175341]\n",
      "loss: 0.628747  [115200/175341]\n",
      "loss: 0.349021  [116800/175341]\n",
      "loss: 0.265632  [118400/175341]\n",
      "loss: 0.069784  [120000/175341]\n",
      "loss: 0.251747  [121600/175341]\n",
      "loss: 0.314537  [123200/175341]\n",
      "loss: 0.158966  [124800/175341]\n",
      "loss: 0.283608  [126400/175341]\n",
      "loss: 0.167219  [128000/175341]\n",
      "loss: 0.505350  [129600/175341]\n",
      "loss: 0.423036  [131200/175341]\n",
      "loss: 0.516330  [132800/175341]\n",
      "loss: 0.866970  [134400/175341]\n",
      "loss: 0.444654  [136000/175341]\n",
      "loss: 0.567561  [137600/175341]\n",
      "loss: 0.164221  [139200/175341]\n",
      "loss: 0.357531  [140800/175341]\n",
      "loss: 0.171973  [142400/175341]\n",
      "loss: 0.567664  [144000/175341]\n",
      "loss: 0.523883  [145600/175341]\n",
      "loss: 0.261593  [147200/175341]\n",
      "loss: 0.388154  [148800/175341]\n",
      "loss: 0.283939  [150400/175341]\n",
      "loss: 0.256109  [152000/175341]\n",
      "loss: 0.390643  [153600/175341]\n",
      "loss: 0.398989  [155200/175341]\n",
      "loss: 0.417413  [156800/175341]\n",
      "loss: 0.354551  [158400/175341]\n",
      "loss: 0.616502  [160000/175341]\n",
      "loss: 0.449729  [161600/175341]\n",
      "loss: 0.637929  [163200/175341]\n",
      "loss: 0.358379  [164800/175341]\n",
      "loss: 0.590703  [166400/175341]\n",
      "loss: 0.111660  [168000/175341]\n",
      "loss: 0.642799  [169600/175341]\n",
      "loss: 0.355804  [171200/175341]\n",
      "loss: 0.369061  [172800/175341]\n",
      "loss: 0.497967  [174400/175341]\n",
      "Train Accuracy: 81.3073%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.537586, F1-score: 76.33%, Macro_F1-Score:  42.35%  \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.369812  [    0/175341]\n",
      "loss: 0.403873  [ 1600/175341]\n",
      "loss: 0.646845  [ 3200/175341]\n",
      "loss: 0.410920  [ 4800/175341]\n",
      "loss: 0.515746  [ 6400/175341]\n",
      "loss: 0.370339  [ 8000/175341]\n",
      "loss: 0.562084  [ 9600/175341]\n",
      "loss: 0.691867  [11200/175341]\n",
      "loss: 0.299832  [12800/175341]\n",
      "loss: 0.297878  [14400/175341]\n",
      "loss: 0.411641  [16000/175341]\n",
      "loss: 0.518360  [17600/175341]\n",
      "loss: 0.437082  [19200/175341]\n",
      "loss: 0.262241  [20800/175341]\n",
      "loss: 0.120017  [22400/175341]\n",
      "loss: 0.702107  [24000/175341]\n",
      "loss: 0.382939  [25600/175341]\n",
      "loss: 0.218458  [27200/175341]\n",
      "loss: 0.303493  [28800/175341]\n",
      "loss: 0.664102  [30400/175341]\n",
      "loss: 0.277296  [32000/175341]\n",
      "loss: 0.440455  [33600/175341]\n",
      "loss: 0.767086  [35200/175341]\n",
      "loss: 0.334530  [36800/175341]\n",
      "loss: 0.671764  [38400/175341]\n",
      "loss: 0.537739  [40000/175341]\n",
      "loss: 0.666532  [41600/175341]\n",
      "loss: 0.212712  [43200/175341]\n",
      "loss: 0.737653  [44800/175341]\n",
      "loss: 0.883671  [46400/175341]\n",
      "loss: 1.031131  [48000/175341]\n",
      "loss: 0.275592  [49600/175341]\n",
      "loss: 0.367726  [51200/175341]\n",
      "loss: 0.454791  [52800/175341]\n",
      "loss: 0.674328  [54400/175341]\n",
      "loss: 0.565943  [56000/175341]\n",
      "loss: 0.414014  [57600/175341]\n",
      "loss: 0.769149  [59200/175341]\n",
      "loss: 0.458286  [60800/175341]\n",
      "loss: 0.430399  [62400/175341]\n",
      "loss: 1.046660  [64000/175341]\n",
      "loss: 0.614990  [65600/175341]\n",
      "loss: 0.273287  [67200/175341]\n",
      "loss: 0.399171  [68800/175341]\n",
      "loss: 1.131268  [70400/175341]\n",
      "loss: 0.290622  [72000/175341]\n",
      "loss: 0.638053  [73600/175341]\n",
      "loss: 0.599321  [75200/175341]\n",
      "loss: 0.467182  [76800/175341]\n",
      "loss: 0.268234  [78400/175341]\n",
      "loss: 0.510025  [80000/175341]\n",
      "loss: 0.452071  [81600/175341]\n",
      "loss: 0.457670  [83200/175341]\n",
      "loss: 1.033710  [84800/175341]\n",
      "loss: 0.339035  [86400/175341]\n",
      "loss: 0.450513  [88000/175341]\n",
      "loss: 0.237992  [89600/175341]\n",
      "loss: 0.908883  [91200/175341]\n",
      "loss: 0.278605  [92800/175341]\n",
      "loss: 0.735760  [94400/175341]\n",
      "loss: 0.152441  [96000/175341]\n",
      "loss: 0.307197  [97600/175341]\n",
      "loss: 0.450914  [99200/175341]\n",
      "loss: 0.242115  [100800/175341]\n",
      "loss: 0.579441  [102400/175341]\n",
      "loss: 0.236246  [104000/175341]\n",
      "loss: 0.457113  [105600/175341]\n",
      "loss: 0.181477  [107200/175341]\n",
      "loss: 0.397144  [108800/175341]\n",
      "loss: 0.400821  [110400/175341]\n",
      "loss: 0.910733  [112000/175341]\n",
      "loss: 0.377384  [113600/175341]\n",
      "loss: 0.515534  [115200/175341]\n",
      "loss: 0.243886  [116800/175341]\n",
      "loss: 0.314938  [118400/175341]\n",
      "loss: 0.638757  [120000/175341]\n",
      "loss: 0.056476  [121600/175341]\n",
      "loss: 0.750047  [123200/175341]\n",
      "loss: 0.294570  [124800/175341]\n",
      "loss: 0.593741  [126400/175341]\n",
      "loss: 0.442276  [128000/175341]\n",
      "loss: 0.324090  [129600/175341]\n",
      "loss: 0.183759  [131200/175341]\n",
      "loss: 0.565418  [132800/175341]\n",
      "loss: 0.195266  [134400/175341]\n",
      "loss: 0.366668  [136000/175341]\n",
      "loss: 0.762263  [137600/175341]\n",
      "loss: 0.544524  [139200/175341]\n",
      "loss: 0.369574  [140800/175341]\n",
      "loss: 0.451375  [142400/175341]\n",
      "loss: 0.415439  [144000/175341]\n",
      "loss: 0.446066  [145600/175341]\n",
      "loss: 0.370675  [147200/175341]\n",
      "loss: 0.490760  [148800/175341]\n",
      "loss: 0.170037  [150400/175341]\n",
      "loss: 0.429456  [152000/175341]\n",
      "loss: 1.142296  [153600/175341]\n",
      "loss: 0.617763  [155200/175341]\n",
      "loss: 0.948633  [156800/175341]\n",
      "loss: 0.172562  [158400/175341]\n",
      "loss: 0.564176  [160000/175341]\n",
      "loss: 0.561904  [161600/175341]\n",
      "loss: 0.261577  [163200/175341]\n",
      "loss: 0.579250  [164800/175341]\n",
      "loss: 0.627618  [166400/175341]\n",
      "loss: 0.620049  [168000/175341]\n",
      "loss: 0.436413  [169600/175341]\n",
      "loss: 0.429185  [171200/175341]\n",
      "loss: 0.651135  [172800/175341]\n",
      "loss: 0.170232  [174400/175341]\n",
      "Train Accuracy: 81.3295%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.544443, F1-score: 76.26%, Macro_F1-Score:  43.10%  \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.417101  [    0/175341]\n",
      "loss: 0.612646  [ 1600/175341]\n",
      "loss: 0.268438  [ 3200/175341]\n",
      "loss: 0.950275  [ 4800/175341]\n",
      "loss: 0.115815  [ 6400/175341]\n",
      "loss: 1.035281  [ 8000/175341]\n",
      "loss: 0.489822  [ 9600/175341]\n",
      "loss: 0.361930  [11200/175341]\n",
      "loss: 0.371792  [12800/175341]\n",
      "loss: 0.746505  [14400/175341]\n",
      "loss: 0.187514  [16000/175341]\n",
      "loss: 0.213272  [17600/175341]\n",
      "loss: 0.498051  [19200/175341]\n",
      "loss: 0.298264  [20800/175341]\n",
      "loss: 0.727831  [22400/175341]\n",
      "loss: 0.553590  [24000/175341]\n",
      "loss: 0.633764  [25600/175341]\n",
      "loss: 0.228686  [27200/175341]\n",
      "loss: 0.551489  [28800/175341]\n",
      "loss: 0.299118  [30400/175341]\n",
      "loss: 0.822083  [32000/175341]\n",
      "loss: 0.370261  [33600/175341]\n",
      "loss: 0.445492  [35200/175341]\n",
      "loss: 0.354992  [36800/175341]\n",
      "loss: 0.587854  [38400/175341]\n",
      "loss: 0.491226  [40000/175341]\n",
      "loss: 0.234100  [41600/175341]\n",
      "loss: 0.863471  [43200/175341]\n",
      "loss: 0.282781  [44800/175341]\n",
      "loss: 0.444644  [46400/175341]\n",
      "loss: 0.287677  [48000/175341]\n",
      "loss: 0.349016  [49600/175341]\n",
      "loss: 0.557704  [51200/175341]\n",
      "loss: 0.411021  [52800/175341]\n",
      "loss: 0.285044  [54400/175341]\n",
      "loss: 0.315396  [56000/175341]\n",
      "loss: 0.187252  [57600/175341]\n",
      "loss: 0.632788  [59200/175341]\n",
      "loss: 0.482543  [60800/175341]\n",
      "loss: 0.734258  [62400/175341]\n",
      "loss: 0.587990  [64000/175341]\n",
      "loss: 0.681668  [65600/175341]\n",
      "loss: 0.527300  [67200/175341]\n",
      "loss: 0.556739  [68800/175341]\n",
      "loss: 0.366305  [70400/175341]\n",
      "loss: 0.346319  [72000/175341]\n",
      "loss: 0.554227  [73600/175341]\n",
      "loss: 0.538162  [75200/175341]\n",
      "loss: 0.362032  [76800/175341]\n",
      "loss: 0.629224  [78400/175341]\n",
      "loss: 0.445789  [80000/175341]\n",
      "loss: 0.634059  [81600/175341]\n",
      "loss: 0.403225  [83200/175341]\n",
      "loss: 0.648063  [84800/175341]\n",
      "loss: 0.640518  [86400/175341]\n",
      "loss: 0.347505  [88000/175341]\n",
      "loss: 0.876553  [89600/175341]\n",
      "loss: 0.376563  [91200/175341]\n",
      "loss: 0.241939  [92800/175341]\n",
      "loss: 0.757489  [94400/175341]\n",
      "loss: 0.148408  [96000/175341]\n",
      "loss: 0.324682  [97600/175341]\n",
      "loss: 0.424909  [99200/175341]\n",
      "loss: 0.433668  [100800/175341]\n",
      "loss: 0.982688  [102400/175341]\n",
      "loss: 0.613652  [104000/175341]\n",
      "loss: 0.264273  [105600/175341]\n",
      "loss: 0.433975  [107200/175341]\n",
      "loss: 0.668748  [108800/175341]\n",
      "loss: 0.196614  [110400/175341]\n",
      "loss: 0.548433  [112000/175341]\n",
      "loss: 0.370118  [113600/175341]\n",
      "loss: 0.316488  [115200/175341]\n",
      "loss: 0.413736  [116800/175341]\n",
      "loss: 0.324044  [118400/175341]\n",
      "loss: 0.304401  [120000/175341]\n",
      "loss: 0.567924  [121600/175341]\n",
      "loss: 0.580957  [123200/175341]\n",
      "loss: 0.309229  [124800/175341]\n",
      "loss: 1.114894  [126400/175341]\n",
      "loss: 0.430480  [128000/175341]\n",
      "loss: 0.102388  [129600/175341]\n",
      "loss: 0.273868  [131200/175341]\n",
      "loss: 0.540039  [132800/175341]\n",
      "loss: 0.898543  [134400/175341]\n",
      "loss: 0.753794  [136000/175341]\n",
      "loss: 0.206000  [137600/175341]\n",
      "loss: 0.128369  [139200/175341]\n",
      "loss: 0.712322  [140800/175341]\n",
      "loss: 0.580191  [142400/175341]\n",
      "loss: 0.406542  [144000/175341]\n",
      "loss: 0.721206  [145600/175341]\n",
      "loss: 0.318866  [147200/175341]\n",
      "loss: 0.237136  [148800/175341]\n",
      "loss: 0.451373  [150400/175341]\n",
      "loss: 0.552167  [152000/175341]\n",
      "loss: 0.645363  [153600/175341]\n",
      "loss: 0.553478  [155200/175341]\n",
      "loss: 0.487325  [156800/175341]\n",
      "loss: 0.902415  [158400/175341]\n",
      "loss: 0.507174  [160000/175341]\n",
      "loss: 0.884506  [161600/175341]\n",
      "loss: 0.348656  [163200/175341]\n",
      "loss: 0.216253  [164800/175341]\n",
      "loss: 0.482227  [166400/175341]\n",
      "loss: 0.499199  [168000/175341]\n",
      "loss: 0.317178  [169600/175341]\n",
      "loss: 0.523379  [171200/175341]\n",
      "loss: 0.970899  [172800/175341]\n",
      "loss: 0.433692  [174400/175341]\n",
      "Train Accuracy: 81.2691%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.541156, F1-score: 75.92%, Macro_F1-Score:  42.53%  \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.567527  [    0/175341]\n",
      "loss: 0.244428  [ 1600/175341]\n",
      "loss: 0.439184  [ 3200/175341]\n",
      "loss: 0.819103  [ 4800/175341]\n",
      "loss: 0.493073  [ 6400/175341]\n",
      "loss: 0.554477  [ 8000/175341]\n",
      "loss: 0.603361  [ 9600/175341]\n",
      "loss: 0.606412  [11200/175341]\n",
      "loss: 0.554835  [12800/175341]\n",
      "loss: 0.501841  [14400/175341]\n",
      "loss: 0.743567  [16000/175341]\n",
      "loss: 0.836443  [17600/175341]\n",
      "loss: 0.152632  [19200/175341]\n",
      "loss: 0.319929  [20800/175341]\n",
      "loss: 0.401219  [22400/175341]\n",
      "loss: 0.593653  [24000/175341]\n",
      "loss: 0.458799  [25600/175341]\n",
      "loss: 0.495769  [27200/175341]\n",
      "loss: 0.365634  [28800/175341]\n",
      "loss: 0.488263  [30400/175341]\n",
      "loss: 0.185464  [32000/175341]\n",
      "loss: 0.406551  [33600/175341]\n",
      "loss: 0.190075  [35200/175341]\n",
      "loss: 0.590187  [36800/175341]\n",
      "loss: 0.176430  [38400/175341]\n",
      "loss: 0.632871  [40000/175341]\n",
      "loss: 0.234239  [41600/175341]\n",
      "loss: 0.232635  [43200/175341]\n",
      "loss: 0.632981  [44800/175341]\n",
      "loss: 0.548997  [46400/175341]\n",
      "loss: 0.331980  [48000/175341]\n",
      "loss: 0.665818  [49600/175341]\n",
      "loss: 0.494278  [51200/175341]\n",
      "loss: 0.743104  [52800/175341]\n",
      "loss: 0.516229  [54400/175341]\n",
      "loss: 0.750992  [56000/175341]\n",
      "loss: 0.269328  [57600/175341]\n",
      "loss: 0.276967  [59200/175341]\n",
      "loss: 0.415343  [60800/175341]\n",
      "loss: 0.398626  [62400/175341]\n",
      "loss: 0.466537  [64000/175341]\n",
      "loss: 0.353836  [65600/175341]\n",
      "loss: 0.547581  [67200/175341]\n",
      "loss: 0.303728  [68800/175341]\n",
      "loss: 0.356681  [70400/175341]\n",
      "loss: 0.310719  [72000/175341]\n",
      "loss: 0.375063  [73600/175341]\n",
      "loss: 0.220709  [75200/175341]\n",
      "loss: 0.446605  [76800/175341]\n",
      "loss: 0.427178  [78400/175341]\n",
      "loss: 0.382354  [80000/175341]\n",
      "loss: 0.544335  [81600/175341]\n",
      "loss: 0.659884  [83200/175341]\n",
      "loss: 0.452279  [84800/175341]\n",
      "loss: 0.226434  [86400/175341]\n",
      "loss: 0.456178  [88000/175341]\n",
      "loss: 1.182812  [89600/175341]\n",
      "loss: 0.745622  [91200/175341]\n",
      "loss: 0.183187  [92800/175341]\n",
      "loss: 0.189736  [94400/175341]\n",
      "loss: 0.058722  [96000/175341]\n",
      "loss: 0.366873  [97600/175341]\n",
      "loss: 0.209852  [99200/175341]\n",
      "loss: 0.444985  [100800/175341]\n",
      "loss: 0.608545  [102400/175341]\n",
      "loss: 0.602449  [104000/175341]\n",
      "loss: 0.149107  [105600/175341]\n",
      "loss: 0.338706  [107200/175341]\n",
      "loss: 0.531147  [108800/175341]\n",
      "loss: 0.591233  [110400/175341]\n",
      "loss: 1.003422  [112000/175341]\n",
      "loss: 0.869149  [113600/175341]\n",
      "loss: 0.709441  [115200/175341]\n",
      "loss: 0.399612  [116800/175341]\n",
      "loss: 0.327200  [118400/175341]\n",
      "loss: 0.523484  [120000/175341]\n",
      "loss: 0.341675  [121600/175341]\n",
      "loss: 0.409737  [123200/175341]\n",
      "loss: 0.217395  [124800/175341]\n",
      "loss: 0.596126  [126400/175341]\n",
      "loss: 0.188448  [128000/175341]\n",
      "loss: 0.192386  [129600/175341]\n",
      "loss: 0.295758  [131200/175341]\n",
      "loss: 0.470424  [132800/175341]\n",
      "loss: 0.509327  [134400/175341]\n",
      "loss: 0.349230  [136000/175341]\n",
      "loss: 0.303778  [137600/175341]\n",
      "loss: 0.306910  [139200/175341]\n",
      "loss: 0.807824  [140800/175341]\n",
      "loss: 0.750335  [142400/175341]\n",
      "loss: 0.616132  [144000/175341]\n",
      "loss: 0.427664  [145600/175341]\n",
      "loss: 0.287186  [147200/175341]\n",
      "loss: 0.199312  [148800/175341]\n",
      "loss: 0.409204  [150400/175341]\n",
      "loss: 0.219112  [152000/175341]\n",
      "loss: 0.874592  [153600/175341]\n",
      "loss: 0.133248  [155200/175341]\n",
      "loss: 0.372178  [156800/175341]\n",
      "loss: 0.288475  [158400/175341]\n",
      "loss: 0.996600  [160000/175341]\n",
      "loss: 0.421406  [161600/175341]\n",
      "loss: 0.341959  [163200/175341]\n",
      "loss: 0.093243  [164800/175341]\n",
      "loss: 0.901376  [166400/175341]\n",
      "loss: 0.543610  [168000/175341]\n",
      "loss: 0.118596  [169600/175341]\n",
      "loss: 0.477566  [171200/175341]\n",
      "loss: 1.167133  [172800/175341]\n",
      "loss: 0.702172  [174400/175341]\n",
      "Train Accuracy: 81.3466%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.547058, F1-score: 76.30%, Macro_F1-Score:  42.31%  \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.306534  [    0/175341]\n",
      "loss: 0.405133  [ 1600/175341]\n",
      "loss: 0.441690  [ 3200/175341]\n",
      "loss: 0.531007  [ 4800/175341]\n",
      "loss: 0.238426  [ 6400/175341]\n",
      "loss: 0.586746  [ 8000/175341]\n",
      "loss: 1.038805  [ 9600/175341]\n",
      "loss: 0.835704  [11200/175341]\n",
      "loss: 0.100609  [12800/175341]\n",
      "loss: 0.481753  [14400/175341]\n",
      "loss: 0.280578  [16000/175341]\n",
      "loss: 0.365225  [17600/175341]\n",
      "loss: 0.712013  [19200/175341]\n",
      "loss: 0.289499  [20800/175341]\n",
      "loss: 0.418018  [22400/175341]\n",
      "loss: 0.196742  [24000/175341]\n",
      "loss: 0.405653  [25600/175341]\n",
      "loss: 0.646923  [27200/175341]\n",
      "loss: 0.338913  [28800/175341]\n",
      "loss: 0.264930  [30400/175341]\n",
      "loss: 0.285688  [32000/175341]\n",
      "loss: 0.403746  [33600/175341]\n",
      "loss: 0.274544  [35200/175341]\n",
      "loss: 0.311335  [36800/175341]\n",
      "loss: 0.428673  [38400/175341]\n",
      "loss: 1.097108  [40000/175341]\n",
      "loss: 0.756221  [41600/175341]\n",
      "loss: 0.532257  [43200/175341]\n",
      "loss: 0.459188  [44800/175341]\n",
      "loss: 0.528883  [46400/175341]\n",
      "loss: 0.516499  [48000/175341]\n",
      "loss: 0.664964  [49600/175341]\n",
      "loss: 0.616751  [51200/175341]\n",
      "loss: 0.391412  [52800/175341]\n",
      "loss: 0.264123  [54400/175341]\n",
      "loss: 0.637444  [56000/175341]\n",
      "loss: 0.436394  [57600/175341]\n",
      "loss: 0.565635  [59200/175341]\n",
      "loss: 0.207672  [60800/175341]\n",
      "loss: 0.560978  [62400/175341]\n",
      "loss: 0.454778  [64000/175341]\n",
      "loss: 0.352345  [65600/175341]\n",
      "loss: 0.318578  [67200/175341]\n",
      "loss: 0.091613  [68800/175341]\n",
      "loss: 0.138478  [70400/175341]\n",
      "loss: 0.270153  [72000/175341]\n",
      "loss: 0.339504  [73600/175341]\n",
      "loss: 0.315660  [75200/175341]\n",
      "loss: 0.449059  [76800/175341]\n",
      "loss: 0.625604  [78400/175341]\n",
      "loss: 0.569706  [80000/175341]\n",
      "loss: 0.694979  [81600/175341]\n",
      "loss: 0.493574  [83200/175341]\n",
      "loss: 0.433068  [84800/175341]\n",
      "loss: 0.614031  [86400/175341]\n",
      "loss: 0.282968  [88000/175341]\n",
      "loss: 0.219993  [89600/175341]\n",
      "loss: 0.154812  [91200/175341]\n",
      "loss: 0.674834  [92800/175341]\n",
      "loss: 0.347792  [94400/175341]\n",
      "loss: 0.522594  [96000/175341]\n",
      "loss: 0.378126  [97600/175341]\n",
      "loss: 0.361059  [99200/175341]\n",
      "loss: 0.383053  [100800/175341]\n",
      "loss: 0.353437  [102400/175341]\n",
      "loss: 0.446637  [104000/175341]\n",
      "loss: 0.216482  [105600/175341]\n",
      "loss: 0.329493  [107200/175341]\n",
      "loss: 0.142547  [108800/175341]\n",
      "loss: 0.159681  [110400/175341]\n",
      "loss: 0.654880  [112000/175341]\n",
      "loss: 0.352592  [113600/175341]\n",
      "loss: 0.879368  [115200/175341]\n",
      "loss: 0.797510  [116800/175341]\n",
      "loss: 0.623104  [118400/175341]\n",
      "loss: 0.728588  [120000/175341]\n",
      "loss: 0.135480  [121600/175341]\n",
      "loss: 0.261613  [123200/175341]\n",
      "loss: 0.272950  [124800/175341]\n",
      "loss: 0.779407  [126400/175341]\n",
      "loss: 0.211284  [128000/175341]\n",
      "loss: 0.514866  [129600/175341]\n",
      "loss: 0.337631  [131200/175341]\n",
      "loss: 0.438366  [132800/175341]\n",
      "loss: 0.378924  [134400/175341]\n",
      "loss: 1.174113  [136000/175341]\n",
      "loss: 0.597693  [137600/175341]\n",
      "loss: 0.394090  [139200/175341]\n",
      "loss: 0.167368  [140800/175341]\n",
      "loss: 0.551413  [142400/175341]\n",
      "loss: 0.636786  [144000/175341]\n",
      "loss: 0.301009  [145600/175341]\n",
      "loss: 0.527089  [147200/175341]\n",
      "loss: 0.180971  [148800/175341]\n",
      "loss: 0.464108  [150400/175341]\n",
      "loss: 0.644353  [152000/175341]\n",
      "loss: 0.409500  [153600/175341]\n",
      "loss: 0.307741  [155200/175341]\n",
      "loss: 0.654581  [156800/175341]\n",
      "loss: 0.802662  [158400/175341]\n",
      "loss: 0.577601  [160000/175341]\n",
      "loss: 0.266815  [161600/175341]\n",
      "loss: 0.540225  [163200/175341]\n",
      "loss: 0.080082  [164800/175341]\n",
      "loss: 0.406118  [166400/175341]\n",
      "loss: 0.846161  [168000/175341]\n",
      "loss: 0.175231  [169600/175341]\n",
      "loss: 0.857326  [171200/175341]\n",
      "loss: 0.564025  [172800/175341]\n",
      "loss: 0.457087  [174400/175341]\n",
      "Train Accuracy: 81.3033%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.526880, F1-score: 76.60%, Macro_F1-Score:  42.59%  \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.167594  [    0/175341]\n",
      "loss: 0.266336  [ 1600/175341]\n",
      "loss: 0.615973  [ 3200/175341]\n",
      "loss: 0.318575  [ 4800/175341]\n",
      "loss: 0.642336  [ 6400/175341]\n",
      "loss: 0.395884  [ 8000/175341]\n",
      "loss: 0.412242  [ 9600/175341]\n",
      "loss: 0.730066  [11200/175341]\n",
      "loss: 0.278739  [12800/175341]\n",
      "loss: 0.265153  [14400/175341]\n",
      "loss: 0.567430  [16000/175341]\n",
      "loss: 0.276049  [17600/175341]\n",
      "loss: 0.465172  [19200/175341]\n",
      "loss: 0.267903  [20800/175341]\n",
      "loss: 0.576747  [22400/175341]\n",
      "loss: 0.475824  [24000/175341]\n",
      "loss: 0.738180  [25600/175341]\n",
      "loss: 0.460917  [27200/175341]\n",
      "loss: 0.298072  [28800/175341]\n",
      "loss: 0.740327  [30400/175341]\n",
      "loss: 0.411804  [32000/175341]\n",
      "loss: 0.570217  [33600/175341]\n",
      "loss: 0.599246  [35200/175341]\n",
      "loss: 0.228151  [36800/175341]\n",
      "loss: 0.542064  [38400/175341]\n",
      "loss: 0.459109  [40000/175341]\n",
      "loss: 0.513492  [41600/175341]\n",
      "loss: 0.512926  [43200/175341]\n",
      "loss: 0.180650  [44800/175341]\n",
      "loss: 0.895420  [46400/175341]\n",
      "loss: 0.262366  [48000/175341]\n",
      "loss: 0.882854  [49600/175341]\n",
      "loss: 0.314013  [51200/175341]\n",
      "loss: 0.830530  [52800/175341]\n",
      "loss: 0.185081  [54400/175341]\n",
      "loss: 0.493505  [56000/175341]\n",
      "loss: 0.499327  [57600/175341]\n",
      "loss: 0.343916  [59200/175341]\n",
      "loss: 0.676509  [60800/175341]\n",
      "loss: 0.288481  [62400/175341]\n",
      "loss: 0.736973  [64000/175341]\n",
      "loss: 0.539157  [65600/175341]\n",
      "loss: 0.532569  [67200/175341]\n",
      "loss: 0.659646  [68800/175341]\n",
      "loss: 0.464188  [70400/175341]\n",
      "loss: 0.402483  [72000/175341]\n",
      "loss: 0.416450  [73600/175341]\n",
      "loss: 0.474861  [75200/175341]\n",
      "loss: 0.262499  [76800/175341]\n",
      "loss: 0.568757  [78400/175341]\n",
      "loss: 0.429947  [80000/175341]\n",
      "loss: 0.827903  [81600/175341]\n",
      "loss: 0.341384  [83200/175341]\n",
      "loss: 0.347596  [84800/175341]\n",
      "loss: 0.550007  [86400/175341]\n",
      "loss: 0.339739  [88000/175341]\n",
      "loss: 0.469896  [89600/175341]\n",
      "loss: 0.232481  [91200/175341]\n",
      "loss: 0.467720  [92800/175341]\n",
      "loss: 0.955027  [94400/175341]\n",
      "loss: 0.208006  [96000/175341]\n",
      "loss: 0.548960  [97600/175341]\n",
      "loss: 0.159161  [99200/175341]\n",
      "loss: 0.366826  [100800/175341]\n",
      "loss: 0.447201  [102400/175341]\n",
      "loss: 0.276930  [104000/175341]\n",
      "loss: 0.368208  [105600/175341]\n",
      "loss: 0.719039  [107200/175341]\n",
      "loss: 0.559095  [108800/175341]\n",
      "loss: 0.197109  [110400/175341]\n",
      "loss: 0.755565  [112000/175341]\n",
      "loss: 0.662647  [113600/175341]\n",
      "loss: 0.249763  [115200/175341]\n",
      "loss: 0.489309  [116800/175341]\n",
      "loss: 0.266356  [118400/175341]\n",
      "loss: 0.621503  [120000/175341]\n",
      "loss: 0.501517  [121600/175341]\n",
      "loss: 0.429849  [123200/175341]\n",
      "loss: 0.573591  [124800/175341]\n",
      "loss: 0.290757  [126400/175341]\n",
      "loss: 0.382700  [128000/175341]\n",
      "loss: 0.497895  [129600/175341]\n",
      "loss: 0.822973  [131200/175341]\n",
      "loss: 0.096192  [132800/175341]\n",
      "loss: 0.345947  [134400/175341]\n",
      "loss: 0.330015  [136000/175341]\n",
      "loss: 0.410377  [137600/175341]\n",
      "loss: 0.739657  [139200/175341]\n",
      "loss: 0.478617  [140800/175341]\n",
      "loss: 0.955165  [142400/175341]\n",
      "loss: 0.513206  [144000/175341]\n",
      "loss: 0.687971  [145600/175341]\n",
      "loss: 0.664675  [147200/175341]\n",
      "loss: 0.438622  [148800/175341]\n",
      "loss: 0.248719  [150400/175341]\n",
      "loss: 0.499170  [152000/175341]\n",
      "loss: 0.428096  [153600/175341]\n",
      "loss: 0.922695  [155200/175341]\n",
      "loss: 0.522242  [156800/175341]\n",
      "loss: 0.255675  [158400/175341]\n",
      "loss: 0.154334  [160000/175341]\n",
      "loss: 0.534121  [161600/175341]\n",
      "loss: 0.626805  [163200/175341]\n",
      "loss: 0.641365  [164800/175341]\n",
      "loss: 0.399786  [166400/175341]\n",
      "loss: 0.225740  [168000/175341]\n",
      "loss: 0.258705  [169600/175341]\n",
      "loss: 0.221415  [171200/175341]\n",
      "loss: 0.829981  [172800/175341]\n",
      "loss: 0.387786  [174400/175341]\n",
      "Train Accuracy: 81.3592%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.535372, F1-score: 76.20%, Macro_F1-Score:  41.98%  \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.243600  [    0/175341]\n",
      "loss: 0.459492  [ 1600/175341]\n",
      "loss: 0.596746  [ 3200/175341]\n",
      "loss: 0.540550  [ 4800/175341]\n",
      "loss: 0.333795  [ 6400/175341]\n",
      "loss: 0.318790  [ 8000/175341]\n",
      "loss: 0.337146  [ 9600/175341]\n",
      "loss: 0.407534  [11200/175341]\n",
      "loss: 0.493971  [12800/175341]\n",
      "loss: 0.335643  [14400/175341]\n",
      "loss: 0.667840  [16000/175341]\n",
      "loss: 0.343983  [17600/175341]\n",
      "loss: 0.501968  [19200/175341]\n",
      "loss: 0.672440  [20800/175341]\n",
      "loss: 0.580024  [22400/175341]\n",
      "loss: 0.456807  [24000/175341]\n",
      "loss: 0.486547  [25600/175341]\n",
      "loss: 0.689345  [27200/175341]\n",
      "loss: 0.359116  [28800/175341]\n",
      "loss: 0.336114  [30400/175341]\n",
      "loss: 0.362914  [32000/175341]\n",
      "loss: 0.688271  [33600/175341]\n",
      "loss: 0.244062  [35200/175341]\n",
      "loss: 0.226496  [36800/175341]\n",
      "loss: 0.750801  [38400/175341]\n",
      "loss: 0.414447  [40000/175341]\n",
      "loss: 0.339015  [41600/175341]\n",
      "loss: 0.948568  [43200/175341]\n",
      "loss: 0.255987  [44800/175341]\n",
      "loss: 0.547303  [46400/175341]\n",
      "loss: 0.358683  [48000/175341]\n",
      "loss: 0.158008  [49600/175341]\n",
      "loss: 0.505334  [51200/175341]\n",
      "loss: 0.556450  [52800/175341]\n",
      "loss: 0.572443  [54400/175341]\n",
      "loss: 0.255223  [56000/175341]\n",
      "loss: 0.243405  [57600/175341]\n",
      "loss: 0.464375  [59200/175341]\n",
      "loss: 0.367221  [60800/175341]\n",
      "loss: 0.540558  [62400/175341]\n",
      "loss: 0.454100  [64000/175341]\n",
      "loss: 0.462607  [65600/175341]\n",
      "loss: 0.554954  [67200/175341]\n",
      "loss: 0.125446  [68800/175341]\n",
      "loss: 0.437344  [70400/175341]\n",
      "loss: 0.431008  [72000/175341]\n",
      "loss: 0.405991  [73600/175341]\n",
      "loss: 0.518871  [75200/175341]\n",
      "loss: 0.639036  [76800/175341]\n",
      "loss: 0.818105  [78400/175341]\n",
      "loss: 0.471309  [80000/175341]\n",
      "loss: 0.501072  [81600/175341]\n",
      "loss: 0.303913  [83200/175341]\n",
      "loss: 0.771824  [84800/175341]\n",
      "loss: 0.455409  [86400/175341]\n",
      "loss: 0.296008  [88000/175341]\n",
      "loss: 0.376314  [89600/175341]\n",
      "loss: 0.373410  [91200/175341]\n",
      "loss: 0.432707  [92800/175341]\n",
      "loss: 0.666501  [94400/175341]\n",
      "loss: 0.203317  [96000/175341]\n",
      "loss: 0.291278  [97600/175341]\n",
      "loss: 0.373576  [99200/175341]\n",
      "loss: 0.202305  [100800/175341]\n",
      "loss: 0.544400  [102400/175341]\n",
      "loss: 0.430527  [104000/175341]\n",
      "loss: 0.496193  [105600/175341]\n",
      "loss: 0.368234  [107200/175341]\n",
      "loss: 0.382214  [108800/175341]\n",
      "loss: 0.157768  [110400/175341]\n",
      "loss: 0.486000  [112000/175341]\n",
      "loss: 0.226589  [113600/175341]\n",
      "loss: 0.346694  [115200/175341]\n",
      "loss: 0.096149  [116800/175341]\n",
      "loss: 0.351042  [118400/175341]\n",
      "loss: 1.057270  [120000/175341]\n",
      "loss: 0.632705  [121600/175341]\n",
      "loss: 0.684818  [123200/175341]\n",
      "loss: 0.237364  [124800/175341]\n",
      "loss: 0.617657  [126400/175341]\n",
      "loss: 0.886229  [128000/175341]\n",
      "loss: 0.583529  [129600/175341]\n",
      "loss: 0.612805  [131200/175341]\n",
      "loss: 0.319782  [132800/175341]\n",
      "loss: 0.816904  [134400/175341]\n",
      "loss: 0.965078  [136000/175341]\n",
      "loss: 0.601376  [137600/175341]\n",
      "loss: 0.486625  [139200/175341]\n",
      "loss: 0.187709  [140800/175341]\n",
      "loss: 0.473047  [142400/175341]\n",
      "loss: 0.275310  [144000/175341]\n",
      "loss: 0.467540  [145600/175341]\n",
      "loss: 0.459424  [147200/175341]\n",
      "loss: 0.187228  [148800/175341]\n",
      "loss: 0.854705  [150400/175341]\n",
      "loss: 0.756764  [152000/175341]\n",
      "loss: 0.284100  [153600/175341]\n",
      "loss: 0.269036  [155200/175341]\n",
      "loss: 0.492994  [156800/175341]\n",
      "loss: 0.216817  [158400/175341]\n",
      "loss: 0.320670  [160000/175341]\n",
      "loss: 0.514583  [161600/175341]\n",
      "loss: 0.649663  [163200/175341]\n",
      "loss: 0.713547  [164800/175341]\n",
      "loss: 0.175871  [166400/175341]\n",
      "loss: 0.677139  [168000/175341]\n",
      "loss: 0.306733  [169600/175341]\n",
      "loss: 0.517120  [171200/175341]\n",
      "loss: 0.476227  [172800/175341]\n",
      "loss: 0.382238  [174400/175341]\n",
      "Train Accuracy: 81.3421%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.541262, F1-score: 76.01%, Macro_F1-Score:  42.64%  \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.550521  [    0/175341]\n",
      "loss: 0.645485  [ 1600/175341]\n",
      "loss: 0.265946  [ 3200/175341]\n",
      "loss: 0.626407  [ 4800/175341]\n",
      "loss: 0.257829  [ 6400/175341]\n",
      "loss: 0.519911  [ 8000/175341]\n",
      "loss: 0.425745  [ 9600/175341]\n",
      "loss: 0.218688  [11200/175341]\n",
      "loss: 0.184238  [12800/175341]\n",
      "loss: 0.501372  [14400/175341]\n",
      "loss: 0.628276  [16000/175341]\n",
      "loss: 0.244148  [17600/175341]\n",
      "loss: 0.532446  [19200/175341]\n",
      "loss: 0.760856  [20800/175341]\n",
      "loss: 0.133627  [22400/175341]\n",
      "loss: 0.332082  [24000/175341]\n",
      "loss: 0.408786  [25600/175341]\n",
      "loss: 0.725441  [27200/175341]\n",
      "loss: 0.275325  [28800/175341]\n",
      "loss: 0.495092  [30400/175341]\n",
      "loss: 0.620651  [32000/175341]\n",
      "loss: 0.396875  [33600/175341]\n",
      "loss: 0.599020  [35200/175341]\n",
      "loss: 0.665745  [36800/175341]\n",
      "loss: 0.209409  [38400/175341]\n",
      "loss: 0.428756  [40000/175341]\n",
      "loss: 0.258131  [41600/175341]\n",
      "loss: 0.297745  [43200/175341]\n",
      "loss: 0.700438  [44800/175341]\n",
      "loss: 0.334997  [46400/175341]\n",
      "loss: 0.344229  [48000/175341]\n",
      "loss: 0.896682  [49600/175341]\n",
      "loss: 0.482941  [51200/175341]\n",
      "loss: 0.329327  [52800/175341]\n",
      "loss: 0.872591  [54400/175341]\n",
      "loss: 0.569749  [56000/175341]\n",
      "loss: 0.444487  [57600/175341]\n",
      "loss: 0.363668  [59200/175341]\n",
      "loss: 0.625327  [60800/175341]\n",
      "loss: 0.428006  [62400/175341]\n",
      "loss: 0.431812  [64000/175341]\n",
      "loss: 0.350315  [65600/175341]\n",
      "loss: 1.185138  [67200/175341]\n",
      "loss: 0.223966  [68800/175341]\n",
      "loss: 0.491868  [70400/175341]\n",
      "loss: 0.196206  [72000/175341]\n",
      "loss: 0.457609  [73600/175341]\n",
      "loss: 0.255328  [75200/175341]\n",
      "loss: 0.505323  [76800/175341]\n",
      "loss: 0.488319  [78400/175341]\n",
      "loss: 0.339979  [80000/175341]\n",
      "loss: 0.392890  [81600/175341]\n",
      "loss: 0.422929  [83200/175341]\n",
      "loss: 0.702833  [84800/175341]\n",
      "loss: 0.622794  [86400/175341]\n",
      "loss: 0.813483  [88000/175341]\n",
      "loss: 0.368127  [89600/175341]\n",
      "loss: 0.422936  [91200/175341]\n",
      "loss: 0.562928  [92800/175341]\n",
      "loss: 0.399243  [94400/175341]\n",
      "loss: 0.446639  [96000/175341]\n",
      "loss: 0.535942  [97600/175341]\n",
      "loss: 0.154734  [99200/175341]\n",
      "loss: 0.763094  [100800/175341]\n",
      "loss: 0.462270  [102400/175341]\n",
      "loss: 0.457729  [104000/175341]\n",
      "loss: 0.589665  [105600/175341]\n",
      "loss: 0.356374  [107200/175341]\n",
      "loss: 0.418771  [108800/175341]\n",
      "loss: 0.532643  [110400/175341]\n",
      "loss: 0.472670  [112000/175341]\n",
      "loss: 0.591393  [113600/175341]\n",
      "loss: 0.227408  [115200/175341]\n",
      "loss: 0.308713  [116800/175341]\n",
      "loss: 0.342949  [118400/175341]\n",
      "loss: 0.351088  [120000/175341]\n",
      "loss: 0.180045  [121600/175341]\n",
      "loss: 0.292275  [123200/175341]\n",
      "loss: 0.169864  [124800/175341]\n",
      "loss: 0.294059  [126400/175341]\n",
      "loss: 0.982128  [128000/175341]\n",
      "loss: 0.306660  [129600/175341]\n",
      "loss: 0.218621  [131200/175341]\n",
      "loss: 0.428722  [132800/175341]\n",
      "loss: 0.343289  [134400/175341]\n",
      "loss: 0.372393  [136000/175341]\n",
      "loss: 0.247985  [137600/175341]\n",
      "loss: 0.184736  [139200/175341]\n",
      "loss: 0.475281  [140800/175341]\n",
      "loss: 0.518887  [142400/175341]\n",
      "loss: 0.454874  [144000/175341]\n",
      "loss: 0.436359  [145600/175341]\n",
      "loss: 0.457250  [147200/175341]\n",
      "loss: 0.123429  [148800/175341]\n",
      "loss: 0.632476  [150400/175341]\n",
      "loss: 0.516417  [152000/175341]\n",
      "loss: 0.559577  [153600/175341]\n",
      "loss: 0.437145  [155200/175341]\n",
      "loss: 0.638316  [156800/175341]\n",
      "loss: 0.395995  [158400/175341]\n",
      "loss: 0.168913  [160000/175341]\n",
      "loss: 0.570926  [161600/175341]\n",
      "loss: 0.276328  [163200/175341]\n",
      "loss: 0.282700  [164800/175341]\n",
      "loss: 0.369399  [166400/175341]\n",
      "loss: 0.620205  [168000/175341]\n",
      "loss: 0.887252  [169600/175341]\n",
      "loss: 0.825234  [171200/175341]\n",
      "loss: 0.313779  [172800/175341]\n",
      "loss: 0.528742  [174400/175341]\n",
      "Train Accuracy: 81.3603%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.554844, F1-score: 75.59%, Macro_F1-Score:  41.69%  \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.572801  [    0/175341]\n",
      "loss: 0.737221  [ 1600/175341]\n",
      "loss: 0.529371  [ 3200/175341]\n",
      "loss: 0.665165  [ 4800/175341]\n",
      "loss: 0.665178  [ 6400/175341]\n",
      "loss: 0.189721  [ 8000/175341]\n",
      "loss: 0.793983  [ 9600/175341]\n",
      "loss: 0.438040  [11200/175341]\n",
      "loss: 0.318738  [12800/175341]\n",
      "loss: 0.260788  [14400/175341]\n",
      "loss: 0.807423  [16000/175341]\n",
      "loss: 0.853308  [17600/175341]\n",
      "loss: 0.555707  [19200/175341]\n",
      "loss: 0.516634  [20800/175341]\n",
      "loss: 0.733401  [22400/175341]\n",
      "loss: 0.367770  [24000/175341]\n",
      "loss: 0.828232  [25600/175341]\n",
      "loss: 0.309092  [27200/175341]\n",
      "loss: 0.643370  [28800/175341]\n",
      "loss: 0.531354  [30400/175341]\n",
      "loss: 0.272180  [32000/175341]\n",
      "loss: 0.684902  [33600/175341]\n",
      "loss: 0.346876  [35200/175341]\n",
      "loss: 0.158931  [36800/175341]\n",
      "loss: 0.443671  [38400/175341]\n",
      "loss: 0.552912  [40000/175341]\n",
      "loss: 0.315325  [41600/175341]\n",
      "loss: 0.640951  [43200/175341]\n",
      "loss: 0.229709  [44800/175341]\n",
      "loss: 0.199482  [46400/175341]\n",
      "loss: 0.406671  [48000/175341]\n",
      "loss: 0.980661  [49600/175341]\n",
      "loss: 0.097266  [51200/175341]\n",
      "loss: 0.313049  [52800/175341]\n",
      "loss: 0.687158  [54400/175341]\n",
      "loss: 0.653147  [56000/175341]\n",
      "loss: 0.293933  [57600/175341]\n",
      "loss: 0.795707  [59200/175341]\n",
      "loss: 0.343491  [60800/175341]\n",
      "loss: 0.607841  [62400/175341]\n",
      "loss: 0.485440  [64000/175341]\n",
      "loss: 0.434331  [65600/175341]\n",
      "loss: 0.210413  [67200/175341]\n",
      "loss: 0.440517  [68800/175341]\n",
      "loss: 0.347914  [70400/175341]\n",
      "loss: 0.493533  [72000/175341]\n",
      "loss: 0.719216  [73600/175341]\n",
      "loss: 0.082723  [75200/175341]\n",
      "loss: 0.671259  [76800/175341]\n",
      "loss: 0.303470  [78400/175341]\n",
      "loss: 0.642210  [80000/175341]\n",
      "loss: 0.504216  [81600/175341]\n",
      "loss: 0.577916  [83200/175341]\n",
      "loss: 0.508155  [84800/175341]\n",
      "loss: 0.323672  [86400/175341]\n",
      "loss: 0.265882  [88000/175341]\n",
      "loss: 0.519281  [89600/175341]\n",
      "loss: 0.291637  [91200/175341]\n",
      "loss: 0.416657  [92800/175341]\n",
      "loss: 0.451925  [94400/175341]\n",
      "loss: 0.649302  [96000/175341]\n",
      "loss: 0.309586  [97600/175341]\n",
      "loss: 0.313504  [99200/175341]\n",
      "loss: 1.045172  [100800/175341]\n",
      "loss: 0.788383  [102400/175341]\n",
      "loss: 0.646111  [104000/175341]\n",
      "loss: 0.607647  [105600/175341]\n",
      "loss: 0.382029  [107200/175341]\n",
      "loss: 0.646912  [108800/175341]\n",
      "loss: 0.173213  [110400/175341]\n",
      "loss: 0.492998  [112000/175341]\n",
      "loss: 0.550996  [113600/175341]\n",
      "loss: 0.448039  [115200/175341]\n",
      "loss: 0.249076  [116800/175341]\n",
      "loss: 0.515780  [118400/175341]\n",
      "loss: 0.359738  [120000/175341]\n",
      "loss: 0.513982  [121600/175341]\n",
      "loss: 0.357991  [123200/175341]\n",
      "loss: 0.484721  [124800/175341]\n",
      "loss: 0.515130  [126400/175341]\n",
      "loss: 0.577548  [128000/175341]\n",
      "loss: 0.438296  [129600/175341]\n",
      "loss: 0.675363  [131200/175341]\n",
      "loss: 0.414206  [132800/175341]\n",
      "loss: 0.797560  [134400/175341]\n",
      "loss: 0.446538  [136000/175341]\n",
      "loss: 0.739816  [137600/175341]\n",
      "loss: 0.505005  [139200/175341]\n",
      "loss: 0.737503  [140800/175341]\n",
      "loss: 0.472964  [142400/175341]\n",
      "loss: 0.552528  [144000/175341]\n",
      "loss: 0.632100  [145600/175341]\n",
      "loss: 0.332870  [147200/175341]\n",
      "loss: 0.533966  [148800/175341]\n",
      "loss: 0.849979  [150400/175341]\n",
      "loss: 0.364857  [152000/175341]\n",
      "loss: 0.479815  [153600/175341]\n",
      "loss: 0.504000  [155200/175341]\n",
      "loss: 0.703098  [156800/175341]\n",
      "loss: 0.585032  [158400/175341]\n",
      "loss: 0.588723  [160000/175341]\n",
      "loss: 0.362690  [161600/175341]\n",
      "loss: 0.409430  [163200/175341]\n",
      "loss: 0.274752  [164800/175341]\n",
      "loss: 0.583125  [166400/175341]\n",
      "loss: 0.678832  [168000/175341]\n",
      "loss: 0.433022  [169600/175341]\n",
      "loss: 0.383684  [171200/175341]\n",
      "loss: 0.571789  [172800/175341]\n",
      "loss: 0.574591  [174400/175341]\n",
      "Train Accuracy: 81.3489%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.534863, F1-score: 76.29%, Macro_F1-Score:  41.92%  \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.498166  [    0/175341]\n",
      "loss: 0.550696  [ 1600/175341]\n",
      "loss: 0.962096  [ 3200/175341]\n",
      "loss: 0.870690  [ 4800/175341]\n",
      "loss: 0.263849  [ 6400/175341]\n",
      "loss: 0.716901  [ 8000/175341]\n",
      "loss: 0.358250  [ 9600/175341]\n",
      "loss: 0.919648  [11200/175341]\n",
      "loss: 0.364887  [12800/175341]\n",
      "loss: 0.503301  [14400/175341]\n",
      "loss: 0.203586  [16000/175341]\n",
      "loss: 0.483349  [17600/175341]\n",
      "loss: 0.763615  [19200/175341]\n",
      "loss: 0.413956  [20800/175341]\n",
      "loss: 0.322806  [22400/175341]\n",
      "loss: 0.768018  [24000/175341]\n",
      "loss: 0.512827  [25600/175341]\n",
      "loss: 0.555879  [27200/175341]\n",
      "loss: 0.470097  [28800/175341]\n",
      "loss: 0.820067  [30400/175341]\n",
      "loss: 0.510532  [32000/175341]\n",
      "loss: 0.293327  [33600/175341]\n",
      "loss: 0.314182  [35200/175341]\n",
      "loss: 0.287521  [36800/175341]\n",
      "loss: 0.531280  [38400/175341]\n",
      "loss: 0.210774  [40000/175341]\n",
      "loss: 0.689823  [41600/175341]\n",
      "loss: 0.488461  [43200/175341]\n",
      "loss: 0.558160  [44800/175341]\n",
      "loss: 0.847902  [46400/175341]\n",
      "loss: 0.370323  [48000/175341]\n",
      "loss: 0.454402  [49600/175341]\n",
      "loss: 0.548594  [51200/175341]\n",
      "loss: 0.302331  [52800/175341]\n",
      "loss: 0.816439  [54400/175341]\n",
      "loss: 0.316978  [56000/175341]\n",
      "loss: 0.585164  [57600/175341]\n",
      "loss: 0.825034  [59200/175341]\n",
      "loss: 0.239543  [60800/175341]\n",
      "loss: 0.363106  [62400/175341]\n",
      "loss: 0.602540  [64000/175341]\n",
      "loss: 0.274393  [65600/175341]\n",
      "loss: 0.444755  [67200/175341]\n",
      "loss: 0.355132  [68800/175341]\n",
      "loss: 0.308699  [70400/175341]\n",
      "loss: 0.751179  [72000/175341]\n",
      "loss: 0.074753  [73600/175341]\n",
      "loss: 0.716472  [75200/175341]\n",
      "loss: 0.272916  [76800/175341]\n",
      "loss: 0.378345  [78400/175341]\n",
      "loss: 0.550976  [80000/175341]\n",
      "loss: 0.329935  [81600/175341]\n",
      "loss: 0.713423  [83200/175341]\n",
      "loss: 0.587984  [84800/175341]\n",
      "loss: 0.315782  [86400/175341]\n",
      "loss: 0.939350  [88000/175341]\n",
      "loss: 0.272736  [89600/175341]\n",
      "loss: 0.209466  [91200/175341]\n",
      "loss: 0.276305  [92800/175341]\n",
      "loss: 0.426867  [94400/175341]\n",
      "loss: 0.547304  [96000/175341]\n",
      "loss: 0.381149  [97600/175341]\n",
      "loss: 0.685072  [99200/175341]\n",
      "loss: 0.267675  [100800/175341]\n",
      "loss: 0.426730  [102400/175341]\n",
      "loss: 0.646902  [104000/175341]\n",
      "loss: 0.424535  [105600/175341]\n",
      "loss: 0.430977  [107200/175341]\n",
      "loss: 0.376403  [108800/175341]\n",
      "loss: 0.334353  [110400/175341]\n",
      "loss: 0.480849  [112000/175341]\n",
      "loss: 0.163156  [113600/175341]\n",
      "loss: 0.718366  [115200/175341]\n",
      "loss: 0.467288  [116800/175341]\n",
      "loss: 0.250788  [118400/175341]\n",
      "loss: 0.473339  [120000/175341]\n",
      "loss: 0.264441  [121600/175341]\n",
      "loss: 0.449014  [123200/175341]\n",
      "loss: 0.865935  [124800/175341]\n",
      "loss: 0.466352  [126400/175341]\n",
      "loss: 0.583962  [128000/175341]\n",
      "loss: 0.276078  [129600/175341]\n",
      "loss: 0.590177  [131200/175341]\n",
      "loss: 0.551245  [132800/175341]\n",
      "loss: 0.572782  [134400/175341]\n",
      "loss: 0.373513  [136000/175341]\n",
      "loss: 0.155663  [137600/175341]\n",
      "loss: 0.671696  [139200/175341]\n",
      "loss: 0.240532  [140800/175341]\n",
      "loss: 1.013967  [142400/175341]\n",
      "loss: 0.278758  [144000/175341]\n",
      "loss: 0.520305  [145600/175341]\n",
      "loss: 0.600744  [147200/175341]\n",
      "loss: 0.319575  [148800/175341]\n",
      "loss: 0.867988  [150400/175341]\n",
      "loss: 0.282646  [152000/175341]\n",
      "loss: 0.494486  [153600/175341]\n",
      "loss: 0.513158  [155200/175341]\n",
      "loss: 0.277610  [156800/175341]\n",
      "loss: 0.789354  [158400/175341]\n",
      "loss: 0.296417  [160000/175341]\n",
      "loss: 0.325135  [161600/175341]\n",
      "loss: 0.413565  [163200/175341]\n",
      "loss: 0.641271  [164800/175341]\n",
      "loss: 0.532781  [166400/175341]\n",
      "loss: 0.503086  [168000/175341]\n",
      "loss: 0.854271  [169600/175341]\n",
      "loss: 0.374936  [171200/175341]\n",
      "loss: 0.476302  [172800/175341]\n",
      "loss: 0.255855  [174400/175341]\n",
      "Train Accuracy: 81.3329%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.544996, F1-score: 75.65%, Macro_F1-Score:  41.95%  \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.391240  [    0/175341]\n",
      "loss: 0.179887  [ 1600/175341]\n",
      "loss: 0.306395  [ 3200/175341]\n",
      "loss: 0.181726  [ 4800/175341]\n",
      "loss: 0.499865  [ 6400/175341]\n",
      "loss: 0.821054  [ 8000/175341]\n",
      "loss: 0.822162  [ 9600/175341]\n",
      "loss: 0.340019  [11200/175341]\n",
      "loss: 0.395529  [12800/175341]\n",
      "loss: 0.501692  [14400/175341]\n",
      "loss: 0.493615  [16000/175341]\n",
      "loss: 0.435883  [17600/175341]\n",
      "loss: 0.459730  [19200/175341]\n",
      "loss: 0.371596  [20800/175341]\n",
      "loss: 0.473846  [22400/175341]\n",
      "loss: 0.236217  [24000/175341]\n",
      "loss: 0.848694  [25600/175341]\n",
      "loss: 0.466127  [27200/175341]\n",
      "loss: 0.822186  [28800/175341]\n",
      "loss: 0.460442  [30400/175341]\n",
      "loss: 0.806437  [32000/175341]\n",
      "loss: 0.360935  [33600/175341]\n",
      "loss: 0.313548  [35200/175341]\n",
      "loss: 0.796488  [36800/175341]\n",
      "loss: 0.362829  [38400/175341]\n",
      "loss: 0.508177  [40000/175341]\n",
      "loss: 0.498970  [41600/175341]\n",
      "loss: 0.391722  [43200/175341]\n",
      "loss: 0.151178  [44800/175341]\n",
      "loss: 1.010336  [46400/175341]\n",
      "loss: 0.439663  [48000/175341]\n",
      "loss: 0.469497  [49600/175341]\n",
      "loss: 0.368732  [51200/175341]\n",
      "loss: 0.376997  [52800/175341]\n",
      "loss: 0.220987  [54400/175341]\n",
      "loss: 0.567195  [56000/175341]\n",
      "loss: 0.483579  [57600/175341]\n",
      "loss: 0.216880  [59200/175341]\n",
      "loss: 0.362159  [60800/175341]\n",
      "loss: 0.633980  [62400/175341]\n",
      "loss: 0.235233  [64000/175341]\n",
      "loss: 0.398993  [65600/175341]\n",
      "loss: 0.168173  [67200/175341]\n",
      "loss: 0.654579  [68800/175341]\n",
      "loss: 0.572919  [70400/175341]\n",
      "loss: 0.836379  [72000/175341]\n",
      "loss: 0.362923  [73600/175341]\n",
      "loss: 0.397243  [75200/175341]\n",
      "loss: 0.335413  [76800/175341]\n",
      "loss: 0.736591  [78400/175341]\n",
      "loss: 0.303995  [80000/175341]\n",
      "loss: 0.216533  [81600/175341]\n",
      "loss: 0.375070  [83200/175341]\n",
      "loss: 0.383873  [84800/175341]\n",
      "loss: 0.367489  [86400/175341]\n",
      "loss: 0.260555  [88000/175341]\n",
      "loss: 0.494954  [89600/175341]\n",
      "loss: 0.238353  [91200/175341]\n",
      "loss: 0.437407  [92800/175341]\n",
      "loss: 0.454535  [94400/175341]\n",
      "loss: 0.392729  [96000/175341]\n",
      "loss: 0.459298  [97600/175341]\n",
      "loss: 0.357458  [99200/175341]\n",
      "loss: 0.279182  [100800/175341]\n",
      "loss: 0.615759  [102400/175341]\n",
      "loss: 0.496994  [104000/175341]\n",
      "loss: 0.458448  [105600/175341]\n",
      "loss: 0.509711  [107200/175341]\n",
      "loss: 0.652991  [108800/175341]\n",
      "loss: 0.081754  [110400/175341]\n",
      "loss: 0.416551  [112000/175341]\n",
      "loss: 0.187886  [113600/175341]\n",
      "loss: 0.770612  [115200/175341]\n",
      "loss: 0.222945  [116800/175341]\n",
      "loss: 0.556641  [118400/175341]\n",
      "loss: 0.429167  [120000/175341]\n",
      "loss: 0.147822  [121600/175341]\n",
      "loss: 0.552321  [123200/175341]\n",
      "loss: 0.350817  [124800/175341]\n",
      "loss: 0.416324  [126400/175341]\n",
      "loss: 0.243245  [128000/175341]\n",
      "loss: 0.438641  [129600/175341]\n",
      "loss: 0.370885  [131200/175341]\n",
      "loss: 0.289969  [132800/175341]\n",
      "loss: 0.596436  [134400/175341]\n",
      "loss: 0.454335  [136000/175341]\n",
      "loss: 0.189704  [137600/175341]\n",
      "loss: 0.571882  [139200/175341]\n",
      "loss: 0.426509  [140800/175341]\n",
      "loss: 0.333777  [142400/175341]\n",
      "loss: 0.192240  [144000/175341]\n",
      "loss: 0.364936  [145600/175341]\n",
      "loss: 0.513109  [147200/175341]\n",
      "loss: 0.211596  [148800/175341]\n",
      "loss: 0.490005  [150400/175341]\n",
      "loss: 0.283335  [152000/175341]\n",
      "loss: 0.256350  [153600/175341]\n",
      "loss: 0.233765  [155200/175341]\n",
      "loss: 0.517740  [156800/175341]\n",
      "loss: 0.633301  [158400/175341]\n",
      "loss: 0.518484  [160000/175341]\n",
      "loss: 0.808941  [161600/175341]\n",
      "loss: 0.635116  [163200/175341]\n",
      "loss: 0.275348  [164800/175341]\n",
      "loss: 0.195916  [166400/175341]\n",
      "loss: 0.131371  [168000/175341]\n",
      "loss: 0.377087  [169600/175341]\n",
      "loss: 0.336936  [171200/175341]\n",
      "loss: 0.273411  [172800/175341]\n",
      "loss: 0.264949  [174400/175341]\n",
      "Train Accuracy: 81.3615%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.536393, F1-score: 76.98%, Macro_F1-Score:  43.52%  \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.461735  [    0/175341]\n",
      "loss: 0.515223  [ 1600/175341]\n",
      "loss: 0.145371  [ 3200/175341]\n",
      "loss: 0.494410  [ 4800/175341]\n",
      "loss: 0.519374  [ 6400/175341]\n",
      "loss: 0.336794  [ 8000/175341]\n",
      "loss: 0.475155  [ 9600/175341]\n",
      "loss: 0.272477  [11200/175341]\n",
      "loss: 0.564622  [12800/175341]\n",
      "loss: 0.237721  [14400/175341]\n",
      "loss: 0.425151  [16000/175341]\n",
      "loss: 0.420129  [17600/175341]\n",
      "loss: 0.457476  [19200/175341]\n",
      "loss: 0.467149  [20800/175341]\n",
      "loss: 0.401589  [22400/175341]\n",
      "loss: 0.265871  [24000/175341]\n",
      "loss: 0.258141  [25600/175341]\n",
      "loss: 0.313706  [27200/175341]\n",
      "loss: 0.324431  [28800/175341]\n",
      "loss: 0.550780  [30400/175341]\n",
      "loss: 0.681476  [32000/175341]\n",
      "loss: 0.281022  [33600/175341]\n",
      "loss: 0.229174  [35200/175341]\n",
      "loss: 0.965053  [36800/175341]\n",
      "loss: 0.406336  [38400/175341]\n",
      "loss: 0.274979  [40000/175341]\n",
      "loss: 0.889660  [41600/175341]\n",
      "loss: 0.343783  [43200/175341]\n",
      "loss: 0.342501  [44800/175341]\n",
      "loss: 0.819485  [46400/175341]\n",
      "loss: 0.133327  [48000/175341]\n",
      "loss: 0.482560  [49600/175341]\n",
      "loss: 0.857936  [51200/175341]\n",
      "loss: 0.268754  [52800/175341]\n",
      "loss: 0.515438  [54400/175341]\n",
      "loss: 0.421187  [56000/175341]\n",
      "loss: 0.263140  [57600/175341]\n",
      "loss: 0.667068  [59200/175341]\n",
      "loss: 0.352362  [60800/175341]\n",
      "loss: 0.449984  [62400/175341]\n",
      "loss: 0.700842  [64000/175341]\n",
      "loss: 0.491299  [65600/175341]\n",
      "loss: 0.707519  [67200/175341]\n",
      "loss: 0.184464  [68800/175341]\n",
      "loss: 0.183812  [70400/175341]\n",
      "loss: 0.522916  [72000/175341]\n",
      "loss: 0.538972  [73600/175341]\n",
      "loss: 0.350216  [75200/175341]\n",
      "loss: 0.470270  [76800/175341]\n",
      "loss: 0.424310  [78400/175341]\n",
      "loss: 0.090451  [80000/175341]\n",
      "loss: 0.363498  [81600/175341]\n",
      "loss: 0.687085  [83200/175341]\n",
      "loss: 0.536932  [84800/175341]\n",
      "loss: 1.029829  [86400/175341]\n",
      "loss: 0.321769  [88000/175341]\n",
      "loss: 0.926242  [89600/175341]\n",
      "loss: 0.578380  [91200/175341]\n",
      "loss: 0.573380  [92800/175341]\n",
      "loss: 0.239052  [94400/175341]\n",
      "loss: 0.394919  [96000/175341]\n",
      "loss: 0.391818  [97600/175341]\n",
      "loss: 0.462240  [99200/175341]\n",
      "loss: 0.711205  [100800/175341]\n",
      "loss: 0.309320  [102400/175341]\n",
      "loss: 0.468270  [104000/175341]\n",
      "loss: 0.427329  [105600/175341]\n",
      "loss: 0.455943  [107200/175341]\n",
      "loss: 0.564010  [108800/175341]\n",
      "loss: 0.422358  [110400/175341]\n",
      "loss: 0.602343  [112000/175341]\n",
      "loss: 0.403118  [113600/175341]\n",
      "loss: 0.481432  [115200/175341]\n",
      "loss: 0.248901  [116800/175341]\n",
      "loss: 0.306129  [118400/175341]\n",
      "loss: 0.454606  [120000/175341]\n",
      "loss: 0.685224  [121600/175341]\n",
      "loss: 0.632519  [123200/175341]\n",
      "loss: 0.576516  [124800/175341]\n",
      "loss: 0.797640  [126400/175341]\n",
      "loss: 0.260400  [128000/175341]\n",
      "loss: 0.345825  [129600/175341]\n",
      "loss: 0.155529  [131200/175341]\n",
      "loss: 0.219333  [132800/175341]\n",
      "loss: 0.961633  [134400/175341]\n",
      "loss: 0.328816  [136000/175341]\n",
      "loss: 0.550632  [137600/175341]\n",
      "loss: 0.307645  [139200/175341]\n",
      "loss: 0.551499  [140800/175341]\n",
      "loss: 0.802302  [142400/175341]\n",
      "loss: 0.507797  [144000/175341]\n",
      "loss: 0.306212  [145600/175341]\n",
      "loss: 0.587639  [147200/175341]\n",
      "loss: 0.145451  [148800/175341]\n",
      "loss: 0.390882  [150400/175341]\n",
      "loss: 0.226270  [152000/175341]\n",
      "loss: 0.491008  [153600/175341]\n",
      "loss: 0.245762  [155200/175341]\n",
      "loss: 0.750579  [156800/175341]\n",
      "loss: 0.508292  [158400/175341]\n",
      "loss: 0.524433  [160000/175341]\n",
      "loss: 0.297973  [161600/175341]\n",
      "loss: 0.271369  [163200/175341]\n",
      "loss: 0.851643  [164800/175341]\n",
      "loss: 0.454336  [166400/175341]\n",
      "loss: 0.753826  [168000/175341]\n",
      "loss: 0.697566  [169600/175341]\n",
      "loss: 0.256649  [171200/175341]\n",
      "loss: 0.236060  [172800/175341]\n",
      "loss: 0.769794  [174400/175341]\n",
      "Train Accuracy: 81.3831%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.546142, F1-score: 75.60%, Macro_F1-Score:  42.42%  \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.425887  [    0/175341]\n",
      "loss: 0.439878  [ 1600/175341]\n",
      "loss: 0.466669  [ 3200/175341]\n",
      "loss: 0.763408  [ 4800/175341]\n",
      "loss: 0.315485  [ 6400/175341]\n",
      "loss: 0.444772  [ 8000/175341]\n",
      "loss: 0.527291  [ 9600/175341]\n",
      "loss: 0.830622  [11200/175341]\n",
      "loss: 0.952135  [12800/175341]\n",
      "loss: 0.491806  [14400/175341]\n",
      "loss: 0.640275  [16000/175341]\n",
      "loss: 0.594234  [17600/175341]\n",
      "loss: 0.100729  [19200/175341]\n",
      "loss: 0.268716  [20800/175341]\n",
      "loss: 0.385974  [22400/175341]\n",
      "loss: 0.685126  [24000/175341]\n",
      "loss: 0.396196  [25600/175341]\n",
      "loss: 0.394186  [27200/175341]\n",
      "loss: 0.961295  [28800/175341]\n",
      "loss: 0.389965  [30400/175341]\n",
      "loss: 0.487485  [32000/175341]\n",
      "loss: 0.276155  [33600/175341]\n",
      "loss: 0.346587  [35200/175341]\n",
      "loss: 0.478964  [36800/175341]\n",
      "loss: 0.204092  [38400/175341]\n",
      "loss: 0.215078  [40000/175341]\n",
      "loss: 0.785022  [41600/175341]\n",
      "loss: 0.629718  [43200/175341]\n",
      "loss: 0.289535  [44800/175341]\n",
      "loss: 0.603301  [46400/175341]\n",
      "loss: 0.657234  [48000/175341]\n",
      "loss: 0.360382  [49600/175341]\n",
      "loss: 0.445607  [51200/175341]\n",
      "loss: 0.421329  [52800/175341]\n",
      "loss: 0.445225  [54400/175341]\n",
      "loss: 0.262279  [56000/175341]\n",
      "loss: 0.667125  [57600/175341]\n",
      "loss: 0.247748  [59200/175341]\n",
      "loss: 0.301690  [60800/175341]\n",
      "loss: 0.315170  [62400/175341]\n",
      "loss: 0.379514  [64000/175341]\n",
      "loss: 0.780284  [65600/175341]\n",
      "loss: 0.244194  [67200/175341]\n",
      "loss: 0.550930  [68800/175341]\n",
      "loss: 0.490825  [70400/175341]\n",
      "loss: 0.305283  [72000/175341]\n",
      "loss: 0.695071  [73600/175341]\n",
      "loss: 0.745091  [75200/175341]\n",
      "loss: 0.254966  [76800/175341]\n",
      "loss: 0.615838  [78400/175341]\n",
      "loss: 0.346602  [80000/175341]\n",
      "loss: 0.730739  [81600/175341]\n",
      "loss: 0.568319  [83200/175341]\n",
      "loss: 0.413282  [84800/175341]\n",
      "loss: 0.584556  [86400/175341]\n",
      "loss: 0.456039  [88000/175341]\n",
      "loss: 0.315190  [89600/175341]\n",
      "loss: 0.135873  [91200/175341]\n",
      "loss: 0.337491  [92800/175341]\n",
      "loss: 0.356371  [94400/175341]\n",
      "loss: 0.348888  [96000/175341]\n",
      "loss: 0.411918  [97600/175341]\n",
      "loss: 0.591821  [99200/175341]\n",
      "loss: 0.535600  [100800/175341]\n",
      "loss: 0.334207  [102400/175341]\n",
      "loss: 0.445168  [104000/175341]\n",
      "loss: 0.494524  [105600/175341]\n",
      "loss: 0.214273  [107200/175341]\n",
      "loss: 0.467881  [108800/175341]\n",
      "loss: 0.926917  [110400/175341]\n",
      "loss: 0.504561  [112000/175341]\n",
      "loss: 0.642326  [113600/175341]\n",
      "loss: 0.450516  [115200/175341]\n",
      "loss: 0.824694  [116800/175341]\n",
      "loss: 0.523956  [118400/175341]\n",
      "loss: 0.550975  [120000/175341]\n",
      "loss: 0.213194  [121600/175341]\n",
      "loss: 0.589676  [123200/175341]\n",
      "loss: 0.476931  [124800/175341]\n",
      "loss: 0.696863  [126400/175341]\n",
      "loss: 0.664021  [128000/175341]\n",
      "loss: 0.644615  [129600/175341]\n",
      "loss: 0.241810  [131200/175341]\n",
      "loss: 0.563607  [132800/175341]\n",
      "loss: 0.621581  [134400/175341]\n",
      "loss: 0.562974  [136000/175341]\n",
      "loss: 0.461586  [137600/175341]\n",
      "loss: 0.392146  [139200/175341]\n",
      "loss: 0.489758  [140800/175341]\n",
      "loss: 0.208392  [142400/175341]\n",
      "loss: 0.109043  [144000/175341]\n",
      "loss: 0.311810  [145600/175341]\n",
      "loss: 0.177831  [147200/175341]\n",
      "loss: 0.474761  [148800/175341]\n",
      "loss: 0.261496  [150400/175341]\n",
      "loss: 0.230975  [152000/175341]\n",
      "loss: 0.956388  [153600/175341]\n",
      "loss: 0.208568  [155200/175341]\n",
      "loss: 0.587530  [156800/175341]\n",
      "loss: 0.275479  [158400/175341]\n",
      "loss: 0.676382  [160000/175341]\n",
      "loss: 0.376551  [161600/175341]\n",
      "loss: 0.586085  [163200/175341]\n",
      "loss: 0.146712  [164800/175341]\n",
      "loss: 0.589076  [166400/175341]\n",
      "loss: 0.375619  [168000/175341]\n",
      "loss: 0.203769  [169600/175341]\n",
      "loss: 0.721448  [171200/175341]\n",
      "loss: 0.937059  [172800/175341]\n",
      "loss: 0.786327  [174400/175341]\n",
      "Train Accuracy: 81.3820%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.541966, F1-score: 76.63%, Macro_F1-Score:  42.50%  \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.223673  [    0/175341]\n",
      "loss: 0.285317  [ 1600/175341]\n",
      "loss: 0.710910  [ 3200/175341]\n",
      "loss: 0.467556  [ 4800/175341]\n",
      "loss: 0.473181  [ 6400/175341]\n",
      "loss: 0.505743  [ 8000/175341]\n",
      "loss: 0.711294  [ 9600/175341]\n",
      "loss: 0.519076  [11200/175341]\n",
      "loss: 0.451665  [12800/175341]\n",
      "loss: 0.504018  [14400/175341]\n",
      "loss: 0.496438  [16000/175341]\n",
      "loss: 0.249809  [17600/175341]\n",
      "loss: 0.153471  [19200/175341]\n",
      "loss: 0.724258  [20800/175341]\n",
      "loss: 0.458182  [22400/175341]\n",
      "loss: 0.169690  [24000/175341]\n",
      "loss: 0.222796  [25600/175341]\n",
      "loss: 0.407336  [27200/175341]\n",
      "loss: 0.709878  [28800/175341]\n",
      "loss: 0.266965  [30400/175341]\n",
      "loss: 0.216227  [32000/175341]\n",
      "loss: 0.596527  [33600/175341]\n",
      "loss: 0.477601  [35200/175341]\n",
      "loss: 0.849182  [36800/175341]\n",
      "loss: 0.394537  [38400/175341]\n",
      "loss: 0.492820  [40000/175341]\n",
      "loss: 0.616283  [41600/175341]\n",
      "loss: 0.330883  [43200/175341]\n",
      "loss: 0.447334  [44800/175341]\n",
      "loss: 0.526540  [46400/175341]\n",
      "loss: 0.656129  [48000/175341]\n",
      "loss: 0.220224  [49600/175341]\n",
      "loss: 0.781095  [51200/175341]\n",
      "loss: 0.556539  [52800/175341]\n",
      "loss: 0.233545  [54400/175341]\n",
      "loss: 0.546517  [56000/175341]\n",
      "loss: 0.202540  [57600/175341]\n",
      "loss: 0.251805  [59200/175341]\n",
      "loss: 0.508538  [60800/175341]\n",
      "loss: 0.387084  [62400/175341]\n",
      "loss: 0.440656  [64000/175341]\n",
      "loss: 0.473999  [65600/175341]\n",
      "loss: 0.127298  [67200/175341]\n",
      "loss: 0.159686  [68800/175341]\n",
      "loss: 0.592520  [70400/175341]\n",
      "loss: 0.669879  [72000/175341]\n",
      "loss: 0.211605  [73600/175341]\n",
      "loss: 0.842135  [75200/175341]\n",
      "loss: 0.435786  [76800/175341]\n",
      "loss: 0.352526  [78400/175341]\n",
      "loss: 0.491419  [80000/175341]\n",
      "loss: 0.297163  [81600/175341]\n",
      "loss: 0.556367  [83200/175341]\n",
      "loss: 0.576212  [84800/175341]\n",
      "loss: 0.270851  [86400/175341]\n",
      "loss: 0.513356  [88000/175341]\n",
      "loss: 0.297695  [89600/175341]\n",
      "loss: 0.480937  [91200/175341]\n",
      "loss: 0.276547  [92800/175341]\n",
      "loss: 0.497748  [94400/175341]\n",
      "loss: 0.212700  [96000/175341]\n",
      "loss: 0.266971  [97600/175341]\n",
      "loss: 0.971601  [99200/175341]\n",
      "loss: 0.306735  [100800/175341]\n",
      "loss: 0.385559  [102400/175341]\n",
      "loss: 0.186149  [104000/175341]\n",
      "loss: 0.550421  [105600/175341]\n",
      "loss: 0.565940  [107200/175341]\n",
      "loss: 0.397162  [108800/175341]\n",
      "loss: 0.573407  [110400/175341]\n",
      "loss: 0.522010  [112000/175341]\n",
      "loss: 0.213571  [113600/175341]\n",
      "loss: 0.907909  [115200/175341]\n",
      "loss: 0.604753  [116800/175341]\n",
      "loss: 0.835997  [118400/175341]\n",
      "loss: 0.460471  [120000/175341]\n",
      "loss: 0.489508  [121600/175341]\n",
      "loss: 0.360465  [123200/175341]\n",
      "loss: 0.399316  [124800/175341]\n",
      "loss: 0.478561  [126400/175341]\n",
      "loss: 0.315379  [128000/175341]\n",
      "loss: 0.214275  [129600/175341]\n",
      "loss: 0.388266  [131200/175341]\n",
      "loss: 0.353696  [132800/175341]\n",
      "loss: 0.258676  [134400/175341]\n",
      "loss: 0.699331  [136000/175341]\n",
      "loss: 0.225298  [137600/175341]\n",
      "loss: 0.267044  [139200/175341]\n",
      "loss: 0.448478  [140800/175341]\n",
      "loss: 0.746119  [142400/175341]\n",
      "loss: 0.285301  [144000/175341]\n",
      "loss: 0.470680  [145600/175341]\n",
      "loss: 0.495758  [147200/175341]\n",
      "loss: 0.777337  [148800/175341]\n",
      "loss: 0.669776  [150400/175341]\n",
      "loss: 0.376833  [152000/175341]\n",
      "loss: 0.569576  [153600/175341]\n",
      "loss: 0.076966  [155200/175341]\n",
      "loss: 0.216444  [156800/175341]\n",
      "loss: 0.437359  [158400/175341]\n",
      "loss: 0.636461  [160000/175341]\n",
      "loss: 0.972487  [161600/175341]\n",
      "loss: 0.538810  [163200/175341]\n",
      "loss: 0.584882  [164800/175341]\n",
      "loss: 0.406599  [166400/175341]\n",
      "loss: 0.352863  [168000/175341]\n",
      "loss: 0.151440  [169600/175341]\n",
      "loss: 0.286419  [171200/175341]\n",
      "loss: 0.489126  [172800/175341]\n",
      "loss: 0.147514  [174400/175341]\n",
      "Train Accuracy: 81.3558%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.535786, F1-score: 76.41%, Macro_F1-Score:  43.32%  \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 1.198216  [    0/175341]\n",
      "loss: 0.421342  [ 1600/175341]\n",
      "loss: 0.806513  [ 3200/175341]\n",
      "loss: 0.824927  [ 4800/175341]\n",
      "loss: 0.184602  [ 6400/175341]\n",
      "loss: 0.454360  [ 8000/175341]\n",
      "loss: 0.713766  [ 9600/175341]\n",
      "loss: 0.379373  [11200/175341]\n",
      "loss: 0.456227  [12800/175341]\n",
      "loss: 0.332022  [14400/175341]\n",
      "loss: 0.622364  [16000/175341]\n",
      "loss: 0.679108  [17600/175341]\n",
      "loss: 0.324960  [19200/175341]\n",
      "loss: 0.240940  [20800/175341]\n",
      "loss: 0.188090  [22400/175341]\n",
      "loss: 0.311596  [24000/175341]\n",
      "loss: 0.321424  [25600/175341]\n",
      "loss: 0.419175  [27200/175341]\n",
      "loss: 0.829039  [28800/175341]\n",
      "loss: 0.080889  [30400/175341]\n",
      "loss: 0.394606  [32000/175341]\n",
      "loss: 0.623136  [33600/175341]\n",
      "loss: 0.462822  [35200/175341]\n",
      "loss: 0.319157  [36800/175341]\n",
      "loss: 0.434793  [38400/175341]\n",
      "loss: 0.506583  [40000/175341]\n",
      "loss: 0.456163  [41600/175341]\n",
      "loss: 0.401487  [43200/175341]\n",
      "loss: 0.777621  [44800/175341]\n",
      "loss: 0.621402  [46400/175341]\n",
      "loss: 0.348201  [48000/175341]\n",
      "loss: 0.384058  [49600/175341]\n",
      "loss: 0.338885  [51200/175341]\n",
      "loss: 0.881854  [52800/175341]\n",
      "loss: 0.518298  [54400/175341]\n",
      "loss: 0.638245  [56000/175341]\n",
      "loss: 0.746971  [57600/175341]\n",
      "loss: 0.225543  [59200/175341]\n",
      "loss: 0.461837  [60800/175341]\n",
      "loss: 0.513330  [62400/175341]\n",
      "loss: 0.828206  [64000/175341]\n",
      "loss: 0.426057  [65600/175341]\n",
      "loss: 0.392078  [67200/175341]\n",
      "loss: 0.221348  [68800/175341]\n",
      "loss: 0.563088  [70400/175341]\n",
      "loss: 0.279471  [72000/175341]\n",
      "loss: 0.223281  [73600/175341]\n",
      "loss: 0.582859  [75200/175341]\n",
      "loss: 0.170131  [76800/175341]\n",
      "loss: 0.503922  [78400/175341]\n",
      "loss: 0.153900  [80000/175341]\n",
      "loss: 0.284438  [81600/175341]\n",
      "loss: 0.275127  [83200/175341]\n",
      "loss: 0.447533  [84800/175341]\n",
      "loss: 0.251609  [86400/175341]\n",
      "loss: 0.954273  [88000/175341]\n",
      "loss: 0.313414  [89600/175341]\n",
      "loss: 0.193912  [91200/175341]\n",
      "loss: 0.207362  [92800/175341]\n",
      "loss: 0.367262  [94400/175341]\n",
      "loss: 0.389377  [96000/175341]\n",
      "loss: 0.537488  [97600/175341]\n",
      "loss: 0.287154  [99200/175341]\n",
      "loss: 0.267043  [100800/175341]\n",
      "loss: 0.344903  [102400/175341]\n",
      "loss: 0.492017  [104000/175341]\n",
      "loss: 0.647559  [105600/175341]\n",
      "loss: 0.452075  [107200/175341]\n",
      "loss: 0.502037  [108800/175341]\n",
      "loss: 0.696204  [110400/175341]\n",
      "loss: 0.284472  [112000/175341]\n",
      "loss: 0.592703  [113600/175341]\n",
      "loss: 0.445308  [115200/175341]\n",
      "loss: 0.259254  [116800/175341]\n",
      "loss: 0.565439  [118400/175341]\n",
      "loss: 0.425398  [120000/175341]\n",
      "loss: 0.316208  [121600/175341]\n",
      "loss: 0.342741  [123200/175341]\n",
      "loss: 0.703811  [124800/175341]\n",
      "loss: 0.580301  [126400/175341]\n",
      "loss: 0.529533  [128000/175341]\n",
      "loss: 0.432036  [129600/175341]\n",
      "loss: 0.836794  [131200/175341]\n",
      "loss: 0.521526  [132800/175341]\n",
      "loss: 0.262223  [134400/175341]\n",
      "loss: 0.498760  [136000/175341]\n",
      "loss: 0.440000  [137600/175341]\n",
      "loss: 0.143745  [139200/175341]\n",
      "loss: 0.441690  [140800/175341]\n",
      "loss: 0.456289  [142400/175341]\n",
      "loss: 0.684285  [144000/175341]\n",
      "loss: 0.296562  [145600/175341]\n",
      "loss: 0.342658  [147200/175341]\n",
      "loss: 0.212039  [148800/175341]\n",
      "loss: 0.209353  [150400/175341]\n",
      "loss: 0.892204  [152000/175341]\n",
      "loss: 0.365748  [153600/175341]\n",
      "loss: 0.737801  [155200/175341]\n",
      "loss: 0.391011  [156800/175341]\n",
      "loss: 0.532150  [158400/175341]\n",
      "loss: 0.548292  [160000/175341]\n",
      "loss: 0.360166  [161600/175341]\n",
      "loss: 0.739503  [163200/175341]\n",
      "loss: 0.841919  [164800/175341]\n",
      "loss: 0.552848  [166400/175341]\n",
      "loss: 0.519856  [168000/175341]\n",
      "loss: 0.648293  [169600/175341]\n",
      "loss: 0.427803  [171200/175341]\n",
      "loss: 0.497044  [172800/175341]\n",
      "loss: 0.522100  [174400/175341]\n",
      "Train Accuracy: 81.3666%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.562794, F1-score: 75.05%, Macro_F1-Score:  42.84%  \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.937634  [    0/175341]\n",
      "loss: 0.201435  [ 1600/175341]\n",
      "loss: 0.438057  [ 3200/175341]\n",
      "loss: 0.923022  [ 4800/175341]\n",
      "loss: 0.575224  [ 6400/175341]\n",
      "loss: 0.811506  [ 8000/175341]\n",
      "loss: 0.384298  [ 9600/175341]\n",
      "loss: 1.170526  [11200/175341]\n",
      "loss: 0.868732  [12800/175341]\n",
      "loss: 0.488545  [14400/175341]\n",
      "loss: 0.640850  [16000/175341]\n",
      "loss: 0.594865  [17600/175341]\n",
      "loss: 0.836214  [19200/175341]\n",
      "loss: 0.691511  [20800/175341]\n",
      "loss: 0.382350  [22400/175341]\n",
      "loss: 0.472458  [24000/175341]\n",
      "loss: 0.768880  [25600/175341]\n",
      "loss: 0.378618  [27200/175341]\n",
      "loss: 0.554474  [28800/175341]\n",
      "loss: 0.682324  [30400/175341]\n",
      "loss: 1.045027  [32000/175341]\n",
      "loss: 0.247422  [33600/175341]\n",
      "loss: 0.394739  [35200/175341]\n",
      "loss: 0.547416  [36800/175341]\n",
      "loss: 0.515730  [38400/175341]\n",
      "loss: 0.341972  [40000/175341]\n",
      "loss: 0.907013  [41600/175341]\n",
      "loss: 0.280524  [43200/175341]\n",
      "loss: 0.304944  [44800/175341]\n",
      "loss: 0.381725  [46400/175341]\n",
      "loss: 0.636791  [48000/175341]\n",
      "loss: 0.457956  [49600/175341]\n",
      "loss: 0.241285  [51200/175341]\n",
      "loss: 0.317579  [52800/175341]\n",
      "loss: 0.874102  [54400/175341]\n",
      "loss: 0.512293  [56000/175341]\n",
      "loss: 0.311230  [57600/175341]\n",
      "loss: 0.706777  [59200/175341]\n",
      "loss: 0.297242  [60800/175341]\n",
      "loss: 0.508405  [62400/175341]\n",
      "loss: 0.352218  [64000/175341]\n",
      "loss: 0.672863  [65600/175341]\n",
      "loss: 0.423989  [67200/175341]\n",
      "loss: 0.379788  [68800/175341]\n",
      "loss: 0.383091  [70400/175341]\n",
      "loss: 0.480565  [72000/175341]\n",
      "loss: 0.913960  [73600/175341]\n",
      "loss: 0.639435  [75200/175341]\n",
      "loss: 0.102910  [76800/175341]\n",
      "loss: 0.006859  [78400/175341]\n",
      "loss: 0.645602  [80000/175341]\n",
      "loss: 0.718544  [81600/175341]\n",
      "loss: 0.562967  [83200/175341]\n",
      "loss: 0.250897  [84800/175341]\n",
      "loss: 0.540630  [86400/175341]\n",
      "loss: 0.382980  [88000/175341]\n",
      "loss: 0.729093  [89600/175341]\n",
      "loss: 0.645411  [91200/175341]\n",
      "loss: 0.710021  [92800/175341]\n",
      "loss: 0.532772  [94400/175341]\n",
      "loss: 0.656773  [96000/175341]\n",
      "loss: 0.289713  [97600/175341]\n",
      "loss: 0.546627  [99200/175341]\n",
      "loss: 0.149500  [100800/175341]\n",
      "loss: 0.801255  [102400/175341]\n",
      "loss: 0.454408  [104000/175341]\n",
      "loss: 0.559341  [105600/175341]\n",
      "loss: 0.208970  [107200/175341]\n",
      "loss: 0.469664  [108800/175341]\n",
      "loss: 0.422500  [110400/175341]\n",
      "loss: 0.355699  [112000/175341]\n",
      "loss: 0.566987  [113600/175341]\n",
      "loss: 0.722258  [115200/175341]\n",
      "loss: 0.558133  [116800/175341]\n",
      "loss: 0.820123  [118400/175341]\n",
      "loss: 0.699340  [120000/175341]\n",
      "loss: 0.655264  [121600/175341]\n",
      "loss: 0.378101  [123200/175341]\n",
      "loss: 0.316312  [124800/175341]\n",
      "loss: 0.318069  [126400/175341]\n",
      "loss: 0.738077  [128000/175341]\n",
      "loss: 0.230518  [129600/175341]\n",
      "loss: 0.635355  [131200/175341]\n",
      "loss: 0.253033  [132800/175341]\n",
      "loss: 0.405616  [134400/175341]\n",
      "loss: 0.691286  [136000/175341]\n",
      "loss: 0.619744  [137600/175341]\n",
      "loss: 0.363719  [139200/175341]\n",
      "loss: 0.340894  [140800/175341]\n",
      "loss: 0.936378  [142400/175341]\n",
      "loss: 0.382151  [144000/175341]\n",
      "loss: 0.251610  [145600/175341]\n",
      "loss: 0.651374  [147200/175341]\n",
      "loss: 0.331831  [148800/175341]\n",
      "loss: 0.799972  [150400/175341]\n",
      "loss: 1.009822  [152000/175341]\n",
      "loss: 0.617579  [153600/175341]\n",
      "loss: 0.678766  [155200/175341]\n",
      "loss: 0.578797  [156800/175341]\n",
      "loss: 0.223528  [158400/175341]\n",
      "loss: 0.493232  [160000/175341]\n",
      "loss: 0.366511  [161600/175341]\n",
      "loss: 0.254092  [163200/175341]\n",
      "loss: 0.265093  [164800/175341]\n",
      "loss: 0.206555  [166400/175341]\n",
      "loss: 0.663319  [168000/175341]\n",
      "loss: 0.626492  [169600/175341]\n",
      "loss: 0.511462  [171200/175341]\n",
      "loss: 0.523960  [172800/175341]\n",
      "loss: 0.572459  [174400/175341]\n",
      "Train Accuracy: 81.3375%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.553567, F1-score: 75.40%, Macro_F1-Score:  42.58%  \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.502282  [    0/175341]\n",
      "loss: 0.374030  [ 1600/175341]\n",
      "loss: 0.178879  [ 3200/175341]\n",
      "loss: 0.480716  [ 4800/175341]\n",
      "loss: 0.918096  [ 6400/175341]\n",
      "loss: 0.358117  [ 8000/175341]\n",
      "loss: 0.461628  [ 9600/175341]\n",
      "loss: 0.565934  [11200/175341]\n",
      "loss: 0.789619  [12800/175341]\n",
      "loss: 0.429447  [14400/175341]\n",
      "loss: 0.553732  [16000/175341]\n",
      "loss: 0.357535  [17600/175341]\n",
      "loss: 0.232504  [19200/175341]\n",
      "loss: 0.208216  [20800/175341]\n",
      "loss: 0.313419  [22400/175341]\n",
      "loss: 0.552011  [24000/175341]\n",
      "loss: 0.502990  [25600/175341]\n",
      "loss: 0.514051  [27200/175341]\n",
      "loss: 0.647056  [28800/175341]\n",
      "loss: 0.198902  [30400/175341]\n",
      "loss: 0.646094  [32000/175341]\n",
      "loss: 0.340205  [33600/175341]\n",
      "loss: 0.345054  [35200/175341]\n",
      "loss: 0.207726  [36800/175341]\n",
      "loss: 0.546723  [38400/175341]\n",
      "loss: 0.734623  [40000/175341]\n",
      "loss: 0.652469  [41600/175341]\n",
      "loss: 0.555985  [43200/175341]\n",
      "loss: 0.450320  [44800/175341]\n",
      "loss: 0.589238  [46400/175341]\n",
      "loss: 0.663388  [48000/175341]\n",
      "loss: 0.626446  [49600/175341]\n",
      "loss: 0.403345  [51200/175341]\n",
      "loss: 0.518660  [52800/175341]\n",
      "loss: 0.401284  [54400/175341]\n",
      "loss: 0.483900  [56000/175341]\n",
      "loss: 0.311672  [57600/175341]\n",
      "loss: 0.900290  [59200/175341]\n",
      "loss: 0.438876  [60800/175341]\n",
      "loss: 0.724338  [62400/175341]\n",
      "loss: 0.259630  [64000/175341]\n",
      "loss: 0.543979  [65600/175341]\n",
      "loss: 0.188207  [67200/175341]\n",
      "loss: 0.305223  [68800/175341]\n",
      "loss: 0.346060  [70400/175341]\n",
      "loss: 0.353466  [72000/175341]\n",
      "loss: 0.765357  [73600/175341]\n",
      "loss: 0.923987  [75200/175341]\n",
      "loss: 0.410509  [76800/175341]\n",
      "loss: 0.564439  [78400/175341]\n",
      "loss: 0.655728  [80000/175341]\n",
      "loss: 0.207625  [81600/175341]\n",
      "loss: 0.367195  [83200/175341]\n",
      "loss: 0.390656  [84800/175341]\n",
      "loss: 0.322505  [86400/175341]\n",
      "loss: 0.240223  [88000/175341]\n",
      "loss: 0.669962  [89600/175341]\n",
      "loss: 0.588728  [91200/175341]\n",
      "loss: 0.440134  [92800/175341]\n",
      "loss: 0.716044  [94400/175341]\n",
      "loss: 0.449982  [96000/175341]\n",
      "loss: 0.093308  [97600/175341]\n",
      "loss: 0.481532  [99200/175341]\n",
      "loss: 0.308639  [100800/175341]\n",
      "loss: 0.732788  [102400/175341]\n",
      "loss: 0.338109  [104000/175341]\n",
      "loss: 0.554055  [105600/175341]\n",
      "loss: 0.414696  [107200/175341]\n",
      "loss: 0.588677  [108800/175341]\n",
      "loss: 0.535643  [110400/175341]\n",
      "loss: 0.281541  [112000/175341]\n",
      "loss: 0.301137  [113600/175341]\n",
      "loss: 1.062279  [115200/175341]\n",
      "loss: 0.526612  [116800/175341]\n",
      "loss: 0.482869  [118400/175341]\n",
      "loss: 0.381442  [120000/175341]\n",
      "loss: 0.482102  [121600/175341]\n",
      "loss: 0.646559  [123200/175341]\n",
      "loss: 0.330023  [124800/175341]\n",
      "loss: 0.537975  [126400/175341]\n",
      "loss: 0.442541  [128000/175341]\n",
      "loss: 0.102782  [129600/175341]\n",
      "loss: 0.452652  [131200/175341]\n",
      "loss: 0.707479  [132800/175341]\n",
      "loss: 0.210945  [134400/175341]\n",
      "loss: 0.603060  [136000/175341]\n",
      "loss: 0.841893  [137600/175341]\n",
      "loss: 0.387649  [139200/175341]\n",
      "loss: 0.533908  [140800/175341]\n",
      "loss: 0.274905  [142400/175341]\n",
      "loss: 0.598192  [144000/175341]\n",
      "loss: 1.013890  [145600/175341]\n",
      "loss: 0.786331  [147200/175341]\n",
      "loss: 0.145290  [148800/175341]\n",
      "loss: 0.877894  [150400/175341]\n",
      "loss: 0.395846  [152000/175341]\n",
      "loss: 0.739773  [153600/175341]\n",
      "loss: 0.481947  [155200/175341]\n",
      "loss: 0.503085  [156800/175341]\n",
      "loss: 0.696297  [158400/175341]\n",
      "loss: 0.674005  [160000/175341]\n",
      "loss: 0.332647  [161600/175341]\n",
      "loss: 0.470976  [163200/175341]\n",
      "loss: 0.618619  [164800/175341]\n",
      "loss: 0.437078  [166400/175341]\n",
      "loss: 0.254783  [168000/175341]\n",
      "loss: 0.246892  [169600/175341]\n",
      "loss: 0.310702  [171200/175341]\n",
      "loss: 0.443738  [172800/175341]\n",
      "loss: 0.439949  [174400/175341]\n",
      "Train Accuracy: 81.3643%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.538855, F1-score: 76.26%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.292298  [    0/175341]\n",
      "loss: 0.704490  [ 1600/175341]\n",
      "loss: 0.209608  [ 3200/175341]\n",
      "loss: 0.342982  [ 4800/175341]\n",
      "loss: 0.390783  [ 6400/175341]\n",
      "loss: 0.499871  [ 8000/175341]\n",
      "loss: 0.455599  [ 9600/175341]\n",
      "loss: 0.222595  [11200/175341]\n",
      "loss: 0.445095  [12800/175341]\n",
      "loss: 0.693441  [14400/175341]\n",
      "loss: 0.789861  [16000/175341]\n",
      "loss: 0.491741  [17600/175341]\n",
      "loss: 0.545473  [19200/175341]\n",
      "loss: 0.726319  [20800/175341]\n",
      "loss: 0.349171  [22400/175341]\n",
      "loss: 0.171718  [24000/175341]\n",
      "loss: 0.705883  [25600/175341]\n",
      "loss: 0.166197  [27200/175341]\n",
      "loss: 0.500383  [28800/175341]\n",
      "loss: 0.580675  [30400/175341]\n",
      "loss: 0.416369  [32000/175341]\n",
      "loss: 0.578499  [33600/175341]\n",
      "loss: 0.561687  [35200/175341]\n",
      "loss: 0.412466  [36800/175341]\n",
      "loss: 0.200713  [38400/175341]\n",
      "loss: 0.365268  [40000/175341]\n",
      "loss: 0.510331  [41600/175341]\n",
      "loss: 0.449599  [43200/175341]\n",
      "loss: 0.337247  [44800/175341]\n",
      "loss: 0.355666  [46400/175341]\n",
      "loss: 0.714001  [48000/175341]\n",
      "loss: 0.422966  [49600/175341]\n",
      "loss: 0.532060  [51200/175341]\n",
      "loss: 0.364729  [52800/175341]\n",
      "loss: 0.780055  [54400/175341]\n",
      "loss: 0.152525  [56000/175341]\n",
      "loss: 0.571649  [57600/175341]\n",
      "loss: 0.162536  [59200/175341]\n",
      "loss: 0.477854  [60800/175341]\n",
      "loss: 0.387458  [62400/175341]\n",
      "loss: 0.417284  [64000/175341]\n",
      "loss: 0.492880  [65600/175341]\n",
      "loss: 0.493150  [67200/175341]\n",
      "loss: 0.138571  [68800/175341]\n",
      "loss: 0.313414  [70400/175341]\n",
      "loss: 0.278377  [72000/175341]\n",
      "loss: 0.364350  [73600/175341]\n",
      "loss: 0.264802  [75200/175341]\n",
      "loss: 0.625185  [76800/175341]\n",
      "loss: 0.501151  [78400/175341]\n",
      "loss: 0.719598  [80000/175341]\n",
      "loss: 0.483392  [81600/175341]\n",
      "loss: 0.448072  [83200/175341]\n",
      "loss: 0.557871  [84800/175341]\n",
      "loss: 0.869648  [86400/175341]\n",
      "loss: 0.323262  [88000/175341]\n",
      "loss: 0.213585  [89600/175341]\n",
      "loss: 0.297005  [91200/175341]\n",
      "loss: 0.330699  [92800/175341]\n",
      "loss: 0.530099  [94400/175341]\n",
      "loss: 0.624805  [96000/175341]\n",
      "loss: 0.240309  [97600/175341]\n",
      "loss: 0.356882  [99200/175341]\n",
      "loss: 0.502333  [100800/175341]\n",
      "loss: 0.348467  [102400/175341]\n",
      "loss: 0.335024  [104000/175341]\n",
      "loss: 0.752545  [105600/175341]\n",
      "loss: 0.406547  [107200/175341]\n",
      "loss: 0.428141  [108800/175341]\n",
      "loss: 0.872933  [110400/175341]\n",
      "loss: 0.620701  [112000/175341]\n",
      "loss: 0.394170  [113600/175341]\n",
      "loss: 0.427500  [115200/175341]\n",
      "loss: 0.411437  [116800/175341]\n",
      "loss: 0.191450  [118400/175341]\n",
      "loss: 0.517935  [120000/175341]\n",
      "loss: 0.183919  [121600/175341]\n",
      "loss: 0.431940  [123200/175341]\n",
      "loss: 0.279223  [124800/175341]\n",
      "loss: 0.180455  [126400/175341]\n",
      "loss: 0.330610  [128000/175341]\n",
      "loss: 0.619270  [129600/175341]\n",
      "loss: 0.138804  [131200/175341]\n",
      "loss: 0.542958  [132800/175341]\n",
      "loss: 0.865122  [134400/175341]\n",
      "loss: 0.360684  [136000/175341]\n",
      "loss: 0.733631  [137600/175341]\n",
      "loss: 0.414138  [139200/175341]\n",
      "loss: 0.143478  [140800/175341]\n",
      "loss: 0.383017  [142400/175341]\n",
      "loss: 0.676174  [144000/175341]\n",
      "loss: 0.186019  [145600/175341]\n",
      "loss: 0.510030  [147200/175341]\n",
      "loss: 0.443768  [148800/175341]\n",
      "loss: 0.471223  [150400/175341]\n",
      "loss: 0.252427  [152000/175341]\n",
      "loss: 0.248690  [153600/175341]\n",
      "loss: 0.449663  [155200/175341]\n",
      "loss: 0.560451  [156800/175341]\n",
      "loss: 0.575350  [158400/175341]\n",
      "loss: 0.461097  [160000/175341]\n",
      "loss: 0.463350  [161600/175341]\n",
      "loss: 0.436111  [163200/175341]\n",
      "loss: 0.416008  [164800/175341]\n",
      "loss: 0.316485  [166400/175341]\n",
      "loss: 0.271011  [168000/175341]\n",
      "loss: 0.273353  [169600/175341]\n",
      "loss: 0.289284  [171200/175341]\n",
      "loss: 0.390755  [172800/175341]\n",
      "loss: 0.243542  [174400/175341]\n",
      "Train Accuracy: 81.3449%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.560946, F1-score: 75.68%, Macro_F1-Score:  43.89%  \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.200661  [    0/175341]\n",
      "loss: 0.670471  [ 1600/175341]\n",
      "loss: 0.475120  [ 3200/175341]\n",
      "loss: 0.194841  [ 4800/175341]\n",
      "loss: 0.356462  [ 6400/175341]\n",
      "loss: 0.336517  [ 8000/175341]\n",
      "loss: 0.381912  [ 9600/175341]\n",
      "loss: 0.329337  [11200/175341]\n",
      "loss: 0.300945  [12800/175341]\n",
      "loss: 0.253264  [14400/175341]\n",
      "loss: 0.528311  [16000/175341]\n",
      "loss: 0.461808  [17600/175341]\n",
      "loss: 0.537936  [19200/175341]\n",
      "loss: 1.222277  [20800/175341]\n",
      "loss: 0.616619  [22400/175341]\n",
      "loss: 0.383489  [24000/175341]\n",
      "loss: 0.712600  [25600/175341]\n",
      "loss: 0.441530  [27200/175341]\n",
      "loss: 0.422421  [28800/175341]\n",
      "loss: 0.321195  [30400/175341]\n",
      "loss: 0.281242  [32000/175341]\n",
      "loss: 0.347812  [33600/175341]\n",
      "loss: 0.452078  [35200/175341]\n",
      "loss: 0.429164  [36800/175341]\n",
      "loss: 0.423447  [38400/175341]\n",
      "loss: 0.578531  [40000/175341]\n",
      "loss: 0.612821  [41600/175341]\n",
      "loss: 0.564752  [43200/175341]\n",
      "loss: 0.655037  [44800/175341]\n",
      "loss: 0.256636  [46400/175341]\n",
      "loss: 0.252015  [48000/175341]\n",
      "loss: 0.362997  [49600/175341]\n",
      "loss: 0.212815  [51200/175341]\n",
      "loss: 0.214396  [52800/175341]\n",
      "loss: 0.348933  [54400/175341]\n",
      "loss: 0.272649  [56000/175341]\n",
      "loss: 0.443959  [57600/175341]\n",
      "loss: 0.482506  [59200/175341]\n",
      "loss: 0.527456  [60800/175341]\n",
      "loss: 0.427526  [62400/175341]\n",
      "loss: 0.350148  [64000/175341]\n",
      "loss: 0.183954  [65600/175341]\n",
      "loss: 0.695915  [67200/175341]\n",
      "loss: 0.438794  [68800/175341]\n",
      "loss: 0.447874  [70400/175341]\n",
      "loss: 0.337435  [72000/175341]\n",
      "loss: 0.420638  [73600/175341]\n",
      "loss: 0.959300  [75200/175341]\n",
      "loss: 0.143787  [76800/175341]\n",
      "loss: 0.502142  [78400/175341]\n",
      "loss: 0.415883  [80000/175341]\n",
      "loss: 0.402322  [81600/175341]\n",
      "loss: 0.359364  [83200/175341]\n",
      "loss: 0.485009  [84800/175341]\n",
      "loss: 0.363675  [86400/175341]\n",
      "loss: 0.423439  [88000/175341]\n",
      "loss: 0.620055  [89600/175341]\n",
      "loss: 0.221062  [91200/175341]\n",
      "loss: 0.434454  [92800/175341]\n",
      "loss: 0.822927  [94400/175341]\n",
      "loss: 0.218445  [96000/175341]\n",
      "loss: 0.366609  [97600/175341]\n",
      "loss: 1.006967  [99200/175341]\n",
      "loss: 0.377902  [100800/175341]\n",
      "loss: 0.558407  [102400/175341]\n",
      "loss: 0.761644  [104000/175341]\n",
      "loss: 0.527597  [105600/175341]\n",
      "loss: 0.221879  [107200/175341]\n",
      "loss: 0.462523  [108800/175341]\n",
      "loss: 0.590091  [110400/175341]\n",
      "loss: 0.352655  [112000/175341]\n",
      "loss: 0.498010  [113600/175341]\n",
      "loss: 0.476512  [115200/175341]\n",
      "loss: 0.504759  [116800/175341]\n",
      "loss: 0.473283  [118400/175341]\n",
      "loss: 0.565331  [120000/175341]\n",
      "loss: 0.705402  [121600/175341]\n",
      "loss: 0.286070  [123200/175341]\n",
      "loss: 0.673456  [124800/175341]\n",
      "loss: 0.479249  [126400/175341]\n",
      "loss: 0.486994  [128000/175341]\n",
      "loss: 0.305105  [129600/175341]\n",
      "loss: 0.579089  [131200/175341]\n",
      "loss: 0.531044  [132800/175341]\n",
      "loss: 0.466787  [134400/175341]\n",
      "loss: 0.415901  [136000/175341]\n",
      "loss: 0.392830  [137600/175341]\n",
      "loss: 0.602811  [139200/175341]\n",
      "loss: 0.605732  [140800/175341]\n",
      "loss: 0.438892  [142400/175341]\n",
      "loss: 0.693430  [144000/175341]\n",
      "loss: 0.467136  [145600/175341]\n",
      "loss: 0.386745  [147200/175341]\n",
      "loss: 0.830632  [148800/175341]\n",
      "loss: 0.373596  [150400/175341]\n",
      "loss: 0.440758  [152000/175341]\n",
      "loss: 0.339960  [153600/175341]\n",
      "loss: 0.437529  [155200/175341]\n",
      "loss: 0.346602  [156800/175341]\n",
      "loss: 0.569362  [158400/175341]\n",
      "loss: 0.688562  [160000/175341]\n",
      "loss: 0.181192  [161600/175341]\n",
      "loss: 0.611521  [163200/175341]\n",
      "loss: 0.502363  [164800/175341]\n",
      "loss: 0.425224  [166400/175341]\n",
      "loss: 0.636526  [168000/175341]\n",
      "loss: 0.676236  [169600/175341]\n",
      "loss: 0.626634  [171200/175341]\n",
      "loss: 0.636640  [172800/175341]\n",
      "loss: 0.500537  [174400/175341]\n",
      "Train Accuracy: 81.3552%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.538914, F1-score: 76.38%, Macro_F1-Score:  42.73%  \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.556715  [    0/175341]\n",
      "loss: 0.567863  [ 1600/175341]\n",
      "loss: 0.430473  [ 3200/175341]\n",
      "loss: 0.506338  [ 4800/175341]\n",
      "loss: 0.625063  [ 6400/175341]\n",
      "loss: 0.300726  [ 8000/175341]\n",
      "loss: 0.720428  [ 9600/175341]\n",
      "loss: 0.437062  [11200/175341]\n",
      "loss: 0.633480  [12800/175341]\n",
      "loss: 0.377884  [14400/175341]\n",
      "loss: 0.550436  [16000/175341]\n",
      "loss: 0.235175  [17600/175341]\n",
      "loss: 0.560438  [19200/175341]\n",
      "loss: 0.378137  [20800/175341]\n",
      "loss: 0.420554  [22400/175341]\n",
      "loss: 0.816497  [24000/175341]\n",
      "loss: 0.203750  [25600/175341]\n",
      "loss: 0.343778  [27200/175341]\n",
      "loss: 0.043556  [28800/175341]\n",
      "loss: 0.815936  [30400/175341]\n",
      "loss: 0.524865  [32000/175341]\n",
      "loss: 0.470695  [33600/175341]\n",
      "loss: 0.257448  [35200/175341]\n",
      "loss: 0.668235  [36800/175341]\n",
      "loss: 0.468912  [38400/175341]\n",
      "loss: 0.351862  [40000/175341]\n",
      "loss: 0.193206  [41600/175341]\n",
      "loss: 0.563382  [43200/175341]\n",
      "loss: 0.428605  [44800/175341]\n",
      "loss: 0.858750  [46400/175341]\n",
      "loss: 0.450205  [48000/175341]\n",
      "loss: 0.654812  [49600/175341]\n",
      "loss: 0.515489  [51200/175341]\n",
      "loss: 0.405832  [52800/175341]\n",
      "loss: 0.530646  [54400/175341]\n",
      "loss: 0.543482  [56000/175341]\n",
      "loss: 0.384497  [57600/175341]\n",
      "loss: 0.399864  [59200/175341]\n",
      "loss: 0.641903  [60800/175341]\n",
      "loss: 0.527017  [62400/175341]\n",
      "loss: 0.462989  [64000/175341]\n",
      "loss: 0.307651  [65600/175341]\n",
      "loss: 0.609795  [67200/175341]\n",
      "loss: 0.230011  [68800/175341]\n",
      "loss: 0.333652  [70400/175341]\n",
      "loss: 0.567526  [72000/175341]\n",
      "loss: 0.219288  [73600/175341]\n",
      "loss: 0.697885  [75200/175341]\n",
      "loss: 0.354227  [76800/175341]\n",
      "loss: 0.234307  [78400/175341]\n",
      "loss: 0.362198  [80000/175341]\n",
      "loss: 0.354879  [81600/175341]\n",
      "loss: 0.600038  [83200/175341]\n",
      "loss: 0.412613  [84800/175341]\n",
      "loss: 0.924827  [86400/175341]\n",
      "loss: 0.828702  [88000/175341]\n",
      "loss: 0.223985  [89600/175341]\n",
      "loss: 0.498191  [91200/175341]\n",
      "loss: 0.362892  [92800/175341]\n",
      "loss: 0.414308  [94400/175341]\n",
      "loss: 0.564825  [96000/175341]\n",
      "loss: 0.675980  [97600/175341]\n",
      "loss: 0.597528  [99200/175341]\n",
      "loss: 0.330302  [100800/175341]\n",
      "loss: 0.990454  [102400/175341]\n",
      "loss: 0.601017  [104000/175341]\n",
      "loss: 0.537358  [105600/175341]\n",
      "loss: 0.478497  [107200/175341]\n",
      "loss: 0.512273  [108800/175341]\n",
      "loss: 0.491064  [110400/175341]\n",
      "loss: 0.521974  [112000/175341]\n",
      "loss: 0.820245  [113600/175341]\n",
      "loss: 0.937082  [115200/175341]\n",
      "loss: 0.675761  [116800/175341]\n",
      "loss: 0.548644  [118400/175341]\n",
      "loss: 0.689785  [120000/175341]\n",
      "loss: 0.331315  [121600/175341]\n",
      "loss: 0.641450  [123200/175341]\n",
      "loss: 0.809426  [124800/175341]\n",
      "loss: 0.355699  [126400/175341]\n",
      "loss: 0.183252  [128000/175341]\n",
      "loss: 0.303144  [129600/175341]\n",
      "loss: 0.392919  [131200/175341]\n",
      "loss: 0.511972  [132800/175341]\n",
      "loss: 0.828230  [134400/175341]\n",
      "loss: 0.568098  [136000/175341]\n",
      "loss: 0.370849  [137600/175341]\n",
      "loss: 0.582664  [139200/175341]\n",
      "loss: 0.494206  [140800/175341]\n",
      "loss: 0.530564  [142400/175341]\n",
      "loss: 0.147868  [144000/175341]\n",
      "loss: 0.548942  [145600/175341]\n",
      "loss: 0.512072  [147200/175341]\n",
      "loss: 0.497912  [148800/175341]\n",
      "loss: 0.338023  [150400/175341]\n",
      "loss: 0.740821  [152000/175341]\n",
      "loss: 0.386964  [153600/175341]\n",
      "loss: 0.568530  [155200/175341]\n",
      "loss: 0.354692  [156800/175341]\n",
      "loss: 0.355627  [158400/175341]\n",
      "loss: 0.535993  [160000/175341]\n",
      "loss: 0.216559  [161600/175341]\n",
      "loss: 0.157664  [163200/175341]\n",
      "loss: 0.881882  [164800/175341]\n",
      "loss: 0.368272  [166400/175341]\n",
      "loss: 0.249239  [168000/175341]\n",
      "loss: 0.592683  [169600/175341]\n",
      "loss: 0.484240  [171200/175341]\n",
      "loss: 0.653362  [172800/175341]\n",
      "loss: 0.183367  [174400/175341]\n",
      "Train Accuracy: 81.3746%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.535999, F1-score: 77.14%, Macro_F1-Score:  44.21%  \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.107380  [    0/175341]\n",
      "loss: 0.173630  [ 1600/175341]\n",
      "loss: 0.077230  [ 3200/175341]\n",
      "loss: 0.393966  [ 4800/175341]\n",
      "loss: 0.170544  [ 6400/175341]\n",
      "loss: 0.606785  [ 8000/175341]\n",
      "loss: 0.546162  [ 9600/175341]\n",
      "loss: 0.388939  [11200/175341]\n",
      "loss: 0.638578  [12800/175341]\n",
      "loss: 0.635402  [14400/175341]\n",
      "loss: 0.273497  [16000/175341]\n",
      "loss: 0.659295  [17600/175341]\n",
      "loss: 0.354466  [19200/175341]\n",
      "loss: 0.138247  [20800/175341]\n",
      "loss: 0.493866  [22400/175341]\n",
      "loss: 0.900404  [24000/175341]\n",
      "loss: 0.106975  [25600/175341]\n",
      "loss: 0.388350  [27200/175341]\n",
      "loss: 0.540320  [28800/175341]\n",
      "loss: 0.628645  [30400/175341]\n",
      "loss: 0.264896  [32000/175341]\n",
      "loss: 0.526111  [33600/175341]\n",
      "loss: 0.255520  [35200/175341]\n",
      "loss: 0.409710  [36800/175341]\n",
      "loss: 0.472718  [38400/175341]\n",
      "loss: 0.512525  [40000/175341]\n",
      "loss: 0.504827  [41600/175341]\n",
      "loss: 0.453851  [43200/175341]\n",
      "loss: 0.223360  [44800/175341]\n",
      "loss: 0.345465  [46400/175341]\n",
      "loss: 0.565627  [48000/175341]\n",
      "loss: 0.439661  [49600/175341]\n",
      "loss: 0.395201  [51200/175341]\n",
      "loss: 0.496987  [52800/175341]\n",
      "loss: 0.792985  [54400/175341]\n",
      "loss: 0.222271  [56000/175341]\n",
      "loss: 0.436377  [57600/175341]\n",
      "loss: 0.152233  [59200/175341]\n",
      "loss: 0.857916  [60800/175341]\n",
      "loss: 0.422907  [62400/175341]\n",
      "loss: 0.573815  [64000/175341]\n",
      "loss: 0.244364  [65600/175341]\n",
      "loss: 0.480368  [67200/175341]\n",
      "loss: 0.337545  [68800/175341]\n",
      "loss: 0.374351  [70400/175341]\n",
      "loss: 0.414247  [72000/175341]\n",
      "loss: 0.311480  [73600/175341]\n",
      "loss: 0.134922  [75200/175341]\n",
      "loss: 0.307513  [76800/175341]\n",
      "loss: 0.612341  [78400/175341]\n",
      "loss: 0.245774  [80000/175341]\n",
      "loss: 0.323657  [81600/175341]\n",
      "loss: 0.264437  [83200/175341]\n",
      "loss: 0.511086  [84800/175341]\n",
      "loss: 0.291577  [86400/175341]\n",
      "loss: 0.481150  [88000/175341]\n",
      "loss: 0.108792  [89600/175341]\n",
      "loss: 0.563329  [91200/175341]\n",
      "loss: 0.434663  [92800/175341]\n",
      "loss: 0.382286  [94400/175341]\n",
      "loss: 0.493196  [96000/175341]\n",
      "loss: 0.533279  [97600/175341]\n",
      "loss: 0.982251  [99200/175341]\n",
      "loss: 0.444020  [100800/175341]\n",
      "loss: 0.513588  [102400/175341]\n",
      "loss: 0.225912  [104000/175341]\n",
      "loss: 0.493486  [105600/175341]\n",
      "loss: 0.413472  [107200/175341]\n",
      "loss: 0.469270  [108800/175341]\n",
      "loss: 0.454314  [110400/175341]\n",
      "loss: 0.803054  [112000/175341]\n",
      "loss: 0.381983  [113600/175341]\n",
      "loss: 0.688481  [115200/175341]\n",
      "loss: 0.441812  [116800/175341]\n",
      "loss: 0.075637  [118400/175341]\n",
      "loss: 0.531205  [120000/175341]\n",
      "loss: 0.502245  [121600/175341]\n",
      "loss: 0.420423  [123200/175341]\n",
      "loss: 0.914501  [124800/175341]\n",
      "loss: 0.171739  [126400/175341]\n",
      "loss: 0.322368  [128000/175341]\n",
      "loss: 0.173111  [129600/175341]\n",
      "loss: 0.193978  [131200/175341]\n",
      "loss: 0.464083  [132800/175341]\n",
      "loss: 0.587172  [134400/175341]\n",
      "loss: 1.017683  [136000/175341]\n",
      "loss: 0.289053  [137600/175341]\n",
      "loss: 0.656083  [139200/175341]\n",
      "loss: 0.307303  [140800/175341]\n",
      "loss: 0.279806  [142400/175341]\n",
      "loss: 0.094429  [144000/175341]\n",
      "loss: 0.530140  [145600/175341]\n",
      "loss: 0.402172  [147200/175341]\n",
      "loss: 0.112014  [148800/175341]\n",
      "loss: 0.380865  [150400/175341]\n",
      "loss: 0.711083  [152000/175341]\n",
      "loss: 0.500958  [153600/175341]\n",
      "loss: 0.614991  [155200/175341]\n",
      "loss: 1.071698  [156800/175341]\n",
      "loss: 0.542422  [158400/175341]\n",
      "loss: 0.313661  [160000/175341]\n",
      "loss: 0.356113  [161600/175341]\n",
      "loss: 0.704774  [163200/175341]\n",
      "loss: 0.382086  [164800/175341]\n",
      "loss: 0.319647  [166400/175341]\n",
      "loss: 0.357614  [168000/175341]\n",
      "loss: 0.576654  [169600/175341]\n",
      "loss: 0.181295  [171200/175341]\n",
      "loss: 0.431284  [172800/175341]\n",
      "loss: 0.196843  [174400/175341]\n",
      "Train Accuracy: 81.3734%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.532324, F1-score: 76.21%, Macro_F1-Score:  42.23%  \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.390636  [    0/175341]\n",
      "loss: 0.775030  [ 1600/175341]\n",
      "loss: 0.392826  [ 3200/175341]\n",
      "loss: 0.285825  [ 4800/175341]\n",
      "loss: 0.435232  [ 6400/175341]\n",
      "loss: 0.326259  [ 8000/175341]\n",
      "loss: 0.472488  [ 9600/175341]\n",
      "loss: 0.310498  [11200/175341]\n",
      "loss: 0.209715  [12800/175341]\n",
      "loss: 0.324229  [14400/175341]\n",
      "loss: 0.278930  [16000/175341]\n",
      "loss: 0.127820  [17600/175341]\n",
      "loss: 0.645308  [19200/175341]\n",
      "loss: 0.257080  [20800/175341]\n",
      "loss: 0.306132  [22400/175341]\n",
      "loss: 0.792665  [24000/175341]\n",
      "loss: 0.327758  [25600/175341]\n",
      "loss: 0.411928  [27200/175341]\n",
      "loss: 0.643457  [28800/175341]\n",
      "loss: 0.232992  [30400/175341]\n",
      "loss: 0.506003  [32000/175341]\n",
      "loss: 0.558917  [33600/175341]\n",
      "loss: 0.296674  [35200/175341]\n",
      "loss: 0.432039  [36800/175341]\n",
      "loss: 0.195711  [38400/175341]\n",
      "loss: 0.508670  [40000/175341]\n",
      "loss: 0.235202  [41600/175341]\n",
      "loss: 0.714392  [43200/175341]\n",
      "loss: 0.712096  [44800/175341]\n",
      "loss: 0.845633  [46400/175341]\n",
      "loss: 0.327618  [48000/175341]\n",
      "loss: 0.903530  [49600/175341]\n",
      "loss: 0.321820  [51200/175341]\n",
      "loss: 0.287866  [52800/175341]\n",
      "loss: 0.543998  [54400/175341]\n",
      "loss: 0.451805  [56000/175341]\n",
      "loss: 0.473161  [57600/175341]\n",
      "loss: 0.296842  [59200/175341]\n",
      "loss: 0.390247  [60800/175341]\n",
      "loss: 0.670463  [62400/175341]\n",
      "loss: 0.286841  [64000/175341]\n",
      "loss: 0.191689  [65600/175341]\n",
      "loss: 0.536336  [67200/175341]\n",
      "loss: 0.256331  [68800/175341]\n",
      "loss: 0.801265  [70400/175341]\n",
      "loss: 0.245452  [72000/175341]\n",
      "loss: 0.253396  [73600/175341]\n",
      "loss: 0.215338  [75200/175341]\n",
      "loss: 0.460092  [76800/175341]\n",
      "loss: 0.518071  [78400/175341]\n",
      "loss: 0.382992  [80000/175341]\n",
      "loss: 0.258697  [81600/175341]\n",
      "loss: 0.383376  [83200/175341]\n",
      "loss: 0.334132  [84800/175341]\n",
      "loss: 0.595823  [86400/175341]\n",
      "loss: 0.297510  [88000/175341]\n",
      "loss: 0.645418  [89600/175341]\n",
      "loss: 0.354117  [91200/175341]\n",
      "loss: 0.285329  [92800/175341]\n",
      "loss: 0.456697  [94400/175341]\n",
      "loss: 0.946689  [96000/175341]\n",
      "loss: 0.299176  [97600/175341]\n",
      "loss: 0.354577  [99200/175341]\n",
      "loss: 0.540602  [100800/175341]\n",
      "loss: 0.654282  [102400/175341]\n",
      "loss: 0.353918  [104000/175341]\n",
      "loss: 0.561576  [105600/175341]\n",
      "loss: 0.346299  [107200/175341]\n",
      "loss: 0.446026  [108800/175341]\n",
      "loss: 0.341910  [110400/175341]\n",
      "loss: 0.220117  [112000/175341]\n",
      "loss: 0.995514  [113600/175341]\n",
      "loss: 0.220157  [115200/175341]\n",
      "loss: 0.347336  [116800/175341]\n",
      "loss: 0.256447  [118400/175341]\n",
      "loss: 0.324927  [120000/175341]\n",
      "loss: 0.481464  [121600/175341]\n",
      "loss: 0.484544  [123200/175341]\n",
      "loss: 0.474188  [124800/175341]\n",
      "loss: 0.493708  [126400/175341]\n",
      "loss: 0.331672  [128000/175341]\n",
      "loss: 0.326701  [129600/175341]\n",
      "loss: 0.728938  [131200/175341]\n",
      "loss: 0.248658  [132800/175341]\n",
      "loss: 0.777411  [134400/175341]\n",
      "loss: 0.416717  [136000/175341]\n",
      "loss: 0.377289  [137600/175341]\n",
      "loss: 0.482528  [139200/175341]\n",
      "loss: 0.434526  [140800/175341]\n",
      "loss: 0.542171  [142400/175341]\n",
      "loss: 0.427304  [144000/175341]\n",
      "loss: 0.292217  [145600/175341]\n",
      "loss: 0.450195  [147200/175341]\n",
      "loss: 0.142935  [148800/175341]\n",
      "loss: 0.414817  [150400/175341]\n",
      "loss: 0.606574  [152000/175341]\n",
      "loss: 0.406294  [153600/175341]\n",
      "loss: 0.753347  [155200/175341]\n",
      "loss: 0.625262  [156800/175341]\n",
      "loss: 0.930090  [158400/175341]\n",
      "loss: 0.370089  [160000/175341]\n",
      "loss: 0.395051  [161600/175341]\n",
      "loss: 0.480313  [163200/175341]\n",
      "loss: 0.369352  [164800/175341]\n",
      "loss: 0.404086  [166400/175341]\n",
      "loss: 0.391045  [168000/175341]\n",
      "loss: 0.601157  [169600/175341]\n",
      "loss: 0.315994  [171200/175341]\n",
      "loss: 0.266260  [172800/175341]\n",
      "loss: 0.556740  [174400/175341]\n",
      "Train Accuracy: 81.4059%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.550849, F1-score: 75.85%, Macro_F1-Score:  43.21%  \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.236273  [    0/175341]\n",
      "loss: 0.516198  [ 1600/175341]\n",
      "loss: 0.408686  [ 3200/175341]\n",
      "loss: 0.260073  [ 4800/175341]\n",
      "loss: 0.530262  [ 6400/175341]\n",
      "loss: 0.240854  [ 8000/175341]\n",
      "loss: 0.302501  [ 9600/175341]\n",
      "loss: 0.297453  [11200/175341]\n",
      "loss: 0.289184  [12800/175341]\n",
      "loss: 0.618547  [14400/175341]\n",
      "loss: 0.432326  [16000/175341]\n",
      "loss: 0.587186  [17600/175341]\n",
      "loss: 0.810592  [19200/175341]\n",
      "loss: 0.450203  [20800/175341]\n",
      "loss: 0.606733  [22400/175341]\n",
      "loss: 0.147213  [24000/175341]\n",
      "loss: 0.247799  [25600/175341]\n",
      "loss: 0.561108  [27200/175341]\n",
      "loss: 0.432586  [28800/175341]\n",
      "loss: 0.669024  [30400/175341]\n",
      "loss: 0.407240  [32000/175341]\n",
      "loss: 0.255494  [33600/175341]\n",
      "loss: 0.249798  [35200/175341]\n",
      "loss: 0.391681  [36800/175341]\n",
      "loss: 0.702575  [38400/175341]\n",
      "loss: 0.361608  [40000/175341]\n",
      "loss: 0.226656  [41600/175341]\n",
      "loss: 0.909585  [43200/175341]\n",
      "loss: 0.637734  [44800/175341]\n",
      "loss: 0.570054  [46400/175341]\n",
      "loss: 0.137999  [48000/175341]\n",
      "loss: 0.733838  [49600/175341]\n",
      "loss: 0.256056  [51200/175341]\n",
      "loss: 0.331696  [52800/175341]\n",
      "loss: 0.325689  [54400/175341]\n",
      "loss: 0.266779  [56000/175341]\n",
      "loss: 0.496391  [57600/175341]\n",
      "loss: 0.373060  [59200/175341]\n",
      "loss: 0.320774  [60800/175341]\n",
      "loss: 0.477760  [62400/175341]\n",
      "loss: 0.384284  [64000/175341]\n",
      "loss: 0.258801  [65600/175341]\n",
      "loss: 0.322346  [67200/175341]\n",
      "loss: 0.548815  [68800/175341]\n",
      "loss: 0.571904  [70400/175341]\n",
      "loss: 0.790359  [72000/175341]\n",
      "loss: 0.158955  [73600/175341]\n",
      "loss: 0.570316  [75200/175341]\n",
      "loss: 0.416521  [76800/175341]\n",
      "loss: 0.669090  [78400/175341]\n",
      "loss: 0.329152  [80000/175341]\n",
      "loss: 0.448465  [81600/175341]\n",
      "loss: 0.344314  [83200/175341]\n",
      "loss: 0.376248  [84800/175341]\n",
      "loss: 0.433193  [86400/175341]\n",
      "loss: 0.580885  [88000/175341]\n",
      "loss: 0.455069  [89600/175341]\n",
      "loss: 0.423256  [91200/175341]\n",
      "loss: 0.468388  [92800/175341]\n",
      "loss: 0.470097  [94400/175341]\n",
      "loss: 0.545268  [96000/175341]\n",
      "loss: 0.664882  [97600/175341]\n",
      "loss: 0.404252  [99200/175341]\n",
      "loss: 0.754007  [100800/175341]\n",
      "loss: 0.593246  [102400/175341]\n",
      "loss: 0.476011  [104000/175341]\n",
      "loss: 0.624292  [105600/175341]\n",
      "loss: 0.265879  [107200/175341]\n",
      "loss: 0.452623  [108800/175341]\n",
      "loss: 0.543393  [110400/175341]\n",
      "loss: 0.396754  [112000/175341]\n",
      "loss: 0.684101  [113600/175341]\n",
      "loss: 0.358275  [115200/175341]\n",
      "loss: 0.479779  [116800/175341]\n",
      "loss: 0.442798  [118400/175341]\n",
      "loss: 0.922727  [120000/175341]\n",
      "loss: 0.699401  [121600/175341]\n",
      "loss: 0.416308  [123200/175341]\n",
      "loss: 0.739866  [124800/175341]\n",
      "loss: 0.422051  [126400/175341]\n",
      "loss: 0.790633  [128000/175341]\n",
      "loss: 0.307491  [129600/175341]\n",
      "loss: 0.667685  [131200/175341]\n",
      "loss: 0.804582  [132800/175341]\n",
      "loss: 0.650286  [134400/175341]\n",
      "loss: 0.322113  [136000/175341]\n",
      "loss: 0.567410  [137600/175341]\n",
      "loss: 0.764854  [139200/175341]\n",
      "loss: 0.350349  [140800/175341]\n",
      "loss: 0.460053  [142400/175341]\n",
      "loss: 0.497042  [144000/175341]\n",
      "loss: 0.246382  [145600/175341]\n",
      "loss: 0.338786  [147200/175341]\n",
      "loss: 0.309189  [148800/175341]\n",
      "loss: 0.616761  [150400/175341]\n",
      "loss: 0.827754  [152000/175341]\n",
      "loss: 0.788841  [153600/175341]\n",
      "loss: 0.147168  [155200/175341]\n",
      "loss: 0.727077  [156800/175341]\n",
      "loss: 0.796274  [158400/175341]\n",
      "loss: 0.128314  [160000/175341]\n",
      "loss: 0.292809  [161600/175341]\n",
      "loss: 0.380030  [163200/175341]\n",
      "loss: 0.433094  [164800/175341]\n",
      "loss: 0.562062  [166400/175341]\n",
      "loss: 0.509838  [168000/175341]\n",
      "loss: 0.798488  [169600/175341]\n",
      "loss: 0.276588  [171200/175341]\n",
      "loss: 0.807139  [172800/175341]\n",
      "loss: 0.555122  [174400/175341]\n",
      "Train Accuracy: 81.3900%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.532809, F1-score: 76.38%, Macro_F1-Score:  42.84%  \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.215255  [    0/175341]\n",
      "loss: 0.154885  [ 1600/175341]\n",
      "loss: 0.384282  [ 3200/175341]\n",
      "loss: 0.226190  [ 4800/175341]\n",
      "loss: 0.631460  [ 6400/175341]\n",
      "loss: 0.271131  [ 8000/175341]\n",
      "loss: 0.774987  [ 9600/175341]\n",
      "loss: 0.737392  [11200/175341]\n",
      "loss: 0.849275  [12800/175341]\n",
      "loss: 0.383648  [14400/175341]\n",
      "loss: 0.353797  [16000/175341]\n",
      "loss: 0.198106  [17600/175341]\n",
      "loss: 0.187994  [19200/175341]\n",
      "loss: 0.288717  [20800/175341]\n",
      "loss: 0.055384  [22400/175341]\n",
      "loss: 0.687590  [24000/175341]\n",
      "loss: 0.451484  [25600/175341]\n",
      "loss: 0.877684  [27200/175341]\n",
      "loss: 0.536243  [28800/175341]\n",
      "loss: 0.523110  [30400/175341]\n",
      "loss: 0.335077  [32000/175341]\n",
      "loss: 0.281741  [33600/175341]\n",
      "loss: 0.353303  [35200/175341]\n",
      "loss: 0.697667  [36800/175341]\n",
      "loss: 0.542850  [38400/175341]\n",
      "loss: 0.669060  [40000/175341]\n",
      "loss: 0.236243  [41600/175341]\n",
      "loss: 0.418014  [43200/175341]\n",
      "loss: 0.383062  [44800/175341]\n",
      "loss: 0.589810  [46400/175341]\n",
      "loss: 0.384721  [48000/175341]\n",
      "loss: 0.648177  [49600/175341]\n",
      "loss: 0.416220  [51200/175341]\n",
      "loss: 0.276442  [52800/175341]\n",
      "loss: 0.983350  [54400/175341]\n",
      "loss: 0.135550  [56000/175341]\n",
      "loss: 0.612857  [57600/175341]\n",
      "loss: 0.310898  [59200/175341]\n",
      "loss: 0.379865  [60800/175341]\n",
      "loss: 0.287894  [62400/175341]\n",
      "loss: 0.328125  [64000/175341]\n",
      "loss: 0.287641  [65600/175341]\n",
      "loss: 0.321372  [67200/175341]\n",
      "loss: 0.663435  [68800/175341]\n",
      "loss: 0.543094  [70400/175341]\n",
      "loss: 0.212682  [72000/175341]\n",
      "loss: 0.203291  [73600/175341]\n",
      "loss: 0.327620  [75200/175341]\n",
      "loss: 1.086813  [76800/175341]\n",
      "loss: 0.669694  [78400/175341]\n",
      "loss: 0.610219  [80000/175341]\n",
      "loss: 0.313786  [81600/175341]\n",
      "loss: 0.732216  [83200/175341]\n",
      "loss: 0.324554  [84800/175341]\n",
      "loss: 0.499547  [86400/175341]\n",
      "loss: 0.397841  [88000/175341]\n",
      "loss: 0.511770  [89600/175341]\n",
      "loss: 0.138489  [91200/175341]\n",
      "loss: 0.572565  [92800/175341]\n",
      "loss: 0.769684  [94400/175341]\n",
      "loss: 0.228673  [96000/175341]\n",
      "loss: 0.744139  [97600/175341]\n",
      "loss: 0.264394  [99200/175341]\n",
      "loss: 0.196092  [100800/175341]\n",
      "loss: 0.582696  [102400/175341]\n",
      "loss: 0.138143  [104000/175341]\n",
      "loss: 0.418691  [105600/175341]\n",
      "loss: 0.676686  [107200/175341]\n",
      "loss: 0.500520  [108800/175341]\n",
      "loss: 0.384531  [110400/175341]\n",
      "loss: 0.511422  [112000/175341]\n",
      "loss: 0.338261  [113600/175341]\n",
      "loss: 0.416704  [115200/175341]\n",
      "loss: 0.946681  [116800/175341]\n",
      "loss: 0.372676  [118400/175341]\n",
      "loss: 0.625640  [120000/175341]\n",
      "loss: 0.444793  [121600/175341]\n",
      "loss: 0.693402  [123200/175341]\n",
      "loss: 0.281795  [124800/175341]\n",
      "loss: 0.814160  [126400/175341]\n",
      "loss: 0.364035  [128000/175341]\n",
      "loss: 0.890648  [129600/175341]\n",
      "loss: 0.361188  [131200/175341]\n",
      "loss: 0.654624  [132800/175341]\n",
      "loss: 0.761968  [134400/175341]\n",
      "loss: 0.735925  [136000/175341]\n",
      "loss: 0.107023  [137600/175341]\n",
      "loss: 0.228097  [139200/175341]\n",
      "loss: 0.480858  [140800/175341]\n",
      "loss: 0.839932  [142400/175341]\n",
      "loss: 0.553790  [144000/175341]\n",
      "loss: 0.644329  [145600/175341]\n",
      "loss: 0.427221  [147200/175341]\n",
      "loss: 0.573726  [148800/175341]\n",
      "loss: 0.765096  [150400/175341]\n",
      "loss: 0.552840  [152000/175341]\n",
      "loss: 0.836917  [153600/175341]\n",
      "loss: 0.395069  [155200/175341]\n",
      "loss: 0.507848  [156800/175341]\n",
      "loss: 0.586644  [158400/175341]\n",
      "loss: 0.938851  [160000/175341]\n",
      "loss: 0.284524  [161600/175341]\n",
      "loss: 0.517590  [163200/175341]\n",
      "loss: 0.363162  [164800/175341]\n",
      "loss: 0.263282  [166400/175341]\n",
      "loss: 0.593363  [168000/175341]\n",
      "loss: 0.335805  [169600/175341]\n",
      "loss: 0.373382  [171200/175341]\n",
      "loss: 0.338061  [172800/175341]\n",
      "loss: 0.654865  [174400/175341]\n",
      "Train Accuracy: 81.3660%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.544422, F1-score: 75.85%, Macro_F1-Score:  42.33%  \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.306756  [    0/175341]\n",
      "loss: 0.278293  [ 1600/175341]\n",
      "loss: 0.237874  [ 3200/175341]\n",
      "loss: 0.248133  [ 4800/175341]\n",
      "loss: 0.480380  [ 6400/175341]\n",
      "loss: 0.695637  [ 8000/175341]\n",
      "loss: 0.444674  [ 9600/175341]\n",
      "loss: 0.466696  [11200/175341]\n",
      "loss: 0.177844  [12800/175341]\n",
      "loss: 0.574768  [14400/175341]\n",
      "loss: 0.676762  [16000/175341]\n",
      "loss: 0.148149  [17600/175341]\n",
      "loss: 0.409297  [19200/175341]\n",
      "loss: 0.522991  [20800/175341]\n",
      "loss: 1.285098  [22400/175341]\n",
      "loss: 0.501946  [24000/175341]\n",
      "loss: 0.597726  [25600/175341]\n",
      "loss: 0.653959  [27200/175341]\n",
      "loss: 0.747844  [28800/175341]\n",
      "loss: 0.754541  [30400/175341]\n",
      "loss: 0.547427  [32000/175341]\n",
      "loss: 0.466559  [33600/175341]\n",
      "loss: 0.558839  [35200/175341]\n",
      "loss: 0.841417  [36800/175341]\n",
      "loss: 0.549722  [38400/175341]\n",
      "loss: 0.347767  [40000/175341]\n",
      "loss: 0.639384  [41600/175341]\n",
      "loss: 0.521246  [43200/175341]\n",
      "loss: 0.393830  [44800/175341]\n",
      "loss: 0.379650  [46400/175341]\n",
      "loss: 0.345437  [48000/175341]\n",
      "loss: 0.416186  [49600/175341]\n",
      "loss: 0.690825  [51200/175341]\n",
      "loss: 0.218122  [52800/175341]\n",
      "loss: 0.498636  [54400/175341]\n",
      "loss: 0.410103  [56000/175341]\n",
      "loss: 0.654016  [57600/175341]\n",
      "loss: 0.387156  [59200/175341]\n",
      "loss: 0.215300  [60800/175341]\n",
      "loss: 0.640994  [62400/175341]\n",
      "loss: 0.485414  [64000/175341]\n",
      "loss: 0.624664  [65600/175341]\n",
      "loss: 0.591506  [67200/175341]\n",
      "loss: 0.118862  [68800/175341]\n",
      "loss: 0.487485  [70400/175341]\n",
      "loss: 0.397113  [72000/175341]\n",
      "loss: 0.365935  [73600/175341]\n",
      "loss: 0.804977  [75200/175341]\n",
      "loss: 0.331970  [76800/175341]\n",
      "loss: 0.479501  [78400/175341]\n",
      "loss: 0.305889  [80000/175341]\n",
      "loss: 0.405814  [81600/175341]\n",
      "loss: 0.405404  [83200/175341]\n",
      "loss: 0.712415  [84800/175341]\n",
      "loss: 0.467093  [86400/175341]\n",
      "loss: 0.655312  [88000/175341]\n",
      "loss: 0.176572  [89600/175341]\n",
      "loss: 0.561235  [91200/175341]\n",
      "loss: 0.462841  [92800/175341]\n",
      "loss: 0.522344  [94400/175341]\n",
      "loss: 0.386504  [96000/175341]\n",
      "loss: 0.083629  [97600/175341]\n",
      "loss: 0.533130  [99200/175341]\n",
      "loss: 0.286860  [100800/175341]\n",
      "loss: 0.599554  [102400/175341]\n",
      "loss: 0.849487  [104000/175341]\n",
      "loss: 0.264843  [105600/175341]\n",
      "loss: 0.255861  [107200/175341]\n",
      "loss: 0.444461  [108800/175341]\n",
      "loss: 0.581870  [110400/175341]\n",
      "loss: 0.490236  [112000/175341]\n",
      "loss: 0.256088  [113600/175341]\n",
      "loss: 0.263725  [115200/175341]\n",
      "loss: 0.205950  [116800/175341]\n",
      "loss: 0.497957  [118400/175341]\n",
      "loss: 0.415698  [120000/175341]\n",
      "loss: 0.546872  [121600/175341]\n",
      "loss: 0.464347  [123200/175341]\n",
      "loss: 0.447853  [124800/175341]\n",
      "loss: 0.597342  [126400/175341]\n",
      "loss: 0.446409  [128000/175341]\n",
      "loss: 0.446417  [129600/175341]\n",
      "loss: 0.444865  [131200/175341]\n",
      "loss: 0.233401  [132800/175341]\n",
      "loss: 0.387892  [134400/175341]\n",
      "loss: 0.477680  [136000/175341]\n",
      "loss: 0.535542  [137600/175341]\n",
      "loss: 0.603038  [139200/175341]\n",
      "loss: 0.564031  [140800/175341]\n",
      "loss: 0.470087  [142400/175341]\n",
      "loss: 0.520230  [144000/175341]\n",
      "loss: 0.700386  [145600/175341]\n",
      "loss: 0.881828  [147200/175341]\n",
      "loss: 0.362753  [148800/175341]\n",
      "loss: 0.447423  [150400/175341]\n",
      "loss: 0.676556  [152000/175341]\n",
      "loss: 0.239293  [153600/175341]\n",
      "loss: 0.501958  [155200/175341]\n",
      "loss: 0.197191  [156800/175341]\n",
      "loss: 0.527325  [158400/175341]\n",
      "loss: 0.316154  [160000/175341]\n",
      "loss: 0.218675  [161600/175341]\n",
      "loss: 0.253868  [163200/175341]\n",
      "loss: 0.526810  [164800/175341]\n",
      "loss: 0.487793  [166400/175341]\n",
      "loss: 0.673653  [168000/175341]\n",
      "loss: 0.149895  [169600/175341]\n",
      "loss: 0.295722  [171200/175341]\n",
      "loss: 0.376197  [172800/175341]\n",
      "loss: 0.522406  [174400/175341]\n",
      "Train Accuracy: 81.3694%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.551488, F1-score: 75.46%, Macro_F1-Score:  42.38%  \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.601707  [    0/175341]\n",
      "loss: 0.200900  [ 1600/175341]\n",
      "loss: 0.589228  [ 3200/175341]\n",
      "loss: 0.948871  [ 4800/175341]\n",
      "loss: 0.217347  [ 6400/175341]\n",
      "loss: 0.823738  [ 8000/175341]\n",
      "loss: 0.325408  [ 9600/175341]\n",
      "loss: 0.428611  [11200/175341]\n",
      "loss: 0.596142  [12800/175341]\n",
      "loss: 0.167614  [14400/175341]\n",
      "loss: 0.358569  [16000/175341]\n",
      "loss: 0.704419  [17600/175341]\n",
      "loss: 0.212701  [19200/175341]\n",
      "loss: 0.448062  [20800/175341]\n",
      "loss: 0.279369  [22400/175341]\n",
      "loss: 0.582648  [24000/175341]\n",
      "loss: 0.194032  [25600/175341]\n",
      "loss: 0.661612  [27200/175341]\n",
      "loss: 0.227843  [28800/175341]\n",
      "loss: 0.730715  [30400/175341]\n",
      "loss: 0.321863  [32000/175341]\n",
      "loss: 0.221473  [33600/175341]\n",
      "loss: 0.283939  [35200/175341]\n",
      "loss: 0.332447  [36800/175341]\n",
      "loss: 0.702000  [38400/175341]\n",
      "loss: 0.654646  [40000/175341]\n",
      "loss: 0.198942  [41600/175341]\n",
      "loss: 0.463934  [43200/175341]\n",
      "loss: 0.396107  [44800/175341]\n",
      "loss: 0.530790  [46400/175341]\n",
      "loss: 0.534850  [48000/175341]\n",
      "loss: 0.431787  [49600/175341]\n",
      "loss: 0.383347  [51200/175341]\n",
      "loss: 0.658829  [52800/175341]\n",
      "loss: 0.302573  [54400/175341]\n",
      "loss: 0.188979  [56000/175341]\n",
      "loss: 0.192440  [57600/175341]\n",
      "loss: 0.328178  [59200/175341]\n",
      "loss: 1.056942  [60800/175341]\n",
      "loss: 0.290435  [62400/175341]\n",
      "loss: 0.459285  [64000/175341]\n",
      "loss: 0.334325  [65600/175341]\n",
      "loss: 0.244474  [67200/175341]\n",
      "loss: 0.145796  [68800/175341]\n",
      "loss: 0.628265  [70400/175341]\n",
      "loss: 0.499367  [72000/175341]\n",
      "loss: 0.100260  [73600/175341]\n",
      "loss: 0.806987  [75200/175341]\n",
      "loss: 0.505534  [76800/175341]\n",
      "loss: 0.478304  [78400/175341]\n",
      "loss: 0.676092  [80000/175341]\n",
      "loss: 0.406563  [81600/175341]\n",
      "loss: 0.747715  [83200/175341]\n",
      "loss: 0.843548  [84800/175341]\n",
      "loss: 1.385717  [86400/175341]\n",
      "loss: 0.653713  [88000/175341]\n",
      "loss: 0.276070  [89600/175341]\n",
      "loss: 0.465875  [91200/175341]\n",
      "loss: 0.403892  [92800/175341]\n",
      "loss: 0.437068  [94400/175341]\n",
      "loss: 0.677658  [96000/175341]\n",
      "loss: 0.645709  [97600/175341]\n",
      "loss: 0.401294  [99200/175341]\n",
      "loss: 0.306781  [100800/175341]\n",
      "loss: 0.312677  [102400/175341]\n",
      "loss: 0.498223  [104000/175341]\n",
      "loss: 0.750454  [105600/175341]\n",
      "loss: 0.213711  [107200/175341]\n",
      "loss: 0.475583  [108800/175341]\n",
      "loss: 0.428166  [110400/175341]\n",
      "loss: 0.651061  [112000/175341]\n",
      "loss: 0.576836  [113600/175341]\n",
      "loss: 0.319451  [115200/175341]\n",
      "loss: 0.284851  [116800/175341]\n",
      "loss: 1.170835  [118400/175341]\n",
      "loss: 0.247024  [120000/175341]\n",
      "loss: 0.513690  [121600/175341]\n",
      "loss: 0.704504  [123200/175341]\n",
      "loss: 0.461130  [124800/175341]\n",
      "loss: 0.703934  [126400/175341]\n",
      "loss: 0.310927  [128000/175341]\n",
      "loss: 0.356925  [129600/175341]\n",
      "loss: 0.365345  [131200/175341]\n",
      "loss: 0.613819  [132800/175341]\n",
      "loss: 0.725014  [134400/175341]\n",
      "loss: 0.323596  [136000/175341]\n",
      "loss: 0.575263  [137600/175341]\n",
      "loss: 0.338280  [139200/175341]\n",
      "loss: 0.351518  [140800/175341]\n",
      "loss: 0.368396  [142400/175341]\n",
      "loss: 0.317335  [144000/175341]\n",
      "loss: 0.400673  [145600/175341]\n",
      "loss: 0.345656  [147200/175341]\n",
      "loss: 0.416025  [148800/175341]\n",
      "loss: 0.251763  [150400/175341]\n",
      "loss: 0.643859  [152000/175341]\n",
      "loss: 0.427709  [153600/175341]\n",
      "loss: 0.979957  [155200/175341]\n",
      "loss: 0.141315  [156800/175341]\n",
      "loss: 0.420346  [158400/175341]\n",
      "loss: 0.404560  [160000/175341]\n",
      "loss: 1.035718  [161600/175341]\n",
      "loss: 0.437610  [163200/175341]\n",
      "loss: 0.336333  [164800/175341]\n",
      "loss: 0.458402  [166400/175341]\n",
      "loss: 0.524613  [168000/175341]\n",
      "loss: 0.243523  [169600/175341]\n",
      "loss: 0.418348  [171200/175341]\n",
      "loss: 0.392930  [172800/175341]\n",
      "loss: 0.646123  [174400/175341]\n",
      "Train Accuracy: 81.3957%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.536682, F1-score: 76.40%, Macro_F1-Score:  42.93%  \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.958480  [    0/175341]\n",
      "loss: 0.132887  [ 1600/175341]\n",
      "loss: 0.355720  [ 3200/175341]\n",
      "loss: 0.627727  [ 4800/175341]\n",
      "loss: 0.680942  [ 6400/175341]\n",
      "loss: 0.602983  [ 8000/175341]\n",
      "loss: 0.264547  [ 9600/175341]\n",
      "loss: 0.710168  [11200/175341]\n",
      "loss: 0.213260  [12800/175341]\n",
      "loss: 0.233189  [14400/175341]\n",
      "loss: 0.248182  [16000/175341]\n",
      "loss: 0.407625  [17600/175341]\n",
      "loss: 0.273737  [19200/175341]\n",
      "loss: 0.971827  [20800/175341]\n",
      "loss: 0.584764  [22400/175341]\n",
      "loss: 0.228864  [24000/175341]\n",
      "loss: 0.616164  [25600/175341]\n",
      "loss: 0.280352  [27200/175341]\n",
      "loss: 0.324029  [28800/175341]\n",
      "loss: 1.105534  [30400/175341]\n",
      "loss: 0.530225  [32000/175341]\n",
      "loss: 0.336372  [33600/175341]\n",
      "loss: 0.550820  [35200/175341]\n",
      "loss: 0.431398  [36800/175341]\n",
      "loss: 0.767835  [38400/175341]\n",
      "loss: 0.612918  [40000/175341]\n",
      "loss: 0.195202  [41600/175341]\n",
      "loss: 0.604100  [43200/175341]\n",
      "loss: 0.617733  [44800/175341]\n",
      "loss: 1.122026  [46400/175341]\n",
      "loss: 0.857927  [48000/175341]\n",
      "loss: 0.331897  [49600/175341]\n",
      "loss: 0.444913  [51200/175341]\n",
      "loss: 0.488238  [52800/175341]\n",
      "loss: 0.394466  [54400/175341]\n",
      "loss: 0.342255  [56000/175341]\n",
      "loss: 0.423250  [57600/175341]\n",
      "loss: 0.694144  [59200/175341]\n",
      "loss: 0.509211  [60800/175341]\n",
      "loss: 0.257799  [62400/175341]\n",
      "loss: 0.730328  [64000/175341]\n",
      "loss: 0.433465  [65600/175341]\n",
      "loss: 0.387451  [67200/175341]\n",
      "loss: 0.236434  [68800/175341]\n",
      "loss: 0.326901  [70400/175341]\n",
      "loss: 0.294386  [72000/175341]\n",
      "loss: 0.453753  [73600/175341]\n",
      "loss: 0.243893  [75200/175341]\n",
      "loss: 0.381569  [76800/175341]\n",
      "loss: 0.666285  [78400/175341]\n",
      "loss: 0.485539  [80000/175341]\n",
      "loss: 0.265085  [81600/175341]\n",
      "loss: 0.421712  [83200/175341]\n",
      "loss: 0.457343  [84800/175341]\n",
      "loss: 0.540799  [86400/175341]\n",
      "loss: 0.646355  [88000/175341]\n",
      "loss: 0.295924  [89600/175341]\n",
      "loss: 0.675771  [91200/175341]\n",
      "loss: 0.360099  [92800/175341]\n",
      "loss: 0.398786  [94400/175341]\n",
      "loss: 0.380749  [96000/175341]\n",
      "loss: 0.484876  [97600/175341]\n",
      "loss: 0.450507  [99200/175341]\n",
      "loss: 0.789210  [100800/175341]\n",
      "loss: 0.223768  [102400/175341]\n",
      "loss: 0.462380  [104000/175341]\n",
      "loss: 0.273806  [105600/175341]\n",
      "loss: 0.441883  [107200/175341]\n",
      "loss: 0.488070  [108800/175341]\n",
      "loss: 0.437208  [110400/175341]\n",
      "loss: 0.170256  [112000/175341]\n",
      "loss: 0.445489  [113600/175341]\n",
      "loss: 0.307663  [115200/175341]\n",
      "loss: 0.502494  [116800/175341]\n",
      "loss: 0.328901  [118400/175341]\n",
      "loss: 0.175253  [120000/175341]\n",
      "loss: 0.333119  [121600/175341]\n",
      "loss: 0.634866  [123200/175341]\n",
      "loss: 0.489940  [124800/175341]\n",
      "loss: 0.122434  [126400/175341]\n",
      "loss: 0.423995  [128000/175341]\n",
      "loss: 0.897551  [129600/175341]\n",
      "loss: 0.878386  [131200/175341]\n",
      "loss: 0.184391  [132800/175341]\n",
      "loss: 0.476078  [134400/175341]\n",
      "loss: 0.570237  [136000/175341]\n",
      "loss: 0.628861  [137600/175341]\n",
      "loss: 0.546274  [139200/175341]\n",
      "loss: 0.630390  [140800/175341]\n",
      "loss: 0.356333  [142400/175341]\n",
      "loss: 0.650648  [144000/175341]\n",
      "loss: 1.094419  [145600/175341]\n",
      "loss: 0.685339  [147200/175341]\n",
      "loss: 0.441972  [148800/175341]\n",
      "loss: 0.784186  [150400/175341]\n",
      "loss: 0.590990  [152000/175341]\n",
      "loss: 0.576041  [153600/175341]\n",
      "loss: 0.369479  [155200/175341]\n",
      "loss: 0.320640  [156800/175341]\n",
      "loss: 0.467568  [158400/175341]\n",
      "loss: 0.545826  [160000/175341]\n",
      "loss: 0.841931  [161600/175341]\n",
      "loss: 0.156583  [163200/175341]\n",
      "loss: 0.378690  [164800/175341]\n",
      "loss: 0.429564  [166400/175341]\n",
      "loss: 0.325390  [168000/175341]\n",
      "loss: 0.334662  [169600/175341]\n",
      "loss: 0.093153  [171200/175341]\n",
      "loss: 0.806623  [172800/175341]\n",
      "loss: 0.472722  [174400/175341]\n",
      "Train Accuracy: 81.3666%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.537389, F1-score: 76.83%, Macro_F1-Score:  43.92%  \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.157574  [    0/175341]\n",
      "loss: 0.359316  [ 1600/175341]\n",
      "loss: 0.382083  [ 3200/175341]\n",
      "loss: 0.799745  [ 4800/175341]\n",
      "loss: 0.957184  [ 6400/175341]\n",
      "loss: 0.415188  [ 8000/175341]\n",
      "loss: 0.297786  [ 9600/175341]\n",
      "loss: 0.541549  [11200/175341]\n",
      "loss: 0.582095  [12800/175341]\n",
      "loss: 0.734331  [14400/175341]\n",
      "loss: 0.475823  [16000/175341]\n",
      "loss: 0.200679  [17600/175341]\n",
      "loss: 0.187680  [19200/175341]\n",
      "loss: 0.830786  [20800/175341]\n",
      "loss: 0.407021  [22400/175341]\n",
      "loss: 0.565245  [24000/175341]\n",
      "loss: 0.377026  [25600/175341]\n",
      "loss: 0.349601  [27200/175341]\n",
      "loss: 0.435906  [28800/175341]\n",
      "loss: 0.417735  [30400/175341]\n",
      "loss: 0.586318  [32000/175341]\n",
      "loss: 0.653571  [33600/175341]\n",
      "loss: 0.457292  [35200/175341]\n",
      "loss: 0.717200  [36800/175341]\n",
      "loss: 0.643220  [38400/175341]\n",
      "loss: 0.616195  [40000/175341]\n",
      "loss: 0.086097  [41600/175341]\n",
      "loss: 0.219258  [43200/175341]\n",
      "loss: 0.397078  [44800/175341]\n",
      "loss: 0.487334  [46400/175341]\n",
      "loss: 0.041376  [48000/175341]\n",
      "loss: 0.657466  [49600/175341]\n",
      "loss: 0.386218  [51200/175341]\n",
      "loss: 0.315006  [52800/175341]\n",
      "loss: 0.511413  [54400/175341]\n",
      "loss: 0.209923  [56000/175341]\n",
      "loss: 0.898928  [57600/175341]\n",
      "loss: 0.655061  [59200/175341]\n",
      "loss: 0.553644  [60800/175341]\n",
      "loss: 0.650166  [62400/175341]\n",
      "loss: 0.457844  [64000/175341]\n",
      "loss: 0.694262  [65600/175341]\n",
      "loss: 0.807059  [67200/175341]\n",
      "loss: 0.760379  [68800/175341]\n",
      "loss: 0.390602  [70400/175341]\n",
      "loss: 0.252097  [72000/175341]\n",
      "loss: 0.489672  [73600/175341]\n",
      "loss: 0.423364  [75200/175341]\n",
      "loss: 0.238407  [76800/175341]\n",
      "loss: 0.975638  [78400/175341]\n",
      "loss: 0.671694  [80000/175341]\n",
      "loss: 0.309452  [81600/175341]\n",
      "loss: 0.391933  [83200/175341]\n",
      "loss: 0.339471  [84800/175341]\n",
      "loss: 0.276931  [86400/175341]\n",
      "loss: 1.155497  [88000/175341]\n",
      "loss: 0.314442  [89600/175341]\n",
      "loss: 0.321305  [91200/175341]\n",
      "loss: 0.437913  [92800/175341]\n",
      "loss: 0.390951  [94400/175341]\n",
      "loss: 0.326482  [96000/175341]\n",
      "loss: 0.437766  [97600/175341]\n",
      "loss: 0.458604  [99200/175341]\n",
      "loss: 0.410847  [100800/175341]\n",
      "loss: 0.489709  [102400/175341]\n",
      "loss: 0.586628  [104000/175341]\n",
      "loss: 0.522250  [105600/175341]\n",
      "loss: 0.148966  [107200/175341]\n",
      "loss: 0.399222  [108800/175341]\n",
      "loss: 0.755224  [110400/175341]\n",
      "loss: 0.589921  [112000/175341]\n",
      "loss: 0.679634  [113600/175341]\n",
      "loss: 0.217858  [115200/175341]\n",
      "loss: 0.250258  [116800/175341]\n",
      "loss: 0.457177  [118400/175341]\n",
      "loss: 0.246285  [120000/175341]\n",
      "loss: 0.739723  [121600/175341]\n",
      "loss: 0.204432  [123200/175341]\n",
      "loss: 0.210811  [124800/175341]\n",
      "loss: 0.372882  [126400/175341]\n",
      "loss: 0.516631  [128000/175341]\n",
      "loss: 0.271436  [129600/175341]\n",
      "loss: 0.415415  [131200/175341]\n",
      "loss: 0.695617  [132800/175341]\n",
      "loss: 0.339060  [134400/175341]\n",
      "loss: 0.472281  [136000/175341]\n",
      "loss: 0.452234  [137600/175341]\n",
      "loss: 0.573474  [139200/175341]\n",
      "loss: 0.669257  [140800/175341]\n",
      "loss: 0.819899  [142400/175341]\n",
      "loss: 0.308632  [144000/175341]\n",
      "loss: 0.648557  [145600/175341]\n",
      "loss: 0.375852  [147200/175341]\n",
      "loss: 0.437283  [148800/175341]\n",
      "loss: 0.642155  [150400/175341]\n",
      "loss: 0.408123  [152000/175341]\n",
      "loss: 0.534475  [153600/175341]\n",
      "loss: 0.721931  [155200/175341]\n",
      "loss: 0.719287  [156800/175341]\n",
      "loss: 0.325029  [158400/175341]\n",
      "loss: 0.900204  [160000/175341]\n",
      "loss: 1.149898  [161600/175341]\n",
      "loss: 0.281822  [163200/175341]\n",
      "loss: 0.233489  [164800/175341]\n",
      "loss: 0.510667  [166400/175341]\n",
      "loss: 0.209318  [168000/175341]\n",
      "loss: 0.464057  [169600/175341]\n",
      "loss: 0.933651  [171200/175341]\n",
      "loss: 0.912830  [172800/175341]\n",
      "loss: 0.642440  [174400/175341]\n",
      "Train Accuracy: 81.4128%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.546411, F1-score: 75.66%, Macro_F1-Score:  42.82%  \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.184384  [    0/175341]\n",
      "loss: 0.424203  [ 1600/175341]\n",
      "loss: 0.458422  [ 3200/175341]\n",
      "loss: 0.341650  [ 4800/175341]\n",
      "loss: 0.575380  [ 6400/175341]\n",
      "loss: 0.388429  [ 8000/175341]\n",
      "loss: 0.437091  [ 9600/175341]\n",
      "loss: 0.096754  [11200/175341]\n",
      "loss: 0.407020  [12800/175341]\n",
      "loss: 0.179155  [14400/175341]\n",
      "loss: 0.340671  [16000/175341]\n",
      "loss: 0.431327  [17600/175341]\n",
      "loss: 1.064918  [19200/175341]\n",
      "loss: 0.522357  [20800/175341]\n",
      "loss: 0.641733  [22400/175341]\n",
      "loss: 0.899115  [24000/175341]\n",
      "loss: 0.635261  [25600/175341]\n",
      "loss: 0.334467  [27200/175341]\n",
      "loss: 0.449990  [28800/175341]\n",
      "loss: 0.302346  [30400/175341]\n",
      "loss: 0.515826  [32000/175341]\n",
      "loss: 0.202408  [33600/175341]\n",
      "loss: 0.699721  [35200/175341]\n",
      "loss: 0.175752  [36800/175341]\n",
      "loss: 0.447840  [38400/175341]\n",
      "loss: 0.124631  [40000/175341]\n",
      "loss: 0.790064  [41600/175341]\n",
      "loss: 0.284614  [43200/175341]\n",
      "loss: 0.533701  [44800/175341]\n",
      "loss: 0.306859  [46400/175341]\n",
      "loss: 0.357169  [48000/175341]\n",
      "loss: 0.259102  [49600/175341]\n",
      "loss: 0.451436  [51200/175341]\n",
      "loss: 0.403125  [52800/175341]\n",
      "loss: 0.661257  [54400/175341]\n",
      "loss: 0.485087  [56000/175341]\n",
      "loss: 0.209999  [57600/175341]\n",
      "loss: 0.663186  [59200/175341]\n",
      "loss: 0.630765  [60800/175341]\n",
      "loss: 0.441351  [62400/175341]\n",
      "loss: 0.619300  [64000/175341]\n",
      "loss: 0.556265  [65600/175341]\n",
      "loss: 0.390528  [67200/175341]\n",
      "loss: 0.197872  [68800/175341]\n",
      "loss: 0.076742  [70400/175341]\n",
      "loss: 0.186805  [72000/175341]\n",
      "loss: 0.739619  [73600/175341]\n",
      "loss: 0.378597  [75200/175341]\n",
      "loss: 0.769705  [76800/175341]\n",
      "loss: 0.298591  [78400/175341]\n",
      "loss: 0.319515  [80000/175341]\n",
      "loss: 0.477954  [81600/175341]\n",
      "loss: 0.252436  [83200/175341]\n",
      "loss: 0.280180  [84800/175341]\n",
      "loss: 0.384755  [86400/175341]\n",
      "loss: 0.834619  [88000/175341]\n",
      "loss: 0.164155  [89600/175341]\n",
      "loss: 0.678883  [91200/175341]\n",
      "loss: 0.846023  [92800/175341]\n",
      "loss: 0.839019  [94400/175341]\n",
      "loss: 0.492747  [96000/175341]\n",
      "loss: 0.689881  [97600/175341]\n",
      "loss: 0.409527  [99200/175341]\n",
      "loss: 0.196904  [100800/175341]\n",
      "loss: 0.830404  [102400/175341]\n",
      "loss: 0.621029  [104000/175341]\n",
      "loss: 0.487163  [105600/175341]\n",
      "loss: 1.086251  [107200/175341]\n",
      "loss: 0.267404  [108800/175341]\n",
      "loss: 0.668299  [110400/175341]\n",
      "loss: 0.283885  [112000/175341]\n",
      "loss: 0.348812  [113600/175341]\n",
      "loss: 0.109158  [115200/175341]\n",
      "loss: 0.457737  [116800/175341]\n",
      "loss: 0.494936  [118400/175341]\n",
      "loss: 0.316958  [120000/175341]\n",
      "loss: 0.226274  [121600/175341]\n",
      "loss: 0.338317  [123200/175341]\n",
      "loss: 0.379679  [124800/175341]\n",
      "loss: 0.274525  [126400/175341]\n",
      "loss: 0.370152  [128000/175341]\n",
      "loss: 0.352364  [129600/175341]\n",
      "loss: 0.282062  [131200/175341]\n",
      "loss: 0.533624  [132800/175341]\n",
      "loss: 0.478302  [134400/175341]\n",
      "loss: 0.264037  [136000/175341]\n",
      "loss: 0.230729  [137600/175341]\n",
      "loss: 0.446180  [139200/175341]\n",
      "loss: 0.426210  [140800/175341]\n",
      "loss: 0.912606  [142400/175341]\n",
      "loss: 0.685969  [144000/175341]\n",
      "loss: 0.763823  [145600/175341]\n",
      "loss: 0.416039  [147200/175341]\n",
      "loss: 0.410444  [148800/175341]\n",
      "loss: 0.331928  [150400/175341]\n",
      "loss: 0.586991  [152000/175341]\n",
      "loss: 0.701773  [153600/175341]\n",
      "loss: 0.245836  [155200/175341]\n",
      "loss: 0.214556  [156800/175341]\n",
      "loss: 0.109424  [158400/175341]\n",
      "loss: 0.538540  [160000/175341]\n",
      "loss: 0.805048  [161600/175341]\n",
      "loss: 0.989532  [163200/175341]\n",
      "loss: 0.374551  [164800/175341]\n",
      "loss: 0.386021  [166400/175341]\n",
      "loss: 0.201766  [168000/175341]\n",
      "loss: 0.328908  [169600/175341]\n",
      "loss: 0.391381  [171200/175341]\n",
      "loss: 0.401751  [172800/175341]\n",
      "loss: 0.467862  [174400/175341]\n",
      "Train Accuracy: 81.4185%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.527620, F1-score: 77.53%, Macro_F1-Score:  43.88%  \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.988661  [    0/175341]\n",
      "loss: 0.913868  [ 1600/175341]\n",
      "loss: 0.613607  [ 3200/175341]\n",
      "loss: 0.528234  [ 4800/175341]\n",
      "loss: 0.227984  [ 6400/175341]\n",
      "loss: 0.659062  [ 8000/175341]\n",
      "loss: 0.461865  [ 9600/175341]\n",
      "loss: 0.315195  [11200/175341]\n",
      "loss: 0.429503  [12800/175341]\n",
      "loss: 0.356152  [14400/175341]\n",
      "loss: 0.497203  [16000/175341]\n",
      "loss: 0.367858  [17600/175341]\n",
      "loss: 0.776052  [19200/175341]\n",
      "loss: 0.582139  [20800/175341]\n",
      "loss: 0.492444  [22400/175341]\n",
      "loss: 0.518279  [24000/175341]\n",
      "loss: 0.323837  [25600/175341]\n",
      "loss: 0.570400  [27200/175341]\n",
      "loss: 0.460884  [28800/175341]\n",
      "loss: 0.449169  [30400/175341]\n",
      "loss: 0.573865  [32000/175341]\n",
      "loss: 0.961146  [33600/175341]\n",
      "loss: 0.558529  [35200/175341]\n",
      "loss: 0.216258  [36800/175341]\n",
      "loss: 0.445758  [38400/175341]\n",
      "loss: 0.561464  [40000/175341]\n",
      "loss: 0.495022  [41600/175341]\n",
      "loss: 0.373002  [43200/175341]\n",
      "loss: 0.644066  [44800/175341]\n",
      "loss: 0.314849  [46400/175341]\n",
      "loss: 0.351940  [48000/175341]\n",
      "loss: 0.307352  [49600/175341]\n",
      "loss: 0.407372  [51200/175341]\n",
      "loss: 0.297048  [52800/175341]\n",
      "loss: 0.580358  [54400/175341]\n",
      "loss: 0.392476  [56000/175341]\n",
      "loss: 0.655072  [57600/175341]\n",
      "loss: 0.255275  [59200/175341]\n",
      "loss: 0.250445  [60800/175341]\n",
      "loss: 0.477712  [62400/175341]\n",
      "loss: 0.366309  [64000/175341]\n",
      "loss: 0.448114  [65600/175341]\n",
      "loss: 0.691655  [67200/175341]\n",
      "loss: 0.639479  [68800/175341]\n",
      "loss: 0.454022  [70400/175341]\n",
      "loss: 0.419933  [72000/175341]\n",
      "loss: 0.407675  [73600/175341]\n",
      "loss: 0.372329  [75200/175341]\n",
      "loss: 0.320810  [76800/175341]\n",
      "loss: 0.455125  [78400/175341]\n",
      "loss: 0.315721  [80000/175341]\n",
      "loss: 0.471906  [81600/175341]\n",
      "loss: 0.322720  [83200/175341]\n",
      "loss: 1.055395  [84800/175341]\n",
      "loss: 0.689837  [86400/175341]\n",
      "loss: 0.483734  [88000/175341]\n",
      "loss: 0.138187  [89600/175341]\n",
      "loss: 0.746943  [91200/175341]\n",
      "loss: 0.308451  [92800/175341]\n",
      "loss: 0.412699  [94400/175341]\n",
      "loss: 0.904137  [96000/175341]\n",
      "loss: 0.568314  [97600/175341]\n",
      "loss: 0.563929  [99200/175341]\n",
      "loss: 0.537777  [100800/175341]\n",
      "loss: 0.457835  [102400/175341]\n",
      "loss: 0.572433  [104000/175341]\n",
      "loss: 0.597797  [105600/175341]\n",
      "loss: 0.260193  [107200/175341]\n",
      "loss: 0.287881  [108800/175341]\n",
      "loss: 0.185354  [110400/175341]\n",
      "loss: 0.178225  [112000/175341]\n",
      "loss: 0.753581  [113600/175341]\n",
      "loss: 0.371981  [115200/175341]\n",
      "loss: 0.309582  [116800/175341]\n",
      "loss: 0.255073  [118400/175341]\n",
      "loss: 0.475101  [120000/175341]\n",
      "loss: 0.699383  [121600/175341]\n",
      "loss: 0.370768  [123200/175341]\n",
      "loss: 0.740394  [124800/175341]\n",
      "loss: 0.605425  [126400/175341]\n",
      "loss: 0.541703  [128000/175341]\n",
      "loss: 0.332952  [129600/175341]\n",
      "loss: 0.403735  [131200/175341]\n",
      "loss: 0.539579  [132800/175341]\n",
      "loss: 1.073921  [134400/175341]\n",
      "loss: 0.230134  [136000/175341]\n",
      "loss: 0.411276  [137600/175341]\n",
      "loss: 0.436684  [139200/175341]\n",
      "loss: 0.432705  [140800/175341]\n",
      "loss: 0.522222  [142400/175341]\n",
      "loss: 0.594064  [144000/175341]\n",
      "loss: 0.764425  [145600/175341]\n",
      "loss: 0.356157  [147200/175341]\n",
      "loss: 0.440626  [148800/175341]\n",
      "loss: 0.523067  [150400/175341]\n",
      "loss: 0.160497  [152000/175341]\n",
      "loss: 0.521705  [153600/175341]\n",
      "loss: 0.344582  [155200/175341]\n",
      "loss: 0.305104  [156800/175341]\n",
      "loss: 0.498172  [158400/175341]\n",
      "loss: 0.302480  [160000/175341]\n",
      "loss: 0.781789  [161600/175341]\n",
      "loss: 1.026932  [163200/175341]\n",
      "loss: 0.542444  [164800/175341]\n",
      "loss: 0.327673  [166400/175341]\n",
      "loss: 0.189032  [168000/175341]\n",
      "loss: 0.482290  [169600/175341]\n",
      "loss: 0.694857  [171200/175341]\n",
      "loss: 0.236967  [172800/175341]\n",
      "loss: 0.296515  [174400/175341]\n",
      "Train Accuracy: 81.3991%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.548711, F1-score: 75.98%, Macro_F1-Score:  42.37%  \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.544501  [    0/175341]\n",
      "loss: 0.181961  [ 1600/175341]\n",
      "loss: 0.431892  [ 3200/175341]\n",
      "loss: 0.391090  [ 4800/175341]\n",
      "loss: 0.104750  [ 6400/175341]\n",
      "loss: 0.444216  [ 8000/175341]\n",
      "loss: 0.769085  [ 9600/175341]\n",
      "loss: 0.313672  [11200/175341]\n",
      "loss: 0.732482  [12800/175341]\n",
      "loss: 0.998243  [14400/175341]\n",
      "loss: 0.492393  [16000/175341]\n",
      "loss: 0.664702  [17600/175341]\n",
      "loss: 0.390073  [19200/175341]\n",
      "loss: 1.160197  [20800/175341]\n",
      "loss: 0.670094  [22400/175341]\n",
      "loss: 0.617088  [24000/175341]\n",
      "loss: 0.572235  [25600/175341]\n",
      "loss: 0.345557  [27200/175341]\n",
      "loss: 0.257027  [28800/175341]\n",
      "loss: 0.445201  [30400/175341]\n",
      "loss: 0.517870  [32000/175341]\n",
      "loss: 0.423572  [33600/175341]\n",
      "loss: 0.705767  [35200/175341]\n",
      "loss: 0.453095  [36800/175341]\n",
      "loss: 0.568074  [38400/175341]\n",
      "loss: 0.326621  [40000/175341]\n",
      "loss: 0.432629  [41600/175341]\n",
      "loss: 0.457694  [43200/175341]\n",
      "loss: 0.576550  [44800/175341]\n",
      "loss: 0.453280  [46400/175341]\n",
      "loss: 0.200581  [48000/175341]\n",
      "loss: 0.258634  [49600/175341]\n",
      "loss: 0.447082  [51200/175341]\n",
      "loss: 0.367390  [52800/175341]\n",
      "loss: 0.552130  [54400/175341]\n",
      "loss: 0.194127  [56000/175341]\n",
      "loss: 0.411601  [57600/175341]\n",
      "loss: 0.245838  [59200/175341]\n",
      "loss: 0.587956  [60800/175341]\n",
      "loss: 0.498215  [62400/175341]\n",
      "loss: 1.141515  [64000/175341]\n",
      "loss: 0.708346  [65600/175341]\n",
      "loss: 0.309249  [67200/175341]\n",
      "loss: 0.173746  [68800/175341]\n",
      "loss: 0.699974  [70400/175341]\n",
      "loss: 0.833284  [72000/175341]\n",
      "loss: 0.308906  [73600/175341]\n",
      "loss: 0.710753  [75200/175341]\n",
      "loss: 0.330914  [76800/175341]\n",
      "loss: 0.364082  [78400/175341]\n",
      "loss: 0.237888  [80000/175341]\n",
      "loss: 0.443148  [81600/175341]\n",
      "loss: 0.490707  [83200/175341]\n",
      "loss: 0.324478  [84800/175341]\n",
      "loss: 0.427121  [86400/175341]\n",
      "loss: 0.486522  [88000/175341]\n",
      "loss: 0.155987  [89600/175341]\n",
      "loss: 0.588721  [91200/175341]\n",
      "loss: 0.335832  [92800/175341]\n",
      "loss: 0.277000  [94400/175341]\n",
      "loss: 0.362185  [96000/175341]\n",
      "loss: 0.416478  [97600/175341]\n",
      "loss: 0.364405  [99200/175341]\n",
      "loss: 0.926384  [100800/175341]\n",
      "loss: 0.377522  [102400/175341]\n",
      "loss: 0.352989  [104000/175341]\n",
      "loss: 0.155055  [105600/175341]\n",
      "loss: 1.056641  [107200/175341]\n",
      "loss: 0.385529  [108800/175341]\n",
      "loss: 0.679368  [110400/175341]\n",
      "loss: 0.888642  [112000/175341]\n",
      "loss: 0.704734  [113600/175341]\n",
      "loss: 0.320809  [115200/175341]\n",
      "loss: 0.546747  [116800/175341]\n",
      "loss: 0.435459  [118400/175341]\n",
      "loss: 0.551323  [120000/175341]\n",
      "loss: 0.370684  [121600/175341]\n",
      "loss: 0.533054  [123200/175341]\n",
      "loss: 0.372466  [124800/175341]\n",
      "loss: 0.709931  [126400/175341]\n",
      "loss: 0.444781  [128000/175341]\n",
      "loss: 0.197565  [129600/175341]\n",
      "loss: 0.101558  [131200/175341]\n",
      "loss: 0.203073  [132800/175341]\n",
      "loss: 0.490974  [134400/175341]\n",
      "loss: 0.285202  [136000/175341]\n",
      "loss: 0.384597  [137600/175341]\n",
      "loss: 0.633886  [139200/175341]\n",
      "loss: 0.591567  [140800/175341]\n",
      "loss: 0.461973  [142400/175341]\n",
      "loss: 0.627021  [144000/175341]\n",
      "loss: 0.770395  [145600/175341]\n",
      "loss: 0.458315  [147200/175341]\n",
      "loss: 0.430001  [148800/175341]\n",
      "loss: 0.402830  [150400/175341]\n",
      "loss: 0.660101  [152000/175341]\n",
      "loss: 0.696561  [153600/175341]\n",
      "loss: 0.386879  [155200/175341]\n",
      "loss: 0.399111  [156800/175341]\n",
      "loss: 0.908569  [158400/175341]\n",
      "loss: 0.490987  [160000/175341]\n",
      "loss: 0.371460  [161600/175341]\n",
      "loss: 0.387400  [163200/175341]\n",
      "loss: 0.452751  [164800/175341]\n",
      "loss: 0.395822  [166400/175341]\n",
      "loss: 0.483395  [168000/175341]\n",
      "loss: 0.580090  [169600/175341]\n",
      "loss: 0.701546  [171200/175341]\n",
      "loss: 1.472943  [172800/175341]\n",
      "loss: 0.750558  [174400/175341]\n",
      "Train Accuracy: 81.4561%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.543725, F1-score: 75.79%, Macro_F1-Score:  42.30%  \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.208872  [    0/175341]\n",
      "loss: 0.341643  [ 1600/175341]\n",
      "loss: 0.325621  [ 3200/175341]\n",
      "loss: 0.516312  [ 4800/175341]\n",
      "loss: 0.228953  [ 6400/175341]\n",
      "loss: 0.178196  [ 8000/175341]\n",
      "loss: 0.393141  [ 9600/175341]\n",
      "loss: 0.573903  [11200/175341]\n",
      "loss: 0.320788  [12800/175341]\n",
      "loss: 0.383259  [14400/175341]\n",
      "loss: 0.334749  [16000/175341]\n",
      "loss: 0.733701  [17600/175341]\n",
      "loss: 0.431342  [19200/175341]\n",
      "loss: 0.420683  [20800/175341]\n",
      "loss: 0.605956  [22400/175341]\n",
      "loss: 0.459286  [24000/175341]\n",
      "loss: 0.682360  [25600/175341]\n",
      "loss: 0.300569  [27200/175341]\n",
      "loss: 0.434050  [28800/175341]\n",
      "loss: 0.378086  [30400/175341]\n",
      "loss: 0.436233  [32000/175341]\n",
      "loss: 1.069273  [33600/175341]\n",
      "loss: 0.730450  [35200/175341]\n",
      "loss: 0.275130  [36800/175341]\n",
      "loss: 0.909013  [38400/175341]\n",
      "loss: 0.924564  [40000/175341]\n",
      "loss: 0.500669  [41600/175341]\n",
      "loss: 0.630703  [43200/175341]\n",
      "loss: 0.338125  [44800/175341]\n",
      "loss: 0.302528  [46400/175341]\n",
      "loss: 0.425477  [48000/175341]\n",
      "loss: 0.202214  [49600/175341]\n",
      "loss: 0.297776  [51200/175341]\n",
      "loss: 0.481720  [52800/175341]\n",
      "loss: 0.398011  [54400/175341]\n",
      "loss: 0.570748  [56000/175341]\n",
      "loss: 0.362541  [57600/175341]\n",
      "loss: 0.322569  [59200/175341]\n",
      "loss: 0.166862  [60800/175341]\n",
      "loss: 0.574488  [62400/175341]\n",
      "loss: 0.261461  [64000/175341]\n",
      "loss: 0.641072  [65600/175341]\n",
      "loss: 0.754360  [67200/175341]\n",
      "loss: 0.396174  [68800/175341]\n",
      "loss: 0.242554  [70400/175341]\n",
      "loss: 0.274254  [72000/175341]\n",
      "loss: 0.437566  [73600/175341]\n",
      "loss: 0.398282  [75200/175341]\n",
      "loss: 0.443897  [76800/175341]\n",
      "loss: 0.450822  [78400/175341]\n",
      "loss: 0.433094  [80000/175341]\n",
      "loss: 0.339785  [81600/175341]\n",
      "loss: 0.410296  [83200/175341]\n",
      "loss: 0.407429  [84800/175341]\n",
      "loss: 0.480960  [86400/175341]\n",
      "loss: 0.471618  [88000/175341]\n",
      "loss: 0.725071  [89600/175341]\n",
      "loss: 0.569189  [91200/175341]\n",
      "loss: 0.565595  [92800/175341]\n",
      "loss: 0.399701  [94400/175341]\n",
      "loss: 0.417063  [96000/175341]\n",
      "loss: 0.218039  [97600/175341]\n",
      "loss: 0.552592  [99200/175341]\n",
      "loss: 0.925111  [100800/175341]\n",
      "loss: 0.377848  [102400/175341]\n",
      "loss: 0.966366  [104000/175341]\n",
      "loss: 0.503348  [105600/175341]\n",
      "loss: 0.237183  [107200/175341]\n",
      "loss: 0.684279  [108800/175341]\n",
      "loss: 1.143193  [110400/175341]\n",
      "loss: 0.448672  [112000/175341]\n",
      "loss: 0.413025  [113600/175341]\n",
      "loss: 0.488898  [115200/175341]\n",
      "loss: 0.196061  [116800/175341]\n",
      "loss: 0.138288  [118400/175341]\n",
      "loss: 0.626814  [120000/175341]\n",
      "loss: 0.152289  [121600/175341]\n",
      "loss: 0.904158  [123200/175341]\n",
      "loss: 0.465720  [124800/175341]\n",
      "loss: 0.667150  [126400/175341]\n",
      "loss: 0.301226  [128000/175341]\n",
      "loss: 0.489786  [129600/175341]\n",
      "loss: 0.455836  [131200/175341]\n",
      "loss: 0.380078  [132800/175341]\n",
      "loss: 0.560723  [134400/175341]\n",
      "loss: 0.839846  [136000/175341]\n",
      "loss: 0.774664  [137600/175341]\n",
      "loss: 0.669341  [139200/175341]\n",
      "loss: 0.223780  [140800/175341]\n",
      "loss: 0.081801  [142400/175341]\n",
      "loss: 0.897202  [144000/175341]\n",
      "loss: 0.882051  [145600/175341]\n",
      "loss: 0.458878  [147200/175341]\n",
      "loss: 0.438538  [148800/175341]\n",
      "loss: 0.427316  [150400/175341]\n",
      "loss: 0.427674  [152000/175341]\n",
      "loss: 0.484355  [153600/175341]\n",
      "loss: 0.269124  [155200/175341]\n",
      "loss: 0.561265  [156800/175341]\n",
      "loss: 0.250284  [158400/175341]\n",
      "loss: 0.439694  [160000/175341]\n",
      "loss: 0.684599  [161600/175341]\n",
      "loss: 0.696051  [163200/175341]\n",
      "loss: 1.167308  [164800/175341]\n",
      "loss: 0.292811  [166400/175341]\n",
      "loss: 0.306003  [168000/175341]\n",
      "loss: 0.319779  [169600/175341]\n",
      "loss: 0.699744  [171200/175341]\n",
      "loss: 0.366379  [172800/175341]\n",
      "loss: 0.322004  [174400/175341]\n",
      "Train Accuracy: 81.4299%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.531634, F1-score: 76.46%, Macro_F1-Score:  42.55%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4) #using L2 regularization\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add252-99f2-4d32-bb41-6e558870ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noise scale is low (0.2)\n",
    "plt.plot(loss_over_train, label=\"Training Loss\")\n",
    "plt.plot(loss_over_test, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_data_y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n\u001b[0;32m      3\u001b[0m label_binarizer \u001b[38;5;241m=\u001b[39m LabelBinarizer()\u001b[38;5;241m.\u001b[39mfit(train_data_y)\n\u001b[1;32m----> 4\u001b[0m y_onehot_test \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_binarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_onehot_test\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# (n_samples, n_classes)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:363\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_is_multilabel \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_type_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe object was not fitted with multilabel input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlabel_binarize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:530\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    528\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m y_type:\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultioutput target data is not supported with label binarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of target data is not known\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b54eef",
   "metadata": {},
   "source": [
    "## Feature Selection using ANOVA F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the top 40 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=40)\n",
    "X_selected = selector.fit_transform(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = train_data_X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
