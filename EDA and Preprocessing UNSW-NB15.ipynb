{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Thesis Space Desktop\\\\UNSW_NB15_testing-set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mADMIN\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mThesis Space Desktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUNSW_NB15_testing-set.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mADMIN\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThesis Space Desktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUNSW_NB15_training-set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Thesis Space Desktop\\\\UNSW_NB15_testing-set.csv'"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_testing-set.csv\")\n",
    "train_data = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_training-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4fc6f-5f2b-4a23-940a-c67baa5aa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[\"proto\"].unique()), len(test_data[\"proto\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed6f0e-91c3-495f-af48-05bd68d96ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = train_data.select_dtypes(include=\"number\").drop(columns=\"id\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117f765-2b1f-4a7d-be31-0fd7a920609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfa7a1-44a4-40cf-bef1-02c4bbd9265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = [\n",
    "    col for col in columns_numeric\n",
    "    if not len(train_data[col].value_counts()) < 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb8512-1c45-445b-8697-96927ba3a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca2383-4965-47ae-86d6-b03bfc13fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba62952-362d-4233-b35c-2052726b2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac6c23-b9e6-4b62-94c3-e9b7259e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 120)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09aa3d-fd42-4598-bb3e-3e2a20e3dc56",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b710-6197-484a-93e8-abd2a6587929",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb095de-80f2-4667-9666-d3dbb6d83b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f296d-cacd-463c-a461-1db6ad2f8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimate_contamination_iqr(data, columns_numeric):\n",
    "    \"\"\"\n",
    "    Estimate a reasonable contamination value based on IQR.\n",
    "    \"\"\"\n",
    "    contamination_rates = []\n",
    "    for col in columns_numeric:\n",
    "        q1 = np.percentile(data[col], 25)\n",
    "        q3 = np.percentile(data[col], 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Count outliers\n",
    "        num_outliers = np.sum((data[col] < lower_bound) | (data[col] > upper_bound))\n",
    "        contamination_rates.append(num_outliers / len(data))\n",
    "\n",
    "    return np.mean(contamination_rates)  # Average contamination rate across all numeric columns\n",
    "\n",
    "# Example usage\n",
    "estimated_contamination = estimate_contamination_iqr(train_data, columns_numeric)\n",
    "print(f\"Estimated contamination: {estimated_contamination:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197decca-5200-4368-95fc-73728f966457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class OutlierIsolationForest:\n",
    "    def __init__(self, contamination=0.05, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Isolation Forest for outlier detection.\n",
    "\n",
    "        Parameters:\n",
    "        - contamination (float): The proportion of data points to be considered as outliers.\n",
    "        - random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.iso_forest = None\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Fit the Isolation Forest model on the numeric features to detect outliers.\n",
    "\n",
    "        Parameters:\n",
    "        - dataframe (pd.DataFrame): The dataset containing numeric features.\n",
    "        - columns_numeric (list): List of numeric columns to process.\n",
    "        \"\"\"\n",
    "        print(\"Fitting Isolation Forest for outlier detection...\")\n",
    "\n",
    "        # Ensure numeric columns are float type\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Initialize and fit Isolation Forest\n",
    "        self.iso_forest = IsolationForest(contamination=self.contamination, random_state=self.random_state)\n",
    "        self.iso_forest.fit(dataframe[columns_numeric])\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply Isolation Forest to remove detected outliers.\n",
    "\n",
    "        Parameters:\n",
    "        - dataframe (pd.DataFrame): The dataset to process.\n",
    "        - columns_numeric (list): List of numeric columns to process.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: The cleaned dataset without outliers.\n",
    "        \"\"\"\n",
    "        print(\"Applying Isolation Forest for outlier removal...\")\n",
    "\n",
    "        # Ensure numeric columns are float type\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Predict outliers (-1 indicates an outlier, 1 indicates normal data)\n",
    "        outlier_predictions = self.iso_forest.predict(dataframe[columns_numeric])\n",
    "        \n",
    "        # Keep only non-outliers\n",
    "        dataframe_cleaned = dataframe[outlier_predictions == 1].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Outliers removed: {len(dataframe) - len(dataframe_cleaned)}\")\n",
    "        return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab1353-df57-4f09-810d-7ecec2972963",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest_outlier = OutlierIsolationForest(contamination=0.1)  # Adjust contamination as needed\n",
    "iso_forest_outlier.fit(train_data, columns_numeric)\n",
    "train_data = iso_forest_outlier.transform(train_data, columns_numeric)\n",
    "test_data = iso_forest_outlier.transform(test_data, columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6b2824-0782-4dfc-8f18-fa011ce1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "class OutlierKNN:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierTreatment class, which stores IQR bounds.\n",
    "        The outliers will be imputed after scaling and normalization using KNNImputer.\n",
    "        \"\"\"\n",
    "        self.iqr_bounds = {}  # Store IQR bounds for each column\n",
    "        self.scaler = None  # Store StandardScaler\n",
    "        self.minmax_scaler = None  # Store MinMaxScaler\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=n_neighbors)  # KNN Imputer\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Compute IQR bounds and fit scalers and imputer on training data only.\n",
    "        \"\"\"\n",
    "        print(\"Fitting outlier treatment...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Compute IQR bounds\n",
    "        for col in columns_numeric:\n",
    "            q1 = dataframe[col].quantile(0.25)\n",
    "            q3 = dataframe[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.iqr_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        # Replace outliers with NaN to be imputed\n",
    "        for col in columns_numeric:\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Fit StandardScaler and MinMaxScaler\n",
    "        self.scaler = StandardScaler()\n",
    "        self.minmax_scaler = MinMaxScaler()\n",
    "\n",
    "        dataframe_scaled = self.scaler.fit_transform(dataframe[columns_numeric])\n",
    "        dataframe_scaled = self.minmax_scaler.fit_transform(dataframe_scaled)\n",
    "\n",
    "        # Fit KNN Imputer on scaled data\n",
    "        self.knn_imputer.fit(dataframe_scaled)\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply outlier treatment using precomputed IQR bounds, scaling, normalization, and KNN imputation.\n",
    "        \"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Replace outliers with NaN to be imputed\n",
    "        for col in columns_numeric:\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Apply scaling and normalization\n",
    "        dataframe_scaled = self.scaler.transform(dataframe[columns_numeric])\n",
    "        dataframe_scaled = self.minmax_scaler.transform(dataframe_scaled)\n",
    "\n",
    "        # Apply KNN Imputation\n",
    "        dataframe_imputed = self.knn_imputer.transform(dataframe_scaled)\n",
    "\n",
    "        # Reverse scaling to restore original range\n",
    "        dataframe_restored = self.scaler.inverse_transform(self.minmax_scaler.inverse_transform(dataframe_imputed))\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        dataframe[columns_numeric] = dataframe_restored\n",
    "\n",
    "        print(\"Transformation completed.\")\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30af3c68-bba4-486e-b958-d9e39c9b2204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m KNNOutlierApplier \u001b[38;5;241m=\u001b[39m OutlierKNN(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m KNNOutlierApplier\u001b[38;5;241m.\u001b[39mfit(train_data,columns_numeric)\n\u001b[1;32m----> 5\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mKNNOutlierApplier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m test_data \u001b[38;5;241m=\u001b[39m KNNOutlierApplier\u001b[38;5;241m.\u001b[39mtransform(test_data, columns_numeric)\n",
      "Cell \u001b[1;32mIn[14], line 71\u001b[0m, in \u001b[0;36mOutlierKNN.transform\u001b[1;34m(self, dataframe, columns_numeric)\u001b[0m\n\u001b[0;32m     68\u001b[0m dataframe_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminmax_scaler\u001b[38;5;241m.\u001b[39mtransform(dataframe_scaled)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Apply KNN Imputation\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m dataframe_imputed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn_imputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Reverse scaling to restore original range\u001b[39;00m\n\u001b[0;32m     74\u001b[0m dataframe_restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminmax_scaler\u001b[38;5;241m.\u001b[39minverse_transform(dataframe_imputed))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_knn.py:376\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[0;32m    368\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[0;32m    369\u001b[0m     X[row_missing_idx, :],\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[0;32m    375\u001b[0m )\n\u001b[1;32m--> 376\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# process_chunk modifies X in place. No return value.\u001b[39;49;00m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2252\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2251\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[1;32m-> 2252\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   2254\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   2256\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2480\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2477\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   2478\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1973\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1970\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1976\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:552\u001b[0m, in \u001b[0;36mnan_euclidean_distances\u001b[1;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[0;32m    549\u001b[0m X[missing_X] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    550\u001b[0m Y[missing_Y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 552\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# Adjust distances for missing values\u001b[39;00m\n\u001b[0;32m    555\u001b[0m XX \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m*\u001b[39m X\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:388\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m         )\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:428\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    425\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[0;32m    426\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n\u001b[1;32m--> 428\u001b[0m xp_zero \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m distances \u001b[38;5;241m=\u001b[39m _modify_in_place_if_numpy(\n\u001b[0;32m    430\u001b[0m     xp, xp\u001b[38;5;241m.\u001b[39mmaximum, distances, xp_zero, out\u001b[38;5;241m=\u001b[39mdistances\n\u001b[0;32m    431\u001b[0m )\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# This may not be the case due to floating point rounding errors.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:401\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.asarray\u001b[1;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, dtype, \u001b[38;5;241m*\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# astype is not defined in the top level NumPy namespace\u001b[39;00m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[1;32m--> 401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21masarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     _check_device_cpu(device)\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Support copy in NumPy namespace\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#KNNOutlierApplier = OutlierKNN(n_neighbors=3)\n",
    "\n",
    "#KNNOutlierApplier.fit(train_data,columns_numeric)\n",
    "\n",
    "#train_data = KNNOutlierApplier.transform(train_data,columns_numeric)\n",
    "#test_data = KNNOutlierApplier.transform(test_data, columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141f1c2-3e0e-4494-b4c5-5bff0c88c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class specific = PROBLEMATIC\n",
    "\n",
    "def treat_outliers(dataframe, columns_numeric, isTest=False):\n",
    "    \"\"\"\n",
    "    Treats outliers in the dataset for the specified numeric columns.\n",
    "\n",
    "    On the training set:\n",
    "        - Identifies outliers using the IQR method for each column grouped by \"attack_cat\".\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "        - Stores the IQR bounds for each column and class.\n",
    "\n",
    "    On the test set:\n",
    "        - Uses the stored IQR bounds from training for each column and class.\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The dataset to process.\n",
    "        columns_numeric (list): List of numeric columns to focus on for outlier treatment.\n",
    "        isTest (bool): Whether the function is applied to the test set.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset with outliers treated.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(treat_outliers, \"iqr_bounds\"):\n",
    "        treat_outliers.iqr_bounds = {}  # Store IQR bounds for training (keyed by column and class)\n",
    "\n",
    "    if isTest and not treat_outliers.iqr_bounds:\n",
    "        raise ValueError(\"IQR bounds are not set. Train the function on the training data first.\")\n",
    "\n",
    "    print(\"Treating outliers...\")\n",
    "    \n",
    "    # Ensure all numeric columns are cast to float to handle outliers consistently\n",
    "    dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "    # Process each numeric column\n",
    "    for col in columns_numeric:\n",
    "        if not isTest:\n",
    "            # Initialize dictionary for column if not already present\n",
    "            treat_outliers.iqr_bounds[col] = {}\n",
    "\n",
    "            # Group by \"attack_cat\" to calculate class-specific IQR bounds\n",
    "            for attack_cat, group in dataframe.groupby(\"attack_cat\"):\n",
    "                q1 = group[col].quantile(0.25)\n",
    "                q3 = group[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                treat_outliers.iqr_bounds[col][attack_cat] = (lower_bound, upper_bound)\n",
    "\n",
    "                # Identify outliers for this class\n",
    "                outliers = (group[col] < lower_bound) | (group[col] > upper_bound)\n",
    "\n",
    "                # Replace outliers with the class mean\n",
    "                replacement_mean = group[col].mean()\n",
    "                dataframe.loc[group[outliers].index, col] = replacement_mean\n",
    "        else:\n",
    "            # Use stored IQR bounds for test set\n",
    "            for attack_cat, bounds in treat_outliers.iqr_bounds[col].items():\n",
    "                lower_bound, upper_bound = bounds\n",
    "        \n",
    "                # Get test data rows for this attack_cat\n",
    "                class_rows = dataframe[dataframe[\"attack_cat\"] == attack_cat]\n",
    "        \n",
    "                # Identify outliers\n",
    "                outliers = (class_rows[col] < lower_bound) | (class_rows[col] > upper_bound)\n",
    "        \n",
    "                # Replace outliers with the training class mean\n",
    "                replacement_mean = dataframe[dataframe[\"attack_cat\"] == attack_cat][col].mean()\n",
    "                dataframe.loc[class_rows[outliers].index, col] = replacement_mean\n",
    "\n",
    "\n",
    "    print(\"Outlier treatment completed.\")\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8a262-a511-4715-bce5-8f2f7a8a16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class OutlierTreatment:\n",
    "    def __init__(self, strategy=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierTreatment class, which stores IQR bounds and global means/medians from training.\n",
    "        \"\"\"\n",
    "        self.iqr_bounds = {}  # Store IQR bounds for each column\n",
    "        self.global_imputer = None  # Store imputer for handling outliers\n",
    "\n",
    "        if strategy not in [\"mean\", \"median\"]:\n",
    "            raise ValueError(\"Invalid strategy. Use 'mean' or 'median'.\")\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Compute IQR bounds and global statistics (mean/median) from training data only.\n",
    "        \"\"\"\n",
    "        print(\"Fitting outlier treatment...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        self.global_imputer = SimpleImputer(strategy=self.strategy)\n",
    "        self.global_imputer.fit(dataframe[columns_numeric])  # Fit global imputer\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            q1 = dataframe[col].quantile(0.25)\n",
    "            q3 = dataframe[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.iqr_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply outlier treatment using precomputed IQR bounds and global imputation.\n",
    "        \"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            if col not in self.iqr_bounds:\n",
    "                raise ValueError(f\"IQR bounds for column '{col}' are not set. Ensure you fit on training data first.\")\n",
    "\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "\n",
    "            # Replace outliers with NaN to be imputed\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Apply global imputation\n",
    "        dataframe[columns_numeric] = self.global_imputer.transform(dataframe[columns_numeric])\n",
    "\n",
    "        print(\"Transformation completed.\")\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67678e7e-d83c-4d5a-91b1-63da92b1f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = treat_outliers(train_data, columns_numeric)\n",
    "#test_data = treat_outliers(test_data, columns_numeric, isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba3182-5181-4560-a21d-983aab6abf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlierhandler = OutlierTreatment(strategy = \"median\")\n",
    "#outlierhandler.fit(train_data,columns_numeric)\n",
    "#train_data = outlierhandler.transform(train_data,columns_numeric)\n",
    "#test_data = outlierhandler.transform(test_data,columns_numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba1d4f-2e0e-4a35-aa77-2c5eea7171cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcab144-8fd9-4407-b3ea-51359b1cc28b",
   "metadata": {},
   "source": [
    "A lot more quantiles are displayed now !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab53e0-79a2-453a-9792-1b758967e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "train_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23f0c7-76e5-4eac-982b-ac547a0a9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce categories in categorical columns while preserving the target column\n",
    "def reduce_categories(train_data, test_data, target_column, threshold=6, debug=False):\n",
    "    # Select categorical columns in train and test data, excluding the target column\n",
    "    train_cat = train_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    test_cat = test_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    \n",
    "    # Iterate through categorical columns\n",
    "    for feature in train_cat.columns:\n",
    "        if debug:\n",
    "            print(f\"Processing feature: {feature}\")\n",
    "            print(f\"Number of unique categories before reduction: {train_cat[feature].nunique()}\")\n",
    "            print('----------------------------------------------------')\n",
    "        \n",
    "        # Check if the number of unique categories exceeds the threshold\n",
    "        if train_cat[feature].nunique() > threshold:\n",
    "            # Identify the top categories in the train set\n",
    "            top_categories = train_data[feature].value_counts().head(threshold).index\n",
    "            \n",
    "            # Reduce train and test data to these top categories, replacing others with '-'\n",
    "            train_data[feature] = np.where(train_data[feature].isin(top_categories), train_data[feature], '-')\n",
    "            test_data[feature] = np.where(test_data[feature].isin(top_categories), test_data[feature], '-')\n",
    "    \n",
    "    # Return the modified train and test datasets\n",
    "    return train_data, test_data\n",
    "\n",
    "# Apply the function to your train and test datasets\n",
    "train_data, test_data = reduce_categories(train_data, test_data, target_column='attack_cat', threshold=6, debug=False)\n",
    "\n",
    "# Check the resulting categorical columns\n",
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "test_cat = test_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "print(\"Train categorical summary after reduction:\")\n",
    "print(train_cat.describe(include='all'))\n",
    "\n",
    "print(\"\\nTest categorical summary after reduction:\")\n",
    "print(test_cat.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9250b9-da54-49ea-823c-70d2c65101bc",
   "metadata": {},
   "source": [
    "## Normalizing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb003f-055e-441c-9e29-69e0880e29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc7eb1-09f6-4301-a811-adcf13a479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train_data.select_dtypes(include=\"number\").columns\n",
    "\n",
    "normalizer = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "r_scaler = RobustScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "train_data[numeric] = r_scaler.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = r_scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = scaler.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = normalizer.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = normalizer.transform(test_data[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f57f7-4b70-4b50-b7cb-3af10c44bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237a713-240f-4f46-82d4-0c55bdf1d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[\"state\"].unique()), len(test_data[\"state\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f098c9-1990-49e0-9d8a-1047acdeb567",
   "metadata": {},
   "source": [
    "## Treating Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3c56b-3f26-4db7-9fcf-1421a44e91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set skewness threshold (e.g., |skewness| > 1 is considered highly skewed)\n",
    "skew_threshold = 1\n",
    "\n",
    "numeric_cols = train_data.select_dtypes(include = \"number\").columns\n",
    "\n",
    "# Select numeric columns\n",
    "# Calculate skewness for numeric columns\n",
    "skewness = train_data[numeric_cols].skew()\n",
    "\n",
    "# Identify columns with skewness above the threshold\n",
    "skewed_cols = skewness[skewness.abs() > skew_threshold].index\n",
    "print(\"Skewed columns:\", skewed_cols)\n",
    "\n",
    "# Apply log transformation to skewed columns\n",
    "for col in skewed_cols:\n",
    "    # Add 1 to avoid issues with log(0)\n",
    "    train_data[col] = np.log1p(train_data[col])\n",
    "    test_data[col] = np.log1p(test_data[col])  # Apply same transformation to test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f467fa9-3cd3-4393-8b49-165534f19504",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdc915-e753-4a4a-9f19-977ce4c395c9",
   "metadata": {},
   "source": [
    "## OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09567e29-cacc-4d54-abf8-474f88fcedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "train_data_X = train_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "train_data_y = train_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "test_data_X = test_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "test_data_y = test_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = train_data_X.select_dtypes(include=\"object\").columns\n",
    "print(categorical_cols)  # Verify the categorical columns\n",
    "\n",
    "# Use OneHotEncoder with handle_unknown='ignore' to ensure consistency\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define and fit the encoder on training data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', drop='first')  # Drop first category to avoid dummy variable trap\n",
    "ct = ColumnTransformer(transformers=[('encoder', encoder, categorical_cols)], remainder='passthrough')\n",
    "\n",
    "# Fit the encoder on train data\n",
    "train_data_X_encoded = ct.fit_transform(train_data_X)\n",
    "\n",
    "# Transform the test data using the same encoder\n",
    "test_data_X_encoded = ct.transform(test_data_X)\n",
    "\n",
    "# Convert the encoded data back to DataFrame for alignment\n",
    "import pandas as pd\n",
    "train_data_X_encoded = pd.DataFrame(train_data_X_encoded, columns=ct.get_feature_names_out(), index=train_data_X.index)\n",
    "test_data_X_encoded = pd.DataFrame(test_data_X_encoded, columns=ct.get_feature_names_out(), index=test_data_X.index)\n",
    "\n",
    "# Ensure consistent columns between train and test\n",
    "train_data_X, test_data_X = train_data_X_encoded.align(test_data_X_encoded, join=\"outer\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc749ff-5d8c-42fb-bd0f-8632dc76be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, column_name in enumerate(test_data_X.columns):\n",
    "    print(index, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Ensure you work with copies of the DataFrames\n",
    "train_data_X = train_data_X.copy()\n",
    "test_data_X = test_data_X.copy()\n",
    "\n",
    "# Filter only numeric columns for scaling\n",
    "numeric_cols_train = train_data_X.select_dtypes(include=['number']).columns\n",
    "numeric_cols_test = test_data_X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Ensure numeric columns are cast to float before scaling\n",
    "train_data_X[numeric_cols_train] = train_data_X[numeric_cols_train].astype(float)\n",
    "test_data_X[numeric_cols_test] = test_data_X[numeric_cols_test].astype(float)\n",
    "\n",
    "# Initialize scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply StandardScaler to train_data\n",
    "train_data_scaled = standard_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply StandardScaler to test_data\n",
    "test_data_scaled = standard_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to train_data\n",
    "train_data_scaled = minmax_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to test_data\n",
    "test_data_scaled = minmax_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51262960-8085-401f-a7da-44c9b682258d",
   "metadata": {},
   "source": [
    "### Converting data to required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8a649-8f38-4489-a686-caa97f61f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data_X = pd.DataFrame(scaler.fit_transform(train_data_X), columns=train_data_X.columns)\n",
    "test_data_X = pd.DataFrame(scaler.transform(test_data_X), columns=test_data_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eb3fd-f663-480d-b324-df1bdc58a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = pd.DataFrame(label_encoder.fit_transform(train_data_y[\"attack_cat\"]))\n",
    "test_labels_encoded = pd.DataFrame(label_encoder.transform(test_data_y[\"attack_cat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad226e-b843-4273-82ee-9809c5d04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\", index=False)\n",
    "test_data_X.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\", index=False)\n",
    "train_labels_encoded.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\", index=False)\n",
    "test_labels_encoded.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491aeaf5-7cdd-4e18-ae14-043b15453f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_class_labels = label_encoder.classes_\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Save the LabelEncoder object to a file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646596b7-a6c0-466d-a559-7509d1e1678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004cb90-c27d-4926-a5d7-6b11d3cb17db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142e740-6dc1-48c3-91e3-4d364d2140b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a9a89-a036-4913-b787-9add6c251cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f312a2-9d06-415e-914e-2e46866d13df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbee27-9515-46f4-96ac-8ebed45448f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236eb12-063b-44b0-b3b6-41a1bd0a1f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcaa09-f0dd-4e2b-bc95-ddd2e187c2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3dd1d-836a-41a6-9196-3b785874898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6399c-3fa9-4cc6-a063-962e37923dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b499f-b1e7-4f06-b751-13c93c96b378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47351b-e56f-4f9e-92cb-3e29e96458a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165a3d2-5dd3-47fb-99c0-b492dee8adbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7341c-73f8-4d54-9148-daf1212e8f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
