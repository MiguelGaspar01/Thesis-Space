{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"all\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "smote_enn = SMOTEENN(sampling_strategy=\"auto\", enn = enn, random_state=42)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_labels_encoded)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 12,6, 3,10],grid_size = 4, scale_noise=0.8, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303873  [    0/481450]\n",
      "loss: 2.302816  [ 3200/481450]\n",
      "loss: 2.297785  [ 6400/481450]\n",
      "loss: 2.296031  [ 9600/481450]\n",
      "loss: 2.288023  [12800/481450]\n",
      "loss: 2.277872  [16000/481450]\n",
      "loss: 2.235938  [19200/481450]\n",
      "loss: 2.178953  [22400/481450]\n",
      "loss: 2.070969  [25600/481450]\n",
      "loss: 2.067049  [28800/481450]\n",
      "loss: 2.037387  [32000/481450]\n",
      "loss: 1.985152  [35200/481450]\n",
      "loss: 1.862195  [38400/481450]\n",
      "loss: 1.924645  [41600/481450]\n",
      "loss: 1.855682  [44800/481450]\n",
      "loss: 1.714819  [48000/481450]\n",
      "loss: 2.010747  [51200/481450]\n",
      "loss: 2.028619  [54400/481450]\n",
      "loss: 1.706535  [57600/481450]\n",
      "loss: 1.761786  [60800/481450]\n",
      "loss: 1.650376  [64000/481450]\n",
      "loss: 1.897968  [67200/481450]\n",
      "loss: 1.528954  [70400/481450]\n",
      "loss: 1.595967  [73600/481450]\n",
      "loss: 1.435996  [76800/481450]\n",
      "loss: 1.479889  [80000/481450]\n",
      "loss: 1.480312  [83200/481450]\n",
      "loss: 1.585973  [86400/481450]\n",
      "loss: 1.508904  [89600/481450]\n",
      "loss: 1.325055  [92800/481450]\n",
      "loss: 1.312539  [96000/481450]\n",
      "loss: 1.546192  [99200/481450]\n",
      "loss: 1.231118  [102400/481450]\n",
      "loss: 1.474196  [105600/481450]\n",
      "loss: 1.494893  [108800/481450]\n",
      "loss: 1.316959  [112000/481450]\n",
      "loss: 1.681700  [115200/481450]\n",
      "loss: 1.529127  [118400/481450]\n",
      "loss: 1.268122  [121600/481450]\n",
      "loss: 1.506580  [124800/481450]\n",
      "loss: 1.656167  [128000/481450]\n",
      "loss: 1.308796  [131200/481450]\n",
      "loss: 1.398517  [134400/481450]\n",
      "loss: 1.100171  [137600/481450]\n",
      "loss: 1.232177  [140800/481450]\n",
      "loss: 1.238866  [144000/481450]\n",
      "loss: 1.457727  [147200/481450]\n",
      "loss: 1.261930  [150400/481450]\n",
      "loss: 1.221967  [153600/481450]\n",
      "loss: 1.230982  [156800/481450]\n",
      "loss: 1.278268  [160000/481450]\n",
      "loss: 1.132662  [163200/481450]\n",
      "loss: 1.073883  [166400/481450]\n",
      "loss: 1.171490  [169600/481450]\n",
      "loss: 1.105073  [172800/481450]\n",
      "loss: 1.059875  [176000/481450]\n",
      "loss: 1.145698  [179200/481450]\n",
      "loss: 1.036083  [182400/481450]\n",
      "loss: 1.110263  [185600/481450]\n",
      "loss: 1.335220  [188800/481450]\n",
      "loss: 1.021522  [192000/481450]\n",
      "loss: 1.016870  [195200/481450]\n",
      "loss: 1.008809  [198400/481450]\n",
      "loss: 1.150310  [201600/481450]\n",
      "loss: 1.143446  [204800/481450]\n",
      "loss: 1.131747  [208000/481450]\n",
      "loss: 1.188166  [211200/481450]\n",
      "loss: 1.147888  [214400/481450]\n",
      "loss: 0.800938  [217600/481450]\n",
      "loss: 1.025549  [220800/481450]\n",
      "loss: 0.814390  [224000/481450]\n",
      "loss: 1.295744  [227200/481450]\n",
      "loss: 0.929968  [230400/481450]\n",
      "loss: 1.140617  [233600/481450]\n",
      "loss: 0.779464  [236800/481450]\n",
      "loss: 0.847682  [240000/481450]\n",
      "loss: 0.903409  [243200/481450]\n",
      "loss: 0.573645  [246400/481450]\n",
      "loss: 1.029950  [249600/481450]\n",
      "loss: 0.762832  [252800/481450]\n",
      "loss: 1.033998  [256000/481450]\n",
      "loss: 0.933532  [259200/481450]\n",
      "loss: 0.856361  [262400/481450]\n",
      "loss: 0.628264  [265600/481450]\n",
      "loss: 0.747080  [268800/481450]\n",
      "loss: 0.887821  [272000/481450]\n",
      "loss: 0.649923  [275200/481450]\n",
      "loss: 0.793809  [278400/481450]\n",
      "loss: 0.866693  [281600/481450]\n",
      "loss: 1.026863  [284800/481450]\n",
      "loss: 0.807795  [288000/481450]\n",
      "loss: 1.123703  [291200/481450]\n",
      "loss: 0.950396  [294400/481450]\n",
      "loss: 0.741085  [297600/481450]\n",
      "loss: 0.646328  [300800/481450]\n",
      "loss: 0.784540  [304000/481450]\n",
      "loss: 0.541354  [307200/481450]\n",
      "loss: 0.612877  [310400/481450]\n",
      "loss: 0.565532  [313600/481450]\n",
      "loss: 0.617424  [316800/481450]\n",
      "loss: 0.787419  [320000/481450]\n",
      "loss: 0.870635  [323200/481450]\n",
      "loss: 0.830984  [326400/481450]\n",
      "loss: 0.681614  [329600/481450]\n",
      "loss: 0.819576  [332800/481450]\n",
      "loss: 0.603740  [336000/481450]\n",
      "loss: 0.707299  [339200/481450]\n",
      "loss: 0.743055  [342400/481450]\n",
      "loss: 0.496641  [345600/481450]\n",
      "loss: 0.693308  [348800/481450]\n",
      "loss: 0.721531  [352000/481450]\n",
      "loss: 0.748250  [355200/481450]\n",
      "loss: 0.760275  [358400/481450]\n",
      "loss: 0.885153  [361600/481450]\n",
      "loss: 0.698536  [364800/481450]\n",
      "loss: 0.876220  [368000/481450]\n",
      "loss: 0.858084  [371200/481450]\n",
      "loss: 0.808324  [374400/481450]\n",
      "loss: 0.904600  [377600/481450]\n",
      "loss: 0.775363  [380800/481450]\n",
      "loss: 0.883015  [384000/481450]\n",
      "loss: 0.774155  [387200/481450]\n",
      "loss: 0.617692  [390400/481450]\n",
      "loss: 0.812352  [393600/481450]\n",
      "loss: 0.673213  [396800/481450]\n",
      "loss: 0.502179  [400000/481450]\n",
      "loss: 0.866659  [403200/481450]\n",
      "loss: 0.557522  [406400/481450]\n",
      "loss: 0.773617  [409600/481450]\n",
      "loss: 0.447325  [412800/481450]\n",
      "loss: 0.627371  [416000/481450]\n",
      "loss: 0.613677  [419200/481450]\n",
      "loss: 0.651694  [422400/481450]\n",
      "loss: 0.775910  [425600/481450]\n",
      "loss: 0.704129  [428800/481450]\n",
      "loss: 0.584952  [432000/481450]\n",
      "loss: 0.811646  [435200/481450]\n",
      "loss: 0.604666  [438400/481450]\n",
      "loss: 0.665494  [441600/481450]\n",
      "loss: 0.692758  [444800/481450]\n",
      "loss: 0.663030  [448000/481450]\n",
      "loss: 0.675393  [451200/481450]\n",
      "loss: 0.521713  [454400/481450]\n",
      "loss: 0.919646  [457600/481450]\n",
      "loss: 0.462332  [460800/481450]\n",
      "loss: 0.669725  [464000/481450]\n",
      "loss: 0.462278  [467200/481450]\n",
      "loss: 0.510200  [470400/481450]\n",
      "loss: 0.603693  [473600/481450]\n",
      "loss: 0.602505  [476800/481450]\n",
      "loss: 0.633448  [480000/481450]\n",
      "Train Accuracy: 62.1811%\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.617605, F1-score: 82.11% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.564160  [    0/481450]\n",
      "loss: 0.501400  [ 3200/481450]\n",
      "loss: 0.543068  [ 6400/481450]\n",
      "loss: 0.620027  [ 9600/481450]\n",
      "loss: 0.698540  [12800/481450]\n",
      "loss: 0.714395  [16000/481450]\n",
      "loss: 0.762351  [19200/481450]\n",
      "loss: 0.770652  [22400/481450]\n",
      "loss: 0.686026  [25600/481450]\n",
      "loss: 0.585501  [28800/481450]\n",
      "loss: 0.582488  [32000/481450]\n",
      "loss: 0.546628  [35200/481450]\n",
      "loss: 0.473546  [38400/481450]\n",
      "loss: 0.642924  [41600/481450]\n",
      "loss: 0.662224  [44800/481450]\n",
      "loss: 0.523117  [48000/481450]\n",
      "loss: 0.543241  [51200/481450]\n",
      "loss: 0.692559  [54400/481450]\n",
      "loss: 0.590855  [57600/481450]\n",
      "loss: 0.717613  [60800/481450]\n",
      "loss: 0.558585  [64000/481450]\n",
      "loss: 0.403257  [67200/481450]\n",
      "loss: 0.484515  [70400/481450]\n",
      "loss: 0.623852  [73600/481450]\n",
      "loss: 0.473937  [76800/481450]\n",
      "loss: 0.498273  [80000/481450]\n",
      "loss: 0.477172  [83200/481450]\n",
      "loss: 0.644184  [86400/481450]\n",
      "loss: 0.858913  [89600/481450]\n",
      "loss: 0.441991  [92800/481450]\n",
      "loss: 0.474304  [96000/481450]\n",
      "loss: 0.552009  [99200/481450]\n",
      "loss: 0.531494  [102400/481450]\n",
      "loss: 0.294158  [105600/481450]\n",
      "loss: 0.587179  [108800/481450]\n",
      "loss: 0.492244  [112000/481450]\n",
      "loss: 0.524260  [115200/481450]\n",
      "loss: 0.571141  [118400/481450]\n",
      "loss: 0.471821  [121600/481450]\n",
      "loss: 0.663453  [124800/481450]\n",
      "loss: 0.344800  [128000/481450]\n",
      "loss: 0.558365  [131200/481450]\n",
      "loss: 0.520408  [134400/481450]\n",
      "loss: 0.370737  [137600/481450]\n",
      "loss: 0.654038  [140800/481450]\n",
      "loss: 0.225805  [144000/481450]\n",
      "loss: 0.370672  [147200/481450]\n",
      "loss: 0.579952  [150400/481450]\n",
      "loss: 0.452032  [153600/481450]\n",
      "loss: 0.580801  [156800/481450]\n",
      "loss: 0.421348  [160000/481450]\n",
      "loss: 0.531510  [163200/481450]\n",
      "loss: 0.336323  [166400/481450]\n",
      "loss: 0.417078  [169600/481450]\n",
      "loss: 0.439486  [172800/481450]\n",
      "loss: 0.587727  [176000/481450]\n",
      "loss: 0.377145  [179200/481450]\n",
      "loss: 0.482465  [182400/481450]\n",
      "loss: 0.540299  [185600/481450]\n",
      "loss: 0.504759  [188800/481450]\n",
      "loss: 0.939687  [192000/481450]\n",
      "loss: 0.559904  [195200/481450]\n",
      "loss: 0.559324  [198400/481450]\n",
      "loss: 0.390650  [201600/481450]\n",
      "loss: 0.319613  [204800/481450]\n",
      "loss: 0.631125  [208000/481450]\n",
      "loss: 0.379922  [211200/481450]\n",
      "loss: 0.470432  [214400/481450]\n",
      "loss: 0.697380  [217600/481450]\n",
      "loss: 0.483873  [220800/481450]\n",
      "loss: 0.570428  [224000/481450]\n",
      "loss: 0.592842  [227200/481450]\n",
      "loss: 0.565133  [230400/481450]\n",
      "loss: 0.581268  [233600/481450]\n",
      "loss: 0.156740  [236800/481450]\n",
      "loss: 0.362389  [240000/481450]\n",
      "loss: 0.357143  [243200/481450]\n",
      "loss: 0.393589  [246400/481450]\n",
      "loss: 0.357797  [249600/481450]\n",
      "loss: 0.356319  [252800/481450]\n",
      "loss: 0.244575  [256000/481450]\n",
      "loss: 0.485555  [259200/481450]\n",
      "loss: 0.455749  [262400/481450]\n",
      "loss: 0.457227  [265600/481450]\n",
      "loss: 0.510907  [268800/481450]\n",
      "loss: 0.229796  [272000/481450]\n",
      "loss: 0.489738  [275200/481450]\n",
      "loss: 0.303385  [278400/481450]\n",
      "loss: 0.392328  [281600/481450]\n",
      "loss: 0.248430  [284800/481450]\n",
      "loss: 0.467333  [288000/481450]\n",
      "loss: 0.451846  [291200/481450]\n",
      "loss: 0.576203  [294400/481450]\n",
      "loss: 0.394502  [297600/481450]\n",
      "loss: 0.769004  [300800/481450]\n",
      "loss: 0.369792  [304000/481450]\n",
      "loss: 0.746180  [307200/481450]\n",
      "loss: 0.335058  [310400/481450]\n",
      "loss: 0.449125  [313600/481450]\n",
      "loss: 0.815511  [316800/481450]\n",
      "loss: 0.551572  [320000/481450]\n",
      "loss: 0.429078  [323200/481450]\n",
      "loss: 0.736431  [326400/481450]\n",
      "loss: 0.655041  [329600/481450]\n",
      "loss: 0.373423  [332800/481450]\n",
      "loss: 0.451881  [336000/481450]\n",
      "loss: 0.397157  [339200/481450]\n",
      "loss: 0.660216  [342400/481450]\n",
      "loss: 0.684946  [345600/481450]\n",
      "loss: 0.512235  [348800/481450]\n",
      "loss: 0.567810  [352000/481450]\n",
      "loss: 0.331990  [355200/481450]\n",
      "loss: 0.448139  [358400/481450]\n",
      "loss: 0.420008  [361600/481450]\n",
      "loss: 0.514960  [364800/481450]\n",
      "loss: 0.441924  [368000/481450]\n",
      "loss: 0.464416  [371200/481450]\n",
      "loss: 0.427173  [374400/481450]\n",
      "loss: 0.623438  [377600/481450]\n",
      "loss: 0.435355  [380800/481450]\n",
      "loss: 0.484409  [384000/481450]\n",
      "loss: 0.569751  [387200/481450]\n",
      "loss: 0.320499  [390400/481450]\n",
      "loss: 0.434205  [393600/481450]\n",
      "loss: 0.728910  [396800/481450]\n",
      "loss: 0.744317  [400000/481450]\n",
      "loss: 0.367702  [403200/481450]\n",
      "loss: 0.476635  [406400/481450]\n",
      "loss: 0.426254  [409600/481450]\n",
      "loss: 0.484233  [412800/481450]\n",
      "loss: 0.488971  [416000/481450]\n",
      "loss: 0.365012  [419200/481450]\n",
      "loss: 0.657597  [422400/481450]\n",
      "loss: 0.596684  [425600/481450]\n",
      "loss: 0.397815  [428800/481450]\n",
      "loss: 0.605564  [432000/481450]\n",
      "loss: 0.267354  [435200/481450]\n",
      "loss: 0.412495  [438400/481450]\n",
      "loss: 0.700551  [441600/481450]\n",
      "loss: 0.489357  [444800/481450]\n",
      "loss: 0.309933  [448000/481450]\n",
      "loss: 0.428647  [451200/481450]\n",
      "loss: 0.367225  [454400/481450]\n",
      "loss: 0.482403  [457600/481450]\n",
      "loss: 0.373690  [460800/481450]\n",
      "loss: 0.343778  [464000/481450]\n",
      "loss: 0.651008  [467200/481450]\n",
      "loss: 0.670919  [470400/481450]\n",
      "loss: 0.468633  [473600/481450]\n",
      "loss: 0.459203  [476800/481450]\n",
      "loss: 0.657335  [480000/481450]\n",
      "Train Accuracy: 80.9648%\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.552664, F1-score: 82.36% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.405726  [    0/481450]\n",
      "loss: 0.325214  [ 3200/481450]\n",
      "loss: 0.503985  [ 6400/481450]\n",
      "loss: 0.497425  [ 9600/481450]\n",
      "loss: 0.228459  [12800/481450]\n",
      "loss: 0.659756  [16000/481450]\n",
      "loss: 0.289993  [19200/481450]\n",
      "loss: 0.407252  [22400/481450]\n",
      "loss: 0.278160  [25600/481450]\n",
      "loss: 0.332673  [28800/481450]\n",
      "loss: 0.396970  [32000/481450]\n",
      "loss: 0.312453  [35200/481450]\n",
      "loss: 0.379049  [38400/481450]\n",
      "loss: 0.423386  [41600/481450]\n",
      "loss: 0.287655  [44800/481450]\n",
      "loss: 0.642869  [48000/481450]\n",
      "loss: 0.794826  [51200/481450]\n",
      "loss: 0.284913  [54400/481450]\n",
      "loss: 0.453777  [57600/481450]\n",
      "loss: 0.483607  [60800/481450]\n",
      "loss: 0.325212  [64000/481450]\n",
      "loss: 0.486196  [67200/481450]\n",
      "loss: 0.420256  [70400/481450]\n",
      "loss: 0.287554  [73600/481450]\n",
      "loss: 0.458415  [76800/481450]\n",
      "loss: 0.445502  [80000/481450]\n",
      "loss: 0.416336  [83200/481450]\n",
      "loss: 0.353295  [86400/481450]\n",
      "loss: 0.318656  [89600/481450]\n",
      "loss: 0.659878  [92800/481450]\n",
      "loss: 0.290639  [96000/481450]\n",
      "loss: 0.458490  [99200/481450]\n",
      "loss: 0.453602  [102400/481450]\n",
      "loss: 0.271794  [105600/481450]\n",
      "loss: 0.335813  [108800/481450]\n",
      "loss: 0.173087  [112000/481450]\n",
      "loss: 0.414376  [115200/481450]\n",
      "loss: 0.340914  [118400/481450]\n",
      "loss: 0.398083  [121600/481450]\n",
      "loss: 0.486441  [124800/481450]\n",
      "loss: 0.262067  [128000/481450]\n",
      "loss: 0.413472  [131200/481450]\n",
      "loss: 0.318532  [134400/481450]\n",
      "loss: 0.791157  [137600/481450]\n",
      "loss: 0.468370  [140800/481450]\n",
      "loss: 0.374505  [144000/481450]\n",
      "loss: 0.422454  [147200/481450]\n",
      "loss: 0.491600  [150400/481450]\n",
      "loss: 0.380875  [153600/481450]\n",
      "loss: 0.460730  [156800/481450]\n",
      "loss: 0.405906  [160000/481450]\n",
      "loss: 0.368868  [163200/481450]\n",
      "loss: 0.503606  [166400/481450]\n",
      "loss: 0.432976  [169600/481450]\n",
      "loss: 0.537621  [172800/481450]\n",
      "loss: 0.554145  [176000/481450]\n",
      "loss: 0.467484  [179200/481450]\n",
      "loss: 0.434592  [182400/481450]\n",
      "loss: 0.282261  [185600/481450]\n",
      "loss: 0.173953  [188800/481450]\n",
      "loss: 0.575742  [192000/481450]\n",
      "loss: 0.247651  [195200/481450]\n",
      "loss: 0.369191  [198400/481450]\n",
      "loss: 0.385346  [201600/481450]\n",
      "loss: 0.428571  [204800/481450]\n",
      "loss: 0.376220  [208000/481450]\n",
      "loss: 0.415404  [211200/481450]\n",
      "loss: 0.467076  [214400/481450]\n",
      "loss: 0.576974  [217600/481450]\n",
      "loss: 0.307704  [220800/481450]\n",
      "loss: 0.375133  [224000/481450]\n",
      "loss: 0.421875  [227200/481450]\n",
      "loss: 0.397084  [230400/481450]\n",
      "loss: 0.259349  [233600/481450]\n",
      "loss: 0.334620  [236800/481450]\n",
      "loss: 0.430742  [240000/481450]\n",
      "loss: 0.287184  [243200/481450]\n",
      "loss: 0.555221  [246400/481450]\n",
      "loss: 0.167711  [249600/481450]\n",
      "loss: 0.501434  [252800/481450]\n",
      "loss: 0.464254  [256000/481450]\n",
      "loss: 0.330087  [259200/481450]\n",
      "loss: 0.404158  [262400/481450]\n",
      "loss: 0.391532  [265600/481450]\n",
      "loss: 0.325307  [268800/481450]\n",
      "loss: 0.657714  [272000/481450]\n",
      "loss: 0.423447  [275200/481450]\n",
      "loss: 0.281330  [278400/481450]\n",
      "loss: 0.555872  [281600/481450]\n",
      "loss: 0.242222  [284800/481450]\n",
      "loss: 0.358640  [288000/481450]\n",
      "loss: 0.322149  [291200/481450]\n",
      "loss: 0.313594  [294400/481450]\n",
      "loss: 0.585517  [297600/481450]\n",
      "loss: 0.274007  [300800/481450]\n",
      "loss: 0.504407  [304000/481450]\n",
      "loss: 0.641743  [307200/481450]\n",
      "loss: 0.299442  [310400/481450]\n",
      "loss: 0.426545  [313600/481450]\n",
      "loss: 0.490388  [316800/481450]\n",
      "loss: 0.865823  [320000/481450]\n",
      "loss: 0.234797  [323200/481450]\n",
      "loss: 0.353660  [326400/481450]\n",
      "loss: 0.279889  [329600/481450]\n",
      "loss: 0.666394  [332800/481450]\n",
      "loss: 0.477233  [336000/481450]\n",
      "loss: 0.465080  [339200/481450]\n",
      "loss: 0.505927  [342400/481450]\n",
      "loss: 0.408563  [345600/481450]\n",
      "loss: 0.452241  [348800/481450]\n",
      "loss: 0.387817  [352000/481450]\n",
      "loss: 0.259747  [355200/481450]\n",
      "loss: 0.252581  [358400/481450]\n",
      "loss: 0.652287  [361600/481450]\n",
      "loss: 0.427796  [364800/481450]\n",
      "loss: 0.537170  [368000/481450]\n",
      "loss: 0.411418  [371200/481450]\n",
      "loss: 0.416742  [374400/481450]\n",
      "loss: 0.438060  [377600/481450]\n",
      "loss: 0.238618  [380800/481450]\n",
      "loss: 0.327507  [384000/481450]\n",
      "loss: 0.600937  [387200/481450]\n",
      "loss: 0.378873  [390400/481450]\n",
      "loss: 0.203687  [393600/481450]\n",
      "loss: 0.270985  [396800/481450]\n",
      "loss: 0.350158  [400000/481450]\n",
      "loss: 0.795691  [403200/481450]\n",
      "loss: 0.449272  [406400/481450]\n",
      "loss: 0.396139  [409600/481450]\n",
      "loss: 0.544792  [412800/481450]\n",
      "loss: 0.268424  [416000/481450]\n",
      "loss: 0.269682  [419200/481450]\n",
      "loss: 0.318136  [422400/481450]\n",
      "loss: 0.288944  [425600/481450]\n",
      "loss: 0.257570  [428800/481450]\n",
      "loss: 0.346423  [432000/481450]\n",
      "loss: 0.436898  [435200/481450]\n",
      "loss: 0.429152  [438400/481450]\n",
      "loss: 0.371790  [441600/481450]\n",
      "loss: 0.498210  [444800/481450]\n",
      "loss: 0.412438  [448000/481450]\n",
      "loss: 0.381998  [451200/481450]\n",
      "loss: 0.450167  [454400/481450]\n",
      "loss: 0.498056  [457600/481450]\n",
      "loss: 0.477489  [460800/481450]\n",
      "loss: 0.355349  [464000/481450]\n",
      "loss: 0.326290  [467200/481450]\n",
      "loss: 0.398600  [470400/481450]\n",
      "loss: 0.465258  [473600/481450]\n",
      "loss: 0.729446  [476800/481450]\n",
      "loss: 0.526953  [480000/481450]\n",
      "Train Accuracy: 83.4625%\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.630287, F1-score: 80.96% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.375080  [    0/481450]\n",
      "loss: 0.446128  [ 3200/481450]\n",
      "loss: 0.421201  [ 6400/481450]\n",
      "loss: 0.390591  [ 9600/481450]\n",
      "loss: 0.221623  [12800/481450]\n",
      "loss: 0.447497  [16000/481450]\n",
      "loss: 0.614379  [19200/481450]\n",
      "loss: 0.385302  [22400/481450]\n",
      "loss: 0.355858  [25600/481450]\n",
      "loss: 0.431601  [28800/481450]\n",
      "loss: 0.446059  [32000/481450]\n",
      "loss: 0.456558  [35200/481450]\n",
      "loss: 0.399099  [38400/481450]\n",
      "loss: 0.433197  [41600/481450]\n",
      "loss: 0.269663  [44800/481450]\n",
      "loss: 0.422387  [48000/481450]\n",
      "loss: 0.333181  [51200/481450]\n",
      "loss: 0.498198  [54400/481450]\n",
      "loss: 0.340039  [57600/481450]\n",
      "loss: 0.385041  [60800/481450]\n",
      "loss: 0.484063  [64000/481450]\n",
      "loss: 0.486310  [67200/481450]\n",
      "loss: 0.139373  [70400/481450]\n",
      "loss: 0.252222  [73600/481450]\n",
      "loss: 0.332428  [76800/481450]\n",
      "loss: 0.295929  [80000/481450]\n",
      "loss: 0.179142  [83200/481450]\n",
      "loss: 0.307429  [86400/481450]\n",
      "loss: 0.640207  [89600/481450]\n",
      "loss: 0.428035  [92800/481450]\n",
      "loss: 0.254925  [96000/481450]\n",
      "loss: 0.492751  [99200/481450]\n",
      "loss: 0.477515  [102400/481450]\n",
      "loss: 0.614776  [105600/481450]\n",
      "loss: 0.387638  [108800/481450]\n",
      "loss: 0.434519  [112000/481450]\n",
      "loss: 0.574795  [115200/481450]\n",
      "loss: 0.340494  [118400/481450]\n",
      "loss: 0.248567  [121600/481450]\n",
      "loss: 0.467805  [124800/481450]\n",
      "loss: 0.332177  [128000/481450]\n",
      "loss: 0.321855  [131200/481450]\n",
      "loss: 0.365120  [134400/481450]\n",
      "loss: 0.269082  [137600/481450]\n",
      "loss: 0.446234  [140800/481450]\n",
      "loss: 0.448630  [144000/481450]\n",
      "loss: 0.385322  [147200/481450]\n",
      "loss: 0.355608  [150400/481450]\n",
      "loss: 0.231873  [153600/481450]\n",
      "loss: 0.486915  [156800/481450]\n",
      "loss: 0.458352  [160000/481450]\n",
      "loss: 0.586897  [163200/481450]\n",
      "loss: 0.518636  [166400/481450]\n",
      "loss: 0.322561  [169600/481450]\n",
      "loss: 0.203665  [172800/481450]\n",
      "loss: 0.388589  [176000/481450]\n",
      "loss: 0.299579  [179200/481450]\n",
      "loss: 0.651444  [182400/481450]\n",
      "loss: 0.278981  [185600/481450]\n",
      "loss: 0.431724  [188800/481450]\n",
      "loss: 0.456179  [192000/481450]\n",
      "loss: 0.557451  [195200/481450]\n",
      "loss: 0.357372  [198400/481450]\n",
      "loss: 0.453964  [201600/481450]\n",
      "loss: 0.510915  [204800/481450]\n",
      "loss: 0.412583  [208000/481450]\n",
      "loss: 0.262410  [211200/481450]\n",
      "loss: 0.327734  [214400/481450]\n",
      "loss: 0.481631  [217600/481450]\n",
      "loss: 0.411182  [220800/481450]\n",
      "loss: 0.380644  [224000/481450]\n",
      "loss: 0.590544  [227200/481450]\n",
      "loss: 0.359393  [230400/481450]\n",
      "loss: 0.359238  [233600/481450]\n",
      "loss: 0.431547  [236800/481450]\n",
      "loss: 0.279384  [240000/481450]\n",
      "loss: 0.401043  [243200/481450]\n",
      "loss: 0.365726  [246400/481450]\n",
      "loss: 0.272352  [249600/481450]\n",
      "loss: 0.274613  [252800/481450]\n",
      "loss: 0.515284  [256000/481450]\n",
      "loss: 0.709265  [259200/481450]\n",
      "loss: 0.438873  [262400/481450]\n",
      "loss: 0.397332  [265600/481450]\n",
      "loss: 0.662855  [268800/481450]\n",
      "loss: 0.360598  [272000/481450]\n",
      "loss: 0.423967  [275200/481450]\n",
      "loss: 0.198934  [278400/481450]\n",
      "loss: 0.197805  [281600/481450]\n",
      "loss: 0.186831  [284800/481450]\n",
      "loss: 0.285786  [288000/481450]\n",
      "loss: 0.649011  [291200/481450]\n",
      "loss: 0.408582  [294400/481450]\n",
      "loss: 0.196713  [297600/481450]\n",
      "loss: 0.591960  [300800/481450]\n",
      "loss: 0.340921  [304000/481450]\n",
      "loss: 0.629707  [307200/481450]\n",
      "loss: 0.281518  [310400/481450]\n",
      "loss: 0.381229  [313600/481450]\n",
      "loss: 0.281924  [316800/481450]\n",
      "loss: 0.383906  [320000/481450]\n",
      "loss: 0.483146  [323200/481450]\n",
      "loss: 0.591276  [326400/481450]\n",
      "loss: 0.575243  [329600/481450]\n",
      "loss: 0.514739  [332800/481450]\n",
      "loss: 0.363320  [336000/481450]\n",
      "loss: 0.349838  [339200/481450]\n",
      "loss: 0.255401  [342400/481450]\n",
      "loss: 0.684211  [345600/481450]\n",
      "loss: 0.298538  [348800/481450]\n",
      "loss: 0.181315  [352000/481450]\n",
      "loss: 0.267444  [355200/481450]\n",
      "loss: 0.317409  [358400/481450]\n",
      "loss: 0.339766  [361600/481450]\n",
      "loss: 0.263118  [364800/481450]\n",
      "loss: 0.318326  [368000/481450]\n",
      "loss: 0.338847  [371200/481450]\n",
      "loss: 0.270707  [374400/481450]\n",
      "loss: 0.405199  [377600/481450]\n",
      "loss: 0.409997  [380800/481450]\n",
      "loss: 0.517773  [384000/481450]\n",
      "loss: 0.251037  [387200/481450]\n",
      "loss: 0.484033  [390400/481450]\n",
      "loss: 0.327673  [393600/481450]\n",
      "loss: 0.199566  [396800/481450]\n",
      "loss: 0.297180  [400000/481450]\n",
      "loss: 0.357831  [403200/481450]\n",
      "loss: 0.254409  [406400/481450]\n",
      "loss: 0.717402  [409600/481450]\n",
      "loss: 0.457120  [412800/481450]\n",
      "loss: 0.306176  [416000/481450]\n",
      "loss: 0.212321  [419200/481450]\n",
      "loss: 0.524770  [422400/481450]\n",
      "loss: 0.697487  [425600/481450]\n",
      "loss: 0.451591  [428800/481450]\n",
      "loss: 0.634668  [432000/481450]\n",
      "loss: 0.390448  [435200/481450]\n",
      "loss: 0.455640  [438400/481450]\n",
      "loss: 0.464531  [441600/481450]\n",
      "loss: 0.411215  [444800/481450]\n",
      "loss: 0.446906  [448000/481450]\n",
      "loss: 0.374529  [451200/481450]\n",
      "loss: 0.086278  [454400/481450]\n",
      "loss: 0.340402  [457600/481450]\n",
      "loss: 0.447986  [460800/481450]\n",
      "loss: 0.419011  [464000/481450]\n",
      "loss: 0.401048  [467200/481450]\n",
      "loss: 0.532877  [470400/481450]\n",
      "loss: 0.547216  [473600/481450]\n",
      "loss: 0.527165  [476800/481450]\n",
      "loss: 0.346752  [480000/481450]\n",
      "Train Accuracy: 84.5124%\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.603063, F1-score: 81.57% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.220983  [    0/481450]\n",
      "loss: 0.329930  [ 3200/481450]\n",
      "loss: 0.208471  [ 6400/481450]\n",
      "loss: 0.495156  [ 9600/481450]\n",
      "loss: 0.333022  [12800/481450]\n",
      "loss: 0.295482  [16000/481450]\n",
      "loss: 0.409786  [19200/481450]\n",
      "loss: 0.535087  [22400/481450]\n",
      "loss: 0.624230  [25600/481450]\n",
      "loss: 0.256426  [28800/481450]\n",
      "loss: 0.656212  [32000/481450]\n",
      "loss: 0.372179  [35200/481450]\n",
      "loss: 0.236710  [38400/481450]\n",
      "loss: 0.164927  [41600/481450]\n",
      "loss: 0.491815  [44800/481450]\n",
      "loss: 0.205165  [48000/481450]\n",
      "loss: 0.541486  [51200/481450]\n",
      "loss: 0.283875  [54400/481450]\n",
      "loss: 0.383279  [57600/481450]\n",
      "loss: 0.305283  [60800/481450]\n",
      "loss: 0.647001  [64000/481450]\n",
      "loss: 0.371707  [67200/481450]\n",
      "loss: 0.383337  [70400/481450]\n",
      "loss: 0.429075  [73600/481450]\n",
      "loss: 0.462073  [76800/481450]\n",
      "loss: 0.417481  [80000/481450]\n",
      "loss: 0.221391  [83200/481450]\n",
      "loss: 0.289954  [86400/481450]\n",
      "loss: 0.350431  [89600/481450]\n",
      "loss: 0.329865  [92800/481450]\n",
      "loss: 0.495991  [96000/481450]\n",
      "loss: 0.329192  [99200/481450]\n",
      "loss: 0.389125  [102400/481450]\n",
      "loss: 0.252600  [105600/481450]\n",
      "loss: 0.372835  [108800/481450]\n",
      "loss: 0.504367  [112000/481450]\n",
      "loss: 0.406078  [115200/481450]\n",
      "loss: 0.252213  [118400/481450]\n",
      "loss: 0.470661  [121600/481450]\n",
      "loss: 0.615862  [124800/481450]\n",
      "loss: 0.204230  [128000/481450]\n",
      "loss: 0.632095  [131200/481450]\n",
      "loss: 0.375410  [134400/481450]\n",
      "loss: 0.118815  [137600/481450]\n",
      "loss: 0.213956  [140800/481450]\n",
      "loss: 0.220140  [144000/481450]\n",
      "loss: 0.189263  [147200/481450]\n",
      "loss: 0.389325  [150400/481450]\n",
      "loss: 0.297064  [153600/481450]\n",
      "loss: 0.308207  [156800/481450]\n",
      "loss: 0.474815  [160000/481450]\n",
      "loss: 0.306694  [163200/481450]\n",
      "loss: 0.272674  [166400/481450]\n",
      "loss: 0.292244  [169600/481450]\n",
      "loss: 0.357481  [172800/481450]\n",
      "loss: 0.193735  [176000/481450]\n",
      "loss: 0.392088  [179200/481450]\n",
      "loss: 0.192817  [182400/481450]\n",
      "loss: 0.312850  [185600/481450]\n",
      "loss: 0.308315  [188800/481450]\n",
      "loss: 0.596738  [192000/481450]\n",
      "loss: 0.537079  [195200/481450]\n",
      "loss: 0.217593  [198400/481450]\n",
      "loss: 0.260682  [201600/481450]\n",
      "loss: 0.243107  [204800/481450]\n",
      "loss: 0.367787  [208000/481450]\n",
      "loss: 0.287314  [211200/481450]\n",
      "loss: 0.366625  [214400/481450]\n",
      "loss: 0.338434  [217600/481450]\n",
      "loss: 0.411108  [220800/481450]\n",
      "loss: 0.310147  [224000/481450]\n",
      "loss: 0.268104  [227200/481450]\n",
      "loss: 0.423054  [230400/481450]\n",
      "loss: 0.234233  [233600/481450]\n",
      "loss: 0.139309  [236800/481450]\n",
      "loss: 0.292480  [240000/481450]\n",
      "loss: 0.365576  [243200/481450]\n",
      "loss: 0.493380  [246400/481450]\n",
      "loss: 0.292942  [249600/481450]\n",
      "loss: 0.394894  [252800/481450]\n",
      "loss: 0.322843  [256000/481450]\n",
      "loss: 0.412747  [259200/481450]\n",
      "loss: 0.457077  [262400/481450]\n",
      "loss: 0.496655  [265600/481450]\n",
      "loss: 0.509222  [268800/481450]\n",
      "loss: 0.483721  [272000/481450]\n",
      "loss: 0.345149  [275200/481450]\n",
      "loss: 0.357964  [278400/481450]\n",
      "loss: 0.233928  [281600/481450]\n",
      "loss: 0.496698  [284800/481450]\n",
      "loss: 0.382595  [288000/481450]\n",
      "loss: 0.158300  [291200/481450]\n",
      "loss: 0.346045  [294400/481450]\n",
      "loss: 0.324094  [297600/481450]\n",
      "loss: 0.453090  [300800/481450]\n",
      "loss: 0.382257  [304000/481450]\n",
      "loss: 0.357067  [307200/481450]\n",
      "loss: 0.402796  [310400/481450]\n",
      "loss: 0.451794  [313600/481450]\n",
      "loss: 0.640983  [316800/481450]\n",
      "loss: 0.280053  [320000/481450]\n",
      "loss: 0.355470  [323200/481450]\n",
      "loss: 0.447125  [326400/481450]\n",
      "loss: 0.413233  [329600/481450]\n",
      "loss: 0.259351  [332800/481450]\n",
      "loss: 0.449005  [336000/481450]\n",
      "loss: 0.577767  [339200/481450]\n",
      "loss: 0.209293  [342400/481450]\n",
      "loss: 0.546447  [345600/481450]\n",
      "loss: 0.251858  [348800/481450]\n",
      "loss: 0.230727  [352000/481450]\n",
      "loss: 0.263113  [355200/481450]\n",
      "loss: 0.472972  [358400/481450]\n",
      "loss: 0.392025  [361600/481450]\n",
      "loss: 0.307722  [364800/481450]\n",
      "loss: 0.372065  [368000/481450]\n",
      "loss: 0.209838  [371200/481450]\n",
      "loss: 0.391788  [374400/481450]\n",
      "loss: 0.468760  [377600/481450]\n",
      "loss: 0.252290  [380800/481450]\n",
      "loss: 0.374442  [384000/481450]\n",
      "loss: 0.441059  [387200/481450]\n",
      "loss: 0.643470  [390400/481450]\n",
      "loss: 0.474209  [393600/481450]\n",
      "loss: 0.207976  [396800/481450]\n",
      "loss: 0.322660  [400000/481450]\n",
      "loss: 0.455929  [403200/481450]\n",
      "loss: 0.480905  [406400/481450]\n",
      "loss: 0.341295  [409600/481450]\n",
      "loss: 0.393096  [412800/481450]\n",
      "loss: 0.272625  [416000/481450]\n",
      "loss: 0.125378  [419200/481450]\n",
      "loss: 0.359950  [422400/481450]\n",
      "loss: 0.301779  [425600/481450]\n",
      "loss: 0.350157  [428800/481450]\n",
      "loss: 0.305701  [432000/481450]\n",
      "loss: 0.437744  [435200/481450]\n",
      "loss: 0.261899  [438400/481450]\n",
      "loss: 0.599716  [441600/481450]\n",
      "loss: 0.280815  [444800/481450]\n",
      "loss: 0.288589  [448000/481450]\n",
      "loss: 0.156098  [451200/481450]\n",
      "loss: 0.534891  [454400/481450]\n",
      "loss: 0.328557  [457600/481450]\n",
      "loss: 0.503293  [460800/481450]\n",
      "loss: 0.270560  [464000/481450]\n",
      "loss: 0.371628  [467200/481450]\n",
      "loss: 0.543373  [470400/481450]\n",
      "loss: 0.334911  [473600/481450]\n",
      "loss: 0.330146  [476800/481450]\n",
      "loss: 0.324761  [480000/481450]\n",
      "Train Accuracy: 85.2685%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.809947, F1-score: 77.15% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.289118  [    0/481450]\n",
      "loss: 0.357825  [ 3200/481450]\n",
      "loss: 0.257647  [ 6400/481450]\n",
      "loss: 0.340055  [ 9600/481450]\n",
      "loss: 0.358327  [12800/481450]\n",
      "loss: 0.417490  [16000/481450]\n",
      "loss: 0.381055  [19200/481450]\n",
      "loss: 0.505459  [22400/481450]\n",
      "loss: 0.407577  [25600/481450]\n",
      "loss: 0.298119  [28800/481450]\n",
      "loss: 0.225314  [32000/481450]\n",
      "loss: 0.336511  [35200/481450]\n",
      "loss: 0.410700  [38400/481450]\n",
      "loss: 0.312428  [41600/481450]\n",
      "loss: 0.341671  [44800/481450]\n",
      "loss: 0.303376  [48000/481450]\n",
      "loss: 0.257779  [51200/481450]\n",
      "loss: 0.460130  [54400/481450]\n",
      "loss: 0.310060  [57600/481450]\n",
      "loss: 0.541169  [60800/481450]\n",
      "loss: 0.309177  [64000/481450]\n",
      "loss: 0.369566  [67200/481450]\n",
      "loss: 0.351236  [70400/481450]\n",
      "loss: 0.264097  [73600/481450]\n",
      "loss: 0.264063  [76800/481450]\n",
      "loss: 0.308891  [80000/481450]\n",
      "loss: 0.331310  [83200/481450]\n",
      "loss: 0.417585  [86400/481450]\n",
      "loss: 0.463835  [89600/481450]\n",
      "loss: 0.243365  [92800/481450]\n",
      "loss: 0.312825  [96000/481450]\n",
      "loss: 0.310738  [99200/481450]\n",
      "loss: 0.267453  [102400/481450]\n",
      "loss: 0.164673  [105600/481450]\n",
      "loss: 0.491335  [108800/481450]\n",
      "loss: 0.371768  [112000/481450]\n",
      "loss: 0.391516  [115200/481450]\n",
      "loss: 0.432700  [118400/481450]\n",
      "loss: 0.136939  [121600/481450]\n",
      "loss: 0.275681  [124800/481450]\n",
      "loss: 0.124259  [128000/481450]\n",
      "loss: 0.240406  [131200/481450]\n",
      "loss: 0.636645  [134400/481450]\n",
      "loss: 0.489997  [137600/481450]\n",
      "loss: 0.446392  [140800/481450]\n",
      "loss: 0.273222  [144000/481450]\n",
      "loss: 0.281606  [147200/481450]\n",
      "loss: 0.479894  [150400/481450]\n",
      "loss: 0.329383  [153600/481450]\n",
      "loss: 0.468004  [156800/481450]\n",
      "loss: 0.244714  [160000/481450]\n",
      "loss: 0.144722  [163200/481450]\n",
      "loss: 0.415517  [166400/481450]\n",
      "loss: 0.311593  [169600/481450]\n",
      "loss: 0.126721  [172800/481450]\n",
      "loss: 0.169094  [176000/481450]\n",
      "loss: 0.263058  [179200/481450]\n",
      "loss: 0.464066  [182400/481450]\n",
      "loss: 0.157626  [185600/481450]\n",
      "loss: 0.233380  [188800/481450]\n",
      "loss: 0.338517  [192000/481450]\n",
      "loss: 0.283759  [195200/481450]\n",
      "loss: 0.235701  [198400/481450]\n",
      "loss: 0.445770  [201600/481450]\n",
      "loss: 0.371752  [204800/481450]\n",
      "loss: 0.436521  [208000/481450]\n",
      "loss: 0.273549  [211200/481450]\n",
      "loss: 0.163906  [214400/481450]\n",
      "loss: 0.716161  [217600/481450]\n",
      "loss: 0.281489  [220800/481450]\n",
      "loss: 0.442590  [224000/481450]\n",
      "loss: 0.202500  [227200/481450]\n",
      "loss: 0.445261  [230400/481450]\n",
      "loss: 0.318773  [233600/481450]\n",
      "loss: 0.182502  [236800/481450]\n",
      "loss: 0.525683  [240000/481450]\n",
      "loss: 0.296960  [243200/481450]\n",
      "loss: 0.328290  [246400/481450]\n",
      "loss: 0.237411  [249600/481450]\n",
      "loss: 0.257346  [252800/481450]\n",
      "loss: 0.428395  [256000/481450]\n",
      "loss: 0.454368  [259200/481450]\n",
      "loss: 0.369042  [262400/481450]\n",
      "loss: 0.397977  [265600/481450]\n",
      "loss: 0.403307  [268800/481450]\n",
      "loss: 0.292926  [272000/481450]\n",
      "loss: 0.572482  [275200/481450]\n",
      "loss: 0.477534  [278400/481450]\n",
      "loss: 0.261991  [281600/481450]\n",
      "loss: 0.604403  [284800/481450]\n",
      "loss: 0.371707  [288000/481450]\n",
      "loss: 0.395790  [291200/481450]\n",
      "loss: 0.310069  [294400/481450]\n",
      "loss: 0.124595  [297600/481450]\n",
      "loss: 0.224985  [300800/481450]\n",
      "loss: 0.382811  [304000/481450]\n",
      "loss: 0.288226  [307200/481450]\n",
      "loss: 0.283911  [310400/481450]\n",
      "loss: 0.264552  [313600/481450]\n",
      "loss: 0.286736  [316800/481450]\n",
      "loss: 0.395042  [320000/481450]\n",
      "loss: 0.632418  [323200/481450]\n",
      "loss: 0.245809  [326400/481450]\n",
      "loss: 0.316754  [329600/481450]\n",
      "loss: 0.366048  [332800/481450]\n",
      "loss: 0.260967  [336000/481450]\n",
      "loss: 0.450439  [339200/481450]\n",
      "loss: 0.229013  [342400/481450]\n",
      "loss: 0.372865  [345600/481450]\n",
      "loss: 0.225862  [348800/481450]\n",
      "loss: 0.423956  [352000/481450]\n",
      "loss: 0.283322  [355200/481450]\n",
      "loss: 0.715559  [358400/481450]\n",
      "loss: 0.513449  [361600/481450]\n",
      "loss: 0.206210  [364800/481450]\n",
      "loss: 0.359834  [368000/481450]\n",
      "loss: 0.259223  [371200/481450]\n",
      "loss: 0.253777  [374400/481450]\n",
      "loss: 0.139557  [377600/481450]\n",
      "loss: 0.438806  [380800/481450]\n",
      "loss: 0.291465  [384000/481450]\n",
      "loss: 0.249759  [387200/481450]\n",
      "loss: 0.286415  [390400/481450]\n",
      "loss: 0.352619  [393600/481450]\n",
      "loss: 0.476766  [396800/481450]\n",
      "loss: 0.391496  [400000/481450]\n",
      "loss: 0.368939  [403200/481450]\n",
      "loss: 0.302801  [406400/481450]\n",
      "loss: 0.325692  [409600/481450]\n",
      "loss: 0.336475  [412800/481450]\n",
      "loss: 0.375713  [416000/481450]\n",
      "loss: 0.554570  [419200/481450]\n",
      "loss: 0.211024  [422400/481450]\n",
      "loss: 0.330451  [425600/481450]\n",
      "loss: 0.236618  [428800/481450]\n",
      "loss: 0.322901  [432000/481450]\n",
      "loss: 0.269425  [435200/481450]\n",
      "loss: 0.382462  [438400/481450]\n",
      "loss: 0.255620  [441600/481450]\n",
      "loss: 0.226802  [444800/481450]\n",
      "loss: 0.207496  [448000/481450]\n",
      "loss: 0.307447  [451200/481450]\n",
      "loss: 0.388480  [454400/481450]\n",
      "loss: 0.225653  [457600/481450]\n",
      "loss: 0.436859  [460800/481450]\n",
      "loss: 0.485935  [464000/481450]\n",
      "loss: 0.214621  [467200/481450]\n",
      "loss: 0.250462  [470400/481450]\n",
      "loss: 0.396459  [473600/481450]\n",
      "loss: 0.451415  [476800/481450]\n",
      "loss: 0.565654  [480000/481450]\n",
      "Train Accuracy: 85.7094%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.760265, F1-score: 78.54% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.295207  [    0/481450]\n",
      "loss: 0.505324  [ 3200/481450]\n",
      "loss: 0.495112  [ 6400/481450]\n",
      "loss: 0.163507  [ 9600/481450]\n",
      "loss: 0.349751  [12800/481450]\n",
      "loss: 0.270061  [16000/481450]\n",
      "loss: 0.383576  [19200/481450]\n",
      "loss: 0.580127  [22400/481450]\n",
      "loss: 0.434331  [25600/481450]\n",
      "loss: 0.440183  [28800/481450]\n",
      "loss: 0.480264  [32000/481450]\n",
      "loss: 0.380933  [35200/481450]\n",
      "loss: 0.289375  [38400/481450]\n",
      "loss: 0.398234  [41600/481450]\n",
      "loss: 0.216058  [44800/481450]\n",
      "loss: 0.410101  [48000/481450]\n",
      "loss: 0.405224  [51200/481450]\n",
      "loss: 0.324922  [54400/481450]\n",
      "loss: 0.245774  [57600/481450]\n",
      "loss: 0.470748  [60800/481450]\n",
      "loss: 0.191435  [64000/481450]\n",
      "loss: 0.284413  [67200/481450]\n",
      "loss: 0.380744  [70400/481450]\n",
      "loss: 0.166771  [73600/481450]\n",
      "loss: 0.352768  [76800/481450]\n",
      "loss: 0.258371  [80000/481450]\n",
      "loss: 0.279316  [83200/481450]\n",
      "loss: 0.224054  [86400/481450]\n",
      "loss: 0.331132  [89600/481450]\n",
      "loss: 0.356595  [92800/481450]\n",
      "loss: 0.273720  [96000/481450]\n",
      "loss: 0.519391  [99200/481450]\n",
      "loss: 0.537293  [102400/481450]\n",
      "loss: 0.300216  [105600/481450]\n",
      "loss: 0.418111  [108800/481450]\n",
      "loss: 0.458083  [112000/481450]\n",
      "loss: 0.409699  [115200/481450]\n",
      "loss: 0.354708  [118400/481450]\n",
      "loss: 0.239718  [121600/481450]\n",
      "loss: 0.659416  [124800/481450]\n",
      "loss: 0.398055  [128000/481450]\n",
      "loss: 0.252677  [131200/481450]\n",
      "loss: 0.241689  [134400/481450]\n",
      "loss: 0.471155  [137600/481450]\n",
      "loss: 0.496923  [140800/481450]\n",
      "loss: 0.181985  [144000/481450]\n",
      "loss: 0.260405  [147200/481450]\n",
      "loss: 0.238193  [150400/481450]\n",
      "loss: 0.446258  [153600/481450]\n",
      "loss: 0.396850  [156800/481450]\n",
      "loss: 0.177128  [160000/481450]\n",
      "loss: 0.396334  [163200/481450]\n",
      "loss: 0.270820  [166400/481450]\n",
      "loss: 0.524272  [169600/481450]\n",
      "loss: 0.336150  [172800/481450]\n",
      "loss: 0.481624  [176000/481450]\n",
      "loss: 0.170347  [179200/481450]\n",
      "loss: 0.454840  [182400/481450]\n",
      "loss: 0.297567  [185600/481450]\n",
      "loss: 0.514612  [188800/481450]\n",
      "loss: 0.377960  [192000/481450]\n",
      "loss: 0.384340  [195200/481450]\n",
      "loss: 0.595442  [198400/481450]\n",
      "loss: 0.329658  [201600/481450]\n",
      "loss: 0.457248  [204800/481450]\n",
      "loss: 0.284027  [208000/481450]\n",
      "loss: 0.406990  [211200/481450]\n",
      "loss: 0.201321  [214400/481450]\n",
      "loss: 0.180657  [217600/481450]\n",
      "loss: 0.508982  [220800/481450]\n",
      "loss: 0.429807  [224000/481450]\n",
      "loss: 0.453521  [227200/481450]\n",
      "loss: 0.353122  [230400/481450]\n",
      "loss: 0.261137  [233600/481450]\n",
      "loss: 0.473282  [236800/481450]\n",
      "loss: 0.364390  [240000/481450]\n",
      "loss: 0.280855  [243200/481450]\n",
      "loss: 0.386447  [246400/481450]\n",
      "loss: 0.249336  [249600/481450]\n",
      "loss: 0.357753  [252800/481450]\n",
      "loss: 0.160562  [256000/481450]\n",
      "loss: 0.485036  [259200/481450]\n",
      "loss: 0.304581  [262400/481450]\n",
      "loss: 0.491364  [265600/481450]\n",
      "loss: 0.371802  [268800/481450]\n",
      "loss: 0.282622  [272000/481450]\n",
      "loss: 0.438079  [275200/481450]\n",
      "loss: 0.319603  [278400/481450]\n",
      "loss: 0.213591  [281600/481450]\n",
      "loss: 0.266836  [284800/481450]\n",
      "loss: 0.287982  [288000/481450]\n",
      "loss: 0.397397  [291200/481450]\n",
      "loss: 0.385880  [294400/481450]\n",
      "loss: 0.235019  [297600/481450]\n",
      "loss: 0.248951  [300800/481450]\n",
      "loss: 0.280700  [304000/481450]\n",
      "loss: 0.276943  [307200/481450]\n",
      "loss: 0.283327  [310400/481450]\n",
      "loss: 0.392703  [313600/481450]\n",
      "loss: 0.325540  [316800/481450]\n",
      "loss: 0.409495  [320000/481450]\n",
      "loss: 0.141560  [323200/481450]\n",
      "loss: 0.219090  [326400/481450]\n",
      "loss: 0.429180  [329600/481450]\n",
      "loss: 0.384402  [332800/481450]\n",
      "loss: 0.169622  [336000/481450]\n",
      "loss: 0.345658  [339200/481450]\n",
      "loss: 0.209444  [342400/481450]\n",
      "loss: 0.316838  [345600/481450]\n",
      "loss: 0.271513  [348800/481450]\n",
      "loss: 0.334662  [352000/481450]\n",
      "loss: 0.175868  [355200/481450]\n",
      "loss: 0.578911  [358400/481450]\n",
      "loss: 0.404441  [361600/481450]\n",
      "loss: 0.512091  [364800/481450]\n",
      "loss: 0.208637  [368000/481450]\n",
      "loss: 0.182580  [371200/481450]\n",
      "loss: 0.247353  [374400/481450]\n",
      "loss: 0.346305  [377600/481450]\n",
      "loss: 0.491299  [380800/481450]\n",
      "loss: 0.256038  [384000/481450]\n",
      "loss: 0.565723  [387200/481450]\n",
      "loss: 0.385403  [390400/481450]\n",
      "loss: 0.426939  [393600/481450]\n",
      "loss: 0.295290  [396800/481450]\n",
      "loss: 0.215141  [400000/481450]\n",
      "loss: 0.222214  [403200/481450]\n",
      "loss: 0.300599  [406400/481450]\n",
      "loss: 0.227014  [409600/481450]\n",
      "loss: 0.147992  [412800/481450]\n",
      "loss: 0.306028  [416000/481450]\n",
      "loss: 0.428378  [419200/481450]\n",
      "loss: 0.452596  [422400/481450]\n",
      "loss: 0.462428  [425600/481450]\n",
      "loss: 0.297042  [428800/481450]\n",
      "loss: 0.324221  [432000/481450]\n",
      "loss: 0.552618  [435200/481450]\n",
      "loss: 0.244583  [438400/481450]\n",
      "loss: 0.381465  [441600/481450]\n",
      "loss: 0.159077  [444800/481450]\n",
      "loss: 0.341869  [448000/481450]\n",
      "loss: 0.591235  [451200/481450]\n",
      "loss: 0.721037  [454400/481450]\n",
      "loss: 0.308380  [457600/481450]\n",
      "loss: 0.298029  [460800/481450]\n",
      "loss: 0.384470  [464000/481450]\n",
      "loss: 0.419463  [467200/481450]\n",
      "loss: 0.290692  [470400/481450]\n",
      "loss: 0.320669  [473600/481450]\n",
      "loss: 0.239279  [476800/481450]\n",
      "loss: 0.237607  [480000/481450]\n",
      "Train Accuracy: 86.1321%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.793809, F1-score: 78.14% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.288160  [    0/481450]\n",
      "loss: 0.145464  [ 3200/481450]\n",
      "loss: 0.209356  [ 6400/481450]\n",
      "loss: 0.660226  [ 9600/481450]\n",
      "loss: 0.505461  [12800/481450]\n",
      "loss: 0.353147  [16000/481450]\n",
      "loss: 0.285192  [19200/481450]\n",
      "loss: 0.499817  [22400/481450]\n",
      "loss: 0.216709  [25600/481450]\n",
      "loss: 0.475532  [28800/481450]\n",
      "loss: 0.307492  [32000/481450]\n",
      "loss: 0.185453  [35200/481450]\n",
      "loss: 0.468623  [38400/481450]\n",
      "loss: 0.260626  [41600/481450]\n",
      "loss: 0.546769  [44800/481450]\n",
      "loss: 0.252240  [48000/481450]\n",
      "loss: 0.220574  [51200/481450]\n",
      "loss: 0.249641  [54400/481450]\n",
      "loss: 0.307314  [57600/481450]\n",
      "loss: 0.326041  [60800/481450]\n",
      "loss: 0.313301  [64000/481450]\n",
      "loss: 0.272020  [67200/481450]\n",
      "loss: 0.316814  [70400/481450]\n",
      "loss: 0.385838  [73600/481450]\n",
      "loss: 0.130041  [76800/481450]\n",
      "loss: 0.281551  [80000/481450]\n",
      "loss: 0.595080  [83200/481450]\n",
      "loss: 0.452724  [86400/481450]\n",
      "loss: 0.281270  [89600/481450]\n",
      "loss: 0.123156  [92800/481450]\n",
      "loss: 0.341632  [96000/481450]\n",
      "loss: 0.370870  [99200/481450]\n",
      "loss: 0.183142  [102400/481450]\n",
      "loss: 0.395184  [105600/481450]\n",
      "loss: 0.545027  [108800/481450]\n",
      "loss: 0.368283  [112000/481450]\n",
      "loss: 0.361957  [115200/481450]\n",
      "loss: 0.502682  [118400/481450]\n",
      "loss: 0.582624  [121600/481450]\n",
      "loss: 0.111951  [124800/481450]\n",
      "loss: 0.296670  [128000/481450]\n",
      "loss: 0.318873  [131200/481450]\n",
      "loss: 0.305370  [134400/481450]\n",
      "loss: 0.299532  [137600/481450]\n",
      "loss: 0.316253  [140800/481450]\n",
      "loss: 0.356495  [144000/481450]\n",
      "loss: 0.294751  [147200/481450]\n",
      "loss: 0.335781  [150400/481450]\n",
      "loss: 0.175722  [153600/481450]\n",
      "loss: 0.291017  [156800/481450]\n",
      "loss: 0.347047  [160000/481450]\n",
      "loss: 0.352013  [163200/481450]\n",
      "loss: 0.330094  [166400/481450]\n",
      "loss: 0.307434  [169600/481450]\n",
      "loss: 0.455092  [172800/481450]\n",
      "loss: 0.226037  [176000/481450]\n",
      "loss: 0.104746  [179200/481450]\n",
      "loss: 0.446912  [182400/481450]\n",
      "loss: 0.364188  [185600/481450]\n",
      "loss: 0.320099  [188800/481450]\n",
      "loss: 0.175571  [192000/481450]\n",
      "loss: 0.236600  [195200/481450]\n",
      "loss: 0.737166  [198400/481450]\n",
      "loss: 0.518313  [201600/481450]\n",
      "loss: 0.169606  [204800/481450]\n",
      "loss: 0.441939  [208000/481450]\n",
      "loss: 0.329098  [211200/481450]\n",
      "loss: 0.174064  [214400/481450]\n",
      "loss: 0.057084  [217600/481450]\n",
      "loss: 0.302519  [220800/481450]\n",
      "loss: 0.302047  [224000/481450]\n",
      "loss: 0.272688  [227200/481450]\n",
      "loss: 0.195381  [230400/481450]\n",
      "loss: 0.313342  [233600/481450]\n",
      "loss: 0.217263  [236800/481450]\n",
      "loss: 0.359483  [240000/481450]\n",
      "loss: 0.241952  [243200/481450]\n",
      "loss: 0.371827  [246400/481450]\n",
      "loss: 0.709781  [249600/481450]\n",
      "loss: 0.221645  [252800/481450]\n",
      "loss: 0.348542  [256000/481450]\n",
      "loss: 0.321075  [259200/481450]\n",
      "loss: 0.223776  [262400/481450]\n",
      "loss: 0.294758  [265600/481450]\n",
      "loss: 0.280464  [268800/481450]\n",
      "loss: 0.149590  [272000/481450]\n",
      "loss: 0.267712  [275200/481450]\n",
      "loss: 0.371342  [278400/481450]\n",
      "loss: 0.216764  [281600/481450]\n",
      "loss: 0.341405  [284800/481450]\n",
      "loss: 0.140198  [288000/481450]\n",
      "loss: 0.167712  [291200/481450]\n",
      "loss: 0.293762  [294400/481450]\n",
      "loss: 0.395931  [297600/481450]\n",
      "loss: 0.284718  [300800/481450]\n",
      "loss: 0.325817  [304000/481450]\n",
      "loss: 0.625019  [307200/481450]\n",
      "loss: 0.299030  [310400/481450]\n",
      "loss: 0.397543  [313600/481450]\n",
      "loss: 0.398768  [316800/481450]\n",
      "loss: 0.332649  [320000/481450]\n",
      "loss: 0.411193  [323200/481450]\n",
      "loss: 0.363210  [326400/481450]\n",
      "loss: 0.187233  [329600/481450]\n",
      "loss: 0.213968  [332800/481450]\n",
      "loss: 0.238785  [336000/481450]\n",
      "loss: 0.309993  [339200/481450]\n",
      "loss: 0.341807  [342400/481450]\n",
      "loss: 0.241532  [345600/481450]\n",
      "loss: 0.374081  [348800/481450]\n",
      "loss: 0.336028  [352000/481450]\n",
      "loss: 0.205565  [355200/481450]\n",
      "loss: 0.374380  [358400/481450]\n",
      "loss: 0.368357  [361600/481450]\n",
      "loss: 0.632454  [364800/481450]\n",
      "loss: 0.184751  [368000/481450]\n",
      "loss: 0.388287  [371200/481450]\n",
      "loss: 0.264610  [374400/481450]\n",
      "loss: 0.290527  [377600/481450]\n",
      "loss: 0.322412  [380800/481450]\n",
      "loss: 0.368887  [384000/481450]\n",
      "loss: 0.295678  [387200/481450]\n",
      "loss: 0.161847  [390400/481450]\n",
      "loss: 0.374620  [393600/481450]\n",
      "loss: 0.219744  [396800/481450]\n",
      "loss: 0.630896  [400000/481450]\n",
      "loss: 0.348976  [403200/481450]\n",
      "loss: 0.196368  [406400/481450]\n",
      "loss: 0.428710  [409600/481450]\n",
      "loss: 0.512820  [412800/481450]\n",
      "loss: 0.102381  [416000/481450]\n",
      "loss: 0.118384  [419200/481450]\n",
      "loss: 0.353301  [422400/481450]\n",
      "loss: 0.175055  [425600/481450]\n",
      "loss: 0.306801  [428800/481450]\n",
      "loss: 0.243383  [432000/481450]\n",
      "loss: 0.375737  [435200/481450]\n",
      "loss: 0.206414  [438400/481450]\n",
      "loss: 0.612354  [441600/481450]\n",
      "loss: 0.223358  [444800/481450]\n",
      "loss: 0.188415  [448000/481450]\n",
      "loss: 0.326135  [451200/481450]\n",
      "loss: 0.344231  [454400/481450]\n",
      "loss: 0.284062  [457600/481450]\n",
      "loss: 0.406950  [460800/481450]\n",
      "loss: 0.511353  [464000/481450]\n",
      "loss: 0.221630  [467200/481450]\n",
      "loss: 0.338766  [470400/481450]\n",
      "loss: 0.429073  [473600/481450]\n",
      "loss: 0.311526  [476800/481450]\n",
      "loss: 0.341933  [480000/481450]\n",
      "Train Accuracy: 86.5525%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.751867, F1-score: 78.92% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.253921  [    0/481450]\n",
      "loss: 0.224495  [ 3200/481450]\n",
      "loss: 0.365556  [ 6400/481450]\n",
      "loss: 0.488570  [ 9600/481450]\n",
      "loss: 0.242597  [12800/481450]\n",
      "loss: 0.182086  [16000/481450]\n",
      "loss: 0.254196  [19200/481450]\n",
      "loss: 0.283156  [22400/481450]\n",
      "loss: 0.132827  [25600/481450]\n",
      "loss: 0.212454  [28800/481450]\n",
      "loss: 0.304519  [32000/481450]\n",
      "loss: 0.297874  [35200/481450]\n",
      "loss: 0.200968  [38400/481450]\n",
      "loss: 0.746570  [41600/481450]\n",
      "loss: 0.511871  [44800/481450]\n",
      "loss: 0.192925  [48000/481450]\n",
      "loss: 0.186111  [51200/481450]\n",
      "loss: 0.311500  [54400/481450]\n",
      "loss: 0.261404  [57600/481450]\n",
      "loss: 0.215800  [60800/481450]\n",
      "loss: 0.180784  [64000/481450]\n",
      "loss: 0.213233  [67200/481450]\n",
      "loss: 0.497175  [70400/481450]\n",
      "loss: 0.349166  [73600/481450]\n",
      "loss: 0.366398  [76800/481450]\n",
      "loss: 0.334948  [80000/481450]\n",
      "loss: 0.426446  [83200/481450]\n",
      "loss: 0.195635  [86400/481450]\n",
      "loss: 0.449826  [89600/481450]\n",
      "loss: 0.188100  [92800/481450]\n",
      "loss: 0.170710  [96000/481450]\n",
      "loss: 0.133882  [99200/481450]\n",
      "loss: 0.276368  [102400/481450]\n",
      "loss: 0.328121  [105600/481450]\n",
      "loss: 0.248723  [108800/481450]\n",
      "loss: 0.548089  [112000/481450]\n",
      "loss: 0.370947  [115200/481450]\n",
      "loss: 0.194675  [118400/481450]\n",
      "loss: 0.256349  [121600/481450]\n",
      "loss: 0.203623  [124800/481450]\n",
      "loss: 0.223887  [128000/481450]\n",
      "loss: 0.261637  [131200/481450]\n",
      "loss: 0.305999  [134400/481450]\n",
      "loss: 0.258249  [137600/481450]\n",
      "loss: 0.306793  [140800/481450]\n",
      "loss: 0.244784  [144000/481450]\n",
      "loss: 0.213852  [147200/481450]\n",
      "loss: 0.461506  [150400/481450]\n",
      "loss: 0.375055  [153600/481450]\n",
      "loss: 0.292180  [156800/481450]\n",
      "loss: 0.404876  [160000/481450]\n",
      "loss: 0.475557  [163200/481450]\n",
      "loss: 0.677231  [166400/481450]\n",
      "loss: 0.341363  [169600/481450]\n",
      "loss: 0.132885  [172800/481450]\n",
      "loss: 0.192447  [176000/481450]\n",
      "loss: 0.172235  [179200/481450]\n",
      "loss: 0.219543  [182400/481450]\n",
      "loss: 0.381772  [185600/481450]\n",
      "loss: 0.303607  [188800/481450]\n",
      "loss: 0.191122  [192000/481450]\n",
      "loss: 0.299217  [195200/481450]\n",
      "loss: 0.196868  [198400/481450]\n",
      "loss: 0.397156  [201600/481450]\n",
      "loss: 0.254311  [204800/481450]\n",
      "loss: 0.405645  [208000/481450]\n",
      "loss: 0.293421  [211200/481450]\n",
      "loss: 0.444281  [214400/481450]\n",
      "loss: 0.227247  [217600/481450]\n",
      "loss: 0.239219  [220800/481450]\n",
      "loss: 0.358889  [224000/481450]\n",
      "loss: 0.360872  [227200/481450]\n",
      "loss: 0.576113  [230400/481450]\n",
      "loss: 0.382882  [233600/481450]\n",
      "loss: 0.368728  [236800/481450]\n",
      "loss: 0.406157  [240000/481450]\n",
      "loss: 0.298475  [243200/481450]\n",
      "loss: 0.247045  [246400/481450]\n",
      "loss: 0.428718  [249600/481450]\n",
      "loss: 0.529147  [252800/481450]\n",
      "loss: 0.470248  [256000/481450]\n",
      "loss: 0.386497  [259200/481450]\n",
      "loss: 0.277226  [262400/481450]\n",
      "loss: 0.572001  [265600/481450]\n",
      "loss: 0.565855  [268800/481450]\n",
      "loss: 0.174081  [272000/481450]\n",
      "loss: 0.427597  [275200/481450]\n",
      "loss: 0.412476  [278400/481450]\n",
      "loss: 0.296629  [281600/481450]\n",
      "loss: 0.325433  [284800/481450]\n",
      "loss: 0.341789  [288000/481450]\n",
      "loss: 0.739076  [291200/481450]\n",
      "loss: 0.376803  [294400/481450]\n",
      "loss: 0.188799  [297600/481450]\n",
      "loss: 0.276427  [300800/481450]\n",
      "loss: 0.152068  [304000/481450]\n",
      "loss: 0.325819  [307200/481450]\n",
      "loss: 0.338405  [310400/481450]\n",
      "loss: 0.342655  [313600/481450]\n",
      "loss: 0.380886  [316800/481450]\n",
      "loss: 0.420636  [320000/481450]\n",
      "loss: 0.215581  [323200/481450]\n",
      "loss: 0.453793  [326400/481450]\n",
      "loss: 0.468868  [329600/481450]\n",
      "loss: 0.203538  [332800/481450]\n",
      "loss: 0.321437  [336000/481450]\n",
      "loss: 0.483278  [339200/481450]\n",
      "loss: 0.275751  [342400/481450]\n",
      "loss: 0.388493  [345600/481450]\n",
      "loss: 0.048635  [348800/481450]\n",
      "loss: 0.317646  [352000/481450]\n",
      "loss: 0.277876  [355200/481450]\n",
      "loss: 0.200878  [358400/481450]\n",
      "loss: 0.545929  [361600/481450]\n",
      "loss: 0.284645  [364800/481450]\n",
      "loss: 0.279325  [368000/481450]\n",
      "loss: 0.120035  [371200/481450]\n",
      "loss: 0.385788  [374400/481450]\n",
      "loss: 0.365505  [377600/481450]\n",
      "loss: 0.345338  [380800/481450]\n",
      "loss: 0.450213  [384000/481450]\n",
      "loss: 0.209203  [387200/481450]\n",
      "loss: 0.313036  [390400/481450]\n",
      "loss: 0.383049  [393600/481450]\n",
      "loss: 0.397964  [396800/481450]\n",
      "loss: 0.321336  [400000/481450]\n",
      "loss: 0.195741  [403200/481450]\n",
      "loss: 0.120407  [406400/481450]\n",
      "loss: 0.445892  [409600/481450]\n",
      "loss: 0.329635  [412800/481450]\n",
      "loss: 0.324724  [416000/481450]\n",
      "loss: 0.221328  [419200/481450]\n",
      "loss: 0.399782  [422400/481450]\n",
      "loss: 0.437672  [425600/481450]\n",
      "loss: 0.376912  [428800/481450]\n",
      "loss: 0.155541  [432000/481450]\n",
      "loss: 0.276075  [435200/481450]\n",
      "loss: 0.295296  [438400/481450]\n",
      "loss: 0.144280  [441600/481450]\n",
      "loss: 0.357690  [444800/481450]\n",
      "loss: 0.183706  [448000/481450]\n",
      "loss: 0.229489  [451200/481450]\n",
      "loss: 0.285996  [454400/481450]\n",
      "loss: 0.212967  [457600/481450]\n",
      "loss: 0.377710  [460800/481450]\n",
      "loss: 0.175565  [464000/481450]\n",
      "loss: 0.477595  [467200/481450]\n",
      "loss: 0.376562  [470400/481450]\n",
      "loss: 0.225267  [473600/481450]\n",
      "loss: 0.298433  [476800/481450]\n",
      "loss: 0.246503  [480000/481450]\n",
      "Train Accuracy: 87.0128%\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.587572, F1-score: 82.43% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.427593  [    0/481450]\n",
      "loss: 0.255467  [ 3200/481450]\n",
      "loss: 0.315774  [ 6400/481450]\n",
      "loss: 0.480200  [ 9600/481450]\n",
      "loss: 0.327716  [12800/481450]\n",
      "loss: 0.158441  [16000/481450]\n",
      "loss: 0.351111  [19200/481450]\n",
      "loss: 0.338027  [22400/481450]\n",
      "loss: 0.290272  [25600/481450]\n",
      "loss: 0.137045  [28800/481450]\n",
      "loss: 0.268262  [32000/481450]\n",
      "loss: 0.462034  [35200/481450]\n",
      "loss: 0.372698  [38400/481450]\n",
      "loss: 0.430658  [41600/481450]\n",
      "loss: 0.310987  [44800/481450]\n",
      "loss: 0.041161  [48000/481450]\n",
      "loss: 0.481672  [51200/481450]\n",
      "loss: 0.222217  [54400/481450]\n",
      "loss: 0.359744  [57600/481450]\n",
      "loss: 0.618016  [60800/481450]\n",
      "loss: 0.209504  [64000/481450]\n",
      "loss: 0.242471  [67200/481450]\n",
      "loss: 0.394332  [70400/481450]\n",
      "loss: 0.457287  [73600/481450]\n",
      "loss: 0.374038  [76800/481450]\n",
      "loss: 0.131869  [80000/481450]\n",
      "loss: 0.240543  [83200/481450]\n",
      "loss: 0.536495  [86400/481450]\n",
      "loss: 0.358104  [89600/481450]\n",
      "loss: 0.376876  [92800/481450]\n",
      "loss: 0.197131  [96000/481450]\n",
      "loss: 0.352977  [99200/481450]\n",
      "loss: 0.260690  [102400/481450]\n",
      "loss: 0.331199  [105600/481450]\n",
      "loss: 0.190375  [108800/481450]\n",
      "loss: 0.511498  [112000/481450]\n",
      "loss: 0.282981  [115200/481450]\n",
      "loss: 0.771755  [118400/481450]\n",
      "loss: 0.438058  [121600/481450]\n",
      "loss: 0.429683  [124800/481450]\n",
      "loss: 0.406047  [128000/481450]\n",
      "loss: 0.173681  [131200/481450]\n",
      "loss: 0.212575  [134400/481450]\n",
      "loss: 0.375361  [137600/481450]\n",
      "loss: 0.252116  [140800/481450]\n",
      "loss: 0.262298  [144000/481450]\n",
      "loss: 0.389325  [147200/481450]\n",
      "loss: 0.286971  [150400/481450]\n",
      "loss: 0.128564  [153600/481450]\n",
      "loss: 0.227701  [156800/481450]\n",
      "loss: 0.258018  [160000/481450]\n",
      "loss: 0.308761  [163200/481450]\n",
      "loss: 0.068609  [166400/481450]\n",
      "loss: 0.458878  [169600/481450]\n",
      "loss: 0.286619  [172800/481450]\n",
      "loss: 0.258400  [176000/481450]\n",
      "loss: 0.663798  [179200/481450]\n",
      "loss: 0.336485  [182400/481450]\n",
      "loss: 0.260652  [185600/481450]\n",
      "loss: 0.125635  [188800/481450]\n",
      "loss: 0.286275  [192000/481450]\n",
      "loss: 0.202216  [195200/481450]\n",
      "loss: 0.377732  [198400/481450]\n",
      "loss: 0.309093  [201600/481450]\n",
      "loss: 0.298252  [204800/481450]\n",
      "loss: 0.432613  [208000/481450]\n",
      "loss: 0.146732  [211200/481450]\n",
      "loss: 0.276133  [214400/481450]\n",
      "loss: 0.610117  [217600/481450]\n",
      "loss: 0.255580  [220800/481450]\n",
      "loss: 0.339862  [224000/481450]\n",
      "loss: 0.345836  [227200/481450]\n",
      "loss: 0.148091  [230400/481450]\n",
      "loss: 0.340103  [233600/481450]\n",
      "loss: 0.165992  [236800/481450]\n",
      "loss: 0.247470  [240000/481450]\n",
      "loss: 0.250901  [243200/481450]\n",
      "loss: 0.269208  [246400/481450]\n",
      "loss: 0.253684  [249600/481450]\n",
      "loss: 0.222014  [252800/481450]\n",
      "loss: 0.523653  [256000/481450]\n",
      "loss: 0.201576  [259200/481450]\n",
      "loss: 0.297727  [262400/481450]\n",
      "loss: 0.322908  [265600/481450]\n",
      "loss: 0.352921  [268800/481450]\n",
      "loss: 0.274263  [272000/481450]\n",
      "loss: 0.462742  [275200/481450]\n",
      "loss: 0.370401  [278400/481450]\n",
      "loss: 0.413259  [281600/481450]\n",
      "loss: 0.146943  [284800/481450]\n",
      "loss: 0.293948  [288000/481450]\n",
      "loss: 0.462201  [291200/481450]\n",
      "loss: 0.470409  [294400/481450]\n",
      "loss: 0.333574  [297600/481450]\n",
      "loss: 0.366264  [300800/481450]\n",
      "loss: 0.227564  [304000/481450]\n",
      "loss: 0.271183  [307200/481450]\n",
      "loss: 0.295159  [310400/481450]\n",
      "loss: 0.228874  [313600/481450]\n",
      "loss: 0.340469  [316800/481450]\n",
      "loss: 0.303144  [320000/481450]\n",
      "loss: 0.471553  [323200/481450]\n",
      "loss: 0.439541  [326400/481450]\n",
      "loss: 0.325878  [329600/481450]\n",
      "loss: 0.327401  [332800/481450]\n",
      "loss: 0.351539  [336000/481450]\n",
      "loss: 0.353887  [339200/481450]\n",
      "loss: 0.222156  [342400/481450]\n",
      "loss: 0.319340  [345600/481450]\n",
      "loss: 0.151604  [348800/481450]\n",
      "loss: 0.133993  [352000/481450]\n",
      "loss: 0.315286  [355200/481450]\n",
      "loss: 0.478826  [358400/481450]\n",
      "loss: 0.040784  [361600/481450]\n",
      "loss: 0.623288  [364800/481450]\n",
      "loss: 0.438752  [368000/481450]\n",
      "loss: 0.349671  [371200/481450]\n",
      "loss: 0.407383  [374400/481450]\n",
      "loss: 0.441665  [377600/481450]\n",
      "loss: 0.308552  [380800/481450]\n",
      "loss: 0.484988  [384000/481450]\n",
      "loss: 0.301108  [387200/481450]\n",
      "loss: 0.197499  [390400/481450]\n",
      "loss: 0.207277  [393600/481450]\n",
      "loss: 0.366826  [396800/481450]\n",
      "loss: 0.483109  [400000/481450]\n",
      "loss: 0.556628  [403200/481450]\n",
      "loss: 0.323429  [406400/481450]\n",
      "loss: 0.386207  [409600/481450]\n",
      "loss: 0.108341  [412800/481450]\n",
      "loss: 0.412559  [416000/481450]\n",
      "loss: 0.374188  [419200/481450]\n",
      "loss: 0.370271  [422400/481450]\n",
      "loss: 0.324048  [425600/481450]\n",
      "loss: 0.379607  [428800/481450]\n",
      "loss: 0.190673  [432000/481450]\n",
      "loss: 0.371479  [435200/481450]\n",
      "loss: 0.356496  [438400/481450]\n",
      "loss: 0.354765  [441600/481450]\n",
      "loss: 0.448211  [444800/481450]\n",
      "loss: 0.424620  [448000/481450]\n",
      "loss: 0.240512  [451200/481450]\n",
      "loss: 0.178601  [454400/481450]\n",
      "loss: 0.516066  [457600/481450]\n",
      "loss: 0.201595  [460800/481450]\n",
      "loss: 0.309586  [464000/481450]\n",
      "loss: 0.280328  [467200/481450]\n",
      "loss: 0.437790  [470400/481450]\n",
      "loss: 0.466617  [473600/481450]\n",
      "loss: 0.269677  [476800/481450]\n",
      "loss: 0.260899  [480000/481450]\n",
      "Train Accuracy: 87.4047%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.635968, F1-score: 83.06% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.160823  [    0/481450]\n",
      "loss: 0.559044  [ 3200/481450]\n",
      "loss: 0.236022  [ 6400/481450]\n",
      "loss: 0.333669  [ 9600/481450]\n",
      "loss: 0.112028  [12800/481450]\n",
      "loss: 0.465336  [16000/481450]\n",
      "loss: 0.277035  [19200/481450]\n",
      "loss: 0.242763  [22400/481450]\n",
      "loss: 0.207965  [25600/481450]\n",
      "loss: 0.348230  [28800/481450]\n",
      "loss: 0.415930  [32000/481450]\n",
      "loss: 0.169562  [35200/481450]\n",
      "loss: 0.347031  [38400/481450]\n",
      "loss: 0.199845  [41600/481450]\n",
      "loss: 0.300729  [44800/481450]\n",
      "loss: 0.265424  [48000/481450]\n",
      "loss: 0.188944  [51200/481450]\n",
      "loss: 0.288931  [54400/481450]\n",
      "loss: 0.418969  [57600/481450]\n",
      "loss: 0.544084  [60800/481450]\n",
      "loss: 0.386907  [64000/481450]\n",
      "loss: 0.488378  [67200/481450]\n",
      "loss: 0.531477  [70400/481450]\n",
      "loss: 0.192566  [73600/481450]\n",
      "loss: 0.250230  [76800/481450]\n",
      "loss: 0.546273  [80000/481450]\n",
      "loss: 0.307574  [83200/481450]\n",
      "loss: 0.383162  [86400/481450]\n",
      "loss: 0.198080  [89600/481450]\n",
      "loss: 0.223326  [92800/481450]\n",
      "loss: 0.229291  [96000/481450]\n",
      "loss: 0.281401  [99200/481450]\n",
      "loss: 0.352031  [102400/481450]\n",
      "loss: 0.462937  [105600/481450]\n",
      "loss: 0.364784  [108800/481450]\n",
      "loss: 0.257155  [112000/481450]\n",
      "loss: 0.408386  [115200/481450]\n",
      "loss: 0.364750  [118400/481450]\n",
      "loss: 0.358994  [121600/481450]\n",
      "loss: 0.492568  [124800/481450]\n",
      "loss: 0.301462  [128000/481450]\n",
      "loss: 0.577042  [131200/481450]\n",
      "loss: 0.390288  [134400/481450]\n",
      "loss: 0.436212  [137600/481450]\n",
      "loss: 0.063991  [140800/481450]\n",
      "loss: 0.298202  [144000/481450]\n",
      "loss: 0.440200  [147200/481450]\n",
      "loss: 0.495983  [150400/481450]\n",
      "loss: 0.410725  [153600/481450]\n",
      "loss: 0.354335  [156800/481450]\n",
      "loss: 0.473168  [160000/481450]\n",
      "loss: 0.415230  [163200/481450]\n",
      "loss: 0.290880  [166400/481450]\n",
      "loss: 0.403445  [169600/481450]\n",
      "loss: 0.237849  [172800/481450]\n",
      "loss: 0.326348  [176000/481450]\n",
      "loss: 0.320568  [179200/481450]\n",
      "loss: 0.287217  [182400/481450]\n",
      "loss: 0.104338  [185600/481450]\n",
      "loss: 0.553913  [188800/481450]\n",
      "loss: 0.159429  [192000/481450]\n",
      "loss: 0.290949  [195200/481450]\n",
      "loss: 0.179779  [198400/481450]\n",
      "loss: 0.131189  [201600/481450]\n",
      "loss: 0.245985  [204800/481450]\n",
      "loss: 0.337707  [208000/481450]\n",
      "loss: 0.365818  [211200/481450]\n",
      "loss: 0.457456  [214400/481450]\n",
      "loss: 0.455280  [217600/481450]\n",
      "loss: 0.307855  [220800/481450]\n",
      "loss: 0.320631  [224000/481450]\n",
      "loss: 0.423020  [227200/481450]\n",
      "loss: 0.202121  [230400/481450]\n",
      "loss: 0.251367  [233600/481450]\n",
      "loss: 0.208096  [236800/481450]\n",
      "loss: 0.458192  [240000/481450]\n",
      "loss: 0.181587  [243200/481450]\n",
      "loss: 0.397415  [246400/481450]\n",
      "loss: 0.393110  [249600/481450]\n",
      "loss: 0.329109  [252800/481450]\n",
      "loss: 0.375887  [256000/481450]\n",
      "loss: 0.565571  [259200/481450]\n",
      "loss: 0.251024  [262400/481450]\n",
      "loss: 0.571536  [265600/481450]\n",
      "loss: 0.250125  [268800/481450]\n",
      "loss: 0.152393  [272000/481450]\n",
      "loss: 0.239329  [275200/481450]\n",
      "loss: 0.267603  [278400/481450]\n",
      "loss: 0.193583  [281600/481450]\n",
      "loss: 0.462403  [284800/481450]\n",
      "loss: 0.261682  [288000/481450]\n",
      "loss: 0.378599  [291200/481450]\n",
      "loss: 0.224954  [294400/481450]\n",
      "loss: 0.302569  [297600/481450]\n",
      "loss: 0.402441  [300800/481450]\n",
      "loss: 0.332118  [304000/481450]\n",
      "loss: 0.365109  [307200/481450]\n",
      "loss: 0.222871  [310400/481450]\n",
      "loss: 0.178192  [313600/481450]\n",
      "loss: 0.332528  [316800/481450]\n",
      "loss: 0.155529  [320000/481450]\n",
      "loss: 0.228705  [323200/481450]\n",
      "loss: 0.195009  [326400/481450]\n",
      "loss: 0.585707  [329600/481450]\n",
      "loss: 0.239491  [332800/481450]\n",
      "loss: 0.289595  [336000/481450]\n",
      "loss: 0.248216  [339200/481450]\n",
      "loss: 0.393081  [342400/481450]\n",
      "loss: 0.453739  [345600/481450]\n",
      "loss: 0.048621  [348800/481450]\n",
      "loss: 0.346140  [352000/481450]\n",
      "loss: 0.287440  [355200/481450]\n",
      "loss: 0.192703  [358400/481450]\n",
      "loss: 0.357491  [361600/481450]\n",
      "loss: 0.231085  [364800/481450]\n",
      "loss: 0.242858  [368000/481450]\n",
      "loss: 0.400469  [371200/481450]\n",
      "loss: 0.330776  [374400/481450]\n",
      "loss: 0.352031  [377600/481450]\n",
      "loss: 0.468113  [380800/481450]\n",
      "loss: 0.227595  [384000/481450]\n",
      "loss: 0.131211  [387200/481450]\n",
      "loss: 0.522202  [390400/481450]\n",
      "loss: 0.449910  [393600/481450]\n",
      "loss: 0.183307  [396800/481450]\n",
      "loss: 0.293532  [400000/481450]\n",
      "loss: 0.208090  [403200/481450]\n",
      "loss: 0.347450  [406400/481450]\n",
      "loss: 0.215694  [409600/481450]\n",
      "loss: 0.445594  [412800/481450]\n",
      "loss: 0.259005  [416000/481450]\n",
      "loss: 0.215353  [419200/481450]\n",
      "loss: 0.472623  [422400/481450]\n",
      "loss: 0.506940  [425600/481450]\n",
      "loss: 0.469169  [428800/481450]\n",
      "loss: 0.422311  [432000/481450]\n",
      "loss: 0.134284  [435200/481450]\n",
      "loss: 0.233688  [438400/481450]\n",
      "loss: 0.524874  [441600/481450]\n",
      "loss: 0.387429  [444800/481450]\n",
      "loss: 0.290550  [448000/481450]\n",
      "loss: 0.305517  [451200/481450]\n",
      "loss: 0.355097  [454400/481450]\n",
      "loss: 0.173061  [457600/481450]\n",
      "loss: 0.155849  [460800/481450]\n",
      "loss: 0.344811  [464000/481450]\n",
      "loss: 0.110394  [467200/481450]\n",
      "loss: 0.330225  [470400/481450]\n",
      "loss: 0.241586  [473600/481450]\n",
      "loss: 0.290092  [476800/481450]\n",
      "loss: 0.334270  [480000/481450]\n",
      "Train Accuracy: 87.7057%\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.696134, F1-score: 83.30% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.222603  [    0/481450]\n",
      "loss: 0.282730  [ 3200/481450]\n",
      "loss: 0.383208  [ 6400/481450]\n",
      "loss: 0.281340  [ 9600/481450]\n",
      "loss: 0.109638  [12800/481450]\n",
      "loss: 0.555395  [16000/481450]\n",
      "loss: 0.280424  [19200/481450]\n",
      "loss: 0.303959  [22400/481450]\n",
      "loss: 0.250952  [25600/481450]\n",
      "loss: 0.143322  [28800/481450]\n",
      "loss: 0.248372  [32000/481450]\n",
      "loss: 0.353707  [35200/481450]\n",
      "loss: 0.236866  [38400/481450]\n",
      "loss: 0.441330  [41600/481450]\n",
      "loss: 0.207498  [44800/481450]\n",
      "loss: 0.258489  [48000/481450]\n",
      "loss: 0.197515  [51200/481450]\n",
      "loss: 0.382242  [54400/481450]\n",
      "loss: 0.223090  [57600/481450]\n",
      "loss: 0.275405  [60800/481450]\n",
      "loss: 0.376493  [64000/481450]\n",
      "loss: 0.191542  [67200/481450]\n",
      "loss: 0.304260  [70400/481450]\n",
      "loss: 0.438819  [73600/481450]\n",
      "loss: 0.265218  [76800/481450]\n",
      "loss: 0.622904  [80000/481450]\n",
      "loss: 0.358842  [83200/481450]\n",
      "loss: 0.119415  [86400/481450]\n",
      "loss: 0.486501  [89600/481450]\n",
      "loss: 0.206165  [92800/481450]\n",
      "loss: 0.280721  [96000/481450]\n",
      "loss: 0.132187  [99200/481450]\n",
      "loss: 0.134143  [102400/481450]\n",
      "loss: 0.373057  [105600/481450]\n",
      "loss: 0.205072  [108800/481450]\n",
      "loss: 0.287829  [112000/481450]\n",
      "loss: 0.241221  [115200/481450]\n",
      "loss: 0.201610  [118400/481450]\n",
      "loss: 0.459573  [121600/481450]\n",
      "loss: 0.362794  [124800/481450]\n",
      "loss: 0.461262  [128000/481450]\n",
      "loss: 0.120697  [131200/481450]\n",
      "loss: 0.269158  [134400/481450]\n",
      "loss: 0.483019  [137600/481450]\n",
      "loss: 0.242718  [140800/481450]\n",
      "loss: 0.378421  [144000/481450]\n",
      "loss: 0.286510  [147200/481450]\n",
      "loss: 0.349330  [150400/481450]\n",
      "loss: 0.162449  [153600/481450]\n",
      "loss: 0.111844  [156800/481450]\n",
      "loss: 0.413897  [160000/481450]\n",
      "loss: 0.398848  [163200/481450]\n",
      "loss: 0.241041  [166400/481450]\n",
      "loss: 0.187767  [169600/481450]\n",
      "loss: 0.577785  [172800/481450]\n",
      "loss: 0.188876  [176000/481450]\n",
      "loss: 0.350673  [179200/481450]\n",
      "loss: 0.307255  [182400/481450]\n",
      "loss: 0.135380  [185600/481450]\n",
      "loss: 0.266123  [188800/481450]\n",
      "loss: 0.501113  [192000/481450]\n",
      "loss: 0.199339  [195200/481450]\n",
      "loss: 0.338799  [198400/481450]\n",
      "loss: 0.242537  [201600/481450]\n",
      "loss: 0.292994  [204800/481450]\n",
      "loss: 0.483129  [208000/481450]\n",
      "loss: 0.247428  [211200/481450]\n",
      "loss: 0.430015  [214400/481450]\n",
      "loss: 0.263333  [217600/481450]\n",
      "loss: 0.248774  [220800/481450]\n",
      "loss: 0.389415  [224000/481450]\n",
      "loss: 0.401672  [227200/481450]\n",
      "loss: 0.193680  [230400/481450]\n",
      "loss: 0.228596  [233600/481450]\n",
      "loss: 0.210971  [236800/481450]\n",
      "loss: 0.181319  [240000/481450]\n",
      "loss: 0.147759  [243200/481450]\n",
      "loss: 0.070949  [246400/481450]\n",
      "loss: 0.299580  [249600/481450]\n",
      "loss: 0.300195  [252800/481450]\n",
      "loss: 0.306841  [256000/481450]\n",
      "loss: 0.305600  [259200/481450]\n",
      "loss: 0.398301  [262400/481450]\n",
      "loss: 0.291415  [265600/481450]\n",
      "loss: 0.406751  [268800/481450]\n",
      "loss: 0.374682  [272000/481450]\n",
      "loss: 0.231439  [275200/481450]\n",
      "loss: 0.355251  [278400/481450]\n",
      "loss: 0.208159  [281600/481450]\n",
      "loss: 0.197966  [284800/481450]\n",
      "loss: 0.255259  [288000/481450]\n",
      "loss: 0.211643  [291200/481450]\n",
      "loss: 0.422668  [294400/481450]\n",
      "loss: 0.317727  [297600/481450]\n",
      "loss: 0.542088  [300800/481450]\n",
      "loss: 0.330508  [304000/481450]\n",
      "loss: 0.316103  [307200/481450]\n",
      "loss: 0.398216  [310400/481450]\n",
      "loss: 0.384182  [313600/481450]\n",
      "loss: 0.622180  [316800/481450]\n",
      "loss: 0.189130  [320000/481450]\n",
      "loss: 0.156193  [323200/481450]\n",
      "loss: 0.169449  [326400/481450]\n",
      "loss: 0.426701  [329600/481450]\n",
      "loss: 0.225329  [332800/481450]\n",
      "loss: 0.279607  [336000/481450]\n",
      "loss: 0.444056  [339200/481450]\n",
      "loss: 0.143235  [342400/481450]\n",
      "loss: 0.293017  [345600/481450]\n",
      "loss: 0.557140  [348800/481450]\n",
      "loss: 0.261319  [352000/481450]\n",
      "loss: 0.184673  [355200/481450]\n",
      "loss: 0.457875  [358400/481450]\n",
      "loss: 0.280348  [361600/481450]\n",
      "loss: 0.278561  [364800/481450]\n",
      "loss: 0.156551  [368000/481450]\n",
      "loss: 0.261523  [371200/481450]\n",
      "loss: 0.217382  [374400/481450]\n",
      "loss: 0.448210  [377600/481450]\n",
      "loss: 0.264456  [380800/481450]\n",
      "loss: 0.374853  [384000/481450]\n",
      "loss: 0.329661  [387200/481450]\n",
      "loss: 0.326478  [390400/481450]\n",
      "loss: 0.356339  [393600/481450]\n",
      "loss: 0.392016  [396800/481450]\n",
      "loss: 0.289476  [400000/481450]\n",
      "loss: 0.413596  [403200/481450]\n",
      "loss: 0.105291  [406400/481450]\n",
      "loss: 0.222328  [409600/481450]\n",
      "loss: 0.153602  [412800/481450]\n",
      "loss: 0.278695  [416000/481450]\n",
      "loss: 0.345937  [419200/481450]\n",
      "loss: 0.203906  [422400/481450]\n",
      "loss: 0.230771  [425600/481450]\n",
      "loss: 0.227658  [428800/481450]\n",
      "loss: 0.258815  [432000/481450]\n",
      "loss: 0.482382  [435200/481450]\n",
      "loss: 0.232265  [438400/481450]\n",
      "loss: 0.246588  [441600/481450]\n",
      "loss: 0.380574  [444800/481450]\n",
      "loss: 0.276884  [448000/481450]\n",
      "loss: 0.319344  [451200/481450]\n",
      "loss: 0.294935  [454400/481450]\n",
      "loss: 0.398537  [457600/481450]\n",
      "loss: 0.222453  [460800/481450]\n",
      "loss: 0.423355  [464000/481450]\n",
      "loss: 0.542467  [467200/481450]\n",
      "loss: 0.247346  [470400/481450]\n",
      "loss: 0.194813  [473600/481450]\n",
      "loss: 0.419055  [476800/481450]\n",
      "loss: 0.283642  [480000/481450]\n",
      "Train Accuracy: 87.8758%\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.657049, F1-score: 84.84% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.419702  [    0/481450]\n",
      "loss: 0.197737  [ 3200/481450]\n",
      "loss: 0.219143  [ 6400/481450]\n",
      "loss: 0.196394  [ 9600/481450]\n",
      "loss: 0.219094  [12800/481450]\n",
      "loss: 0.347015  [16000/481450]\n",
      "loss: 0.316044  [19200/481450]\n",
      "loss: 0.203778  [22400/481450]\n",
      "loss: 0.272968  [25600/481450]\n",
      "loss: 0.323087  [28800/481450]\n",
      "loss: 0.153578  [32000/481450]\n",
      "loss: 0.388682  [35200/481450]\n",
      "loss: 0.177610  [38400/481450]\n",
      "loss: 0.256738  [41600/481450]\n",
      "loss: 0.302946  [44800/481450]\n",
      "loss: 0.291084  [48000/481450]\n",
      "loss: 0.336028  [51200/481450]\n",
      "loss: 0.097373  [54400/481450]\n",
      "loss: 0.255851  [57600/481450]\n",
      "loss: 0.228311  [60800/481450]\n",
      "loss: 0.181717  [64000/481450]\n",
      "loss: 0.486021  [67200/481450]\n",
      "loss: 0.210267  [70400/481450]\n",
      "loss: 0.314952  [73600/481450]\n",
      "loss: 0.308502  [76800/481450]\n",
      "loss: 0.341873  [80000/481450]\n",
      "loss: 0.477124  [83200/481450]\n",
      "loss: 0.337258  [86400/481450]\n",
      "loss: 0.201487  [89600/481450]\n",
      "loss: 0.388156  [92800/481450]\n",
      "loss: 0.322105  [96000/481450]\n",
      "loss: 0.080960  [99200/481450]\n",
      "loss: 0.245475  [102400/481450]\n",
      "loss: 0.434326  [105600/481450]\n",
      "loss: 0.386413  [108800/481450]\n",
      "loss: 0.256292  [112000/481450]\n",
      "loss: 0.281817  [115200/481450]\n",
      "loss: 0.359093  [118400/481450]\n",
      "loss: 0.277592  [121600/481450]\n",
      "loss: 0.283109  [124800/481450]\n",
      "loss: 0.208345  [128000/481450]\n",
      "loss: 0.255298  [131200/481450]\n",
      "loss: 0.339650  [134400/481450]\n",
      "loss: 0.159482  [137600/481450]\n",
      "loss: 0.298013  [140800/481450]\n",
      "loss: 0.419061  [144000/481450]\n",
      "loss: 0.292406  [147200/481450]\n",
      "loss: 0.185373  [150400/481450]\n",
      "loss: 0.461093  [153600/481450]\n",
      "loss: 0.512537  [156800/481450]\n",
      "loss: 0.163285  [160000/481450]\n",
      "loss: 0.206241  [163200/481450]\n",
      "loss: 0.191409  [166400/481450]\n",
      "loss: 0.248651  [169600/481450]\n",
      "loss: 0.129881  [172800/481450]\n",
      "loss: 0.640453  [176000/481450]\n",
      "loss: 0.144791  [179200/481450]\n",
      "loss: 0.531367  [182400/481450]\n",
      "loss: 0.208940  [185600/481450]\n",
      "loss: 0.236590  [188800/481450]\n",
      "loss: 0.320493  [192000/481450]\n",
      "loss: 0.210651  [195200/481450]\n",
      "loss: 0.452167  [198400/481450]\n",
      "loss: 0.208630  [201600/481450]\n",
      "loss: 0.268688  [204800/481450]\n",
      "loss: 0.296589  [208000/481450]\n",
      "loss: 0.500989  [211200/481450]\n",
      "loss: 0.380444  [214400/481450]\n",
      "loss: 0.252760  [217600/481450]\n",
      "loss: 0.161706  [220800/481450]\n",
      "loss: 0.454852  [224000/481450]\n",
      "loss: 0.331506  [227200/481450]\n",
      "loss: 0.171391  [230400/481450]\n",
      "loss: 0.288653  [233600/481450]\n",
      "loss: 0.176811  [236800/481450]\n",
      "loss: 0.253659  [240000/481450]\n",
      "loss: 0.160208  [243200/481450]\n",
      "loss: 0.525454  [246400/481450]\n",
      "loss: 0.432065  [249600/481450]\n",
      "loss: 0.332082  [252800/481450]\n",
      "loss: 0.180302  [256000/481450]\n",
      "loss: 0.526030  [259200/481450]\n",
      "loss: 0.150323  [262400/481450]\n",
      "loss: 0.253940  [265600/481450]\n",
      "loss: 0.208982  [268800/481450]\n",
      "loss: 0.116121  [272000/481450]\n",
      "loss: 0.521497  [275200/481450]\n",
      "loss: 0.101758  [278400/481450]\n",
      "loss: 0.280859  [281600/481450]\n",
      "loss: 0.378604  [284800/481450]\n",
      "loss: 0.313144  [288000/481450]\n",
      "loss: 0.397361  [291200/481450]\n",
      "loss: 0.230849  [294400/481450]\n",
      "loss: 0.262313  [297600/481450]\n",
      "loss: 0.447111  [300800/481450]\n",
      "loss: 0.269083  [304000/481450]\n",
      "loss: 0.102912  [307200/481450]\n",
      "loss: 0.349605  [310400/481450]\n",
      "loss: 0.357992  [313600/481450]\n",
      "loss: 0.217093  [316800/481450]\n",
      "loss: 0.244980  [320000/481450]\n",
      "loss: 0.361331  [323200/481450]\n",
      "loss: 0.309847  [326400/481450]\n",
      "loss: 0.232385  [329600/481450]\n",
      "loss: 0.289040  [332800/481450]\n",
      "loss: 0.316747  [336000/481450]\n",
      "loss: 0.494073  [339200/481450]\n",
      "loss: 0.484442  [342400/481450]\n",
      "loss: 0.261134  [345600/481450]\n",
      "loss: 0.200713  [348800/481450]\n",
      "loss: 0.304667  [352000/481450]\n",
      "loss: 0.365115  [355200/481450]\n",
      "loss: 0.408029  [358400/481450]\n",
      "loss: 0.241719  [361600/481450]\n",
      "loss: 0.295249  [364800/481450]\n",
      "loss: 0.190951  [368000/481450]\n",
      "loss: 0.226904  [371200/481450]\n",
      "loss: 0.207620  [374400/481450]\n",
      "loss: 0.421675  [377600/481450]\n",
      "loss: 0.258471  [380800/481450]\n",
      "loss: 0.247753  [384000/481450]\n",
      "loss: 0.264601  [387200/481450]\n",
      "loss: 0.359390  [390400/481450]\n",
      "loss: 0.187171  [393600/481450]\n",
      "loss: 0.202272  [396800/481450]\n",
      "loss: 0.172943  [400000/481450]\n",
      "loss: 0.276916  [403200/481450]\n",
      "loss: 0.400542  [406400/481450]\n",
      "loss: 0.196586  [409600/481450]\n",
      "loss: 0.588161  [412800/481450]\n",
      "loss: 0.400603  [416000/481450]\n",
      "loss: 0.176980  [419200/481450]\n",
      "loss: 0.157921  [422400/481450]\n",
      "loss: 0.166580  [425600/481450]\n",
      "loss: 0.342404  [428800/481450]\n",
      "loss: 0.264355  [432000/481450]\n",
      "loss: 0.439234  [435200/481450]\n",
      "loss: 0.317926  [438400/481450]\n",
      "loss: 0.396040  [441600/481450]\n",
      "loss: 0.224627  [444800/481450]\n",
      "loss: 0.392844  [448000/481450]\n",
      "loss: 0.414730  [451200/481450]\n",
      "loss: 0.212887  [454400/481450]\n",
      "loss: 0.284401  [457600/481450]\n",
      "loss: 0.294633  [460800/481450]\n",
      "loss: 0.274651  [464000/481450]\n",
      "loss: 0.244890  [467200/481450]\n",
      "loss: 0.440279  [470400/481450]\n",
      "loss: 0.293791  [473600/481450]\n",
      "loss: 0.638425  [476800/481450]\n",
      "loss: 0.240648  [480000/481450]\n",
      "Train Accuracy: 88.1319%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.617638, F1-score: 85.58% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.410978  [    0/481450]\n",
      "loss: 0.308753  [ 3200/481450]\n",
      "loss: 0.167475  [ 6400/481450]\n",
      "loss: 0.084385  [ 9600/481450]\n",
      "loss: 0.436976  [12800/481450]\n",
      "loss: 0.180575  [16000/481450]\n",
      "loss: 0.480494  [19200/481450]\n",
      "loss: 0.147481  [22400/481450]\n",
      "loss: 0.221523  [25600/481450]\n",
      "loss: 0.120790  [28800/481450]\n",
      "loss: 0.313212  [32000/481450]\n",
      "loss: 0.522912  [35200/481450]\n",
      "loss: 0.318531  [38400/481450]\n",
      "loss: 0.253984  [41600/481450]\n",
      "loss: 0.257137  [44800/481450]\n",
      "loss: 0.161340  [48000/481450]\n",
      "loss: 0.249833  [51200/481450]\n",
      "loss: 0.193897  [54400/481450]\n",
      "loss: 0.107891  [57600/481450]\n",
      "loss: 0.540851  [60800/481450]\n",
      "loss: 0.172848  [64000/481450]\n",
      "loss: 0.029686  [67200/481450]\n",
      "loss: 0.143304  [70400/481450]\n",
      "loss: 0.248292  [73600/481450]\n",
      "loss: 0.348622  [76800/481450]\n",
      "loss: 0.157416  [80000/481450]\n",
      "loss: 0.510198  [83200/481450]\n",
      "loss: 0.394262  [86400/481450]\n",
      "loss: 0.179118  [89600/481450]\n",
      "loss: 0.127442  [92800/481450]\n",
      "loss: 0.157055  [96000/481450]\n",
      "loss: 0.334463  [99200/481450]\n",
      "loss: 0.223188  [102400/481450]\n",
      "loss: 0.236613  [105600/481450]\n",
      "loss: 0.293459  [108800/481450]\n",
      "loss: 0.323098  [112000/481450]\n",
      "loss: 0.272755  [115200/481450]\n",
      "loss: 0.241188  [118400/481450]\n",
      "loss: 0.188368  [121600/481450]\n",
      "loss: 0.232509  [124800/481450]\n",
      "loss: 0.249754  [128000/481450]\n",
      "loss: 0.335935  [131200/481450]\n",
      "loss: 0.314202  [134400/481450]\n",
      "loss: 0.222475  [137600/481450]\n",
      "loss: 0.319002  [140800/481450]\n",
      "loss: 0.269964  [144000/481450]\n",
      "loss: 0.215440  [147200/481450]\n",
      "loss: 0.381957  [150400/481450]\n",
      "loss: 0.237652  [153600/481450]\n",
      "loss: 0.288838  [156800/481450]\n",
      "loss: 0.337784  [160000/481450]\n",
      "loss: 0.380904  [163200/481450]\n",
      "loss: 0.200456  [166400/481450]\n",
      "loss: 0.299178  [169600/481450]\n",
      "loss: 0.210661  [172800/481450]\n",
      "loss: 0.404057  [176000/481450]\n",
      "loss: 0.157118  [179200/481450]\n",
      "loss: 0.283459  [182400/481450]\n",
      "loss: 0.469074  [185600/481450]\n",
      "loss: 0.375861  [188800/481450]\n",
      "loss: 0.088970  [192000/481450]\n",
      "loss: 0.289402  [195200/481450]\n",
      "loss: 0.249992  [198400/481450]\n",
      "loss: 0.262674  [201600/481450]\n",
      "loss: 0.325830  [204800/481450]\n",
      "loss: 0.309007  [208000/481450]\n",
      "loss: 0.383569  [211200/481450]\n",
      "loss: 0.351242  [214400/481450]\n",
      "loss: 0.229807  [217600/481450]\n",
      "loss: 0.280269  [220800/481450]\n",
      "loss: 0.431216  [224000/481450]\n",
      "loss: 0.323381  [227200/481450]\n",
      "loss: 0.316708  [230400/481450]\n",
      "loss: 0.098215  [233600/481450]\n",
      "loss: 0.269144  [236800/481450]\n",
      "loss: 0.299465  [240000/481450]\n",
      "loss: 0.163386  [243200/481450]\n",
      "loss: 0.255622  [246400/481450]\n",
      "loss: 0.172888  [249600/481450]\n",
      "loss: 0.600578  [252800/481450]\n",
      "loss: 0.204769  [256000/481450]\n",
      "loss: 0.212122  [259200/481450]\n",
      "loss: 0.391204  [262400/481450]\n",
      "loss: 0.530261  [265600/481450]\n",
      "loss: 0.344890  [268800/481450]\n",
      "loss: 0.304351  [272000/481450]\n",
      "loss: 0.421921  [275200/481450]\n",
      "loss: 0.368618  [278400/481450]\n",
      "loss: 0.298915  [281600/481450]\n",
      "loss: 0.401398  [284800/481450]\n",
      "loss: 0.315793  [288000/481450]\n",
      "loss: 0.205786  [291200/481450]\n",
      "loss: 0.389021  [294400/481450]\n",
      "loss: 0.208388  [297600/481450]\n",
      "loss: 0.441057  [300800/481450]\n",
      "loss: 0.286722  [304000/481450]\n",
      "loss: 0.268268  [307200/481450]\n",
      "loss: 0.329712  [310400/481450]\n",
      "loss: 0.399835  [313600/481450]\n",
      "loss: 0.445098  [316800/481450]\n",
      "loss: 0.369633  [320000/481450]\n",
      "loss: 0.214813  [323200/481450]\n",
      "loss: 0.082512  [326400/481450]\n",
      "loss: 0.387063  [329600/481450]\n",
      "loss: 0.353488  [332800/481450]\n",
      "loss: 0.079272  [336000/481450]\n",
      "loss: 0.086270  [339200/481450]\n",
      "loss: 0.508493  [342400/481450]\n",
      "loss: 0.248172  [345600/481450]\n",
      "loss: 0.389397  [348800/481450]\n",
      "loss: 0.413723  [352000/481450]\n",
      "loss: 0.532035  [355200/481450]\n",
      "loss: 0.268273  [358400/481450]\n",
      "loss: 0.272568  [361600/481450]\n",
      "loss: 0.273443  [364800/481450]\n",
      "loss: 0.197192  [368000/481450]\n",
      "loss: 0.165167  [371200/481450]\n",
      "loss: 0.366437  [374400/481450]\n",
      "loss: 0.437194  [377600/481450]\n",
      "loss: 0.122127  [380800/481450]\n",
      "loss: 0.333772  [384000/481450]\n",
      "loss: 0.316382  [387200/481450]\n",
      "loss: 0.108802  [390400/481450]\n",
      "loss: 0.196091  [393600/481450]\n",
      "loss: 0.189035  [396800/481450]\n",
      "loss: 0.315811  [400000/481450]\n",
      "loss: 0.186116  [403200/481450]\n",
      "loss: 0.144386  [406400/481450]\n",
      "loss: 0.196073  [409600/481450]\n",
      "loss: 0.246700  [412800/481450]\n",
      "loss: 0.179605  [416000/481450]\n",
      "loss: 0.373398  [419200/481450]\n",
      "loss: 0.500601  [422400/481450]\n",
      "loss: 0.231533  [425600/481450]\n",
      "loss: 0.230687  [428800/481450]\n",
      "loss: 0.384986  [432000/481450]\n",
      "loss: 0.243760  [435200/481450]\n",
      "loss: 0.424164  [438400/481450]\n",
      "loss: 0.520046  [441600/481450]\n",
      "loss: 0.344315  [444800/481450]\n",
      "loss: 0.305853  [448000/481450]\n",
      "loss: 0.454836  [451200/481450]\n",
      "loss: 0.358472  [454400/481450]\n",
      "loss: 0.508460  [457600/481450]\n",
      "loss: 0.324390  [460800/481450]\n",
      "loss: 0.272574  [464000/481450]\n",
      "loss: 0.310520  [467200/481450]\n",
      "loss: 0.339054  [470400/481450]\n",
      "loss: 0.457200  [473600/481450]\n",
      "loss: 0.232500  [476800/481450]\n",
      "loss: 0.359617  [480000/481450]\n",
      "Train Accuracy: 88.2964%\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.776986, F1-score: 83.32% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.288287  [    0/481450]\n",
      "loss: 0.219179  [ 3200/481450]\n",
      "loss: 0.125564  [ 6400/481450]\n",
      "loss: 0.206708  [ 9600/481450]\n",
      "loss: 0.257457  [12800/481450]\n",
      "loss: 0.194814  [16000/481450]\n",
      "loss: 0.456337  [19200/481450]\n",
      "loss: 0.254336  [22400/481450]\n",
      "loss: 0.503128  [25600/481450]\n",
      "loss: 0.184528  [28800/481450]\n",
      "loss: 0.334679  [32000/481450]\n",
      "loss: 0.534953  [35200/481450]\n",
      "loss: 0.276846  [38400/481450]\n",
      "loss: 0.333226  [41600/481450]\n",
      "loss: 0.287585  [44800/481450]\n",
      "loss: 0.160000  [48000/481450]\n",
      "loss: 0.284135  [51200/481450]\n",
      "loss: 0.308967  [54400/481450]\n",
      "loss: 0.500894  [57600/481450]\n",
      "loss: 0.419715  [60800/481450]\n",
      "loss: 0.227920  [64000/481450]\n",
      "loss: 0.338203  [67200/481450]\n",
      "loss: 0.227041  [70400/481450]\n",
      "loss: 0.539020  [73600/481450]\n",
      "loss: 0.182038  [76800/481450]\n",
      "loss: 0.264838  [80000/481450]\n",
      "loss: 0.335221  [83200/481450]\n",
      "loss: 0.133770  [86400/481450]\n",
      "loss: 0.178069  [89600/481450]\n",
      "loss: 0.200077  [92800/481450]\n",
      "loss: 0.158882  [96000/481450]\n",
      "loss: 0.292857  [99200/481450]\n",
      "loss: 0.376068  [102400/481450]\n",
      "loss: 0.603767  [105600/481450]\n",
      "loss: 0.369965  [108800/481450]\n",
      "loss: 0.419608  [112000/481450]\n",
      "loss: 0.117959  [115200/481450]\n",
      "loss: 0.142868  [118400/481450]\n",
      "loss: 0.162063  [121600/481450]\n",
      "loss: 0.167199  [124800/481450]\n",
      "loss: 0.360902  [128000/481450]\n",
      "loss: 0.190382  [131200/481450]\n",
      "loss: 0.237228  [134400/481450]\n",
      "loss: 0.060819  [137600/481450]\n",
      "loss: 0.274858  [140800/481450]\n",
      "loss: 0.258602  [144000/481450]\n",
      "loss: 0.140221  [147200/481450]\n",
      "loss: 0.332479  [150400/481450]\n",
      "loss: 0.249458  [153600/481450]\n",
      "loss: 0.332819  [156800/481450]\n",
      "loss: 0.266406  [160000/481450]\n",
      "loss: 0.381792  [163200/481450]\n",
      "loss: 0.261406  [166400/481450]\n",
      "loss: 0.508879  [169600/481450]\n",
      "loss: 0.095263  [172800/481450]\n",
      "loss: 0.305379  [176000/481450]\n",
      "loss: 0.463228  [179200/481450]\n",
      "loss: 0.324559  [182400/481450]\n",
      "loss: 0.191286  [185600/481450]\n",
      "loss: 0.185741  [188800/481450]\n",
      "loss: 0.299126  [192000/481450]\n",
      "loss: 0.519121  [195200/481450]\n",
      "loss: 0.435503  [198400/481450]\n",
      "loss: 0.355334  [201600/481450]\n",
      "loss: 0.259254  [204800/481450]\n",
      "loss: 0.297162  [208000/481450]\n",
      "loss: 0.135451  [211200/481450]\n",
      "loss: 0.246844  [214400/481450]\n",
      "loss: 0.383366  [217600/481450]\n",
      "loss: 0.272461  [220800/481450]\n",
      "loss: 0.287839  [224000/481450]\n",
      "loss: 0.127811  [227200/481450]\n",
      "loss: 0.202380  [230400/481450]\n",
      "loss: 0.299473  [233600/481450]\n",
      "loss: 0.299711  [236800/481450]\n",
      "loss: 0.420473  [240000/481450]\n",
      "loss: 0.465000  [243200/481450]\n",
      "loss: 0.183332  [246400/481450]\n",
      "loss: 0.408144  [249600/481450]\n",
      "loss: 0.179906  [252800/481450]\n",
      "loss: 0.152128  [256000/481450]\n",
      "loss: 0.449475  [259200/481450]\n",
      "loss: 0.303110  [262400/481450]\n",
      "loss: 0.399872  [265600/481450]\n",
      "loss: 0.376060  [268800/481450]\n",
      "loss: 0.329767  [272000/481450]\n",
      "loss: 0.289173  [275200/481450]\n",
      "loss: 0.295880  [278400/481450]\n",
      "loss: 0.189157  [281600/481450]\n",
      "loss: 0.383494  [284800/481450]\n",
      "loss: 0.226341  [288000/481450]\n",
      "loss: 0.260196  [291200/481450]\n",
      "loss: 0.301958  [294400/481450]\n",
      "loss: 0.173082  [297600/481450]\n",
      "loss: 0.347434  [300800/481450]\n",
      "loss: 0.123622  [304000/481450]\n",
      "loss: 0.022020  [307200/481450]\n",
      "loss: 0.172770  [310400/481450]\n",
      "loss: 0.444790  [313600/481450]\n",
      "loss: 0.438458  [316800/481450]\n",
      "loss: 0.344540  [320000/481450]\n",
      "loss: 0.246164  [323200/481450]\n",
      "loss: 0.352688  [326400/481450]\n",
      "loss: 0.229708  [329600/481450]\n",
      "loss: 0.282830  [332800/481450]\n",
      "loss: 0.290970  [336000/481450]\n",
      "loss: 0.097920  [339200/481450]\n",
      "loss: 0.225426  [342400/481450]\n",
      "loss: 0.298965  [345600/481450]\n",
      "loss: 0.278712  [348800/481450]\n",
      "loss: 0.214651  [352000/481450]\n",
      "loss: 0.339864  [355200/481450]\n",
      "loss: 0.368561  [358400/481450]\n",
      "loss: 0.193872  [361600/481450]\n",
      "loss: 0.236549  [364800/481450]\n",
      "loss: 0.245025  [368000/481450]\n",
      "loss: 0.443819  [371200/481450]\n",
      "loss: 0.320842  [374400/481450]\n",
      "loss: 0.432676  [377600/481450]\n",
      "loss: 0.356991  [380800/481450]\n",
      "loss: 0.241929  [384000/481450]\n",
      "loss: 0.314309  [387200/481450]\n",
      "loss: 0.285188  [390400/481450]\n",
      "loss: 0.275123  [393600/481450]\n",
      "loss: 0.132734  [396800/481450]\n",
      "loss: 0.242486  [400000/481450]\n",
      "loss: 0.169777  [403200/481450]\n",
      "loss: 0.399032  [406400/481450]\n",
      "loss: 0.193572  [409600/481450]\n",
      "loss: 0.416889  [412800/481450]\n",
      "loss: 0.146187  [416000/481450]\n",
      "loss: 0.243995  [419200/481450]\n",
      "loss: 0.241721  [422400/481450]\n",
      "loss: 0.365133  [425600/481450]\n",
      "loss: 0.232938  [428800/481450]\n",
      "loss: 0.094756  [432000/481450]\n",
      "loss: 0.190170  [435200/481450]\n",
      "loss: 0.196000  [438400/481450]\n",
      "loss: 0.652522  [441600/481450]\n",
      "loss: 0.347500  [444800/481450]\n",
      "loss: 0.369677  [448000/481450]\n",
      "loss: 0.199330  [451200/481450]\n",
      "loss: 0.191191  [454400/481450]\n",
      "loss: 0.377739  [457600/481450]\n",
      "loss: 0.127689  [460800/481450]\n",
      "loss: 0.410464  [464000/481450]\n",
      "loss: 0.226463  [467200/481450]\n",
      "loss: 0.306218  [470400/481450]\n",
      "loss: 0.591022  [473600/481450]\n",
      "loss: 0.535132  [476800/481450]\n",
      "loss: 0.157863  [480000/481450]\n",
      "Train Accuracy: 88.5016%\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.807938, F1-score: 83.47% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.122746  [    0/481450]\n",
      "loss: 0.215623  [ 3200/481450]\n",
      "loss: 0.414922  [ 6400/481450]\n",
      "loss: 0.250231  [ 9600/481450]\n",
      "loss: 0.134833  [12800/481450]\n",
      "loss: 0.263815  [16000/481450]\n",
      "loss: 0.316679  [19200/481450]\n",
      "loss: 0.374953  [22400/481450]\n",
      "loss: 0.101133  [25600/481450]\n",
      "loss: 0.163702  [28800/481450]\n",
      "loss: 0.371572  [32000/481450]\n",
      "loss: 0.158033  [35200/481450]\n",
      "loss: 0.501733  [38400/481450]\n",
      "loss: 0.478814  [41600/481450]\n",
      "loss: 0.248739  [44800/481450]\n",
      "loss: 0.303209  [48000/481450]\n",
      "loss: 0.215870  [51200/481450]\n",
      "loss: 0.326735  [54400/481450]\n",
      "loss: 0.272975  [57600/481450]\n",
      "loss: 0.298537  [60800/481450]\n",
      "loss: 0.070616  [64000/481450]\n",
      "loss: 0.263178  [67200/481450]\n",
      "loss: 0.307636  [70400/481450]\n",
      "loss: 0.272246  [73600/481450]\n",
      "loss: 0.170403  [76800/481450]\n",
      "loss: 0.220865  [80000/481450]\n",
      "loss: 0.193662  [83200/481450]\n",
      "loss: 0.286426  [86400/481450]\n",
      "loss: 0.356758  [89600/481450]\n",
      "loss: 0.162671  [92800/481450]\n",
      "loss: 0.534817  [96000/481450]\n",
      "loss: 0.360223  [99200/481450]\n",
      "loss: 0.192448  [102400/481450]\n",
      "loss: 0.277760  [105600/481450]\n",
      "loss: 0.335556  [108800/481450]\n",
      "loss: 0.166234  [112000/481450]\n",
      "loss: 0.163876  [115200/481450]\n",
      "loss: 0.322006  [118400/481450]\n",
      "loss: 0.253125  [121600/481450]\n",
      "loss: 0.246167  [124800/481450]\n",
      "loss: 0.294976  [128000/481450]\n",
      "loss: 0.196069  [131200/481450]\n",
      "loss: 0.228404  [134400/481450]\n",
      "loss: 0.256983  [137600/481450]\n",
      "loss: 0.226219  [140800/481450]\n",
      "loss: 0.317055  [144000/481450]\n",
      "loss: 0.335476  [147200/481450]\n",
      "loss: 0.363854  [150400/481450]\n",
      "loss: 0.372068  [153600/481450]\n",
      "loss: 0.237529  [156800/481450]\n",
      "loss: 0.282373  [160000/481450]\n",
      "loss: 0.098014  [163200/481450]\n",
      "loss: 0.239561  [166400/481450]\n",
      "loss: 0.489912  [169600/481450]\n",
      "loss: 0.187711  [172800/481450]\n",
      "loss: 0.221449  [176000/481450]\n",
      "loss: 0.300615  [179200/481450]\n",
      "loss: 0.463787  [182400/481450]\n",
      "loss: 0.263695  [185600/481450]\n",
      "loss: 0.574346  [188800/481450]\n",
      "loss: 0.197581  [192000/481450]\n",
      "loss: 0.432770  [195200/481450]\n",
      "loss: 0.318172  [198400/481450]\n",
      "loss: 0.213224  [201600/481450]\n",
      "loss: 0.354338  [204800/481450]\n",
      "loss: 0.196979  [208000/481450]\n",
      "loss: 0.149445  [211200/481450]\n",
      "loss: 0.298606  [214400/481450]\n",
      "loss: 0.270292  [217600/481450]\n",
      "loss: 0.207125  [220800/481450]\n",
      "loss: 0.268330  [224000/481450]\n",
      "loss: 0.392885  [227200/481450]\n",
      "loss: 0.301244  [230400/481450]\n",
      "loss: 0.314058  [233600/481450]\n",
      "loss: 0.180342  [236800/481450]\n",
      "loss: 0.353542  [240000/481450]\n",
      "loss: 0.255975  [243200/481450]\n",
      "loss: 0.272107  [246400/481450]\n",
      "loss: 0.327494  [249600/481450]\n",
      "loss: 0.190180  [252800/481450]\n",
      "loss: 0.307002  [256000/481450]\n",
      "loss: 0.280267  [259200/481450]\n",
      "loss: 0.233330  [262400/481450]\n",
      "loss: 0.420074  [265600/481450]\n",
      "loss: 0.340150  [268800/481450]\n",
      "loss: 0.265006  [272000/481450]\n",
      "loss: 0.291578  [275200/481450]\n",
      "loss: 0.257146  [278400/481450]\n",
      "loss: 0.173760  [281600/481450]\n",
      "loss: 0.227446  [284800/481450]\n",
      "loss: 0.666499  [288000/481450]\n",
      "loss: 0.467039  [291200/481450]\n",
      "loss: 0.367183  [294400/481450]\n",
      "loss: 0.307032  [297600/481450]\n",
      "loss: 0.287123  [300800/481450]\n",
      "loss: 0.313148  [304000/481450]\n",
      "loss: 0.192094  [307200/481450]\n",
      "loss: 0.382892  [310400/481450]\n",
      "loss: 0.534031  [313600/481450]\n",
      "loss: 0.229715  [316800/481450]\n",
      "loss: 0.427744  [320000/481450]\n",
      "loss: 0.212574  [323200/481450]\n",
      "loss: 0.408805  [326400/481450]\n",
      "loss: 0.212746  [329600/481450]\n",
      "loss: 0.190548  [332800/481450]\n",
      "loss: 0.405739  [336000/481450]\n",
      "loss: 0.478449  [339200/481450]\n",
      "loss: 0.352566  [342400/481450]\n",
      "loss: 0.207002  [345600/481450]\n",
      "loss: 0.116277  [348800/481450]\n",
      "loss: 0.486337  [352000/481450]\n",
      "loss: 0.107279  [355200/481450]\n",
      "loss: 0.471350  [358400/481450]\n",
      "loss: 0.289559  [361600/481450]\n",
      "loss: 0.193146  [364800/481450]\n",
      "loss: 0.046581  [368000/481450]\n",
      "loss: 0.196360  [371200/481450]\n",
      "loss: 0.193679  [374400/481450]\n",
      "loss: 0.308524  [377600/481450]\n",
      "loss: 0.169503  [380800/481450]\n",
      "loss: 0.584437  [384000/481450]\n",
      "loss: 0.337711  [387200/481450]\n",
      "loss: 0.319189  [390400/481450]\n",
      "loss: 0.123544  [393600/481450]\n",
      "loss: 0.261248  [396800/481450]\n",
      "loss: 0.197619  [400000/481450]\n",
      "loss: 0.191916  [403200/481450]\n",
      "loss: 0.267237  [406400/481450]\n",
      "loss: 0.271961  [409600/481450]\n",
      "loss: 0.140479  [412800/481450]\n",
      "loss: 0.119705  [416000/481450]\n",
      "loss: 0.149639  [419200/481450]\n",
      "loss: 0.092491  [422400/481450]\n",
      "loss: 0.222221  [425600/481450]\n",
      "loss: 0.236586  [428800/481450]\n",
      "loss: 0.231296  [432000/481450]\n",
      "loss: 0.357139  [435200/481450]\n",
      "loss: 0.234078  [438400/481450]\n",
      "loss: 0.341478  [441600/481450]\n",
      "loss: 0.176900  [444800/481450]\n",
      "loss: 0.258525  [448000/481450]\n",
      "loss: 0.333640  [451200/481450]\n",
      "loss: 0.448299  [454400/481450]\n",
      "loss: 0.272456  [457600/481450]\n",
      "loss: 0.057607  [460800/481450]\n",
      "loss: 0.220267  [464000/481450]\n",
      "loss: 0.237267  [467200/481450]\n",
      "loss: 0.159726  [470400/481450]\n",
      "loss: 0.426864  [473600/481450]\n",
      "loss: 0.314115  [476800/481450]\n",
      "loss: 0.375733  [480000/481450]\n",
      "Train Accuracy: 88.7139%\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.841931, F1-score: 83.65% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.090716  [    0/481450]\n",
      "loss: 0.187900  [ 3200/481450]\n",
      "loss: 0.251528  [ 6400/481450]\n",
      "loss: 0.151796  [ 9600/481450]\n",
      "loss: 0.194067  [12800/481450]\n",
      "loss: 0.348512  [16000/481450]\n",
      "loss: 0.269599  [19200/481450]\n",
      "loss: 0.180058  [22400/481450]\n",
      "loss: 0.290940  [25600/481450]\n",
      "loss: 0.311301  [28800/481450]\n",
      "loss: 0.209445  [32000/481450]\n",
      "loss: 0.117623  [35200/481450]\n",
      "loss: 0.219297  [38400/481450]\n",
      "loss: 0.110509  [41600/481450]\n",
      "loss: 0.178695  [44800/481450]\n",
      "loss: 0.403911  [48000/481450]\n",
      "loss: 0.410592  [51200/481450]\n",
      "loss: 0.366055  [54400/481450]\n",
      "loss: 0.281659  [57600/481450]\n",
      "loss: 0.257625  [60800/481450]\n",
      "loss: 0.432802  [64000/481450]\n",
      "loss: 0.253833  [67200/481450]\n",
      "loss: 0.292274  [70400/481450]\n",
      "loss: 0.134803  [73600/481450]\n",
      "loss: 0.400396  [76800/481450]\n",
      "loss: 0.378931  [80000/481450]\n",
      "loss: 0.489001  [83200/481450]\n",
      "loss: 0.202834  [86400/481450]\n",
      "loss: 0.400967  [89600/481450]\n",
      "loss: 0.324774  [92800/481450]\n",
      "loss: 0.235229  [96000/481450]\n",
      "loss: 0.111248  [99200/481450]\n",
      "loss: 0.450283  [102400/481450]\n",
      "loss: 0.311730  [105600/481450]\n",
      "loss: 0.255213  [108800/481450]\n",
      "loss: 0.231105  [112000/481450]\n",
      "loss: 0.116040  [115200/481450]\n",
      "loss: 0.155263  [118400/481450]\n",
      "loss: 0.344555  [121600/481450]\n",
      "loss: 0.275043  [124800/481450]\n",
      "loss: 0.239857  [128000/481450]\n",
      "loss: 0.539247  [131200/481450]\n",
      "loss: 0.144974  [134400/481450]\n",
      "loss: 0.556034  [137600/481450]\n",
      "loss: 0.185979  [140800/481450]\n",
      "loss: 0.261467  [144000/481450]\n",
      "loss: 0.073315  [147200/481450]\n",
      "loss: 0.364827  [150400/481450]\n",
      "loss: 0.288023  [153600/481450]\n",
      "loss: 0.362954  [156800/481450]\n",
      "loss: 0.219818  [160000/481450]\n",
      "loss: 0.355441  [163200/481450]\n",
      "loss: 0.381723  [166400/481450]\n",
      "loss: 0.289901  [169600/481450]\n",
      "loss: 0.327546  [172800/481450]\n",
      "loss: 0.235349  [176000/481450]\n",
      "loss: 0.151790  [179200/481450]\n",
      "loss: 0.450365  [182400/481450]\n",
      "loss: 0.312548  [185600/481450]\n",
      "loss: 0.277761  [188800/481450]\n",
      "loss: 0.253125  [192000/481450]\n",
      "loss: 0.142892  [195200/481450]\n",
      "loss: 0.082443  [198400/481450]\n",
      "loss: 0.158401  [201600/481450]\n",
      "loss: 0.385154  [204800/481450]\n",
      "loss: 0.180405  [208000/481450]\n",
      "loss: 0.236066  [211200/481450]\n",
      "loss: 0.462840  [214400/481450]\n",
      "loss: 0.343126  [217600/481450]\n",
      "loss: 0.295828  [220800/481450]\n",
      "loss: 0.110404  [224000/481450]\n",
      "loss: 0.399525  [227200/481450]\n",
      "loss: 0.250710  [230400/481450]\n",
      "loss: 0.213930  [233600/481450]\n",
      "loss: 0.239344  [236800/481450]\n",
      "loss: 0.357594  [240000/481450]\n",
      "loss: 0.137947  [243200/481450]\n",
      "loss: 0.146846  [246400/481450]\n",
      "loss: 0.149002  [249600/481450]\n",
      "loss: 0.053622  [252800/481450]\n",
      "loss: 0.390444  [256000/481450]\n",
      "loss: 0.355196  [259200/481450]\n",
      "loss: 0.215443  [262400/481450]\n",
      "loss: 0.124900  [265600/481450]\n",
      "loss: 0.400441  [268800/481450]\n",
      "loss: 0.165463  [272000/481450]\n",
      "loss: 0.303652  [275200/481450]\n",
      "loss: 0.221794  [278400/481450]\n",
      "loss: 0.563121  [281600/481450]\n",
      "loss: 0.327119  [284800/481450]\n",
      "loss: 0.410849  [288000/481450]\n",
      "loss: 0.316092  [291200/481450]\n",
      "loss: 0.433305  [294400/481450]\n",
      "loss: 0.300378  [297600/481450]\n",
      "loss: 0.206841  [300800/481450]\n",
      "loss: 0.104496  [304000/481450]\n",
      "loss: 0.299767  [307200/481450]\n",
      "loss: 0.324508  [310400/481450]\n",
      "loss: 0.196771  [313600/481450]\n",
      "loss: 0.368021  [316800/481450]\n",
      "loss: 0.360693  [320000/481450]\n",
      "loss: 0.349146  [323200/481450]\n",
      "loss: 0.442516  [326400/481450]\n",
      "loss: 0.288998  [329600/481450]\n",
      "loss: 0.130399  [332800/481450]\n",
      "loss: 0.217343  [336000/481450]\n",
      "loss: 0.245125  [339200/481450]\n",
      "loss: 0.214231  [342400/481450]\n",
      "loss: 0.158270  [345600/481450]\n",
      "loss: 0.307955  [348800/481450]\n",
      "loss: 0.279860  [352000/481450]\n",
      "loss: 0.093746  [355200/481450]\n",
      "loss: 0.216795  [358400/481450]\n",
      "loss: 0.272546  [361600/481450]\n",
      "loss: 0.220367  [364800/481450]\n",
      "loss: 0.281958  [368000/481450]\n",
      "loss: 0.242428  [371200/481450]\n",
      "loss: 0.202413  [374400/481450]\n",
      "loss: 0.200383  [377600/481450]\n",
      "loss: 0.366620  [380800/481450]\n",
      "loss: 0.248499  [384000/481450]\n",
      "loss: 0.467959  [387200/481450]\n",
      "loss: 0.106490  [390400/481450]\n",
      "loss: 0.469560  [393600/481450]\n",
      "loss: 0.470573  [396800/481450]\n",
      "loss: 0.232302  [400000/481450]\n",
      "loss: 0.535819  [403200/481450]\n",
      "loss: 0.307215  [406400/481450]\n",
      "loss: 0.266064  [409600/481450]\n",
      "loss: 0.250360  [412800/481450]\n",
      "loss: 0.237940  [416000/481450]\n",
      "loss: 0.188357  [419200/481450]\n",
      "loss: 0.183805  [422400/481450]\n",
      "loss: 0.306633  [425600/481450]\n",
      "loss: 0.438836  [428800/481450]\n",
      "loss: 0.238265  [432000/481450]\n",
      "loss: 0.437386  [435200/481450]\n",
      "loss: 0.278258  [438400/481450]\n",
      "loss: 0.213132  [441600/481450]\n",
      "loss: 0.264277  [444800/481450]\n",
      "loss: 0.155455  [448000/481450]\n",
      "loss: 0.348289  [451200/481450]\n",
      "loss: 0.196396  [454400/481450]\n",
      "loss: 0.293321  [457600/481450]\n",
      "loss: 0.297256  [460800/481450]\n",
      "loss: 0.201303  [464000/481450]\n",
      "loss: 0.296146  [467200/481450]\n",
      "loss: 0.184090  [470400/481450]\n",
      "loss: 0.204369  [473600/481450]\n",
      "loss: 0.294745  [476800/481450]\n",
      "loss: 0.239878  [480000/481450]\n",
      "Train Accuracy: 88.8973%\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.845474, F1-score: 83.44% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.207347  [    0/481450]\n",
      "loss: 0.395004  [ 3200/481450]\n",
      "loss: 0.398434  [ 6400/481450]\n",
      "loss: 0.145629  [ 9600/481450]\n",
      "loss: 0.164222  [12800/481450]\n",
      "loss: 0.101131  [16000/481450]\n",
      "loss: 0.272649  [19200/481450]\n",
      "loss: 0.181153  [22400/481450]\n",
      "loss: 0.130215  [25600/481450]\n",
      "loss: 0.113769  [28800/481450]\n",
      "loss: 0.151279  [32000/481450]\n",
      "loss: 0.200232  [35200/481450]\n",
      "loss: 0.119722  [38400/481450]\n",
      "loss: 0.287066  [41600/481450]\n",
      "loss: 0.358984  [44800/481450]\n",
      "loss: 0.129240  [48000/481450]\n",
      "loss: 0.276531  [51200/481450]\n",
      "loss: 0.230379  [54400/481450]\n",
      "loss: 0.314709  [57600/481450]\n",
      "loss: 0.256552  [60800/481450]\n",
      "loss: 0.170958  [64000/481450]\n",
      "loss: 0.157699  [67200/481450]\n",
      "loss: 0.434228  [70400/481450]\n",
      "loss: 0.306986  [73600/481450]\n",
      "loss: 0.250225  [76800/481450]\n",
      "loss: 0.267891  [80000/481450]\n",
      "loss: 0.236077  [83200/481450]\n",
      "loss: 0.262517  [86400/481450]\n",
      "loss: 0.092527  [89600/481450]\n",
      "loss: 0.248561  [92800/481450]\n",
      "loss: 0.297663  [96000/481450]\n",
      "loss: 0.205752  [99200/481450]\n",
      "loss: 0.274364  [102400/481450]\n",
      "loss: 0.138021  [105600/481450]\n",
      "loss: 0.185147  [108800/481450]\n",
      "loss: 0.186641  [112000/481450]\n",
      "loss: 0.220456  [115200/481450]\n",
      "loss: 0.260507  [118400/481450]\n",
      "loss: 0.256045  [121600/481450]\n",
      "loss: 0.060885  [124800/481450]\n",
      "loss: 0.438441  [128000/481450]\n",
      "loss: 0.317299  [131200/481450]\n",
      "loss: 0.141701  [134400/481450]\n",
      "loss: 0.344727  [137600/481450]\n",
      "loss: 0.281689  [140800/481450]\n",
      "loss: 0.114075  [144000/481450]\n",
      "loss: 0.348541  [147200/481450]\n",
      "loss: 0.271843  [150400/481450]\n",
      "loss: 0.255180  [153600/481450]\n",
      "loss: 0.298940  [156800/481450]\n",
      "loss: 0.387748  [160000/481450]\n",
      "loss: 0.163998  [163200/481450]\n",
      "loss: 0.208055  [166400/481450]\n",
      "loss: 0.292125  [169600/481450]\n",
      "loss: 0.443180  [172800/481450]\n",
      "loss: 0.505044  [176000/481450]\n",
      "loss: 0.047718  [179200/481450]\n",
      "loss: 0.149958  [182400/481450]\n",
      "loss: 0.338545  [185600/481450]\n",
      "loss: 0.132025  [188800/481450]\n",
      "loss: 0.323609  [192000/481450]\n",
      "loss: 0.146740  [195200/481450]\n",
      "loss: 0.111597  [198400/481450]\n",
      "loss: 0.374859  [201600/481450]\n",
      "loss: 0.314577  [204800/481450]\n",
      "loss: 0.173140  [208000/481450]\n",
      "loss: 0.245438  [211200/481450]\n",
      "loss: 0.341961  [214400/481450]\n",
      "loss: 0.175943  [217600/481450]\n",
      "loss: 0.187378  [220800/481450]\n",
      "loss: 0.225391  [224000/481450]\n",
      "loss: 0.364490  [227200/481450]\n",
      "loss: 0.470996  [230400/481450]\n",
      "loss: 0.175802  [233600/481450]\n",
      "loss: 0.307061  [236800/481450]\n",
      "loss: 0.309030  [240000/481450]\n",
      "loss: 0.291167  [243200/481450]\n",
      "loss: 0.209101  [246400/481450]\n",
      "loss: 0.284721  [249600/481450]\n",
      "loss: 0.150007  [252800/481450]\n",
      "loss: 0.338656  [256000/481450]\n",
      "loss: 0.165893  [259200/481450]\n",
      "loss: 0.292196  [262400/481450]\n",
      "loss: 0.231956  [265600/481450]\n",
      "loss: 0.350105  [268800/481450]\n",
      "loss: 0.237483  [272000/481450]\n",
      "loss: 0.200889  [275200/481450]\n",
      "loss: 0.473004  [278400/481450]\n",
      "loss: 0.378839  [281600/481450]\n",
      "loss: 0.331323  [284800/481450]\n",
      "loss: 0.371372  [288000/481450]\n",
      "loss: 0.205545  [291200/481450]\n",
      "loss: 0.316822  [294400/481450]\n",
      "loss: 0.200808  [297600/481450]\n",
      "loss: 0.367725  [300800/481450]\n",
      "loss: 0.154533  [304000/481450]\n",
      "loss: 0.606861  [307200/481450]\n",
      "loss: 0.209998  [310400/481450]\n",
      "loss: 0.167105  [313600/481450]\n",
      "loss: 0.467967  [316800/481450]\n",
      "loss: 0.160302  [320000/481450]\n",
      "loss: 0.249032  [323200/481450]\n",
      "loss: 0.195518  [326400/481450]\n",
      "loss: 0.185120  [329600/481450]\n",
      "loss: 0.420992  [332800/481450]\n",
      "loss: 0.269057  [336000/481450]\n",
      "loss: 0.382325  [339200/481450]\n",
      "loss: 0.588071  [342400/481450]\n",
      "loss: 0.228074  [345600/481450]\n",
      "loss: 0.365401  [348800/481450]\n",
      "loss: 0.307650  [352000/481450]\n",
      "loss: 0.185283  [355200/481450]\n",
      "loss: 0.106322  [358400/481450]\n",
      "loss: 0.246884  [361600/481450]\n",
      "loss: 0.432767  [364800/481450]\n",
      "loss: 0.280109  [368000/481450]\n",
      "loss: 0.266514  [371200/481450]\n",
      "loss: 0.245778  [374400/481450]\n",
      "loss: 0.572144  [377600/481450]\n",
      "loss: 0.346311  [380800/481450]\n",
      "loss: 0.377256  [384000/481450]\n",
      "loss: 0.225918  [387200/481450]\n",
      "loss: 0.398236  [390400/481450]\n",
      "loss: 0.388441  [393600/481450]\n",
      "loss: 0.317304  [396800/481450]\n",
      "loss: 0.197446  [400000/481450]\n",
      "loss: 0.176069  [403200/481450]\n",
      "loss: 0.144275  [406400/481450]\n",
      "loss: 0.168287  [409600/481450]\n",
      "loss: 0.255620  [412800/481450]\n",
      "loss: 0.233557  [416000/481450]\n",
      "loss: 0.142752  [419200/481450]\n",
      "loss: 0.105755  [422400/481450]\n",
      "loss: 0.410454  [425600/481450]\n",
      "loss: 0.185635  [428800/481450]\n",
      "loss: 0.177601  [432000/481450]\n",
      "loss: 0.146650  [435200/481450]\n",
      "loss: 0.266679  [438400/481450]\n",
      "loss: 0.202788  [441600/481450]\n",
      "loss: 0.295343  [444800/481450]\n",
      "loss: 0.077589  [448000/481450]\n",
      "loss: 0.542917  [451200/481450]\n",
      "loss: 0.250118  [454400/481450]\n",
      "loss: 0.320731  [457600/481450]\n",
      "loss: 0.065248  [460800/481450]\n",
      "loss: 0.279739  [464000/481450]\n",
      "loss: 0.271470  [467200/481450]\n",
      "loss: 0.251485  [470400/481450]\n",
      "loss: 0.214186  [473600/481450]\n",
      "loss: 0.169512  [476800/481450]\n",
      "loss: 0.218964  [480000/481450]\n",
      "Train Accuracy: 89.0647%\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 1.012203, F1-score: 81.99% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.222312  [    0/481450]\n",
      "loss: 0.158334  [ 3200/481450]\n",
      "loss: 0.453450  [ 6400/481450]\n",
      "loss: 0.221788  [ 9600/481450]\n",
      "loss: 0.170892  [12800/481450]\n",
      "loss: 0.347131  [16000/481450]\n",
      "loss: 0.423991  [19200/481450]\n",
      "loss: 0.189018  [22400/481450]\n",
      "loss: 0.358343  [25600/481450]\n",
      "loss: 0.457758  [28800/481450]\n",
      "loss: 0.205771  [32000/481450]\n",
      "loss: 0.448963  [35200/481450]\n",
      "loss: 0.234933  [38400/481450]\n",
      "loss: 0.217584  [41600/481450]\n",
      "loss: 0.158740  [44800/481450]\n",
      "loss: 0.029259  [48000/481450]\n",
      "loss: 0.232384  [51200/481450]\n",
      "loss: 0.275685  [54400/481450]\n",
      "loss: 0.478907  [57600/481450]\n",
      "loss: 0.407141  [60800/481450]\n",
      "loss: 0.260987  [64000/481450]\n",
      "loss: 0.214941  [67200/481450]\n",
      "loss: 0.425761  [70400/481450]\n",
      "loss: 0.100932  [73600/481450]\n",
      "loss: 0.227371  [76800/481450]\n",
      "loss: 0.302450  [80000/481450]\n",
      "loss: 0.119159  [83200/481450]\n",
      "loss: 0.155241  [86400/481450]\n",
      "loss: 0.158131  [89600/481450]\n",
      "loss: 0.229445  [92800/481450]\n",
      "loss: 0.461768  [96000/481450]\n",
      "loss: 0.194163  [99200/481450]\n",
      "loss: 0.424487  [102400/481450]\n",
      "loss: 0.176796  [105600/481450]\n",
      "loss: 0.163926  [108800/481450]\n",
      "loss: 0.525061  [112000/481450]\n",
      "loss: 0.329186  [115200/481450]\n",
      "loss: 0.105665  [118400/481450]\n",
      "loss: 0.233444  [121600/481450]\n",
      "loss: 0.426899  [124800/481450]\n",
      "loss: 0.074696  [128000/481450]\n",
      "loss: 0.706970  [131200/481450]\n",
      "loss: 0.260470  [134400/481450]\n",
      "loss: 0.192008  [137600/481450]\n",
      "loss: 0.225015  [140800/481450]\n",
      "loss: 0.329060  [144000/481450]\n",
      "loss: 0.177381  [147200/481450]\n",
      "loss: 0.226892  [150400/481450]\n",
      "loss: 0.471016  [153600/481450]\n",
      "loss: 0.260305  [156800/481450]\n",
      "loss: 0.266896  [160000/481450]\n",
      "loss: 0.274861  [163200/481450]\n",
      "loss: 0.274557  [166400/481450]\n",
      "loss: 0.270364  [169600/481450]\n",
      "loss: 0.231943  [172800/481450]\n",
      "loss: 0.330699  [176000/481450]\n",
      "loss: 0.107174  [179200/481450]\n",
      "loss: 0.311667  [182400/481450]\n",
      "loss: 0.235153  [185600/481450]\n",
      "loss: 0.210230  [188800/481450]\n",
      "loss: 0.037643  [192000/481450]\n",
      "loss: 0.358244  [195200/481450]\n",
      "loss: 0.184958  [198400/481450]\n",
      "loss: 0.323540  [201600/481450]\n",
      "loss: 0.239003  [204800/481450]\n",
      "loss: 0.204256  [208000/481450]\n",
      "loss: 0.220203  [211200/481450]\n",
      "loss: 0.338185  [214400/481450]\n",
      "loss: 0.366407  [217600/481450]\n",
      "loss: 0.166860  [220800/481450]\n",
      "loss: 0.222930  [224000/481450]\n",
      "loss: 0.396659  [227200/481450]\n",
      "loss: 0.484183  [230400/481450]\n",
      "loss: 0.308224  [233600/481450]\n",
      "loss: 0.181764  [236800/481450]\n",
      "loss: 0.173644  [240000/481450]\n",
      "loss: 0.135488  [243200/481450]\n",
      "loss: 0.265590  [246400/481450]\n",
      "loss: 0.383134  [249600/481450]\n",
      "loss: 0.137941  [252800/481450]\n",
      "loss: 0.349710  [256000/481450]\n",
      "loss: 0.145713  [259200/481450]\n",
      "loss: 0.326304  [262400/481450]\n",
      "loss: 0.447749  [265600/481450]\n",
      "loss: 0.255021  [268800/481450]\n",
      "loss: 0.241298  [272000/481450]\n",
      "loss: 0.124709  [275200/481450]\n",
      "loss: 0.152582  [278400/481450]\n",
      "loss: 0.110490  [281600/481450]\n",
      "loss: 0.198560  [284800/481450]\n",
      "loss: 0.599560  [288000/481450]\n",
      "loss: 0.285688  [291200/481450]\n",
      "loss: 0.333801  [294400/481450]\n",
      "loss: 0.093679  [297600/481450]\n",
      "loss: 0.279410  [300800/481450]\n",
      "loss: 0.443746  [304000/481450]\n",
      "loss: 0.256443  [307200/481450]\n",
      "loss: 0.222470  [310400/481450]\n",
      "loss: 0.350088  [313600/481450]\n",
      "loss: 0.159186  [316800/481450]\n",
      "loss: 0.220674  [320000/481450]\n",
      "loss: 0.394989  [323200/481450]\n",
      "loss: 0.282659  [326400/481450]\n",
      "loss: 0.161538  [329600/481450]\n",
      "loss: 0.310852  [332800/481450]\n",
      "loss: 0.293299  [336000/481450]\n",
      "loss: 0.072950  [339200/481450]\n",
      "loss: 0.356085  [342400/481450]\n",
      "loss: 0.240457  [345600/481450]\n",
      "loss: 0.222873  [348800/481450]\n",
      "loss: 0.281363  [352000/481450]\n",
      "loss: 0.187168  [355200/481450]\n",
      "loss: 0.180079  [358400/481450]\n",
      "loss: 0.189123  [361600/481450]\n",
      "loss: 0.603109  [364800/481450]\n",
      "loss: 0.135563  [368000/481450]\n",
      "loss: 0.294611  [371200/481450]\n",
      "loss: 0.397381  [374400/481450]\n",
      "loss: 0.457494  [377600/481450]\n",
      "loss: 0.421603  [380800/481450]\n",
      "loss: 0.330648  [384000/481450]\n",
      "loss: 0.421901  [387200/481450]\n",
      "loss: 0.153297  [390400/481450]\n",
      "loss: 0.250428  [393600/481450]\n",
      "loss: 0.253138  [396800/481450]\n",
      "loss: 0.124435  [400000/481450]\n",
      "loss: 0.192725  [403200/481450]\n",
      "loss: 0.266787  [406400/481450]\n",
      "loss: 0.198147  [409600/481450]\n",
      "loss: 0.084164  [412800/481450]\n",
      "loss: 0.182614  [416000/481450]\n",
      "loss: 0.272681  [419200/481450]\n",
      "loss: 0.052304  [422400/481450]\n",
      "loss: 0.226054  [425600/481450]\n",
      "loss: 0.404864  [428800/481450]\n",
      "loss: 0.323614  [432000/481450]\n",
      "loss: 0.260524  [435200/481450]\n",
      "loss: 0.460764  [438400/481450]\n",
      "loss: 0.216725  [441600/481450]\n",
      "loss: 0.348142  [444800/481450]\n",
      "loss: 0.482045  [448000/481450]\n",
      "loss: 0.276621  [451200/481450]\n",
      "loss: 0.309578  [454400/481450]\n",
      "loss: 0.128676  [457600/481450]\n",
      "loss: 0.345335  [460800/481450]\n",
      "loss: 0.119491  [464000/481450]\n",
      "loss: 0.300517  [467200/481450]\n",
      "loss: 0.157967  [470400/481450]\n",
      "loss: 0.131975  [473600/481450]\n",
      "loss: 0.185865  [476800/481450]\n",
      "loss: 0.284372  [480000/481450]\n",
      "Train Accuracy: 89.2703%\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.972039, F1-score: 83.19% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.158529  [    0/481450]\n",
      "loss: 0.161080  [ 3200/481450]\n",
      "loss: 0.426777  [ 6400/481450]\n",
      "loss: 0.255369  [ 9600/481450]\n",
      "loss: 0.135004  [12800/481450]\n",
      "loss: 0.368821  [16000/481450]\n",
      "loss: 0.128625  [19200/481450]\n",
      "loss: 0.296972  [22400/481450]\n",
      "loss: 0.533509  [25600/481450]\n",
      "loss: 0.158693  [28800/481450]\n",
      "loss: 0.257913  [32000/481450]\n",
      "loss: 0.289223  [35200/481450]\n",
      "loss: 0.261086  [38400/481450]\n",
      "loss: 0.156167  [41600/481450]\n",
      "loss: 0.075663  [44800/481450]\n",
      "loss: 0.335727  [48000/481450]\n",
      "loss: 0.073397  [51200/481450]\n",
      "loss: 0.265686  [54400/481450]\n",
      "loss: 0.139397  [57600/481450]\n",
      "loss: 0.268948  [60800/481450]\n",
      "loss: 0.153422  [64000/481450]\n",
      "loss: 0.389914  [67200/481450]\n",
      "loss: 0.607349  [70400/481450]\n",
      "loss: 0.182592  [73600/481450]\n",
      "loss: 0.103660  [76800/481450]\n",
      "loss: 0.436422  [80000/481450]\n",
      "loss: 0.135069  [83200/481450]\n",
      "loss: 0.186620  [86400/481450]\n",
      "loss: 0.166823  [89600/481450]\n",
      "loss: 0.197075  [92800/481450]\n",
      "loss: 0.413842  [96000/481450]\n",
      "loss: 0.350320  [99200/481450]\n",
      "loss: 0.241091  [102400/481450]\n",
      "loss: 0.254306  [105600/481450]\n",
      "loss: 0.261810  [108800/481450]\n",
      "loss: 0.239858  [112000/481450]\n",
      "loss: 0.443927  [115200/481450]\n",
      "loss: 0.303524  [118400/481450]\n",
      "loss: 0.417834  [121600/481450]\n",
      "loss: 0.315514  [124800/481450]\n",
      "loss: 0.310612  [128000/481450]\n",
      "loss: 0.433198  [131200/481450]\n",
      "loss: 0.354068  [134400/481450]\n",
      "loss: 0.236323  [137600/481450]\n",
      "loss: 0.359887  [140800/481450]\n",
      "loss: 0.348747  [144000/481450]\n",
      "loss: 0.157563  [147200/481450]\n",
      "loss: 0.422103  [150400/481450]\n",
      "loss: 0.136228  [153600/481450]\n",
      "loss: 0.430720  [156800/481450]\n",
      "loss: 0.205940  [160000/481450]\n",
      "loss: 0.238926  [163200/481450]\n",
      "loss: 0.365227  [166400/481450]\n",
      "loss: 0.175999  [169600/481450]\n",
      "loss: 0.229696  [172800/481450]\n",
      "loss: 0.442350  [176000/481450]\n",
      "loss: 0.475554  [179200/481450]\n",
      "loss: 0.301120  [182400/481450]\n",
      "loss: 0.098896  [185600/481450]\n",
      "loss: 0.531704  [188800/481450]\n",
      "loss: 0.263490  [192000/481450]\n",
      "loss: 0.548691  [195200/481450]\n",
      "loss: 0.350287  [198400/481450]\n",
      "loss: 0.343318  [201600/481450]\n",
      "loss: 0.186382  [204800/481450]\n",
      "loss: 0.293545  [208000/481450]\n",
      "loss: 0.207812  [211200/481450]\n",
      "loss: 0.165895  [214400/481450]\n",
      "loss: 0.067269  [217600/481450]\n",
      "loss: 0.139483  [220800/481450]\n",
      "loss: 0.400761  [224000/481450]\n",
      "loss: 0.396305  [227200/481450]\n",
      "loss: 0.445556  [230400/481450]\n",
      "loss: 0.572113  [233600/481450]\n",
      "loss: 0.146663  [236800/481450]\n",
      "loss: 0.453253  [240000/481450]\n",
      "loss: 0.316457  [243200/481450]\n",
      "loss: 0.185728  [246400/481450]\n",
      "loss: 0.249502  [249600/481450]\n",
      "loss: 0.278673  [252800/481450]\n",
      "loss: 0.347953  [256000/481450]\n",
      "loss: 0.412747  [259200/481450]\n",
      "loss: 0.093979  [262400/481450]\n",
      "loss: 0.181308  [265600/481450]\n",
      "loss: 0.083652  [268800/481450]\n",
      "loss: 0.113610  [272000/481450]\n",
      "loss: 0.148934  [275200/481450]\n",
      "loss: 0.381703  [278400/481450]\n",
      "loss: 0.141818  [281600/481450]\n",
      "loss: 0.243327  [284800/481450]\n",
      "loss: 0.144975  [288000/481450]\n",
      "loss: 0.239004  [291200/481450]\n",
      "loss: 0.244424  [294400/481450]\n",
      "loss: 0.340123  [297600/481450]\n",
      "loss: 0.429485  [300800/481450]\n",
      "loss: 0.080111  [304000/481450]\n",
      "loss: 0.434062  [307200/481450]\n",
      "loss: 0.376479  [310400/481450]\n",
      "loss: 0.436463  [313600/481450]\n",
      "loss: 0.531430  [316800/481450]\n",
      "loss: 0.247111  [320000/481450]\n",
      "loss: 0.252397  [323200/481450]\n",
      "loss: 0.300943  [326400/481450]\n",
      "loss: 0.075179  [329600/481450]\n",
      "loss: 0.324634  [332800/481450]\n",
      "loss: 0.015293  [336000/481450]\n",
      "loss: 0.109186  [339200/481450]\n",
      "loss: 0.328840  [342400/481450]\n",
      "loss: 0.226443  [345600/481450]\n",
      "loss: 0.209183  [348800/481450]\n",
      "loss: 0.311104  [352000/481450]\n",
      "loss: 0.203098  [355200/481450]\n",
      "loss: 0.308207  [358400/481450]\n",
      "loss: 0.257091  [361600/481450]\n",
      "loss: 0.258565  [364800/481450]\n",
      "loss: 0.342562  [368000/481450]\n",
      "loss: 0.234499  [371200/481450]\n",
      "loss: 0.114552  [374400/481450]\n",
      "loss: 0.257111  [377600/481450]\n",
      "loss: 0.306855  [380800/481450]\n",
      "loss: 0.166722  [384000/481450]\n",
      "loss: 0.076117  [387200/481450]\n",
      "loss: 0.707705  [390400/481450]\n",
      "loss: 0.395798  [393600/481450]\n",
      "loss: 0.449848  [396800/481450]\n",
      "loss: 0.433027  [400000/481450]\n",
      "loss: 0.227658  [403200/481450]\n",
      "loss: 0.226686  [406400/481450]\n",
      "loss: 0.166475  [409600/481450]\n",
      "loss: 0.112394  [412800/481450]\n",
      "loss: 0.179634  [416000/481450]\n",
      "loss: 0.339852  [419200/481450]\n",
      "loss: 0.280183  [422400/481450]\n",
      "loss: 0.280544  [425600/481450]\n",
      "loss: 0.213414  [428800/481450]\n",
      "loss: 0.150180  [432000/481450]\n",
      "loss: 0.159845  [435200/481450]\n",
      "loss: 0.314096  [438400/481450]\n",
      "loss: 0.158996  [441600/481450]\n",
      "loss: 0.190860  [444800/481450]\n",
      "loss: 0.151472  [448000/481450]\n",
      "loss: 0.276212  [451200/481450]\n",
      "loss: 0.194762  [454400/481450]\n",
      "loss: 0.249449  [457600/481450]\n",
      "loss: 0.365374  [460800/481450]\n",
      "loss: 0.355856  [464000/481450]\n",
      "loss: 0.164278  [467200/481450]\n",
      "loss: 0.380978  [470400/481450]\n",
      "loss: 0.342393  [473600/481450]\n",
      "loss: 0.188582  [476800/481450]\n",
      "loss: 0.193723  [480000/481450]\n",
      "Train Accuracy: 89.3769%\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 1.087928, F1-score: 81.92% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a37a33c4-8087-4b5c-80cf-c10334d2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 15,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ada289ca-73cc-4ac6-8f1c-89c10d83b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301274  [    0/481450]\n",
      "loss: 2.301294  [ 3200/481450]\n",
      "loss: 2.304498  [ 6400/481450]\n",
      "loss: 2.282434  [ 9600/481450]\n",
      "loss: 2.293563  [12800/481450]\n",
      "loss: 2.281860  [16000/481450]\n",
      "loss: 2.257150  [19200/481450]\n",
      "loss: 2.191207  [22400/481450]\n",
      "loss: 2.114937  [25600/481450]\n",
      "loss: 2.208935  [28800/481450]\n",
      "loss: 2.202476  [32000/481450]\n",
      "loss: 2.015805  [35200/481450]\n",
      "loss: 1.955566  [38400/481450]\n",
      "loss: 2.077241  [41600/481450]\n",
      "loss: 1.912466  [44800/481450]\n",
      "loss: 1.915923  [48000/481450]\n",
      "loss: 1.718047  [51200/481450]\n",
      "loss: 1.702236  [54400/481450]\n",
      "loss: 1.732891  [57600/481450]\n",
      "loss: 1.761831  [60800/481450]\n",
      "loss: 1.929140  [64000/481450]\n",
      "loss: 1.679223  [67200/481450]\n",
      "loss: 1.805556  [70400/481450]\n",
      "loss: 1.505187  [73600/481450]\n",
      "loss: 1.397231  [76800/481450]\n",
      "loss: 1.350367  [80000/481450]\n",
      "loss: 1.515566  [83200/481450]\n",
      "loss: 1.460700  [86400/481450]\n",
      "loss: 1.365919  [89600/481450]\n",
      "loss: 1.382560  [92800/481450]\n",
      "loss: 1.491446  [96000/481450]\n",
      "loss: 1.400762  [99200/481450]\n",
      "loss: 1.309344  [102400/481450]\n",
      "loss: 1.383598  [105600/481450]\n",
      "loss: 1.418424  [108800/481450]\n",
      "loss: 1.407600  [112000/481450]\n",
      "loss: 1.456261  [115200/481450]\n",
      "loss: 1.492739  [118400/481450]\n",
      "loss: 1.293789  [121600/481450]\n",
      "loss: 1.349197  [124800/481450]\n",
      "loss: 1.273367  [128000/481450]\n",
      "loss: 1.079574  [131200/481450]\n",
      "loss: 1.305293  [134400/481450]\n",
      "loss: 0.992954  [137600/481450]\n",
      "loss: 1.170173  [140800/481450]\n",
      "loss: 1.313309  [144000/481450]\n",
      "loss: 1.171298  [147200/481450]\n",
      "loss: 1.173410  [150400/481450]\n",
      "loss: 1.172598  [153600/481450]\n",
      "loss: 1.048985  [156800/481450]\n",
      "loss: 0.960430  [160000/481450]\n",
      "loss: 0.820628  [163200/481450]\n",
      "loss: 1.199270  [166400/481450]\n",
      "loss: 1.015558  [169600/481450]\n",
      "loss: 0.936016  [172800/481450]\n",
      "loss: 1.121892  [176000/481450]\n",
      "loss: 1.004682  [179200/481450]\n",
      "loss: 1.342807  [182400/481450]\n",
      "loss: 1.177406  [185600/481450]\n",
      "loss: 1.321299  [188800/481450]\n",
      "loss: 1.189105  [192000/481450]\n",
      "loss: 1.184116  [195200/481450]\n",
      "loss: 1.107222  [198400/481450]\n",
      "loss: 1.072215  [201600/481450]\n",
      "loss: 0.987947  [204800/481450]\n",
      "loss: 0.930279  [208000/481450]\n",
      "loss: 0.815905  [211200/481450]\n",
      "loss: 1.191824  [214400/481450]\n",
      "loss: 0.933697  [217600/481450]\n",
      "loss: 0.868134  [220800/481450]\n",
      "loss: 0.857519  [224000/481450]\n",
      "loss: 1.090598  [227200/481450]\n",
      "loss: 0.882806  [230400/481450]\n",
      "loss: 0.869877  [233600/481450]\n",
      "loss: 0.816996  [236800/481450]\n",
      "loss: 0.691638  [240000/481450]\n",
      "loss: 0.681204  [243200/481450]\n",
      "loss: 0.842011  [246400/481450]\n",
      "loss: 0.866472  [249600/481450]\n",
      "loss: 0.786009  [252800/481450]\n",
      "loss: 0.882952  [256000/481450]\n",
      "loss: 0.896226  [259200/481450]\n",
      "loss: 0.875926  [262400/481450]\n",
      "loss: 0.784560  [265600/481450]\n",
      "loss: 0.961161  [268800/481450]\n",
      "loss: 0.887267  [272000/481450]\n",
      "loss: 0.734422  [275200/481450]\n",
      "loss: 0.775591  [278400/481450]\n",
      "loss: 0.894979  [281600/481450]\n",
      "loss: 0.804137  [284800/481450]\n",
      "loss: 0.706074  [288000/481450]\n",
      "loss: 0.674842  [291200/481450]\n",
      "loss: 0.709442  [294400/481450]\n",
      "loss: 0.825608  [297600/481450]\n",
      "loss: 0.804832  [300800/481450]\n",
      "loss: 0.632242  [304000/481450]\n",
      "loss: 0.837860  [307200/481450]\n",
      "loss: 0.548442  [310400/481450]\n",
      "loss: 0.625988  [313600/481450]\n",
      "loss: 0.579767  [316800/481450]\n",
      "loss: 0.698697  [320000/481450]\n",
      "loss: 0.735570  [323200/481450]\n",
      "loss: 0.410578  [326400/481450]\n",
      "loss: 0.622662  [329600/481450]\n",
      "loss: 0.752302  [332800/481450]\n",
      "loss: 0.647198  [336000/481450]\n",
      "loss: 0.681186  [339200/481450]\n",
      "loss: 0.453575  [342400/481450]\n",
      "loss: 0.775689  [345600/481450]\n",
      "loss: 0.680043  [348800/481450]\n",
      "loss: 0.575144  [352000/481450]\n",
      "loss: 0.655931  [355200/481450]\n",
      "loss: 0.679995  [358400/481450]\n",
      "loss: 0.811496  [361600/481450]\n",
      "loss: 0.580781  [364800/481450]\n",
      "loss: 0.723627  [368000/481450]\n",
      "loss: 0.568490  [371200/481450]\n",
      "loss: 0.494275  [374400/481450]\n",
      "loss: 0.683913  [377600/481450]\n",
      "loss: 0.407711  [380800/481450]\n",
      "loss: 0.640488  [384000/481450]\n",
      "loss: 0.634664  [387200/481450]\n",
      "loss: 0.673353  [390400/481450]\n",
      "loss: 0.668053  [393600/481450]\n",
      "loss: 0.515610  [396800/481450]\n",
      "loss: 0.713452  [400000/481450]\n",
      "loss: 0.703059  [403200/481450]\n",
      "loss: 0.588014  [406400/481450]\n",
      "loss: 0.450438  [409600/481450]\n",
      "loss: 0.619251  [412800/481450]\n",
      "loss: 0.507235  [416000/481450]\n",
      "loss: 0.660813  [419200/481450]\n",
      "loss: 0.729782  [422400/481450]\n",
      "loss: 0.404555  [425600/481450]\n",
      "loss: 0.566957  [428800/481450]\n",
      "loss: 0.596270  [432000/481450]\n",
      "loss: 0.460168  [435200/481450]\n",
      "loss: 0.908239  [438400/481450]\n",
      "loss: 0.593750  [441600/481450]\n",
      "loss: 0.636297  [444800/481450]\n",
      "loss: 0.646008  [448000/481450]\n",
      "loss: 0.448659  [451200/481450]\n",
      "loss: 0.531743  [454400/481450]\n",
      "loss: 0.617441  [457600/481450]\n",
      "loss: 0.326568  [460800/481450]\n",
      "loss: 0.524480  [464000/481450]\n",
      "loss: 0.685516  [467200/481450]\n",
      "loss: 0.542386  [470400/481450]\n",
      "loss: 0.476451  [473600/481450]\n",
      "loss: 0.487522  [476800/481450]\n",
      "loss: 0.748966  [480000/481450]\n",
      "Train Accuracy: 62.2372%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.395806, F1-score: 88.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.587289  [    0/481450]\n",
      "loss: 0.438215  [ 3200/481450]\n",
      "loss: 0.435070  [ 6400/481450]\n",
      "loss: 0.542492  [ 9600/481450]\n",
      "loss: 0.449340  [12800/481450]\n",
      "loss: 0.699953  [16000/481450]\n",
      "loss: 0.498226  [19200/481450]\n",
      "loss: 0.323338  [22400/481450]\n",
      "loss: 0.438128  [25600/481450]\n",
      "loss: 0.563131  [28800/481450]\n",
      "loss: 0.535274  [32000/481450]\n",
      "loss: 0.484155  [35200/481450]\n",
      "loss: 0.538041  [38400/481450]\n",
      "loss: 0.455687  [41600/481450]\n",
      "loss: 0.483983  [44800/481450]\n",
      "loss: 0.367825  [48000/481450]\n",
      "loss: 0.835974  [51200/481450]\n",
      "loss: 0.308691  [54400/481450]\n",
      "loss: 0.499450  [57600/481450]\n",
      "loss: 0.426939  [60800/481450]\n",
      "loss: 0.635557  [64000/481450]\n",
      "loss: 0.467355  [67200/481450]\n",
      "loss: 0.272447  [70400/481450]\n",
      "loss: 0.517937  [73600/481450]\n",
      "loss: 0.401017  [76800/481450]\n",
      "loss: 0.685099  [80000/481450]\n",
      "loss: 0.773927  [83200/481450]\n",
      "loss: 0.616483  [86400/481450]\n",
      "loss: 0.667198  [89600/481450]\n",
      "loss: 0.384587  [92800/481450]\n",
      "loss: 0.534406  [96000/481450]\n",
      "loss: 0.822595  [99200/481450]\n",
      "loss: 0.493634  [102400/481450]\n",
      "loss: 0.495802  [105600/481450]\n",
      "loss: 0.591562  [108800/481450]\n",
      "loss: 0.341262  [112000/481450]\n",
      "loss: 0.466152  [115200/481450]\n",
      "loss: 0.450525  [118400/481450]\n",
      "loss: 0.505431  [121600/481450]\n",
      "loss: 0.567801  [124800/481450]\n",
      "loss: 0.346636  [128000/481450]\n",
      "loss: 0.463042  [131200/481450]\n",
      "loss: 0.652027  [134400/481450]\n",
      "loss: 0.544597  [137600/481450]\n",
      "loss: 0.408357  [140800/481450]\n",
      "loss: 0.277963  [144000/481450]\n",
      "loss: 0.655361  [147200/481450]\n",
      "loss: 0.524326  [150400/481450]\n",
      "loss: 0.458218  [153600/481450]\n",
      "loss: 0.393824  [156800/481450]\n",
      "loss: 0.573394  [160000/481450]\n",
      "loss: 0.504708  [163200/481450]\n",
      "loss: 0.372752  [166400/481450]\n",
      "loss: 0.428040  [169600/481450]\n",
      "loss: 0.541288  [172800/481450]\n",
      "loss: 0.421922  [176000/481450]\n",
      "loss: 0.398594  [179200/481450]\n",
      "loss: 0.530507  [182400/481450]\n",
      "loss: 0.454714  [185600/481450]\n",
      "loss: 0.410437  [188800/481450]\n",
      "loss: 0.353362  [192000/481450]\n",
      "loss: 0.523393  [195200/481450]\n",
      "loss: 0.447974  [198400/481450]\n",
      "loss: 0.348984  [201600/481450]\n",
      "loss: 0.448028  [204800/481450]\n",
      "loss: 0.646912  [208000/481450]\n",
      "loss: 0.318897  [211200/481450]\n",
      "loss: 0.378912  [214400/481450]\n",
      "loss: 0.325778  [217600/481450]\n",
      "loss: 0.516092  [220800/481450]\n",
      "loss: 0.616258  [224000/481450]\n",
      "loss: 0.614584  [227200/481450]\n",
      "loss: 0.541972  [230400/481450]\n",
      "loss: 0.250267  [233600/481450]\n",
      "loss: 0.410207  [236800/481450]\n",
      "loss: 0.439521  [240000/481450]\n",
      "loss: 0.510156  [243200/481450]\n",
      "loss: 0.490664  [246400/481450]\n",
      "loss: 0.381896  [249600/481450]\n",
      "loss: 0.478573  [252800/481450]\n",
      "loss: 0.550238  [256000/481450]\n",
      "loss: 0.257010  [259200/481450]\n",
      "loss: 0.436749  [262400/481450]\n",
      "loss: 0.338624  [265600/481450]\n",
      "loss: 0.569103  [268800/481450]\n",
      "loss: 0.392915  [272000/481450]\n",
      "loss: 0.486468  [275200/481450]\n",
      "loss: 0.414062  [278400/481450]\n",
      "loss: 0.433829  [281600/481450]\n",
      "loss: 0.358183  [284800/481450]\n",
      "loss: 0.326893  [288000/481450]\n",
      "loss: 0.637289  [291200/481450]\n",
      "loss: 0.523263  [294400/481450]\n",
      "loss: 0.388439  [297600/481450]\n",
      "loss: 0.696213  [300800/481450]\n",
      "loss: 0.554625  [304000/481450]\n",
      "loss: 0.316397  [307200/481450]\n",
      "loss: 0.564706  [310400/481450]\n",
      "loss: 0.387243  [313600/481450]\n",
      "loss: 0.410815  [316800/481450]\n",
      "loss: 0.166865  [320000/481450]\n",
      "loss: 0.330119  [323200/481450]\n",
      "loss: 0.339655  [326400/481450]\n",
      "loss: 0.741182  [329600/481450]\n",
      "loss: 0.433171  [332800/481450]\n",
      "loss: 0.758477  [336000/481450]\n",
      "loss: 0.499796  [339200/481450]\n",
      "loss: 0.476009  [342400/481450]\n",
      "loss: 0.270292  [345600/481450]\n",
      "loss: 0.316320  [348800/481450]\n",
      "loss: 0.406990  [352000/481450]\n",
      "loss: 0.380981  [355200/481450]\n",
      "loss: 0.323782  [358400/481450]\n",
      "loss: 0.481759  [361600/481450]\n",
      "loss: 0.279876  [364800/481450]\n",
      "loss: 0.447050  [368000/481450]\n",
      "loss: 0.441475  [371200/481450]\n",
      "loss: 0.414648  [374400/481450]\n",
      "loss: 0.350610  [377600/481450]\n",
      "loss: 0.417638  [380800/481450]\n",
      "loss: 0.574354  [384000/481450]\n",
      "loss: 0.219653  [387200/481450]\n",
      "loss: 0.299063  [390400/481450]\n",
      "loss: 0.205644  [393600/481450]\n",
      "loss: 0.252118  [396800/481450]\n",
      "loss: 0.474224  [400000/481450]\n",
      "loss: 0.287474  [403200/481450]\n",
      "loss: 0.468450  [406400/481450]\n",
      "loss: 0.603605  [409600/481450]\n",
      "loss: 0.568972  [412800/481450]\n",
      "loss: 0.551705  [416000/481450]\n",
      "loss: 0.419680  [419200/481450]\n",
      "loss: 0.421716  [422400/481450]\n",
      "loss: 0.219237  [425600/481450]\n",
      "loss: 0.404308  [428800/481450]\n",
      "loss: 0.428407  [432000/481450]\n",
      "loss: 0.376361  [435200/481450]\n",
      "loss: 0.284326  [438400/481450]\n",
      "loss: 0.731609  [441600/481450]\n",
      "loss: 0.402897  [444800/481450]\n",
      "loss: 0.287498  [448000/481450]\n",
      "loss: 0.258248  [451200/481450]\n",
      "loss: 0.396096  [454400/481450]\n",
      "loss: 0.507970  [457600/481450]\n",
      "loss: 0.608203  [460800/481450]\n",
      "loss: 0.462216  [464000/481450]\n",
      "loss: 0.329657  [467200/481450]\n",
      "loss: 0.547430  [470400/481450]\n",
      "loss: 0.532891  [473600/481450]\n",
      "loss: 0.455834  [476800/481450]\n",
      "loss: 0.286336  [480000/481450]\n",
      "Train Accuracy: 82.3153%\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.334399, F1-score: 89.23% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.294012  [    0/481450]\n",
      "loss: 0.442567  [ 3200/481450]\n",
      "loss: 0.262710  [ 6400/481450]\n",
      "loss: 0.346951  [ 9600/481450]\n",
      "loss: 0.285318  [12800/481450]\n",
      "loss: 0.403031  [16000/481450]\n",
      "loss: 0.252990  [19200/481450]\n",
      "loss: 0.496006  [22400/481450]\n",
      "loss: 0.451252  [25600/481450]\n",
      "loss: 0.255152  [28800/481450]\n",
      "loss: 0.711611  [32000/481450]\n",
      "loss: 0.356346  [35200/481450]\n",
      "loss: 0.343269  [38400/481450]\n",
      "loss: 0.460651  [41600/481450]\n",
      "loss: 0.200365  [44800/481450]\n",
      "loss: 0.261423  [48000/481450]\n",
      "loss: 0.519059  [51200/481450]\n",
      "loss: 0.324031  [54400/481450]\n",
      "loss: 0.160280  [57600/481450]\n",
      "loss: 0.181116  [60800/481450]\n",
      "loss: 0.337468  [64000/481450]\n",
      "loss: 0.472576  [67200/481450]\n",
      "loss: 0.480961  [70400/481450]\n",
      "loss: 0.484909  [73600/481450]\n",
      "loss: 0.747318  [76800/481450]\n",
      "loss: 0.341016  [80000/481450]\n",
      "loss: 0.368455  [83200/481450]\n",
      "loss: 0.288564  [86400/481450]\n",
      "loss: 0.589868  [89600/481450]\n",
      "loss: 0.564354  [92800/481450]\n",
      "loss: 0.476689  [96000/481450]\n",
      "loss: 0.587904  [99200/481450]\n",
      "loss: 0.186838  [102400/481450]\n",
      "loss: 0.271215  [105600/481450]\n",
      "loss: 0.313050  [108800/481450]\n",
      "loss: 0.270158  [112000/481450]\n",
      "loss: 0.567042  [115200/481450]\n",
      "loss: 0.448272  [118400/481450]\n",
      "loss: 0.413763  [121600/481450]\n",
      "loss: 0.644117  [124800/481450]\n",
      "loss: 0.213354  [128000/481450]\n",
      "loss: 0.366552  [131200/481450]\n",
      "loss: 0.329950  [134400/481450]\n",
      "loss: 0.461736  [137600/481450]\n",
      "loss: 0.368878  [140800/481450]\n",
      "loss: 0.438750  [144000/481450]\n",
      "loss: 0.511559  [147200/481450]\n",
      "loss: 0.268781  [150400/481450]\n",
      "loss: 0.329569  [153600/481450]\n",
      "loss: 0.481030  [156800/481450]\n",
      "loss: 0.635498  [160000/481450]\n",
      "loss: 0.256897  [163200/481450]\n",
      "loss: 0.616313  [166400/481450]\n",
      "loss: 0.384993  [169600/481450]\n",
      "loss: 0.370058  [172800/481450]\n",
      "loss: 0.263370  [176000/481450]\n",
      "loss: 0.412185  [179200/481450]\n",
      "loss: 0.351464  [182400/481450]\n",
      "loss: 0.381508  [185600/481450]\n",
      "loss: 0.290527  [188800/481450]\n",
      "loss: 0.371508  [192000/481450]\n",
      "loss: 0.330315  [195200/481450]\n",
      "loss: 0.534584  [198400/481450]\n",
      "loss: 0.306063  [201600/481450]\n",
      "loss: 0.336278  [204800/481450]\n",
      "loss: 0.382638  [208000/481450]\n",
      "loss: 0.863005  [211200/481450]\n",
      "loss: 0.318867  [214400/481450]\n",
      "loss: 0.507584  [217600/481450]\n",
      "loss: 0.634312  [220800/481450]\n",
      "loss: 0.452989  [224000/481450]\n",
      "loss: 0.345232  [227200/481450]\n",
      "loss: 0.284968  [230400/481450]\n",
      "loss: 0.162841  [233600/481450]\n",
      "loss: 0.459970  [236800/481450]\n",
      "loss: 0.463111  [240000/481450]\n",
      "loss: 0.422865  [243200/481450]\n",
      "loss: 0.392000  [246400/481450]\n",
      "loss: 0.256082  [249600/481450]\n",
      "loss: 0.493309  [252800/481450]\n",
      "loss: 0.649149  [256000/481450]\n",
      "loss: 0.375630  [259200/481450]\n",
      "loss: 0.699728  [262400/481450]\n",
      "loss: 0.841375  [265600/481450]\n",
      "loss: 0.493966  [268800/481450]\n",
      "loss: 0.435333  [272000/481450]\n",
      "loss: 0.435004  [275200/481450]\n",
      "loss: 0.383350  [278400/481450]\n",
      "loss: 0.332406  [281600/481450]\n",
      "loss: 0.379820  [284800/481450]\n",
      "loss: 0.431208  [288000/481450]\n",
      "loss: 0.293180  [291200/481450]\n",
      "loss: 0.496076  [294400/481450]\n",
      "loss: 0.286925  [297600/481450]\n",
      "loss: 0.236933  [300800/481450]\n",
      "loss: 0.376831  [304000/481450]\n",
      "loss: 0.408748  [307200/481450]\n",
      "loss: 0.256934  [310400/481450]\n",
      "loss: 0.378946  [313600/481450]\n",
      "loss: 0.415312  [316800/481450]\n",
      "loss: 0.370576  [320000/481450]\n",
      "loss: 0.454676  [323200/481450]\n",
      "loss: 0.271736  [326400/481450]\n",
      "loss: 0.371053  [329600/481450]\n",
      "loss: 0.392913  [332800/481450]\n",
      "loss: 0.331210  [336000/481450]\n",
      "loss: 0.377178  [339200/481450]\n",
      "loss: 0.322170  [342400/481450]\n",
      "loss: 0.210019  [345600/481450]\n",
      "loss: 0.704808  [348800/481450]\n",
      "loss: 0.432798  [352000/481450]\n",
      "loss: 0.229867  [355200/481450]\n",
      "loss: 0.255173  [358400/481450]\n",
      "loss: 0.706982  [361600/481450]\n",
      "loss: 0.261731  [364800/481450]\n",
      "loss: 0.383346  [368000/481450]\n",
      "loss: 0.568804  [371200/481450]\n",
      "loss: 0.425442  [374400/481450]\n",
      "loss: 0.282357  [377600/481450]\n",
      "loss: 0.352457  [380800/481450]\n",
      "loss: 0.305416  [384000/481450]\n",
      "loss: 0.511198  [387200/481450]\n",
      "loss: 0.462419  [390400/481450]\n",
      "loss: 0.152249  [393600/481450]\n",
      "loss: 0.252492  [396800/481450]\n",
      "loss: 0.596237  [400000/481450]\n",
      "loss: 0.457844  [403200/481450]\n",
      "loss: 0.500605  [406400/481450]\n",
      "loss: 0.653480  [409600/481450]\n",
      "loss: 0.196005  [412800/481450]\n",
      "loss: 0.201168  [416000/481450]\n",
      "loss: 0.262529  [419200/481450]\n",
      "loss: 0.285360  [422400/481450]\n",
      "loss: 0.556011  [425600/481450]\n",
      "loss: 0.369309  [428800/481450]\n",
      "loss: 0.426458  [432000/481450]\n",
      "loss: 0.418127  [435200/481450]\n",
      "loss: 0.222582  [438400/481450]\n",
      "loss: 0.286828  [441600/481450]\n",
      "loss: 0.460622  [444800/481450]\n",
      "loss: 0.532429  [448000/481450]\n",
      "loss: 0.359403  [451200/481450]\n",
      "loss: 0.463896  [454400/481450]\n",
      "loss: 0.336319  [457600/481450]\n",
      "loss: 0.225755  [460800/481450]\n",
      "loss: 0.325361  [464000/481450]\n",
      "loss: 0.580629  [467200/481450]\n",
      "loss: 0.232652  [470400/481450]\n",
      "loss: 0.319211  [473600/481450]\n",
      "loss: 0.330872  [476800/481450]\n",
      "loss: 0.356055  [480000/481450]\n",
      "Train Accuracy: 84.5749%\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.325968, F1-score: 89.71% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.603321  [    0/481450]\n",
      "loss: 0.236854  [ 3200/481450]\n",
      "loss: 0.536890  [ 6400/481450]\n",
      "loss: 0.224384  [ 9600/481450]\n",
      "loss: 0.438545  [12800/481450]\n",
      "loss: 0.577012  [16000/481450]\n",
      "loss: 0.279532  [19200/481450]\n",
      "loss: 0.260466  [22400/481450]\n",
      "loss: 0.549298  [25600/481450]\n",
      "loss: 0.154142  [28800/481450]\n",
      "loss: 0.381011  [32000/481450]\n",
      "loss: 0.306144  [35200/481450]\n",
      "loss: 0.452824  [38400/481450]\n",
      "loss: 0.460970  [41600/481450]\n",
      "loss: 0.119135  [44800/481450]\n",
      "loss: 0.299050  [48000/481450]\n",
      "loss: 0.357191  [51200/481450]\n",
      "loss: 0.644073  [54400/481450]\n",
      "loss: 0.530140  [57600/481450]\n",
      "loss: 0.325613  [60800/481450]\n",
      "loss: 0.416907  [64000/481450]\n",
      "loss: 0.398565  [67200/481450]\n",
      "loss: 0.283468  [70400/481450]\n",
      "loss: 0.417734  [73600/481450]\n",
      "loss: 0.268070  [76800/481450]\n",
      "loss: 0.325017  [80000/481450]\n",
      "loss: 0.393485  [83200/481450]\n",
      "loss: 0.240035  [86400/481450]\n",
      "loss: 0.449144  [89600/481450]\n",
      "loss: 0.239616  [92800/481450]\n",
      "loss: 0.248311  [96000/481450]\n",
      "loss: 0.244359  [99200/481450]\n",
      "loss: 0.324528  [102400/481450]\n",
      "loss: 0.251458  [105600/481450]\n",
      "loss: 0.419354  [108800/481450]\n",
      "loss: 0.467891  [112000/481450]\n",
      "loss: 0.391163  [115200/481450]\n",
      "loss: 0.192533  [118400/481450]\n",
      "loss: 0.134840  [121600/481450]\n",
      "loss: 0.630864  [124800/481450]\n",
      "loss: 0.511320  [128000/481450]\n",
      "loss: 0.529715  [131200/481450]\n",
      "loss: 0.278338  [134400/481450]\n",
      "loss: 0.308211  [137600/481450]\n",
      "loss: 0.248679  [140800/481450]\n",
      "loss: 0.471776  [144000/481450]\n",
      "loss: 0.205169  [147200/481450]\n",
      "loss: 0.454800  [150400/481450]\n",
      "loss: 0.263543  [153600/481450]\n",
      "loss: 0.214998  [156800/481450]\n",
      "loss: 0.217007  [160000/481450]\n",
      "loss: 0.291745  [163200/481450]\n",
      "loss: 0.383499  [166400/481450]\n",
      "loss: 0.468480  [169600/481450]\n",
      "loss: 0.449553  [172800/481450]\n",
      "loss: 0.280991  [176000/481450]\n",
      "loss: 0.109948  [179200/481450]\n",
      "loss: 0.345305  [182400/481450]\n",
      "loss: 0.482310  [185600/481450]\n",
      "loss: 0.625125  [188800/481450]\n",
      "loss: 0.266567  [192000/481450]\n",
      "loss: 0.508878  [195200/481450]\n",
      "loss: 0.328750  [198400/481450]\n",
      "loss: 0.274897  [201600/481450]\n",
      "loss: 0.262745  [204800/481450]\n",
      "loss: 0.355162  [208000/481450]\n",
      "loss: 0.279154  [211200/481450]\n",
      "loss: 0.456570  [214400/481450]\n",
      "loss: 0.272158  [217600/481450]\n",
      "loss: 0.506473  [220800/481450]\n",
      "loss: 0.431570  [224000/481450]\n",
      "loss: 0.403587  [227200/481450]\n",
      "loss: 0.263941  [230400/481450]\n",
      "loss: 0.168817  [233600/481450]\n",
      "loss: 0.314083  [236800/481450]\n",
      "loss: 0.529686  [240000/481450]\n",
      "loss: 0.288349  [243200/481450]\n",
      "loss: 0.386098  [246400/481450]\n",
      "loss: 0.342504  [249600/481450]\n",
      "loss: 0.278273  [252800/481450]\n",
      "loss: 0.382392  [256000/481450]\n",
      "loss: 0.294284  [259200/481450]\n",
      "loss: 0.366879  [262400/481450]\n",
      "loss: 0.413271  [265600/481450]\n",
      "loss: 0.225684  [268800/481450]\n",
      "loss: 0.444788  [272000/481450]\n",
      "loss: 0.300465  [275200/481450]\n",
      "loss: 0.373515  [278400/481450]\n",
      "loss: 0.432070  [281600/481450]\n",
      "loss: 0.254676  [284800/481450]\n",
      "loss: 0.288144  [288000/481450]\n",
      "loss: 0.398315  [291200/481450]\n",
      "loss: 0.378523  [294400/481450]\n",
      "loss: 0.317879  [297600/481450]\n",
      "loss: 0.502995  [300800/481450]\n",
      "loss: 0.172658  [304000/481450]\n",
      "loss: 0.267317  [307200/481450]\n",
      "loss: 0.322216  [310400/481450]\n",
      "loss: 0.432770  [313600/481450]\n",
      "loss: 0.499738  [316800/481450]\n",
      "loss: 0.516935  [320000/481450]\n",
      "loss: 0.331825  [323200/481450]\n",
      "loss: 0.342061  [326400/481450]\n",
      "loss: 0.302047  [329600/481450]\n",
      "loss: 0.598792  [332800/481450]\n",
      "loss: 0.293508  [336000/481450]\n",
      "loss: 0.275969  [339200/481450]\n",
      "loss: 0.454842  [342400/481450]\n",
      "loss: 0.253587  [345600/481450]\n",
      "loss: 0.231006  [348800/481450]\n",
      "loss: 0.347860  [352000/481450]\n",
      "loss: 0.398180  [355200/481450]\n",
      "loss: 0.417985  [358400/481450]\n",
      "loss: 0.516405  [361600/481450]\n",
      "loss: 0.347694  [364800/481450]\n",
      "loss: 0.211688  [368000/481450]\n",
      "loss: 0.359661  [371200/481450]\n",
      "loss: 0.591381  [374400/481450]\n",
      "loss: 0.240996  [377600/481450]\n",
      "loss: 0.209168  [380800/481450]\n",
      "loss: 0.177819  [384000/481450]\n",
      "loss: 0.102465  [387200/481450]\n",
      "loss: 0.517568  [390400/481450]\n",
      "loss: 0.459049  [393600/481450]\n",
      "loss: 0.180572  [396800/481450]\n",
      "loss: 0.309190  [400000/481450]\n",
      "loss: 0.443992  [403200/481450]\n",
      "loss: 0.417190  [406400/481450]\n",
      "loss: 0.447309  [409600/481450]\n",
      "loss: 0.285819  [412800/481450]\n",
      "loss: 0.459662  [416000/481450]\n",
      "loss: 0.372270  [419200/481450]\n",
      "loss: 0.246856  [422400/481450]\n",
      "loss: 0.480222  [425600/481450]\n",
      "loss: 0.412871  [428800/481450]\n",
      "loss: 0.431533  [432000/481450]\n",
      "loss: 0.264318  [435200/481450]\n",
      "loss: 0.368964  [438400/481450]\n",
      "loss: 0.241637  [441600/481450]\n",
      "loss: 0.103098  [444800/481450]\n",
      "loss: 0.235156  [448000/481450]\n",
      "loss: 0.343057  [451200/481450]\n",
      "loss: 0.360075  [454400/481450]\n",
      "loss: 0.204758  [457600/481450]\n",
      "loss: 0.123316  [460800/481450]\n",
      "loss: 0.308545  [464000/481450]\n",
      "loss: 0.139396  [467200/481450]\n",
      "loss: 0.540768  [470400/481450]\n",
      "loss: 0.176816  [473600/481450]\n",
      "loss: 0.484078  [476800/481450]\n",
      "loss: 0.161906  [480000/481450]\n",
      "Train Accuracy: 86.0615%\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.313676, F1-score: 89.51% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.179889  [    0/481450]\n",
      "loss: 0.396618  [ 3200/481450]\n",
      "loss: 0.159200  [ 6400/481450]\n",
      "loss: 0.222594  [ 9600/481450]\n",
      "loss: 0.218392  [12800/481450]\n",
      "loss: 0.285267  [16000/481450]\n",
      "loss: 0.334533  [19200/481450]\n",
      "loss: 0.206249  [22400/481450]\n",
      "loss: 0.489739  [25600/481450]\n",
      "loss: 0.566476  [28800/481450]\n",
      "loss: 0.194473  [32000/481450]\n",
      "loss: 0.315081  [35200/481450]\n",
      "loss: 0.307185  [38400/481450]\n",
      "loss: 0.552956  [41600/481450]\n",
      "loss: 0.347475  [44800/481450]\n",
      "loss: 0.468873  [48000/481450]\n",
      "loss: 0.483659  [51200/481450]\n",
      "loss: 0.248660  [54400/481450]\n",
      "loss: 0.263250  [57600/481450]\n",
      "loss: 0.239442  [60800/481450]\n",
      "loss: 0.363360  [64000/481450]\n",
      "loss: 0.182611  [67200/481450]\n",
      "loss: 0.141283  [70400/481450]\n",
      "loss: 0.525361  [73600/481450]\n",
      "loss: 0.249626  [76800/481450]\n",
      "loss: 0.509410  [80000/481450]\n",
      "loss: 0.453054  [83200/481450]\n",
      "loss: 0.340868  [86400/481450]\n",
      "loss: 0.342896  [89600/481450]\n",
      "loss: 0.150043  [92800/481450]\n",
      "loss: 0.122135  [96000/481450]\n",
      "loss: 0.382243  [99200/481450]\n",
      "loss: 0.141060  [102400/481450]\n",
      "loss: 0.103066  [105600/481450]\n",
      "loss: 0.180580  [108800/481450]\n",
      "loss: 0.150249  [112000/481450]\n",
      "loss: 0.271060  [115200/481450]\n",
      "loss: 0.312661  [118400/481450]\n",
      "loss: 0.338191  [121600/481450]\n",
      "loss: 0.356141  [124800/481450]\n",
      "loss: 0.233314  [128000/481450]\n",
      "loss: 0.317940  [131200/481450]\n",
      "loss: 0.111086  [134400/481450]\n",
      "loss: 0.331561  [137600/481450]\n",
      "loss: 0.270432  [140800/481450]\n",
      "loss: 0.453729  [144000/481450]\n",
      "loss: 0.210249  [147200/481450]\n",
      "loss: 0.270813  [150400/481450]\n",
      "loss: 0.240370  [153600/481450]\n",
      "loss: 0.208014  [156800/481450]\n",
      "loss: 0.514510  [160000/481450]\n",
      "loss: 0.240422  [163200/481450]\n",
      "loss: 0.414595  [166400/481450]\n",
      "loss: 0.403557  [169600/481450]\n",
      "loss: 0.422849  [172800/481450]\n",
      "loss: 0.349616  [176000/481450]\n",
      "loss: 0.379554  [179200/481450]\n",
      "loss: 0.507523  [182400/481450]\n",
      "loss: 0.543084  [185600/481450]\n",
      "loss: 0.299107  [188800/481450]\n",
      "loss: 0.271656  [192000/481450]\n",
      "loss: 0.202462  [195200/481450]\n",
      "loss: 0.218977  [198400/481450]\n",
      "loss: 0.453891  [201600/481450]\n",
      "loss: 0.214547  [204800/481450]\n",
      "loss: 0.296089  [208000/481450]\n",
      "loss: 0.423339  [211200/481450]\n",
      "loss: 0.476949  [214400/481450]\n",
      "loss: 0.248649  [217600/481450]\n",
      "loss: 0.201877  [220800/481450]\n",
      "loss: 0.319703  [224000/481450]\n",
      "loss: 0.365358  [227200/481450]\n",
      "loss: 0.384864  [230400/481450]\n",
      "loss: 0.262321  [233600/481450]\n",
      "loss: 0.271271  [236800/481450]\n",
      "loss: 0.495500  [240000/481450]\n",
      "loss: 0.264600  [243200/481450]\n",
      "loss: 0.470100  [246400/481450]\n",
      "loss: 0.241119  [249600/481450]\n",
      "loss: 0.580726  [252800/481450]\n",
      "loss: 0.348074  [256000/481450]\n",
      "loss: 0.200332  [259200/481450]\n",
      "loss: 0.121761  [262400/481450]\n",
      "loss: 0.637323  [265600/481450]\n",
      "loss: 0.392447  [268800/481450]\n",
      "loss: 0.369549  [272000/481450]\n",
      "loss: 0.197392  [275200/481450]\n",
      "loss: 0.401511  [278400/481450]\n",
      "loss: 0.349511  [281600/481450]\n",
      "loss: 0.201821  [284800/481450]\n",
      "loss: 0.409355  [288000/481450]\n",
      "loss: 0.444785  [291200/481450]\n",
      "loss: 0.335500  [294400/481450]\n",
      "loss: 0.233272  [297600/481450]\n",
      "loss: 0.155585  [300800/481450]\n",
      "loss: 0.181217  [304000/481450]\n",
      "loss: 0.451657  [307200/481450]\n",
      "loss: 0.272526  [310400/481450]\n",
      "loss: 0.412055  [313600/481450]\n",
      "loss: 0.394415  [316800/481450]\n",
      "loss: 0.189753  [320000/481450]\n",
      "loss: 0.485146  [323200/481450]\n",
      "loss: 0.262304  [326400/481450]\n",
      "loss: 0.534584  [329600/481450]\n",
      "loss: 0.535752  [332800/481450]\n",
      "loss: 0.488113  [336000/481450]\n",
      "loss: 0.499202  [339200/481450]\n",
      "loss: 0.220303  [342400/481450]\n",
      "loss: 0.416419  [345600/481450]\n",
      "loss: 0.493940  [348800/481450]\n",
      "loss: 0.170235  [352000/481450]\n",
      "loss: 0.395325  [355200/481450]\n",
      "loss: 0.297208  [358400/481450]\n",
      "loss: 0.254363  [361600/481450]\n",
      "loss: 0.340168  [364800/481450]\n",
      "loss: 0.380989  [368000/481450]\n",
      "loss: 0.271449  [371200/481450]\n",
      "loss: 0.429684  [374400/481450]\n",
      "loss: 0.303246  [377600/481450]\n",
      "loss: 0.581874  [380800/481450]\n",
      "loss: 0.210638  [384000/481450]\n",
      "loss: 0.172727  [387200/481450]\n",
      "loss: 0.155750  [390400/481450]\n",
      "loss: 0.500969  [393600/481450]\n",
      "loss: 0.434537  [396800/481450]\n",
      "loss: 0.417598  [400000/481450]\n",
      "loss: 0.627590  [403200/481450]\n",
      "loss: 0.371591  [406400/481450]\n",
      "loss: 0.215890  [409600/481450]\n",
      "loss: 0.423493  [412800/481450]\n",
      "loss: 0.232403  [416000/481450]\n",
      "loss: 0.351048  [419200/481450]\n",
      "loss: 0.354247  [422400/481450]\n",
      "loss: 0.257680  [425600/481450]\n",
      "loss: 0.187827  [428800/481450]\n",
      "loss: 0.050802  [432000/481450]\n",
      "loss: 0.201155  [435200/481450]\n",
      "loss: 0.347531  [438400/481450]\n",
      "loss: 0.382351  [441600/481450]\n",
      "loss: 0.222447  [444800/481450]\n",
      "loss: 0.465314  [448000/481450]\n",
      "loss: 0.344630  [451200/481450]\n",
      "loss: 0.243687  [454400/481450]\n",
      "loss: 0.100313  [457600/481450]\n",
      "loss: 0.454608  [460800/481450]\n",
      "loss: 0.462955  [464000/481450]\n",
      "loss: 0.231178  [467200/481450]\n",
      "loss: 0.436590  [470400/481450]\n",
      "loss: 0.387370  [473600/481450]\n",
      "loss: 0.105928  [476800/481450]\n",
      "loss: 0.409420  [480000/481450]\n",
      "Train Accuracy: 87.2485%\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.307623, F1-score: 89.96% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.241397  [    0/481450]\n",
      "loss: 0.270448  [ 3200/481450]\n",
      "loss: 0.124250  [ 6400/481450]\n",
      "loss: 0.569470  [ 9600/481450]\n",
      "loss: 0.139897  [12800/481450]\n",
      "loss: 0.267227  [16000/481450]\n",
      "loss: 0.283594  [19200/481450]\n",
      "loss: 0.192432  [22400/481450]\n",
      "loss: 0.157722  [25600/481450]\n",
      "loss: 0.338033  [28800/481450]\n",
      "loss: 0.132649  [32000/481450]\n",
      "loss: 0.254340  [35200/481450]\n",
      "loss: 0.428112  [38400/481450]\n",
      "loss: 0.452691  [41600/481450]\n",
      "loss: 0.166135  [44800/481450]\n",
      "loss: 0.283093  [48000/481450]\n",
      "loss: 0.301883  [51200/481450]\n",
      "loss: 0.205897  [54400/481450]\n",
      "loss: 0.422244  [57600/481450]\n",
      "loss: 0.473295  [60800/481450]\n",
      "loss: 0.278787  [64000/481450]\n",
      "loss: 0.385304  [67200/481450]\n",
      "loss: 0.594893  [70400/481450]\n",
      "loss: 0.220259  [73600/481450]\n",
      "loss: 0.188248  [76800/481450]\n",
      "loss: 0.400372  [80000/481450]\n",
      "loss: 0.361675  [83200/481450]\n",
      "loss: 0.401538  [86400/481450]\n",
      "loss: 0.395202  [89600/481450]\n",
      "loss: 0.411925  [92800/481450]\n",
      "loss: 0.319366  [96000/481450]\n",
      "loss: 0.254629  [99200/481450]\n",
      "loss: 0.250789  [102400/481450]\n",
      "loss: 0.315550  [105600/481450]\n",
      "loss: 0.209999  [108800/481450]\n",
      "loss: 0.228863  [112000/481450]\n",
      "loss: 0.510025  [115200/481450]\n",
      "loss: 0.217095  [118400/481450]\n",
      "loss: 0.307103  [121600/481450]\n",
      "loss: 0.417527  [124800/481450]\n",
      "loss: 0.362120  [128000/481450]\n",
      "loss: 0.273150  [131200/481450]\n",
      "loss: 0.337969  [134400/481450]\n",
      "loss: 0.268840  [137600/481450]\n",
      "loss: 0.272221  [140800/481450]\n",
      "loss: 0.530918  [144000/481450]\n",
      "loss: 0.235543  [147200/481450]\n",
      "loss: 0.205946  [150400/481450]\n",
      "loss: 0.261977  [153600/481450]\n",
      "loss: 0.341440  [156800/481450]\n",
      "loss: 0.210628  [160000/481450]\n",
      "loss: 0.156507  [163200/481450]\n",
      "loss: 0.319163  [166400/481450]\n",
      "loss: 0.354341  [169600/481450]\n",
      "loss: 0.144244  [172800/481450]\n",
      "loss: 0.257006  [176000/481450]\n",
      "loss: 0.501571  [179200/481450]\n",
      "loss: 0.395236  [182400/481450]\n",
      "loss: 0.324440  [185600/481450]\n",
      "loss: 0.344487  [188800/481450]\n",
      "loss: 0.393860  [192000/481450]\n",
      "loss: 0.146884  [195200/481450]\n",
      "loss: 0.224045  [198400/481450]\n",
      "loss: 0.149629  [201600/481450]\n",
      "loss: 0.218176  [204800/481450]\n",
      "loss: 0.468245  [208000/481450]\n",
      "loss: 0.498820  [211200/481450]\n",
      "loss: 0.325026  [214400/481450]\n",
      "loss: 0.323441  [217600/481450]\n",
      "loss: 0.149454  [220800/481450]\n",
      "loss: 0.330699  [224000/481450]\n",
      "loss: 0.286967  [227200/481450]\n",
      "loss: 0.194461  [230400/481450]\n",
      "loss: 0.310510  [233600/481450]\n",
      "loss: 0.276843  [236800/481450]\n",
      "loss: 0.301436  [240000/481450]\n",
      "loss: 0.136991  [243200/481450]\n",
      "loss: 0.420302  [246400/481450]\n",
      "loss: 0.269460  [249600/481450]\n",
      "loss: 0.209339  [252800/481450]\n",
      "loss: 0.510174  [256000/481450]\n",
      "loss: 0.227771  [259200/481450]\n",
      "loss: 0.198869  [262400/481450]\n",
      "loss: 0.141153  [265600/481450]\n",
      "loss: 0.105910  [268800/481450]\n",
      "loss: 0.370346  [272000/481450]\n",
      "loss: 0.186660  [275200/481450]\n",
      "loss: 0.261920  [278400/481450]\n",
      "loss: 0.260189  [281600/481450]\n",
      "loss: 0.390326  [284800/481450]\n",
      "loss: 0.522447  [288000/481450]\n",
      "loss: 0.310259  [291200/481450]\n",
      "loss: 0.323572  [294400/481450]\n",
      "loss: 0.315623  [297600/481450]\n",
      "loss: 0.137819  [300800/481450]\n",
      "loss: 0.348665  [304000/481450]\n",
      "loss: 0.413645  [307200/481450]\n",
      "loss: 0.259049  [310400/481450]\n",
      "loss: 0.249123  [313600/481450]\n",
      "loss: 0.378036  [316800/481450]\n",
      "loss: 0.275639  [320000/481450]\n",
      "loss: 0.204774  [323200/481450]\n",
      "loss: 0.217316  [326400/481450]\n",
      "loss: 0.238271  [329600/481450]\n",
      "loss: 0.288540  [332800/481450]\n",
      "loss: 0.274852  [336000/481450]\n",
      "loss: 0.306598  [339200/481450]\n",
      "loss: 0.258070  [342400/481450]\n",
      "loss: 0.209639  [345600/481450]\n",
      "loss: 0.179812  [348800/481450]\n",
      "loss: 0.331901  [352000/481450]\n",
      "loss: 0.292493  [355200/481450]\n",
      "loss: 0.278381  [358400/481450]\n",
      "loss: 0.458773  [361600/481450]\n",
      "loss: 0.403654  [364800/481450]\n",
      "loss: 0.379540  [368000/481450]\n",
      "loss: 0.426917  [371200/481450]\n",
      "loss: 0.182544  [374400/481450]\n",
      "loss: 0.185511  [377600/481450]\n",
      "loss: 0.149350  [380800/481450]\n",
      "loss: 0.140849  [384000/481450]\n",
      "loss: 0.197102  [387200/481450]\n",
      "loss: 0.161482  [390400/481450]\n",
      "loss: 0.196603  [393600/481450]\n",
      "loss: 0.515972  [396800/481450]\n",
      "loss: 0.156673  [400000/481450]\n",
      "loss: 0.089224  [403200/481450]\n",
      "loss: 0.406393  [406400/481450]\n",
      "loss: 0.344261  [409600/481450]\n",
      "loss: 0.246169  [412800/481450]\n",
      "loss: 0.390375  [416000/481450]\n",
      "loss: 0.223990  [419200/481450]\n",
      "loss: 0.245093  [422400/481450]\n",
      "loss: 0.205442  [425600/481450]\n",
      "loss: 0.230866  [428800/481450]\n",
      "loss: 0.230973  [432000/481450]\n",
      "loss: 0.358867  [435200/481450]\n",
      "loss: 0.158112  [438400/481450]\n",
      "loss: 0.365664  [441600/481450]\n",
      "loss: 0.326526  [444800/481450]\n",
      "loss: 0.222829  [448000/481450]\n",
      "loss: 0.119872  [451200/481450]\n",
      "loss: 0.302074  [454400/481450]\n",
      "loss: 0.370812  [457600/481450]\n",
      "loss: 0.375042  [460800/481450]\n",
      "loss: 0.043830  [464000/481450]\n",
      "loss: 0.106952  [467200/481450]\n",
      "loss: 0.266016  [470400/481450]\n",
      "loss: 0.112335  [473600/481450]\n",
      "loss: 0.305611  [476800/481450]\n",
      "loss: 0.230543  [480000/481450]\n",
      "Train Accuracy: 88.2206%\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.330973, F1-score: 88.63% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.149115  [    0/481450]\n",
      "loss: 0.220670  [ 3200/481450]\n",
      "loss: 0.342752  [ 6400/481450]\n",
      "loss: 0.273929  [ 9600/481450]\n",
      "loss: 0.313728  [12800/481450]\n",
      "loss: 0.461020  [16000/481450]\n",
      "loss: 0.123139  [19200/481450]\n",
      "loss: 0.136923  [22400/481450]\n",
      "loss: 0.484221  [25600/481450]\n",
      "loss: 0.415651  [28800/481450]\n",
      "loss: 0.242720  [32000/481450]\n",
      "loss: 0.354794  [35200/481450]\n",
      "loss: 0.298927  [38400/481450]\n",
      "loss: 0.121521  [41600/481450]\n",
      "loss: 0.281047  [44800/481450]\n",
      "loss: 0.288117  [48000/481450]\n",
      "loss: 0.289594  [51200/481450]\n",
      "loss: 0.288295  [54400/481450]\n",
      "loss: 0.344151  [57600/481450]\n",
      "loss: 0.281454  [60800/481450]\n",
      "loss: 0.281052  [64000/481450]\n",
      "loss: 0.367981  [67200/481450]\n",
      "loss: 0.652753  [70400/481450]\n",
      "loss: 0.428624  [73600/481450]\n",
      "loss: 0.426547  [76800/481450]\n",
      "loss: 0.246027  [80000/481450]\n",
      "loss: 0.163584  [83200/481450]\n",
      "loss: 0.543156  [86400/481450]\n",
      "loss: 0.208155  [89600/481450]\n",
      "loss: 0.183853  [92800/481450]\n",
      "loss: 0.254272  [96000/481450]\n",
      "loss: 0.554337  [99200/481450]\n",
      "loss: 0.516896  [102400/481450]\n",
      "loss: 0.408387  [105600/481450]\n",
      "loss: 0.142262  [108800/481450]\n",
      "loss: 0.425615  [112000/481450]\n",
      "loss: 0.179613  [115200/481450]\n",
      "loss: 0.351424  [118400/481450]\n",
      "loss: 0.194608  [121600/481450]\n",
      "loss: 0.271370  [124800/481450]\n",
      "loss: 0.307230  [128000/481450]\n",
      "loss: 0.474880  [131200/481450]\n",
      "loss: 0.252534  [134400/481450]\n",
      "loss: 0.256096  [137600/481450]\n",
      "loss: 0.190135  [140800/481450]\n",
      "loss: 0.269698  [144000/481450]\n",
      "loss: 0.308329  [147200/481450]\n",
      "loss: 0.173727  [150400/481450]\n",
      "loss: 0.154224  [153600/481450]\n",
      "loss: 0.207709  [156800/481450]\n",
      "loss: 0.338999  [160000/481450]\n",
      "loss: 0.173787  [163200/481450]\n",
      "loss: 0.289552  [166400/481450]\n",
      "loss: 0.137208  [169600/481450]\n",
      "loss: 0.196734  [172800/481450]\n",
      "loss: 0.372567  [176000/481450]\n",
      "loss: 0.150884  [179200/481450]\n",
      "loss: 0.368713  [182400/481450]\n",
      "loss: 0.309661  [185600/481450]\n",
      "loss: 0.182071  [188800/481450]\n",
      "loss: 0.310075  [192000/481450]\n",
      "loss: 0.220823  [195200/481450]\n",
      "loss: 0.371327  [198400/481450]\n",
      "loss: 0.358891  [201600/481450]\n",
      "loss: 0.322711  [204800/481450]\n",
      "loss: 0.156582  [208000/481450]\n",
      "loss: 0.284075  [211200/481450]\n",
      "loss: 0.216485  [214400/481450]\n",
      "loss: 0.599608  [217600/481450]\n",
      "loss: 0.496179  [220800/481450]\n",
      "loss: 0.217080  [224000/481450]\n",
      "loss: 0.131542  [227200/481450]\n",
      "loss: 0.268043  [230400/481450]\n",
      "loss: 0.194972  [233600/481450]\n",
      "loss: 0.240782  [236800/481450]\n",
      "loss: 0.107449  [240000/481450]\n",
      "loss: 0.438202  [243200/481450]\n",
      "loss: 0.149021  [246400/481450]\n",
      "loss: 0.347432  [249600/481450]\n",
      "loss: 0.326108  [252800/481450]\n",
      "loss: 0.222662  [256000/481450]\n",
      "loss: 0.444298  [259200/481450]\n",
      "loss: 0.260828  [262400/481450]\n",
      "loss: 0.434716  [265600/481450]\n",
      "loss: 0.181062  [268800/481450]\n",
      "loss: 0.265933  [272000/481450]\n",
      "loss: 0.271770  [275200/481450]\n",
      "loss: 0.416128  [278400/481450]\n",
      "loss: 0.524366  [281600/481450]\n",
      "loss: 0.245068  [284800/481450]\n",
      "loss: 0.223723  [288000/481450]\n",
      "loss: 0.197395  [291200/481450]\n",
      "loss: 0.134384  [294400/481450]\n",
      "loss: 0.372713  [297600/481450]\n",
      "loss: 0.329207  [300800/481450]\n",
      "loss: 0.195058  [304000/481450]\n",
      "loss: 0.158144  [307200/481450]\n",
      "loss: 0.182471  [310400/481450]\n",
      "loss: 0.466791  [313600/481450]\n",
      "loss: 0.441468  [316800/481450]\n",
      "loss: 0.276170  [320000/481450]\n",
      "loss: 0.191833  [323200/481450]\n",
      "loss: 0.223098  [326400/481450]\n",
      "loss: 0.222862  [329600/481450]\n",
      "loss: 0.328418  [332800/481450]\n",
      "loss: 0.232427  [336000/481450]\n",
      "loss: 0.064258  [339200/481450]\n",
      "loss: 0.292745  [342400/481450]\n",
      "loss: 0.738070  [345600/481450]\n",
      "loss: 0.232763  [348800/481450]\n",
      "loss: 0.099262  [352000/481450]\n",
      "loss: 0.220454  [355200/481450]\n",
      "loss: 0.230530  [358400/481450]\n",
      "loss: 0.285580  [361600/481450]\n",
      "loss: 0.214507  [364800/481450]\n",
      "loss: 0.298787  [368000/481450]\n",
      "loss: 0.142741  [371200/481450]\n",
      "loss: 0.096280  [374400/481450]\n",
      "loss: 0.218875  [377600/481450]\n",
      "loss: 0.380714  [380800/481450]\n",
      "loss: 0.216569  [384000/481450]\n",
      "loss: 0.196559  [387200/481450]\n",
      "loss: 0.243035  [390400/481450]\n",
      "loss: 0.108888  [393600/481450]\n",
      "loss: 0.221974  [396800/481450]\n",
      "loss: 0.160957  [400000/481450]\n",
      "loss: 0.255085  [403200/481450]\n",
      "loss: 0.216877  [406400/481450]\n",
      "loss: 0.153239  [409600/481450]\n",
      "loss: 0.290005  [412800/481450]\n",
      "loss: 0.099678  [416000/481450]\n",
      "loss: 0.026172  [419200/481450]\n",
      "loss: 0.257125  [422400/481450]\n",
      "loss: 0.188586  [425600/481450]\n",
      "loss: 0.137812  [428800/481450]\n",
      "loss: 0.107631  [432000/481450]\n",
      "loss: 0.110637  [435200/481450]\n",
      "loss: 0.313747  [438400/481450]\n",
      "loss: 0.109816  [441600/481450]\n",
      "loss: 0.288545  [444800/481450]\n",
      "loss: 0.275580  [448000/481450]\n",
      "loss: 0.264889  [451200/481450]\n",
      "loss: 0.033163  [454400/481450]\n",
      "loss: 0.267489  [457600/481450]\n",
      "loss: 0.284431  [460800/481450]\n",
      "loss: 0.068266  [464000/481450]\n",
      "loss: 0.390398  [467200/481450]\n",
      "loss: 0.231940  [470400/481450]\n",
      "loss: 0.295520  [473600/481450]\n",
      "loss: 0.133757  [476800/481450]\n",
      "loss: 0.206438  [480000/481450]\n",
      "Train Accuracy: 88.7629%\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.342761, F1-score: 88.38% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.283409  [    0/481450]\n",
      "loss: 0.179956  [ 3200/481450]\n",
      "loss: 0.182247  [ 6400/481450]\n",
      "loss: 0.280961  [ 9600/481450]\n",
      "loss: 0.223416  [12800/481450]\n",
      "loss: 0.260639  [16000/481450]\n",
      "loss: 0.305058  [19200/481450]\n",
      "loss: 0.144366  [22400/481450]\n",
      "loss: 0.258696  [25600/481450]\n",
      "loss: 0.176170  [28800/481450]\n",
      "loss: 0.303513  [32000/481450]\n",
      "loss: 0.115754  [35200/481450]\n",
      "loss: 0.404388  [38400/481450]\n",
      "loss: 0.447172  [41600/481450]\n",
      "loss: 0.132710  [44800/481450]\n",
      "loss: 0.351570  [48000/481450]\n",
      "loss: 0.275908  [51200/481450]\n",
      "loss: 0.262550  [54400/481450]\n",
      "loss: 0.215729  [57600/481450]\n",
      "loss: 0.353370  [60800/481450]\n",
      "loss: 0.536622  [64000/481450]\n",
      "loss: 0.393571  [67200/481450]\n",
      "loss: 0.153281  [70400/481450]\n",
      "loss: 0.131309  [73600/481450]\n",
      "loss: 0.191276  [76800/481450]\n",
      "loss: 0.203885  [80000/481450]\n",
      "loss: 0.206785  [83200/481450]\n",
      "loss: 0.275556  [86400/481450]\n",
      "loss: 0.284322  [89600/481450]\n",
      "loss: 0.346538  [92800/481450]\n",
      "loss: 0.474549  [96000/481450]\n",
      "loss: 0.224037  [99200/481450]\n",
      "loss: 0.410800  [102400/481450]\n",
      "loss: 0.277541  [105600/481450]\n",
      "loss: 0.176411  [108800/481450]\n",
      "loss: 0.348523  [112000/481450]\n",
      "loss: 0.292382  [115200/481450]\n",
      "loss: 0.485340  [118400/481450]\n",
      "loss: 0.313244  [121600/481450]\n",
      "loss: 0.222105  [124800/481450]\n",
      "loss: 0.191132  [128000/481450]\n",
      "loss: 0.297280  [131200/481450]\n",
      "loss: 0.320154  [134400/481450]\n",
      "loss: 0.290583  [137600/481450]\n",
      "loss: 0.279846  [140800/481450]\n",
      "loss: 0.192966  [144000/481450]\n",
      "loss: 0.215804  [147200/481450]\n",
      "loss: 0.370025  [150400/481450]\n",
      "loss: 0.230882  [153600/481450]\n",
      "loss: 0.321544  [156800/481450]\n",
      "loss: 0.299991  [160000/481450]\n",
      "loss: 0.481345  [163200/481450]\n",
      "loss: 0.375504  [166400/481450]\n",
      "loss: 0.194254  [169600/481450]\n",
      "loss: 0.410505  [172800/481450]\n",
      "loss: 0.225457  [176000/481450]\n",
      "loss: 0.353070  [179200/481450]\n",
      "loss: 0.255152  [182400/481450]\n",
      "loss: 0.286768  [185600/481450]\n",
      "loss: 0.431174  [188800/481450]\n",
      "loss: 0.246622  [192000/481450]\n",
      "loss: 0.307688  [195200/481450]\n",
      "loss: 0.466906  [198400/481450]\n",
      "loss: 0.241768  [201600/481450]\n",
      "loss: 0.358095  [204800/481450]\n",
      "loss: 0.368355  [208000/481450]\n",
      "loss: 0.309543  [211200/481450]\n",
      "loss: 0.474252  [214400/481450]\n",
      "loss: 0.309493  [217600/481450]\n",
      "loss: 0.189636  [220800/481450]\n",
      "loss: 0.454305  [224000/481450]\n",
      "loss: 0.110022  [227200/481450]\n",
      "loss: 0.209953  [230400/481450]\n",
      "loss: 0.386495  [233600/481450]\n",
      "loss: 0.405410  [236800/481450]\n",
      "loss: 0.264385  [240000/481450]\n",
      "loss: 0.424580  [243200/481450]\n",
      "loss: 0.314505  [246400/481450]\n",
      "loss: 0.302539  [249600/481450]\n",
      "loss: 0.114942  [252800/481450]\n",
      "loss: 0.406451  [256000/481450]\n",
      "loss: 0.401629  [259200/481450]\n",
      "loss: 0.410040  [262400/481450]\n",
      "loss: 0.302030  [265600/481450]\n",
      "loss: 0.376167  [268800/481450]\n",
      "loss: 0.145048  [272000/481450]\n",
      "loss: 0.509365  [275200/481450]\n",
      "loss: 0.134840  [278400/481450]\n",
      "loss: 0.267669  [281600/481450]\n",
      "loss: 0.476182  [284800/481450]\n",
      "loss: 0.342504  [288000/481450]\n",
      "loss: 0.396264  [291200/481450]\n",
      "loss: 0.403848  [294400/481450]\n",
      "loss: 0.254553  [297600/481450]\n",
      "loss: 0.415184  [300800/481450]\n",
      "loss: 0.269072  [304000/481450]\n",
      "loss: 0.472846  [307200/481450]\n",
      "loss: 0.117384  [310400/481450]\n",
      "loss: 0.454507  [313600/481450]\n",
      "loss: 0.276487  [316800/481450]\n",
      "loss: 0.226244  [320000/481450]\n",
      "loss: 0.180812  [323200/481450]\n",
      "loss: 0.199663  [326400/481450]\n",
      "loss: 0.224353  [329600/481450]\n",
      "loss: 0.198832  [332800/481450]\n",
      "loss: 0.240841  [336000/481450]\n",
      "loss: 0.192658  [339200/481450]\n",
      "loss: 0.291951  [342400/481450]\n",
      "loss: 0.350595  [345600/481450]\n",
      "loss: 0.144886  [348800/481450]\n",
      "loss: 0.215727  [352000/481450]\n",
      "loss: 0.414628  [355200/481450]\n",
      "loss: 0.457316  [358400/481450]\n",
      "loss: 0.350649  [361600/481450]\n",
      "loss: 0.103091  [364800/481450]\n",
      "loss: 0.244398  [368000/481450]\n",
      "loss: 0.451810  [371200/481450]\n",
      "loss: 0.230772  [374400/481450]\n",
      "loss: 0.194691  [377600/481450]\n",
      "loss: 0.256936  [380800/481450]\n",
      "loss: 0.329797  [384000/481450]\n",
      "loss: 0.122120  [387200/481450]\n",
      "loss: 0.176898  [390400/481450]\n",
      "loss: 0.268768  [393600/481450]\n",
      "loss: 0.208834  [396800/481450]\n",
      "loss: 0.212955  [400000/481450]\n",
      "loss: 0.232662  [403200/481450]\n",
      "loss: 0.185768  [406400/481450]\n",
      "loss: 0.366365  [409600/481450]\n",
      "loss: 0.198370  [412800/481450]\n",
      "loss: 0.167242  [416000/481450]\n",
      "loss: 0.484085  [419200/481450]\n",
      "loss: 0.211228  [422400/481450]\n",
      "loss: 0.304361  [425600/481450]\n",
      "loss: 0.226358  [428800/481450]\n",
      "loss: 0.251514  [432000/481450]\n",
      "loss: 0.153038  [435200/481450]\n",
      "loss: 0.332135  [438400/481450]\n",
      "loss: 0.190577  [441600/481450]\n",
      "loss: 0.353947  [444800/481450]\n",
      "loss: 0.429421  [448000/481450]\n",
      "loss: 0.219091  [451200/481450]\n",
      "loss: 0.200554  [454400/481450]\n",
      "loss: 0.092962  [457600/481450]\n",
      "loss: 0.441362  [460800/481450]\n",
      "loss: 0.262024  [464000/481450]\n",
      "loss: 0.153215  [467200/481450]\n",
      "loss: 0.469120  [470400/481450]\n",
      "loss: 0.320774  [473600/481450]\n",
      "loss: 0.288802  [476800/481450]\n",
      "loss: 0.353833  [480000/481450]\n",
      "Train Accuracy: 89.2248%\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.349639, F1-score: 88.29% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.268423  [    0/481450]\n",
      "loss: 0.281512  [ 3200/481450]\n",
      "loss: 0.171518  [ 6400/481450]\n",
      "loss: 0.277206  [ 9600/481450]\n",
      "loss: 0.281676  [12800/481450]\n",
      "loss: 0.052977  [16000/481450]\n",
      "loss: 0.236670  [19200/481450]\n",
      "loss: 0.225853  [22400/481450]\n",
      "loss: 0.117922  [25600/481450]\n",
      "loss: 0.200570  [28800/481450]\n",
      "loss: 0.329582  [32000/481450]\n",
      "loss: 0.214974  [35200/481450]\n",
      "loss: 0.312374  [38400/481450]\n",
      "loss: 0.362182  [41600/481450]\n",
      "loss: 0.188525  [44800/481450]\n",
      "loss: 0.363414  [48000/481450]\n",
      "loss: 0.120489  [51200/481450]\n",
      "loss: 0.311056  [54400/481450]\n",
      "loss: 0.109294  [57600/481450]\n",
      "loss: 0.243626  [60800/481450]\n",
      "loss: 0.352190  [64000/481450]\n",
      "loss: 0.341895  [67200/481450]\n",
      "loss: 0.208551  [70400/481450]\n",
      "loss: 0.230102  [73600/481450]\n",
      "loss: 0.380838  [76800/481450]\n",
      "loss: 0.233280  [80000/481450]\n",
      "loss: 0.192400  [83200/481450]\n",
      "loss: 0.194260  [86400/481450]\n",
      "loss: 0.198489  [89600/481450]\n",
      "loss: 0.391806  [92800/481450]\n",
      "loss: 0.334147  [96000/481450]\n",
      "loss: 0.226710  [99200/481450]\n",
      "loss: 0.322861  [102400/481450]\n",
      "loss: 0.249380  [105600/481450]\n",
      "loss: 0.202057  [108800/481450]\n",
      "loss: 0.292916  [112000/481450]\n",
      "loss: 0.518430  [115200/481450]\n",
      "loss: 0.394139  [118400/481450]\n",
      "loss: 0.242889  [121600/481450]\n",
      "loss: 0.249934  [124800/481450]\n",
      "loss: 0.302013  [128000/481450]\n",
      "loss: 0.181740  [131200/481450]\n",
      "loss: 0.362828  [134400/481450]\n",
      "loss: 0.192056  [137600/481450]\n",
      "loss: 0.381008  [140800/481450]\n",
      "loss: 0.390574  [144000/481450]\n",
      "loss: 0.078530  [147200/481450]\n",
      "loss: 0.216722  [150400/481450]\n",
      "loss: 0.349868  [153600/481450]\n",
      "loss: 0.412913  [156800/481450]\n",
      "loss: 0.193267  [160000/481450]\n",
      "loss: 0.298259  [163200/481450]\n",
      "loss: 0.242177  [166400/481450]\n",
      "loss: 0.121089  [169600/481450]\n",
      "loss: 0.158238  [172800/481450]\n",
      "loss: 0.301248  [176000/481450]\n",
      "loss: 0.046365  [179200/481450]\n",
      "loss: 0.263403  [182400/481450]\n",
      "loss: 0.281018  [185600/481450]\n",
      "loss: 0.186255  [188800/481450]\n",
      "loss: 0.231028  [192000/481450]\n",
      "loss: 0.325873  [195200/481450]\n",
      "loss: 0.177940  [198400/481450]\n",
      "loss: 0.505675  [201600/481450]\n",
      "loss: 0.149694  [204800/481450]\n",
      "loss: 0.384211  [208000/481450]\n",
      "loss: 0.343445  [211200/481450]\n",
      "loss: 0.417344  [214400/481450]\n",
      "loss: 0.371126  [217600/481450]\n",
      "loss: 0.358515  [220800/481450]\n",
      "loss: 0.413005  [224000/481450]\n",
      "loss: 0.121252  [227200/481450]\n",
      "loss: 0.156204  [230400/481450]\n",
      "loss: 0.139178  [233600/481450]\n",
      "loss: 0.289723  [236800/481450]\n",
      "loss: 0.235996  [240000/481450]\n",
      "loss: 0.200873  [243200/481450]\n",
      "loss: 0.292802  [246400/481450]\n",
      "loss: 0.144452  [249600/481450]\n",
      "loss: 0.195255  [252800/481450]\n",
      "loss: 0.329610  [256000/481450]\n",
      "loss: 0.296220  [259200/481450]\n",
      "loss: 0.339281  [262400/481450]\n",
      "loss: 0.214904  [265600/481450]\n",
      "loss: 0.105763  [268800/481450]\n",
      "loss: 0.134809  [272000/481450]\n",
      "loss: 0.162672  [275200/481450]\n",
      "loss: 0.210626  [278400/481450]\n",
      "loss: 0.250413  [281600/481450]\n",
      "loss: 0.221363  [284800/481450]\n",
      "loss: 0.370917  [288000/481450]\n",
      "loss: 0.241027  [291200/481450]\n",
      "loss: 0.202415  [294400/481450]\n",
      "loss: 0.109896  [297600/481450]\n",
      "loss: 0.376937  [300800/481450]\n",
      "loss: 0.307341  [304000/481450]\n",
      "loss: 0.171454  [307200/481450]\n",
      "loss: 0.397236  [310400/481450]\n",
      "loss: 0.317441  [313600/481450]\n",
      "loss: 0.323212  [316800/481450]\n",
      "loss: 0.339335  [320000/481450]\n",
      "loss: 0.278892  [323200/481450]\n",
      "loss: 0.187011  [326400/481450]\n",
      "loss: 0.132250  [329600/481450]\n",
      "loss: 0.223581  [332800/481450]\n",
      "loss: 0.114895  [336000/481450]\n",
      "loss: 0.275880  [339200/481450]\n",
      "loss: 0.212656  [342400/481450]\n",
      "loss: 0.358966  [345600/481450]\n",
      "loss: 0.121996  [348800/481450]\n",
      "loss: 0.421215  [352000/481450]\n",
      "loss: 0.224266  [355200/481450]\n",
      "loss: 0.380492  [358400/481450]\n",
      "loss: 0.266345  [361600/481450]\n",
      "loss: 0.137042  [364800/481450]\n",
      "loss: 0.069569  [368000/481450]\n",
      "loss: 0.172430  [371200/481450]\n",
      "loss: 0.376881  [374400/481450]\n",
      "loss: 0.200449  [377600/481450]\n",
      "loss: 0.127419  [380800/481450]\n",
      "loss: 0.214221  [384000/481450]\n",
      "loss: 0.298241  [387200/481450]\n",
      "loss: 0.256527  [390400/481450]\n",
      "loss: 0.289698  [393600/481450]\n",
      "loss: 0.256656  [396800/481450]\n",
      "loss: 0.125811  [400000/481450]\n",
      "loss: 0.344146  [403200/481450]\n",
      "loss: 0.264411  [406400/481450]\n",
      "loss: 0.111594  [409600/481450]\n",
      "loss: 0.339793  [412800/481450]\n",
      "loss: 0.313894  [416000/481450]\n",
      "loss: 0.351357  [419200/481450]\n",
      "loss: 0.154606  [422400/481450]\n",
      "loss: 0.172164  [425600/481450]\n",
      "loss: 0.310428  [428800/481450]\n",
      "loss: 0.242984  [432000/481450]\n",
      "loss: 0.295371  [435200/481450]\n",
      "loss: 0.220380  [438400/481450]\n",
      "loss: 0.063348  [441600/481450]\n",
      "loss: 0.378446  [444800/481450]\n",
      "loss: 0.096279  [448000/481450]\n",
      "loss: 0.333188  [451200/481450]\n",
      "loss: 0.117707  [454400/481450]\n",
      "loss: 0.266249  [457600/481450]\n",
      "loss: 0.281549  [460800/481450]\n",
      "loss: 0.240997  [464000/481450]\n",
      "loss: 0.118190  [467200/481450]\n",
      "loss: 0.434632  [470400/481450]\n",
      "loss: 0.185624  [473600/481450]\n",
      "loss: 0.226908  [476800/481450]\n",
      "loss: 0.233514  [480000/481450]\n",
      "Train Accuracy: 89.6020%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.380936, F1-score: 87.63% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.165175  [    0/481450]\n",
      "loss: 0.175454  [ 3200/481450]\n",
      "loss: 0.282082  [ 6400/481450]\n",
      "loss: 0.195234  [ 9600/481450]\n",
      "loss: 0.408138  [12800/481450]\n",
      "loss: 0.188621  [16000/481450]\n",
      "loss: 0.279488  [19200/481450]\n",
      "loss: 0.086398  [22400/481450]\n",
      "loss: 0.272009  [25600/481450]\n",
      "loss: 0.453657  [28800/481450]\n",
      "loss: 0.169909  [32000/481450]\n",
      "loss: 0.098750  [35200/481450]\n",
      "loss: 0.204722  [38400/481450]\n",
      "loss: 0.200050  [41600/481450]\n",
      "loss: 0.420614  [44800/481450]\n",
      "loss: 0.425523  [48000/481450]\n",
      "loss: 0.218702  [51200/481450]\n",
      "loss: 0.290083  [54400/481450]\n",
      "loss: 0.232086  [57600/481450]\n",
      "loss: 0.335111  [60800/481450]\n",
      "loss: 0.264934  [64000/481450]\n",
      "loss: 0.301827  [67200/481450]\n",
      "loss: 0.157646  [70400/481450]\n",
      "loss: 0.217383  [73600/481450]\n",
      "loss: 0.384668  [76800/481450]\n",
      "loss: 0.284402  [80000/481450]\n",
      "loss: 0.272775  [83200/481450]\n",
      "loss: 0.093055  [86400/481450]\n",
      "loss: 0.302544  [89600/481450]\n",
      "loss: 0.289063  [92800/481450]\n",
      "loss: 0.234639  [96000/481450]\n",
      "loss: 0.118038  [99200/481450]\n",
      "loss: 0.117713  [102400/481450]\n",
      "loss: 0.241494  [105600/481450]\n",
      "loss: 0.125053  [108800/481450]\n",
      "loss: 0.308631  [112000/481450]\n",
      "loss: 0.292158  [115200/481450]\n",
      "loss: 0.338713  [118400/481450]\n",
      "loss: 0.262800  [121600/481450]\n",
      "loss: 0.169634  [124800/481450]\n",
      "loss: 0.269389  [128000/481450]\n",
      "loss: 0.353626  [131200/481450]\n",
      "loss: 0.205216  [134400/481450]\n",
      "loss: 0.509228  [137600/481450]\n",
      "loss: 0.095887  [140800/481450]\n",
      "loss: 0.260509  [144000/481450]\n",
      "loss: 0.289373  [147200/481450]\n",
      "loss: 0.253555  [150400/481450]\n",
      "loss: 0.494695  [153600/481450]\n",
      "loss: 0.261078  [156800/481450]\n",
      "loss: 0.138596  [160000/481450]\n",
      "loss: 0.463010  [163200/481450]\n",
      "loss: 0.507024  [166400/481450]\n",
      "loss: 0.116555  [169600/481450]\n",
      "loss: 0.270805  [172800/481450]\n",
      "loss: 0.150321  [176000/481450]\n",
      "loss: 0.470526  [179200/481450]\n",
      "loss: 0.332328  [182400/481450]\n",
      "loss: 0.116193  [185600/481450]\n",
      "loss: 0.201467  [188800/481450]\n",
      "loss: 0.120236  [192000/481450]\n",
      "loss: 0.255297  [195200/481450]\n",
      "loss: 0.344842  [198400/481450]\n",
      "loss: 0.323884  [201600/481450]\n",
      "loss: 0.272129  [204800/481450]\n",
      "loss: 0.336533  [208000/481450]\n",
      "loss: 0.316542  [211200/481450]\n",
      "loss: 0.369841  [214400/481450]\n",
      "loss: 0.371835  [217600/481450]\n",
      "loss: 0.197272  [220800/481450]\n",
      "loss: 0.389578  [224000/481450]\n",
      "loss: 0.215666  [227200/481450]\n",
      "loss: 0.324998  [230400/481450]\n",
      "loss: 0.330402  [233600/481450]\n",
      "loss: 0.265573  [236800/481450]\n",
      "loss: 0.178209  [240000/481450]\n",
      "loss: 0.383489  [243200/481450]\n",
      "loss: 0.146314  [246400/481450]\n",
      "loss: 0.450937  [249600/481450]\n",
      "loss: 0.197264  [252800/481450]\n",
      "loss: 0.421966  [256000/481450]\n",
      "loss: 0.139525  [259200/481450]\n",
      "loss: 0.196153  [262400/481450]\n",
      "loss: 0.315393  [265600/481450]\n",
      "loss: 0.286269  [268800/481450]\n",
      "loss: 0.160445  [272000/481450]\n",
      "loss: 0.478288  [275200/481450]\n",
      "loss: 0.253110  [278400/481450]\n",
      "loss: 0.237558  [281600/481450]\n",
      "loss: 0.098786  [284800/481450]\n",
      "loss: 0.158539  [288000/481450]\n",
      "loss: 0.348301  [291200/481450]\n",
      "loss: 0.187118  [294400/481450]\n",
      "loss: 0.420518  [297600/481450]\n",
      "loss: 0.204340  [300800/481450]\n",
      "loss: 0.134211  [304000/481450]\n",
      "loss: 0.180127  [307200/481450]\n",
      "loss: 0.134075  [310400/481450]\n",
      "loss: 0.178533  [313600/481450]\n",
      "loss: 0.317536  [316800/481450]\n",
      "loss: 0.327284  [320000/481450]\n",
      "loss: 0.217482  [323200/481450]\n",
      "loss: 0.158427  [326400/481450]\n",
      "loss: 0.018920  [329600/481450]\n",
      "loss: 0.234722  [332800/481450]\n",
      "loss: 0.197694  [336000/481450]\n",
      "loss: 0.434455  [339200/481450]\n",
      "loss: 0.171726  [342400/481450]\n",
      "loss: 0.225739  [345600/481450]\n",
      "loss: 0.091437  [348800/481450]\n",
      "loss: 0.362833  [352000/481450]\n",
      "loss: 0.382136  [355200/481450]\n",
      "loss: 0.271682  [358400/481450]\n",
      "loss: 0.207462  [361600/481450]\n",
      "loss: 0.348455  [364800/481450]\n",
      "loss: 0.133533  [368000/481450]\n",
      "loss: 0.144600  [371200/481450]\n",
      "loss: 0.391528  [374400/481450]\n",
      "loss: 0.210015  [377600/481450]\n",
      "loss: 0.186419  [380800/481450]\n",
      "loss: 0.351200  [384000/481450]\n",
      "loss: 0.194635  [387200/481450]\n",
      "loss: 0.187145  [390400/481450]\n",
      "loss: 0.409266  [393600/481450]\n",
      "loss: 0.181246  [396800/481450]\n",
      "loss: 0.274283  [400000/481450]\n",
      "loss: 0.101350  [403200/481450]\n",
      "loss: 0.093160  [406400/481450]\n",
      "loss: 0.280120  [409600/481450]\n",
      "loss: 0.345425  [412800/481450]\n",
      "loss: 0.094326  [416000/481450]\n",
      "loss: 0.432977  [419200/481450]\n",
      "loss: 0.442019  [422400/481450]\n",
      "loss: 0.411713  [425600/481450]\n",
      "loss: 0.157816  [428800/481450]\n",
      "loss: 0.657749  [432000/481450]\n",
      "loss: 0.202235  [435200/481450]\n",
      "loss: 0.232460  [438400/481450]\n",
      "loss: 0.877298  [441600/481450]\n",
      "loss: 0.167369  [444800/481450]\n",
      "loss: 0.283377  [448000/481450]\n",
      "loss: 0.284739  [451200/481450]\n",
      "loss: 0.299178  [454400/481450]\n",
      "loss: 0.499513  [457600/481450]\n",
      "loss: 0.190908  [460800/481450]\n",
      "loss: 0.296977  [464000/481450]\n",
      "loss: 0.289671  [467200/481450]\n",
      "loss: 0.217582  [470400/481450]\n",
      "loss: 0.265348  [473600/481450]\n",
      "loss: 0.041964  [476800/481450]\n",
      "loss: 0.331888  [480000/481450]\n",
      "Train Accuracy: 89.9389%\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.389606, F1-score: 87.64% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.467270  [    0/481450]\n",
      "loss: 0.206787  [ 3200/481450]\n",
      "loss: 0.268609  [ 6400/481450]\n",
      "loss: 0.086149  [ 9600/481450]\n",
      "loss: 0.228577  [12800/481450]\n",
      "loss: 0.217503  [16000/481450]\n",
      "loss: 0.552004  [19200/481450]\n",
      "loss: 0.224667  [22400/481450]\n",
      "loss: 0.381599  [25600/481450]\n",
      "loss: 0.172407  [28800/481450]\n",
      "loss: 0.256106  [32000/481450]\n",
      "loss: 0.286923  [35200/481450]\n",
      "loss: 0.212463  [38400/481450]\n",
      "loss: 0.360455  [41600/481450]\n",
      "loss: 0.131434  [44800/481450]\n",
      "loss: 0.105330  [48000/481450]\n",
      "loss: 0.321716  [51200/481450]\n",
      "loss: 0.039738  [54400/481450]\n",
      "loss: 0.331720  [57600/481450]\n",
      "loss: 0.249298  [60800/481450]\n",
      "loss: 0.203581  [64000/481450]\n",
      "loss: 0.074788  [67200/481450]\n",
      "loss: 0.185648  [70400/481450]\n",
      "loss: 0.100047  [73600/481450]\n",
      "loss: 0.153946  [76800/481450]\n",
      "loss: 0.284732  [80000/481450]\n",
      "loss: 0.448362  [83200/481450]\n",
      "loss: 0.117558  [86400/481450]\n",
      "loss: 0.281500  [89600/481450]\n",
      "loss: 0.377223  [92800/481450]\n",
      "loss: 0.324045  [96000/481450]\n",
      "loss: 0.210296  [99200/481450]\n",
      "loss: 0.202501  [102400/481450]\n",
      "loss: 0.182247  [105600/481450]\n",
      "loss: 0.188947  [108800/481450]\n",
      "loss: 0.131664  [112000/481450]\n",
      "loss: 0.306544  [115200/481450]\n",
      "loss: 0.128745  [118400/481450]\n",
      "loss: 0.156927  [121600/481450]\n",
      "loss: 0.108270  [124800/481450]\n",
      "loss: 0.466362  [128000/481450]\n",
      "loss: 0.303714  [131200/481450]\n",
      "loss: 0.248242  [134400/481450]\n",
      "loss: 0.281121  [137600/481450]\n",
      "loss: 0.206211  [140800/481450]\n",
      "loss: 0.129635  [144000/481450]\n",
      "loss: 0.506326  [147200/481450]\n",
      "loss: 0.238332  [150400/481450]\n",
      "loss: 0.332484  [153600/481450]\n",
      "loss: 0.216413  [156800/481450]\n",
      "loss: 0.453149  [160000/481450]\n",
      "loss: 0.222671  [163200/481450]\n",
      "loss: 0.149816  [166400/481450]\n",
      "loss: 0.267552  [169600/481450]\n",
      "loss: 0.332409  [172800/481450]\n",
      "loss: 0.114713  [176000/481450]\n",
      "loss: 0.140568  [179200/481450]\n",
      "loss: 0.162069  [182400/481450]\n",
      "loss: 0.341645  [185600/481450]\n",
      "loss: 0.215810  [188800/481450]\n",
      "loss: 0.204254  [192000/481450]\n",
      "loss: 0.140396  [195200/481450]\n",
      "loss: 0.066714  [198400/481450]\n",
      "loss: 0.441577  [201600/481450]\n",
      "loss: 0.513105  [204800/481450]\n",
      "loss: 0.202084  [208000/481450]\n",
      "loss: 0.334030  [211200/481450]\n",
      "loss: 0.493391  [214400/481450]\n",
      "loss: 0.222617  [217600/481450]\n",
      "loss: 0.165186  [220800/481450]\n",
      "loss: 0.107737  [224000/481450]\n",
      "loss: 0.234159  [227200/481450]\n",
      "loss: 0.237424  [230400/481450]\n",
      "loss: 0.272728  [233600/481450]\n",
      "loss: 0.568969  [236800/481450]\n",
      "loss: 0.395324  [240000/481450]\n",
      "loss: 0.105715  [243200/481450]\n",
      "loss: 0.399972  [246400/481450]\n",
      "loss: 0.395868  [249600/481450]\n",
      "loss: 0.150931  [252800/481450]\n",
      "loss: 0.123578  [256000/481450]\n",
      "loss: 0.197518  [259200/481450]\n",
      "loss: 0.100554  [262400/481450]\n",
      "loss: 0.190302  [265600/481450]\n",
      "loss: 0.435443  [268800/481450]\n",
      "loss: 0.450886  [272000/481450]\n",
      "loss: 0.177437  [275200/481450]\n",
      "loss: 0.250864  [278400/481450]\n",
      "loss: 0.526734  [281600/481450]\n",
      "loss: 0.161217  [284800/481450]\n",
      "loss: 0.268500  [288000/481450]\n",
      "loss: 0.266494  [291200/481450]\n",
      "loss: 0.302402  [294400/481450]\n",
      "loss: 0.016849  [297600/481450]\n",
      "loss: 0.157743  [300800/481450]\n",
      "loss: 0.203174  [304000/481450]\n",
      "loss: 0.173784  [307200/481450]\n",
      "loss: 0.109912  [310400/481450]\n",
      "loss: 0.111663  [313600/481450]\n",
      "loss: 0.165105  [316800/481450]\n",
      "loss: 0.359730  [320000/481450]\n",
      "loss: 0.189370  [323200/481450]\n",
      "loss: 0.258590  [326400/481450]\n",
      "loss: 0.141983  [329600/481450]\n",
      "loss: 0.286516  [332800/481450]\n",
      "loss: 0.280610  [336000/481450]\n",
      "loss: 0.196340  [339200/481450]\n",
      "loss: 0.049127  [342400/481450]\n",
      "loss: 0.493430  [345600/481450]\n",
      "loss: 0.303408  [348800/481450]\n",
      "loss: 0.210660  [352000/481450]\n",
      "loss: 0.055392  [355200/481450]\n",
      "loss: 0.223329  [358400/481450]\n",
      "loss: 0.238545  [361600/481450]\n",
      "loss: 0.325078  [364800/481450]\n",
      "loss: 0.316645  [368000/481450]\n",
      "loss: 0.154787  [371200/481450]\n",
      "loss: 0.220181  [374400/481450]\n",
      "loss: 0.277855  [377600/481450]\n",
      "loss: 0.168794  [380800/481450]\n",
      "loss: 0.330669  [384000/481450]\n",
      "loss: 0.300542  [387200/481450]\n",
      "loss: 0.116279  [390400/481450]\n",
      "loss: 0.189766  [393600/481450]\n",
      "loss: 0.059226  [396800/481450]\n",
      "loss: 0.264649  [400000/481450]\n",
      "loss: 0.573949  [403200/481450]\n",
      "loss: 0.447318  [406400/481450]\n",
      "loss: 0.406994  [409600/481450]\n",
      "loss: 0.386239  [412800/481450]\n",
      "loss: 0.406576  [416000/481450]\n",
      "loss: 0.195115  [419200/481450]\n",
      "loss: 0.185446  [422400/481450]\n",
      "loss: 0.319583  [425600/481450]\n",
      "loss: 0.302160  [428800/481450]\n",
      "loss: 0.200815  [432000/481450]\n",
      "loss: 0.335735  [435200/481450]\n",
      "loss: 0.381306  [438400/481450]\n",
      "loss: 0.070855  [441600/481450]\n",
      "loss: 0.141866  [444800/481450]\n",
      "loss: 0.076681  [448000/481450]\n",
      "loss: 0.191862  [451200/481450]\n",
      "loss: 0.085649  [454400/481450]\n",
      "loss: 0.180389  [457600/481450]\n",
      "loss: 0.177802  [460800/481450]\n",
      "loss: 0.056313  [464000/481450]\n",
      "loss: 0.328187  [467200/481450]\n",
      "loss: 0.175798  [470400/481450]\n",
      "loss: 0.739047  [473600/481450]\n",
      "loss: 0.132406  [476800/481450]\n",
      "loss: 0.165781  [480000/481450]\n",
      "Train Accuracy: 90.2210%\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.425283, F1-score: 86.98% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.248545  [    0/481450]\n",
      "loss: 0.231028  [ 3200/481450]\n",
      "loss: 0.196304  [ 6400/481450]\n",
      "loss: 0.445425  [ 9600/481450]\n",
      "loss: 0.106207  [12800/481450]\n",
      "loss: 0.071025  [16000/481450]\n",
      "loss: 0.297566  [19200/481450]\n",
      "loss: 0.352504  [22400/481450]\n",
      "loss: 0.307445  [25600/481450]\n",
      "loss: 0.376050  [28800/481450]\n",
      "loss: 0.067362  [32000/481450]\n",
      "loss: 0.180539  [35200/481450]\n",
      "loss: 0.175673  [38400/481450]\n",
      "loss: 0.177924  [41600/481450]\n",
      "loss: 0.286758  [44800/481450]\n",
      "loss: 0.400830  [48000/481450]\n",
      "loss: 0.279140  [51200/481450]\n",
      "loss: 0.104754  [54400/481450]\n",
      "loss: 0.228002  [57600/481450]\n",
      "loss: 0.275591  [60800/481450]\n",
      "loss: 0.220797  [64000/481450]\n",
      "loss: 0.214238  [67200/481450]\n",
      "loss: 0.128405  [70400/481450]\n",
      "loss: 0.085470  [73600/481450]\n",
      "loss: 0.148036  [76800/481450]\n",
      "loss: 0.193812  [80000/481450]\n",
      "loss: 0.098738  [83200/481450]\n",
      "loss: 0.321994  [86400/481450]\n",
      "loss: 0.175727  [89600/481450]\n",
      "loss: 0.164933  [92800/481450]\n",
      "loss: 0.373308  [96000/481450]\n",
      "loss: 0.233867  [99200/481450]\n",
      "loss: 0.316320  [102400/481450]\n",
      "loss: 0.272105  [105600/481450]\n",
      "loss: 0.117634  [108800/481450]\n",
      "loss: 0.142716  [112000/481450]\n",
      "loss: 0.254660  [115200/481450]\n",
      "loss: 0.249604  [118400/481450]\n",
      "loss: 0.230639  [121600/481450]\n",
      "loss: 0.088810  [124800/481450]\n",
      "loss: 0.208512  [128000/481450]\n",
      "loss: 0.147630  [131200/481450]\n",
      "loss: 0.185436  [134400/481450]\n",
      "loss: 0.077860  [137600/481450]\n",
      "loss: 0.164956  [140800/481450]\n",
      "loss: 0.544862  [144000/481450]\n",
      "loss: 0.346723  [147200/481450]\n",
      "loss: 0.348450  [150400/481450]\n",
      "loss: 0.254710  [153600/481450]\n",
      "loss: 0.159683  [156800/481450]\n",
      "loss: 0.217013  [160000/481450]\n",
      "loss: 0.342629  [163200/481450]\n",
      "loss: 0.140070  [166400/481450]\n",
      "loss: 0.230265  [169600/481450]\n",
      "loss: 0.274705  [172800/481450]\n",
      "loss: 0.293368  [176000/481450]\n",
      "loss: 0.335848  [179200/481450]\n",
      "loss: 0.363376  [182400/481450]\n",
      "loss: 0.293481  [185600/481450]\n",
      "loss: 0.187251  [188800/481450]\n",
      "loss: 0.105650  [192000/481450]\n",
      "loss: 0.044380  [195200/481450]\n",
      "loss: 0.311443  [198400/481450]\n",
      "loss: 0.447510  [201600/481450]\n",
      "loss: 0.046398  [204800/481450]\n",
      "loss: 0.384162  [208000/481450]\n",
      "loss: 0.076660  [211200/481450]\n",
      "loss: 0.459799  [214400/481450]\n",
      "loss: 0.315045  [217600/481450]\n",
      "loss: 0.451330  [220800/481450]\n",
      "loss: 0.215314  [224000/481450]\n",
      "loss: 0.149643  [227200/481450]\n",
      "loss: 0.255350  [230400/481450]\n",
      "loss: 0.248368  [233600/481450]\n",
      "loss: 0.320195  [236800/481450]\n",
      "loss: 0.262575  [240000/481450]\n",
      "loss: 0.272250  [243200/481450]\n",
      "loss: 0.160553  [246400/481450]\n",
      "loss: 0.307744  [249600/481450]\n",
      "loss: 0.305294  [252800/481450]\n",
      "loss: 0.551225  [256000/481450]\n",
      "loss: 0.174319  [259200/481450]\n",
      "loss: 0.140613  [262400/481450]\n",
      "loss: 0.381002  [265600/481450]\n",
      "loss: 0.130485  [268800/481450]\n",
      "loss: 0.306406  [272000/481450]\n",
      "loss: 0.141585  [275200/481450]\n",
      "loss: 0.192782  [278400/481450]\n",
      "loss: 0.204774  [281600/481450]\n",
      "loss: 0.150797  [284800/481450]\n",
      "loss: 0.347242  [288000/481450]\n",
      "loss: 0.233200  [291200/481450]\n",
      "loss: 0.051855  [294400/481450]\n",
      "loss: 0.081638  [297600/481450]\n",
      "loss: 0.302992  [300800/481450]\n",
      "loss: 0.345083  [304000/481450]\n",
      "loss: 0.266710  [307200/481450]\n",
      "loss: 0.206055  [310400/481450]\n",
      "loss: 0.178908  [313600/481450]\n",
      "loss: 0.240527  [316800/481450]\n",
      "loss: 0.250154  [320000/481450]\n",
      "loss: 0.182200  [323200/481450]\n",
      "loss: 0.418993  [326400/481450]\n",
      "loss: 0.295808  [329600/481450]\n",
      "loss: 0.440278  [332800/481450]\n",
      "loss: 0.338842  [336000/481450]\n",
      "loss: 0.322605  [339200/481450]\n",
      "loss: 0.320389  [342400/481450]\n",
      "loss: 0.105449  [345600/481450]\n",
      "loss: 0.445673  [348800/481450]\n",
      "loss: 0.219013  [352000/481450]\n",
      "loss: 0.237711  [355200/481450]\n",
      "loss: 0.436889  [358400/481450]\n",
      "loss: 0.400641  [361600/481450]\n",
      "loss: 0.355657  [364800/481450]\n",
      "loss: 0.227942  [368000/481450]\n",
      "loss: 0.199091  [371200/481450]\n",
      "loss: 0.210312  [374400/481450]\n",
      "loss: 0.120237  [377600/481450]\n",
      "loss: 0.145010  [380800/481450]\n",
      "loss: 0.057918  [384000/481450]\n",
      "loss: 0.231164  [387200/481450]\n",
      "loss: 0.173796  [390400/481450]\n",
      "loss: 0.144694  [393600/481450]\n",
      "loss: 0.400764  [396800/481450]\n",
      "loss: 0.361596  [400000/481450]\n",
      "loss: 0.181446  [403200/481450]\n",
      "loss: 0.249600  [406400/481450]\n",
      "loss: 0.218075  [409600/481450]\n",
      "loss: 0.229519  [412800/481450]\n",
      "loss: 0.157763  [416000/481450]\n",
      "loss: 0.311765  [419200/481450]\n",
      "loss: 0.276248  [422400/481450]\n",
      "loss: 0.387663  [425600/481450]\n",
      "loss: 0.193662  [428800/481450]\n",
      "loss: 0.176847  [432000/481450]\n",
      "loss: 0.047164  [435200/481450]\n",
      "loss: 0.103383  [438400/481450]\n",
      "loss: 0.360632  [441600/481450]\n",
      "loss: 0.149462  [444800/481450]\n",
      "loss: 0.266984  [448000/481450]\n",
      "loss: 0.258446  [451200/481450]\n",
      "loss: 0.284854  [454400/481450]\n",
      "loss: 0.995902  [457600/481450]\n",
      "loss: 0.353681  [460800/481450]\n",
      "loss: 0.352913  [464000/481450]\n",
      "loss: 0.281944  [467200/481450]\n",
      "loss: 0.176364  [470400/481450]\n",
      "loss: 0.328649  [473600/481450]\n",
      "loss: 0.056505  [476800/481450]\n",
      "loss: 0.080090  [480000/481450]\n",
      "Train Accuracy: 90.4368%\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.442893, F1-score: 86.29% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.096055  [    0/481450]\n",
      "loss: 0.333723  [ 3200/481450]\n",
      "loss: 0.189231  [ 6400/481450]\n",
      "loss: 0.059920  [ 9600/481450]\n",
      "loss: 0.305105  [12800/481450]\n",
      "loss: 0.259268  [16000/481450]\n",
      "loss: 0.148986  [19200/481450]\n",
      "loss: 0.504801  [22400/481450]\n",
      "loss: 0.445106  [25600/481450]\n",
      "loss: 0.270903  [28800/481450]\n",
      "loss: 0.221218  [32000/481450]\n",
      "loss: 0.122407  [35200/481450]\n",
      "loss: 0.160384  [38400/481450]\n",
      "loss: 0.279717  [41600/481450]\n",
      "loss: 0.151988  [44800/481450]\n",
      "loss: 0.164314  [48000/481450]\n",
      "loss: 0.240521  [51200/481450]\n",
      "loss: 0.168368  [54400/481450]\n",
      "loss: 0.160867  [57600/481450]\n",
      "loss: 0.128184  [60800/481450]\n",
      "loss: 0.268707  [64000/481450]\n",
      "loss: 0.353413  [67200/481450]\n",
      "loss: 0.152325  [70400/481450]\n",
      "loss: 0.099749  [73600/481450]\n",
      "loss: 0.148044  [76800/481450]\n",
      "loss: 0.063345  [80000/481450]\n",
      "loss: 0.095811  [83200/481450]\n",
      "loss: 0.154690  [86400/481450]\n",
      "loss: 0.308930  [89600/481450]\n",
      "loss: 0.461914  [92800/481450]\n",
      "loss: 0.503137  [96000/481450]\n",
      "loss: 0.179434  [99200/481450]\n",
      "loss: 0.140934  [102400/481450]\n",
      "loss: 0.279618  [105600/481450]\n",
      "loss: 0.141218  [108800/481450]\n",
      "loss: 0.150405  [112000/481450]\n",
      "loss: 0.344716  [115200/481450]\n",
      "loss: 0.407346  [118400/481450]\n",
      "loss: 0.324852  [121600/481450]\n",
      "loss: 0.262944  [124800/481450]\n",
      "loss: 0.130235  [128000/481450]\n",
      "loss: 0.171511  [131200/481450]\n",
      "loss: 0.181296  [134400/481450]\n",
      "loss: 0.224182  [137600/481450]\n",
      "loss: 0.092126  [140800/481450]\n",
      "loss: 0.105837  [144000/481450]\n",
      "loss: 0.301709  [147200/481450]\n",
      "loss: 0.211653  [150400/481450]\n",
      "loss: 0.260043  [153600/481450]\n",
      "loss: 0.238739  [156800/481450]\n",
      "loss: 0.172493  [160000/481450]\n",
      "loss: 0.121896  [163200/481450]\n",
      "loss: 0.303498  [166400/481450]\n",
      "loss: 0.349665  [169600/481450]\n",
      "loss: 0.258104  [172800/481450]\n",
      "loss: 0.134926  [176000/481450]\n",
      "loss: 0.235974  [179200/481450]\n",
      "loss: 0.151817  [182400/481450]\n",
      "loss: 0.191131  [185600/481450]\n",
      "loss: 0.187594  [188800/481450]\n",
      "loss: 0.231910  [192000/481450]\n",
      "loss: 0.398653  [195200/481450]\n",
      "loss: 0.445932  [198400/481450]\n",
      "loss: 0.230771  [201600/481450]\n",
      "loss: 0.276893  [204800/481450]\n",
      "loss: 0.408717  [208000/481450]\n",
      "loss: 0.118619  [211200/481450]\n",
      "loss: 0.251712  [214400/481450]\n",
      "loss: 0.217491  [217600/481450]\n",
      "loss: 0.292872  [220800/481450]\n",
      "loss: 0.437216  [224000/481450]\n",
      "loss: 0.040560  [227200/481450]\n",
      "loss: 0.336084  [230400/481450]\n",
      "loss: 0.185264  [233600/481450]\n",
      "loss: 0.263068  [236800/481450]\n",
      "loss: 0.236912  [240000/481450]\n",
      "loss: 0.084911  [243200/481450]\n",
      "loss: 0.108507  [246400/481450]\n",
      "loss: 0.348507  [249600/481450]\n",
      "loss: 0.410420  [252800/481450]\n",
      "loss: 0.359842  [256000/481450]\n",
      "loss: 0.131990  [259200/481450]\n",
      "loss: 0.222922  [262400/481450]\n",
      "loss: 0.294812  [265600/481450]\n",
      "loss: 0.229585  [268800/481450]\n",
      "loss: 0.284574  [272000/481450]\n",
      "loss: 0.308686  [275200/481450]\n",
      "loss: 0.243436  [278400/481450]\n",
      "loss: 0.178850  [281600/481450]\n",
      "loss: 0.197131  [284800/481450]\n",
      "loss: 0.308724  [288000/481450]\n",
      "loss: 0.417538  [291200/481450]\n",
      "loss: 0.144516  [294400/481450]\n",
      "loss: 0.225146  [297600/481450]\n",
      "loss: 0.252091  [300800/481450]\n",
      "loss: 0.148897  [304000/481450]\n",
      "loss: 0.427232  [307200/481450]\n",
      "loss: 0.192244  [310400/481450]\n",
      "loss: 0.217705  [313600/481450]\n",
      "loss: 0.243601  [316800/481450]\n",
      "loss: 0.136897  [320000/481450]\n",
      "loss: 0.406861  [323200/481450]\n",
      "loss: 0.382841  [326400/481450]\n",
      "loss: 0.460604  [329600/481450]\n",
      "loss: 0.294327  [332800/481450]\n",
      "loss: 0.321976  [336000/481450]\n",
      "loss: 0.304473  [339200/481450]\n",
      "loss: 0.236365  [342400/481450]\n",
      "loss: 0.302405  [345600/481450]\n",
      "loss: 0.269208  [348800/481450]\n",
      "loss: 0.287872  [352000/481450]\n",
      "loss: 0.342497  [355200/481450]\n",
      "loss: 0.331691  [358400/481450]\n",
      "loss: 0.469218  [361600/481450]\n",
      "loss: 0.259878  [364800/481450]\n",
      "loss: 0.297518  [368000/481450]\n",
      "loss: 0.404549  [371200/481450]\n",
      "loss: 0.294256  [374400/481450]\n",
      "loss: 0.147323  [377600/481450]\n",
      "loss: 0.534343  [380800/481450]\n",
      "loss: 0.223399  [384000/481450]\n",
      "loss: 0.266685  [387200/481450]\n",
      "loss: 0.210482  [390400/481450]\n",
      "loss: 0.273555  [393600/481450]\n",
      "loss: 0.057674  [396800/481450]\n",
      "loss: 0.300027  [400000/481450]\n",
      "loss: 0.265530  [403200/481450]\n",
      "loss: 0.245966  [406400/481450]\n",
      "loss: 0.369854  [409600/481450]\n",
      "loss: 0.404574  [412800/481450]\n",
      "loss: 0.140393  [416000/481450]\n",
      "loss: 0.240659  [419200/481450]\n",
      "loss: 0.121959  [422400/481450]\n",
      "loss: 0.289997  [425600/481450]\n",
      "loss: 0.148130  [428800/481450]\n",
      "loss: 0.444204  [432000/481450]\n",
      "loss: 0.076078  [435200/481450]\n",
      "loss: 0.217587  [438400/481450]\n",
      "loss: 0.136963  [441600/481450]\n",
      "loss: 0.465975  [444800/481450]\n",
      "loss: 0.380594  [448000/481450]\n",
      "loss: 0.197788  [451200/481450]\n",
      "loss: 0.287872  [454400/481450]\n",
      "loss: 0.244875  [457600/481450]\n",
      "loss: 0.371422  [460800/481450]\n",
      "loss: 0.137845  [464000/481450]\n",
      "loss: 0.227294  [467200/481450]\n",
      "loss: 0.177602  [470400/481450]\n",
      "loss: 0.335653  [473600/481450]\n",
      "loss: 0.139384  [476800/481450]\n",
      "loss: 0.247053  [480000/481450]\n",
      "Train Accuracy: 90.6329%\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.453547, F1-score: 86.24% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.181961  [    0/481450]\n",
      "loss: 0.125061  [ 3200/481450]\n",
      "loss: 0.314669  [ 6400/481450]\n",
      "loss: 0.087636  [ 9600/481450]\n",
      "loss: 0.193767  [12800/481450]\n",
      "loss: 0.238245  [16000/481450]\n",
      "loss: 0.475562  [19200/481450]\n",
      "loss: 0.150108  [22400/481450]\n",
      "loss: 0.144418  [25600/481450]\n",
      "loss: 0.413482  [28800/481450]\n",
      "loss: 0.158305  [32000/481450]\n",
      "loss: 0.192836  [35200/481450]\n",
      "loss: 0.221444  [38400/481450]\n",
      "loss: 0.094931  [41600/481450]\n",
      "loss: 0.169851  [44800/481450]\n",
      "loss: 0.160565  [48000/481450]\n",
      "loss: 0.091330  [51200/481450]\n",
      "loss: 0.401783  [54400/481450]\n",
      "loss: 0.278994  [57600/481450]\n",
      "loss: 0.212596  [60800/481450]\n",
      "loss: 0.203706  [64000/481450]\n",
      "loss: 0.142958  [67200/481450]\n",
      "loss: 0.300566  [70400/481450]\n",
      "loss: 0.144636  [73600/481450]\n",
      "loss: 0.263641  [76800/481450]\n",
      "loss: 0.220436  [80000/481450]\n",
      "loss: 0.339744  [83200/481450]\n",
      "loss: 0.132472  [86400/481450]\n",
      "loss: 0.308717  [89600/481450]\n",
      "loss: 0.497381  [92800/481450]\n",
      "loss: 0.244211  [96000/481450]\n",
      "loss: 0.190929  [99200/481450]\n",
      "loss: 0.361813  [102400/481450]\n",
      "loss: 0.102763  [105600/481450]\n",
      "loss: 0.237441  [108800/481450]\n",
      "loss: 0.209750  [112000/481450]\n",
      "loss: 0.132097  [115200/481450]\n",
      "loss: 0.188340  [118400/481450]\n",
      "loss: 0.147214  [121600/481450]\n",
      "loss: 0.319461  [124800/481450]\n",
      "loss: 0.332038  [128000/481450]\n",
      "loss: 0.133478  [131200/481450]\n",
      "loss: 0.192962  [134400/481450]\n",
      "loss: 0.154992  [137600/481450]\n",
      "loss: 0.572581  [140800/481450]\n",
      "loss: 0.303101  [144000/481450]\n",
      "loss: 0.138232  [147200/481450]\n",
      "loss: 0.507378  [150400/481450]\n",
      "loss: 0.149652  [153600/481450]\n",
      "loss: 0.264842  [156800/481450]\n",
      "loss: 0.167471  [160000/481450]\n",
      "loss: 0.165939  [163200/481450]\n",
      "loss: 0.186987  [166400/481450]\n",
      "loss: 0.231245  [169600/481450]\n",
      "loss: 0.320933  [172800/481450]\n",
      "loss: 0.324361  [176000/481450]\n",
      "loss: 0.164034  [179200/481450]\n",
      "loss: 0.264351  [182400/481450]\n",
      "loss: 0.120383  [185600/481450]\n",
      "loss: 0.202944  [188800/481450]\n",
      "loss: 0.077735  [192000/481450]\n",
      "loss: 0.323546  [195200/481450]\n",
      "loss: 0.185542  [198400/481450]\n",
      "loss: 0.286028  [201600/481450]\n",
      "loss: 0.198445  [204800/481450]\n",
      "loss: 0.211722  [208000/481450]\n",
      "loss: 0.303194  [211200/481450]\n",
      "loss: 0.147463  [214400/481450]\n",
      "loss: 0.484384  [217600/481450]\n",
      "loss: 0.293900  [220800/481450]\n",
      "loss: 0.170569  [224000/481450]\n",
      "loss: 0.212681  [227200/481450]\n",
      "loss: 0.449954  [230400/481450]\n",
      "loss: 0.241057  [233600/481450]\n",
      "loss: 0.267072  [236800/481450]\n",
      "loss: 0.176105  [240000/481450]\n",
      "loss: 0.405601  [243200/481450]\n",
      "loss: 0.148999  [246400/481450]\n",
      "loss: 0.290561  [249600/481450]\n",
      "loss: 0.179142  [252800/481450]\n",
      "loss: 0.282141  [256000/481450]\n",
      "loss: 0.165088  [259200/481450]\n",
      "loss: 0.363610  [262400/481450]\n",
      "loss: 0.384093  [265600/481450]\n",
      "loss: 0.080190  [268800/481450]\n",
      "loss: 0.091545  [272000/481450]\n",
      "loss: 0.146310  [275200/481450]\n",
      "loss: 0.391107  [278400/481450]\n",
      "loss: 0.302519  [281600/481450]\n",
      "loss: 0.250153  [284800/481450]\n",
      "loss: 0.103813  [288000/481450]\n",
      "loss: 0.102991  [291200/481450]\n",
      "loss: 0.201228  [294400/481450]\n",
      "loss: 0.377618  [297600/481450]\n",
      "loss: 0.172699  [300800/481450]\n",
      "loss: 0.050962  [304000/481450]\n",
      "loss: 0.084844  [307200/481450]\n",
      "loss: 0.304075  [310400/481450]\n",
      "loss: 0.198671  [313600/481450]\n",
      "loss: 0.264765  [316800/481450]\n",
      "loss: 0.283011  [320000/481450]\n",
      "loss: 0.303786  [323200/481450]\n",
      "loss: 0.107132  [326400/481450]\n",
      "loss: 0.324146  [329600/481450]\n",
      "loss: 0.214526  [332800/481450]\n",
      "loss: 0.489169  [336000/481450]\n",
      "loss: 0.107781  [339200/481450]\n",
      "loss: 0.216362  [342400/481450]\n",
      "loss: 0.321796  [345600/481450]\n",
      "loss: 0.375909  [348800/481450]\n",
      "loss: 0.198877  [352000/481450]\n",
      "loss: 0.102606  [355200/481450]\n",
      "loss: 0.223948  [358400/481450]\n",
      "loss: 0.383111  [361600/481450]\n",
      "loss: 0.317314  [364800/481450]\n",
      "loss: 0.288825  [368000/481450]\n",
      "loss: 0.342002  [371200/481450]\n",
      "loss: 0.150025  [374400/481450]\n",
      "loss: 0.272432  [377600/481450]\n",
      "loss: 0.579989  [380800/481450]\n",
      "loss: 0.161162  [384000/481450]\n",
      "loss: 0.268211  [387200/481450]\n",
      "loss: 0.173381  [390400/481450]\n",
      "loss: 0.492772  [393600/481450]\n",
      "loss: 0.210016  [396800/481450]\n",
      "loss: 0.218434  [400000/481450]\n",
      "loss: 0.290758  [403200/481450]\n",
      "loss: 0.448516  [406400/481450]\n",
      "loss: 0.270362  [409600/481450]\n",
      "loss: 0.300528  [412800/481450]\n",
      "loss: 0.477665  [416000/481450]\n",
      "loss: 0.378731  [419200/481450]\n",
      "loss: 0.263492  [422400/481450]\n",
      "loss: 0.226801  [425600/481450]\n",
      "loss: 0.236044  [428800/481450]\n",
      "loss: 0.405182  [432000/481450]\n",
      "loss: 0.251373  [435200/481450]\n",
      "loss: 0.186471  [438400/481450]\n",
      "loss: 0.204948  [441600/481450]\n",
      "loss: 0.174010  [444800/481450]\n",
      "loss: 0.307962  [448000/481450]\n",
      "loss: 0.350460  [451200/481450]\n",
      "loss: 0.087961  [454400/481450]\n",
      "loss: 0.295711  [457600/481450]\n",
      "loss: 0.118516  [460800/481450]\n",
      "loss: 0.223437  [464000/481450]\n",
      "loss: 0.202519  [467200/481450]\n",
      "loss: 0.260059  [470400/481450]\n",
      "loss: 0.104673  [473600/481450]\n",
      "loss: 0.170780  [476800/481450]\n",
      "loss: 0.252793  [480000/481450]\n",
      "Train Accuracy: 90.7243%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.475540, F1-score: 85.77% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.431122  [    0/481450]\n",
      "loss: 0.129951  [ 3200/481450]\n",
      "loss: 0.255083  [ 6400/481450]\n",
      "loss: 0.191011  [ 9600/481450]\n",
      "loss: 0.284978  [12800/481450]\n",
      "loss: 0.249740  [16000/481450]\n",
      "loss: 0.171454  [19200/481450]\n",
      "loss: 0.128404  [22400/481450]\n",
      "loss: 0.097558  [25600/481450]\n",
      "loss: 0.174314  [28800/481450]\n",
      "loss: 0.191561  [32000/481450]\n",
      "loss: 0.298865  [35200/481450]\n",
      "loss: 0.187166  [38400/481450]\n",
      "loss: 0.143016  [41600/481450]\n",
      "loss: 0.254260  [44800/481450]\n",
      "loss: 0.380921  [48000/481450]\n",
      "loss: 0.268325  [51200/481450]\n",
      "loss: 0.199166  [54400/481450]\n",
      "loss: 0.219308  [57600/481450]\n",
      "loss: 0.407407  [60800/481450]\n",
      "loss: 0.267970  [64000/481450]\n",
      "loss: 0.486611  [67200/481450]\n",
      "loss: 0.241564  [70400/481450]\n",
      "loss: 0.084424  [73600/481450]\n",
      "loss: 0.396795  [76800/481450]\n",
      "loss: 0.268645  [80000/481450]\n",
      "loss: 0.112060  [83200/481450]\n",
      "loss: 0.267920  [86400/481450]\n",
      "loss: 0.367685  [89600/481450]\n",
      "loss: 0.226399  [92800/481450]\n",
      "loss: 0.107798  [96000/481450]\n",
      "loss: 0.157261  [99200/481450]\n",
      "loss: 0.164141  [102400/481450]\n",
      "loss: 0.140397  [105600/481450]\n",
      "loss: 0.045199  [108800/481450]\n",
      "loss: 0.196073  [112000/481450]\n",
      "loss: 0.286209  [115200/481450]\n",
      "loss: 0.174332  [118400/481450]\n",
      "loss: 0.133773  [121600/481450]\n",
      "loss: 0.361335  [124800/481450]\n",
      "loss: 0.183270  [128000/481450]\n",
      "loss: 0.271077  [131200/481450]\n",
      "loss: 0.190498  [134400/481450]\n",
      "loss: 0.172538  [137600/481450]\n",
      "loss: 0.324689  [140800/481450]\n",
      "loss: 0.322751  [144000/481450]\n",
      "loss: 0.300288  [147200/481450]\n",
      "loss: 0.229734  [150400/481450]\n",
      "loss: 0.292481  [153600/481450]\n",
      "loss: 0.135658  [156800/481450]\n",
      "loss: 0.236614  [160000/481450]\n",
      "loss: 0.355984  [163200/481450]\n",
      "loss: 0.139114  [166400/481450]\n",
      "loss: 0.331783  [169600/481450]\n",
      "loss: 0.131311  [172800/481450]\n",
      "loss: 0.185177  [176000/481450]\n",
      "loss: 0.358406  [179200/481450]\n",
      "loss: 0.462715  [182400/481450]\n",
      "loss: 0.164910  [185600/481450]\n",
      "loss: 0.160775  [188800/481450]\n",
      "loss: 0.144379  [192000/481450]\n",
      "loss: 0.427565  [195200/481450]\n",
      "loss: 0.181540  [198400/481450]\n",
      "loss: 0.379190  [201600/481450]\n",
      "loss: 0.151644  [204800/481450]\n",
      "loss: 0.145017  [208000/481450]\n",
      "loss: 0.418698  [211200/481450]\n",
      "loss: 0.409694  [214400/481450]\n",
      "loss: 0.199797  [217600/481450]\n",
      "loss: 0.278664  [220800/481450]\n",
      "loss: 0.217253  [224000/481450]\n",
      "loss: 0.226288  [227200/481450]\n",
      "loss: 0.178956  [230400/481450]\n",
      "loss: 0.121103  [233600/481450]\n",
      "loss: 0.107959  [236800/481450]\n",
      "loss: 0.476109  [240000/481450]\n",
      "loss: 0.252613  [243200/481450]\n",
      "loss: 0.178898  [246400/481450]\n",
      "loss: 0.115793  [249600/481450]\n",
      "loss: 0.237454  [252800/481450]\n",
      "loss: 0.206884  [256000/481450]\n",
      "loss: 0.394265  [259200/481450]\n",
      "loss: 0.108546  [262400/481450]\n",
      "loss: 0.157229  [265600/481450]\n",
      "loss: 0.233951  [268800/481450]\n",
      "loss: 0.080090  [272000/481450]\n",
      "loss: 0.128598  [275200/481450]\n",
      "loss: 0.146242  [278400/481450]\n",
      "loss: 0.362224  [281600/481450]\n",
      "loss: 0.213202  [284800/481450]\n",
      "loss: 0.311191  [288000/481450]\n",
      "loss: 0.149066  [291200/481450]\n",
      "loss: 0.111656  [294400/481450]\n",
      "loss: 0.300159  [297600/481450]\n",
      "loss: 0.066866  [300800/481450]\n",
      "loss: 0.517465  [304000/481450]\n",
      "loss: 0.245268  [307200/481450]\n",
      "loss: 0.194965  [310400/481450]\n",
      "loss: 0.314921  [313600/481450]\n",
      "loss: 0.150386  [316800/481450]\n",
      "loss: 0.141547  [320000/481450]\n",
      "loss: 0.306218  [323200/481450]\n",
      "loss: 0.265780  [326400/481450]\n",
      "loss: 0.218397  [329600/481450]\n",
      "loss: 0.405047  [332800/481450]\n",
      "loss: 0.175050  [336000/481450]\n",
      "loss: 0.227941  [339200/481450]\n",
      "loss: 0.128181  [342400/481450]\n",
      "loss: 0.288664  [345600/481450]\n",
      "loss: 0.538815  [348800/481450]\n",
      "loss: 0.138801  [352000/481450]\n",
      "loss: 0.378668  [355200/481450]\n",
      "loss: 0.090549  [358400/481450]\n",
      "loss: 0.054777  [361600/481450]\n",
      "loss: 0.348987  [364800/481450]\n",
      "loss: 0.216729  [368000/481450]\n",
      "loss: 0.517554  [371200/481450]\n",
      "loss: 0.468757  [374400/481450]\n",
      "loss: 0.164988  [377600/481450]\n",
      "loss: 0.201021  [380800/481450]\n",
      "loss: 0.281258  [384000/481450]\n",
      "loss: 0.039156  [387200/481450]\n",
      "loss: 0.077834  [390400/481450]\n",
      "loss: 0.200039  [393600/481450]\n",
      "loss: 0.185101  [396800/481450]\n",
      "loss: 0.252111  [400000/481450]\n",
      "loss: 0.308766  [403200/481450]\n",
      "loss: 0.212786  [406400/481450]\n",
      "loss: 0.080296  [409600/481450]\n",
      "loss: 0.439691  [412800/481450]\n",
      "loss: 0.495081  [416000/481450]\n",
      "loss: 0.255596  [419200/481450]\n",
      "loss: 0.090755  [422400/481450]\n",
      "loss: 0.279143  [425600/481450]\n",
      "loss: 0.116168  [428800/481450]\n",
      "loss: 0.150605  [432000/481450]\n",
      "loss: 0.135489  [435200/481450]\n",
      "loss: 0.467447  [438400/481450]\n",
      "loss: 0.229150  [441600/481450]\n",
      "loss: 0.252140  [444800/481450]\n",
      "loss: 0.181049  [448000/481450]\n",
      "loss: 0.362939  [451200/481450]\n",
      "loss: 0.179619  [454400/481450]\n",
      "loss: 0.103930  [457600/481450]\n",
      "loss: 0.150794  [460800/481450]\n",
      "loss: 0.209565  [464000/481450]\n",
      "loss: 0.229122  [467200/481450]\n",
      "loss: 0.288946  [470400/481450]\n",
      "loss: 0.175322  [473600/481450]\n",
      "loss: 0.358753  [476800/481450]\n",
      "loss: 0.140723  [480000/481450]\n",
      "Train Accuracy: 90.7860%\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.509982, F1-score: 85.10% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.090455  [    0/481450]\n",
      "loss: 0.189872  [ 3200/481450]\n",
      "loss: 0.280569  [ 6400/481450]\n",
      "loss: 0.259225  [ 9600/481450]\n",
      "loss: 0.278204  [12800/481450]\n",
      "loss: 0.219545  [16000/481450]\n",
      "loss: 0.230771  [19200/481450]\n",
      "loss: 0.341031  [22400/481450]\n",
      "loss: 0.433400  [25600/481450]\n",
      "loss: 0.152032  [28800/481450]\n",
      "loss: 0.409549  [32000/481450]\n",
      "loss: 0.131366  [35200/481450]\n",
      "loss: 0.284332  [38400/481450]\n",
      "loss: 0.247491  [41600/481450]\n",
      "loss: 0.191715  [44800/481450]\n",
      "loss: 0.519078  [48000/481450]\n",
      "loss: 0.216035  [51200/481450]\n",
      "loss: 0.211679  [54400/481450]\n",
      "loss: 0.408104  [57600/481450]\n",
      "loss: 0.241328  [60800/481450]\n",
      "loss: 0.139193  [64000/481450]\n",
      "loss: 0.204046  [67200/481450]\n",
      "loss: 0.213820  [70400/481450]\n",
      "loss: 0.164178  [73600/481450]\n",
      "loss: 0.195175  [76800/481450]\n",
      "loss: 0.228620  [80000/481450]\n",
      "loss: 0.292078  [83200/481450]\n",
      "loss: 0.163012  [86400/481450]\n",
      "loss: 0.289706  [89600/481450]\n",
      "loss: 0.284199  [92800/481450]\n",
      "loss: 0.240373  [96000/481450]\n",
      "loss: 0.208954  [99200/481450]\n",
      "loss: 0.161412  [102400/481450]\n",
      "loss: 0.193257  [105600/481450]\n",
      "loss: 0.131568  [108800/481450]\n",
      "loss: 0.223349  [112000/481450]\n",
      "loss: 0.203458  [115200/481450]\n",
      "loss: 0.471500  [118400/481450]\n",
      "loss: 0.181177  [121600/481450]\n",
      "loss: 0.214315  [124800/481450]\n",
      "loss: 0.306234  [128000/481450]\n",
      "loss: 0.416924  [131200/481450]\n",
      "loss: 0.178909  [134400/481450]\n",
      "loss: 0.216537  [137600/481450]\n",
      "loss: 0.103330  [140800/481450]\n",
      "loss: 0.167543  [144000/481450]\n",
      "loss: 0.513839  [147200/481450]\n",
      "loss: 0.315792  [150400/481450]\n",
      "loss: 0.228637  [153600/481450]\n",
      "loss: 0.230161  [156800/481450]\n",
      "loss: 0.319841  [160000/481450]\n",
      "loss: 0.108580  [163200/481450]\n",
      "loss: 0.315574  [166400/481450]\n",
      "loss: 0.152007  [169600/481450]\n",
      "loss: 0.261104  [172800/481450]\n",
      "loss: 0.108067  [176000/481450]\n",
      "loss: 0.084604  [179200/481450]\n",
      "loss: 0.298805  [182400/481450]\n",
      "loss: 0.372194  [185600/481450]\n",
      "loss: 0.098363  [188800/481450]\n",
      "loss: 0.437505  [192000/481450]\n",
      "loss: 0.148038  [195200/481450]\n",
      "loss: 0.220137  [198400/481450]\n",
      "loss: 0.305130  [201600/481450]\n",
      "loss: 0.394063  [204800/481450]\n",
      "loss: 0.121976  [208000/481450]\n",
      "loss: 0.186004  [211200/481450]\n",
      "loss: 0.133316  [214400/481450]\n",
      "loss: 0.311604  [217600/481450]\n",
      "loss: 0.218383  [220800/481450]\n",
      "loss: 0.079221  [224000/481450]\n",
      "loss: 0.337694  [227200/481450]\n",
      "loss: 0.157383  [230400/481450]\n",
      "loss: 0.269778  [233600/481450]\n",
      "loss: 0.409447  [236800/481450]\n",
      "loss: 0.322961  [240000/481450]\n",
      "loss: 0.242260  [243200/481450]\n",
      "loss: 0.295681  [246400/481450]\n",
      "loss: 0.377975  [249600/481450]\n",
      "loss: 0.123753  [252800/481450]\n",
      "loss: 0.282027  [256000/481450]\n",
      "loss: 0.255899  [259200/481450]\n",
      "loss: 0.159985  [262400/481450]\n",
      "loss: 0.224752  [265600/481450]\n",
      "loss: 0.265196  [268800/481450]\n",
      "loss: 0.198652  [272000/481450]\n",
      "loss: 0.150852  [275200/481450]\n",
      "loss: 0.423639  [278400/481450]\n",
      "loss: 0.579506  [281600/481450]\n",
      "loss: 0.366401  [284800/481450]\n",
      "loss: 0.188427  [288000/481450]\n",
      "loss: 0.280347  [291200/481450]\n",
      "loss: 0.206561  [294400/481450]\n",
      "loss: 0.352908  [297600/481450]\n",
      "loss: 0.131712  [300800/481450]\n",
      "loss: 0.094793  [304000/481450]\n",
      "loss: 0.283166  [307200/481450]\n",
      "loss: 0.140710  [310400/481450]\n",
      "loss: 0.284492  [313600/481450]\n",
      "loss: 0.177739  [316800/481450]\n",
      "loss: 0.157086  [320000/481450]\n",
      "loss: 0.432134  [323200/481450]\n",
      "loss: 0.114240  [326400/481450]\n",
      "loss: 0.143954  [329600/481450]\n",
      "loss: 0.073707  [332800/481450]\n",
      "loss: 0.315792  [336000/481450]\n",
      "loss: 0.434888  [339200/481450]\n",
      "loss: 0.257721  [342400/481450]\n",
      "loss: 0.091375  [345600/481450]\n",
      "loss: 0.417923  [348800/481450]\n",
      "loss: 0.187171  [352000/481450]\n",
      "loss: 0.303521  [355200/481450]\n",
      "loss: 0.119684  [358400/481450]\n",
      "loss: 0.019011  [361600/481450]\n",
      "loss: 0.296946  [364800/481450]\n",
      "loss: 0.305923  [368000/481450]\n",
      "loss: 0.248275  [371200/481450]\n",
      "loss: 0.165365  [374400/481450]\n",
      "loss: 0.272109  [377600/481450]\n",
      "loss: 0.288329  [380800/481450]\n",
      "loss: 0.240110  [384000/481450]\n",
      "loss: 0.395842  [387200/481450]\n",
      "loss: 0.268944  [390400/481450]\n",
      "loss: 0.119800  [393600/481450]\n",
      "loss: 0.190081  [396800/481450]\n",
      "loss: 0.199943  [400000/481450]\n",
      "loss: 0.279441  [403200/481450]\n",
      "loss: 0.177339  [406400/481450]\n",
      "loss: 0.221575  [409600/481450]\n",
      "loss: 0.272662  [412800/481450]\n",
      "loss: 0.176212  [416000/481450]\n",
      "loss: 0.248841  [419200/481450]\n",
      "loss: 0.127138  [422400/481450]\n",
      "loss: 0.133511  [425600/481450]\n",
      "loss: 0.327829  [428800/481450]\n",
      "loss: 0.189357  [432000/481450]\n",
      "loss: 0.358629  [435200/481450]\n",
      "loss: 0.193759  [438400/481450]\n",
      "loss: 0.142348  [441600/481450]\n",
      "loss: 0.267217  [444800/481450]\n",
      "loss: 0.505079  [448000/481450]\n",
      "loss: 0.108657  [451200/481450]\n",
      "loss: 0.385196  [454400/481450]\n",
      "loss: 0.257460  [457600/481450]\n",
      "loss: 0.208115  [460800/481450]\n",
      "loss: 0.087497  [464000/481450]\n",
      "loss: 0.176157  [467200/481450]\n",
      "loss: 0.176609  [470400/481450]\n",
      "loss: 0.099670  [473600/481450]\n",
      "loss: 0.053146  [476800/481450]\n",
      "loss: 0.147974  [480000/481450]\n",
      "Train Accuracy: 90.8464%\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.498507, F1-score: 85.40% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.127808  [    0/481450]\n",
      "loss: 0.180158  [ 3200/481450]\n",
      "loss: 0.299631  [ 6400/481450]\n",
      "loss: 0.246491  [ 9600/481450]\n",
      "loss: 0.070153  [12800/481450]\n",
      "loss: 0.105169  [16000/481450]\n",
      "loss: 0.184365  [19200/481450]\n",
      "loss: 0.196132  [22400/481450]\n",
      "loss: 0.285872  [25600/481450]\n",
      "loss: 0.276514  [28800/481450]\n",
      "loss: 0.291608  [32000/481450]\n",
      "loss: 0.161472  [35200/481450]\n",
      "loss: 0.147979  [38400/481450]\n",
      "loss: 0.474630  [41600/481450]\n",
      "loss: 0.360272  [44800/481450]\n",
      "loss: 0.304911  [48000/481450]\n",
      "loss: 0.165114  [51200/481450]\n",
      "loss: 0.250044  [54400/481450]\n",
      "loss: 0.306664  [57600/481450]\n",
      "loss: 0.136765  [60800/481450]\n",
      "loss: 0.414514  [64000/481450]\n",
      "loss: 0.226783  [67200/481450]\n",
      "loss: 0.042474  [70400/481450]\n",
      "loss: 0.160643  [73600/481450]\n",
      "loss: 0.178359  [76800/481450]\n",
      "loss: 0.352608  [80000/481450]\n",
      "loss: 0.146803  [83200/481450]\n",
      "loss: 0.327190  [86400/481450]\n",
      "loss: 0.270533  [89600/481450]\n",
      "loss: 0.363183  [92800/481450]\n",
      "loss: 0.386853  [96000/481450]\n",
      "loss: 0.330039  [99200/481450]\n",
      "loss: 0.094956  [102400/481450]\n",
      "loss: 0.304546  [105600/481450]\n",
      "loss: 0.335178  [108800/481450]\n",
      "loss: 0.337608  [112000/481450]\n",
      "loss: 0.273806  [115200/481450]\n",
      "loss: 0.286675  [118400/481450]\n",
      "loss: 0.336824  [121600/481450]\n",
      "loss: 0.093566  [124800/481450]\n",
      "loss: 0.321773  [128000/481450]\n",
      "loss: 0.254424  [131200/481450]\n",
      "loss: 0.208459  [134400/481450]\n",
      "loss: 0.145831  [137600/481450]\n",
      "loss: 0.157669  [140800/481450]\n",
      "loss: 0.248647  [144000/481450]\n",
      "loss: 0.256891  [147200/481450]\n",
      "loss: 0.327636  [150400/481450]\n",
      "loss: 0.378715  [153600/481450]\n",
      "loss: 0.192002  [156800/481450]\n",
      "loss: 0.167521  [160000/481450]\n",
      "loss: 0.168777  [163200/481450]\n",
      "loss: 0.196944  [166400/481450]\n",
      "loss: 0.233717  [169600/481450]\n",
      "loss: 0.263051  [172800/481450]\n",
      "loss: 0.141069  [176000/481450]\n",
      "loss: 0.133164  [179200/481450]\n",
      "loss: 0.301588  [182400/481450]\n",
      "loss: 0.228249  [185600/481450]\n",
      "loss: 0.184588  [188800/481450]\n",
      "loss: 0.289042  [192000/481450]\n",
      "loss: 0.137101  [195200/481450]\n",
      "loss: 0.163703  [198400/481450]\n",
      "loss: 0.223980  [201600/481450]\n",
      "loss: 0.090162  [204800/481450]\n",
      "loss: 0.164130  [208000/481450]\n",
      "loss: 0.301001  [211200/481450]\n",
      "loss: 0.421455  [214400/481450]\n",
      "loss: 0.124356  [217600/481450]\n",
      "loss: 0.082510  [220800/481450]\n",
      "loss: 0.417139  [224000/481450]\n",
      "loss: 0.261677  [227200/481450]\n",
      "loss: 0.393243  [230400/481450]\n",
      "loss: 0.283016  [233600/481450]\n",
      "loss: 0.122478  [236800/481450]\n",
      "loss: 0.126396  [240000/481450]\n",
      "loss: 0.132465  [243200/481450]\n",
      "loss: 0.230460  [246400/481450]\n",
      "loss: 0.171059  [249600/481450]\n",
      "loss: 0.342450  [252800/481450]\n",
      "loss: 0.497418  [256000/481450]\n",
      "loss: 0.162979  [259200/481450]\n",
      "loss: 0.224851  [262400/481450]\n",
      "loss: 0.098376  [265600/481450]\n",
      "loss: 0.099526  [268800/481450]\n",
      "loss: 0.260637  [272000/481450]\n",
      "loss: 0.096840  [275200/481450]\n",
      "loss: 0.198877  [278400/481450]\n",
      "loss: 0.070164  [281600/481450]\n",
      "loss: 0.121037  [284800/481450]\n",
      "loss: 0.320141  [288000/481450]\n",
      "loss: 0.213956  [291200/481450]\n",
      "loss: 0.088274  [294400/481450]\n",
      "loss: 0.270659  [297600/481450]\n",
      "loss: 0.236245  [300800/481450]\n",
      "loss: 0.111986  [304000/481450]\n",
      "loss: 0.177546  [307200/481450]\n",
      "loss: 0.234642  [310400/481450]\n",
      "loss: 0.302818  [313600/481450]\n",
      "loss: 0.171501  [316800/481450]\n",
      "loss: 0.178160  [320000/481450]\n",
      "loss: 0.300162  [323200/481450]\n",
      "loss: 0.168289  [326400/481450]\n",
      "loss: 0.147930  [329600/481450]\n",
      "loss: 0.107491  [332800/481450]\n",
      "loss: 0.261625  [336000/481450]\n",
      "loss: 0.177677  [339200/481450]\n",
      "loss: 0.193744  [342400/481450]\n",
      "loss: 0.180326  [345600/481450]\n",
      "loss: 0.158779  [348800/481450]\n",
      "loss: 0.108882  [352000/481450]\n",
      "loss: 0.384153  [355200/481450]\n",
      "loss: 0.396519  [358400/481450]\n",
      "loss: 0.232138  [361600/481450]\n",
      "loss: 0.184213  [364800/481450]\n",
      "loss: 0.166142  [368000/481450]\n",
      "loss: 0.243352  [371200/481450]\n",
      "loss: 0.194640  [374400/481450]\n",
      "loss: 0.125742  [377600/481450]\n",
      "loss: 0.163508  [380800/481450]\n",
      "loss: 0.110214  [384000/481450]\n",
      "loss: 0.054866  [387200/481450]\n",
      "loss: 0.239939  [390400/481450]\n",
      "loss: 0.226572  [393600/481450]\n",
      "loss: 0.096944  [396800/481450]\n",
      "loss: 0.261898  [400000/481450]\n",
      "loss: 0.150720  [403200/481450]\n",
      "loss: 0.213161  [406400/481450]\n",
      "loss: 0.109518  [409600/481450]\n",
      "loss: 0.224653  [412800/481450]\n",
      "loss: 0.231021  [416000/481450]\n",
      "loss: 0.168599  [419200/481450]\n",
      "loss: 0.037748  [422400/481450]\n",
      "loss: 0.213019  [425600/481450]\n",
      "loss: 0.359669  [428800/481450]\n",
      "loss: 0.173099  [432000/481450]\n",
      "loss: 0.433764  [435200/481450]\n",
      "loss: 0.093395  [438400/481450]\n",
      "loss: 0.174430  [441600/481450]\n",
      "loss: 0.204935  [444800/481450]\n",
      "loss: 0.358645  [448000/481450]\n",
      "loss: 0.404160  [451200/481450]\n",
      "loss: 0.117284  [454400/481450]\n",
      "loss: 0.223689  [457600/481450]\n",
      "loss: 0.338955  [460800/481450]\n",
      "loss: 0.150577  [464000/481450]\n",
      "loss: 0.234516  [467200/481450]\n",
      "loss: 0.359268  [470400/481450]\n",
      "loss: 0.112493  [473600/481450]\n",
      "loss: 0.151261  [476800/481450]\n",
      "loss: 0.049158  [480000/481450]\n",
      "Train Accuracy: 90.9401%\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.497553, F1-score: 85.50% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.191421  [    0/481450]\n",
      "loss: 0.087901  [ 3200/481450]\n",
      "loss: 0.112699  [ 6400/481450]\n",
      "loss: 0.465857  [ 9600/481450]\n",
      "loss: 0.170562  [12800/481450]\n",
      "loss: 0.190875  [16000/481450]\n",
      "loss: 0.080613  [19200/481450]\n",
      "loss: 0.259269  [22400/481450]\n",
      "loss: 0.254862  [25600/481450]\n",
      "loss: 0.077621  [28800/481450]\n",
      "loss: 0.083015  [32000/481450]\n",
      "loss: 0.162521  [35200/481450]\n",
      "loss: 0.172388  [38400/481450]\n",
      "loss: 0.281690  [41600/481450]\n",
      "loss: 0.207768  [44800/481450]\n",
      "loss: 0.477435  [48000/481450]\n",
      "loss: 0.174560  [51200/481450]\n",
      "loss: 0.425006  [54400/481450]\n",
      "loss: 0.188653  [57600/481450]\n",
      "loss: 0.200188  [60800/481450]\n",
      "loss: 0.288538  [64000/481450]\n",
      "loss: 0.415472  [67200/481450]\n",
      "loss: 0.129460  [70400/481450]\n",
      "loss: 0.264744  [73600/481450]\n",
      "loss: 0.301231  [76800/481450]\n",
      "loss: 0.254437  [80000/481450]\n",
      "loss: 0.300800  [83200/481450]\n",
      "loss: 0.638075  [86400/481450]\n",
      "loss: 0.270483  [89600/481450]\n",
      "loss: 0.092768  [92800/481450]\n",
      "loss: 0.441047  [96000/481450]\n",
      "loss: 0.258075  [99200/481450]\n",
      "loss: 0.300899  [102400/481450]\n",
      "loss: 0.437134  [105600/481450]\n",
      "loss: 0.200702  [108800/481450]\n",
      "loss: 0.051837  [112000/481450]\n",
      "loss: 0.258496  [115200/481450]\n",
      "loss: 0.292255  [118400/481450]\n",
      "loss: 0.124476  [121600/481450]\n",
      "loss: 0.283486  [124800/481450]\n",
      "loss: 0.254036  [128000/481450]\n",
      "loss: 0.103376  [131200/481450]\n",
      "loss: 0.276122  [134400/481450]\n",
      "loss: 0.265232  [137600/481450]\n",
      "loss: 0.078546  [140800/481450]\n",
      "loss: 0.146709  [144000/481450]\n",
      "loss: 0.093207  [147200/481450]\n",
      "loss: 0.238412  [150400/481450]\n",
      "loss: 0.231123  [153600/481450]\n",
      "loss: 0.086679  [156800/481450]\n",
      "loss: 0.159541  [160000/481450]\n",
      "loss: 0.111781  [163200/481450]\n",
      "loss: 0.253186  [166400/481450]\n",
      "loss: 0.375893  [169600/481450]\n",
      "loss: 0.313777  [172800/481450]\n",
      "loss: 0.336130  [176000/481450]\n",
      "loss: 0.147925  [179200/481450]\n",
      "loss: 0.125083  [182400/481450]\n",
      "loss: 0.201098  [185600/481450]\n",
      "loss: 0.207958  [188800/481450]\n",
      "loss: 0.292738  [192000/481450]\n",
      "loss: 0.327012  [195200/481450]\n",
      "loss: 0.155477  [198400/481450]\n",
      "loss: 0.181606  [201600/481450]\n",
      "loss: 0.161492  [204800/481450]\n",
      "loss: 0.224425  [208000/481450]\n",
      "loss: 0.096211  [211200/481450]\n",
      "loss: 0.140384  [214400/481450]\n",
      "loss: 0.211370  [217600/481450]\n",
      "loss: 0.209141  [220800/481450]\n",
      "loss: 0.072304  [224000/481450]\n",
      "loss: 0.441165  [227200/481450]\n",
      "loss: 0.180432  [230400/481450]\n",
      "loss: 0.245109  [233600/481450]\n",
      "loss: 0.155150  [236800/481450]\n",
      "loss: 0.222019  [240000/481450]\n",
      "loss: 0.159114  [243200/481450]\n",
      "loss: 0.130481  [246400/481450]\n",
      "loss: 0.265720  [249600/481450]\n",
      "loss: 0.156307  [252800/481450]\n",
      "loss: 0.316740  [256000/481450]\n",
      "loss: 0.497509  [259200/481450]\n",
      "loss: 0.159531  [262400/481450]\n",
      "loss: 0.258895  [265600/481450]\n",
      "loss: 0.267755  [268800/481450]\n",
      "loss: 0.211421  [272000/481450]\n",
      "loss: 0.256432  [275200/481450]\n",
      "loss: 0.216529  [278400/481450]\n",
      "loss: 0.085305  [281600/481450]\n",
      "loss: 0.098755  [284800/481450]\n",
      "loss: 0.122826  [288000/481450]\n",
      "loss: 0.229688  [291200/481450]\n",
      "loss: 0.123662  [294400/481450]\n",
      "loss: 0.291302  [297600/481450]\n",
      "loss: 0.214514  [300800/481450]\n",
      "loss: 0.159912  [304000/481450]\n",
      "loss: 0.102835  [307200/481450]\n",
      "loss: 0.153063  [310400/481450]\n",
      "loss: 0.220158  [313600/481450]\n",
      "loss: 0.163047  [316800/481450]\n",
      "loss: 0.270473  [320000/481450]\n",
      "loss: 0.365227  [323200/481450]\n",
      "loss: 0.080639  [326400/481450]\n",
      "loss: 0.337912  [329600/481450]\n",
      "loss: 0.089348  [332800/481450]\n",
      "loss: 0.182964  [336000/481450]\n",
      "loss: 0.156059  [339200/481450]\n",
      "loss: 0.470723  [342400/481450]\n",
      "loss: 0.178431  [345600/481450]\n",
      "loss: 0.106688  [348800/481450]\n",
      "loss: 0.116877  [352000/481450]\n",
      "loss: 0.259404  [355200/481450]\n",
      "loss: 0.112889  [358400/481450]\n",
      "loss: 0.149385  [361600/481450]\n",
      "loss: 0.231161  [364800/481450]\n",
      "loss: 0.264832  [368000/481450]\n",
      "loss: 0.356353  [371200/481450]\n",
      "loss: 0.110579  [374400/481450]\n",
      "loss: 0.288545  [377600/481450]\n",
      "loss: 0.171275  [380800/481450]\n",
      "loss: 0.074181  [384000/481450]\n",
      "loss: 0.282789  [387200/481450]\n",
      "loss: 0.069789  [390400/481450]\n",
      "loss: 0.094188  [393600/481450]\n",
      "loss: 0.278761  [396800/481450]\n",
      "loss: 0.254231  [400000/481450]\n",
      "loss: 0.191345  [403200/481450]\n",
      "loss: 0.120957  [406400/481450]\n",
      "loss: 0.285036  [409600/481450]\n",
      "loss: 0.139871  [412800/481450]\n",
      "loss: 0.088092  [416000/481450]\n",
      "loss: 0.222357  [419200/481450]\n",
      "loss: 0.291855  [422400/481450]\n",
      "loss: 0.295615  [425600/481450]\n",
      "loss: 0.271876  [428800/481450]\n",
      "loss: 0.399356  [432000/481450]\n",
      "loss: 0.024845  [435200/481450]\n",
      "loss: 0.286641  [438400/481450]\n",
      "loss: 0.221110  [441600/481450]\n",
      "loss: 0.297817  [444800/481450]\n",
      "loss: 0.114757  [448000/481450]\n",
      "loss: 0.230993  [451200/481450]\n",
      "loss: 0.135712  [454400/481450]\n",
      "loss: 0.478003  [457600/481450]\n",
      "loss: 0.408470  [460800/481450]\n",
      "loss: 0.197358  [464000/481450]\n",
      "loss: 0.281412  [467200/481450]\n",
      "loss: 0.373143  [470400/481450]\n",
      "loss: 0.247008  [473600/481450]\n",
      "loss: 0.147054  [476800/481450]\n",
      "loss: 0.208755  [480000/481450]\n",
      "Train Accuracy: 90.9559%\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.550858, F1-score: 85.56% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.118758  [    0/481450]\n",
      "loss: 0.145953  [ 3200/481450]\n",
      "loss: 0.422773  [ 6400/481450]\n",
      "loss: 0.369682  [ 9600/481450]\n",
      "loss: 0.187395  [12800/481450]\n",
      "loss: 0.223566  [16000/481450]\n",
      "loss: 0.173948  [19200/481450]\n",
      "loss: 0.090307  [22400/481450]\n",
      "loss: 0.173116  [25600/481450]\n",
      "loss: 0.356773  [28800/481450]\n",
      "loss: 0.150396  [32000/481450]\n",
      "loss: 0.253677  [35200/481450]\n",
      "loss: 0.163523  [38400/481450]\n",
      "loss: 0.123907  [41600/481450]\n",
      "loss: 0.245102  [44800/481450]\n",
      "loss: 0.133768  [48000/481450]\n",
      "loss: 0.318494  [51200/481450]\n",
      "loss: 0.048253  [54400/481450]\n",
      "loss: 0.435742  [57600/481450]\n",
      "loss: 0.146609  [60800/481450]\n",
      "loss: 0.097354  [64000/481450]\n",
      "loss: 0.208227  [67200/481450]\n",
      "loss: 0.154489  [70400/481450]\n",
      "loss: 0.143555  [73600/481450]\n",
      "loss: 0.417838  [76800/481450]\n",
      "loss: 0.234780  [80000/481450]\n",
      "loss: 0.223175  [83200/481450]\n",
      "loss: 0.287431  [86400/481450]\n",
      "loss: 0.282880  [89600/481450]\n",
      "loss: 0.256049  [92800/481450]\n",
      "loss: 0.174271  [96000/481450]\n",
      "loss: 0.206995  [99200/481450]\n",
      "loss: 0.309260  [102400/481450]\n",
      "loss: 0.039647  [105600/481450]\n",
      "loss: 0.091894  [108800/481450]\n",
      "loss: 0.154407  [112000/481450]\n",
      "loss: 0.239734  [115200/481450]\n",
      "loss: 0.229601  [118400/481450]\n",
      "loss: 0.284213  [121600/481450]\n",
      "loss: 0.122983  [124800/481450]\n",
      "loss: 0.350216  [128000/481450]\n",
      "loss: 0.351942  [131200/481450]\n",
      "loss: 0.385536  [134400/481450]\n",
      "loss: 0.550936  [137600/481450]\n",
      "loss: 0.303495  [140800/481450]\n",
      "loss: 0.327728  [144000/481450]\n",
      "loss: 0.315704  [147200/481450]\n",
      "loss: 0.322417  [150400/481450]\n",
      "loss: 0.015520  [153600/481450]\n",
      "loss: 0.271302  [156800/481450]\n",
      "loss: 0.316007  [160000/481450]\n",
      "loss: 0.333869  [163200/481450]\n",
      "loss: 0.290349  [166400/481450]\n",
      "loss: 0.113618  [169600/481450]\n",
      "loss: 0.418879  [172800/481450]\n",
      "loss: 0.185002  [176000/481450]\n",
      "loss: 0.283924  [179200/481450]\n",
      "loss: 0.337939  [182400/481450]\n",
      "loss: 0.369213  [185600/481450]\n",
      "loss: 0.149790  [188800/481450]\n",
      "loss: 0.133770  [192000/481450]\n",
      "loss: 0.366970  [195200/481450]\n",
      "loss: 0.191359  [198400/481450]\n",
      "loss: 0.162306  [201600/481450]\n",
      "loss: 0.123088  [204800/481450]\n",
      "loss: 0.169131  [208000/481450]\n",
      "loss: 0.084495  [211200/481450]\n",
      "loss: 0.138538  [214400/481450]\n",
      "loss: 0.264092  [217600/481450]\n",
      "loss: 0.316690  [220800/481450]\n",
      "loss: 0.200757  [224000/481450]\n",
      "loss: 0.253265  [227200/481450]\n",
      "loss: 0.141366  [230400/481450]\n",
      "loss: 0.434029  [233600/481450]\n",
      "loss: 0.155388  [236800/481450]\n",
      "loss: 0.304276  [240000/481450]\n",
      "loss: 0.324115  [243200/481450]\n",
      "loss: 0.347838  [246400/481450]\n",
      "loss: 0.063269  [249600/481450]\n",
      "loss: 0.333715  [252800/481450]\n",
      "loss: 0.112526  [256000/481450]\n",
      "loss: 0.265306  [259200/481450]\n",
      "loss: 0.349189  [262400/481450]\n",
      "loss: 0.173545  [265600/481450]\n",
      "loss: 0.292773  [268800/481450]\n",
      "loss: 0.470410  [272000/481450]\n",
      "loss: 0.275280  [275200/481450]\n",
      "loss: 0.200195  [278400/481450]\n",
      "loss: 0.099947  [281600/481450]\n",
      "loss: 0.112956  [284800/481450]\n",
      "loss: 0.244200  [288000/481450]\n",
      "loss: 0.314553  [291200/481450]\n",
      "loss: 0.177524  [294400/481450]\n",
      "loss: 0.201515  [297600/481450]\n",
      "loss: 0.273944  [300800/481450]\n",
      "loss: 0.299381  [304000/481450]\n",
      "loss: 0.049647  [307200/481450]\n",
      "loss: 0.199435  [310400/481450]\n",
      "loss: 0.285068  [313600/481450]\n",
      "loss: 0.537255  [316800/481450]\n",
      "loss: 0.359786  [320000/481450]\n",
      "loss: 0.112363  [323200/481450]\n",
      "loss: 0.181903  [326400/481450]\n",
      "loss: 0.105772  [329600/481450]\n",
      "loss: 0.138824  [332800/481450]\n",
      "loss: 0.133532  [336000/481450]\n",
      "loss: 0.248552  [339200/481450]\n",
      "loss: 0.065998  [342400/481450]\n",
      "loss: 0.262351  [345600/481450]\n",
      "loss: 0.295228  [348800/481450]\n",
      "loss: 0.147966  [352000/481450]\n",
      "loss: 0.305508  [355200/481450]\n",
      "loss: 0.238227  [358400/481450]\n",
      "loss: 0.157147  [361600/481450]\n",
      "loss: 0.246621  [364800/481450]\n",
      "loss: 0.081387  [368000/481450]\n",
      "loss: 0.069703  [371200/481450]\n",
      "loss: 0.203423  [374400/481450]\n",
      "loss: 0.204097  [377600/481450]\n",
      "loss: 0.319538  [380800/481450]\n",
      "loss: 0.223468  [384000/481450]\n",
      "loss: 0.150784  [387200/481450]\n",
      "loss: 0.456918  [390400/481450]\n",
      "loss: 0.523254  [393600/481450]\n",
      "loss: 0.049155  [396800/481450]\n",
      "loss: 0.315689  [400000/481450]\n",
      "loss: 0.177485  [403200/481450]\n",
      "loss: 0.200514  [406400/481450]\n",
      "loss: 0.025003  [409600/481450]\n",
      "loss: 0.164655  [412800/481450]\n",
      "loss: 0.479362  [416000/481450]\n",
      "loss: 0.128235  [419200/481450]\n",
      "loss: 0.171345  [422400/481450]\n",
      "loss: 0.279253  [425600/481450]\n",
      "loss: 0.148646  [428800/481450]\n",
      "loss: 0.223750  [432000/481450]\n",
      "loss: 0.115274  [435200/481450]\n",
      "loss: 0.126110  [438400/481450]\n",
      "loss: 0.151927  [441600/481450]\n",
      "loss: 0.209824  [444800/481450]\n",
      "loss: 0.304484  [448000/481450]\n",
      "loss: 0.191218  [451200/481450]\n",
      "loss: 0.181746  [454400/481450]\n",
      "loss: 0.114464  [457600/481450]\n",
      "loss: 0.242944  [460800/481450]\n",
      "loss: 0.345402  [464000/481450]\n",
      "loss: 0.190297  [467200/481450]\n",
      "loss: 0.252577  [470400/481450]\n",
      "loss: 0.238951  [473600/481450]\n",
      "loss: 0.248713  [476800/481450]\n",
      "loss: 0.120813  [480000/481450]\n",
      "Train Accuracy: 91.0259%\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.592966, F1-score: 85.32% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.168084  [    0/481450]\n",
      "loss: 0.181060  [ 3200/481450]\n",
      "loss: 0.233560  [ 6400/481450]\n",
      "loss: 0.236654  [ 9600/481450]\n",
      "loss: 0.307989  [12800/481450]\n",
      "loss: 0.132279  [16000/481450]\n",
      "loss: 0.092332  [19200/481450]\n",
      "loss: 0.364082  [22400/481450]\n",
      "loss: 0.241129  [25600/481450]\n",
      "loss: 0.278858  [28800/481450]\n",
      "loss: 0.116607  [32000/481450]\n",
      "loss: 0.314535  [35200/481450]\n",
      "loss: 0.211626  [38400/481450]\n",
      "loss: 0.232022  [41600/481450]\n",
      "loss: 0.102616  [44800/481450]\n",
      "loss: 0.201091  [48000/481450]\n",
      "loss: 0.246883  [51200/481450]\n",
      "loss: 0.098735  [54400/481450]\n",
      "loss: 0.188519  [57600/481450]\n",
      "loss: 0.118360  [60800/481450]\n",
      "loss: 0.173522  [64000/481450]\n",
      "loss: 0.176280  [67200/481450]\n",
      "loss: 0.183430  [70400/481450]\n",
      "loss: 0.112972  [73600/481450]\n",
      "loss: 0.368135  [76800/481450]\n",
      "loss: 0.037277  [80000/481450]\n",
      "loss: 0.077695  [83200/481450]\n",
      "loss: 0.076133  [86400/481450]\n",
      "loss: 0.167040  [89600/481450]\n",
      "loss: 0.176870  [92800/481450]\n",
      "loss: 0.188706  [96000/481450]\n",
      "loss: 0.150368  [99200/481450]\n",
      "loss: 0.145898  [102400/481450]\n",
      "loss: 0.329352  [105600/481450]\n",
      "loss: 0.090334  [108800/481450]\n",
      "loss: 0.192741  [112000/481450]\n",
      "loss: 0.213699  [115200/481450]\n",
      "loss: 0.206084  [118400/481450]\n",
      "loss: 0.263792  [121600/481450]\n",
      "loss: 0.085383  [124800/481450]\n",
      "loss: 0.176540  [128000/481450]\n",
      "loss: 0.160440  [131200/481450]\n",
      "loss: 0.368703  [134400/481450]\n",
      "loss: 0.099433  [137600/481450]\n",
      "loss: 0.204781  [140800/481450]\n",
      "loss: 0.180000  [144000/481450]\n",
      "loss: 0.268639  [147200/481450]\n",
      "loss: 0.147949  [150400/481450]\n",
      "loss: 0.432653  [153600/481450]\n",
      "loss: 0.203065  [156800/481450]\n",
      "loss: 0.172473  [160000/481450]\n",
      "loss: 0.158540  [163200/481450]\n",
      "loss: 0.407232  [166400/481450]\n",
      "loss: 0.129399  [169600/481450]\n",
      "loss: 0.161049  [172800/481450]\n",
      "loss: 0.110244  [176000/481450]\n",
      "loss: 0.264292  [179200/481450]\n",
      "loss: 0.161768  [182400/481450]\n",
      "loss: 0.346112  [185600/481450]\n",
      "loss: 0.264690  [188800/481450]\n",
      "loss: 0.172806  [192000/481450]\n",
      "loss: 0.176368  [195200/481450]\n",
      "loss: 0.197402  [198400/481450]\n",
      "loss: 0.331500  [201600/481450]\n",
      "loss: 0.093014  [204800/481450]\n",
      "loss: 0.240001  [208000/481450]\n",
      "loss: 0.245702  [211200/481450]\n",
      "loss: 0.136569  [214400/481450]\n",
      "loss: 0.322257  [217600/481450]\n",
      "loss: 0.233818  [220800/481450]\n",
      "loss: 0.427379  [224000/481450]\n",
      "loss: 0.143052  [227200/481450]\n",
      "loss: 0.109527  [230400/481450]\n",
      "loss: 0.141372  [233600/481450]\n",
      "loss: 0.126813  [236800/481450]\n",
      "loss: 0.283675  [240000/481450]\n",
      "loss: 0.069476  [243200/481450]\n",
      "loss: 0.324435  [246400/481450]\n",
      "loss: 0.231125  [249600/481450]\n",
      "loss: 0.218495  [252800/481450]\n",
      "loss: 0.195834  [256000/481450]\n",
      "loss: 0.501428  [259200/481450]\n",
      "loss: 0.175097  [262400/481450]\n",
      "loss: 0.252906  [265600/481450]\n",
      "loss: 0.047518  [268800/481450]\n",
      "loss: 0.043166  [272000/481450]\n",
      "loss: 0.319703  [275200/481450]\n",
      "loss: 0.288205  [278400/481450]\n",
      "loss: 0.123668  [281600/481450]\n",
      "loss: 0.065829  [284800/481450]\n",
      "loss: 0.281941  [288000/481450]\n",
      "loss: 0.141660  [291200/481450]\n",
      "loss: 0.111120  [294400/481450]\n",
      "loss: 0.184042  [297600/481450]\n",
      "loss: 0.346402  [300800/481450]\n",
      "loss: 0.237786  [304000/481450]\n",
      "loss: 0.231808  [307200/481450]\n",
      "loss: 0.053335  [310400/481450]\n",
      "loss: 0.273318  [313600/481450]\n",
      "loss: 0.207693  [316800/481450]\n",
      "loss: 0.217237  [320000/481450]\n",
      "loss: 0.465887  [323200/481450]\n",
      "loss: 0.136745  [326400/481450]\n",
      "loss: 0.206573  [329600/481450]\n",
      "loss: 0.137225  [332800/481450]\n",
      "loss: 0.251571  [336000/481450]\n",
      "loss: 0.103014  [339200/481450]\n",
      "loss: 0.064637  [342400/481450]\n",
      "loss: 0.075239  [345600/481450]\n",
      "loss: 0.203278  [348800/481450]\n",
      "loss: 0.115871  [352000/481450]\n",
      "loss: 0.201603  [355200/481450]\n",
      "loss: 0.361304  [358400/481450]\n",
      "loss: 0.204613  [361600/481450]\n",
      "loss: 0.175656  [364800/481450]\n",
      "loss: 0.121278  [368000/481450]\n",
      "loss: 0.141404  [371200/481450]\n",
      "loss: 0.205463  [374400/481450]\n",
      "loss: 0.235482  [377600/481450]\n",
      "loss: 0.224550  [380800/481450]\n",
      "loss: 0.329445  [384000/481450]\n",
      "loss: 0.228965  [387200/481450]\n",
      "loss: 0.318196  [390400/481450]\n",
      "loss: 0.616948  [393600/481450]\n",
      "loss: 0.198114  [396800/481450]\n",
      "loss: 0.223589  [400000/481450]\n",
      "loss: 0.126992  [403200/481450]\n",
      "loss: 0.174135  [406400/481450]\n",
      "loss: 0.229918  [409600/481450]\n",
      "loss: 0.287918  [412800/481450]\n",
      "loss: 0.377142  [416000/481450]\n",
      "loss: 0.108063  [419200/481450]\n",
      "loss: 0.149148  [422400/481450]\n",
      "loss: 0.206158  [425600/481450]\n",
      "loss: 0.341612  [428800/481450]\n",
      "loss: 0.245166  [432000/481450]\n",
      "loss: 0.395434  [435200/481450]\n",
      "loss: 0.306108  [438400/481450]\n",
      "loss: 0.209658  [441600/481450]\n",
      "loss: 0.117018  [444800/481450]\n",
      "loss: 0.239951  [448000/481450]\n",
      "loss: 0.253344  [451200/481450]\n",
      "loss: 0.146608  [454400/481450]\n",
      "loss: 0.063959  [457600/481450]\n",
      "loss: 0.269506  [460800/481450]\n",
      "loss: 0.169880  [464000/481450]\n",
      "loss: 0.127906  [467200/481450]\n",
      "loss: 0.227124  [470400/481450]\n",
      "loss: 0.199560  [473600/481450]\n",
      "loss: 0.239785  [476800/481450]\n",
      "loss: 0.175828  [480000/481450]\n",
      "Train Accuracy: 91.0763%\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.590213, F1-score: 85.20% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431a6d93-0220-4085-bb3f-8965b97240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e34344-c96e-42c9-b318-96c5e291cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299554  [    0/481450]\n",
      "loss: 2.301553  [ 3200/481450]\n",
      "loss: 2.295126  [ 6400/481450]\n",
      "loss: 2.291452  [ 9600/481450]\n",
      "loss: 2.284972  [12800/481450]\n",
      "loss: 2.269739  [16000/481450]\n",
      "loss: 2.189251  [19200/481450]\n",
      "loss: 2.225468  [22400/481450]\n",
      "loss: 2.036146  [25600/481450]\n",
      "loss: 2.160871  [28800/481450]\n",
      "loss: 1.978571  [32000/481450]\n",
      "loss: 1.906189  [35200/481450]\n",
      "loss: 1.914405  [38400/481450]\n",
      "loss: 1.971960  [41600/481450]\n",
      "loss: 1.897991  [44800/481450]\n",
      "loss: 1.906381  [48000/481450]\n",
      "loss: 1.966027  [51200/481450]\n",
      "loss: 1.875139  [54400/481450]\n",
      "loss: 1.954501  [57600/481450]\n",
      "loss: 1.790950  [60800/481450]\n",
      "loss: 1.728896  [64000/481450]\n",
      "loss: 1.609065  [67200/481450]\n",
      "loss: 1.730175  [70400/481450]\n",
      "loss: 1.890268  [73600/481450]\n",
      "loss: 1.607302  [76800/481450]\n",
      "loss: 1.749880  [80000/481450]\n",
      "loss: 1.549158  [83200/481450]\n",
      "loss: 1.623417  [86400/481450]\n",
      "loss: 1.741142  [89600/481450]\n",
      "loss: 1.583601  [92800/481450]\n",
      "loss: 1.636041  [96000/481450]\n",
      "loss: 1.515680  [99200/481450]\n",
      "loss: 1.551420  [102400/481450]\n",
      "loss: 1.397777  [105600/481450]\n",
      "loss: 1.732817  [108800/481450]\n",
      "loss: 1.500405  [112000/481450]\n",
      "loss: 1.610054  [115200/481450]\n",
      "loss: 1.385825  [118400/481450]\n",
      "loss: 1.503126  [121600/481450]\n",
      "loss: 1.468029  [124800/481450]\n",
      "loss: 1.701162  [128000/481450]\n",
      "loss: 1.466733  [131200/481450]\n",
      "loss: 1.447070  [134400/481450]\n",
      "loss: 1.540469  [137600/481450]\n",
      "loss: 1.366319  [140800/481450]\n",
      "loss: 1.447986  [144000/481450]\n",
      "loss: 1.383605  [147200/481450]\n",
      "loss: 1.522320  [150400/481450]\n",
      "loss: 1.281763  [153600/481450]\n",
      "loss: 1.624419  [156800/481450]\n",
      "loss: 1.485643  [160000/481450]\n",
      "loss: 1.399402  [163200/481450]\n",
      "loss: 1.382413  [166400/481450]\n",
      "loss: 1.142830  [169600/481450]\n",
      "loss: 1.302864  [172800/481450]\n",
      "loss: 1.136420  [176000/481450]\n",
      "loss: 1.144632  [179200/481450]\n",
      "loss: 1.411052  [182400/481450]\n",
      "loss: 1.150003  [185600/481450]\n",
      "loss: 1.210703  [188800/481450]\n",
      "loss: 1.209647  [192000/481450]\n",
      "loss: 1.184294  [195200/481450]\n",
      "loss: 1.256051  [198400/481450]\n",
      "loss: 1.118176  [201600/481450]\n",
      "loss: 0.835797  [204800/481450]\n",
      "loss: 0.945242  [208000/481450]\n",
      "loss: 1.040646  [211200/481450]\n",
      "loss: 0.854339  [214400/481450]\n",
      "loss: 1.246210  [217600/481450]\n",
      "loss: 1.007830  [220800/481450]\n",
      "loss: 0.850981  [224000/481450]\n",
      "loss: 0.853289  [227200/481450]\n",
      "loss: 0.866751  [230400/481450]\n",
      "loss: 1.062877  [233600/481450]\n",
      "loss: 1.012681  [236800/481450]\n",
      "loss: 0.911072  [240000/481450]\n",
      "loss: 1.202080  [243200/481450]\n",
      "loss: 0.919028  [246400/481450]\n",
      "loss: 0.967026  [249600/481450]\n",
      "loss: 0.895167  [252800/481450]\n",
      "loss: 0.757264  [256000/481450]\n",
      "loss: 1.023457  [259200/481450]\n",
      "loss: 0.859670  [262400/481450]\n",
      "loss: 0.994559  [265600/481450]\n",
      "loss: 0.997858  [268800/481450]\n",
      "loss: 1.078647  [272000/481450]\n",
      "loss: 0.928273  [275200/481450]\n",
      "loss: 0.874435  [278400/481450]\n",
      "loss: 0.869427  [281600/481450]\n",
      "loss: 0.742351  [284800/481450]\n",
      "loss: 1.025938  [288000/481450]\n",
      "loss: 0.956714  [291200/481450]\n",
      "loss: 0.928800  [294400/481450]\n",
      "loss: 0.886214  [297600/481450]\n",
      "loss: 0.800780  [300800/481450]\n",
      "loss: 0.763357  [304000/481450]\n",
      "loss: 0.851758  [307200/481450]\n",
      "loss: 0.589397  [310400/481450]\n",
      "loss: 0.755204  [313600/481450]\n",
      "loss: 0.607739  [316800/481450]\n",
      "loss: 0.753753  [320000/481450]\n",
      "loss: 0.631726  [323200/481450]\n",
      "loss: 0.518193  [326400/481450]\n",
      "loss: 0.787731  [329600/481450]\n",
      "loss: 0.732393  [332800/481450]\n",
      "loss: 0.584266  [336000/481450]\n",
      "loss: 0.475160  [339200/481450]\n",
      "loss: 0.489937  [342400/481450]\n",
      "loss: 0.582605  [345600/481450]\n",
      "loss: 0.684758  [348800/481450]\n",
      "loss: 0.744645  [352000/481450]\n",
      "loss: 0.657788  [355200/481450]\n",
      "loss: 0.685893  [358400/481450]\n",
      "loss: 0.611006  [361600/481450]\n",
      "loss: 0.691291  [364800/481450]\n",
      "loss: 0.478617  [368000/481450]\n",
      "loss: 0.719696  [371200/481450]\n",
      "loss: 0.608254  [374400/481450]\n",
      "loss: 0.481778  [377600/481450]\n",
      "loss: 0.710812  [380800/481450]\n",
      "loss: 0.624868  [384000/481450]\n",
      "loss: 0.570412  [387200/481450]\n",
      "loss: 0.635719  [390400/481450]\n",
      "loss: 0.877773  [393600/481450]\n",
      "loss: 0.755940  [396800/481450]\n",
      "loss: 0.776570  [400000/481450]\n",
      "loss: 0.742357  [403200/481450]\n",
      "loss: 0.608174  [406400/481450]\n",
      "loss: 0.623187  [409600/481450]\n",
      "loss: 0.924247  [412800/481450]\n",
      "loss: 0.572516  [416000/481450]\n",
      "loss: 0.757366  [419200/481450]\n",
      "loss: 0.621336  [422400/481450]\n",
      "loss: 0.666014  [425600/481450]\n",
      "loss: 0.542630  [428800/481450]\n",
      "loss: 0.556336  [432000/481450]\n",
      "loss: 0.635425  [435200/481450]\n",
      "loss: 0.455155  [438400/481450]\n",
      "loss: 0.369195  [441600/481450]\n",
      "loss: 0.381284  [444800/481450]\n",
      "loss: 0.516927  [448000/481450]\n",
      "loss: 0.434112  [451200/481450]\n",
      "loss: 0.505051  [454400/481450]\n",
      "loss: 0.503181  [457600/481450]\n",
      "loss: 0.426077  [460800/481450]\n",
      "loss: 0.477459  [464000/481450]\n",
      "loss: 0.301170  [467200/481450]\n",
      "loss: 0.599495  [470400/481450]\n",
      "loss: 0.638821  [473600/481450]\n",
      "loss: 0.743693  [476800/481450]\n",
      "loss: 0.553819  [480000/481450]\n",
      "Train Accuracy: 59.0832%\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.464275, F1-score: 84.84% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.549526  [    0/481450]\n",
      "loss: 0.416353  [ 3200/481450]\n",
      "loss: 0.535704  [ 6400/481450]\n",
      "loss: 0.648393  [ 9600/481450]\n",
      "loss: 0.603772  [12800/481450]\n",
      "loss: 0.473544  [16000/481450]\n",
      "loss: 0.559136  [19200/481450]\n",
      "loss: 0.624310  [22400/481450]\n",
      "loss: 0.554427  [25600/481450]\n",
      "loss: 0.522819  [28800/481450]\n",
      "loss: 0.494496  [32000/481450]\n",
      "loss: 0.319717  [35200/481450]\n",
      "loss: 0.500331  [38400/481450]\n",
      "loss: 0.405252  [41600/481450]\n",
      "loss: 0.516679  [44800/481450]\n",
      "loss: 0.423506  [48000/481450]\n",
      "loss: 0.451188  [51200/481450]\n",
      "loss: 0.485627  [54400/481450]\n",
      "loss: 0.615074  [57600/481450]\n",
      "loss: 0.651293  [60800/481450]\n",
      "loss: 0.376539  [64000/481450]\n",
      "loss: 0.499610  [67200/481450]\n",
      "loss: 0.681735  [70400/481450]\n",
      "loss: 0.407024  [73600/481450]\n",
      "loss: 0.442168  [76800/481450]\n",
      "loss: 0.416999  [80000/481450]\n",
      "loss: 0.403378  [83200/481450]\n",
      "loss: 0.531816  [86400/481450]\n",
      "loss: 0.470179  [89600/481450]\n",
      "loss: 0.425767  [92800/481450]\n",
      "loss: 0.401102  [96000/481450]\n",
      "loss: 0.321074  [99200/481450]\n",
      "loss: 0.516974  [102400/481450]\n",
      "loss: 0.315020  [105600/481450]\n",
      "loss: 0.169802  [108800/481450]\n",
      "loss: 0.466422  [112000/481450]\n",
      "loss: 0.402740  [115200/481450]\n",
      "loss: 0.536289  [118400/481450]\n",
      "loss: 0.480476  [121600/481450]\n",
      "loss: 0.655534  [124800/481450]\n",
      "loss: 0.445576  [128000/481450]\n",
      "loss: 0.642903  [131200/481450]\n",
      "loss: 0.349741  [134400/481450]\n",
      "loss: 0.452668  [137600/481450]\n",
      "loss: 0.376170  [140800/481450]\n",
      "loss: 0.526224  [144000/481450]\n",
      "loss: 0.949526  [147200/481450]\n",
      "loss: 0.336913  [150400/481450]\n",
      "loss: 0.344020  [153600/481450]\n",
      "loss: 0.370687  [156800/481450]\n",
      "loss: 0.373828  [160000/481450]\n",
      "loss: 0.245490  [163200/481450]\n",
      "loss: 0.384817  [166400/481450]\n",
      "loss: 0.550883  [169600/481450]\n",
      "loss: 0.443630  [172800/481450]\n",
      "loss: 0.241597  [176000/481450]\n",
      "loss: 0.442614  [179200/481450]\n",
      "loss: 0.513900  [182400/481450]\n",
      "loss: 0.260038  [185600/481450]\n",
      "loss: 0.766483  [188800/481450]\n",
      "loss: 0.365958  [192000/481450]\n",
      "loss: 0.681617  [195200/481450]\n",
      "loss: 0.239555  [198400/481450]\n",
      "loss: 0.641453  [201600/481450]\n",
      "loss: 0.417118  [204800/481450]\n",
      "loss: 0.436869  [208000/481450]\n",
      "loss: 0.359115  [211200/481450]\n",
      "loss: 0.357038  [214400/481450]\n",
      "loss: 0.516348  [217600/481450]\n",
      "loss: 0.167713  [220800/481450]\n",
      "loss: 0.492942  [224000/481450]\n",
      "loss: 0.156663  [227200/481450]\n",
      "loss: 0.235717  [230400/481450]\n",
      "loss: 0.740446  [233600/481450]\n",
      "loss: 0.570598  [236800/481450]\n",
      "loss: 0.497533  [240000/481450]\n",
      "loss: 0.476158  [243200/481450]\n",
      "loss: 0.178865  [246400/481450]\n",
      "loss: 0.450120  [249600/481450]\n",
      "loss: 0.375664  [252800/481450]\n",
      "loss: 0.248106  [256000/481450]\n",
      "loss: 0.226232  [259200/481450]\n",
      "loss: 0.428131  [262400/481450]\n",
      "loss: 0.438990  [265600/481450]\n",
      "loss: 0.353311  [268800/481450]\n",
      "loss: 0.525331  [272000/481450]\n",
      "loss: 0.590552  [275200/481450]\n",
      "loss: 0.250151  [278400/481450]\n",
      "loss: 0.405133  [281600/481450]\n",
      "loss: 0.322668  [284800/481450]\n",
      "loss: 0.424601  [288000/481450]\n",
      "loss: 0.255873  [291200/481450]\n",
      "loss: 0.300956  [294400/481450]\n",
      "loss: 0.277466  [297600/481450]\n",
      "loss: 0.511586  [300800/481450]\n",
      "loss: 0.487045  [304000/481450]\n",
      "loss: 0.417903  [307200/481450]\n",
      "loss: 0.359831  [310400/481450]\n",
      "loss: 0.343692  [313600/481450]\n",
      "loss: 0.477147  [316800/481450]\n",
      "loss: 0.388046  [320000/481450]\n",
      "loss: 0.340059  [323200/481450]\n",
      "loss: 0.105179  [326400/481450]\n",
      "loss: 0.415727  [329600/481450]\n",
      "loss: 0.375352  [332800/481450]\n",
      "loss: 0.481901  [336000/481450]\n",
      "loss: 0.824188  [339200/481450]\n",
      "loss: 0.286492  [342400/481450]\n",
      "loss: 0.135430  [345600/481450]\n",
      "loss: 0.519584  [348800/481450]\n",
      "loss: 0.255646  [352000/481450]\n",
      "loss: 0.258521  [355200/481450]\n",
      "loss: 0.543415  [358400/481450]\n",
      "loss: 0.238088  [361600/481450]\n",
      "loss: 0.302256  [364800/481450]\n",
      "loss: 0.283171  [368000/481450]\n",
      "loss: 0.348099  [371200/481450]\n",
      "loss: 0.275121  [374400/481450]\n",
      "loss: 0.437690  [377600/481450]\n",
      "loss: 0.418821  [380800/481450]\n",
      "loss: 0.532590  [384000/481450]\n",
      "loss: 0.371987  [387200/481450]\n",
      "loss: 0.385879  [390400/481450]\n",
      "loss: 0.350186  [393600/481450]\n",
      "loss: 0.302868  [396800/481450]\n",
      "loss: 0.348871  [400000/481450]\n",
      "loss: 0.311688  [403200/481450]\n",
      "loss: 0.332887  [406400/481450]\n",
      "loss: 0.551198  [409600/481450]\n",
      "loss: 0.514694  [412800/481450]\n",
      "loss: 0.288080  [416000/481450]\n",
      "loss: 0.277868  [419200/481450]\n",
      "loss: 0.226365  [422400/481450]\n",
      "loss: 0.864810  [425600/481450]\n",
      "loss: 0.285065  [428800/481450]\n",
      "loss: 0.441291  [432000/481450]\n",
      "loss: 0.432169  [435200/481450]\n",
      "loss: 0.301856  [438400/481450]\n",
      "loss: 0.312341  [441600/481450]\n",
      "loss: 0.517047  [444800/481450]\n",
      "loss: 0.585974  [448000/481450]\n",
      "loss: 0.286127  [451200/481450]\n",
      "loss: 0.650979  [454400/481450]\n",
      "loss: 0.408150  [457600/481450]\n",
      "loss: 0.247523  [460800/481450]\n",
      "loss: 0.237400  [464000/481450]\n",
      "loss: 0.448388  [467200/481450]\n",
      "loss: 0.358032  [470400/481450]\n",
      "loss: 0.221832  [473600/481450]\n",
      "loss: 0.276747  [476800/481450]\n",
      "loss: 0.518246  [480000/481450]\n",
      "Train Accuracy: 84.7251%\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.473067, F1-score: 86.13% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.351789  [    0/481450]\n",
      "loss: 0.686640  [ 3200/481450]\n",
      "loss: 0.281664  [ 6400/481450]\n",
      "loss: 0.525843  [ 9600/481450]\n",
      "loss: 0.557119  [12800/481450]\n",
      "loss: 0.402680  [16000/481450]\n",
      "loss: 0.343929  [19200/481450]\n",
      "loss: 0.243546  [22400/481450]\n",
      "loss: 0.282876  [25600/481450]\n",
      "loss: 0.361540  [28800/481450]\n",
      "loss: 0.304771  [32000/481450]\n",
      "loss: 0.402314  [35200/481450]\n",
      "loss: 0.424836  [38400/481450]\n",
      "loss: 0.174665  [41600/481450]\n",
      "loss: 0.179429  [44800/481450]\n",
      "loss: 0.267281  [48000/481450]\n",
      "loss: 0.437208  [51200/481450]\n",
      "loss: 0.138819  [54400/481450]\n",
      "loss: 0.381551  [57600/481450]\n",
      "loss: 0.272940  [60800/481450]\n",
      "loss: 0.415267  [64000/481450]\n",
      "loss: 0.240828  [67200/481450]\n",
      "loss: 0.390515  [70400/481450]\n",
      "loss: 0.236121  [73600/481450]\n",
      "loss: 0.330558  [76800/481450]\n",
      "loss: 0.238007  [80000/481450]\n",
      "loss: 0.154570  [83200/481450]\n",
      "loss: 0.366637  [86400/481450]\n",
      "loss: 0.416084  [89600/481450]\n",
      "loss: 0.396860  [92800/481450]\n",
      "loss: 0.567622  [96000/481450]\n",
      "loss: 0.246696  [99200/481450]\n",
      "loss: 0.315719  [102400/481450]\n",
      "loss: 0.540346  [105600/481450]\n",
      "loss: 0.196841  [108800/481450]\n",
      "loss: 0.214863  [112000/481450]\n",
      "loss: 0.355505  [115200/481450]\n",
      "loss: 0.209295  [118400/481450]\n",
      "loss: 0.366912  [121600/481450]\n",
      "loss: 0.402583  [124800/481450]\n",
      "loss: 0.595722  [128000/481450]\n",
      "loss: 0.438945  [131200/481450]\n",
      "loss: 0.317188  [134400/481450]\n",
      "loss: 0.269349  [137600/481450]\n",
      "loss: 0.535460  [140800/481450]\n",
      "loss: 0.377842  [144000/481450]\n",
      "loss: 0.307539  [147200/481450]\n",
      "loss: 0.223152  [150400/481450]\n",
      "loss: 0.432109  [153600/481450]\n",
      "loss: 0.581440  [156800/481450]\n",
      "loss: 0.487696  [160000/481450]\n",
      "loss: 0.631791  [163200/481450]\n",
      "loss: 0.483119  [166400/481450]\n",
      "loss: 0.204325  [169600/481450]\n",
      "loss: 0.425894  [172800/481450]\n",
      "loss: 0.501989  [176000/481450]\n",
      "loss: 0.461927  [179200/481450]\n",
      "loss: 0.307172  [182400/481450]\n",
      "loss: 0.190406  [185600/481450]\n",
      "loss: 0.318869  [188800/481450]\n",
      "loss: 0.348161  [192000/481450]\n",
      "loss: 0.142369  [195200/481450]\n",
      "loss: 0.470005  [198400/481450]\n",
      "loss: 0.339096  [201600/481450]\n",
      "loss: 0.425520  [204800/481450]\n",
      "loss: 0.409748  [208000/481450]\n",
      "loss: 0.340291  [211200/481450]\n",
      "loss: 0.258866  [214400/481450]\n",
      "loss: 0.244442  [217600/481450]\n",
      "loss: 0.172447  [220800/481450]\n",
      "loss: 0.282636  [224000/481450]\n",
      "loss: 0.356751  [227200/481450]\n",
      "loss: 0.394865  [230400/481450]\n",
      "loss: 0.397348  [233600/481450]\n",
      "loss: 0.238319  [236800/481450]\n",
      "loss: 0.500887  [240000/481450]\n",
      "loss: 0.277645  [243200/481450]\n",
      "loss: 0.247285  [246400/481450]\n",
      "loss: 0.328062  [249600/481450]\n",
      "loss: 0.350513  [252800/481450]\n",
      "loss: 0.332516  [256000/481450]\n",
      "loss: 0.165806  [259200/481450]\n",
      "loss: 0.186483  [262400/481450]\n",
      "loss: 0.115978  [265600/481450]\n",
      "loss: 0.307728  [268800/481450]\n",
      "loss: 0.436030  [272000/481450]\n",
      "loss: 0.449383  [275200/481450]\n",
      "loss: 0.608085  [278400/481450]\n",
      "loss: 0.370096  [281600/481450]\n",
      "loss: 0.348158  [284800/481450]\n",
      "loss: 0.352549  [288000/481450]\n",
      "loss: 0.618697  [291200/481450]\n",
      "loss: 0.206013  [294400/481450]\n",
      "loss: 0.501522  [297600/481450]\n",
      "loss: 0.242486  [300800/481450]\n",
      "loss: 0.462760  [304000/481450]\n",
      "loss: 0.259296  [307200/481450]\n",
      "loss: 0.365518  [310400/481450]\n",
      "loss: 0.499341  [313600/481450]\n",
      "loss: 0.385196  [316800/481450]\n",
      "loss: 0.376804  [320000/481450]\n",
      "loss: 0.466071  [323200/481450]\n",
      "loss: 0.392930  [326400/481450]\n",
      "loss: 0.388628  [329600/481450]\n",
      "loss: 0.222910  [332800/481450]\n",
      "loss: 0.395989  [336000/481450]\n",
      "loss: 0.218767  [339200/481450]\n",
      "loss: 0.120300  [342400/481450]\n",
      "loss: 0.205707  [345600/481450]\n",
      "loss: 0.282373  [348800/481450]\n",
      "loss: 0.525168  [352000/481450]\n",
      "loss: 0.199083  [355200/481450]\n",
      "loss: 0.208832  [358400/481450]\n",
      "loss: 0.301241  [361600/481450]\n",
      "loss: 0.351062  [364800/481450]\n",
      "loss: 0.281362  [368000/481450]\n",
      "loss: 0.412280  [371200/481450]\n",
      "loss: 0.335903  [374400/481450]\n",
      "loss: 0.263967  [377600/481450]\n",
      "loss: 0.323144  [380800/481450]\n",
      "loss: 0.335594  [384000/481450]\n",
      "loss: 0.556340  [387200/481450]\n",
      "loss: 0.322638  [390400/481450]\n",
      "loss: 0.267255  [393600/481450]\n",
      "loss: 0.522413  [396800/481450]\n",
      "loss: 0.246169  [400000/481450]\n",
      "loss: 0.260074  [403200/481450]\n",
      "loss: 0.210819  [406400/481450]\n",
      "loss: 0.516754  [409600/481450]\n",
      "loss: 0.278641  [412800/481450]\n",
      "loss: 0.581824  [416000/481450]\n",
      "loss: 0.438277  [419200/481450]\n",
      "loss: 0.415456  [422400/481450]\n",
      "loss: 0.258090  [425600/481450]\n",
      "loss: 0.470903  [428800/481450]\n",
      "loss: 0.214939  [432000/481450]\n",
      "loss: 0.338060  [435200/481450]\n",
      "loss: 0.201817  [438400/481450]\n",
      "loss: 0.278200  [441600/481450]\n",
      "loss: 0.586352  [444800/481450]\n",
      "loss: 0.292289  [448000/481450]\n",
      "loss: 0.530274  [451200/481450]\n",
      "loss: 0.236794  [454400/481450]\n",
      "loss: 0.381521  [457600/481450]\n",
      "loss: 0.314065  [460800/481450]\n",
      "loss: 0.255252  [464000/481450]\n",
      "loss: 0.064051  [467200/481450]\n",
      "loss: 0.292549  [470400/481450]\n",
      "loss: 0.262337  [473600/481450]\n",
      "loss: 0.279872  [476800/481450]\n",
      "loss: 0.116122  [480000/481450]\n",
      "Train Accuracy: 86.4673%\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.533732, F1-score: 85.33% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.412361  [    0/481450]\n",
      "loss: 0.319233  [ 3200/481450]\n",
      "loss: 0.331643  [ 6400/481450]\n",
      "loss: 0.239849  [ 9600/481450]\n",
      "loss: 0.327761  [12800/481450]\n",
      "loss: 0.539056  [16000/481450]\n",
      "loss: 0.282825  [19200/481450]\n",
      "loss: 0.447964  [22400/481450]\n",
      "loss: 0.266734  [25600/481450]\n",
      "loss: 0.128777  [28800/481450]\n",
      "loss: 0.431133  [32000/481450]\n",
      "loss: 0.380159  [35200/481450]\n",
      "loss: 0.298074  [38400/481450]\n",
      "loss: 0.076756  [41600/481450]\n",
      "loss: 0.217031  [44800/481450]\n",
      "loss: 0.389751  [48000/481450]\n",
      "loss: 0.437929  [51200/481450]\n",
      "loss: 0.265320  [54400/481450]\n",
      "loss: 0.244029  [57600/481450]\n",
      "loss: 0.318427  [60800/481450]\n",
      "loss: 0.414190  [64000/481450]\n",
      "loss: 0.209449  [67200/481450]\n",
      "loss: 0.380812  [70400/481450]\n",
      "loss: 0.187011  [73600/481450]\n",
      "loss: 0.434738  [76800/481450]\n",
      "loss: 0.392288  [80000/481450]\n",
      "loss: 0.084515  [83200/481450]\n",
      "loss: 0.164389  [86400/481450]\n",
      "loss: 0.233617  [89600/481450]\n",
      "loss: 0.661681  [92800/481450]\n",
      "loss: 0.206789  [96000/481450]\n",
      "loss: 0.305148  [99200/481450]\n",
      "loss: 0.402926  [102400/481450]\n",
      "loss: 0.328955  [105600/481450]\n",
      "loss: 0.196848  [108800/481450]\n",
      "loss: 1.008110  [112000/481450]\n",
      "loss: 0.218677  [115200/481450]\n",
      "loss: 0.198432  [118400/481450]\n",
      "loss: 0.665024  [121600/481450]\n",
      "loss: 0.264659  [124800/481450]\n",
      "loss: 0.296352  [128000/481450]\n",
      "loss: 0.253449  [131200/481450]\n",
      "loss: 0.297972  [134400/481450]\n",
      "loss: 0.182091  [137600/481450]\n",
      "loss: 0.588609  [140800/481450]\n",
      "loss: 0.217221  [144000/481450]\n",
      "loss: 0.229864  [147200/481450]\n",
      "loss: 0.414931  [150400/481450]\n",
      "loss: 0.487171  [153600/481450]\n",
      "loss: 0.437958  [156800/481450]\n",
      "loss: 0.473172  [160000/481450]\n",
      "loss: 0.288959  [163200/481450]\n",
      "loss: 0.369139  [166400/481450]\n",
      "loss: 0.437655  [169600/481450]\n",
      "loss: 0.198255  [172800/481450]\n",
      "loss: 0.303798  [176000/481450]\n",
      "loss: 0.304964  [179200/481450]\n",
      "loss: 0.399350  [182400/481450]\n",
      "loss: 0.068174  [185600/481450]\n",
      "loss: 0.361435  [188800/481450]\n",
      "loss: 0.324644  [192000/481450]\n",
      "loss: 0.230597  [195200/481450]\n",
      "loss: 0.482790  [198400/481450]\n",
      "loss: 0.378021  [201600/481450]\n",
      "loss: 0.320287  [204800/481450]\n",
      "loss: 0.246981  [208000/481450]\n",
      "loss: 0.289639  [211200/481450]\n",
      "loss: 0.354521  [214400/481450]\n",
      "loss: 0.133941  [217600/481450]\n",
      "loss: 0.205415  [220800/481450]\n",
      "loss: 0.486960  [224000/481450]\n",
      "loss: 0.243947  [227200/481450]\n",
      "loss: 0.114435  [230400/481450]\n",
      "loss: 0.319032  [233600/481450]\n",
      "loss: 0.382885  [236800/481450]\n",
      "loss: 0.251669  [240000/481450]\n",
      "loss: 0.272059  [243200/481450]\n",
      "loss: 0.374974  [246400/481450]\n",
      "loss: 0.279956  [249600/481450]\n",
      "loss: 0.391384  [252800/481450]\n",
      "loss: 0.301112  [256000/481450]\n",
      "loss: 0.392802  [259200/481450]\n",
      "loss: 0.367357  [262400/481450]\n",
      "loss: 0.136921  [265600/481450]\n",
      "loss: 0.407524  [268800/481450]\n",
      "loss: 0.438395  [272000/481450]\n",
      "loss: 0.302526  [275200/481450]\n",
      "loss: 0.523394  [278400/481450]\n",
      "loss: 0.412882  [281600/481450]\n",
      "loss: 0.154258  [284800/481450]\n",
      "loss: 0.168436  [288000/481450]\n",
      "loss: 0.274394  [291200/481450]\n",
      "loss: 0.192785  [294400/481450]\n",
      "loss: 0.242757  [297600/481450]\n",
      "loss: 0.468281  [300800/481450]\n",
      "loss: 0.322693  [304000/481450]\n",
      "loss: 0.254569  [307200/481450]\n",
      "loss: 0.250592  [310400/481450]\n",
      "loss: 0.214923  [313600/481450]\n",
      "loss: 0.371705  [316800/481450]\n",
      "loss: 0.513155  [320000/481450]\n",
      "loss: 0.317516  [323200/481450]\n",
      "loss: 0.300799  [326400/481450]\n",
      "loss: 0.399451  [329600/481450]\n",
      "loss: 0.339916  [332800/481450]\n",
      "loss: 0.197415  [336000/481450]\n",
      "loss: 0.212631  [339200/481450]\n",
      "loss: 0.428565  [342400/481450]\n",
      "loss: 0.418912  [345600/481450]\n",
      "loss: 0.348797  [348800/481450]\n",
      "loss: 0.305253  [352000/481450]\n",
      "loss: 0.380463  [355200/481450]\n",
      "loss: 0.244235  [358400/481450]\n",
      "loss: 0.309690  [361600/481450]\n",
      "loss: 0.395419  [364800/481450]\n",
      "loss: 0.469685  [368000/481450]\n",
      "loss: 0.330315  [371200/481450]\n",
      "loss: 0.351808  [374400/481450]\n",
      "loss: 0.315100  [377600/481450]\n",
      "loss: 0.207414  [380800/481450]\n",
      "loss: 0.274279  [384000/481450]\n",
      "loss: 0.283009  [387200/481450]\n",
      "loss: 0.158108  [390400/481450]\n",
      "loss: 0.295292  [393600/481450]\n",
      "loss: 0.344053  [396800/481450]\n",
      "loss: 0.143843  [400000/481450]\n",
      "loss: 0.317153  [403200/481450]\n",
      "loss: 0.210927  [406400/481450]\n",
      "loss: 0.343953  [409600/481450]\n",
      "loss: 0.502374  [412800/481450]\n",
      "loss: 0.232793  [416000/481450]\n",
      "loss: 0.322040  [419200/481450]\n",
      "loss: 0.402516  [422400/481450]\n",
      "loss: 0.347256  [425600/481450]\n",
      "loss: 0.355773  [428800/481450]\n",
      "loss: 0.120612  [432000/481450]\n",
      "loss: 0.322601  [435200/481450]\n",
      "loss: 0.226720  [438400/481450]\n",
      "loss: 0.266239  [441600/481450]\n",
      "loss: 0.441005  [444800/481450]\n",
      "loss: 0.265355  [448000/481450]\n",
      "loss: 0.279925  [451200/481450]\n",
      "loss: 0.438584  [454400/481450]\n",
      "loss: 0.214853  [457600/481450]\n",
      "loss: 0.202593  [460800/481450]\n",
      "loss: 0.189513  [464000/481450]\n",
      "loss: 0.436237  [467200/481450]\n",
      "loss: 0.392567  [470400/481450]\n",
      "loss: 0.272770  [473600/481450]\n",
      "loss: 0.338188  [476800/481450]\n",
      "loss: 0.320040  [480000/481450]\n",
      "Train Accuracy: 87.2421%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.586245, F1-score: 85.65% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.178394  [    0/481450]\n",
      "loss: 0.243608  [ 3200/481450]\n",
      "loss: 0.630504  [ 6400/481450]\n",
      "loss: 0.499693  [ 9600/481450]\n",
      "loss: 0.254738  [12800/481450]\n",
      "loss: 0.404805  [16000/481450]\n",
      "loss: 0.196862  [19200/481450]\n",
      "loss: 0.327363  [22400/481450]\n",
      "loss: 0.209479  [25600/481450]\n",
      "loss: 0.308986  [28800/481450]\n",
      "loss: 0.435077  [32000/481450]\n",
      "loss: 0.152942  [35200/481450]\n",
      "loss: 0.422749  [38400/481450]\n",
      "loss: 0.289341  [41600/481450]\n",
      "loss: 0.184281  [44800/481450]\n",
      "loss: 0.256719  [48000/481450]\n",
      "loss: 0.247143  [51200/481450]\n",
      "loss: 0.297428  [54400/481450]\n",
      "loss: 0.329392  [57600/481450]\n",
      "loss: 0.269235  [60800/481450]\n",
      "loss: 0.173227  [64000/481450]\n",
      "loss: 0.462555  [67200/481450]\n",
      "loss: 0.547615  [70400/481450]\n",
      "loss: 0.214285  [73600/481450]\n",
      "loss: 0.397174  [76800/481450]\n",
      "loss: 0.487726  [80000/481450]\n",
      "loss: 0.311390  [83200/481450]\n",
      "loss: 0.301337  [86400/481450]\n",
      "loss: 0.350634  [89600/481450]\n",
      "loss: 0.301935  [92800/481450]\n",
      "loss: 0.357325  [96000/481450]\n",
      "loss: 0.292316  [99200/481450]\n",
      "loss: 0.270204  [102400/481450]\n",
      "loss: 0.322985  [105600/481450]\n",
      "loss: 0.167226  [108800/481450]\n",
      "loss: 0.270543  [112000/481450]\n",
      "loss: 0.415671  [115200/481450]\n",
      "loss: 0.187194  [118400/481450]\n",
      "loss: 0.160646  [121600/481450]\n",
      "loss: 0.119289  [124800/481450]\n",
      "loss: 0.246529  [128000/481450]\n",
      "loss: 0.225352  [131200/481450]\n",
      "loss: 0.175739  [134400/481450]\n",
      "loss: 0.414775  [137600/481450]\n",
      "loss: 0.483140  [140800/481450]\n",
      "loss: 0.183878  [144000/481450]\n",
      "loss: 0.203944  [147200/481450]\n",
      "loss: 0.369495  [150400/481450]\n",
      "loss: 0.285884  [153600/481450]\n",
      "loss: 0.190469  [156800/481450]\n",
      "loss: 0.247806  [160000/481450]\n",
      "loss: 0.296282  [163200/481450]\n",
      "loss: 0.390376  [166400/481450]\n",
      "loss: 0.213144  [169600/481450]\n",
      "loss: 0.148683  [172800/481450]\n",
      "loss: 0.217738  [176000/481450]\n",
      "loss: 0.439940  [179200/481450]\n",
      "loss: 0.171094  [182400/481450]\n",
      "loss: 0.439751  [185600/481450]\n",
      "loss: 0.315518  [188800/481450]\n",
      "loss: 0.478858  [192000/481450]\n",
      "loss: 0.429466  [195200/481450]\n",
      "loss: 0.418770  [198400/481450]\n",
      "loss: 0.224847  [201600/481450]\n",
      "loss: 0.337052  [204800/481450]\n",
      "loss: 0.235941  [208000/481450]\n",
      "loss: 0.295891  [211200/481450]\n",
      "loss: 0.367865  [214400/481450]\n",
      "loss: 0.474662  [217600/481450]\n",
      "loss: 0.383526  [220800/481450]\n",
      "loss: 0.252423  [224000/481450]\n",
      "loss: 0.579020  [227200/481450]\n",
      "loss: 0.183122  [230400/481450]\n",
      "loss: 0.273622  [233600/481450]\n",
      "loss: 0.254866  [236800/481450]\n",
      "loss: 0.270453  [240000/481450]\n",
      "loss: 0.107818  [243200/481450]\n",
      "loss: 0.372863  [246400/481450]\n",
      "loss: 0.148264  [249600/481450]\n",
      "loss: 0.165611  [252800/481450]\n",
      "loss: 0.434451  [256000/481450]\n",
      "loss: 0.473391  [259200/481450]\n",
      "loss: 0.306402  [262400/481450]\n",
      "loss: 0.164058  [265600/481450]\n",
      "loss: 0.367781  [268800/481450]\n",
      "loss: 0.151797  [272000/481450]\n",
      "loss: 0.167650  [275200/481450]\n",
      "loss: 0.319968  [278400/481450]\n",
      "loss: 0.358222  [281600/481450]\n",
      "loss: 0.181128  [284800/481450]\n",
      "loss: 0.324973  [288000/481450]\n",
      "loss: 0.308883  [291200/481450]\n",
      "loss: 0.461680  [294400/481450]\n",
      "loss: 0.170876  [297600/481450]\n",
      "loss: 0.236146  [300800/481450]\n",
      "loss: 0.106013  [304000/481450]\n",
      "loss: 0.269641  [307200/481450]\n",
      "loss: 0.251486  [310400/481450]\n",
      "loss: 0.369259  [313600/481450]\n",
      "loss: 0.272234  [316800/481450]\n",
      "loss: 0.223195  [320000/481450]\n",
      "loss: 0.394757  [323200/481450]\n",
      "loss: 0.251859  [326400/481450]\n",
      "loss: 0.158514  [329600/481450]\n",
      "loss: 0.365049  [332800/481450]\n",
      "loss: 0.496092  [336000/481450]\n",
      "loss: 0.161339  [339200/481450]\n",
      "loss: 0.334439  [342400/481450]\n",
      "loss: 0.412118  [345600/481450]\n",
      "loss: 0.332049  [348800/481450]\n",
      "loss: 0.192776  [352000/481450]\n",
      "loss: 0.221900  [355200/481450]\n",
      "loss: 0.411608  [358400/481450]\n",
      "loss: 0.222703  [361600/481450]\n",
      "loss: 0.431602  [364800/481450]\n",
      "loss: 0.325050  [368000/481450]\n",
      "loss: 0.207377  [371200/481450]\n",
      "loss: 0.209531  [374400/481450]\n",
      "loss: 0.244761  [377600/481450]\n",
      "loss: 0.301123  [380800/481450]\n",
      "loss: 0.459766  [384000/481450]\n",
      "loss: 0.439647  [387200/481450]\n",
      "loss: 0.325192  [390400/481450]\n",
      "loss: 0.262642  [393600/481450]\n",
      "loss: 0.251528  [396800/481450]\n",
      "loss: 0.339169  [400000/481450]\n",
      "loss: 0.298051  [403200/481450]\n",
      "loss: 0.387075  [406400/481450]\n",
      "loss: 0.394090  [409600/481450]\n",
      "loss: 0.218602  [412800/481450]\n",
      "loss: 0.260270  [416000/481450]\n",
      "loss: 0.355421  [419200/481450]\n",
      "loss: 0.283206  [422400/481450]\n",
      "loss: 0.180363  [425600/481450]\n",
      "loss: 0.154005  [428800/481450]\n",
      "loss: 0.193350  [432000/481450]\n",
      "loss: 0.297108  [435200/481450]\n",
      "loss: 0.177438  [438400/481450]\n",
      "loss: 0.263316  [441600/481450]\n",
      "loss: 0.301336  [444800/481450]\n",
      "loss: 0.300960  [448000/481450]\n",
      "loss: 0.288530  [451200/481450]\n",
      "loss: 0.397805  [454400/481450]\n",
      "loss: 0.537997  [457600/481450]\n",
      "loss: 0.207263  [460800/481450]\n",
      "loss: 0.199630  [464000/481450]\n",
      "loss: 0.352835  [467200/481450]\n",
      "loss: 0.554976  [470400/481450]\n",
      "loss: 0.324958  [473600/481450]\n",
      "loss: 0.300120  [476800/481450]\n",
      "loss: 0.281273  [480000/481450]\n",
      "Train Accuracy: 87.9142%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.635155, F1-score: 85.81% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.163958  [    0/481450]\n",
      "loss: 0.241551  [ 3200/481450]\n",
      "loss: 0.209360  [ 6400/481450]\n",
      "loss: 0.613982  [ 9600/481450]\n",
      "loss: 0.421081  [12800/481450]\n",
      "loss: 0.365781  [16000/481450]\n",
      "loss: 0.254621  [19200/481450]\n",
      "loss: 0.262691  [22400/481450]\n",
      "loss: 0.318541  [25600/481450]\n",
      "loss: 0.306475  [28800/481450]\n",
      "loss: 0.164327  [32000/481450]\n",
      "loss: 0.413298  [35200/481450]\n",
      "loss: 0.210271  [38400/481450]\n",
      "loss: 0.323233  [41600/481450]\n",
      "loss: 0.178471  [44800/481450]\n",
      "loss: 0.314318  [48000/481450]\n",
      "loss: 0.135324  [51200/481450]\n",
      "loss: 0.270715  [54400/481450]\n",
      "loss: 0.384935  [57600/481450]\n",
      "loss: 0.364239  [60800/481450]\n",
      "loss: 0.274544  [64000/481450]\n",
      "loss: 0.169558  [67200/481450]\n",
      "loss: 0.309209  [70400/481450]\n",
      "loss: 0.266555  [73600/481450]\n",
      "loss: 0.262690  [76800/481450]\n",
      "loss: 0.416164  [80000/481450]\n",
      "loss: 0.382281  [83200/481450]\n",
      "loss: 0.341710  [86400/481450]\n",
      "loss: 0.439978  [89600/481450]\n",
      "loss: 0.240832  [92800/481450]\n",
      "loss: 0.357654  [96000/481450]\n",
      "loss: 0.258044  [99200/481450]\n",
      "loss: 0.253571  [102400/481450]\n",
      "loss: 0.351884  [105600/481450]\n",
      "loss: 0.120302  [108800/481450]\n",
      "loss: 0.292260  [112000/481450]\n",
      "loss: 0.257964  [115200/481450]\n",
      "loss: 0.298189  [118400/481450]\n",
      "loss: 0.248263  [121600/481450]\n",
      "loss: 0.127451  [124800/481450]\n",
      "loss: 0.276016  [128000/481450]\n",
      "loss: 0.345472  [131200/481450]\n",
      "loss: 0.243501  [134400/481450]\n",
      "loss: 0.176802  [137600/481450]\n",
      "loss: 0.224535  [140800/481450]\n",
      "loss: 0.211259  [144000/481450]\n",
      "loss: 0.441977  [147200/481450]\n",
      "loss: 0.267726  [150400/481450]\n",
      "loss: 0.316580  [153600/481450]\n",
      "loss: 0.305090  [156800/481450]\n",
      "loss: 0.176642  [160000/481450]\n",
      "loss: 0.340189  [163200/481450]\n",
      "loss: 0.292423  [166400/481450]\n",
      "loss: 0.241955  [169600/481450]\n",
      "loss: 0.269003  [172800/481450]\n",
      "loss: 0.598814  [176000/481450]\n",
      "loss: 0.419934  [179200/481450]\n",
      "loss: 0.273355  [182400/481450]\n",
      "loss: 0.336352  [185600/481450]\n",
      "loss: 0.181591  [188800/481450]\n",
      "loss: 0.444195  [192000/481450]\n",
      "loss: 0.145761  [195200/481450]\n",
      "loss: 0.341925  [198400/481450]\n",
      "loss: 0.258374  [201600/481450]\n",
      "loss: 0.477507  [204800/481450]\n",
      "loss: 0.329519  [208000/481450]\n",
      "loss: 0.157282  [211200/481450]\n",
      "loss: 0.213295  [214400/481450]\n",
      "loss: 0.269324  [217600/481450]\n",
      "loss: 0.251852  [220800/481450]\n",
      "loss: 0.298223  [224000/481450]\n",
      "loss: 0.299868  [227200/481450]\n",
      "loss: 0.111739  [230400/481450]\n",
      "loss: 0.432366  [233600/481450]\n",
      "loss: 0.245783  [236800/481450]\n",
      "loss: 0.182156  [240000/481450]\n",
      "loss: 0.294796  [243200/481450]\n",
      "loss: 0.222678  [246400/481450]\n",
      "loss: 0.182914  [249600/481450]\n",
      "loss: 0.280075  [252800/481450]\n",
      "loss: 0.315041  [256000/481450]\n",
      "loss: 0.217819  [259200/481450]\n",
      "loss: 0.382491  [262400/481450]\n",
      "loss: 0.170348  [265600/481450]\n",
      "loss: 0.171442  [268800/481450]\n",
      "loss: 0.203904  [272000/481450]\n",
      "loss: 0.234326  [275200/481450]\n",
      "loss: 0.197333  [278400/481450]\n",
      "loss: 0.164355  [281600/481450]\n",
      "loss: 0.510247  [284800/481450]\n",
      "loss: 0.249895  [288000/481450]\n",
      "loss: 0.315056  [291200/481450]\n",
      "loss: 0.289243  [294400/481450]\n",
      "loss: 0.162359  [297600/481450]\n",
      "loss: 0.376574  [300800/481450]\n",
      "loss: 0.172664  [304000/481450]\n",
      "loss: 0.319506  [307200/481450]\n",
      "loss: 0.425055  [310400/481450]\n",
      "loss: 0.232925  [313600/481450]\n",
      "loss: 0.124397  [316800/481450]\n",
      "loss: 0.367727  [320000/481450]\n",
      "loss: 0.417274  [323200/481450]\n",
      "loss: 0.281511  [326400/481450]\n",
      "loss: 0.309181  [329600/481450]\n",
      "loss: 0.400862  [332800/481450]\n",
      "loss: 0.201302  [336000/481450]\n",
      "loss: 0.108841  [339200/481450]\n",
      "loss: 0.236147  [342400/481450]\n",
      "loss: 0.184363  [345600/481450]\n",
      "loss: 0.075777  [348800/481450]\n",
      "loss: 0.412698  [352000/481450]\n",
      "loss: 0.305338  [355200/481450]\n",
      "loss: 0.432518  [358400/481450]\n",
      "loss: 0.422855  [361600/481450]\n",
      "loss: 0.288145  [364800/481450]\n",
      "loss: 0.235794  [368000/481450]\n",
      "loss: 0.179807  [371200/481450]\n",
      "loss: 0.208002  [374400/481450]\n",
      "loss: 0.295735  [377600/481450]\n",
      "loss: 0.339111  [380800/481450]\n",
      "loss: 0.148544  [384000/481450]\n",
      "loss: 0.374479  [387200/481450]\n",
      "loss: 0.353498  [390400/481450]\n",
      "loss: 0.282240  [393600/481450]\n",
      "loss: 0.519486  [396800/481450]\n",
      "loss: 0.095019  [400000/481450]\n",
      "loss: 0.238815  [403200/481450]\n",
      "loss: 0.471025  [406400/481450]\n",
      "loss: 0.335152  [409600/481450]\n",
      "loss: 0.176773  [412800/481450]\n",
      "loss: 0.148049  [416000/481450]\n",
      "loss: 0.230200  [419200/481450]\n",
      "loss: 0.162677  [422400/481450]\n",
      "loss: 0.372290  [425600/481450]\n",
      "loss: 0.549952  [428800/481450]\n",
      "loss: 0.202219  [432000/481450]\n",
      "loss: 0.050406  [435200/481450]\n",
      "loss: 0.145021  [438400/481450]\n",
      "loss: 0.062185  [441600/481450]\n",
      "loss: 0.282617  [444800/481450]\n",
      "loss: 0.257213  [448000/481450]\n",
      "loss: 0.097251  [451200/481450]\n",
      "loss: 0.127396  [454400/481450]\n",
      "loss: 0.420515  [457600/481450]\n",
      "loss: 0.375790  [460800/481450]\n",
      "loss: 0.369029  [464000/481450]\n",
      "loss: 0.156610  [467200/481450]\n",
      "loss: 0.383036  [470400/481450]\n",
      "loss: 0.170908  [473600/481450]\n",
      "loss: 0.572815  [476800/481450]\n",
      "loss: 0.116359  [480000/481450]\n",
      "Train Accuracy: 88.6470%\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.613336, F1-score: 86.64% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.220444  [    0/481450]\n",
      "loss: 0.289053  [ 3200/481450]\n",
      "loss: 0.259646  [ 6400/481450]\n",
      "loss: 0.134867  [ 9600/481450]\n",
      "loss: 0.230007  [12800/481450]\n",
      "loss: 0.279796  [16000/481450]\n",
      "loss: 0.242750  [19200/481450]\n",
      "loss: 0.215956  [22400/481450]\n",
      "loss: 0.424596  [25600/481450]\n",
      "loss: 0.367061  [28800/481450]\n",
      "loss: 0.228487  [32000/481450]\n",
      "loss: 0.130405  [35200/481450]\n",
      "loss: 0.276026  [38400/481450]\n",
      "loss: 0.112380  [41600/481450]\n",
      "loss: 0.209376  [44800/481450]\n",
      "loss: 0.196497  [48000/481450]\n",
      "loss: 0.277256  [51200/481450]\n",
      "loss: 0.182717  [54400/481450]\n",
      "loss: 0.300346  [57600/481450]\n",
      "loss: 0.201388  [60800/481450]\n",
      "loss: 0.144873  [64000/481450]\n",
      "loss: 0.221623  [67200/481450]\n",
      "loss: 0.297305  [70400/481450]\n",
      "loss: 0.219182  [73600/481450]\n",
      "loss: 0.476310  [76800/481450]\n",
      "loss: 0.394110  [80000/481450]\n",
      "loss: 0.326546  [83200/481450]\n",
      "loss: 0.232502  [86400/481450]\n",
      "loss: 0.176611  [89600/481450]\n",
      "loss: 0.311457  [92800/481450]\n",
      "loss: 0.257724  [96000/481450]\n",
      "loss: 0.322820  [99200/481450]\n",
      "loss: 0.177791  [102400/481450]\n",
      "loss: 0.171301  [105600/481450]\n",
      "loss: 0.242656  [108800/481450]\n",
      "loss: 0.246710  [112000/481450]\n",
      "loss: 0.245033  [115200/481450]\n",
      "loss: 0.404658  [118400/481450]\n",
      "loss: 0.260136  [121600/481450]\n",
      "loss: 0.146588  [124800/481450]\n",
      "loss: 0.354746  [128000/481450]\n",
      "loss: 0.198485  [131200/481450]\n",
      "loss: 0.168262  [134400/481450]\n",
      "loss: 0.151465  [137600/481450]\n",
      "loss: 0.263647  [140800/481450]\n",
      "loss: 0.250525  [144000/481450]\n",
      "loss: 0.167964  [147200/481450]\n",
      "loss: 0.206809  [150400/481450]\n",
      "loss: 0.315171  [153600/481450]\n",
      "loss: 0.400280  [156800/481450]\n",
      "loss: 0.325113  [160000/481450]\n",
      "loss: 0.487832  [163200/481450]\n",
      "loss: 0.231584  [166400/481450]\n",
      "loss: 0.319052  [169600/481450]\n",
      "loss: 0.137253  [172800/481450]\n",
      "loss: 0.129715  [176000/481450]\n",
      "loss: 0.385837  [179200/481450]\n",
      "loss: 0.474060  [182400/481450]\n",
      "loss: 0.432629  [185600/481450]\n",
      "loss: 0.293322  [188800/481450]\n",
      "loss: 0.048519  [192000/481450]\n",
      "loss: 0.417530  [195200/481450]\n",
      "loss: 0.202065  [198400/481450]\n",
      "loss: 0.144589  [201600/481450]\n",
      "loss: 0.451817  [204800/481450]\n",
      "loss: 0.212245  [208000/481450]\n",
      "loss: 0.373333  [211200/481450]\n",
      "loss: 0.208701  [214400/481450]\n",
      "loss: 0.232845  [217600/481450]\n",
      "loss: 0.149803  [220800/481450]\n",
      "loss: 0.471735  [224000/481450]\n",
      "loss: 0.258937  [227200/481450]\n",
      "loss: 0.249211  [230400/481450]\n",
      "loss: 0.323600  [233600/481450]\n",
      "loss: 0.197465  [236800/481450]\n",
      "loss: 0.349676  [240000/481450]\n",
      "loss: 0.255994  [243200/481450]\n",
      "loss: 0.157419  [246400/481450]\n",
      "loss: 0.319555  [249600/481450]\n",
      "loss: 0.151746  [252800/481450]\n",
      "loss: 0.215309  [256000/481450]\n",
      "loss: 0.369524  [259200/481450]\n",
      "loss: 0.286346  [262400/481450]\n",
      "loss: 0.221966  [265600/481450]\n",
      "loss: 0.213440  [268800/481450]\n",
      "loss: 0.189423  [272000/481450]\n",
      "loss: 0.353000  [275200/481450]\n",
      "loss: 0.216320  [278400/481450]\n",
      "loss: 0.197685  [281600/481450]\n",
      "loss: 0.188256  [284800/481450]\n",
      "loss: 0.201115  [288000/481450]\n",
      "loss: 0.339864  [291200/481450]\n",
      "loss: 0.321527  [294400/481450]\n",
      "loss: 0.232117  [297600/481450]\n",
      "loss: 0.137887  [300800/481450]\n",
      "loss: 0.406792  [304000/481450]\n",
      "loss: 0.161061  [307200/481450]\n",
      "loss: 0.242953  [310400/481450]\n",
      "loss: 0.115170  [313600/481450]\n",
      "loss: 0.169224  [316800/481450]\n",
      "loss: 0.234509  [320000/481450]\n",
      "loss: 0.154176  [323200/481450]\n",
      "loss: 0.323217  [326400/481450]\n",
      "loss: 0.156376  [329600/481450]\n",
      "loss: 0.225339  [332800/481450]\n",
      "loss: 0.208974  [336000/481450]\n",
      "loss: 0.186463  [339200/481450]\n",
      "loss: 0.352096  [342400/481450]\n",
      "loss: 0.213020  [345600/481450]\n",
      "loss: 0.279875  [348800/481450]\n",
      "loss: 0.307819  [352000/481450]\n",
      "loss: 0.183865  [355200/481450]\n",
      "loss: 0.251331  [358400/481450]\n",
      "loss: 0.205938  [361600/481450]\n",
      "loss: 0.231691  [364800/481450]\n",
      "loss: 0.256609  [368000/481450]\n",
      "loss: 0.356200  [371200/481450]\n",
      "loss: 0.359607  [374400/481450]\n",
      "loss: 0.372746  [377600/481450]\n",
      "loss: 0.382169  [380800/481450]\n",
      "loss: 0.443744  [384000/481450]\n",
      "loss: 0.287900  [387200/481450]\n",
      "loss: 0.190325  [390400/481450]\n",
      "loss: 0.213738  [393600/481450]\n",
      "loss: 0.216314  [396800/481450]\n",
      "loss: 0.356790  [400000/481450]\n",
      "loss: 0.371436  [403200/481450]\n",
      "loss: 0.333963  [406400/481450]\n",
      "loss: 0.289091  [409600/481450]\n",
      "loss: 0.272351  [412800/481450]\n",
      "loss: 0.308824  [416000/481450]\n",
      "loss: 0.457845  [419200/481450]\n",
      "loss: 0.228278  [422400/481450]\n",
      "loss: 0.264632  [425600/481450]\n",
      "loss: 0.348465  [428800/481450]\n",
      "loss: 0.194189  [432000/481450]\n",
      "loss: 0.243913  [435200/481450]\n",
      "loss: 0.376750  [438400/481450]\n",
      "loss: 0.188653  [441600/481450]\n",
      "loss: 0.218074  [444800/481450]\n",
      "loss: 0.213679  [448000/481450]\n",
      "loss: 0.497489  [451200/481450]\n",
      "loss: 0.263427  [454400/481450]\n",
      "loss: 0.171013  [457600/481450]\n",
      "loss: 0.229704  [460800/481450]\n",
      "loss: 0.350191  [464000/481450]\n",
      "loss: 0.276650  [467200/481450]\n",
      "loss: 0.132618  [470400/481450]\n",
      "loss: 0.282002  [473600/481450]\n",
      "loss: 0.426634  [476800/481450]\n",
      "loss: 0.306912  [480000/481450]\n",
      "Train Accuracy: 89.1326%\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.610479, F1-score: 86.92% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.163418  [    0/481450]\n",
      "loss: 0.409668  [ 3200/481450]\n",
      "loss: 0.106288  [ 6400/481450]\n",
      "loss: 0.236960  [ 9600/481450]\n",
      "loss: 0.244920  [12800/481450]\n",
      "loss: 0.155754  [16000/481450]\n",
      "loss: 0.164348  [19200/481450]\n",
      "loss: 0.393429  [22400/481450]\n",
      "loss: 0.279325  [25600/481450]\n",
      "loss: 0.308085  [28800/481450]\n",
      "loss: 0.337805  [32000/481450]\n",
      "loss: 0.340259  [35200/481450]\n",
      "loss: 0.230415  [38400/481450]\n",
      "loss: 0.273043  [41600/481450]\n",
      "loss: 0.215701  [44800/481450]\n",
      "loss: 0.251371  [48000/481450]\n",
      "loss: 0.031759  [51200/481450]\n",
      "loss: 0.414418  [54400/481450]\n",
      "loss: 0.495043  [57600/481450]\n",
      "loss: 0.285004  [60800/481450]\n",
      "loss: 0.303924  [64000/481450]\n",
      "loss: 0.249891  [67200/481450]\n",
      "loss: 0.358534  [70400/481450]\n",
      "loss: 0.276158  [73600/481450]\n",
      "loss: 0.504901  [76800/481450]\n",
      "loss: 0.423582  [80000/481450]\n",
      "loss: 0.287961  [83200/481450]\n",
      "loss: 0.506026  [86400/481450]\n",
      "loss: 0.179753  [89600/481450]\n",
      "loss: 0.095644  [92800/481450]\n",
      "loss: 0.130307  [96000/481450]\n",
      "loss: 0.191524  [99200/481450]\n",
      "loss: 0.162176  [102400/481450]\n",
      "loss: 0.109027  [105600/481450]\n",
      "loss: 0.188209  [108800/481450]\n",
      "loss: 0.407791  [112000/481450]\n",
      "loss: 0.136650  [115200/481450]\n",
      "loss: 0.192103  [118400/481450]\n",
      "loss: 0.198280  [121600/481450]\n",
      "loss: 0.175415  [124800/481450]\n",
      "loss: 0.148606  [128000/481450]\n",
      "loss: 0.328513  [131200/481450]\n",
      "loss: 0.247116  [134400/481450]\n",
      "loss: 0.145430  [137600/481450]\n",
      "loss: 0.216767  [140800/481450]\n",
      "loss: 0.436652  [144000/481450]\n",
      "loss: 0.414333  [147200/481450]\n",
      "loss: 0.445738  [150400/481450]\n",
      "loss: 0.479576  [153600/481450]\n",
      "loss: 0.249372  [156800/481450]\n",
      "loss: 0.196167  [160000/481450]\n",
      "loss: 0.198872  [163200/481450]\n",
      "loss: 0.328940  [166400/481450]\n",
      "loss: 0.157400  [169600/481450]\n",
      "loss: 0.305477  [172800/481450]\n",
      "loss: 0.226357  [176000/481450]\n",
      "loss: 0.245209  [179200/481450]\n",
      "loss: 0.221344  [182400/481450]\n",
      "loss: 0.297654  [185600/481450]\n",
      "loss: 0.263138  [188800/481450]\n",
      "loss: 0.108446  [192000/481450]\n",
      "loss: 0.227657  [195200/481450]\n",
      "loss: 0.285385  [198400/481450]\n",
      "loss: 0.262286  [201600/481450]\n",
      "loss: 0.206241  [204800/481450]\n",
      "loss: 0.184938  [208000/481450]\n",
      "loss: 0.232654  [211200/481450]\n",
      "loss: 0.220053  [214400/481450]\n",
      "loss: 0.164904  [217600/481450]\n",
      "loss: 0.558800  [220800/481450]\n",
      "loss: 0.294200  [224000/481450]\n",
      "loss: 0.235574  [227200/481450]\n",
      "loss: 0.356057  [230400/481450]\n",
      "loss: 0.181396  [233600/481450]\n",
      "loss: 0.162275  [236800/481450]\n",
      "loss: 0.346032  [240000/481450]\n",
      "loss: 0.302238  [243200/481450]\n",
      "loss: 0.113670  [246400/481450]\n",
      "loss: 0.220216  [249600/481450]\n",
      "loss: 0.165357  [252800/481450]\n",
      "loss: 0.147639  [256000/481450]\n",
      "loss: 0.216230  [259200/481450]\n",
      "loss: 0.253183  [262400/481450]\n",
      "loss: 0.138742  [265600/481450]\n",
      "loss: 0.312923  [268800/481450]\n",
      "loss: 0.367410  [272000/481450]\n",
      "loss: 0.330963  [275200/481450]\n",
      "loss: 0.099767  [278400/481450]\n",
      "loss: 0.175135  [281600/481450]\n",
      "loss: 0.210794  [284800/481450]\n",
      "loss: 0.332982  [288000/481450]\n",
      "loss: 0.268682  [291200/481450]\n",
      "loss: 0.543036  [294400/481450]\n",
      "loss: 0.246724  [297600/481450]\n",
      "loss: 0.442585  [300800/481450]\n",
      "loss: 0.349704  [304000/481450]\n",
      "loss: 0.324714  [307200/481450]\n",
      "loss: 0.402819  [310400/481450]\n",
      "loss: 0.124257  [313600/481450]\n",
      "loss: 0.135968  [316800/481450]\n",
      "loss: 0.376111  [320000/481450]\n",
      "loss: 0.234757  [323200/481450]\n",
      "loss: 0.214822  [326400/481450]\n",
      "loss: 0.370228  [329600/481450]\n",
      "loss: 0.148902  [332800/481450]\n",
      "loss: 0.120350  [336000/481450]\n",
      "loss: 0.233123  [339200/481450]\n",
      "loss: 0.249464  [342400/481450]\n",
      "loss: 0.358244  [345600/481450]\n",
      "loss: 0.161169  [348800/481450]\n",
      "loss: 0.071872  [352000/481450]\n",
      "loss: 0.191931  [355200/481450]\n",
      "loss: 0.084938  [358400/481450]\n",
      "loss: 0.243729  [361600/481450]\n",
      "loss: 0.409398  [364800/481450]\n",
      "loss: 0.305831  [368000/481450]\n",
      "loss: 0.151559  [371200/481450]\n",
      "loss: 0.344516  [374400/481450]\n",
      "loss: 0.116429  [377600/481450]\n",
      "loss: 0.244994  [380800/481450]\n",
      "loss: 0.390018  [384000/481450]\n",
      "loss: 0.189757  [387200/481450]\n",
      "loss: 0.247216  [390400/481450]\n",
      "loss: 0.363218  [393600/481450]\n",
      "loss: 0.382198  [396800/481450]\n",
      "loss: 0.305798  [400000/481450]\n",
      "loss: 0.184324  [403200/481450]\n",
      "loss: 0.183678  [406400/481450]\n",
      "loss: 0.284777  [409600/481450]\n",
      "loss: 0.084568  [412800/481450]\n",
      "loss: 0.323459  [416000/481450]\n",
      "loss: 0.366593  [419200/481450]\n",
      "loss: 0.250881  [422400/481450]\n",
      "loss: 0.377387  [425600/481450]\n",
      "loss: 0.210862  [428800/481450]\n",
      "loss: 0.324407  [432000/481450]\n",
      "loss: 0.295547  [435200/481450]\n",
      "loss: 0.360335  [438400/481450]\n",
      "loss: 0.364878  [441600/481450]\n",
      "loss: 0.277978  [444800/481450]\n",
      "loss: 0.254600  [448000/481450]\n",
      "loss: 0.233654  [451200/481450]\n",
      "loss: 0.246136  [454400/481450]\n",
      "loss: 0.390171  [457600/481450]\n",
      "loss: 0.167913  [460800/481450]\n",
      "loss: 0.112404  [464000/481450]\n",
      "loss: 0.173169  [467200/481450]\n",
      "loss: 0.320908  [470400/481450]\n",
      "loss: 0.094651  [473600/481450]\n",
      "loss: 0.226813  [476800/481450]\n",
      "loss: 0.309700  [480000/481450]\n",
      "Train Accuracy: 89.5626%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.556944, F1-score: 87.38% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.197585  [    0/481450]\n",
      "loss: 0.164822  [ 3200/481450]\n",
      "loss: 0.145368  [ 6400/481450]\n",
      "loss: 0.319550  [ 9600/481450]\n",
      "loss: 0.309729  [12800/481450]\n",
      "loss: 0.095946  [16000/481450]\n",
      "loss: 0.258557  [19200/481450]\n",
      "loss: 0.226458  [22400/481450]\n",
      "loss: 0.241718  [25600/481450]\n",
      "loss: 0.108890  [28800/481450]\n",
      "loss: 0.395568  [32000/481450]\n",
      "loss: 0.327897  [35200/481450]\n",
      "loss: 0.444376  [38400/481450]\n",
      "loss: 0.303829  [41600/481450]\n",
      "loss: 0.288875  [44800/481450]\n",
      "loss: 0.587372  [48000/481450]\n",
      "loss: 0.181860  [51200/481450]\n",
      "loss: 0.480441  [54400/481450]\n",
      "loss: 0.357508  [57600/481450]\n",
      "loss: 0.296356  [60800/481450]\n",
      "loss: 0.228428  [64000/481450]\n",
      "loss: 0.236203  [67200/481450]\n",
      "loss: 0.275061  [70400/481450]\n",
      "loss: 0.233368  [73600/481450]\n",
      "loss: 0.192078  [76800/481450]\n",
      "loss: 0.102775  [80000/481450]\n",
      "loss: 0.310503  [83200/481450]\n",
      "loss: 0.169340  [86400/481450]\n",
      "loss: 0.139034  [89600/481450]\n",
      "loss: 0.258526  [92800/481450]\n",
      "loss: 0.287040  [96000/481450]\n",
      "loss: 0.078704  [99200/481450]\n",
      "loss: 0.457634  [102400/481450]\n",
      "loss: 0.261500  [105600/481450]\n",
      "loss: 0.238631  [108800/481450]\n",
      "loss: 0.190045  [112000/481450]\n",
      "loss: 0.237086  [115200/481450]\n",
      "loss: 0.153657  [118400/481450]\n",
      "loss: 0.207370  [121600/481450]\n",
      "loss: 0.448912  [124800/481450]\n",
      "loss: 0.214893  [128000/481450]\n",
      "loss: 0.124815  [131200/481450]\n",
      "loss: 0.334936  [134400/481450]\n",
      "loss: 0.225809  [137600/481450]\n",
      "loss: 0.263755  [140800/481450]\n",
      "loss: 0.200381  [144000/481450]\n",
      "loss: 0.301836  [147200/481450]\n",
      "loss: 0.302806  [150400/481450]\n",
      "loss: 0.167355  [153600/481450]\n",
      "loss: 0.052365  [156800/481450]\n",
      "loss: 0.513650  [160000/481450]\n",
      "loss: 0.175837  [163200/481450]\n",
      "loss: 0.189652  [166400/481450]\n",
      "loss: 0.226411  [169600/481450]\n",
      "loss: 0.038727  [172800/481450]\n",
      "loss: 0.187302  [176000/481450]\n",
      "loss: 0.215494  [179200/481450]\n",
      "loss: 0.312296  [182400/481450]\n",
      "loss: 0.150993  [185600/481450]\n",
      "loss: 0.229323  [188800/481450]\n",
      "loss: 0.222827  [192000/481450]\n",
      "loss: 0.251350  [195200/481450]\n",
      "loss: 0.251169  [198400/481450]\n",
      "loss: 0.171146  [201600/481450]\n",
      "loss: 0.208157  [204800/481450]\n",
      "loss: 0.282019  [208000/481450]\n",
      "loss: 0.286475  [211200/481450]\n",
      "loss: 0.571339  [214400/481450]\n",
      "loss: 0.106712  [217600/481450]\n",
      "loss: 0.219126  [220800/481450]\n",
      "loss: 0.217142  [224000/481450]\n",
      "loss: 0.167260  [227200/481450]\n",
      "loss: 0.205208  [230400/481450]\n",
      "loss: 0.279507  [233600/481450]\n",
      "loss: 0.190866  [236800/481450]\n",
      "loss: 0.208226  [240000/481450]\n",
      "loss: 0.537489  [243200/481450]\n",
      "loss: 0.296116  [246400/481450]\n",
      "loss: 0.134204  [249600/481450]\n",
      "loss: 0.355031  [252800/481450]\n",
      "loss: 0.281411  [256000/481450]\n",
      "loss: 0.211899  [259200/481450]\n",
      "loss: 0.469706  [262400/481450]\n",
      "loss: 0.265527  [265600/481450]\n",
      "loss: 0.258079  [268800/481450]\n",
      "loss: 0.158802  [272000/481450]\n",
      "loss: 0.182673  [275200/481450]\n",
      "loss: 0.278433  [278400/481450]\n",
      "loss: 0.220150  [281600/481450]\n",
      "loss: 0.291329  [284800/481450]\n",
      "loss: 0.146519  [288000/481450]\n",
      "loss: 0.212506  [291200/481450]\n",
      "loss: 0.118357  [294400/481450]\n",
      "loss: 0.124961  [297600/481450]\n",
      "loss: 0.089053  [300800/481450]\n",
      "loss: 0.173206  [304000/481450]\n",
      "loss: 0.355215  [307200/481450]\n",
      "loss: 0.210974  [310400/481450]\n",
      "loss: 0.164231  [313600/481450]\n",
      "loss: 0.270042  [316800/481450]\n",
      "loss: 0.260921  [320000/481450]\n",
      "loss: 0.168027  [323200/481450]\n",
      "loss: 0.287316  [326400/481450]\n",
      "loss: 0.400658  [329600/481450]\n",
      "loss: 0.067343  [332800/481450]\n",
      "loss: 0.382057  [336000/481450]\n",
      "loss: 0.112551  [339200/481450]\n",
      "loss: 0.290631  [342400/481450]\n",
      "loss: 0.167764  [345600/481450]\n",
      "loss: 0.129842  [348800/481450]\n",
      "loss: 0.157478  [352000/481450]\n",
      "loss: 0.238345  [355200/481450]\n",
      "loss: 0.651394  [358400/481450]\n",
      "loss: 0.273987  [361600/481450]\n",
      "loss: 0.369599  [364800/481450]\n",
      "loss: 0.274759  [368000/481450]\n",
      "loss: 0.170653  [371200/481450]\n",
      "loss: 0.258794  [374400/481450]\n",
      "loss: 0.209757  [377600/481450]\n",
      "loss: 0.432535  [380800/481450]\n",
      "loss: 0.379607  [384000/481450]\n",
      "loss: 0.292744  [387200/481450]\n",
      "loss: 0.314859  [390400/481450]\n",
      "loss: 0.232070  [393600/481450]\n",
      "loss: 0.274749  [396800/481450]\n",
      "loss: 0.524370  [400000/481450]\n",
      "loss: 0.228284  [403200/481450]\n",
      "loss: 0.185811  [406400/481450]\n",
      "loss: 0.200235  [409600/481450]\n",
      "loss: 0.248883  [412800/481450]\n",
      "loss: 0.256854  [416000/481450]\n",
      "loss: 0.479977  [419200/481450]\n",
      "loss: 0.309526  [422400/481450]\n",
      "loss: 0.155730  [425600/481450]\n",
      "loss: 0.392760  [428800/481450]\n",
      "loss: 0.237248  [432000/481450]\n",
      "loss: 0.322044  [435200/481450]\n",
      "loss: 0.171243  [438400/481450]\n",
      "loss: 0.364217  [441600/481450]\n",
      "loss: 0.392235  [444800/481450]\n",
      "loss: 0.105702  [448000/481450]\n",
      "loss: 0.380282  [451200/481450]\n",
      "loss: 0.373746  [454400/481450]\n",
      "loss: 0.294384  [457600/481450]\n",
      "loss: 0.267228  [460800/481450]\n",
      "loss: 0.591015  [464000/481450]\n",
      "loss: 0.281554  [467200/481450]\n",
      "loss: 0.131638  [470400/481450]\n",
      "loss: 0.362603  [473600/481450]\n",
      "loss: 0.118607  [476800/481450]\n",
      "loss: 0.354557  [480000/481450]\n",
      "Train Accuracy: 89.9015%\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.569279, F1-score: 87.12% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.205557  [    0/481450]\n",
      "loss: 0.110355  [ 3200/481450]\n",
      "loss: 0.070342  [ 6400/481450]\n",
      "loss: 0.219523  [ 9600/481450]\n",
      "loss: 0.137510  [12800/481450]\n",
      "loss: 0.325950  [16000/481450]\n",
      "loss: 0.212999  [19200/481450]\n",
      "loss: 0.190419  [22400/481450]\n",
      "loss: 0.175115  [25600/481450]\n",
      "loss: 0.270821  [28800/481450]\n",
      "loss: 0.341030  [32000/481450]\n",
      "loss: 0.114483  [35200/481450]\n",
      "loss: 0.157352  [38400/481450]\n",
      "loss: 0.094957  [41600/481450]\n",
      "loss: 0.185875  [44800/481450]\n",
      "loss: 0.187703  [48000/481450]\n",
      "loss: 0.182071  [51200/481450]\n",
      "loss: 0.102275  [54400/481450]\n",
      "loss: 0.188211  [57600/481450]\n",
      "loss: 0.152966  [60800/481450]\n",
      "loss: 0.438318  [64000/481450]\n",
      "loss: 0.104840  [67200/481450]\n",
      "loss: 0.288033  [70400/481450]\n",
      "loss: 0.224763  [73600/481450]\n",
      "loss: 0.171478  [76800/481450]\n",
      "loss: 0.228393  [80000/481450]\n",
      "loss: 0.247134  [83200/481450]\n",
      "loss: 0.292695  [86400/481450]\n",
      "loss: 0.240213  [89600/481450]\n",
      "loss: 0.293323  [92800/481450]\n",
      "loss: 0.110180  [96000/481450]\n",
      "loss: 0.257337  [99200/481450]\n",
      "loss: 0.150742  [102400/481450]\n",
      "loss: 0.272430  [105600/481450]\n",
      "loss: 0.189163  [108800/481450]\n",
      "loss: 0.162052  [112000/481450]\n",
      "loss: 0.166313  [115200/481450]\n",
      "loss: 0.480282  [118400/481450]\n",
      "loss: 0.072120  [121600/481450]\n",
      "loss: 0.287086  [124800/481450]\n",
      "loss: 0.284023  [128000/481450]\n",
      "loss: 0.331375  [131200/481450]\n",
      "loss: 0.218243  [134400/481450]\n",
      "loss: 0.155307  [137600/481450]\n",
      "loss: 0.139227  [140800/481450]\n",
      "loss: 0.251409  [144000/481450]\n",
      "loss: 0.177304  [147200/481450]\n",
      "loss: 0.104233  [150400/481450]\n",
      "loss: 0.202805  [153600/481450]\n",
      "loss: 0.349547  [156800/481450]\n",
      "loss: 0.245625  [160000/481450]\n",
      "loss: 0.266874  [163200/481450]\n",
      "loss: 0.465881  [166400/481450]\n",
      "loss: 0.054148  [169600/481450]\n",
      "loss: 0.396283  [172800/481450]\n",
      "loss: 0.126495  [176000/481450]\n",
      "loss: 0.266025  [179200/481450]\n",
      "loss: 0.109913  [182400/481450]\n",
      "loss: 0.197429  [185600/481450]\n",
      "loss: 0.146638  [188800/481450]\n",
      "loss: 0.276930  [192000/481450]\n",
      "loss: 0.119150  [195200/481450]\n",
      "loss: 0.237294  [198400/481450]\n",
      "loss: 0.120249  [201600/481450]\n",
      "loss: 0.248940  [204800/481450]\n",
      "loss: 0.335262  [208000/481450]\n",
      "loss: 0.301915  [211200/481450]\n",
      "loss: 0.222511  [214400/481450]\n",
      "loss: 0.219547  [217600/481450]\n",
      "loss: 0.092764  [220800/481450]\n",
      "loss: 0.361931  [224000/481450]\n",
      "loss: 0.240489  [227200/481450]\n",
      "loss: 0.280400  [230400/481450]\n",
      "loss: 0.051721  [233600/481450]\n",
      "loss: 0.109789  [236800/481450]\n",
      "loss: 0.215840  [240000/481450]\n",
      "loss: 0.445071  [243200/481450]\n",
      "loss: 0.167400  [246400/481450]\n",
      "loss: 0.297973  [249600/481450]\n",
      "loss: 0.195431  [252800/481450]\n",
      "loss: 0.148267  [256000/481450]\n",
      "loss: 0.313015  [259200/481450]\n",
      "loss: 0.201364  [262400/481450]\n",
      "loss: 0.522908  [265600/481450]\n",
      "loss: 0.317854  [268800/481450]\n",
      "loss: 0.179089  [272000/481450]\n",
      "loss: 0.333861  [275200/481450]\n",
      "loss: 0.292821  [278400/481450]\n",
      "loss: 0.193605  [281600/481450]\n",
      "loss: 0.169446  [284800/481450]\n",
      "loss: 0.099828  [288000/481450]\n",
      "loss: 0.343343  [291200/481450]\n",
      "loss: 0.202049  [294400/481450]\n",
      "loss: 0.181489  [297600/481450]\n",
      "loss: 0.246908  [300800/481450]\n",
      "loss: 0.091180  [304000/481450]\n",
      "loss: 0.181950  [307200/481450]\n",
      "loss: 0.209134  [310400/481450]\n",
      "loss: 0.489296  [313600/481450]\n",
      "loss: 0.157635  [316800/481450]\n",
      "loss: 0.242718  [320000/481450]\n",
      "loss: 0.338784  [323200/481450]\n",
      "loss: 0.368874  [326400/481450]\n",
      "loss: 0.085229  [329600/481450]\n",
      "loss: 0.240074  [332800/481450]\n",
      "loss: 0.426083  [336000/481450]\n",
      "loss: 0.052948  [339200/481450]\n",
      "loss: 0.278027  [342400/481450]\n",
      "loss: 0.409150  [345600/481450]\n",
      "loss: 0.239397  [348800/481450]\n",
      "loss: 0.087777  [352000/481450]\n",
      "loss: 0.312829  [355200/481450]\n",
      "loss: 0.123948  [358400/481450]\n",
      "loss: 0.257843  [361600/481450]\n",
      "loss: 0.184184  [364800/481450]\n",
      "loss: 0.430111  [368000/481450]\n",
      "loss: 0.231497  [371200/481450]\n",
      "loss: 0.157471  [374400/481450]\n",
      "loss: 0.101531  [377600/481450]\n",
      "loss: 0.183683  [380800/481450]\n",
      "loss: 0.104929  [384000/481450]\n",
      "loss: 0.137127  [387200/481450]\n",
      "loss: 0.440731  [390400/481450]\n",
      "loss: 0.386334  [393600/481450]\n",
      "loss: 0.221240  [396800/481450]\n",
      "loss: 0.222151  [400000/481450]\n",
      "loss: 0.209433  [403200/481450]\n",
      "loss: 0.457444  [406400/481450]\n",
      "loss: 0.467519  [409600/481450]\n",
      "loss: 0.161347  [412800/481450]\n",
      "loss: 0.256661  [416000/481450]\n",
      "loss: 0.309161  [419200/481450]\n",
      "loss: 0.220258  [422400/481450]\n",
      "loss: 0.156512  [425600/481450]\n",
      "loss: 0.349608  [428800/481450]\n",
      "loss: 0.244074  [432000/481450]\n",
      "loss: 0.180236  [435200/481450]\n",
      "loss: 0.168390  [438400/481450]\n",
      "loss: 0.133692  [441600/481450]\n",
      "loss: 0.269893  [444800/481450]\n",
      "loss: 0.146349  [448000/481450]\n",
      "loss: 0.224820  [451200/481450]\n",
      "loss: 0.254962  [454400/481450]\n",
      "loss: 0.087154  [457600/481450]\n",
      "loss: 0.345950  [460800/481450]\n",
      "loss: 0.174462  [464000/481450]\n",
      "loss: 0.264098  [467200/481450]\n",
      "loss: 0.168620  [470400/481450]\n",
      "loss: 0.389196  [473600/481450]\n",
      "loss: 0.339820  [476800/481450]\n",
      "loss: 0.445982  [480000/481450]\n",
      "Train Accuracy: 90.1822%\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.578018, F1-score: 87.12% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.445651  [    0/481450]\n",
      "loss: 0.246544  [ 3200/481450]\n",
      "loss: 0.349853  [ 6400/481450]\n",
      "loss: 0.184639  [ 9600/481450]\n",
      "loss: 0.157060  [12800/481450]\n",
      "loss: 0.314502  [16000/481450]\n",
      "loss: 0.276381  [19200/481450]\n",
      "loss: 0.202409  [22400/481450]\n",
      "loss: 0.287625  [25600/481450]\n",
      "loss: 0.074178  [28800/481450]\n",
      "loss: 0.285092  [32000/481450]\n",
      "loss: 0.328331  [35200/481450]\n",
      "loss: 0.208847  [38400/481450]\n",
      "loss: 0.247427  [41600/481450]\n",
      "loss: 0.232533  [44800/481450]\n",
      "loss: 0.106905  [48000/481450]\n",
      "loss: 0.313032  [51200/481450]\n",
      "loss: 0.129184  [54400/481450]\n",
      "loss: 0.278587  [57600/481450]\n",
      "loss: 0.320810  [60800/481450]\n",
      "loss: 0.200003  [64000/481450]\n",
      "loss: 0.254002  [67200/481450]\n",
      "loss: 0.247070  [70400/481450]\n",
      "loss: 0.203489  [73600/481450]\n",
      "loss: 0.418885  [76800/481450]\n",
      "loss: 0.377932  [80000/481450]\n",
      "loss: 0.239055  [83200/481450]\n",
      "loss: 0.097362  [86400/481450]\n",
      "loss: 0.374863  [89600/481450]\n",
      "loss: 0.203513  [92800/481450]\n",
      "loss: 0.237921  [96000/481450]\n",
      "loss: 0.223438  [99200/481450]\n",
      "loss: 0.253854  [102400/481450]\n",
      "loss: 0.400292  [105600/481450]\n",
      "loss: 0.108621  [108800/481450]\n",
      "loss: 0.252372  [112000/481450]\n",
      "loss: 0.312405  [115200/481450]\n",
      "loss: 0.145576  [118400/481450]\n",
      "loss: 0.379632  [121600/481450]\n",
      "loss: 0.335814  [124800/481450]\n",
      "loss: 0.079499  [128000/481450]\n",
      "loss: 0.379630  [131200/481450]\n",
      "loss: 0.268490  [134400/481450]\n",
      "loss: 0.336169  [137600/481450]\n",
      "loss: 0.179529  [140800/481450]\n",
      "loss: 0.144727  [144000/481450]\n",
      "loss: 0.355158  [147200/481450]\n",
      "loss: 0.241725  [150400/481450]\n",
      "loss: 0.378603  [153600/481450]\n",
      "loss: 0.264538  [156800/481450]\n",
      "loss: 0.241612  [160000/481450]\n",
      "loss: 0.193326  [163200/481450]\n",
      "loss: 0.226138  [166400/481450]\n",
      "loss: 0.176262  [169600/481450]\n",
      "loss: 0.331831  [172800/481450]\n",
      "loss: 0.028066  [176000/481450]\n",
      "loss: 0.294804  [179200/481450]\n",
      "loss: 0.079224  [182400/481450]\n",
      "loss: 0.225520  [185600/481450]\n",
      "loss: 0.226789  [188800/481450]\n",
      "loss: 0.172361  [192000/481450]\n",
      "loss: 0.162827  [195200/481450]\n",
      "loss: 0.118317  [198400/481450]\n",
      "loss: 0.175034  [201600/481450]\n",
      "loss: 0.108832  [204800/481450]\n",
      "loss: 0.213343  [208000/481450]\n",
      "loss: 0.667275  [211200/481450]\n",
      "loss: 0.333770  [214400/481450]\n",
      "loss: 0.209064  [217600/481450]\n",
      "loss: 0.333601  [220800/481450]\n",
      "loss: 0.365968  [224000/481450]\n",
      "loss: 0.233999  [227200/481450]\n",
      "loss: 0.189734  [230400/481450]\n",
      "loss: 0.149627  [233600/481450]\n",
      "loss: 0.270602  [236800/481450]\n",
      "loss: 0.344694  [240000/481450]\n",
      "loss: 0.244893  [243200/481450]\n",
      "loss: 0.420560  [246400/481450]\n",
      "loss: 0.292429  [249600/481450]\n",
      "loss: 0.398601  [252800/481450]\n",
      "loss: 0.073938  [256000/481450]\n",
      "loss: 0.119958  [259200/481450]\n",
      "loss: 0.136313  [262400/481450]\n",
      "loss: 0.201794  [265600/481450]\n",
      "loss: 0.048481  [268800/481450]\n",
      "loss: 0.319077  [272000/481450]\n",
      "loss: 0.477450  [275200/481450]\n",
      "loss: 0.273812  [278400/481450]\n",
      "loss: 0.362672  [281600/481450]\n",
      "loss: 0.179172  [284800/481450]\n",
      "loss: 0.131670  [288000/481450]\n",
      "loss: 0.102533  [291200/481450]\n",
      "loss: 0.164790  [294400/481450]\n",
      "loss: 0.050877  [297600/481450]\n",
      "loss: 0.219575  [300800/481450]\n",
      "loss: 0.199554  [304000/481450]\n",
      "loss: 0.377056  [307200/481450]\n",
      "loss: 0.242266  [310400/481450]\n",
      "loss: 0.300106  [313600/481450]\n",
      "loss: 0.184962  [316800/481450]\n",
      "loss: 0.337241  [320000/481450]\n",
      "loss: 0.146261  [323200/481450]\n",
      "loss: 0.194463  [326400/481450]\n",
      "loss: 0.341974  [329600/481450]\n",
      "loss: 0.194761  [332800/481450]\n",
      "loss: 0.200004  [336000/481450]\n",
      "loss: 0.256745  [339200/481450]\n",
      "loss: 0.159177  [342400/481450]\n",
      "loss: 0.243273  [345600/481450]\n",
      "loss: 0.282835  [348800/481450]\n",
      "loss: 0.435141  [352000/481450]\n",
      "loss: 0.129339  [355200/481450]\n",
      "loss: 0.357236  [358400/481450]\n",
      "loss: 0.514968  [361600/481450]\n",
      "loss: 0.198964  [364800/481450]\n",
      "loss: 0.139382  [368000/481450]\n",
      "loss: 0.320687  [371200/481450]\n",
      "loss: 0.170493  [374400/481450]\n",
      "loss: 0.337931  [377600/481450]\n",
      "loss: 0.214170  [380800/481450]\n",
      "loss: 0.112206  [384000/481450]\n",
      "loss: 0.132951  [387200/481450]\n",
      "loss: 0.247962  [390400/481450]\n",
      "loss: 0.132792  [393600/481450]\n",
      "loss: 0.235393  [396800/481450]\n",
      "loss: 0.476104  [400000/481450]\n",
      "loss: 0.396408  [403200/481450]\n",
      "loss: 0.284688  [406400/481450]\n",
      "loss: 0.347543  [409600/481450]\n",
      "loss: 0.242780  [412800/481450]\n",
      "loss: 0.097419  [416000/481450]\n",
      "loss: 0.114968  [419200/481450]\n",
      "loss: 0.161505  [422400/481450]\n",
      "loss: 0.280244  [425600/481450]\n",
      "loss: 0.170407  [428800/481450]\n",
      "loss: 0.138897  [432000/481450]\n",
      "loss: 0.232563  [435200/481450]\n",
      "loss: 0.128222  [438400/481450]\n",
      "loss: 0.501169  [441600/481450]\n",
      "loss: 0.248743  [444800/481450]\n",
      "loss: 0.200161  [448000/481450]\n",
      "loss: 0.193820  [451200/481450]\n",
      "loss: 0.333330  [454400/481450]\n",
      "loss: 0.046604  [457600/481450]\n",
      "loss: 0.268688  [460800/481450]\n",
      "loss: 0.120398  [464000/481450]\n",
      "loss: 0.282372  [467200/481450]\n",
      "loss: 0.136000  [470400/481450]\n",
      "loss: 0.107257  [473600/481450]\n",
      "loss: 0.242309  [476800/481450]\n",
      "loss: 0.130671  [480000/481450]\n",
      "Train Accuracy: 90.4360%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.551584, F1-score: 87.62% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.245588  [    0/481450]\n",
      "loss: 0.327295  [ 3200/481450]\n",
      "loss: 0.289880  [ 6400/481450]\n",
      "loss: 0.284917  [ 9600/481450]\n",
      "loss: 0.269638  [12800/481450]\n",
      "loss: 0.229182  [16000/481450]\n",
      "loss: 0.226634  [19200/481450]\n",
      "loss: 0.308655  [22400/481450]\n",
      "loss: 0.201886  [25600/481450]\n",
      "loss: 0.114902  [28800/481450]\n",
      "loss: 0.351612  [32000/481450]\n",
      "loss: 0.185355  [35200/481450]\n",
      "loss: 0.581807  [38400/481450]\n",
      "loss: 0.188020  [41600/481450]\n",
      "loss: 0.158410  [44800/481450]\n",
      "loss: 0.120183  [48000/481450]\n",
      "loss: 0.235182  [51200/481450]\n",
      "loss: 0.338509  [54400/481450]\n",
      "loss: 0.404992  [57600/481450]\n",
      "loss: 0.196351  [60800/481450]\n",
      "loss: 0.328204  [64000/481450]\n",
      "loss: 0.447378  [67200/481450]\n",
      "loss: 0.230250  [70400/481450]\n",
      "loss: 0.239008  [73600/481450]\n",
      "loss: 0.110419  [76800/481450]\n",
      "loss: 0.204177  [80000/481450]\n",
      "loss: 0.139158  [83200/481450]\n",
      "loss: 0.231914  [86400/481450]\n",
      "loss: 0.101522  [89600/481450]\n",
      "loss: 0.181905  [92800/481450]\n",
      "loss: 0.197721  [96000/481450]\n",
      "loss: 0.308833  [99200/481450]\n",
      "loss: 0.137558  [102400/481450]\n",
      "loss: 0.480292  [105600/481450]\n",
      "loss: 0.230065  [108800/481450]\n",
      "loss: 0.163667  [112000/481450]\n",
      "loss: 0.182350  [115200/481450]\n",
      "loss: 0.207311  [118400/481450]\n",
      "loss: 0.138488  [121600/481450]\n",
      "loss: 0.297713  [124800/481450]\n",
      "loss: 0.237855  [128000/481450]\n",
      "loss: 0.401664  [131200/481450]\n",
      "loss: 0.201031  [134400/481450]\n",
      "loss: 0.083215  [137600/481450]\n",
      "loss: 0.238209  [140800/481450]\n",
      "loss: 0.259712  [144000/481450]\n",
      "loss: 0.465399  [147200/481450]\n",
      "loss: 0.213845  [150400/481450]\n",
      "loss: 0.656021  [153600/481450]\n",
      "loss: 0.232692  [156800/481450]\n",
      "loss: 0.201173  [160000/481450]\n",
      "loss: 0.168971  [163200/481450]\n",
      "loss: 0.188810  [166400/481450]\n",
      "loss: 0.312274  [169600/481450]\n",
      "loss: 0.185847  [172800/481450]\n",
      "loss: 0.231938  [176000/481450]\n",
      "loss: 0.350594  [179200/481450]\n",
      "loss: 0.251913  [182400/481450]\n",
      "loss: 0.315285  [185600/481450]\n",
      "loss: 0.364252  [188800/481450]\n",
      "loss: 0.349814  [192000/481450]\n",
      "loss: 0.385495  [195200/481450]\n",
      "loss: 0.367618  [198400/481450]\n",
      "loss: 0.168701  [201600/481450]\n",
      "loss: 0.257010  [204800/481450]\n",
      "loss: 0.237149  [208000/481450]\n",
      "loss: 0.399208  [211200/481450]\n",
      "loss: 0.209469  [214400/481450]\n",
      "loss: 0.158614  [217600/481450]\n",
      "loss: 0.383666  [220800/481450]\n",
      "loss: 0.086061  [224000/481450]\n",
      "loss: 0.287933  [227200/481450]\n",
      "loss: 0.125305  [230400/481450]\n",
      "loss: 0.387984  [233600/481450]\n",
      "loss: 0.216595  [236800/481450]\n",
      "loss: 0.267892  [240000/481450]\n",
      "loss: 0.135224  [243200/481450]\n",
      "loss: 0.078158  [246400/481450]\n",
      "loss: 0.283008  [249600/481450]\n",
      "loss: 0.079868  [252800/481450]\n",
      "loss: 0.285990  [256000/481450]\n",
      "loss: 0.286544  [259200/481450]\n",
      "loss: 0.170135  [262400/481450]\n",
      "loss: 0.303551  [265600/481450]\n",
      "loss: 0.145635  [268800/481450]\n",
      "loss: 0.143922  [272000/481450]\n",
      "loss: 0.139074  [275200/481450]\n",
      "loss: 0.245117  [278400/481450]\n",
      "loss: 0.239138  [281600/481450]\n",
      "loss: 0.026879  [284800/481450]\n",
      "loss: 0.187458  [288000/481450]\n",
      "loss: 0.310385  [291200/481450]\n",
      "loss: 0.187100  [294400/481450]\n",
      "loss: 0.216236  [297600/481450]\n",
      "loss: 0.199193  [300800/481450]\n",
      "loss: 0.128292  [304000/481450]\n",
      "loss: 0.147480  [307200/481450]\n",
      "loss: 0.178992  [310400/481450]\n",
      "loss: 0.397444  [313600/481450]\n",
      "loss: 0.160396  [316800/481450]\n",
      "loss: 0.129023  [320000/481450]\n",
      "loss: 0.269042  [323200/481450]\n",
      "loss: 0.201341  [326400/481450]\n",
      "loss: 0.160679  [329600/481450]\n",
      "loss: 0.207642  [332800/481450]\n",
      "loss: 0.215778  [336000/481450]\n",
      "loss: 0.187289  [339200/481450]\n",
      "loss: 0.236147  [342400/481450]\n",
      "loss: 0.418510  [345600/481450]\n",
      "loss: 0.077726  [348800/481450]\n",
      "loss: 0.083871  [352000/481450]\n",
      "loss: 0.222284  [355200/481450]\n",
      "loss: 0.125429  [358400/481450]\n",
      "loss: 0.276978  [361600/481450]\n",
      "loss: 0.030972  [364800/481450]\n",
      "loss: 0.404467  [368000/481450]\n",
      "loss: 0.100471  [371200/481450]\n",
      "loss: 0.153267  [374400/481450]\n",
      "loss: 0.509557  [377600/481450]\n",
      "loss: 0.274845  [380800/481450]\n",
      "loss: 0.065279  [384000/481450]\n",
      "loss: 0.123560  [387200/481450]\n",
      "loss: 0.454803  [390400/481450]\n",
      "loss: 0.408510  [393600/481450]\n",
      "loss: 0.333842  [396800/481450]\n",
      "loss: 0.237099  [400000/481450]\n",
      "loss: 0.269971  [403200/481450]\n",
      "loss: 0.194162  [406400/481450]\n",
      "loss: 0.164165  [409600/481450]\n",
      "loss: 0.354553  [412800/481450]\n",
      "loss: 0.211737  [416000/481450]\n",
      "loss: 0.360825  [419200/481450]\n",
      "loss: 0.097629  [422400/481450]\n",
      "loss: 0.229586  [425600/481450]\n",
      "loss: 0.200249  [428800/481450]\n",
      "loss: 0.273434  [432000/481450]\n",
      "loss: 0.377199  [435200/481450]\n",
      "loss: 0.169521  [438400/481450]\n",
      "loss: 0.183292  [441600/481450]\n",
      "loss: 0.221317  [444800/481450]\n",
      "loss: 0.125294  [448000/481450]\n",
      "loss: 0.330585  [451200/481450]\n",
      "loss: 0.074831  [454400/481450]\n",
      "loss: 0.131298  [457600/481450]\n",
      "loss: 0.188934  [460800/481450]\n",
      "loss: 0.124021  [464000/481450]\n",
      "loss: 0.177085  [467200/481450]\n",
      "loss: 0.180353  [470400/481450]\n",
      "loss: 0.511938  [473600/481450]\n",
      "loss: 0.132885  [476800/481450]\n",
      "loss: 0.167889  [480000/481450]\n",
      "Train Accuracy: 90.6393%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.614149, F1-score: 87.38% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.260609  [    0/481450]\n",
      "loss: 0.221552  [ 3200/481450]\n",
      "loss: 0.120837  [ 6400/481450]\n",
      "loss: 0.099391  [ 9600/481450]\n",
      "loss: 0.389780  [12800/481450]\n",
      "loss: 0.129264  [16000/481450]\n",
      "loss: 0.198680  [19200/481450]\n",
      "loss: 0.155483  [22400/481450]\n",
      "loss: 0.433370  [25600/481450]\n",
      "loss: 0.087714  [28800/481450]\n",
      "loss: 0.534451  [32000/481450]\n",
      "loss: 0.231920  [35200/481450]\n",
      "loss: 0.287293  [38400/481450]\n",
      "loss: 0.185998  [41600/481450]\n",
      "loss: 0.091606  [44800/481450]\n",
      "loss: 0.243607  [48000/481450]\n",
      "loss: 0.234371  [51200/481450]\n",
      "loss: 0.142300  [54400/481450]\n",
      "loss: 0.215546  [57600/481450]\n",
      "loss: 0.169531  [60800/481450]\n",
      "loss: 0.243455  [64000/481450]\n",
      "loss: 0.238149  [67200/481450]\n",
      "loss: 0.167015  [70400/481450]\n",
      "loss: 0.243356  [73600/481450]\n",
      "loss: 0.179799  [76800/481450]\n",
      "loss: 0.495126  [80000/481450]\n",
      "loss: 0.179699  [83200/481450]\n",
      "loss: 0.255597  [86400/481450]\n",
      "loss: 0.191950  [89600/481450]\n",
      "loss: 0.495985  [92800/481450]\n",
      "loss: 0.330503  [96000/481450]\n",
      "loss: 0.191794  [99200/481450]\n",
      "loss: 0.157222  [102400/481450]\n",
      "loss: 0.227640  [105600/481450]\n",
      "loss: 0.128888  [108800/481450]\n",
      "loss: 0.131015  [112000/481450]\n",
      "loss: 0.253659  [115200/481450]\n",
      "loss: 0.102315  [118400/481450]\n",
      "loss: 0.158487  [121600/481450]\n",
      "loss: 0.103053  [124800/481450]\n",
      "loss: 0.171940  [128000/481450]\n",
      "loss: 0.239602  [131200/481450]\n",
      "loss: 0.399250  [134400/481450]\n",
      "loss: 0.170699  [137600/481450]\n",
      "loss: 0.198728  [140800/481450]\n",
      "loss: 0.225017  [144000/481450]\n",
      "loss: 0.219681  [147200/481450]\n",
      "loss: 0.187094  [150400/481450]\n",
      "loss: 0.394734  [153600/481450]\n",
      "loss: 0.379810  [156800/481450]\n",
      "loss: 0.290895  [160000/481450]\n",
      "loss: 0.320606  [163200/481450]\n",
      "loss: 0.217177  [166400/481450]\n",
      "loss: 0.163876  [169600/481450]\n",
      "loss: 0.258280  [172800/481450]\n",
      "loss: 0.121355  [176000/481450]\n",
      "loss: 0.092592  [179200/481450]\n",
      "loss: 0.287203  [182400/481450]\n",
      "loss: 0.159357  [185600/481450]\n",
      "loss: 0.171502  [188800/481450]\n",
      "loss: 0.312144  [192000/481450]\n",
      "loss: 0.299492  [195200/481450]\n",
      "loss: 0.165973  [198400/481450]\n",
      "loss: 0.100606  [201600/481450]\n",
      "loss: 0.176117  [204800/481450]\n",
      "loss: 0.225705  [208000/481450]\n",
      "loss: 0.255144  [211200/481450]\n",
      "loss: 0.211173  [214400/481450]\n",
      "loss: 0.221534  [217600/481450]\n",
      "loss: 0.179632  [220800/481450]\n",
      "loss: 0.228525  [224000/481450]\n",
      "loss: 0.261412  [227200/481450]\n",
      "loss: 0.073082  [230400/481450]\n",
      "loss: 0.418232  [233600/481450]\n",
      "loss: 0.227359  [236800/481450]\n",
      "loss: 0.171944  [240000/481450]\n",
      "loss: 0.255247  [243200/481450]\n",
      "loss: 0.570700  [246400/481450]\n",
      "loss: 0.420639  [249600/481450]\n",
      "loss: 0.164206  [252800/481450]\n",
      "loss: 0.234805  [256000/481450]\n",
      "loss: 0.130698  [259200/481450]\n",
      "loss: 0.177415  [262400/481450]\n",
      "loss: 0.279410  [265600/481450]\n",
      "loss: 0.257629  [268800/481450]\n",
      "loss: 0.179839  [272000/481450]\n",
      "loss: 0.085578  [275200/481450]\n",
      "loss: 0.368416  [278400/481450]\n",
      "loss: 0.176747  [281600/481450]\n",
      "loss: 0.171625  [284800/481450]\n",
      "loss: 0.315026  [288000/481450]\n",
      "loss: 0.300333  [291200/481450]\n",
      "loss: 0.524067  [294400/481450]\n",
      "loss: 0.600917  [297600/481450]\n",
      "loss: 0.094568  [300800/481450]\n",
      "loss: 0.125529  [304000/481450]\n",
      "loss: 0.267729  [307200/481450]\n",
      "loss: 0.082214  [310400/481450]\n",
      "loss: 0.171095  [313600/481450]\n",
      "loss: 0.082579  [316800/481450]\n",
      "loss: 0.134558  [320000/481450]\n",
      "loss: 0.171271  [323200/481450]\n",
      "loss: 0.499653  [326400/481450]\n",
      "loss: 0.186957  [329600/481450]\n",
      "loss: 0.126405  [332800/481450]\n",
      "loss: 0.223820  [336000/481450]\n",
      "loss: 0.202338  [339200/481450]\n",
      "loss: 0.395595  [342400/481450]\n",
      "loss: 0.281583  [345600/481450]\n",
      "loss: 0.303005  [348800/481450]\n",
      "loss: 0.119716  [352000/481450]\n",
      "loss: 0.085201  [355200/481450]\n",
      "loss: 0.250463  [358400/481450]\n",
      "loss: 0.356939  [361600/481450]\n",
      "loss: 0.159677  [364800/481450]\n",
      "loss: 0.373644  [368000/481450]\n",
      "loss: 0.291393  [371200/481450]\n",
      "loss: 0.151721  [374400/481450]\n",
      "loss: 0.086303  [377600/481450]\n",
      "loss: 0.248691  [380800/481450]\n",
      "loss: 0.264958  [384000/481450]\n",
      "loss: 0.226190  [387200/481450]\n",
      "loss: 0.264764  [390400/481450]\n",
      "loss: 0.098846  [393600/481450]\n",
      "loss: 0.172962  [396800/481450]\n",
      "loss: 0.185785  [400000/481450]\n",
      "loss: 0.366184  [403200/481450]\n",
      "loss: 0.114989  [406400/481450]\n",
      "loss: 0.259280  [409600/481450]\n",
      "loss: 0.166932  [412800/481450]\n",
      "loss: 0.133206  [416000/481450]\n",
      "loss: 0.186656  [419200/481450]\n",
      "loss: 0.148368  [422400/481450]\n",
      "loss: 0.159689  [425600/481450]\n",
      "loss: 0.166025  [428800/481450]\n",
      "loss: 0.218800  [432000/481450]\n",
      "loss: 0.247328  [435200/481450]\n",
      "loss: 0.176031  [438400/481450]\n",
      "loss: 0.268161  [441600/481450]\n",
      "loss: 0.353304  [444800/481450]\n",
      "loss: 0.215095  [448000/481450]\n",
      "loss: 0.331181  [451200/481450]\n",
      "loss: 0.211136  [454400/481450]\n",
      "loss: 0.293856  [457600/481450]\n",
      "loss: 0.125810  [460800/481450]\n",
      "loss: 0.113946  [464000/481450]\n",
      "loss: 0.070586  [467200/481450]\n",
      "loss: 0.162544  [470400/481450]\n",
      "loss: 0.126224  [473600/481450]\n",
      "loss: 0.264267  [476800/481450]\n",
      "loss: 0.317864  [480000/481450]\n",
      "Train Accuracy: 90.8067%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.575540, F1-score: 87.64% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.226288  [    0/481450]\n",
      "loss: 0.113808  [ 3200/481450]\n",
      "loss: 0.166338  [ 6400/481450]\n",
      "loss: 0.385344  [ 9600/481450]\n",
      "loss: 0.403088  [12800/481450]\n",
      "loss: 0.214286  [16000/481450]\n",
      "loss: 0.024687  [19200/481450]\n",
      "loss: 0.136596  [22400/481450]\n",
      "loss: 0.035125  [25600/481450]\n",
      "loss: 0.156097  [28800/481450]\n",
      "loss: 0.154795  [32000/481450]\n",
      "loss: 0.213253  [35200/481450]\n",
      "loss: 0.278605  [38400/481450]\n",
      "loss: 0.282860  [41600/481450]\n",
      "loss: 0.103105  [44800/481450]\n",
      "loss: 0.162486  [48000/481450]\n",
      "loss: 0.202502  [51200/481450]\n",
      "loss: 0.232077  [54400/481450]\n",
      "loss: 0.170727  [57600/481450]\n",
      "loss: 0.126441  [60800/481450]\n",
      "loss: 0.095000  [64000/481450]\n",
      "loss: 0.138377  [67200/481450]\n",
      "loss: 0.370073  [70400/481450]\n",
      "loss: 0.115866  [73600/481450]\n",
      "loss: 0.302511  [76800/481450]\n",
      "loss: 0.244377  [80000/481450]\n",
      "loss: 0.041020  [83200/481450]\n",
      "loss: 0.181969  [86400/481450]\n",
      "loss: 0.475134  [89600/481450]\n",
      "loss: 0.247410  [92800/481450]\n",
      "loss: 0.220094  [96000/481450]\n",
      "loss: 0.115227  [99200/481450]\n",
      "loss: 0.252002  [102400/481450]\n",
      "loss: 0.230991  [105600/481450]\n",
      "loss: 0.257687  [108800/481450]\n",
      "loss: 0.193033  [112000/481450]\n",
      "loss: 0.149338  [115200/481450]\n",
      "loss: 0.103574  [118400/481450]\n",
      "loss: 0.339017  [121600/481450]\n",
      "loss: 0.239401  [124800/481450]\n",
      "loss: 0.192783  [128000/481450]\n",
      "loss: 0.297844  [131200/481450]\n",
      "loss: 0.357746  [134400/481450]\n",
      "loss: 0.084912  [137600/481450]\n",
      "loss: 0.166874  [140800/481450]\n",
      "loss: 0.272015  [144000/481450]\n",
      "loss: 0.282834  [147200/481450]\n",
      "loss: 0.167340  [150400/481450]\n",
      "loss: 0.421295  [153600/481450]\n",
      "loss: 0.222127  [156800/481450]\n",
      "loss: 0.226748  [160000/481450]\n",
      "loss: 0.175694  [163200/481450]\n",
      "loss: 0.170802  [166400/481450]\n",
      "loss: 0.272258  [169600/481450]\n",
      "loss: 0.396016  [172800/481450]\n",
      "loss: 0.078339  [176000/481450]\n",
      "loss: 0.422716  [179200/481450]\n",
      "loss: 0.266503  [182400/481450]\n",
      "loss: 0.363478  [185600/481450]\n",
      "loss: 0.189349  [188800/481450]\n",
      "loss: 0.398906  [192000/481450]\n",
      "loss: 0.098168  [195200/481450]\n",
      "loss: 0.101043  [198400/481450]\n",
      "loss: 0.245554  [201600/481450]\n",
      "loss: 0.167178  [204800/481450]\n",
      "loss: 0.103014  [208000/481450]\n",
      "loss: 0.215648  [211200/481450]\n",
      "loss: 0.400582  [214400/481450]\n",
      "loss: 0.242830  [217600/481450]\n",
      "loss: 0.278644  [220800/481450]\n",
      "loss: 0.234626  [224000/481450]\n",
      "loss: 0.160336  [227200/481450]\n",
      "loss: 0.183248  [230400/481450]\n",
      "loss: 0.141843  [233600/481450]\n",
      "loss: 0.192512  [236800/481450]\n",
      "loss: 0.128434  [240000/481450]\n",
      "loss: 0.200871  [243200/481450]\n",
      "loss: 0.137773  [246400/481450]\n",
      "loss: 0.368776  [249600/481450]\n",
      "loss: 0.297995  [252800/481450]\n",
      "loss: 0.336684  [256000/481450]\n",
      "loss: 0.126661  [259200/481450]\n",
      "loss: 0.256382  [262400/481450]\n",
      "loss: 0.117977  [265600/481450]\n",
      "loss: 0.144587  [268800/481450]\n",
      "loss: 0.161159  [272000/481450]\n",
      "loss: 0.159511  [275200/481450]\n",
      "loss: 0.141982  [278400/481450]\n",
      "loss: 0.210719  [281600/481450]\n",
      "loss: 0.445980  [284800/481450]\n",
      "loss: 0.674570  [288000/481450]\n",
      "loss: 0.139475  [291200/481450]\n",
      "loss: 0.571691  [294400/481450]\n",
      "loss: 0.100747  [297600/481450]\n",
      "loss: 0.102015  [300800/481450]\n",
      "loss: 0.161805  [304000/481450]\n",
      "loss: 0.280738  [307200/481450]\n",
      "loss: 0.500113  [310400/481450]\n",
      "loss: 0.381278  [313600/481450]\n",
      "loss: 0.241540  [316800/481450]\n",
      "loss: 0.276327  [320000/481450]\n",
      "loss: 0.146980  [323200/481450]\n",
      "loss: 0.077403  [326400/481450]\n",
      "loss: 0.209210  [329600/481450]\n",
      "loss: 0.201147  [332800/481450]\n",
      "loss: 0.370601  [336000/481450]\n",
      "loss: 0.177734  [339200/481450]\n",
      "loss: 0.195194  [342400/481450]\n",
      "loss: 0.354539  [345600/481450]\n",
      "loss: 0.284766  [348800/481450]\n",
      "loss: 0.143188  [352000/481450]\n",
      "loss: 0.246349  [355200/481450]\n",
      "loss: 0.460844  [358400/481450]\n",
      "loss: 0.346226  [361600/481450]\n",
      "loss: 0.606258  [364800/481450]\n",
      "loss: 0.011541  [368000/481450]\n",
      "loss: 0.250479  [371200/481450]\n",
      "loss: 0.278783  [374400/481450]\n",
      "loss: 0.414711  [377600/481450]\n",
      "loss: 0.361730  [380800/481450]\n",
      "loss: 0.199758  [384000/481450]\n",
      "loss: 0.373177  [387200/481450]\n",
      "loss: 0.212370  [390400/481450]\n",
      "loss: 0.230289  [393600/481450]\n",
      "loss: 0.248130  [396800/481450]\n",
      "loss: 0.096506  [400000/481450]\n",
      "loss: 0.115553  [403200/481450]\n",
      "loss: 0.161360  [406400/481450]\n",
      "loss: 0.314800  [409600/481450]\n",
      "loss: 0.085525  [412800/481450]\n",
      "loss: 0.214984  [416000/481450]\n",
      "loss: 0.352295  [419200/481450]\n",
      "loss: 0.275417  [422400/481450]\n",
      "loss: 0.153237  [425600/481450]\n",
      "loss: 0.343582  [428800/481450]\n",
      "loss: 0.207747  [432000/481450]\n",
      "loss: 0.267053  [435200/481450]\n",
      "loss: 0.303332  [438400/481450]\n",
      "loss: 0.266291  [441600/481450]\n",
      "loss: 0.124982  [444800/481450]\n",
      "loss: 0.201454  [448000/481450]\n",
      "loss: 0.394665  [451200/481450]\n",
      "loss: 0.282602  [454400/481450]\n",
      "loss: 0.181405  [457600/481450]\n",
      "loss: 0.238125  [460800/481450]\n",
      "loss: 0.337486  [464000/481450]\n",
      "loss: 0.075931  [467200/481450]\n",
      "loss: 0.109316  [470400/481450]\n",
      "loss: 0.307816  [473600/481450]\n",
      "loss: 0.404669  [476800/481450]\n",
      "loss: 0.056322  [480000/481450]\n",
      "Train Accuracy: 90.9989%\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.570937, F1-score: 87.81% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.248403  [    0/481450]\n",
      "loss: 0.093107  [ 3200/481450]\n",
      "loss: 0.165574  [ 6400/481450]\n",
      "loss: 0.241910  [ 9600/481450]\n",
      "loss: 0.304427  [12800/481450]\n",
      "loss: 0.167970  [16000/481450]\n",
      "loss: 0.396180  [19200/481450]\n",
      "loss: 0.309053  [22400/481450]\n",
      "loss: 0.217900  [25600/481450]\n",
      "loss: 0.258284  [28800/481450]\n",
      "loss: 0.254540  [32000/481450]\n",
      "loss: 0.070104  [35200/481450]\n",
      "loss: 0.232924  [38400/481450]\n",
      "loss: 0.168844  [41600/481450]\n",
      "loss: 0.297101  [44800/481450]\n",
      "loss: 0.076624  [48000/481450]\n",
      "loss: 0.082470  [51200/481450]\n",
      "loss: 0.169431  [54400/481450]\n",
      "loss: 0.097821  [57600/481450]\n",
      "loss: 0.123894  [60800/481450]\n",
      "loss: 0.193655  [64000/481450]\n",
      "loss: 0.126334  [67200/481450]\n",
      "loss: 0.109465  [70400/481450]\n",
      "loss: 0.319877  [73600/481450]\n",
      "loss: 0.175305  [76800/481450]\n",
      "loss: 0.199511  [80000/481450]\n",
      "loss: 0.222197  [83200/481450]\n",
      "loss: 0.171271  [86400/481450]\n",
      "loss: 0.186278  [89600/481450]\n",
      "loss: 0.188766  [92800/481450]\n",
      "loss: 0.253780  [96000/481450]\n",
      "loss: 0.123499  [99200/481450]\n",
      "loss: 0.209448  [102400/481450]\n",
      "loss: 0.213344  [105600/481450]\n",
      "loss: 0.249636  [108800/481450]\n",
      "loss: 0.224511  [112000/481450]\n",
      "loss: 0.228821  [115200/481450]\n",
      "loss: 0.270065  [118400/481450]\n",
      "loss: 0.341796  [121600/481450]\n",
      "loss: 0.377398  [124800/481450]\n",
      "loss: 0.206114  [128000/481450]\n",
      "loss: 0.433491  [131200/481450]\n",
      "loss: 0.194468  [134400/481450]\n",
      "loss: 0.149563  [137600/481450]\n",
      "loss: 0.166804  [140800/481450]\n",
      "loss: 0.335419  [144000/481450]\n",
      "loss: 0.198792  [147200/481450]\n",
      "loss: 0.200812  [150400/481450]\n",
      "loss: 0.103392  [153600/481450]\n",
      "loss: 0.102871  [156800/481450]\n",
      "loss: 0.153366  [160000/481450]\n",
      "loss: 0.139104  [163200/481450]\n",
      "loss: 0.351437  [166400/481450]\n",
      "loss: 0.114824  [169600/481450]\n",
      "loss: 0.094608  [172800/481450]\n",
      "loss: 0.251673  [176000/481450]\n",
      "loss: 0.233741  [179200/481450]\n",
      "loss: 0.305458  [182400/481450]\n",
      "loss: 0.293596  [185600/481450]\n",
      "loss: 0.187413  [188800/481450]\n",
      "loss: 0.215542  [192000/481450]\n",
      "loss: 0.119244  [195200/481450]\n",
      "loss: 0.329289  [198400/481450]\n",
      "loss: 0.079044  [201600/481450]\n",
      "loss: 0.266145  [204800/481450]\n",
      "loss: 0.218870  [208000/481450]\n",
      "loss: 0.102529  [211200/481450]\n",
      "loss: 0.057768  [214400/481450]\n",
      "loss: 0.329604  [217600/481450]\n",
      "loss: 0.195510  [220800/481450]\n",
      "loss: 0.254487  [224000/481450]\n",
      "loss: 0.214722  [227200/481450]\n",
      "loss: 0.070112  [230400/481450]\n",
      "loss: 0.165043  [233600/481450]\n",
      "loss: 0.213616  [236800/481450]\n",
      "loss: 0.116309  [240000/481450]\n",
      "loss: 0.252291  [243200/481450]\n",
      "loss: 0.086689  [246400/481450]\n",
      "loss: 0.160341  [249600/481450]\n",
      "loss: 0.172142  [252800/481450]\n",
      "loss: 0.192604  [256000/481450]\n",
      "loss: 0.395374  [259200/481450]\n",
      "loss: 0.155911  [262400/481450]\n",
      "loss: 0.210186  [265600/481450]\n",
      "loss: 0.180701  [268800/481450]\n",
      "loss: 0.223208  [272000/481450]\n",
      "loss: 0.136259  [275200/481450]\n",
      "loss: 0.088919  [278400/481450]\n",
      "loss: 0.238866  [281600/481450]\n",
      "loss: 0.066076  [284800/481450]\n",
      "loss: 0.204272  [288000/481450]\n",
      "loss: 0.169297  [291200/481450]\n",
      "loss: 0.256367  [294400/481450]\n",
      "loss: 0.378403  [297600/481450]\n",
      "loss: 0.367499  [300800/481450]\n",
      "loss: 0.259594  [304000/481450]\n",
      "loss: 0.191764  [307200/481450]\n",
      "loss: 0.404111  [310400/481450]\n",
      "loss: 0.104050  [313600/481450]\n",
      "loss: 0.281172  [316800/481450]\n",
      "loss: 0.152799  [320000/481450]\n",
      "loss: 0.322930  [323200/481450]\n",
      "loss: 0.343277  [326400/481450]\n",
      "loss: 0.399678  [329600/481450]\n",
      "loss: 0.262731  [332800/481450]\n",
      "loss: 0.163354  [336000/481450]\n",
      "loss: 0.200779  [339200/481450]\n",
      "loss: 0.138507  [342400/481450]\n",
      "loss: 0.191426  [345600/481450]\n",
      "loss: 0.053843  [348800/481450]\n",
      "loss: 0.208356  [352000/481450]\n",
      "loss: 0.530637  [355200/481450]\n",
      "loss: 0.088217  [358400/481450]\n",
      "loss: 0.274951  [361600/481450]\n",
      "loss: 0.236433  [364800/481450]\n",
      "loss: 0.184660  [368000/481450]\n",
      "loss: 0.347002  [371200/481450]\n",
      "loss: 0.128194  [374400/481450]\n",
      "loss: 0.056049  [377600/481450]\n",
      "loss: 0.192183  [380800/481450]\n",
      "loss: 0.102749  [384000/481450]\n",
      "loss: 0.158340  [387200/481450]\n",
      "loss: 0.629777  [390400/481450]\n",
      "loss: 0.372435  [393600/481450]\n",
      "loss: 0.255706  [396800/481450]\n",
      "loss: 0.118533  [400000/481450]\n",
      "loss: 0.169526  [403200/481450]\n",
      "loss: 0.212985  [406400/481450]\n",
      "loss: 0.274736  [409600/481450]\n",
      "loss: 0.196660  [412800/481450]\n",
      "loss: 0.290235  [416000/481450]\n",
      "loss: 0.094413  [419200/481450]\n",
      "loss: 0.088119  [422400/481450]\n",
      "loss: 0.414903  [425600/481450]\n",
      "loss: 0.276895  [428800/481450]\n",
      "loss: 0.418991  [432000/481450]\n",
      "loss: 0.159710  [435200/481450]\n",
      "loss: 0.098446  [438400/481450]\n",
      "loss: 0.232078  [441600/481450]\n",
      "loss: 0.232401  [444800/481450]\n",
      "loss: 0.267436  [448000/481450]\n",
      "loss: 0.083771  [451200/481450]\n",
      "loss: 0.164081  [454400/481450]\n",
      "loss: 0.161738  [457600/481450]\n",
      "loss: 0.369505  [460800/481450]\n",
      "loss: 0.055162  [464000/481450]\n",
      "loss: 0.546317  [467200/481450]\n",
      "loss: 0.390229  [470400/481450]\n",
      "loss: 0.310589  [473600/481450]\n",
      "loss: 0.122921  [476800/481450]\n",
      "loss: 0.119871  [480000/481450]\n",
      "Train Accuracy: 91.1096%\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.599008, F1-score: 87.10% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.288887  [    0/481450]\n",
      "loss: 0.212876  [ 3200/481450]\n",
      "loss: 0.112996  [ 6400/481450]\n",
      "loss: 0.132295  [ 9600/481450]\n",
      "loss: 0.339119  [12800/481450]\n",
      "loss: 0.203048  [16000/481450]\n",
      "loss: 0.373742  [19200/481450]\n",
      "loss: 0.133693  [22400/481450]\n",
      "loss: 0.099137  [25600/481450]\n",
      "loss: 0.235881  [28800/481450]\n",
      "loss: 0.163123  [32000/481450]\n",
      "loss: 0.095579  [35200/481450]\n",
      "loss: 0.142459  [38400/481450]\n",
      "loss: 0.240348  [41600/481450]\n",
      "loss: 0.227900  [44800/481450]\n",
      "loss: 0.192226  [48000/481450]\n",
      "loss: 0.050986  [51200/481450]\n",
      "loss: 0.399874  [54400/481450]\n",
      "loss: 0.111409  [57600/481450]\n",
      "loss: 0.390055  [60800/481450]\n",
      "loss: 0.155101  [64000/481450]\n",
      "loss: 0.083494  [67200/481450]\n",
      "loss: 0.338101  [70400/481450]\n",
      "loss: 0.251650  [73600/481450]\n",
      "loss: 0.153422  [76800/481450]\n",
      "loss: 0.222603  [80000/481450]\n",
      "loss: 0.284610  [83200/481450]\n",
      "loss: 0.152985  [86400/481450]\n",
      "loss: 0.258936  [89600/481450]\n",
      "loss: 0.248434  [92800/481450]\n",
      "loss: 0.084150  [96000/481450]\n",
      "loss: 0.166419  [99200/481450]\n",
      "loss: 0.188487  [102400/481450]\n",
      "loss: 0.134890  [105600/481450]\n",
      "loss: 0.370615  [108800/481450]\n",
      "loss: 0.482718  [112000/481450]\n",
      "loss: 0.184896  [115200/481450]\n",
      "loss: 0.133193  [118400/481450]\n",
      "loss: 0.221426  [121600/481450]\n",
      "loss: 0.164863  [124800/481450]\n",
      "loss: 0.084758  [128000/481450]\n",
      "loss: 0.283890  [131200/481450]\n",
      "loss: 0.259347  [134400/481450]\n",
      "loss: 0.191400  [137600/481450]\n",
      "loss: 0.169427  [140800/481450]\n",
      "loss: 0.062065  [144000/481450]\n",
      "loss: 0.468188  [147200/481450]\n",
      "loss: 0.120215  [150400/481450]\n",
      "loss: 0.052930  [153600/481450]\n",
      "loss: 0.231723  [156800/481450]\n",
      "loss: 0.198533  [160000/481450]\n",
      "loss: 0.184477  [163200/481450]\n",
      "loss: 0.222421  [166400/481450]\n",
      "loss: 0.175296  [169600/481450]\n",
      "loss: 0.162246  [172800/481450]\n",
      "loss: 0.144813  [176000/481450]\n",
      "loss: 0.554164  [179200/481450]\n",
      "loss: 0.283071  [182400/481450]\n",
      "loss: 0.210659  [185600/481450]\n",
      "loss: 0.334353  [188800/481450]\n",
      "loss: 0.102199  [192000/481450]\n",
      "loss: 0.163151  [195200/481450]\n",
      "loss: 0.265092  [198400/481450]\n",
      "loss: 0.272501  [201600/481450]\n",
      "loss: 0.221805  [204800/481450]\n",
      "loss: 0.112887  [208000/481450]\n",
      "loss: 0.226432  [211200/481450]\n",
      "loss: 0.113110  [214400/481450]\n",
      "loss: 0.269383  [217600/481450]\n",
      "loss: 0.311553  [220800/481450]\n",
      "loss: 0.303955  [224000/481450]\n",
      "loss: 0.333363  [227200/481450]\n",
      "loss: 0.090534  [230400/481450]\n",
      "loss: 0.144811  [233600/481450]\n",
      "loss: 0.431817  [236800/481450]\n",
      "loss: 0.236854  [240000/481450]\n",
      "loss: 0.562418  [243200/481450]\n",
      "loss: 0.117955  [246400/481450]\n",
      "loss: 0.118980  [249600/481450]\n",
      "loss: 0.153586  [252800/481450]\n",
      "loss: 0.288655  [256000/481450]\n",
      "loss: 0.342439  [259200/481450]\n",
      "loss: 0.244328  [262400/481450]\n",
      "loss: 0.299915  [265600/481450]\n",
      "loss: 0.230142  [268800/481450]\n",
      "loss: 0.142090  [272000/481450]\n",
      "loss: 0.273476  [275200/481450]\n",
      "loss: 0.190431  [278400/481450]\n",
      "loss: 0.327473  [281600/481450]\n",
      "loss: 0.082332  [284800/481450]\n",
      "loss: 0.280543  [288000/481450]\n",
      "loss: 0.219414  [291200/481450]\n",
      "loss: 0.231961  [294400/481450]\n",
      "loss: 0.056334  [297600/481450]\n",
      "loss: 0.229317  [300800/481450]\n",
      "loss: 0.196548  [304000/481450]\n",
      "loss: 0.162613  [307200/481450]\n",
      "loss: 0.433725  [310400/481450]\n",
      "loss: 0.077288  [313600/481450]\n",
      "loss: 0.202727  [316800/481450]\n",
      "loss: 0.189266  [320000/481450]\n",
      "loss: 0.263356  [323200/481450]\n",
      "loss: 0.181593  [326400/481450]\n",
      "loss: 0.243560  [329600/481450]\n",
      "loss: 0.232724  [332800/481450]\n",
      "loss: 0.283279  [336000/481450]\n",
      "loss: 0.181581  [339200/481450]\n",
      "loss: 0.236217  [342400/481450]\n",
      "loss: 0.059533  [345600/481450]\n",
      "loss: 0.047280  [348800/481450]\n",
      "loss: 0.112525  [352000/481450]\n",
      "loss: 0.244327  [355200/481450]\n",
      "loss: 0.189013  [358400/481450]\n",
      "loss: 0.098996  [361600/481450]\n",
      "loss: 0.108175  [364800/481450]\n",
      "loss: 0.660521  [368000/481450]\n",
      "loss: 0.120320  [371200/481450]\n",
      "loss: 0.222968  [374400/481450]\n",
      "loss: 0.212797  [377600/481450]\n",
      "loss: 0.176672  [380800/481450]\n",
      "loss: 0.107373  [384000/481450]\n",
      "loss: 0.496203  [387200/481450]\n",
      "loss: 0.192299  [390400/481450]\n",
      "loss: 0.179756  [393600/481450]\n",
      "loss: 0.263069  [396800/481450]\n",
      "loss: 0.422092  [400000/481450]\n",
      "loss: 0.181031  [403200/481450]\n",
      "loss: 0.160251  [406400/481450]\n",
      "loss: 0.051026  [409600/481450]\n",
      "loss: 0.205304  [412800/481450]\n",
      "loss: 0.276363  [416000/481450]\n",
      "loss: 0.134493  [419200/481450]\n",
      "loss: 0.308828  [422400/481450]\n",
      "loss: 0.215369  [425600/481450]\n",
      "loss: 0.201724  [428800/481450]\n",
      "loss: 0.101934  [432000/481450]\n",
      "loss: 0.175745  [435200/481450]\n",
      "loss: 0.168015  [438400/481450]\n",
      "loss: 0.151592  [441600/481450]\n",
      "loss: 0.210872  [444800/481450]\n",
      "loss: 0.130767  [448000/481450]\n",
      "loss: 0.236819  [451200/481450]\n",
      "loss: 0.446942  [454400/481450]\n",
      "loss: 0.137213  [457600/481450]\n",
      "loss: 0.297997  [460800/481450]\n",
      "loss: 0.470000  [464000/481450]\n",
      "loss: 0.141743  [467200/481450]\n",
      "loss: 0.263902  [470400/481450]\n",
      "loss: 0.257401  [473600/481450]\n",
      "loss: 0.216132  [476800/481450]\n",
      "loss: 0.210583  [480000/481450]\n",
      "Train Accuracy: 91.2346%\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.561929, F1-score: 87.64% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.272069  [    0/481450]\n",
      "loss: 0.134475  [ 3200/481450]\n",
      "loss: 0.187908  [ 6400/481450]\n",
      "loss: 0.168490  [ 9600/481450]\n",
      "loss: 0.200842  [12800/481450]\n",
      "loss: 0.363314  [16000/481450]\n",
      "loss: 0.026997  [19200/481450]\n",
      "loss: 0.161962  [22400/481450]\n",
      "loss: 0.088063  [25600/481450]\n",
      "loss: 0.149417  [28800/481450]\n",
      "loss: 0.233182  [32000/481450]\n",
      "loss: 0.119272  [35200/481450]\n",
      "loss: 0.262730  [38400/481450]\n",
      "loss: 0.130136  [41600/481450]\n",
      "loss: 0.234282  [44800/481450]\n",
      "loss: 0.178482  [48000/481450]\n",
      "loss: 0.189869  [51200/481450]\n",
      "loss: 0.251356  [54400/481450]\n",
      "loss: 0.175586  [57600/481450]\n",
      "loss: 0.251712  [60800/481450]\n",
      "loss: 0.200714  [64000/481450]\n",
      "loss: 0.159582  [67200/481450]\n",
      "loss: 0.158779  [70400/481450]\n",
      "loss: 0.231292  [73600/481450]\n",
      "loss: 0.323986  [76800/481450]\n",
      "loss: 0.132202  [80000/481450]\n",
      "loss: 0.183444  [83200/481450]\n",
      "loss: 0.282849  [86400/481450]\n",
      "loss: 0.298630  [89600/481450]\n",
      "loss: 0.078314  [92800/481450]\n",
      "loss: 0.136789  [96000/481450]\n",
      "loss: 0.252815  [99200/481450]\n",
      "loss: 0.123930  [102400/481450]\n",
      "loss: 0.205580  [105600/481450]\n",
      "loss: 0.093641  [108800/481450]\n",
      "loss: 0.092396  [112000/481450]\n",
      "loss: 0.458373  [115200/481450]\n",
      "loss: 0.205577  [118400/481450]\n",
      "loss: 0.241949  [121600/481450]\n",
      "loss: 0.360110  [124800/481450]\n",
      "loss: 0.238884  [128000/481450]\n",
      "loss: 0.122919  [131200/481450]\n",
      "loss: 0.256433  [134400/481450]\n",
      "loss: 0.058417  [137600/481450]\n",
      "loss: 0.166545  [140800/481450]\n",
      "loss: 0.196595  [144000/481450]\n",
      "loss: 0.158515  [147200/481450]\n",
      "loss: 0.198302  [150400/481450]\n",
      "loss: 0.155482  [153600/481450]\n",
      "loss: 0.124707  [156800/481450]\n",
      "loss: 0.108060  [160000/481450]\n",
      "loss: 0.211222  [163200/481450]\n",
      "loss: 0.187114  [166400/481450]\n",
      "loss: 0.249730  [169600/481450]\n",
      "loss: 0.119101  [172800/481450]\n",
      "loss: 0.134474  [176000/481450]\n",
      "loss: 0.224241  [179200/481450]\n",
      "loss: 0.041748  [182400/481450]\n",
      "loss: 0.167781  [185600/481450]\n",
      "loss: 0.333522  [188800/481450]\n",
      "loss: 0.070523  [192000/481450]\n",
      "loss: 0.164141  [195200/481450]\n",
      "loss: 0.042782  [198400/481450]\n",
      "loss: 0.201883  [201600/481450]\n",
      "loss: 0.111952  [204800/481450]\n",
      "loss: 0.165058  [208000/481450]\n",
      "loss: 0.272116  [211200/481450]\n",
      "loss: 0.587530  [214400/481450]\n",
      "loss: 0.192677  [217600/481450]\n",
      "loss: 0.258003  [220800/481450]\n",
      "loss: 0.140143  [224000/481450]\n",
      "loss: 0.318532  [227200/481450]\n",
      "loss: 0.023669  [230400/481450]\n",
      "loss: 0.109691  [233600/481450]\n",
      "loss: 0.168803  [236800/481450]\n",
      "loss: 0.105356  [240000/481450]\n",
      "loss: 0.215755  [243200/481450]\n",
      "loss: 0.129949  [246400/481450]\n",
      "loss: 0.093844  [249600/481450]\n",
      "loss: 0.073169  [252800/481450]\n",
      "loss: 0.349912  [256000/481450]\n",
      "loss: 0.368201  [259200/481450]\n",
      "loss: 0.157496  [262400/481450]\n",
      "loss: 0.257968  [265600/481450]\n",
      "loss: 0.331768  [268800/481450]\n",
      "loss: 0.133660  [272000/481450]\n",
      "loss: 0.161209  [275200/481450]\n",
      "loss: 0.362411  [278400/481450]\n",
      "loss: 0.178954  [281600/481450]\n",
      "loss: 0.187362  [284800/481450]\n",
      "loss: 0.304333  [288000/481450]\n",
      "loss: 0.185566  [291200/481450]\n",
      "loss: 0.346347  [294400/481450]\n",
      "loss: 0.212022  [297600/481450]\n",
      "loss: 0.179648  [300800/481450]\n",
      "loss: 0.195016  [304000/481450]\n",
      "loss: 0.297446  [307200/481450]\n",
      "loss: 0.242549  [310400/481450]\n",
      "loss: 0.152866  [313600/481450]\n",
      "loss: 0.154359  [316800/481450]\n",
      "loss: 0.324088  [320000/481450]\n",
      "loss: 0.065824  [323200/481450]\n",
      "loss: 0.247351  [326400/481450]\n",
      "loss: 0.097965  [329600/481450]\n",
      "loss: 0.181905  [332800/481450]\n",
      "loss: 0.168673  [336000/481450]\n",
      "loss: 0.379500  [339200/481450]\n",
      "loss: 0.051041  [342400/481450]\n",
      "loss: 0.206871  [345600/481450]\n",
      "loss: 0.050497  [348800/481450]\n",
      "loss: 0.069729  [352000/481450]\n",
      "loss: 0.155625  [355200/481450]\n",
      "loss: 0.108535  [358400/481450]\n",
      "loss: 0.044512  [361600/481450]\n",
      "loss: 0.141638  [364800/481450]\n",
      "loss: 0.258594  [368000/481450]\n",
      "loss: 0.324773  [371200/481450]\n",
      "loss: 0.155599  [374400/481450]\n",
      "loss: 0.226131  [377600/481450]\n",
      "loss: 0.181156  [380800/481450]\n",
      "loss: 0.357957  [384000/481450]\n",
      "loss: 0.121760  [387200/481450]\n",
      "loss: 0.463833  [390400/481450]\n",
      "loss: 0.121057  [393600/481450]\n",
      "loss: 0.200355  [396800/481450]\n",
      "loss: 0.137723  [400000/481450]\n",
      "loss: 0.101145  [403200/481450]\n",
      "loss: 0.119105  [406400/481450]\n",
      "loss: 0.150901  [409600/481450]\n",
      "loss: 0.165322  [412800/481450]\n",
      "loss: 0.073397  [416000/481450]\n",
      "loss: 0.163801  [419200/481450]\n",
      "loss: 0.103845  [422400/481450]\n",
      "loss: 0.159111  [425600/481450]\n",
      "loss: 0.186914  [428800/481450]\n",
      "loss: 0.174117  [432000/481450]\n",
      "loss: 0.403812  [435200/481450]\n",
      "loss: 0.157076  [438400/481450]\n",
      "loss: 0.064733  [441600/481450]\n",
      "loss: 0.201379  [444800/481450]\n",
      "loss: 0.164916  [448000/481450]\n",
      "loss: 0.057837  [451200/481450]\n",
      "loss: 0.227474  [454400/481450]\n",
      "loss: 0.414810  [457600/481450]\n",
      "loss: 0.366047  [460800/481450]\n",
      "loss: 0.139026  [464000/481450]\n",
      "loss: 0.209004  [467200/481450]\n",
      "loss: 0.164422  [470400/481450]\n",
      "loss: 0.192733  [473600/481450]\n",
      "loss: 0.144014  [476800/481450]\n",
      "loss: 0.036428  [480000/481450]\n",
      "Train Accuracy: 91.3470%\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.576606, F1-score: 87.33% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.292872  [    0/481450]\n",
      "loss: 0.188458  [ 3200/481450]\n",
      "loss: 0.242187  [ 6400/481450]\n",
      "loss: 0.042271  [ 9600/481450]\n",
      "loss: 0.194654  [12800/481450]\n",
      "loss: 0.168633  [16000/481450]\n",
      "loss: 0.216931  [19200/481450]\n",
      "loss: 0.240222  [22400/481450]\n",
      "loss: 0.133691  [25600/481450]\n",
      "loss: 0.120070  [28800/481450]\n",
      "loss: 0.283265  [32000/481450]\n",
      "loss: 0.236512  [35200/481450]\n",
      "loss: 0.364750  [38400/481450]\n",
      "loss: 0.202559  [41600/481450]\n",
      "loss: 0.172482  [44800/481450]\n",
      "loss: 0.197384  [48000/481450]\n",
      "loss: 0.094371  [51200/481450]\n",
      "loss: 0.344622  [54400/481450]\n",
      "loss: 0.212549  [57600/481450]\n",
      "loss: 0.139651  [60800/481450]\n",
      "loss: 0.111542  [64000/481450]\n",
      "loss: 0.067198  [67200/481450]\n",
      "loss: 0.230564  [70400/481450]\n",
      "loss: 0.243065  [73600/481450]\n",
      "loss: 0.264099  [76800/481450]\n",
      "loss: 0.364179  [80000/481450]\n",
      "loss: 0.163239  [83200/481450]\n",
      "loss: 0.159155  [86400/481450]\n",
      "loss: 0.294187  [89600/481450]\n",
      "loss: 0.177650  [92800/481450]\n",
      "loss: 0.328891  [96000/481450]\n",
      "loss: 0.234197  [99200/481450]\n",
      "loss: 0.289181  [102400/481450]\n",
      "loss: 0.145335  [105600/481450]\n",
      "loss: 0.032239  [108800/481450]\n",
      "loss: 0.132429  [112000/481450]\n",
      "loss: 0.317044  [115200/481450]\n",
      "loss: 0.328926  [118400/481450]\n",
      "loss: 0.216999  [121600/481450]\n",
      "loss: 0.219195  [124800/481450]\n",
      "loss: 0.216120  [128000/481450]\n",
      "loss: 0.137552  [131200/481450]\n",
      "loss: 0.206496  [134400/481450]\n",
      "loss: 0.244038  [137600/481450]\n",
      "loss: 0.113986  [140800/481450]\n",
      "loss: 0.170619  [144000/481450]\n",
      "loss: 0.134669  [147200/481450]\n",
      "loss: 0.343703  [150400/481450]\n",
      "loss: 0.204078  [153600/481450]\n",
      "loss: 0.390122  [156800/481450]\n",
      "loss: 0.310382  [160000/481450]\n",
      "loss: 0.294106  [163200/481450]\n",
      "loss: 0.380948  [166400/481450]\n",
      "loss: 0.153985  [169600/481450]\n",
      "loss: 0.096072  [172800/481450]\n",
      "loss: 0.156803  [176000/481450]\n",
      "loss: 0.253003  [179200/481450]\n",
      "loss: 0.224030  [182400/481450]\n",
      "loss: 0.284764  [185600/481450]\n",
      "loss: 0.102384  [188800/481450]\n",
      "loss: 0.086227  [192000/481450]\n",
      "loss: 0.164047  [195200/481450]\n",
      "loss: 0.576647  [198400/481450]\n",
      "loss: 0.173092  [201600/481450]\n",
      "loss: 0.289385  [204800/481450]\n",
      "loss: 0.106735  [208000/481450]\n",
      "loss: 0.175761  [211200/481450]\n",
      "loss: 0.107064  [214400/481450]\n",
      "loss: 0.329669  [217600/481450]\n",
      "loss: 0.162745  [220800/481450]\n",
      "loss: 0.285267  [224000/481450]\n",
      "loss: 0.048873  [227200/481450]\n",
      "loss: 0.210659  [230400/481450]\n",
      "loss: 0.071252  [233600/481450]\n",
      "loss: 0.283440  [236800/481450]\n",
      "loss: 0.203284  [240000/481450]\n",
      "loss: 0.178348  [243200/481450]\n",
      "loss: 0.119960  [246400/481450]\n",
      "loss: 0.148647  [249600/481450]\n",
      "loss: 0.315336  [252800/481450]\n",
      "loss: 0.176933  [256000/481450]\n",
      "loss: 0.147852  [259200/481450]\n",
      "loss: 0.200982  [262400/481450]\n",
      "loss: 0.013782  [265600/481450]\n",
      "loss: 0.260683  [268800/481450]\n",
      "loss: 0.078921  [272000/481450]\n",
      "loss: 0.064678  [275200/481450]\n",
      "loss: 0.132338  [278400/481450]\n",
      "loss: 0.247117  [281600/481450]\n",
      "loss: 0.074957  [284800/481450]\n",
      "loss: 0.213603  [288000/481450]\n",
      "loss: 0.140958  [291200/481450]\n",
      "loss: 0.253653  [294400/481450]\n",
      "loss: 0.108652  [297600/481450]\n",
      "loss: 0.148219  [300800/481450]\n",
      "loss: 0.286188  [304000/481450]\n",
      "loss: 0.225552  [307200/481450]\n",
      "loss: 0.132659  [310400/481450]\n",
      "loss: 0.272556  [313600/481450]\n",
      "loss: 0.118043  [316800/481450]\n",
      "loss: 0.230074  [320000/481450]\n",
      "loss: 0.183082  [323200/481450]\n",
      "loss: 0.127618  [326400/481450]\n",
      "loss: 0.149680  [329600/481450]\n",
      "loss: 0.013885  [332800/481450]\n",
      "loss: 0.201832  [336000/481450]\n",
      "loss: 0.241626  [339200/481450]\n",
      "loss: 0.132213  [342400/481450]\n",
      "loss: 0.038276  [345600/481450]\n",
      "loss: 0.286076  [348800/481450]\n",
      "loss: 0.298235  [352000/481450]\n",
      "loss: 0.118128  [355200/481450]\n",
      "loss: 0.146093  [358400/481450]\n",
      "loss: 0.174643  [361600/481450]\n",
      "loss: 0.106268  [364800/481450]\n",
      "loss: 0.129367  [368000/481450]\n",
      "loss: 0.305458  [371200/481450]\n",
      "loss: 0.204401  [374400/481450]\n",
      "loss: 0.082327  [377600/481450]\n",
      "loss: 0.190037  [380800/481450]\n",
      "loss: 0.077016  [384000/481450]\n",
      "loss: 0.212242  [387200/481450]\n",
      "loss: 0.107582  [390400/481450]\n",
      "loss: 0.361734  [393600/481450]\n",
      "loss: 0.095038  [396800/481450]\n",
      "loss: 0.337314  [400000/481450]\n",
      "loss: 0.069217  [403200/481450]\n",
      "loss: 0.233272  [406400/481450]\n",
      "loss: 0.247606  [409600/481450]\n",
      "loss: 0.140772  [412800/481450]\n",
      "loss: 0.299943  [416000/481450]\n",
      "loss: 0.107210  [419200/481450]\n",
      "loss: 0.110035  [422400/481450]\n",
      "loss: 0.311249  [425600/481450]\n",
      "loss: 0.156236  [428800/481450]\n",
      "loss: 0.129388  [432000/481450]\n",
      "loss: 0.377772  [435200/481450]\n",
      "loss: 0.105507  [438400/481450]\n",
      "loss: 0.197601  [441600/481450]\n",
      "loss: 0.393539  [444800/481450]\n",
      "loss: 0.202473  [448000/481450]\n",
      "loss: 0.260624  [451200/481450]\n",
      "loss: 0.389614  [454400/481450]\n",
      "loss: 0.314666  [457600/481450]\n",
      "loss: 0.100060  [460800/481450]\n",
      "loss: 0.105093  [464000/481450]\n",
      "loss: 0.078157  [467200/481450]\n",
      "loss: 0.143483  [470400/481450]\n",
      "loss: 0.091785  [473600/481450]\n",
      "loss: 0.193001  [476800/481450]\n",
      "loss: 0.193959  [480000/481450]\n",
      "Train Accuracy: 91.4244%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.601537, F1-score: 87.30% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.293852  [    0/481450]\n",
      "loss: 0.140145  [ 3200/481450]\n",
      "loss: 0.338805  [ 6400/481450]\n",
      "loss: 0.346190  [ 9600/481450]\n",
      "loss: 0.200652  [12800/481450]\n",
      "loss: 0.133985  [16000/481450]\n",
      "loss: 0.182211  [19200/481450]\n",
      "loss: 0.297239  [22400/481450]\n",
      "loss: 0.233935  [25600/481450]\n",
      "loss: 0.100926  [28800/481450]\n",
      "loss: 0.296997  [32000/481450]\n",
      "loss: 0.224021  [35200/481450]\n",
      "loss: 0.223484  [38400/481450]\n",
      "loss: 0.280052  [41600/481450]\n",
      "loss: 0.158745  [44800/481450]\n",
      "loss: 0.179346  [48000/481450]\n",
      "loss: 0.378757  [51200/481450]\n",
      "loss: 0.372829  [54400/481450]\n",
      "loss: 0.135202  [57600/481450]\n",
      "loss: 0.164658  [60800/481450]\n",
      "loss: 0.120805  [64000/481450]\n",
      "loss: 0.231280  [67200/481450]\n",
      "loss: 0.194715  [70400/481450]\n",
      "loss: 0.153476  [73600/481450]\n",
      "loss: 0.083133  [76800/481450]\n",
      "loss: 0.155785  [80000/481450]\n",
      "loss: 0.223236  [83200/481450]\n",
      "loss: 0.254190  [86400/481450]\n",
      "loss: 0.035368  [89600/481450]\n",
      "loss: 0.182514  [92800/481450]\n",
      "loss: 0.078676  [96000/481450]\n",
      "loss: 0.147350  [99200/481450]\n",
      "loss: 0.080668  [102400/481450]\n",
      "loss: 0.238862  [105600/481450]\n",
      "loss: 0.496540  [108800/481450]\n",
      "loss: 0.103488  [112000/481450]\n",
      "loss: 0.257350  [115200/481450]\n",
      "loss: 0.129760  [118400/481450]\n",
      "loss: 0.136764  [121600/481450]\n",
      "loss: 0.334034  [124800/481450]\n",
      "loss: 0.306695  [128000/481450]\n",
      "loss: 0.308579  [131200/481450]\n",
      "loss: 0.123456  [134400/481450]\n",
      "loss: 0.291586  [137600/481450]\n",
      "loss: 0.195290  [140800/481450]\n",
      "loss: 0.166599  [144000/481450]\n",
      "loss: 0.065715  [147200/481450]\n",
      "loss: 0.154556  [150400/481450]\n",
      "loss: 0.174486  [153600/481450]\n",
      "loss: 0.309006  [156800/481450]\n",
      "loss: 0.226020  [160000/481450]\n",
      "loss: 0.099349  [163200/481450]\n",
      "loss: 0.145871  [166400/481450]\n",
      "loss: 0.169761  [169600/481450]\n",
      "loss: 0.208148  [172800/481450]\n",
      "loss: 0.192269  [176000/481450]\n",
      "loss: 0.525598  [179200/481450]\n",
      "loss: 0.103518  [182400/481450]\n",
      "loss: 0.268483  [185600/481450]\n",
      "loss: 0.477904  [188800/481450]\n",
      "loss: 0.312029  [192000/481450]\n",
      "loss: 0.198815  [195200/481450]\n",
      "loss: 0.130438  [198400/481450]\n",
      "loss: 0.284000  [201600/481450]\n",
      "loss: 0.189185  [204800/481450]\n",
      "loss: 0.479137  [208000/481450]\n",
      "loss: 0.104480  [211200/481450]\n",
      "loss: 0.217888  [214400/481450]\n",
      "loss: 0.068480  [217600/481450]\n",
      "loss: 0.255133  [220800/481450]\n",
      "loss: 0.082090  [224000/481450]\n",
      "loss: 0.250062  [227200/481450]\n",
      "loss: 0.107683  [230400/481450]\n",
      "loss: 0.241433  [233600/481450]\n",
      "loss: 0.206217  [236800/481450]\n",
      "loss: 0.240499  [240000/481450]\n",
      "loss: 0.204232  [243200/481450]\n",
      "loss: 0.122913  [246400/481450]\n",
      "loss: 0.234735  [249600/481450]\n",
      "loss: 0.212584  [252800/481450]\n",
      "loss: 0.105122  [256000/481450]\n",
      "loss: 0.191109  [259200/481450]\n",
      "loss: 0.165366  [262400/481450]\n",
      "loss: 0.083238  [265600/481450]\n",
      "loss: 0.197832  [268800/481450]\n",
      "loss: 0.155268  [272000/481450]\n",
      "loss: 0.091083  [275200/481450]\n",
      "loss: 0.333946  [278400/481450]\n",
      "loss: 0.100532  [281600/481450]\n",
      "loss: 0.409436  [284800/481450]\n",
      "loss: 0.253065  [288000/481450]\n",
      "loss: 0.087352  [291200/481450]\n",
      "loss: 0.208431  [294400/481450]\n",
      "loss: 0.288316  [297600/481450]\n",
      "loss: 0.158058  [300800/481450]\n",
      "loss: 0.157977  [304000/481450]\n",
      "loss: 0.193853  [307200/481450]\n",
      "loss: 0.123617  [310400/481450]\n",
      "loss: 0.079272  [313600/481450]\n",
      "loss: 0.142205  [316800/481450]\n",
      "loss: 0.205977  [320000/481450]\n",
      "loss: 0.330177  [323200/481450]\n",
      "loss: 0.232332  [326400/481450]\n",
      "loss: 0.209838  [329600/481450]\n",
      "loss: 0.077678  [332800/481450]\n",
      "loss: 0.140148  [336000/481450]\n",
      "loss: 0.212220  [339200/481450]\n",
      "loss: 0.244509  [342400/481450]\n",
      "loss: 0.151635  [345600/481450]\n",
      "loss: 0.151774  [348800/481450]\n",
      "loss: 0.240836  [352000/481450]\n",
      "loss: 0.385093  [355200/481450]\n",
      "loss: 0.072556  [358400/481450]\n",
      "loss: 0.175327  [361600/481450]\n",
      "loss: 0.171345  [364800/481450]\n",
      "loss: 0.257373  [368000/481450]\n",
      "loss: 0.043542  [371200/481450]\n",
      "loss: 0.270181  [374400/481450]\n",
      "loss: 0.191341  [377600/481450]\n",
      "loss: 0.223152  [380800/481450]\n",
      "loss: 0.074661  [384000/481450]\n",
      "loss: 0.096822  [387200/481450]\n",
      "loss: 0.191838  [390400/481450]\n",
      "loss: 0.148737  [393600/481450]\n",
      "loss: 0.088559  [396800/481450]\n",
      "loss: 0.212745  [400000/481450]\n",
      "loss: 0.323554  [403200/481450]\n",
      "loss: 0.407631  [406400/481450]\n",
      "loss: 0.061950  [409600/481450]\n",
      "loss: 0.034184  [412800/481450]\n",
      "loss: 0.102531  [416000/481450]\n",
      "loss: 0.300922  [419200/481450]\n",
      "loss: 0.213182  [422400/481450]\n",
      "loss: 0.069836  [425600/481450]\n",
      "loss: 0.099199  [428800/481450]\n",
      "loss: 0.235002  [432000/481450]\n",
      "loss: 0.207293  [435200/481450]\n",
      "loss: 0.250413  [438400/481450]\n",
      "loss: 0.444315  [441600/481450]\n",
      "loss: 0.164359  [444800/481450]\n",
      "loss: 0.353796  [448000/481450]\n",
      "loss: 0.110196  [451200/481450]\n",
      "loss: 0.143503  [454400/481450]\n",
      "loss: 0.105295  [457600/481450]\n",
      "loss: 0.292640  [460800/481450]\n",
      "loss: 0.254417  [464000/481450]\n",
      "loss: 0.160041  [467200/481450]\n",
      "loss: 0.197808  [470400/481450]\n",
      "loss: 0.182284  [473600/481450]\n",
      "loss: 0.359994  [476800/481450]\n",
      "loss: 0.310075  [480000/481450]\n",
      "Train Accuracy: 91.5225%\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.690509, F1-score: 86.63% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.019843  [    0/481450]\n",
      "loss: 0.091699  [ 3200/481450]\n",
      "loss: 0.334064  [ 6400/481450]\n",
      "loss: 0.151981  [ 9600/481450]\n",
      "loss: 0.119755  [12800/481450]\n",
      "loss: 0.130635  [16000/481450]\n",
      "loss: 0.352371  [19200/481450]\n",
      "loss: 0.150827  [22400/481450]\n",
      "loss: 0.138937  [25600/481450]\n",
      "loss: 0.228565  [28800/481450]\n",
      "loss: 0.305333  [32000/481450]\n",
      "loss: 0.188663  [35200/481450]\n",
      "loss: 0.073065  [38400/481450]\n",
      "loss: 0.036045  [41600/481450]\n",
      "loss: 0.044202  [44800/481450]\n",
      "loss: 0.058292  [48000/481450]\n",
      "loss: 0.198426  [51200/481450]\n",
      "loss: 0.054154  [54400/481450]\n",
      "loss: 0.290930  [57600/481450]\n",
      "loss: 0.316506  [60800/481450]\n",
      "loss: 0.180975  [64000/481450]\n",
      "loss: 0.213443  [67200/481450]\n",
      "loss: 0.151495  [70400/481450]\n",
      "loss: 0.113272  [73600/481450]\n",
      "loss: 0.234522  [76800/481450]\n",
      "loss: 0.186086  [80000/481450]\n",
      "loss: 0.469977  [83200/481450]\n",
      "loss: 0.235719  [86400/481450]\n",
      "loss: 0.209004  [89600/481450]\n",
      "loss: 0.305321  [92800/481450]\n",
      "loss: 0.316962  [96000/481450]\n",
      "loss: 0.389251  [99200/481450]\n",
      "loss: 0.220825  [102400/481450]\n",
      "loss: 0.234050  [105600/481450]\n",
      "loss: 0.226694  [108800/481450]\n",
      "loss: 0.378516  [112000/481450]\n",
      "loss: 0.235387  [115200/481450]\n",
      "loss: 0.295964  [118400/481450]\n",
      "loss: 0.152503  [121600/481450]\n",
      "loss: 0.156231  [124800/481450]\n",
      "loss: 0.470513  [128000/481450]\n",
      "loss: 0.245164  [131200/481450]\n",
      "loss: 0.105744  [134400/481450]\n",
      "loss: 0.175230  [137600/481450]\n",
      "loss: 0.231375  [140800/481450]\n",
      "loss: 0.259438  [144000/481450]\n",
      "loss: 0.277102  [147200/481450]\n",
      "loss: 0.357066  [150400/481450]\n",
      "loss: 0.244166  [153600/481450]\n",
      "loss: 0.192282  [156800/481450]\n",
      "loss: 0.130993  [160000/481450]\n",
      "loss: 0.177858  [163200/481450]\n",
      "loss: 0.345773  [166400/481450]\n",
      "loss: 0.292023  [169600/481450]\n",
      "loss: 0.267983  [172800/481450]\n",
      "loss: 0.110088  [176000/481450]\n",
      "loss: 0.173080  [179200/481450]\n",
      "loss: 0.130211  [182400/481450]\n",
      "loss: 0.071846  [185600/481450]\n",
      "loss: 0.396844  [188800/481450]\n",
      "loss: 0.171384  [192000/481450]\n",
      "loss: 0.191743  [195200/481450]\n",
      "loss: 0.219783  [198400/481450]\n",
      "loss: 0.114111  [201600/481450]\n",
      "loss: 0.167881  [204800/481450]\n",
      "loss: 0.135037  [208000/481450]\n",
      "loss: 0.193742  [211200/481450]\n",
      "loss: 0.196251  [214400/481450]\n",
      "loss: 0.398506  [217600/481450]\n",
      "loss: 0.267323  [220800/481450]\n",
      "loss: 0.164218  [224000/481450]\n",
      "loss: 0.057629  [227200/481450]\n",
      "loss: 0.065881  [230400/481450]\n",
      "loss: 0.118746  [233600/481450]\n",
      "loss: 0.114694  [236800/481450]\n",
      "loss: 0.274695  [240000/481450]\n",
      "loss: 0.088339  [243200/481450]\n",
      "loss: 0.186252  [246400/481450]\n",
      "loss: 0.340437  [249600/481450]\n",
      "loss: 0.166172  [252800/481450]\n",
      "loss: 0.020099  [256000/481450]\n",
      "loss: 0.107344  [259200/481450]\n",
      "loss: 0.139782  [262400/481450]\n",
      "loss: 0.265492  [265600/481450]\n",
      "loss: 0.304966  [268800/481450]\n",
      "loss: 0.159357  [272000/481450]\n",
      "loss: 0.301496  [275200/481450]\n",
      "loss: 0.213824  [278400/481450]\n",
      "loss: 0.092622  [281600/481450]\n",
      "loss: 0.164826  [284800/481450]\n",
      "loss: 0.254954  [288000/481450]\n",
      "loss: 0.110353  [291200/481450]\n",
      "loss: 0.217034  [294400/481450]\n",
      "loss: 0.118823  [297600/481450]\n",
      "loss: 0.329982  [300800/481450]\n",
      "loss: 0.162979  [304000/481450]\n",
      "loss: 0.159083  [307200/481450]\n",
      "loss: 0.254024  [310400/481450]\n",
      "loss: 0.296924  [313600/481450]\n",
      "loss: 0.135600  [316800/481450]\n",
      "loss: 0.069635  [320000/481450]\n",
      "loss: 0.157145  [323200/481450]\n",
      "loss: 0.281387  [326400/481450]\n",
      "loss: 0.121641  [329600/481450]\n",
      "loss: 0.265109  [332800/481450]\n",
      "loss: 0.522636  [336000/481450]\n",
      "loss: 0.174307  [339200/481450]\n",
      "loss: 0.409139  [342400/481450]\n",
      "loss: 0.191284  [345600/481450]\n",
      "loss: 0.058593  [348800/481450]\n",
      "loss: 0.107798  [352000/481450]\n",
      "loss: 0.194370  [355200/481450]\n",
      "loss: 0.200887  [358400/481450]\n",
      "loss: 0.178726  [361600/481450]\n",
      "loss: 0.095830  [364800/481450]\n",
      "loss: 0.262110  [368000/481450]\n",
      "loss: 0.387504  [371200/481450]\n",
      "loss: 0.122650  [374400/481450]\n",
      "loss: 0.219505  [377600/481450]\n",
      "loss: 0.155869  [380800/481450]\n",
      "loss: 0.147170  [384000/481450]\n",
      "loss: 0.311747  [387200/481450]\n",
      "loss: 0.227913  [390400/481450]\n",
      "loss: 0.216800  [393600/481450]\n",
      "loss: 0.243844  [396800/481450]\n",
      "loss: 0.160015  [400000/481450]\n",
      "loss: 0.339557  [403200/481450]\n",
      "loss: 0.233783  [406400/481450]\n",
      "loss: 0.216685  [409600/481450]\n",
      "loss: 0.100560  [412800/481450]\n",
      "loss: 0.132891  [416000/481450]\n",
      "loss: 0.173283  [419200/481450]\n",
      "loss: 0.121102  [422400/481450]\n",
      "loss: 0.298985  [425600/481450]\n",
      "loss: 0.073788  [428800/481450]\n",
      "loss: 0.337812  [432000/481450]\n",
      "loss: 0.146777  [435200/481450]\n",
      "loss: 0.176287  [438400/481450]\n",
      "loss: 0.087019  [441600/481450]\n",
      "loss: 0.465211  [444800/481450]\n",
      "loss: 0.169889  [448000/481450]\n",
      "loss: 0.029444  [451200/481450]\n",
      "loss: 0.156445  [454400/481450]\n",
      "loss: 0.043826  [457600/481450]\n",
      "loss: 0.016972  [460800/481450]\n",
      "loss: 0.140068  [464000/481450]\n",
      "loss: 0.155015  [467200/481450]\n",
      "loss: 0.100514  [470400/481450]\n",
      "loss: 0.236709  [473600/481450]\n",
      "loss: 0.244277  [476800/481450]\n",
      "loss: 0.188534  [480000/481450]\n",
      "Train Accuracy: 91.5929%\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.630268, F1-score: 87.33% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Smooth and plot training loss for each fold\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, losses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mall_losses\u001b[49m):\n\u001b[0;32m     17\u001b[0m     smoothed_loss \u001b[38;5;241m=\u001b[39m smooth_loss(losses, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Adjust window_size as needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(smoothed_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
