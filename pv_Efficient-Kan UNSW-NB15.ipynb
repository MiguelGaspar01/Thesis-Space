{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"all\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after resampling: Counter({'0': 1})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "smote_enn = SMOTEENN(sampling_strategy=\"auto\", enn = enn, random_state=42)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_labels_encoded)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.000311\n",
       "encoder__proto_ospf             0.001044\n",
       "encoder__proto_sctp             0.001308\n",
       "encoder__proto_tcp              0.000619\n",
       "encoder__proto_udp              0.016380\n",
       "encoder__proto_unas             0.002054\n",
       "encoder__service_dns            0.010718\n",
       "encoder__service_ftp            0.000065\n",
       "encoder__service_ftp-data       0.000163\n",
       "encoder__service_http           0.014931\n",
       "encoder__service_smtp           0.000242\n",
       "encoder__state_CON              0.000703\n",
       "encoder__state_ECO              0.000002\n",
       "encoder__state_FIN              0.000307\n",
       "encoder__state_INT              0.001511\n",
       "encoder__state_REQ              0.000159\n",
       "encoder__state_RST              0.000004\n",
       "remainder__dur                  0.019442\n",
       "remainder__spkts                0.017945\n",
       "remainder__dpkts                0.012584\n",
       "remainder__sbytes               0.068248\n",
       "remainder__dbytes               0.024869\n",
       "remainder__rate                 0.020755\n",
       "remainder__sttl                 0.045384\n",
       "remainder__dttl                 0.027071\n",
       "remainder__sload                0.026382\n",
       "remainder__dload                0.025673\n",
       "remainder__sloss                0.009783\n",
       "remainder__dloss                0.007623\n",
       "remainder__sinpkt               0.018103\n",
       "remainder__dinpkt               0.008567\n",
       "remainder__sjit                 0.018018\n",
       "remainder__djit                 0.006013\n",
       "remainder__swin                 0.005968\n",
       "remainder__stcpb                0.004121\n",
       "remainder__dtcpb                0.002351\n",
       "remainder__dwin                 0.004511\n",
       "remainder__tcprtt               0.017455\n",
       "remainder__synack               0.009202\n",
       "remainder__ackdat               0.009084\n",
       "remainder__smean                0.069373\n",
       "remainder__dmean                0.026209\n",
       "remainder__trans_depth          0.054308\n",
       "remainder__response_body_len    0.004144\n",
       "remainder__ct_srv_src           0.023065\n",
       "remainder__ct_state_ttl         0.028406\n",
       "remainder__ct_dst_ltm           0.042347\n",
       "remainder__ct_src_dport_ltm     0.046356\n",
       "remainder__ct_dst_sport_ltm     0.032865\n",
       "remainder__ct_dst_src_ltm       0.059473\n",
       "remainder__is_ftp_login         0.000113\n",
       "remainder__ct_ftp_cmd           0.000225\n",
       "remainder__ct_flw_http_mthd     0.056274\n",
       "remainder__ct_src_ltm           0.038922\n",
       "remainder__ct_srv_dst           0.057708\n",
       "remainder__is_sm_ips_ports      0.000540\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(encoder__proto_arp              False\n",
       " encoder__proto_ospf             False\n",
       " encoder__proto_sctp             False\n",
       " encoder__proto_tcp               True\n",
       " encoder__proto_udp               True\n",
       " encoder__proto_unas             False\n",
       " encoder__service_dns             True\n",
       " encoder__service_ftp            False\n",
       " encoder__service_ftp-data       False\n",
       " encoder__service_http            True\n",
       " encoder__service_smtp           False\n",
       " encoder__state_CON              False\n",
       " encoder__state_ECO              False\n",
       " encoder__state_FIN               True\n",
       " encoder__state_INT               True\n",
       " encoder__state_REQ              False\n",
       " encoder__state_RST              False\n",
       " remainder__dur                   True\n",
       " remainder__spkts                 True\n",
       " remainder__dpkts                 True\n",
       " remainder__sbytes                True\n",
       " remainder__dbytes                True\n",
       " remainder__rate                  True\n",
       " remainder__sttl                  True\n",
       " remainder__dttl                  True\n",
       " remainder__sload                 True\n",
       " remainder__dload                 True\n",
       " remainder__sloss                 True\n",
       " remainder__dloss                 True\n",
       " remainder__sinpkt               False\n",
       " remainder__dinpkt                True\n",
       " remainder__sjit                  True\n",
       " remainder__djit                  True\n",
       " remainder__swin                  True\n",
       " remainder__stcpb                 True\n",
       " remainder__dtcpb                 True\n",
       " remainder__dwin                  True\n",
       " remainder__tcprtt                True\n",
       " remainder__synack                True\n",
       " remainder__ackdat                True\n",
       " remainder__smean                 True\n",
       " remainder__dmean                 True\n",
       " remainder__trans_depth           True\n",
       " remainder__response_body_len    False\n",
       " remainder__ct_srv_src            True\n",
       " remainder__ct_state_ttl          True\n",
       " remainder__ct_dst_ltm            True\n",
       " remainder__ct_src_dport_ltm      True\n",
       " remainder__ct_dst_sport_ltm      True\n",
       " remainder__ct_dst_src_ltm        True\n",
       " remainder__is_ftp_login         False\n",
       " remainder__ct_ftp_cmd           False\n",
       " remainder__ct_flw_http_mthd      True\n",
       " remainder__ct_src_ltm            True\n",
       " remainder__ct_srv_dst            True\n",
       " remainder__is_sm_ips_ports      False\n",
       " dtype: bool,\n",
       "                          Feature        F-value       p-value\n",
       " 47   remainder__ct_src_dport_ltm  735652.423060  0.000000e+00\n",
       " 48   remainder__ct_dst_sport_ltm  444776.721694  0.000000e+00\n",
       " 46         remainder__ct_dst_ltm  409489.907129  0.000000e+00\n",
       " 53         remainder__ct_src_ltm  337279.850869  0.000000e+00\n",
       " 49     remainder__ct_dst_src_ltm  254951.681875  0.000000e+00\n",
       " 6           encoder__service_dns  253669.382684  0.000000e+00\n",
       " 42        remainder__trans_depth  248620.697186  0.000000e+00\n",
       " 54         remainder__ct_srv_dst  222472.551262  0.000000e+00\n",
       " 44         remainder__ct_srv_src  205693.457395  0.000000e+00\n",
       " 52   remainder__ct_flw_http_mthd  179118.491655  0.000000e+00\n",
       " 23               remainder__sttl  165408.429033  0.000000e+00\n",
       " 45       remainder__ct_state_ttl   70848.025294  0.000000e+00\n",
       " 9          encoder__service_http   41117.919060  0.000000e+00\n",
       " 24               remainder__dttl   41063.239166  0.000000e+00\n",
       " 4             encoder__proto_udp   38679.565008  0.000000e+00\n",
       " 26              remainder__dload   35914.983855  0.000000e+00\n",
       " 33               remainder__swin   34916.221382  0.000000e+00\n",
       " 36               remainder__dwin   34492.773640  0.000000e+00\n",
       " 39             remainder__ackdat   30804.871717  0.000000e+00\n",
       " 22               remainder__rate   29798.075582  0.000000e+00\n",
       " 37             remainder__tcprtt   29159.454724  0.000000e+00\n",
       " 38             remainder__synack   26043.146803  0.000000e+00\n",
       " 41              remainder__dmean   25443.087112  0.000000e+00\n",
       " 19              remainder__dpkts   23981.025893  0.000000e+00\n",
       " 14            encoder__state_INT   21683.989438  0.000000e+00\n",
       " 34              remainder__stcpb   20862.273122  0.000000e+00\n",
       " 18              remainder__spkts   20454.696465  0.000000e+00\n",
       " 40              remainder__smean   20032.731889  0.000000e+00\n",
       " 28              remainder__dloss   19345.024993  0.000000e+00\n",
       " 27              remainder__sloss   18581.861219  0.000000e+00\n",
       " 35              remainder__dtcpb   18430.476449  0.000000e+00\n",
       " 21             remainder__dbytes   17986.255994  0.000000e+00\n",
       " 3             encoder__proto_tcp   17786.791960  0.000000e+00\n",
       " 13            encoder__state_FIN   17022.751600  0.000000e+00\n",
       " 30             remainder__dinpkt   15748.181964  0.000000e+00\n",
       " 31               remainder__sjit   13687.709302  0.000000e+00\n",
       " 25              remainder__sload   13039.688825  0.000000e+00\n",
       " 32               remainder__djit   11908.712429  0.000000e+00\n",
       " 20             remainder__sbytes   11339.856506  0.000000e+00\n",
       " 17                remainder__dur   10666.521808  0.000000e+00\n",
       " 43  remainder__response_body_len    9079.086020  0.000000e+00\n",
       " 11            encoder__state_CON    8066.344525  0.000000e+00\n",
       " 5            encoder__proto_unas    7260.733806  0.000000e+00\n",
       " 29             remainder__sinpkt    3855.793624  0.000000e+00\n",
       " 10         encoder__service_smtp    3795.025326  0.000000e+00\n",
       " 1            encoder__proto_ospf    3249.005451  0.000000e+00\n",
       " 0             encoder__proto_arp    2543.205362  0.000000e+00\n",
       " 55    remainder__is_sm_ips_ports    2452.443102  0.000000e+00\n",
       " 51         remainder__ct_ftp_cmd    2416.496632  0.000000e+00\n",
       " 50       remainder__is_ftp_login    2416.496632  0.000000e+00\n",
       " 8      encoder__service_ftp-data    2163.595438  0.000000e+00\n",
       " 7           encoder__service_ftp    1464.542755  0.000000e+00\n",
       " 2            encoder__proto_sctp    1404.419164  0.000000e+00\n",
       " 15            encoder__state_REQ     612.018547  0.000000e+00\n",
       " 16            encoder__state_RST      45.437879  1.840978e-82\n",
       " 12            encoder__state_ECO      10.131723  9.446784e-16,\n",
       " 'encoder__proto_tcp\", \"encoder__proto_udp\", \"encoder__service_dns\", \"encoder__service_http\", \"encoder__state_FIN\", \"encoder__state_INT\", \"remainder__dur\", \"remainder__spkts\", \"remainder__dpkts\", \"remainder__sbytes\", \"remainder__dbytes\", \"remainder__rate\", \"remainder__sttl\", \"remainder__dttl\", \"remainder__sload\", \"remainder__dload\", \"remainder__sloss\", \"remainder__dloss\", \"remainder__dinpkt\", \"remainder__sjit\", \"remainder__djit\", \"remainder__swin\", \"remainder__stcpb\", \"remainder__dtcpb\", \"remainder__dwin\", \"remainder__tcprtt\", \"remainder__synack\", \"remainder__ackdat\", \"remainder__smean\", \"remainder__dmean\", \"remainder__trans_depth\", \"remainder__ct_srv_src\", \"remainder__ct_state_ttl\", \"remainder__ct_dst_ltm\", \"remainder__ct_src_dport_ltm\", \"remainder__ct_dst_sport_ltm\", \"remainder__ct_dst_src_ltm\", \"remainder__ct_flw_http_mthd\", \"remainder__ct_src_ltm\", \"remainder__ct_srv_dst')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8a6f04-5f16-4b7e-ad82-025e6fed9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"encoder__proto_tcp\", \"encoder__proto_udp\", \"encoder__service_dns\", \"encoder__service_http\", \"encoder__state_FIN\", \"encoder__state_INT\", \"remainder__dur\", \"remainder__spkts\", \"remainder__dpkts\", \"remainder__sbytes\", \"remainder__dbytes\", \"remainder__rate\", \"remainder__sttl\", \"remainder__dttl\", \"remainder__sload\", \"remainder__dload\", \"remainder__sloss\", \"remainder__dloss\", \"remainder__dinpkt\", \"remainder__sjit\", \"remainder__djit\", \"remainder__swin\", \"remainder__stcpb\", \"remainder__dtcpb\", \"remainder__dwin\", \"remainder__tcprtt\", \"remainder__synack\", \"remainder__ackdat\", \"remainder__smean\", \"remainder__dmean\", \"remainder__trans_depth\", \"remainder__ct_srv_src\", \"remainder__ct_state_ttl\", \"remainder__ct_dst_ltm\", \"remainder__ct_src_dport_ltm\", \"remainder__ct_dst_sport_ltm\", \"remainder__ct_dst_src_ltm\", \"remainder__ct_flw_http_mthd\", \"remainder__ct_src_ltm\", \"remainder__ct_srv_dst\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac8aea9-b0a7-4656-82da-643f3aee5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d2cbf24-0adf-4818-8af1-ff10d2b9462e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch 500: 2.181\n",
      "Loss after mini-batch 1000: 1.630\n",
      "Loss after mini-batch 1500: 1.248\n",
      "Loss after mini-batch 2000: 0.988\n",
      "Loss after mini-batch 2500: 0.835\n",
      "Loss after mini-batch 3000: 0.747\n",
      "Loss after mini-batch 3500: 0.717\n",
      "Loss after mini-batch 4000: 0.691\n",
      "Loss after mini-batch 4500: 0.653\n",
      "Loss after mini-batch 5000: 0.606\n",
      "Loss after mini-batch 5500: 0.573\n",
      "Loss after mini-batch 6000: 0.536\n",
      "Loss after mini-batch 6500: 0.517\n",
      "Loss after mini-batch 7000: 0.503\n",
      "Loss after mini-batch 7500: 0.471\n",
      "Loss after mini-batch 8000: 0.478\n",
      "Loss after mini-batch 8500: 0.466\n",
      "Loss after mini-batch 9000: 0.439\n",
      "Loss after mini-batch 9500: 0.438\n",
      "Loss after mini-batch 10000: 0.422\n",
      "Loss after mini-batch 10500: 0.433\n",
      "Loss after mini-batch 11000: 0.398\n",
      "Loss after mini-batch 11500: 0.412\n",
      "Loss after mini-batch 12000: 0.386\n",
      "Loss after mini-batch 12500: 0.389\n",
      "Loss after mini-batch 13000: 0.390\n",
      "Loss after mini-batch 13500: 0.375\n",
      "Loss after mini-batch 14000: 0.373\n",
      "Loss after mini-batch 14500: 0.388\n",
      "Loss after mini-batch 15000: 0.360\n",
      "Loss after mini-batch 15500: 0.361\n",
      "Loss after mini-batch 16000: 0.367\n",
      "Loss after mini-batch 16500: 0.363\n",
      "Loss after mini-batch 17000: 0.356\n",
      "Loss after mini-batch 17500: 0.351\n",
      "Loss after mini-batch 18000: 0.341\n",
      "Loss after mini-batch 18500: 0.343\n",
      "Loss after mini-batch 19000: 0.353\n",
      "Loss after mini-batch 19500: 0.320\n",
      "Loss after mini-batch 20000: 0.324\n",
      "Loss after mini-batch 20500: 0.322\n",
      "Loss after mini-batch 21000: 0.332\n",
      "Loss after mini-batch 21500: 0.325\n",
      "Loss after mini-batch 22000: 0.333\n",
      "Loss after mini-batch 22500: 0.328\n",
      "Loss after mini-batch 23000: 0.337\n",
      "Loss after mini-batch 23500: 0.315\n",
      "Loss after mini-batch 24000: 0.329\n",
      "Loss after mini-batch 24500: 0.319\n",
      "Loss after mini-batch 25000: 0.324\n",
      "Loss after mini-batch 25500: 0.308\n",
      "Loss after mini-batch 26000: 0.305\n",
      "Loss after mini-batch 26500: 0.287\n",
      "Loss after mini-batch 27000: 0.292\n",
      "Loss after mini-batch 27500: 0.293\n",
      "Loss after mini-batch 28000: 0.294\n",
      "Starting epoch 2\n",
      "Loss after mini-batch 500: 0.302\n",
      "Loss after mini-batch 1000: 0.285\n",
      "Loss after mini-batch 1500: 0.288\n",
      "Loss after mini-batch 2000: 0.285\n",
      "Loss after mini-batch 2500: 0.281\n",
      "Loss after mini-batch 3000: 0.282\n",
      "Loss after mini-batch 3500: 0.286\n",
      "Loss after mini-batch 4000: 0.289\n",
      "Loss after mini-batch 4500: 0.268\n",
      "Loss after mini-batch 5000: 0.269\n",
      "Loss after mini-batch 5500: 0.281\n",
      "Loss after mini-batch 6000: 0.287\n",
      "Loss after mini-batch 6500: 0.274\n",
      "Loss after mini-batch 7000: 0.264\n",
      "Loss after mini-batch 7500: 0.259\n",
      "Loss after mini-batch 8000: 0.265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 82\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Store the loss value\u001b[39;00m\n\u001b[0;32m     85\u001b[0m fold_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    235\u001b[0m         group,\n\u001b[0;32m    236\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m         state_steps,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[1;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\adam.py:423\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    420\u001b[0m     device_beta1 \u001b[38;5;241m=\u001b[39m beta1\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set fixed random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration options\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "results = {}\n",
    "\n",
    "# Convert data to PyTorch tensors (assuming `train_X_tensor`, `test_X_tensor`, `train_Y_tensor`, `test_Y_tensor` exist)\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n",
    "\n",
    "# Concatenate train and test dataset for cross-validation\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "# Initialize a list to store loss values across folds\n",
    "all_losses = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    testloader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "    # Init the neural network\n",
    "    model = KAN([40, 64,32, 16,10],grid_size = 20)\n",
    "    model.grid_size\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Initialize a list to store loss values for this fold\n",
    "    fold_losses = []\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch + 1}')\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, (inputs, targets) in enumerate(trainloader, 0):\n",
    "\n",
    "            inputs, targets = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the loss value\n",
    "            fold_losses.append(loss.item())\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print(f'Loss after mini-batch {i + 1}: {current_loss / 500:.3f}')\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # Store the loss values for this fold\n",
    "    all_losses.append(fold_losses)\n",
    "\n",
    "    print('Training process has finished. Starting testing...')\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # Store predictions and labels for precision, recall, F1\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = 100.0 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "\n",
    "    print(f'Fold {fold} Results:')\n",
    "    print(f'Accuracy: {accuracy:.2f} %')\n",
    "    print(f'Precision: {precision:.2f} %')\n",
    "    print(f'Recall: {recall:.2f} %')\n",
    "    print(f'F1 Score: {f1:.2f} %')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    results[fold] = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n",
    "\n",
    "# Print final K-Fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "\n",
    "# Compute and print average scores\n",
    "avg_accuracy = np.mean([results[fold][\"accuracy\"] for fold in results])\n",
    "avg_precision = np.mean([results[fold][\"precision\"] for fold in results])\n",
    "avg_recall = np.mean([results[fold][\"recall\"] for fold in results])\n",
    "avg_f1 = np.mean([results[fold][\"f1_score\"] for fold in results])\n",
    "\n",
    "for fold, metrics in results.items():\n",
    "    print(f'Fold {fold}: Accuracy: {metrics[\"accuracy\"]:.2f}%, Precision: {metrics[\"precision\"]:.2f}%, Recall: {metrics[\"recall\"]:.2f}%, F1 Score: {metrics[\"f1_score\"]:.2f}%')\n",
    "\n",
    "print('--------------------------------')\n",
    "print(f'Average Accuracy: {avg_accuracy:.2f} %')\n",
    "print(f'Average Precision: {avg_precision:.2f} %')\n",
    "print(f'Average Recall: {avg_recall:.2f} %')\n",
    "print(f'Average F1 Score: {avg_f1:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efd8b34a-7e7d-4854-b095-c732a9afb242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "Loss after mini-batch 500: 2.170\n",
      "Loss after mini-batch 1000: 1.595\n",
      "Loss after mini-batch 1500: 1.113\n",
      "Loss after mini-batch 2000: 0.852\n",
      "Loss after mini-batch 2500: 0.682\n",
      "Loss after mini-batch 3000: 0.558\n",
      "Loss after mini-batch 3500: 0.475\n",
      "Loss after mini-batch 4000: 0.420\n",
      "Loss after mini-batch 4500: 0.383\n",
      "Loss after mini-batch 5000: 0.358\n",
      "Loss after mini-batch 5500: 0.321\n",
      "Loss after mini-batch 6000: 0.313\n",
      "Loss after mini-batch 6500: 0.314\n",
      "Loss after mini-batch 7000: 0.292\n",
      "Loss after mini-batch 7500: 0.277\n",
      "Loss after mini-batch 8000: 0.271\n",
      "Loss after mini-batch 8500: 0.268\n",
      "Loss after mini-batch 9000: 0.259\n",
      "Loss after mini-batch 9500: 0.248\n",
      "Loss after mini-batch 10000: 0.241\n",
      "Loss after mini-batch 10500: 0.232\n",
      "Loss after mini-batch 11000: 0.232\n",
      "Loss after mini-batch 11500: 0.228\n",
      "Loss after mini-batch 12000: 0.231\n",
      "Loss after mini-batch 12500: 0.229\n",
      "Loss after mini-batch 13000: 0.228\n",
      "Loss after mini-batch 13500: 0.218\n",
      "Loss after mini-batch 14000: 0.221\n",
      "Loss after mini-batch 14500: 0.219\n",
      "Loss after mini-batch 15000: 0.229\n",
      "Loss after mini-batch 15500: 0.211\n",
      "Loss after mini-batch 16000: 0.211\n",
      "Loss after mini-batch 16500: 0.194\n",
      "Loss after mini-batch 17000: 0.210\n",
      "Loss after mini-batch 17500: 0.205\n",
      "Loss after mini-batch 18000: 0.193\n",
      "Loss after mini-batch 18500: 0.201\n",
      "Loss after mini-batch 19000: 0.193\n",
      "Loss after mini-batch 19500: 0.193\n",
      "Loss after mini-batch 20000: 0.192\n",
      "Loss after mini-batch 20500: 0.181\n",
      "Loss after mini-batch 21000: 0.193\n",
      "Loss after mini-batch 21500: 0.191\n",
      "Loss after mini-batch 22000: 0.181\n",
      "Loss after mini-batch 22500: 0.182\n",
      "Loss after mini-batch 23000: 0.178\n",
      "Loss after mini-batch 23500: 0.181\n",
      "Loss after mini-batch 24000: 0.172\n",
      "Loss after mini-batch 24500: 0.177\n",
      "Loss after mini-batch 25000: 0.179\n",
      "Loss after mini-batch 25500: 0.188\n",
      "Loss after mini-batch 26000: 0.172\n",
      "Loss after mini-batch 26500: 0.180\n",
      "Loss after mini-batch 27000: 0.180\n",
      "Loss after mini-batch 27500: 0.182\n",
      "Loss after mini-batch 28000: 0.169\n",
      "Starting epoch 2\n",
      "Loss after mini-batch 500: 0.166\n",
      "Loss after mini-batch 1000: 0.169\n",
      "Loss after mini-batch 1500: 0.169\n",
      "Loss after mini-batch 2000: 0.161\n",
      "Loss after mini-batch 2500: 0.166\n",
      "Loss after mini-batch 3000: 0.167\n",
      "Loss after mini-batch 3500: 0.160\n",
      "Loss after mini-batch 4000: 0.161\n",
      "Loss after mini-batch 4500: 0.163\n",
      "Loss after mini-batch 5000: 0.160\n",
      "Loss after mini-batch 5500: 0.159\n",
      "Loss after mini-batch 6000: 0.165\n",
      "Loss after mini-batch 6500: 0.156\n",
      "Loss after mini-batch 7000: 0.162\n",
      "Loss after mini-batch 7500: 0.156\n",
      "Loss after mini-batch 8000: 0.158\n",
      "Loss after mini-batch 8500: 0.156\n",
      "Loss after mini-batch 9000: 0.155\n",
      "Loss after mini-batch 9500: 0.165\n",
      "Loss after mini-batch 10000: 0.161\n",
      "Loss after mini-batch 10500: 0.156\n",
      "Loss after mini-batch 11000: 0.148\n",
      "Loss after mini-batch 11500: 0.152\n",
      "Loss after mini-batch 12000: 0.165\n",
      "Loss after mini-batch 12500: 0.162\n",
      "Loss after mini-batch 13000: 0.153\n",
      "Loss after mini-batch 13500: 0.159\n",
      "Loss after mini-batch 14000: 0.158\n",
      "Loss after mini-batch 14500: 0.144\n",
      "Loss after mini-batch 15000: 0.150\n",
      "Loss after mini-batch 15500: 0.154\n",
      "Loss after mini-batch 16000: 0.166\n",
      "Loss after mini-batch 16500: 0.159\n",
      "Loss after mini-batch 17000: 0.152\n",
      "Loss after mini-batch 17500: 0.153\n",
      "Loss after mini-batch 18000: 0.159\n",
      "Loss after mini-batch 18500: 0.142\n",
      "Loss after mini-batch 19000: 0.156\n",
      "Loss after mini-batch 19500: 0.157\n",
      "Loss after mini-batch 20000: 0.148\n",
      "Loss after mini-batch 20500: 0.146\n",
      "Loss after mini-batch 21000: 0.148\n",
      "Loss after mini-batch 21500: 0.157\n",
      "Loss after mini-batch 22000: 0.149\n",
      "Loss after mini-batch 22500: 0.150\n",
      "Loss after mini-batch 23000: 0.156\n",
      "Loss after mini-batch 23500: 0.148\n",
      "Loss after mini-batch 24000: 0.145\n",
      "Loss after mini-batch 24500: 0.147\n",
      "Loss after mini-batch 25000: 0.140\n",
      "Loss after mini-batch 25500: 0.145\n",
      "Loss after mini-batch 26000: 0.136\n",
      "Loss after mini-batch 26500: 0.146\n",
      "Loss after mini-batch 27000: 0.146\n",
      "Loss after mini-batch 27500: 0.140\n",
      "Loss after mini-batch 28000: 0.142\n",
      "Starting epoch 3\n",
      "Loss after mini-batch 500: 0.145\n",
      "Loss after mini-batch 1000: 0.149\n",
      "Loss after mini-batch 1500: 0.144\n",
      "Loss after mini-batch 2000: 0.148\n",
      "Loss after mini-batch 2500: 0.146\n",
      "Loss after mini-batch 3000: 0.139\n",
      "Loss after mini-batch 3500: 0.147\n",
      "Loss after mini-batch 4000: 0.147\n",
      "Loss after mini-batch 4500: 0.135\n",
      "Loss after mini-batch 5000: 0.145\n",
      "Loss after mini-batch 5500: 0.138\n",
      "Loss after mini-batch 6000: 0.139\n",
      "Loss after mini-batch 6500: 0.138\n",
      "Loss after mini-batch 7000: 0.135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)), targets\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     75\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[0;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[0;32m    743\u001b[0m )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\optimizer.py:975\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 975\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set fixed random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration options\n",
    "k_folds = 5\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "results = {}\n",
    "\n",
    "# Convert data to PyTorch tensors (assuming `train_X_tensor`, `test_X_tensor`, `train_Y_tensor`, `test_Y_tensor` exist)\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n",
    "\n",
    "# Concatenate train and test dataset for cross-validation\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "\n",
    "# Initialize a list to store loss values across folds\n",
    "all_losses = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    trainloader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    testloader = DataLoader(dataset, batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "    # Init the neural network\n",
    "    model = KAN([40, 64,32, 16,10],grid_size = 20)\n",
    "    model.grid_size\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Initialize a list to store loss values for this fold\n",
    "    fold_losses = []\n",
    "\n",
    "    # Run the training loop for defined number of epochs\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch + 1}')\n",
    "        current_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, (inputs, targets) in enumerate(trainloader, 0):\n",
    "\n",
    "            inputs, targets = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the loss value\n",
    "            fold_losses.append(loss.item())\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print(f'Loss after mini-batch {i + 1}: {current_loss / 500:.3f}')\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # Store the loss values for this fold\n",
    "    all_losses.append(fold_losses)\n",
    "\n",
    "    print('Training process has finished. Starting testing...')\n",
    "\n",
    "    # Evaluation for this fold\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), targets.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs,targets)\n",
    "            test_loss =+ loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # Store predictions and labels for precision, recall, F1\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(testloader)\n",
    "    print(f'Test Loss: {avg_test_loss:.2f}')\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = 100.0 * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\") * 100\n",
    "\n",
    "    print(f'Fold {fold} Results:')\n",
    "    print(f'Accuracy: {accuracy:.2f} %')\n",
    "    print(f'Precision: {precision:.2f} %')\n",
    "    print(f'Recall: {recall:.2f} %')\n",
    "    print(f'F1 Score: {f1:.2f} %')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    results[fold] = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n",
    "\n",
    "# Print final K-Fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "\n",
    "# Compute and print average scores\n",
    "avg_accuracy = np.mean([results[fold][\"accuracy\"] for fold in results])\n",
    "avg_precision = np.mean([results[fold][\"precision\"] for fold in results])\n",
    "avg_recall = np.mean([results[fold][\"recall\"] for fold in results])\n",
    "avg_f1 = np.mean([results[fold][\"f1_score\"] for fold in results])\n",
    "\n",
    "for fold, metrics in results.items():\n",
    "    print(f'Fold {fold}: Accuracy: {metrics[\"accuracy\"]:.2f}%, Precision: {metrics[\"precision\"]:.2f}%, Recall: {metrics[\"recall\"]:.2f}%, F1 Score: {metrics[\"f1_score\"]:.2f}%')\n",
    "\n",
    "print('--------------------------------')\n",
    "print(f'Average Accuracy: {avg_accuracy:.2f} %')\n",
    "print(f'Average Precision: {avg_precision:.2f} %')\n",
    "print(f'Average Recall: {avg_recall:.2f} %')\n",
    "print(f'Average F1 Score: {avg_f1:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306367  [    0/481450]\n",
      "loss: 2.304133  [ 3200/481450]\n",
      "loss: 2.297894  [ 6400/481450]\n",
      "loss: 2.294239  [ 9600/481450]\n",
      "loss: 2.293010  [12800/481450]\n",
      "loss: 2.294630  [16000/481450]\n",
      "loss: 2.291759  [19200/481450]\n",
      "loss: 2.258715  [22400/481450]\n",
      "loss: 2.281751  [25600/481450]\n",
      "loss: 2.213294  [28800/481450]\n",
      "loss: 2.157953  [32000/481450]\n",
      "loss: 2.132128  [35200/481450]\n",
      "loss: 2.056380  [38400/481450]\n",
      "loss: 1.993598  [41600/481450]\n",
      "loss: 2.037606  [44800/481450]\n",
      "loss: 1.933084  [48000/481450]\n",
      "loss: 1.902035  [51200/481450]\n",
      "loss: 1.932992  [54400/481450]\n",
      "loss: 1.783628  [57600/481450]\n",
      "loss: 1.859918  [60800/481450]\n",
      "loss: 1.726900  [64000/481450]\n",
      "loss: 1.751661  [67200/481450]\n",
      "loss: 1.753732  [70400/481450]\n",
      "loss: 1.749011  [73600/481450]\n",
      "loss: 1.622834  [76800/481450]\n",
      "loss: 1.654081  [80000/481450]\n",
      "loss: 1.555909  [83200/481450]\n",
      "loss: 1.648704  [86400/481450]\n",
      "loss: 1.520402  [89600/481450]\n",
      "loss: 1.494369  [92800/481450]\n",
      "loss: 1.496597  [96000/481450]\n",
      "loss: 1.464365  [99200/481450]\n",
      "loss: 1.423508  [102400/481450]\n",
      "loss: 1.596869  [105600/481450]\n",
      "loss: 1.401448  [108800/481450]\n",
      "loss: 1.342897  [112000/481450]\n",
      "loss: 1.428689  [115200/481450]\n",
      "loss: 1.419925  [118400/481450]\n",
      "loss: 1.260768  [121600/481450]\n",
      "loss: 1.365284  [124800/481450]\n",
      "loss: 1.404295  [128000/481450]\n",
      "loss: 1.190488  [131200/481450]\n",
      "loss: 1.082269  [134400/481450]\n",
      "loss: 1.180974  [137600/481450]\n",
      "loss: 1.137061  [140800/481450]\n",
      "loss: 1.255072  [144000/481450]\n",
      "loss: 1.280643  [147200/481450]\n",
      "loss: 1.165350  [150400/481450]\n",
      "loss: 1.128417  [153600/481450]\n",
      "loss: 0.855302  [156800/481450]\n",
      "loss: 0.812567  [160000/481450]\n",
      "loss: 1.252156  [163200/481450]\n",
      "loss: 1.068294  [166400/481450]\n",
      "loss: 0.922093  [169600/481450]\n",
      "loss: 1.075138  [172800/481450]\n",
      "loss: 0.874635  [176000/481450]\n",
      "loss: 0.948802  [179200/481450]\n",
      "loss: 0.975281  [182400/481450]\n",
      "loss: 0.982467  [185600/481450]\n",
      "loss: 1.057923  [188800/481450]\n",
      "loss: 0.943242  [192000/481450]\n",
      "loss: 1.042360  [195200/481450]\n",
      "loss: 0.807654  [198400/481450]\n",
      "loss: 1.178262  [201600/481450]\n",
      "loss: 1.039080  [204800/481450]\n",
      "loss: 0.975051  [208000/481450]\n",
      "loss: 0.797708  [211200/481450]\n",
      "loss: 0.775088  [214400/481450]\n",
      "loss: 0.892778  [217600/481450]\n",
      "loss: 0.950593  [220800/481450]\n",
      "loss: 0.995704  [224000/481450]\n",
      "loss: 0.930232  [227200/481450]\n",
      "loss: 0.882543  [230400/481450]\n",
      "loss: 0.803611  [233600/481450]\n",
      "loss: 0.748107  [236800/481450]\n",
      "loss: 0.823961  [240000/481450]\n",
      "loss: 0.885103  [243200/481450]\n",
      "loss: 0.746891  [246400/481450]\n",
      "loss: 0.881002  [249600/481450]\n",
      "loss: 0.879453  [252800/481450]\n",
      "loss: 0.928353  [256000/481450]\n",
      "loss: 0.547119  [259200/481450]\n",
      "loss: 0.794856  [262400/481450]\n",
      "loss: 0.735078  [265600/481450]\n",
      "loss: 0.690576  [268800/481450]\n",
      "loss: 0.791713  [272000/481450]\n",
      "loss: 0.869659  [275200/481450]\n",
      "loss: 0.840766  [278400/481450]\n",
      "loss: 0.716648  [281600/481450]\n",
      "loss: 0.768382  [284800/481450]\n",
      "loss: 0.545468  [288000/481450]\n",
      "loss: 0.872690  [291200/481450]\n",
      "loss: 0.826619  [294400/481450]\n",
      "loss: 0.847056  [297600/481450]\n",
      "loss: 0.654451  [300800/481450]\n",
      "loss: 1.005961  [304000/481450]\n",
      "loss: 0.758457  [307200/481450]\n",
      "loss: 0.860635  [310400/481450]\n",
      "loss: 0.602703  [313600/481450]\n",
      "loss: 0.669126  [316800/481450]\n",
      "loss: 1.008739  [320000/481450]\n",
      "loss: 0.775535  [323200/481450]\n",
      "loss: 0.796331  [326400/481450]\n",
      "loss: 0.620879  [329600/481450]\n",
      "loss: 0.856719  [332800/481450]\n",
      "loss: 0.633773  [336000/481450]\n",
      "loss: 0.610015  [339200/481450]\n",
      "loss: 0.512934  [342400/481450]\n",
      "loss: 0.543727  [345600/481450]\n",
      "loss: 0.716247  [348800/481450]\n",
      "loss: 0.841256  [352000/481450]\n",
      "loss: 0.857334  [355200/481450]\n",
      "loss: 0.975227  [358400/481450]\n",
      "loss: 0.722352  [361600/481450]\n",
      "loss: 0.561238  [364800/481450]\n",
      "loss: 0.859301  [368000/481450]\n",
      "loss: 0.811852  [371200/481450]\n",
      "loss: 0.632307  [374400/481450]\n",
      "loss: 0.663390  [377600/481450]\n",
      "loss: 0.661015  [380800/481450]\n",
      "loss: 0.626912  [384000/481450]\n",
      "loss: 0.830757  [387200/481450]\n",
      "loss: 0.824899  [390400/481450]\n",
      "loss: 0.650700  [393600/481450]\n",
      "loss: 0.499439  [396800/481450]\n",
      "loss: 0.653548  [400000/481450]\n",
      "loss: 0.731687  [403200/481450]\n",
      "loss: 0.996662  [406400/481450]\n",
      "loss: 0.678615  [409600/481450]\n",
      "loss: 0.580467  [412800/481450]\n",
      "loss: 0.476464  [416000/481450]\n",
      "loss: 0.615233  [419200/481450]\n",
      "loss: 0.716704  [422400/481450]\n",
      "loss: 0.828314  [425600/481450]\n",
      "loss: 0.744968  [428800/481450]\n",
      "loss: 0.514197  [432000/481450]\n",
      "loss: 0.633656  [435200/481450]\n",
      "loss: 0.592020  [438400/481450]\n",
      "loss: 0.472851  [441600/481450]\n",
      "loss: 0.518288  [444800/481450]\n",
      "loss: 0.652020  [448000/481450]\n",
      "loss: 0.670883  [451200/481450]\n",
      "loss: 0.671142  [454400/481450]\n",
      "loss: 0.569627  [457600/481450]\n",
      "loss: 0.501377  [460800/481450]\n",
      "loss: 0.574202  [464000/481450]\n",
      "loss: 0.510977  [467200/481450]\n",
      "loss: 0.908300  [470400/481450]\n",
      "loss: 0.875350  [473600/481450]\n",
      "loss: 0.625198  [476800/481450]\n",
      "loss: 0.455782  [480000/481450]\n",
      "Train Accuracy: 62.3926%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.594706, F1-score: 80.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.557794  [    0/481450]\n",
      "loss: 0.703320  [ 3200/481450]\n",
      "loss: 0.579759  [ 6400/481450]\n",
      "loss: 0.439117  [ 9600/481450]\n",
      "loss: 0.542094  [12800/481450]\n",
      "loss: 0.894564  [16000/481450]\n",
      "loss: 0.857356  [19200/481450]\n",
      "loss: 0.545533  [22400/481450]\n",
      "loss: 0.591782  [25600/481450]\n",
      "loss: 0.580753  [28800/481450]\n",
      "loss: 0.717353  [32000/481450]\n",
      "loss: 0.337054  [35200/481450]\n",
      "loss: 0.431033  [38400/481450]\n",
      "loss: 0.940711  [41600/481450]\n",
      "loss: 0.549631  [44800/481450]\n",
      "loss: 0.543314  [48000/481450]\n",
      "loss: 0.448041  [51200/481450]\n",
      "loss: 0.711517  [54400/481450]\n",
      "loss: 0.570732  [57600/481450]\n",
      "loss: 0.544091  [60800/481450]\n",
      "loss: 0.953807  [64000/481450]\n",
      "loss: 0.747723  [67200/481450]\n",
      "loss: 0.615590  [70400/481450]\n",
      "loss: 0.406011  [73600/481450]\n",
      "loss: 0.282164  [76800/481450]\n",
      "loss: 0.461356  [80000/481450]\n",
      "loss: 0.409172  [83200/481450]\n",
      "loss: 0.554900  [86400/481450]\n",
      "loss: 0.655743  [89600/481450]\n",
      "loss: 0.569093  [92800/481450]\n",
      "loss: 0.656567  [96000/481450]\n",
      "loss: 0.643837  [99200/481450]\n",
      "loss: 0.461320  [102400/481450]\n",
      "loss: 0.571920  [105600/481450]\n",
      "loss: 0.678911  [108800/481450]\n",
      "loss: 0.388218  [112000/481450]\n",
      "loss: 0.580619  [115200/481450]\n",
      "loss: 0.547854  [118400/481450]\n",
      "loss: 0.548533  [121600/481450]\n",
      "loss: 0.592482  [124800/481450]\n",
      "loss: 0.704020  [128000/481450]\n",
      "loss: 0.542950  [131200/481450]\n",
      "loss: 0.329963  [134400/481450]\n",
      "loss: 0.381792  [137600/481450]\n",
      "loss: 0.462404  [140800/481450]\n",
      "loss: 0.422352  [144000/481450]\n",
      "loss: 0.444156  [147200/481450]\n",
      "loss: 0.472004  [150400/481450]\n",
      "loss: 0.617432  [153600/481450]\n",
      "loss: 0.358813  [156800/481450]\n",
      "loss: 0.495874  [160000/481450]\n",
      "loss: 0.534643  [163200/481450]\n",
      "loss: 0.783249  [166400/481450]\n",
      "loss: 0.413029  [169600/481450]\n",
      "loss: 0.520261  [172800/481450]\n",
      "loss: 0.372397  [176000/481450]\n",
      "loss: 0.447243  [179200/481450]\n",
      "loss: 0.335316  [182400/481450]\n",
      "loss: 0.730708  [185600/481450]\n",
      "loss: 0.571753  [188800/481450]\n",
      "loss: 0.345237  [192000/481450]\n",
      "loss: 0.411565  [195200/481450]\n",
      "loss: 0.654835  [198400/481450]\n",
      "loss: 0.494413  [201600/481450]\n",
      "loss: 0.252515  [204800/481450]\n",
      "loss: 0.674506  [208000/481450]\n",
      "loss: 0.483153  [211200/481450]\n",
      "loss: 0.630107  [214400/481450]\n",
      "loss: 0.356822  [217600/481450]\n",
      "loss: 0.549951  [220800/481450]\n",
      "loss: 0.472647  [224000/481450]\n",
      "loss: 0.428646  [227200/481450]\n",
      "loss: 0.371250  [230400/481450]\n",
      "loss: 0.672740  [233600/481450]\n",
      "loss: 0.457616  [236800/481450]\n",
      "loss: 0.522830  [240000/481450]\n",
      "loss: 0.562546  [243200/481450]\n",
      "loss: 0.431905  [246400/481450]\n",
      "loss: 0.481606  [249600/481450]\n",
      "loss: 0.362056  [252800/481450]\n",
      "loss: 0.327519  [256000/481450]\n",
      "loss: 0.307785  [259200/481450]\n",
      "loss: 0.464719  [262400/481450]\n",
      "loss: 0.539711  [265600/481450]\n",
      "loss: 0.529728  [268800/481450]\n",
      "loss: 0.427240  [272000/481450]\n",
      "loss: 0.797124  [275200/481450]\n",
      "loss: 0.524390  [278400/481450]\n",
      "loss: 0.322839  [281600/481450]\n",
      "loss: 0.562008  [284800/481450]\n",
      "loss: 0.245596  [288000/481450]\n",
      "loss: 0.452374  [291200/481450]\n",
      "loss: 0.433382  [294400/481450]\n",
      "loss: 0.413295  [297600/481450]\n",
      "loss: 0.476352  [300800/481450]\n",
      "loss: 0.193892  [304000/481450]\n",
      "loss: 0.442151  [307200/481450]\n",
      "loss: 0.396943  [310400/481450]\n",
      "loss: 0.330259  [313600/481450]\n",
      "loss: 0.784217  [316800/481450]\n",
      "loss: 0.418683  [320000/481450]\n",
      "loss: 0.285655  [323200/481450]\n",
      "loss: 0.598416  [326400/481450]\n",
      "loss: 0.322908  [329600/481450]\n",
      "loss: 0.362176  [332800/481450]\n",
      "loss: 0.616570  [336000/481450]\n",
      "loss: 0.715215  [339200/481450]\n",
      "loss: 0.549902  [342400/481450]\n",
      "loss: 0.268243  [345600/481450]\n",
      "loss: 0.518778  [348800/481450]\n",
      "loss: 0.316710  [352000/481450]\n",
      "loss: 0.284108  [355200/481450]\n",
      "loss: 0.358198  [358400/481450]\n",
      "loss: 0.328598  [361600/481450]\n",
      "loss: 0.286163  [364800/481450]\n",
      "loss: 0.433024  [368000/481450]\n",
      "loss: 0.467497  [371200/481450]\n",
      "loss: 0.569380  [374400/481450]\n",
      "loss: 0.480737  [377600/481450]\n",
      "loss: 0.423669  [380800/481450]\n",
      "loss: 0.822450  [384000/481450]\n",
      "loss: 0.391290  [387200/481450]\n",
      "loss: 0.555252  [390400/481450]\n",
      "loss: 0.317106  [393600/481450]\n",
      "loss: 0.499473  [396800/481450]\n",
      "loss: 0.520916  [400000/481450]\n",
      "loss: 0.372970  [403200/481450]\n",
      "loss: 0.383539  [406400/481450]\n",
      "loss: 0.757045  [409600/481450]\n",
      "loss: 0.713838  [412800/481450]\n",
      "loss: 0.555808  [416000/481450]\n",
      "loss: 0.550557  [419200/481450]\n",
      "loss: 0.454939  [422400/481450]\n",
      "loss: 0.269431  [425600/481450]\n",
      "loss: 0.240097  [428800/481450]\n",
      "loss: 0.566077  [432000/481450]\n",
      "loss: 0.524944  [435200/481450]\n",
      "loss: 0.465255  [438400/481450]\n",
      "loss: 1.010519  [441600/481450]\n",
      "loss: 0.637622  [444800/481450]\n",
      "loss: 0.469068  [448000/481450]\n",
      "loss: 0.323514  [451200/481450]\n",
      "loss: 0.169544  [454400/481450]\n",
      "loss: 0.134317  [457600/481450]\n",
      "loss: 0.419482  [460800/481450]\n",
      "loss: 0.537873  [464000/481450]\n",
      "loss: 0.353584  [467200/481450]\n",
      "loss: 0.243835  [470400/481450]\n",
      "loss: 0.271080  [473600/481450]\n",
      "loss: 0.245908  [476800/481450]\n",
      "loss: 0.439728  [480000/481450]\n",
      "Train Accuracy: 81.6654%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.755197, F1-score: 79.23% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.551525  [    0/481450]\n",
      "loss: 0.904138  [ 3200/481450]\n",
      "loss: 0.320675  [ 6400/481450]\n",
      "loss: 0.463757  [ 9600/481450]\n",
      "loss: 0.428013  [12800/481450]\n",
      "loss: 0.383397  [16000/481450]\n",
      "loss: 0.428408  [19200/481450]\n",
      "loss: 0.373653  [22400/481450]\n",
      "loss: 0.259516  [25600/481450]\n",
      "loss: 0.335117  [28800/481450]\n",
      "loss: 0.764580  [32000/481450]\n",
      "loss: 0.408928  [35200/481450]\n",
      "loss: 0.365838  [38400/481450]\n",
      "loss: 0.438799  [41600/481450]\n",
      "loss: 0.828317  [44800/481450]\n",
      "loss: 0.597631  [48000/481450]\n",
      "loss: 0.499950  [51200/481450]\n",
      "loss: 0.515783  [54400/481450]\n",
      "loss: 0.552362  [57600/481450]\n",
      "loss: 0.367032  [60800/481450]\n",
      "loss: 0.365312  [64000/481450]\n",
      "loss: 0.431054  [67200/481450]\n",
      "loss: 0.624206  [70400/481450]\n",
      "loss: 0.470158  [73600/481450]\n",
      "loss: 0.612460  [76800/481450]\n",
      "loss: 0.318562  [80000/481450]\n",
      "loss: 0.440195  [83200/481450]\n",
      "loss: 0.398661  [86400/481450]\n",
      "loss: 0.507635  [89600/481450]\n",
      "loss: 0.293095  [92800/481450]\n",
      "loss: 0.369828  [96000/481450]\n",
      "loss: 0.322760  [99200/481450]\n",
      "loss: 0.421001  [102400/481450]\n",
      "loss: 0.168119  [105600/481450]\n",
      "loss: 0.553238  [108800/481450]\n",
      "loss: 0.304819  [112000/481450]\n",
      "loss: 0.444947  [115200/481450]\n",
      "loss: 0.607041  [118400/481450]\n",
      "loss: 0.700249  [121600/481450]\n",
      "loss: 0.384428  [124800/481450]\n",
      "loss: 0.433081  [128000/481450]\n",
      "loss: 0.327477  [131200/481450]\n",
      "loss: 0.323850  [134400/481450]\n",
      "loss: 0.315886  [137600/481450]\n",
      "loss: 0.315815  [140800/481450]\n",
      "loss: 0.603689  [144000/481450]\n",
      "loss: 0.446987  [147200/481450]\n",
      "loss: 0.463501  [150400/481450]\n",
      "loss: 0.273994  [153600/481450]\n",
      "loss: 0.432089  [156800/481450]\n",
      "loss: 0.646111  [160000/481450]\n",
      "loss: 0.261999  [163200/481450]\n",
      "loss: 0.594787  [166400/481450]\n",
      "loss: 0.543988  [169600/481450]\n",
      "loss: 0.261814  [172800/481450]\n",
      "loss: 0.369024  [176000/481450]\n",
      "loss: 0.649658  [179200/481450]\n",
      "loss: 0.236865  [182400/481450]\n",
      "loss: 0.333026  [185600/481450]\n",
      "loss: 0.438567  [188800/481450]\n",
      "loss: 0.337657  [192000/481450]\n",
      "loss: 0.452009  [195200/481450]\n",
      "loss: 0.269232  [198400/481450]\n",
      "loss: 0.552310  [201600/481450]\n",
      "loss: 0.801118  [204800/481450]\n",
      "loss: 0.387382  [208000/481450]\n",
      "loss: 0.411948  [211200/481450]\n",
      "loss: 0.308402  [214400/481450]\n",
      "loss: 0.444229  [217600/481450]\n",
      "loss: 0.299193  [220800/481450]\n",
      "loss: 0.204050  [224000/481450]\n",
      "loss: 0.392476  [227200/481450]\n",
      "loss: 0.342130  [230400/481450]\n",
      "loss: 0.661535  [233600/481450]\n",
      "loss: 0.831007  [236800/481450]\n",
      "loss: 0.258305  [240000/481450]\n",
      "loss: 0.510929  [243200/481450]\n",
      "loss: 0.452430  [246400/481450]\n",
      "loss: 0.435445  [249600/481450]\n",
      "loss: 0.267553  [252800/481450]\n",
      "loss: 0.408211  [256000/481450]\n",
      "loss: 0.467984  [259200/481450]\n",
      "loss: 0.175782  [262400/481450]\n",
      "loss: 0.570092  [265600/481450]\n",
      "loss: 0.352579  [268800/481450]\n",
      "loss: 0.708714  [272000/481450]\n",
      "loss: 0.346743  [275200/481450]\n",
      "loss: 0.430373  [278400/481450]\n",
      "loss: 0.493009  [281600/481450]\n",
      "loss: 0.212483  [284800/481450]\n",
      "loss: 0.344761  [288000/481450]\n",
      "loss: 0.225687  [291200/481450]\n",
      "loss: 0.570436  [294400/481450]\n",
      "loss: 0.231040  [297600/481450]\n",
      "loss: 0.304599  [300800/481450]\n",
      "loss: 0.511050  [304000/481450]\n",
      "loss: 0.245721  [307200/481450]\n",
      "loss: 0.413825  [310400/481450]\n",
      "loss: 0.525868  [313600/481450]\n",
      "loss: 0.432846  [316800/481450]\n",
      "loss: 0.386294  [320000/481450]\n",
      "loss: 0.345527  [323200/481450]\n",
      "loss: 0.630391  [326400/481450]\n",
      "loss: 0.285970  [329600/481450]\n",
      "loss: 0.454974  [332800/481450]\n",
      "loss: 0.276706  [336000/481450]\n",
      "loss: 0.317968  [339200/481450]\n",
      "loss: 0.558259  [342400/481450]\n",
      "loss: 0.490272  [345600/481450]\n",
      "loss: 0.275813  [348800/481450]\n",
      "loss: 0.256627  [352000/481450]\n",
      "loss: 0.394456  [355200/481450]\n",
      "loss: 0.570927  [358400/481450]\n",
      "loss: 0.384300  [361600/481450]\n",
      "loss: 0.566688  [364800/481450]\n",
      "loss: 0.153884  [368000/481450]\n",
      "loss: 0.233837  [371200/481450]\n",
      "loss: 0.283120  [374400/481450]\n",
      "loss: 0.395697  [377600/481450]\n",
      "loss: 0.418851  [380800/481450]\n",
      "loss: 0.383558  [384000/481450]\n",
      "loss: 0.209850  [387200/481450]\n",
      "loss: 0.439723  [390400/481450]\n",
      "loss: 0.518268  [393600/481450]\n",
      "loss: 0.517628  [396800/481450]\n",
      "loss: 0.419957  [400000/481450]\n",
      "loss: 0.466081  [403200/481450]\n",
      "loss: 0.294193  [406400/481450]\n",
      "loss: 0.561364  [409600/481450]\n",
      "loss: 0.318054  [412800/481450]\n",
      "loss: 0.335849  [416000/481450]\n",
      "loss: 0.271655  [419200/481450]\n",
      "loss: 0.222327  [422400/481450]\n",
      "loss: 0.669460  [425600/481450]\n",
      "loss: 0.603224  [428800/481450]\n",
      "loss: 0.339609  [432000/481450]\n",
      "loss: 0.202220  [435200/481450]\n",
      "loss: 0.347285  [438400/481450]\n",
      "loss: 0.390454  [441600/481450]\n",
      "loss: 0.580184  [444800/481450]\n",
      "loss: 0.462124  [448000/481450]\n",
      "loss: 0.351822  [451200/481450]\n",
      "loss: 0.489358  [454400/481450]\n",
      "loss: 0.278123  [457600/481450]\n",
      "loss: 0.480375  [460800/481450]\n",
      "loss: 0.535702  [464000/481450]\n",
      "loss: 0.297501  [467200/481450]\n",
      "loss: 0.315968  [470400/481450]\n",
      "loss: 0.353895  [473600/481450]\n",
      "loss: 0.296512  [476800/481450]\n",
      "loss: 0.303326  [480000/481450]\n",
      "Train Accuracy: 84.5573%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.811996, F1-score: 79.96% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.239192  [    0/481450]\n",
      "loss: 0.366300  [ 3200/481450]\n",
      "loss: 0.177258  [ 6400/481450]\n",
      "loss: 0.714126  [ 9600/481450]\n",
      "loss: 0.322908  [12800/481450]\n",
      "loss: 0.383352  [16000/481450]\n",
      "loss: 0.503995  [19200/481450]\n",
      "loss: 0.287172  [22400/481450]\n",
      "loss: 0.512793  [25600/481450]\n",
      "loss: 0.510639  [28800/481450]\n",
      "loss: 0.546862  [32000/481450]\n",
      "loss: 0.263758  [35200/481450]\n",
      "loss: 0.191515  [38400/481450]\n",
      "loss: 0.472446  [41600/481450]\n",
      "loss: 0.485314  [44800/481450]\n",
      "loss: 0.277787  [48000/481450]\n",
      "loss: 0.282423  [51200/481450]\n",
      "loss: 0.313429  [54400/481450]\n",
      "loss: 0.376169  [57600/481450]\n",
      "loss: 0.258315  [60800/481450]\n",
      "loss: 0.417206  [64000/481450]\n",
      "loss: 0.381193  [67200/481450]\n",
      "loss: 0.122945  [70400/481450]\n",
      "loss: 0.354251  [73600/481450]\n",
      "loss: 0.381391  [76800/481450]\n",
      "loss: 0.566859  [80000/481450]\n",
      "loss: 0.581050  [83200/481450]\n",
      "loss: 0.338000  [86400/481450]\n",
      "loss: 0.461926  [89600/481450]\n",
      "loss: 0.349149  [92800/481450]\n",
      "loss: 0.226273  [96000/481450]\n",
      "loss: 0.328814  [99200/481450]\n",
      "loss: 0.336865  [102400/481450]\n",
      "loss: 0.501783  [105600/481450]\n",
      "loss: 0.444101  [108800/481450]\n",
      "loss: 0.715011  [112000/481450]\n",
      "loss: 0.366202  [115200/481450]\n",
      "loss: 0.210532  [118400/481450]\n",
      "loss: 0.286412  [121600/481450]\n",
      "loss: 0.354037  [124800/481450]\n",
      "loss: 0.221723  [128000/481450]\n",
      "loss: 0.344946  [131200/481450]\n",
      "loss: 0.221336  [134400/481450]\n",
      "loss: 0.468167  [137600/481450]\n",
      "loss: 0.353677  [140800/481450]\n",
      "loss: 0.224149  [144000/481450]\n",
      "loss: 0.278066  [147200/481450]\n",
      "loss: 0.348167  [150400/481450]\n",
      "loss: 0.544758  [153600/481450]\n",
      "loss: 0.404894  [156800/481450]\n",
      "loss: 0.320656  [160000/481450]\n",
      "loss: 0.603934  [163200/481450]\n",
      "loss: 0.653088  [166400/481450]\n",
      "loss: 0.448094  [169600/481450]\n",
      "loss: 0.431476  [172800/481450]\n",
      "loss: 0.466896  [176000/481450]\n",
      "loss: 0.416902  [179200/481450]\n",
      "loss: 0.183760  [182400/481450]\n",
      "loss: 0.238624  [185600/481450]\n",
      "loss: 0.349842  [188800/481450]\n",
      "loss: 0.180840  [192000/481450]\n",
      "loss: 0.231598  [195200/481450]\n",
      "loss: 0.365321  [198400/481450]\n",
      "loss: 0.225660  [201600/481450]\n",
      "loss: 0.355034  [204800/481450]\n",
      "loss: 0.414430  [208000/481450]\n",
      "loss: 0.109930  [211200/481450]\n",
      "loss: 0.412229  [214400/481450]\n",
      "loss: 0.345114  [217600/481450]\n",
      "loss: 0.353190  [220800/481450]\n",
      "loss: 0.596220  [224000/481450]\n",
      "loss: 0.582473  [227200/481450]\n",
      "loss: 0.294048  [230400/481450]\n",
      "loss: 0.260967  [233600/481450]\n",
      "loss: 0.415496  [236800/481450]\n",
      "loss: 0.361836  [240000/481450]\n",
      "loss: 0.378402  [243200/481450]\n",
      "loss: 0.550943  [246400/481450]\n",
      "loss: 0.273852  [249600/481450]\n",
      "loss: 0.360063  [252800/481450]\n",
      "loss: 0.391107  [256000/481450]\n",
      "loss: 0.337410  [259200/481450]\n",
      "loss: 0.303402  [262400/481450]\n",
      "loss: 0.573124  [265600/481450]\n",
      "loss: 0.464645  [268800/481450]\n",
      "loss: 0.366693  [272000/481450]\n",
      "loss: 0.302659  [275200/481450]\n",
      "loss: 0.266604  [278400/481450]\n",
      "loss: 0.505580  [281600/481450]\n",
      "loss: 0.356431  [284800/481450]\n",
      "loss: 0.450527  [288000/481450]\n",
      "loss: 0.246329  [291200/481450]\n",
      "loss: 0.460383  [294400/481450]\n",
      "loss: 0.280143  [297600/481450]\n",
      "loss: 0.386674  [300800/481450]\n",
      "loss: 0.278365  [304000/481450]\n",
      "loss: 0.393918  [307200/481450]\n",
      "loss: 0.246421  [310400/481450]\n",
      "loss: 0.326343  [313600/481450]\n",
      "loss: 0.435965  [316800/481450]\n",
      "loss: 0.254165  [320000/481450]\n",
      "loss: 0.381016  [323200/481450]\n",
      "loss: 0.312315  [326400/481450]\n",
      "loss: 0.400338  [329600/481450]\n",
      "loss: 0.298564  [332800/481450]\n",
      "loss: 0.516251  [336000/481450]\n",
      "loss: 0.431648  [339200/481450]\n",
      "loss: 0.618115  [342400/481450]\n",
      "loss: 0.241977  [345600/481450]\n",
      "loss: 0.167955  [348800/481450]\n",
      "loss: 0.159823  [352000/481450]\n",
      "loss: 0.258205  [355200/481450]\n",
      "loss: 0.191538  [358400/481450]\n",
      "loss: 0.382853  [361600/481450]\n",
      "loss: 0.283410  [364800/481450]\n",
      "loss: 0.324253  [368000/481450]\n",
      "loss: 0.332723  [371200/481450]\n",
      "loss: 0.314537  [374400/481450]\n",
      "loss: 0.370949  [377600/481450]\n",
      "loss: 0.135440  [380800/481450]\n",
      "loss: 0.437877  [384000/481450]\n",
      "loss: 0.347712  [387200/481450]\n",
      "loss: 0.180870  [390400/481450]\n",
      "loss: 0.296337  [393600/481450]\n",
      "loss: 0.296500  [396800/481450]\n",
      "loss: 0.327763  [400000/481450]\n",
      "loss: 0.222641  [403200/481450]\n",
      "loss: 0.247930  [406400/481450]\n",
      "loss: 0.279854  [409600/481450]\n",
      "loss: 0.385986  [412800/481450]\n",
      "loss: 0.201703  [416000/481450]\n",
      "loss: 0.342590  [419200/481450]\n",
      "loss: 0.485097  [422400/481450]\n",
      "loss: 0.451371  [425600/481450]\n",
      "loss: 0.628112  [428800/481450]\n",
      "loss: 0.214884  [432000/481450]\n",
      "loss: 0.602448  [435200/481450]\n",
      "loss: 0.044693  [438400/481450]\n",
      "loss: 0.342837  [441600/481450]\n",
      "loss: 0.323808  [444800/481450]\n",
      "loss: 0.683302  [448000/481450]\n",
      "loss: 0.345833  [451200/481450]\n",
      "loss: 0.272098  [454400/481450]\n",
      "loss: 0.380026  [457600/481450]\n",
      "loss: 0.391500  [460800/481450]\n",
      "loss: 0.438593  [464000/481450]\n",
      "loss: 0.400398  [467200/481450]\n",
      "loss: 0.303408  [470400/481450]\n",
      "loss: 0.356970  [473600/481450]\n",
      "loss: 0.429435  [476800/481450]\n",
      "loss: 0.163699  [480000/481450]\n",
      "Train Accuracy: 85.7483%\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.822251, F1-score: 80.87% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.274851  [    0/481450]\n",
      "loss: 0.227143  [ 3200/481450]\n",
      "loss: 0.286221  [ 6400/481450]\n",
      "loss: 0.359536  [ 9600/481450]\n",
      "loss: 0.412644  [12800/481450]\n",
      "loss: 0.607048  [16000/481450]\n",
      "loss: 0.184201  [19200/481450]\n",
      "loss: 0.280082  [22400/481450]\n",
      "loss: 0.279483  [25600/481450]\n",
      "loss: 0.323288  [28800/481450]\n",
      "loss: 0.287587  [32000/481450]\n",
      "loss: 0.190525  [35200/481450]\n",
      "loss: 0.140478  [38400/481450]\n",
      "loss: 0.398756  [41600/481450]\n",
      "loss: 0.248287  [44800/481450]\n",
      "loss: 0.379522  [48000/481450]\n",
      "loss: 0.255665  [51200/481450]\n",
      "loss: 0.169422  [54400/481450]\n",
      "loss: 0.440934  [57600/481450]\n",
      "loss: 0.306924  [60800/481450]\n",
      "loss: 0.367311  [64000/481450]\n",
      "loss: 0.387948  [67200/481450]\n",
      "loss: 0.320224  [70400/481450]\n",
      "loss: 0.292507  [73600/481450]\n",
      "loss: 0.273556  [76800/481450]\n",
      "loss: 0.357887  [80000/481450]\n",
      "loss: 0.380741  [83200/481450]\n",
      "loss: 0.235292  [86400/481450]\n",
      "loss: 0.323773  [89600/481450]\n",
      "loss: 0.278276  [92800/481450]\n",
      "loss: 0.317258  [96000/481450]\n",
      "loss: 0.451004  [99200/481450]\n",
      "loss: 0.369339  [102400/481450]\n",
      "loss: 0.242162  [105600/481450]\n",
      "loss: 0.238101  [108800/481450]\n",
      "loss: 0.342211  [112000/481450]\n",
      "loss: 0.389430  [115200/481450]\n",
      "loss: 0.343791  [118400/481450]\n",
      "loss: 0.362546  [121600/481450]\n",
      "loss: 0.196389  [124800/481450]\n",
      "loss: 0.711174  [128000/481450]\n",
      "loss: 0.324288  [131200/481450]\n",
      "loss: 0.236575  [134400/481450]\n",
      "loss: 0.354344  [137600/481450]\n",
      "loss: 0.469032  [140800/481450]\n",
      "loss: 0.632998  [144000/481450]\n",
      "loss: 0.476119  [147200/481450]\n",
      "loss: 0.366150  [150400/481450]\n",
      "loss: 0.269293  [153600/481450]\n",
      "loss: 0.372102  [156800/481450]\n",
      "loss: 0.451899  [160000/481450]\n",
      "loss: 0.423510  [163200/481450]\n",
      "loss: 0.194072  [166400/481450]\n",
      "loss: 0.390356  [169600/481450]\n",
      "loss: 0.428107  [172800/481450]\n",
      "loss: 0.378927  [176000/481450]\n",
      "loss: 0.637119  [179200/481450]\n",
      "loss: 0.240568  [182400/481450]\n",
      "loss: 0.484042  [185600/481450]\n",
      "loss: 0.290518  [188800/481450]\n",
      "loss: 0.451415  [192000/481450]\n",
      "loss: 0.357440  [195200/481450]\n",
      "loss: 0.503948  [198400/481450]\n",
      "loss: 0.413113  [201600/481450]\n",
      "loss: 0.311955  [204800/481450]\n",
      "loss: 0.374946  [208000/481450]\n",
      "loss: 0.192614  [211200/481450]\n",
      "loss: 0.291376  [214400/481450]\n",
      "loss: 0.555063  [217600/481450]\n",
      "loss: 0.547239  [220800/481450]\n",
      "loss: 0.338166  [224000/481450]\n",
      "loss: 0.442819  [227200/481450]\n",
      "loss: 0.202642  [230400/481450]\n",
      "loss: 0.186799  [233600/481450]\n",
      "loss: 0.285941  [236800/481450]\n",
      "loss: 0.255879  [240000/481450]\n",
      "loss: 0.123759  [243200/481450]\n",
      "loss: 0.434165  [246400/481450]\n",
      "loss: 0.580291  [249600/481450]\n",
      "loss: 0.274573  [252800/481450]\n",
      "loss: 0.268281  [256000/481450]\n",
      "loss: 0.294056  [259200/481450]\n",
      "loss: 0.277457  [262400/481450]\n",
      "loss: 0.378392  [265600/481450]\n",
      "loss: 0.392663  [268800/481450]\n",
      "loss: 0.398515  [272000/481450]\n",
      "loss: 0.441296  [275200/481450]\n",
      "loss: 0.368957  [278400/481450]\n",
      "loss: 0.533911  [281600/481450]\n",
      "loss: 0.580036  [284800/481450]\n",
      "loss: 0.399725  [288000/481450]\n",
      "loss: 0.235851  [291200/481450]\n",
      "loss: 0.457086  [294400/481450]\n",
      "loss: 0.339689  [297600/481450]\n",
      "loss: 0.330250  [300800/481450]\n",
      "loss: 0.246596  [304000/481450]\n",
      "loss: 0.274566  [307200/481450]\n",
      "loss: 0.195511  [310400/481450]\n",
      "loss: 0.268706  [313600/481450]\n",
      "loss: 0.214741  [316800/481450]\n",
      "loss: 0.261267  [320000/481450]\n",
      "loss: 0.374924  [323200/481450]\n",
      "loss: 0.543343  [326400/481450]\n",
      "loss: 0.329501  [329600/481450]\n",
      "loss: 0.424433  [332800/481450]\n",
      "loss: 0.221627  [336000/481450]\n",
      "loss: 0.164426  [339200/481450]\n",
      "loss: 0.439470  [342400/481450]\n",
      "loss: 0.298788  [345600/481450]\n",
      "loss: 0.187420  [348800/481450]\n",
      "loss: 0.347690  [352000/481450]\n",
      "loss: 0.384815  [355200/481450]\n",
      "loss: 0.207166  [358400/481450]\n",
      "loss: 0.336369  [361600/481450]\n",
      "loss: 0.369853  [364800/481450]\n",
      "loss: 0.479487  [368000/481450]\n",
      "loss: 0.261158  [371200/481450]\n",
      "loss: 0.270299  [374400/481450]\n",
      "loss: 0.236649  [377600/481450]\n",
      "loss: 0.234933  [380800/481450]\n",
      "loss: 0.350414  [384000/481450]\n",
      "loss: 0.227115  [387200/481450]\n",
      "loss: 0.218802  [390400/481450]\n",
      "loss: 0.496048  [393600/481450]\n",
      "loss: 0.393474  [396800/481450]\n",
      "loss: 0.277765  [400000/481450]\n",
      "loss: 0.600083  [403200/481450]\n",
      "loss: 0.266296  [406400/481450]\n",
      "loss: 0.660802  [409600/481450]\n",
      "loss: 0.615044  [412800/481450]\n",
      "loss: 0.210487  [416000/481450]\n",
      "loss: 0.238900  [419200/481450]\n",
      "loss: 0.118611  [422400/481450]\n",
      "loss: 0.186113  [425600/481450]\n",
      "loss: 0.353085  [428800/481450]\n",
      "loss: 0.422882  [432000/481450]\n",
      "loss: 0.158240  [435200/481450]\n",
      "loss: 0.167720  [438400/481450]\n",
      "loss: 0.409457  [441600/481450]\n",
      "loss: 0.649205  [444800/481450]\n",
      "loss: 0.461713  [448000/481450]\n",
      "loss: 0.430338  [451200/481450]\n",
      "loss: 0.654396  [454400/481450]\n",
      "loss: 0.346144  [457600/481450]\n",
      "loss: 0.470010  [460800/481450]\n",
      "loss: 0.215812  [464000/481450]\n",
      "loss: 0.382284  [467200/481450]\n",
      "loss: 0.222224  [470400/481450]\n",
      "loss: 0.419452  [473600/481450]\n",
      "loss: 0.264493  [476800/481450]\n",
      "loss: 0.151025  [480000/481450]\n",
      "Train Accuracy: 86.6470%\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.830657, F1-score: 81.82% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.449436  [    0/481450]\n",
      "loss: 0.410520  [ 3200/481450]\n",
      "loss: 0.520203  [ 6400/481450]\n",
      "loss: 0.489165  [ 9600/481450]\n",
      "loss: 0.190378  [12800/481450]\n",
      "loss: 0.285093  [16000/481450]\n",
      "loss: 0.208771  [19200/481450]\n",
      "loss: 0.404630  [22400/481450]\n",
      "loss: 0.176654  [25600/481450]\n",
      "loss: 0.181718  [28800/481450]\n",
      "loss: 0.170955  [32000/481450]\n",
      "loss: 0.452860  [35200/481450]\n",
      "loss: 0.246462  [38400/481450]\n",
      "loss: 0.192578  [41600/481450]\n",
      "loss: 0.432067  [44800/481450]\n",
      "loss: 0.488361  [48000/481450]\n",
      "loss: 0.415225  [51200/481450]\n",
      "loss: 0.376209  [54400/481450]\n",
      "loss: 0.259226  [57600/481450]\n",
      "loss: 0.394715  [60800/481450]\n",
      "loss: 0.292737  [64000/481450]\n",
      "loss: 0.227282  [67200/481450]\n",
      "loss: 0.317435  [70400/481450]\n",
      "loss: 0.308801  [73600/481450]\n",
      "loss: 0.136800  [76800/481450]\n",
      "loss: 0.148773  [80000/481450]\n",
      "loss: 0.421967  [83200/481450]\n",
      "loss: 0.242489  [86400/481450]\n",
      "loss: 0.266654  [89600/481450]\n",
      "loss: 0.532742  [92800/481450]\n",
      "loss: 0.321958  [96000/481450]\n",
      "loss: 0.213777  [99200/481450]\n",
      "loss: 0.394337  [102400/481450]\n",
      "loss: 0.373404  [105600/481450]\n",
      "loss: 0.214227  [108800/481450]\n",
      "loss: 0.383873  [112000/481450]\n",
      "loss: 0.253668  [115200/481450]\n",
      "loss: 0.447206  [118400/481450]\n",
      "loss: 0.399787  [121600/481450]\n",
      "loss: 0.100947  [124800/481450]\n",
      "loss: 0.285169  [128000/481450]\n",
      "loss: 0.191793  [131200/481450]\n",
      "loss: 0.302142  [134400/481450]\n",
      "loss: 0.247346  [137600/481450]\n",
      "loss: 0.380260  [140800/481450]\n",
      "loss: 0.221342  [144000/481450]\n",
      "loss: 0.241039  [147200/481450]\n",
      "loss: 0.354948  [150400/481450]\n",
      "loss: 0.210445  [153600/481450]\n",
      "loss: 0.414381  [156800/481450]\n",
      "loss: 0.381417  [160000/481450]\n",
      "loss: 0.273363  [163200/481450]\n",
      "loss: 0.645460  [166400/481450]\n",
      "loss: 0.164560  [169600/481450]\n",
      "loss: 0.415295  [172800/481450]\n",
      "loss: 0.452834  [176000/481450]\n",
      "loss: 0.269956  [179200/481450]\n",
      "loss: 0.411472  [182400/481450]\n",
      "loss: 0.281939  [185600/481450]\n",
      "loss: 0.312781  [188800/481450]\n",
      "loss: 0.376521  [192000/481450]\n",
      "loss: 0.382840  [195200/481450]\n",
      "loss: 0.364971  [198400/481450]\n",
      "loss: 0.407982  [201600/481450]\n",
      "loss: 0.309273  [204800/481450]\n",
      "loss: 0.420530  [208000/481450]\n",
      "loss: 0.266091  [211200/481450]\n",
      "loss: 0.305050  [214400/481450]\n",
      "loss: 0.238424  [217600/481450]\n",
      "loss: 0.241090  [220800/481450]\n",
      "loss: 0.199530  [224000/481450]\n",
      "loss: 0.402483  [227200/481450]\n",
      "loss: 0.247466  [230400/481450]\n",
      "loss: 0.226377  [233600/481450]\n",
      "loss: 0.179627  [236800/481450]\n",
      "loss: 0.198176  [240000/481450]\n",
      "loss: 0.191589  [243200/481450]\n",
      "loss: 0.375238  [246400/481450]\n",
      "loss: 0.360097  [249600/481450]\n",
      "loss: 0.251204  [252800/481450]\n",
      "loss: 0.268961  [256000/481450]\n",
      "loss: 0.249072  [259200/481450]\n",
      "loss: 0.313088  [262400/481450]\n",
      "loss: 0.425463  [265600/481450]\n",
      "loss: 0.259339  [268800/481450]\n",
      "loss: 0.292841  [272000/481450]\n",
      "loss: 0.554170  [275200/481450]\n",
      "loss: 0.307973  [278400/481450]\n",
      "loss: 0.526412  [281600/481450]\n",
      "loss: 0.596878  [284800/481450]\n",
      "loss: 0.307934  [288000/481450]\n",
      "loss: 0.606692  [291200/481450]\n",
      "loss: 0.502376  [294400/481450]\n",
      "loss: 0.514911  [297600/481450]\n",
      "loss: 0.333996  [300800/481450]\n",
      "loss: 0.334260  [304000/481450]\n",
      "loss: 0.304920  [307200/481450]\n",
      "loss: 0.404366  [310400/481450]\n",
      "loss: 0.188887  [313600/481450]\n",
      "loss: 0.240812  [316800/481450]\n",
      "loss: 0.416513  [320000/481450]\n",
      "loss: 0.258617  [323200/481450]\n",
      "loss: 0.218875  [326400/481450]\n",
      "loss: 0.367505  [329600/481450]\n",
      "loss: 0.581178  [332800/481450]\n",
      "loss: 0.688232  [336000/481450]\n",
      "loss: 0.387182  [339200/481450]\n",
      "loss: 0.206057  [342400/481450]\n",
      "loss: 0.289631  [345600/481450]\n",
      "loss: 0.230550  [348800/481450]\n",
      "loss: 0.193746  [352000/481450]\n",
      "loss: 0.429839  [355200/481450]\n",
      "loss: 0.253235  [358400/481450]\n",
      "loss: 0.376367  [361600/481450]\n",
      "loss: 0.418304  [364800/481450]\n",
      "loss: 0.346390  [368000/481450]\n",
      "loss: 0.228743  [371200/481450]\n",
      "loss: 0.471813  [374400/481450]\n",
      "loss: 0.165781  [377600/481450]\n",
      "loss: 0.234296  [380800/481450]\n",
      "loss: 0.242947  [384000/481450]\n",
      "loss: 0.254290  [387200/481450]\n",
      "loss: 0.573087  [390400/481450]\n",
      "loss: 0.282434  [393600/481450]\n",
      "loss: 0.307945  [396800/481450]\n",
      "loss: 0.475868  [400000/481450]\n",
      "loss: 0.330808  [403200/481450]\n",
      "loss: 0.353982  [406400/481450]\n",
      "loss: 0.333320  [409600/481450]\n",
      "loss: 0.749790  [412800/481450]\n",
      "loss: 0.237670  [416000/481450]\n",
      "loss: 0.479063  [419200/481450]\n",
      "loss: 0.234558  [422400/481450]\n",
      "loss: 0.398511  [425600/481450]\n",
      "loss: 0.169065  [428800/481450]\n",
      "loss: 0.398115  [432000/481450]\n",
      "loss: 0.285045  [435200/481450]\n",
      "loss: 0.650078  [438400/481450]\n",
      "loss: 0.433061  [441600/481450]\n",
      "loss: 0.391671  [444800/481450]\n",
      "loss: 0.353189  [448000/481450]\n",
      "loss: 0.133013  [451200/481450]\n",
      "loss: 0.194990  [454400/481450]\n",
      "loss: 0.386959  [457600/481450]\n",
      "loss: 0.166667  [460800/481450]\n",
      "loss: 0.233728  [464000/481450]\n",
      "loss: 0.325365  [467200/481450]\n",
      "loss: 0.360098  [470400/481450]\n",
      "loss: 0.186762  [473600/481450]\n",
      "loss: 0.346984  [476800/481450]\n",
      "loss: 0.442719  [480000/481450]\n",
      "Train Accuracy: 87.1881%\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.777186, F1-score: 83.00% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.243218  [    0/481450]\n",
      "loss: 0.369209  [ 3200/481450]\n",
      "loss: 0.228891  [ 6400/481450]\n",
      "loss: 0.264959  [ 9600/481450]\n",
      "loss: 0.366954  [12800/481450]\n",
      "loss: 0.143270  [16000/481450]\n",
      "loss: 0.398916  [19200/481450]\n",
      "loss: 0.200308  [22400/481450]\n",
      "loss: 0.190592  [25600/481450]\n",
      "loss: 0.392712  [28800/481450]\n",
      "loss: 0.777182  [32000/481450]\n",
      "loss: 0.299634  [35200/481450]\n",
      "loss: 0.346268  [38400/481450]\n",
      "loss: 0.334023  [41600/481450]\n",
      "loss: 0.416363  [44800/481450]\n",
      "loss: 0.179015  [48000/481450]\n",
      "loss: 0.259911  [51200/481450]\n",
      "loss: 0.180337  [54400/481450]\n",
      "loss: 0.400783  [57600/481450]\n",
      "loss: 0.176370  [60800/481450]\n",
      "loss: 0.545610  [64000/481450]\n",
      "loss: 0.074071  [67200/481450]\n",
      "loss: 0.466075  [70400/481450]\n",
      "loss: 0.238593  [73600/481450]\n",
      "loss: 0.608699  [76800/481450]\n",
      "loss: 0.408699  [80000/481450]\n",
      "loss: 0.208551  [83200/481450]\n",
      "loss: 0.219280  [86400/481450]\n",
      "loss: 0.245330  [89600/481450]\n",
      "loss: 0.349061  [92800/481450]\n",
      "loss: 0.175477  [96000/481450]\n",
      "loss: 0.195992  [99200/481450]\n",
      "loss: 0.290650  [102400/481450]\n",
      "loss: 0.278567  [105600/481450]\n",
      "loss: 0.546165  [108800/481450]\n",
      "loss: 0.289493  [112000/481450]\n",
      "loss: 0.159788  [115200/481450]\n",
      "loss: 0.325543  [118400/481450]\n",
      "loss: 0.303807  [121600/481450]\n",
      "loss: 0.376658  [124800/481450]\n",
      "loss: 0.454727  [128000/481450]\n",
      "loss: 0.333830  [131200/481450]\n",
      "loss: 0.296728  [134400/481450]\n",
      "loss: 0.220515  [137600/481450]\n",
      "loss: 0.228004  [140800/481450]\n",
      "loss: 0.073667  [144000/481450]\n",
      "loss: 0.361114  [147200/481450]\n",
      "loss: 0.361256  [150400/481450]\n",
      "loss: 0.241537  [153600/481450]\n",
      "loss: 0.635264  [156800/481450]\n",
      "loss: 0.306199  [160000/481450]\n",
      "loss: 0.602433  [163200/481450]\n",
      "loss: 0.338954  [166400/481450]\n",
      "loss: 0.144359  [169600/481450]\n",
      "loss: 0.672121  [172800/481450]\n",
      "loss: 0.180323  [176000/481450]\n",
      "loss: 0.159831  [179200/481450]\n",
      "loss: 0.242805  [182400/481450]\n",
      "loss: 0.195854  [185600/481450]\n",
      "loss: 0.170234  [188800/481450]\n",
      "loss: 0.508066  [192000/481450]\n",
      "loss: 0.345725  [195200/481450]\n",
      "loss: 0.201197  [198400/481450]\n",
      "loss: 0.189476  [201600/481450]\n",
      "loss: 0.120657  [204800/481450]\n",
      "loss: 0.491196  [208000/481450]\n",
      "loss: 0.233251  [211200/481450]\n",
      "loss: 0.637968  [214400/481450]\n",
      "loss: 0.296468  [217600/481450]\n",
      "loss: 0.353169  [220800/481450]\n",
      "loss: 0.216433  [224000/481450]\n",
      "loss: 0.310944  [227200/481450]\n",
      "loss: 0.136789  [230400/481450]\n",
      "loss: 0.238867  [233600/481450]\n",
      "loss: 0.169708  [236800/481450]\n",
      "loss: 0.439528  [240000/481450]\n",
      "loss: 0.403471  [243200/481450]\n",
      "loss: 0.457478  [246400/481450]\n",
      "loss: 0.344358  [249600/481450]\n",
      "loss: 0.234361  [252800/481450]\n",
      "loss: 0.346001  [256000/481450]\n",
      "loss: 0.213332  [259200/481450]\n",
      "loss: 0.206798  [262400/481450]\n",
      "loss: 0.329675  [265600/481450]\n",
      "loss: 0.288834  [268800/481450]\n",
      "loss: 0.374268  [272000/481450]\n",
      "loss: 0.096566  [275200/481450]\n",
      "loss: 0.290817  [278400/481450]\n",
      "loss: 0.365403  [281600/481450]\n",
      "loss: 0.098869  [284800/481450]\n",
      "loss: 0.597401  [288000/481450]\n",
      "loss: 0.420876  [291200/481450]\n",
      "loss: 0.175082  [294400/481450]\n",
      "loss: 0.159187  [297600/481450]\n",
      "loss: 0.385490  [300800/481450]\n",
      "loss: 0.334906  [304000/481450]\n",
      "loss: 0.304412  [307200/481450]\n",
      "loss: 0.211804  [310400/481450]\n",
      "loss: 0.345480  [313600/481450]\n",
      "loss: 0.459562  [316800/481450]\n",
      "loss: 0.307322  [320000/481450]\n",
      "loss: 0.294606  [323200/481450]\n",
      "loss: 0.174438  [326400/481450]\n",
      "loss: 0.288859  [329600/481450]\n",
      "loss: 0.418397  [332800/481450]\n",
      "loss: 0.535806  [336000/481450]\n",
      "loss: 0.243713  [339200/481450]\n",
      "loss: 0.415780  [342400/481450]\n",
      "loss: 0.439013  [345600/481450]\n",
      "loss: 0.306570  [348800/481450]\n",
      "loss: 0.353510  [352000/481450]\n",
      "loss: 0.188624  [355200/481450]\n",
      "loss: 0.517902  [358400/481450]\n",
      "loss: 0.143118  [361600/481450]\n",
      "loss: 0.214913  [364800/481450]\n",
      "loss: 0.340114  [368000/481450]\n",
      "loss: 0.280254  [371200/481450]\n",
      "loss: 0.482955  [374400/481450]\n",
      "loss: 0.419895  [377600/481450]\n",
      "loss: 0.439766  [380800/481450]\n",
      "loss: 0.156882  [384000/481450]\n",
      "loss: 0.187330  [387200/481450]\n",
      "loss: 0.475834  [390400/481450]\n",
      "loss: 0.228059  [393600/481450]\n",
      "loss: 0.409962  [396800/481450]\n",
      "loss: 0.261554  [400000/481450]\n",
      "loss: 0.263748  [403200/481450]\n",
      "loss: 0.246019  [406400/481450]\n",
      "loss: 0.301628  [409600/481450]\n",
      "loss: 0.647365  [412800/481450]\n",
      "loss: 0.309654  [416000/481450]\n",
      "loss: 0.398439  [419200/481450]\n",
      "loss: 0.388824  [422400/481450]\n",
      "loss: 0.304317  [425600/481450]\n",
      "loss: 0.355299  [428800/481450]\n",
      "loss: 0.212736  [432000/481450]\n",
      "loss: 0.217279  [435200/481450]\n",
      "loss: 0.531699  [438400/481450]\n",
      "loss: 0.305655  [441600/481450]\n",
      "loss: 0.520802  [444800/481450]\n",
      "loss: 0.231874  [448000/481450]\n",
      "loss: 0.299195  [451200/481450]\n",
      "loss: 0.422941  [454400/481450]\n",
      "loss: 0.463765  [457600/481450]\n",
      "loss: 0.314356  [460800/481450]\n",
      "loss: 0.367191  [464000/481450]\n",
      "loss: 0.281521  [467200/481450]\n",
      "loss: 0.315729  [470400/481450]\n",
      "loss: 0.511571  [473600/481450]\n",
      "loss: 0.487159  [476800/481450]\n",
      "loss: 0.251518  [480000/481450]\n",
      "Train Accuracy: 87.5325%\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.729520, F1-score: 84.19% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.338627  [    0/481450]\n",
      "loss: 0.341250  [ 3200/481450]\n",
      "loss: 0.249033  [ 6400/481450]\n",
      "loss: 0.607668  [ 9600/481450]\n",
      "loss: 0.354185  [12800/481450]\n",
      "loss: 0.133979  [16000/481450]\n",
      "loss: 0.221783  [19200/481450]\n",
      "loss: 0.127117  [22400/481450]\n",
      "loss: 0.209898  [25600/481450]\n",
      "loss: 0.131518  [28800/481450]\n",
      "loss: 0.278885  [32000/481450]\n",
      "loss: 0.063491  [35200/481450]\n",
      "loss: 0.106896  [38400/481450]\n",
      "loss: 0.409613  [41600/481450]\n",
      "loss: 0.221211  [44800/481450]\n",
      "loss: 0.254386  [48000/481450]\n",
      "loss: 0.432795  [51200/481450]\n",
      "loss: 0.141913  [54400/481450]\n",
      "loss: 0.345983  [57600/481450]\n",
      "loss: 0.429552  [60800/481450]\n",
      "loss: 0.345305  [64000/481450]\n",
      "loss: 0.229039  [67200/481450]\n",
      "loss: 0.255003  [70400/481450]\n",
      "loss: 0.239338  [73600/481450]\n",
      "loss: 0.415767  [76800/481450]\n",
      "loss: 0.293386  [80000/481450]\n",
      "loss: 0.200443  [83200/481450]\n",
      "loss: 0.316248  [86400/481450]\n",
      "loss: 0.277776  [89600/481450]\n",
      "loss: 0.113361  [92800/481450]\n",
      "loss: 0.192135  [96000/481450]\n",
      "loss: 0.266495  [99200/481450]\n",
      "loss: 0.241461  [102400/481450]\n",
      "loss: 0.241273  [105600/481450]\n",
      "loss: 0.379910  [108800/481450]\n",
      "loss: 0.231353  [112000/481450]\n",
      "loss: 0.221939  [115200/481450]\n",
      "loss: 0.350047  [118400/481450]\n",
      "loss: 0.515080  [121600/481450]\n",
      "loss: 0.417632  [124800/481450]\n",
      "loss: 0.221681  [128000/481450]\n",
      "loss: 0.212533  [131200/481450]\n",
      "loss: 0.357285  [134400/481450]\n",
      "loss: 0.252453  [137600/481450]\n",
      "loss: 0.384512  [140800/481450]\n",
      "loss: 0.158682  [144000/481450]\n",
      "loss: 0.674349  [147200/481450]\n",
      "loss: 0.448884  [150400/481450]\n",
      "loss: 0.201838  [153600/481450]\n",
      "loss: 0.252540  [156800/481450]\n",
      "loss: 0.208407  [160000/481450]\n",
      "loss: 0.506205  [163200/481450]\n",
      "loss: 0.279541  [166400/481450]\n",
      "loss: 0.301093  [169600/481450]\n",
      "loss: 0.138296  [172800/481450]\n",
      "loss: 0.497400  [176000/481450]\n",
      "loss: 0.311511  [179200/481450]\n",
      "loss: 0.415889  [182400/481450]\n",
      "loss: 0.172792  [185600/481450]\n",
      "loss: 0.297251  [188800/481450]\n",
      "loss: 0.294551  [192000/481450]\n",
      "loss: 0.229489  [195200/481450]\n",
      "loss: 0.217189  [198400/481450]\n",
      "loss: 0.284097  [201600/481450]\n",
      "loss: 0.089502  [204800/481450]\n",
      "loss: 0.217689  [208000/481450]\n",
      "loss: 0.329952  [211200/481450]\n",
      "loss: 0.254947  [214400/481450]\n",
      "loss: 0.382506  [217600/481450]\n",
      "loss: 0.378531  [220800/481450]\n",
      "loss: 0.214557  [224000/481450]\n",
      "loss: 0.226072  [227200/481450]\n",
      "loss: 0.531792  [230400/481450]\n",
      "loss: 0.384952  [233600/481450]\n",
      "loss: 0.226396  [236800/481450]\n",
      "loss: 0.503561  [240000/481450]\n",
      "loss: 0.268523  [243200/481450]\n",
      "loss: 0.373597  [246400/481450]\n",
      "loss: 0.203304  [249600/481450]\n",
      "loss: 0.288680  [252800/481450]\n",
      "loss: 0.258949  [256000/481450]\n",
      "loss: 0.519735  [259200/481450]\n",
      "loss: 0.254519  [262400/481450]\n",
      "loss: 0.087163  [265600/481450]\n",
      "loss: 0.298857  [268800/481450]\n",
      "loss: 0.301082  [272000/481450]\n",
      "loss: 0.278621  [275200/481450]\n",
      "loss: 0.244144  [278400/481450]\n",
      "loss: 0.122959  [281600/481450]\n",
      "loss: 0.452200  [284800/481450]\n",
      "loss: 0.170593  [288000/481450]\n",
      "loss: 0.260446  [291200/481450]\n",
      "loss: 0.254721  [294400/481450]\n",
      "loss: 0.218733  [297600/481450]\n",
      "loss: 0.226140  [300800/481450]\n",
      "loss: 0.350521  [304000/481450]\n",
      "loss: 0.267893  [307200/481450]\n",
      "loss: 0.426531  [310400/481450]\n",
      "loss: 0.357529  [313600/481450]\n",
      "loss: 0.154970  [316800/481450]\n",
      "loss: 0.159670  [320000/481450]\n",
      "loss: 0.231440  [323200/481450]\n",
      "loss: 0.202911  [326400/481450]\n",
      "loss: 0.138087  [329600/481450]\n",
      "loss: 0.143045  [332800/481450]\n",
      "loss: 0.253451  [336000/481450]\n",
      "loss: 0.511351  [339200/481450]\n",
      "loss: 0.309040  [342400/481450]\n",
      "loss: 0.297444  [345600/481450]\n",
      "loss: 0.437395  [348800/481450]\n",
      "loss: 0.262887  [352000/481450]\n",
      "loss: 0.510890  [355200/481450]\n",
      "loss: 0.201053  [358400/481450]\n",
      "loss: 0.243941  [361600/481450]\n",
      "loss: 0.383655  [364800/481450]\n",
      "loss: 0.116279  [368000/481450]\n",
      "loss: 0.153631  [371200/481450]\n",
      "loss: 0.269260  [374400/481450]\n",
      "loss: 0.491734  [377600/481450]\n",
      "loss: 0.393323  [380800/481450]\n",
      "loss: 0.284969  [384000/481450]\n",
      "loss: 0.465997  [387200/481450]\n",
      "loss: 0.312116  [390400/481450]\n",
      "loss: 0.308177  [393600/481450]\n",
      "loss: 0.237114  [396800/481450]\n",
      "loss: 0.334355  [400000/481450]\n",
      "loss: 0.319476  [403200/481450]\n",
      "loss: 0.359626  [406400/481450]\n",
      "loss: 0.364025  [409600/481450]\n",
      "loss: 0.295397  [412800/481450]\n",
      "loss: 0.338047  [416000/481450]\n",
      "loss: 0.141505  [419200/481450]\n",
      "loss: 0.368356  [422400/481450]\n",
      "loss: 0.493884  [425600/481450]\n",
      "loss: 0.557497  [428800/481450]\n",
      "loss: 0.236322  [432000/481450]\n",
      "loss: 0.405003  [435200/481450]\n",
      "loss: 0.321846  [438400/481450]\n",
      "loss: 0.282417  [441600/481450]\n",
      "loss: 0.349327  [444800/481450]\n",
      "loss: 0.272288  [448000/481450]\n",
      "loss: 0.302696  [451200/481450]\n",
      "loss: 0.212744  [454400/481450]\n",
      "loss: 0.179519  [457600/481450]\n",
      "loss: 0.103370  [460800/481450]\n",
      "loss: 0.366475  [464000/481450]\n",
      "loss: 0.199513  [467200/481450]\n",
      "loss: 0.396709  [470400/481450]\n",
      "loss: 0.201607  [473600/481450]\n",
      "loss: 0.308717  [476800/481450]\n",
      "loss: 0.192616  [480000/481450]\n",
      "Train Accuracy: 87.9466%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.689033, F1-score: 85.45% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.177970  [    0/481450]\n",
      "loss: 0.228337  [ 3200/481450]\n",
      "loss: 0.408560  [ 6400/481450]\n",
      "loss: 0.355390  [ 9600/481450]\n",
      "loss: 0.356118  [12800/481450]\n",
      "loss: 0.265197  [16000/481450]\n",
      "loss: 0.198511  [19200/481450]\n",
      "loss: 0.164532  [22400/481450]\n",
      "loss: 0.215287  [25600/481450]\n",
      "loss: 0.279885  [28800/481450]\n",
      "loss: 0.295264  [32000/481450]\n",
      "loss: 0.150702  [35200/481450]\n",
      "loss: 0.373805  [38400/481450]\n",
      "loss: 0.321938  [41600/481450]\n",
      "loss: 0.284817  [44800/481450]\n",
      "loss: 0.272463  [48000/481450]\n",
      "loss: 0.168794  [51200/481450]\n",
      "loss: 0.159978  [54400/481450]\n",
      "loss: 0.260319  [57600/481450]\n",
      "loss: 0.312277  [60800/481450]\n",
      "loss: 0.432951  [64000/481450]\n",
      "loss: 0.427536  [67200/481450]\n",
      "loss: 0.443078  [70400/481450]\n",
      "loss: 0.226451  [73600/481450]\n",
      "loss: 0.314638  [76800/481450]\n",
      "loss: 0.285940  [80000/481450]\n",
      "loss: 0.392399  [83200/481450]\n",
      "loss: 0.307973  [86400/481450]\n",
      "loss: 0.214969  [89600/481450]\n",
      "loss: 0.097227  [92800/481450]\n",
      "loss: 0.200863  [96000/481450]\n",
      "loss: 0.258988  [99200/481450]\n",
      "loss: 0.274299  [102400/481450]\n",
      "loss: 0.107029  [105600/481450]\n",
      "loss: 0.342576  [108800/481450]\n",
      "loss: 0.368945  [112000/481450]\n",
      "loss: 0.264714  [115200/481450]\n",
      "loss: 0.172131  [118400/481450]\n",
      "loss: 0.339661  [121600/481450]\n",
      "loss: 0.153878  [124800/481450]\n",
      "loss: 0.457033  [128000/481450]\n",
      "loss: 0.419070  [131200/481450]\n",
      "loss: 0.318783  [134400/481450]\n",
      "loss: 0.227248  [137600/481450]\n",
      "loss: 0.441521  [140800/481450]\n",
      "loss: 0.096022  [144000/481450]\n",
      "loss: 0.324631  [147200/481450]\n",
      "loss: 0.191005  [150400/481450]\n",
      "loss: 0.347766  [153600/481450]\n",
      "loss: 0.425450  [156800/481450]\n",
      "loss: 0.241676  [160000/481450]\n",
      "loss: 0.325530  [163200/481450]\n",
      "loss: 0.249962  [166400/481450]\n",
      "loss: 0.101624  [169600/481450]\n",
      "loss: 0.502650  [172800/481450]\n",
      "loss: 0.247303  [176000/481450]\n",
      "loss: 0.281119  [179200/481450]\n",
      "loss: 0.243324  [182400/481450]\n",
      "loss: 0.189711  [185600/481450]\n",
      "loss: 0.189326  [188800/481450]\n",
      "loss: 0.231906  [192000/481450]\n",
      "loss: 0.323592  [195200/481450]\n",
      "loss: 0.373464  [198400/481450]\n",
      "loss: 0.285403  [201600/481450]\n",
      "loss: 0.203183  [204800/481450]\n",
      "loss: 0.275844  [208000/481450]\n",
      "loss: 0.207941  [211200/481450]\n",
      "loss: 0.511942  [214400/481450]\n",
      "loss: 0.165718  [217600/481450]\n",
      "loss: 0.094736  [220800/481450]\n",
      "loss: 0.303640  [224000/481450]\n",
      "loss: 0.258706  [227200/481450]\n",
      "loss: 0.314628  [230400/481450]\n",
      "loss: 0.438026  [233600/481450]\n",
      "loss: 0.271088  [236800/481450]\n",
      "loss: 0.358679  [240000/481450]\n",
      "loss: 0.065821  [243200/481450]\n",
      "loss: 0.453686  [246400/481450]\n",
      "loss: 0.248615  [249600/481450]\n",
      "loss: 0.523237  [252800/481450]\n",
      "loss: 0.336138  [256000/481450]\n",
      "loss: 0.321394  [259200/481450]\n",
      "loss: 0.309771  [262400/481450]\n",
      "loss: 0.262295  [265600/481450]\n",
      "loss: 0.234655  [268800/481450]\n",
      "loss: 0.252549  [272000/481450]\n",
      "loss: 0.213119  [275200/481450]\n",
      "loss: 0.178191  [278400/481450]\n",
      "loss: 0.371062  [281600/481450]\n",
      "loss: 0.151109  [284800/481450]\n",
      "loss: 0.252088  [288000/481450]\n",
      "loss: 0.388355  [291200/481450]\n",
      "loss: 0.330077  [294400/481450]\n",
      "loss: 0.230033  [297600/481450]\n",
      "loss: 0.270794  [300800/481450]\n",
      "loss: 0.261003  [304000/481450]\n",
      "loss: 0.173864  [307200/481450]\n",
      "loss: 0.264639  [310400/481450]\n",
      "loss: 0.467578  [313600/481450]\n",
      "loss: 0.410490  [316800/481450]\n",
      "loss: 0.340935  [320000/481450]\n",
      "loss: 0.108548  [323200/481450]\n",
      "loss: 0.288221  [326400/481450]\n",
      "loss: 0.347669  [329600/481450]\n",
      "loss: 0.198212  [332800/481450]\n",
      "loss: 0.149638  [336000/481450]\n",
      "loss: 0.545069  [339200/481450]\n",
      "loss: 0.135122  [342400/481450]\n",
      "loss: 0.114529  [345600/481450]\n",
      "loss: 0.640766  [348800/481450]\n",
      "loss: 0.219242  [352000/481450]\n",
      "loss: 0.354251  [355200/481450]\n",
      "loss: 0.281308  [358400/481450]\n",
      "loss: 0.356076  [361600/481450]\n",
      "loss: 0.474647  [364800/481450]\n",
      "loss: 0.189136  [368000/481450]\n",
      "loss: 0.346147  [371200/481450]\n",
      "loss: 0.285722  [374400/481450]\n",
      "loss: 0.493067  [377600/481450]\n",
      "loss: 0.385686  [380800/481450]\n",
      "loss: 0.464610  [384000/481450]\n",
      "loss: 0.329352  [387200/481450]\n",
      "loss: 0.184744  [390400/481450]\n",
      "loss: 0.206475  [393600/481450]\n",
      "loss: 0.103511  [396800/481450]\n",
      "loss: 0.276845  [400000/481450]\n",
      "loss: 0.197682  [403200/481450]\n",
      "loss: 0.528040  [406400/481450]\n",
      "loss: 0.403261  [409600/481450]\n",
      "loss: 0.215155  [412800/481450]\n",
      "loss: 0.215944  [416000/481450]\n",
      "loss: 0.402181  [419200/481450]\n",
      "loss: 0.343760  [422400/481450]\n",
      "loss: 0.148025  [425600/481450]\n",
      "loss: 0.212814  [428800/481450]\n",
      "loss: 0.261606  [432000/481450]\n",
      "loss: 0.195025  [435200/481450]\n",
      "loss: 0.314133  [438400/481450]\n",
      "loss: 0.276315  [441600/481450]\n",
      "loss: 0.284059  [444800/481450]\n",
      "loss: 0.346632  [448000/481450]\n",
      "loss: 0.316056  [451200/481450]\n",
      "loss: 0.276796  [454400/481450]\n",
      "loss: 0.197400  [457600/481450]\n",
      "loss: 0.213251  [460800/481450]\n",
      "loss: 0.327056  [464000/481450]\n",
      "loss: 0.220281  [467200/481450]\n",
      "loss: 0.173441  [470400/481450]\n",
      "loss: 0.203351  [473600/481450]\n",
      "loss: 0.178026  [476800/481450]\n",
      "loss: 0.265063  [480000/481450]\n",
      "Train Accuracy: 88.2630%\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.686485, F1-score: 85.88% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.381155  [    0/481450]\n",
      "loss: 0.335968  [ 3200/481450]\n",
      "loss: 0.106259  [ 6400/481450]\n",
      "loss: 0.439751  [ 9600/481450]\n",
      "loss: 0.537386  [12800/481450]\n",
      "loss: 0.496215  [16000/481450]\n",
      "loss: 0.391112  [19200/481450]\n",
      "loss: 0.442874  [22400/481450]\n",
      "loss: 0.472340  [25600/481450]\n",
      "loss: 0.343956  [28800/481450]\n",
      "loss: 0.377126  [32000/481450]\n",
      "loss: 0.400730  [35200/481450]\n",
      "loss: 0.224782  [38400/481450]\n",
      "loss: 0.392300  [41600/481450]\n",
      "loss: 0.417773  [44800/481450]\n",
      "loss: 0.275801  [48000/481450]\n",
      "loss: 0.166965  [51200/481450]\n",
      "loss: 0.294064  [54400/481450]\n",
      "loss: 0.389089  [57600/481450]\n",
      "loss: 0.370266  [60800/481450]\n",
      "loss: 0.221054  [64000/481450]\n",
      "loss: 0.410181  [67200/481450]\n",
      "loss: 0.281449  [70400/481450]\n",
      "loss: 0.307691  [73600/481450]\n",
      "loss: 0.330540  [76800/481450]\n",
      "loss: 0.255370  [80000/481450]\n",
      "loss: 0.277058  [83200/481450]\n",
      "loss: 0.265274  [86400/481450]\n",
      "loss: 0.385216  [89600/481450]\n",
      "loss: 0.322381  [92800/481450]\n",
      "loss: 0.408881  [96000/481450]\n",
      "loss: 0.165818  [99200/481450]\n",
      "loss: 0.330109  [102400/481450]\n",
      "loss: 0.175295  [105600/481450]\n",
      "loss: 0.688335  [108800/481450]\n",
      "loss: 0.452819  [112000/481450]\n",
      "loss: 0.304458  [115200/481450]\n",
      "loss: 0.273195  [118400/481450]\n",
      "loss: 0.254842  [121600/481450]\n",
      "loss: 0.278009  [124800/481450]\n",
      "loss: 0.142694  [128000/481450]\n",
      "loss: 0.277401  [131200/481450]\n",
      "loss: 0.262128  [134400/481450]\n",
      "loss: 0.286720  [137600/481450]\n",
      "loss: 0.296633  [140800/481450]\n",
      "loss: 0.546024  [144000/481450]\n",
      "loss: 0.377389  [147200/481450]\n",
      "loss: 0.160421  [150400/481450]\n",
      "loss: 0.204968  [153600/481450]\n",
      "loss: 0.257473  [156800/481450]\n",
      "loss: 0.198975  [160000/481450]\n",
      "loss: 0.213493  [163200/481450]\n",
      "loss: 0.294998  [166400/481450]\n",
      "loss: 0.132103  [169600/481450]\n",
      "loss: 0.262420  [172800/481450]\n",
      "loss: 0.216634  [176000/481450]\n",
      "loss: 0.268352  [179200/481450]\n",
      "loss: 0.234606  [182400/481450]\n",
      "loss: 0.329648  [185600/481450]\n",
      "loss: 0.339667  [188800/481450]\n",
      "loss: 0.189611  [192000/481450]\n",
      "loss: 0.467018  [195200/481450]\n",
      "loss: 0.144739  [198400/481450]\n",
      "loss: 0.277076  [201600/481450]\n",
      "loss: 0.194391  [204800/481450]\n",
      "loss: 0.367817  [208000/481450]\n",
      "loss: 0.283008  [211200/481450]\n",
      "loss: 0.199850  [214400/481450]\n",
      "loss: 0.352454  [217600/481450]\n",
      "loss: 0.247588  [220800/481450]\n",
      "loss: 0.240492  [224000/481450]\n",
      "loss: 0.272671  [227200/481450]\n",
      "loss: 0.272480  [230400/481450]\n",
      "loss: 0.296544  [233600/481450]\n",
      "loss: 0.462362  [236800/481450]\n",
      "loss: 0.154971  [240000/481450]\n",
      "loss: 0.658540  [243200/481450]\n",
      "loss: 0.252176  [246400/481450]\n",
      "loss: 0.246530  [249600/481450]\n",
      "loss: 0.084827  [252800/481450]\n",
      "loss: 0.270157  [256000/481450]\n",
      "loss: 0.222944  [259200/481450]\n",
      "loss: 0.244160  [262400/481450]\n",
      "loss: 0.543360  [265600/481450]\n",
      "loss: 0.408361  [268800/481450]\n",
      "loss: 0.597770  [272000/481450]\n",
      "loss: 0.160950  [275200/481450]\n",
      "loss: 0.364055  [278400/481450]\n",
      "loss: 0.276980  [281600/481450]\n",
      "loss: 0.182115  [284800/481450]\n",
      "loss: 0.117002  [288000/481450]\n",
      "loss: 0.427423  [291200/481450]\n",
      "loss: 0.183772  [294400/481450]\n",
      "loss: 0.512507  [297600/481450]\n",
      "loss: 0.046387  [300800/481450]\n",
      "loss: 0.178411  [304000/481450]\n",
      "loss: 0.569667  [307200/481450]\n",
      "loss: 0.165051  [310400/481450]\n",
      "loss: 0.203012  [313600/481450]\n",
      "loss: 0.131481  [316800/481450]\n",
      "loss: 0.265927  [320000/481450]\n",
      "loss: 0.110860  [323200/481450]\n",
      "loss: 0.330259  [326400/481450]\n",
      "loss: 0.204307  [329600/481450]\n",
      "loss: 0.212445  [332800/481450]\n",
      "loss: 0.179367  [336000/481450]\n",
      "loss: 0.081724  [339200/481450]\n",
      "loss: 0.370558  [342400/481450]\n",
      "loss: 0.304264  [345600/481450]\n",
      "loss: 0.197493  [348800/481450]\n",
      "loss: 0.440887  [352000/481450]\n",
      "loss: 0.227383  [355200/481450]\n",
      "loss: 0.495316  [358400/481450]\n",
      "loss: 0.351595  [361600/481450]\n",
      "loss: 0.457531  [364800/481450]\n",
      "loss: 0.304794  [368000/481450]\n",
      "loss: 0.296452  [371200/481450]\n",
      "loss: 0.189699  [374400/481450]\n",
      "loss: 0.144138  [377600/481450]\n",
      "loss: 0.304712  [380800/481450]\n",
      "loss: 0.367958  [384000/481450]\n",
      "loss: 0.190443  [387200/481450]\n",
      "loss: 0.389041  [390400/481450]\n",
      "loss: 0.621069  [393600/481450]\n",
      "loss: 0.170178  [396800/481450]\n",
      "loss: 0.142074  [400000/481450]\n",
      "loss: 0.215465  [403200/481450]\n",
      "loss: 0.140687  [406400/481450]\n",
      "loss: 0.302064  [409600/481450]\n",
      "loss: 0.225046  [412800/481450]\n",
      "loss: 0.233906  [416000/481450]\n",
      "loss: 0.192311  [419200/481450]\n",
      "loss: 0.184094  [422400/481450]\n",
      "loss: 0.098840  [425600/481450]\n",
      "loss: 0.309125  [428800/481450]\n",
      "loss: 0.286937  [432000/481450]\n",
      "loss: 0.174918  [435200/481450]\n",
      "loss: 0.094904  [438400/481450]\n",
      "loss: 0.277685  [441600/481450]\n",
      "loss: 0.094837  [444800/481450]\n",
      "loss: 0.110695  [448000/481450]\n",
      "loss: 0.093060  [451200/481450]\n",
      "loss: 0.284822  [454400/481450]\n",
      "loss: 0.272357  [457600/481450]\n",
      "loss: 0.214153  [460800/481450]\n",
      "loss: 0.069858  [464000/481450]\n",
      "loss: 0.312337  [467200/481450]\n",
      "loss: 0.289797  [470400/481450]\n",
      "loss: 0.465075  [473600/481450]\n",
      "loss: 0.304830  [476800/481450]\n",
      "loss: 0.394117  [480000/481450]\n",
      "Train Accuracy: 88.6451%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.714583, F1-score: 85.63% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.244472  [    0/481450]\n",
      "loss: 0.231306  [ 3200/481450]\n",
      "loss: 0.325406  [ 6400/481450]\n",
      "loss: 0.333481  [ 9600/481450]\n",
      "loss: 0.210789  [12800/481450]\n",
      "loss: 0.285179  [16000/481450]\n",
      "loss: 0.388796  [19200/481450]\n",
      "loss: 0.099680  [22400/481450]\n",
      "loss: 0.291982  [25600/481450]\n",
      "loss: 0.252777  [28800/481450]\n",
      "loss: 0.235531  [32000/481450]\n",
      "loss: 0.253938  [35200/481450]\n",
      "loss: 0.212056  [38400/481450]\n",
      "loss: 0.405618  [41600/481450]\n",
      "loss: 0.382987  [44800/481450]\n",
      "loss: 0.151942  [48000/481450]\n",
      "loss: 0.325915  [51200/481450]\n",
      "loss: 0.178513  [54400/481450]\n",
      "loss: 0.214984  [57600/481450]\n",
      "loss: 0.231985  [60800/481450]\n",
      "loss: 0.373741  [64000/481450]\n",
      "loss: 0.313417  [67200/481450]\n",
      "loss: 0.450301  [70400/481450]\n",
      "loss: 0.263337  [73600/481450]\n",
      "loss: 0.162386  [76800/481450]\n",
      "loss: 0.818223  [80000/481450]\n",
      "loss: 0.258427  [83200/481450]\n",
      "loss: 0.272471  [86400/481450]\n",
      "loss: 0.119308  [89600/481450]\n",
      "loss: 0.208199  [92800/481450]\n",
      "loss: 0.181828  [96000/481450]\n",
      "loss: 0.242799  [99200/481450]\n",
      "loss: 0.447364  [102400/481450]\n",
      "loss: 0.296290  [105600/481450]\n",
      "loss: 0.176648  [108800/481450]\n",
      "loss: 0.144140  [112000/481450]\n",
      "loss: 0.448265  [115200/481450]\n",
      "loss: 0.180893  [118400/481450]\n",
      "loss: 0.361181  [121600/481450]\n",
      "loss: 0.367824  [124800/481450]\n",
      "loss: 0.150942  [128000/481450]\n",
      "loss: 0.240820  [131200/481450]\n",
      "loss: 0.310812  [134400/481450]\n",
      "loss: 0.285857  [137600/481450]\n",
      "loss: 0.358268  [140800/481450]\n",
      "loss: 0.284077  [144000/481450]\n",
      "loss: 0.059100  [147200/481450]\n",
      "loss: 0.185673  [150400/481450]\n",
      "loss: 0.064428  [153600/481450]\n",
      "loss: 0.411271  [156800/481450]\n",
      "loss: 0.095616  [160000/481450]\n",
      "loss: 0.452215  [163200/481450]\n",
      "loss: 0.059698  [166400/481450]\n",
      "loss: 0.011405  [169600/481450]\n",
      "loss: 0.252031  [172800/481450]\n",
      "loss: 0.355274  [176000/481450]\n",
      "loss: 0.107377  [179200/481450]\n",
      "loss: 0.189602  [182400/481450]\n",
      "loss: 0.289829  [185600/481450]\n",
      "loss: 0.229641  [188800/481450]\n",
      "loss: 0.130488  [192000/481450]\n",
      "loss: 0.178970  [195200/481450]\n",
      "loss: 0.277395  [198400/481450]\n",
      "loss: 0.282369  [201600/481450]\n",
      "loss: 0.347388  [204800/481450]\n",
      "loss: 0.189734  [208000/481450]\n",
      "loss: 0.223280  [211200/481450]\n",
      "loss: 0.400860  [214400/481450]\n",
      "loss: 0.332376  [217600/481450]\n",
      "loss: 0.223414  [220800/481450]\n",
      "loss: 0.266585  [224000/481450]\n",
      "loss: 0.287346  [227200/481450]\n",
      "loss: 0.247450  [230400/481450]\n",
      "loss: 0.062035  [233600/481450]\n",
      "loss: 0.335728  [236800/481450]\n",
      "loss: 0.152630  [240000/481450]\n",
      "loss: 0.260389  [243200/481450]\n",
      "loss: 0.191863  [246400/481450]\n",
      "loss: 0.185421  [249600/481450]\n",
      "loss: 0.214599  [252800/481450]\n",
      "loss: 0.205711  [256000/481450]\n",
      "loss: 0.245110  [259200/481450]\n",
      "loss: 0.436031  [262400/481450]\n",
      "loss: 0.182297  [265600/481450]\n",
      "loss: 0.146445  [268800/481450]\n",
      "loss: 0.353736  [272000/481450]\n",
      "loss: 0.247067  [275200/481450]\n",
      "loss: 0.320950  [278400/481450]\n",
      "loss: 0.171328  [281600/481450]\n",
      "loss: 0.396010  [284800/481450]\n",
      "loss: 0.245962  [288000/481450]\n",
      "loss: 0.394496  [291200/481450]\n",
      "loss: 0.154048  [294400/481450]\n",
      "loss: 0.300787  [297600/481450]\n",
      "loss: 0.360016  [300800/481450]\n",
      "loss: 0.339109  [304000/481450]\n",
      "loss: 0.198939  [307200/481450]\n",
      "loss: 0.199639  [310400/481450]\n",
      "loss: 0.366093  [313600/481450]\n",
      "loss: 0.442401  [316800/481450]\n",
      "loss: 0.124812  [320000/481450]\n",
      "loss: 0.346708  [323200/481450]\n",
      "loss: 0.236895  [326400/481450]\n",
      "loss: 0.164609  [329600/481450]\n",
      "loss: 0.205484  [332800/481450]\n",
      "loss: 0.218282  [336000/481450]\n",
      "loss: 0.238038  [339200/481450]\n",
      "loss: 0.286481  [342400/481450]\n",
      "loss: 0.231919  [345600/481450]\n",
      "loss: 0.358335  [348800/481450]\n",
      "loss: 0.153005  [352000/481450]\n",
      "loss: 0.398470  [355200/481450]\n",
      "loss: 0.363128  [358400/481450]\n",
      "loss: 0.301191  [361600/481450]\n",
      "loss: 0.146244  [364800/481450]\n",
      "loss: 0.218535  [368000/481450]\n",
      "loss: 0.255283  [371200/481450]\n",
      "loss: 0.279868  [374400/481450]\n",
      "loss: 0.160079  [377600/481450]\n",
      "loss: 0.459321  [380800/481450]\n",
      "loss: 0.495581  [384000/481450]\n",
      "loss: 0.159921  [387200/481450]\n",
      "loss: 0.163120  [390400/481450]\n",
      "loss: 0.338254  [393600/481450]\n",
      "loss: 0.088445  [396800/481450]\n",
      "loss: 0.349190  [400000/481450]\n",
      "loss: 0.311421  [403200/481450]\n",
      "loss: 0.373793  [406400/481450]\n",
      "loss: 0.152068  [409600/481450]\n",
      "loss: 0.321918  [412800/481450]\n",
      "loss: 0.425603  [416000/481450]\n",
      "loss: 0.381141  [419200/481450]\n",
      "loss: 0.151096  [422400/481450]\n",
      "loss: 0.136676  [425600/481450]\n",
      "loss: 0.235677  [428800/481450]\n",
      "loss: 0.356068  [432000/481450]\n",
      "loss: 0.190134  [435200/481450]\n",
      "loss: 0.489751  [438400/481450]\n",
      "loss: 0.307024  [441600/481450]\n",
      "loss: 0.220971  [444800/481450]\n",
      "loss: 0.229646  [448000/481450]\n",
      "loss: 0.208705  [451200/481450]\n",
      "loss: 0.307054  [454400/481450]\n",
      "loss: 0.191045  [457600/481450]\n",
      "loss: 0.392976  [460800/481450]\n",
      "loss: 0.218415  [464000/481450]\n",
      "loss: 0.434941  [467200/481450]\n",
      "loss: 0.122539  [470400/481450]\n",
      "loss: 0.267469  [473600/481450]\n",
      "loss: 0.337598  [476800/481450]\n",
      "loss: 0.462164  [480000/481450]\n",
      "Train Accuracy: 89.1509%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.726574, F1-score: 85.38% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.310965  [    0/481450]\n",
      "loss: 0.116938  [ 3200/481450]\n",
      "loss: 0.361809  [ 6400/481450]\n",
      "loss: 0.323753  [ 9600/481450]\n",
      "loss: 0.279951  [12800/481450]\n",
      "loss: 0.260444  [16000/481450]\n",
      "loss: 0.262991  [19200/481450]\n",
      "loss: 0.224426  [22400/481450]\n",
      "loss: 0.238556  [25600/481450]\n",
      "loss: 0.216343  [28800/481450]\n",
      "loss: 0.371618  [32000/481450]\n",
      "loss: 0.375698  [35200/481450]\n",
      "loss: 0.332870  [38400/481450]\n",
      "loss: 0.319416  [41600/481450]\n",
      "loss: 0.458010  [44800/481450]\n",
      "loss: 0.331854  [48000/481450]\n",
      "loss: 0.144553  [51200/481450]\n",
      "loss: 0.412590  [54400/481450]\n",
      "loss: 0.125698  [57600/481450]\n",
      "loss: 0.295023  [60800/481450]\n",
      "loss: 0.308675  [64000/481450]\n",
      "loss: 0.216019  [67200/481450]\n",
      "loss: 0.329100  [70400/481450]\n",
      "loss: 0.048841  [73600/481450]\n",
      "loss: 0.641275  [76800/481450]\n",
      "loss: 0.272616  [80000/481450]\n",
      "loss: 0.254923  [83200/481450]\n",
      "loss: 0.358147  [86400/481450]\n",
      "loss: 0.266644  [89600/481450]\n",
      "loss: 0.280895  [92800/481450]\n",
      "loss: 0.184829  [96000/481450]\n",
      "loss: 0.149937  [99200/481450]\n",
      "loss: 0.128550  [102400/481450]\n",
      "loss: 0.250622  [105600/481450]\n",
      "loss: 0.233196  [108800/481450]\n",
      "loss: 0.239028  [112000/481450]\n",
      "loss: 0.260770  [115200/481450]\n",
      "loss: 0.310971  [118400/481450]\n",
      "loss: 0.227214  [121600/481450]\n",
      "loss: 0.325881  [124800/481450]\n",
      "loss: 0.221638  [128000/481450]\n",
      "loss: 0.120668  [131200/481450]\n",
      "loss: 0.474899  [134400/481450]\n",
      "loss: 0.360984  [137600/481450]\n",
      "loss: 0.295098  [140800/481450]\n",
      "loss: 0.161171  [144000/481450]\n",
      "loss: 0.325252  [147200/481450]\n",
      "loss: 0.383658  [150400/481450]\n",
      "loss: 0.246655  [153600/481450]\n",
      "loss: 0.212558  [156800/481450]\n",
      "loss: 0.253092  [160000/481450]\n",
      "loss: 0.329263  [163200/481450]\n",
      "loss: 0.327138  [166400/481450]\n",
      "loss: 0.375265  [169600/481450]\n",
      "loss: 0.302858  [172800/481450]\n",
      "loss: 0.156113  [176000/481450]\n",
      "loss: 0.145467  [179200/481450]\n",
      "loss: 0.381602  [182400/481450]\n",
      "loss: 0.229095  [185600/481450]\n",
      "loss: 0.305815  [188800/481450]\n",
      "loss: 0.125247  [192000/481450]\n",
      "loss: 0.216276  [195200/481450]\n",
      "loss: 0.220519  [198400/481450]\n",
      "loss: 0.315738  [201600/481450]\n",
      "loss: 0.202412  [204800/481450]\n",
      "loss: 0.308638  [208000/481450]\n",
      "loss: 0.282944  [211200/481450]\n",
      "loss: 0.243870  [214400/481450]\n",
      "loss: 0.095877  [217600/481450]\n",
      "loss: 0.396015  [220800/481450]\n",
      "loss: 0.278652  [224000/481450]\n",
      "loss: 0.073353  [227200/481450]\n",
      "loss: 0.095879  [230400/481450]\n",
      "loss: 0.264397  [233600/481450]\n",
      "loss: 0.542866  [236800/481450]\n",
      "loss: 0.282144  [240000/481450]\n",
      "loss: 0.489146  [243200/481450]\n",
      "loss: 0.136187  [246400/481450]\n",
      "loss: 0.224160  [249600/481450]\n",
      "loss: 0.464220  [252800/481450]\n",
      "loss: 0.238018  [256000/481450]\n",
      "loss: 0.268171  [259200/481450]\n",
      "loss: 0.172810  [262400/481450]\n",
      "loss: 0.240935  [265600/481450]\n",
      "loss: 0.356624  [268800/481450]\n",
      "loss: 0.165086  [272000/481450]\n",
      "loss: 0.198393  [275200/481450]\n",
      "loss: 0.382273  [278400/481450]\n",
      "loss: 0.249904  [281600/481450]\n",
      "loss: 0.185606  [284800/481450]\n",
      "loss: 0.132861  [288000/481450]\n",
      "loss: 0.187338  [291200/481450]\n",
      "loss: 0.335145  [294400/481450]\n",
      "loss: 0.306010  [297600/481450]\n",
      "loss: 0.208309  [300800/481450]\n",
      "loss: 0.090499  [304000/481450]\n",
      "loss: 0.337037  [307200/481450]\n",
      "loss: 0.179037  [310400/481450]\n",
      "loss: 0.213940  [313600/481450]\n",
      "loss: 0.268003  [316800/481450]\n",
      "loss: 0.126438  [320000/481450]\n",
      "loss: 0.382998  [323200/481450]\n",
      "loss: 0.221656  [326400/481450]\n",
      "loss: 0.346921  [329600/481450]\n",
      "loss: 0.385439  [332800/481450]\n",
      "loss: 0.150752  [336000/481450]\n",
      "loss: 0.248623  [339200/481450]\n",
      "loss: 0.070842  [342400/481450]\n",
      "loss: 0.136368  [345600/481450]\n",
      "loss: 0.266890  [348800/481450]\n",
      "loss: 0.648097  [352000/481450]\n",
      "loss: 0.279183  [355200/481450]\n",
      "loss: 0.405557  [358400/481450]\n",
      "loss: 0.262867  [361600/481450]\n",
      "loss: 0.242994  [364800/481450]\n",
      "loss: 0.278634  [368000/481450]\n",
      "loss: 0.152511  [371200/481450]\n",
      "loss: 0.237240  [374400/481450]\n",
      "loss: 0.166302  [377600/481450]\n",
      "loss: 0.177519  [380800/481450]\n",
      "loss: 0.184818  [384000/481450]\n",
      "loss: 0.257145  [387200/481450]\n",
      "loss: 0.300747  [390400/481450]\n",
      "loss: 0.333799  [393600/481450]\n",
      "loss: 0.361284  [396800/481450]\n",
      "loss: 0.181442  [400000/481450]\n",
      "loss: 0.374215  [403200/481450]\n",
      "loss: 0.207789  [406400/481450]\n",
      "loss: 0.159832  [409600/481450]\n",
      "loss: 0.449965  [412800/481450]\n",
      "loss: 0.109440  [416000/481450]\n",
      "loss: 0.143747  [419200/481450]\n",
      "loss: 0.242690  [422400/481450]\n",
      "loss: 0.166577  [425600/481450]\n",
      "loss: 0.292902  [428800/481450]\n",
      "loss: 0.257432  [432000/481450]\n",
      "loss: 0.206955  [435200/481450]\n",
      "loss: 0.254984  [438400/481450]\n",
      "loss: 0.240133  [441600/481450]\n",
      "loss: 0.434885  [444800/481450]\n",
      "loss: 0.227447  [448000/481450]\n",
      "loss: 0.309366  [451200/481450]\n",
      "loss: 0.263040  [454400/481450]\n",
      "loss: 0.244234  [457600/481450]\n",
      "loss: 0.253883  [460800/481450]\n",
      "loss: 0.323593  [464000/481450]\n",
      "loss: 0.177051  [467200/481450]\n",
      "loss: 0.278137  [470400/481450]\n",
      "loss: 0.273629  [473600/481450]\n",
      "loss: 0.172704  [476800/481450]\n",
      "loss: 0.188160  [480000/481450]\n",
      "Train Accuracy: 89.4882%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.764151, F1-score: 85.38% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.217728  [    0/481450]\n",
      "loss: 0.093889  [ 3200/481450]\n",
      "loss: 0.393950  [ 6400/481450]\n",
      "loss: 0.229710  [ 9600/481450]\n",
      "loss: 0.150471  [12800/481450]\n",
      "loss: 0.268731  [16000/481450]\n",
      "loss: 0.127093  [19200/481450]\n",
      "loss: 0.396216  [22400/481450]\n",
      "loss: 0.532216  [25600/481450]\n",
      "loss: 0.255278  [28800/481450]\n",
      "loss: 0.326349  [32000/481450]\n",
      "loss: 0.130047  [35200/481450]\n",
      "loss: 0.513970  [38400/481450]\n",
      "loss: 0.185464  [41600/481450]\n",
      "loss: 0.253250  [44800/481450]\n",
      "loss: 0.361782  [48000/481450]\n",
      "loss: 0.276861  [51200/481450]\n",
      "loss: 0.350932  [54400/481450]\n",
      "loss: 0.166324  [57600/481450]\n",
      "loss: 0.214055  [60800/481450]\n",
      "loss: 0.281395  [64000/481450]\n",
      "loss: 0.432464  [67200/481450]\n",
      "loss: 0.048012  [70400/481450]\n",
      "loss: 0.310680  [73600/481450]\n",
      "loss: 0.212381  [76800/481450]\n",
      "loss: 0.122849  [80000/481450]\n",
      "loss: 0.230657  [83200/481450]\n",
      "loss: 0.241729  [86400/481450]\n",
      "loss: 0.364474  [89600/481450]\n",
      "loss: 0.190340  [92800/481450]\n",
      "loss: 0.331898  [96000/481450]\n",
      "loss: 0.360310  [99200/481450]\n",
      "loss: 0.105838  [102400/481450]\n",
      "loss: 0.388737  [105600/481450]\n",
      "loss: 0.264927  [108800/481450]\n",
      "loss: 0.190600  [112000/481450]\n",
      "loss: 0.422901  [115200/481450]\n",
      "loss: 0.396224  [118400/481450]\n",
      "loss: 0.213245  [121600/481450]\n",
      "loss: 0.364609  [124800/481450]\n",
      "loss: 0.082142  [128000/481450]\n",
      "loss: 0.203465  [131200/481450]\n",
      "loss: 0.360864  [134400/481450]\n",
      "loss: 0.293267  [137600/481450]\n",
      "loss: 0.185464  [140800/481450]\n",
      "loss: 0.450218  [144000/481450]\n",
      "loss: 0.315783  [147200/481450]\n",
      "loss: 0.174019  [150400/481450]\n",
      "loss: 0.124360  [153600/481450]\n",
      "loss: 0.249920  [156800/481450]\n",
      "loss: 0.427073  [160000/481450]\n",
      "loss: 0.201825  [163200/481450]\n",
      "loss: 0.290499  [166400/481450]\n",
      "loss: 0.315696  [169600/481450]\n",
      "loss: 0.159115  [172800/481450]\n",
      "loss: 0.218661  [176000/481450]\n",
      "loss: 0.118858  [179200/481450]\n",
      "loss: 0.337749  [182400/481450]\n",
      "loss: 0.309607  [185600/481450]\n",
      "loss: 0.262971  [188800/481450]\n",
      "loss: 0.371006  [192000/481450]\n",
      "loss: 0.306082  [195200/481450]\n",
      "loss: 0.532185  [198400/481450]\n",
      "loss: 0.311497  [201600/481450]\n",
      "loss: 0.136653  [204800/481450]\n",
      "loss: 0.261583  [208000/481450]\n",
      "loss: 0.109183  [211200/481450]\n",
      "loss: 0.200679  [214400/481450]\n",
      "loss: 0.471490  [217600/481450]\n",
      "loss: 0.179656  [220800/481450]\n",
      "loss: 0.335624  [224000/481450]\n",
      "loss: 0.287459  [227200/481450]\n",
      "loss: 0.294498  [230400/481450]\n",
      "loss: 0.181736  [233600/481450]\n",
      "loss: 0.293612  [236800/481450]\n",
      "loss: 0.405832  [240000/481450]\n",
      "loss: 0.337040  [243200/481450]\n",
      "loss: 0.151359  [246400/481450]\n",
      "loss: 0.155032  [249600/481450]\n",
      "loss: 0.123836  [252800/481450]\n",
      "loss: 0.265630  [256000/481450]\n",
      "loss: 0.387098  [259200/481450]\n",
      "loss: 0.208527  [262400/481450]\n",
      "loss: 0.458346  [265600/481450]\n",
      "loss: 0.197407  [268800/481450]\n",
      "loss: 0.233559  [272000/481450]\n",
      "loss: 0.130514  [275200/481450]\n",
      "loss: 0.256367  [278400/481450]\n",
      "loss: 0.327762  [281600/481450]\n",
      "loss: 0.646881  [284800/481450]\n",
      "loss: 0.317795  [288000/481450]\n",
      "loss: 0.367567  [291200/481450]\n",
      "loss: 0.138276  [294400/481450]\n",
      "loss: 0.281350  [297600/481450]\n",
      "loss: 0.447434  [300800/481450]\n",
      "loss: 0.379733  [304000/481450]\n",
      "loss: 0.334205  [307200/481450]\n",
      "loss: 0.309330  [310400/481450]\n",
      "loss: 0.261295  [313600/481450]\n",
      "loss: 0.254519  [316800/481450]\n",
      "loss: 0.267908  [320000/481450]\n",
      "loss: 0.441367  [323200/481450]\n",
      "loss: 0.267932  [326400/481450]\n",
      "loss: 0.124825  [329600/481450]\n",
      "loss: 0.252681  [332800/481450]\n",
      "loss: 0.542871  [336000/481450]\n",
      "loss: 0.187071  [339200/481450]\n",
      "loss: 0.319440  [342400/481450]\n",
      "loss: 0.295904  [345600/481450]\n",
      "loss: 0.074197  [348800/481450]\n",
      "loss: 0.455598  [352000/481450]\n",
      "loss: 0.327378  [355200/481450]\n",
      "loss: 0.156848  [358400/481450]\n",
      "loss: 0.407826  [361600/481450]\n",
      "loss: 0.185432  [364800/481450]\n",
      "loss: 0.215547  [368000/481450]\n",
      "loss: 0.308438  [371200/481450]\n",
      "loss: 0.162165  [374400/481450]\n",
      "loss: 0.275007  [377600/481450]\n",
      "loss: 0.320107  [380800/481450]\n",
      "loss: 0.372074  [384000/481450]\n",
      "loss: 0.317153  [387200/481450]\n",
      "loss: 0.507604  [390400/481450]\n",
      "loss: 0.056035  [393600/481450]\n",
      "loss: 0.092250  [396800/481450]\n",
      "loss: 0.358578  [400000/481450]\n",
      "loss: 0.273712  [403200/481450]\n",
      "loss: 0.111664  [406400/481450]\n",
      "loss: 0.249633  [409600/481450]\n",
      "loss: 0.451177  [412800/481450]\n",
      "loss: 0.265599  [416000/481450]\n",
      "loss: 0.353574  [419200/481450]\n",
      "loss: 0.574854  [422400/481450]\n",
      "loss: 0.226331  [425600/481450]\n",
      "loss: 0.250801  [428800/481450]\n",
      "loss: 0.274126  [432000/481450]\n",
      "loss: 0.182967  [435200/481450]\n",
      "loss: 0.256748  [438400/481450]\n",
      "loss: 0.102339  [441600/481450]\n",
      "loss: 0.296606  [444800/481450]\n",
      "loss: 0.404572  [448000/481450]\n",
      "loss: 0.340015  [451200/481450]\n",
      "loss: 0.325265  [454400/481450]\n",
      "loss: 0.223022  [457600/481450]\n",
      "loss: 0.202182  [460800/481450]\n",
      "loss: 0.161114  [464000/481450]\n",
      "loss: 0.239260  [467200/481450]\n",
      "loss: 0.379739  [470400/481450]\n",
      "loss: 0.180458  [473600/481450]\n",
      "loss: 0.334477  [476800/481450]\n",
      "loss: 0.288174  [480000/481450]\n",
      "Train Accuracy: 89.7348%\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.768779, F1-score: 85.67% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.180142  [    0/481450]\n",
      "loss: 0.239852  [ 3200/481450]\n",
      "loss: 0.260718  [ 6400/481450]\n",
      "loss: 0.393783  [ 9600/481450]\n",
      "loss: 0.258376  [12800/481450]\n",
      "loss: 0.150933  [16000/481450]\n",
      "loss: 0.223958  [19200/481450]\n",
      "loss: 0.102195  [22400/481450]\n",
      "loss: 0.253183  [25600/481450]\n",
      "loss: 0.227412  [28800/481450]\n",
      "loss: 0.217773  [32000/481450]\n",
      "loss: 0.708974  [35200/481450]\n",
      "loss: 0.150063  [38400/481450]\n",
      "loss: 0.534888  [41600/481450]\n",
      "loss: 0.266430  [44800/481450]\n",
      "loss: 0.276328  [48000/481450]\n",
      "loss: 0.291535  [51200/481450]\n",
      "loss: 0.109845  [54400/481450]\n",
      "loss: 0.187393  [57600/481450]\n",
      "loss: 0.144551  [60800/481450]\n",
      "loss: 0.296754  [64000/481450]\n",
      "loss: 0.336890  [67200/481450]\n",
      "loss: 0.369437  [70400/481450]\n",
      "loss: 0.274524  [73600/481450]\n",
      "loss: 0.135390  [76800/481450]\n",
      "loss: 0.241392  [80000/481450]\n",
      "loss: 0.334282  [83200/481450]\n",
      "loss: 0.134118  [86400/481450]\n",
      "loss: 0.278849  [89600/481450]\n",
      "loss: 0.512850  [92800/481450]\n",
      "loss: 0.095685  [96000/481450]\n",
      "loss: 0.108597  [99200/481450]\n",
      "loss: 0.190195  [102400/481450]\n",
      "loss: 0.138825  [105600/481450]\n",
      "loss: 0.249439  [108800/481450]\n",
      "loss: 0.455323  [112000/481450]\n",
      "loss: 0.222787  [115200/481450]\n",
      "loss: 0.230829  [118400/481450]\n",
      "loss: 0.335402  [121600/481450]\n",
      "loss: 0.162873  [124800/481450]\n",
      "loss: 0.345097  [128000/481450]\n",
      "loss: 0.612169  [131200/481450]\n",
      "loss: 0.310313  [134400/481450]\n",
      "loss: 0.349964  [137600/481450]\n",
      "loss: 0.223092  [140800/481450]\n",
      "loss: 0.273566  [144000/481450]\n",
      "loss: 0.083041  [147200/481450]\n",
      "loss: 0.280817  [150400/481450]\n",
      "loss: 0.464717  [153600/481450]\n",
      "loss: 0.127659  [156800/481450]\n",
      "loss: 0.202613  [160000/481450]\n",
      "loss: 0.167428  [163200/481450]\n",
      "loss: 0.346206  [166400/481450]\n",
      "loss: 0.198725  [169600/481450]\n",
      "loss: 0.189545  [172800/481450]\n",
      "loss: 0.312920  [176000/481450]\n",
      "loss: 0.341386  [179200/481450]\n",
      "loss: 0.208726  [182400/481450]\n",
      "loss: 0.836812  [185600/481450]\n",
      "loss: 0.172966  [188800/481450]\n",
      "loss: 0.372561  [192000/481450]\n",
      "loss: 0.104604  [195200/481450]\n",
      "loss: 0.223166  [198400/481450]\n",
      "loss: 0.579408  [201600/481450]\n",
      "loss: 0.123077  [204800/481450]\n",
      "loss: 0.241234  [208000/481450]\n",
      "loss: 0.317043  [211200/481450]\n",
      "loss: 0.339216  [214400/481450]\n",
      "loss: 0.177345  [217600/481450]\n",
      "loss: 0.262589  [220800/481450]\n",
      "loss: 0.070991  [224000/481450]\n",
      "loss: 0.375112  [227200/481450]\n",
      "loss: 0.179432  [230400/481450]\n",
      "loss: 0.144164  [233600/481450]\n",
      "loss: 0.219243  [236800/481450]\n",
      "loss: 0.301126  [240000/481450]\n",
      "loss: 0.197122  [243200/481450]\n",
      "loss: 0.373800  [246400/481450]\n",
      "loss: 0.321198  [249600/481450]\n",
      "loss: 0.332906  [252800/481450]\n",
      "loss: 0.148225  [256000/481450]\n",
      "loss: 0.168625  [259200/481450]\n",
      "loss: 0.529583  [262400/481450]\n",
      "loss: 0.273259  [265600/481450]\n",
      "loss: 0.044104  [268800/481450]\n",
      "loss: 0.305193  [272000/481450]\n",
      "loss: 0.213306  [275200/481450]\n",
      "loss: 0.343966  [278400/481450]\n",
      "loss: 0.306114  [281600/481450]\n",
      "loss: 0.116347  [284800/481450]\n",
      "loss: 0.166360  [288000/481450]\n",
      "loss: 0.193771  [291200/481450]\n",
      "loss: 0.237644  [294400/481450]\n",
      "loss: 0.164884  [297600/481450]\n",
      "loss: 0.251628  [300800/481450]\n",
      "loss: 0.221109  [304000/481450]\n",
      "loss: 0.273997  [307200/481450]\n",
      "loss: 0.250746  [310400/481450]\n",
      "loss: 0.318535  [313600/481450]\n",
      "loss: 0.216932  [316800/481450]\n",
      "loss: 0.105103  [320000/481450]\n",
      "loss: 0.211738  [323200/481450]\n",
      "loss: 0.184888  [326400/481450]\n",
      "loss: 0.367727  [329600/481450]\n",
      "loss: 0.221142  [332800/481450]\n",
      "loss: 0.173029  [336000/481450]\n",
      "loss: 0.148117  [339200/481450]\n",
      "loss: 0.235394  [342400/481450]\n",
      "loss: 0.159994  [345600/481450]\n",
      "loss: 0.292421  [348800/481450]\n",
      "loss: 0.356483  [352000/481450]\n",
      "loss: 0.505777  [355200/481450]\n",
      "loss: 0.247824  [358400/481450]\n",
      "loss: 0.186689  [361600/481450]\n",
      "loss: 0.317915  [364800/481450]\n",
      "loss: 0.165394  [368000/481450]\n",
      "loss: 0.206830  [371200/481450]\n",
      "loss: 0.225156  [374400/481450]\n",
      "loss: 0.254747  [377600/481450]\n",
      "loss: 0.307289  [380800/481450]\n",
      "loss: 0.436320  [384000/481450]\n",
      "loss: 0.076883  [387200/481450]\n",
      "loss: 0.253660  [390400/481450]\n",
      "loss: 0.497684  [393600/481450]\n",
      "loss: 0.308905  [396800/481450]\n",
      "loss: 0.232479  [400000/481450]\n",
      "loss: 0.306238  [403200/481450]\n",
      "loss: 0.281868  [406400/481450]\n",
      "loss: 0.079830  [409600/481450]\n",
      "loss: 0.337616  [412800/481450]\n",
      "loss: 0.616752  [416000/481450]\n",
      "loss: 0.257738  [419200/481450]\n",
      "loss: 0.295317  [422400/481450]\n",
      "loss: 0.246139  [425600/481450]\n",
      "loss: 0.124075  [428800/481450]\n",
      "loss: 0.208702  [432000/481450]\n",
      "loss: 0.127973  [435200/481450]\n",
      "loss: 0.179482  [438400/481450]\n",
      "loss: 0.459361  [441600/481450]\n",
      "loss: 0.212445  [444800/481450]\n",
      "loss: 0.249680  [448000/481450]\n",
      "loss: 0.220061  [451200/481450]\n",
      "loss: 0.219653  [454400/481450]\n",
      "loss: 0.132582  [457600/481450]\n",
      "loss: 0.258556  [460800/481450]\n",
      "loss: 0.146814  [464000/481450]\n",
      "loss: 0.173075  [467200/481450]\n",
      "loss: 0.108629  [470400/481450]\n",
      "loss: 0.331752  [473600/481450]\n",
      "loss: 0.137873  [476800/481450]\n",
      "loss: 0.295051  [480000/481450]\n",
      "Train Accuracy: 89.9042%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.757323, F1-score: 85.60% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.155085  [    0/481450]\n",
      "loss: 0.074140  [ 3200/481450]\n",
      "loss: 0.299499  [ 6400/481450]\n",
      "loss: 0.130096  [ 9600/481450]\n",
      "loss: 0.288419  [12800/481450]\n",
      "loss: 0.185937  [16000/481450]\n",
      "loss: 0.163558  [19200/481450]\n",
      "loss: 0.429469  [22400/481450]\n",
      "loss: 0.245284  [25600/481450]\n",
      "loss: 0.295991  [28800/481450]\n",
      "loss: 0.056457  [32000/481450]\n",
      "loss: 0.145287  [35200/481450]\n",
      "loss: 0.249437  [38400/481450]\n",
      "loss: 0.347598  [41600/481450]\n",
      "loss: 0.299347  [44800/481450]\n",
      "loss: 0.346633  [48000/481450]\n",
      "loss: 0.230323  [51200/481450]\n",
      "loss: 0.105958  [54400/481450]\n",
      "loss: 0.223923  [57600/481450]\n",
      "loss: 0.302284  [60800/481450]\n",
      "loss: 0.374047  [64000/481450]\n",
      "loss: 0.201026  [67200/481450]\n",
      "loss: 0.445696  [70400/481450]\n",
      "loss: 0.259343  [73600/481450]\n",
      "loss: 0.200668  [76800/481450]\n",
      "loss: 0.078152  [80000/481450]\n",
      "loss: 0.111708  [83200/481450]\n",
      "loss: 0.206139  [86400/481450]\n",
      "loss: 0.213013  [89600/481450]\n",
      "loss: 0.199693  [92800/481450]\n",
      "loss: 0.272968  [96000/481450]\n",
      "loss: 0.272790  [99200/481450]\n",
      "loss: 0.128070  [102400/481450]\n",
      "loss: 0.156114  [105600/481450]\n",
      "loss: 0.364656  [108800/481450]\n",
      "loss: 0.327471  [112000/481450]\n",
      "loss: 0.559836  [115200/481450]\n",
      "loss: 0.072758  [118400/481450]\n",
      "loss: 0.344329  [121600/481450]\n",
      "loss: 0.285602  [124800/481450]\n",
      "loss: 0.240000  [128000/481450]\n",
      "loss: 0.185304  [131200/481450]\n",
      "loss: 0.130390  [134400/481450]\n",
      "loss: 0.266191  [137600/481450]\n",
      "loss: 0.274098  [140800/481450]\n",
      "loss: 0.219367  [144000/481450]\n",
      "loss: 0.155919  [147200/481450]\n",
      "loss: 0.145126  [150400/481450]\n",
      "loss: 0.267054  [153600/481450]\n",
      "loss: 0.098560  [156800/481450]\n",
      "loss: 0.168048  [160000/481450]\n",
      "loss: 0.141977  [163200/481450]\n",
      "loss: 0.304782  [166400/481450]\n",
      "loss: 0.245967  [169600/481450]\n",
      "loss: 0.282222  [172800/481450]\n",
      "loss: 0.204164  [176000/481450]\n",
      "loss: 0.358297  [179200/481450]\n",
      "loss: 0.318884  [182400/481450]\n",
      "loss: 0.182011  [185600/481450]\n",
      "loss: 0.144439  [188800/481450]\n",
      "loss: 0.232027  [192000/481450]\n",
      "loss: 0.111233  [195200/481450]\n",
      "loss: 0.088712  [198400/481450]\n",
      "loss: 0.334311  [201600/481450]\n",
      "loss: 0.162286  [204800/481450]\n",
      "loss: 0.147404  [208000/481450]\n",
      "loss: 0.263730  [211200/481450]\n",
      "loss: 0.101891  [214400/481450]\n",
      "loss: 0.175661  [217600/481450]\n",
      "loss: 0.218195  [220800/481450]\n",
      "loss: 0.133179  [224000/481450]\n",
      "loss: 0.431308  [227200/481450]\n",
      "loss: 0.212714  [230400/481450]\n",
      "loss: 0.209841  [233600/481450]\n",
      "loss: 0.225133  [236800/481450]\n",
      "loss: 0.117436  [240000/481450]\n",
      "loss: 0.110797  [243200/481450]\n",
      "loss: 0.350781  [246400/481450]\n",
      "loss: 0.205542  [249600/481450]\n",
      "loss: 0.206997  [252800/481450]\n",
      "loss: 0.159512  [256000/481450]\n",
      "loss: 0.348172  [259200/481450]\n",
      "loss: 0.367545  [262400/481450]\n",
      "loss: 0.300971  [265600/481450]\n",
      "loss: 0.265993  [268800/481450]\n",
      "loss: 0.200902  [272000/481450]\n",
      "loss: 0.530240  [275200/481450]\n",
      "loss: 0.206209  [278400/481450]\n",
      "loss: 0.242300  [281600/481450]\n",
      "loss: 0.598204  [284800/481450]\n",
      "loss: 0.136342  [288000/481450]\n",
      "loss: 0.252433  [291200/481450]\n",
      "loss: 0.148714  [294400/481450]\n",
      "loss: 0.192391  [297600/481450]\n",
      "loss: 0.367105  [300800/481450]\n",
      "loss: 0.126080  [304000/481450]\n",
      "loss: 0.101047  [307200/481450]\n",
      "loss: 0.158508  [310400/481450]\n",
      "loss: 0.226432  [313600/481450]\n",
      "loss: 0.287413  [316800/481450]\n",
      "loss: 0.800247  [320000/481450]\n",
      "loss: 0.247934  [323200/481450]\n",
      "loss: 0.135274  [326400/481450]\n",
      "loss: 0.339539  [329600/481450]\n",
      "loss: 0.357725  [332800/481450]\n",
      "loss: 0.212419  [336000/481450]\n",
      "loss: 0.124083  [339200/481450]\n",
      "loss: 0.287231  [342400/481450]\n",
      "loss: 0.083500  [345600/481450]\n",
      "loss: 0.197857  [348800/481450]\n",
      "loss: 0.239326  [352000/481450]\n",
      "loss: 0.270165  [355200/481450]\n",
      "loss: 0.236806  [358400/481450]\n",
      "loss: 0.148211  [361600/481450]\n",
      "loss: 0.313445  [364800/481450]\n",
      "loss: 0.237138  [368000/481450]\n",
      "loss: 0.214850  [371200/481450]\n",
      "loss: 0.217881  [374400/481450]\n",
      "loss: 0.224767  [377600/481450]\n",
      "loss: 0.343877  [380800/481450]\n",
      "loss: 0.208234  [384000/481450]\n",
      "loss: 0.112791  [387200/481450]\n",
      "loss: 0.162575  [390400/481450]\n",
      "loss: 0.285339  [393600/481450]\n",
      "loss: 0.268000  [396800/481450]\n",
      "loss: 0.248240  [400000/481450]\n",
      "loss: 0.353295  [403200/481450]\n",
      "loss: 0.084909  [406400/481450]\n",
      "loss: 0.143755  [409600/481450]\n",
      "loss: 0.169424  [412800/481450]\n",
      "loss: 0.295900  [416000/481450]\n",
      "loss: 0.195587  [419200/481450]\n",
      "loss: 0.255908  [422400/481450]\n",
      "loss: 0.384041  [425600/481450]\n",
      "loss: 0.216537  [428800/481450]\n",
      "loss: 0.358881  [432000/481450]\n",
      "loss: 0.218199  [435200/481450]\n",
      "loss: 0.219585  [438400/481450]\n",
      "loss: 0.263662  [441600/481450]\n",
      "loss: 0.273155  [444800/481450]\n",
      "loss: 0.117378  [448000/481450]\n",
      "loss: 0.226487  [451200/481450]\n",
      "loss: 0.300087  [454400/481450]\n",
      "loss: 0.257347  [457600/481450]\n",
      "loss: 0.270552  [460800/481450]\n",
      "loss: 0.210840  [464000/481450]\n",
      "loss: 0.172039  [467200/481450]\n",
      "loss: 0.226312  [470400/481450]\n",
      "loss: 0.275306  [473600/481450]\n",
      "loss: 0.259804  [476800/481450]\n",
      "loss: 0.191352  [480000/481450]\n",
      "Train Accuracy: 90.0424%\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.758729, F1-score: 85.60% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.261755  [    0/481450]\n",
      "loss: 0.312532  [ 3200/481450]\n",
      "loss: 0.224691  [ 6400/481450]\n",
      "loss: 0.391143  [ 9600/481450]\n",
      "loss: 0.128718  [12800/481450]\n",
      "loss: 0.364092  [16000/481450]\n",
      "loss: 0.114532  [19200/481450]\n",
      "loss: 0.137761  [22400/481450]\n",
      "loss: 0.196940  [25600/481450]\n",
      "loss: 0.190530  [28800/481450]\n",
      "loss: 0.124312  [32000/481450]\n",
      "loss: 0.226783  [35200/481450]\n",
      "loss: 0.223365  [38400/481450]\n",
      "loss: 0.123930  [41600/481450]\n",
      "loss: 0.209235  [44800/481450]\n",
      "loss: 0.402861  [48000/481450]\n",
      "loss: 0.218320  [51200/481450]\n",
      "loss: 0.094199  [54400/481450]\n",
      "loss: 0.452533  [57600/481450]\n",
      "loss: 0.219356  [60800/481450]\n",
      "loss: 0.016567  [64000/481450]\n",
      "loss: 0.275529  [67200/481450]\n",
      "loss: 0.483102  [70400/481450]\n",
      "loss: 0.328759  [73600/481450]\n",
      "loss: 0.312766  [76800/481450]\n",
      "loss: 0.241970  [80000/481450]\n",
      "loss: 0.209931  [83200/481450]\n",
      "loss: 0.402909  [86400/481450]\n",
      "loss: 0.245859  [89600/481450]\n",
      "loss: 0.225943  [92800/481450]\n",
      "loss: 0.305480  [96000/481450]\n",
      "loss: 0.212771  [99200/481450]\n",
      "loss: 0.110829  [102400/481450]\n",
      "loss: 0.290394  [105600/481450]\n",
      "loss: 0.311280  [108800/481450]\n",
      "loss: 0.234078  [112000/481450]\n",
      "loss: 0.470472  [115200/481450]\n",
      "loss: 0.177111  [118400/481450]\n",
      "loss: 0.145360  [121600/481450]\n",
      "loss: 0.384821  [124800/481450]\n",
      "loss: 0.177515  [128000/481450]\n",
      "loss: 0.328244  [131200/481450]\n",
      "loss: 0.138280  [134400/481450]\n",
      "loss: 0.257093  [137600/481450]\n",
      "loss: 0.172802  [140800/481450]\n",
      "loss: 0.450662  [144000/481450]\n",
      "loss: 0.176766  [147200/481450]\n",
      "loss: 0.467074  [150400/481450]\n",
      "loss: 0.230760  [153600/481450]\n",
      "loss: 0.208141  [156800/481450]\n",
      "loss: 0.189323  [160000/481450]\n",
      "loss: 0.149895  [163200/481450]\n",
      "loss: 0.423341  [166400/481450]\n",
      "loss: 0.471685  [169600/481450]\n",
      "loss: 0.242280  [172800/481450]\n",
      "loss: 0.295403  [176000/481450]\n",
      "loss: 0.218728  [179200/481450]\n",
      "loss: 0.132532  [182400/481450]\n",
      "loss: 0.176667  [185600/481450]\n",
      "loss: 0.466576  [188800/481450]\n",
      "loss: 0.123657  [192000/481450]\n",
      "loss: 0.125188  [195200/481450]\n",
      "loss: 0.179849  [198400/481450]\n",
      "loss: 0.271350  [201600/481450]\n",
      "loss: 0.168043  [204800/481450]\n",
      "loss: 0.142347  [208000/481450]\n",
      "loss: 0.198352  [211200/481450]\n",
      "loss: 0.097108  [214400/481450]\n",
      "loss: 0.286464  [217600/481450]\n",
      "loss: 0.110661  [220800/481450]\n",
      "loss: 0.096427  [224000/481450]\n",
      "loss: 0.337571  [227200/481450]\n",
      "loss: 0.250675  [230400/481450]\n",
      "loss: 0.215602  [233600/481450]\n",
      "loss: 0.195860  [236800/481450]\n",
      "loss: 0.183885  [240000/481450]\n",
      "loss: 0.234930  [243200/481450]\n",
      "loss: 0.288841  [246400/481450]\n",
      "loss: 0.279015  [249600/481450]\n",
      "loss: 0.201947  [252800/481450]\n",
      "loss: 0.186454  [256000/481450]\n",
      "loss: 0.086028  [259200/481450]\n",
      "loss: 0.392998  [262400/481450]\n",
      "loss: 0.172610  [265600/481450]\n",
      "loss: 0.171310  [268800/481450]\n",
      "loss: 0.315991  [272000/481450]\n",
      "loss: 0.238629  [275200/481450]\n",
      "loss: 0.111648  [278400/481450]\n",
      "loss: 0.191009  [281600/481450]\n",
      "loss: 0.129396  [284800/481450]\n",
      "loss: 0.304076  [288000/481450]\n",
      "loss: 0.288318  [291200/481450]\n",
      "loss: 0.205532  [294400/481450]\n",
      "loss: 0.109801  [297600/481450]\n",
      "loss: 0.144961  [300800/481450]\n",
      "loss: 0.386828  [304000/481450]\n",
      "loss: 0.521413  [307200/481450]\n",
      "loss: 0.339403  [310400/481450]\n",
      "loss: 0.223802  [313600/481450]\n",
      "loss: 0.357751  [316800/481450]\n",
      "loss: 0.182927  [320000/481450]\n",
      "loss: 0.276148  [323200/481450]\n",
      "loss: 0.186450  [326400/481450]\n",
      "loss: 0.633570  [329600/481450]\n",
      "loss: 0.164758  [332800/481450]\n",
      "loss: 0.336505  [336000/481450]\n",
      "loss: 0.210887  [339200/481450]\n",
      "loss: 0.173771  [342400/481450]\n",
      "loss: 0.477948  [345600/481450]\n",
      "loss: 0.161018  [348800/481450]\n",
      "loss: 0.292830  [352000/481450]\n",
      "loss: 0.387676  [355200/481450]\n",
      "loss: 0.206937  [358400/481450]\n",
      "loss: 0.242181  [361600/481450]\n",
      "loss: 0.174146  [364800/481450]\n",
      "loss: 0.321658  [368000/481450]\n",
      "loss: 0.199994  [371200/481450]\n",
      "loss: 0.246655  [374400/481450]\n",
      "loss: 0.431050  [377600/481450]\n",
      "loss: 0.230743  [380800/481450]\n",
      "loss: 0.154843  [384000/481450]\n",
      "loss: 0.428938  [387200/481450]\n",
      "loss: 0.255706  [390400/481450]\n",
      "loss: 0.153848  [393600/481450]\n",
      "loss: 0.368127  [396800/481450]\n",
      "loss: 0.374535  [400000/481450]\n",
      "loss: 0.150325  [403200/481450]\n",
      "loss: 0.231231  [406400/481450]\n",
      "loss: 0.155636  [409600/481450]\n",
      "loss: 0.132632  [412800/481450]\n",
      "loss: 0.187081  [416000/481450]\n",
      "loss: 0.182259  [419200/481450]\n",
      "loss: 0.237901  [422400/481450]\n",
      "loss: 0.300286  [425600/481450]\n",
      "loss: 0.153306  [428800/481450]\n",
      "loss: 0.167256  [432000/481450]\n",
      "loss: 0.076625  [435200/481450]\n",
      "loss: 0.252486  [438400/481450]\n",
      "loss: 0.073635  [441600/481450]\n",
      "loss: 0.270920  [444800/481450]\n",
      "loss: 0.361507  [448000/481450]\n",
      "loss: 0.297666  [451200/481450]\n",
      "loss: 0.133876  [454400/481450]\n",
      "loss: 0.252392  [457600/481450]\n",
      "loss: 0.287896  [460800/481450]\n",
      "loss: 0.123666  [464000/481450]\n",
      "loss: 0.096995  [467200/481450]\n",
      "loss: 0.064098  [470400/481450]\n",
      "loss: 0.254300  [473600/481450]\n",
      "loss: 0.165767  [476800/481450]\n",
      "loss: 0.276225  [480000/481450]\n",
      "Train Accuracy: 90.2198%\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.775049, F1-score: 85.40% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.294348  [    0/481450]\n",
      "loss: 0.170006  [ 3200/481450]\n",
      "loss: 0.268365  [ 6400/481450]\n",
      "loss: 0.172905  [ 9600/481450]\n",
      "loss: 0.772798  [12800/481450]\n",
      "loss: 0.403838  [16000/481450]\n",
      "loss: 0.235987  [19200/481450]\n",
      "loss: 0.266354  [22400/481450]\n",
      "loss: 0.214499  [25600/481450]\n",
      "loss: 0.275565  [28800/481450]\n",
      "loss: 0.388180  [32000/481450]\n",
      "loss: 0.127508  [35200/481450]\n",
      "loss: 0.194358  [38400/481450]\n",
      "loss: 0.235801  [41600/481450]\n",
      "loss: 0.130898  [44800/481450]\n",
      "loss: 0.299926  [48000/481450]\n",
      "loss: 0.287508  [51200/481450]\n",
      "loss: 0.228670  [54400/481450]\n",
      "loss: 0.277091  [57600/481450]\n",
      "loss: 0.146508  [60800/481450]\n",
      "loss: 0.529064  [64000/481450]\n",
      "loss: 0.309376  [67200/481450]\n",
      "loss: 0.152443  [70400/481450]\n",
      "loss: 0.140209  [73600/481450]\n",
      "loss: 0.190966  [76800/481450]\n",
      "loss: 0.266564  [80000/481450]\n",
      "loss: 0.101871  [83200/481450]\n",
      "loss: 0.194629  [86400/481450]\n",
      "loss: 0.249431  [89600/481450]\n",
      "loss: 0.127449  [92800/481450]\n",
      "loss: 0.301833  [96000/481450]\n",
      "loss: 0.176244  [99200/481450]\n",
      "loss: 0.132914  [102400/481450]\n",
      "loss: 0.447344  [105600/481450]\n",
      "loss: 0.177081  [108800/481450]\n",
      "loss: 0.204179  [112000/481450]\n",
      "loss: 0.064699  [115200/481450]\n",
      "loss: 0.333107  [118400/481450]\n",
      "loss: 0.244852  [121600/481450]\n",
      "loss: 0.209627  [124800/481450]\n",
      "loss: 0.211995  [128000/481450]\n",
      "loss: 0.229676  [131200/481450]\n",
      "loss: 0.236378  [134400/481450]\n",
      "loss: 0.119737  [137600/481450]\n",
      "loss: 0.233238  [140800/481450]\n",
      "loss: 0.242046  [144000/481450]\n",
      "loss: 0.119054  [147200/481450]\n",
      "loss: 0.192832  [150400/481450]\n",
      "loss: 0.200164  [153600/481450]\n",
      "loss: 0.274378  [156800/481450]\n",
      "loss: 0.291510  [160000/481450]\n",
      "loss: 0.177180  [163200/481450]\n",
      "loss: 0.096032  [166400/481450]\n",
      "loss: 0.232164  [169600/481450]\n",
      "loss: 0.140329  [172800/481450]\n",
      "loss: 0.333911  [176000/481450]\n",
      "loss: 0.206170  [179200/481450]\n",
      "loss: 0.144428  [182400/481450]\n",
      "loss: 0.201597  [185600/481450]\n",
      "loss: 0.258180  [188800/481450]\n",
      "loss: 0.329212  [192000/481450]\n",
      "loss: 0.216486  [195200/481450]\n",
      "loss: 0.311704  [198400/481450]\n",
      "loss: 0.290903  [201600/481450]\n",
      "loss: 0.171670  [204800/481450]\n",
      "loss: 0.260206  [208000/481450]\n",
      "loss: 0.230946  [211200/481450]\n",
      "loss: 0.476095  [214400/481450]\n",
      "loss: 0.147159  [217600/481450]\n",
      "loss: 0.302824  [220800/481450]\n",
      "loss: 0.154315  [224000/481450]\n",
      "loss: 0.323307  [227200/481450]\n",
      "loss: 0.363613  [230400/481450]\n",
      "loss: 0.179577  [233600/481450]\n",
      "loss: 0.342021  [236800/481450]\n",
      "loss: 0.263590  [240000/481450]\n",
      "loss: 0.268414  [243200/481450]\n",
      "loss: 0.213243  [246400/481450]\n",
      "loss: 0.323688  [249600/481450]\n",
      "loss: 0.249633  [252800/481450]\n",
      "loss: 0.201859  [256000/481450]\n",
      "loss: 0.223009  [259200/481450]\n",
      "loss: 0.206263  [262400/481450]\n",
      "loss: 0.296701  [265600/481450]\n",
      "loss: 0.227380  [268800/481450]\n",
      "loss: 0.181258  [272000/481450]\n",
      "loss: 0.194186  [275200/481450]\n",
      "loss: 0.211199  [278400/481450]\n",
      "loss: 0.268796  [281600/481450]\n",
      "loss: 0.128138  [284800/481450]\n",
      "loss: 0.445760  [288000/481450]\n",
      "loss: 0.219989  [291200/481450]\n",
      "loss: 0.198923  [294400/481450]\n",
      "loss: 0.055372  [297600/481450]\n",
      "loss: 0.096842  [300800/481450]\n",
      "loss: 0.319080  [304000/481450]\n",
      "loss: 0.252300  [307200/481450]\n",
      "loss: 0.206773  [310400/481450]\n",
      "loss: 0.161726  [313600/481450]\n",
      "loss: 0.278746  [316800/481450]\n",
      "loss: 0.145454  [320000/481450]\n",
      "loss: 0.253586  [323200/481450]\n",
      "loss: 0.384331  [326400/481450]\n",
      "loss: 0.373107  [329600/481450]\n",
      "loss: 0.206333  [332800/481450]\n",
      "loss: 0.337872  [336000/481450]\n",
      "loss: 0.238748  [339200/481450]\n",
      "loss: 0.269303  [342400/481450]\n",
      "loss: 0.314782  [345600/481450]\n",
      "loss: 0.468157  [348800/481450]\n",
      "loss: 0.436524  [352000/481450]\n",
      "loss: 0.213069  [355200/481450]\n",
      "loss: 0.201964  [358400/481450]\n",
      "loss: 0.174937  [361600/481450]\n",
      "loss: 0.266482  [364800/481450]\n",
      "loss: 0.310722  [368000/481450]\n",
      "loss: 0.374035  [371200/481450]\n",
      "loss: 0.148147  [374400/481450]\n",
      "loss: 0.222255  [377600/481450]\n",
      "loss: 0.223548  [380800/481450]\n",
      "loss: 0.165106  [384000/481450]\n",
      "loss: 0.121978  [387200/481450]\n",
      "loss: 0.219588  [390400/481450]\n",
      "loss: 0.159083  [393600/481450]\n",
      "loss: 0.370129  [396800/481450]\n",
      "loss: 0.299021  [400000/481450]\n",
      "loss: 0.402990  [403200/481450]\n",
      "loss: 0.286780  [406400/481450]\n",
      "loss: 0.201756  [409600/481450]\n",
      "loss: 0.207724  [412800/481450]\n",
      "loss: 0.257755  [416000/481450]\n",
      "loss: 0.254569  [419200/481450]\n",
      "loss: 0.351247  [422400/481450]\n",
      "loss: 0.232311  [425600/481450]\n",
      "loss: 0.368502  [428800/481450]\n",
      "loss: 0.398198  [432000/481450]\n",
      "loss: 0.263837  [435200/481450]\n",
      "loss: 0.337330  [438400/481450]\n",
      "loss: 0.319684  [441600/481450]\n",
      "loss: 0.299495  [444800/481450]\n",
      "loss: 0.459719  [448000/481450]\n",
      "loss: 0.264479  [451200/481450]\n",
      "loss: 0.149815  [454400/481450]\n",
      "loss: 0.274347  [457600/481450]\n",
      "loss: 0.295465  [460800/481450]\n",
      "loss: 0.170627  [464000/481450]\n",
      "loss: 0.231617  [467200/481450]\n",
      "loss: 0.272382  [470400/481450]\n",
      "loss: 0.269901  [473600/481450]\n",
      "loss: 0.169852  [476800/481450]\n",
      "loss: 0.135990  [480000/481450]\n",
      "Train Accuracy: 90.3101%\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.812963, F1-score: 85.13% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.340027  [    0/481450]\n",
      "loss: 0.104602  [ 3200/481450]\n",
      "loss: 0.250550  [ 6400/481450]\n",
      "loss: 0.187225  [ 9600/481450]\n",
      "loss: 0.303004  [12800/481450]\n",
      "loss: 0.153820  [16000/481450]\n",
      "loss: 0.276194  [19200/481450]\n",
      "loss: 0.145767  [22400/481450]\n",
      "loss: 0.203011  [25600/481450]\n",
      "loss: 0.273502  [28800/481450]\n",
      "loss: 0.097935  [32000/481450]\n",
      "loss: 0.091734  [35200/481450]\n",
      "loss: 0.096401  [38400/481450]\n",
      "loss: 0.380512  [41600/481450]\n",
      "loss: 0.231370  [44800/481450]\n",
      "loss: 0.253049  [48000/481450]\n",
      "loss: 0.158972  [51200/481450]\n",
      "loss: 0.176755  [54400/481450]\n",
      "loss: 0.127146  [57600/481450]\n",
      "loss: 0.249694  [60800/481450]\n",
      "loss: 0.372946  [64000/481450]\n",
      "loss: 0.133106  [67200/481450]\n",
      "loss: 0.255481  [70400/481450]\n",
      "loss: 0.290658  [73600/481450]\n",
      "loss: 0.133395  [76800/481450]\n",
      "loss: 0.131287  [80000/481450]\n",
      "loss: 0.455152  [83200/481450]\n",
      "loss: 0.186663  [86400/481450]\n",
      "loss: 0.106455  [89600/481450]\n",
      "loss: 0.093863  [92800/481450]\n",
      "loss: 0.247332  [96000/481450]\n",
      "loss: 0.231884  [99200/481450]\n",
      "loss: 0.145731  [102400/481450]\n",
      "loss: 0.223916  [105600/481450]\n",
      "loss: 0.195544  [108800/481450]\n",
      "loss: 0.176310  [112000/481450]\n",
      "loss: 0.197000  [115200/481450]\n",
      "loss: 0.203192  [118400/481450]\n",
      "loss: 0.174812  [121600/481450]\n",
      "loss: 0.242386  [124800/481450]\n",
      "loss: 0.517247  [128000/481450]\n",
      "loss: 0.129201  [131200/481450]\n",
      "loss: 0.412874  [134400/481450]\n",
      "loss: 0.627913  [137600/481450]\n",
      "loss: 0.350925  [140800/481450]\n",
      "loss: 0.176028  [144000/481450]\n",
      "loss: 0.214803  [147200/481450]\n",
      "loss: 0.152056  [150400/481450]\n",
      "loss: 0.110939  [153600/481450]\n",
      "loss: 0.159337  [156800/481450]\n",
      "loss: 0.154428  [160000/481450]\n",
      "loss: 0.303473  [163200/481450]\n",
      "loss: 0.217050  [166400/481450]\n",
      "loss: 0.263179  [169600/481450]\n",
      "loss: 0.243180  [172800/481450]\n",
      "loss: 0.313490  [176000/481450]\n",
      "loss: 0.221161  [179200/481450]\n",
      "loss: 0.162294  [182400/481450]\n",
      "loss: 0.215970  [185600/481450]\n",
      "loss: 0.331823  [188800/481450]\n",
      "loss: 0.344281  [192000/481450]\n",
      "loss: 0.228330  [195200/481450]\n",
      "loss: 0.391066  [198400/481450]\n",
      "loss: 0.275821  [201600/481450]\n",
      "loss: 0.379676  [204800/481450]\n",
      "loss: 0.423000  [208000/481450]\n",
      "loss: 0.126360  [211200/481450]\n",
      "loss: 0.353716  [214400/481450]\n",
      "loss: 0.368588  [217600/481450]\n",
      "loss: 0.141684  [220800/481450]\n",
      "loss: 0.060310  [224000/481450]\n",
      "loss: 0.582769  [227200/481450]\n",
      "loss: 0.213747  [230400/481450]\n",
      "loss: 0.553317  [233600/481450]\n",
      "loss: 0.217902  [236800/481450]\n",
      "loss: 0.335341  [240000/481450]\n",
      "loss: 0.209780  [243200/481450]\n",
      "loss: 0.265538  [246400/481450]\n",
      "loss: 0.219671  [249600/481450]\n",
      "loss: 0.086553  [252800/481450]\n",
      "loss: 0.214671  [256000/481450]\n",
      "loss: 0.287641  [259200/481450]\n",
      "loss: 0.141098  [262400/481450]\n",
      "loss: 0.444149  [265600/481450]\n",
      "loss: 0.131826  [268800/481450]\n",
      "loss: 0.114047  [272000/481450]\n",
      "loss: 0.226570  [275200/481450]\n",
      "loss: 0.263699  [278400/481450]\n",
      "loss: 0.361539  [281600/481450]\n",
      "loss: 0.196175  [284800/481450]\n",
      "loss: 0.130614  [288000/481450]\n",
      "loss: 0.125510  [291200/481450]\n",
      "loss: 0.128624  [294400/481450]\n",
      "loss: 0.140379  [297600/481450]\n",
      "loss: 0.236734  [300800/481450]\n",
      "loss: 0.484711  [304000/481450]\n",
      "loss: 0.136298  [307200/481450]\n",
      "loss: 0.274135  [310400/481450]\n",
      "loss: 0.105069  [313600/481450]\n",
      "loss: 0.376263  [316800/481450]\n",
      "loss: 0.427566  [320000/481450]\n",
      "loss: 0.367121  [323200/481450]\n",
      "loss: 0.261537  [326400/481450]\n",
      "loss: 0.302268  [329600/481450]\n",
      "loss: 0.182536  [332800/481450]\n",
      "loss: 0.202335  [336000/481450]\n",
      "loss: 0.306912  [339200/481450]\n",
      "loss: 0.114274  [342400/481450]\n",
      "loss: 0.126710  [345600/481450]\n",
      "loss: 0.385507  [348800/481450]\n",
      "loss: 0.211557  [352000/481450]\n",
      "loss: 0.239329  [355200/481450]\n",
      "loss: 0.226262  [358400/481450]\n",
      "loss: 0.320415  [361600/481450]\n",
      "loss: 0.125316  [364800/481450]\n",
      "loss: 0.194262  [368000/481450]\n",
      "loss: 0.278122  [371200/481450]\n",
      "loss: 0.340258  [374400/481450]\n",
      "loss: 0.162250  [377600/481450]\n",
      "loss: 0.351006  [380800/481450]\n",
      "loss: 0.214960  [384000/481450]\n",
      "loss: 0.340874  [387200/481450]\n",
      "loss: 0.144323  [390400/481450]\n",
      "loss: 0.207741  [393600/481450]\n",
      "loss: 0.277332  [396800/481450]\n",
      "loss: 0.415344  [400000/481450]\n",
      "loss: 0.394362  [403200/481450]\n",
      "loss: 0.195870  [406400/481450]\n",
      "loss: 0.342496  [409600/481450]\n",
      "loss: 0.317928  [412800/481450]\n",
      "loss: 0.240895  [416000/481450]\n",
      "loss: 0.241952  [419200/481450]\n",
      "loss: 0.415815  [422400/481450]\n",
      "loss: 0.184299  [425600/481450]\n",
      "loss: 0.228657  [428800/481450]\n",
      "loss: 0.136263  [432000/481450]\n",
      "loss: 0.129962  [435200/481450]\n",
      "loss: 0.143887  [438400/481450]\n",
      "loss: 0.213560  [441600/481450]\n",
      "loss: 0.327052  [444800/481450]\n",
      "loss: 0.360251  [448000/481450]\n",
      "loss: 0.143193  [451200/481450]\n",
      "loss: 0.344602  [454400/481450]\n",
      "loss: 0.194586  [457600/481450]\n",
      "loss: 0.096688  [460800/481450]\n",
      "loss: 0.316733  [464000/481450]\n",
      "loss: 0.172011  [467200/481450]\n",
      "loss: 0.267582  [470400/481450]\n",
      "loss: 0.341176  [473600/481450]\n",
      "loss: 0.220998  [476800/481450]\n",
      "loss: 0.153816  [480000/481450]\n",
      "Train Accuracy: 90.4254%\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.819137, F1-score: 85.35% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.206447  [    0/481450]\n",
      "loss: 0.095173  [ 3200/481450]\n",
      "loss: 0.167290  [ 6400/481450]\n",
      "loss: 0.198475  [ 9600/481450]\n",
      "loss: 0.073495  [12800/481450]\n",
      "loss: 0.232844  [16000/481450]\n",
      "loss: 0.323984  [19200/481450]\n",
      "loss: 0.131700  [22400/481450]\n",
      "loss: 0.338105  [25600/481450]\n",
      "loss: 0.179880  [28800/481450]\n",
      "loss: 0.335339  [32000/481450]\n",
      "loss: 0.081896  [35200/481450]\n",
      "loss: 0.130361  [38400/481450]\n",
      "loss: 0.179850  [41600/481450]\n",
      "loss: 0.177776  [44800/481450]\n",
      "loss: 0.269681  [48000/481450]\n",
      "loss: 0.171631  [51200/481450]\n",
      "loss: 0.087772  [54400/481450]\n",
      "loss: 0.256893  [57600/481450]\n",
      "loss: 0.267244  [60800/481450]\n",
      "loss: 0.481218  [64000/481450]\n",
      "loss: 0.212642  [67200/481450]\n",
      "loss: 0.232884  [70400/481450]\n",
      "loss: 0.132787  [73600/481450]\n",
      "loss: 0.156079  [76800/481450]\n",
      "loss: 0.289059  [80000/481450]\n",
      "loss: 0.259288  [83200/481450]\n",
      "loss: 0.091834  [86400/481450]\n",
      "loss: 0.308227  [89600/481450]\n",
      "loss: 0.236041  [92800/481450]\n",
      "loss: 0.128072  [96000/481450]\n",
      "loss: 0.201480  [99200/481450]\n",
      "loss: 0.168775  [102400/481450]\n",
      "loss: 0.131095  [105600/481450]\n",
      "loss: 0.114518  [108800/481450]\n",
      "loss: 0.286707  [112000/481450]\n",
      "loss: 0.105199  [115200/481450]\n",
      "loss: 0.146617  [118400/481450]\n",
      "loss: 0.274697  [121600/481450]\n",
      "loss: 0.206740  [124800/481450]\n",
      "loss: 0.145590  [128000/481450]\n",
      "loss: 0.159139  [131200/481450]\n",
      "loss: 0.195864  [134400/481450]\n",
      "loss: 0.237096  [137600/481450]\n",
      "loss: 0.165842  [140800/481450]\n",
      "loss: 0.135364  [144000/481450]\n",
      "loss: 0.229570  [147200/481450]\n",
      "loss: 0.344264  [150400/481450]\n",
      "loss: 0.223512  [153600/481450]\n",
      "loss: 0.149187  [156800/481450]\n",
      "loss: 0.141434  [160000/481450]\n",
      "loss: 0.177611  [163200/481450]\n",
      "loss: 0.196319  [166400/481450]\n",
      "loss: 0.247779  [169600/481450]\n",
      "loss: 0.378073  [172800/481450]\n",
      "loss: 0.269634  [176000/481450]\n",
      "loss: 0.088052  [179200/481450]\n",
      "loss: 0.479307  [182400/481450]\n",
      "loss: 0.117250  [185600/481450]\n",
      "loss: 0.313155  [188800/481450]\n",
      "loss: 0.159781  [192000/481450]\n",
      "loss: 0.370855  [195200/481450]\n",
      "loss: 0.230991  [198400/481450]\n",
      "loss: 0.171995  [201600/481450]\n",
      "loss: 0.219070  [204800/481450]\n",
      "loss: 0.263897  [208000/481450]\n",
      "loss: 0.090454  [211200/481450]\n",
      "loss: 0.412530  [214400/481450]\n",
      "loss: 0.213990  [217600/481450]\n",
      "loss: 0.426507  [220800/481450]\n",
      "loss: 0.346384  [224000/481450]\n",
      "loss: 0.264043  [227200/481450]\n",
      "loss: 0.117833  [230400/481450]\n",
      "loss: 0.242398  [233600/481450]\n",
      "loss: 0.159031  [236800/481450]\n",
      "loss: 0.181291  [240000/481450]\n",
      "loss: 0.394733  [243200/481450]\n",
      "loss: 0.389873  [246400/481450]\n",
      "loss: 0.273762  [249600/481450]\n",
      "loss: 0.105568  [252800/481450]\n",
      "loss: 0.170401  [256000/481450]\n",
      "loss: 0.194314  [259200/481450]\n",
      "loss: 0.473265  [262400/481450]\n",
      "loss: 0.333107  [265600/481450]\n",
      "loss: 0.360043  [268800/481450]\n",
      "loss: 0.375783  [272000/481450]\n",
      "loss: 0.115112  [275200/481450]\n",
      "loss: 0.154270  [278400/481450]\n",
      "loss: 0.322790  [281600/481450]\n",
      "loss: 0.336556  [284800/481450]\n",
      "loss: 0.337409  [288000/481450]\n",
      "loss: 0.150529  [291200/481450]\n",
      "loss: 0.219393  [294400/481450]\n",
      "loss: 0.626465  [297600/481450]\n",
      "loss: 0.157318  [300800/481450]\n",
      "loss: 0.174992  [304000/481450]\n",
      "loss: 0.344384  [307200/481450]\n",
      "loss: 0.410820  [310400/481450]\n",
      "loss: 0.329077  [313600/481450]\n",
      "loss: 0.218032  [316800/481450]\n",
      "loss: 0.192318  [320000/481450]\n",
      "loss: 0.137880  [323200/481450]\n",
      "loss: 0.160386  [326400/481450]\n",
      "loss: 0.167222  [329600/481450]\n",
      "loss: 0.074804  [332800/481450]\n",
      "loss: 0.132355  [336000/481450]\n",
      "loss: 0.464658  [339200/481450]\n",
      "loss: 0.229560  [342400/481450]\n",
      "loss: 0.262819  [345600/481450]\n",
      "loss: 0.275017  [348800/481450]\n",
      "loss: 0.103028  [352000/481450]\n",
      "loss: 0.404757  [355200/481450]\n",
      "loss: 0.275418  [358400/481450]\n",
      "loss: 0.305471  [361600/481450]\n",
      "loss: 0.223084  [364800/481450]\n",
      "loss: 0.187716  [368000/481450]\n",
      "loss: 0.338729  [371200/481450]\n",
      "loss: 0.196672  [374400/481450]\n",
      "loss: 0.162851  [377600/481450]\n",
      "loss: 0.280514  [380800/481450]\n",
      "loss: 0.248463  [384000/481450]\n",
      "loss: 0.352300  [387200/481450]\n",
      "loss: 0.276548  [390400/481450]\n",
      "loss: 0.301268  [393600/481450]\n",
      "loss: 0.093878  [396800/481450]\n",
      "loss: 0.142633  [400000/481450]\n",
      "loss: 0.258315  [403200/481450]\n",
      "loss: 0.278717  [406400/481450]\n",
      "loss: 0.334946  [409600/481450]\n",
      "loss: 0.337041  [412800/481450]\n",
      "loss: 0.249680  [416000/481450]\n",
      "loss: 0.163351  [419200/481450]\n",
      "loss: 0.183166  [422400/481450]\n",
      "loss: 0.104744  [425600/481450]\n",
      "loss: 0.347171  [428800/481450]\n",
      "loss: 0.166387  [432000/481450]\n",
      "loss: 0.130485  [435200/481450]\n",
      "loss: 0.354733  [438400/481450]\n",
      "loss: 0.210613  [441600/481450]\n",
      "loss: 0.141769  [444800/481450]\n",
      "loss: 0.195026  [448000/481450]\n",
      "loss: 0.074301  [451200/481450]\n",
      "loss: 0.267051  [454400/481450]\n",
      "loss: 0.474242  [457600/481450]\n",
      "loss: 0.289335  [460800/481450]\n",
      "loss: 0.415604  [464000/481450]\n",
      "loss: 0.249941  [467200/481450]\n",
      "loss: 0.229536  [470400/481450]\n",
      "loss: 0.213994  [473600/481450]\n",
      "loss: 0.125909  [476800/481450]\n",
      "loss: 0.240275  [480000/481450]\n",
      "Train Accuracy: 90.5467%\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.818205, F1-score: 85.66% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.276681  [    0/481450]\n",
      "loss: 0.086052  [ 3200/481450]\n",
      "loss: 0.205607  [ 6400/481450]\n",
      "loss: 0.244650  [ 9600/481450]\n",
      "loss: 0.033581  [12800/481450]\n",
      "loss: 0.264437  [16000/481450]\n",
      "loss: 0.262237  [19200/481450]\n",
      "loss: 0.439786  [22400/481450]\n",
      "loss: 0.137305  [25600/481450]\n",
      "loss: 0.190549  [28800/481450]\n",
      "loss: 0.257774  [32000/481450]\n",
      "loss: 0.439433  [35200/481450]\n",
      "loss: 0.325077  [38400/481450]\n",
      "loss: 0.464086  [41600/481450]\n",
      "loss: 0.137868  [44800/481450]\n",
      "loss: 0.228793  [48000/481450]\n",
      "loss: 0.099704  [51200/481450]\n",
      "loss: 0.183760  [54400/481450]\n",
      "loss: 0.088334  [57600/481450]\n",
      "loss: 0.145446  [60800/481450]\n",
      "loss: 0.251130  [64000/481450]\n",
      "loss: 0.141997  [67200/481450]\n",
      "loss: 0.260257  [70400/481450]\n",
      "loss: 0.322958  [73600/481450]\n",
      "loss: 0.175401  [76800/481450]\n",
      "loss: 0.220108  [80000/481450]\n",
      "loss: 0.209593  [83200/481450]\n",
      "loss: 0.438484  [86400/481450]\n",
      "loss: 0.255931  [89600/481450]\n",
      "loss: 0.331223  [92800/481450]\n",
      "loss: 0.578102  [96000/481450]\n",
      "loss: 0.196901  [99200/481450]\n",
      "loss: 0.382690  [102400/481450]\n",
      "loss: 0.137299  [105600/481450]\n",
      "loss: 0.200511  [108800/481450]\n",
      "loss: 0.356732  [112000/481450]\n",
      "loss: 0.028359  [115200/481450]\n",
      "loss: 0.105895  [118400/481450]\n",
      "loss: 0.192795  [121600/481450]\n",
      "loss: 0.243193  [124800/481450]\n",
      "loss: 0.133255  [128000/481450]\n",
      "loss: 0.153520  [131200/481450]\n",
      "loss: 0.267056  [134400/481450]\n",
      "loss: 0.236939  [137600/481450]\n",
      "loss: 0.165416  [140800/481450]\n",
      "loss: 0.589168  [144000/481450]\n",
      "loss: 0.054791  [147200/481450]\n",
      "loss: 0.271665  [150400/481450]\n",
      "loss: 0.153842  [153600/481450]\n",
      "loss: 0.249680  [156800/481450]\n",
      "loss: 0.373588  [160000/481450]\n",
      "loss: 0.290724  [163200/481450]\n",
      "loss: 0.186538  [166400/481450]\n",
      "loss: 0.141824  [169600/481450]\n",
      "loss: 0.265213  [172800/481450]\n",
      "loss: 0.108174  [176000/481450]\n",
      "loss: 0.139700  [179200/481450]\n",
      "loss: 0.194829  [182400/481450]\n",
      "loss: 0.037774  [185600/481450]\n",
      "loss: 0.167724  [188800/481450]\n",
      "loss: 0.158994  [192000/481450]\n",
      "loss: 0.313393  [195200/481450]\n",
      "loss: 0.073880  [198400/481450]\n",
      "loss: 0.414597  [201600/481450]\n",
      "loss: 0.085818  [204800/481450]\n",
      "loss: 0.189904  [208000/481450]\n",
      "loss: 0.185830  [211200/481450]\n",
      "loss: 0.128857  [214400/481450]\n",
      "loss: 0.165328  [217600/481450]\n",
      "loss: 0.088752  [220800/481450]\n",
      "loss: 0.119880  [224000/481450]\n",
      "loss: 0.377716  [227200/481450]\n",
      "loss: 0.118466  [230400/481450]\n",
      "loss: 0.164325  [233600/481450]\n",
      "loss: 0.378578  [236800/481450]\n",
      "loss: 0.179542  [240000/481450]\n",
      "loss: 0.157156  [243200/481450]\n",
      "loss: 0.165018  [246400/481450]\n",
      "loss: 0.135496  [249600/481450]\n",
      "loss: 0.078550  [252800/481450]\n",
      "loss: 0.078565  [256000/481450]\n",
      "loss: 0.464292  [259200/481450]\n",
      "loss: 0.188570  [262400/481450]\n",
      "loss: 0.156053  [265600/481450]\n",
      "loss: 0.164074  [268800/481450]\n",
      "loss: 0.235725  [272000/481450]\n",
      "loss: 0.221168  [275200/481450]\n",
      "loss: 0.345481  [278400/481450]\n",
      "loss: 0.158989  [281600/481450]\n",
      "loss: 0.068129  [284800/481450]\n",
      "loss: 0.033615  [288000/481450]\n",
      "loss: 0.113071  [291200/481450]\n",
      "loss: 0.232483  [294400/481450]\n",
      "loss: 0.349106  [297600/481450]\n",
      "loss: 0.246813  [300800/481450]\n",
      "loss: 0.275028  [304000/481450]\n",
      "loss: 0.315069  [307200/481450]\n",
      "loss: 0.294881  [310400/481450]\n",
      "loss: 0.084955  [313600/481450]\n",
      "loss: 0.103748  [316800/481450]\n",
      "loss: 0.223076  [320000/481450]\n",
      "loss: 0.644876  [323200/481450]\n",
      "loss: 0.045545  [326400/481450]\n",
      "loss: 0.401320  [329600/481450]\n",
      "loss: 0.221748  [332800/481450]\n",
      "loss: 0.104518  [336000/481450]\n",
      "loss: 0.055317  [339200/481450]\n",
      "loss: 0.290311  [342400/481450]\n",
      "loss: 0.096721  [345600/481450]\n",
      "loss: 0.301603  [348800/481450]\n",
      "loss: 0.232652  [352000/481450]\n",
      "loss: 0.213252  [355200/481450]\n",
      "loss: 0.349334  [358400/481450]\n",
      "loss: 0.281945  [361600/481450]\n",
      "loss: 0.299972  [364800/481450]\n",
      "loss: 0.384832  [368000/481450]\n",
      "loss: 0.251429  [371200/481450]\n",
      "loss: 0.235037  [374400/481450]\n",
      "loss: 0.058526  [377600/481450]\n",
      "loss: 0.304228  [380800/481450]\n",
      "loss: 0.335959  [384000/481450]\n",
      "loss: 0.541051  [387200/481450]\n",
      "loss: 0.241189  [390400/481450]\n",
      "loss: 0.266864  [393600/481450]\n",
      "loss: 0.351544  [396800/481450]\n",
      "loss: 0.143912  [400000/481450]\n",
      "loss: 0.116154  [403200/481450]\n",
      "loss: 0.105153  [406400/481450]\n",
      "loss: 0.375140  [409600/481450]\n",
      "loss: 0.318338  [412800/481450]\n",
      "loss: 0.129067  [416000/481450]\n",
      "loss: 0.162307  [419200/481450]\n",
      "loss: 0.040983  [422400/481450]\n",
      "loss: 0.226090  [425600/481450]\n",
      "loss: 0.134035  [428800/481450]\n",
      "loss: 0.127347  [432000/481450]\n",
      "loss: 0.145631  [435200/481450]\n",
      "loss: 0.022731  [438400/481450]\n",
      "loss: 0.150453  [441600/481450]\n",
      "loss: 0.253901  [444800/481450]\n",
      "loss: 0.225962  [448000/481450]\n",
      "loss: 0.378385  [451200/481450]\n",
      "loss: 0.204081  [454400/481450]\n",
      "loss: 0.274953  [457600/481450]\n",
      "loss: 0.281215  [460800/481450]\n",
      "loss: 0.076489  [464000/481450]\n",
      "loss: 0.222031  [467200/481450]\n",
      "loss: 0.207191  [470400/481450]\n",
      "loss: 0.201174  [473600/481450]\n",
      "loss: 0.074778  [476800/481450]\n",
      "loss: 0.238783  [480000/481450]\n",
      "Train Accuracy: 90.6165%\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.830471, F1-score: 85.53% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,scheduler):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.407161  [    0/481450]\n",
      "loss: 0.248885  [ 3200/481450]\n",
      "loss: 0.302694  [ 6400/481450]\n",
      "loss: 0.290368  [ 9600/481450]\n",
      "loss: 0.248356  [12800/481450]\n",
      "loss: 0.347584  [16000/481450]\n",
      "loss: 0.260450  [19200/481450]\n",
      "loss: 0.153373  [22400/481450]\n",
      "loss: 0.211395  [25600/481450]\n",
      "loss: 0.280048  [28800/481450]\n",
      "loss: 0.551072  [32000/481450]\n",
      "loss: 0.242100  [35200/481450]\n",
      "loss: 0.376839  [38400/481450]\n",
      "loss: 0.530344  [41600/481450]\n",
      "loss: 0.047740  [44800/481450]\n",
      "loss: 0.382037  [48000/481450]\n",
      "loss: 0.133002  [51200/481450]\n",
      "loss: 0.175415  [54400/481450]\n",
      "loss: 0.266727  [57600/481450]\n",
      "loss: 0.356649  [60800/481450]\n",
      "loss: 0.295790  [64000/481450]\n",
      "loss: 0.259346  [67200/481450]\n",
      "loss: 0.229986  [70400/481450]\n",
      "loss: 0.121399  [73600/481450]\n",
      "loss: 0.195167  [76800/481450]\n",
      "loss: 0.277340  [80000/481450]\n",
      "loss: 0.217338  [83200/481450]\n",
      "loss: 0.231709  [86400/481450]\n",
      "loss: 0.324257  [89600/481450]\n",
      "loss: 0.237235  [92800/481450]\n",
      "loss: 0.321577  [96000/481450]\n",
      "loss: 0.341236  [99200/481450]\n",
      "loss: 0.145905  [102400/481450]\n",
      "loss: 0.330484  [105600/481450]\n",
      "loss: 0.294633  [108800/481450]\n",
      "loss: 0.231578  [112000/481450]\n",
      "loss: 0.222409  [115200/481450]\n",
      "loss: 0.461695  [118400/481450]\n",
      "loss: 0.374904  [121600/481450]\n",
      "loss: 0.114602  [124800/481450]\n",
      "loss: 0.353723  [128000/481450]\n",
      "loss: 0.183473  [131200/481450]\n",
      "loss: 0.436538  [134400/481450]\n",
      "loss: 0.075138  [137600/481450]\n",
      "loss: 0.300795  [140800/481450]\n",
      "loss: 0.529032  [144000/481450]\n",
      "loss: 0.270004  [147200/481450]\n",
      "loss: 0.366657  [150400/481450]\n",
      "loss: 0.311112  [153600/481450]\n",
      "loss: 0.211928  [156800/481450]\n",
      "loss: 0.268379  [160000/481450]\n",
      "loss: 0.611624  [163200/481450]\n",
      "loss: 0.400922  [166400/481450]\n",
      "loss: 0.316510  [169600/481450]\n",
      "loss: 0.549950  [172800/481450]\n",
      "loss: 0.154515  [176000/481450]\n",
      "loss: 0.431937  [179200/481450]\n",
      "loss: 0.287123  [182400/481450]\n",
      "loss: 0.369408  [185600/481450]\n",
      "loss: 0.239024  [188800/481450]\n",
      "loss: 0.154488  [192000/481450]\n",
      "loss: 0.500535  [195200/481450]\n",
      "loss: 0.262849  [198400/481450]\n",
      "loss: 0.297963  [201600/481450]\n",
      "loss: 0.255245  [204800/481450]\n",
      "loss: 0.232667  [208000/481450]\n",
      "loss: 0.243727  [211200/481450]\n",
      "loss: 0.390564  [214400/481450]\n",
      "loss: 0.401098  [217600/481450]\n",
      "loss: 0.323042  [220800/481450]\n",
      "loss: 0.279806  [224000/481450]\n",
      "loss: 0.169533  [227200/481450]\n",
      "loss: 0.267745  [230400/481450]\n",
      "loss: 0.258768  [233600/481450]\n",
      "loss: 0.283328  [236800/481450]\n",
      "loss: 0.362440  [240000/481450]\n",
      "loss: 0.299089  [243200/481450]\n",
      "loss: 0.230293  [246400/481450]\n",
      "loss: 0.178301  [249600/481450]\n",
      "loss: 0.198777  [252800/481450]\n",
      "loss: 0.397015  [256000/481450]\n",
      "loss: 0.160599  [259200/481450]\n",
      "loss: 0.179553  [262400/481450]\n",
      "loss: 0.232552  [265600/481450]\n",
      "loss: 0.423253  [268800/481450]\n",
      "loss: 0.337919  [272000/481450]\n",
      "loss: 0.583646  [275200/481450]\n",
      "loss: 0.085314  [278400/481450]\n",
      "loss: 0.299244  [281600/481450]\n",
      "loss: 0.263522  [284800/481450]\n",
      "loss: 0.263142  [288000/481450]\n",
      "loss: 0.225995  [291200/481450]\n",
      "loss: 0.201337  [294400/481450]\n",
      "loss: 0.242055  [297600/481450]\n",
      "loss: 0.544868  [300800/481450]\n",
      "loss: 0.288992  [304000/481450]\n",
      "loss: 0.232154  [307200/481450]\n",
      "loss: 0.327430  [310400/481450]\n",
      "loss: 0.363444  [313600/481450]\n",
      "loss: 0.120751  [316800/481450]\n",
      "loss: 0.183201  [320000/481450]\n",
      "loss: 0.429792  [323200/481450]\n",
      "loss: 0.318540  [326400/481450]\n",
      "loss: 0.253163  [329600/481450]\n",
      "loss: 0.262380  [332800/481450]\n",
      "loss: 0.065487  [336000/481450]\n",
      "loss: 0.188800  [339200/481450]\n",
      "loss: 0.328083  [342400/481450]\n",
      "loss: 0.235447  [345600/481450]\n",
      "loss: 0.232013  [348800/481450]\n",
      "loss: 0.342847  [352000/481450]\n",
      "loss: 0.583130  [355200/481450]\n",
      "loss: 0.245348  [358400/481450]\n",
      "loss: 0.412512  [361600/481450]\n",
      "loss: 0.357966  [364800/481450]\n",
      "loss: 0.363364  [368000/481450]\n",
      "loss: 0.430090  [371200/481450]\n",
      "loss: 0.153763  [374400/481450]\n",
      "loss: 0.200360  [377600/481450]\n",
      "loss: 0.485753  [380800/481450]\n",
      "loss: 0.338388  [384000/481450]\n",
      "loss: 0.066615  [387200/481450]\n",
      "loss: 0.148382  [390400/481450]\n",
      "loss: 0.192716  [393600/481450]\n",
      "loss: 0.297759  [396800/481450]\n",
      "loss: 0.452961  [400000/481450]\n",
      "loss: 0.243701  [403200/481450]\n",
      "loss: 0.454874  [406400/481450]\n",
      "loss: 0.168116  [409600/481450]\n",
      "loss: 0.150864  [412800/481450]\n",
      "loss: 0.392234  [416000/481450]\n",
      "loss: 0.319392  [419200/481450]\n",
      "loss: 0.288071  [422400/481450]\n",
      "loss: 0.215188  [425600/481450]\n",
      "loss: 0.714944  [428800/481450]\n",
      "loss: 0.113952  [432000/481450]\n",
      "loss: 0.210401  [435200/481450]\n",
      "loss: 0.425279  [438400/481450]\n",
      "loss: 0.233624  [441600/481450]\n",
      "loss: 0.507988  [444800/481450]\n",
      "loss: 0.320946  [448000/481450]\n",
      "loss: 0.155006  [451200/481450]\n",
      "loss: 0.334917  [454400/481450]\n",
      "loss: 0.357269  [457600/481450]\n",
      "loss: 0.281123  [460800/481450]\n",
      "loss: 0.294178  [464000/481450]\n",
      "loss: 0.239332  [467200/481450]\n",
      "loss: 0.181043  [470400/481450]\n",
      "loss: 0.162510  [473600/481450]\n",
      "loss: 0.343567  [476800/481450]\n",
      "loss: 0.146843  [480000/481450]\n",
      "Train Accuracy: 89.0082%\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.739847, F1-score: 82.21% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.136654  [    0/481450]\n",
      "loss: 0.368329  [ 3200/481450]\n",
      "loss: 0.243074  [ 6400/481450]\n",
      "loss: 0.204710  [ 9600/481450]\n",
      "loss: 0.167110  [12800/481450]\n",
      "loss: 0.313853  [16000/481450]\n",
      "loss: 0.122421  [19200/481450]\n",
      "loss: 0.276794  [22400/481450]\n",
      "loss: 0.138305  [25600/481450]\n",
      "loss: 0.157936  [28800/481450]\n",
      "loss: 0.232937  [32000/481450]\n",
      "loss: 0.459152  [35200/481450]\n",
      "loss: 0.398669  [38400/481450]\n",
      "loss: 0.200324  [41600/481450]\n",
      "loss: 0.217362  [44800/481450]\n",
      "loss: 0.241058  [48000/481450]\n",
      "loss: 0.239651  [51200/481450]\n",
      "loss: 0.150701  [54400/481450]\n",
      "loss: 0.304414  [57600/481450]\n",
      "loss: 0.268291  [60800/481450]\n",
      "loss: 0.491986  [64000/481450]\n",
      "loss: 0.110887  [67200/481450]\n",
      "loss: 0.231932  [70400/481450]\n",
      "loss: 0.207168  [73600/481450]\n",
      "loss: 0.204157  [76800/481450]\n",
      "loss: 0.202611  [80000/481450]\n",
      "loss: 0.259685  [83200/481450]\n",
      "loss: 0.274524  [86400/481450]\n",
      "loss: 0.111567  [89600/481450]\n",
      "loss: 0.202459  [92800/481450]\n",
      "loss: 0.215723  [96000/481450]\n",
      "loss: 0.280482  [99200/481450]\n",
      "loss: 0.270081  [102400/481450]\n",
      "loss: 0.660749  [105600/481450]\n",
      "loss: 0.171807  [108800/481450]\n",
      "loss: 0.102703  [112000/481450]\n",
      "loss: 0.143221  [115200/481450]\n",
      "loss: 0.259784  [118400/481450]\n",
      "loss: 0.203459  [121600/481450]\n",
      "loss: 0.263079  [124800/481450]\n",
      "loss: 0.329582  [128000/481450]\n",
      "loss: 0.375748  [131200/481450]\n",
      "loss: 0.048494  [134400/481450]\n",
      "loss: 0.102307  [137600/481450]\n",
      "loss: 0.253004  [140800/481450]\n",
      "loss: 0.282339  [144000/481450]\n",
      "loss: 0.156198  [147200/481450]\n",
      "loss: 0.138395  [150400/481450]\n",
      "loss: 0.215177  [153600/481450]\n",
      "loss: 0.276198  [156800/481450]\n",
      "loss: 0.252224  [160000/481450]\n",
      "loss: 0.174219  [163200/481450]\n",
      "loss: 0.351637  [166400/481450]\n",
      "loss: 0.238186  [169600/481450]\n",
      "loss: 0.459725  [172800/481450]\n",
      "loss: 0.327901  [176000/481450]\n",
      "loss: 0.251471  [179200/481450]\n",
      "loss: 0.110704  [182400/481450]\n",
      "loss: 0.299679  [185600/481450]\n",
      "loss: 0.149147  [188800/481450]\n",
      "loss: 0.433021  [192000/481450]\n",
      "loss: 0.467700  [195200/481450]\n",
      "loss: 0.121317  [198400/481450]\n",
      "loss: 0.422833  [201600/481450]\n",
      "loss: 0.268249  [204800/481450]\n",
      "loss: 0.243460  [208000/481450]\n",
      "loss: 0.293902  [211200/481450]\n",
      "loss: 0.362537  [214400/481450]\n",
      "loss: 0.227617  [217600/481450]\n",
      "loss: 0.116834  [220800/481450]\n",
      "loss: 0.464069  [224000/481450]\n",
      "loss: 0.476940  [227200/481450]\n",
      "loss: 0.234702  [230400/481450]\n",
      "loss: 0.290182  [233600/481450]\n",
      "loss: 0.245807  [236800/481450]\n",
      "loss: 0.180711  [240000/481450]\n",
      "loss: 0.374632  [243200/481450]\n",
      "loss: 0.409156  [246400/481450]\n",
      "loss: 0.240222  [249600/481450]\n",
      "loss: 0.212800  [252800/481450]\n",
      "loss: 0.251950  [256000/481450]\n",
      "loss: 0.190250  [259200/481450]\n",
      "loss: 0.226821  [262400/481450]\n",
      "loss: 0.397436  [265600/481450]\n",
      "loss: 0.574393  [268800/481450]\n",
      "loss: 0.468974  [272000/481450]\n",
      "loss: 0.237940  [275200/481450]\n",
      "loss: 0.234878  [278400/481450]\n",
      "loss: 0.268609  [281600/481450]\n",
      "loss: 0.139865  [284800/481450]\n",
      "loss: 0.392539  [288000/481450]\n",
      "loss: 0.142212  [291200/481450]\n",
      "loss: 0.238843  [294400/481450]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,scheduler)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
