{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae4763c-f817-4e3b-9e5b-b1972548dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121f074-7805-4171-8b4c-ff05aa6e74ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis-Space\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis-Space\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis-Space\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis-Space\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75dd1d13-a1c1-4150-8e41-0d8261c19cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              1.000000\n",
       "encoder__proto_ospf             1.000000\n",
       "encoder__proto_sctp             1.000000\n",
       "encoder__proto_tcp              1.000000\n",
       "encoder__proto_udp              1.000000\n",
       "encoder__proto_unas             1.000000\n",
       "encoder__service_dns            1.000000\n",
       "encoder__service_ftp            1.000000\n",
       "encoder__service_ftp-data       1.000000\n",
       "encoder__service_http           1.000000\n",
       "encoder__service_smtp           1.000000\n",
       "encoder__state_CON              1.000000\n",
       "encoder__state_ECO              0.000000\n",
       "encoder__state_FIN              1.000000\n",
       "encoder__state_INT              1.000000\n",
       "encoder__state_REQ              1.000000\n",
       "encoder__state_RST              1.000000\n",
       "remainder__dur                  1.000000\n",
       "remainder__spkts                1.000000\n",
       "remainder__dpkts                1.000000\n",
       "remainder__sbytes               1.000000\n",
       "remainder__dbytes               1.000000\n",
       "remainder__rate                 1.000000\n",
       "remainder__sttl                 1.000000\n",
       "remainder__dttl                 1.000000\n",
       "remainder__sload                1.000000\n",
       "remainder__dload                0.835339\n",
       "remainder__sloss                1.000000\n",
       "remainder__dloss                1.000000\n",
       "remainder__sinpkt               1.000000\n",
       "remainder__dinpkt               1.000000\n",
       "remainder__sjit                 1.000000\n",
       "remainder__djit                 1.000000\n",
       "remainder__swin                 1.000000\n",
       "remainder__stcpb                1.000000\n",
       "remainder__dtcpb                1.000000\n",
       "remainder__dwin                 1.000000\n",
       "remainder__tcprtt               1.000000\n",
       "remainder__synack               1.000000\n",
       "remainder__ackdat               1.000000\n",
       "remainder__smean                1.000000\n",
       "remainder__dmean                1.000000\n",
       "remainder__trans_depth          0.874469\n",
       "remainder__response_body_len    1.000000\n",
       "remainder__ct_srv_src           1.000000\n",
       "remainder__ct_state_ttl         1.000000\n",
       "remainder__ct_dst_ltm           1.000000\n",
       "remainder__ct_src_dport_ltm     1.000000\n",
       "remainder__ct_dst_sport_ltm     0.823317\n",
       "remainder__ct_dst_src_ltm       1.000000\n",
       "remainder__is_ftp_login         1.000000\n",
       "remainder__ct_ftp_cmd           1.000000\n",
       "remainder__ct_flw_http_mthd     0.814779\n",
       "remainder__ct_src_ltm           1.000000\n",
       "remainder__ct_srv_dst           1.000000\n",
       "remainder__is_sm_ips_ports      1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb2e4ac-d59e-4e27-9f18-e9f0aa674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y_binary = train_labels_encoded[\"label\"]\n",
    "test_data_y_binary = test_labels_encoded[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32a12a0-caa3-4e41-98f8-e89a7075914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175336</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175337</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175338</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175339</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175340</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175341 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        attack_cat  label\n",
       "0                6      0\n",
       "1                6      0\n",
       "2                6      0\n",
       "3                6      0\n",
       "4                6      0\n",
       "...            ...    ...\n",
       "175336           5      1\n",
       "175337           8      1\n",
       "175338           5      1\n",
       "175339           5      1\n",
       "175340           5      1\n",
       "\n",
       "[175341 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbdef64-5462-455b-96fa-863d4b36bdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat  label\n",
       "6           0        56000\n",
       "5           1        40000\n",
       "3           1        33393\n",
       "4           1        18184\n",
       "2           1        12264\n",
       "7           1        10491\n",
       "0           1         2000\n",
       "1           1         1746\n",
       "8           1         1133\n",
       "9           1          130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2c096b-e87b-4597-a60e-d432d11ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = {\n",
    "    0: 20000,  # Class 0 should have 56000 samples\n",
    "    1: 20000,  # Class 1 should have 56000 samples\n",
    "    2: 30000,  # Class 2 should have 56000 samples\n",
    "    3: 34000,  # Class 3 should have 56000 samples\n",
    "    4: 30000,  # Class 4 should have 56000 samples\n",
    "    5: 41000,  # Class 5 should have 56000 samples\n",
    "    6: 56000,  # Class 6 should have 56000 samples\n",
    "    7: 30000,  # Class 7 should have 56000 samples\n",
    "    8: 20000,  # Class 8 should have 56000 samples\n",
    "    9: 10000,  # Class 9 should have 56000 samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45addaf4-c17e-4cb2-94b3-a5c6629a8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 6\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96863f09-6fdd-47a1-aa2a-0c75a380e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 5\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011c575f-d547-4b41-84c0-1af4ba290d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded[\"attack_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b95a23d-e0ba-48a9-8e8f-a76411be9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = test_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"majority\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af5f4e3-48a5-4a60-b951-1cb02855acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102d84fa-c8e2-425c-84f8-25b8a6272498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy = \"minority\", random_state=42, n_neighbors= 3)\n",
    "#train_data_X,train_data_y = adasyn.fit_resample(train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "#smote_enn = SMOTEENN(sampling_strategy=\"not majority\", enn = enn, random_state=42,n_neighbors = 5)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "#train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_data_y)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "#print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4374a-08e4-4ef5-ae4d-28e91f23e8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAANECAYAAADIZnw3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsvQu4zWXe/3/nEEPN84ikKTRSQwlRoRMVQw7z1BSSQRoSopKSQw5lHFMpJDnklHOpqZyJ8eSsSM4KTyWimqcS8eR/vT6/617/71p77bXXXvtsvV/XtWb2/h7v+/6u3fV9e38O55w5c+aME0IIIYQQQgiRa8mX0wMQQgghhBBCCBEbCTchhBBCCCGEyOVIuAkhhBBCCCFELkfCTQghhBBCCCFyORJuQgghhBBCCJHLkXATQgghhBBCiFyOhJsQQgghhBBC5HIk3IQQQgghhBAilyPhJoQQQgghhBC5HAk3IYQQOcobb7zhzjnnHLd///6cHorIAni2/fv3z+lh5Br0fU8e+N7zrBPhgQcecJdddlmmj0nkbSTchBAih17con2efvrpLLnnRx99ZC8RP/zwQ5ZcP5k5fvy4re2HH36Y00MRARBGwb+tfPnyuQsuuMDdeeedbs2aNTk9vFy7TsFPzZo1XW7kzTffdC+99FLcxyOAmE/dunWj7n/99ddDc964cWMmjlSIzKVAJl9PCCFEnDz77LPuj3/8Y9i2SpUqZZlwGzBggP0r7n/+53+63ESrVq3cfffd5woVKuTyqnBjbaFOnTo5PZxcxy+//OIKFMi5140WLVq4hg0buv/7v/9zu3fvdmPGjHG33Xab27Bhg7vmmmtybFy5Db9OQS688EKXW4Xbtm3b3GOPPRb3OYULF3YrVqxw33zzjStVqlTYvunTp9v+EydOZMFohcg8JNyEECKH4F/+r7vuOpeX+fnnn13RokUzdI38+fPbJ6/x22+/uV9//TWnh5Hr4YU4J6lWrZr729/+Fvr9lltusb+9V1991USciL5OmQVi6NxzzzXHMye56aabTKzPmjXLPfroo6HtX375pfvXv/7l7r77bjdv3rwcHaMQaaFQSSGEyKUsWLDAXjIRRueff75r1KiR++yzz8KO2bp1q7lo5cqVsxdk/iX5wQcfdMeOHQsdQxjfk08+aT/j8PmQIEKkfJgU4Ztp5Sb5fI3t27e7+++/3xUrVszdfPPNof3Tpk1z1atXd7/73e8sJA0X7X/+538SyvkhtKlx48YWfoi45Zq4Iz4c8a233rLfmTP3/Pjjj8OuyZqcd9557vPPP3f169e3NfzDH/5gLueZM2dSiM8nnnjClS5d2ly/P/3pT+75559PcRxjfOSRR+xf56+++mo7duzYsSFXAtfNr61ft3ieT3Bt9+7dG3JF/+M//sO1bdvWHL1IWOsbbrjBFSlSxJ7Drbfe6hYvXpzu70968nKiPSfCyljfEiVK2DPi+8X84vkexTNX3LquXbva9ZnDX/7yF/fVV19lKG+ONYF9+/aFbZ80aZK7/fbbXcmSJe3ZXnXVVSbuIvHfzdWrV9sz4LnyfKdMmZLiWNaba7I2l156qRs4cKAJ/mggIv33iu9q586dU4Q24+jiyvO9ql27tj3/8uXLu7lz59r+lStXuho1atj9+B4vXbrUZRb8LTVt2tT+trkvYZTvv/9+2DH8ffJsZs6c6fr06eMuueQSO/Z///d/bf+6detcgwYN7HmznTn893//d9g1fvzxR3PSWGfWgudRr149t3nz5tAacN8DBw6E/t7iyQXjOf31r381ty7IjBkz7G+I73E0li9fHvo74rv6X//1X27Hjh0pjuP7cP3119t9Lr/8cvfaa6+lOpZE/1sphBw3IYTIIf7973+7o0ePhm3jBRWmTp3q2rRpYy8TQ4cOtRdaXiIRSogU/6KyZMkSe6HipRdRwIviuHHj7P/Xrl1rLzW8rBAixgvKiy++GLoHguPbb79N97h5ebviiivcoEGDQuLmH//4h3vmmWdcs2bNXLt27ey6r7zyigkKxptIeCYv9gjEDh06mBOAmGrSpImJpV69erlOnTrZcYMHD7b77tq1K+xf9QmN4yWRF8xhw4a5hQsXun79+rnTp0+bgAPGjxgghOrvf/+7q1q1qlu0aJEJXQQC6xX5Ejd79mwTcKxjlSpV7Ll07NjR/sWetYbKlSvH/XyCMA/ED3PiRXX8+PH24sp3wINARLTceOONNg/cDF6IGduf//zndH1/MsKRI0fsfnyPyM3kGSPqENXxEM9cEXasN+G0PEeECQI0I3jhyct6ENYH4cT3gdDOf/7zn/YdQ2ghoiK/m/fee699Z1jniRMn2lh5GecaQEgeIZl831gfXvx59rysR8Lz5LmSg8V3ie8y48EhQtgULFgwdOz3339vwpGXff4WOY6f+QcFBM/DDz9sfzfDhw+3MSIIEL1pwXck8r9HCCzuffjwYfu+cQxCunjx4m7y5Mm2VohGvvtBnnvuOftedu/e3Z08edJ+5vuJ08ka8XfI36oXyzheiGBg/FyTvzHEM//IgShCLOEK9u7d2/7biVPm/z75R5p4YF34ziLaEVeAkGOdgmvsQfgyZoQ5z4h/SOC/a7h3fGf939Gnn34a+lvgOJ45c7zoootSXDMr/lspkogzQgghspVJkyahdqJ+4Mcffzzzn//5n2fat28fdt4333xz5j/+4z/Cth8/fjzF9WfMmGHXWrVqVWjb8OHDbdsXX3wRdiy/s50xRcL2fv36hX7nZ7a1aNEi7Lj9+/efyZ8//5l//OMfYds//fTTMwUKFEixPbX1CI6tbNmytu2jjz4KbVu0aJFt+93vfnfmwIEDoe2vvfaabV+xYkVoW5s2bWxbly5dQtt+++23M40aNTpz7rnnnvn2229t2/z58+24gQMHho3p3nvvPXPOOeec2bt3b9h65MuX78xnn30WdizXilyr9D4fv7YPPvhg2LF33333meLFi4d+37Nnj42B7f/3f/8XdizzS+/3Jxp+LGk9p7ffftt+37BhQ8zrpfY9SmuumzZtsuMee+yxsOMeeOCBVNc72nd7wIAB9oyY/7/+9a8z119/vW2fM2dOms+qfv36Z8qVKxe2zX83g8/vyJEjZwoVKnTmiSeeCG1j3By3bt26sON4BsF1ZBvfyT//+c9hz3TUqFF23MSJE0PbateubdvefPPN0LadO3eGvptr165N8fcS7W872jpF+/i/KT8X1s/D9+yPf/zjmcsuuyw0bo7nONYsuJ58N6+44gpbT/899WvONerVqxfaxvp07tw55pj5O+Y5xAvHcs7p06fPlCpV6sxzzz1n27dv327jXblyZej7Hfw+V61a9UzJkiXPHDt2LLRty5YtttatW7cObbvrrrvOFC5cOOy/S1yb/y4G/5bS899K/huWnjmK5EChkkIIkUOMHj3aHJngB/h/QqQoFsC/gPsPeWCEQeEOeYL/ek8uCcf5SnA+tCiz4V/Eg+Cw4ErwL8jB8eIw4cwFx5se+Nf2WrVqhX5n7sC/0JcpUybFdpytSPhX+8hQR/LSfAjZBx98YOuKixCE0Ek0B+GGQQjtYlzxkt7nE7m2hGjhOPhQs/nz59ta9+3bN0XOkHfv0vP9yQjeGXjvvffcqVOn0n1+WnPFIQXvrHq6dOmSrvvgfOCE8H3kHjg3I0aMMJcltWfl3XCeN98rfg/Cd8CHXALXJzQx+B3ku8Wz9k6SP65ly5Zh1+K7yHcStyz4TNu3b+9+//vfpwhHxF3CYfNwX55FxYoVQ38Laf1dROOhhx5K8d8jHGU/F+YRDI1mHJyDg0n4dBBcyOB6fvLJJ27Pnj3mePGM/XeSMOU77rjDrVq1KhRCylxwkL/++muX2fA3wH+niD4AXEpCpIPP0nPo0CEbN04q4Ywe3HRCN1kT7+zj0t91111h/13ieUSGX2bVfytF8qBQSSGEyCF4EYpWnIQXHC9QosHLnOe7776zECtySghdCxL5splZRFbCZLyIHF48ohEtBCkegi9BPmwLeNGKtp0QsiC8BBPiFOTKK68MC5cjT4Z8oshQMl66/P5Yc0+L9D6fyDn7cD7mxnMnxIt5xRKP6fn+ZAREzT333GPzI2SN3CNeXnk5j6dCaFpzZe2Za+Sak9OVHhAXhBQinAnXe/nll+1lOxJCEhF5tAqIzLXjWfnvWbSx+/EHv4OMPyikgkIriP+ORW4nvJDvb+R3kFy5yBBbxhbv30Vq8PebWrn81OYS/DsJVsSN9t8IL+hSgzVmDQlr5jjmQ1gllS5bt26d4m85Ufh+8h3YsmWLhUkigqPldKb2XPy8EWsIT3LyCKGM9t8/zvUCLyv/WymSBwk3IYTIZfh/eSZPKbJsNQRLq/Mvt5T6JyeL/Cz+FZzzye1KrQhCkNSaw0Z7sfVE5uhwH66DOxWtOmS8+SeRpFZpMrXtkcVEsoJo+UmxSO/zyYy5pef7k5HvBMeRi0SuHvlgvMhSmAQ3i21pPffseo5BQUJuGPcl54z8M/8PJwhinJ8KFSq4F154wUQDwomXbkRp5LPKye9gbvy7iOe/EUDeHX8H0fDfF/5mcMDefvttK7jDOeQ94laRb5ZREKDkt+FwfvHFFybksous+m+lSB4k3IQQIpfhk+Yp1JDav4D7f0lftmyZOR6EzkX+63Y8L+Pe5YisXhf5r/xpjZeXQ/6V3TtauQFekggTC46JIi3giwqULVvWQtX4V/Og67Zz587Q/rRIbW3T83zSs9bMi9C01F6A4/3+pEbwOxEslJDad4JwQD4UXcDBIBQQh5HCCxmBtWeuvFwHHQoKg2QEilvQcJmqhz4cE+FJEY133303zE3LSOga44/2rCk8Enmc3x50lQifZO6JPMPMhjFGjjs9fyf+O4mTGs98Lr74YguR5YNTTVESvl9euKX2NxcvhBFT4RPnLLW/o+BziTZvihNRcIYqkgjVeJ51bv1vpcg7KMdNCCFyGeRF8IJD1cZouUO+EqT/F9vIf1F/6aWXUpzje61FCjTuwwsIOSZB0tPfikqKjAWBEjkWfo8sfZ+djBo1Kmws/E44Eu4K+MbMweMAl4WXw3j+hZ+y5tHWNj3PJ14IRSR8kGqSkS6Qv0+835+0XrKD3wlCwqgiGClMI+fmX4IRQRnF5wdFfhepwJcREKNUKsUhJIcptWdF6B5VDxOF7xbO4/r168PWnryqIAgZ3D3C94L3nzBhgo0ho1U0MwPmwjwIIw1+J6iSyT+CpJX3Scgj3ysqw/7000+pfif5W4wMIeYfIAhnDn6n+O9ZRkLB+UcFwmJxh2OJR77PfO+Df9s0/sYJ9M3K+e7wXSX/9ODBg6HjyKXkO5ZX/lsp8gZy3IQQIpfBSzclvimBzr80k4NBUQNeCihUQClqhAbHUUKanBBe0OmZxAsF/0of7cXJuw1cD/FCaX1egHiJGTJkiP0/oWO8sHtnKh54IeNfr3v27Gm5Y4gL3CvGQbgTOUaUBc9u+JdwHBXyZQiPIjyJ9aOVgO+9xhoQMse6MHaKMbCG77zzjoVSeRETC/61nRdXGvvyr+gUMiDfh0+8zydeyO9irJRbJ5yMF0HyySgbz8stpfXj/f6kBmXNcZ0odU+IJy+alLv31/DwQouoohQ864RriZPF/f1LbUbgO0sOHUKXF1rfDsB/NzPiutCAmevyvccdZM6IJ74PiDrEBXNBNFCkIhGeeuopC1clLJb7+XYAODn0YfOwrvzt8DLPsZTYx6lhbekLlhVNsdMLoaUU9OAfMijkw3ec5893mabVaTXXZj/tHjifdgm0x+DvgZYbuJp8Z3A9+Q6Rw0fhGP4WCR3EEef7HRRZfDf4e+vWrZutEcfx7OKFZxBPH0DCNBkzRZL4e/DtAMgfDJ7Ps+O/NfxN4hLSDoDjmGvwWefW/1aKPEROl7UUQohkI1rZ6WhQWpvy2ZTHptT05ZdfbqXQN27cGDrmyy+/tDLqlH/nuKZNm575+uuvo5ZLpwT2JZdcYqWsg+XIKcn997//3c4///zzzzRr1sxKlKdWxt2X0o9k3rx5Z26++eYzRYsWtU+FChWsrPeuXbsSagdA+e5IOC6yVLgvZ07Lg2Apbcawb98+K7NepEiRMxdddJHNIbKMPmXNH3/88TN/+MMfzhQsWNDKlnOtYNny1O7toW1B9erVrax7cN3ifT6prW20tQFKxF977bVWgr5YsWJWJn7JkiXp/v6kBqX4a9SoYfMpU6bMmRdeeCHFWDZv3mytIdjPOCib3rhx4xTXz8hcf/75Z1vzCy644Mx5551nZdf5PnHckCFDYs4h2vciCGtBaXbf8uHdd989U7lyZVsrStwPHTrU1jne7ybPgE+QrVu32jauyd8ef4MTJkyI+kwp/8/fDN9BvqsdO3Y88/3336e4x9VXX53i3un5e0nvOnn4W6JNBt9l5nPDDTecee+998KO8e0AIlsteD7++OMzf/3rX63tA98Zxs1/b5YtW2b7T548eebJJ588U6VKFftvEX/D/DxmzJiw6/z0009n7r//fhsL90urbH5q6xPPf5eXLl165qabbrI2JL///e/PNGnSxEr9R0JLAf/fANohjB07NtXWGvH8t1LtAEQ0zuF/clo8CiGEEJkJJbwpnBEtLEvkbQhvvPbaa920adNSlNYXQoizGeW4CSGEECJXQmhaJIQ4EnpHGKoQQiQTynETQgghRK6E/MBNmzZZHiJtDMhT5EMuUGTfMiGEONuRcBNCCCFEruTGG290S5YssWIshL1SNIWiEBRoEUKIZEM5bkIIIYQQQgiRy1GOmxBCCCGEEELkciTchBBCCCGEECKXoxw3IfIYv/32m/v666+taWdGGtAKIYQQQoisgWw0msr/4Q9/SLNJfbxIuAmRx0C0qZqaEEIIIUTu53/+53/cpZdeminXknATIo+B0+b/Q/D73/8+p4cjhBBCCCEi+N///V/7h3b/3pYZSLiJbOeBBx5wP/zwg5s/f36GrkOY4Ntvv+3uuusul5epU6eOq1q1qjWVjQcfHolok3ATQgghhMi9ZGZai4qTiGxn5MiR7o033nBnM4ixxx57LGzbhx9+aH+8iFYhhBBCCCHSgxy3JOXXX3915557bo7c+z/+4z9csq9BZlCp3yKXr1CRnB6GEEIIIUSuZ/+QRi6vI8ctScABeuSRR8wFKlGihKtfv77btm2bu/POO915553nLrroIteqVSt39OjRsHO6dOli5xQrVsyOef31193PP//s2rZtazG75cuXdwsWLAid83//93/u73//u/vjH//ofve737k//elP5rBFhkoGwxu5T9euXd1TTz3lLrjgAleqVCnXv3//sHP27Nnjbr31Vle4cGF31VVXuSVLlqSYIzlfzZo1c//5n/9p1/mv//ovt3///hT3/cc//mEVfhhbRhgzZoy74oorbEyszb333hu6z8qVK23eOGx8GMdtt91m+1lLtnGcEEIIIYQQ8SDhlkRMnjzZHKb//u//dkOGDHG33367u/baa93GjRvdwoUL3eHDh034RJ6D0Fu/fr2JuI4dO7qmTZu6G2+80W3evNn9+c9/NsF3/PjxUKl6KufMmTPHbd++3fXt29f16tXLzZ49O82xFS1a1K1bt84NGzbMPfvssyFxxjX/+te/2tjZP3bsWNejR4+w80+dOmViFDH5r3/9y+aIIG3QoIE5a55ly5a5Xbt22bXfe++9hNeSNUNsMk6ux/ohLAHBVqtWLde+fXt36NAh+5CcOm/ePNvP8WyLFLSpcfLkSUtwDX6EEEIIIURyoVDJJAJ3CFEEAwcONNE2aNCg0P6JEyeawNi9e7e78sorbVuVKlVcnz597OeePXua4EPIIUoAYfbqq6+6rVu3upo1a7qCBQu6AQMGhK6J87ZmzRoTbpGiMEjlypVdv379QuMcNWqUiax69eq5pUuXup07d7pFixaZUwaMG7fQM2vWLBN448ePDyWBTpo0ydw3cssQmIA45JiMhkgePHjQrtW4cWMTi2XLlrX19KGgXL9IkSLmHnpwAaFkyZI2rngZPHhw2JoKIYQQQojkQ45bElG9evXQz1u2bHErVqwwV8p/KlSoYPv27dsXJqg8+fPnd8WLF3fXXHNNaBshgnDkyJHQttGjR9u9LrzwQrvuuHHjTOjEIngfuPjii0PX3LFjhwlKL9oARysI89m7d6+JKD8fhNKJEyfC5sPYMyOvDUGJWCtXrpw5jtOnTw+5jpkNgvnf//536ENIqBBCCCGESC7kuCUROESen376yTVp0sQNHTo0xXGIJg8OWhDcrOA2727hdsHMmTNd9+7d3YgRI0xcIaSGDx9uIY6xiHYff814YD6IRQRUJAjIaGuQEZgXoaK4eYsXLzbnkby8DRs2pMtNi4dChQrZRwghhBBCJC8SbklKtWrVLOfqsssucwUKZN7XgNwy8t86deoU2hZ0vBKhYsWK5jKRF+ZF5dq1a1PMh3BJwhCzq7cZ61a3bl37EOaJYFu+fHkoH49CLUG80xe5XQghhBBCiLSQcEtSOnfubBUiW7RoEarmSKghjhk5YIRFJgL5aVOmTLF8NPLbpk6dai4UPycKwoicuzZt2ph7R3GO3r17hx3TsmVL20clSQqGUCDlwIED7q233rL58XtmQmGTzz//3AqSUCXygw8+MIfQV6pEEOMyUk3Sh20SWomTyLkNGza0qpvsS5RtA+qrAbcQQgghRJKgHLckhXwx3DHcHwp3kPtF2X9co3z5Ev9adOjQwRyn5s2buxo1arhjx46FuW+JwHjefvtt98svv7gbbrjBtWvXzkr6B6EQyKpVq1yZMmXs/rh0tCUgxy0rxA3rhCikMif3otLljBkz3NVXX237CRdF/NK6gFBNcvwuueQSKzLy9NNPW24g7RmEEEIIIYSIh3POnDlzJq4jhRC5AhxHKldSqESOmxBCCCFEcryvyXETQgghhBBCiFyOctxE0kKj7mAvuCCEZZKDFquKpRBCCCGEENmFhJtIwQMPPOB++OEHN3/+/Axdh0Ic5KbdddddLrdRp04dV6lSJffJJ58kJNyEEEIIIYTITiTcRApGjhzpkiH1kXL+5cuXz3GBmyiV+i1y+QoVyZF7CyGEEHmV/UMa5fQQhEgICbdcyq+//hrq+5XdkEiZ7GsghBBCCCFEbkLFSXJR6B7l4SnJX6JECVe/fn23bds2y8Gi1xfl41u1auWOHj0adk6XLl3sHHqJcQy92X7++WfXtm1bd/7555ujtGDBgtA5lP+nTD591QgFpO8YDlukkxQMb+Q+Xbt2DfV7K1WqlOvfv3/YOXv27LGeZoULF7YS+EuWLEkxR5poN2vWzErpcx16rtHnLPK+lPqnXYHviZYoY8aMsb5yjIm1uffee8P2nz592tYcocqaP/PMMyGnkV5whFJGUrVqVTuO+U+ePNm98847FhLK58MPP4xrnhxHW4OiRYvaMTfddJP1nBNCCCGEECI1JNxyEQgBHCb6qw0ZMsR6hF177bVu48aNbuHChe7w4cMmCCLPQXSsX7/eRFzHjh1d06ZN3Y033ug2b95sPdoQfMePH7fjaRJNM+o5c+a47du3u759+7pevXq52bNnpzk2hAZNpYcNG2bCxoszrknvNMbOfnqa9ejRI+z8U6dOmRhFTFIUhDkiSBs0aGDOmmfZsmVu165ddm0aVScKa4bYZJxcj/VDWEbOiXBJ1g7x+sILL1jzcXjwwQfdjh07rHm45+OPP3Zbt241UUyfNp4F4z906JB9WPO05olYRJzWrl3brrVmzRr30EMPmfBLjZMnT1pJ2eBHCCGEEEIkF+rjlkvA1eKFHLEFAwcOtBf/RYsWhY758ssvXenSpU2IXHnllXYODhrHAT/jHiGipkyZYtu++eYbd/HFF5tAqFmzZtR74zpx3Ny5c6PmbkXeB3CMEJYIzMWLF7tGjRqZa4RTBggl3EJfnGTatGk2J8SQFykIGRwn7oPA5L6cR7PqjIZI0hwbgcWaIaKirfeRI0fcZ599FhoPjbHfffddE7TQsGFDd9lll5lzBwjBTz/91K1YsSLqOkFa87zuuutc8eLFzXVDvMUD7h6NuyMp/dhs5bgJIYQQ6UQ5biI7UB+3s5zq1auHft6yZYsJBNwa/6lQoYLt27dvX+i4ypUrh37Onz+/iYJrrrkmtI0QQUCkeEaPHm33uvDCC+2648aNM7EUi+B9ADHor4lIQVB60Qa1atUKO5757N2710SUnw9hhCdOnAibD2PPjLy2evXqubJly7py5cqZ4zh9+vSQ6+hByAadLsZMyCciFdq3b+9mzJhhY0R8vfnmm+bExSKtefIzgg9XrkmTJub04dbFomfPnvZH7z+EYgohhBBCiORCxUlyEYQiBvuE8WI/dOjQFMchmjwFCxYM24cQCW7zwoRwRpg5c6aF+Y0YMcKECgJj+PDhFuIYi2j38deMB+aDWERARYKAjLYGGYF54V7ibOEIEhKKc0XoI+5XPLD+hQoVMtcQMUkYZGSeXCLznDRpkrl3uIuzZs1yffr0sdDQ1BxRxsBHCCGEEEIkLxJuuZRq1aq5efPmWageeViZBTlX5GJ16tQptC3oeCVCxYoVzQXCOfKicu3atSnmg0gpWbJkptnFacG61a1b1z79+vUzwbZ8+XILJYVIscqYKWaCc+nPb9OmjQkthNt9990X1tuNbd6dS+88yV3kg5uGgMbNS024pca2AfWzbS2FEEIIIUTOolDJXErnzp3dd99951q0aGEuEeKKfDfytiLFQnpAmFC4g2vt3r3bKiQGC3AkAsKInDtEDqGC5ML17t077JiWLVtaERUqLLL/iy++MDcM54k8tMyGwiYvv/yyNdgm946cPxzCYKVKwkO7detmOYOERL7yyivu0UcfDbtOu3btTOzhjkWGSSKqKTDC+VT7xJFLa578jlgj55Bx4QYSnon4FUIIIYQQIjUk3HIp5IvhjiHSKNxB7hdl/3GN8uVL/LF16NDBHKfmzZu7GjVquGPHjoW5b4nAeAgn/OWXX6xoCWKHkv5BihQp4latWuXKlClj90eo0JaA3K+scI1YJwqUUECFe1HpEnF29dVXh45p3bp1aMwIZUQbFR4jhS4OJfmFrFcQcuAQghQcIQyS55XWPNm/c+dOd88995jY5X7cm+cihBBCCCFEaqiqpBAx4M8D8Ya4xZ07W6sUCSGEEEKI3P2+phw3IVLh22+/tWIutEogRFUIIYQQQoicQsItDxKtf1giUBnS91nLjZAjRi+4aBDiGCwUEq26Y0ahwAj5arRLKFasWLY+GyGEEEIIIYJIuOVB6P11tke4UtDjtttuc5s2bYpqL6cl3DKDrF5jiTwhhBBCCBEvEm4JQkPmzGgUnQjEyybLGtBAO96+a3nl+WUWlfotcvkKFcnpYQghhMgF7B/SKKeHIITIYlRVMk7q1KnjHnnkEavsSPhc/fr13bZt2yyU77zzznMXXXSRa9WqlZWFD57TpUsXO4dQO455/fXX3c8//2w5UzSJLl++vFuwYEHoHKpIUoXwj3/8ozlKVC3EYYt0aoLhjdyHcvNPPfWUu+CCC1ypUqWs2XQQSs7feuutrnDhwu6qq66yhs+R0IutWbNmJpS4DiXt9+/fn+K+VIyk6mWwtH4inDx50vXo0cOVLl3aGkyzFhMmTLB74rYB60ZIJ/dOi7lz51r1TdatePHi1qaAtU5t7L169UpRKRKqVKninn322TTvx7OiYAnrxf1Y/0iXLrUx8XwmT57s3nnnHZsfH1xGIYQQQgghoiHhlg540caloez7kCFDrNQ8TZTpi0afr8OHD5vwiTwHobd+/XoTcR07dnRNmza1EvObN2+2Uv8IvuPHj9vx9Bq79NJL3Zw5c9z27dtd3759TWDMnj07zbEVLVrUmkoPGzbMhIcXZ1yT0vSMnf2UxkcwBaEHGWIUMUluGXNEkDZo0MDcKc+yZcusbxnXpldaRqAcPyX66be2Y8cO99prr9k9EXI0HwfuRWPvSPEaCcfQ845ea1wLEcScg0Iqcuz0XOO5BBuQf/bZZ9ab7f77709z/CNGjHBvvPGGmzhxolu9erX13SNnMJ4xde/e3b4rrC/H8eE7IYQQQgghRDQUKpkOKAuPKIKBAweaaBs0aFBoPy/wiA4aW9Ojy7s3ffr0sZ9pvIzgQ8jRAwwQZq+++qqJhZo1a7qCBQu6AQMGhK6J80azZoRbpCgMUrlyZdevX7/QOEeNGmVCpV69em7p0qXWO4ym27hNwLiDhT9mzZplAm/8+PHm/sCkSZPMTUJwIDABccgxGQ0zZI2YEyIKF8qHRXpw/HyBkHhCJRE+p0+fNmFUtmxZ24bTFSTa2Hk+b775pjUih+nTp5sLh/uXFi+99JI9U+4JCGLWON4x4cLhOuKQxoJj+ATLywohhBBCiORCjls6qF69eujnLVu2uBUrVphD5D80aYagg4Og8uTPn9/C5YIv74RPwpEjR0LbRo8ebfeiqTPXparhwYMHY44teB+4+OKLQ9fE7UFQetEGtWrVCjue+ezdu9ccNz8fxBONo4PzYeyZkRv2ySef2HrUrl3bZQYIsDvuuMPGh6NJSOr3338fdky0seO6IdwAJwwHkG1pQU8OhFkw1LJAgQLWjDs9Y4qHwYMHW16j//AshRBCCCFEciHhlg5wbILl5ps0aWICJPjxuWQeHLQguFnBbd7dwu0C+oYRRkee2+LFi+2a5MMFwxWjEe0+/prxwHwQi5HzwRkLhg0G1yAjZHZFSEQg7h35guTwvfLKK5bH9sUXX8QcO6GMhE8StvrRRx9Znl/z5s2zbUzxgKuHUPQfxiiEEEIIIZILCbcEqVatmuVDXXbZZRZWF/xkRNyQW0auU6dOnSwUk+sFHa9EqFixor3s4xB51q5dm2I+iE5CEyPnkxVVLHGhEJYrV66Mut87YxQAiRfE6k033WShph9//LFdI5hzFg3yCXH9CJHkQ2gpa5AWrAmuJjmDHsIiaV8Q75j4OZ75UbiFlgjBjxBCCCGESC6U45YgnTt3ttA3HBtfzZFQQxwz8qhwWxKB/LQpU6ZYrhT5bVOnTnUbNmywnxOFHDJy7tq0aeOGDx9uOVK9e/cOO4bwQPZRSZLCJgiaAwcOuLfeesvmx++ZCYKX8VC4g+IkhBVyP8I7yeUjJwzRQxGRhg0bmkNH+GZqIKDI6SMXD+HF799++62J1rRg7uQH4mq++OKLcc/h0UcftZxFnhlhsi+88IL1ZYt3TKwBzxnHjxBaxGCkcxqLbQPqS8QJIYQQQiQJctwShHwx3DEcE17McZAo+08hjXz5El/WDh06WDELwvXInzp27Ji5bxmB8eDy0LT6hhtucO3atbOy+EGKFCniVq1a5cqUKWP3R1wQrkmOW1aJA4qy3HvvvTY/hA8FW3z5/ksuucRcqqefftryAGnFEAvGyPgReYhUCsJQ9TFYgCU1GAPrTGXPYJuFtHjiiSesIigClJxB8gPvvvvuuMfEfAmdJC+OfEa+T0IIIYQQQkTjnDORjaeEELkaHFPcOfLd5LgJIYQQQiTH+5ocNyGEEEIIIYTI5SjHTSQMjbpTC0UkLDNW5UiqWKYH2iFQmTE1aFZOmGdmEiunjkqRt9xyS6beTwghhBBCiNSQcBNhPPDAA1ZgY/78+WkeS24WLQOiQcEOip1QpTGzcgpTu5ffn9nEuh85eEIIIYQQQmQXEm4ijJEjR1oj6njAUaNdQGpQLj/W/vRAc+vMula8ZPf9hBBCCCGESA0Jt1wIZel9H7PsJit6tuW1NcgrVOq3yOUrVCSnhyGEELmK/UMa5fQQhBAiS1BxklxAnTp1rNw97QRKlCjh6tev77Zt22b5Y+RZUQ6fsvNHjx4NO6dLly52TrFixewY+spRTr9t27ZWmh7HiFwsD60LKPFPTzjcMkrR47BFhkoGS+Jzn65du4Z61ZUqVcr1798/7Bwad996662ucOHCloe2ZMmSFHOkATj92WiXwHXoF7d///4U96VNAWGPjC0jjBkzxsI1GRNrQ8n/jKwdpPVMFi5c6G6++WabI33ZGjduHNY8nfnSm47eeLfddpu1YKB/3Zo1azI0VyGEEEIIcfYj4ZZLmDx5sjlM9PKiqfPtt9/urr32Wrdx40YTBIcPHzbhE3kOQm/9+vUmRDp27OiaNm3qbrzxRrd582brL4e4oD8Z/Pbbb9ZIe86cOVbMo2/fvq5Xr15u9uzZaY6taNGi1kB62LBh1qDbizOuSd83xs7+sWPHuh49eoSdf+rUKROjCCIKmjBHxE+DBg3MWfPQrJpm1FybxtuJwpohNhkn12P9EJYZWTvy/tJ6Jgi/bt262X7mQv88+rqxRkFoft69e3fLoaO/G03cT58+nep8Tp48aSVlgx8hhBBCCJFcqI9bLgAHiJdxBAMMHDjQBM6iRYtCx3z55ZeudOnSJkR42eccHDSOA34mzBERNWXKFNv2zTffWJ4Zjk7NmjWj3hunj+Pmzp0btThJ5H2AJt6IGATm4sWLXaNGjdyBAwdCBUIQNThTNP3GRZs2bZrNaceOHeY4AYINZ4r7IJK4L+dRPTKjIZI4WjhnrBliMdp6p3ft4nkmkeDG0Vj7008/dZUqVTLHDbdz/Pjx5nwCAvrqq6+2taEJeTRwOGlGHknpx2YrVFIIISJQqKQQIjegPm5nMdWrVw/9vGXLFrdixQpzpfzHv9QHQ+8qV64c+jl//vwWnnfNNdeEthHOB0eOHAltGz16tN0LQcF1x40bZ2IpFsH7AILGXxPBgXgJVnWsVatW2PHMZ+/evSai/HwIlzxx4kTYfBh7ZuS1UcmybNmyrly5cuaaTZ8+PeScJbp28TwTQkZxz7gvf6CXXXaZbY9c3+C9WcvgfaLRs2dP+6P3H8JOhRBCCCFEcqHiJLkEQhGDPc6aNGnihg4dmuI4/6IPBQsWDNuHmxXc5t0tH6o3c+ZMC9EbMWKEiSuEFCX7CXGMRbT7RIb/xYL5IBYRUJEgIKOtQUZgXriXH374oTmChITiWm3YsMFcvkTWLp5nwn4EI/lyCFnOxWkLhoNG3jvyPtEoVKiQfYQQQgghRPIi4ZYLqVatmps3b545NpTBzyzILSOHq1OnTqFtQccrESpWrGgO0KFDh0ICZu3atSnmM2vWLFeyZMlMs4rTgnWrW7euffr162eCbfny5RYOmRXP5NixYxYyiWjzjblXr16d4XkIIYQQQggBEm65kM6dO5sAIOzOV3Mk1BDHjPwoQvsSgSqL5HCRp0Wu1dSpU82F4udEQRiR39WmTRtz74jnpfhGkJYtW9o+KklSMIQCKeTEkYvG/Pg9M6Gwyeeff24FSaga+cEHH5ijlZFKlWk9E+5DuCWhpwhYwiOffvppl5VsG1A/24SwEEIIIYTIWZTjlgshzA53jKIZFO4g94rS9bhGVCpMlA4dOpjj1Lx5c1ejRg1ziYLuWyIwHoqQ/PLLL1a0pF27dlbSPwhl71etWuXKlClj98elozgHOW5ZITxYJ0QhBVS4F5UuZ8yYYUVAsuqZ8EHEbdq0ycIjH3/8cROrQgghhBBCZAaqKilEHiMrqhQJIYQQQojMQ1UlhRBCCCGEECIJUY6byJXQM41ecNEgLPN3v/tdqudSAVIIIYQQQoizCQm3XEpkI+xEody8b4Sdl7juuuvcJ598kpBwy2vQqoDnnNp8hRBCCCGEkHDLpYwcOdKd7emH9Fm77bbb3Pfffx/qr+ZBmJUvXz5bxG5WCKfUrplXhbQQQgghhMhZJNxiQOPkc889N0fuTTJjsq+BiE2lfotcvkJFcnoYQgiRaewf0iinhyCEELkWFScJUKdOHffII49YmfcSJUq4+vXru23btlmu1Xnnnecuuugi16pVK3f06NGwc7p06WLn0MuLY+j39fPPP7u2bdu6888/35yjBQsWhM6hpDzl8OmfhrNEfzEctkj3KOjKcJ+uXbuGeoiVKlXKXJ0ge/bssd5lhQsXdldddZVbsmRJijnSLLtZs2bmcHEdeqvt378/xX0p6U8J/Iz0PoOTJ0+6Hj16uNKlS7tChQrZWkyYMMHuidsGrBtOFPdOi7lz51opftaNvmn0kWOtWYvJkye7d955x67FB0cPuD+95mhLUK5cOffMM8+4U6dO2b433njDDRgwwG3ZsiV0HtsA9472BhdeeKFVA6K9AMelRWrXpHk33H333bbN/y6EEEIIIURayHGLgJf/jh07Ws8uXtx5Wefl/cUXX7TcKkQAwmf58uVh5yCo1q9f72bNmmXnEw7HC3qvXr3sXAQfTZkRDzSDpun0nDlzTHx89NFH7qGHHrLGzVw71ti6devm1q1b59asWWNC56abbnL16tWza9IjDeHIfkqPIiaDIFYQo7Vq1bLiHwUKFHADBw50DRo0cFu3bg05a8uWLTOhEk34pZfWrVvbWF9++WVXpUoV98UXX5jwRcjNmzfP3XPPPW7Xrl12v7Ty1g4dOmQNsIcNG2Zr++OPP9o8CCnt3r2727Fjh5VenTRpkh2PMAXEM8IJIfrpp5+69u3b2zaeGT3tEOcLFy50S5cuDXM7mzZtamNCdLPttddec3fccYfbvXt36NrRSO2ajRo1ciVLlrTxsebxNlJH/PLxMEchhBBCCJFcSLhFcMUVV5gwAETNtdde6wYNGhTaP3HiRBMdvLzj4gCCpE+fPvZzz5493ZAhQ8yxQyBA37593auvvmriqGbNmq5gwYLmyHhw3hA3s2fPjincKleu7Pr16xca56hRo0xkIdwQCDt37nSLFi0ygQKMO1iZEVGJwBs/frw5PoCIwH3DnaKxNBQtWtSOyWiIJGvEnBCAOGOA4+Xx4gcxE5njlppwO336tAnUsmXL2jbcNw8iC4GDGxnEPxvA5ULk0Swb4cY5uKmI2OB5q1evNiF+5MgRcwrh+eeft7w1XD+Edmqkdk0vTJlr5BhjMXjw4LDvixBCCCGESD4k3CKoXr166GdC3VasWGEv4ZHs27cvJNwQVB5cFFy0oKDABQNEgGf06NEmAnHhcPLIJatatWrMsQXvAzh0/pq4TQhKL9oAZy0I89m7d6+5TUFOnDhh8/Ew9szIa6MwB+tRu3ZtlxkgkHG8GB/OIULz3nvvtVDLWCBYcfyYI60CEH9pNUJkrTiWZxmEZxVcq+yAfwzAaQ06bjxrIYQQQgiRPEi4RYDb5OHFvUmTJm7o0KEpjkM0eXDQguBmBbd5dwu3C3B7cH1GjBhh4gohNXz4cAtxjEW0+/hrxgPzQZhOnz49xT7yuKKtQUbI7JL9iEDcO0JLFy9e7F555RXXu3dvWzdcy2jgZLZs2dIcK8QeIYusP2uf1lrxjH2eXJB43MHMBMfPu35CCCGEECI5kXCLQbVq1SwPi/A6wt4yC/LnbrzxRtepU6fQtoy6OBUrVrTCI4QTelG5du3aFPPBfSI0MS3HKTPAGUNYrly5MhQqGcS7ehRriRfEKnl9fAhBJWSSfEIcKa4XeS1EHscg8DwHDhxIMY7I81irb775xp57IkVEol3Ti+/0zFcIIYQQQgiQcItB586drUIkBTF8NUdCDXFsyAGLt7hEJOSnTZkyxfLRcIqmTp3qNmzYkKprFA8II0I327RpY+4d4XRBsQI4T+yjkuSzzz5rBVIQMW+99ZbNj98zEwQP43nwwQdDxUm4H+Gd5PIhqBBi7733nmvYsGEoNyw1cNbI6SNEEvHJ799++62JVn8/1pRiJ4Q44q6x1oSj8syuv/569/7775vQixwnRVMI7WQNcEBZT9xQKmyS88jafv3113Y+hVFoEJ7W3COviWvGduaA8OT3tMI8Y7FtQP1sEeBCCCGEECLnUTuAGJAvhjuGQ4JYwEGiUiOhcvnyJb50HTp0sAIbVB+sUaOGO3bsWJj7lgiMB0FCDtYNN9xglTAp6R+EiparVq1yZcqUsfsjeGhLQI5bVgkAirKQh8b8KlSoYAVbKN8Pl1xyiYUwPv3005YHSCuGWDBGxo/IQ0hRdISQR1+AhWvTvgBRRegnz+4vf/mLe/zxx+3a5BDiwNEOIAiVLanySHsCzpsxY4YJyg8++MDaK9DWgfvdd999Jjx9zmIsol0TGC/hnuSoUfhGCCGEEEKIeDjnDLXUhRB5BtxU3ERaPshxE0IIIYRIjvc1OW5CCCGEEEIIkcuRcBMxocE1eWfRPuT4pbYvVq5aapCLFut67M8tXH311amOM1rVTiGEEEIIITKCipM45x544AH3ww8/WHPljEBeFHlmFLQ4WyBfjAIbwQIn5MaRX0Y+XWaW/CenMHivaPtzCzQ9pxDKP//5zxT74smBE0IIIYQQIj1IuDnnRo4c6c72VD/6kVEo4/vvv09XHzKEWfny5cN+5/zgttSoU6eOFQR56aWX4roXpffjuW5WkF7RzRpQFdKPN7PEf3qo1G+Ry1eoSLbdTwghMsL+IY1yeghCCJGnyTWhkr/++muO3ZvEwexuqpzb1iBZ0ZoLIYQQQoi8QI4JN9wYSrRTXr9EiRKufv36btu2bVbanTwhws1atWrljh49GnZOly5d7Bz6X3EMfdYoL0/Jdnpl4YAsWLAgdA6l/Cl5T4803CLKxeOwBcEtCTot3Kdr166h3m2lSpVy/fv3Dztnz549Viq+cOHC7qqrrrIS75HQEJt+ZYhCrkP/tP3796e4L2X7CQNkbBnh5MmTrkePHlZq3rtBEyZMsHvitgHrhrvEvdOCdW3durU9D5p6U8o+kjFjxlivNNaB50Hpfz83Gm+z1tyPT3Du0cANJBST8vk8K647adIk28e5XIN+bDQv536VKlWyewThd9ohMH/GTKuB06dPx/ze+Qbb9GfjHultuM13Y/Lkye6dd94JzRWH04959uzZ7pZbbrE50Utu9+7d1rePMFTWlu88/eiEEEIIIYTIlY4bL7vnnnuu9dsaMmSIu/3226231caNG93ChQvd4cOHTfhEnsML9/r1603EdezY0TVt2tRe5jdv3mz91hB8x48ft+N/++03a4A8Z84ct337dte3b1/Xq1cve5lOa2xFixa1Js80YKZhtRdnXJM+aIyd/WPHjjXBFOTUqVMmChCTFPhgjryk09sr6PLQjJmG0VybRtQZAZFFvzCaXe/YscO99tprdk+E3Lx58+wY7nXo0KEU4jUaTz75pAkhBMnixYtNjLDGHp4TApe14bo8M8QscH0aWNNbjfvxYRyxoL8azwjhzfjpAcezjhzTE0884T7++GO7fpMmTawPHnz11VfW4w1xtGXLFjsf4Tpw4MBUv3c8O0QUIBIZp/89Xrp3727fU56tnyvfR0+/fv0sJ5C1Ixz0/vvvt38UYI34btDUne9lLEFOSdngRwghhBBCJBc5muOGo4IoAl6uEW2DBg0K7Z84caK97ONQ0AAZqlSpYi/B0LNnTxN8vNwjEIAXYF7Yt27d6mrWrOkKFixoTZ49OG9r1qwx4RYpCoNUrlzZXrj9OEeNGmUii6IUS5cudTt37rTiFL5gBuP2jaBh1qxZJvDGjx9vrosXBrhvCCAEJiAOOQYhkRFYI+aEAKxbt65tK1euXGg/jh+ULFkyrrDQn376yUTPtGnT3B133BESPIhgD1UeGX/jxo1NoJYtWzbUVJrwU+ZE028cy3jgepyPEwXRnC/cMppbA88Zscg4EUK4f3xfeFasOQ2/v/76axPVfC980/Tg9y4I6xLvWIMgjnHTEFjRzkfYIeLh0UcfdS1atLDv0k033WTbcITfeOONVK8/ePDgsO+wEEIIIYRIPnLUcatevXroZxySFStWhJVV58Ub9u3bFyaoPJSjL168uLvmmmtSVPQ7cuRIaNvo0aPtXoTgcd1x48alWVo+eB8g7M5fEzcIgRCscoj7E4T54KQgaPx8EE8nTpwImw9jz6hoA6oxsh61a9d2mQFjxBmsUaNGaBvjD4ZzImIRawhEXE7K4HunMxFwTwmFpKAJQuyjjz5KcUxwnXGvEHk8D+D/2e+FMiCOEKFffvll1O9ddhD8LvnvZ+R3Nvh9jYR/oKB5o/8QgiuEEEIIIZKLHHXccGs8vFwT9jZ06NAUxyGaPDhoQXhJD27zL+24XYAQwPEgP4uXeoTU8OHDLcQxFtHu468ZD8wHgRCtpxcCMtoaZITMLMsfL6wl4X84iIRS4mqR70WoYSLFXnAsDxw44D744ANzDnH6Onfu7J5//vlMHXdmrXm8RPt+Rm6L9d0iX4+PEEIIIYRIXnJNO4Bq1apZHhbhcTgpmQV5TOQbderUKbQt6HglAn3McD3IZfKicu3atSnmQ7gkoYm///3vXVaDg8PLPzlpPlQyiHf1KNYSD5dffrmJCwRumTJlQsVDCMkMuno8K+7Hh9BSBNvy5ctDOYDx3i8oatu0aWMfCnqQ0xYUbqyzz6Oj6MimTZssfNI/F75DtHbwAonnj8AMhnhGg7mmd6xBEplrRtk2oH62fLeEEEIIIUTOk2vaAeCsfPfdd5b/g2ODuCKHjGqRGXkhJp+JIhpcC9FBAYz0Fp+IBJFCzh3igpBICkz07t077BiqI5J7RyVJ9n/xxRfmTFHMIxi2l1kgeBnPgw8+aL3E/P18ERZCGhEzFEChgiGOYCwI7ST3CuGEEKPiJ5UifZ4YcC0KoRCmiVM2ZcoUE48+nJIxIfyorkh10LQcSxw7CqEQYvrZZ5/Z9RFjQQh7pd8aOYZ8ZxCTzBkQ5whqitawn2shJrt16xY27tTWj7yzb775xq6ZXjifvEqKtDBXitMIIYQQQghx1gk38sVwRxBpFO7AQaJkOw5OWi/dsejQoYO5P82bN7d8LSoQBt23RGA8iIdffvnFSs+3a9fOSvoHoSjHqlWrzK3i/ggQhBA5blnlklCsg3L8zI/8QAq2UNIfLrnkEitwQXl8cqq8SxULQkpxvQhhRazefPPNYflhPJu33nrLqoEyPyo0UtXy6quvtv2EqJJ3R7sEnLS08gpxrcjnIicMV41zCXUNQjEaPhSpWb16tXv33XdDlSeZI2GWVBxl/8MPP2xr7ovZxIJQWsIzyV30BVbSA2uNYCXnjrnyXRZCCCGEECKzOOcMcWVC5HJw7agIShsAipckM7QDoGonhUoUKimEEEIIkRzva7nGcRNCCCGEEEIIEZ2kF27kbd11110Zvg75Y+SWZQRy4YLtEIIfwgZT28cnvRC2GOt6aYU1JgKhi6ndj31BqE6Zk84a4Z6pjZVKoZnxvIUQQgghhMhzVSVzipEjR1oVwtwA+VEU+ogG+XSJlvynSMltt91mRTd8mX5yClO7l9+f2Tz77LOW9xaNtCxkin9k1XNCJCLCgutBrlxqBUZ8LzaPwjiFEEIIIURSCDcaPWdGE+pEIPY0t6wBwqx8+fLZcj/K+GfWveJ9frRG4JNbQAimVrGUKpy5nUr9Frl8hYrk9DCEEEnG/iGNcnoIQgiRlORIqGSdOnWsqiFVI6kIWL9+fSs3TwNmQtFwNFq1amVl1YPnUOadc4oVK2bHvP7661Y1kZYB9OpCiCxYsCB0Di/lVBXEDUEUUfUPhy1WqCT3oWT/U0895S644AJXqlQpc2SC7Nmzx6oeFi5c2ComUo0wEsrSN2vWzBwurkNbAJyZyPtSjRJ3y5fQT5STJ0+6Hj16WFVEmjWzFhMmTLB74rYB60aIH/dOi7lz51plT9atePHiVlXSV6hMbey0OaCdA/OlyTUOYlqNzj1UiuSZ8hx99c0g/p5UxqRqIw4d4ZWIxuAa8OwQhzwbqmAGWz/gPDJ/viNUx2Sdpk2bZtekrQP7+LzxxhsuPfD9AqpRcj7foeCYBw0aZHPju4DrSP852iywTvSXmzRpUrruJ4QQQgghko8cy3GbPHmyuTSUTeelnZLyvPjSc23hwoXu8OHDJnwiz0HoUe4dEdexY0fXtGlTa7C9efNmayOA4Dt+/LgdT98wXoznzJnjtm/fbn3CevXqFeptFmtsCA9Ex7Bhw+xl24szrumbS7OfEvgIpiCE2CFGESHkrTFHBGmDBg3ChAZ9w+j7xbXpWZYRWrdubaX46au2Y8cO99prr9k9EXI0pQbuRdPwSPEaCccgwOiPxrUQPMw5GKoYOXb6wtGY+6uvvrIS/QghxG9avduA54E4RuDw/GlqPmbMmBTHcU8/HuZKKwJEl4f7MVeeH98HxCvPgf6AQWiJwHeOa9WrV8898cQTltPGvPnQOiI98H2EpUuX2vmMy0MPvK+//tpaQ7zwwgvWV65x48Ymovn+ID5pWZEVvf2EEEIIIcTZQ460A8CRoEQmL9cwcOBAEzg0yfbwIovoQBzQ7JpzcNA4DviZMEcEBY2fgebJvPSvWbPG1axZM+q9cfo4DkfJuyI//PBDqNBE5H2AXm0IS172Fy9e7Bo1amQNp30eGEITt5DebjgsuDjMCWGAAwMINhwX7oPA5L6cRxGQjIaJ0lgc1wsRhTMWT45bLHguOFK4ddFCBqONfdy4cZa/xjk4SekB4Y1op7m2h+eH6+bzzrjnP//5T3My6ZEHiGacK8qskgOIGMItu//++0MCmtw4XFqO8+vAM8ABjZXjlhY8V/+8U8txY8zc8/PPPw/1IqS/Ho4gQi74PR4/fry77777ot4LJ5GPh78d/jZKPzZboZJCiGxHoZJCCJFk7QCCjZxxZ1asWBFWuY8XXNi3b1/oOBoze6iySAgf4XyRRSOOHDkS2oYY4F6E13FdBEZaFROD9wHEoL8mYoyX5mDxjlq1aoUdz3z27t1rjpufD2IGIRKcD2PPjNw+BAfrgeOVGdC8+o477rDx4WgSkoroCxI5dsaA+EqvaPNrSnP0IJFr6sflRZs/BqcPMce6ItRuuumm0P6CBQua6Ob6QQjhzC5w8oIN5PmOBr+z/nsc/M5GMnjwYPvD9x++f0IIIYQQIrnIseIkhCJ6ePlu0qSJGzp0aIrjEE3BF/FI1yO4zbtbPjxv5syZ5gKNGDHCXvIRUsOHD08z7yrafeIJ+QvOB7FI2fhIEJDR1iAjJFptMjUQE7h3H330kTmMr7zyiuvdu7etm8/nihx7Zo8hK8msdY+HtL6z8Xy/evbs6bp165bCcRNCCCGEEMlDrujjVq1aNffZZ59ZWBt5ScFPRl6yyS0jDK9Tp07mBnG9oOOVCBUrVjSHh1wmz9q1a1PMhwImhMRFzicrqlji4PDiv3Llyqj7vTOWWgXFaCAmcK/IISMEkGsQGhjLpcR1i8wni3dNI8V05Jp6J5OQyOAxPo/v8ssvD+VMenDgKE5CAZlYcF561iba+ZCRa8SCIipY7MGPEEIIIYRILnJFO4DOnTtbOB4FMXw1R0INcczI/cEBSoQrrrjC8t/IncMpmjp1qr3Ie9coEcghI+euTZs25t7hfuBGBWnZsqXtI4+KwiYUSCEnjqIVzI/fMxMEL+OhmAjFSQgp5H6E31HghTw1hBhFRBo2bGjuWKym3YgoCoGQi4f45Pdvv/3WBFZq8OwoLkLOF6F9OKUIPkJKo4U9Bnn00UctH4wQRsQiTiVCvly5cmHHkSdIxck+ffpYXhmFPshZJBQRgU+xGl+tsUyZMlZYhkI1nJPW+n3xxRcmPHk2OLOIpXhhjVhT8v44n4qW2dFmYtuA+hJxQgghhBBJQq5w3Hi5xynBsUAs4CBRUIJCGsH8oPRCtT6Kl1AlkByqY8eOmfuWERgPzhPOD/lT7dq1s7L4QcjDovgE4oH7I3h8ifusetF+9dVX3b333mvzIz+wffv2ofL9l1xyiTlnVFMkxwqxEwvGyPgReYhUhBLhphRgieU6EVaJiOE8niHFXOIR3TyfZ555xkQtIaaITkRYJOTdIcZpxcA5f/nLX8JaNXC/e+65xyqL4noi/hHtFC2JBedQ8ZPCJYSyUrEyvT3xEMxU8uS7HCx8IoQQQgghRJ6tKilEeoms/pnMZEWVIiGEEEIIkXmcVVUlhRBCCCGEEELEh4RbLoG+ccF2CMEP4Yap7YuVq5YatEOIdb202iUkUhI/tXtFq7yZ0zCm1MbLXIQQQgghhMhuFCqZSyBn7quvvkp1X6xy+1SrBJpPkxtISGEsTp8+bcU9YhXrIG8rsyBnjQqP0SDnjmIguSmM8scff3SHDx+Ouo9S/pFNybM7jFOhkkIIIYQQuZuseF/LFVUlxf/rg+YFWFaDKMuue0Gk0MlsRo4c6TLz3x8QkhkVk0IIIYQQQmQmEm4iDJyxyAbROQXl/32PtFhkR+n93EilfotcvkJFcnoYQog8zv4hjXJ6CEIIIeJAOW4JQsNr+pXREw63jN5pc+fOtX0ffvih9U2jFxq9yWgPQCPwXbt2hV3jn//8p7v++uut71eJEiXc3XffHdr3/fffu9atW1spe86nFD9NvYMQGknLAfZzLu0OInnnnXesND73oC8abQEIlfQwTloJUFqfXmiRrQ3SA2Omhx0l9VkTSvdPmjQptJ/G5fSVo80DvdYomx8M2STkkD5wjIGy+n/6059cr169rJVDJKw3PfKC5wWfDT3ccBXpx8YaBeeV1jhiQcuKbt262bnFixe3FgaRbl+dOnVc165dQz0JS5UqFda2gOP5nXExPubK8UIIIYQQQqSGhFuCINpo7j127FhrFv3444+7v/3tb27lypWhY2jMTf+zjRs3WngiDbI977//voktep7RqBqRR184D2KE89599123Zs0ae9nnWJ8rRlNsesPRk43G0fQgGzhwYIqCJ4g/Glxv377d+owh9iLFGSKCsXz66adhY0wv9GLjPgsWLHA7duwwQYggBcZdv359C0FkXPTto9gH/dNw1jysAwJ3yZIl1jAcIbh+/Xq3b9++0DGs99atW939998fdRw9e/a0nm5+PG+++abl0qVnHKnB82QNJ06c6FavXu2+++476+sXyeTJk00I85wQkYhM5gTz5s1zL774oj0PxDi5cfS9S42TJ09anHTwI4QQQgghkgsVJ0kAXqRxUpYuXepq1aoV2k4z7uPHj7uHHnrIhBT7aRoNH3zwgWvUqJEVGsH9woHDAZs2bVqK6/MyT+NrRAXHAW5a6dKlTRA0bdrURAvJjghAz3333ecWLlwYKk5St25duz9CxsP9cIK+/vrrkONGQROEREbBtUOoIWoi4b4ISwQd9wSEEs4VwoXG64hVxk9Vy2CIZNWqVa1JNkIMcOGWL1/u1q5dm6I4CIVFcPxGjRplzyORccQCdwyR/uSTT9rvuJe4rjQO98VJcNxw5hCGHkT57bffboLyhRdeMNG2bdu2uMJSEdY4pZGUfmy2QiWFEBlGoZJCCJH5qI9bLmHv3r0m0OrVqxdWKh4HLugMVa5cOfTzxRdfbP9/5MgR+39cMi/qIkFU4NAFQwQJyyN0kH3+mMgQwqCIhC1btpjTExxj+/bt3aFDh2z8HsI5M4OOHTu6mTNnmtBCHH700UdhY2HdcLr8WBC/J06cCFsznKfIvDZcN1wz4N8ZZsyYYduiwbogrFNb23jHEQ3+8Fi74LrznKKtX/DZ++fvnz3CGwGPcOd54NgFw1cjQXhzb/8h1FMIIYQQQiQXKk6SAD/99JP9P27XJZdcEraPnCUvAIJuind3yL+CWOX9M3OcODV//etfU+zD9fMQ0pcZkIdH6X/cRcICEU+dO3d2zz//vI0FVypa3zYcslhjadGihevRo4fbvHmzCR6ES/PmzaOOIa11jXccGSXSSeP5+2ePc0o4KI4s69SpUyc3fPhwC7ON5sDxneIjhBBCCCGSFzluCXDVVVfZizQhfRTACH54KY8HHBnyuaJRsWJFc2DIj/IQKsnLPvf2xwT3gw8d9FCUhHMix8gnX76sefSInzZt2lhI4ksvveTGjRsXGgshoCVLlkwxlrSqQl566aWudu3aJrb44HRynWhQEAXxltraZmQc7Mc5C647z2nTpk0uvTDGJk2auJdfftmK2ZDHSI6hEEIIIYQQ0ZDjlgCE2XXv3t1ynXBRbr75ZgthIyeNGNZ4+pb169fPHKnLL7/cctMQADhVOEuIDyodEkZHLhT3e/rpp83dYztQhfCmm24yN4ttixYtsvywIH379nWNGze26oX33nuviTVCBcmtiixkkhlwP9ysq6++2sIVKS6CwARCG3GVGCvhm4gx3Lm33nrLwir5PRacz5qRjxYrHw8nkTXkmoRcskbffvutFTShmEtGx0GhF/LUeEYVKlSwfLW0Gp5HQnETcuAIuaQiKCIXIZfefnfbBtRXA24hhBBCiCRBjluCPPfcc1Ysg+qSiBOqEhI6SaGKeKCAxZw5c6xqJDlhFK6geqKHMvqIIIQXuWvkdiHsfChdzZo13euvv27NpymNv3jxYtenT5+we1A9EfHEPtoOcA6iJ6saYiOUyMfCTbz11ltd/vz5LecNECirVq0yEUnoJmuGkCK3LB7xgfDEdSQ3L1j6Pxo8lyeeeMKEJPchrNLnl2V0HFy3VatW5iryXBDVwTYO8UAhFJ4dopK1ImSS1hDkMQohhBBCCBENVZUUIo+RFVWKhBBCCCFE5qGqkkIIIYQQQgiRhEi4iTAefvjhsPYBwU9a+84GUpsfn2BfNiGEEEIIIbIThUpmkGDz54xAuXj6eaWVv5XVkAuGtRsNbN5Y+1Kr9BjM6yOfj2qTuRV6vKUGxWF8u4GcfF4KlRRCCCGEyN1kxfuaqkpmEIqDnE3aF/EVKcAoV3/bbbe577//3srmZwc5JfISmd/+/futKM3HH39sYxZCCCGEECKzOSuEGyXiqWiYE6TV+ysZ1iAvcurUqajNrvMSlfotcvkKFcnpYQgh8hj7hzTK6SEIIYRIlhw33JhHHnnEPfbYY65EiRJW9p7eZHfeeaflIl100UVWsv3o0aNh53Tp0sXOKVasmB1DSfaff/7ZtW3b1sq647YsWLAgdA69tigVj5tCiNyf/vQnc9giQyWD4XLchx5r9AS74IILXKlSpVz//v3DzqEBNOXy6TlGQ+0lS5akmOP//M//uGbNmlnpeK5D3zGcncj7/uMf/3B/+MMfbGwZgb5r9D+jgTjNxVmLCRMm2D1x24B1I0SQe6cF69q6dWt7HjStHjFiRIpjxowZY/3QWAeeByX//dxWrlxpa839+ATnHg3cQHq00QCcZ8V1aakAnMs1Zs2aZY28uR+NvGHixInWd445M06+V+nFt4C49tpr7T58B4LPaNCgQTY/niW94+jZ9+STT9pzpW+cH6cQQgghhBBnlXCDyZMnm8NE02saItMHjRfnjRs3WiPqw4cPm/CJPAehR780RFzHjh1d06ZN3Y033ug2b97s/vznP5vgo1cY0FybF2v6rW3fvt36gvXq1cvNnj07zbEVLVrUrVu3zg0bNsxe1r0445r0D2Ps7B87dqwJpkg3CDGKmKQgBnNEANErDmfNs2zZMrdr1y67Nv3aMgIia8aMGe7ll192O3bssMbf3BMhN2/ePDuGex06dCiFeI0GwgTx9c4771gfOcItWWMPzwmBy9pwXZ4ZYha4Pj3SaEDO/fgwjrR6t/GMEN6M/9VXX7VnHYQm5jTQZj/ryzGdO3d2Dz30kPv000+tp14ioZK+/x792Bgrzbw9y5cvd19//bX1jqNZN03E6c2HCOb5U9SlQ4cO7ssvv4wpqomTDn6EEEIIIURykWdDJXFUEEUwcOBAE204Gx6cFF72d+/e7a688krbRqNq36SaRtEIPl7uEQiAMONlfuvWrdasmlC6AQMGhDkra9asMeEWKQqD0FSZF3Q/zlGjRpnIqlevnr3c79y50y1atMicMmDcuIUenCEE3vjx483BAVwZHBsEEAITEIcck9EQSdaIOSEA69ata9vKlSsX2o8zBOS+MYa0+Omnn8ytmzZtmrvjjjtCYhYR7Dl48KCNHxGDQKUpOM/Qh58yJ5pl41jGA9fj/Ouuu85+v+yyy1Icg9uKaPbwvaGhNmLOQ6Py9ILLBzTQjhwva4cYzpcvn7mifGf5hwH+ASD4PVy9erW77777ol6fJu/B76EQQgghhEg+8qzjVr169dDPW7ZscStWrAgr3V6hQgXbt2/fvjBB5cmfP7+9aF9zzTWhbYSz+cqKntGjR9u9eDnnuuPGjTOREIvgfYAQPH9N3B4EpRdtgLsUhPlQ3RBB4+eDADhx4kTYfBh7ZuS1ffLJJ7YehBFmBowRZ7BGjRqhbYw/GM6JiEWsIRBxOQld9E5nIuCezpw504qDEKb60UcfpTjGizrgeeCEeWGZVRCGiWgLfseC3zn/PQx+5yJB3FGRyH8IoxVCCCGEEMlFnnXccGuCDk+TJk3c0KFDUxyHaPJEFqPAzQpu8+4WbhcgBLp37275WYgrhNTw4cMtxC0W0e7jrxkPzAex6POwork7kWuQEXyJ++yEtSR0EgeRUErcTnIBN2zYEJerFwmO5YEDB9wHH3xgziGCjDDI559/Pup6Zdec0/rOxfP9IP+OjxBCCCGESF7yrOMWpFq1au6zzz6z8DhylIKfjIgbcsvIf+vUqZOF4XG9oOOVCBUrVjTHhFwoz9q1a1PMhwImhCZGzicrqljiACEcyEmLhnf1KNYSD5dffrmJk6DApXgIIZlBChQoYKGZhA8SnkoREXLC/D3jvV9Q1LZp08ZCNGkjgDsaSzjyfSGENaOkd32EEEIIIYRIGsctCM4KFSJbtGgRquZIqCGOGTlghKMlAvlpU6ZMsXw08tumTp1qjpCvIpgICBVy7hAYuHcUmujdu3fYMVRHZB+VJCneQW4YbhJFL5hfMFcsM0DAMJ4HH3zQ8rHIBeR+hO+Ry0dII64QBVAaNmxobhXhm6nBPqpxUqCEMEAEKHMMhgxyrc8//9wKklCoA6cM8ejDKRkTwg8x50NFg+dHgmOHS0loIsU8uD4iORY4fBQHYXw4dj/++KOJdQrXpAfOZ00osMKzoWpldrSJ2DagvhpwCyGEEEIkCWeF40a+GC/cOB4U7sBBohAFIXexXvbTgmp/FLNo3ry55WsdO3bM3LeMwHjefvtt98svv7gbbrjBtWvXzkr6B6EoB1UIy5QpY/dHgCCEyHHLqhd1irJQjp/5kR9IwRZK+sMll1xixTGoykiOVjwl8xGet9xyi4WwIlZvvvnmsLxEng1ClGqgzI/qmlS1RHgBIaoIbtol4KSllVeI60UuGPmFiEHORbjHArGKM0dbAu5LoRSczvSCc4jgpRIn30UEtxBCCCGEEJnJOWfOnDmTqVcUQmQpuLQ4ehQqkeMmhBBCCJEc72tnheMmhBBCCCGEEGczEm5nCTTqDrZDCH4IG0xtX6xctdQgbDHW9dIKa0wEctFSux/7MhOqeaZ2Lx/KKYQQQgghRHaiUMk8ygMPPOB++OEHN3/+fPudnLmvvvoq6rHsS638PQVYyLm766674r736dOnrWhIalBYhLyvzIRCKVjO0cB+pkBIZkGRksOHD0fdR7VMirXkJAqVFEIIIYTI3WTF+9pZUVUyGRk5cqQLam6EGe0CsgNEWXbcix5vt912m7USQJhlpjiLBa0C+OR2KvVb5PIVKpLTwxDirGL/kEY5PQQhhBAiKgqVzAC//vprjt0bBZ9Io+qzaQ3y2tjz8loJIYQQQoicRcItHdSpU8dK4dNqoESJEq5+/fpu27Zt1gOM/CdK5bdq1codPXo07Bz6gnEO/co4hp5zlNpv27atOTu4VwsWLAidQ1sDyv/TLw4njd5mOGyRoZLB8Ebu07Vr11Afu1KlSlmfsiCUuqdUPn3GKLO/ZMmSFHOkOTi92xCFXIfS9sGwSH9fWhhQ+t73XUsUeq716NHDlS5d2hUqVMjWYsKECXZP3DZg3egjx73TYu7cudYOgnWjhxytCHxbg2hj79Wrl7V6iIRedvTQi8cVpK0Djd5Zs5tuusl64AHrX7VqVeslyLNk3YEQV1pN8F1gW6VKlazvnBBCCCGEEKmhUMl0MnnyZNexY0frG8cLOH3I6MX24osvWi4ZIgThs3z58rBzEFTr1693s2bNsvPJK7v77rtNOHAugo+iHvRwoxE1jZznzJlj4uOjjz5yDz30kLv44ovt2rHG1q1bN2tcvWbNGhMqCIl69erZNekJh1hgP/G2iMkgp06dMjFaq1YtK3ZCSOTAgQNdgwYN3NatW61XGixbtsxidaMJv/TSunVrG6tv/P3FF1+Y8EXIzZs3z91zzz1u165ddr/U8vQ8hw4dsibsw4YNs7UlV415BENKo4198ODBbt++fe7yyy+33z/77DObL/dPK9cPIUjPO3rQ4ajxjBGZHhrBcx161lEkhufgm31PmzbN7rl9+/aYTeIRt3w8qeX6CSGEEEKIsxcJt3RCMQ+EASBqrr32Wjdo0KDQ/okTJ5ro2L17t7vyyittG4KkT58+9jNNoocMGWKOHS/80LdvX2uAjVioWbOmFcCg4bUHtwZxM3v27JjCjebT/fr1C41z1KhRJlQQbkuXLnU7d+50ixYtMrcJGDciwoOoRFjgEHnxMWnSJHOScJZobg64SxzjhVyisEbMCRGFMwblypUL7cfxA3Lb4gkLRbghphCovoAI7luQaGPn+bz55pvumWeeCVWVxIVLK48PAYUApnG3F300Ew+CmJsyZYo1EYfFixebuNuxY0fo+xGcczQQlsHvgxBCCCGESD4UKplOqlevHvp5y5YtbsWKFWHl4itUqGD7cHCCgsqDs4KLFhQUuGC+cqJn9OjRdi9e+LnuuHHj0iyzH7wP4ND5ayIUEJRetAHOWhDmg0NE+KafD+LpxIkTYfNh7BkVbfDJJ5/YetSuXdtlBgiwO+64w8bXtGlTC0mlsEmQaGNv2bKlCTfAncM9Y1tasDa4mriUTZo0sXBWxGMQBKQXbX7OuKletMUDYh+B6D+EswohhBBCiORCwi2d4Nh4fvrpJ3th52U8+PG5ZB4ctCC4WcFt3t3C7YKZM2e67t27W54bDg3XJB8ureIW0e7jrxkPzAexGDkfnLH7778/6hpkhLRCH9MLIhD3jnxBcvheeeUVy2Mj/DLW2AmvJBxz8+bNFpaKMGrevHlc98SRxA298cYbzbFEkK1duzbV+yUyZ3L/CO8MfoQQQgghRHKhUMkMUK1aNctfyuy+ZeTPIQQ6deoU2hZ0vBKBED4ECY4QThwEBYafD+KD0MTsEAe4XwjLlStXhkIlg3hnjGIt8YJYJa+PDyGoOF7kE5L7lxo4YLh+hEiSp0hoaXpaDxAuywdnDBcT946Q19Rc0S+//DIslDZRtg2oLxEnhBBCCJEkyHHLAJ07d3bfffedOTYbNmwwcUUOGe5YesRGJOSnbdy40a7FCz65V1w/IyCMEApt2rSxkEiKdvTu3TvsGMIDyb2jkiT7carIbaNaJWIjs0HwMp4HH3zQGon7+5H3BoguhBgVF7/99ltzBGNB0RXy9lg7wkopCMJ5kXln0WDuOJ0UhIknTBIYL2INx41KkrijuK2x7odAxI2l6AruINfAIVy4cGFc9xRCCCGEEMmJhFsGIF8MdwyRRuEOHCQqNVJII1++xJeWUvEU2CBcjyIZx44dC3PfEoHx4DzhKFG+nkqYlMUPQkXLVatWuTJlytj9ESCEa5LjllXODkVZ7r33Xpsf+YEUbPHl+y+55BIryvH0009bHiCtGGLBGBl/w4YNTaRSEGbEiBFhBVhSgzGwzsePHw9rsxAL1ouCL4gw7kflT8Q8zy8WuLTXX3+9CX5COqk4mhGhL4QQQgghzn7OOROslS6EyPVQzZIG7BQqUaikEEIIIURyvK/JcRNCCCGEEEKIXI6Em0gXlL8PhhKSCxdshxD8UOUxtX3krpHXlh7IW0vtenzSapcQT87dSy+9FPqdMVIFMrX7MffMIJG1EEIIIYQQyYWqSop0Qa+yYHTtddddZy0DokE+XWrl7ynAkkhOYWr38vszmxdeeMGqTEaDHDwhhBBCCCGyAwm3PAj93DKjAXYiEKsbBGFWvnz5bLk3LRf8vbJrDWidkF3zSy+V+i1y+QoVyelhCJFr2T+kUU4PQQghhMg0FCqZB6hTp45VVKRiJeX669ev77Zt22bVEgnZo+Jiq1at3NGjR8PO6dKli51TrFgxO+b111+3io20Kzj//PNNkFCK3kNlQ6pI/vGPfzRBRvNqHLZYoZLch3YBVEa84IILXKlSpVz//v3DzvENyQsXLmxVFCmDHwk95po1a2YVObkOLQn279+f4r5UwsRZY2wZ4ciRI9Y8nXkyX3q4pcWnn37qbr/9djunePHiVkUy2KKAlg24czwjBC6l/2nqnd61EEIIIYQQIhIJtzzC5MmTzWGi/cCQIUNMQND0mZ5l9AA7fPiwCZ/IcxAR69evNxHXsWNH17RpU2vujaCghQGCjxL4QDNsmlHTy2z79u3WwLpXr16hvmqxxla0aFHrozZs2DD37LPPhgQJ16S1AGNn/9ixY12PHj3Czj916pSJUcQkeWPMEUHaoEEDc9Y8y5Ytc7t27bJr09stIyAEEYsrVqxwc+fOdWPGjDExlxoIXsaICEagsUZLly4Na1Hw448/Wl+61atXW3NzwkFpTcD2eNdCCCGEEEKIaChUMo+ACEAUwcCBA0200WzaM3HiRFe6dGlr2E1PMahSpYr1MgMaRSP4EHL0SgOEGX3Utm7d6mrWrOkKFixofdM8OFE0l0a4RYrCIJUrV3b9+vULjXPUqFEmsnCfEDf0OqOZuM9BY9zB3mqzZs0yUTN+/Hgr1AGTJk0y942G3AhMQBxyTEZDJFkjnEYELf3UYMKECTEbZ7/55pvWz27KlCk2DmCeuHZDhw41RxMxHWTcuHE2h5UrV7rGjRvHtRbROHnypH2C5WWFEEIIIURyIcctj1C9evXQz1u2bDGnKFjhkObVsG/fvjBB5aHCI+F9NAn3IDYg6DSNHj3a7nXhhRfadREfaVVrDN7H54X5a+7YscMEZbBwSK1atcKOZz579+41x83Ph3BJhFJwPow9M/LaGBP5csE1Zf0QWbHOQQh70QY33XSTCU5cQMD1RBQjXgmVpGcHoZR+/eJZi2gMHjzYruc/XEMIIYQQQiQXctzyCEHBgBjwTk8kiCYPDloQ3KzgNu9uIT5g5syZrnv37m7EiBEmKBBSw4cPt7C+WES7j79mPDAfRFS0PDMEZLQ1yI0QJnns2DHLCyxbtqwrVKiQrWMw3DMRcEu7desW5rhJvAkhhBBCJBcSbnmQatWquXnz5lnfMZyjzILcMvLfOnXqFNoWdLwSgfBDcskOHToUEpXkf0XOh3DJkiVLZlpn+Vjgrp0+fdpt2rQpFCqJa/bDDz/EnMcbb7xhuW5eQLJe+fLlCxVK4Xdy5chrA+YdLBgTz1pEAwHIRwghhBBCJC8SbnmQzp07W4XIFi1ahKo5EmqIY0YOGGGRiUCIHzlc5GCR3zZ16lQrxMHPiVK3bl3LucONwr3DLerdu3fYMS1btrR9VJKksAkFUg4cOODeeustmx+/ZyYILQqfdOjQwXL8EL9U30yt55wfI3l8zIOqmd9++60VfKG4iw85Zf1YM3rbMc8nn3wy7JrxrEV62DagfrYIXSGEEEIIkfMoxy0PQo4U7g7l+yncQe4XwoMcLRygREHIUPWwefPmrkaNGhb2F3TfEoHxvP3229aM+4YbbnDt2rWzkv5BihQp4latWuXKlClj98eZoi0BOW5ZJUwofsI6UrKfe1LaH8cvNRgjgva7774zl+7ee+91d9xxhxUo8VDg5PvvvzcHEUFHm4TgNeNZCyGEEEIIIaJxzpkzZ85E3SOEyJXg1FGk5N///rccNyGEEEKIJHlfk+MmhBBCCCGEELkc5biJPAmNulPrf0YoYqx8NapYCiGEEEIIkZeQcBMJ8cADD1gVxvnz52foOrQOIO/rrrvuStd5FAD55JNPEhJu8VKnTh1XtWpV99JLL2X4WrnhPkIIIYQQIu8i4SYSgl5lOZkeiTArX758jt1fCCGEEEKI7ETCLQ9DY+dzzz03R+5NsmWyr0FOU6nfIpevUJGcHoYQOcr+IY1yeghCCCFEtqDiJHkIQuoeeeQRK/1fokQJV79+fbdt2zbL9TrvvPOsnxhl6INNnzmHfmOcU6xYMTuGHnA0km7btq07//zzzblasGBB6BzaDFCOn/5tOFv0PcNhiwyVDIY3ch/K3/u+cqVKlbJ+Z0H27Nnjbr31Vle4cGF31VVXuSVLlqSYIw2qmzVrZq0NuA693fbv35/ivpTRp5y/b36dKDTMpv8aY2JtKPOfGpT6b926ta0j7QFYd+bkoX0CvfUuueQS20+bhhkzZoRdg3XnGjwvmnCPGDEiQ+MXQgghhBDJgYRbHmPy5MnmMNHHbciQIe7222931157rdu4caNbuHChO3z4sAmfyHMQeuvXrzcR17FjR9e0aVN34403us2bN1svOATf8ePH7fjffvvNml7PmTPHbd++3fXt29f16tXLzZ49O82xFS1a1K1bt84NGzbMmml7ccY16ZfG2Nk/duxY16NHj7DzT506ZWIUMUnxEeaIwKFZNs6aZ9myZW7Xrl127ffeey/htWTNEJuMk+uxfgjL1EA0cs67777r1qxZY6GiDRs2tHEDfeeqV6/u3n//fRPU9IZjXVl3D025V65c6d555x23ePFi9+GHH9oziMXJkyetpGzwI4QQQgghkgv1cctD4Grx0u5f9AcOHGgCh8bQni+//NKVLl3ahMiVV15p5+CgcRzwM2GOiKgpU6bYtm+++cbcH8RIzZo1o94bp4/j5s6dG7U4SeR9gCbTCEsEJiKlUaNG7sCBA+aUAUIJ18oXJ5k2bZrNaceOHVa0BBBsuG/cB4HJfTnv4MGDGQ6RfOutt8x1ZM0Qi7GKhuCssZ6ISQSvd9hYawQrQjgajRs3dhUqVHDPP/+8VbMsXry4zdMfT0NvRDIiL7XiJDiXAwYMSLG99GOzFSopkh6FSgohhEiWPm7Kcctj4Oh4tmzZ4lasWGGuVCT79u0zoQGVK1cObc+fP7+JB8L4PIQIwpEjR0LbRo8e7SZOnGgCiSqNCChETCyC9wHEoL8mYgyR40Ub1KpVK+x45rN3794UIgoni/l4GHtm5LXVq1fPlS1b1pUrV85cPT533323hTlGwvgLFCjgatSoEdrGOhKqyT5AuA4aNMicya+++srWDLfMX485sC14DcJB0wr37Nmzp+vWrVvYfwhYSyGEEEIIkTxIuOUxCEX04OA0adLEDR06NMVxiCZPwYIFw/bhZgW3eXeLcEaYOXOm6969u+VfIa4QUsOHD7cQx1hEu4+/ZjwwH4Tp9OnTU+y78MILo65BRmBeuJeEK+IIEhKKu7VhwwZz+dILa0QuIM4Z4pJxklsYDPNMhEKFCtlHCCGEEEIkLxJueZhq1aq5efPmucsuu8zcoMzChwN26tQptC3oeCVCxYoVrfDIoUOHQqJy7dq1KeYza9YsV7JkyUyzlNOCdatbt659+vXrZ4Jt+fLlFkoaOf7Tp0+beA2GShKSSqEVv24UU/nb3/5mvyNad+/eHdp/+eWXm7jlGmXKlAkVPOGY2rVrZ8t8hRBCCCFE3kTCLQ/TuXNnqxBJJUNfzZFQQxyz8ePHW1hkIlBlkfw3cueoLDl16lRzofg5URBGhG62adPGnCnC/Xr37h12TMuWLW0f4oeCIeR+kRNHLhrz4/fMhMImn3/+uRUkoVLkBx98YGIrWugia8K42rdv71577TVz655++mmrIMl2fww5gB999JFd74UXXrBiMV64EdJKtU4KlBBmiUBlDfLlS6xG0LYB9bNN4AohhBBCiJxFVSXzMOSL4fKQW0XhDsLzCM3DNUpUDECHDh3McWrevLnlY+EsBd23RGA8FCEhX46iJe3atbOS/kHIBVu1apW5UdwflwuhQ45bVggU1glRSAEV7kWlS8r3X3311VGPnzRpkoVyUnCEEFLq+iD2fIhonz59zDWkMiaFTWiJEGyZAAjTW265xUJcEbM333xzWN6iEEIIIYQQ0VBVSSHyGFlRpUgIIYQQQuTu9zU5bkIIIYQQQgiRy1GOm8jT0DeOXnDRICzzd7/7XcwqlkIIIYQQQuQFJNxElhDZoDtRaCngG3RH47rrrnOffPJJQsJNCCGEEEKIvIKEm8gS6GeWHemTCLPy5cu73Ab94BCtqYlKIYQQQggh0oOE21kMjZ/PPffcHLk3yZhn4xrk5JpGUqnfIpevUJGcHoZIQvYPaZTTQxBCCCGSDhUnOYugBP0jjzxiLQFKlChhZem3bdtmOWD0ELvoootcq1at3NGjR8PO6dKli51D7zGOoTfczz//7Nq2bWv9ynC0FixYEDqH9gOU6aevG44Xfc9w2CJDJYPhjdyna9euoX5zlMrHlQqyZ88e66lWuHBh6322ZMmSFHOkiXezZs2slD/XoYfa/v37U9yXVgO0S4jWky090Nz8ueeec61bt7aKQA899JBt79Gjh/Wlo4VBuXLl3DPPPONOnTpl+9544w03YMAAt2XLFgv15MM2IHyUVggXXnihXY9WBBwnhBBCCCFELCTczjImT55sjhD93YYMGWLC4Nprr3UbN250CxcutIbQCJ/IcxB669evNxHXsWNH17RpU3fjjTe6zZs3W484BN/x48fteJpU0wx7zpw5bvv27a5v376uV69ebvbs2WmOrWjRom7dunVu2LBh1mTbizOuSe82xs5+eqohjoIgjBCjiEmKkjBHBGmDBg3MCfMsW7bM7dq1y65Nk+2M8vzzz7sqVaq4jz/+2AQaMAbEGPNHtCJ2X3zxRdtH/7snnnjC+sEdOnTIPmwD1vXIkSMmhDdt2mR93+644w733XffpXr/kydPWknZ4EcIIYQQQiQX6uN2FoGrxUs9YgsGDhxoAmfRokWhY7788ktXunRpEzY4RpyDg8ZxwM+EOSKipkyZYtu++eYbd/HFF7s1a9a4mjVrRr03Th/HzZ07N2pxksj7AI24EZYIzMWLF7tGjRq5AwcOmFMGCE3cQl+cZNq0aTanHTt2mIsFCDbcN+6DwOS+nHfw4MFMCWnEcUP4Moa0xN3MmTNNIKeW47Z69WqbI8KtUKFCoe04mjiR3s2LhGvh4EVS+rHZCpUUOYJCJYUQQojs7+OmHLezjOrVq4d+JgRvxYoV5kpFsm/fPhNuULly5dD2/Pnzu+LFi7trrrkmtI3wSUBweEaPHu0mTpxoAonqjQioqlWrxhxb8D6AGPTXRIwhKL1og1q1aoUdz3z27t1rbleQEydO2Hw8jD0z89CoXBnJrFmz3Msvv2z3pa3A6dOn0/yjZPwcy/oGYf2C44+kZ8+erlu3bmH/IWCthBBCCCFE8iDhdpZBKKIHkdCkSRM3dOjQFMchmjwFCxYM24ebFdzm3S3CGQFnqXv37m7EiBEmrhBSw4cPtxDHWES7j79mPDAfhOn06dNT7CNnLNoaZAaR18N5bNmypblghG7yrymsCeuR1vhZ9w8//DDFPlzD1MCdCzp0QgghhBAi+ZBwO4shf2revHkW7legQOY9anLLyH/r1KlTaFssxygeKlasaIVHyAfzonLt2rUp5oPTVbJkyUyznBPho48+cmXLlnW9e/cObSPEMwiOH6GhkeMnnJRnwTMRQgghhBAiXiTczmI6d+5sRTNatGgRquZIqCHu0Pjx4y0sMhGuuOIKy38jd47KklOnTnUbNmywnxOlbt26FrrZpk0bc+8IBwwKI8DlYh+VJClsQoEUBNNbb71l8+P37ID5EyLKOl5//fXu/fffT5EDhzD74osvLMeNceFKMkccSvL1KM7CfL/++ms7/+67744akhmLbQPq56iAFUIIIYQQ2YeqSp7FkC+GO4bzQ+EOcr8o+09YXr58iT/6Dh06WPESKiXWqFHDHTt2LMx9SwTGg/gh34uiJZTMp6R/EErvr1q1ypUpU8buj0tHWwJy3LJTwPzlL39xjz/+uBVkIa8PB85Xm/Tcc889Vu3ytttuszDOGTNmWGjoBx98YC0PaLWAcLvvvvtMfPo8QiGEEEIIIaKhqpJC5DGyokqREEIIIYTI3e9rctyEEEIIIYQQIpejHDdxVkPfOHrBRYOwzN/97ncxq0AKIYQQQgiRG5BwEyEim2YnCrlcvml2TkPBDwqE1K5d2+ZHblm8wk0IIYQQQojcgoSbCDFy5Eh3tqU8IszKly9vPeQoEsLPZwuV+i1y+QoVyelhiCRi/5BGOT0EIYQQImmRcMtl/Prrr9YDLCcggTLZ10AIIYQQQojciIqT5DB16tSxsvKU6S9RooSrX7++27Ztm+VlnXfeeVYmvlWrVu7o0aNh53Tp0sXOKVasmB1Dv7aff/7ZQgHpGYaztGDBgtA5tASgdD691nCh/vSnP5nDFoRQwmB4I/fp2rVrqAdcqVKlXP/+/cPO2bNnj5W3L1y4sLvqqqvckiVLUsyRxtrNmjWzNgRchz5s+/fvT3Ffyv/TwoCxZYQjR464Jk2a2DyZ7/Tp06OGc7722muucePG1maA1gJr1qyxPnfMu2jRotZkPLKx+DvvvGONtJlvuXLl3IABA9zp06dD+1944QVru8D5pUuXtjYJwVy5N954w9aBHnjck2dM2wAajwshhBBCCJEaEm65gMmTJ5vDRM+1IUOGuNtvv91de+21buPGjW7hwoXu8OHDJnwiz0HorV+/3kRcx44dXdOmTU1sbN682fq2IfiOHz9ux//222/WCHrOnDlu+/btrm/fvq5Xr15u9uzZaY4NEbJu3TprGk3jay/OuCb91Bg7+8eOHet69OgRdv6pU6dMjCImKRTCHL1YwVnzLFu2zO3atcuu/d5772VoPRGCiMUVK1a4uXPnujFjxpiYi+S5555zrVu3thy4ChUquPvvv9961PXs2dPWnrBRRLWH8XP8o48+amuI8EOIBfvN0Y/u5Zdfdp999pmt3fLly034BuGZPP/889a4nL50NPPu3r17qvM5efKklZQNfoQQQgghRHKhPm45DO4OL+KILRg4cKAJBBwZz5dffmnuDcKGps2cg4PGccDPhDkioqZMmWLbvvnmG3fxxRebi1SzZs2o90aUcBziJlpxksj7AM2xEZYIzMWLF7tGjRpZA2mcMkBo4hb64iTTpk2zOe3YscNcLkCw4TpxHwQm9+U8BExGQyR3795tjh2C9vrrr7dtO3fuNHfrxRdfNJcSGEufPn1MvMHatWtdrVq13IQJE9yDDz5o22bOnGkOJkVMoG7duu6OO+4wYedhfgizr7/+Oup4WNuHH3445Jgi9Lgmzt7ll19u2xCWCGKeRTRwOXH2Iin92GzluIlsRTluQgghRM71cVOOWy6gevXqoZ+3bNliThGuVCSE7SHcoHLlyqHt+fPnd8WLF7cQPQ/hkxB0mkaPHu0mTpxoAgkxgoCqWrVqzLEF7wOIQX9NxBiC0os2QPwEYT6IFBy3ICdOnAgLQ2TsmZHXxpgKFCgQtqa4aQjFWHPz6xW5hoyTPzz+4JgLjmHQYUPYcgwuGiGXS5cudYMHDzaxyHmEUQb3A//vRVvkmkYDoditW7fQ71yXdRdCCCGEEMmDhFsugFBED/lQ5GcNHTo0xXG84HuokhgEBym4zbtbhDN694hwvBEjRpi4QkgNHz7cQhxjEe0+/prxwHwQUdHyzKjyGG0Nsoto6xVrDZkLzhfOZiTkvJG3R84cYauIO/L5Vq9ebbmFiGQv3KKtaSzju1ChQvYRQgghhBDJi4RbLoPCF/PmzXOXXXaZOUeZBU4R+W8Uy/BEFt5IL4QfkktGYQ0vKgk5jJzPrFmzXMmSJTPNJo4F7hou16ZNm0KhkoSYEgKaUZgL10qtpQD3ROQhjsl1g7RyCDPCtgH1s2VNhRBCCCFEzqPiJLmMzp07u++++861aNHCbdiwwcQV+W7kRRGWlyhXXHGFFdzgWuSBPfPMM3b9jEDOF6Gbbdq0sTBCcuF69+4ddkzLli2tiAqVJNn/xRdfuA8//NCqVZK7l9mQ30bhE4qM4CYiptq1a5cpjbYp6EIOIa4bxUcIy8TJJFcOEHQUY3nllVfc559/bsVHKNgihBBCCCFERpFwy2WQL4Y7hkijcAc5VxTUIEfLuziJgJAhxK958+auRo0a7tixY2HuWyIwHoqQkC9H0RIEUjD/CwgPpHJimTJl7P64dIQOkveVVW7RpEmTbB1r165t93zooYfM8csoVMek4iVFWXDzKPpCwZOyZcva/ipVqlg7AMJcK1WqZOGh5LsJIYQQQgiRUVRVUog8RlZUKRJCCCGEELn7fU2OmxBCCCGEEELkciTckhT6iUUrkZ8bIBeOdgjRPrQ+SG1ftBYKOQ096uhnJ4QQQgghREZQVUmR67juuuvcJ598EnUf+XRpFRpBlJIXmBmVJIUQQgghhMgNSLiJhKGCYmRPsswAYRat5D4FW+h5lpEiLWcTlfotcvkK/b/ecEKkh/1DGuX0EIQQQgiRTvQGnA3Q24vqgn/84x9NlFB9cO7cubaP0viIkWXLlpnTRBVG+q3RLyzIP//5T6tkSKNnyuvffffdoX3ff/+9a926tStWrJidf+edd7o9e/akcKGo7Mh+zqWqZCTvvPOO9SrjHuXKlbOy9/RE8zDOV1991f3lL3+xhtmRFSTTg5/3+++/7ypXrmz3pErjtm3bUoRzvvvuu+6qq66yJtQHDx6MOV+uS+sEEkG5Pp/+/fvHvU6pwTWqVq0atu2ll16yfntBYdmtWzcbc/Hixd1TTz2VorF2nTp13COPPGIfElZ5lrRmUI0gIYQQQggRCwm3bADRRv8venrR/+vxxx93f/vb39zKlStDx9D/jMbN9Fqj8faDDz4Y2oe4QWw1bNjQffzxxybyKL8fzKPiPATOmjVrTARwLI4Y0M+MEvyIBUIQb7vtNjdw4MAUeWWImkcffdRt377dvfbaayacIsUZAoaxfPrpp2FjTJQnn3zS5k1PuQsvvNA1adIkNG44fvy4ldcfP368rR1l/WPNF9GLoKJ6D43B+XTv3j2udcoozIM1mzhxolu9erX146NdQiSTJ0+2Z7x+/Xo3cuRIayHA/IQQQgghhEgNtQPIYk6ePOkuuOACt3TpUlerVq3QdnqeIUroMYaQYv8dd9xh+z744APXqFEjy+fCiUKM4IBNmzYtxfVxjGiCTe83jgPctNKlS5tAaNq0qbv//vvNgUIAeu677z63cOHCUB4YzbS5f8+ePUPHcD9co6+//tp+x70id4zeZRkFZ4x508Ca3nKA0Ln00ktN/DRr1sz+H/cMsYlLGe98o+W4xXNeLBCs8+fPD8u9QyDy2b9/v/1O7zhEOWIUcCtxWatXr27nesftyJEjJkJZT3j66adNTCKYU/sO8QmWl2XcpR+brVBJkRAKlRRCCCGyFrUDyIPs3bvXBFq9evXCqh/iwO3bty90HOGCnosvvtj+nxd8QCx4URfJjh07zL2hqbaHML0//elPts8fE9wPQREJW7Zscc8++2zYGNu3b2+OFeP3EM6ZmQTHgcANjhvOPffcsLWJZ77RSPS8eOGPkrUKXp/7RVsvQkK9aPNrgLAk1DI1x5Y/fP9BtAkhhBBCiORCxUmymJ9++sn+H7frkksuCdtHzpYXb8EiH/6lntw4SKuKYmaNk5y2v/71ryn24fp5yG3LTph7UOTkFBREiTSnMyvEMi1wQcmdi3TchBBCCCFE8iDHLYsJFtWgUmLwE+/LN44TeW3RqFixooXkkcfmIQSQ4ibc2x8T3A9r164N+52iJJwTOUY+WVnFMTgOiofs3r3bxpsa8cwXly7SvYrnvFiQf/fNN9+Eibdg2CROGE5p8Prcb9OmTSmuFe1ZXHHFFdajLhp8f7DYgx8hhBBCCJFcyHHLYs4//3wrjkHuEw7azTffbGF15FrxAl62bNk0r9GvXz8Llbz88sstNw1BQB5cjx497IX/v/7rvyyskYIi3I+cKdw9tkPXrl3dTTfd5J5//nnbtmjRIstvC9K3b1/XuHFjqzx57733mlgjfJIqj5GFTDITwjMJWbzooousQAtVFmM1rI5nvlR6xEFE7JIbRwXJeM6LBblp3377rRs2bJitD+u3YMGCMBFFYZchQ4bYvSpUqGBFR6L1kkPE46B16NDBbd682b3yyitW2CS9bBtQXyJOCCGEECJJkOOWDTz33HNW8p1cJZyfBg0aWOgkhSviAdEwZ84cK2BBSfrbb7/dKhJ6Jk2aZAUwEF7kS+EKIex8+CU5Va+//rpVMETILF682PXp0yfsHvXr13fvvfee7aPtAOdQhCQeYZkREDoIHsaPo0XbAxyzWKQ1X4qPPPzww1b0BKcMsRXPebHguY0ZM8aNHj3a1pD199UqPU888YRr1aqVa9OmjV0fcRhs2+CheieFZ6gM2rlzZ5s/RWqEEEIIIYRIDVWVFDmCrypJeCR9z5IFRDjim2qUualKkRBCCCGEyDxUVVIIIYQQQgghkhAJN5EwhCMG2wcEP2nty23ceeedqY530KBBOT08IYQQQgiR5ChUMg/xwAMPWLEL38w5USiv//bbb8csAhIP9JnDBo4GlnCsfSVLlnS5KSTxq6++sryzaNBfjk9uCfNUqKQQQgghRO4mK97XVFUyD0FxkdyksxFfsQRYIuIsJ0RRRvPO3njjDffYY49FrSAphBBCCCFEZiDhlk5+/fXXNKseZhWo9mRfA/H/U6nfIpevUJGcHobI5ewf0iinhyCEEEKITEA5bnG4MY888og5KvQYo2w+vc18ThT9xygBf/To0bBzunTpYucUK1bMjqEc/88//+zatm1rZeJpbE0fMA8No//+979bi4Df/e537k9/+pM5bJGhksHwRu5Dj7annnrKQvlKlSrl+vfvH3bOnj173K233uoKFy5sjaaXLFmSYo7/8z//45o1a2YOF9ehr9n+/ftT3Pcf//iH+8Mf/mBjywgnT560HnQ0IKe5NGsxYcIEuyduG7BuhHRy77RgXSmxz/OgCXa0nmiU8qe/GuvA86AXm5/bypUrba25H5/g3ONxCHmm2OD+fP8M6CdHDzw/Nlor0NKBfnCsMdtorr5x48Z0rJ4QQgghhEhGJNziYPLkyeYw0TSbvmP0Ubv22mvthZtGzIcPHzbhE3kOQo9+X4i4jh07uqZNm1qPMZou//nPfzbBd/z4cTue5tyXXnqp9Wvbvn27NcTu1auXmz17dppjK1q0qFu3bp31K6OhtRdnXPOvf/2rjZ39Y8eONcEU5NSpUyZGEZP/+te/bI4ICnrN4ax5aGa9a9cuuzb93jICQmbGjBnu5Zdfdjt27LCG2NwTITdv3jw7hnsdOnQohXiNxpNPPmni65133rE+dIgp1tjDc0LgsjZcl2eGmAWuT881GnNzPz6MI154noRYErvszw/2d6MXHs3PP/74Y9eoUSN75sz/b3/7m42Rpur8HisEFqFLnHTwI4QQQgghkguFSsYBTo1v4oyDgmgLVhqcOHGivezv3r3bXXnllbaNJs2+yXXPnj1N8CHkEAiAMHv11Vfd1q1brdk1TaAHDBgQuibO25o1a0y4RYrCIDg2/fr1C41z1KhRJrLq1avnli5d6nbu3OkWLVpkThkwbtxCz6xZs0zgjR8/3twi36ga9w0BhMAExCHHZDREkjViTgjAunXr2rZy5cqF9vsiIOTHxZPj9tNPP5lbN23aNHfHHXeExCwi2HPw4EEbP423Eag4XzxDH37KnIoUKWKOZXrhXK7B2kU7v2HDhq5Dhw5hz5wG54h4QEgjHBH/qd2fxu3B74YQQgghhEg+5LjFQfXq1UM/b9myxa1YsSKsXHyFChVs3759+8IElSd//vyuePHi7pprrgltI1zPV2b0jB492u514YUX2nXHjRtnoiMWwfsAoYL+mrhZCEov2gCREIT57N271wSNnw/i6cSJE2HzYeyZkdf2ySef2HrUrl3bZQaMEWewRo0aoW2MPxjOiYhFrCEQcbymT58ecjqzmuDz8c88re9BJAh/QjH9h9BWIYQQQgiRXMhxiwPcmqDD06RJEzd06NAUxyGaPDhoQXBkgtu8u4XbBTNnzrQQO/KzEFcIqeHDh1uIYyyi3cdfMx6YD2IRMRMJAjLaGmQE8veyG9aSsEQcREIpcb7IQ9uwYUOWV66M9sxjfQ+iQR4gHyGEEEIIkbxIuKWTatWqWR4WhScKFMi85SO3jHypTp06hbYFHa9EqFixorkz5F15Ubl27doU8yFcktDE7OgJhtuESCEnzYdKBvGuHsVa4oEcMYQQArdMmTK2jVYChGQGXT2eFffjQ2gpgm358uWhHMB47xeNjJ4vhBBCCCFEWki4pZPOnTtbhcgWLVqEqjkSaohjRg4YYYCJQH7alClTLB+N/LapU6eaI8TPiYJIIeeuTZs25t5R1KJ3795hx7Rs2dL2UeWQ4h3khh04cMC99dZbNr9grlhmgOBlPA8++KAVJyEXkPsRKkguHyGNuFAUQCE/DIeO8M3UYB/VOClQQjgqApQ55sv3/0cBc63PP//cCpJQrfKDDz4w8ejDKRkTwo9qkj5UNHh+PHPCuSS3kPmQL8cnq9k2oL4acAshhBBCJAnKcUsn5IvhjuGwULgDB4my/zg46XnZj4QCFrg/zZs3t3ytY8eOhblvicB43n77bffLL7+4G264wbVr185K+gdBYKxatcrcKu6PS4cQIsctq0QBBToox8/8yA+kYAsl/eGSSy6xQhxPP/205X/RiiEtEJ633HKLhbAiVm+++eawvESeDUKUaqDMj+qaVLW8+uqrbT8hqghu2iUQHppWXmEkOKUPP/ywPTvO94VshBBCCCGEyCzOOROrDrkQIteBc0olSwqVyHETQgghhEiO9zU5bkIIIYQQQgiRy5FwE+mGRt3BdgjBDyGHqe2LlauWGoQtxrpeesMa44Gwx9Tuxz4hhBBCCCGyG4VKijAeeOAB98MPP7j58+enegw5c1999VWq+ygoQrGVMWPGWA+1IOXLl0/XeE6fPm1FQ1IjPdU9OZZ8RD5AERRyAO+6666w4yiUgr0dDaxuCqDkJAqVFEIIIYTI3WTF+5qqSoowRo4c6dLS8gizeAQYLQjSK9QiQZRl9BrpBWEWrziLFINCCCGEEEJkBRJuuZBff/011M8su+FfBpJ9DfIKlfotcvkKZX3bAZE32T+kUU4PQQghhBCZiHLccgF16tSxsve4NiVKlHD169d327Ztc3feeaflVVEWv1WrVu7o0aNh53Tp0sXOoTcZx9BfjrL6bdu2deeff745VQsWLAidQwsDSv3TGw7XjD5mOGyRoZLB0EHu07Vr11DPulKlSrn+/fuHnbNnzx7rkVa4cGErqb9kyZIUc6QROH3aKM3PdegbFwyB9PelXQEtF3yPtUQh3JH2AMyT+U6fPj3qcTQnZ505rly5cm7u3LmhfbQPiGxH8O2335qgpGcba0MPuscff9zCLvl4Vq9ebS0KuG7p0qVtDX3LAyCMlHBS1oxnR3sEIYQQQgghUkPCLZcwefJkEwT0iBsyZIiJhmuvvdZt3LjRLVy40B0+fNiET+Q5CL3169ebiOvYsaNr2rSp9RXbvHmz9ZlD8B0/ftyOp+k0DbXnzJnjtm/f7vr27et69erlZs+enebYihYtak2q6VFGo24vzrgm/d8YO/vpkdajR4+w80+dOmViFDFJYRPmiCBt0KCBOWsexNCuXbvs2jTNzggIQcTiihUrTIwhlBBzkTzzzDPunnvucVu2bLFm5Pfdd5/bsWOH7aPv3ZtvvulOnjwZOn7atGnWa47nQ2841pP1QADygX379tncuO7WrVvdrFmzTMh5EcgzRchxHvPl+SJ8U4P7Eycd/AghhBBCiORCxUlyATg3vIwjtmDgwIEmcBYtWhQ65ssvvzTnhhf9K6+80s7BQeM44GfCHBFRU6ZMsW3ffPON5ZmtWbPG1axZM+q9ERMc552myOIkkfcBmnkjXBCYixcvdo0aNTLnCacMECK4WL7wB2KHOSGIvCuFYMN94z4ITO7LeVSJzGiI5O7du82xQ9Bef/31tm3nzp3WfPvFF18MK05ClUgagntYp2rVqpnQowk5c0KMetFcpUoVW+N+/fqlmuOG4KO65muvvRbahnCrXbu2uW4ffPCBuaI8U8RsWuBw0pQ8ktKPzVaopEgVhUoKIYQQOYf6uJ3FVK9ePfQz7g9OUbAMfYUKFUJujqdy5cqhnxEKxYsXd9dcc01oGyF4EHSaRo8ebfe68MIL7brjxo1Ls6R+8D6AGPTXRIwhKL1og1q1aoUdz3z27t1rIsXPh3BJhFFwPow9M/LaGBNFTYJryvohFCOJHCu/e8eNMEYcy4kTJ9rvCGtCWBGZsWC+b7zxRtjzw3HEnfziiy+s0mbZsmUtNJPrE8bpXdFo9OzZ0/7o/QcnUQghhBBCJBcqTpJLIBTR89NPP1l+1tChQ1Mch2jyFCxYMGwfDlJwm3e3EAwwc+ZM1717dzdixAgTKAip4cOHW4hjLKLdx18zHpgPIipanhkCMtoa5BZwz6pWrWru2KRJk8xpRHSlNd8OHTpYOGQkZcqUMXGKCPzwww/NsSRkFVdtw4YNUcVloUKF7COEEEIIIZIXCbdcCKF68+bNS1ePsnggt4z8t06dOoW2BR2vRCD8EAeI/C4vKteuXZtiPuR5UWI/O/qO4a7R/23Tpk2hUElCTAkBjYSxtm7dOux3cguDLuB1111nhV/Idxs1alTY+YgwQkkj50sOYaw2BjzXunXr2oewSwTb8uXLLQxTCCGEEEKISCTcciGdO3c2odCiRYtQNUdCDXHMxo8fb2GRiUAVQ/LfyJ2j0uLUqVPN5eHnREF4kHPXpk0bc++I5+3du3fYMRT9YB+VJCnIQUEPcuIo7sH8+D0zIb+N4iC4XuSvIZLIQaPCYyQUakGY3XzzzeYIkhc3YcKEFK4buYA4gnfffXfYPsT1qlWrrKgJrhjFYijOQq4c53Au5yHkKLqC8KPwyueff24FSagISs4bDmZ6K2luG1BfDbiFEEIIIZIE5bjlQsgXwx3DyaFwB64PwgNXJl++xB8ZQgZHp3nz5q5GjRru2LFjYe5bIjAeipD88ssvVrQEoUJJ/yBFihQxcUOYIPfHpaMtATluWSU8CGtkHSkIwj0feuihqE21KfqBICaPD1E7Y8YMa2kQBAGN+OP/yXsLghClrcHll18eCvvkWitXrrQiKbQEwMEjHNLnAfIcEa2EXbIWFD/hvldffXWWrIUQQgghhMj7qKqkEGnghRnuJGGQZ2OVIiGEEEIIkbvf1xQqKUQq0H8OV7JPnz6hNgFCCCGEEELkBBJuIldC3zh6wUWDsMxo+WrBqo6ZAeGqt912m+Xw+T53QgghhBBC5AQSbiLbiGzuHQsKhnzyySepFlmh2An90LISmo/HG0kcrRG3EEIIIYQQmYWEm8g2Ro4cGbcQwlGLVU6f1gOx9icDlfotcvkKFcnpYYgcZP+QRjk9BCGEEEJkExJuScavv/5qvcdyAhI0k30NUgNBSxXRzOzbJ4QQQgghzh7UDuAsh3A/+okRwkePsfr167tt27ZZ/th5553nLrroIteqVSt39OjRsHO6dOli59BnjGPoK/fzzz+7tm3buvPPP9/crgULFoTOQXRQ4p+ecLhl9CTDYYsMlbzrrrvC7tO1a9dQr7pSpUq5/v37h52zZ88e63dGGX7K9NMLLRIagDdr1szK7HMd+sVRCTLyvrQpoCR/evulRXLkyBHXpEkTmyfzpf9bEO59zjnnhIV6EiLKtg8//NB+5//5nTWsXr269YBbvXp1hsYlhBBCCCHOXiTckoDJkyebw0SxjSFDhlj/MHqLbdy40S1cuNAdPnzYhE/kOQg9GlIj4jp27OiaNm3qbrzxRrd582brL4fgO378uB1PA2kaadPQmmbT9C3r1auXmz17dppjo0H1unXr3LBhw6wvmhdnXJMebIyd/fQ7o7l1ZOVHxChikoImzBFBSgNunDXPsmXL3K5du+zaNMDOCAhBxOKKFSusaMmYMWNMzCXC008/bc9kx44d1v8tGidPnrSSssGPEEIIIYRILhSXlQRQzANRBAMHDjTRNmjQoND+iRMnutKlS1vDaCooQpUqVawMPvTs2dPEBUKuffv2tg1h9uqrr7qtW7daqfyCBQtaM2sPTtSaNWtMuEWKwiCIlX79+oXGOWrUKBNZFB5ZunSp27lzp1u0aFGoeTXjDlabnDVrlgm88ePHm4Plm2/jvuFqITABccgxGQ2RZI1wyRC0119/vW2bMGGCNdJOBIRqWkVWBg8eHLa2QgghhBAi+ZBwSwIIxfNs2bLFnCJcqUj27dsXEm5B9yd//vyuePHi7pprrgltI3wSgk7T6NGjTQQePHjQSvbjeFWtWjXm2CJdJoqO+GviQiEovWiDWrVqhR3PfPbu3WuOW5ATJ07YfDyMPTPy2hgTeWjBNa1QoYIJxUSgemZaIJy7desW+h3HjXURQgghhBDJg4RbEoDbFOxxRn7W0KFDUxyHaPLgoAXBzQpu8+4WbhfMnDnTde/e3Y0YMcLEFUKKkv2EOMYi2n38NeOB+SCiIvPM4MILL4y6BllNvnz/LwI5WEGTkM5oxDMu8t/4CCGEEEKI5EXCLcmoVq2amzdvnvUdy8wKhuSWkf/WqVOn0Lag45UIhB+SS3bo0KGQqFy7dm2K+RAuWbJkSff73//eZTW4a6dPn3abNm0KhUqSO0fxkUjByLgJS4XUetJlhG0D6mfLnIUQQgghRM6j4iRJRufOnd13333nWrRo4TZs2GDiihwyqkVSGTJRyE+j2AnXIg/smWeesetnhLp161roZps2bSwkkuIjvXv3DjumZcuWlntHJUn2f/HFF5bbRrXKL7/80mU2VKSk8EmHDh3MTUTAtWvXzipMeviZvD9fdGTlypWhfEEhhBBCCCESQcItySBfDHcMkUbhDnK/KPtPjpYP8UsEhAwVIJs3b+5q1Kjhjh07Fua+JQLjefvtty1f7oYbbjCBREn/IEWKFHGrVq1yZcqUsfvj0tGWgBy3rHKjKH7COtauXdvu+dBDD5njF4RcP5w5wjhZX4rCCCGEEEIIkSjnnAkm4gghcj0UJ6GZ+b///W+FSgohhBBCJMn7mhw3IYQQQgghhMjlSLiJUFPpu+66K8PXoSrk/PnzXW6GXDjaIUT70PogtX3RWiikl/79+6fZIkEIIYQQQohIVFVSGCNHjgwrX382Q++01Ko8kk8XLDQihBBCCCFEbkDCLRdBw+rMaBKdCMTgJssaIMzKly/v8jqV+i1y+QoVyelhiAywf0ijnB6CEEIIIfIICpXMQerUqeMeeeQRqzpISfv69eu7bdu2uTvvvNPC8i666CLXqlUrd/To0bBzunTpYucUK1bMjnn99dfdzz//bCX9aXyNKFmwYEHoHCpIUmnxj3/8o4kWStrjsMUKleQ+lNR/6qmn3AUXXOBKlSplYX5B9uzZ42699VZXuHBhd9VVV7klS5akmCN92Jo1a2ZVK7kOZfv379+f4r5Ui6RSI2PLCGPGjLHWBIyJtbn33ntt+5QpU1zx4sXdyZMnw47n3qxxMIxx6tSp1ucOMXvfffe5H3/8MXT8woUL3c0332zz4XqNGzdO0a+ONgS0W2C+NNjG4UutETnnlitXzr4HyeJ4CiGEEEKI9CPhlsNMnjzZHCZK9NP36/bbb7emzfREQyQcPnzYhE/kOQi99evXm4jr2LGja9q0qTXA3rx5s5X5R4wcP37cjv/tt9/cpZde6ubMmeO2b9/u+vbt63r16uVmz56d5tgQHoiOYcOGuWeffTYkzrgmpfAZO/vHjh3revToEXb+qVOnTIwiJskrY44IUvqg4ax5li1bZk2sufZ7772X8FqyZohNxsn1WD+EJbA+CNh33303dPyRI0fc+++/7x588MEwIUWOHuPgQw82nosHgdytWze7F+OmZcHdd99t6wE//fSTtQn46quv7F70n0P8+v1Btm7daiLw/vvvd6NGjbL8QCGEEEIIIaKhUMkcBncIUQT0+kK0DRo0KKwfWOnSpa2pNc2ooUqVKqGGzj179jRhgZBr3769bUOYvfrqqyYMaARdsGBBN2DAgNA1cd7WrFljwi1SFAapXLmy69evX2iciAvESr169dzSpUvdzp07reE2ThkwbtxCz6xZs0ywjB8/PiRK6IGGW0WTbAQmIA45JqMhkgcPHrRr4YIhFsuWLWvrCTiNCCTuj4iDadOmWf833EUP433jjTfsfEAAM2ffP+6ee+4JuyfP58ILLzRBXKlSJffmm2+6b7/91pqP47hBtLDMjz76yMZJQ/Ennngi5rxwCYNOIeVlhRBCCCFEciHHLYehQbMHd2bFihVhVQwrVKhg+4LheAgqD1UQCdmjkbaHEEHvKHlGjx5t90JkcN1x48aZ0IlF8D5w8cUXh665Y8cOE5RetEGtWrXCjmc+e/fuNRHk54OYoTl2cD6MPTPy2hCUiDVCDxFc06dPD7mOgLBdvHixuWGAQCNUM+h0ESLpRVvknH14KGGQ3IOeHBwPfi0peoJY9KItGhzLWBHYaYk2GDx4sIVt+g/rLoQQQgghkgs5bjkMDpGHMLsmTZq4oUOHpjgOAeHBQQuC8Ahu80LEh+fNnDnTde/e3Y0YMcLEFcJk+PDhqeZdxbpPtJC/1GA+iEUEVCQIyGhrkBGYF6GiuHkINIQReWu4X7h8CCrcSvLdcPs+++wzC5VMz5x5PohD8goRrezDafOhn/FUpGTunDtjxgwL00yrKSOuKuGZQcdN4k0IIYQQIrmQcMtFVKtWzc2bN89cnAIFMu/RkFtG/lunTp1C2yILaqSXihUrWuGRQ4cOhUTl2rVrU8yHcMmSJUtmWsf4tGDd6tatax/CPBFsy5cvt3w8aNeunXvppZfMdeOY9AigY8eOWe4cou2WW26xbatXr07hUhL2+d1336XquiHuyJ9r2LCh5QAiMoMuXySFChWyjxBCCCGESF4k3HIRnTt3NlFAKJ6v5kioIY4ZYoCwyEQgPw2XiXw08tuomogLxc+Jgugh565Nmzbm3uECka8VpGXLlraPSpIUDKFAyoEDB9xbb71l8+P3zAQx9Pnnn1tBEipufvDBB+aIBStVkueG+8g6sybpgWsSlkqYKWKVkMenn3467BieHbl+VKskxJHjPv74Y3PYgqGkuIy4feQE8qGQSnobfG8bUD/bBLEQQgghhMhZlOOWi+DlHneM6oeE8pH7Rdl/XCOqFyZKhw4dzHFq3ry5q1GjhjlHQfctERjP22+/bQ2rb7jhBnOyfAEPT5EiRdyqVausAAj3x6WjLQE5blkhOFgnRCGVObkXlS4JR7z66qtDx5AjRoERRFKw/UG8c0ZEb9q0ycIjH3/8cROmQcjVw0HDZcRR4xlSPCaa6GYMtG2gDUCjRo2sYqUQQgghhBDROOeMmkeJJOOOO+4wMffyyy+7vAjuJgL03//+txw3IYQQQogkeV9TqKRIGr7//nsrXMKHRt1CCCGEEELkFSTcRK6CRt3BXnBBCMuMVbWRKpaxoKok4o2qncG8NyGEEEIIIXI7Em55EHqP/fDDD27+/PkZug6l7slTS2+uV1Zy3XXXWS+0RIRbWuzfv99lFrQZYP1TG6sQQgghhBCZiYRbHmTkyJFW0OJsBGFWvnx5C2e87bbbzCGj6EiyiGkhhBBCCCGiIeGWIDRcpoJgTkCiY7KvQTKP3VOp3yKXr1CRnB6GSJD9Qxrl9BCEEEIIkYdQO4A4qVOnjnvkkUesPH+JEiWscfK2bdssH4uy7hdddJFr1aqVO3r0aNg5Xbp0sXPoAcYx9A+j7Hvbtm2t6TLuEiXhPbQCoGQ+PdZwn8jFwmGLdHeC4Y3cp2vXrqHeb6VKlbJQviB79uyx/maFCxd2V111lVuyZEmKOdJQu1mzZuZwcR36rwXDC/19KftP64KM5omdPHnS9ejRw5pg02CatZgwYYLdE7cNWDdCOrl3WsydO9fK77Nu9Fuj15wvsZ/a2L/88kvrvcZ86a1GqOa6devSNQ/WevLkye6dd96xsfLBMWQe/Dx79mxr2M24rr/+erd7927ro8e9+O7wHfr2228TWkMhhBBCCJEcSLilA17OcWnotUZvLvqFUfBi48aN1kD58OHDJnwiz0HorV+/3kRcx44dXdOmTd2NN97oNm/ebP3aEHzHjx+342kYTWPqOXPmuO3bt7u+ffu6Xr162ct/WmNDeCA6hg0bZg2vvTjjmvRRY+zsp78ZginIqVOnTIwiJikQwhwRFQ0aNDB3yrNs2TK3a9cuuzYNrzNC69atrc8aZfl37NjhXnvtNbsnQm7evHl2DPc6dOhQCvEaCccgwB588EG7FsKJOQdDSiPHTjGT2rVru6+++sq9++67bsuWLSZ+Wa/0QENvnjtrxTj48Hw9/fr1c3369LHnXaBAAWsCzn2YE2tNk3WecyyBS0nZ4EcIIYQQQiQXCpVMB1dccYWJIhg4cKCJtkGDBoX2T5w40UQHjsqVV15p26pUqWIv7dCzZ08TfAi59u3b2zZe2F999VW3detWV7NmTVewYEE3YMCA0DVx3tasWWPCLVIUBqlcubIJBD/OUaNGmVCpV6+eW7p0qdu5c6dbtGiRuU3AuIPVG2fNmmWCZfz48eYSwaRJk8x9QwQhMAFxyDEZDTNkjZgTIgpnDMqVKxfajwMGNLKOJ8cNsXT69GkTa2XLlrVtuG9BIsc+btw4c7pwv/z9cP3SC2ITNw2BhdsZTdghiuHRRx81gcmzuemmm2wbDusbb7yR6vUHDx4c9p0QQgghhBDJhxy3dFC9evXQz7gzK1assJd2/6lQoYLt27dvX5ig8uTPn99C+IKCgvBJOHLkSGjb6NGj7V4XXnihXReBcfDgwZhjC94HLr744tA1caAQlF60Qa1atcKOZz44Pzhufj6ImRMnToTNh7FnRm4Y1RhZDxyvzACBTGNtxoejSUgqhU2CRI6dMSC+vWjLKoLPxj/vyO9A8PlHguCneaP/ENIqhBBCCCGSCzlu6QDHxkOYXZMmTawnWCSIJg8OWhDcrOA272758LyZM2eaQzNixAgTVwip4cOHp5l3Fe0+6Qn5Yz6IxenTp6fYh4CMtgYZISNl/aOBCMS9++ijj9zixYvdK6+84nr37m3rhmsZbeyZPYbUiPa8I7fFelbk//ERQgghhBDJixy3BKlWrZr77LPP3GWXXWbhdcFPRsQNuWXkR3Xq1MncIK4XdLwSoWLFiubSEE7oWbt2bYr5UMCE0MTI+WRFFUscJ8TKypUro+73zhjFWuIFAUT4IWGFH3/8sV2DPnWxnDBct++++y6BGaQcb3rGKoQQQgghRHqQ45YgnTt3tnA88pV8NUdCDXHMyKPCAUoE8tOmTJli+Wg4RVOnTrUcLO8aJQI5ZOTctWnTxtw7ilvgRgVp2bKl7aOSJIVNKJBy4MAB99Zbb9n8+D0zQfAyHoqJUJyEUEfuR8gguXzkqSHEKCLSsGFDc8cI30wNnDXyxsjFQ3zyO/lriNbU4NmR60e1SfLIcEoRfISURoaSxjMfnhnFTwiHzY6WDdsG1He///3vs/w+QgghhBAi55HjliC83OOO4bIgFnCQKPtPIY18+RJf1g4dOliBjebNm7saNWq4Y8eOmfuWERgPztMvv/zibrjhBteuXTsrix+kSJEibtWqVa5MmTJ2fwQPRTPIccsqcUBRlnvvvdfmR34gBVt8+f5LLrnEnLOnn37acsBoxRALxsj4EXmIVArCEG4aLMASzSUjrBKhx3k8Q4rHJCK6GTstBijxT2gp3w0hhBBCCCEyi3POBOulCyFyPTimOHoUKpHjJoQQQgiRHO9rctyEEEIIIYQQIpcj4SYShubRwXYIwQ/hhqnti5Wrlhq0Q4h1vbTaJaSXq6++OtV7Rau8KYQQQgghRFai4iQiXTzwwAPuhx9+cPPnz7d8LqoyRoN8uljl9ik8Qt4dhUHizSlM7V5+f2bywQcfuFOnTkXd53uxCSGEEEIIkV1IuIl0MXLkSOfTIhFmtAvIDgoUKJCp96IKJMVk+ESDqpaZxf79+60qKBUrq1atGlUECyGEEEIIEQsJtzzIr7/+Gupzlt1kR5n73L4G6R1nVlGp3yKXr1CRLLu+yDz2D2mU00MQQgghRB5HOW55gDp16lg5fNyhEiVKuPr167tt27ZZqXtyrgjda9WqlTt69GjYOV26dLFzihUrZsfQd45y+23btnXnn3++OVgLFiwInUNrA1oA4A7hplHeHoctCC5RMLyR+3Tt2jXUy65UqVKuf//+YefQ2PvWW291hQsXdldddZVbsmRJijnSIJz+bbRT4Dr0k8OpirwvbQwIi2RsGVlPesY9/vjjFrLJx0MZf/bTHoF1Y62///77sOfABwHLs3jmmWdCDqR38p577jnXunVrqyD00EMPhXrw0VCde3Ed1mjy5MnunXfeCY3hww8/THhOQgghhBDi7EbCLY/ASz4OE8KCXmO33367CYGNGze6hQsXusOHD5vwiTwHcbF+/XoTcR07dnRNmzZ1N954o9u8ebP1n0PwHT9+3I7/7bffrNH2nDlz3Pbt213fvn1dr1693OzZs9McW9GiRa3p9bBhw6yBtxdnXJO+cIyd/WPHjnU9evQIO59cMgQSYpKCJ8wRQdqgQYMwx4oG2zS45to05k4UmoozT8Z56NAh+wA5dHfccYeJyzVr1rjVq1e7Jk2amKANzpWwTdYUUfvCCy9Yw/Ugzz//vDUUJzQSYcexsHTpUrsX9+/evbs9L+box8BzicbJkyetpGzwI4QQQgghkguFSuYRrrjiChNFMHDgQBNtgwYNCu2fOHGiK126tNu9e7c1oAbEA42ooWfPnib4EHI0iwaEGU2wt27d6mrWrOkKFixoTa89OEUIGIRbpCgMUrlyZdevX7/QOEeNGmUiq169eiZWdu7c6RYtWhQqIMK4g42xZ82aZQIPAeTdr0mTJpn7hguFwATEIcdkNEQSR4+qlwhFHEIP60vBlTFjxoRVlwzCGr/44os2Tly/Tz/91H73awqI6ieeeCL0u2/oXbx48bD74WoiyoLbojF48OCw5yKEEEIIIZIPOW55hOrVq4d+3rJli1uxYkVYifoKFSrYvn379oUJqqB4QDhcc801KaojHjlyJLRt9OjRdq8LL7zQrjtu3Lg0S+0H7wMXX3xx6Jo7duwwsROs+lirVq2w45nP3r17TUj5+SCuTpw4ETYfxp6VeW3ecYsFAjcYWslcCAUNunKIv8wE0U3zRv8hrFQIIYQQQiQXctzyCLhNnp9++slC+IYOHZriOESTBwctCIIjuM0LENwumDlzpoXwjRgxwgQJQmr48OEW4hiLaPfx14wH5oNYjNYfDQEZbQ2ygljtC9JDZo+zUKFC9hFCCCGEEMmLhFsepFq1am7evHlWCIN8q8yC3DLyrDp16hTaFnS8EqFixYrmEJHD5UXl2rVrU8yHcMmSJUtaQY/sAOcu6JJ555AQz1hhiZEilrkQHurDIVO7F0TeL9oYhBBCCCGEiIaEWx6kc+fOViGyRYsWoWqOhBrimJEDFktExAIBMmXKFMtHI79t6tSpbsOGDaGqiIlQt25dy7lr06aNuXcU1ujdu3fYMS1btrR9VJKkYAiFQ6j6SBEP5sfvmQ2id9WqVe6+++4zN4vcP0ISCcdEuD788MMmrAhJpaAL+4Gw0W7durkOHTpYgZdXXnnFHMpYIEhx8ygiw1yorklVSsbAWlNwhTBWtkW6l7HYNqB+tgldIYQQQgiRsyjHLQ9CvhjuGG4NhTsQG5T9p5hHvnyJP1LECBUgmzdv7mrUqOGOHTsW5r4lAuN5++233S+//OJuuOEG165dOyvpH4TS+4ioMmXK2P1x6WhLQI5bVgkTBCLtBi6//PJQOCYCc/HixZZzx1gJF6Vcf9DVpMy/nwsC+tFHH7WS/7Hg/Jdfftm99tpr9uwQqEBBEwqckBPHGHimQgghhBBCROOcM8EmVEKIVKH/WtWqVd1LL72Uo+PAtcSdo1CJHDchhBBCiNxHVryvyXETQgghhBBCiFyOctxEnoRG3cFecEEIZYxVIZIqlkIIIYQQQuQlJNxEmjzwwAPuhx9+cPPnz8/QdWgTQL7bXXfdleExkRdG37VEhFuiYZA0A08PHH/bbbe577//3vIPhRBCCCGESBQJN5EmI0eOdLktFRJhVr58+Uy7HhUs01PRMTOgOAoVOz/++GMTjemlUr9FLl+hIlkyNpE4+4c0yukhCCGEEOIsRMItj/Drr7+G+oFlNyRWnu1rQEsFIYQQQgghcisqTpJLIXTvkUcesTL/9BCrX7++27Ztm+V1nXfeee6iiy5yrVq1ckePHg07p0uXLnZOsWLF7Bj6vf3888+ubdu27vzzzzeXasGCBaFzaClA6X2cH1wsytPjsEWGSgbDG7lP165dQz3kSpUq5fr37x92zp49e9ytt95qPcuuuuoqt2TJkhRzpDF3s2bNLIyQ61AmHxcq8r60D6CMPmPLCGPGjLFedYyJtbn33nvD5sS6eeixNmjQIPfggw/autGqYNy4caH9jJPQT3rn0bSca1aqVMmtXLky1fsfP37cnt9NN91koae+P961115r12IMQgghhBBCREPCLRczefJkc5jo7zVkyBB3++2320v+xo0brZnz4cOHTfhEnoPQW79+vYm4jh07WgNpxAUNo+n7huBDRMBvv/1mTaHnzJnjtm/f7vr27et69erlZs+enebYihYt6tatW+eGDRtmfdG8OOOa9GNj7OwfO3as69GjR9j5p06dMjGKKKLQCHNEkDZo0MCcNc+yZcusQTXXfu+99xJeS9YMsck4uR7rh7CMBY21yaUjlJF+dqwl5wZ58skn3RNPPGHH0PetSZMm1v8uEoRavXr1bG2YC2KVZwRLly51hw4dsnDNaJw8edJKygY/QgghhBAiuZBwy8XgDiGKcJp42Ue04QJVqFDBfp44caJbsWKF2717d+icKlWquD59+ti5PXv2NCcIIUezZ7YhzBAWW7dutePJ6xowYIAJFBygli1bmjuXlnCrXLmy69evn12TptScj8jyQmTnzp1uypQpNh4EEuMOMmvWLBMx48ePtwbiNN2eNGmSO3jwYFgREMQhx1x99dX2SRSuy7UaN27sypYta+uHkItFw4YNTbDhUiI8WUfWOwiu6D333GPjf/XVVy2sdMKECWHHfPPNN6527dru4osvdv/85z+t4Tj4xt/Fixc31zK1cM3Bgwfbdf2ndOnSCa+DEEIIIYTIm0i45WKqV68e+nnLli0mGnCl/AcBB/v27QsTVJ78+fObKEAYeQgRhCNHjoS2jR492u6FkOC6hAQidGIRvA8gSvw1d+zYYeKC8EYPblQQ5rN3715z3Px8EC4nTpwImw9jz4y8NtwuBFu5cuXMcZw+fXrIdYxnjoQyIq6C6xY5rwIFCpiAZf6R90b8IVYTmQsCnOaN/kOIqRBCCCGESC5UnCQXg0MU7D1GGN7QoUNTHIdo8kRWRkRwBLfxO+B2ATla3bt3t7BARAhCavjw4RbiGIto9/HXjAfmg1hEQEXinajINcgIzItQUdy8xYsXm/NIXt6GDRtSLdWf0Tl6GjVq5ObNm2ehqEERHS+FChWyjxBCCCGESF4k3PII1apVs5d/imbg7GQW5JaR/0ZIoCfoeCUCYYO4QuRteVG5du3aFPPBgSpZsqT7/e9/77ID1q1u3br2IcwTwbZ8+XLLx0sU5uVz5U6fPu02bdpk4ZNByE/EUbzjjjtMOFKsBbz7RoGYRNg2oH62rZ0QQgghhMhZFCqZR+jcubP77rvvXIsWLcwlQlwtWrTI8tESffEHctQo3MG1yJV75pln7PoZAWF05ZVXujZt2lhIJMVHevfuHXYMuXTkjFFJkv1ffPGFiRryzr788kuX2VDY5OWXX7am3QcOHLD8O9yzjFaqJMyUpuLk9PGMaLZNJcpInn/+eZszBWY4FhCtVPL0hWYIgxRCCCGEECIaEm55BPLFcMcQaVSGJOSO8vW4RvnyJf4YO3ToYI5T8+bNXY0aNaxwSdB9SwTGg5j55Zdf3A033ODatWtnJf2DUKBj1apVVmaf++PS0ZaAHLescJFYJ6o2Ipy4F5UuZ8yYkaGCJ95N40MRltWrV7t3333XBGk0XnzxRasCyhgQyTiAiMnXXnvNni8iVgghhBBCiGicc+bMmTNR9wghUoU+blThpA1A1apVs/XetAOguiQOnUIlhRBCCCFyH1nxvibHTQghhBBCCCFyORJuIsQDDzzg7rrrrgxfh+qL8+fPd5kNuXDBdgjBD60PUtvHJ6vXgQqV2e28CSGEEEKI5EFVJUWIkSNHutwcOUuPNIqLRIN8Ogp9ZBdU98zoWiFwyQXMDLEshBBCCCHObiTcchm//vprpjScTgTicHPzGiDMaGQt/h+V+i1y+QoVyelhJD37hzTK6SEIIYQQIglQqGQOU6dOHev7RYVIqhHWr1/fbdu2zd15550W4nfRRRe5Vq1auaNHj4ad06VLFzunWLFidszrr7/ufv75Z2sPQLNpBM6CBQtC51CNkqqNFNRAAFEGH4ctVogg96E8/1NPPeUuuOACV6pUKQsJDLJnzx7rY1a4cGHrT7ZkyZIUc6SnG9UUqezIdaieSHGPyPtSeZLqihkt0T916lRz51gHxnz//fe7I0eOhB3z2WefucaNG1uyKMfdcsstqfavoz0CTcGDzc+pJMm6c66vhhl5Tr169eyZIohr165tDcCDjh3cfffd5rz534UQQgghhIiGhFsuYPLkyeYwUe4fQUC5+Guvvdb6q/keXwifyHMQBevXrzcR17FjR9e0aVNrpo1AoGUAgu/48eN2PD3LLr30Ujdnzhy3fft217dvX9erVy83e/bsNMdWtGhRt27dOjds2DD37LPPhsQZ16SUP2NnPyX2e/ToEXb+qVOnTIwicMhRY44I0gYNGpiz5lm2bJnbtWuXXZueaxmBez733HPWQ45cO0Qi4tDz1VdfmdgsVKiQNeCmaTa912igHQn7EWCISj831gwBO2jQIHtGNBkfM2ZM2Hk//vij9bGjRQBNuumX17BhQ9sOvlfepEmTrFF5RnvnCSGEEEKIsxu1A8hhcLUoF+rdmIEDB5rAoSG2h4bUpUuXNmFDY2vOwUHjOOBnXB1EFI2l4ZtvvjFBsWbNGlezZs2o98bp47i5c+fa74ibH374IVRYJPI+QF82hCUCc/Hixa5Ro0bW0BqnDBCauIU+d2vatGk2px07dpizBAg23Dfug8Dkvpx38ODBLAkTRVxdf/31JpoQjQjWmTNn2noWLFgwxfF+HRBerVu3duPHj7c+dx7EMcKa5tse1hjXLbUcPEQuc37zzTfN6UtPjtvJkyft4+H7wveh9GOzFSqZC1CopBBCCCEiUTuAs5Tq1auHfsYlWrFiRVhFxAoVKti+YChf5cqVQz9TUbF48eLWlNtDGB8EQwQRGtyLsD+uO27cOBNLsQjeBxCD/pqIMQSEF21Qq1atsOOZz969e81x8/MhXBKRE5wPY88s0YaD1qRJE2vuzX0JUwQ/V8QVoZHRRJsHBxEHk7DLoGjz86ZZeZDIeeOStm/f3pw2/mj5g/3pp5/SXO9oDB482K7hP6y5EEIIIYRILlScJBdAKKKHl3tERzCfKiiaPJGiA/cmuM27Wzg9gMPUvXt3N2LECBMZCJrhw4ebQIlFtPv4a8YD80EsTp8+PcU+BGS0NcgI5PkRmsmHe3IPxBK/+9DMeKpPXn755SaGJ06caK5iLJEXDdy6Y8eOWR5h2bJlLSyTdQ+Gh8ZLz549Xbdu3VI4bkIIIYQQInmQcMtlVKtWzc2bN8+KVRQokHmPh9wyQvw6deoU2pZaMY54qVixohUeIUfLi0ryuSLnM2vWLFeyZMlMs4ljsXPnThNMhHJ6cUOoZKSLSO4euXCpCTLyB9966y0LFyW/kLw2fyzzRvASRumJnDfrTd4beW3AOgULzADXIxQ1LRB9fIQQQgghRPIi4ZbL6Ny5s1WIbNGiRaiaI6GGOGbkWhEWmQiE7JH/Ru4clSUJAaQgBj8nSt26dS3nDncJ9w4nqHfv3mHHtGzZ0vZRSZLCJhRIIScOUcT8+D0zITySkMtXXnnFPfzww1ahk0Ilkbl97L/vvvvMzSL8EOFF/l6woiVik+Ikt912mz0PngFi+tFHH7U8OCpX3nTTTebsUaWyXLlyYevtq1uyLk8++WQKpw9xTlEWroEwo0Joetg2oH62iGEhhBBCCJHzKMctl0G+GG4NTgyFO8j9ouw/hS3y5Uv8cXXo0MGKl5CvRX4WrlTQfUsExkNxDZpfI3ratWtn1ReDFClSxK1atcoEFffHrfLl87NCdBAa+cYbb1j1TNoT4Lw9//zzYccQAokgI4yT/DdCORHL0dw32glw7KeffmoilOfCGj7zzDMmPDkXIUpVzyATJkxw33//vTmOVPekrQJCMAhhq1TRxBmk2IkQQgghhBCpoaqSQuQxsqJKkRBCCCGEyDxUVVIIIYQQQgghkhDluIlcB33j6AUXDcIyY1WFJPxRCCGEEEKIsw0JN5EqkQ25EyXeRtMeCnqk1sg6LeEWLxQGIXeQTyJjjIf+/fvb2qU2FyGEEEIIIeJFwk2kCj3IciIFEmFWvnz5bL0nLQ3SW9Uxo1BEBeGIOBZCCCGEECIWEm65HBo2U94+JyChMlnWgOqReY1K/Ra5fIWK5PQwzhr2D2mU00MQQgghhEgVFSfJZdDwmT5jODE0ga5fv771IiPn67zzznMXXXSRlZcPNnPmnC5dutg5uEYcQ3n7n3/+2bVt29adf/755mAtWLAgdA5l7SnLTx83HC76l+GwRYZKBkMHuQ9l7X1/OcQO4YBB9uzZ42699VZXuHBhK8dPuftIaEZNU2taHHAderzt378/xX1pLUB7hGBvtUQ4cuSIa9Kkic2T+dJ3LRJCJX1IKGPhd3rN0cONlgZVqlRxa9asCXPLGD/n0LON+fKsmFtq0PCcXm883xUrVtizodIQ9+ITuZZCCCGEEEJ4JNxyIZMnTzaHiX5u9CG7/fbbrc/Xxo0b3cKFC93hw4dN+ESeg9Bbv369iTj6ijVt2tTdeOONbvPmzdYTDsF3/PhxO/63336z5tf0O9u+fbvr27ev69Wrl5s9e3aaYytatKhbt26dGzZsmDXV9uKMa9KrjbGzf+zYsa5Hjx5h5586dcoEDmKSIiTMEUHaoEEDc9Y8NKbetWuXXfu9997L0HoiBBFUiKW5c+e6MWPGmJhLC5qJd+/e3XLUaDROE+7Tp0+H9rOWiEsamzMPQh5p6h2NrVu3uptvvtndf//9btSoUdZ0+6WXXrLysIRp8uFe0Th58qSVlA1+hBBCCCFEcqFQyVwIDg6iCAYOHGiibdCgQaH9EydOtKbNu3fvNkEBOEJ9+vSxn3v27GmCDyHXvn1724Ywe/XVV01A1KxZ05pNDxgwIHRNnCgcJYRbpCgMUrlyZdevX7/QOBEhiKx69eq5pUuXup07d7pFixaZUwaMO1ghctasWSbwxo8fby4TTJo0ydyrDz/80AQmIA45JqMhkqwRTiOC9vrrrw81x6YReFogpBo1+n/hc6zV1Vdf7fbu3esqVKgQEqHMn4bmXtRyXe5FQ3LPRx995Bo3bmxC8IknnrBtzItQVNYgrTDNwYMHhz0rIYQQQgiRfMhxy4VUr1499POWLVvMKcKV8h8vHAi9CwoqT/78+V3x4sXdNddcE9pG+CQEnabRo0fbvS688EK77rhx49zBgwdjji14H7j44otD19yxY4cJSi/aoFatWmHHMx/ED46bnw/hkidOnAibD2PPjLw2xlSgQIGwNWX9EIppEZwr84xcP67rxWDwutzTw3oiahHOXrSlF4Q4IZX+EyscUwghhBBCnJ3IccuF4DYF+5KRnzV06NAUx3kxAThoQXBygtu8u4XbBTNnzjRHacSIESauEFLDhw+3EMdYRLuPv2Y8MB9EVLQ8MwRktDXIKWKtX7wwJ4TsjBkz3IMPPmihkemlUKFC9hFCCCGEEMmLhFsup1q1am7evHnWdwyHJ7MgJ4v8t06dOoW2BR2vRCBMEDeIfC0vKteuXZtiPoRLlixZMiERk15wwchL27RpU8gdI3cuM0rwc13yDn1YpL9uMAyTgijk6DVs2NBy+xYvXmwiGXAUKRIjhBBCCCFEWki45XI6d+5sFSIpjOGrORJqiGNGDhhhkYlAfhpFNchHI79t6tSpbsOGDfZzotStW9dy7tq0aWPuHUU0yOsK0rJlS9tHJUkKm1Ag5cCBA1bBkfnxe2ZCRUoKn3To0MFy/BC/VN/MjCbeOHIUgnn55ZftulSLJH8wmN/m3cP333/fcv34UGCGEFHEOA4kOYLkKFK9kk+8bBtQP1vErxBCCCGEyHmU45bLIcwOdwxnhsId5H4hPMilypcv8ceHkKECZPPmza24xrFjx8Lct0RgPG+//bb75ZdfTLy0a9fOqi4GQZisWrXKlSlTxu6PO0VbAnLcskqEUPyEdaxdu7bd86GHHjLHL6MwF6pmUimSKpGIMdzEaLCPIik0NKfgCa0acDwffvhhewaEVPqCNEIIIYQQQkRyzhneJIUQ6YI+bgjozAi5TC84mVSkpFCJHDchhBBCiNxHVryvyXETQgghhBBCiFyOctxErodG3cFecEEIy4yVr0YOmRBCCCGEEHkdhUqKbOOBBx6w0ML58+en6zzE2VdffRVWWGXMmDHWHy0t4Va+fHmXHVBohNBJPumFVgPkBt51111xHa9QSSGEEEKI3E1WvK/JcRPZxsiRI604R3pBmEUKMNoNZJcoS4RoYqx///4mWj/55JMcHZsQQgghhMh7SLglGb/++qv1D8sJ+FeHZF+DzKRSv0UuX6H42weI2Owf0iinhyCEEEIIkSoqTnKWU6dOHesvRghfiRIlrAn0tm3bLGeMEvUXXXSRa9WqlTt69GjYOfQn45xixYrZMfSSo4R927ZtrYE0bhfl7T20K6CsP33gcMjon4bDFhkqGXSguE/Xrl1D/elKlSplrlSQPXv2uFtvvdUVLlzYXXXVVW7JkiUp5kjT72bNmlmLBK5Dj7j9+/enuC+tCWgLwNgywpEjR1yTJk1snsx3+vTpKcIm4e677zbnjd+pQjlgwAC3ZcsW28aHbUIIIYQQQsSDhFsSMHnyZHOY6Ac3ZMgQd/vtt7trr73Wbdy40ZpBHz582IRP5DkIvfXr15uI69ixo2vatKn1Htu8ebP1lEPwHT9+3I7/7bffrHn2nDlz3Pbt213fvn1dr1693OzZs9McGw2q161bZ33MaMrtxRnXpO8aY2f/2LFjrW9akFOnTpkYRUxSxIQ5Ikhpuo2z5qHJ9a5du+za7733XobWEyGIWFyxYoWbO3eu5dsh5jw0Mvf94w4dOmS/06vtiSeecFdffbVt48O2eDh58qTFSQc/QgghhBAiuVCoZBJAMQ/f3HngwIEm2gYNGhTaP3HiRFe6dGm3e/dud+WVV9q2KlWquD59+tjPPXv2NMGHkGvfvr1tQ5i9+uqrbuvWra5mzZquYMGC5ih5cKLWrFljwi1SFAapXLmy69evX2ico0aNMpFF4ZGlS5e6nTt3ukWLFplTBow7WGGShtcIvPHjx5uL5QUT7tuHH35oAhMQhxyT0RBJ1ginEUF7/fXX27YJEyZYI3EPzbSBMeAiehCUBQoUCNsWD4MHDw5bWyGEEEIIkXxIuCUB1atXD/1MqB5OESIikn379oWEG4LKkz9/fle8eHF3zTXXhLYRPglBp2n06NEmAg8ePGjVHnG8qlatGnNswfv4oiP+mjt27DBB6UUb1KpVK+x45rN3715z3IKcOHHC5uNh7JmR18aYEF/BNa1QoYKJtKwC4dytW7fQ7zhurIsQQgghhEgeJNySANymYF8z8rOGDh2a4jhEkwcHLQhuVnCbd7dwu2DmzJmue/fubsSIESauEFLDhw+3EMdYRLuPv2Y8MB9EVGSeWdD5ilyDvEahQoXsI4QQQgghkhcJtySjWrVqbt68eVYwA+cosyC3jPy3Tp06hbYFHa9EIPyQXDLywbyoXLt2bYr5EC5ZsmTJbOlphrt2+vRpt2nTplCoJLlz9KeLFKQUbAmC4xe5TQghhBBCiHiQcEsyOnfubBUiW7RoEarmSKghjhk5YIRFJgL5aVOmTLF8NPLbpk6dakU5+DlR6tata6Gbbdq0MfeOEMHevXuHHdOyZUvbRyVJCptQIOXAgQPurbfesvnxe2ZCRUoKn3To0MFy/BC/VN+MbAKOMCZX76abbjK3jOqcbPviiy+sjxvjwpXMiJO2bUB9NeAWQgghhEgSVFUyySBfDHcM54fCHeR+ITzI0cqXL/GvA0KGCpBUSqxRo4Y7duxYmPuWCIyHJtbky91www2uXbt2VtI/SJEiRdyqVatcmTJl7P64dLQlIMctq0QNxU9Yx9q1a9s9H3roIXP8ghAySgVLctEoBgP33HOPib7bbrvNwjhnzJiRJeMTQgghhBBnH+ecOXPmTE4PQggRPziPNDP/97//LcdNCCGEECJJ3tfkuAkhhBBCCCFELkc5biLpoFF3sBdcEMIyI/PVIqtYCiGEEEIIkd1IuIkc4YEHHrBKjPPnz8/QdWgfQB7cXXfdFfc51113nRUISUS4ZWSOderUsb52L730UqZcXwghhBBCJA/KcRM5AvG+fPUy2rg6EeGWU3P87rvvrE2AbxZOlUkKw/BJJGa69GOzXb5CRbJk7MnE/iGNcnoIQgghhDjL+N8syHGT45bE/Prrr9ZbLCfgi3y2r0HkHGm9IIQQQgghRCKoOEkSQajeI488Yg5PiRIlXP369d22bdss3+u8885zF110kWvVqpU7evRo2DldunSxc+hFxjH0gfv5559d27ZtzT0qX768W7BgQegcWg1Qkp8eboQd0vts5MiRKcIIgy4Z9+natWuot1ypUqVc//79w87Zs2ePu/XWW13hwoXdVVddZeX2I6Fhd7Nmzczl4jr0d9u/f3+K+9JWgJL+jC0jjBkzxnrYMSbW5t577405R++u8TP95h5//HFzDfkIIYQQQgiRGhJuScbkyZPNYaKX25AhQ9ztt99ufcY2btzoFi5c6A4fPmzCJ/IchN769etNxHXs2NE1bdrU3XjjjW7z5s3WDw7Bd/z4cTv+t99+swbTc+bMcdu3b3d9+/Z1vXr1crNnz05zbEWLFnXr1q1zw4YNs4baXpxxTXqmMXb2jx071vXo0SPs/FOnTpkYRUxSgIQ5IkjpnYaz5qEx9q5du+za7733XsJryZohNhkn12P9EJbxQINw1ohzDx06ZJ/UOHnypNntwY8QQgghhEguFCqZZOAOIYpg4MCBJtoGDRoU2j9x4kRrGr1792535ZVX2rYqVaq4Pn362M89e/Y0wYeQa9++vW1DmL366qtu69atrmbNmpbHNWDAgNA1cd7WrFljwi1SFAapXLmy69evX2ico0aNMpFVr149t3TpUrdz5063aNEic8qAcQerQ86aNcsE3vjx40MOFs2ycd8+/PBDE5iAOOSYjIZIHjx40K7VuHFjE4tly5YNNdtOC9zA/Pnz23m4i7EYPHhw2HoKIYQQQojkQ45bklG9evXQz1u2bHErVqwwV8p/KlSoYPv27dsXJqg8iI3ixYu7a665JrSNEEE4cuRIaNvo0aPtXhdeeKFdd9y4cSZ0YhG8D1x88cWha+7YscMEpRdtUKtWrbDjmc/evXtNDPn5IJBOnDgRNh/Gnhl5bQhKxFq5cuXMcZw+fXrIdcxMEMsktvoP4aBCCCGEECK5kOOWZOAQBXuSNWnSxA0dOjTFcYgmDw5aENys4DbvbuF2wcyZM1337t3diBEjTFwhpIYPH24hjrGIdh9/zXhgPohFBFQkCMhoa5ARmBehorh5ixcvNueRvLwNGzZkuFpmkEKFCtlHCCGEEEIkLxJuSUy1atXcvHnzrCx9gQKZ91Ugt4z8t06dOoW2BR2vRKhYsaI5TeSCeVG5du3aFPMhXLJkyZKZVnY1LVi3unXr2ocwTwTb8uXLLR8vLXD9KOSSKNsG1M+2eQohhBBCiJxFoZJJTOfOna23WIsWLcwlQlyRQ0a1yIwICvLTKNzBtciVe+aZZ+z6GQFhRM5dmzZtLCSS4iO9e/cOO6Zly5aWe0clSfZ/8cUX5oZRQOTLL790mQ2FTV5++WVr5k2FyClTpphDGG+lSgTzqlWr3FdffRVWyVMIIYQQQohIJNySGPLFcMcQaRTuIPeLcvW4RvnyJf7V6NChgzlOzZs3dzVq1HDHjh0Lc98SgfHQaPuXX35xN9xwg2vXrp2V9A9SpEgRE0Jlyvx/7L0JuJXz+v//ORVNhi9p2Id0TkWSJqWBCEmUDJEklVSUkmRokHaJJnUqpE6pVIZGIWkQCU1KAw00EOl0UIaDBmH9r9f9u57n/6y111577b3Xntrv13U9p72e8fP5rHWu63m77/t9n23PJ0pHWwJq3LIiMsU64Q6JMyfPwunylVdecVWqVInrehwlaVVQoUKFsFROIYQQQgghIvlbKBQKpdgrhMg0RDIxc3nxxRcTel/aAdDcG6MSpUoKIYQQQuQ+suJ9TRE3IRLMH3/8Yf3raIEQb/RNCCGEEEKIWEi4iWznzjvvdDfeeGOm74Pr5GuvvZape1ALF2yHENyIlqV2jC21ujXs+2vXrm2i7bnnnnNjxoxJ6JiFEEIIIUT+Q66SItsZO3asyy0ZuggszEWiQT1d0aJF033PM8880+/n9v3334e1H8AV87TTTrO/qW+jOfnGjRtdjRo1MjwHIYQQQghx/CPhlk/5/fffE9KEOiOQ75tb1gBhVrFixSx7RqTpSJkyZRJ27wuSl7gChYsl7H75jT3DmuX0EIQQQggh4kapkvmEyy+/3HXv3t1cI7HMb9KkiduyZYu79tprLe2vdOnSrm3btmG29Fxz33332TVEiThn0qRJ7rfffrOWATSgRvQsWrTIvwaHSpwciSQhirDGJ8IWK1WS52DZ/8gjj7jTTz/dxA2NrIPs3LnTXXbZZa5IkSLu/PPPd2+//XaKOdLn7dZbbzW3R+5DWwCiWpHPxY0SR814bftT47vvvrMG5syT+UZr/E3qZGqpklwDNWvWtP2sgxBCCCGEENGQcMtHTJs2zaJstAAYNmyY2dgjGui5tnjxYvftt9+a8Im8BqH30UcfmYjr2rWra9mypTXY3rBhg7URQPB5qYH0MTvrrLPcnDlzzKBjwIABrl+/fm727Nlpjo2UwrVr17oRI0aYVb4nzrgn9v6MnePY7vfu3Tvs+mPHjpkYRUxSt8YcEaTXXHONRdY83nnnHff555/bvenDlhkQgojF5cuXu7lz51o9G2IuXlhTWLZsmaVQ0lpACCGEEEKIaChVMh9BY2xEETzxxBMm2oYMGeIfnzJliitbtqw1zabZNVSvXt3179/f/sZ0A8GHkOvcubPtQ5iNHz/effLJJ65evXruhBNOcIMGDfLvSVQJd0WEW6QoDFKtWjWXnJzsj/PZZ581kdW4cWMTNp999pk19CZSBoybaKHHrFmzTOA9//zzFr2CqVOnWvSNJtwITEAcck5m00RZIyKNiK+LLrrI9k2ePNn6uaU3jbJEiRIxUyiPHj1qW9BeVgghhBBC5C8k3PIRtWrV8v/evHmzRYqiuSPu3r3bF24IKg9cFhEZNOr2IH0SgpGmcePGmQj8+uuvzeCDiFda5hvB50BSUpJ/z+3bt5ug9EQb1K9fP+x85rNr1y6LuAWh+Tbz8WDsiajtY0yFChUKW9PzzjvPhGKiGTp0aJgYFkIIIYQQ+Q8Jt3xE0N3w119/tfqs4cOHpzgP0eRBBC0I0azgPi+6RbQLZs6c6R566CE3atQoE1cIqaeeespSHGMR7TnePeOB+SCiotWZBQ1CgmuQVyDS2atXr7CIG0JWCCGEEELkHyTc8ikXXnihmzdvnplnEDlKFNSWUf927733+vuCEa+MQPohtWTUgXmics2aNSnmQ7pkqVKlEtadPhZE12i0/fHHH/upktTO/fTTT3Hfw4v8YegSi8KFC9smhBBCCCHyLxJu+ZRu3bqZQ2Tr1q19N0dSDYmYUQNGWmRGoD5t+vTpVo9GfduMGTPcunXrfAfFjHDVVVdZ6mb79u0tekfE6dFHHw07p02bNnYMJ0mMTTBI+eqrr8zwg/nxOZHgSInxyT333GM1fohf3DfT0/cNkcn5GMMwPhwz09MqYcugJtkiUoUQQgghRM4jV8l8CvViRMeI9mDcQe0XwoMarQIFMv6zQMjgANmqVStXt25dd/DgwbDoW0ZgPPPnz7d6uTp16rhOnTqZpX+QYsWKuffff9+dffbZ9nyidLQloMYtq8QN5iesY8OGDe2Zd999t4mxeEHsPf300+7f//633QfRKYQQQgghRDT+FgqFQlGPCCEyDamdgwcPNrGZKIg4Epn7+eefFXETQgghhMiFZMX7mlIlhcgC6GtHRJPeeFWqVMnp4QghhBBCiDyOhJvIt9CoO9gLLghpmbHq1XCxjMXEiRMt0kb6aWTrAiGEEEIIIdKLUiVFhrnzzjvNRfG1117L1H2w/qeG7cYbb3TZCeJs3759UY9ddtllrkOHDrZ5pivPPfecNQSHihUrupxCqZJCCCGEELkbpUqKXMXYsWNdXtb9RNRSE2BY9dP/LXicerWcFGxCCCGEECL/IuGWx/n999/9fmDZTXqs64/XNchJLkhe4goULpbTw8h17BnWLKeHIIQQQgiRcNQOII9x+eWXu+7du1vt1BlnnOGaNGnitmzZYrVaJ510kitdurRr27atO3DgQNg19913n11z2mmn2Tn0cPvtt98sFfDkk0+2SNKiRYv8a2gTgJ0+/deITNG3jAhbZKpkML2R5/To0cPvC1emTBk3cODAsGt27txpaYj0LDv//PPd22+/nWKONNu+9dZbrTUB98Emf8+ePSmeS0sAbPQZW2b47rvvXPPmzW2ezPell15K85pPP/3UXXnllXZNiRIlrBVAsO7tvffes9YFxYsXt3lccskl1lcONm/e7K644gpbd0LntWrVcuvXr8/UHIQQQgghxPGNhFseZNq0aRZhwrVw2LBhJiBq1qxpL/80c8bJEOETeQ1C76OPPjIR17VrV9eyZUt38cUXuw0bNlgvNwQfbojw119/WVPoOXPmuG3btrkBAwa4fv36udmzZ6c5NsTK2rVr3YgRI6wZtifOuCf9zhg7xydMmOB69+4ddv2xY8dMjCJqMA9hjghSml0TWfN455133Oeff273fvPNNzO1nghBxOLy5cvd3LlzrZYNMZcaCF7GiAimuThrtGzZMhPU8Mcff5iwpL/bJ5984lavXm3Cjlo+r1k4a8u1H3/8sevTp4874YQTUn3e0aNHLU86uAkhhBBCiPyFUiXzIBhlIIrgiSeeMNE2ZMgQ//iUKVNc2bJl3Y4dO9y5555r+6pXr+769+9vf/ft29cEH0Kuc+fOtg9hNn78eBMa9erVMyExaNAg/55EohAgCLdIURikWrVqLjk52R/ns88+ayILUw/EzWeffeaWLFlikTJg3EFnx1mzZpnAe/75532hQ6NrolZEsRCYgDjknMymSLJGRBoRtBdddJHtmzx5sjXwTo2XX37ZGntPnz7dxgHMk6jd8OHDbe0oRL3uuutchQoV7Hjwfl9//bV7+OGH3XnnneevUyyGDh0a9l0IIYQQQoj8hyJueRBS6zxIuyNSRFTK2zxBsHv37jBB5VGwYEFL76tataq/j/RJCEaaxo0bZ8/CpIP7YnGP6IhF8DmeoYd3z+3bt5ug9EQbRFrlM59du3ZZxM2bD+mSCKXgfBh7IuraGFOhQoXC1pT1QyjGugYh7Ik2IBUSwUkUkPESxSMqh5gjxXT//v3+ub169bKG3FdddZUJ6OC8ooHQRgh6G9FBIYQQQgiRv5Bwy4MEBQN1VYiDTZs2hW1eLZlHZCoe0azgPi+6hfiAmTNnuoceesjq3JYuXWr3pB4umK4YjWjP8e4ZD8wHERU5HyJjt99+e9Q1yI0QJSRCSSoqUUQin2vWrLFj1P1t3brVNWvWzL377rtW60c7hNQoXLiw1cIFNyGEEEIIkb9QqmQe58ILL3Tz5s1z//jHPyxylCioLUN03Hvvvf6+tCJDaUG6INEiok9E4sATM8H5IHRKlSqVLQKF6Bo1adSaeamSRM3oTxdrHi+88ILVunkCkvUqUKBAmFEKKaxsRMyILJJiSRoqIOTYHnjgAde6dWsTejfddFOWz1cIIYQQQuRNJNzyON26dTOHSF7+PTdHUg2JmFEDRlpkRqDuihou6tGob5sxY4aZafB3RiE1ELHSvn1799RTT5nJxqOPPhp2DsYdHMNJEmMTTDxwY3z11VdtfnxOJAgtjE/uueceq/FD/OK+iVtkajBG6viYB9Gz77//3gxfMHch5fTLL7+0tNLrr7/e0kIRgkRA27VrZ02/qW+75ZZbbC2/+eYbW9ebb7453WPfMqiJom9CCCGEEPkEpUrmcRAGRHuw78e4g9ovhAc1WkSAMgpCBgfIVq1aubp167qDBw+GRd8yAuMhJRDxglU+dV5Y+gcpVqyYe//9993ZZ59tzye6RbomNW5ZJVKIdrGOuEDyTBwgifilBmNE0P7www8WpUOENWrUyAxKvOOYsCDGEKrcD4HNmiKkWUtEHMcwesGcReYjQgghhBAiFn8LhUKhmGcIIXIVRCppfo5RiSJuQgghhBD5431NETchhBBCCCGEyOWoxk3keWjUHewFF4S0zFj1arhYCiGEEEIIkduRcBMZgj5lOC++9tprmboP7QKoe7vxxhszfI/atWtby4CMCLfMgpsnNYVsQgghhBBCZBUSbiJD0FQ6t5RHIswqVqzociOXX365q1GjhhszZkxOD0UIIYQQQuRhJNzyMDTDPvHEE3Pk2RRb5vc1yGkuSF7iChQu5vILe4Y1y+khCCGEEELkGDInyUMQvenevbul5Z1xxhmuSZMmbsuWLVbfddJJJ1kPMXqJHThwIOwaeoxxzWmnnWbn0PeN5tEdOnRwJ598skWrFi1a5F9DawEs+OkzRjSLXmdE2CJTJYPpjTynR48efi+5MmXKWI+zIPQyu+yyy1yRIkXc+eef795+++0Uc6RBNxb5tDPgPvRz27NnT4rn0kYAC/9gw+uM8Nxzz1nPOsbE2mDtH7nebAhV1vyxxx6LGWmkdx5jf+edd2ysK1assLUjJZSNufz444/WC65kyZK2vjyflgRCCCGEEEKkhoRbHmPatGkWYaJ327Bhw9yVV17patas6davX+8WL17svv32WxM+kdcgOj766CMTcV27dnUtW7Z0F198sduwYYP1f0PwHTp0yM7/66+/rNH1nDlz3LZt29yAAQNcv3793OzZs9McW/Hixd3atWvdiBEjrIG2J864Jz3SGDvHJ0yY4Hr37h12/bFjx0yMIiYxHGGOCFIaZBNZ80AU0dSae7/55psZXkvWDLHJOLkf64ewjJwTTblZOwTYv/71LxNn0WDOffr0cUuXLrW+bpxfv35917lzZ7d//37bypYta+KPdUUsb9++3Rp/8/2kxtGjR81SNrgJIYQQQoj8hVIl8xhEZxAI8MQTT5hoGzJkiH98ypQpJg527NhhDZ6hevXqrn///vZ33759TfAhFBAUgDBDPHzyySeuXr167oQTTghrCE3kbfXq1SbcIkVhkGrVqrnk5GR/nDSkRmQ1btzYLVu2zJpS07iaSBkw7qAb5KxZs0zgIYyITgGRKCJY7733nglMQBxyTmZTJL/++mu713XXXWdisVy5craeQVjL0aNH23iI7n366af22Vs7D0TojBkzLMJWpUoV20eUjjHSkJsIZPC5PAdTFc/gJBZDhw5Vg24hhBBCiHyOIm55jFq1avl/b9682S1fvtyiUt523nnn2bHdu3eHCSqPggULuhIlSriqVav6+0gRhO+++87fN27cOHsW6Xzcd+LEiSY4YhF8DiQlJfn3JLKECPJEGxCNCsJ8du3aZSLKmw/pkkeOHAmbD2NPRF0bghKxVr58eYs4vvTSS37U0QMh64lIb8ykfJJO6jFq1ChLP/3www990RYLIp4zZ8400xJSS1etWhXzfMQ2zRu9jXRSIYQQQgiRv5Bwy2MQIQr2IGvevLlZ4Qc3r5bMgwhaEIRIcJ8nTIh2AaLioYcesjo30v64J/VwwXTFaER7jnfPeGA+iMXI+RA9vP3226OuQWZAIJIq+sorr5jIJPJIdJI2B+nh0ksvNSGXViqpB1HGr776yj3wwAPuP//5j6VVst6pUbhwYXfKKaeEbUIIIYQQIn+hVMk8zIUXXujmzZtnqXbUYSUKasuof7v33nv9fcGIV0aoXLmyRYqo80IkwZo1a1LMh3TJUqVKZZs4Yd2uuuoq20jzJC3z3XfftXo8oB4vCGMmDZTIpUedOnXMwIRaPO4XFGFEBoPROQ8ime3bt7cN4ffwww+7kSNHZulchRBCCCFE3kXCLQ/TrVs3S9Fr3bq17+ZIqiERM2rAguIiPSBMpk+fbvVo1LdRu7Vu3Tr7O6MgjKi5Q6g89dRTZrDx6KOPhp2D0yLHcJLEMASDFCJTr776qs2Pz4kEY5MvvvjCopM4br711lsWIQw6VZIe2qtXL3fPPfdYdO6ZZ56x1MhIELpcTzQN8eY15EZUI/5wk/RSP3HbJLJIWiXGI4wDYZtetgxqouibEEIIIUQ+QamSeRjqxYiOEdHBuIPaLwQDUaMCBTL+1SJSiDi1atXK1a1b1x08eDAs+pYRGM/8+fPd4cOHLULVqVMns/QPgonH+++/784++2x7PmKGdE1q3LJCoLBOiEKcOXkWTpekTQbr1Nq1a+ePGaF8//33u7vvvjvq/Ro0aOAWLlxoRjAIPCD6hoCm/QFRNoQgUTjq1qgJRDRyHLEthBBCCCFEavwtFKsplRD5GPq4YSAyZswYl5sgWoljJUYlirgJIYQQQuQ+suJ9TRE3IYQQQgghhMjlqMZN5Glo1B3sBReEFMeiRYvGdLEUQgghhBAiLyDhJtLFnXfeaXb5r732WqbuQ6sAat5uvPHGTN2HJta0DMiIcIsEIxFqBD1jEZp+xwLDEQxbNm7caCmVQgghhBBCZBUSbiJdjB071uWmskiEWcWKFV1uJVECNRoXJC9xBQoXc8cre4Y1y+khCCGEEELkGiTc8iA0wsaZMCegyDK/r4EQQgghhBDZjcxJ8oi7IQ2eSeE744wzXJMmTdyWLVustoveYKVLl3Zt27Z1Bw4cCLvmvvvus2voUcY59Hz77bffXIcOHdzJJ59skapFixb519BWAPt90v+IZNHPjAhbZKpkMHrEc3r06OH3kStTpoz1KQuyc+dOs70vUqSI2eK//fbbKeZIc+5bb73VLPq5D73cSEWMfC4tBGiDEOy1lhG+++4717x5c5sn833ppZeiRsvGjx9v68x55cuXd3Pnzk31nqzfXXfd5c477zyz/Sf1Em666Sa7l/d58+bN7oorrrDvAJcherqtX78+U/MRQgghhBDHNxJueYRp06ZZhIm+bcOGDbPeYzVr1rQX/sWLF7tvv/3WhE/kNQi9jz76yERc165dXcuWLa1ZNM2k6f2G4Dt06JCdT/NpmlzPmTPHbdu2zQ0YMMD169fPzZ49O82xFS9e3BpNjxgxwppne+KMe9KTjbFznF5pvXv3Drv+2LFjJkYRMpiNMEcE6TXXXGORNY933nnHff7553ZvmlZnBoQgYnH58uUmxp577jkTc5E89thj7uabbzaxRYPw2267zW3fvj3FeTTSZm2pt2MO9KKjaTlMnTrV7d+/3//MfVhnPn/88ceuT58+7oQTTkh1rNwbS9ngJoQQQggh8hdKlcwjnHPOOSaK4IknnjDRNmTIEP/4lClTXNmyZd2OHTvcueeea/uqV69uzaCBhs8IPoRc586dbR/CjIjSJ5984urVq2fiYdCgQf49iUStXr3ahFukKAxCI+nk5GR/nM8++6yJrMaNG7tly5a5zz77zC1ZssQiZcC4g06Qs2bNMoH3/PPPW2TKEztE3zAIQWAC4pBzMpsiyRoRaUTQXnTRRbZv8uTJ1oQ7EsQYzcJh8ODBJhppro3QC7pTNmvWzAQWQtBLJ6XhNjAPIpEeROMefvhhi8x5axaLoUOHhn0vQgghhBAi/6GIWx6BdDoPoj8IBKJS3uaJgN27d4cJKo+CBQu6EiVKuKpVq/r7SJ+EYKRp3Lhx9ixEB/edOHGiCY1YBJ8DSUlJ/j2JTiEoPdEG9evXDzuf+ezatcsibt58SJc8cuRI2HwYeyLq2hhToUKFwtaU9UNgRRI5Vj5HRtxat25tKahLly6NqwawV69eJgavuuoqE9PBOUYD0U3zRm8jUiiEEEIIIfIXirjlEYg2BSM81GcNHz48xXmIJo/I9DuiWcF9XnSLaBfMnDnTPfTQQ27UqFEmUBBSTz31lKU4xiLac7x7xgPzQURFqzPzolaRa5CbaNq0qXvxxRctOkkKa1pQA3j77be7hQsXWuSPaCVrTy1cNAoXLmybEEIIIYTIv0i45UEuvPBCN2/ePDO7IHKUKKgto/7t3nvv9felFQ1KC9IPiRBR4+WJyjVr1qSYD+mSpUqVMrOOrIbo2h9//GH1ZV6qJLVz9KeLhLG2a9cu7DNpqkGoHbzgggvc9ddfb2KsYcOGYaIW05JISGdle+CBByxiR2poasItNbYMapIt6yWEEEIIIXIepUrmQbp16+Z++OEHe+HH4AJxRQ0ZbpHRREK8UGuF2Qn3og4MYw7PUCOjkA6IQGnfvr2lRGLc8eijj4adg1kHtXc4SXL8yy+/tNo23Cq/+eYbl2hwpMT45J577rFoIgKO1MVozboxaqF+kPUgMkZdHA6fkWD+Qu3hdddd5z788EN/P+Kaer///ve/7scff7Sm4FzP/L766isTy6xxtPo6IYQQQgghPCTc8iDUi/HCj0jDuIPaL2z/qdEqUCDjXylCBgfIVq1aubp167qDBw+GRd8yAuOhATWCpU6dOiaQsPQPUqxYMff++++bEyPPR8TQloAat6yKKBHhYh2JjvHMu+++2yJ+kWAKQhojdXzTp093r7zyirU0iAbfAeeTOrlq1SrbR9ophibU+RGpo9aQdSWKh6DF9AWjFpmPCCGEEEKIWPwtFAqFYp4hRD6FWj1EZ7BvXW6AdgCYoGBUolRJIYQQQojcR1a8ryniJoQQQgghhBC5HAk3ka6m1YmIPhHJeu211zJ1D2rhgu0QghvpiKkdY4sGtWhjxoxxWU0i5i6EEEIIIfIfcpUUcTN27FiXWzJra9eu7TZt2hT1GPV00YxGjoc0SSGEEEIIkT+RcMtj/P777wlpQp0R4mkunV1rgDCrWLGiy89ckLzEFShczB0P7BnWLKeHIIQQQgiRq1GqZC7n8ssvN/t4HAuxzG/SpInbsmWLORGS9le6dGnXtm1bd+DAgbBrsKfnmtNOO83OmTRpkvvtt9+sZQCNtRE9NH/2wKESJ8d//vOfJoqwzCfCFitVkudg2f/II4+4008/3ZUpU8aaSwfZuXOnu+yyy1yRIkXMjRGHxUjo84a7Iq6Y3Ie2AHv27EnxXNwocYJkbJnhu+++swbmzJP5Rjb+Jm0S6KtG5M37DAsWLLDeb8yH7yPYe43zBg8ebG0aaBZ+5plnunHjxqV4Pj3t+P54fvny5d3cuXMzNR8hhBBCCHH8I+GWB5g2bZpF2WgBMGzYMHfllVeatTw91xYvXuy+/fZbEz6R1yAs6DuGiKNJdMuWLa3B9oYNG6yNAILv0KFDdv5ff/3lzjrrLOtbtm3bNjdgwADXr18/N3v27DTHhkihH9qIESPc448/7osz7onVPmPn+IQJE1zv3r3Drj927JiJUcQkdWvMEUFKnzUiax70QqNJNvd+8803M7WeCEHE4vLly000PffccybmPLzedbQMQGR5n2mujVDD7n/jxo02JlocBHnqqadc9erV7XifPn3c/fffn0Ks0h/v5ptvtr529LC77bbb3Pbt2zM1JyGEEEIIcXyjdgC5HKJa2IkitoAmzwgcmmR70KSaPmEIG3qDcQ0RNM4D/ibNERFFLzKgIXRSUpJbvXq1q1evXtRnE+njPC8ihOD56aeffHONyOcAQgZhicBcunSpa9asmTWaJlIGCE2iTV792IsvvmhzQrgQ3QIEG9E3noPA5Llc9/XXX2c6TZRG2kTsELREzuCzzz6z3nGjR4+2KGVqNW6IXiJkjDkaRNy4TzCSiSjj+3vrrbf8+3bp0sWNHz/eP4f1v/DCC01ARuPo0aO2eXA/vu+yPWcrVVIIIYQQIheidgD5lFq1avl/E6UhUhR0STzvvPPs2O7du/3zaBjtgctiiRIlrFG3B+mTEIw0kdbHs0qWLGn3nThxoomlWASfA4hB756IMQSGJ9qgfv36Yeczn127dlnEzZsP6ZI03w7Oh7EnoraPMRUqVChsTVk/hGJaYIbSqFGjmOdEzo/PkdG0eM4JMnToUPs/vrexpkIIIYQQIn8hc5I8AKmIHr/++qvVZw0fPjzFeYgmjxNOOCHsGJGe4D4vukU6I8ycOdM99NBDbtSoUSYkEFKk/ZHiGItoz/HuGQ/MBxEVWWcGCMhoa5BTZNapMqP07dvX9erVK0XETQghhBBC5B8k3PIYpNTNmzfP0vKIHCUKastIBbz33nv9fcGIV0YgbZBaMurEPFG5Zs2aFPOZNWuWK1WqVMLCyLEguvbHH3+4jz/+2E+VJMWUFNBIQUoaaGR0kbo2DF5SI3J+fGYdIve1a9cu7DM1i6lRuHBh24QQQgghRP5Fwi2P0a1bN3OIxLnQc3Mk1ZCI2fPPP29pkRnhnHPOsfo3audwWpwxY4aZcvB3Rrnqqqus5q59+/YWvSNS9Oijj4adgzkHx3CSxNgEgxRq4l599VWbH58TCfVtGJ/cc889VmeG+KWuLTKahjBGpF1yySUmmnDnTE5OtlTJChUqWO0aApDataDhCgIYkxZq4zAlwewFU5Mg7KMPXYMGDSzSSL3d5MmT0z2XLYOaZIvYFUIIIYQQOY9q3PIY1IshDogGYdxB7RfCgxqtAgUy/nUiZDAvadWqlatbt647ePBgWPQtIzAeDD5oiI1pSadOnczSP0ixYsXc+++/784++2x7PtEp2hJQ45ZVogS3SNaxYcOG9sy7777bIn5BSBlFeJGS6EXDMGNBdL3xxhuuRo0aZsKC6Ary4IMPmtsn12C68q9//ctcM4MMGjTIhDYRPMTyK6+8Yq0ShBBCCCGESA25SgqRIIjSIaI9Z8q85FIkhBBCCCESh1wlhRBCCCGEECIfoho3keegbxy94KJBWmYs90dcLIUQQgghhMhrSLiJdBPZiDujRGtyHQ8Ye9BTLSPCLSvZs2dPmudQJ0d93JgxY7JlTEIIIYQQ4vhAwk2km7Fjx7qcLI1EmFWsWDFLnyGBJYQQQgghchMSbnmU33//3Z144ok58mwKLfP7GuQGLkhe4goULubyInuGNcvpIQghhBBC5ClkTpJHIALUvXt3cyw844wzzGJ+y5YtVut10kknudKlS7u2bdu6AwcOhF1z33332TX0IeMcesD99ttv1kT65JNPtsjVokWL/GtoM4AdP/3biGzR94wIW2SqZDC9kef06NHD7ytXpkwZN3DgwLBrdu7c6S677DJXpEgRs77Haj8SmnXfeuut1tqA+9DbLZh+6D2XlgLY+TO2zPDcc89Z/zrGxNrccsst/nNWrFhh8yadk80bx9atW911111n7kCs36WXXuo3KvfGh91/yZIl7ZwuXbqYwAxC/ze+SwQw3+Vjjz2WoxFMIYQQQgiR+5Fwy0NMmzbNIkz0cRs2bJj1EaNfGH3DFi9e7L799lsTPpHXIA7oN4aI69q1q2vZsqW7+OKL3YYNG6wXHILv0KFDdv5ff/1lTa/pV7Zt2zY3YMAA169fPzd79uw0x1a8eHG3du1aa0BNM21PnHFP+qUxdo5PmDAhrGk1HDt2zMQoYgjzEeaIIKVZdlD40BT7888/t3u/+eabGV5L1gyxyTi5H+uHsAQEW/369V3nzp3d/v37baOf2759++wcGnK/++677uOPP3Z33XWXCbHg+LZv3+7ee+89689GI3GEXORa0fib74Rn0euN5umpcfToUbOUDW5CCCGEECJ/oT5ueQSiWrywI7aA5s4InCVLlvjnfPPNNyYwECLnnnuuXUMEjfOAv4nyIKJo/Az//e9/XVJSklu9erWrV69e1GcTHeK8uXPnRjUniXwO0HAbYYnAXLp0qWvWrJn76quvLFIGCCWihZ45yYsvvmhzQvQQ4QIEG9E3noPA5Llc9/XXX2c6RRJBRdSRNUMsxlPjhoClcTbre8IJJ6S4hvEtWLDAIoc0FgdE6sMPP2w9PGhIzn2/++47i9x58+zTp4819UYoR4PoZaT4g7I9ZytVUgghhBAiF6I+bvmcWrVq+X9v3rzZLV++3KJS3nbeeefZMS91D6pVq+b/XbBgQVeiRAlXtWpVfx8pgoCY8Bg3bpw9i3Q/7jtx4kQTS7EIPgcQg949EWMISk+0ARGtIMxn165dJqK8+ZAueeTIkbD5MPZE1LU1btzYlStXzpUvX94iji+99JIfdUwNnCxJjYwm2jyqV6/uizZvnrQgQMx5IJA90eadQyop4jcaffv2tf/Te1vwXkIIIYQQIn8gc5I8BKmIHoiB5s2bu+HDh6c4D9HkESkyEAzBfZ6AIJ0RiCg99NBDbtSoUSYoEFJPPfWUpTjGItpzvHvGA/NBLCKgIkFARluDzMC8iF6S0khEkJRQIlvr1q2zKF80cqrNAKmZbEIIIYQQIv8i4ZZHufDCC928efPcP/7xD6uXShTUllH/du+99/r7ghGvjFC5cmWLElEr5onKNWvWpJjPrFmzXKlSpRIWTk4L1u2qq66yLTk52QQbtWtePV5kBIyoIvVp1OOlFnUjchjsJcc8iR4ScfSIFMGcg0kKEVEhhBBCCCGiIeGWR+nWrZs5RLZu3dp3cyTVkIgZRhcZFQEICOrfqJ3DWXLGjBkWheLvjIIwouauffv2Fr0j5/fRRx8NO6dNmzZ2DCdJDEMwSKEmjlo05sfnRIKxyRdffGFmIzhuvvXWWxYh9JwqEcQILNwkvbRNav2eeeYZd9ttt1n6InnLiC7q+bzrqMvDlbN///52LYKQ66hv8yDttFevXu6ee+6xqB/3JMKZXrYMapJtIlcIIYQQQuQsqnHLo1AvRnSMqBDGHdR+YftP1CgoEtILYoKIU6tWrVzdunXdwYMHw6JvGYHxYEJCJAqR06lTJ7P0D0Jd2Pvvv+/OPvtsez5ROgQQNW5ZIU5YJ0QhBio8CxMRXCCrVKlix0kXRfzSuoBUTcQW9YFE5EjrbNiwoaV2Ip6D0bdGjRqZ+EUQsobXX399itYI7dq189cCAX7//fe7u+++O+FzFEIIIYQQxw9ylRQiQUS6beYllyIhhBBCCJE45CophBBCCCGEEPkQ1biJPAt94+gFF42gQUg0SHcUQgghhBAir6BUSZFrUgRpIeA15I4HxNm+ffsyJNwqVqyYrrFFa8idUyhVUgghhBAid5MV72uKuIlMM3bsWJcT+h9hll4BJoQQQgghRF5Ewu04ARt6eo/lBPzXhPy+BjnBBclLXIHCxVxeZM+wZjk9BCGEEEKIPIXMSfIopO7RH4wWAGeccYZr0qSJ27Jli9V80XesdOnSrm3btu7AgQNh19x33312Db3LOAc7+99++8116NDBnXzyyRbBWrRokX8N7Qaw5aePGxEu+pURYYtMlQymN/KcHj16+P3lypQpk8ISf+fOnWaZX6RIEbPcf/vtt1PMkabdt956q1n3cx96vNEbLfK5tBagPYLXSy2jPPfcc2blz5hYm1tuuSXVc3/88Uez9WcdaWXAujMnD3rQNW/e3I4XL17c2gzQK867lr51tBlgTXnm1KlTMzV2IYQQQghxfCPhloeZNm2aRZjo5zZs2DDrSVazZk23fv16t3jxYvftt9+a8Im8BqH30UcfmYjr2rWra9mypbv44outGTQ94RB8hw4dsvNpSk3z6zlz5rht27a5AQMGuH79+rnZs2enOTYEC02sR4wYYU21PXHGPenVxtg5Tg+13r17h11/7NgxE6OISUxImCOC9JprrrHImsc777zjPv/8c7s3TbUzCmuG2GSc3I/1Q1imBqKRa9544w23evVqSxVt2rSpjRvoz3b06FHrTffpp5+64cOH2/jhscces7VEIG/fvt2NHz/evpPU4D7kSQc3IYQQQgiRv1CqZB6GSA2iCJ544gkTbUOGDPGPT5kyxZUtW9bt2LHDnXvuubavevXqrn///vZ33759TfAhGjp37mz7EGYIiU8++cTVq1fPmksPGjTIvyeRN4QKwi1SFAapVq2aS05O9sf57LPPmshq3LixW7Zsmfvss8/ckiVLLFIGjDvoEDlr1iwTeM8//7yZlgBRKaJv7733nglMQBxyTmZTJGmwzb2uu+46E4vlypWz9YwGkTUEG2ISwQsvvfSSrTUGLQhh7nfzzTdbY3QoX7582LO4d+3ate3zP/7xj5hjGzp0aNh3IIQQQggh8h+KuOVhatWq5f+9efNmt3z5covqeNt5551nx3bv3h0mqDwKFizoSpQo4YsLIEUQvvvuO3/fuHHj7Fmk9nHfiRMnmviIRfA5kJSU5N+TKBMixxNtUL9+/bDzmc+uXbtMRHnzIV3yyJEjYfNh7Imoa0NQItYQWEQcEWJe1DESxl+oUCFXt25dfx/rSKomx4DoHWL6kksuMQGLEPYgyjlz5kxzqSSddNWqVTHHhsDGkcjbSCEVQgghhBD5Cwm3PAwRomBfMmqqNm3aFLZ5tWQeRNCCEM0K7vOiW0S7AIHx0EMPWZ3b0qVL7Z7UwwXTFaMR7TnePeOB+SAWI+dD9PD222+PugaZAYFIqugrr7xiIpPII9FJ2hxkhE6dOrkvvvjCRCCpkkTXnnnmGTtGZJEauAceeMD95z//cY0aNbI1To3ChQubjWxwE0IIIYQQ+QsJt+OECy+80G3dutXS7jAYCW6ZETdeOuC9995r6X3cLxjxygiVK1e2qNH+/fv9fWvWrEkxH0RnqVKlUswnq1wsiaJdddVVln5KhAwjlHfffTfq+P/44w+rz/M4ePCg1cZhtOJBVLFLly7u1VdfdQ8++KAZwXgQvWzfvr178cUXrTccUUwhhBBCCCFSQzVuxwmYYSAMWrdu7bs5kmpIxIwaMNIiMwL1adOnT7d6NOrbZsyY4datW2d/ZxTEETV3CJennnrKzDYeffTRsHNwXeQYTpIYhmCQQpQKEcT8+JxIMDYhQkZ0EidIHCCJEEZzqmRNGBd1gf/+978tWtenTx935pln2n7AuZPIGvPERZI0VgQfEM0jmojTJMYjPNs7lh62DGqi6JsQQgghRD5BEbfjBOrFiI5h349xB7VfiAfMPAoUyPjXfM8995gDZKtWraymi8gS0bfMwHjmz5/vDh8+7OrUqWNphVj6B8FiH0fGs88+256PsCFdkxq3rBArrBOiEGdOnoXTJWmTiKtoYJSC+MLMhPo8XCURe16KKN8DYpp74YSJgKPdAFCTR90adYAIRUQ1AlsIIYQQQojU+FuIN04hRJ6BCCXpohiVKOImhBBCCJE/3tcUcRNCCCGEEEKIXI5q3MRxA426g73ggpCWWbRo0ZgulkIIIYQQQuRWJNxEGC+88ILVxmXUBj8nwXKflgEZEW5CCCGEEELkZiTcRJ4Be37cLDdu3GjNqyNBmNEuIBr0kcMQ5cYbb0zYeGi9gMhl8z7jfLl69WpXr149/zyOIyjfe+89/5zUwGkT8RwPFyQvcQUKF3N5jT3DmuX0EIQQQggh8hwSbiLhHDt2LEUD7vxCkSJFXO/evd2KFSuiHqeVAo6TsGrVKnfzzTdb/zevaFVRQSGEEEIIEQ2Zk+Qg9AkbOnSoRZF4Ya9evbqbO3euHSM6Q5TonXfesRRA7PFphM1LfpAFCxa4iy66yATDGWec4W666Sb/GP3D2rVrZ33JuJ76L5paByG6g+U+x7kWu/9IXn/9dWuIzTPKly/vBg0aZA2oPRjn+PHj3fXXX2/NviOt/dMDY6aHGw2qWRN6pmG9D17vOBqB88zLL7/cF0ONGze2+ePe07BhQ7dhwwb/nkS5gPlxnfc5nrmll7vvvtuaidMaIBrMq0yZMrbRaw9oMu7ty6rm4kIIIYQQIm8j4ZaDINpobk3PsK1bt7oHHnjA3XHHHWHRGhpTjxo1yq1fv94VKlTI3XXXXf6xhQsXmhhp2rSppQ8i8uiL5nHnnXfadW+88Yal79H5gXOJiMHatWutN1r37t0tle+KK65wTzzxRArDD8Tf/fff77Zt22YNpxF7keJs4MCBNpZPP/00bIzp5bHHHrPnLFq0yG3fvt0EIYIMPvroI/t32bJlbv/+/dZ3DX755RdLMfzwww9NNCH2mCf7PWEHCECu8z7HO7f0gLjs0qWL9WlDmCcCmnRjKRvchBBCCCFE/kJ93HIIXsaJuCBCaODsQTPqQ4cOWeQGIcXxRo0a2TGiOM2aNTOjDSJEROCIEr344osp7k9kjabPNOXmPCCaVrZsWTdt2jTXsmVLd/vtt1tvCQSgx2233eYWL17sm5NcddVV9nyEiAfPe+SRR9x//vMf+0wUizqu0aNHZ3pdiNoh1KZMmZLuGjcPBBMNtV9++WVrkJ1ajVs8c0tvjRt/EzGsUKGCGzdunGvbtm1YjVsQPvMdE2VkvKmBKCYSGEnZnrNV4yaEEEIIkQtRH7fjiF27dplAI8XvpJNO8jcicLt37/bPq1atmv93UlKS/fvdd9/Zv4gBT9RFQrSKCF3dunX9fSVKlHCVKlWyY945weMQFJGwefNm9/jjj4eNsXPnzha5YvwepHMmgq5du7qZM2eaMENAUQeWFt9++62NiUgb/wfh/xzY+3/99dcxr4t3bumFdMiHHnrIDRgwwP3+++8usyAs+T+9t+3duzfT9xRCCCGEEHkLmZPkEF7fMKJdZ555ZtixwoUL++ItaPJB1Ai8FLzsMLJgnER7WrRokeIYUT8PatsSAXV4uC4SXXz77bdNmHbr1s2NHDky1WtIkySaOHbsWFeuXDlbPwRoWqIp3rllhF69ernnnnvOtszCfNiEEEIIIUT+RcIthzj//PPtZZyoEGYakQSjbqlBNI66tg4dOqQ4VrlyZTPZoI4tmCqJuQnP9s7heBBqxIJg3ME1qdnsZwVErBBjbJdeeql7+OGHTbideOKJdtxzZfQgHRSBRF0bEJE6cOBA2DkI4MjrsnJuRO+o1yPNkfTPrGDLoCYJC70LIYQQQojcjYRbDnHyySdbOh2GJETQGjRoYGlwiBBexokcpUVycrJFpKinojYNoUakCjt60gZvuOEGS/3DdIPn9enTx6J77IcePXq4Sy65xEQR+5YsWWL1bUFI96NODOfJW265xRUoUMBSDLds2ZLCyCQR8LxatWq5KlWqWB3gm2++aQLTc18kysgYzzrrLIuKkRrJXGfMmGHpmuQTI/Qio5HUnyFymS+CGafNrJ4bdYrU/VFrF5mSKoQQQgghRHpQjVsOMnjwYIvK4C6JOLnmmmssddKzvU8L7PDnzJljrpHUhF155ZW+86LnoogIQpyQOogPDcLOS7+kSfSkSZMsxZBWBEuXLnX9+/cPe0aTJk1MPHGMtgNcgxiJR1hmBKJq1HQRTbzssstcwYIFreYNqNl7+umnTYj+/e9/9wXo5MmTzeCDCBpmIAhSRF4QnDlJvcSchXYC2TE31pnv+MiRIwm5nxBCCCGEyL/IVVKIPEZWuBQJIYQQQojEIVdJIYQQQgghhMiHSLjlYWgWHav/V05BA+qgxX5wS+tYPNBYPNiPLSt46aWXbEykO5KiGRwn9XdCCCGEEEJkJ0qVzOPCjebOXrPs3AJ95ggPR4NQcaxjkbVp0SDkzM82K0XrL7/8Yv3h6CXHeCdMmOAfQ8wF6+AiG3Fn9XejVEkhhBBCiNxNVryvyVUyn3Ps2LGwXnGJAPEVS4CldizeZtX8nyCrwYWTjf+j4fqZne0Q4uWC5CWuQOFi2f7cPcOaZfszhRBCCCHyO0qVjBNe3nF/xPERq3lcGOfOnWvH3nvvPWuOjd08lvTFihWz3mn0CAuyYMECcy/Exv6MM85wN910k38MV8R27dqZTT3X04h6586dYdcTxcG6nuNcS1+2SF5//XVzV+QZ5cuXtwbTtAnwYJzjx4+33mI0zX7yySczvCaMuU2bNtZ3jTXBlh8nSw/6qd16660WGTv99NPNBXLPnj0pUh4ZAy6RlSpVcv369Ytqnc96P/7442HXBb+bESNGmLjC6p81Cs4rrXHEAy0TkpKSXIkSJawhOILXc/akYThtHVhbNn4P9Nbjv7B4++jn5kXncJps3bq1rT/tGcaNG5eusQghhBBCiPyHhFucINqmT59uKXNbt261F/U77rjDrVixwj/n0UcfNdv59evXW13UXXfd5R/D5h+xRZPojRs3msirU6eOfxwxwnVY+69evdpSATnXEwg0yu7YsaPr3r2727Rpk7viiitS9Br74IMPTPzdf//9btu2bWabj9iLFGeICMby6aefho0xvdDKgOcsWrTIbd++3QQhghQYN3b7RK0YF/3pqA+j5UEwssY6IHCx6seaHyFIS4NgA3LW+5NPPnG333571HHQPmDYsGH+eOibVrp06XSNIxbLly+38fDvtGnTbE3Z4NVXX7WecojK/fv324ZoHzNmjEXrvH307PN46qmnTIjyO6C3Ht8X8xdCCCGEECI1VOMWBzSCJlKzbNky64fm0alTJ3fo0CFrtIyQ4jgNsYF+ac2aNXOHDx+26Bcv80TAXnzxxRT3J7J27rnnmqjgPCCaRs8xhELLli1NtBDBQQB60HSbZtReHdVVV11lz0fIePA86rT+85//2GeiP9Re0a8ssxC1Q6hNmTIlxTGei7BE0PFMQCgR9Xrttdfc1VdfbWKV8X/99dfWv82DnnQ333yzCTEgCvfuu++6NWvW2GeuY87ch1o0In7PPvusfR8ZGUcseBYRNIQbPeWA6B3Nur3+cumpceNcevYhdoPfI3nQ/GZS+/2xeXAuv42yPWcrVVIIIYQQIheidgA5xK5du0ygNW7cOMxdkAhcMDJE02gP0uo8ow4gSuaJukgQFUTogimCpOSROsgx75zIFMKgiITNmzdb5Cc4xs6dO1vEh/F7kM6ZCLp27WriBaGFOFy1alXYWFg3Il3eWBC/NKMOrlnVqlXDRBsQdSNqBvx3hVdeecX2RYN1QdSktrbxjiMWuEh6os37br3vNSNEfm989r7n1KK9/B/f2xBtQgghhBAifyFzkjj49ddf7V+iXdQkBaGmyhMAQZMPL7pD/RVQA5Yd46SmrUWLFimOEfXzoLYqEVCHR30XkSJS/RBP1H9RD8ZYatWqZbb6kRAhizUW6r969+7tNmzYYBFLatRatWoVdQxprWu844hFpHkL3633vWYHRFB79eqVIuImhBBCCCHyDxJucXD++eebQCOlr2HDhimOxxO5IRpHPRemFZGQOoeBCHVswVRJar94tncOx4N4qYMemJJwTXY6ICJ+2rdvb9ull17qHn74YRNujGXWrFnmIJne8DA1Y6wzYgvhRqQzNSdKDFEQb6xttFTJzIwjXogY/vnnn2nuS+174zPfb2rw22MTQgghhBD5Fwm3OCDNDnMJDEmItDRo0MDyValJQwwEe3qlRnJyskWkKlSoYDVNCDUiVUSWEB84HZLWiKEIz8O0guge+6FHjx7ukksuMVHEviVLllh9WJABAwa46667zlwVb7nlFqvDIlVwy5YtKYxMEgHPI5pFKiHpipiLeAKE1EZMOBgr6ZuIMaJzmHmQVsnnWHA9a0Y9Wqx6PCKJrCH3RCyxRt9//70ZmmDmktlxxAN1a++//759rwgs6v7YR7QPQYkRCU6gbMDvBhdMnDGJVM6ZMyesdjFetgxqoj5uQgghhBD5BNW4xQkW7phlUG+EOMGVkJdt2gPEA7bxvKDjGklN2JVXXmnuiR7Y6COCEF7UPFHbhbDz0vTq1avnJk2a5MaOHWtCYOnSpa5///5hz8A9EfHEMdoOcA2iJx5hmREQSqTxEU287LLLrA7MM+xApCBmEJGkbrJmCClqy+IRGwhPoo7U5gWt/6PB9/Lggw+akOQ5pFV6NWiZHUc8IAhpL4Ao99IviZx26dLFxsI+hJoHY8VBtGbNmiao//Wvf9l3J4QQQgghRGrIVVKIbCSaA2VucCkSQgghhBCJQ66SQgghhBBCCJEPkXDL55DOF2wfENzSOnY8kNr82GjYLYQQQgghRG4gX5mTBBs3Zwbs4OfPn59m7VVegPosaq6ol8PshLDu66+/7p588kkzYqH/XDQyGvIdOHCgrX9q981uYo0jsvVDItIgqYWL/Eyd5MaNG632UQghhBBCCJffhRvGHsd7Sd97773nrrjiCvfjjz+6//u//0vzfGzyP/74YxOiXFu+fHlzRcQwA2OP7GwtEDRyQcSMGTMmw/OKl4zM74UXXjBxxn8EEEIIIYQQ4rgUbti740aYE1AgmBvIyTWIBn3okpKS/B5yIm9wQfISV6Dw/2sxkAj2DGuWsHsJIYQQQog8VuNG9KR79+4WoSCSg+05fcWuvfZaqyMqXbq0a9u2rTtw4EDYNffdd59dc9ppp9k5WOH/9ttv1sCaPmdEShYtWuRfQ7NjbN5JO6Mhc6VKlSzCFpkqGUxv5Dn0R6Of1+mnn+7KlCljqXxBdu7caVb39AujGTZ9tyLZu3evu/XWWy0SxH3oGRZMifOeS/rh3//+dxtbZqBnGr3LypYta33DWIvJkyfbM4lKAetGSifPjgXHWWuai3M+6X6RPPvss+6CCy7wP5PqyLkTJkzw91111VUp2hPEYsaMGfYsxDT9z3755Rd/PCtWrLDvjmewxZqX9/ti4178xmgPEG9klTFgyd+uXTv7PdI6gZYN9ILje2Qf7Q6w7/cif/wGcQjyxhf8zdC+4K677rLfKC0IJk6cGPY8WkDQBoDfU+3atS1FUgghhBBCiFxhTjJt2jSLMNF4eNiwYdbDjJdXXoapq/r2229N+ERew0s4L7oIi65du7qWLVtaVGjDhg3u6quvNsHHizJQj0UzZXqlbdu2zXp69evXz82ePTvNsRUvXtytXbvWem1R8+WJM+5J7y/GznGECoIpyLFjx0yM8qKOmQVz5GWfPm9E1jxoxPz555/bvem1lhkQGa+88op7+umn3fbt261pN89EyM2bN8/O4Vn79+9PIV4j4bjXmJrz161bl+Kchg0b2poiZgBhxXeDiPHWYPXq1Sai4o3wIf5YBzbux+/CGw997GhGznjY0gipBFYAAKT4SURBVJoX32GhQoXst8J+0jyff/75uNeTXnc07kZENWvWzH5XrPEdd9xhvzX6s/EZMcjvjxROavy88dGc3WPUqFG+ILv33nvtd8uYgYbc9OnjPwCQnorgC14bS6hTexjchBBCCCFE/iJbUiXPOeccvwEx0Q1E25AhQ/zjU6ZMsZfzHTt2uHPPPdf20WTai+DQ5JkXe8QCL/SAMBs/frz75JNPrNE0jaoHDRrk35PIG2IC4RYpCoMQTUlOTvbHSXQJkdW4cWO3bNky99lnn7klS5ZYpAwYN9FCj1mzZpnAQygQffGaaRN9Q9ggMAFxyDmZTZFkjZgTApAoF1CX5kHEz6tdi6cWjCgVopPm2UQco0G0jfsisGiMzbwwNPHEE4IJ8RZvqiXrRZ0YzwWEEmtORJLxsEbU1wXHE2te/HYQX6w/0cxPP/3UPnu/lbRo2rSpu+eee8J+VzQw5z8UAGIdMcl/YGBMjJFnRVsv7oVg865jHMuXL7dxvfzyyzZ3oqNE3KpUqeK++eYbE3exoOl78LcthBBCCCHyH9kScatVq5b/9+bNm+1FNmi7ft555/mRmKCg8kBUlChRwlWtWtXfR/okfPfdd/6+cePG2bNKlixp9yVNjRTAWASfA9R6efckmoUo8EQb8AIfhPns2rXLRIg3H0TGkSNHwubD2BNR14YLIutBFCy7QKSQLopgw5CD6BvihEgQwhZBh9BBbMWbnuiJtsg1zwgId080e98RKa6kz8ZD8Dfg/a7S+q3Fcy9P3AV/TxxHtAXHmhb8hwtSM72N1FwhhBBCCJG/yJaIG9EmD9LFmjdv7oYPH57iPF7gPYigBeElOLjPe1EnggEzZ860tDNS1XgZRhg89dRTluIYi2jP8e4ZD8wHsfjSSy+lOIaAjLYGmYH6vZyANEiEMOmgRExJFfTEHMItPUIys2ueaKL9rmL91uK9V6LmRh0jmxBCCCGEyL9ku6vkhRdeaPVKRF2oS0oU1JaRquelqUEw4pURKleubNEN6pg8UblmzZoU8yFdkhS+jPY2Sw9EghACiCUvVTKIF9WLN9oULwgzzGKoIfRq2fiXdFLWntTJRMEcIscfa16R4pzviLRXIpNZQbTxxft7wpSFaKwXdYv8PQkhhBBCCJErhFu3bt3MIbJ169a+myOphkTMqAHL6Ms2L+rTp0+3ejTq23hBxmiDvzMKwoiau/bt21v0DlOIRx99NOycNm3a2DEcCD2Tj6+++sq9+uqrNj8+JxIEL+PBuRBzEmoBeR7peNTy4YpIlAfTD+qtiNCRvplZSPHD0ZE6Lc9cBeFGlJPnYe6RyDkixnCT9FJPY82LdNhevXpZnRpmIs8884xFXrMKxkeklbo81p8U0XjSRG+//Xb7/VB7R/oj8xs5cmSGx7FlUJNs+Y8FQgghhBAin9S4BaFejAgNEQuMO4ggEcnBcKJAgYwPh5d2HCBbtWrl6tat6w4ePBgWfcsIjIfG1IcPH3Z16tRxnTp1MgONILywv//++2b9zvOJqtCWgKhKVr1UY56BSQjzoz4QIUCrBDjzzDPNyKJPnz5Wm4VNfiJANF166aX2b4MGDXwxxxxxUUxUKiggBhHwuC+SboowizUvHB+974j/MHD//fe7u+++22UVRHa7dOlivzXG5xnvpAVCc8GCBWaeQropIi5ayrAQQgghhBCR/C0Ub8MrIXIhRP1q1KhhFv35BSK/OFtiVKKImxBCCCFE/nhfy/aImxBCCCGEEEKI9CHhlgPgzBhshxDcSBFM7VhGatVIM4x1v7TaJaQXepOl9qxozps5tc6JqPsTQgghhBAiu8izqZJ33nmn9RR77bXXMnUfaraoY7vxxhtddkE91r59+1I9Fsvyv2LFiul61h9//GEmGKmRaHdPjFJoxh0NatOC/duy+nuJts4YxhC6njBhQrrXMpKBAwfa74/eetmJUiWFEEIIIXI3WfG+lu2ukoli7NixLo9qThNmmRUN8YIoy65nAe6PuXmd+T8O7RSyc02EEEIIIYTIUeH2+++/+/21shsUbG4g0WuQk2sq8hYXJC9xBQqn3YYgFnuGNUvYeIQQQgghRC6pccPBDxt27PvPOOMM16RJE7dlyxZ37bXXWs0QqXBt27Z1Bw4cCLvmvvvus2voA8Y59HHDvr5Dhw6WOkf0Y9GiRf41tArAUp8ebERNKlWqZBG2yFTJYBodz+nRo4ffG65MmTKWyhZk586d7rLLLrPmx1jNv/322ynmSMNt+qHRnoD70J8tmGroPZe2ALQ2YGyZgVTFwYMHm6U90SDPxv7DDz80+33mX7ZsWZubZ/kPzz33nPWuYy6sKe0BIr8nNgQu39Vjjz0WFqH88ccf7Zl8J7Q04DtkfTxeeOEFWwP64tHigO/3mmuusWbkHu+9955Z8NMKgHPp5UaqpMfrr79uDcoZY/ny5c3On9TNeOFZjIs14Pq5c+eGHcdW/8orr7TjJUqUsLWjv1rwd0R/N8bGcX4bwTWg7x/7jx49GnZfvl9+xxmBXoSsF3OmVQPfkwe/I1JA6fF3xRVX2LrTB2716tUZepYQQgghhMg/pNucZNq0aRYRohfbsGHD7MWZnlTr1693ixcvdt9++60Jn8hrEA8fffSRibiuXbu6li1bWj8sGibTz40X5UOHDtn5pLLRuHrOnDlu27ZtbsCAAa5fv35u9uzZaY4NEUHzZnpr0RDbE2fckz5rjJ3j1Dj17t077HpqsxCjiEmMLZijJ1iIhHnQePnzzz+3e3vNqDMDTZh5gd+4caMJrN27d9szb775ZvfJJ5+4WbNmmZDzepex1gg55sc4WHcEaeRakCbJmiN6//Wvf5moCApQ7vPGG2+YcEDQ0Ng6WJ/G98HYaGZOrzqMTOixBggwBE7Dhg1tjNwD4YQwAdYPYUhPNb7Df//73yYGI/vgxYK1YA02b95sjc5vu+02t337djuGiOW7QnjSaJ3fyrJly8L6u9GEm2dOmTLF1u+HH36wujkPfoOIO9bAg0bmCxcutAbn6QXzFX6rzJFxDhkyxObAdxGE/m2sI7VxNHinGX0sQYuwJE86uAkhhBBCiHxGKB00bNgwVLNmTf/z4MGDQ1dffXXYOXv37iWkEfr888/9axo0aOAf/+OPP0LFixcPtW3b1t+3f/9+u2b16tWpPrtbt26hm2++2f/cvn370A033BA2tuBz4KKLLgr17t3b/l6yZEmoUKFCoX379vnHFy1aZM+dP3++fZ4xY0aoUqVKob/++ss/5+jRo6GiRYva9d5zS5cubfsTQbly5UI33nhj2L6OHTuG7r777rB9H3zwQahAgQKhw4cPh+bNmxc65ZRTQv/73/+i3pO1qFy5ctg8WAf2wY4dO2zeK1eu9I8fOHDA5jl79mz7PHXqVDtn165d/jnjxo2zucPBgwft+HvvvRd1DI0aNQoNGTIkbB/rm5SUFNe6cO8uXbqE7atbt26oa9eu9vfEiRNDp512WujXX3/1jy9cuNDW6L///a995lkjRozwjx87dix01llnhf1uuN+1117rfx41alSofPnyYWuXGsnJyaHq1av7nytUqBB6+eWXw87h/yP169e3v7/88kub1/PPP+8f37p1q+3bvn17zOdwTuRWtufsULneb2ZqE0IIIYQQiefnn3+29zX+TRTprnGrVauW/zeRkOXLl0e1VidqRDQBqlWr5u/H7p70tKpVq/r7SPXzoh0e48aNs0gJUR7cAYl40Wg5FsHnQFJSkn9PIiCkHJLe6FG/fv2w85nPrl27UjgfHjlyxObjwdgTWYdWu3btFOMgihW0z0fLEDX88ssvXePGjc0EhPRBInNsN910k6XeedSrV8+PfnlzJQJFhIm1IBpXt25d/zjfCWmfXkQLuF+FChWiridppETtiHoxnquuusoirZzjzYGIZTDCxrNZSyJ5wbGmRuT3w2fPwZFxEqUkwupBqiZrRBSSVEVSLYNzZM6sdTBdsnPnzu6iiy4y98kzzzzTInTMK7h28UAEkN8IKb7c04NIWmQ9ZvB36q0X60pqZTT69u1rKZ8eRNz4LQshhBBCiPxDuoVb8EWZeqLmzZu74cOHpzjPeyGFE044IewYL8XBfd5LMi/dMHPmTEslQ2jwso6QeuqppyzFMRbRnuPdMx6YD8I0Wr+xkiVLRl2DRBB5P8Zxzz33WDpkJGeffbaJRlJMqTFbunSppedRz0fKIPVciSLaegZFz9SpU22MpGqSztm/f39LH0U0Mgdq2khPjQRRlVsgzRcBSL0bKbtbt261VMn04tXWUb8ZFIvef6wIEuu3H43ChQvbJoQQQggh8i+ZcpXEeGLevHkJ7wVGpIb6t3vvvdffF4x4ZQQMIzAeIQrjico1a9akmA8CpFSpUjnaH4txUBcWy7Ke9SbKxZacnGyC7d133/WFUqTIZa6YmSAiWAsiQZzDOsPBgwctUoVpS3qFDxtRIUT2yy+/bMKNOXC/zNjuM2bq5IKfeRYwB6JjRLo84cvvpkCBAhY5JMrF98wcvfo/5vzxxx/b2IJ06tTJjRkzxqJurGdGollEjYnmfvHFF1aPJ4QQQgghRCLJlNrq1q2bRRgwV/DcHEk1JGKGEUZkpCFeEBhEQHA0xFkScwyiSfydUXghJ3Wzffv2Fr0j3QyTiCC8cHMMJ0mMPzBIwSURF0Dmx+fsANMUxA9GG4gKhAlCjmjWs88+a4YoCAQECeYcb731lkVsgg6XpJiSXkfkjujcM888YxFMb32ZIyl9mIYQ0ezTp4+lCrI/HkjZnDhxorv++utNsCDScKX0hBZRwOuuu84ihDheIqhIn8SF9IknnojrGRiOkNrYoEEDi4JitDJ58mT/u0Kw8n0Sbfz+++/N+AaTGy/1FmMUDHSYL2mIGLTQtD2S22+/3SK8/Jb53WUUIoxEIBGNpK9iKoIBDA6ewVTHRLFlUBM14BZCCCGEyCek21UyCC/sRDmoXSLNjNovbP+J/vCinlEQG0SOWrVqZWlnRIOC0beMwHhwFKReDgt7BFGkwyF1V7gnIjZ4PlEdapaoy8rOF2RqoFasWOF27NhhLQGIMiGEvPo81hcxiaMnY8Qh85VXXnFVqlTx74GA8uaKwEbEeK0GvDRH0kIRV0TKSIFEAEamR6YGa/XZZ5+Z6yOCmHvzHL47oPYNgUkqJzVkCNHRo0enq0E3Qoj/CMB6IKiYoxcR5PkIe5wiuT/isFGjRiZsPR588EETcog7L+WWWsBIEFrMg1rNYIuJ9MJviv9gwdry/wUcN4kKZuY/OAghhBBCCAF/w6FES3F8QR83jFxI/xPxgehD+D799NMut0O0GLH5888/K+ImhBBCCJFP3tcSV5gmRB6ENEZMXtiCzbKFEEIIIYTITUi4ZRIaTV977bVRj5GqWLRo0TSdCPMb1Kt5KZWRkEqJs2N2QRoq4g1n1GCNIBCBo8YxGtQGyoRECCGEEEJkF0qVzCSIM9wIMyLcMuO4mBGot6IGMZpBR3byyy+/uG+//TbqMWrs0lMHl5Ug2o4dOxb1GAYokf3+sgulSgohhBBC5G6UKpkLQZhltwDL6yB4IkXPnj17zMRj48aN6RZu9ELDeCYzxiLRwPhkyJAhZljD/+loE0D94MMPPxw2/mnTppkpCpFCnFRpN8A5GL94kIp5xRVXmLkKzdWDjquYzVCPSOPv9HBB8hJXoHDajcyjsWdYswxdJ4QQQggh8qCrpMh/pBaBOt7AERMnTCz9Se3cvn27e/HFF+2/nDz22GP+ebQRIO0TB1QEGS0LaF9AW4Wgw6UHbRwy03JACCGEEELkTyTcsgj6qg0dOtSiSETlqlev7ubOnetHX4gSvfPOO9anDGt7GmHTCy3IggULzOq+SJEi7owzzgizsqcuC8t/+rhxPXV29FGLTI2ktQHHuZa2CpG8/vrrFiHiGeXLlzcLfhpVezDO8ePHW782+slFtlBID4yZurCSJUvamtBfDet88CzzqTnjmUS2gP59jRs3tvkjmrDYpy+dB83fgflxnfc5nrmlxqFDh1yHDh1c06ZN3RtvvGE9ABkfrSlGjhxp9W1eQ3B649H7DwFH5JX2DKwRKan0bqPpexB6zdF/DkEohBBCCCFEvEi4ZRGINiIr9Fgjhe6BBx5wd9xxh/Vn86ABOC/+NGkuVKiQu+uuu/xjCxcuNDGCeCB9EJFHTzYP0uq4DmGxevVq68PGuV5EbO3atdaDjibemzZtsjS9yMbXGKsg/ujxRoNvBAliL1Kc0eCasXz66adhY0wvRKp4zqJFiyyChSBEkAGRKli2bJnbv3+/9anz6uHow/bhhx+aUELsMU/2e8IOEIBc532Od27RoD/cgQMHrOl6NEhtBPrK0fstmtEKPeT4LubNmxe2H0GHeKQherwg8siTDm5CCCGEECJ/IXOSLIAX7dNPP91ECI2fgw2aiebQrBohxXH6hwHNr5s1a2aGJkSIiMARJSI9LxIiazS9pvk55wHRNGqwqLdq2bKlu/32260uCwHocdttt7nFixf75iREknh+3759/XN4HoLlP//5j30mioXYoHl2ZiFqh1CbMmVKimPBGjd60MWKZCKcXn75Zb+GLFqNWzxzS40RI0a43r17W40bEc3UIMqJWEQYR4MIIRFG2gx4NW5EHWfNmuX69etnaZOck1aNG8KZaGEkZXvOVo2bEEIIIUQ+MSdRxC0L2LVrlwk0UvyIyHgbEbjdu3f751WrVs3/Oykpyf797rvv7F/EgCfqIiFaRYSO1D2PEiVKmJ09x7xzgschKCJh8+bN7vHHHw8bY+fOnU2MMH4P0jkTQdeuXd3MmTNNmCGgVq1aleY1uE8yJiJt/Pj54dNG4euvv455Xbxzi0Z6/ltGRv67B5FQvi9aEMQD4pP/03tbZPqlEEIIIYQ4/pGrZBbg9Wcj2nXmmWeGHStcuLAv3rC+9yBq5EWUIFYbgUSOk0hOixYtUhwj6udBbVsiIEKFxT7RxbffftuEabdu3axuLDVIkySaOHbsWHObZP0QoL///ntC5hYNopnw2WefpRC7keeRwslYTjzxxLBjRPX4Ly3evYIguknZJMJGKmtaMGc2IYQQQgiRf5FwywKwfOdFm6gQZhqRBKNuqUE0jro2TDIiwQCDOinq2IKpkpib8GzvHI4HoUYsCMYdXJOd7QwwJkGMsV166aVmm49w84TPn3/+GXY+6aCkGlLXBkSbqD8LggCOvC4zc7v66qstpZOUSVIwIyHVlPRGUk+ffvppq5/DdCQIc2JcN998c9RnkM6KqUm0FMh42TKoifq4CSGEEELkEyTcsgB6fOEyiCEJETTs4UlxQ4Twoh1PnzKcB4lIVahQwQQCQo1IFbVXpA1iN0/qH6KB5/Xp08eie+yHHj16uEsuucQEBPsw3KC+LciAAQOsTgznyVtuucUVKFDAUgy3bNmSwsgkEfC8WrVquSpVqlgdIJb7CEwoVaqURRkZ41lnnWVRMVIjmeuMGTMsXZMIFkIvMhqJkyQil/kimKlLy8zciDA+//zzJq6oy2MtEYAIxtmzZ5sgJ+WTaBzmJ4yJqBs1dhiSUEtHhJC6NeoOU2PYsGGuSZMmCVpdIYQQQghxXIM5iUg8f/31V2jMmDGhSpUqhU444YRQyZIlQ02aNAmtWLEitHz5cgqjQj/++KN//saNG23fl19+6e+bN29eqEaNGqETTzwxdMYZZ4RatGjhH/vhhx9Cbdu2DZ166qmhokWL2r137NgRNobJkyeHzjrrLDvevHnz0MiRI+38IIsXLw5dfPHFds4pp5wSqlOnTmjixIn+ccY0f/78hKzJ4MGDQ5UrV7ZnnX766aEbbrgh9MUXX/jHJ02aFCpbtmyoQIECoYYNG9q+DRs2hGrXrh0qUqRI6JxzzgnNmTMnVK5cudDo0aP96954441QxYoVQ4UKFbJj8c4tLdatW2drzndXuHBhe8bdd98d2rlzZ4p1rlWrlo2xePHioUsvvdTGFCTadw5XX3217Z86dWrc4/r555/tGv4VQgghhBC5j6x4X5OrpBB5jKxwKRJCCCGEEIlDrpJCCCGEEEIIkQ+RcBPpokuXLmEW+8GNYxhy4JoY7Vh6wGXztddey5I5vPTSS6nOgfq7RELTb69htxBCCCGEEBlFqZIiXdBnjtBvNAgD79u3z3qbRYaE+YwBSbxEa6qdKH755RfrDxcNhGc85jHxQkN1npeeuaeFUiWFEEIIIXI3WfG+JlfJ45BofcUSBQIklghJpEDJqjXAhZMtO8ABM6t68l2QvMQVKFwsQ9fuGdYs4eMRQgghhBBZh1IljwMuv/xya+Tcs2dP6z+GxTy29zS8Jv2vdOnSrm3btmH9z7iG3mNcg30+50yaNMn99ttv1jsOYYMF/qJFi/xr6JXWsWNH989//tPESKVKlcz2PghNpYNRMp6Dnf4jjzziTj/9dFemTBk3cODAsGt27tzpLrvsMmsBQB86mnNHQv+2W2+91dIOuQ8tDvbs2ZPiuTS2/vvf/25jywz0jqMVAWNibWgpALQwYAxe37hNmzZZdJB2DB6dOnVyd9xxR9RUSeZeo0YNa3FAGwP+SwztHojKCSGEEEIIkRoSbscJ06ZNswgTveLoD3bllVe6mjVruvXr11tvNFIDET6R1yD0PvroIxNxXbt2td5lNPXesGGDNaJG8B06dMjOpycdPdbmzJnjtm3bZr3S+vXrZ73N0hobvdFoCE5T68cff9wXZ9yzRYsWNnaOT5gwwXrVBaE3GmIUMfnBBx/YHBGk11xzjUXWPOjlRtNt7o3AyiisGWKTcXI/1g9hCTQNR2Rt3LjRPq9YscLW8L333vOvZx+CNTVowE79HmNk43y+MyGEEEIIIVJDqZLHCUSHEEVAg2lE25AhQ/zjU6ZMsWbQO3bscOeee67tq169uuvfv7/93bdvXxMPiBAaewPCbPz48e6TTz5x9erVs/qvQYMG+fck8rZ69WoTbpGiMEi1atWsobg3zmeffdZEVuPGjd2yZcvcZ599Zg3CiZQB4yZa6DFr1iwTeDTFJroFU6dOtUgWggmBGWycndk0URpscy8aeCMWqXljPYEIGREznktTcP6l0Trr8uuvv1oe865du1zDhg1TvT9zIRLnpWsijlkPooXRoFk5m0dqNYZCCCGEEOL4RRG344RatWr5f2/evNktX748zC3xvPPO86M9QUHlUbBgQVeiRAlXtWpVfx8pgp4hice4cePsWSVLlrT7Tpw40YROLILPgaSkJP+e27dvN0HpiTaoX79+2PnMBzGE0PHmQ7rkkSNHwubD2BNR24egRKyVL1/eRBUulF7UERBlCDZMWIgAEjGsXLmy+/DDDy16xlwQqKlBimSwxi64HtEYOnSoCUZvY72EEEIIIUT+QhG34wQiRB5Efpo3b+6GDx+e4jxEggcRtCBEs4L7vOgWESKYOXOme+ihh9yoUaNMXCE+nnrqKUtxjEW053j3jAfmg1hEQEWCgIy2BpmBeZEqijhbunSpRR6pTVu3bp1F+UiDJIKJoGRuiGL2cf6PP/4YM9qWkfUgGtqrV6+wiJvEmxBCCCFE/kLC7TjkwgsvdPPmzbPIDj3VEgW1ZdS/3Xvvvf6+YMQrIxCpwnhk//79vqhcs2ZNivmQLoljZXbZ37NuV111lW2keSLY3n33XYuueXVuo0eP9kUawo1UU4Tbgw8+mNCxFC5c2DYhhBBCCJF/kXA7DunWrZs5RLZu3dp3cyTVkIgZNWCkRWYE0v+mT59u9WjUt+GMSBSKvzMKwoiau/bt21v0jmjSo48+GnZOmzZt7BhOkhiGYJDy1VdfuVdffdXmx+dEgmHIF198YYYkOG6+9dZbFhHznCrZR/onEUDq9YBzqfPDSCWtiFui2DKoifq4CSGEEELkE1TjdhxCjRXRMSzrMe6g9gvbf6JGBQpk/Cu/5557LOLUqlUrV7duXXfw4MGw6FtGYDw02qZRdZ06dcxKP9Kko1ixYu799993Z599tl9PRlsCatyyQriwTohCnDl5Fk6Xr7zyiqtSpYp/DuKM9fXcIxHHtDKg3UFmWxEIIYQQQggRyd9COCwIIfIMRCUxKcHBUhE3IYQQQoj88b6miJsQQgghhBBC5HJU4yaOS7DpD/aCC0JaZtGiRWO6WAohhBBCCJGbkHDLYe688073008/uddeey1T98FSnlqxG2+8MWFjy8vQHHvTpk1xCbdvvvnGXXHFFe7111+3OrWshMbb1BvynQshhBBCCBEvEm45zNixY62Rc16H1gMIErbcAMKsYsWKcZ3rtUzA/CTea/LimgghhBBCiLyLhJtz7vfff3cnnnhijjybosX8sga4MBIZzIyzpfj/uSB5iStQuFiGrt0zrFnCxyOEEEIIIbKOfPkGjYV79+7dLRJyxhlnuCZNmrgtW7ZYTdRJJ53kSpcu7dq2besOHDgQds19991n19DHi3Polfbbb7+5Dh06uJNPPtmiNYsWLQoTKtjW0+eMCBA28UTYIlMlg+mNPKdHjx5+/zXs5QcOHBh2zc6dO61vWJEiRSy17+23304xR5pa01cMa3vuQw+0PXv2pHgu1vu0D8iMhT1jpq/aAw88YMKMzUsL5PlvvPGGjZMm0l9//bX1fmvcuLGtPcIVa/0NGzaE3ZN70HPupptusnYA9JDjPh40uqa/W8mSJW1tOT516tS4xvvRRx+5mjVr2vqRUrlx48YU58Tze+A3xMYcmMtjjz3mR09TWxMPeuHRaoD7X3PNNdaAXAghhBBCiNTIl8INpk2bZhEm+p0NGzbMenbxMr9+/Xq3ePFi9+2335rwibyGF3Re/BFxXbt2dS1btnQXX3yxCQ96pvGCf+jQITufps00h54zZ47btm2bGzBggOvXr5+bPXt2mmMrXry4W7t2rRsxYoQ1nfbEGfeklxlj5zg9xnr37h12PU2gEaOISUw6mKMnEIisebzzzjvu888/t3vTdDqj0POMeTJOBEhQhLAWw4cPNxG2detWV6pUKffLL79Yw+0PP/zQrVmzxkRX06ZNbX+QQYMG2XfwySef2HGE2g8//GDHEEmsKUJ5+/btbvz48fbdpAXGI9ddd50JyY8//thE8UMPPRR2DvVn8f4eSLPk94Ag/9e//mXzjGdNRo4caQ3M6U+HmI0cQ5CjR4+apWxwE0IIIYQQ+YxQPqRhw4ahmjVr+p8HDx4cuvrqq8PO2bt3L6GT0Oeff+5f06BBA//4H3/8ESpevHiobdu2/r79+/fbNatXr0712d26dQvdfPPN/uf27duHbrjhhrCxBZ8DF110Uah3797295IlS0KFChUK7du3zz++aNEie+78+fPt84wZM0KVKlUK/fXXX/45R48eDRUtWtSu955bunRp258IypUrFxo9enTYvqlTp9q4Nm3aFPPaP//8M3TyySeHFixY4O/juv79+/uff/31V9vHXKF58+ahDh06pHuc//73v0MlSpQIHT582N83fvx4u/fGjRvT9XuoXLly2BrzHbEvnjXZtWuXv2/cuHH2XaRGcnKyXRO5le05O1Su95sZ2oQQQgghRNbx888/2/sa/yaKfBtxq1Wrlv/35s2b3fLlyy0q5W3nnXeeHdu9e7d/XrVq1fy/CxYs6EqUKOGqVq3q7yOlDr777jt/37hx4+xZpPRx34kTJ1qEJRbB50BSUpJ/T6JLZcuWtfRGj/r164edz3x27dplETdvPqRLHjlyJGw+jD2r69q4f+R8iF517tzZIm2kGdKUkEhY5LoEryMCyXneOhDtnDlzpqtRo4alla5atSqu8bB+3Jc0yVjrF8/voV69emEpkNyHNFZSZGNB6meFChWifr/R6Nu3rzVv9DbSYIUQQgghRP4i35qTIAQ8EA3Nmze3lL5IeKn2OOGEE8KO8dIe3Oe9xJPOCAgLUuBGjRplL/UIqaeeespSHGMR7TnePeOB+SAWX3rppRTHEJDR1iCroP4ssr6LNMmDBw9aemG5cuWs9o31CaZxprUO1J9RQ/bWW29ZqmejRo1ct27dLAUxs8T7e8go0eYVy1mU9WETQgghhBD5l3wr3IJceOGFbt68eWbf7lnDJwJqy6h/u/fee/19wYhNRsDQgogLNVOeiKBOLHI+s2bNsnoyolTZAZG1tCJNwXV57rnnrG4NmE/Q+CNeEKGIQLZLL73UPfzww2kKN9aP2jKij17ULdr6xfN7iBTgXr0e0dj0rokQQgghhBCxkHBzziI1OES2bt3ad3Mk1ZCIGWYT3ot4euElfvr06eYgiLMkggFHRf7OKFdddZU799xzTawQvcOo4tFHHw07BxMPjuEkiTkGJhlEpzDMYH58TjSIHIw2brvtNosOxTIKYV1YCxwdGT+CK9gQOx4weiGqWKVKFTPvwFwFUZYWt99+u60XqZqkIOK0GSn24v09kNrZq1cvd88995g5zTPPPGPR1YysSUbYMqhJtglzIYQQQgiRs+TbGrcg1IsRBSI6gjMktV/Y/mNln5meY7zQ4wDZqlUrV7duXUsPDEbfMgLjmT9/vjt8+LCrU6eO69Spk1n6R9ZQIRhoKM3zETS0JSDKlFUv+ghERBC1W8F0zGhMnjzZ7PyJbOHCSfsDooPpgWgWwot6NVojIKYQVmlBvdqCBQvcp59+aq6RiLjIlMh4fw/t2rXzvwfE3v333+/uvvvuDK2JEEIIIYQQsfgbDiUxzxBCpIA+bRijjBkzJtufTZQSUxeMShRxE0IIIYTIfWTF+5oibkIIIYQQQgiRy5FwEwaNuoP298GNNMTUjrHlJoYMGZLqOHGiFEIIIYQQIi8icxLh9zfDln7jxo0pjlHHlV7zkJyiS5cu7tZbb416LJFzeO+99xJ2LyGEEEIIIdJCwk34Zh9E1ipWrOjyMjhAsgV54YUXzFzkp59+yrFxCSGEEEIIkRkk3ETCOHbsWIrm0lkJro9ECTPj/Jmd0GAcgZwoLkhe4goULpaua/YMa5aw5wshhBBCiOwjb7zxHmf89ddfbujQodbPjfS96tWru7lz5/opeIiRd955x/qcYe1PE+/PP/887B5Y2l900UXWRJr+YDfddJN/DKt9rOpPO+00u57arp07d6aIQtEugONcS6uCSF5//XWz7OcZ5cuXd4MGDXJ//PGHf5xxjh8/3l1//fWuePHiKdoSpAdv3gsXLjSLf55Zr149t2XLlrAxY8n/xhtvuPPPP996o9FLLdZ8uW+HDh3M0Yf7sw0cODDudUoN1os+b2eeeaZdS8uAV155JYXzZPfu3S3ax3fUpEmTsHXjeXz/rK33/QshhBBCCBENCbccANFGY+4JEya4rVu3ugceeMDdcccdbsWKFf459BejmfP69etdoUKF3F133eUfQ9wgtpo2bWo1aYg8eol53HnnnXYdAmf16tWOjg+cS0QM1q5da33dEBWbNm1yV1xxhXviiSdSmJUgauhNtm3bNvfvf//bhFOkOEMEMRb6ogXHmFFoxs28aVRO77PmzZv744ZDhw5Z3zUaYbN29H+LNV9EL5b92LDu37/ftoceeiiudYoFPfFoAM53gbikfxs96T766KOw86ZNm2ZRNvrC8X17PPbYY+7mm292mzdvtobpNOmmzjAaNBjHUja4CSGEEEKIfAZ93ET2ceTIkVCxYsVCq1atCtvfsWPHUOvWrUPLly+nr15o2bJl/rGFCxfavsOHD9vn+vXrh9q0aRP1/jt27LBzV65c6e87cOBAqGjRoqHZs2fbZ57TtGnTsOtatWoVOvXUU/3PjRo1Cg0ZMiTsnBkzZoSSkpL8zzynZ8+eoUTgzXvmzJn+voMHD9q4Z82aZZ+nTp1q52zatCld8+W64NzivS69NGvWLPTggw/6nxs2bBiqWbNmivN4bpcuXcL21a1bN9S1a9eo901OTrZrIreyPWeHyvV+M12bEEIIIYTIen7++Wd7X+PfRKGIWzaza9cuixo1btw4zKqeCNzu3bv980gX9EhKSrJ/v/vuO/uXKFmjRo2i3p+oDRG6unXr+vtKlCjhKlWq5Ed0+Dd4HOrXrx/2mUjQ448/HjbGzp07W8SK8XuQzplIguPAZCQ4biB6FVybeOYbjYxeF6yvGzx4sKVIMk7WZ8mSJZa6GYSoXFrz9D6n9ty+fftaqqe37d27N83xCSGEEEKI4wuZk2Qzv/76q/1Lih31UUGo2fLEW9Dkg5oorzYOssOan3FS09aiRYsUx6g/86C2LTth7t565CRPPfWUGzt2rKVhIt5YB2rZMCAJkoj14XfBJoQQQggh8i+KuGUzQVMNrPeDW9myZeO6BxEn6tqiUblyZTMQoY4taKSBuQnP9s4JHoc1a9aEfcaUhGsix8iWlS6OwXFgHrJjxw4bb2rEM1+idETI0ntdLKhZu+GGG6w2EXMZDEYYa0bm6X2ONU8hhBBCCJG/UcQtmzn55JPNHANDEiJoDRo0sPQ3hAAGGuXKlUvzHsnJyZYqWaFCBTO1QIC89dZbrnfv3u6cc84xQUFaI4YiPK9Pnz4W3WM/9OjRw11yySVu5MiRto8Uv8WLF4c9Y8CAAe66664z58lbbrnFxBrpkxhxRBqZJBLSM0lZLF26tBm04MZ44403pnp+PPP9xz/+YRFExC4iCxfIeK6LBdfjBLlq1SpzpfzXv/7lvv3227hEH8yZM8fSTPn+X3rpJTM1mTx5cjpWyrktg5rYb0YIIYQQQhz/KOKWA1Abhasg7pJEWa655hpLnaQ9QDxgM8+LP26INWrUcFdeeWWYm+HUqVOttgrhRe0UfhgIOy/9Epv9SZMmWaofQmbp0qWuf//+Yc/Auv7NN9+0Y7Qd4JrRo0fHJSwzw7Bhw8zJkvH/97//tbYHafU+S2u+OEt26dLFtWrVypwqR4wYEdd1sWC9iEqyTnwfZcqUiSkwIyENdebMmRY9pb6RVgLxij4hhBBCCJH/+BsOJTk9CCHot0ZbAtIj6dV2PEON3vz589Ml9ILQDuDUU0+1SK0ibkIIIYQQuY+seF9TxE0IIYQQQgghcjkSbiJhkI4YbB8Q3NI6ltu49tprUx3vkCFDcnp4QgghhBAin6FUyRjceeed7qeffnKvvfZajqbGZRbMObCqZ8tK6DNHWDgahIhjHStVqlSuSp3ct2+fO3z4cNRj9G1jyymUKimEEEIIkbvJivc1uUrGAPOO40HXrlu3Llv6rSG+IgVY5PF4wVCEZt/84HOCyB57GSUrBegFyUtcgcLF0jxvz7BmCX2uEEIIIYTIfnK9cKOhcVqugllFTomGRK8BTop5DeaLU2Ne5tixYzk9BCGEEEIIcZyQ62rcsFbv3r27pfXRwwu7dXqHeTVH9Pdq27atO3DgQNg19913n11DTy3Owe7+t99+cx06dLAeXTSOXrRokX8NDZk7duxoFvxFixZ1lSpVsghbZKpkML2R59AD7ZFHHrFUOYTFwIEDw67ZuXOnu+yyy1yRIkXM3v3tt99OMce9e/e6W2+91SIw3Ie+YXv27Enx3CeffNL9/e9/t7FlNlVyzJgx9jcRRMZMfzYagXN/5hQPzz33nPUvY26sMf3dMvMdpBWpIsWUVFV44YUXbL1IW/XGwG+DtfSgzxzRLZ5FSBqr//Xr16f5rHjuDePHj7feeYhKvpMZM2aEHWe8nHP99ddbhJMecYwHWBOO890CPeCqVq1qvz361l111VW2VkIIIYQQQuQJ4QbTpk2zl2OaUtPXiz5lNWvWtJdwGkXT6BjhE3kNQo9+ZgiIrl27upYtW1rK3YYNG9zVV19tgu/QoUN2Ps2vzzrrLOuHtm3bNms43a9fPzd79uw0x8ZL+dq1a60fGA2jPXHGPVu0aGFj5/iECROsKXZkFAZRgLj44IMPbI4IUnq5EVnzoFn0559/bvemn1qimDdvnvVjo+k0IhOxgoBIC9Yegcd8GRffAwI1M99BeuE6xCx9z1g3RB0NyD3atGlj3ympoR9//LE11I6nJ1s896ZGkf5yDz74oP2HhHvuuccE6fLly8Pugyi+6aab3Keffmq92lhvYM1I/eQ/DvBv69at3V133eW2b99uIpXfTWppuUePHrU86eAmhBBCCCHyGaFcRsOGDUM1a9b0Pw8ePDh09dVXh52zd+9e3nBDn3/+uX9NgwYN/ON//PFHqHjx4qG2bdv6+/bv32/XrF69OtVnd+vWLXTzzTf7n9u3bx+64YYbwsYWfA5cdNFFod69e9vfS5YsCRUqVCi0b98+//iiRYvsufPnz7fPM2bMCFWqVCn0119/+eccPXo0VLRoUbvee27p0qVtfyIoV65caPTo0fb3qFGjQueee27o999/T9c95s2bFzrllFNC//vf/6IeT9R34LF8+XI798cff7TPU6dOtc9r1qzxz9m+fbvtW7t2rX0++eSTQy+88EK65hXvvS+++OJQ586dw65r2bJlqGnTpv5nzu/Zs2fMecDHH39s+/bs2RPX+JKTk+38yK1sz9mhcr3fTHMTQgghhBDZy88//2zva/ybKHJlxI0Ut2D6G1GNoB37eeedZ8d2797tn1etWjX/74IFC1r6WTCSROqe53zoMW7cOHsWNWDcd+LEie7rr7+OObbgcyApKcm/J9GTsmXLWvqhR/369cPOZz67du2yiJs3H9Iljxw5EjYfxp4VtX1EwHBLLF++vKXyEUn6448/0ryucePGrly5cnYdUbOXXnopReQsI99BeihUqJC76KKL/M/8DkhxZN2hV69erlOnTpZ2SKQ2uJ6ZvTf/XnLJJWHX8Nk77lG7du00n1W9enXXqFEjWxu+D1JKMS9Jjb59+5ojkbdFpnAKIYQQQojjn1wp3IIOiL/++qtr3ry527RpU9jm1ZJ5RKbEUU8U3MdnL50RZs6c6R566CGrc1u6dKndk9S3YLpiNKI9x7tnPDAfxGLkfHbs2OFuv/32qGuQSBCWpO1Rr0Z91b333mvrmJaRBkKTdMdXXnnFxCqppQgQrwYtI99BoiFNcevWra5Zs2bu3XfftRpDhGl2Es/3hqglBZZ6P8b4zDPPWM3cl19+GfV8ahGp2QtuQgghhBAif5HrXSUvvPBCqxPCYIOoSKKgjonaK4SLR3oiNNGoXLmyRUOoYULcwJo1a1LMZ9asWWaNn1Mv4Ag2xDBbt27dLLpETRZjiwXrTzSLLTk52SJSCCTqs7IDIoPU2tWpU8c+I0ARjqy7x7nnnmvbAw88YHVkU6dOtZqzzN6bf/nNtG/f3r+GzwivWHhRU8xwgiBiidixIYKJZiIyiRrGy5ZBTSTihBBCCCHyCbky4hYEYfHDDz/YSzimE4irJUuWWHQs8mU4PeAeyIs69yLa9dhjj9n9MwOCBtHAyz0pkZiPPProo2HnYKCBgQdOkhwnyoI5BcYf33zzjctqcFCcPHmyGWx88cUX7sUXXzQhh3CIBQYpTz/9tEUHv/rqKzPxIHKWWcfL9ED0DtMTjF8wH8GhsV69eia2SP/EjZS1ZHyIKr7PoKjL6L3h4YcftrXDNZJo77/+9S/36quvWtQ2FqwrIo31+/777y3iyjOGDBlivz9Sc7kPx+IdqxBCCCGEyH/keuFGvRgv4Yg0XAmpC8JynmhPgQIZHz6ugESKWrVq5erWresOHjwYFn3LCIyHqAkighd+6q1wKgxSrFgx9/7775sdP8/nZZ10TWrcsiN6wrpRU0Wkh5q0ZcuWuQULFlg9WlrXITBw+GTMOGaSNlmlShWXXbB2uHSSUsr4qQ8keumlH/IdtmvXzsQzrqO0kMDZMbP3Btoz4Ag5cuRImzOunETzaIOQViNvxoDDJTV+iEu+Z34DTZs2tbH279/fjRo1ysYrhBBCCCFENP6GQ0nUI0LkIoh2IdiDNXV54d5ZAe0AaA6PUYlSJYUQQggh8sf7Wq6PuAkhhBBCCCFEfkfCLQ9ALVywHUJwI0UwtWNsRJNIc8zMM9gSTZcuXVJ9FscSCSmIqT2LWjMhhBBCCCFyO0qVzANQM7dv375Uj2EukhoffvhhXGmAsZ4BFStWdImEXm6EkKNBOBnXzUSlODIv5hcNeuix5SWUKimEEEIIkf/e13J9OwDx/+z7MyqcEG6ZfUZaPd4yAsIsNXGGEQ2OlZkxn4k0CDkeuSB5iStQuFia5+0Z1ixbxiOEEEIIIbIOpUomEMTG0KFD3T//+U8TQjSonjt3rh3Dph5b+HfeecfVrl3bXAzpI0e/sCA4PF500UWuSJEi1jYg2IPsxx9/NNfE0047za4nBRBr+sgoFI6VHOdanBYjef31161nG88oX768uR7Sx8yDcWJ7f/3111tD6UhnzPTgzXvhwoXmYskzsdmnHUFwzKRzvvHGG9YXjYbT2OTHmi/3pSUE/xWD+7PRgDvedUoN7lGjRo2wfWPGjLE+gh60CsBlEodJ+vXhyEnbiqDAnTFjhn3PNC4vU6aMuVUSZfRgjLSGKFmypP1WaE+BS6UQQgghhBDRkHBLIIg2+pthlb9161ZrAn3HHXe4FStW+OfQ1w3rd3p40dD6rrvu8o8hbhBb2MRv3LjRRJ7XR8wTDFyHwFm9erUjy5VzPcFAfzBaC2A5T7+1K664wj3xxBMpatkQNffff7/btm2b2dojnCLFGQKGsdCYOzjGjEIfNOZNbzXECs2/g0Ln0KFDbvjw4e7555+3tSMaF2u+iF4EFaFnGp6zeT3V0lqnRLB8+XLrKci/06ZNszVk8+BZgwcPtn5+r732mtuzZ4+Ny4O+gaz/okWL3Pbt200oI9SFEEIIIYSICjVuIvMcOXIkVKxYsdCqVavC9nfs2DHUunXr0PLly6klDC1btsw/tnDhQtt3+PBh+1y/fv1QmzZtot5/x44ddu7KlSv9fQcOHAgVLVo0NHv2bPvMc5o2bRp2XatWrUKnnnqq/7lRo0ahIUOGhJ0zY8aMUFJSkv+Z5/Ts2TOUCLx5z5w509938OBBG/esWbPs89SpU+2cTZs2pWu+XBecW7zXxSI5OTlUvXr1sH2jR48OlStXzv/cvn17+/zHH3/4+1q2bGlrnRrr1q2zcf3yyy/2uXnz5qEOHTqE4v1t/fzzz/62d+9eu1fZnrND5Xq/meYmhBBCCCGyF97ZeF/j30ShiFuC2LVrl0WNGjduHOZaSASOyIwH6YIepNmBl0JHlKxRo0ZR709UhggdzcI9SNGrVKmSHfPOCR6H+vXrh30mAvT444+HjbFz584WsWL8HqT5JZLgODADCY4bTjzxxLC1iWe+0cjodemFJtw4ega/y2Aq5Mcff2xRRdJWSZds2LCh7ScFFLp27epmzpxpaZmPPPKIW7VqVcxILsWt3la2bNmEzUMIIYQQQuQNZE6SIH799Vc/3THSDIOaLU+8nXDCCf5+6rK82jiI5Q6ZyHFS09aiRYsUx6g/86C2LTth7t565CQYokQarUZLsQx+j8DYve/xt99+c02aNLHtpZdestRQBBuff//9dzuHuruvvvrKvfXWW+7tt982wU6dHHVzkfTt29f16tUrzKVI4k0IIYQQIn+hiFuCCJpq4M4Y3OJ9ySbiRF1bNCpXrmwGItSxeWA8grkJz/bOCR6HNWvWhH3GlIRrIsfIligXx2gEx4Exx44dO2y8qRHPfInS4UCZ3utigcj673//GybeiISmh88++8yeOWzYMHfppZe68847LywaF3xW+/bt3Ysvvmj1ehMnTox6P35X1PIFNyGEEEIIkb9QxC1BkA6HOQaGJEReGjRoYI6HK1eutBftcuXKpXmP5ORki7xUqFDB3XbbbSZAiMj07t3bXAdvuOEGS2vEUITn9enTx6J77IcePXq4Sy65xKI27FuyZIlbvHhx2DMGDBjgrrvuOkvhu+WWW0yskT6Jy2OkkUkiIT2TlMXSpUubQQtGHDgzpkY888XpkQgiYhcHTxwk47kuFpdffrn7/vvv3YgRI2x9WD8MRNIjllhbROUzzzxjzcRZW4xKIr+HWrVqWcrl0aNH3ZtvvhlTyEZjy6AmEnFCCCGEEPkERdwSCC/nuAVSk8RL+DXXXGOpk7QHiAdEw5w5c8wNkdqnK6+80n300Uf+ceziedlHeFEzRlQIYeel7WGzP2nSJDd27FgTMkuXLnX9+/cPewbpeogEjtF2gGtGjx4dl7DMDESfcLJk/ES0aHuAuIlFWvPFWRJh1KpVK4teIbbiuS4WfG/PPfecGzdunK0h6++5VcYLY8Fhku+SKB9zj0yBZO6kQBJlveyyy6xejpo3IYQQQgghovE3HEqiHhEiAdBvjbYEpEfSq01kHmrcMCkhoquImxBCCCFE/nhfU8RNCCGEEEIIIXI5Em4iTUhHDLYPCG5pHctt4OaY2niHDBmS08MTQgghhBAiKkqVFD7UZfXs2dP99NNPYftxRCTcGw1Cv7GOlSpVyuUm9u3b5w4fPhz1GP3l2HI7SpUUQgghhMh/72tylRRpgviKJcBymzhLjyjNbfcUQgghhBAiGhJuIqHQrDoe98ZEQR83ml9nZQ+63MoFyUtcgcLFUj2+Z1izbB2PEEIIIYTIOvLf224ugV5vtA2gVUDRokXNen7u3Lm+EyNihP5ktWvXtv5kWN/TRDoIlvpY+hcpUsT6ot10003+MVwc27Vr50477TS7ntqunTt3pogY0XOM41xL0+hIXn/9dWvazTPKly/vBg0aZP3lPBjn+PHj3fXXX++KFy/unnzyyQyviTdvWihgk88zaVdAH7TgmHGnpGVCsOl5rPly3w4dOliomvuzDRw4MO51Sm2sqd2Tvmz03qPxOuOjufnkyZPjnqMQQgghhBCRSLjlEIi26dOnuwkTJritW7da4+477rjDrVixwj+HRtWjRo1y69evd4UKFXJ33XWXf4wXf8RW06ZN3caNG03k1alTxz9+55132nUInNWrV1svM84lIgZr1651HTt2dN27d3ebNm0yy/7IBtwffPCBiRr6r23bts0aWiOcIsUZgoWxfPrpp2FjzCgPP/ywzXvdunXWE6158+b+uOHQoUNu+PDh7vnnn7e1I1Uz1nwRvWPGjLH84v3799vm9WZLa51SI9Y9WbNXXnnFPf3002779u22bpifpGeOQRCC5EkHNyGEEEIIkc/AnERkL0eOHAkVK1YstGrVqrD9HTt2DLVu3Tq0fPlyDGNCy5Yt848tXLjQ9h0+fNg+169fP9SmTZuo99+xY4edu3LlSn/fgQMHQkWLFg3Nnj3bPvOcpk2bhl3XqlWr0Kmnnup/btSoUWjIkCFh58yYMSOUlJTkf+Y5PXv2DCUCb94zZ8709x08eNDGPWvWLPs8depUO2fTpk3pmi/XBecW73WxiHbPzz//3O759ttvZ3iOkSQnJ9s1kVvZnrND5Xq/meomhBBCCCFyhp9//tne1/g3USjilgPs2rXLokaNGzcOs6MnArd7927/PFLpPJKSknyHRyBK1qhRo6j3J8pDhK5u3br+vhIlSrhKlSrZMe+c4HGoX79+2OfNmze7xx9/PGyMnTt3tugS4/cgnTORBMeBy2Nw3HDiiSeGrU08841GRq+LBd9LwYIFXcOGDTM1xyB9+/a1lExv27t3b4bGJoQQQggh8i4yJ8kBfv31Vz/d8cwzzww7Rk2UJ96CJh/URXm1cUBdXHaMk5q2Fi1apDhGbZYHtW3ZCXP31iO3kRXfC78JNiGEEEIIkX9RxC0HCJpqYFwR3DC0iAciTtS1RaNy5cpmIEIdmwfGI5ib8GzvnOBxWLNmTdhnTEm4JnKMbFnp4hgcB+YhO3bssPGmRjzzJUqHA2V6r4tFtHtWrVrVxHWwVjERcxRCCCGEEPkbRdxygJNPPtmMLDAk4SW/QYMGlgK3cuVKM7soV65cmvdITk62VMkKFSq42267zQTIW2+9ZW6G55xzjrvhhhssrRFjDJ7Xp08fi+6xH3r06OEuueQSN3LkSNu3ZMkSt3jx4rBnDBgwwF133XXmPHnLLbeYWCN9EgfESCOTREJ6JimLpUuXNoMWHDNvvPHGVM+PZ77/+Mc/LIKI2MXBEwfJeK6LRbR7sq99+/Zm0oI5Cfu/+uorS3G99dZbMzzHaGwZ1EQNuIUQQggh8gsJq5YT6eKvv/4KjRkzJlSpUqXQCSecECpZsmSoSZMmoRUrVvgGFj/++KN//saNG23fl19+6e+bN29eqEaNGqETTzwxdMYZZ4RatGjhH/vhhx9Cbdu2NfMMjC+4N2YcQSZPnhw666yz7Hjz5s1DI0eOTGG2sXjx4tDFF19s55xyyimhOnXqhCZOnOgfZ0zz589PyJp4816wYEGoSpUqNi+et3nz5piGIPHOt0uXLqESJUrYMzD8iPe6WES7JwYyDzzwgJm4MIeKFSuGpkyZEvccc6LYVQghhBBCJI6seF/7G/+T0+JRCK/HGW0JSB2kV9vxSCLmSDuAU0891aK0irgJIYQQQuQ+suJ9TTVuQgghhBBCCJHLkXATCaVLly5h7QOCW1rHchvXXnttquMdMmRITg9PCCGEEELkI5QqmQd44YUXXM+ePd1PP/3kcjuYcBAajgZh4ljHSpUqFbbvzjvvtDm/9tprLifYt2+fO3z4cNRj9F5jywmUKimEEEIIkbvJivc1uUqKhIL4ihRgkcfjZezYsZjnuJwissdeeqHX3Pz589PtFimEEEIIIUQkEm75hGPHjoU19M5Jfv/9d+uBlhb8VwqROhckL3EFChcL27dnWLMcG48QQgghhMg6VOMWAX3Vhg4d6v75z3+6okWLWh+uuXPn+o6ARFHo21W7dm3r23XxxRdbw+YgCxYscBdddJErUqSI9ee66aab/GO4CbZr186ddtppdj11VDt37kyRGknvNI5zLU2hI3n99detQTbPKF++vBs0aJD1cvNgnOPHj3fXX3+9K168uHvyySczvCaMuU2bNq5kyZK2JvQ/mzp1qn9879691qMMl0TSB+mBtmfPnrCUR6JOjOHvf/+7q1SpkuvXr5+rW7duimex3vQ4C14X/G5GjBhhDcBpYM4aBeeV1jhiwXdbp04dWyuup8cd/ddg4MCBrkaNGm7KlCn2TGrc7r33Xmu+zXjKlCljkcTgWOjnBnx/fBfeZ+9e9I2j2TrfMWMmjC6EEEIIIURqSLhFgGibPn26mzBhgtu6das1yb7jjjvcihUr/HNomDxq1Ci3fv16V6hQIWu27LFw4UJ7WW/atKnbuHGjiTwEgQdihOveeOMNt3r1aksF5FwiYrB27VrXsWNH1717d7dp0yazjo9sdv3BBx+Y+Lv//vvdtm3bTAQg9iLFGSKBsXz66adhY0wvjz32mD1n0aJFbvv27SYIEaTAuJs0aWLNqxkXTcQRNtdcc41F1jxYBwTu22+/7d58800Tgh999JHbvXu3fw7r/cknn7jbb7896jj69u3rhg0b5o/n5ZdftgbW6RlHNBC8CMSGDRva8/le7r77bhNcHoyT+dOk/JVXXnGTJ092zZo1c9988439NoYPH+769+9v3x+sW7fO/kXg7t+/3/8Mu3btcrNnzzaBz/34nSAEU+Po0aOWJx3chBBCCCFEPiNhHeGOA44cORIqVqxYaNWqVWH7O3bsGGrdurXfPHnZsmX+sYULF9o+mi5D/fr1Q23atIl6fxo7c+7KlSv9fQcOHLDGz7Nnz7bPPKdp06Zh17Vq1Sqs6XSjRo1CQ4YMCTtnxowZ1vDZg+f07NkzlAhozt2hQ4eox3guTcRpKO5x9OhRm9OSJUvsc/v27UOlS5e2/UGqV68eevzxx/3Pffv2DdWtW9f/zHU33HCD/f2///0vVLhw4dCkSZMyPI7UOHjwoK3Xe++9F/U4jbX5XTAGDxp1/+Mf/wj9+eef/j6eP3To0JjNyblXwYIFQ998842/b9GiRaECBQqE9u/fn+rzuVfkVrbn7FC53m+GbUIIIYQQ4vhswK2IWwAiIYcOHXKNGzcOs34nAheMDFWrVs3/OykpyXdTBKJkjRo1inp/olVE6IIpgiVKlLDUQY5550SmENavXz/s8+bNmy2dMDjGzp07W2SH8XuQzpkIunbt6mbOnGkpfo888ohbtWpV2FhYNyJd3lhIUzxy5EjYmlWtWjVFXRtRN6JmgM4hksW+aLAuRJ5SW9t4xxENziMSSsSuefPmZorCWgYh1ZF7exDpO//8812BAgXC9nm/g1iQbhk0PuH7JQ00MuU2GGkkldLbSAkVQgghhBD5C5mTBPj111/9dMdIR0FqqjwBEDT58NLpePEGasCyY5zUtLVo0SLFMWrePKjXSgTU4VHv9dZbb1mqI+KpW7dubuTIkTaWWrVquZdeeinFddTExRpL69atXe/evd2GDRvMdh9B0qpVq6hjSGtd4x1HapDS2KNHD0tdnDVrlqU9Mtd69erZ8UhjF773aPu830Ei4bfHJoQQQggh8i8SbgGIoPCC/PXXX1u9UyRpRW68aBz1XB06dEhxrHLlylZPRR0UpiaA8QiRFp7tnePVSXmsWbMm7DOmJFyDSUd2gfhp3769bZdeeql7+OGHTbgxFoQO5hzp7VFx1lln2TojthBuRDpTaxeAIQrijbXt1KlTiuOZGYdHzZo1bSPCRRSMaKAn3DICwg4Dk0j4ff3nP/8xoxbv+yVyR+RVCCGEEEKIaEi4BSAV7qGHHjJDEiInDRo0sNQ0jC4QA+XKlUvzHsnJyRaRqlChgrvttttMqBGpIrKE+MDpkLRGDEV4Xp8+fSy6x34g6oOjIaKIfUuWLLEoUJABAwa46667zlLubrnlFnvpJ1Vwy5YtKYxMEgHPI5pVpUoVS1fEXASBCaQ2PvXUUzZW0jcRY0TnXn31VUur5HMsuJ41w0Bk9OjRqZ5HJJE15J6kXLJG33//vRmaYOaSmXF8+eWXbuLEiebAiZhCFOP0iQFMZiC9EqHJWPkPAjiJenNBAPMdYzTCd46zJO6U6WHLoCZqwC2EEEIIkU9QjVsEgwcPNtdC3CURJ7gSkjpJe4B4uPzyy92cOXPMNZKasCuvvNLcE4MpeYgghBdRHWq7EHZe2h0RnkmTJlmdFdb4S5cutbS9INRiIZ44RtsBrkH0xCMsMwJCiSgU0cTLLrvMFSxY0GreADv7999/30QkqZusGUKK2rJ4RAXCk6gjtXlpNarme3nwwQdNSPIc0iq9mrLMjINrP/vsM3fzzTe7c8891xwlSQW95557XGbAeZR0S2z/ieR5EClljLiJXn311bauzz33XKaeJYQQQgghjm/+hkNJTg9CiPwCLRpee+01M7HJKETpaE5ONFgRNyGEEEKI3EdWvK8p4iaEEEIIIYQQuRwJt3xCly5dwtoHBLe0jh0PpDY/Nhp2CyGEEEIIkZtRqmSCeeGFF1zPnj3dTz/95HIT1IIRso0G4dtYx1JzegxCHzTmTBpgdqUcjh8/3uY1f/78NOvj6PGWGpjDpNVuILvnFwulSgohhBBC5G6y4n1NrpL5BMRXLAEWjziLBWYq2fXfAGjGTR87BBvGLLg14uCIYGaLRna2ToA9e/aYoc3GjRvNpCYruCB5iStQuNj//8xhzbLkOUIIIYQQIueRcMuFHDt2LEVz55wCm35cJdOC/6KQXXj99LD+9xqgCyGEEEIIcTyTp2vc6LWGbT+RDVLdsM+fO3euHXvvvffspZ4+WrVr1zbLd5pe06MryIIFC8xSn95aZ5xxhrvpppv8Yz/++KP18iKiw/XXXnut9feKTI3Egp7jXIu1fSSvv/66NYjmGeXLl7doEf3dPBgnaX/0EStevLh78sknM7wmjJmeZjTMZk3oHUcLAo+9e/daz7D/+7//c6effrqJH6JDwZRA0g4ZAz3NaArdr18/V7du3RTPYr3pmRa8LvjdjBgxwiJd9DBjjYLzSmscsVIkmzdvbn/Tv461owUDPdvov8dnT8zx3XB/0htZB9afVgo8OxY0ze7Vq5ddW6JECesDFxlNpLceff68c2jvEGzQ7rWPoA2AN0ZYt26dNRrnt4bYpQH5hg0b0py3EEIIIYTI3+Rp4YZomz59upswYYI1YubF/Y477nArVqzwz3n00Uetn9b69etdoUKF3F133eUfoz8bYot+WqS0IfLq1KnjH0eMcB092VavXm0v75xLRAzWrl1rvcK6d+9u9u5XXHFFigbYGF8g/u6//363bds2a7yNoIgUZwgSxvLpp5+GjTG90OuM5yxatMhSChGEiARg3AgXGn8zLhqLY85Brzoiax6sAwKXHmT0i0MI0osuKExY708++cTdfvvtUcdB37dhw4b543n55Zdd6dKl0zWOaNAg3ROi+/fvt40m2zTYRkR6+zzoD8da8zvhOdSp0Rg9Fvxe+I6mTJniPvzwQ/fDDz9YWmaQ3377zcQdvw/WCxHJ94dgBa9337Jly/wxwi+//GLNt7nvmjVrTFDym2J/atD0nDzp4CaEEEIIIfIZoTzKkSNHQsWKFQutWrUqbH/Hjh1DrVu3Di1fvpwQSWjZsmX+sYULF9q+w4cP2+f69euH2rRpE/X+O3bssHNXrlzp7ztw4ECoaNGiodmzZ9tnntO0adOw61q1ahU69dRT/c+NGjUKDRkyJOycGTNmhJKSkvzPPKdnz56hRNC8efNQhw4doh7juZUqVQr99ddf/r6jR4/anJYsWWKf27dvHypdurTtD1K9evXQ448/7n/u27dvqG7duv5nrrvhhhvs7//973+hwoULhyZNmpThccRi/vz5tmZBypUrFxo9enTYvqlTp9p5a9as8fdt377d9q1duzbV+/PdjBgxwv987Nix0FlnneXPLxrff/+93ffTTz+1z19++aV93rhxY8y5/Pnnn6GTTz45tGDBglTPSU5OtntFbmV7zg6V6/2mvwkhhBBCiNzBzz//bO9r/Jso8mzEDZdAoimknQWt3YmsBCND1apV8/9OSkqyf3EiBKJkjRo1inp/olVE6IIpgqTEkTrIMe+cyBTC+vXrh33evHmzRYKCY+zcubNFYRi/B+mciaBr165u5syZZohBit+qVavCxsK6EenyxkKa4pEjR8LWrGrVqinq2oi6ETUDtOYrr7xi+6LBuhAlSm1t4x1HIuA7JBXW47zzzrP0Rsb49ddfh30vQ4YMMecfvpvg98o9Ir8fUmZbt25tqa84BWGOAtwzFt9++619/0TaSJXk2l9//TXmdUQvGZe3pZXqKYQQQgghjj/yrDkJL7teuiN27kGoqfIEQNDkw6t98tLZ0rKAT9Q4qWlr0aJFimPUXHlQ25YIqMOj3uutt96yVEfEU7du3dzIkSNtLLVq1XIvvfRSiuuoiYs1FkRK7969rR7r8OHDJh5atWoVdQxprWu848hqqOFDvHsgHuOFOrty5cq5SZMm2X34TV1wwQVppnqSJkkdJC6cXM9vFbEf6zrOYRNCCCGEEPmXPCvczj//fHuZJVKBwUMk8URuiMZRn9ShQ4cUxypXrmwGItSxYWoCvHBT+8WzvXM4HoS6pSCYknBNdtrRI34QCGyXXnqpe/jhh024MZZZs2aZ9X96+0lQQ8Y6I7YQbkQ6U2shQDQJ8cbadurUKcXxzIwjNYgQYioSCd8hdWhe7SLfBXVufHdE0qJ9L0Rm+V4vu+wy/x4ff/yxjTv4O0C0sb5AzVrkeCByTNTZPffcc1bXBgjgAwcOZGjOWwY1UR83IYQQQoh8Qp5NlSTNDqMKDEmmTZtmQo1o0DPPPGOf4yE5OdlS/viX1DmMQYYPH+6LD5wOSWvjpZz0PoxPiO6xH3r06GHugogiUueeffZZ+xxkwIABlr5J1A1DD55DKmP//v2zYFX+3/NwsSQVkedhLoJIAVIbMSph/JiCfPnll+a+yTy++eabNO/N9Yx9zpw5qaZJepFEonOkanqpqwjayZMnJ2Qc0SBV8f3333f79u0LE0JEXO+77z4TYogvDGfo/RY0oYkEIxmMVXCj/Oyzz9y9994b1lAdl1HSZidOnGjr/O6775pRSRBEKeKV3wPpkaQ4er+rGTNm2O+AMbEW2RH5FUIIIYQQeZs8K9xg8ODB5lqIuyTiBFdCUic9K/a0wKIdEYJrJDVhV155pe8GCLgXktKH1TvpbNR2kYLopV8iAIi6kPaGNf7SpUtTCDLcExFPHKPWimtGjx5taXJZAZEeaqKIJhIxKliwoIktoGUB4gZrflI3WTNcMaktiydyc8stt1i0idq8oPV/NPheHnzwQROSPIe0Sq+2MLPjiAZ1hLQTqFChQli6Jc9CROJ+eckll1gtG9G+WDDutm3bWsSS753/SBBsE4GDJGuKECQ9kv948NRTT4Xdg2je008/bS6ipFJ6Yh/xSssGonc8A7Ga2ebnQgghhBDi+OdvOJTk9CCEyAqw9O/Zs2dYtOx4gHYAGJsQxVOqpBBCCCFE/nhfy9MRNyGEEEIIIYTID0i45UK6dOkSZlMf3NI6llYECiv83E5q82OjJu54grTTm2++2f5LDK6nx1t0UAghhBBCJAalSuZCqAUjvBoNXvBjHYtVL5VXUgcx/EgNzGGimXnklblFMn78eDPHweAEw5bSpUv7bStSQ6mSQgghhBC5m6x4X8uz7QCOZxBfsQRYTppZHDt2LKw3XlYQtOjHTh8hgyHI8QiOm5izYHKSXi5IXuIKFC7mf94zrFmCRyeEEEIIIXILx+fbcC6Ahsy4XeJwSYQI18m5c+faMazvESP0Oatdu7Y5H9Irjt5gQRYsWGBOlNjrE40JOhviTNiuXTuzpud6Gm/TkiAyCoVzI8e5FkfISGgdgMMhzyhfvry1LaBvmQfjJCp0/fXXW2PuJ598MsNr4s0b509cL3kmLptbtmxJkc6J02ewV1+s+XJfevHxXzS4P9vAgQPjXqdYzJs3z1WpUsXGQcuBUaNGhR2nJxsW/8yFaBnOm0HX0u7du9vGf3HhO8Rt0wtyc5z74bDJmPkshBBCCCFEVEiVFInniSeeCJ133nmhxYsXh3bv3h2aOnVqqHDhwqH33nsvtHz5ct7cQ3Xr1rXPW7duDV166aWhiy++2L/+zTffDBUsWDA0YMCA0LZt20KbNm0KDRkyxD9+/fXXhypXrhx6//337ViTJk1CFStWDP3+++92fM2aNaECBQqEhg8fHvr8889DY8eODf3f//1f6NRTT/XvwbWnnHJK6IUXXrAxLl26NPSPf/wjNHDgQP8cxlmqVKnQlClT7Jyvvvoqw2vizZtx86xPPvkkdN1119kzvXGzTieccIKtxcqVK0OfffZZ6Lfffos536NHj4bGjBljc9m/f79tv/zyS1zrFIv169fbGj7++OO2hoytaNGi9i+sW7fOvqOXX345tGfPntCGDRtsnT0aNmwYOumkk0L333+/zePFF18MFStWLDRx4kQ7fvDgwVDnzp1D9evXtzHzOR5+/vlnW8eyPWeHyvV+09+EEEIIIUTuwHtf499EIeGWBRw5csRe0FetWhW2v2PHjqHWrVv7AmbZsmX+sYULF9q+w4cP22de5tu0aRP1/jt27LBzETYeBw4cMFExe/Zs+8xzmjZtGnZdq1atwoRbo0aNwsQgzJgxI5SUlOR/5jk9e/YMJQJv3jNnzvT3IVYY96xZs+wzoohzEFnpmS/XBecW73WxuP3220ONGzcO2/fwww+Hzj//fPt73rx5Jhb/97//Rb0e4YZo/Ouvv/x9vXv3tn0eiDrOS+v3xP/pvW3v3r0SbkIIIYQQ+Uy4KVUyi8w1cAts3LhxmCPi9OnTrabJg3RBj6SkJPvXa1K9adMm16hRo6j33759uzV4rlu3rr+vRIkSrlKlSnbMOyd4HGgmHWTz5s3WuDo4xs6dO7v9+/fb+D1I50wkwXGcfvrpYeP2mogH1yae+UYjo9cFr6dpdxA+k2pJ7R3fL43USTGlmfZLL70Utm5AKmjQbIS5e9fHCym3pFp6W9myZeO+VgghhBBCHB/InCQL+PXXX+1farlwQQxCrZQn3oImH97LPbVxEM05MSvGSU1bixYtUhyjZsuD2rbshLmn5ayYGzj55JPdhg0brMZu6dKlbsCAAVZbt27duoS2Xejbt6/r1atXmEuRxJsQQgghRP5CEbcsIGiqgUNicIv3hZuIE+Yl0cCFEAORtWvX+vswHsHchGd75wSPw5o1a8I+Y0rCNZFjZMtKF8fgODAP2bFjh403NeKZL1G6yChWPNfFgutXrlwZto/P5557ritYsKB9JqJ31VVXuREjRrhPPvnE7dmzx6z9PaJ9B5iZeNfHA78lbGSDmxBCCCGEyF8o4pZFkZiHHnrIPfDAAxZBa9CggTke8tLPSzfpdWlBby9SJStUqOBuu+02EyBvvfWW6927t73433DDDZbW+O9//9ue16dPH4vusR969OhhaX0jR460fUuWLHGLFy8OewYRouuuu86cJ3FDRKyRPonL4xNPPJFl60N6JimLuDA++uij5rZ44403pnp+PPPF8ZEIImIXB08cJOO5LhYPPviguXoOHjzYtWrVyq1evdo9++yz5iQJb775pvviiy/cZZddZq6VfD9836RieiDeiZbdc889Fp175plnUjhTZpQtg5pIxAkhhBBC5BcSVi0nwsCQAqfDSpUqmUtiyZIlzdFwxYoVvknHjz/+6J+/ceNG2/fll1/6+zC/qFGjRujEE08MnXHGGaEWLVr4x3744YdQ27ZtzZADsw3ujRlHkMmTJ4fOOussO968efPQyJEjUxh44HqJgyPnYLRRp04d3/UQGNP8+fMTsibevBcsWBCqUqWKzYvnbd682T8nmslIvPPt0qVLqESJEvaM5OTkuK+Lxdy5c82MhO/w7LPPDj311FP+sQ8++MCMRU477TS7d7Vq1XyTFeDYvffea+NibTmvX79+YWYl8ZiTZEexqxBCCCGESBxZ8b72N/4np8WjyB9QC3bFFVdYemQia8ByK/Rlq1GjhhszZkxC70uNGyYlRHEVcRNCCCGEyH1kxfuaatyEEEIIIYQQIpcj4SbSRZcuXcLaBwS3tI7lNq699tpUxztkyJCcHp4QQgghhBA+SpXMZ7zwwguuZ8+e7qeffsrQ9fSZI/QbDcLAsY6VKlXK5Sb27dvnDh8+HPUY/eXYciNKlRRCCCGEyN1kxfuaXCVFukB8xRJgOSnO0itKI3vsJZo777zTxvLaa69l6XOEEEIIIcTxj4SbSDfHjh0Lax6e1dCfjYbcWdlbLi9yQfISV6BwMf/znmHNcnQ8QgghhBAi69CbcBZCT6+hQ4e6f/7zn65o0aLWX2zu3Lm+wyJihL5jtWvXtr5jF198sTWHDrJgwQLrJVakSBHrd3bTTTf5x3BnbNeunfUQ43pqtnbu3JkiCkWfNo5zLQ2oI3n99detGTfPKF++vBs0aJD1jfNgnOPHj3fXX3+9K168uHvyySczvCbevBcuXGhNxnlmvXr1rHdccMy4Tr7xxhthzcxjzZf7dujQwcLR3J9t4MCBca9Tanz11VeuefPmdi1zr1KlivVr89i6dav1wiMETp+4Sy+91O3evduePW3aNFtbbzyMkQbd/D1z5kz7vpn/BRdc4FasWJHhNRVCCCGEEMc/Em5ZCKJt+vTpbsKECfaCT0PuO+64I+wlnQbUNGRev369K1SokLvrrrv8Y4gbxFbTpk3dxo0bTeTVqVMnLBWP6xA4NIemXJFziYjB2rVrXceOHV337t3dpk2bzIo/srH2Bx98YKLm/vvvd9u2bbNG1QinSHGGEGEsn376adgYM8rDDz9s8163bp0rWbKkiSNv3HDo0CE3fPhw9/zzz9vakYIZa76IIGz3EVD79++3jSbo8axTLLp16+aOHj3q3n//fZs7Y8K8xKuRo/k2wvLdd991H3/8sa0Nopdn33rrre6aa67xx8MYg/OnwTffa/369W3+0UQ18HzypIObEEIIIYTIZySsI5wI48iRI6FixYqFVq1aFba/Y8eOodatW/vNqJctW+YfW7hwoe07fPiwfa5fv36oTZs2Ue9PE2nOXblypb/vwIED1gh69uzZ9pnnNG3aNOy6Vq1ahTW4btSoUWjIkCFh58yYMSOUlJTkf+Y5PXv2DCUCb94zZ8709x08eNDG7TWvpgk352zatCld843WvDue62JRtWrV0MCBA6Me69u3b+if//xn6Pfff496vH379qEbbrghbB8N1hnPsGHD/H3Hjh2zRunDhw+Peh+aiXNN5Fa25+xQud5v+psQQgghhDh+G3Ar4pZF7Nq1y6JGjRs3DrOZJwJHKp0H6YIeSUlJvnMjECVr1KhR1Ptv377dInR169b195UoUcJVqlTJjnnnBI8D0Z0gmzdvdo8//njYGDt37mwRIsbvQTpnIgmOA/fG4LjhxBNPDFubeOYbjYxe59GjRw+LUl5yySUuOTnZffLJJ/4xvh9SIzNS7xecP+NjfVMbT9++fS0F1Nv27t2b7ucJIYQQQoi8jcxJsohff/3VT3eMdC8ktc4Tb8GXfmqfvNo4oC4uO8ZJTVuLFi1SHKP+yoP6ruyEuXvrkZN06tTJNWnSxL7HpUuXWvorKZ733Xdftnw/3u+FTQghhBBC5F8UccsigqYaFStWDNvKli0b1z2IOFHXFo3KlStbLRV1bB7USGFuwrO9c4LHYc2aNWGfMSXhmsgxsmWli2NwHJiH7Nixw8abGvHMlygdDpTpvS4t+L5oIP7qq69aXdqkSZP874cawdRq5aKNJ9r8GR/1cbHmL4QQQggh8jeKuGUROAxiUIEhCRG0Bg0aWJrbypUrzUCjXLlyad6D1DxSJStUqOBuu+02e8HH0bB3797unHPOcTfccIOlNWIowvP69Olj0T32e2l+pPiNHDnS9i1ZssQtXrw47BkDBgwwV0ScJ2+55RYTa6RP4vIYaWSSSEjPJGWxdOnSZtCCY+aNN96Y6vnxzPcf//iHRRARuzh44iAZz3WxoC8cLpTnnnuuCczly5f7AgvTl2eeeca+G9IZabKIIMNAhlRMxsOaIxKZK8c9xo0bZ2PjXqNHj7Z7p9f0ZcugJmrALYQQQgiRX0hYtZxIwV9//RUaM2ZMqFKlSqETTjghVLJkyVCTJk1CK1as8E06fvzxR//8jRs32j4MLDzmzZsXqlGjRujEE08MnXHGGaEWLVr4x3744YdQ27ZtzZADsw3ujRlHkMmTJ5vxBcebN28eGjlyZAoDj8WLF4cuvvhiO+eUU04J1alTJzRx4kT/OGOaP39+QtbEm/eCBQtCVapUsXnxvM2bN/vnRDMZiXe+Xbp0CZUoUcKegalHvNelRvfu3UMVKlQIFS5c2L4/7oO5iQfjvvrqq82I5uSTTw5deumlod27d9ux7777LtS4cePQSSedZONh7p45ycsvv2zzZv7nn39+6N13383RYlchhBBCCJE4suJ97W/8T06LR5F/oJcZbQmIMNGrLb9BHzf6+tEGoEaNGhm6B+0AiN4RwVXETQghhBAi95EV72uqcRNCCCGEEEKIXI6Em0g3GHUE2wcEt7SO5TaoX0ttvEOGDMnp4QkhhBBCCGEoVVKE8cILL5ghx08//ZTqOfSZI/wbDULBsY6VKlXK5Sb27dvnDh8+HPUY/eXYchtKlRRCCCGEyN1kxfuaXCVFukF8xRJgWSXOMlMfRk+4+fPnp3CujOyxlx5wjfzqq69S3O+bb77xjyOC2YLnr1692tWrV8+/huM086b+TwghhBBCiGhIuImEQ1+zYGPx4xnaGtBqwKNgwYIxz6epOe0cVqxYkelnX5C8xBUoXMz+3jOsWabvJ4QQQgghci+qcctB6O82dOhQiyIVLVrUeo/NnTvXjhF9IUpET7LatWtbT7KLL77YeoIFWbBggbvoootMENAL7aabbvKP4dzYrl07d9ppp9n11HPt3LkzRWokPdw4zrU0p47k9ddft0bdPKN8+fJu0KBB1lPOg3GOHz/eXX/99a548eLuySefzPCaMOY2bdq4kiVL2prQ62zq1Kl2jHWCmjVr2jMvv/xy+7xu3TrXuHFjmz8h6YYNG7oNGzb49yTSBcyP67zP8cwtLegLV6ZMGX9j3LG4++67rdcb/fiEEEIIIYSIFwm3HATRNn36dDdhwgS3detWa9Z9xx13hEVjaE49atQot379eleoUKGwJs0LFy40MdK0aVNLH0Tk0fzZ484777Tr3njjDUvPo5yRc4mIwdq1a13Hjh2tkTSpetj0Rzbd/uCDD0z83X///W7btm3WxBqxFynOBg4caGP59NNP091IOshjjz1mz1m0aJHbvn27CUIEGXz00Uf277Jly9z+/fvdq6++ap9/+eUX1759e/fhhx+aKELsMU/2e8IOEIBc532Od26JBPGJSQsNuxHu8XD06FHLkw5uQgghhBAin5GwjnAiXRw5csSaNq9atSpsf8eOHUOtW7f2G1UvW7bMP7Zw4ULbd/jwYftcv379UJs2baLenwbTnLty5Up/H42jaUA9e/Zs+8xzmjZtGnZdq1atwppfN2rUKDRkyJCwc2bMmBFKSkryP/Ocnj17hhIBTcI7dOgQ9ZjXvJpG5bH4888/rRk2Tb5jNRGPZ26xKFeunDXQLl68uL+NHTs27Pjo0aNTfKYxN+ObPn267b///vtDDRs2TPU5NBJn/JFb2Z6zQ+V6v2mbEEIIIYQ4vhtwq8Yth9i1a5c7dOiQpfgF+f333y0V0KNatWr+30lJSb6rI+mNRMmC9VVBiFYRoatbt66/r0SJEq5SpUp2zDsnmFoJ9evXd4sXL/Y/b9682a1cuTIsCvXnn3+6I0eO2PhJsQTSORNB165d3c0332ypjldffbWZiZAiGotvv/3W9e/f39JLWRvGx9i+/vrrmNfFO7dYPPzwwxbZ9PCig7EgnfKhhx5yAwYMcK1atUrzfKJzvXr18j8TcStbtmya1wkhhBBCiOMHCbcc4tdff/XTHSOdDQsXLux2795tfwdNPqjPAi/Fjhqw7BgndV8tWrRIcYy6MA9q2xIBdXg4L1ID9vbbb7tGjRq5bt26uZEjR6Z6DWmS1OaNHTvWlStXztYPAYoITsTcYoFQq1ixoksvCLHnnnvOtrRgPmxCCCGEECL/IuGWQ5x//vn2Mk5UCDONSDzhFguicdS1dejQIcWxypUrm8kGdWxexApxg7kJz/bO4XgQasSCYNzBNRkRJxmFiBRijO3SSy+1qBbC7cQTT/SjYkGImiGAqGuDvXv3ugMHDoSdgwCOvC4n5uZBg2/q+agNxNRFCCGEEEKIWEi45RC4EZIuhyEJEbQGDRpYgz5ECE36iBylRXJyskWkKlSo4G677TYTakSqsJvHoOOGG26wVEpMN3henz59LLrHfujRo4e75JJLTBSxb8mSJWFpkkA633XXXWepmbfccosrUKCApRhu2bIlhZFJIuB5tWrVclWqVDFTjjfffNMEptcfjigjYzzrrLMsKoaLJHOdMWOGpWuSRojQi4xG4iSJyGW+CGacNrN7btEcJkePHu1efvnlsJTWeNkyqIkacAshhBBC5BPkKpmDDB482KIuuEsiTq655hpLnfRs79MCO/w5c+aYayQNqa+88krfedFzUUQEIU5IHcSjA2HnpV/SBHrSpEmWYkgrgqVLl1qtWJAmTZqYeOIYbQe4BrERj7DMCETVqOkimnjZZZdZX7SZM2faMWr2nn76aROif//7330BOnnyZGsjQAStbdu2Jkgjm4DjzEnqJbVhXg1hds8tEr4HfgPU1AkhhBBCCBGLv+FQEvMMIUSugqgikUYitIq4CSGEEELkj/c1RdyEEEIIIYQQIpcj4SYSDg2mMd+ItqV1LLfw0ksvpTpO6u+EEEIIIYTITpQqmQ954YUXXM+ePd1PP/2UJfenlxrh4WgQKo51LLI2Laf45ZdfrD9carVp2VUHFw2lSgohhBBC5G6y4n1NrpIi4SC+YgmwWMf27Nlj5iwbN240w5X0QJ+7+fPnW9PuzIILJxtulPSVA5wqcfC8//77XadOnfxzafx9xRVXRL3P/v37XZkyZezvH374wT3++OM2RvbTAw5DGloC4GyZXi5IXuIKFP5/TcL3DGuWwZkKIYQQQoi8gFIlRYY4duyYyy8gthBatAm44447rMXCokWLUpxHTzjOC26eSEW04Vq5bNkyN2HCBLdr1y5zy+RfHC2/+OKLHJiZEEIIIYTIK0i4ZTH0aMPunygSERts9+fOnetHaogS0V+MHmTFihWzZtkIgCALFiywl3v6lhGluemmm/xj2OC3a9fO+pJx/bXXXut27tyZIjWSiA7HuZZG3JG8/vrrZqfPM8qXL+8GDRpkfeE8GOf48eOtWXTx4sXdk08+meE1Ycxt2rSxRtusCX3YaF0AXisELPt5Ji0PYN26da5x48Y2f8LONC3fsGGDf08iY8D8uM77HM/c0oLIG1EzrqVH3umnn26tBSJBpHFecKM3HDz66KPuP//5jwk3viO+D9od0DuP1Mtu3bpleD2FEEIIIcTxj4RbFoNomz59ukVZtm7dag23idqsWLHCP4eXevqMrV+/3nqV3XXXXf4x+rohRpo2bWrpg4i8OnXq+MfvvPNOu45ebqtXr7ZebZzrRcTWrl3rOnbs6Lp37+42bdpkKX2RzaU/+OADE3+kAG7bts36pCH2IsUZKX2M5dNPPw0bY3qhdx3PIWq1fft2E4QIMvD60CFwiFi9+uqrfs1Z+/bt3YcffujWrFljYo95st8TdoAA5Drvc7xzi1eEz5s3z4Qn/ebScx3RNcSqlzbpgXC99957TcARlYsGjcjJkw5uQgghhBAin4E5icgajhw5EipWrFho1apVYfs7duwYat26dWj58uUYw4SWLVvmH1u4cKHtO3z4sH2uX79+qE2bNlHvv2PHDjt35cqV/r4DBw6EihYtGpo9e7Z95jlNmzYNu65Vq1ahU0891f/cqFGj0JAhQ8LOmTFjRigpKcn/zHN69uwZSgTNmzcPdejQIeqxL7/80p61cePGmPf4888/QyeffHJowYIFYWOcP39+2HnxzC0W5cqVC5144omh4sWLhwoVKmTPOP3000M7d+70z/G+R84Jbueff74d/+9//2vHR48eHfUZr776qh1fu3Zt1OPJycl2PHIr23N2qFzvN20TQgghhBC5h59//tne1/g3UcicJAuhfunQoUOW4hfk999/t1RAj2rVqvl/JyUl+c6MpNMRJaOmKhpEq4jQ1a1b199XokQJV6lSJTvmnRNMrYT69eu7xYsX+583b97sVq5cGRaF+vPPP92RI0ds/KRYAumciaBr167u5ptvtlTHq6++2sxESBGNBQ6P/fv3t/RS1obxMbavv/465nXxzi0WDz/8sEU2ieTxNxGyihUrpjiP6B5plR6kQAZJy8A1tShe3759Xa9evfzPRNzKli2b5riFEEIIIcTxg4RbFvLrr7/66Y5nnnlm2LHChQu73bt3p3jBpz7LS6/zUumyY5zUfbVo0SLFMerCPKhtSwTUeOHU+NZbb1mtWKNGjazGa+TIkaleQ5oktXljx441K37WDwGKCE7E3GJBGidCjW3OnDmuatWqJmLPP//8sPOoz/u///u/FNdTy8d+T0ynJsC9+r5ImCubEEIIIYTIv0i4ZSG82PPCTVQIM41IPOEWC6Jx1LV16NAhxbHKlSubyQZ1bF7ECnGDuYknKjiH40GoEQuCcQfXRIsiZRWIGcQY26WXXmqRLISbF3UiKhaEqNlzzz1ndW2wd+9ed+DAgbBzEMCR1yV6bkS6WrVqZVEwTE/iAYOSW2+91Zp641AZrHM7fPiwzYuoKKYr6WHLoCbq4yaEEEIIkU+QcMtCSJt76KGHzJCECFqDBg2sCR8ihBfueJo4JycnW0SK/mG33XabCTUiVbgbYtBxww03WColphs8r0+fPhbdYz/06NHDXXLJJSaK2IcJRjBNEgYMGOCuu+46S8285ZZbTGiQYoj9faSRSSLgebVq1XJVqlQx440333zTBKbnzEiUkTGeddZZFhVD0DDXGTNmWKSLVEGEXmQ0EidJRC7zRTDjtJkVc8Po5IILLjBTmGD6KCmcpGAGIXUVQUmqJmMjbXbEiBF2/Zdffmnpn4yJSKIQQgghhBCpkrBqORGVv/76KzRmzJhQpUqVQieccEKoZMmSoSZNmoRWrFjhm1r8+OOP/vmYcrAPkw6PefPmhWrUqGEmGWeccUaoRYsW/rEffvgh1LZtWzMbwZSEe2NaEmTy5Mmhs846y45jDDJy5MgwcxJYvHhx6OKLL7ZzTjnllFCdOnVCEydOjGn8kVEGDx4cqly5sj0Lo48bbrgh9MUXX/jHJ02aFCpbtmyoQIECoYYNG9q+DRs2hGrXrh0qUqRI6JxzzgnNmTPHjEOChh9vvPFGqGLFimYiwrF45xaLyGd4sM7XXnut/e19j9G21atX+9d8//33ofvuu8/mVrBgQTvOuA4ePJjjxa5CCCGEECJxZMX72t/4n9RlnRAiq5g8ebIZncyaNcsMWuKFiCNRSKK3SpUUQgghhMh9ZMX7mvq4CZFD0F+P/m6Yk1DrJoQQQgghRGpIuIkM0aVLF3fSSSf5G3VcOCPyd+Sx4MaxWOCq+dprr2XLHDALSW2c1N8Fa+fGjBmToWfQRiBWNA1TEoxOssM9VAghhBBC5F1kTiIyBO6IGK94/PLLL9anjFAwW/BYkNyU2nf99deH9cALEtmDLaNgOqJsZCGEEEIIkVkk3I4j6GmWWhPnRIP7I1us47l9DXDhDDbMzgrSa/GfHi5IXuIKFC7m9gxrlmXPEEIIIYQQuQOlSuZhLr/8cte9e3fXs2dPaxLdpEkTs7mnwTXpfqVLl3Zt27YN63fGNffdd59dg10+50yaNMn99ttv1isOIUPPs0WLFvnX0BuNeiwaRJPSV6lSpRT29ZEpgTyHVgSPPPKIO/3006132cCBA8Ou2blzp7vsssvM8p++czTjjoR+bfRAo4E196GlwZ49e1I8F7v9v//97za2zIClf/PmzW2ezJd0yiBEEmkv4EEKJemdwRYLrN/zzz+f4XURQgghhBAiEgm3PM60adMswkRvuGHDhrkrr7zS1axZ03qMISa+/fZbEz6R1yD0PvroIxNxXbt2dS1btrQm3hs2bHBXX321Cb5Dhw7Z+fSgo6fanDlz3LZt26w3Wr9+/dzs2bPTHFvx4sWtATi9y0iv9MQZ92zRooWNneMTJkyw3nRBjh07ZmIUMfnBBx/YHBGk11xzjUXWPOiPRpNt7k1PuMyA0EIsLl++3M2dO9eaYyPmPGik/uGHH/qNvlesWGFr+d5779nnffv2WWN1BFpG1kUIIYQQQoioJKyxgMh26HFWs2bNsP5oV199ddg5e/futR4Sn3/+uX9NgwYN/ON//PFHqHjx4tYLzmP//v0pepBF0q1bt9DNN9/sf27fvr31YwuOLfgcuOiii0K9e/e2v5csWWL91vbt2+cfX7RoUVi/uBkzZlj/O3rheRw9etT6sXG999zSpUvb/szCGvH8jz76yN+3fft22+f1cqPnHv3l1q1bZ+OiD93QoUNDdevWteMvvvhi6Mwzz8zwukTjyJEj1gPE27zvtGzP2aFyvd/M9LyFEEIIIUTu7+OmGrc8Tq1atfy/N2/ebJEiolKREAU699xz7e9q1ar5+wsWLOhKlCjhqlat6u8jfRKCkaZx48a5KVOmuK+//tqs64l41ahRI+bYgs+BpKQk/55Y4JctW9bSGz3q168fdj7z2bVrV4o6tCNHjth8PBh7Imr7GBPOmME1Pe+88yxN04O/q1evbhE2nsl29913u+TkZPfrr79aBI6oXEbXJRpDhw51gwYNytTchBBCCCFE3kbCLY9Dyp0HwoH6rOHDh6c4D3GQmmMiNVrBfXz20hmBXmPUdo0aNcrEFULqqaeeslS/WER7jnfPeGA+iKjIOjMoWbJk1DXIDkiDRLgVLlzYRBq1apUrV7YUSoTbgw8+mNB1oV1Ar169who6InqFEEIIIUT+QcLtOOLCCy908+bNs75jRI4SBbVl1L/de++9/r5gxCsjIHSoJdu/f78vKtesWZNiPrNmzTKHyuxoI0B07Y8//nAff/yxu+iii2wftXM//fRT2HmINaKPrDH1dp6Ye+WVV9yOHTti1rdlBAQimxBCCCGEyL/InOQ4olu3bu6HH35wrVu3duvWrTNxtWTJEnOL9Mw0MsI555xjZifcC2Hy2GOP2f0zw1VXXWWpm+3bt7eUSMxHHn300bBz2rRpY8YfOEly/Msvv7RIF66M33zzjUs0OFIixO655x6LJiLgOnXqlKI5Nk6Y9K3DCMUTafxLZBAR6qWkZjVbBjVRKwAhhBBCiHyChNtxBPViRMcQaThDUvuF7T91WQUKZPyrRsjgANmqVStrWH3w4MGw6FtGYDzz58+3erk6deqYQMLSP0ixYsXc+++/784++2x7PlE62hJQ45ZVEbipU6faOhJV45nUr0X2pKONAmtLuiZROk/Mke6YVn2bEEIIIYQQGeFvOJRk6EohRI5AjRuNvX/++edsSSEVQgghhBA5/76miJsQQgghhBBC5HJkTiKOK6iFu/baa6MeIy0zsl4t0sVSCCGEEEKI3IiEWz7ghRdesFq3SHfE45HatWu7TZs22d+PPPKIhaknTJgQl3DLLAMHDnTjx4+3nmzU7914441Z9iwhhBBCCJG/kHATxxUIs4oVK/pGI5RwBhtoZxU076ZJNoKtXr16ZmBCWwYEM5sQQgghhBCZQTVuIi6OHTvmcgu///57XOdREJodoi3Y147WBWXKlMmWvmsXJC/J8mcIIYQQQojcgYRbgsESfujQoe6f//ynRX+qV6/u5s6da8foQfa3v/3NvfPOO5bSh909ja1p8hxkwYIF1gC6SJEi1sfspptu8o/9+OOPrl27dhbR4XrquXbu3JkiNRILfY5zLfb9kbz++uvW4JpnlC9f3qJFNJ/2YJyk/V1//fWuePHiKaz60wNjpicb9vmsCX3hiIZ50Ij71ltvNZF1+umnm/jZs2ePf/zOO++0tEPGgFU//db69etnrQkiYb0ff/zxsOuC382IESMsIoewYo2C80prHLFSJJs3b+63OWDt6Ov21VdfuQceeMA+s3nfDfd/7bXXbB1Y/yZNmtizhRBCCCGESA0JtwSDaJs+fbrVVW3dutVe3O+44w63YsUK/xwaTY8aNcqaWhcqVMjddddd/rGFCxea2GratKnbuHGjiTz6nHkgRrjujTfecKtXr7ZUQM71ImI0jqbXWffu3a3W64orrnBPPPFECgMPxN/999/vtm3b5v7973+boIgUZwgSxvLpp5+GjTG90LCb5yxatMhSChGECFJg3AiXk08+2cZFH7qTTjrJGmEHI2usAwL37bfftsbXCMGPPvrIj3QB6/3JJ5+422+/Peo4+vbt64YNG+aP5+WXX3alS5dO1zii8dBDD/lCdP/+/ba9+uqr7qyzzjIR6e3zOHTokK01vxOeQ+3hbbfdlur9jx49arV6wU0IIYQQQuQz6OMmEsORI0dCxYoVC61atSpsf8eOHUOtW7cOLV++nJ55oWXLlvnHFi5caPsOHz5sn+vXrx9q06ZN1Pvv2LHDzl25cqW/78CBA6GiRYuGZs+ebZ95TtOmTcOua9WqVejUU0/1Pzdq1Cg0ZMiQsHNmzJgRSkpK8j/znJ49e4YSQfPmzUMdOnSIeoznVqpUKfTXX3/5+44ePWpzWrJkiX1u3759qHTp0rY/SPXq1UOPP/64/7lv376hunXr+p+57oYbbrC///e//4UKFy4cmjRpUobHEYv58+fbmgUpV65caPTo0WH7pk6dauetWbPG37d9+3bbt3bt2qj3Tk5OtuORW9me/+87F0IIIYQQuYuff/7Z3tf4N1Eo4pZAdu3aZdGUxo0bW7TG24isBCND1apV8/9OSkqyf3EiBKJkjRo1inp/olVE6IIpgiVKlLDUQY5550SmENavXz/s8+bNmy0SFBxj586dLSrE+D1I50wEXbt2dTNnznQ1atQwp8dVq1aFjYV1I9LljYU0xSNHjoStWdWqVd2JJ54Ydl+ibkTNAK35yiuv2L5osC5ErlJb23jHkQj4DkmF9TjvvPMsfdL7DqNFCmne6G1KqxRCCCGEyH/IVTKBeH3ASHc888wzw45RU+UJgBNOOMHf79U+UX8FWWlXHxwnNW0tWrRIcYyaKw9q2xIBdXjUe7311luW6oh46tatmxs5cqSNpVatWu6ll15KcR01cbHG0rp1a9e7d2+3YcMGs/pH0LRq1SrqGNJa13jHkRPw28kOsxMhhBBCCJF7kXBLIOeff769YH/99deuYcOGKY7HE7khGkc9V4cOHVIcq1y5shmIUMeGqQlgPELtF8/2zuF4kDVr1oR9xpSEazzb/OwA8dO+fXvbLr30Uvfwww+bcGMss2bNcqVKlXKnnHJKuu5JDRnrjNhCuBHp5D7RwAgE8cbadurUKcXxzIwjNYgQ/vnnnyn28x1Sp+jVLvJdUOfGdyeEEEIIIUQ0lCqZQEizw6gCQ5Jp06aZUCMa9Mwzz9jneEhOTraUP/4ldQ5jkOHDh/viA6dD0ho//PBDS+/D+IToHvuhR48ebvHixSaKcJt89tln7XOQAQMGWPomUTcMPXgOqYz9+/fPglX5f8/DxZJURJ6HuYgnUkhtxKiE8WMK8uWXX5r7JvP45ptv0rw31zP2OXPmpJom6UUSic6RqumlriJoJ0+enJBxRIM+bu+//77bt2+fO3DggL+fiOt9991nAvvjjz82wxl6vwVNaOJhy6AmGRqXEEIIIYTIe0i4JZjBgwebayHukogTXAlJnaQ9QDxgI48IwTWSmrArr7zS3BM9cC8kpe+6666z2jVqu0hB9NIvEQCTJk1yY8eONWv8pUuXphBkuCcinjhGrRXXjB492pUrV85lBUSeqNMimnjZZZe5ggULmtgCWhYgbrDmJ3WTNcMVk9qyeCJft9xyi0Udqc0LWv9Hg+/lwQcfNCHJc0ir9GoLMzuOaFBHSDuBChUqhKVb8ixEJO6Xl1xyidXTEe0TQgghhBAiNf6GQ0mqR4UQCYW2Cz179rTUyIxCOwCai2NUkqi0TiGEEEIIkTiy4n1NETchhBBCCCGEyOVIuIm46NKlS1j7gOCW1rHjgdTmx0ZNnBBCCCGEEFmJUiVzMZhWkFL32muvZeo+tByYP39+mjVgsaAWjJBvNAj/xjqWmtNjRuEne88997i5c+e6H3/80W3cuNHqAbMSjFVSA3OYzLRxGDhwoH3H9PCLB6VKCiGEEELkbrLifU3tAHIxGIzkFl2N+IolwDIqznBuvOKKK0yA0YQ6HnDJpFaMa8uXL29ukIkQp7HIztYJQgghhBBCRCLhlga///67uSLmBKj0/L4G0cDKPykpye9lJ4QQQgghxPGOatyi2PF3797dnP+I5GCdv2XLFnfttddaPVPp0qVd27Ztw/pycQ19ubjmtNNOs3Ow5P/tt9+skTb93YjYLFq0yL+GxszYzdMmgDS7SpUqWYQtMlUyGEHiOfQVoxfZ6aef7sqUKWNpdkHo3YblPn3LaMr99ttvp5jj3r173a233moRLu5D7zJs6yOf++STT7q///3vNrbMcPToUbO/L1u2rDUoZy3on8YzibYB60bUjGfHguOsNU3OOZ9eaWxw0003+fuAtSGF8t///rc9Gxt+5k3IOl6mTJniqlSpYuNGLPLb8OBZ3JvWDNybFgKrV6+2tEq+q+LFi5u4jGy8PmzYMPuN8LvwWg4IIYQQQggRCwm3KNAsmwjTypUr7SWbXmo1a9Z069evtzS9b7/91gRA5DUIPXquISy6du3qWrZsaS/uNOG++uqrTfDRbwz++usvd9ZZZ1nPtm3btllvsX79+rnZs2enOTYEwf/X3p2AVVmm/wN/AtyyKcNyiyIrQVvcSAkzsTC3UkpMYgyxsjRp1CZNy4XSxt1wmdTsMp0hL7fM0lBxGXLFZUY0U0NNMhfModTGFZPnf33v6/e+//cczjkc4Bw4wvdzXSc47/6+D9C5fZ7nvnfs2KEmTpwotcKM4AzHRA0yXDvWz549WwImq2vXrkkwiqABSTVwjwhIUW8OPWuGDRs2qKysLDk2ar6VRK9evaSo+PTp06XYN4IdnBPB1LJly2QbnCsnJ6dA8GoP63HPeHbYfteuXfIyatwZywwIovBMV65cKW2H+XD9+/d367pnzZqlEhMT1euvvy6F0FFbz37IJOr24f4wP61hw4ZSmw3z71C3Dj8vGOpqDfZwLQgox44dK+sRDM6cObPQwBfjpK0vIiIiIqpgkJyE/r/IyEjdrFkz8/2YMWN0+/btbbY5fvw4Jp7prKwsc5/WrVub6//44w9dvXp1HR8fby7LycmRfTIyMpyeOzExUcfExJjvExISdHR0tM21Wc8DLVq00EOHDpXv09LSdEBAgD558qS5fvXq1XLe5cuXy/uUlBQdGhqq8/PzzW2uXr2qq1WrJvsb561du7YsLyk8I5x/3bp1Dtenp6fL+rNnz7p9zOTkZB0cHGyzzHqPhqSkJO3v769PnDhh8zz8/PykPQpTr149PXz4cKfrcc4RI0aY79G2WDZ37lxz2cKFC3XVqlXN9xEREbp///42xwkPD9dNmjRxeh7cB45r/zp//nyh90BEREREpQ+f0zz9eY09bg6EhYWZ3+/du1elp6fbpH9HzwpYh8A1btzY/N7f31/VrFlTPfLII+YyDI0zsjMaPv74YznXnXfeKcedM2eODAF0xXoeQI+NcUz0ZqEXC8MbDRERETbb437QC4UeN+N+MFwSw/Ws94Nr98S8NvRE4XlERkaqsnDPPfdI1kfr80DPJHr4XMEzPXXqlIqKinK7PYw2tm93PFujlwxtFB4ebnMM+zayh947DO80XhjqSkREREQVC5OTOIChiIYLFy6oLl26qAkTJhTYDkGToVKlSjbrMP/JugzvAUEDLFq0SA0ePFhNmTJFPrgjkJo0aZIMcXTF0XmMY7oD94NgccGCBQXWIYB09AxKoiRp8suSu9ftqI1dtXtxYH4dXkRERERUcTFwK0Tz5s1lHhYSXgQEeO5xYW4Z5r9Z51vZJ7EoKiTHQG8M5nkZQeX27dsL3M/ixYslfX9p1ABD7xOClo0bN6p27doVWG/06iFZS0kgWHJ0DPRgoufM6IXE8/Dz8ys04QoCabQ55voZCVQ8AW2E4Bzz4gz2bUREREREZI9DJQuB5BS//fabiouLk6QXCK7S0tIkW2RJgo0GDRpIcgoc69ChQ2rkyJE2STWKA4FRSEiISkhIkCGRSD4yfPhwm2169uwpSVSQSRLrs7OzpR4aslWeOHFCeRqCH1zPK6+8IkWmjfMZSViCg4OlVwoJUP773/9Kj2Bxz4Mg6/Tp01ITzoDsmtbngftEYhlk5CwMkoigRxRJVZCtE0lmZsyYoUpi4MCBkqkSiVTQ7klJSWr//v0lOiYRERERlX8M3AqBnhr0jiFIQ2ZI9CAh7T9S6aPnpriQeRAZIGNjY2XO06+//up2tkNncD0oQn358mXVsmVL1adPH0npb4W09Zs2bZK5Xzg/eoCMlPTe6oFDdsbu3bvL/WF+4GuvvSalEgDzzz744AM1bNgwmQ9mzcBYFAiwkAETc/yQAdSALJC4z86dO0v7YU5aYVkcDQj4pk6dKtujJADS/iOAKwm0N4J0lHTAkNVjx45JBlIiIiIiIlduQoYSl1sQ3aDQY4ZePiRIKU+Q6ATF2ZGopDSGuxIRERFR2X9eY48bERERERGRj2NyEioU5oZ16tTJ4ToMy3SVgbGoc9aQTOTBBx90uh7FyjHM0xNQCsGZ1atXqyeeeMIj5yEiIiIiKikOlfRBvXv3VufOnZNhfiWBpB+Y8/bcc8+V6DgIzk6ePFnkwA2124YMGSJzAt31xx9/qJ9++qnAOVA6AXMNMTcOyUcwx7CkUM/OPnEL5vyNGDFC5t55opSBN4ZrcqgkERERkW/zxuc19rj5oGnTpilfiqcRwCDJR1EhWClqPTiUXLA/F5KbZGZmStp8ZMRE4Hb77bfLsqZNm6risj8P7hMBYXHu1RkEnH/5y188djwiIiIiqpgYuDmRl5dn1hgrbYjOy8MzsBb0LgmUYEBP2MMPPyzv7XvkfBmGY7oakklERERE5A4mJ/k/bdu2lVT0GNaHXp0OHTqo77//XuZ24YM3UtXHx8er3Nxcm33Qm4J90AOEbT799FMZzoc6byjijN4bzJcyoKwA0u/Xr19fenhQCBo9bPZDJa3DG3Ee1B9DCvnAwECpQYYheFZIU9+mTRupW4Y5YkiNbw/FuVHDDL1KOA5quVmDIOO8KCGAMgiFFal2p7Ya0ukDehBxzZifVqVKFTk+7qkwuHek+kcJAwz9xHs8O0Daf2OZ9fpRXgBBI7ql+/XrJwFocaBnD4Wy0bYoo4CfBftyAGhvlCDA+ueff1599NFHNsM4cc/WXkHjGidPnixF0mvWrCm1Aq9du1asayQiIiKiioGBm8U//vEP6WHCXKrx48erp556SoIDFMpes2aN+uWXXyTwsd8Hgd7OnTsliENNrhdeeEG1atVKCjajdhgCvkuXLsn2+fn5KigoSC1dulQSbYwaNUq99957ZkFqV9eGYYc7duxQEydOVKNHjzaDMxwTtcpw7Vg/e/ZsNXToUJv9ERggGEUwiWQjuEcEpB07drQJbFDEOisrS46NotiesmzZMpWcnKw++eQTCX4w7ws18Qrz5ZdfSt23iIgIlZOTI+/xrGH9+vXmMuv1Hzx4UIp8L1y4UNYhkCsOBFlo+xUrVqiMjAwJPlEPzgiy8AwRGKKoNoaFPv300wXq5jmSnp4uvYj4inadP3++vJy5evWqjJO2voiIiIiogkFyEtI6MjJSN2vWzHw/ZswY3b59e5ttjh8/jolnOisry9yndevW5vo//vhDV69eXcfHx5vLcnJyZJ+MjAyn505MTNQxMTHm+4SEBB0dHW1zbdbzQIsWLfTQoUPl+7S0NB0QEKBPnjxprl+9erWcd/ny5fI+JSVFh4aG6vz8fHObq1ev6mrVqsn+xnlr164tyz0hODhYJycny/dTpkzRISEhOi8vr8jHGThwoDwDQ3Z2ttxbZmamzXa4/sDAQH3x4kVz2axZs/Qtt9yir1+/Xuh5cA6cCw4dOiTn2Lp1q7k+NzdXnteSJUvkfWxsrH7mmWdsjtGzZ0992223me+TkpJ0kyZNbK4RzwU/K4YXXnhBjuUMjoFrsX+dP3++0HsiIiIiotKHz2me/rzGHjeLsLAw8/u9e/dKj4gxRwmvhg0byjr0lhgaN25sfu/v7y9D36w9SRg+CWfOnDGXffzxx3IuDOfDcefMmSNp8F2xngcwzM44JnqYMFwPww8N6KGywv0giyJ63Iz7wXDJK1eu2NwPrt0bc/vQC4nskPfdd5/0oCHbJTJIelqTJk1k2KL1OaAkAYaJFgWeKRKlhIeHm8vQthg+inWAnsmWLVva7Gf/3pGHHnpIflYctaUj7777rmQkMl5FvRciIiIiuvExOYmFNQMiPux36dJFTZgwocB2+KBtqFSpks06zLmyLsN7YzgjLFq0SDINYt4WggoEUpMmTZIhjq44Oo9xTHfgfhAsLliwwGUSkaJmgXQXAksEOhjeiGGY/fv3l/veuHFjgXsr74ralpgTiBcRERERVVwM3Jxo3ry5zMtCgg30vHgK5kVh/hsCF4O1x6s4kHERvTCY72UElUidb38/ixcvVrVq1Sqz2l9IxoJgGC8k5EAP5r59++TaisLoEUSiF3voWbTWlsNzQO8iAseiPlP0CCKgRnvBr7/+KsGnUSAcvW+7du2y2c/+PRERERGRJ3CopBMILH777TcVFxcnH8YRXKWlpUm2SEcBg7saNGggCS9wrEOHDqmRI0eW+MN+u3btVEhIiEpISJDABclHhg8fXqC4NJKoIJMk1mdnZ0sCD2R2PHHihPI2JN+YO3euZOo8evSo+vzzzyW4Cg4OLvKxEHxiXyNhDIYPGpBoBVk7kfhl1apVKikpSbKF+vn5Fbmd8KwwrHPLli3yXF966SUpzI3lgGQ0OAcySSLhChKvIIOo0ctKREREROQpDNycwHwx9I4hSENmSMz9Qtp/pHovahBg1bdvX8kAGRsbK/On0Itj7X0rDlwP5oyhpwlzrPr06VMguyHmfSGlPtLx4/zoUUKAgzlupdEDh+eG1PmPP/64zNfDkMmVK1fKvLGiQg/o9OnTJVBCOxmBFERFRUnQhdIIeMZdu3YtUDrBXfPmzZPhpc8++6wMa0VWSQRqxlBH3AsyeCJww9w6BJJvvfWWlGQgIiIiIvKkm5ChxKNHJCojSN9/7tw5KTVQVtBD98MPP0ivpregHACKtKOnsayGvRIRERFR6X5e4xw3ohJAIW3Ub0NSFwyTRF22mTNnlvVlEREREVE5w6GSFRDmm2HoYmHQa2Qth2B9IZ29s3V4ucvVOYpynMKg3IKr8xRWjsEZFANH4IahtBg2iSGcGKpKRERERORJHCpZQQM3zNfDsEJXMGfu5MmTTtcZmRsdeeCBB9y6FvtzIFHKk08+qb7++mvJ3ujucQBJQTDX77nnniuwDhkif/rpJ6f7OsseiuXHjh0rsHzcuHFq2LBh5ntkIJ0xY4bKzMyUeZGoV9e9e3dJjIJ6eca9jh8/Xi1cuFCOiVIQuFfMwUNtN3dxqCQRERGRb+NQSSpVCMycBU7Xrl3zSP01+3MYwROSqBQlaCsMjlvc440ePVrmrlkh6DIggyfq/SExydixYyVhCrJMogcuJSVFDRw4UF29elWyf6JnDzX8kJgGGTERAOJ7JGt57LHHSnyfRERERFQ+caikl6GwMj6c169fX4IUZB/84osvZB3S8aOXaMOGDerRRx+VzI+oGYZaYVbIvtiiRQvJVoiU/s8//7y57uzZs6pXr17q9ttvl/07deokQYN9DxsCIazHvshkaQ89XKinhnOgt+iDDz6QXioDrnPWrFmSpRHzueyzVhYFrhnlCVD4G88EWSCRwRHwnKBZs2ZyzrZt28p7lEzAkETcP/71IjIyUu3evdumZwxwf9jPeO/OvRUGQVqdOnVsXkahcgyVRLCGYAwFxdF+ODeuFb1wKNEAU6dOVRkZGeqbb75RPXr0kDIIyACKbYwMn+z8JiIiIiKnMFSSvOfDDz/UDRs21GvWrNE//vijnjdvnq5SpYr+9ttvdXp6Oj6p6/DwcHm/f/9+/cQTT+hWrVqZ+3/zzTfa399fjxo1Sh84cEDv2bNHjx071lzftWtX3ahRI71p0yZZ16FDB/3AAw/ovLw8Wb99+3bt5+enJ0yYoLOysvS0adN0jRo19G233WYeA/veeuutev78+XKNa9eu1ffee69+//33zW1wnbVq1dKfffaZbHPs2LFiP5PExETdtGlTvWvXLp2dna3XrVunV6xYIet27twp51q/fr3OycnRv/76qyzfsGGDTklJ0QcPHpTn8Oqrr+ratWvr33//XdafOXNG9sPzxX547+69uRIcHKyTk5Odrh8wYIC+5ZZbzOftTOPGjXX79u0drluwYIFce2ZmplvXdP78edkeX4mIiIjI93jj8xoDNy+6cuWKvvnmm/W2bdtsliPoiIuLMwM3BCmG1NRUWXb58mV5HxERoXv27Onw+IcOHZJtt27dai7Lzc3V1apV00uWLJH3OE/nzp1t9ouNjbUJ3KKiomyCQUCQVLduXfM9zjNo0CDtCV26dNEvv/yyw3UI5NwJYq5fv67/9Kc/6ZUrV9pc4/Lly222c+feCgvcKleurKtXr27zQkAInTp1kqCsMFWrVtUDBw50uG737t1y7YsXL3b6c4RfeuN1/PhxBm5EREREFSxw4xw3Lzpy5Ii6dOmSDJuzysvLk6GABhSkNtStW1e+njlzRoY37tmzp8D8KsPBgwdl7hbmSBlQ0Do0NFTWGdtYh1YCikmjWLRh7969UmzcOvwRCTZQnBvXjyGWgOGcnvDGG2+omJgYGeqI4uZIJoIhhq5gPtiIESNkeCmeDa4P11ZYNkh3782VIUOGSI04q7vuuku+FmV4Y3GHQmKoLYZ3EhEREVHFxcDNiy5cuCBfU1NTzQ/6hipVqqgff/xRvrcm+cD8LGNuHLjK3OjJ60Rg0K1btwLrMC/MYMzrKinMw0NWxVWrVql169apqKgolZiYKDXRnMFcMczNmzZtmswPw/NDAIog2BP35grm1TlLbBISEqK2bNlSaLIWbGcE0/aM5djGkXfffVf99a9/tclSdPfdd7t17URERERUPjA5iRchnT0CDPQK4YO/9eXuB2/0xiF5iSNIaoEkGzt27DCXIbhBchOc29jGuh62b99u8x6JO7CP/TXi5efnnR8RJCZBMPb5559L4o45c+bI8sqVK5u9YlboNRswYIDq3LmzpM7Hc83NzbXZBoGT/X7evrc///nPEhw6K7ptlFx48cUXJXMkegCtEKAnJydLeyFxjSO4V6SRtb6IiIiIqGJhj5sXIRvh4MGDJU08PqC3bt1aajkgCMGHb/QcFSYpKUl6pO6//3758I9ADT1VQ4cOlWyM0dHRMpTyk08+kfOhthh697AcEOw8/vjj0puFZWlpaTbDJGHUqFHq2WeflaGZqD2GgAYBxvfff68+/PBDjz8XnC8sLEwCMKTJR6ZFBJhQq1Yt6WXENQYFBUmvGLJI4l6RWh/DNdHjhOGL9r2RyOaIIBf3i2AHmTY9cW//+9//1OnTp22WYYgl2hDDVN955x319ttvSz06DEtFOQAMk0U5ALQ5ygHgZwDZLbt06WJTDgAZKdHjhqDO6G0lIiIiIirAY7PlyKH8/Hw9depUHRoaqitVqqTvvPNOyfy4ceNGMznJ2bNnze2RlAPLkKTDsGzZMsnCiCQZd9xxh+7WrZu57rffftPx8fGSbARJSXBsJC2xmjt3rg4KCpL1SAwyefJkm+QkgKyXyGaJbZCFsWXLlnrOnDkuE38U15gxYyQTJs4VGBioo6Oj9dGjR831n376qb777rslG2ZkZKSZwOPRRx+VJB8NGjTQS5cuLZDxEZkpkVEzICBA1rl7b67gOLh3+1ffvn1ttkNikTZt2kjCFCQvQcKS0aNH27TtxYsX9fDhw+Ua8bOAe4+JidH79u0r0vNjVkkiIiIi3+aNz2s34T8Fwzki8lXocUQvJHpvOWySiIiIqGJ8XuMcNyIiIiIiIh/HwI2KpV+/fuqWW25x+Cpsna9YsGCB0+vE/DsiIiIiIl/BoZIV3Pz589WgQYPM7IfuQi01dAE7gu5gV+uQgMQXIOkIEoQ4ggyV7iSPKQscKklERETk27zxeY1ZJalYEHy5CsA8EZz99NNPqn79+iozM1M1bdq0SPsiQ+Py5culuLczyMKJl7uQtRL15wAZLZHpExkj+/TpY7Pdp59+qv7+979LnT4USMc99OjRQ+qxWY/hCEokIJgmIiIiIrJi4EYlVljx6fJk9OjRUn7h0qVLaunSpfI9yi+gqDh89tln0oM5ffp0FRkZKeUOvvvuOyk/ALt27TJrzW3btk3FxMRInTnjX2JKo+A6EREREd14OMetFKGW27hx46QHBh/QUXD5iy++kHXffvut9BKhDhlqlaFOWKtWreRDvdXKlStVixYtpL7ZHXfcIXXDDGfPnlW9evWS+mXYH8HE4cOHbfZHbw5qmmE99kXBbnuoN4bC1TjHfffdpz744AOpH2fAdc6aNUt17dpVVa9eXf3tb38r9jPBNffs2VMKcuOZoF7bvHnzZB2eEzRr1kzO2bZtWzP4efrpp+X+0QWNAGn37t3mMdGrBbg/7Ge8d+feCoMeujp16si+qKUXGBio1q1bZ65fsWKF9K69+uqrUuQbc+Xi4uLMZ4T7xP54YV+jd9JYhvshIiIiIrLHwK0UIWj75z//KYWZ9+/fL0WZX3rpJbVx40Zzm+HDh0uB5n//+98yzO6VV14x16Wmpkow0rlzZxk+iCCvZcuW5vrevXvLfggeMjIyUKNPtkWPGOzYsUMCijfffFPt2bNHPfnkkwWKUG/evFmCPwwBPHDggBT2RrBnH5y9//77ci379u2zucaiGjlypJxn9erVUogaASECMti5c6d8RXHqnJwc9eWXX5pz0zCkcMuWLWr79u0S7OE+sdwI7AABIPYz3rt7b+4G4cuWLZPAs3LlyuZyBF+4JlfDIYsKvXYYJ219EREREVEF47GKcOTSlStX9M0336y3bdtms/zVV1/VcXFxZjHu9evXm+tSU1Nl2eXLl+V9RESE7tmzp8Pjo+g2tt26dau5LDc3V4pOL1myRN7jPJ07d7bZLzY21qYYd1RUlB47dqzNNikpKbpu3brme5xn0KBB2hNQEPzll192uA5FyHEuFCV35fr161L4euXKlS4Lhrtzb4UV40YRdBTYRpFvnANFtA8fPmxuc+rUKf3YY4/JupCQEJ2QkCDFuXGN9hwVYHckKSnJYRFwFuAmIiIiqjgFuNnjVkqOHDki86IwxM+adh49cEhiYWjcuLH5fd26dc0MjoBesqioKIfHR28VeujCw8PNZTVr1lShoaGyztjGuh4iIiJs3u/du1fmcVmvEfO40HOF6zdgOKcnvPHGG2rRokWSfOSdd96ReV+FQSZIXBN62jC0EPPDLly4oH7++WeX+7l7b64MGTJE2uFf//qXPMvk5GQZEmltM/R2oicSPXsYhonewY4dO0ovXXEgqQkyEhmv48ePF+s4RERERHTjYnKSUoLAwhjuiGQWVlWqVDGDN2uSD8zPAuMDf2kkrsB1Yt5Xt27dCqzDvDAD5rZ5AubhYVjhqlWrZK4YAtPExEQ1efJkp/sgEMLcvGnTpknKfjw/BKB5eXkeuTdXMIwTgRpeSE7yyCOPSBD74IMP2mz38MMPy6t///5Su+6JJ56QIbEYnlpUuD+8iIiIiKjiYuBWSvDBHh++0SuEZBr2rL1uzqA3DvPaXn755QLrGjVqJL07mMeGpCaA4AbJTYygAttgvRXmY1khcQf2sfYieRsSdiAYwwsBDnq1ELgZc8eMLIyGrVu3qpkzZ8q8NkAPVG5urs02CIDt9/P0vd19990qNjZWesSQ9MQZ4/lfvHjRI+clIiIiooqHgVspQTbCwYMHS0IS9KC1bt1ahr0hCMFQP3eKPSclJUmPFOqHvfjiixKooacK2Q0xbDA6OlqG/iHpBs43bNgw6d3DchgwYIB6/PHHJSjCsrS0NLVmzRqbc4waNUo9++yzknmye/fuys/PT4YYIp29fSITT8D5wsLCJPsiknB88803EmAa2RbRy4hrDAoKkl4xDI3EvaakpEhPFxJ1INCz741EJkkEubhfBMzItOmNe8NwSPSsISkMrgdDP+vVq6eeeuopuWYMw8SxEZzaD0slIiIiInIX57iVojFjxkgWRWSXRHCCeU8YOmmkvS8M0uFjeB6yRmJOGIIDI/OikUURQRCCEwQJyNGBwM4YfvnYY49JcWgMMUQpgrVr16oRI0bYnKNDhw4SPGEdyg5gH8zjciewLA70qqHHCr2Jbdq0Uf7+/jLnDTBnD/XQEIgiGDIC0Llz50o2R/SgxcfHS0BqX/AbmTkx9BK9Yign4K17Q29a+/btJSiEdu3aSS/mCy+8oEJCQqROGwJOBJGYc0hEREREVBw3IUNJsfYkojKBntoaNWrIEFGjcDcRERER+Q6MCkMHwrlz5zxWp5dDJYluMEbRdPwxICIiIiLfhTrDDNzIZyBr4ueff+5wHQqMu1qHYuS+YMGCBapv374O12EoJQqm+4rAwED5ikQ3nvpDQL79r3XsXS3/2NYVB9u64mBbV+y21lpL0IbpPp7CoZJUYqgzhx9YR/DD62qd/dy0soJfLNSHcwRzBL01x6848DwRsGHIJP9HUL6xrSsOtnXFwbauONjWFcfvpdTW7HGjEkPw5SoA85XgzBVk4cSLiIiIiMgXMaskERERERGRj2PgRnSDQV061PTDVyrf2NYVB9u64mBbVxxs64qjSim1Nee4ERERERER+Tj2uBEREREREfk4Bm5EREREREQ+joEbERERERGRj2PgRkRERERE5OMYuBH5gI8//ljde++9qmrVqio8PFzt3LnT5fZLly5VDRs2lO0feeQRtWrVKpv1yDk0atQoVbduXVWtWjXVrl07dfjwYS/fBZV2W1+7dk0NHTpUllevXl3Vq1dP9erVS506daoU7oRK+/faql+/fuqmm25SU6dO9cKVU1m388GDB1XXrl2loC9+t1u0aKF+/vlnL94FlUVbX7hwQb355psqKChI/l/94IMPqtmzZ3v5LsjTbb1//34VExMj27v6u1zUnx+HkFWSiMrOokWLdOXKlfVnn32m9+/fr1977TVdo0YN/csvvzjcfuvWrdrf319PnDhRHzhwQI8YMUJXqlRJ79u3z9xm/Pjx+rbbbtNfffWV3rt3r+7atauuX7++vnz5cineGXm7rc+dO6fbtWunFy9erH/44QedkZGhW7ZsqcPCwkr5zqg0fq8NX375pW7SpImuV6+eTk5OLoW7odJs5yNHjujAwEA9ZMgQvXv3bnn/9ddfOz0m3bhtjWPcf//9Oj09XWdnZ+tPPvlE9kF7043T1jt37tSDBw/WCxcu1HXq1HH4d7mox3SGgRtRGcMH7cTERPP99evX5QPZuHHjHG7fo0cP/cwzz9gsCw8P13379pXv8/Pz5Q/HpEmTzPX4gF+lShX5o0Llp62d/Q8E/yZ37NgxD145+UpbnzhxQt911136+++/18HBwQzcymE7x8bG6pdeesmLV02+0tYPPfSQHj16tM02zZs318OHD/f49ZP32trK2d/lkhzTikMlicpQXl6e+s9//iNDGQ1+fn7yPiMjw+E+WG7dHjp06GBun52drU6fPm2zDYbboFve2THpxmxrR86fPy9DNWrUqOHBqydfaOv8/HwVHx+vhgwZoh566CEv3gGVVTujjVNTU1VISIgsr1Wrlvzt/uqrr7x8N1QWv9OtWrVSK1asUCdPnpQpDunp6erQoUOqffv2Xrwb8nRbl+YxGbgRlaHc3Fx1/fp1Vbt2bZvleI/gyxEsd7W98bUox6Qbs63tXblyRea8xcXFqVtvvdWDV0++0NYTJkxQAQEBasCAAV66cirrdj5z5ozMexo/frzq2LGjWrt2rXr++edVt27d1MaNG714N1QWv9MzZsyQeW2Y41a5cmVpc8yDatOmjZfuhLzR1qV5zIBiXQEREfkUJCrp0aOH/KvtrFmzyvpyyMPwr7XTpk1Tu3fvlh5VKp/Q4wbR0dHqrbfeku+bNm2qtm3bJkkrIiMjy/gKyZMQuG3fvl163YKDg9WmTZtUYmKiJJqy760jAva4EZWhO+64Q/n7+6tffvnFZjne16lTx+E+WO5qe+NrUY5JN2Zb2wdtx44dU+vWrWNvWzls682bN0tvzD333CO9bnihvd9++23JUkblo51xTLQtemGsGjVqxKyS5aytL1++rN577z310UcfqS5duqjGjRtLhsnY2Fg1efJkL94NebqtS/OYDNyIyhCGRoSFhakNGzbY/Isr3kdERDjcB8ut2wM+rBvb169fX/4QWLf5/fff1Y4dO5wek27MtrYGbSj3sH79elWzZk0v3gWVVVtjbtt3332n9uzZY77wr/KY75aWlublO6LSamccE6n/s7KybLbBvCf0yFD5aWv87cYLc52s8AHf6HmlG6OtS/WYRUplQkQehxSxyPg4f/58SRn8+uuvS4rY06dPy/r4+Hg9bNgwmxTDAQEBevLkyfrgwYM6KSnJYTkAHAMphb/77jsdHR3NcgDlsK3z8vKk1ENQUJDes2ePzsnJMV9Xr14ts/sk7/xe22NWyfLZzij3gGVz5szRhw8f1jNmzJAU8Zs3by6TeyTvtXVkZKRklkQ5gKNHj+p58+bpqlWr6pkzZ5bJPVLx2hr/v83MzJRX3bp1pTQAvsfvr7vHdBcDNyIfgP8x33PPPVLjAyljt2/fbvOHPSEhwWb7JUuW6JCQENkef/RTU1Nt1qMkwMiRI3Xt2rXlD0VUVJTOysoqtfuh0mlr1P3Bv785euGDAJWv32t7DNzKbzvPnTtXP/DAA/IhHjX7UJOTyl9b4x/ZevfuLWnh0dahoaF6ypQp8v9wunHaOtvJ/4uxnbvHdNdN+E/R+uiIiIiIiIioNHGOGxERERERkY9j4EZEREREROTjGLgRERERERH5OAZuREREREREPo6BGxERERERkY9j4EZEREREROTjGLgRERERERH5OAZuREREREREPo6BGxERERERkY9j4EZEREREROTjGLgRERERERH5OAZuREREREREyrf9P05nPizqSUlNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.002315\n",
       "encoder__proto_ospf             0.000785\n",
       "encoder__proto_sctp             0.000729\n",
       "encoder__proto_tcp              0.001521\n",
       "encoder__proto_udp              0.023331\n",
       "encoder__proto_unas             0.001707\n",
       "encoder__service_dns            0.044820\n",
       "encoder__service_ftp            0.000386\n",
       "encoder__service_ftp-data       0.000823\n",
       "encoder__service_http           0.004992\n",
       "encoder__service_smtp           0.000898\n",
       "encoder__state_CON              0.002625\n",
       "encoder__state_ECO              0.000027\n",
       "encoder__state_FIN              0.000671\n",
       "encoder__state_INT              0.007634\n",
       "encoder__state_REQ              0.000619\n",
       "encoder__state_RST              0.000013\n",
       "remainder__dur                  0.021142\n",
       "remainder__spkts                0.009836\n",
       "remainder__dpkts                0.009263\n",
       "remainder__sbytes               0.075629\n",
       "remainder__dbytes               0.022278\n",
       "remainder__rate                 0.026009\n",
       "remainder__sttl                 0.096046\n",
       "remainder__dttl                 0.013829\n",
       "remainder__sload                0.031769\n",
       "remainder__dload                0.032785\n",
       "remainder__sloss                0.007746\n",
       "remainder__dloss                0.011608\n",
       "remainder__sinpkt               0.016771\n",
       "remainder__dinpkt               0.013839\n",
       "remainder__sjit                 0.013518\n",
       "remainder__djit                 0.013113\n",
       "remainder__swin                 0.002562\n",
       "remainder__stcpb                0.007766\n",
       "remainder__dtcpb                0.008364\n",
       "remainder__dwin                 0.000462\n",
       "remainder__tcprtt               0.017475\n",
       "remainder__synack               0.018497\n",
       "remainder__ackdat               0.016091\n",
       "remainder__smean                0.053321\n",
       "remainder__dmean                0.025720\n",
       "remainder__trans_depth          0.002516\n",
       "remainder__response_body_len    0.002860\n",
       "remainder__ct_srv_src           0.034947\n",
       "remainder__ct_state_ttl         0.050606\n",
       "remainder__ct_dst_ltm           0.022575\n",
       "remainder__ct_src_dport_ltm     0.046816\n",
       "remainder__ct_dst_sport_ltm     0.049057\n",
       "remainder__ct_dst_src_ltm       0.044208\n",
       "remainder__is_ftp_login         0.000109\n",
       "remainder__ct_ftp_cmd           0.000127\n",
       "remainder__ct_flw_http_mthd     0.003376\n",
       "remainder__ct_src_ltm           0.018329\n",
       "remainder__ct_srv_dst           0.063349\n",
       "remainder__is_sm_ips_ports      0.001794\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_data_y.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e2a8d00-9e61-4d23-b6d7-c0b81cf1312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.5, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_over_train = []\n",
    "loss_over_test = []\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    mean_loss = epoch_loss / len(dataloader) \n",
    "    loss_over_train.append(mean_loss)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    mean_loss = test_loss / num_batches  # Divide by the number of batches\n",
    "    loss_over_test.append(mean_loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb5bb5c-603d-4c65-9322-fbb4f687fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 40,20, 10,10],grid_size = 6, spline_order = 3, scale_noise=0.5, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.254344  [    0/175341]\n",
      "loss: 2.049833  [ 1600/175341]\n",
      "loss: 1.909067  [ 3200/175341]\n",
      "loss: 1.864853  [ 4800/175341]\n",
      "loss: 1.796268  [ 6400/175341]\n",
      "loss: 1.168501  [ 8000/175341]\n",
      "loss: 0.978518  [ 9600/175341]\n",
      "loss: 1.227924  [11200/175341]\n",
      "loss: 0.985004  [12800/175341]\n",
      "loss: 0.936896  [14400/175341]\n",
      "loss: 1.366477  [16000/175341]\n",
      "loss: 0.425838  [17600/175341]\n",
      "loss: 0.672801  [19200/175341]\n",
      "loss: 1.072255  [20800/175341]\n",
      "loss: 0.628487  [22400/175341]\n",
      "loss: 0.798620  [24000/175341]\n",
      "loss: 0.848764  [25600/175341]\n",
      "loss: 0.749363  [27200/175341]\n",
      "loss: 0.776945  [28800/175341]\n",
      "loss: 1.081261  [30400/175341]\n",
      "loss: 1.190060  [32000/175341]\n",
      "loss: 0.552510  [33600/175341]\n",
      "loss: 1.127095  [35200/175341]\n",
      "loss: 0.800606  [36800/175341]\n",
      "loss: 0.801070  [38400/175341]\n",
      "loss: 0.378973  [40000/175341]\n",
      "loss: 0.466748  [41600/175341]\n",
      "loss: 1.213287  [43200/175341]\n",
      "loss: 0.405307  [44800/175341]\n",
      "loss: 0.717190  [46400/175341]\n",
      "loss: 1.037059  [48000/175341]\n",
      "loss: 1.517205  [49600/175341]\n",
      "loss: 0.976147  [51200/175341]\n",
      "loss: 0.964060  [52800/175341]\n",
      "loss: 0.826716  [54400/175341]\n",
      "loss: 0.553659  [56000/175341]\n",
      "loss: 1.011434  [57600/175341]\n",
      "loss: 0.915975  [59200/175341]\n",
      "loss: 0.760209  [60800/175341]\n",
      "loss: 0.552977  [62400/175341]\n",
      "loss: 1.229363  [64000/175341]\n",
      "loss: 0.498683  [65600/175341]\n",
      "loss: 0.855665  [67200/175341]\n",
      "loss: 0.636307  [68800/175341]\n",
      "loss: 0.757369  [70400/175341]\n",
      "loss: 0.444651  [72000/175341]\n",
      "loss: 0.679654  [73600/175341]\n",
      "loss: 0.644119  [75200/175341]\n",
      "loss: 0.300966  [76800/175341]\n",
      "loss: 1.142115  [78400/175341]\n",
      "loss: 0.832391  [80000/175341]\n",
      "loss: 0.910448  [81600/175341]\n",
      "loss: 0.573393  [83200/175341]\n",
      "loss: 0.585869  [84800/175341]\n",
      "loss: 0.380092  [86400/175341]\n",
      "loss: 0.914100  [88000/175341]\n",
      "loss: 0.602082  [89600/175341]\n",
      "loss: 0.969168  [91200/175341]\n",
      "loss: 1.235432  [92800/175341]\n",
      "loss: 0.478607  [94400/175341]\n",
      "loss: 0.971317  [96000/175341]\n",
      "loss: 0.805973  [97600/175341]\n",
      "loss: 0.420315  [99200/175341]\n",
      "loss: 0.572945  [100800/175341]\n",
      "loss: 0.698068  [102400/175341]\n",
      "loss: 0.540384  [104000/175341]\n",
      "loss: 1.166763  [105600/175341]\n",
      "loss: 0.583599  [107200/175341]\n",
      "loss: 0.597250  [108800/175341]\n",
      "loss: 0.312956  [110400/175341]\n",
      "loss: 0.662402  [112000/175341]\n",
      "loss: 0.323332  [113600/175341]\n",
      "loss: 0.662245  [115200/175341]\n",
      "loss: 0.441373  [116800/175341]\n",
      "loss: 0.552582  [118400/175341]\n",
      "loss: 0.383852  [120000/175341]\n",
      "loss: 0.547225  [121600/175341]\n",
      "loss: 0.638706  [123200/175341]\n",
      "loss: 0.290684  [124800/175341]\n",
      "loss: 0.450758  [126400/175341]\n",
      "loss: 0.914007  [128000/175341]\n",
      "loss: 0.399074  [129600/175341]\n",
      "loss: 0.627455  [131200/175341]\n",
      "loss: 0.226306  [132800/175341]\n",
      "loss: 0.523862  [134400/175341]\n",
      "loss: 0.536115  [136000/175341]\n",
      "loss: 1.012764  [137600/175341]\n",
      "loss: 0.624768  [139200/175341]\n",
      "loss: 0.682717  [140800/175341]\n",
      "loss: 0.609874  [142400/175341]\n",
      "loss: 0.705787  [144000/175341]\n",
      "loss: 0.809589  [145600/175341]\n",
      "loss: 0.465254  [147200/175341]\n",
      "loss: 0.939022  [148800/175341]\n",
      "loss: 1.017249  [150400/175341]\n",
      "loss: 0.381056  [152000/175341]\n",
      "loss: 0.932067  [153600/175341]\n",
      "loss: 0.517475  [155200/175341]\n",
      "loss: 0.219664  [156800/175341]\n",
      "loss: 0.800416  [158400/175341]\n",
      "loss: 0.509598  [160000/175341]\n",
      "loss: 0.858751  [161600/175341]\n",
      "loss: 0.487950  [163200/175341]\n",
      "loss: 0.849260  [164800/175341]\n",
      "loss: 0.379402  [166400/175341]\n",
      "loss: 0.907976  [168000/175341]\n",
      "loss: 0.598384  [169600/175341]\n",
      "loss: 0.499030  [171200/175341]\n",
      "loss: 0.659976  [172800/175341]\n",
      "loss: 0.558225  [174400/175341]\n",
      "Train Accuracy: 73.7762%\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.698855, F1-score: 70.56%, Macro_F1-Score:  33.06%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.378536  [    0/175341]\n",
      "loss: 0.560273  [ 1600/175341]\n",
      "loss: 0.748966  [ 3200/175341]\n",
      "loss: 0.785902  [ 4800/175341]\n",
      "loss: 0.615691  [ 6400/175341]\n",
      "loss: 0.321071  [ 8000/175341]\n",
      "loss: 0.413186  [ 9600/175341]\n",
      "loss: 0.898367  [11200/175341]\n",
      "loss: 0.431170  [12800/175341]\n",
      "loss: 0.274372  [14400/175341]\n",
      "loss: 0.676494  [16000/175341]\n",
      "loss: 0.354011  [17600/175341]\n",
      "loss: 0.786263  [19200/175341]\n",
      "loss: 0.672003  [20800/175341]\n",
      "loss: 0.389646  [22400/175341]\n",
      "loss: 0.312099  [24000/175341]\n",
      "loss: 0.476683  [25600/175341]\n",
      "loss: 0.522555  [27200/175341]\n",
      "loss: 0.272657  [28800/175341]\n",
      "loss: 1.489071  [30400/175341]\n",
      "loss: 0.234153  [32000/175341]\n",
      "loss: 0.872009  [33600/175341]\n",
      "loss: 0.528879  [35200/175341]\n",
      "loss: 0.316834  [36800/175341]\n",
      "loss: 0.908404  [38400/175341]\n",
      "loss: 0.824148  [40000/175341]\n",
      "loss: 0.664246  [41600/175341]\n",
      "loss: 0.855891  [43200/175341]\n",
      "loss: 0.578852  [44800/175341]\n",
      "loss: 0.443676  [46400/175341]\n",
      "loss: 0.486662  [48000/175341]\n",
      "loss: 1.038994  [49600/175341]\n",
      "loss: 0.487858  [51200/175341]\n",
      "loss: 0.518109  [52800/175341]\n",
      "loss: 0.354030  [54400/175341]\n",
      "loss: 0.302269  [56000/175341]\n",
      "loss: 0.424978  [57600/175341]\n",
      "loss: 0.249811  [59200/175341]\n",
      "loss: 0.412314  [60800/175341]\n",
      "loss: 0.598107  [62400/175341]\n",
      "loss: 0.429904  [64000/175341]\n",
      "loss: 0.496960  [65600/175341]\n",
      "loss: 0.501989  [67200/175341]\n",
      "loss: 0.424873  [68800/175341]\n",
      "loss: 0.701483  [70400/175341]\n",
      "loss: 0.452730  [72000/175341]\n",
      "loss: 0.958363  [73600/175341]\n",
      "loss: 0.596136  [75200/175341]\n",
      "loss: 0.690191  [76800/175341]\n",
      "loss: 1.043818  [78400/175341]\n",
      "loss: 0.991660  [80000/175341]\n",
      "loss: 0.561414  [81600/175341]\n",
      "loss: 0.592111  [83200/175341]\n",
      "loss: 0.858163  [84800/175341]\n",
      "loss: 1.187345  [86400/175341]\n",
      "loss: 0.358093  [88000/175341]\n",
      "loss: 0.587696  [89600/175341]\n",
      "loss: 0.372072  [91200/175341]\n",
      "loss: 0.961158  [92800/175341]\n",
      "loss: 0.909036  [94400/175341]\n",
      "loss: 0.378214  [96000/175341]\n",
      "loss: 0.287352  [97600/175341]\n",
      "loss: 0.544326  [99200/175341]\n",
      "loss: 0.416695  [100800/175341]\n",
      "loss: 0.554420  [102400/175341]\n",
      "loss: 0.661819  [104000/175341]\n",
      "loss: 0.848698  [105600/175341]\n",
      "loss: 0.726126  [107200/175341]\n",
      "loss: 0.932306  [108800/175341]\n",
      "loss: 0.482644  [110400/175341]\n",
      "loss: 0.408695  [112000/175341]\n",
      "loss: 0.214594  [113600/175341]\n",
      "loss: 0.677751  [115200/175341]\n",
      "loss: 0.444816  [116800/175341]\n",
      "loss: 0.535206  [118400/175341]\n",
      "loss: 0.605879  [120000/175341]\n",
      "loss: 0.700473  [121600/175341]\n",
      "loss: 0.171012  [123200/175341]\n",
      "loss: 0.576715  [124800/175341]\n",
      "loss: 0.312729  [126400/175341]\n",
      "loss: 0.455448  [128000/175341]\n",
      "loss: 0.235346  [129600/175341]\n",
      "loss: 0.701089  [131200/175341]\n",
      "loss: 0.861269  [132800/175341]\n",
      "loss: 0.501753  [134400/175341]\n",
      "loss: 0.748915  [136000/175341]\n",
      "loss: 0.616689  [137600/175341]\n",
      "loss: 0.507186  [139200/175341]\n",
      "loss: 0.404020  [140800/175341]\n",
      "loss: 0.657171  [142400/175341]\n",
      "loss: 0.782812  [144000/175341]\n",
      "loss: 0.693032  [145600/175341]\n",
      "loss: 0.546607  [147200/175341]\n",
      "loss: 0.267243  [148800/175341]\n",
      "loss: 0.567580  [150400/175341]\n",
      "loss: 0.575573  [152000/175341]\n",
      "loss: 0.346609  [153600/175341]\n",
      "loss: 0.623584  [155200/175341]\n",
      "loss: 0.181520  [156800/175341]\n",
      "loss: 0.186043  [158400/175341]\n",
      "loss: 0.312434  [160000/175341]\n",
      "loss: 1.099249  [161600/175341]\n",
      "loss: 0.355859  [163200/175341]\n",
      "loss: 0.184985  [164800/175341]\n",
      "loss: 0.521936  [166400/175341]\n",
      "loss: 0.623406  [168000/175341]\n",
      "loss: 0.591817  [169600/175341]\n",
      "loss: 0.443680  [171200/175341]\n",
      "loss: 0.330839  [172800/175341]\n",
      "loss: 0.748241  [174400/175341]\n",
      "Train Accuracy: 78.2310%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.628424, F1-score: 73.45%, Macro_F1-Score:  33.75%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.435880  [    0/175341]\n",
      "loss: 0.683221  [ 1600/175341]\n",
      "loss: 0.722630  [ 3200/175341]\n",
      "loss: 0.803706  [ 4800/175341]\n",
      "loss: 0.183716  [ 6400/175341]\n",
      "loss: 0.179994  [ 8000/175341]\n",
      "loss: 0.119794  [ 9600/175341]\n",
      "loss: 0.885365  [11200/175341]\n",
      "loss: 0.491564  [12800/175341]\n",
      "loss: 0.799336  [14400/175341]\n",
      "loss: 0.421031  [16000/175341]\n",
      "loss: 0.418318  [17600/175341]\n",
      "loss: 0.771727  [19200/175341]\n",
      "loss: 0.682675  [20800/175341]\n",
      "loss: 0.556529  [22400/175341]\n",
      "loss: 0.847764  [24000/175341]\n",
      "loss: 0.230924  [25600/175341]\n",
      "loss: 0.804812  [27200/175341]\n",
      "loss: 0.577706  [28800/175341]\n",
      "loss: 0.905506  [30400/175341]\n",
      "loss: 0.468108  [32000/175341]\n",
      "loss: 0.678661  [33600/175341]\n",
      "loss: 0.268353  [35200/175341]\n",
      "loss: 0.546140  [36800/175341]\n",
      "loss: 0.161942  [38400/175341]\n",
      "loss: 0.449134  [40000/175341]\n",
      "loss: 0.677970  [41600/175341]\n",
      "loss: 0.514905  [43200/175341]\n",
      "loss: 0.430565  [44800/175341]\n",
      "loss: 0.548337  [46400/175341]\n",
      "loss: 0.757265  [48000/175341]\n",
      "loss: 0.492620  [49600/175341]\n",
      "loss: 1.205792  [51200/175341]\n",
      "loss: 0.486407  [52800/175341]\n",
      "loss: 0.280198  [54400/175341]\n",
      "loss: 0.952652  [56000/175341]\n",
      "loss: 0.431763  [57600/175341]\n",
      "loss: 0.789016  [59200/175341]\n",
      "loss: 0.364909  [60800/175341]\n",
      "loss: 0.722161  [62400/175341]\n",
      "loss: 0.839416  [64000/175341]\n",
      "loss: 0.297582  [65600/175341]\n",
      "loss: 0.284207  [67200/175341]\n",
      "loss: 0.608407  [68800/175341]\n",
      "loss: 0.269309  [70400/175341]\n",
      "loss: 0.716293  [72000/175341]\n",
      "loss: 0.221670  [73600/175341]\n",
      "loss: 0.800875  [75200/175341]\n",
      "loss: 0.337352  [76800/175341]\n",
      "loss: 0.407607  [78400/175341]\n",
      "loss: 0.408361  [80000/175341]\n",
      "loss: 0.434097  [81600/175341]\n",
      "loss: 0.577507  [83200/175341]\n",
      "loss: 0.683531  [84800/175341]\n",
      "loss: 0.696167  [86400/175341]\n",
      "loss: 0.624265  [88000/175341]\n",
      "loss: 0.371532  [89600/175341]\n",
      "loss: 0.723868  [91200/175341]\n",
      "loss: 0.788230  [92800/175341]\n",
      "loss: 0.833801  [94400/175341]\n",
      "loss: 0.677191  [96000/175341]\n",
      "loss: 0.247633  [97600/175341]\n",
      "loss: 0.740668  [99200/175341]\n",
      "loss: 0.510525  [100800/175341]\n",
      "loss: 0.657121  [102400/175341]\n",
      "loss: 0.868969  [104000/175341]\n",
      "loss: 0.582300  [105600/175341]\n",
      "loss: 0.292732  [107200/175341]\n",
      "loss: 0.429188  [108800/175341]\n",
      "loss: 0.506374  [110400/175341]\n",
      "loss: 0.467420  [112000/175341]\n",
      "loss: 0.283186  [113600/175341]\n",
      "loss: 0.346889  [115200/175341]\n",
      "loss: 0.458413  [116800/175341]\n",
      "loss: 0.739987  [118400/175341]\n",
      "loss: 0.615412  [120000/175341]\n",
      "loss: 0.404530  [121600/175341]\n",
      "loss: 0.743361  [123200/175341]\n",
      "loss: 0.591503  [124800/175341]\n",
      "loss: 0.335168  [126400/175341]\n",
      "loss: 0.502424  [128000/175341]\n",
      "loss: 0.502045  [129600/175341]\n",
      "loss: 0.304394  [131200/175341]\n",
      "loss: 0.577959  [132800/175341]\n",
      "loss: 0.487094  [134400/175341]\n",
      "loss: 0.568541  [136000/175341]\n",
      "loss: 0.513547  [137600/175341]\n",
      "loss: 0.426876  [139200/175341]\n",
      "loss: 0.631829  [140800/175341]\n",
      "loss: 0.844130  [142400/175341]\n",
      "loss: 0.298897  [144000/175341]\n",
      "loss: 0.236074  [145600/175341]\n",
      "loss: 0.251949  [147200/175341]\n",
      "loss: 0.511090  [148800/175341]\n",
      "loss: 0.502844  [150400/175341]\n",
      "loss: 1.011344  [152000/175341]\n",
      "loss: 0.740269  [153600/175341]\n",
      "loss: 0.551357  [155200/175341]\n",
      "loss: 0.649578  [156800/175341]\n",
      "loss: 0.388215  [158400/175341]\n",
      "loss: 0.366904  [160000/175341]\n",
      "loss: 0.659562  [161600/175341]\n",
      "loss: 0.687002  [163200/175341]\n",
      "loss: 0.809459  [164800/175341]\n",
      "loss: 0.866869  [166400/175341]\n",
      "loss: 0.224112  [168000/175341]\n",
      "loss: 0.383746  [169600/175341]\n",
      "loss: 0.577880  [171200/175341]\n",
      "loss: 0.490017  [172800/175341]\n",
      "loss: 0.591341  [174400/175341]\n",
      "Train Accuracy: 78.7192%\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.627593, F1-score: 72.40%, Macro_F1-Score:  33.72%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.406456  [    0/175341]\n",
      "loss: 0.921800  [ 1600/175341]\n",
      "loss: 0.953755  [ 3200/175341]\n",
      "loss: 0.241610  [ 4800/175341]\n",
      "loss: 0.579561  [ 6400/175341]\n",
      "loss: 0.398094  [ 8000/175341]\n",
      "loss: 0.567590  [ 9600/175341]\n",
      "loss: 0.906667  [11200/175341]\n",
      "loss: 0.474611  [12800/175341]\n",
      "loss: 0.783636  [14400/175341]\n",
      "loss: 0.630319  [16000/175341]\n",
      "loss: 0.708376  [17600/175341]\n",
      "loss: 0.436046  [19200/175341]\n",
      "loss: 0.327010  [20800/175341]\n",
      "loss: 1.046580  [22400/175341]\n",
      "loss: 0.633439  [24000/175341]\n",
      "loss: 0.554585  [25600/175341]\n",
      "loss: 0.880572  [27200/175341]\n",
      "loss: 0.453815  [28800/175341]\n",
      "loss: 0.645241  [30400/175341]\n",
      "loss: 0.423714  [32000/175341]\n",
      "loss: 0.580052  [33600/175341]\n",
      "loss: 0.442548  [35200/175341]\n",
      "loss: 0.567053  [36800/175341]\n",
      "loss: 0.426333  [38400/175341]\n",
      "loss: 0.276688  [40000/175341]\n",
      "loss: 0.215397  [41600/175341]\n",
      "loss: 0.734259  [43200/175341]\n",
      "loss: 0.408116  [44800/175341]\n",
      "loss: 0.322363  [46400/175341]\n",
      "loss: 0.250636  [48000/175341]\n",
      "loss: 1.126838  [49600/175341]\n",
      "loss: 0.495777  [51200/175341]\n",
      "loss: 0.410599  [52800/175341]\n",
      "loss: 0.377681  [54400/175341]\n",
      "loss: 0.706076  [56000/175341]\n",
      "loss: 0.463949  [57600/175341]\n",
      "loss: 0.307113  [59200/175341]\n",
      "loss: 0.614540  [60800/175341]\n",
      "loss: 1.063640  [62400/175341]\n",
      "loss: 0.484892  [64000/175341]\n",
      "loss: 0.674858  [65600/175341]\n",
      "loss: 0.542449  [67200/175341]\n",
      "loss: 0.264352  [68800/175341]\n",
      "loss: 0.630965  [70400/175341]\n",
      "loss: 0.420538  [72000/175341]\n",
      "loss: 0.599141  [73600/175341]\n",
      "loss: 0.470429  [75200/175341]\n",
      "loss: 0.534939  [76800/175341]\n",
      "loss: 0.570111  [78400/175341]\n",
      "loss: 0.599002  [80000/175341]\n",
      "loss: 0.511539  [81600/175341]\n",
      "loss: 0.697513  [83200/175341]\n",
      "loss: 0.927420  [84800/175341]\n",
      "loss: 0.374348  [86400/175341]\n",
      "loss: 0.444040  [88000/175341]\n",
      "loss: 0.432741  [89600/175341]\n",
      "loss: 0.316328  [91200/175341]\n",
      "loss: 1.074018  [92800/175341]\n",
      "loss: 1.279739  [94400/175341]\n",
      "loss: 0.667433  [96000/175341]\n",
      "loss: 0.308127  [97600/175341]\n",
      "loss: 0.487590  [99200/175341]\n",
      "loss: 0.261452  [100800/175341]\n",
      "loss: 0.753896  [102400/175341]\n",
      "loss: 0.453627  [104000/175341]\n",
      "loss: 0.642419  [105600/175341]\n",
      "loss: 0.347446  [107200/175341]\n",
      "loss: 0.282179  [108800/175341]\n",
      "loss: 0.274748  [110400/175341]\n",
      "loss: 0.758931  [112000/175341]\n",
      "loss: 0.312250  [113600/175341]\n",
      "loss: 0.512898  [115200/175341]\n",
      "loss: 0.466277  [116800/175341]\n",
      "loss: 0.736326  [118400/175341]\n",
      "loss: 0.765531  [120000/175341]\n",
      "loss: 0.912115  [121600/175341]\n",
      "loss: 0.428888  [123200/175341]\n",
      "loss: 0.586601  [124800/175341]\n",
      "loss: 0.666183  [126400/175341]\n",
      "loss: 0.354208  [128000/175341]\n",
      "loss: 0.593242  [129600/175341]\n",
      "loss: 0.599452  [131200/175341]\n",
      "loss: 0.333905  [132800/175341]\n",
      "loss: 0.634654  [134400/175341]\n",
      "loss: 0.503181  [136000/175341]\n",
      "loss: 0.227342  [137600/175341]\n",
      "loss: 1.098578  [139200/175341]\n",
      "loss: 0.718296  [140800/175341]\n",
      "loss: 0.666628  [142400/175341]\n",
      "loss: 0.599559  [144000/175341]\n",
      "loss: 0.641792  [145600/175341]\n",
      "loss: 0.971436  [147200/175341]\n",
      "loss: 0.431535  [148800/175341]\n",
      "loss: 0.339946  [150400/175341]\n",
      "loss: 0.423210  [152000/175341]\n",
      "loss: 0.842935  [153600/175341]\n",
      "loss: 0.984143  [155200/175341]\n",
      "loss: 0.319385  [156800/175341]\n",
      "loss: 0.943595  [158400/175341]\n",
      "loss: 0.442390  [160000/175341]\n",
      "loss: 0.634581  [161600/175341]\n",
      "loss: 1.168997  [163200/175341]\n",
      "loss: 0.882652  [164800/175341]\n",
      "loss: 0.933204  [166400/175341]\n",
      "loss: 0.451945  [168000/175341]\n",
      "loss: 0.414192  [169600/175341]\n",
      "loss: 0.961831  [171200/175341]\n",
      "loss: 0.453733  [172800/175341]\n",
      "loss: 0.554212  [174400/175341]\n",
      "Train Accuracy: 79.0066%\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.653108, F1-score: 71.19%, Macro_F1-Score:  33.72%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.439526  [    0/175341]\n",
      "loss: 0.684054  [ 1600/175341]\n",
      "loss: 0.671353  [ 3200/175341]\n",
      "loss: 0.297598  [ 4800/175341]\n",
      "loss: 0.860857  [ 6400/175341]\n",
      "loss: 0.383131  [ 8000/175341]\n",
      "loss: 0.354617  [ 9600/175341]\n",
      "loss: 0.777151  [11200/175341]\n",
      "loss: 0.824892  [12800/175341]\n",
      "loss: 0.659815  [14400/175341]\n",
      "loss: 0.398858  [16000/175341]\n",
      "loss: 0.350906  [17600/175341]\n",
      "loss: 0.379792  [19200/175341]\n",
      "loss: 0.186929  [20800/175341]\n",
      "loss: 0.344328  [22400/175341]\n",
      "loss: 0.375688  [24000/175341]\n",
      "loss: 0.567562  [25600/175341]\n",
      "loss: 0.358745  [27200/175341]\n",
      "loss: 0.649891  [28800/175341]\n",
      "loss: 0.906988  [30400/175341]\n",
      "loss: 0.443748  [32000/175341]\n",
      "loss: 0.357221  [33600/175341]\n",
      "loss: 1.787397  [35200/175341]\n",
      "loss: 0.355999  [36800/175341]\n",
      "loss: 0.902069  [38400/175341]\n",
      "loss: 0.655613  [40000/175341]\n",
      "loss: 0.830260  [41600/175341]\n",
      "loss: 0.526056  [43200/175341]\n",
      "loss: 0.351035  [44800/175341]\n",
      "loss: 0.479817  [46400/175341]\n",
      "loss: 0.474947  [48000/175341]\n",
      "loss: 0.389286  [49600/175341]\n",
      "loss: 0.865576  [51200/175341]\n",
      "loss: 0.344661  [52800/175341]\n",
      "loss: 0.441881  [54400/175341]\n",
      "loss: 0.425079  [56000/175341]\n",
      "loss: 0.732805  [57600/175341]\n",
      "loss: 0.469912  [59200/175341]\n",
      "loss: 0.874470  [60800/175341]\n",
      "loss: 0.653669  [62400/175341]\n",
      "loss: 0.733265  [64000/175341]\n",
      "loss: 0.644498  [65600/175341]\n",
      "loss: 0.456735  [67200/175341]\n",
      "loss: 0.672156  [68800/175341]\n",
      "loss: 0.608765  [70400/175341]\n",
      "loss: 0.626898  [72000/175341]\n",
      "loss: 0.515728  [73600/175341]\n",
      "loss: 0.427816  [75200/175341]\n",
      "loss: 0.606506  [76800/175341]\n",
      "loss: 0.371971  [78400/175341]\n",
      "loss: 0.711897  [80000/175341]\n",
      "loss: 0.334442  [81600/175341]\n",
      "loss: 0.191543  [83200/175341]\n",
      "loss: 0.265585  [84800/175341]\n",
      "loss: 0.543788  [86400/175341]\n",
      "loss: 0.484449  [88000/175341]\n",
      "loss: 0.406347  [89600/175341]\n",
      "loss: 0.597408  [91200/175341]\n",
      "loss: 0.329096  [92800/175341]\n",
      "loss: 0.325255  [94400/175341]\n",
      "loss: 0.774804  [96000/175341]\n",
      "loss: 0.315962  [97600/175341]\n",
      "loss: 0.600901  [99200/175341]\n",
      "loss: 0.343931  [100800/175341]\n",
      "loss: 0.558449  [102400/175341]\n",
      "loss: 0.637026  [104000/175341]\n",
      "loss: 0.572330  [105600/175341]\n",
      "loss: 0.183626  [107200/175341]\n",
      "loss: 0.674322  [108800/175341]\n",
      "loss: 0.490851  [110400/175341]\n",
      "loss: 0.326002  [112000/175341]\n",
      "loss: 0.498465  [113600/175341]\n",
      "loss: 0.912553  [115200/175341]\n",
      "loss: 0.682520  [116800/175341]\n",
      "loss: 0.244801  [118400/175341]\n",
      "loss: 0.523776  [120000/175341]\n",
      "loss: 0.431219  [121600/175341]\n",
      "loss: 0.300256  [123200/175341]\n",
      "loss: 0.514744  [124800/175341]\n",
      "loss: 0.295274  [126400/175341]\n",
      "loss: 0.417947  [128000/175341]\n",
      "loss: 0.377779  [129600/175341]\n",
      "loss: 0.653854  [131200/175341]\n",
      "loss: 0.320913  [132800/175341]\n",
      "loss: 0.843946  [134400/175341]\n",
      "loss: 0.501741  [136000/175341]\n",
      "loss: 0.554374  [137600/175341]\n",
      "loss: 0.462983  [139200/175341]\n",
      "loss: 0.441430  [140800/175341]\n",
      "loss: 0.471782  [142400/175341]\n",
      "loss: 0.260883  [144000/175341]\n",
      "loss: 0.404994  [145600/175341]\n",
      "loss: 0.588949  [147200/175341]\n",
      "loss: 0.153358  [148800/175341]\n",
      "loss: 0.526973  [150400/175341]\n",
      "loss: 0.341302  [152000/175341]\n",
      "loss: 0.418771  [153600/175341]\n",
      "loss: 0.483934  [155200/175341]\n",
      "loss: 0.436831  [156800/175341]\n",
      "loss: 0.709189  [158400/175341]\n",
      "loss: 0.298145  [160000/175341]\n",
      "loss: 0.500617  [161600/175341]\n",
      "loss: 0.461128  [163200/175341]\n",
      "loss: 0.418324  [164800/175341]\n",
      "loss: 0.952321  [166400/175341]\n",
      "loss: 0.294546  [168000/175341]\n",
      "loss: 0.546157  [169600/175341]\n",
      "loss: 0.401594  [171200/175341]\n",
      "loss: 0.372624  [172800/175341]\n",
      "loss: 0.324350  [174400/175341]\n",
      "Train Accuracy: 79.2359%\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.636183, F1-score: 71.90%, Macro_F1-Score:  36.08%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.386637  [    0/175341]\n",
      "loss: 0.715638  [ 1600/175341]\n",
      "loss: 0.178160  [ 3200/175341]\n",
      "loss: 0.365936  [ 4800/175341]\n",
      "loss: 1.150139  [ 6400/175341]\n",
      "loss: 0.529341  [ 8000/175341]\n",
      "loss: 0.291853  [ 9600/175341]\n",
      "loss: 0.611233  [11200/175341]\n",
      "loss: 0.337308  [12800/175341]\n",
      "loss: 0.237188  [14400/175341]\n",
      "loss: 0.533915  [16000/175341]\n",
      "loss: 0.264285  [17600/175341]\n",
      "loss: 0.471445  [19200/175341]\n",
      "loss: 0.864084  [20800/175341]\n",
      "loss: 0.352427  [22400/175341]\n",
      "loss: 0.321111  [24000/175341]\n",
      "loss: 0.674335  [25600/175341]\n",
      "loss: 0.753390  [27200/175341]\n",
      "loss: 0.924474  [28800/175341]\n",
      "loss: 0.438886  [30400/175341]\n",
      "loss: 0.562702  [32000/175341]\n",
      "loss: 0.846792  [33600/175341]\n",
      "loss: 0.429992  [35200/175341]\n",
      "loss: 0.495555  [36800/175341]\n",
      "loss: 0.449291  [38400/175341]\n",
      "loss: 0.220859  [40000/175341]\n",
      "loss: 0.429823  [41600/175341]\n",
      "loss: 0.547874  [43200/175341]\n",
      "loss: 0.488292  [44800/175341]\n",
      "loss: 0.420462  [46400/175341]\n",
      "loss: 0.596429  [48000/175341]\n",
      "loss: 0.536873  [49600/175341]\n",
      "loss: 0.485778  [51200/175341]\n",
      "loss: 0.459888  [52800/175341]\n",
      "loss: 0.396356  [54400/175341]\n",
      "loss: 0.405514  [56000/175341]\n",
      "loss: 0.594553  [57600/175341]\n",
      "loss: 0.410817  [59200/175341]\n",
      "loss: 0.711381  [60800/175341]\n",
      "loss: 0.548473  [62400/175341]\n",
      "loss: 0.486654  [64000/175341]\n",
      "loss: 0.522371  [65600/175341]\n",
      "loss: 0.361667  [67200/175341]\n",
      "loss: 0.404237  [68800/175341]\n",
      "loss: 0.233559  [70400/175341]\n",
      "loss: 0.306969  [72000/175341]\n",
      "loss: 0.489266  [73600/175341]\n",
      "loss: 0.774046  [75200/175341]\n",
      "loss: 0.858301  [76800/175341]\n",
      "loss: 0.893897  [78400/175341]\n",
      "loss: 0.689753  [80000/175341]\n",
      "loss: 0.454853  [81600/175341]\n",
      "loss: 0.205129  [83200/175341]\n",
      "loss: 0.426773  [84800/175341]\n",
      "loss: 0.352238  [86400/175341]\n",
      "loss: 0.564112  [88000/175341]\n",
      "loss: 0.387824  [89600/175341]\n",
      "loss: 0.662129  [91200/175341]\n",
      "loss: 0.948313  [92800/175341]\n",
      "loss: 0.210358  [94400/175341]\n",
      "loss: 0.499061  [96000/175341]\n",
      "loss: 0.642926  [97600/175341]\n",
      "loss: 0.280911  [99200/175341]\n",
      "loss: 0.333720  [100800/175341]\n",
      "loss: 0.686004  [102400/175341]\n",
      "loss: 0.335293  [104000/175341]\n",
      "loss: 0.959522  [105600/175341]\n",
      "loss: 0.355830  [107200/175341]\n",
      "loss: 0.483706  [108800/175341]\n",
      "loss: 0.524475  [110400/175341]\n",
      "loss: 0.203174  [112000/175341]\n",
      "loss: 0.658856  [113600/175341]\n",
      "loss: 0.342439  [115200/175341]\n",
      "loss: 0.306744  [116800/175341]\n",
      "loss: 0.812272  [118400/175341]\n",
      "loss: 0.425013  [120000/175341]\n",
      "loss: 0.150371  [121600/175341]\n",
      "loss: 0.541181  [123200/175341]\n",
      "loss: 0.611055  [124800/175341]\n",
      "loss: 0.544215  [126400/175341]\n",
      "loss: 0.698528  [128000/175341]\n",
      "loss: 0.643782  [129600/175341]\n",
      "loss: 0.327796  [131200/175341]\n",
      "loss: 0.481124  [132800/175341]\n",
      "loss: 0.232602  [134400/175341]\n",
      "loss: 0.434847  [136000/175341]\n",
      "loss: 0.454957  [137600/175341]\n",
      "loss: 0.510745  [139200/175341]\n",
      "loss: 1.010893  [140800/175341]\n",
      "loss: 0.957355  [142400/175341]\n",
      "loss: 0.285139  [144000/175341]\n",
      "loss: 0.215522  [145600/175341]\n",
      "loss: 0.914047  [147200/175341]\n",
      "loss: 0.087110  [148800/175341]\n",
      "loss: 0.206031  [150400/175341]\n",
      "loss: 1.066649  [152000/175341]\n",
      "loss: 0.716715  [153600/175341]\n",
      "loss: 0.829950  [155200/175341]\n",
      "loss: 0.254676  [156800/175341]\n",
      "loss: 0.395368  [158400/175341]\n",
      "loss: 0.351349  [160000/175341]\n",
      "loss: 0.620191  [161600/175341]\n",
      "loss: 0.257609  [163200/175341]\n",
      "loss: 0.571193  [164800/175341]\n",
      "loss: 0.597222  [166400/175341]\n",
      "loss: 0.141303  [168000/175341]\n",
      "loss: 0.808076  [169600/175341]\n",
      "loss: 0.280139  [171200/175341]\n",
      "loss: 0.573371  [172800/175341]\n",
      "loss: 0.623145  [174400/175341]\n",
      "Train Accuracy: 79.5262%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.581345, F1-score: 76.09%, Macro_F1-Score:  38.41%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.421418  [    0/175341]\n",
      "loss: 0.265084  [ 1600/175341]\n",
      "loss: 0.538590  [ 3200/175341]\n",
      "loss: 0.378016  [ 4800/175341]\n",
      "loss: 0.351074  [ 6400/175341]\n",
      "loss: 0.406845  [ 8000/175341]\n",
      "loss: 0.922890  [ 9600/175341]\n",
      "loss: 0.694864  [11200/175341]\n",
      "loss: 0.523507  [12800/175341]\n",
      "loss: 0.733707  [14400/175341]\n",
      "loss: 0.440294  [16000/175341]\n",
      "loss: 0.557431  [17600/175341]\n",
      "loss: 0.580765  [19200/175341]\n",
      "loss: 0.349611  [20800/175341]\n",
      "loss: 0.424364  [22400/175341]\n",
      "loss: 1.056869  [24000/175341]\n",
      "loss: 0.415849  [25600/175341]\n",
      "loss: 0.479627  [27200/175341]\n",
      "loss: 0.612707  [28800/175341]\n",
      "loss: 0.558250  [30400/175341]\n",
      "loss: 0.617053  [32000/175341]\n",
      "loss: 0.396019  [33600/175341]\n",
      "loss: 0.866847  [35200/175341]\n",
      "loss: 0.576283  [36800/175341]\n",
      "loss: 0.638797  [38400/175341]\n",
      "loss: 0.398847  [40000/175341]\n",
      "loss: 0.529612  [41600/175341]\n",
      "loss: 0.770439  [43200/175341]\n",
      "loss: 0.227600  [44800/175341]\n",
      "loss: 0.526326  [46400/175341]\n",
      "loss: 0.642544  [48000/175341]\n",
      "loss: 0.743022  [49600/175341]\n",
      "loss: 0.521683  [51200/175341]\n",
      "loss: 0.400919  [52800/175341]\n",
      "loss: 0.474005  [54400/175341]\n",
      "loss: 0.620596  [56000/175341]\n",
      "loss: 0.395314  [57600/175341]\n",
      "loss: 0.935765  [59200/175341]\n",
      "loss: 0.957014  [60800/175341]\n",
      "loss: 0.549208  [62400/175341]\n",
      "loss: 0.514725  [64000/175341]\n",
      "loss: 0.432992  [65600/175341]\n",
      "loss: 0.270567  [67200/175341]\n",
      "loss: 0.651404  [68800/175341]\n",
      "loss: 0.428789  [70400/175341]\n",
      "loss: 0.524815  [72000/175341]\n",
      "loss: 0.641751  [73600/175341]\n",
      "loss: 0.227844  [75200/175341]\n",
      "loss: 0.954678  [76800/175341]\n",
      "loss: 0.301044  [78400/175341]\n",
      "loss: 0.456676  [80000/175341]\n",
      "loss: 0.407270  [81600/175341]\n",
      "loss: 0.627076  [83200/175341]\n",
      "loss: 0.310828  [84800/175341]\n",
      "loss: 0.970977  [86400/175341]\n",
      "loss: 0.716089  [88000/175341]\n",
      "loss: 0.181551  [89600/175341]\n",
      "loss: 0.405938  [91200/175341]\n",
      "loss: 0.137523  [92800/175341]\n",
      "loss: 0.588708  [94400/175341]\n",
      "loss: 0.494444  [96000/175341]\n",
      "loss: 0.490633  [97600/175341]\n",
      "loss: 0.639635  [99200/175341]\n",
      "loss: 0.865368  [100800/175341]\n",
      "loss: 0.549392  [102400/175341]\n",
      "loss: 0.597307  [104000/175341]\n",
      "loss: 0.780949  [105600/175341]\n",
      "loss: 0.588779  [107200/175341]\n",
      "loss: 0.395111  [108800/175341]\n",
      "loss: 0.230477  [110400/175341]\n",
      "loss: 0.268344  [112000/175341]\n",
      "loss: 0.243476  [113600/175341]\n",
      "loss: 0.345081  [115200/175341]\n",
      "loss: 0.473003  [116800/175341]\n",
      "loss: 0.748761  [118400/175341]\n",
      "loss: 0.654661  [120000/175341]\n",
      "loss: 0.640877  [121600/175341]\n",
      "loss: 0.328955  [123200/175341]\n",
      "loss: 0.270017  [124800/175341]\n",
      "loss: 0.503032  [126400/175341]\n",
      "loss: 0.310239  [128000/175341]\n",
      "loss: 0.504621  [129600/175341]\n",
      "loss: 0.615030  [131200/175341]\n",
      "loss: 0.297312  [132800/175341]\n",
      "loss: 0.377277  [134400/175341]\n",
      "loss: 0.212665  [136000/175341]\n",
      "loss: 0.564954  [137600/175341]\n",
      "loss: 1.109596  [139200/175341]\n",
      "loss: 0.614287  [140800/175341]\n",
      "loss: 0.510319  [142400/175341]\n",
      "loss: 0.489603  [144000/175341]\n",
      "loss: 0.200638  [145600/175341]\n",
      "loss: 0.550539  [147200/175341]\n",
      "loss: 1.046181  [148800/175341]\n",
      "loss: 0.564798  [150400/175341]\n",
      "loss: 0.336563  [152000/175341]\n",
      "loss: 0.229750  [153600/175341]\n",
      "loss: 0.593479  [155200/175341]\n",
      "loss: 0.356210  [156800/175341]\n",
      "loss: 0.316424  [158400/175341]\n",
      "loss: 0.459021  [160000/175341]\n",
      "loss: 0.649878  [161600/175341]\n",
      "loss: 0.395743  [163200/175341]\n",
      "loss: 0.635393  [164800/175341]\n",
      "loss: 0.215373  [166400/175341]\n",
      "loss: 0.326988  [168000/175341]\n",
      "loss: 0.851190  [169600/175341]\n",
      "loss: 0.415659  [171200/175341]\n",
      "loss: 0.427445  [172800/175341]\n",
      "loss: 0.646393  [174400/175341]\n",
      "Train Accuracy: 79.6551%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.578561, F1-score: 75.05%, Macro_F1-Score:  38.60%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.223414  [    0/175341]\n",
      "loss: 1.198393  [ 1600/175341]\n",
      "loss: 0.270645  [ 3200/175341]\n",
      "loss: 0.500879  [ 4800/175341]\n",
      "loss: 0.957255  [ 6400/175341]\n",
      "loss: 0.511083  [ 8000/175341]\n",
      "loss: 0.338311  [ 9600/175341]\n",
      "loss: 0.249273  [11200/175341]\n",
      "loss: 0.545677  [12800/175341]\n",
      "loss: 0.335400  [14400/175341]\n",
      "loss: 0.405004  [16000/175341]\n",
      "loss: 0.312400  [17600/175341]\n",
      "loss: 0.578881  [19200/175341]\n",
      "loss: 0.459546  [20800/175341]\n",
      "loss: 0.336707  [22400/175341]\n",
      "loss: 0.516693  [24000/175341]\n",
      "loss: 0.293294  [25600/175341]\n",
      "loss: 0.515178  [27200/175341]\n",
      "loss: 0.395632  [28800/175341]\n",
      "loss: 0.529571  [30400/175341]\n",
      "loss: 0.568753  [32000/175341]\n",
      "loss: 0.457152  [33600/175341]\n",
      "loss: 0.482602  [35200/175341]\n",
      "loss: 0.400876  [36800/175341]\n",
      "loss: 0.542122  [38400/175341]\n",
      "loss: 0.279299  [40000/175341]\n",
      "loss: 0.353852  [41600/175341]\n",
      "loss: 0.458089  [43200/175341]\n",
      "loss: 0.517767  [44800/175341]\n",
      "loss: 0.544564  [46400/175341]\n",
      "loss: 0.550467  [48000/175341]\n",
      "loss: 0.743911  [49600/175341]\n",
      "loss: 0.541254  [51200/175341]\n",
      "loss: 0.527353  [52800/175341]\n",
      "loss: 0.560402  [54400/175341]\n",
      "loss: 0.330343  [56000/175341]\n",
      "loss: 0.582431  [57600/175341]\n",
      "loss: 0.788104  [59200/175341]\n",
      "loss: 0.435681  [60800/175341]\n",
      "loss: 0.423130  [62400/175341]\n",
      "loss: 0.512149  [64000/175341]\n",
      "loss: 0.460170  [65600/175341]\n",
      "loss: 0.637719  [67200/175341]\n",
      "loss: 0.481338  [68800/175341]\n",
      "loss: 0.694082  [70400/175341]\n",
      "loss: 0.443031  [72000/175341]\n",
      "loss: 0.442376  [73600/175341]\n",
      "loss: 0.403361  [75200/175341]\n",
      "loss: 0.593758  [76800/175341]\n",
      "loss: 0.465394  [78400/175341]\n",
      "loss: 0.525440  [80000/175341]\n",
      "loss: 0.579232  [81600/175341]\n",
      "loss: 0.599846  [83200/175341]\n",
      "loss: 0.490705  [84800/175341]\n",
      "loss: 0.432776  [86400/175341]\n",
      "loss: 0.597391  [88000/175341]\n",
      "loss: 0.781474  [89600/175341]\n",
      "loss: 0.404308  [91200/175341]\n",
      "loss: 0.167793  [92800/175341]\n",
      "loss: 0.433589  [94400/175341]\n",
      "loss: 0.363414  [96000/175341]\n",
      "loss: 0.591358  [97600/175341]\n",
      "loss: 0.801607  [99200/175341]\n",
      "loss: 0.200736  [100800/175341]\n",
      "loss: 0.329164  [102400/175341]\n",
      "loss: 0.504299  [104000/175341]\n",
      "loss: 0.472521  [105600/175341]\n",
      "loss: 0.396698  [107200/175341]\n",
      "loss: 0.838751  [108800/175341]\n",
      "loss: 0.632824  [110400/175341]\n",
      "loss: 0.256612  [112000/175341]\n",
      "loss: 0.809309  [113600/175341]\n",
      "loss: 0.171758  [115200/175341]\n",
      "loss: 0.467162  [116800/175341]\n",
      "loss: 0.554196  [118400/175341]\n",
      "loss: 0.518314  [120000/175341]\n",
      "loss: 0.535083  [121600/175341]\n",
      "loss: 0.435356  [123200/175341]\n",
      "loss: 0.576541  [124800/175341]\n",
      "loss: 0.541686  [126400/175341]\n",
      "loss: 0.163455  [128000/175341]\n",
      "loss: 0.352841  [129600/175341]\n",
      "loss: 0.650904  [131200/175341]\n",
      "loss: 0.241383  [132800/175341]\n",
      "loss: 0.431335  [134400/175341]\n",
      "loss: 0.401508  [136000/175341]\n",
      "loss: 0.514937  [137600/175341]\n",
      "loss: 0.296540  [139200/175341]\n",
      "loss: 0.480963  [140800/175341]\n",
      "loss: 0.265495  [142400/175341]\n",
      "loss: 0.330984  [144000/175341]\n",
      "loss: 1.193573  [145600/175341]\n",
      "loss: 0.374538  [147200/175341]\n",
      "loss: 0.321787  [148800/175341]\n",
      "loss: 0.572805  [150400/175341]\n",
      "loss: 0.915481  [152000/175341]\n",
      "loss: 0.621687  [153600/175341]\n",
      "loss: 0.174568  [155200/175341]\n",
      "loss: 0.267620  [156800/175341]\n",
      "loss: 0.460539  [158400/175341]\n",
      "loss: 0.234762  [160000/175341]\n",
      "loss: 0.522576  [161600/175341]\n",
      "loss: 0.351169  [163200/175341]\n",
      "loss: 0.758791  [164800/175341]\n",
      "loss: 0.432459  [166400/175341]\n",
      "loss: 0.561649  [168000/175341]\n",
      "loss: 0.724414  [169600/175341]\n",
      "loss: 0.660354  [171200/175341]\n",
      "loss: 0.336170  [172800/175341]\n",
      "loss: 0.535572  [174400/175341]\n",
      "Train Accuracy: 79.8570%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.574021, F1-score: 74.49%, Macro_F1-Score:  38.54%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.718583  [    0/175341]\n",
      "loss: 0.847048  [ 1600/175341]\n",
      "loss: 0.280768  [ 3200/175341]\n",
      "loss: 0.191439  [ 4800/175341]\n",
      "loss: 0.214962  [ 6400/175341]\n",
      "loss: 0.370691  [ 8000/175341]\n",
      "loss: 0.464890  [ 9600/175341]\n",
      "loss: 0.990605  [11200/175341]\n",
      "loss: 0.416639  [12800/175341]\n",
      "loss: 0.499344  [14400/175341]\n",
      "loss: 0.475841  [16000/175341]\n",
      "loss: 0.501141  [17600/175341]\n",
      "loss: 0.264915  [19200/175341]\n",
      "loss: 0.413611  [20800/175341]\n",
      "loss: 0.213107  [22400/175341]\n",
      "loss: 0.733434  [24000/175341]\n",
      "loss: 0.194737  [25600/175341]\n",
      "loss: 0.073954  [27200/175341]\n",
      "loss: 0.555704  [28800/175341]\n",
      "loss: 0.650473  [30400/175341]\n",
      "loss: 0.301872  [32000/175341]\n",
      "loss: 0.517261  [33600/175341]\n",
      "loss: 0.275930  [35200/175341]\n",
      "loss: 0.661734  [36800/175341]\n",
      "loss: 1.272425  [38400/175341]\n",
      "loss: 0.476519  [40000/175341]\n",
      "loss: 0.845161  [41600/175341]\n",
      "loss: 0.437823  [43200/175341]\n",
      "loss: 0.242175  [44800/175341]\n",
      "loss: 0.482681  [46400/175341]\n",
      "loss: 0.797647  [48000/175341]\n",
      "loss: 0.260908  [49600/175341]\n",
      "loss: 0.196424  [51200/175341]\n",
      "loss: 0.550741  [52800/175341]\n",
      "loss: 0.627449  [54400/175341]\n",
      "loss: 0.456351  [56000/175341]\n",
      "loss: 0.282579  [57600/175341]\n",
      "loss: 0.228227  [59200/175341]\n",
      "loss: 0.274438  [60800/175341]\n",
      "loss: 0.414891  [62400/175341]\n",
      "loss: 0.278712  [64000/175341]\n",
      "loss: 0.471258  [65600/175341]\n",
      "loss: 0.488336  [67200/175341]\n",
      "loss: 0.598585  [68800/175341]\n",
      "loss: 0.397160  [70400/175341]\n",
      "loss: 0.434275  [72000/175341]\n",
      "loss: 0.401210  [73600/175341]\n",
      "loss: 0.481590  [75200/175341]\n",
      "loss: 0.484750  [76800/175341]\n",
      "loss: 0.411207  [78400/175341]\n",
      "loss: 0.218584  [80000/175341]\n",
      "loss: 0.481216  [81600/175341]\n",
      "loss: 0.560790  [83200/175341]\n",
      "loss: 0.709102  [84800/175341]\n",
      "loss: 0.292673  [86400/175341]\n",
      "loss: 0.720619  [88000/175341]\n",
      "loss: 0.454142  [89600/175341]\n",
      "loss: 0.574272  [91200/175341]\n",
      "loss: 0.883369  [92800/175341]\n",
      "loss: 0.341390  [94400/175341]\n",
      "loss: 0.270438  [96000/175341]\n",
      "loss: 0.658085  [97600/175341]\n",
      "loss: 0.381781  [99200/175341]\n",
      "loss: 0.264227  [100800/175341]\n",
      "loss: 0.602926  [102400/175341]\n",
      "loss: 0.380148  [104000/175341]\n",
      "loss: 0.612621  [105600/175341]\n",
      "loss: 0.349217  [107200/175341]\n",
      "loss: 0.760672  [108800/175341]\n",
      "loss: 0.287114  [110400/175341]\n",
      "loss: 0.328948  [112000/175341]\n",
      "loss: 0.648739  [113600/175341]\n",
      "loss: 1.014283  [115200/175341]\n",
      "loss: 0.315067  [116800/175341]\n",
      "loss: 0.544523  [118400/175341]\n",
      "loss: 0.346126  [120000/175341]\n",
      "loss: 0.529093  [121600/175341]\n",
      "loss: 0.221498  [123200/175341]\n",
      "loss: 0.814210  [124800/175341]\n",
      "loss: 0.567844  [126400/175341]\n",
      "loss: 0.359157  [128000/175341]\n",
      "loss: 0.978753  [129600/175341]\n",
      "loss: 0.268020  [131200/175341]\n",
      "loss: 0.360998  [132800/175341]\n",
      "loss: 0.916839  [134400/175341]\n",
      "loss: 0.599281  [136000/175341]\n",
      "loss: 0.369385  [137600/175341]\n",
      "loss: 0.439653  [139200/175341]\n",
      "loss: 0.388785  [140800/175341]\n",
      "loss: 0.704709  [142400/175341]\n",
      "loss: 0.527552  [144000/175341]\n",
      "loss: 0.904389  [145600/175341]\n",
      "loss: 0.518845  [147200/175341]\n",
      "loss: 0.421353  [148800/175341]\n",
      "loss: 0.356848  [150400/175341]\n",
      "loss: 0.515872  [152000/175341]\n",
      "loss: 0.565618  [153600/175341]\n",
      "loss: 0.417525  [155200/175341]\n",
      "loss: 0.958617  [156800/175341]\n",
      "loss: 0.437002  [158400/175341]\n",
      "loss: 0.493473  [160000/175341]\n",
      "loss: 0.419260  [161600/175341]\n",
      "loss: 0.933776  [163200/175341]\n",
      "loss: 0.876434  [164800/175341]\n",
      "loss: 0.524846  [166400/175341]\n",
      "loss: 0.479921  [168000/175341]\n",
      "loss: 0.242179  [169600/175341]\n",
      "loss: 0.490362  [171200/175341]\n",
      "loss: 0.297334  [172800/175341]\n",
      "loss: 0.555771  [174400/175341]\n",
      "Train Accuracy: 79.9208%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.572160, F1-score: 75.85%, Macro_F1-Score:  39.34%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.385907  [    0/175341]\n",
      "loss: 0.586760  [ 1600/175341]\n",
      "loss: 0.628942  [ 3200/175341]\n",
      "loss: 0.800727  [ 4800/175341]\n",
      "loss: 0.533650  [ 6400/175341]\n",
      "loss: 0.223620  [ 8000/175341]\n",
      "loss: 0.546415  [ 9600/175341]\n",
      "loss: 0.797686  [11200/175341]\n",
      "loss: 0.773491  [12800/175341]\n",
      "loss: 0.509156  [14400/175341]\n",
      "loss: 0.457322  [16000/175341]\n",
      "loss: 0.491806  [17600/175341]\n",
      "loss: 0.433178  [19200/175341]\n",
      "loss: 0.781944  [20800/175341]\n",
      "loss: 0.475395  [22400/175341]\n",
      "loss: 1.225351  [24000/175341]\n",
      "loss: 0.284508  [25600/175341]\n",
      "loss: 0.362052  [27200/175341]\n",
      "loss: 0.794450  [28800/175341]\n",
      "loss: 0.533037  [30400/175341]\n",
      "loss: 0.447281  [32000/175341]\n",
      "loss: 0.224163  [33600/175341]\n",
      "loss: 0.567959  [35200/175341]\n",
      "loss: 0.455173  [36800/175341]\n",
      "loss: 0.313373  [38400/175341]\n",
      "loss: 0.904526  [40000/175341]\n",
      "loss: 0.391509  [41600/175341]\n",
      "loss: 0.651404  [43200/175341]\n",
      "loss: 0.267371  [44800/175341]\n",
      "loss: 0.996129  [46400/175341]\n",
      "loss: 0.271396  [48000/175341]\n",
      "loss: 0.269430  [49600/175341]\n",
      "loss: 0.743280  [51200/175341]\n",
      "loss: 0.554475  [52800/175341]\n",
      "loss: 0.262970  [54400/175341]\n",
      "loss: 0.430362  [56000/175341]\n",
      "loss: 0.476222  [57600/175341]\n",
      "loss: 0.518303  [59200/175341]\n",
      "loss: 0.455901  [60800/175341]\n",
      "loss: 0.485381  [62400/175341]\n",
      "loss: 0.572123  [64000/175341]\n",
      "loss: 0.264745  [65600/175341]\n",
      "loss: 0.196756  [67200/175341]\n",
      "loss: 1.011153  [68800/175341]\n",
      "loss: 0.400133  [70400/175341]\n",
      "loss: 0.282273  [72000/175341]\n",
      "loss: 0.408427  [73600/175341]\n",
      "loss: 0.730713  [75200/175341]\n",
      "loss: 0.152457  [76800/175341]\n",
      "loss: 0.428187  [78400/175341]\n",
      "loss: 0.424101  [80000/175341]\n",
      "loss: 0.306817  [81600/175341]\n",
      "loss: 0.283528  [83200/175341]\n",
      "loss: 0.392443  [84800/175341]\n",
      "loss: 0.541380  [86400/175341]\n",
      "loss: 0.717044  [88000/175341]\n",
      "loss: 0.334505  [89600/175341]\n",
      "loss: 0.366224  [91200/175341]\n",
      "loss: 0.313806  [92800/175341]\n",
      "loss: 0.831977  [94400/175341]\n",
      "loss: 0.371307  [96000/175341]\n",
      "loss: 0.643787  [97600/175341]\n",
      "loss: 0.494526  [99200/175341]\n",
      "loss: 0.231345  [100800/175341]\n",
      "loss: 0.468090  [102400/175341]\n",
      "loss: 0.536447  [104000/175341]\n",
      "loss: 0.763065  [105600/175341]\n",
      "loss: 0.690536  [107200/175341]\n",
      "loss: 0.591363  [108800/175341]\n",
      "loss: 0.502556  [110400/175341]\n",
      "loss: 0.491816  [112000/175341]\n",
      "loss: 0.601570  [113600/175341]\n",
      "loss: 0.663329  [115200/175341]\n",
      "loss: 0.994264  [116800/175341]\n",
      "loss: 0.729293  [118400/175341]\n",
      "loss: 0.241238  [120000/175341]\n",
      "loss: 0.493238  [121600/175341]\n",
      "loss: 0.130726  [123200/175341]\n",
      "loss: 0.777579  [124800/175341]\n",
      "loss: 0.488313  [126400/175341]\n",
      "loss: 0.287233  [128000/175341]\n",
      "loss: 0.858713  [129600/175341]\n",
      "loss: 0.617952  [131200/175341]\n",
      "loss: 0.505566  [132800/175341]\n",
      "loss: 0.471056  [134400/175341]\n",
      "loss: 0.239781  [136000/175341]\n",
      "loss: 0.130738  [137600/175341]\n",
      "loss: 0.150646  [139200/175341]\n",
      "loss: 0.232178  [140800/175341]\n",
      "loss: 0.914895  [142400/175341]\n",
      "loss: 0.558580  [144000/175341]\n",
      "loss: 0.374629  [145600/175341]\n",
      "loss: 0.329063  [147200/175341]\n",
      "loss: 0.452953  [148800/175341]\n",
      "loss: 0.758864  [150400/175341]\n",
      "loss: 0.395771  [152000/175341]\n",
      "loss: 0.733567  [153600/175341]\n",
      "loss: 0.745297  [155200/175341]\n",
      "loss: 0.284832  [156800/175341]\n",
      "loss: 0.736554  [158400/175341]\n",
      "loss: 0.394043  [160000/175341]\n",
      "loss: 0.399950  [161600/175341]\n",
      "loss: 0.544473  [163200/175341]\n",
      "loss: 0.715253  [164800/175341]\n",
      "loss: 0.505707  [166400/175341]\n",
      "loss: 0.511465  [168000/175341]\n",
      "loss: 0.726000  [169600/175341]\n",
      "loss: 0.632070  [171200/175341]\n",
      "loss: 0.666247  [172800/175341]\n",
      "loss: 0.239815  [174400/175341]\n",
      "Train Accuracy: 80.0292%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.581926, F1-score: 74.36%, Macro_F1-Score:  38.26%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.455780  [    0/175341]\n",
      "loss: 0.679642  [ 1600/175341]\n",
      "loss: 0.517499  [ 3200/175341]\n",
      "loss: 0.794086  [ 4800/175341]\n",
      "loss: 0.393199  [ 6400/175341]\n",
      "loss: 0.615940  [ 8000/175341]\n",
      "loss: 0.602507  [ 9600/175341]\n",
      "loss: 0.246328  [11200/175341]\n",
      "loss: 0.608950  [12800/175341]\n",
      "loss: 0.653336  [14400/175341]\n",
      "loss: 0.284327  [16000/175341]\n",
      "loss: 0.544135  [17600/175341]\n",
      "loss: 0.603679  [19200/175341]\n",
      "loss: 0.198361  [20800/175341]\n",
      "loss: 0.375499  [22400/175341]\n",
      "loss: 0.352208  [24000/175341]\n",
      "loss: 0.549191  [25600/175341]\n",
      "loss: 0.458529  [27200/175341]\n",
      "loss: 0.546890  [28800/175341]\n",
      "loss: 0.342674  [30400/175341]\n",
      "loss: 0.344250  [32000/175341]\n",
      "loss: 0.455771  [33600/175341]\n",
      "loss: 0.696628  [35200/175341]\n",
      "loss: 0.738001  [36800/175341]\n",
      "loss: 0.504977  [38400/175341]\n",
      "loss: 0.267182  [40000/175341]\n",
      "loss: 0.524708  [41600/175341]\n",
      "loss: 0.418937  [43200/175341]\n",
      "loss: 0.140957  [44800/175341]\n",
      "loss: 0.385239  [46400/175341]\n",
      "loss: 0.996872  [48000/175341]\n",
      "loss: 0.779426  [49600/175341]\n",
      "loss: 0.311390  [51200/175341]\n",
      "loss: 0.654356  [52800/175341]\n",
      "loss: 0.268891  [54400/175341]\n",
      "loss: 0.367951  [56000/175341]\n",
      "loss: 0.203061  [57600/175341]\n",
      "loss: 0.350168  [59200/175341]\n",
      "loss: 0.208978  [60800/175341]\n",
      "loss: 0.394778  [62400/175341]\n",
      "loss: 0.841522  [64000/175341]\n",
      "loss: 0.627722  [65600/175341]\n",
      "loss: 0.404568  [67200/175341]\n",
      "loss: 0.162299  [68800/175341]\n",
      "loss: 0.882984  [70400/175341]\n",
      "loss: 0.680195  [72000/175341]\n",
      "loss: 0.512931  [73600/175341]\n",
      "loss: 0.109234  [75200/175341]\n",
      "loss: 0.753662  [76800/175341]\n",
      "loss: 0.729205  [78400/175341]\n",
      "loss: 0.626037  [80000/175341]\n",
      "loss: 0.713805  [81600/175341]\n",
      "loss: 0.576389  [83200/175341]\n",
      "loss: 0.488768  [84800/175341]\n",
      "loss: 0.542689  [86400/175341]\n",
      "loss: 0.225048  [88000/175341]\n",
      "loss: 0.931741  [89600/175341]\n",
      "loss: 0.339604  [91200/175341]\n",
      "loss: 0.642927  [92800/175341]\n",
      "loss: 0.443757  [94400/175341]\n",
      "loss: 0.253022  [96000/175341]\n",
      "loss: 0.681572  [97600/175341]\n",
      "loss: 0.480750  [99200/175341]\n",
      "loss: 0.525116  [100800/175341]\n",
      "loss: 0.415015  [102400/175341]\n",
      "loss: 0.126514  [104000/175341]\n",
      "loss: 0.224060  [105600/175341]\n",
      "loss: 0.628054  [107200/175341]\n",
      "loss: 0.183438  [108800/175341]\n",
      "loss: 0.694979  [110400/175341]\n",
      "loss: 0.250513  [112000/175341]\n",
      "loss: 0.371039  [113600/175341]\n",
      "loss: 0.469644  [115200/175341]\n",
      "loss: 0.419240  [116800/175341]\n",
      "loss: 0.501193  [118400/175341]\n",
      "loss: 0.792171  [120000/175341]\n",
      "loss: 0.506886  [121600/175341]\n",
      "loss: 0.764698  [123200/175341]\n",
      "loss: 0.357191  [124800/175341]\n",
      "loss: 0.408109  [126400/175341]\n",
      "loss: 0.673949  [128000/175341]\n",
      "loss: 0.522999  [129600/175341]\n",
      "loss: 0.323302  [131200/175341]\n",
      "loss: 0.381404  [132800/175341]\n",
      "loss: 0.284136  [134400/175341]\n",
      "loss: 0.048321  [136000/175341]\n",
      "loss: 0.601563  [137600/175341]\n",
      "loss: 0.407754  [139200/175341]\n",
      "loss: 0.606031  [140800/175341]\n",
      "loss: 0.172027  [142400/175341]\n",
      "loss: 0.280391  [144000/175341]\n",
      "loss: 0.407832  [145600/175341]\n",
      "loss: 0.883978  [147200/175341]\n",
      "loss: 0.498464  [148800/175341]\n",
      "loss: 0.422640  [150400/175341]\n",
      "loss: 0.337961  [152000/175341]\n",
      "loss: 0.156255  [153600/175341]\n",
      "loss: 0.524307  [155200/175341]\n",
      "loss: 0.637673  [156800/175341]\n",
      "loss: 0.847613  [158400/175341]\n",
      "loss: 0.242674  [160000/175341]\n",
      "loss: 0.324332  [161600/175341]\n",
      "loss: 0.311183  [163200/175341]\n",
      "loss: 0.533411  [164800/175341]\n",
      "loss: 0.337896  [166400/175341]\n",
      "loss: 0.647105  [168000/175341]\n",
      "loss: 0.867151  [169600/175341]\n",
      "loss: 0.262404  [171200/175341]\n",
      "loss: 0.225042  [172800/175341]\n",
      "loss: 0.580542  [174400/175341]\n",
      "Train Accuracy: 80.0560%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.593788, F1-score: 73.94%, Macro_F1-Score:  38.11%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.288880  [    0/175341]\n",
      "loss: 0.550777  [ 1600/175341]\n",
      "loss: 0.567189  [ 3200/175341]\n",
      "loss: 0.423262  [ 4800/175341]\n",
      "loss: 0.561872  [ 6400/175341]\n",
      "loss: 0.300169  [ 8000/175341]\n",
      "loss: 0.408498  [ 9600/175341]\n",
      "loss: 0.545836  [11200/175341]\n",
      "loss: 0.360949  [12800/175341]\n",
      "loss: 0.717436  [14400/175341]\n",
      "loss: 0.474256  [16000/175341]\n",
      "loss: 0.818259  [17600/175341]\n",
      "loss: 0.470589  [19200/175341]\n",
      "loss: 1.097434  [20800/175341]\n",
      "loss: 0.804428  [22400/175341]\n",
      "loss: 0.501124  [24000/175341]\n",
      "loss: 0.306483  [25600/175341]\n",
      "loss: 0.477639  [27200/175341]\n",
      "loss: 0.466081  [28800/175341]\n",
      "loss: 0.261156  [30400/175341]\n",
      "loss: 0.319039  [32000/175341]\n",
      "loss: 0.556140  [33600/175341]\n",
      "loss: 0.687089  [35200/175341]\n",
      "loss: 0.320967  [36800/175341]\n",
      "loss: 0.657596  [38400/175341]\n",
      "loss: 0.248919  [40000/175341]\n",
      "loss: 0.580552  [41600/175341]\n",
      "loss: 0.552412  [43200/175341]\n",
      "loss: 0.564941  [44800/175341]\n",
      "loss: 0.309190  [46400/175341]\n",
      "loss: 1.014672  [48000/175341]\n",
      "loss: 0.279856  [49600/175341]\n",
      "loss: 0.458029  [51200/175341]\n",
      "loss: 0.541018  [52800/175341]\n",
      "loss: 0.474157  [54400/175341]\n",
      "loss: 0.588735  [56000/175341]\n",
      "loss: 0.522027  [57600/175341]\n",
      "loss: 0.559750  [59200/175341]\n",
      "loss: 0.792265  [60800/175341]\n",
      "loss: 0.440681  [62400/175341]\n",
      "loss: 0.603194  [64000/175341]\n",
      "loss: 0.389490  [65600/175341]\n",
      "loss: 0.616823  [67200/175341]\n",
      "loss: 0.469248  [68800/175341]\n",
      "loss: 0.989284  [70400/175341]\n",
      "loss: 0.791634  [72000/175341]\n",
      "loss: 0.770425  [73600/175341]\n",
      "loss: 0.346555  [75200/175341]\n",
      "loss: 0.557663  [76800/175341]\n",
      "loss: 0.592444  [78400/175341]\n",
      "loss: 0.384482  [80000/175341]\n",
      "loss: 0.650239  [81600/175341]\n",
      "loss: 0.267245  [83200/175341]\n",
      "loss: 0.449678  [84800/175341]\n",
      "loss: 0.847685  [86400/175341]\n",
      "loss: 0.645211  [88000/175341]\n",
      "loss: 0.350169  [89600/175341]\n",
      "loss: 0.233615  [91200/175341]\n",
      "loss: 0.316919  [92800/175341]\n",
      "loss: 0.370543  [94400/175341]\n",
      "loss: 0.192205  [96000/175341]\n",
      "loss: 0.369307  [97600/175341]\n",
      "loss: 0.142995  [99200/175341]\n",
      "loss: 0.433142  [100800/175341]\n",
      "loss: 0.307077  [102400/175341]\n",
      "loss: 0.663924  [104000/175341]\n",
      "loss: 0.417163  [105600/175341]\n",
      "loss: 0.539105  [107200/175341]\n",
      "loss: 0.678911  [108800/175341]\n",
      "loss: 0.596159  [110400/175341]\n",
      "loss: 0.230146  [112000/175341]\n",
      "loss: 0.536328  [113600/175341]\n",
      "loss: 0.432038  [115200/175341]\n",
      "loss: 0.622994  [116800/175341]\n",
      "loss: 0.486564  [118400/175341]\n",
      "loss: 0.232524  [120000/175341]\n",
      "loss: 0.462787  [121600/175341]\n",
      "loss: 0.393711  [123200/175341]\n",
      "loss: 0.358451  [124800/175341]\n",
      "loss: 0.415359  [126400/175341]\n",
      "loss: 0.333439  [128000/175341]\n",
      "loss: 0.642994  [129600/175341]\n",
      "loss: 0.787635  [131200/175341]\n",
      "loss: 0.242191  [132800/175341]\n",
      "loss: 0.458815  [134400/175341]\n",
      "loss: 0.377060  [136000/175341]\n",
      "loss: 0.533750  [137600/175341]\n",
      "loss: 0.221213  [139200/175341]\n",
      "loss: 0.451954  [140800/175341]\n",
      "loss: 0.435565  [142400/175341]\n",
      "loss: 0.404477  [144000/175341]\n",
      "loss: 0.576950  [145600/175341]\n",
      "loss: 0.408077  [147200/175341]\n",
      "loss: 0.343397  [148800/175341]\n",
      "loss: 1.116685  [150400/175341]\n",
      "loss: 0.768754  [152000/175341]\n",
      "loss: 0.332174  [153600/175341]\n",
      "loss: 0.468391  [155200/175341]\n",
      "loss: 0.521035  [156800/175341]\n",
      "loss: 0.317782  [158400/175341]\n",
      "loss: 0.858883  [160000/175341]\n",
      "loss: 0.416468  [161600/175341]\n",
      "loss: 0.777277  [163200/175341]\n",
      "loss: 0.173288  [164800/175341]\n",
      "loss: 0.090242  [166400/175341]\n",
      "loss: 0.441871  [168000/175341]\n",
      "loss: 0.656086  [169600/175341]\n",
      "loss: 0.477994  [171200/175341]\n",
      "loss: 0.375107  [172800/175341]\n",
      "loss: 0.475385  [174400/175341]\n",
      "Train Accuracy: 80.0931%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.547516, F1-score: 76.16%, Macro_F1-Score:  39.43%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.291529  [    0/175341]\n",
      "loss: 0.773972  [ 1600/175341]\n",
      "loss: 0.405406  [ 3200/175341]\n",
      "loss: 0.596017  [ 4800/175341]\n",
      "loss: 0.628913  [ 6400/175341]\n",
      "loss: 0.559237  [ 8000/175341]\n",
      "loss: 0.610821  [ 9600/175341]\n",
      "loss: 0.491868  [11200/175341]\n",
      "loss: 0.408220  [12800/175341]\n",
      "loss: 0.280511  [14400/175341]\n",
      "loss: 0.418137  [16000/175341]\n",
      "loss: 0.777778  [17600/175341]\n",
      "loss: 0.589662  [19200/175341]\n",
      "loss: 0.228733  [20800/175341]\n",
      "loss: 0.206673  [22400/175341]\n",
      "loss: 0.296950  [24000/175341]\n",
      "loss: 0.398502  [25600/175341]\n",
      "loss: 0.300492  [27200/175341]\n",
      "loss: 0.457690  [28800/175341]\n",
      "loss: 0.666808  [30400/175341]\n",
      "loss: 0.491192  [32000/175341]\n",
      "loss: 0.891252  [33600/175341]\n",
      "loss: 0.444258  [35200/175341]\n",
      "loss: 0.320859  [36800/175341]\n",
      "loss: 0.563832  [38400/175341]\n",
      "loss: 0.251223  [40000/175341]\n",
      "loss: 0.346985  [41600/175341]\n",
      "loss: 0.191459  [43200/175341]\n",
      "loss: 0.543410  [44800/175341]\n",
      "loss: 0.607077  [46400/175341]\n",
      "loss: 0.244885  [48000/175341]\n",
      "loss: 0.460895  [49600/175341]\n",
      "loss: 0.395484  [51200/175341]\n",
      "loss: 0.074854  [52800/175341]\n",
      "loss: 0.894088  [54400/175341]\n",
      "loss: 1.034508  [56000/175341]\n",
      "loss: 0.407918  [57600/175341]\n",
      "loss: 0.179633  [59200/175341]\n",
      "loss: 0.832640  [60800/175341]\n",
      "loss: 0.363873  [62400/175341]\n",
      "loss: 0.548522  [64000/175341]\n",
      "loss: 0.329780  [65600/175341]\n",
      "loss: 0.606747  [67200/175341]\n",
      "loss: 0.583118  [68800/175341]\n",
      "loss: 0.302527  [70400/175341]\n",
      "loss: 0.399187  [72000/175341]\n",
      "loss: 0.111767  [73600/175341]\n",
      "loss: 0.540375  [75200/175341]\n",
      "loss: 0.686944  [76800/175341]\n",
      "loss: 0.281428  [78400/175341]\n",
      "loss: 0.385604  [80000/175341]\n",
      "loss: 0.591962  [81600/175341]\n",
      "loss: 0.901118  [83200/175341]\n",
      "loss: 0.564897  [84800/175341]\n",
      "loss: 0.819909  [86400/175341]\n",
      "loss: 0.601325  [88000/175341]\n",
      "loss: 0.450768  [89600/175341]\n",
      "loss: 0.733374  [91200/175341]\n",
      "loss: 0.231317  [92800/175341]\n",
      "loss: 0.928199  [94400/175341]\n",
      "loss: 0.246959  [96000/175341]\n",
      "loss: 0.259397  [97600/175341]\n",
      "loss: 0.114986  [99200/175341]\n",
      "loss: 0.845817  [100800/175341]\n",
      "loss: 0.726642  [102400/175341]\n",
      "loss: 0.351735  [104000/175341]\n",
      "loss: 0.229162  [105600/175341]\n",
      "loss: 0.733063  [107200/175341]\n",
      "loss: 0.352754  [108800/175341]\n",
      "loss: 0.706546  [110400/175341]\n",
      "loss: 0.459471  [112000/175341]\n",
      "loss: 1.019280  [113600/175341]\n",
      "loss: 0.422128  [115200/175341]\n",
      "loss: 0.443256  [116800/175341]\n",
      "loss: 0.292408  [118400/175341]\n",
      "loss: 0.308314  [120000/175341]\n",
      "loss: 0.292534  [121600/175341]\n",
      "loss: 0.552535  [123200/175341]\n",
      "loss: 0.550086  [124800/175341]\n",
      "loss: 0.303243  [126400/175341]\n",
      "loss: 0.290580  [128000/175341]\n",
      "loss: 0.222030  [129600/175341]\n",
      "loss: 1.080178  [131200/175341]\n",
      "loss: 0.181578  [132800/175341]\n",
      "loss: 0.282347  [134400/175341]\n",
      "loss: 0.270810  [136000/175341]\n",
      "loss: 0.349030  [137600/175341]\n",
      "loss: 0.424136  [139200/175341]\n",
      "loss: 0.622486  [140800/175341]\n",
      "loss: 0.468056  [142400/175341]\n",
      "loss: 0.703986  [144000/175341]\n",
      "loss: 0.599076  [145600/175341]\n",
      "loss: 0.419469  [147200/175341]\n",
      "loss: 0.288901  [148800/175341]\n",
      "loss: 0.919345  [150400/175341]\n",
      "loss: 0.383319  [152000/175341]\n",
      "loss: 0.360088  [153600/175341]\n",
      "loss: 0.238394  [155200/175341]\n",
      "loss: 0.280083  [156800/175341]\n",
      "loss: 1.010209  [158400/175341]\n",
      "loss: 1.147244  [160000/175341]\n",
      "loss: 0.845217  [161600/175341]\n",
      "loss: 0.621559  [163200/175341]\n",
      "loss: 0.400258  [164800/175341]\n",
      "loss: 0.493735  [166400/175341]\n",
      "loss: 0.535970  [168000/175341]\n",
      "loss: 0.632503  [169600/175341]\n",
      "loss: 0.598399  [171200/175341]\n",
      "loss: 0.472263  [172800/175341]\n",
      "loss: 1.085580  [174400/175341]\n",
      "Train Accuracy: 80.1678%\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.600561, F1-score: 73.25%, Macro_F1-Score:  38.14%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.336586  [    0/175341]\n",
      "loss: 0.802863  [ 1600/175341]\n",
      "loss: 0.831851  [ 3200/175341]\n",
      "loss: 0.703514  [ 4800/175341]\n",
      "loss: 0.340091  [ 6400/175341]\n",
      "loss: 0.662223  [ 8000/175341]\n",
      "loss: 0.662778  [ 9600/175341]\n",
      "loss: 0.312789  [11200/175341]\n",
      "loss: 0.379602  [12800/175341]\n",
      "loss: 0.409319  [14400/175341]\n",
      "loss: 0.516899  [16000/175341]\n",
      "loss: 0.706797  [17600/175341]\n",
      "loss: 0.462516  [19200/175341]\n",
      "loss: 0.267968  [20800/175341]\n",
      "loss: 0.668999  [22400/175341]\n",
      "loss: 0.354141  [24000/175341]\n",
      "loss: 0.460162  [25600/175341]\n",
      "loss: 0.387132  [27200/175341]\n",
      "loss: 0.235621  [28800/175341]\n",
      "loss: 0.535138  [30400/175341]\n",
      "loss: 0.785899  [32000/175341]\n",
      "loss: 0.578782  [33600/175341]\n",
      "loss: 0.283745  [35200/175341]\n",
      "loss: 0.261835  [36800/175341]\n",
      "loss: 0.680678  [38400/175341]\n",
      "loss: 0.727224  [40000/175341]\n",
      "loss: 0.577958  [41600/175341]\n",
      "loss: 0.459518  [43200/175341]\n",
      "loss: 0.250110  [44800/175341]\n",
      "loss: 0.121109  [46400/175341]\n",
      "loss: 0.226924  [48000/175341]\n",
      "loss: 0.815605  [49600/175341]\n",
      "loss: 0.527428  [51200/175341]\n",
      "loss: 0.637593  [52800/175341]\n",
      "loss: 0.411152  [54400/175341]\n",
      "loss: 0.651330  [56000/175341]\n",
      "loss: 0.289888  [57600/175341]\n",
      "loss: 0.633282  [59200/175341]\n",
      "loss: 0.725739  [60800/175341]\n",
      "loss: 0.655722  [62400/175341]\n",
      "loss: 0.345055  [64000/175341]\n",
      "loss: 0.630281  [65600/175341]\n",
      "loss: 0.162876  [67200/175341]\n",
      "loss: 0.580233  [68800/175341]\n",
      "loss: 0.508539  [70400/175341]\n",
      "loss: 1.138640  [72000/175341]\n",
      "loss: 0.435962  [73600/175341]\n",
      "loss: 0.266167  [75200/175341]\n",
      "loss: 0.556873  [76800/175341]\n",
      "loss: 0.762607  [78400/175341]\n",
      "loss: 0.364630  [80000/175341]\n",
      "loss: 0.409700  [81600/175341]\n",
      "loss: 0.137559  [83200/175341]\n",
      "loss: 0.443683  [84800/175341]\n",
      "loss: 0.880105  [86400/175341]\n",
      "loss: 0.286700  [88000/175341]\n",
      "loss: 0.176041  [89600/175341]\n",
      "loss: 0.489750  [91200/175341]\n",
      "loss: 0.670591  [92800/175341]\n",
      "loss: 0.622686  [94400/175341]\n",
      "loss: 0.671454  [96000/175341]\n",
      "loss: 0.425651  [97600/175341]\n",
      "loss: 0.595100  [99200/175341]\n",
      "loss: 0.179283  [100800/175341]\n",
      "loss: 0.415561  [102400/175341]\n",
      "loss: 0.599793  [104000/175341]\n",
      "loss: 0.570762  [105600/175341]\n",
      "loss: 0.481804  [107200/175341]\n",
      "loss: 0.085008  [108800/175341]\n",
      "loss: 0.445515  [110400/175341]\n",
      "loss: 0.737700  [112000/175341]\n",
      "loss: 0.325367  [113600/175341]\n",
      "loss: 0.421663  [115200/175341]\n",
      "loss: 0.452489  [116800/175341]\n",
      "loss: 0.401501  [118400/175341]\n",
      "loss: 0.284861  [120000/175341]\n",
      "loss: 0.429768  [121600/175341]\n",
      "loss: 0.239955  [123200/175341]\n",
      "loss: 0.575892  [124800/175341]\n",
      "loss: 0.292831  [126400/175341]\n",
      "loss: 0.430736  [128000/175341]\n",
      "loss: 0.410327  [129600/175341]\n",
      "loss: 0.224887  [131200/175341]\n",
      "loss: 0.359796  [132800/175341]\n",
      "loss: 0.675485  [134400/175341]\n",
      "loss: 0.507973  [136000/175341]\n",
      "loss: 0.548099  [137600/175341]\n",
      "loss: 0.622098  [139200/175341]\n",
      "loss: 0.389493  [140800/175341]\n",
      "loss: 0.441624  [142400/175341]\n",
      "loss: 0.491678  [144000/175341]\n",
      "loss: 0.428985  [145600/175341]\n",
      "loss: 0.547493  [147200/175341]\n",
      "loss: 0.314369  [148800/175341]\n",
      "loss: 0.479762  [150400/175341]\n",
      "loss: 0.970641  [152000/175341]\n",
      "loss: 0.387507  [153600/175341]\n",
      "loss: 0.439563  [155200/175341]\n",
      "loss: 0.343229  [156800/175341]\n",
      "loss: 0.595725  [158400/175341]\n",
      "loss: 1.040153  [160000/175341]\n",
      "loss: 0.273286  [161600/175341]\n",
      "loss: 0.287525  [163200/175341]\n",
      "loss: 0.973844  [164800/175341]\n",
      "loss: 0.353953  [166400/175341]\n",
      "loss: 0.396414  [168000/175341]\n",
      "loss: 0.373897  [169600/175341]\n",
      "loss: 0.382716  [171200/175341]\n",
      "loss: 0.874496  [172800/175341]\n",
      "loss: 0.224848  [174400/175341]\n",
      "Train Accuracy: 80.2972%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.561303, F1-score: 75.84%, Macro_F1-Score:  39.45%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.327741  [    0/175341]\n",
      "loss: 0.317113  [ 1600/175341]\n",
      "loss: 0.574375  [ 3200/175341]\n",
      "loss: 0.496299  [ 4800/175341]\n",
      "loss: 0.603623  [ 6400/175341]\n",
      "loss: 0.465266  [ 8000/175341]\n",
      "loss: 0.209382  [ 9600/175341]\n",
      "loss: 0.229633  [11200/175341]\n",
      "loss: 0.335742  [12800/175341]\n",
      "loss: 0.273917  [14400/175341]\n",
      "loss: 0.194499  [16000/175341]\n",
      "loss: 0.632819  [17600/175341]\n",
      "loss: 0.399748  [19200/175341]\n",
      "loss: 0.284167  [20800/175341]\n",
      "loss: 0.537796  [22400/175341]\n",
      "loss: 0.738104  [24000/175341]\n",
      "loss: 0.263871  [25600/175341]\n",
      "loss: 0.816785  [27200/175341]\n",
      "loss: 0.545973  [28800/175341]\n",
      "loss: 0.278408  [30400/175341]\n",
      "loss: 0.553672  [32000/175341]\n",
      "loss: 0.503949  [33600/175341]\n",
      "loss: 0.238723  [35200/175341]\n",
      "loss: 0.434076  [36800/175341]\n",
      "loss: 0.226988  [38400/175341]\n",
      "loss: 0.292710  [40000/175341]\n",
      "loss: 0.502180  [41600/175341]\n",
      "loss: 0.660791  [43200/175341]\n",
      "loss: 0.554575  [44800/175341]\n",
      "loss: 0.551530  [46400/175341]\n",
      "loss: 0.333500  [48000/175341]\n",
      "loss: 0.233508  [49600/175341]\n",
      "loss: 0.552413  [51200/175341]\n",
      "loss: 0.366219  [52800/175341]\n",
      "loss: 0.179844  [54400/175341]\n",
      "loss: 0.319801  [56000/175341]\n",
      "loss: 0.363881  [57600/175341]\n",
      "loss: 0.439898  [59200/175341]\n",
      "loss: 0.215221  [60800/175341]\n",
      "loss: 0.839790  [62400/175341]\n",
      "loss: 0.633291  [64000/175341]\n",
      "loss: 0.298214  [65600/175341]\n",
      "loss: 0.364709  [67200/175341]\n",
      "loss: 0.245628  [68800/175341]\n",
      "loss: 0.258592  [70400/175341]\n",
      "loss: 0.655966  [72000/175341]\n",
      "loss: 0.372857  [73600/175341]\n",
      "loss: 0.380074  [75200/175341]\n",
      "loss: 0.794495  [76800/175341]\n",
      "loss: 0.385011  [78400/175341]\n",
      "loss: 0.525689  [80000/175341]\n",
      "loss: 0.528705  [81600/175341]\n",
      "loss: 0.314756  [83200/175341]\n",
      "loss: 0.649663  [84800/175341]\n",
      "loss: 0.219269  [86400/175341]\n",
      "loss: 0.446398  [88000/175341]\n",
      "loss: 0.297909  [89600/175341]\n",
      "loss: 0.553310  [91200/175341]\n",
      "loss: 0.401969  [92800/175341]\n",
      "loss: 0.455513  [94400/175341]\n",
      "loss: 0.441986  [96000/175341]\n",
      "loss: 0.457966  [97600/175341]\n",
      "loss: 0.266672  [99200/175341]\n",
      "loss: 0.722817  [100800/175341]\n",
      "loss: 0.768581  [102400/175341]\n",
      "loss: 0.571111  [104000/175341]\n",
      "loss: 0.485573  [105600/175341]\n",
      "loss: 0.559217  [107200/175341]\n",
      "loss: 0.549504  [108800/175341]\n",
      "loss: 0.611933  [110400/175341]\n",
      "loss: 0.270526  [112000/175341]\n",
      "loss: 0.448988  [113600/175341]\n",
      "loss: 0.507794  [115200/175341]\n",
      "loss: 0.985203  [116800/175341]\n",
      "loss: 0.413596  [118400/175341]\n",
      "loss: 0.810162  [120000/175341]\n",
      "loss: 0.423925  [121600/175341]\n",
      "loss: 0.073276  [123200/175341]\n",
      "loss: 0.323382  [124800/175341]\n",
      "loss: 0.442881  [126400/175341]\n",
      "loss: 0.345872  [128000/175341]\n",
      "loss: 0.543428  [129600/175341]\n",
      "loss: 0.215073  [131200/175341]\n",
      "loss: 0.373276  [132800/175341]\n",
      "loss: 0.487628  [134400/175341]\n",
      "loss: 0.495241  [136000/175341]\n",
      "loss: 0.437976  [137600/175341]\n",
      "loss: 0.657211  [139200/175341]\n",
      "loss: 0.576901  [140800/175341]\n",
      "loss: 0.141599  [142400/175341]\n",
      "loss: 0.545433  [144000/175341]\n",
      "loss: 0.242236  [145600/175341]\n",
      "loss: 0.505910  [147200/175341]\n",
      "loss: 0.354828  [148800/175341]\n",
      "loss: 0.362088  [150400/175341]\n",
      "loss: 0.269063  [152000/175341]\n",
      "loss: 0.460884  [153600/175341]\n",
      "loss: 0.757341  [155200/175341]\n",
      "loss: 0.592104  [156800/175341]\n",
      "loss: 0.534439  [158400/175341]\n",
      "loss: 0.585684  [160000/175341]\n",
      "loss: 0.314854  [161600/175341]\n",
      "loss: 0.293939  [163200/175341]\n",
      "loss: 0.189041  [164800/175341]\n",
      "loss: 0.577399  [166400/175341]\n",
      "loss: 0.534531  [168000/175341]\n",
      "loss: 0.277779  [169600/175341]\n",
      "loss: 0.745823  [171200/175341]\n",
      "loss: 0.162375  [172800/175341]\n",
      "loss: 0.884618  [174400/175341]\n",
      "Train Accuracy: 80.3104%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.548647, F1-score: 76.46%, Macro_F1-Score:  39.55%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.654648  [    0/175341]\n",
      "loss: 0.337523  [ 1600/175341]\n",
      "loss: 0.271950  [ 3200/175341]\n",
      "loss: 0.300577  [ 4800/175341]\n",
      "loss: 0.677254  [ 6400/175341]\n",
      "loss: 0.286104  [ 8000/175341]\n",
      "loss: 0.507465  [ 9600/175341]\n",
      "loss: 0.263925  [11200/175341]\n",
      "loss: 0.459942  [12800/175341]\n",
      "loss: 0.530800  [14400/175341]\n",
      "loss: 0.572037  [16000/175341]\n",
      "loss: 0.825959  [17600/175341]\n",
      "loss: 0.498277  [19200/175341]\n",
      "loss: 0.328422  [20800/175341]\n",
      "loss: 0.488500  [22400/175341]\n",
      "loss: 0.743216  [24000/175341]\n",
      "loss: 0.354259  [25600/175341]\n",
      "loss: 0.440820  [27200/175341]\n",
      "loss: 0.771291  [28800/175341]\n",
      "loss: 0.614917  [30400/175341]\n",
      "loss: 0.470622  [32000/175341]\n",
      "loss: 0.744521  [33600/175341]\n",
      "loss: 0.954959  [35200/175341]\n",
      "loss: 0.375565  [36800/175341]\n",
      "loss: 0.135378  [38400/175341]\n",
      "loss: 1.052961  [40000/175341]\n",
      "loss: 0.371359  [41600/175341]\n",
      "loss: 0.329445  [43200/175341]\n",
      "loss: 0.603631  [44800/175341]\n",
      "loss: 0.271090  [46400/175341]\n",
      "loss: 0.327115  [48000/175341]\n",
      "loss: 0.504095  [49600/175341]\n",
      "loss: 0.269615  [51200/175341]\n",
      "loss: 0.366298  [52800/175341]\n",
      "loss: 0.912845  [54400/175341]\n",
      "loss: 0.382641  [56000/175341]\n",
      "loss: 0.299322  [57600/175341]\n",
      "loss: 0.487339  [59200/175341]\n",
      "loss: 0.732704  [60800/175341]\n",
      "loss: 1.367043  [62400/175341]\n",
      "loss: 0.393699  [64000/175341]\n",
      "loss: 0.500451  [65600/175341]\n",
      "loss: 0.275906  [67200/175341]\n",
      "loss: 0.838712  [68800/175341]\n",
      "loss: 0.657926  [70400/175341]\n",
      "loss: 0.500042  [72000/175341]\n",
      "loss: 0.533830  [73600/175341]\n",
      "loss: 0.770482  [75200/175341]\n",
      "loss: 0.792863  [76800/175341]\n",
      "loss: 0.602753  [78400/175341]\n",
      "loss: 0.574179  [80000/175341]\n",
      "loss: 0.225485  [81600/175341]\n",
      "loss: 0.283048  [83200/175341]\n",
      "loss: 0.271651  [84800/175341]\n",
      "loss: 0.379808  [86400/175341]\n",
      "loss: 0.715280  [88000/175341]\n",
      "loss: 0.660706  [89600/175341]\n",
      "loss: 0.411026  [91200/175341]\n",
      "loss: 0.510764  [92800/175341]\n",
      "loss: 0.485257  [94400/175341]\n",
      "loss: 0.262101  [96000/175341]\n",
      "loss: 0.271845  [97600/175341]\n",
      "loss: 0.593695  [99200/175341]\n",
      "loss: 0.570679  [100800/175341]\n",
      "loss: 0.196570  [102400/175341]\n",
      "loss: 0.540159  [104000/175341]\n",
      "loss: 0.370311  [105600/175341]\n",
      "loss: 0.652081  [107200/175341]\n",
      "loss: 0.524652  [108800/175341]\n",
      "loss: 0.232370  [110400/175341]\n",
      "loss: 0.542338  [112000/175341]\n",
      "loss: 0.341757  [113600/175341]\n",
      "loss: 0.438317  [115200/175341]\n",
      "loss: 0.673174  [116800/175341]\n",
      "loss: 0.289943  [118400/175341]\n",
      "loss: 0.375671  [120000/175341]\n",
      "loss: 0.476489  [121600/175341]\n",
      "loss: 0.310891  [123200/175341]\n",
      "loss: 0.637835  [124800/175341]\n",
      "loss: 0.413505  [126400/175341]\n",
      "loss: 0.658527  [128000/175341]\n",
      "loss: 0.391554  [129600/175341]\n",
      "loss: 0.562239  [131200/175341]\n",
      "loss: 0.638169  [132800/175341]\n",
      "loss: 0.631539  [134400/175341]\n",
      "loss: 0.225958  [136000/175341]\n",
      "loss: 0.570168  [137600/175341]\n",
      "loss: 0.118447  [139200/175341]\n",
      "loss: 0.448630  [140800/175341]\n",
      "loss: 0.393720  [142400/175341]\n",
      "loss: 0.410822  [144000/175341]\n",
      "loss: 0.542520  [145600/175341]\n",
      "loss: 0.300793  [147200/175341]\n",
      "loss: 0.540422  [148800/175341]\n",
      "loss: 0.333509  [150400/175341]\n",
      "loss: 0.485238  [152000/175341]\n",
      "loss: 0.737950  [153600/175341]\n",
      "loss: 0.482392  [155200/175341]\n",
      "loss: 0.240788  [156800/175341]\n",
      "loss: 0.143507  [158400/175341]\n",
      "loss: 0.643996  [160000/175341]\n",
      "loss: 0.350130  [161600/175341]\n",
      "loss: 1.028769  [163200/175341]\n",
      "loss: 0.634481  [164800/175341]\n",
      "loss: 0.694690  [166400/175341]\n",
      "loss: 0.801881  [168000/175341]\n",
      "loss: 0.579181  [169600/175341]\n",
      "loss: 0.300496  [171200/175341]\n",
      "loss: 0.331337  [172800/175341]\n",
      "loss: 0.235944  [174400/175341]\n",
      "Train Accuracy: 80.3817%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.580905, F1-score: 75.11%, Macro_F1-Score:  41.25%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.675605  [    0/175341]\n",
      "loss: 0.797379  [ 1600/175341]\n",
      "loss: 0.115873  [ 3200/175341]\n",
      "loss: 0.746097  [ 4800/175341]\n",
      "loss: 0.568741  [ 6400/175341]\n",
      "loss: 0.111655  [ 8000/175341]\n",
      "loss: 0.240380  [ 9600/175341]\n",
      "loss: 0.446247  [11200/175341]\n",
      "loss: 0.569249  [12800/175341]\n",
      "loss: 0.524307  [14400/175341]\n",
      "loss: 0.365735  [16000/175341]\n",
      "loss: 0.354855  [17600/175341]\n",
      "loss: 0.461721  [19200/175341]\n",
      "loss: 0.297304  [20800/175341]\n",
      "loss: 0.408968  [22400/175341]\n",
      "loss: 0.665402  [24000/175341]\n",
      "loss: 0.480331  [25600/175341]\n",
      "loss: 0.306163  [27200/175341]\n",
      "loss: 0.608243  [28800/175341]\n",
      "loss: 0.445942  [30400/175341]\n",
      "loss: 0.692698  [32000/175341]\n",
      "loss: 0.233045  [33600/175341]\n",
      "loss: 0.564487  [35200/175341]\n",
      "loss: 0.336708  [36800/175341]\n",
      "loss: 0.300809  [38400/175341]\n",
      "loss: 0.518448  [40000/175341]\n",
      "loss: 0.663355  [41600/175341]\n",
      "loss: 0.616407  [43200/175341]\n",
      "loss: 0.748976  [44800/175341]\n",
      "loss: 0.472064  [46400/175341]\n",
      "loss: 0.728915  [48000/175341]\n",
      "loss: 0.407111  [49600/175341]\n",
      "loss: 0.530970  [51200/175341]\n",
      "loss: 0.366107  [52800/175341]\n",
      "loss: 1.010651  [54400/175341]\n",
      "loss: 0.654857  [56000/175341]\n",
      "loss: 0.298109  [57600/175341]\n",
      "loss: 0.469764  [59200/175341]\n",
      "loss: 0.642536  [60800/175341]\n",
      "loss: 0.641074  [62400/175341]\n",
      "loss: 0.346213  [64000/175341]\n",
      "loss: 0.463937  [65600/175341]\n",
      "loss: 0.412761  [67200/175341]\n",
      "loss: 0.719675  [68800/175341]\n",
      "loss: 0.476371  [70400/175341]\n",
      "loss: 0.482199  [72000/175341]\n",
      "loss: 0.268769  [73600/175341]\n",
      "loss: 0.586658  [75200/175341]\n",
      "loss: 0.420829  [76800/175341]\n",
      "loss: 0.410923  [78400/175341]\n",
      "loss: 0.352924  [80000/175341]\n",
      "loss: 0.463740  [81600/175341]\n",
      "loss: 0.136038  [83200/175341]\n",
      "loss: 0.431263  [84800/175341]\n",
      "loss: 0.496196  [86400/175341]\n",
      "loss: 0.548505  [88000/175341]\n",
      "loss: 0.377558  [89600/175341]\n",
      "loss: 0.535724  [91200/175341]\n",
      "loss: 0.282670  [92800/175341]\n",
      "loss: 0.208102  [94400/175341]\n",
      "loss: 0.829926  [96000/175341]\n",
      "loss: 0.308030  [97600/175341]\n",
      "loss: 0.241435  [99200/175341]\n",
      "loss: 0.783451  [100800/175341]\n",
      "loss: 0.348199  [102400/175341]\n",
      "loss: 0.574627  [104000/175341]\n",
      "loss: 0.419758  [105600/175341]\n",
      "loss: 0.453016  [107200/175341]\n",
      "loss: 0.478791  [108800/175341]\n",
      "loss: 0.598174  [110400/175341]\n",
      "loss: 0.307066  [112000/175341]\n",
      "loss: 0.592085  [113600/175341]\n",
      "loss: 0.518750  [115200/175341]\n",
      "loss: 0.596881  [116800/175341]\n",
      "loss: 0.421835  [118400/175341]\n",
      "loss: 0.283806  [120000/175341]\n",
      "loss: 0.622342  [121600/175341]\n",
      "loss: 0.234542  [123200/175341]\n",
      "loss: 0.355572  [124800/175341]\n",
      "loss: 0.249588  [126400/175341]\n",
      "loss: 0.277140  [128000/175341]\n",
      "loss: 0.477062  [129600/175341]\n",
      "loss: 1.048302  [131200/175341]\n",
      "loss: 0.686099  [132800/175341]\n",
      "loss: 0.641955  [134400/175341]\n",
      "loss: 0.513408  [136000/175341]\n",
      "loss: 0.477591  [137600/175341]\n",
      "loss: 0.602018  [139200/175341]\n",
      "loss: 0.711719  [140800/175341]\n",
      "loss: 0.368053  [142400/175341]\n",
      "loss: 0.502004  [144000/175341]\n",
      "loss: 0.732479  [145600/175341]\n",
      "loss: 0.290781  [147200/175341]\n",
      "loss: 0.470135  [148800/175341]\n",
      "loss: 0.412071  [150400/175341]\n",
      "loss: 0.357295  [152000/175341]\n",
      "loss: 0.223161  [153600/175341]\n",
      "loss: 0.628344  [155200/175341]\n",
      "loss: 0.535736  [156800/175341]\n",
      "loss: 0.306952  [158400/175341]\n",
      "loss: 1.026726  [160000/175341]\n",
      "loss: 0.323704  [161600/175341]\n",
      "loss: 0.398692  [163200/175341]\n",
      "loss: 0.421633  [164800/175341]\n",
      "loss: 0.789193  [166400/175341]\n",
      "loss: 0.711174  [168000/175341]\n",
      "loss: 0.542550  [169600/175341]\n",
      "loss: 0.665795  [171200/175341]\n",
      "loss: 0.613401  [172800/175341]\n",
      "loss: 0.542442  [174400/175341]\n",
      "Train Accuracy: 80.4216%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.545323, F1-score: 75.68%, Macro_F1-Score:  39.95%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.390826  [    0/175341]\n",
      "loss: 0.617611  [ 1600/175341]\n",
      "loss: 0.460975  [ 3200/175341]\n",
      "loss: 0.762703  [ 4800/175341]\n",
      "loss: 0.887514  [ 6400/175341]\n",
      "loss: 0.224551  [ 8000/175341]\n",
      "loss: 0.252663  [ 9600/175341]\n",
      "loss: 0.371383  [11200/175341]\n",
      "loss: 0.328273  [12800/175341]\n",
      "loss: 0.434135  [14400/175341]\n",
      "loss: 0.510736  [16000/175341]\n",
      "loss: 0.911901  [17600/175341]\n",
      "loss: 0.532834  [19200/175341]\n",
      "loss: 0.313620  [20800/175341]\n",
      "loss: 0.444486  [22400/175341]\n",
      "loss: 0.370184  [24000/175341]\n",
      "loss: 0.605983  [25600/175341]\n",
      "loss: 0.621146  [27200/175341]\n",
      "loss: 0.695912  [28800/175341]\n",
      "loss: 0.406676  [30400/175341]\n",
      "loss: 0.353172  [32000/175341]\n",
      "loss: 0.386707  [33600/175341]\n",
      "loss: 0.441310  [35200/175341]\n",
      "loss: 0.367074  [36800/175341]\n",
      "loss: 0.161880  [38400/175341]\n",
      "loss: 0.633397  [40000/175341]\n",
      "loss: 0.199319  [41600/175341]\n",
      "loss: 0.819893  [43200/175341]\n",
      "loss: 0.553423  [44800/175341]\n",
      "loss: 0.610501  [46400/175341]\n",
      "loss: 0.447463  [48000/175341]\n",
      "loss: 0.315775  [49600/175341]\n",
      "loss: 0.468370  [51200/175341]\n",
      "loss: 0.236279  [52800/175341]\n",
      "loss: 0.241130  [54400/175341]\n",
      "loss: 0.815284  [56000/175341]\n",
      "loss: 0.345655  [57600/175341]\n",
      "loss: 0.640805  [59200/175341]\n",
      "loss: 0.619392  [60800/175341]\n",
      "loss: 0.559430  [62400/175341]\n",
      "loss: 0.296416  [64000/175341]\n",
      "loss: 0.620045  [65600/175341]\n",
      "loss: 0.302815  [67200/175341]\n",
      "loss: 0.508454  [68800/175341]\n",
      "loss: 0.408996  [70400/175341]\n",
      "loss: 0.694894  [72000/175341]\n",
      "loss: 0.655100  [73600/175341]\n",
      "loss: 0.258284  [75200/175341]\n",
      "loss: 0.399854  [76800/175341]\n",
      "loss: 0.513887  [78400/175341]\n",
      "loss: 0.185569  [80000/175341]\n",
      "loss: 0.504230  [81600/175341]\n",
      "loss: 0.583562  [83200/175341]\n",
      "loss: 0.542809  [84800/175341]\n",
      "loss: 0.311150  [86400/175341]\n",
      "loss: 0.375381  [88000/175341]\n",
      "loss: 0.312234  [89600/175341]\n",
      "loss: 0.810292  [91200/175341]\n",
      "loss: 0.657109  [92800/175341]\n",
      "loss: 0.380146  [94400/175341]\n",
      "loss: 0.583529  [96000/175341]\n",
      "loss: 0.551004  [97600/175341]\n",
      "loss: 0.553368  [99200/175341]\n",
      "loss: 0.502726  [100800/175341]\n",
      "loss: 0.432895  [102400/175341]\n",
      "loss: 0.511892  [104000/175341]\n",
      "loss: 0.469926  [105600/175341]\n",
      "loss: 0.868214  [107200/175341]\n",
      "loss: 0.495316  [108800/175341]\n",
      "loss: 0.343642  [110400/175341]\n",
      "loss: 0.565634  [112000/175341]\n",
      "loss: 0.580434  [113600/175341]\n",
      "loss: 0.432938  [115200/175341]\n",
      "loss: 0.432714  [116800/175341]\n",
      "loss: 0.312288  [118400/175341]\n",
      "loss: 0.742992  [120000/175341]\n",
      "loss: 0.276949  [121600/175341]\n",
      "loss: 0.960250  [123200/175341]\n",
      "loss: 0.512518  [124800/175341]\n",
      "loss: 0.355717  [126400/175341]\n",
      "loss: 0.145616  [128000/175341]\n",
      "loss: 0.742059  [129600/175341]\n",
      "loss: 0.509488  [131200/175341]\n",
      "loss: 0.523891  [132800/175341]\n",
      "loss: 0.945709  [134400/175341]\n",
      "loss: 0.261119  [136000/175341]\n",
      "loss: 0.415364  [137600/175341]\n",
      "loss: 0.627002  [139200/175341]\n",
      "loss: 0.682755  [140800/175341]\n",
      "loss: 0.750950  [142400/175341]\n",
      "loss: 0.306027  [144000/175341]\n",
      "loss: 0.500621  [145600/175341]\n",
      "loss: 0.492389  [147200/175341]\n",
      "loss: 0.184965  [148800/175341]\n",
      "loss: 0.705752  [150400/175341]\n",
      "loss: 0.440771  [152000/175341]\n",
      "loss: 0.557137  [153600/175341]\n",
      "loss: 0.455649  [155200/175341]\n",
      "loss: 0.512128  [156800/175341]\n",
      "loss: 0.558714  [158400/175341]\n",
      "loss: 0.400688  [160000/175341]\n",
      "loss: 0.611205  [161600/175341]\n",
      "loss: 0.141479  [163200/175341]\n",
      "loss: 0.508996  [164800/175341]\n",
      "loss: 0.480717  [166400/175341]\n",
      "loss: 0.733656  [168000/175341]\n",
      "loss: 0.441487  [169600/175341]\n",
      "loss: 0.679755  [171200/175341]\n",
      "loss: 0.477260  [172800/175341]\n",
      "loss: 0.432889  [174400/175341]\n",
      "Train Accuracy: 80.4963%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.556970, F1-score: 75.70%, Macro_F1-Score:  39.33%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.164228  [    0/175341]\n",
      "loss: 0.448162  [ 1600/175341]\n",
      "loss: 0.314431  [ 3200/175341]\n",
      "loss: 0.464722  [ 4800/175341]\n",
      "loss: 0.339349  [ 6400/175341]\n",
      "loss: 0.635685  [ 8000/175341]\n",
      "loss: 0.319224  [ 9600/175341]\n",
      "loss: 0.756278  [11200/175341]\n",
      "loss: 0.855988  [12800/175341]\n",
      "loss: 0.454432  [14400/175341]\n",
      "loss: 0.186836  [16000/175341]\n",
      "loss: 0.853569  [17600/175341]\n",
      "loss: 0.432517  [19200/175341]\n",
      "loss: 0.298512  [20800/175341]\n",
      "loss: 0.176545  [22400/175341]\n",
      "loss: 0.606443  [24000/175341]\n",
      "loss: 0.229987  [25600/175341]\n",
      "loss: 0.820141  [27200/175341]\n",
      "loss: 0.559685  [28800/175341]\n",
      "loss: 0.481409  [30400/175341]\n",
      "loss: 0.443247  [32000/175341]\n",
      "loss: 0.956456  [33600/175341]\n",
      "loss: 0.621603  [35200/175341]\n",
      "loss: 0.495473  [36800/175341]\n",
      "loss: 0.693786  [38400/175341]\n",
      "loss: 0.324078  [40000/175341]\n",
      "loss: 0.784455  [41600/175341]\n",
      "loss: 0.367613  [43200/175341]\n",
      "loss: 0.467936  [44800/175341]\n",
      "loss: 0.736078  [46400/175341]\n",
      "loss: 0.327412  [48000/175341]\n",
      "loss: 0.458075  [49600/175341]\n",
      "loss: 0.452248  [51200/175341]\n",
      "loss: 0.721129  [52800/175341]\n",
      "loss: 0.422966  [54400/175341]\n",
      "loss: 0.378240  [56000/175341]\n",
      "loss: 0.691710  [57600/175341]\n",
      "loss: 0.413001  [59200/175341]\n",
      "loss: 0.458677  [60800/175341]\n",
      "loss: 0.453469  [62400/175341]\n",
      "loss: 0.485342  [64000/175341]\n",
      "loss: 0.504985  [65600/175341]\n",
      "loss: 0.682276  [67200/175341]\n",
      "loss: 0.622326  [68800/175341]\n",
      "loss: 0.456398  [70400/175341]\n",
      "loss: 0.077535  [72000/175341]\n",
      "loss: 0.294420  [73600/175341]\n",
      "loss: 0.218502  [75200/175341]\n",
      "loss: 0.692290  [76800/175341]\n",
      "loss: 0.523233  [78400/175341]\n",
      "loss: 0.458878  [80000/175341]\n",
      "loss: 0.526735  [81600/175341]\n",
      "loss: 0.501128  [83200/175341]\n",
      "loss: 0.632749  [84800/175341]\n",
      "loss: 0.417972  [86400/175341]\n",
      "loss: 0.347512  [88000/175341]\n",
      "loss: 0.559016  [89600/175341]\n",
      "loss: 0.488800  [91200/175341]\n",
      "loss: 0.164380  [92800/175341]\n",
      "loss: 0.309111  [94400/175341]\n",
      "loss: 0.336066  [96000/175341]\n",
      "loss: 0.576491  [97600/175341]\n",
      "loss: 0.237906  [99200/175341]\n",
      "loss: 0.250520  [100800/175341]\n",
      "loss: 0.229190  [102400/175341]\n",
      "loss: 0.518858  [104000/175341]\n",
      "loss: 0.382227  [105600/175341]\n",
      "loss: 0.416342  [107200/175341]\n",
      "loss: 0.262197  [108800/175341]\n",
      "loss: 0.294930  [110400/175341]\n",
      "loss: 0.784778  [112000/175341]\n",
      "loss: 0.462688  [113600/175341]\n",
      "loss: 0.398223  [115200/175341]\n",
      "loss: 0.509804  [116800/175341]\n",
      "loss: 0.390910  [118400/175341]\n",
      "loss: 0.465078  [120000/175341]\n",
      "loss: 0.358589  [121600/175341]\n",
      "loss: 0.457889  [123200/175341]\n",
      "loss: 0.376106  [124800/175341]\n",
      "loss: 0.251546  [126400/175341]\n",
      "loss: 0.479322  [128000/175341]\n",
      "loss: 0.564948  [129600/175341]\n",
      "loss: 0.670706  [131200/175341]\n",
      "loss: 0.584577  [132800/175341]\n",
      "loss: 0.612221  [134400/175341]\n",
      "loss: 0.379587  [136000/175341]\n",
      "loss: 0.307301  [137600/175341]\n",
      "loss: 0.482109  [139200/175341]\n",
      "loss: 0.428943  [140800/175341]\n",
      "loss: 0.314612  [142400/175341]\n",
      "loss: 0.654175  [144000/175341]\n",
      "loss: 0.458980  [145600/175341]\n",
      "loss: 0.621864  [147200/175341]\n",
      "loss: 0.406561  [148800/175341]\n",
      "loss: 0.420883  [150400/175341]\n",
      "loss: 0.439995  [152000/175341]\n",
      "loss: 0.323540  [153600/175341]\n",
      "loss: 0.376099  [155200/175341]\n",
      "loss: 0.926666  [156800/175341]\n",
      "loss: 0.329655  [158400/175341]\n",
      "loss: 0.967711  [160000/175341]\n",
      "loss: 0.696088  [161600/175341]\n",
      "loss: 0.401440  [163200/175341]\n",
      "loss: 0.509991  [164800/175341]\n",
      "loss: 0.157931  [166400/175341]\n",
      "loss: 0.512468  [168000/175341]\n",
      "loss: 0.110911  [169600/175341]\n",
      "loss: 0.417614  [171200/175341]\n",
      "loss: 0.433560  [172800/175341]\n",
      "loss: 0.674193  [174400/175341]\n",
      "Train Accuracy: 80.5111%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.556739, F1-score: 74.69%, Macro_F1-Score:  39.77%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.410631  [    0/175341]\n",
      "loss: 0.502910  [ 1600/175341]\n",
      "loss: 0.516295  [ 3200/175341]\n",
      "loss: 0.311640  [ 4800/175341]\n",
      "loss: 0.363726  [ 6400/175341]\n",
      "loss: 0.514962  [ 8000/175341]\n",
      "loss: 0.597557  [ 9600/175341]\n",
      "loss: 0.262578  [11200/175341]\n",
      "loss: 0.353772  [12800/175341]\n",
      "loss: 1.106693  [14400/175341]\n",
      "loss: 0.545317  [16000/175341]\n",
      "loss: 0.495287  [17600/175341]\n",
      "loss: 0.670775  [19200/175341]\n",
      "loss: 0.235515  [20800/175341]\n",
      "loss: 0.350593  [22400/175341]\n",
      "loss: 0.787960  [24000/175341]\n",
      "loss: 0.602301  [25600/175341]\n",
      "loss: 0.611473  [27200/175341]\n",
      "loss: 0.780771  [28800/175341]\n",
      "loss: 0.594224  [30400/175341]\n",
      "loss: 0.332346  [32000/175341]\n",
      "loss: 0.419859  [33600/175341]\n",
      "loss: 0.237868  [35200/175341]\n",
      "loss: 0.206094  [36800/175341]\n",
      "loss: 0.578896  [38400/175341]\n",
      "loss: 0.573978  [40000/175341]\n",
      "loss: 0.659316  [41600/175341]\n",
      "loss: 0.608190  [43200/175341]\n",
      "loss: 0.619012  [44800/175341]\n",
      "loss: 0.635832  [46400/175341]\n",
      "loss: 0.309458  [48000/175341]\n",
      "loss: 0.326279  [49600/175341]\n",
      "loss: 0.384599  [51200/175341]\n",
      "loss: 0.245216  [52800/175341]\n",
      "loss: 0.522970  [54400/175341]\n",
      "loss: 0.323534  [56000/175341]\n",
      "loss: 0.480497  [57600/175341]\n",
      "loss: 0.902441  [59200/175341]\n",
      "loss: 0.391375  [60800/175341]\n",
      "loss: 0.340127  [62400/175341]\n",
      "loss: 0.771261  [64000/175341]\n",
      "loss: 0.314403  [65600/175341]\n",
      "loss: 0.489361  [67200/175341]\n",
      "loss: 0.403535  [68800/175341]\n",
      "loss: 0.711454  [70400/175341]\n",
      "loss: 0.565336  [72000/175341]\n",
      "loss: 0.344187  [73600/175341]\n",
      "loss: 0.441770  [75200/175341]\n",
      "loss: 0.201855  [76800/175341]\n",
      "loss: 0.365286  [78400/175341]\n",
      "loss: 0.242537  [80000/175341]\n",
      "loss: 0.377639  [81600/175341]\n",
      "loss: 0.581604  [83200/175341]\n",
      "loss: 0.507802  [84800/175341]\n",
      "loss: 0.447041  [86400/175341]\n",
      "loss: 0.098695  [88000/175341]\n",
      "loss: 0.371758  [89600/175341]\n",
      "loss: 0.377601  [91200/175341]\n",
      "loss: 0.429400  [92800/175341]\n",
      "loss: 0.386673  [94400/175341]\n",
      "loss: 0.321176  [96000/175341]\n",
      "loss: 0.409582  [97600/175341]\n",
      "loss: 0.263749  [99200/175341]\n",
      "loss: 0.576376  [100800/175341]\n",
      "loss: 0.305788  [102400/175341]\n",
      "loss: 0.587619  [104000/175341]\n",
      "loss: 0.698482  [105600/175341]\n",
      "loss: 0.319015  [107200/175341]\n",
      "loss: 0.753994  [108800/175341]\n",
      "loss: 0.974076  [110400/175341]\n",
      "loss: 0.497398  [112000/175341]\n",
      "loss: 0.673070  [113600/175341]\n",
      "loss: 0.283363  [115200/175341]\n",
      "loss: 0.578425  [116800/175341]\n",
      "loss: 0.432045  [118400/175341]\n",
      "loss: 0.351793  [120000/175341]\n",
      "loss: 0.780933  [121600/175341]\n",
      "loss: 0.743634  [123200/175341]\n",
      "loss: 0.695914  [124800/175341]\n",
      "loss: 0.529365  [126400/175341]\n",
      "loss: 0.298279  [128000/175341]\n",
      "loss: 0.486062  [129600/175341]\n",
      "loss: 0.686719  [131200/175341]\n",
      "loss: 0.196380  [132800/175341]\n",
      "loss: 0.397870  [134400/175341]\n",
      "loss: 0.438429  [136000/175341]\n",
      "loss: 0.274452  [137600/175341]\n",
      "loss: 0.229784  [139200/175341]\n",
      "loss: 0.438351  [140800/175341]\n",
      "loss: 0.424146  [142400/175341]\n",
      "loss: 0.203851  [144000/175341]\n",
      "loss: 0.785500  [145600/175341]\n",
      "loss: 0.760016  [147200/175341]\n",
      "loss: 0.568736  [148800/175341]\n",
      "loss: 0.278041  [150400/175341]\n",
      "loss: 0.531982  [152000/175341]\n",
      "loss: 0.510682  [153600/175341]\n",
      "loss: 0.223994  [155200/175341]\n",
      "loss: 0.758458  [156800/175341]\n",
      "loss: 0.589169  [158400/175341]\n",
      "loss: 0.373234  [160000/175341]\n",
      "loss: 0.844331  [161600/175341]\n",
      "loss: 0.442516  [163200/175341]\n",
      "loss: 0.748982  [164800/175341]\n",
      "loss: 0.738598  [166400/175341]\n",
      "loss: 0.540797  [168000/175341]\n",
      "loss: 1.027913  [169600/175341]\n",
      "loss: 0.606038  [171200/175341]\n",
      "loss: 0.452928  [172800/175341]\n",
      "loss: 0.384254  [174400/175341]\n",
      "Train Accuracy: 80.5647%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.579194, F1-score: 73.95%, Macro_F1-Score:  39.83%  \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.226560  [    0/175341]\n",
      "loss: 0.578700  [ 1600/175341]\n",
      "loss: 0.556704  [ 3200/175341]\n",
      "loss: 0.526144  [ 4800/175341]\n",
      "loss: 0.443489  [ 6400/175341]\n",
      "loss: 0.556126  [ 8000/175341]\n",
      "loss: 0.231385  [ 9600/175341]\n",
      "loss: 0.401111  [11200/175341]\n",
      "loss: 0.307407  [12800/175341]\n",
      "loss: 0.462836  [14400/175341]\n",
      "loss: 0.438603  [16000/175341]\n",
      "loss: 0.176338  [17600/175341]\n",
      "loss: 0.646747  [19200/175341]\n",
      "loss: 0.542173  [20800/175341]\n",
      "loss: 0.677334  [22400/175341]\n",
      "loss: 0.588714  [24000/175341]\n",
      "loss: 0.836932  [25600/175341]\n",
      "loss: 0.147905  [27200/175341]\n",
      "loss: 0.338281  [28800/175341]\n",
      "loss: 0.767478  [30400/175341]\n",
      "loss: 0.262643  [32000/175341]\n",
      "loss: 0.577841  [33600/175341]\n",
      "loss: 0.422660  [35200/175341]\n",
      "loss: 0.481193  [36800/175341]\n",
      "loss: 0.325266  [38400/175341]\n",
      "loss: 0.437179  [40000/175341]\n",
      "loss: 0.499755  [41600/175341]\n",
      "loss: 0.455327  [43200/175341]\n",
      "loss: 0.463669  [44800/175341]\n",
      "loss: 0.373632  [46400/175341]\n",
      "loss: 0.602132  [48000/175341]\n",
      "loss: 0.291690  [49600/175341]\n",
      "loss: 0.557722  [51200/175341]\n",
      "loss: 0.550918  [52800/175341]\n",
      "loss: 0.323713  [54400/175341]\n",
      "loss: 0.465010  [56000/175341]\n",
      "loss: 0.217584  [57600/175341]\n",
      "loss: 0.147389  [59200/175341]\n",
      "loss: 0.595604  [60800/175341]\n",
      "loss: 0.545933  [62400/175341]\n",
      "loss: 0.461892  [64000/175341]\n",
      "loss: 0.822001  [65600/175341]\n",
      "loss: 0.350254  [67200/175341]\n",
      "loss: 0.690614  [68800/175341]\n",
      "loss: 0.409073  [70400/175341]\n",
      "loss: 0.254111  [72000/175341]\n",
      "loss: 0.429510  [73600/175341]\n",
      "loss: 0.713169  [75200/175341]\n",
      "loss: 0.776618  [76800/175341]\n",
      "loss: 0.182188  [78400/175341]\n",
      "loss: 0.508292  [80000/175341]\n",
      "loss: 0.524992  [81600/175341]\n",
      "loss: 0.796679  [83200/175341]\n",
      "loss: 0.572799  [84800/175341]\n",
      "loss: 0.625912  [86400/175341]\n",
      "loss: 0.208881  [88000/175341]\n",
      "loss: 0.530055  [89600/175341]\n",
      "loss: 0.259873  [91200/175341]\n",
      "loss: 0.587458  [92800/175341]\n",
      "loss: 0.476271  [94400/175341]\n",
      "loss: 0.514672  [96000/175341]\n",
      "loss: 0.672022  [97600/175341]\n",
      "loss: 0.231986  [99200/175341]\n",
      "loss: 0.607002  [100800/175341]\n",
      "loss: 0.573650  [102400/175341]\n",
      "loss: 1.014807  [104000/175341]\n",
      "loss: 0.290463  [105600/175341]\n",
      "loss: 0.703636  [107200/175341]\n",
      "loss: 0.155599  [108800/175341]\n",
      "loss: 0.973215  [110400/175341]\n",
      "loss: 0.363398  [112000/175341]\n",
      "loss: 0.438030  [113600/175341]\n",
      "loss: 0.741807  [115200/175341]\n",
      "loss: 0.418417  [116800/175341]\n",
      "loss: 0.603109  [118400/175341]\n",
      "loss: 0.565162  [120000/175341]\n",
      "loss: 0.571509  [121600/175341]\n",
      "loss: 0.332089  [123200/175341]\n",
      "loss: 0.074587  [124800/175341]\n",
      "loss: 0.552191  [126400/175341]\n",
      "loss: 0.457729  [128000/175341]\n",
      "loss: 0.076437  [129600/175341]\n",
      "loss: 0.639776  [131200/175341]\n",
      "loss: 0.548903  [132800/175341]\n",
      "loss: 0.545302  [134400/175341]\n",
      "loss: 0.252372  [136000/175341]\n",
      "loss: 0.331471  [137600/175341]\n",
      "loss: 0.244711  [139200/175341]\n",
      "loss: 0.601020  [140800/175341]\n",
      "loss: 0.348922  [142400/175341]\n",
      "loss: 0.452170  [144000/175341]\n",
      "loss: 0.618715  [145600/175341]\n",
      "loss: 0.562581  [147200/175341]\n",
      "loss: 0.452033  [148800/175341]\n",
      "loss: 0.423618  [150400/175341]\n",
      "loss: 0.442699  [152000/175341]\n",
      "loss: 0.520741  [153600/175341]\n",
      "loss: 0.432911  [155200/175341]\n",
      "loss: 0.659258  [156800/175341]\n",
      "loss: 0.560803  [158400/175341]\n",
      "loss: 0.140704  [160000/175341]\n",
      "loss: 0.667814  [161600/175341]\n",
      "loss: 0.494999  [163200/175341]\n",
      "loss: 0.573018  [164800/175341]\n",
      "loss: 0.312837  [166400/175341]\n",
      "loss: 0.638728  [168000/175341]\n",
      "loss: 0.577406  [169600/175341]\n",
      "loss: 0.351051  [171200/175341]\n",
      "loss: 0.238618  [172800/175341]\n",
      "loss: 0.435932  [174400/175341]\n",
      "Train Accuracy: 80.5989%\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.538490, F1-score: 76.88%, Macro_F1-Score:  40.45%  \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.378176  [    0/175341]\n",
      "loss: 0.497156  [ 1600/175341]\n",
      "loss: 0.137590  [ 3200/175341]\n",
      "loss: 0.203165  [ 4800/175341]\n",
      "loss: 0.363430  [ 6400/175341]\n",
      "loss: 0.998731  [ 8000/175341]\n",
      "loss: 0.390955  [ 9600/175341]\n",
      "loss: 0.553236  [11200/175341]\n",
      "loss: 0.548545  [12800/175341]\n",
      "loss: 0.477855  [14400/175341]\n",
      "loss: 0.368795  [16000/175341]\n",
      "loss: 0.449248  [17600/175341]\n",
      "loss: 0.411164  [19200/175341]\n",
      "loss: 0.848833  [20800/175341]\n",
      "loss: 0.470351  [22400/175341]\n",
      "loss: 0.247552  [24000/175341]\n",
      "loss: 0.257919  [25600/175341]\n",
      "loss: 0.382911  [27200/175341]\n",
      "loss: 0.890084  [28800/175341]\n",
      "loss: 0.263740  [30400/175341]\n",
      "loss: 0.662707  [32000/175341]\n",
      "loss: 0.601676  [33600/175341]\n",
      "loss: 0.628351  [35200/175341]\n",
      "loss: 0.262451  [36800/175341]\n",
      "loss: 0.900685  [38400/175341]\n",
      "loss: 0.213294  [40000/175341]\n",
      "loss: 0.543997  [41600/175341]\n",
      "loss: 0.484651  [43200/175341]\n",
      "loss: 0.578820  [44800/175341]\n",
      "loss: 0.569309  [46400/175341]\n",
      "loss: 0.774037  [48000/175341]\n",
      "loss: 0.382915  [49600/175341]\n",
      "loss: 0.747010  [51200/175341]\n",
      "loss: 0.445754  [52800/175341]\n",
      "loss: 0.512403  [54400/175341]\n",
      "loss: 0.194096  [56000/175341]\n",
      "loss: 0.458233  [57600/175341]\n",
      "loss: 0.921251  [59200/175341]\n",
      "loss: 0.483838  [60800/175341]\n",
      "loss: 0.410468  [62400/175341]\n",
      "loss: 0.351354  [64000/175341]\n",
      "loss: 0.491868  [65600/175341]\n",
      "loss: 0.596326  [67200/175341]\n",
      "loss: 0.774247  [68800/175341]\n",
      "loss: 0.427976  [70400/175341]\n",
      "loss: 0.278374  [72000/175341]\n",
      "loss: 0.342351  [73600/175341]\n",
      "loss: 0.776896  [75200/175341]\n",
      "loss: 0.581314  [76800/175341]\n",
      "loss: 0.352723  [78400/175341]\n",
      "loss: 0.535777  [80000/175341]\n",
      "loss: 0.698227  [81600/175341]\n",
      "loss: 0.202327  [83200/175341]\n",
      "loss: 0.812588  [84800/175341]\n",
      "loss: 0.540736  [86400/175341]\n",
      "loss: 0.380469  [88000/175341]\n",
      "loss: 0.413804  [89600/175341]\n",
      "loss: 0.223740  [91200/175341]\n",
      "loss: 0.471252  [92800/175341]\n",
      "loss: 0.306836  [94400/175341]\n",
      "loss: 0.337471  [96000/175341]\n",
      "loss: 0.511394  [97600/175341]\n",
      "loss: 0.392600  [99200/175341]\n",
      "loss: 0.139034  [100800/175341]\n",
      "loss: 0.166675  [102400/175341]\n",
      "loss: 0.667674  [104000/175341]\n",
      "loss: 0.205307  [105600/175341]\n",
      "loss: 0.586906  [107200/175341]\n",
      "loss: 0.279805  [108800/175341]\n",
      "loss: 0.691182  [110400/175341]\n",
      "loss: 0.610960  [112000/175341]\n",
      "loss: 0.301625  [113600/175341]\n",
      "loss: 0.622956  [115200/175341]\n",
      "loss: 0.683419  [116800/175341]\n",
      "loss: 0.536466  [118400/175341]\n",
      "loss: 0.373648  [120000/175341]\n",
      "loss: 0.461339  [121600/175341]\n",
      "loss: 0.337105  [123200/175341]\n",
      "loss: 0.461335  [124800/175341]\n",
      "loss: 0.727313  [126400/175341]\n",
      "loss: 0.262449  [128000/175341]\n",
      "loss: 0.446719  [129600/175341]\n",
      "loss: 0.700503  [131200/175341]\n",
      "loss: 0.562480  [132800/175341]\n",
      "loss: 0.807245  [134400/175341]\n",
      "loss: 0.683487  [136000/175341]\n",
      "loss: 0.152102  [137600/175341]\n",
      "loss: 0.437139  [139200/175341]\n",
      "loss: 0.854033  [140800/175341]\n",
      "loss: 0.557576  [142400/175341]\n",
      "loss: 0.198597  [144000/175341]\n",
      "loss: 0.706823  [145600/175341]\n",
      "loss: 0.620299  [147200/175341]\n",
      "loss: 0.253793  [148800/175341]\n",
      "loss: 0.464477  [150400/175341]\n",
      "loss: 0.392375  [152000/175341]\n",
      "loss: 0.059402  [153600/175341]\n",
      "loss: 0.521730  [155200/175341]\n",
      "loss: 0.818666  [156800/175341]\n",
      "loss: 0.605404  [158400/175341]\n",
      "loss: 0.554654  [160000/175341]\n",
      "loss: 0.627292  [161600/175341]\n",
      "loss: 0.314487  [163200/175341]\n",
      "loss: 0.511168  [164800/175341]\n",
      "loss: 0.529558  [166400/175341]\n",
      "loss: 0.461652  [168000/175341]\n",
      "loss: 0.232980  [169600/175341]\n",
      "loss: 0.648686  [171200/175341]\n",
      "loss: 0.568868  [172800/175341]\n",
      "loss: 0.306517  [174400/175341]\n",
      "Train Accuracy: 80.6748%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.550488, F1-score: 75.41%, Macro_F1-Score:  40.29%  \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.410685  [    0/175341]\n",
      "loss: 1.073673  [ 1600/175341]\n",
      "loss: 0.213684  [ 3200/175341]\n",
      "loss: 0.267435  [ 4800/175341]\n",
      "loss: 0.734922  [ 6400/175341]\n",
      "loss: 0.160894  [ 8000/175341]\n",
      "loss: 1.030221  [ 9600/175341]\n",
      "loss: 0.902290  [11200/175341]\n",
      "loss: 0.357262  [12800/175341]\n",
      "loss: 0.231282  [14400/175341]\n",
      "loss: 0.161862  [16000/175341]\n",
      "loss: 0.404926  [17600/175341]\n",
      "loss: 0.516130  [19200/175341]\n",
      "loss: 0.458324  [20800/175341]\n",
      "loss: 0.618137  [22400/175341]\n",
      "loss: 0.319498  [24000/175341]\n",
      "loss: 0.384024  [25600/175341]\n",
      "loss: 0.315400  [27200/175341]\n",
      "loss: 0.148607  [28800/175341]\n",
      "loss: 0.483557  [30400/175341]\n",
      "loss: 0.540745  [32000/175341]\n",
      "loss: 0.386889  [33600/175341]\n",
      "loss: 0.221839  [35200/175341]\n",
      "loss: 0.462704  [36800/175341]\n",
      "loss: 0.288748  [38400/175341]\n",
      "loss: 0.094380  [40000/175341]\n",
      "loss: 0.174336  [41600/175341]\n",
      "loss: 0.486540  [43200/175341]\n",
      "loss: 0.426309  [44800/175341]\n",
      "loss: 0.185878  [46400/175341]\n",
      "loss: 0.448836  [48000/175341]\n",
      "loss: 0.208732  [49600/175341]\n",
      "loss: 0.781866  [51200/175341]\n",
      "loss: 0.278768  [52800/175341]\n",
      "loss: 0.403479  [54400/175341]\n",
      "loss: 0.457349  [56000/175341]\n",
      "loss: 0.221551  [57600/175341]\n",
      "loss: 0.340050  [59200/175341]\n",
      "loss: 0.488937  [60800/175341]\n",
      "loss: 0.600357  [62400/175341]\n",
      "loss: 0.667968  [64000/175341]\n",
      "loss: 0.422373  [65600/175341]\n",
      "loss: 0.311932  [67200/175341]\n",
      "loss: 0.565838  [68800/175341]\n",
      "loss: 0.262168  [70400/175341]\n",
      "loss: 0.686899  [72000/175341]\n",
      "loss: 0.242609  [73600/175341]\n",
      "loss: 0.668095  [75200/175341]\n",
      "loss: 0.334353  [76800/175341]\n",
      "loss: 0.516341  [78400/175341]\n",
      "loss: 0.540787  [80000/175341]\n",
      "loss: 0.478358  [81600/175341]\n",
      "loss: 0.275370  [83200/175341]\n",
      "loss: 0.536738  [84800/175341]\n",
      "loss: 0.203318  [86400/175341]\n",
      "loss: 0.376723  [88000/175341]\n",
      "loss: 0.730512  [89600/175341]\n",
      "loss: 1.283170  [91200/175341]\n",
      "loss: 0.517582  [92800/175341]\n",
      "loss: 1.047077  [94400/175341]\n",
      "loss: 0.404870  [96000/175341]\n",
      "loss: 0.551948  [97600/175341]\n",
      "loss: 0.529651  [99200/175341]\n",
      "loss: 0.713534  [100800/175341]\n",
      "loss: 0.437685  [102400/175341]\n",
      "loss: 0.399379  [104000/175341]\n",
      "loss: 0.703628  [105600/175341]\n",
      "loss: 0.408802  [107200/175341]\n",
      "loss: 0.965056  [108800/175341]\n",
      "loss: 0.718656  [110400/175341]\n",
      "loss: 0.370279  [112000/175341]\n",
      "loss: 0.805782  [113600/175341]\n",
      "loss: 0.819421  [115200/175341]\n",
      "loss: 0.439965  [116800/175341]\n",
      "loss: 0.384310  [118400/175341]\n",
      "loss: 0.224212  [120000/175341]\n",
      "loss: 0.635079  [121600/175341]\n",
      "loss: 0.488748  [123200/175341]\n",
      "loss: 0.544627  [124800/175341]\n",
      "loss: 0.678901  [126400/175341]\n",
      "loss: 0.745753  [128000/175341]\n",
      "loss: 0.244048  [129600/175341]\n",
      "loss: 0.555367  [131200/175341]\n",
      "loss: 0.427264  [132800/175341]\n",
      "loss: 0.283140  [134400/175341]\n",
      "loss: 0.159981  [136000/175341]\n",
      "loss: 0.479623  [137600/175341]\n",
      "loss: 0.554500  [139200/175341]\n",
      "loss: 0.364257  [140800/175341]\n",
      "loss: 0.271400  [142400/175341]\n",
      "loss: 0.379650  [144000/175341]\n",
      "loss: 0.418689  [145600/175341]\n",
      "loss: 0.532342  [147200/175341]\n",
      "loss: 0.341955  [148800/175341]\n",
      "loss: 0.458353  [150400/175341]\n",
      "loss: 0.508886  [152000/175341]\n",
      "loss: 0.606292  [153600/175341]\n",
      "loss: 0.604143  [155200/175341]\n",
      "loss: 0.380983  [156800/175341]\n",
      "loss: 0.543496  [158400/175341]\n",
      "loss: 0.685049  [160000/175341]\n",
      "loss: 0.564584  [161600/175341]\n",
      "loss: 0.490525  [163200/175341]\n",
      "loss: 0.796260  [164800/175341]\n",
      "loss: 0.242686  [166400/175341]\n",
      "loss: 0.235623  [168000/175341]\n",
      "loss: 0.773310  [169600/175341]\n",
      "loss: 0.587948  [171200/175341]\n",
      "loss: 0.525974  [172800/175341]\n",
      "loss: 0.902164  [174400/175341]\n",
      "Train Accuracy: 80.6885%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.543959, F1-score: 75.53%, Macro_F1-Score:  40.39%  \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.501850  [    0/175341]\n",
      "loss: 0.248319  [ 1600/175341]\n",
      "loss: 0.555503  [ 3200/175341]\n",
      "loss: 0.574590  [ 4800/175341]\n",
      "loss: 0.375349  [ 6400/175341]\n",
      "loss: 0.150583  [ 8000/175341]\n",
      "loss: 0.501138  [ 9600/175341]\n",
      "loss: 0.915112  [11200/175341]\n",
      "loss: 0.737718  [12800/175341]\n",
      "loss: 0.719743  [14400/175341]\n",
      "loss: 0.458800  [16000/175341]\n",
      "loss: 0.375399  [17600/175341]\n",
      "loss: 0.295931  [19200/175341]\n",
      "loss: 0.421885  [20800/175341]\n",
      "loss: 0.243343  [22400/175341]\n",
      "loss: 0.853635  [24000/175341]\n",
      "loss: 0.206786  [25600/175341]\n",
      "loss: 0.250818  [27200/175341]\n",
      "loss: 0.499444  [28800/175341]\n",
      "loss: 0.180683  [30400/175341]\n",
      "loss: 0.373767  [32000/175341]\n",
      "loss: 0.400951  [33600/175341]\n",
      "loss: 0.227186  [35200/175341]\n",
      "loss: 0.536787  [36800/175341]\n",
      "loss: 0.347931  [38400/175341]\n",
      "loss: 0.380710  [40000/175341]\n",
      "loss: 1.073203  [41600/175341]\n",
      "loss: 0.671942  [43200/175341]\n",
      "loss: 0.298728  [44800/175341]\n",
      "loss: 0.440893  [46400/175341]\n",
      "loss: 0.427718  [48000/175341]\n",
      "loss: 0.551607  [49600/175341]\n",
      "loss: 0.499255  [51200/175341]\n",
      "loss: 0.616309  [52800/175341]\n",
      "loss: 0.708648  [54400/175341]\n",
      "loss: 0.156398  [56000/175341]\n",
      "loss: 0.527974  [57600/175341]\n",
      "loss: 0.642724  [59200/175341]\n",
      "loss: 0.491808  [60800/175341]\n",
      "loss: 0.551673  [62400/175341]\n",
      "loss: 0.476346  [64000/175341]\n",
      "loss: 0.254796  [65600/175341]\n",
      "loss: 0.634392  [67200/175341]\n",
      "loss: 0.360184  [68800/175341]\n",
      "loss: 0.399511  [70400/175341]\n",
      "loss: 0.429014  [72000/175341]\n",
      "loss: 0.381883  [73600/175341]\n",
      "loss: 0.217959  [75200/175341]\n",
      "loss: 0.794560  [76800/175341]\n",
      "loss: 0.247290  [78400/175341]\n",
      "loss: 0.292319  [80000/175341]\n",
      "loss: 0.561493  [81600/175341]\n",
      "loss: 0.478943  [83200/175341]\n",
      "loss: 0.611950  [84800/175341]\n",
      "loss: 0.488041  [86400/175341]\n",
      "loss: 0.487669  [88000/175341]\n",
      "loss: 0.769440  [89600/175341]\n",
      "loss: 0.797029  [91200/175341]\n",
      "loss: 0.648093  [92800/175341]\n",
      "loss: 0.761865  [94400/175341]\n",
      "loss: 0.258424  [96000/175341]\n",
      "loss: 0.519440  [97600/175341]\n",
      "loss: 0.470608  [99200/175341]\n",
      "loss: 0.412134  [100800/175341]\n",
      "loss: 0.463668  [102400/175341]\n",
      "loss: 0.828292  [104000/175341]\n",
      "loss: 0.873013  [105600/175341]\n",
      "loss: 0.607690  [107200/175341]\n",
      "loss: 0.655514  [108800/175341]\n",
      "loss: 0.571732  [110400/175341]\n",
      "loss: 0.781303  [112000/175341]\n",
      "loss: 0.265724  [113600/175341]\n",
      "loss: 0.313075  [115200/175341]\n",
      "loss: 0.307967  [116800/175341]\n",
      "loss: 0.294808  [118400/175341]\n",
      "loss: 0.156667  [120000/175341]\n",
      "loss: 0.734971  [121600/175341]\n",
      "loss: 0.244630  [123200/175341]\n",
      "loss: 0.642514  [124800/175341]\n",
      "loss: 0.329322  [126400/175341]\n",
      "loss: 0.186824  [128000/175341]\n",
      "loss: 0.191739  [129600/175341]\n",
      "loss: 0.360621  [131200/175341]\n",
      "loss: 0.676770  [132800/175341]\n",
      "loss: 0.260354  [134400/175341]\n",
      "loss: 0.633226  [136000/175341]\n",
      "loss: 0.767440  [137600/175341]\n",
      "loss: 0.143704  [139200/175341]\n",
      "loss: 0.483932  [140800/175341]\n",
      "loss: 0.542264  [142400/175341]\n",
      "loss: 0.398583  [144000/175341]\n",
      "loss: 0.777196  [145600/175341]\n",
      "loss: 0.802456  [147200/175341]\n",
      "loss: 0.161610  [148800/175341]\n",
      "loss: 0.180266  [150400/175341]\n",
      "loss: 0.402427  [152000/175341]\n",
      "loss: 0.480825  [153600/175341]\n",
      "loss: 0.406115  [155200/175341]\n",
      "loss: 0.666641  [156800/175341]\n",
      "loss: 0.248269  [158400/175341]\n",
      "loss: 0.214485  [160000/175341]\n",
      "loss: 0.253612  [161600/175341]\n",
      "loss: 0.335103  [163200/175341]\n",
      "loss: 0.474894  [164800/175341]\n",
      "loss: 0.227473  [166400/175341]\n",
      "loss: 0.309155  [168000/175341]\n",
      "loss: 0.296225  [169600/175341]\n",
      "loss: 0.534811  [171200/175341]\n",
      "loss: 0.488298  [172800/175341]\n",
      "loss: 0.648280  [174400/175341]\n",
      "Train Accuracy: 80.7729%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.538468, F1-score: 75.99%, Macro_F1-Score:  41.20%  \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.977021  [    0/175341]\n",
      "loss: 0.622729  [ 1600/175341]\n",
      "loss: 0.506319  [ 3200/175341]\n",
      "loss: 0.407350  [ 4800/175341]\n",
      "loss: 0.553510  [ 6400/175341]\n",
      "loss: 0.466518  [ 8000/175341]\n",
      "loss: 0.611437  [ 9600/175341]\n",
      "loss: 0.529601  [11200/175341]\n",
      "loss: 0.331951  [12800/175341]\n",
      "loss: 0.614898  [14400/175341]\n",
      "loss: 0.879578  [16000/175341]\n",
      "loss: 0.395848  [17600/175341]\n",
      "loss: 0.369021  [19200/175341]\n",
      "loss: 0.452982  [20800/175341]\n",
      "loss: 0.559324  [22400/175341]\n",
      "loss: 0.418476  [24000/175341]\n",
      "loss: 0.458515  [25600/175341]\n",
      "loss: 0.104175  [27200/175341]\n",
      "loss: 0.493918  [28800/175341]\n",
      "loss: 0.246721  [30400/175341]\n",
      "loss: 0.458417  [32000/175341]\n",
      "loss: 0.486850  [33600/175341]\n",
      "loss: 0.580665  [35200/175341]\n",
      "loss: 0.179316  [36800/175341]\n",
      "loss: 0.238347  [38400/175341]\n",
      "loss: 0.566453  [40000/175341]\n",
      "loss: 0.145812  [41600/175341]\n",
      "loss: 0.647035  [43200/175341]\n",
      "loss: 0.533817  [44800/175341]\n",
      "loss: 0.215965  [46400/175341]\n",
      "loss: 0.700381  [48000/175341]\n",
      "loss: 0.350835  [49600/175341]\n",
      "loss: 0.554457  [51200/175341]\n",
      "loss: 0.364528  [52800/175341]\n",
      "loss: 0.461992  [54400/175341]\n",
      "loss: 0.681809  [56000/175341]\n",
      "loss: 0.476397  [57600/175341]\n",
      "loss: 0.417689  [59200/175341]\n",
      "loss: 0.350451  [60800/175341]\n",
      "loss: 0.584159  [62400/175341]\n",
      "loss: 0.316789  [64000/175341]\n",
      "loss: 0.913710  [65600/175341]\n",
      "loss: 0.506378  [67200/175341]\n",
      "loss: 0.280778  [68800/175341]\n",
      "loss: 0.477846  [70400/175341]\n",
      "loss: 0.426565  [72000/175341]\n",
      "loss: 0.212431  [73600/175341]\n",
      "loss: 0.544799  [75200/175341]\n",
      "loss: 0.397462  [76800/175341]\n",
      "loss: 0.509855  [78400/175341]\n",
      "loss: 0.530658  [80000/175341]\n",
      "loss: 0.471311  [81600/175341]\n",
      "loss: 0.619826  [83200/175341]\n",
      "loss: 0.463088  [84800/175341]\n",
      "loss: 0.187751  [86400/175341]\n",
      "loss: 0.401899  [88000/175341]\n",
      "loss: 0.316253  [89600/175341]\n",
      "loss: 0.311802  [91200/175341]\n",
      "loss: 0.160152  [92800/175341]\n",
      "loss: 0.631221  [94400/175341]\n",
      "loss: 0.162614  [96000/175341]\n",
      "loss: 0.571767  [97600/175341]\n",
      "loss: 0.449769  [99200/175341]\n",
      "loss: 0.204479  [100800/175341]\n",
      "loss: 0.310800  [102400/175341]\n",
      "loss: 0.330771  [104000/175341]\n",
      "loss: 0.384387  [105600/175341]\n",
      "loss: 0.873765  [107200/175341]\n",
      "loss: 0.590678  [108800/175341]\n",
      "loss: 0.057742  [110400/175341]\n",
      "loss: 0.379228  [112000/175341]\n",
      "loss: 0.364043  [113600/175341]\n",
      "loss: 0.764450  [115200/175341]\n",
      "loss: 0.311214  [116800/175341]\n",
      "loss: 0.474625  [118400/175341]\n",
      "loss: 0.346265  [120000/175341]\n",
      "loss: 0.268631  [121600/175341]\n",
      "loss: 0.450611  [123200/175341]\n",
      "loss: 0.360793  [124800/175341]\n",
      "loss: 0.820817  [126400/175341]\n",
      "loss: 0.489135  [128000/175341]\n",
      "loss: 0.336059  [129600/175341]\n",
      "loss: 0.555019  [131200/175341]\n",
      "loss: 0.399234  [132800/175341]\n",
      "loss: 0.451384  [134400/175341]\n",
      "loss: 0.264926  [136000/175341]\n",
      "loss: 0.530773  [137600/175341]\n",
      "loss: 0.739423  [139200/175341]\n",
      "loss: 0.376074  [140800/175341]\n",
      "loss: 0.155287  [142400/175341]\n",
      "loss: 0.936845  [144000/175341]\n",
      "loss: 0.406449  [145600/175341]\n",
      "loss: 0.217254  [147200/175341]\n",
      "loss: 0.458967  [148800/175341]\n",
      "loss: 0.316041  [150400/175341]\n",
      "loss: 0.338863  [152000/175341]\n",
      "loss: 0.351919  [153600/175341]\n",
      "loss: 0.348667  [155200/175341]\n",
      "loss: 0.298986  [156800/175341]\n",
      "loss: 0.383089  [158400/175341]\n",
      "loss: 0.153296  [160000/175341]\n",
      "loss: 0.605522  [161600/175341]\n",
      "loss: 0.358078  [163200/175341]\n",
      "loss: 0.328012  [164800/175341]\n",
      "loss: 0.745169  [166400/175341]\n",
      "loss: 0.318759  [168000/175341]\n",
      "loss: 0.384179  [169600/175341]\n",
      "loss: 0.476570  [171200/175341]\n",
      "loss: 0.658983  [172800/175341]\n",
      "loss: 0.268494  [174400/175341]\n",
      "Train Accuracy: 80.8465%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.568453, F1-score: 75.91%, Macro_F1-Score:  42.62%  \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.729948  [    0/175341]\n",
      "loss: 0.224029  [ 1600/175341]\n",
      "loss: 0.524311  [ 3200/175341]\n",
      "loss: 0.345733  [ 4800/175341]\n",
      "loss: 0.510988  [ 6400/175341]\n",
      "loss: 0.664990  [ 8000/175341]\n",
      "loss: 0.422823  [ 9600/175341]\n",
      "loss: 0.249483  [11200/175341]\n",
      "loss: 0.205396  [12800/175341]\n",
      "loss: 0.630468  [14400/175341]\n",
      "loss: 0.343142  [16000/175341]\n",
      "loss: 0.265213  [17600/175341]\n",
      "loss: 0.673186  [19200/175341]\n",
      "loss: 0.431478  [20800/175341]\n",
      "loss: 0.546154  [22400/175341]\n",
      "loss: 1.114100  [24000/175341]\n",
      "loss: 0.622662  [25600/175341]\n",
      "loss: 0.312382  [27200/175341]\n",
      "loss: 0.718300  [28800/175341]\n",
      "loss: 0.547697  [30400/175341]\n",
      "loss: 0.532414  [32000/175341]\n",
      "loss: 0.486221  [33600/175341]\n",
      "loss: 0.654522  [35200/175341]\n",
      "loss: 0.799265  [36800/175341]\n",
      "loss: 0.529981  [38400/175341]\n",
      "loss: 0.302034  [40000/175341]\n",
      "loss: 0.831993  [41600/175341]\n",
      "loss: 0.288658  [43200/175341]\n",
      "loss: 0.508511  [44800/175341]\n",
      "loss: 0.417363  [46400/175341]\n",
      "loss: 0.820086  [48000/175341]\n",
      "loss: 0.307605  [49600/175341]\n",
      "loss: 0.678448  [51200/175341]\n",
      "loss: 0.296447  [52800/175341]\n",
      "loss: 0.364175  [54400/175341]\n",
      "loss: 0.366961  [56000/175341]\n",
      "loss: 0.543966  [57600/175341]\n",
      "loss: 0.989694  [59200/175341]\n",
      "loss: 0.943835  [60800/175341]\n",
      "loss: 0.322022  [62400/175341]\n",
      "loss: 0.689382  [64000/175341]\n",
      "loss: 0.680132  [65600/175341]\n",
      "loss: 0.221823  [67200/175341]\n",
      "loss: 0.570500  [68800/175341]\n",
      "loss: 0.295766  [70400/175341]\n",
      "loss: 0.436632  [72000/175341]\n",
      "loss: 0.424388  [73600/175341]\n",
      "loss: 0.455623  [75200/175341]\n",
      "loss: 0.503094  [76800/175341]\n",
      "loss: 0.393845  [78400/175341]\n",
      "loss: 0.673819  [80000/175341]\n",
      "loss: 1.084559  [81600/175341]\n",
      "loss: 0.126238  [83200/175341]\n",
      "loss: 0.281329  [84800/175341]\n",
      "loss: 0.549392  [86400/175341]\n",
      "loss: 0.446978  [88000/175341]\n",
      "loss: 0.493167  [89600/175341]\n",
      "loss: 0.460474  [91200/175341]\n",
      "loss: 0.719181  [92800/175341]\n",
      "loss: 0.491230  [94400/175341]\n",
      "loss: 0.297534  [96000/175341]\n",
      "loss: 0.308239  [97600/175341]\n",
      "loss: 0.518044  [99200/175341]\n",
      "loss: 0.431692  [100800/175341]\n",
      "loss: 0.754744  [102400/175341]\n",
      "loss: 0.718409  [104000/175341]\n",
      "loss: 0.222711  [105600/175341]\n",
      "loss: 0.511085  [107200/175341]\n",
      "loss: 0.378611  [108800/175341]\n",
      "loss: 0.684476  [110400/175341]\n",
      "loss: 0.220479  [112000/175341]\n",
      "loss: 0.377413  [113600/175341]\n",
      "loss: 0.474546  [115200/175341]\n",
      "loss: 0.804610  [116800/175341]\n",
      "loss: 0.367082  [118400/175341]\n",
      "loss: 0.141419  [120000/175341]\n",
      "loss: 1.038278  [121600/175341]\n",
      "loss: 0.322425  [123200/175341]\n",
      "loss: 0.664874  [124800/175341]\n",
      "loss: 0.443284  [126400/175341]\n",
      "loss: 0.516719  [128000/175341]\n",
      "loss: 0.398933  [129600/175341]\n",
      "loss: 0.321095  [131200/175341]\n",
      "loss: 0.418940  [132800/175341]\n",
      "loss: 0.660993  [134400/175341]\n",
      "loss: 0.445210  [136000/175341]\n",
      "loss: 0.826078  [137600/175341]\n",
      "loss: 0.500538  [139200/175341]\n",
      "loss: 0.345462  [140800/175341]\n",
      "loss: 0.457864  [142400/175341]\n",
      "loss: 0.750171  [144000/175341]\n",
      "loss: 0.286975  [145600/175341]\n",
      "loss: 0.762640  [147200/175341]\n",
      "loss: 0.647603  [148800/175341]\n",
      "loss: 0.694623  [150400/175341]\n",
      "loss: 0.614135  [152000/175341]\n",
      "loss: 0.272839  [153600/175341]\n",
      "loss: 0.536080  [155200/175341]\n",
      "loss: 0.374048  [156800/175341]\n",
      "loss: 0.515785  [158400/175341]\n",
      "loss: 0.259328  [160000/175341]\n",
      "loss: 0.256345  [161600/175341]\n",
      "loss: 0.454947  [163200/175341]\n",
      "loss: 0.315933  [164800/175341]\n",
      "loss: 0.371319  [166400/175341]\n",
      "loss: 0.356211  [168000/175341]\n",
      "loss: 0.763127  [169600/175341]\n",
      "loss: 0.646150  [171200/175341]\n",
      "loss: 0.311732  [172800/175341]\n",
      "loss: 0.122736  [174400/175341]\n",
      "Train Accuracy: 80.9035%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.524026, F1-score: 76.59%, Macro_F1-Score:  41.45%  \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.539125  [    0/175341]\n",
      "loss: 0.428894  [ 1600/175341]\n",
      "loss: 0.296494  [ 3200/175341]\n",
      "loss: 0.846010  [ 4800/175341]\n",
      "loss: 0.388143  [ 6400/175341]\n",
      "loss: 0.293763  [ 8000/175341]\n",
      "loss: 0.301306  [ 9600/175341]\n",
      "loss: 0.401819  [11200/175341]\n",
      "loss: 0.672148  [12800/175341]\n",
      "loss: 0.365461  [14400/175341]\n",
      "loss: 0.329573  [16000/175341]\n",
      "loss: 0.172015  [17600/175341]\n",
      "loss: 0.533365  [19200/175341]\n",
      "loss: 0.612215  [20800/175341]\n",
      "loss: 0.107461  [22400/175341]\n",
      "loss: 0.512841  [24000/175341]\n",
      "loss: 0.293470  [25600/175341]\n",
      "loss: 0.600294  [27200/175341]\n",
      "loss: 0.281379  [28800/175341]\n",
      "loss: 0.247688  [30400/175341]\n",
      "loss: 0.476642  [32000/175341]\n",
      "loss: 1.136600  [33600/175341]\n",
      "loss: 0.407359  [35200/175341]\n",
      "loss: 0.295114  [36800/175341]\n",
      "loss: 0.500312  [38400/175341]\n",
      "loss: 0.450444  [40000/175341]\n",
      "loss: 0.253658  [41600/175341]\n",
      "loss: 0.292561  [43200/175341]\n",
      "loss: 0.273776  [44800/175341]\n",
      "loss: 0.618583  [46400/175341]\n",
      "loss: 0.590228  [48000/175341]\n",
      "loss: 0.542176  [49600/175341]\n",
      "loss: 0.178838  [51200/175341]\n",
      "loss: 0.407355  [52800/175341]\n",
      "loss: 0.575520  [54400/175341]\n",
      "loss: 0.294636  [56000/175341]\n",
      "loss: 0.390447  [57600/175341]\n",
      "loss: 0.401181  [59200/175341]\n",
      "loss: 0.447535  [60800/175341]\n",
      "loss: 0.668286  [62400/175341]\n",
      "loss: 0.297231  [64000/175341]\n",
      "loss: 0.223881  [65600/175341]\n",
      "loss: 0.237694  [67200/175341]\n",
      "loss: 0.744751  [68800/175341]\n",
      "loss: 0.402435  [70400/175341]\n",
      "loss: 0.029641  [72000/175341]\n",
      "loss: 0.429167  [73600/175341]\n",
      "loss: 0.215763  [75200/175341]\n",
      "loss: 0.464796  [76800/175341]\n",
      "loss: 0.733620  [78400/175341]\n",
      "loss: 0.475256  [80000/175341]\n",
      "loss: 0.626705  [81600/175341]\n",
      "loss: 0.322075  [83200/175341]\n",
      "loss: 0.918051  [84800/175341]\n",
      "loss: 0.423298  [86400/175341]\n",
      "loss: 0.088860  [88000/175341]\n",
      "loss: 0.342298  [89600/175341]\n",
      "loss: 0.717428  [91200/175341]\n",
      "loss: 0.551709  [92800/175341]\n",
      "loss: 0.239151  [94400/175341]\n",
      "loss: 0.376211  [96000/175341]\n",
      "loss: 0.871014  [97600/175341]\n",
      "loss: 0.318516  [99200/175341]\n",
      "loss: 0.503323  [100800/175341]\n",
      "loss: 0.384963  [102400/175341]\n",
      "loss: 0.554479  [104000/175341]\n",
      "loss: 0.539187  [105600/175341]\n",
      "loss: 0.836533  [107200/175341]\n",
      "loss: 0.527059  [108800/175341]\n",
      "loss: 0.691017  [110400/175341]\n",
      "loss: 0.360917  [112000/175341]\n",
      "loss: 0.505707  [113600/175341]\n",
      "loss: 0.268560  [115200/175341]\n",
      "loss: 0.596995  [116800/175341]\n",
      "loss: 0.571984  [118400/175341]\n",
      "loss: 0.566514  [120000/175341]\n",
      "loss: 0.158696  [121600/175341]\n",
      "loss: 0.299176  [123200/175341]\n",
      "loss: 0.379909  [124800/175341]\n",
      "loss: 0.408708  [126400/175341]\n",
      "loss: 0.413323  [128000/175341]\n",
      "loss: 0.723238  [129600/175341]\n",
      "loss: 0.513288  [131200/175341]\n",
      "loss: 0.417768  [132800/175341]\n",
      "loss: 0.593393  [134400/175341]\n",
      "loss: 0.351288  [136000/175341]\n",
      "loss: 0.417084  [137600/175341]\n",
      "loss: 0.466180  [139200/175341]\n",
      "loss: 0.327611  [140800/175341]\n",
      "loss: 0.482004  [142400/175341]\n",
      "loss: 0.499016  [144000/175341]\n",
      "loss: 0.352237  [145600/175341]\n",
      "loss: 0.261439  [147200/175341]\n",
      "loss: 0.309014  [148800/175341]\n",
      "loss: 0.513602  [150400/175341]\n",
      "loss: 0.483070  [152000/175341]\n",
      "loss: 0.372136  [153600/175341]\n",
      "loss: 0.675500  [155200/175341]\n",
      "loss: 0.750284  [156800/175341]\n",
      "loss: 0.347469  [158400/175341]\n",
      "loss: 0.306761  [160000/175341]\n",
      "loss: 0.162363  [161600/175341]\n",
      "loss: 0.512042  [163200/175341]\n",
      "loss: 0.629249  [164800/175341]\n",
      "loss: 0.576896  [166400/175341]\n",
      "loss: 0.633691  [168000/175341]\n",
      "loss: 0.394904  [169600/175341]\n",
      "loss: 0.572745  [171200/175341]\n",
      "loss: 0.609607  [172800/175341]\n",
      "loss: 0.300631  [174400/175341]\n",
      "Train Accuracy: 80.9337%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.550694, F1-score: 75.52%, Macro_F1-Score:  41.69%  \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.312740  [    0/175341]\n",
      "loss: 0.462133  [ 1600/175341]\n",
      "loss: 0.361325  [ 3200/175341]\n",
      "loss: 0.428575  [ 4800/175341]\n",
      "loss: 0.411461  [ 6400/175341]\n",
      "loss: 0.263437  [ 8000/175341]\n",
      "loss: 0.275395  [ 9600/175341]\n",
      "loss: 0.416883  [11200/175341]\n",
      "loss: 0.394002  [12800/175341]\n",
      "loss: 0.478347  [14400/175341]\n",
      "loss: 0.463393  [16000/175341]\n",
      "loss: 0.421123  [17600/175341]\n",
      "loss: 0.565481  [19200/175341]\n",
      "loss: 0.666222  [20800/175341]\n",
      "loss: 0.282886  [22400/175341]\n",
      "loss: 0.433636  [24000/175341]\n",
      "loss: 0.397009  [25600/175341]\n",
      "loss: 0.633310  [27200/175341]\n",
      "loss: 0.512390  [28800/175341]\n",
      "loss: 0.610849  [30400/175341]\n",
      "loss: 0.651467  [32000/175341]\n",
      "loss: 0.279449  [33600/175341]\n",
      "loss: 0.434327  [35200/175341]\n",
      "loss: 0.462228  [36800/175341]\n",
      "loss: 0.733698  [38400/175341]\n",
      "loss: 0.453634  [40000/175341]\n",
      "loss: 0.248961  [41600/175341]\n",
      "loss: 0.596903  [43200/175341]\n",
      "loss: 0.504929  [44800/175341]\n",
      "loss: 0.477053  [46400/175341]\n",
      "loss: 0.354772  [48000/175341]\n",
      "loss: 0.857918  [49600/175341]\n",
      "loss: 0.543498  [51200/175341]\n",
      "loss: 0.185869  [52800/175341]\n",
      "loss: 0.556389  [54400/175341]\n",
      "loss: 0.832998  [56000/175341]\n",
      "loss: 0.595398  [57600/175341]\n",
      "loss: 0.940557  [59200/175341]\n",
      "loss: 0.674519  [60800/175341]\n",
      "loss: 0.411094  [62400/175341]\n",
      "loss: 0.165608  [64000/175341]\n",
      "loss: 0.246409  [65600/175341]\n",
      "loss: 0.396926  [67200/175341]\n",
      "loss: 0.406942  [68800/175341]\n",
      "loss: 0.233233  [70400/175341]\n",
      "loss: 0.390783  [72000/175341]\n",
      "loss: 0.669341  [73600/175341]\n",
      "loss: 0.349677  [75200/175341]\n",
      "loss: 0.574751  [76800/175341]\n",
      "loss: 0.504915  [78400/175341]\n",
      "loss: 0.336645  [80000/175341]\n",
      "loss: 0.321321  [81600/175341]\n",
      "loss: 0.341763  [83200/175341]\n",
      "loss: 0.363134  [84800/175341]\n",
      "loss: 0.415176  [86400/175341]\n",
      "loss: 0.681458  [88000/175341]\n",
      "loss: 0.401866  [89600/175341]\n",
      "loss: 0.237583  [91200/175341]\n",
      "loss: 0.441405  [92800/175341]\n",
      "loss: 0.206064  [94400/175341]\n",
      "loss: 0.314037  [96000/175341]\n",
      "loss: 0.238919  [97600/175341]\n",
      "loss: 0.114834  [99200/175341]\n",
      "loss: 0.767481  [100800/175341]\n",
      "loss: 0.179856  [102400/175341]\n",
      "loss: 0.373569  [104000/175341]\n",
      "loss: 0.396307  [105600/175341]\n",
      "loss: 0.230627  [107200/175341]\n",
      "loss: 0.334668  [108800/175341]\n",
      "loss: 0.359996  [110400/175341]\n",
      "loss: 0.504972  [112000/175341]\n",
      "loss: 0.581092  [113600/175341]\n",
      "loss: 0.396923  [115200/175341]\n",
      "loss: 0.421416  [116800/175341]\n",
      "loss: 0.218589  [118400/175341]\n",
      "loss: 0.735453  [120000/175341]\n",
      "loss: 0.384797  [121600/175341]\n",
      "loss: 0.460430  [123200/175341]\n",
      "loss: 0.529895  [124800/175341]\n",
      "loss: 0.756425  [126400/175341]\n",
      "loss: 0.656661  [128000/175341]\n",
      "loss: 0.663693  [129600/175341]\n",
      "loss: 0.505356  [131200/175341]\n",
      "loss: 0.626728  [132800/175341]\n",
      "loss: 0.428316  [134400/175341]\n",
      "loss: 0.408982  [136000/175341]\n",
      "loss: 0.411705  [137600/175341]\n",
      "loss: 0.525004  [139200/175341]\n",
      "loss: 0.271884  [140800/175341]\n",
      "loss: 0.608577  [142400/175341]\n",
      "loss: 0.811688  [144000/175341]\n",
      "loss: 0.483628  [145600/175341]\n",
      "loss: 0.620430  [147200/175341]\n",
      "loss: 0.503523  [148800/175341]\n",
      "loss: 0.317631  [150400/175341]\n",
      "loss: 0.507397  [152000/175341]\n",
      "loss: 0.510391  [153600/175341]\n",
      "loss: 0.467131  [155200/175341]\n",
      "loss: 0.347533  [156800/175341]\n",
      "loss: 0.580532  [158400/175341]\n",
      "loss: 0.945419  [160000/175341]\n",
      "loss: 0.485980  [161600/175341]\n",
      "loss: 0.483039  [163200/175341]\n",
      "loss: 0.814954  [164800/175341]\n",
      "loss: 0.511122  [166400/175341]\n",
      "loss: 0.417194  [168000/175341]\n",
      "loss: 0.785110  [169600/175341]\n",
      "loss: 0.341391  [171200/175341]\n",
      "loss: 0.654155  [172800/175341]\n",
      "loss: 0.259848  [174400/175341]\n",
      "Train Accuracy: 80.9913%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.534908, F1-score: 76.15%, Macro_F1-Score:  41.69%  \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.493145  [    0/175341]\n",
      "loss: 0.799352  [ 1600/175341]\n",
      "loss: 0.210594  [ 3200/175341]\n",
      "loss: 0.296457  [ 4800/175341]\n",
      "loss: 0.619664  [ 6400/175341]\n",
      "loss: 0.496878  [ 8000/175341]\n",
      "loss: 0.321261  [ 9600/175341]\n",
      "loss: 0.634982  [11200/175341]\n",
      "loss: 0.246996  [12800/175341]\n",
      "loss: 0.503884  [14400/175341]\n",
      "loss: 0.423760  [16000/175341]\n",
      "loss: 0.552416  [17600/175341]\n",
      "loss: 0.311383  [19200/175341]\n",
      "loss: 0.662244  [20800/175341]\n",
      "loss: 0.509830  [22400/175341]\n",
      "loss: 0.786391  [24000/175341]\n",
      "loss: 1.066853  [25600/175341]\n",
      "loss: 0.213537  [27200/175341]\n",
      "loss: 0.848997  [28800/175341]\n",
      "loss: 0.341199  [30400/175341]\n",
      "loss: 0.506214  [32000/175341]\n",
      "loss: 0.379225  [33600/175341]\n",
      "loss: 0.328032  [35200/175341]\n",
      "loss: 0.261126  [36800/175341]\n",
      "loss: 0.298365  [38400/175341]\n",
      "loss: 0.798357  [40000/175341]\n",
      "loss: 0.416135  [41600/175341]\n",
      "loss: 0.379664  [43200/175341]\n",
      "loss: 0.459191  [44800/175341]\n",
      "loss: 0.307630  [46400/175341]\n",
      "loss: 0.500972  [48000/175341]\n",
      "loss: 0.612655  [49600/175341]\n",
      "loss: 0.492632  [51200/175341]\n",
      "loss: 0.504904  [52800/175341]\n",
      "loss: 0.735110  [54400/175341]\n",
      "loss: 0.276792  [56000/175341]\n",
      "loss: 0.506662  [57600/175341]\n",
      "loss: 0.581552  [59200/175341]\n",
      "loss: 0.319990  [60800/175341]\n",
      "loss: 0.412559  [62400/175341]\n",
      "loss: 0.341201  [64000/175341]\n",
      "loss: 0.821767  [65600/175341]\n",
      "loss: 0.426332  [67200/175341]\n",
      "loss: 0.636516  [68800/175341]\n",
      "loss: 0.502927  [70400/175341]\n",
      "loss: 0.446267  [72000/175341]\n",
      "loss: 0.652072  [73600/175341]\n",
      "loss: 0.325214  [75200/175341]\n",
      "loss: 0.668175  [76800/175341]\n",
      "loss: 0.530114  [78400/175341]\n",
      "loss: 0.621819  [80000/175341]\n",
      "loss: 0.275211  [81600/175341]\n",
      "loss: 0.394699  [83200/175341]\n",
      "loss: 0.333917  [84800/175341]\n",
      "loss: 0.379145  [86400/175341]\n",
      "loss: 0.537117  [88000/175341]\n",
      "loss: 0.207961  [89600/175341]\n",
      "loss: 0.602644  [91200/175341]\n",
      "loss: 0.342634  [92800/175341]\n",
      "loss: 0.448173  [94400/175341]\n",
      "loss: 0.115091  [96000/175341]\n",
      "loss: 0.301971  [97600/175341]\n",
      "loss: 0.261359  [99200/175341]\n",
      "loss: 0.318920  [100800/175341]\n",
      "loss: 0.498653  [102400/175341]\n",
      "loss: 0.154333  [104000/175341]\n",
      "loss: 0.260194  [105600/175341]\n",
      "loss: 0.506069  [107200/175341]\n",
      "loss: 0.370269  [108800/175341]\n",
      "loss: 0.735407  [110400/175341]\n",
      "loss: 0.301435  [112000/175341]\n",
      "loss: 0.489293  [113600/175341]\n",
      "loss: 0.338921  [115200/175341]\n",
      "loss: 0.330039  [116800/175341]\n",
      "loss: 0.322637  [118400/175341]\n",
      "loss: 0.830147  [120000/175341]\n",
      "loss: 0.656050  [121600/175341]\n",
      "loss: 0.350773  [123200/175341]\n",
      "loss: 0.721305  [124800/175341]\n",
      "loss: 0.590650  [126400/175341]\n",
      "loss: 0.573940  [128000/175341]\n",
      "loss: 0.744369  [129600/175341]\n",
      "loss: 0.357358  [131200/175341]\n",
      "loss: 0.969537  [132800/175341]\n",
      "loss: 0.385507  [134400/175341]\n",
      "loss: 0.511022  [136000/175341]\n",
      "loss: 0.369952  [137600/175341]\n",
      "loss: 0.261371  [139200/175341]\n",
      "loss: 0.450727  [140800/175341]\n",
      "loss: 0.804093  [142400/175341]\n",
      "loss: 0.383041  [144000/175341]\n",
      "loss: 0.539293  [145600/175341]\n",
      "loss: 0.522870  [147200/175341]\n",
      "loss: 0.428920  [148800/175341]\n",
      "loss: 0.366613  [150400/175341]\n",
      "loss: 0.611485  [152000/175341]\n",
      "loss: 0.295549  [153600/175341]\n",
      "loss: 0.307750  [155200/175341]\n",
      "loss: 0.820894  [156800/175341]\n",
      "loss: 0.630551  [158400/175341]\n",
      "loss: 0.268528  [160000/175341]\n",
      "loss: 0.575211  [161600/175341]\n",
      "loss: 0.495361  [163200/175341]\n",
      "loss: 0.448634  [164800/175341]\n",
      "loss: 0.316695  [166400/175341]\n",
      "loss: 0.384542  [168000/175341]\n",
      "loss: 0.353181  [169600/175341]\n",
      "loss: 0.464607  [171200/175341]\n",
      "loss: 0.197189  [172800/175341]\n",
      "loss: 0.108334  [174400/175341]\n",
      "Train Accuracy: 81.0523%\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.577977, F1-score: 73.85%, Macro_F1-Score:  40.73%  \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.718657  [    0/175341]\n",
      "loss: 0.659094  [ 1600/175341]\n",
      "loss: 0.564442  [ 3200/175341]\n",
      "loss: 0.233352  [ 4800/175341]\n",
      "loss: 0.636351  [ 6400/175341]\n",
      "loss: 0.465334  [ 8000/175341]\n",
      "loss: 0.504390  [ 9600/175341]\n",
      "loss: 0.446798  [11200/175341]\n",
      "loss: 0.643328  [12800/175341]\n",
      "loss: 0.592858  [14400/175341]\n",
      "loss: 0.682583  [16000/175341]\n",
      "loss: 0.303908  [17600/175341]\n",
      "loss: 0.644742  [19200/175341]\n",
      "loss: 0.416968  [20800/175341]\n",
      "loss: 0.385771  [22400/175341]\n",
      "loss: 0.313439  [24000/175341]\n",
      "loss: 0.260632  [25600/175341]\n",
      "loss: 0.547659  [27200/175341]\n",
      "loss: 0.245971  [28800/175341]\n",
      "loss: 0.824913  [30400/175341]\n",
      "loss: 0.564940  [32000/175341]\n",
      "loss: 0.392248  [33600/175341]\n",
      "loss: 0.662817  [35200/175341]\n",
      "loss: 0.378686  [36800/175341]\n",
      "loss: 0.997483  [38400/175341]\n",
      "loss: 0.321616  [40000/175341]\n",
      "loss: 0.574030  [41600/175341]\n",
      "loss: 1.209106  [43200/175341]\n",
      "loss: 0.693306  [44800/175341]\n",
      "loss: 0.426841  [46400/175341]\n",
      "loss: 0.484062  [48000/175341]\n",
      "loss: 0.791864  [49600/175341]\n",
      "loss: 0.590199  [51200/175341]\n",
      "loss: 0.568611  [52800/175341]\n",
      "loss: 0.276291  [54400/175341]\n",
      "loss: 0.411422  [56000/175341]\n",
      "loss: 0.841710  [57600/175341]\n",
      "loss: 0.759007  [59200/175341]\n",
      "loss: 0.461460  [60800/175341]\n",
      "loss: 0.349635  [62400/175341]\n",
      "loss: 0.768540  [64000/175341]\n",
      "loss: 0.532038  [65600/175341]\n",
      "loss: 0.371460  [67200/175341]\n",
      "loss: 1.218445  [68800/175341]\n",
      "loss: 0.318204  [70400/175341]\n",
      "loss: 0.387160  [72000/175341]\n",
      "loss: 0.563667  [73600/175341]\n",
      "loss: 0.443523  [75200/175341]\n",
      "loss: 0.782176  [76800/175341]\n",
      "loss: 0.425998  [78400/175341]\n",
      "loss: 0.727391  [80000/175341]\n",
      "loss: 0.644395  [81600/175341]\n",
      "loss: 0.334524  [83200/175341]\n",
      "loss: 0.535888  [84800/175341]\n",
      "loss: 0.148737  [86400/175341]\n",
      "loss: 0.299555  [88000/175341]\n",
      "loss: 0.202148  [89600/175341]\n",
      "loss: 0.516858  [91200/175341]\n",
      "loss: 0.734508  [92800/175341]\n",
      "loss: 0.313941  [94400/175341]\n",
      "loss: 0.711735  [96000/175341]\n",
      "loss: 0.529931  [97600/175341]\n",
      "loss: 0.405679  [99200/175341]\n",
      "loss: 0.690636  [100800/175341]\n",
      "loss: 1.022512  [102400/175341]\n",
      "loss: 0.570643  [104000/175341]\n",
      "loss: 0.531095  [105600/175341]\n",
      "loss: 0.194814  [107200/175341]\n",
      "loss: 0.349768  [108800/175341]\n",
      "loss: 0.322147  [110400/175341]\n",
      "loss: 0.720636  [112000/175341]\n",
      "loss: 0.251187  [113600/175341]\n",
      "loss: 0.508988  [115200/175341]\n",
      "loss: 0.257042  [116800/175341]\n",
      "loss: 0.353503  [118400/175341]\n",
      "loss: 0.320936  [120000/175341]\n",
      "loss: 0.539818  [121600/175341]\n",
      "loss: 0.519650  [123200/175341]\n",
      "loss: 0.253813  [124800/175341]\n",
      "loss: 0.435689  [126400/175341]\n",
      "loss: 0.957604  [128000/175341]\n",
      "loss: 0.356642  [129600/175341]\n",
      "loss: 0.331664  [131200/175341]\n",
      "loss: 0.839755  [132800/175341]\n",
      "loss: 0.919822  [134400/175341]\n",
      "loss: 0.931066  [136000/175341]\n",
      "loss: 0.248881  [137600/175341]\n",
      "loss: 0.341162  [139200/175341]\n",
      "loss: 0.528172  [140800/175341]\n",
      "loss: 0.274983  [142400/175341]\n",
      "loss: 0.490577  [144000/175341]\n",
      "loss: 0.414219  [145600/175341]\n",
      "loss: 0.287283  [147200/175341]\n",
      "loss: 0.251900  [148800/175341]\n",
      "loss: 0.422183  [150400/175341]\n",
      "loss: 0.502017  [152000/175341]\n",
      "loss: 0.350101  [153600/175341]\n",
      "loss: 0.427207  [155200/175341]\n",
      "loss: 0.243446  [156800/175341]\n",
      "loss: 0.388023  [158400/175341]\n",
      "loss: 0.802134  [160000/175341]\n",
      "loss: 0.422328  [161600/175341]\n",
      "loss: 0.308588  [163200/175341]\n",
      "loss: 0.673140  [164800/175341]\n",
      "loss: 0.250801  [166400/175341]\n",
      "loss: 0.239821  [168000/175341]\n",
      "loss: 0.142954  [169600/175341]\n",
      "loss: 0.211896  [171200/175341]\n",
      "loss: 0.674076  [172800/175341]\n",
      "loss: 0.570109  [174400/175341]\n",
      "Train Accuracy: 81.0717%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.530042, F1-score: 76.75%, Macro_F1-Score:  41.86%  \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.202689  [    0/175341]\n",
      "loss: 0.666849  [ 1600/175341]\n",
      "loss: 0.519144  [ 3200/175341]\n",
      "loss: 0.716681  [ 4800/175341]\n",
      "loss: 0.554515  [ 6400/175341]\n",
      "loss: 0.198090  [ 8000/175341]\n",
      "loss: 0.394988  [ 9600/175341]\n",
      "loss: 0.354173  [11200/175341]\n",
      "loss: 0.829070  [12800/175341]\n",
      "loss: 0.808764  [14400/175341]\n",
      "loss: 0.471003  [16000/175341]\n",
      "loss: 0.394842  [17600/175341]\n",
      "loss: 0.825449  [19200/175341]\n",
      "loss: 0.315453  [20800/175341]\n",
      "loss: 0.149873  [22400/175341]\n",
      "loss: 0.519156  [24000/175341]\n",
      "loss: 0.674881  [25600/175341]\n",
      "loss: 0.385366  [27200/175341]\n",
      "loss: 0.592687  [28800/175341]\n",
      "loss: 0.326985  [30400/175341]\n",
      "loss: 0.524715  [32000/175341]\n",
      "loss: 0.371034  [33600/175341]\n",
      "loss: 0.439862  [35200/175341]\n",
      "loss: 0.558968  [36800/175341]\n",
      "loss: 0.871627  [38400/175341]\n",
      "loss: 0.840661  [40000/175341]\n",
      "loss: 0.444376  [41600/175341]\n",
      "loss: 0.352786  [43200/175341]\n",
      "loss: 0.507231  [44800/175341]\n",
      "loss: 0.434011  [46400/175341]\n",
      "loss: 0.666080  [48000/175341]\n",
      "loss: 0.590311  [49600/175341]\n",
      "loss: 0.202505  [51200/175341]\n",
      "loss: 0.300925  [52800/175341]\n",
      "loss: 0.274004  [54400/175341]\n",
      "loss: 0.740582  [56000/175341]\n",
      "loss: 0.325245  [57600/175341]\n",
      "loss: 0.372804  [59200/175341]\n",
      "loss: 0.184232  [60800/175341]\n",
      "loss: 0.450923  [62400/175341]\n",
      "loss: 0.380385  [64000/175341]\n",
      "loss: 0.222377  [65600/175341]\n",
      "loss: 0.446631  [67200/175341]\n",
      "loss: 0.855541  [68800/175341]\n",
      "loss: 0.176343  [70400/175341]\n",
      "loss: 0.672792  [72000/175341]\n",
      "loss: 0.303046  [73600/175341]\n",
      "loss: 0.417913  [75200/175341]\n",
      "loss: 0.706931  [76800/175341]\n",
      "loss: 0.252863  [78400/175341]\n",
      "loss: 0.570128  [80000/175341]\n",
      "loss: 0.657485  [81600/175341]\n",
      "loss: 0.119176  [83200/175341]\n",
      "loss: 0.471842  [84800/175341]\n",
      "loss: 1.162985  [86400/175341]\n",
      "loss: 0.282365  [88000/175341]\n",
      "loss: 0.334583  [89600/175341]\n",
      "loss: 0.287287  [91200/175341]\n",
      "loss: 0.309898  [92800/175341]\n",
      "loss: 0.653111  [94400/175341]\n",
      "loss: 0.508044  [96000/175341]\n",
      "loss: 0.924007  [97600/175341]\n",
      "loss: 0.515178  [99200/175341]\n",
      "loss: 0.272899  [100800/175341]\n",
      "loss: 0.737804  [102400/175341]\n",
      "loss: 0.444601  [104000/175341]\n",
      "loss: 0.879768  [105600/175341]\n",
      "loss: 0.542979  [107200/175341]\n",
      "loss: 0.200996  [108800/175341]\n",
      "loss: 0.333493  [110400/175341]\n",
      "loss: 0.397179  [112000/175341]\n",
      "loss: 0.292841  [113600/175341]\n",
      "loss: 0.515642  [115200/175341]\n",
      "loss: 0.549665  [116800/175341]\n",
      "loss: 0.345516  [118400/175341]\n",
      "loss: 0.553480  [120000/175341]\n",
      "loss: 0.632336  [121600/175341]\n",
      "loss: 0.513097  [123200/175341]\n",
      "loss: 0.259848  [124800/175341]\n",
      "loss: 0.626953  [126400/175341]\n",
      "loss: 0.520273  [128000/175341]\n",
      "loss: 0.418047  [129600/175341]\n",
      "loss: 0.573009  [131200/175341]\n",
      "loss: 0.305455  [132800/175341]\n",
      "loss: 0.472607  [134400/175341]\n",
      "loss: 0.594312  [136000/175341]\n",
      "loss: 0.244394  [137600/175341]\n",
      "loss: 0.915915  [139200/175341]\n",
      "loss: 0.432141  [140800/175341]\n",
      "loss: 0.264320  [142400/175341]\n",
      "loss: 0.254176  [144000/175341]\n",
      "loss: 0.482277  [145600/175341]\n",
      "loss: 0.468448  [147200/175341]\n",
      "loss: 0.670805  [148800/175341]\n",
      "loss: 0.482208  [150400/175341]\n",
      "loss: 0.576017  [152000/175341]\n",
      "loss: 0.868356  [153600/175341]\n",
      "loss: 0.563434  [155200/175341]\n",
      "loss: 0.374382  [156800/175341]\n",
      "loss: 0.484663  [158400/175341]\n",
      "loss: 0.716905  [160000/175341]\n",
      "loss: 0.776321  [161600/175341]\n",
      "loss: 0.094858  [163200/175341]\n",
      "loss: 0.233876  [164800/175341]\n",
      "loss: 0.467051  [166400/175341]\n",
      "loss: 0.443835  [168000/175341]\n",
      "loss: 0.167300  [169600/175341]\n",
      "loss: 0.753239  [171200/175341]\n",
      "loss: 0.429727  [172800/175341]\n",
      "loss: 0.500508  [174400/175341]\n",
      "Train Accuracy: 81.1316%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.539947, F1-score: 75.73%, Macro_F1-Score:  41.85%  \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.216902  [    0/175341]\n",
      "loss: 0.559603  [ 1600/175341]\n",
      "loss: 0.481812  [ 3200/175341]\n",
      "loss: 0.650259  [ 4800/175341]\n",
      "loss: 0.403614  [ 6400/175341]\n",
      "loss: 0.335369  [ 8000/175341]\n",
      "loss: 0.296564  [ 9600/175341]\n",
      "loss: 0.486375  [11200/175341]\n",
      "loss: 0.326102  [12800/175341]\n",
      "loss: 1.071415  [14400/175341]\n",
      "loss: 0.376759  [16000/175341]\n",
      "loss: 0.364426  [17600/175341]\n",
      "loss: 0.154437  [19200/175341]\n",
      "loss: 0.162792  [20800/175341]\n",
      "loss: 0.394138  [22400/175341]\n",
      "loss: 0.718606  [24000/175341]\n",
      "loss: 0.485074  [25600/175341]\n",
      "loss: 0.301762  [27200/175341]\n",
      "loss: 0.672216  [28800/175341]\n",
      "loss: 0.639202  [30400/175341]\n",
      "loss: 0.324271  [32000/175341]\n",
      "loss: 0.934489  [33600/175341]\n",
      "loss: 0.278244  [35200/175341]\n",
      "loss: 0.287815  [36800/175341]\n",
      "loss: 0.417329  [38400/175341]\n",
      "loss: 0.532393  [40000/175341]\n",
      "loss: 0.654137  [41600/175341]\n",
      "loss: 0.417716  [43200/175341]\n",
      "loss: 0.243899  [44800/175341]\n",
      "loss: 0.545473  [46400/175341]\n",
      "loss: 0.339789  [48000/175341]\n",
      "loss: 0.429290  [49600/175341]\n",
      "loss: 0.382768  [51200/175341]\n",
      "loss: 0.181373  [52800/175341]\n",
      "loss: 0.454816  [54400/175341]\n",
      "loss: 0.503414  [56000/175341]\n",
      "loss: 0.346306  [57600/175341]\n",
      "loss: 0.712871  [59200/175341]\n",
      "loss: 0.195980  [60800/175341]\n",
      "loss: 0.779742  [62400/175341]\n",
      "loss: 0.533458  [64000/175341]\n",
      "loss: 0.798497  [65600/175341]\n",
      "loss: 0.521861  [67200/175341]\n",
      "loss: 0.219449  [68800/175341]\n",
      "loss: 0.649383  [70400/175341]\n",
      "loss: 0.351962  [72000/175341]\n",
      "loss: 0.421398  [73600/175341]\n",
      "loss: 0.493856  [75200/175341]\n",
      "loss: 0.226690  [76800/175341]\n",
      "loss: 0.372623  [78400/175341]\n",
      "loss: 0.426209  [80000/175341]\n",
      "loss: 0.567870  [81600/175341]\n",
      "loss: 0.544017  [83200/175341]\n",
      "loss: 0.785268  [84800/175341]\n",
      "loss: 0.320556  [86400/175341]\n",
      "loss: 0.538137  [88000/175341]\n",
      "loss: 1.038090  [89600/175341]\n",
      "loss: 0.534930  [91200/175341]\n",
      "loss: 0.581343  [92800/175341]\n",
      "loss: 0.502910  [94400/175341]\n",
      "loss: 0.780716  [96000/175341]\n",
      "loss: 0.364499  [97600/175341]\n",
      "loss: 0.376318  [99200/175341]\n",
      "loss: 0.406453  [100800/175341]\n",
      "loss: 0.465983  [102400/175341]\n",
      "loss: 0.139311  [104000/175341]\n",
      "loss: 0.289012  [105600/175341]\n",
      "loss: 0.429156  [107200/175341]\n",
      "loss: 0.135751  [108800/175341]\n",
      "loss: 0.203222  [110400/175341]\n",
      "loss: 0.701018  [112000/175341]\n",
      "loss: 0.379710  [113600/175341]\n",
      "loss: 0.349904  [115200/175341]\n",
      "loss: 0.616730  [116800/175341]\n",
      "loss: 0.623777  [118400/175341]\n",
      "loss: 1.104309  [120000/175341]\n",
      "loss: 0.720502  [121600/175341]\n",
      "loss: 0.104364  [123200/175341]\n",
      "loss: 0.308880  [124800/175341]\n",
      "loss: 0.118146  [126400/175341]\n",
      "loss: 0.664976  [128000/175341]\n",
      "loss: 0.511674  [129600/175341]\n",
      "loss: 0.341358  [131200/175341]\n",
      "loss: 0.501383  [132800/175341]\n",
      "loss: 0.225318  [134400/175341]\n",
      "loss: 0.214176  [136000/175341]\n",
      "loss: 0.330984  [137600/175341]\n",
      "loss: 0.486645  [139200/175341]\n",
      "loss: 0.617531  [140800/175341]\n",
      "loss: 0.321865  [142400/175341]\n",
      "loss: 0.398530  [144000/175341]\n",
      "loss: 0.303051  [145600/175341]\n",
      "loss: 0.324387  [147200/175341]\n",
      "loss: 0.195156  [148800/175341]\n",
      "loss: 0.511305  [150400/175341]\n",
      "loss: 0.228952  [152000/175341]\n",
      "loss: 0.207914  [153600/175341]\n",
      "loss: 0.649127  [155200/175341]\n",
      "loss: 0.210965  [156800/175341]\n",
      "loss: 0.661189  [158400/175341]\n",
      "loss: 0.455976  [160000/175341]\n",
      "loss: 0.284763  [161600/175341]\n",
      "loss: 0.712110  [163200/175341]\n",
      "loss: 0.293226  [164800/175341]\n",
      "loss: 0.427185  [166400/175341]\n",
      "loss: 0.575497  [168000/175341]\n",
      "loss: 0.059584  [169600/175341]\n",
      "loss: 0.224752  [171200/175341]\n",
      "loss: 0.915595  [172800/175341]\n",
      "loss: 0.407633  [174400/175341]\n",
      "Train Accuracy: 81.2023%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.547176, F1-score: 75.52%, Macro_F1-Score:  41.84%  \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.485671  [    0/175341]\n",
      "loss: 0.878015  [ 1600/175341]\n",
      "loss: 0.490353  [ 3200/175341]\n",
      "loss: 0.859906  [ 4800/175341]\n",
      "loss: 0.369469  [ 6400/175341]\n",
      "loss: 0.396312  [ 8000/175341]\n",
      "loss: 0.501056  [ 9600/175341]\n",
      "loss: 0.160855  [11200/175341]\n",
      "loss: 0.841917  [12800/175341]\n",
      "loss: 0.318155  [14400/175341]\n",
      "loss: 0.345988  [16000/175341]\n",
      "loss: 0.292130  [17600/175341]\n",
      "loss: 0.354178  [19200/175341]\n",
      "loss: 0.118972  [20800/175341]\n",
      "loss: 0.440529  [22400/175341]\n",
      "loss: 0.651170  [24000/175341]\n",
      "loss: 0.538945  [25600/175341]\n",
      "loss: 0.541863  [27200/175341]\n",
      "loss: 0.610946  [28800/175341]\n",
      "loss: 0.612906  [30400/175341]\n",
      "loss: 0.523684  [32000/175341]\n",
      "loss: 0.499019  [33600/175341]\n",
      "loss: 0.530838  [35200/175341]\n",
      "loss: 0.083927  [36800/175341]\n",
      "loss: 0.504039  [38400/175341]\n",
      "loss: 0.629235  [40000/175341]\n",
      "loss: 0.209157  [41600/175341]\n",
      "loss: 0.762093  [43200/175341]\n",
      "loss: 0.614211  [44800/175341]\n",
      "loss: 0.474996  [46400/175341]\n",
      "loss: 0.381531  [48000/175341]\n",
      "loss: 0.577298  [49600/175341]\n",
      "loss: 0.487657  [51200/175341]\n",
      "loss: 0.710729  [52800/175341]\n",
      "loss: 0.332306  [54400/175341]\n",
      "loss: 0.295260  [56000/175341]\n",
      "loss: 0.646359  [57600/175341]\n",
      "loss: 0.420459  [59200/175341]\n",
      "loss: 0.419993  [60800/175341]\n",
      "loss: 0.773979  [62400/175341]\n",
      "loss: 0.470834  [64000/175341]\n",
      "loss: 0.412914  [65600/175341]\n",
      "loss: 0.365407  [67200/175341]\n",
      "loss: 0.672943  [68800/175341]\n",
      "loss: 0.253292  [70400/175341]\n",
      "loss: 0.359363  [72000/175341]\n",
      "loss: 0.445774  [73600/175341]\n",
      "loss: 0.610577  [75200/175341]\n",
      "loss: 0.330675  [76800/175341]\n",
      "loss: 0.678276  [78400/175341]\n",
      "loss: 0.750160  [80000/175341]\n",
      "loss: 0.501670  [81600/175341]\n",
      "loss: 0.353592  [83200/175341]\n",
      "loss: 1.016555  [84800/175341]\n",
      "loss: 0.215144  [86400/175341]\n",
      "loss: 0.441438  [88000/175341]\n",
      "loss: 1.064643  [89600/175341]\n",
      "loss: 0.633536  [91200/175341]\n",
      "loss: 0.634974  [92800/175341]\n",
      "loss: 0.649036  [94400/175341]\n",
      "loss: 0.444655  [96000/175341]\n",
      "loss: 0.169555  [97600/175341]\n",
      "loss: 0.650252  [99200/175341]\n",
      "loss: 0.297801  [100800/175341]\n",
      "loss: 0.325026  [102400/175341]\n",
      "loss: 0.245169  [104000/175341]\n",
      "loss: 0.128033  [105600/175341]\n",
      "loss: 0.334308  [107200/175341]\n",
      "loss: 0.426242  [108800/175341]\n",
      "loss: 0.246325  [110400/175341]\n",
      "loss: 0.232263  [112000/175341]\n",
      "loss: 0.609226  [113600/175341]\n",
      "loss: 0.432807  [115200/175341]\n",
      "loss: 0.281197  [116800/175341]\n",
      "loss: 0.470626  [118400/175341]\n",
      "loss: 0.703678  [120000/175341]\n",
      "loss: 0.357588  [121600/175341]\n",
      "loss: 0.461995  [123200/175341]\n",
      "loss: 0.449681  [124800/175341]\n",
      "loss: 0.345187  [126400/175341]\n",
      "loss: 0.432229  [128000/175341]\n",
      "loss: 0.469815  [129600/175341]\n",
      "loss: 0.253348  [131200/175341]\n",
      "loss: 0.426851  [132800/175341]\n",
      "loss: 0.292845  [134400/175341]\n",
      "loss: 0.619000  [136000/175341]\n",
      "loss: 0.192584  [137600/175341]\n",
      "loss: 0.274881  [139200/175341]\n",
      "loss: 0.437826  [140800/175341]\n",
      "loss: 0.604187  [142400/175341]\n",
      "loss: 0.238343  [144000/175341]\n",
      "loss: 0.670570  [145600/175341]\n",
      "loss: 0.585178  [147200/175341]\n",
      "loss: 0.126014  [148800/175341]\n",
      "loss: 0.304899  [150400/175341]\n",
      "loss: 0.413201  [152000/175341]\n",
      "loss: 0.187292  [153600/175341]\n",
      "loss: 0.722265  [155200/175341]\n",
      "loss: 0.548908  [156800/175341]\n",
      "loss: 0.308474  [158400/175341]\n",
      "loss: 0.309307  [160000/175341]\n",
      "loss: 0.148120  [161600/175341]\n",
      "loss: 0.582934  [163200/175341]\n",
      "loss: 0.339294  [164800/175341]\n",
      "loss: 0.170398  [166400/175341]\n",
      "loss: 0.374112  [168000/175341]\n",
      "loss: 0.168368  [169600/175341]\n",
      "loss: 0.321974  [171200/175341]\n",
      "loss: 0.350955  [172800/175341]\n",
      "loss: 0.575849  [174400/175341]\n",
      "Train Accuracy: 81.1755%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.550957, F1-score: 75.46%, Macro_F1-Score:  40.96%  \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.272349  [    0/175341]\n",
      "loss: 0.223782  [ 1600/175341]\n",
      "loss: 0.547377  [ 3200/175341]\n",
      "loss: 0.334055  [ 4800/175341]\n",
      "loss: 0.642050  [ 6400/175341]\n",
      "loss: 0.820027  [ 8000/175341]\n",
      "loss: 0.568745  [ 9600/175341]\n",
      "loss: 0.610272  [11200/175341]\n",
      "loss: 0.085339  [12800/175341]\n",
      "loss: 0.481538  [14400/175341]\n",
      "loss: 0.519547  [16000/175341]\n",
      "loss: 0.618520  [17600/175341]\n",
      "loss: 0.463205  [19200/175341]\n",
      "loss: 0.338009  [20800/175341]\n",
      "loss: 0.429979  [22400/175341]\n",
      "loss: 0.732034  [24000/175341]\n",
      "loss: 0.583700  [25600/175341]\n",
      "loss: 0.366567  [27200/175341]\n",
      "loss: 0.245022  [28800/175341]\n",
      "loss: 0.483510  [30400/175341]\n",
      "loss: 0.293864  [32000/175341]\n",
      "loss: 0.571464  [33600/175341]\n",
      "loss: 0.930523  [35200/175341]\n",
      "loss: 0.415478  [36800/175341]\n",
      "loss: 0.499821  [38400/175341]\n",
      "loss: 0.369364  [40000/175341]\n",
      "loss: 0.594588  [41600/175341]\n",
      "loss: 0.391946  [43200/175341]\n",
      "loss: 0.522663  [44800/175341]\n",
      "loss: 0.789427  [46400/175341]\n",
      "loss: 0.297822  [48000/175341]\n",
      "loss: 0.867649  [49600/175341]\n",
      "loss: 0.449753  [51200/175341]\n",
      "loss: 0.243445  [52800/175341]\n",
      "loss: 0.759744  [54400/175341]\n",
      "loss: 0.548915  [56000/175341]\n",
      "loss: 0.270441  [57600/175341]\n",
      "loss: 0.774134  [59200/175341]\n",
      "loss: 0.215733  [60800/175341]\n",
      "loss: 0.512074  [62400/175341]\n",
      "loss: 0.365457  [64000/175341]\n",
      "loss: 0.609677  [65600/175341]\n",
      "loss: 0.363541  [67200/175341]\n",
      "loss: 0.484795  [68800/175341]\n",
      "loss: 0.508258  [70400/175341]\n",
      "loss: 0.905865  [72000/175341]\n",
      "loss: 0.618598  [73600/175341]\n",
      "loss: 0.533214  [75200/175341]\n",
      "loss: 0.136415  [76800/175341]\n",
      "loss: 0.435444  [78400/175341]\n",
      "loss: 0.663936  [80000/175341]\n",
      "loss: 0.493222  [81600/175341]\n",
      "loss: 0.488782  [83200/175341]\n",
      "loss: 0.538775  [84800/175341]\n",
      "loss: 0.803313  [86400/175341]\n",
      "loss: 0.346087  [88000/175341]\n",
      "loss: 0.239870  [89600/175341]\n",
      "loss: 0.479336  [91200/175341]\n",
      "loss: 0.513128  [92800/175341]\n",
      "loss: 0.190957  [94400/175341]\n",
      "loss: 0.850088  [96000/175341]\n",
      "loss: 0.343860  [97600/175341]\n",
      "loss: 0.306696  [99200/175341]\n",
      "loss: 0.637220  [100800/175341]\n",
      "loss: 0.378852  [102400/175341]\n",
      "loss: 0.439740  [104000/175341]\n",
      "loss: 0.611435  [105600/175341]\n",
      "loss: 0.379325  [107200/175341]\n",
      "loss: 0.298565  [108800/175341]\n",
      "loss: 0.464612  [110400/175341]\n",
      "loss: 0.495299  [112000/175341]\n",
      "loss: 0.241080  [113600/175341]\n",
      "loss: 0.665888  [115200/175341]\n",
      "loss: 0.555365  [116800/175341]\n",
      "loss: 0.658277  [118400/175341]\n",
      "loss: 0.459150  [120000/175341]\n",
      "loss: 0.006727  [121600/175341]\n",
      "loss: 0.706209  [123200/175341]\n",
      "loss: 0.173936  [124800/175341]\n",
      "loss: 0.614303  [126400/175341]\n",
      "loss: 0.214545  [128000/175341]\n",
      "loss: 0.106123  [129600/175341]\n",
      "loss: 1.120432  [131200/175341]\n",
      "loss: 0.340474  [132800/175341]\n",
      "loss: 0.466387  [134400/175341]\n",
      "loss: 0.534823  [136000/175341]\n",
      "loss: 0.530873  [137600/175341]\n",
      "loss: 0.348971  [139200/175341]\n",
      "loss: 0.593871  [140800/175341]\n",
      "loss: 0.271682  [142400/175341]\n",
      "loss: 0.353392  [144000/175341]\n",
      "loss: 0.198586  [145600/175341]\n",
      "loss: 0.518966  [147200/175341]\n",
      "loss: 0.368486  [148800/175341]\n",
      "loss: 0.630927  [150400/175341]\n",
      "loss: 0.667561  [152000/175341]\n",
      "loss: 0.167079  [153600/175341]\n",
      "loss: 0.583827  [155200/175341]\n",
      "loss: 0.370345  [156800/175341]\n",
      "loss: 0.691085  [158400/175341]\n",
      "loss: 0.190879  [160000/175341]\n",
      "loss: 0.379642  [161600/175341]\n",
      "loss: 0.268347  [163200/175341]\n",
      "loss: 0.485667  [164800/175341]\n",
      "loss: 0.465751  [166400/175341]\n",
      "loss: 0.102759  [168000/175341]\n",
      "loss: 0.528065  [169600/175341]\n",
      "loss: 0.104604  [171200/175341]\n",
      "loss: 0.193534  [172800/175341]\n",
      "loss: 0.667060  [174400/175341]\n",
      "Train Accuracy: 81.2366%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.524152, F1-score: 76.63%, Macro_F1-Score:  42.58%  \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.415972  [    0/175341]\n",
      "loss: 0.249097  [ 1600/175341]\n",
      "loss: 0.236009  [ 3200/175341]\n",
      "loss: 0.191380  [ 4800/175341]\n",
      "loss: 0.198132  [ 6400/175341]\n",
      "loss: 0.674710  [ 8000/175341]\n",
      "loss: 0.541089  [ 9600/175341]\n",
      "loss: 0.185779  [11200/175341]\n",
      "loss: 0.264834  [12800/175341]\n",
      "loss: 0.324998  [14400/175341]\n",
      "loss: 0.507676  [16000/175341]\n",
      "loss: 0.317642  [17600/175341]\n",
      "loss: 0.376140  [19200/175341]\n",
      "loss: 0.538667  [20800/175341]\n",
      "loss: 0.485917  [22400/175341]\n",
      "loss: 0.585843  [24000/175341]\n",
      "loss: 0.455188  [25600/175341]\n",
      "loss: 0.350551  [27200/175341]\n",
      "loss: 0.441457  [28800/175341]\n",
      "loss: 0.583743  [30400/175341]\n",
      "loss: 0.622630  [32000/175341]\n",
      "loss: 0.448729  [33600/175341]\n",
      "loss: 0.369221  [35200/175341]\n",
      "loss: 0.351930  [36800/175341]\n",
      "loss: 0.626257  [38400/175341]\n",
      "loss: 0.454837  [40000/175341]\n",
      "loss: 0.611093  [41600/175341]\n",
      "loss: 0.496034  [43200/175341]\n",
      "loss: 0.321091  [44800/175341]\n",
      "loss: 0.878462  [46400/175341]\n",
      "loss: 0.613671  [48000/175341]\n",
      "loss: 0.285238  [49600/175341]\n",
      "loss: 0.356955  [51200/175341]\n",
      "loss: 0.490113  [52800/175341]\n",
      "loss: 0.646295  [54400/175341]\n",
      "loss: 0.320862  [56000/175341]\n",
      "loss: 0.458075  [57600/175341]\n",
      "loss: 0.381569  [59200/175341]\n",
      "loss: 0.570797  [60800/175341]\n",
      "loss: 0.310396  [62400/175341]\n",
      "loss: 0.522740  [64000/175341]\n",
      "loss: 0.640030  [65600/175341]\n",
      "loss: 0.536033  [67200/175341]\n",
      "loss: 0.262047  [68800/175341]\n",
      "loss: 0.699052  [70400/175341]\n",
      "loss: 0.387077  [72000/175341]\n",
      "loss: 0.391139  [73600/175341]\n",
      "loss: 0.525534  [75200/175341]\n",
      "loss: 0.817146  [76800/175341]\n",
      "loss: 0.630200  [78400/175341]\n",
      "loss: 0.624199  [80000/175341]\n",
      "loss: 0.447692  [81600/175341]\n",
      "loss: 0.329738  [83200/175341]\n",
      "loss: 0.778931  [84800/175341]\n",
      "loss: 0.448963  [86400/175341]\n",
      "loss: 0.529148  [88000/175341]\n",
      "loss: 0.638547  [89600/175341]\n",
      "loss: 0.459720  [91200/175341]\n",
      "loss: 0.598399  [92800/175341]\n",
      "loss: 0.580783  [94400/175341]\n",
      "loss: 0.462016  [96000/175341]\n",
      "loss: 0.960306  [97600/175341]\n",
      "loss: 0.342989  [99200/175341]\n",
      "loss: 0.194335  [100800/175341]\n",
      "loss: 0.274219  [102400/175341]\n",
      "loss: 0.297523  [104000/175341]\n",
      "loss: 0.414377  [105600/175341]\n",
      "loss: 0.165102  [107200/175341]\n",
      "loss: 0.307923  [108800/175341]\n",
      "loss: 0.150263  [110400/175341]\n",
      "loss: 0.533007  [112000/175341]\n",
      "loss: 0.611693  [113600/175341]\n",
      "loss: 0.504141  [115200/175341]\n",
      "loss: 0.543180  [116800/175341]\n",
      "loss: 0.311865  [118400/175341]\n",
      "loss: 0.078500  [120000/175341]\n",
      "loss: 0.315368  [121600/175341]\n",
      "loss: 0.969867  [123200/175341]\n",
      "loss: 0.160728  [124800/175341]\n",
      "loss: 0.994029  [126400/175341]\n",
      "loss: 0.766825  [128000/175341]\n",
      "loss: 0.598111  [129600/175341]\n",
      "loss: 0.775118  [131200/175341]\n",
      "loss: 0.752093  [132800/175341]\n",
      "loss: 0.493250  [134400/175341]\n",
      "loss: 0.490485  [136000/175341]\n",
      "loss: 0.469486  [137600/175341]\n",
      "loss: 0.651086  [139200/175341]\n",
      "loss: 0.736559  [140800/175341]\n",
      "loss: 0.613782  [142400/175341]\n",
      "loss: 0.988962  [144000/175341]\n",
      "loss: 0.447821  [145600/175341]\n",
      "loss: 0.275783  [147200/175341]\n",
      "loss: 0.492639  [148800/175341]\n",
      "loss: 0.434733  [150400/175341]\n",
      "loss: 1.026964  [152000/175341]\n",
      "loss: 0.487623  [153600/175341]\n",
      "loss: 0.790175  [155200/175341]\n",
      "loss: 0.460859  [156800/175341]\n",
      "loss: 0.311835  [158400/175341]\n",
      "loss: 0.172415  [160000/175341]\n",
      "loss: 0.419339  [161600/175341]\n",
      "loss: 0.669932  [163200/175341]\n",
      "loss: 0.297519  [164800/175341]\n",
      "loss: 0.190276  [166400/175341]\n",
      "loss: 0.377144  [168000/175341]\n",
      "loss: 0.525131  [169600/175341]\n",
      "loss: 0.233921  [171200/175341]\n",
      "loss: 0.661688  [172800/175341]\n",
      "loss: 0.417647  [174400/175341]\n",
      "Train Accuracy: 81.3147%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.558344, F1-score: 75.03%, Macro_F1-Score:  41.22%  \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.503607  [    0/175341]\n",
      "loss: 0.418937  [ 1600/175341]\n",
      "loss: 0.365722  [ 3200/175341]\n",
      "loss: 0.577025  [ 4800/175341]\n",
      "loss: 0.477819  [ 6400/175341]\n",
      "loss: 0.417826  [ 8000/175341]\n",
      "loss: 0.295641  [ 9600/175341]\n",
      "loss: 0.300749  [11200/175341]\n",
      "loss: 0.518792  [12800/175341]\n",
      "loss: 0.660455  [14400/175341]\n",
      "loss: 0.271855  [16000/175341]\n",
      "loss: 0.627038  [17600/175341]\n",
      "loss: 0.609780  [19200/175341]\n",
      "loss: 0.500249  [20800/175341]\n",
      "loss: 0.081964  [22400/175341]\n",
      "loss: 0.070859  [24000/175341]\n",
      "loss: 0.860777  [25600/175341]\n",
      "loss: 0.612982  [27200/175341]\n",
      "loss: 0.285330  [28800/175341]\n",
      "loss: 0.522875  [30400/175341]\n",
      "loss: 0.425266  [32000/175341]\n",
      "loss: 0.669879  [33600/175341]\n",
      "loss: 0.531582  [35200/175341]\n",
      "loss: 0.428509  [36800/175341]\n",
      "loss: 0.280684  [38400/175341]\n",
      "loss: 0.616004  [40000/175341]\n",
      "loss: 0.229318  [41600/175341]\n",
      "loss: 0.476051  [43200/175341]\n",
      "loss: 0.516846  [44800/175341]\n",
      "loss: 0.465341  [46400/175341]\n",
      "loss: 0.388588  [48000/175341]\n",
      "loss: 0.309050  [49600/175341]\n",
      "loss: 0.692615  [51200/175341]\n",
      "loss: 0.381472  [52800/175341]\n",
      "loss: 0.542801  [54400/175341]\n",
      "loss: 0.565737  [56000/175341]\n",
      "loss: 0.325222  [57600/175341]\n",
      "loss: 0.643164  [59200/175341]\n",
      "loss: 0.544489  [60800/175341]\n",
      "loss: 0.263835  [62400/175341]\n",
      "loss: 0.576638  [64000/175341]\n",
      "loss: 0.761250  [65600/175341]\n",
      "loss: 0.530906  [67200/175341]\n",
      "loss: 0.440497  [68800/175341]\n",
      "loss: 0.624325  [70400/175341]\n",
      "loss: 0.479500  [72000/175341]\n",
      "loss: 0.329354  [73600/175341]\n",
      "loss: 0.646931  [75200/175341]\n",
      "loss: 0.451671  [76800/175341]\n",
      "loss: 0.239497  [78400/175341]\n",
      "loss: 0.829544  [80000/175341]\n",
      "loss: 0.550090  [81600/175341]\n",
      "loss: 0.488479  [83200/175341]\n",
      "loss: 0.573038  [84800/175341]\n",
      "loss: 0.461158  [86400/175341]\n",
      "loss: 0.551454  [88000/175341]\n",
      "loss: 0.523064  [89600/175341]\n",
      "loss: 0.724731  [91200/175341]\n",
      "loss: 0.830676  [92800/175341]\n",
      "loss: 0.541626  [94400/175341]\n",
      "loss: 0.521056  [96000/175341]\n",
      "loss: 0.463531  [97600/175341]\n",
      "loss: 0.223349  [99200/175341]\n",
      "loss: 0.472736  [100800/175341]\n",
      "loss: 0.141475  [102400/175341]\n",
      "loss: 0.039507  [104000/175341]\n",
      "loss: 0.384172  [105600/175341]\n",
      "loss: 0.582796  [107200/175341]\n",
      "loss: 0.500414  [108800/175341]\n",
      "loss: 0.864192  [110400/175341]\n",
      "loss: 0.607293  [112000/175341]\n",
      "loss: 0.618742  [113600/175341]\n",
      "loss: 0.401051  [115200/175341]\n",
      "loss: 0.381197  [116800/175341]\n",
      "loss: 0.620252  [118400/175341]\n",
      "loss: 0.472929  [120000/175341]\n",
      "loss: 0.648233  [121600/175341]\n",
      "loss: 0.270458  [123200/175341]\n",
      "loss: 0.106675  [124800/175341]\n",
      "loss: 0.325719  [126400/175341]\n",
      "loss: 0.294977  [128000/175341]\n",
      "loss: 0.418503  [129600/175341]\n",
      "loss: 0.192696  [131200/175341]\n",
      "loss: 0.245922  [132800/175341]\n",
      "loss: 0.176115  [134400/175341]\n",
      "loss: 0.595268  [136000/175341]\n",
      "loss: 0.767985  [137600/175341]\n",
      "loss: 0.556131  [139200/175341]\n",
      "loss: 0.486761  [140800/175341]\n",
      "loss: 0.447421  [142400/175341]\n",
      "loss: 0.249528  [144000/175341]\n",
      "loss: 0.293419  [145600/175341]\n",
      "loss: 0.897319  [147200/175341]\n",
      "loss: 0.740235  [148800/175341]\n",
      "loss: 0.631994  [150400/175341]\n",
      "loss: 0.528876  [152000/175341]\n",
      "loss: 0.408521  [153600/175341]\n",
      "loss: 0.260055  [155200/175341]\n",
      "loss: 0.470343  [156800/175341]\n",
      "loss: 0.561020  [158400/175341]\n",
      "loss: 0.729948  [160000/175341]\n",
      "loss: 0.622106  [161600/175341]\n",
      "loss: 0.367502  [163200/175341]\n",
      "loss: 0.274270  [164800/175341]\n",
      "loss: 0.492429  [166400/175341]\n",
      "loss: 0.326159  [168000/175341]\n",
      "loss: 0.319651  [169600/175341]\n",
      "loss: 0.181062  [171200/175341]\n",
      "loss: 0.520809  [172800/175341]\n",
      "loss: 0.285308  [174400/175341]\n",
      "Train Accuracy: 81.2993%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.541018, F1-score: 76.43%, Macro_F1-Score:  41.96%  \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.329577  [    0/175341]\n",
      "loss: 0.397946  [ 1600/175341]\n",
      "loss: 0.532026  [ 3200/175341]\n",
      "loss: 0.546797  [ 4800/175341]\n",
      "loss: 0.579296  [ 6400/175341]\n",
      "loss: 0.970375  [ 8000/175341]\n",
      "loss: 0.102276  [ 9600/175341]\n",
      "loss: 0.225516  [11200/175341]\n",
      "loss: 0.555575  [12800/175341]\n",
      "loss: 0.694363  [14400/175341]\n",
      "loss: 0.595407  [16000/175341]\n",
      "loss: 1.046600  [17600/175341]\n",
      "loss: 0.286880  [19200/175341]\n",
      "loss: 0.412648  [20800/175341]\n",
      "loss: 1.073068  [22400/175341]\n",
      "loss: 0.625430  [24000/175341]\n",
      "loss: 0.555839  [25600/175341]\n",
      "loss: 0.903462  [27200/175341]\n",
      "loss: 0.744014  [28800/175341]\n",
      "loss: 0.384978  [30400/175341]\n",
      "loss: 0.171278  [32000/175341]\n",
      "loss: 0.577995  [33600/175341]\n",
      "loss: 0.116114  [35200/175341]\n",
      "loss: 0.227869  [36800/175341]\n",
      "loss: 0.581184  [38400/175341]\n",
      "loss: 0.373702  [40000/175341]\n",
      "loss: 0.473349  [41600/175341]\n",
      "loss: 0.503256  [43200/175341]\n",
      "loss: 0.235719  [44800/175341]\n",
      "loss: 0.512002  [46400/175341]\n",
      "loss: 0.547512  [48000/175341]\n",
      "loss: 0.139042  [49600/175341]\n",
      "loss: 0.699881  [51200/175341]\n",
      "loss: 0.355692  [52800/175341]\n",
      "loss: 0.573094  [54400/175341]\n",
      "loss: 0.611559  [56000/175341]\n",
      "loss: 0.800720  [57600/175341]\n",
      "loss: 0.469613  [59200/175341]\n",
      "loss: 0.284882  [60800/175341]\n",
      "loss: 0.770094  [62400/175341]\n",
      "loss: 0.345588  [64000/175341]\n",
      "loss: 0.282746  [65600/175341]\n",
      "loss: 0.134586  [67200/175341]\n",
      "loss: 0.182405  [68800/175341]\n",
      "loss: 0.444055  [70400/175341]\n",
      "loss: 0.404731  [72000/175341]\n",
      "loss: 0.359976  [73600/175341]\n",
      "loss: 0.469231  [75200/175341]\n",
      "loss: 0.738457  [76800/175341]\n",
      "loss: 0.345392  [78400/175341]\n",
      "loss: 0.491817  [80000/175341]\n",
      "loss: 0.448771  [81600/175341]\n",
      "loss: 0.657719  [83200/175341]\n",
      "loss: 0.387083  [84800/175341]\n",
      "loss: 0.497466  [86400/175341]\n",
      "loss: 0.496282  [88000/175341]\n",
      "loss: 0.292584  [89600/175341]\n",
      "loss: 0.292666  [91200/175341]\n",
      "loss: 0.683554  [92800/175341]\n",
      "loss: 0.498677  [94400/175341]\n",
      "loss: 0.334079  [96000/175341]\n",
      "loss: 0.343798  [97600/175341]\n",
      "loss: 0.669444  [99200/175341]\n",
      "loss: 0.558527  [100800/175341]\n",
      "loss: 0.402798  [102400/175341]\n",
      "loss: 0.296186  [104000/175341]\n",
      "loss: 0.530510  [105600/175341]\n",
      "loss: 0.318979  [107200/175341]\n",
      "loss: 0.675840  [108800/175341]\n",
      "loss: 0.634439  [110400/175341]\n",
      "loss: 0.169312  [112000/175341]\n",
      "loss: 0.599347  [113600/175341]\n",
      "loss: 0.563750  [115200/175341]\n",
      "loss: 0.615177  [116800/175341]\n",
      "loss: 0.249356  [118400/175341]\n",
      "loss: 0.471367  [120000/175341]\n",
      "loss: 0.638824  [121600/175341]\n",
      "loss: 0.468061  [123200/175341]\n",
      "loss: 0.408107  [124800/175341]\n",
      "loss: 0.536415  [126400/175341]\n",
      "loss: 0.404176  [128000/175341]\n",
      "loss: 0.453498  [129600/175341]\n",
      "loss: 0.514753  [131200/175341]\n",
      "loss: 0.243205  [132800/175341]\n",
      "loss: 0.264188  [134400/175341]\n",
      "loss: 0.582337  [136000/175341]\n",
      "loss: 0.251089  [137600/175341]\n",
      "loss: 0.179272  [139200/175341]\n",
      "loss: 0.367630  [140800/175341]\n",
      "loss: 0.259078  [142400/175341]\n",
      "loss: 0.710589  [144000/175341]\n",
      "loss: 0.237338  [145600/175341]\n",
      "loss: 0.252554  [147200/175341]\n",
      "loss: 0.285144  [148800/175341]\n",
      "loss: 0.529629  [150400/175341]\n",
      "loss: 0.458965  [152000/175341]\n",
      "loss: 0.647616  [153600/175341]\n",
      "loss: 0.303160  [155200/175341]\n",
      "loss: 1.010110  [156800/175341]\n",
      "loss: 0.345063  [158400/175341]\n",
      "loss: 0.422531  [160000/175341]\n",
      "loss: 0.304375  [161600/175341]\n",
      "loss: 0.506166  [163200/175341]\n",
      "loss: 0.652645  [164800/175341]\n",
      "loss: 0.457791  [166400/175341]\n",
      "loss: 0.406347  [168000/175341]\n",
      "loss: 0.073527  [169600/175341]\n",
      "loss: 0.502850  [171200/175341]\n",
      "loss: 0.694182  [172800/175341]\n",
      "loss: 0.376313  [174400/175341]\n",
      "Train Accuracy: 81.3444%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.535467, F1-score: 76.04%, Macro_F1-Score:  42.14%  \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.327364  [    0/175341]\n",
      "loss: 0.531550  [ 1600/175341]\n",
      "loss: 0.462036  [ 3200/175341]\n",
      "loss: 0.608569  [ 4800/175341]\n",
      "loss: 0.817773  [ 6400/175341]\n",
      "loss: 0.549996  [ 8000/175341]\n",
      "loss: 0.534410  [ 9600/175341]\n",
      "loss: 0.304708  [11200/175341]\n",
      "loss: 0.138150  [12800/175341]\n",
      "loss: 0.288410  [14400/175341]\n",
      "loss: 1.180318  [16000/175341]\n",
      "loss: 0.695015  [17600/175341]\n",
      "loss: 0.255834  [19200/175341]\n",
      "loss: 0.346492  [20800/175341]\n",
      "loss: 0.484083  [22400/175341]\n",
      "loss: 0.730385  [24000/175341]\n",
      "loss: 0.527126  [25600/175341]\n",
      "loss: 0.384352  [27200/175341]\n",
      "loss: 0.330192  [28800/175341]\n",
      "loss: 0.350553  [30400/175341]\n",
      "loss: 0.230851  [32000/175341]\n",
      "loss: 0.312895  [33600/175341]\n",
      "loss: 0.530278  [35200/175341]\n",
      "loss: 0.555055  [36800/175341]\n",
      "loss: 0.650046  [38400/175341]\n",
      "loss: 0.528084  [40000/175341]\n",
      "loss: 0.542795  [41600/175341]\n",
      "loss: 0.543214  [43200/175341]\n",
      "loss: 0.309949  [44800/175341]\n",
      "loss: 1.031971  [46400/175341]\n",
      "loss: 0.609148  [48000/175341]\n",
      "loss: 0.383656  [49600/175341]\n",
      "loss: 0.468469  [51200/175341]\n",
      "loss: 0.609241  [52800/175341]\n",
      "loss: 0.749196  [54400/175341]\n",
      "loss: 0.681244  [56000/175341]\n",
      "loss: 0.286149  [57600/175341]\n",
      "loss: 0.351888  [59200/175341]\n",
      "loss: 0.782611  [60800/175341]\n",
      "loss: 0.406873  [62400/175341]\n",
      "loss: 0.411251  [64000/175341]\n",
      "loss: 0.576216  [65600/175341]\n",
      "loss: 0.200545  [67200/175341]\n",
      "loss: 0.464402  [68800/175341]\n",
      "loss: 0.602629  [70400/175341]\n",
      "loss: 0.329252  [72000/175341]\n",
      "loss: 0.302592  [73600/175341]\n",
      "loss: 0.474729  [75200/175341]\n",
      "loss: 0.541603  [76800/175341]\n",
      "loss: 0.268563  [78400/175341]\n",
      "loss: 0.660214  [80000/175341]\n",
      "loss: 0.278457  [81600/175341]\n",
      "loss: 0.408582  [83200/175341]\n",
      "loss: 0.116835  [84800/175341]\n",
      "loss: 0.237204  [86400/175341]\n",
      "loss: 0.340756  [88000/175341]\n",
      "loss: 0.908667  [89600/175341]\n",
      "loss: 0.372140  [91200/175341]\n",
      "loss: 0.548190  [92800/175341]\n",
      "loss: 0.191743  [94400/175341]\n",
      "loss: 0.407095  [96000/175341]\n",
      "loss: 0.647289  [97600/175341]\n",
      "loss: 0.668447  [99200/175341]\n",
      "loss: 0.343677  [100800/175341]\n",
      "loss: 0.751787  [102400/175341]\n",
      "loss: 0.682734  [104000/175341]\n",
      "loss: 0.455061  [105600/175341]\n",
      "loss: 0.325957  [107200/175341]\n",
      "loss: 0.651735  [108800/175341]\n",
      "loss: 0.241279  [110400/175341]\n",
      "loss: 0.227654  [112000/175341]\n",
      "loss: 0.160724  [113600/175341]\n",
      "loss: 0.331900  [115200/175341]\n",
      "loss: 0.355595  [116800/175341]\n",
      "loss: 0.520489  [118400/175341]\n",
      "loss: 0.617218  [120000/175341]\n",
      "loss: 0.483257  [121600/175341]\n",
      "loss: 0.907053  [123200/175341]\n",
      "loss: 0.339354  [124800/175341]\n",
      "loss: 0.711041  [126400/175341]\n",
      "loss: 0.366372  [128000/175341]\n",
      "loss: 0.382911  [129600/175341]\n",
      "loss: 0.495861  [131200/175341]\n",
      "loss: 0.486608  [132800/175341]\n",
      "loss: 0.460273  [134400/175341]\n",
      "loss: 0.603806  [136000/175341]\n",
      "loss: 0.224487  [137600/175341]\n",
      "loss: 0.762390  [139200/175341]\n",
      "loss: 0.646863  [140800/175341]\n",
      "loss: 0.471912  [142400/175341]\n",
      "loss: 0.103401  [144000/175341]\n",
      "loss: 0.781608  [145600/175341]\n",
      "loss: 0.386179  [147200/175341]\n",
      "loss: 0.168948  [148800/175341]\n",
      "loss: 0.354853  [150400/175341]\n",
      "loss: 0.899163  [152000/175341]\n",
      "loss: 0.577986  [153600/175341]\n",
      "loss: 0.362453  [155200/175341]\n",
      "loss: 0.433462  [156800/175341]\n",
      "loss: 0.954085  [158400/175341]\n",
      "loss: 0.372051  [160000/175341]\n",
      "loss: 0.644887  [161600/175341]\n",
      "loss: 0.557981  [163200/175341]\n",
      "loss: 0.386300  [164800/175341]\n",
      "loss: 0.382948  [166400/175341]\n",
      "loss: 0.733196  [168000/175341]\n",
      "loss: 0.307013  [169600/175341]\n",
      "loss: 0.651006  [171200/175341]\n",
      "loss: 0.476703  [172800/175341]\n",
      "loss: 0.391439  [174400/175341]\n",
      "Train Accuracy: 81.3860%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.542371, F1-score: 75.88%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.505980  [    0/175341]\n",
      "loss: 0.651199  [ 1600/175341]\n",
      "loss: 0.275149  [ 3200/175341]\n",
      "loss: 0.482184  [ 4800/175341]\n",
      "loss: 0.426944  [ 6400/175341]\n",
      "loss: 0.283011  [ 8000/175341]\n",
      "loss: 1.004738  [ 9600/175341]\n",
      "loss: 0.493356  [11200/175341]\n",
      "loss: 0.561100  [12800/175341]\n",
      "loss: 0.144451  [14400/175341]\n",
      "loss: 0.277025  [16000/175341]\n",
      "loss: 0.514852  [17600/175341]\n",
      "loss: 0.418580  [19200/175341]\n",
      "loss: 0.178241  [20800/175341]\n",
      "loss: 0.812873  [22400/175341]\n",
      "loss: 0.243958  [24000/175341]\n",
      "loss: 0.649449  [25600/175341]\n",
      "loss: 0.326463  [27200/175341]\n",
      "loss: 0.699791  [28800/175341]\n",
      "loss: 0.524275  [30400/175341]\n",
      "loss: 0.697515  [32000/175341]\n",
      "loss: 0.397007  [33600/175341]\n",
      "loss: 0.323459  [35200/175341]\n",
      "loss: 0.358733  [36800/175341]\n",
      "loss: 0.566276  [38400/175341]\n",
      "loss: 0.603616  [40000/175341]\n",
      "loss: 0.959373  [41600/175341]\n",
      "loss: 0.476204  [43200/175341]\n",
      "loss: 0.370468  [44800/175341]\n",
      "loss: 0.654522  [46400/175341]\n",
      "loss: 0.529486  [48000/175341]\n",
      "loss: 0.518033  [49600/175341]\n",
      "loss: 0.762871  [51200/175341]\n",
      "loss: 0.314085  [52800/175341]\n",
      "loss: 0.144083  [54400/175341]\n",
      "loss: 0.638223  [56000/175341]\n",
      "loss: 0.452540  [57600/175341]\n",
      "loss: 0.392964  [59200/175341]\n",
      "loss: 0.252543  [60800/175341]\n",
      "loss: 0.243640  [62400/175341]\n",
      "loss: 0.333348  [64000/175341]\n",
      "loss: 0.736869  [65600/175341]\n",
      "loss: 0.961012  [67200/175341]\n",
      "loss: 0.293369  [68800/175341]\n",
      "loss: 0.509912  [70400/175341]\n",
      "loss: 0.425672  [72000/175341]\n",
      "loss: 0.268568  [73600/175341]\n",
      "loss: 0.316593  [75200/175341]\n",
      "loss: 0.320473  [76800/175341]\n",
      "loss: 0.351806  [78400/175341]\n",
      "loss: 0.604655  [80000/175341]\n",
      "loss: 0.421999  [81600/175341]\n",
      "loss: 0.456375  [83200/175341]\n",
      "loss: 0.173636  [84800/175341]\n",
      "loss: 0.108293  [86400/175341]\n",
      "loss: 0.205503  [88000/175341]\n",
      "loss: 0.165987  [89600/175341]\n",
      "loss: 0.610848  [91200/175341]\n",
      "loss: 0.702229  [92800/175341]\n",
      "loss: 0.101391  [94400/175341]\n",
      "loss: 0.347351  [96000/175341]\n",
      "loss: 0.355653  [97600/175341]\n",
      "loss: 0.562335  [99200/175341]\n",
      "loss: 0.742330  [100800/175341]\n",
      "loss: 0.472809  [102400/175341]\n",
      "loss: 0.271431  [104000/175341]\n",
      "loss: 0.368461  [105600/175341]\n",
      "loss: 0.351765  [107200/175341]\n",
      "loss: 0.605988  [108800/175341]\n",
      "loss: 1.202717  [110400/175341]\n",
      "loss: 0.440451  [112000/175341]\n",
      "loss: 0.208889  [113600/175341]\n",
      "loss: 0.824498  [115200/175341]\n",
      "loss: 0.492203  [116800/175341]\n",
      "loss: 0.702108  [118400/175341]\n",
      "loss: 0.236254  [120000/175341]\n",
      "loss: 1.227618  [121600/175341]\n",
      "loss: 0.307500  [123200/175341]\n",
      "loss: 0.260620  [124800/175341]\n",
      "loss: 0.302067  [126400/175341]\n",
      "loss: 0.337527  [128000/175341]\n",
      "loss: 0.192673  [129600/175341]\n",
      "loss: 0.508898  [131200/175341]\n",
      "loss: 0.821609  [132800/175341]\n",
      "loss: 0.254083  [134400/175341]\n",
      "loss: 0.347255  [136000/175341]\n",
      "loss: 0.631382  [137600/175341]\n",
      "loss: 0.182086  [139200/175341]\n",
      "loss: 0.439882  [140800/175341]\n",
      "loss: 0.454758  [142400/175341]\n",
      "loss: 0.590065  [144000/175341]\n",
      "loss: 0.204981  [145600/175341]\n",
      "loss: 0.371707  [147200/175341]\n",
      "loss: 0.435163  [148800/175341]\n",
      "loss: 0.591529  [150400/175341]\n",
      "loss: 0.281196  [152000/175341]\n",
      "loss: 0.243258  [153600/175341]\n",
      "loss: 0.358834  [155200/175341]\n",
      "loss: 0.563583  [156800/175341]\n",
      "loss: 0.477786  [158400/175341]\n",
      "loss: 0.382133  [160000/175341]\n",
      "loss: 0.272297  [161600/175341]\n",
      "loss: 0.497253  [163200/175341]\n",
      "loss: 0.720506  [164800/175341]\n",
      "loss: 0.737950  [166400/175341]\n",
      "loss: 0.885233  [168000/175341]\n",
      "loss: 0.421601  [169600/175341]\n",
      "loss: 0.537353  [171200/175341]\n",
      "loss: 0.544225  [172800/175341]\n",
      "loss: 0.694451  [174400/175341]\n",
      "Train Accuracy: 81.3860%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.545613, F1-score: 75.24%, Macro_F1-Score:  42.13%  \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.129421  [    0/175341]\n",
      "loss: 0.537046  [ 1600/175341]\n",
      "loss: 0.212840  [ 3200/175341]\n",
      "loss: 0.379579  [ 4800/175341]\n",
      "loss: 0.386990  [ 6400/175341]\n",
      "loss: 0.361460  [ 8000/175341]\n",
      "loss: 0.378447  [ 9600/175341]\n",
      "loss: 0.145902  [11200/175341]\n",
      "loss: 0.120411  [12800/175341]\n",
      "loss: 0.417581  [14400/175341]\n",
      "loss: 0.374558  [16000/175341]\n",
      "loss: 0.718374  [17600/175341]\n",
      "loss: 0.214664  [19200/175341]\n",
      "loss: 0.197067  [20800/175341]\n",
      "loss: 0.549264  [22400/175341]\n",
      "loss: 0.701985  [24000/175341]\n",
      "loss: 0.460623  [25600/175341]\n",
      "loss: 0.559018  [27200/175341]\n",
      "loss: 0.576411  [28800/175341]\n",
      "loss: 0.465745  [30400/175341]\n",
      "loss: 0.422674  [32000/175341]\n",
      "loss: 0.378385  [33600/175341]\n",
      "loss: 0.471379  [35200/175341]\n",
      "loss: 0.197376  [36800/175341]\n",
      "loss: 0.526711  [38400/175341]\n",
      "loss: 0.484635  [40000/175341]\n",
      "loss: 0.314987  [41600/175341]\n",
      "loss: 0.571182  [43200/175341]\n",
      "loss: 0.564409  [44800/175341]\n",
      "loss: 0.987287  [46400/175341]\n",
      "loss: 0.491745  [48000/175341]\n",
      "loss: 0.317227  [49600/175341]\n",
      "loss: 0.178450  [51200/175341]\n",
      "loss: 0.757991  [52800/175341]\n",
      "loss: 0.234424  [54400/175341]\n",
      "loss: 0.368767  [56000/175341]\n",
      "loss: 0.405639  [57600/175341]\n",
      "loss: 0.434171  [59200/175341]\n",
      "loss: 0.266515  [60800/175341]\n",
      "loss: 0.576952  [62400/175341]\n",
      "loss: 0.708843  [64000/175341]\n",
      "loss: 0.450520  [65600/175341]\n",
      "loss: 0.803027  [67200/175341]\n",
      "loss: 0.420736  [68800/175341]\n",
      "loss: 0.586120  [70400/175341]\n",
      "loss: 0.889288  [72000/175341]\n",
      "loss: 0.489268  [73600/175341]\n",
      "loss: 0.779135  [75200/175341]\n",
      "loss: 0.396138  [76800/175341]\n",
      "loss: 0.356810  [78400/175341]\n",
      "loss: 0.788378  [80000/175341]\n",
      "loss: 0.427601  [81600/175341]\n",
      "loss: 0.715301  [83200/175341]\n",
      "loss: 0.570953  [84800/175341]\n",
      "loss: 0.439496  [86400/175341]\n",
      "loss: 0.824149  [88000/175341]\n",
      "loss: 0.683185  [89600/175341]\n",
      "loss: 0.073883  [91200/175341]\n",
      "loss: 0.290580  [92800/175341]\n",
      "loss: 0.602841  [94400/175341]\n",
      "loss: 0.484038  [96000/175341]\n",
      "loss: 0.775436  [97600/175341]\n",
      "loss: 0.617548  [99200/175341]\n",
      "loss: 0.367290  [100800/175341]\n",
      "loss: 0.650036  [102400/175341]\n",
      "loss: 0.587309  [104000/175341]\n",
      "loss: 0.652123  [105600/175341]\n",
      "loss: 0.430034  [107200/175341]\n",
      "loss: 0.641510  [108800/175341]\n",
      "loss: 0.513002  [110400/175341]\n",
      "loss: 0.482612  [112000/175341]\n",
      "loss: 0.215498  [113600/175341]\n",
      "loss: 0.624543  [115200/175341]\n",
      "loss: 0.385806  [116800/175341]\n",
      "loss: 0.297984  [118400/175341]\n",
      "loss: 0.501455  [120000/175341]\n",
      "loss: 0.211242  [121600/175341]\n",
      "loss: 0.797346  [123200/175341]\n",
      "loss: 0.317532  [124800/175341]\n",
      "loss: 0.844782  [126400/175341]\n",
      "loss: 0.484367  [128000/175341]\n",
      "loss: 0.687356  [129600/175341]\n",
      "loss: 0.358003  [131200/175341]\n",
      "loss: 0.220735  [132800/175341]\n",
      "loss: 0.508699  [134400/175341]\n",
      "loss: 0.068976  [136000/175341]\n",
      "loss: 0.432962  [137600/175341]\n",
      "loss: 0.294021  [139200/175341]\n",
      "loss: 0.352644  [140800/175341]\n",
      "loss: 0.653081  [142400/175341]\n",
      "loss: 0.019886  [144000/175341]\n",
      "loss: 0.380304  [145600/175341]\n",
      "loss: 0.294547  [147200/175341]\n",
      "loss: 0.168942  [148800/175341]\n",
      "loss: 0.317300  [150400/175341]\n",
      "loss: 0.592411  [152000/175341]\n",
      "loss: 0.342094  [153600/175341]\n",
      "loss: 0.626343  [155200/175341]\n",
      "loss: 0.236375  [156800/175341]\n",
      "loss: 0.341391  [158400/175341]\n",
      "loss: 0.571444  [160000/175341]\n",
      "loss: 0.560683  [161600/175341]\n",
      "loss: 0.379368  [163200/175341]\n",
      "loss: 0.294684  [164800/175341]\n",
      "loss: 0.741276  [166400/175341]\n",
      "loss: 0.077763  [168000/175341]\n",
      "loss: 0.233852  [169600/175341]\n",
      "loss: 0.296664  [171200/175341]\n",
      "loss: 0.422463  [172800/175341]\n",
      "loss: 0.369419  [174400/175341]\n",
      "Train Accuracy: 81.3769%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.546458, F1-score: 75.49%, Macro_F1-Score:  42.14%  \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.336591  [    0/175341]\n",
      "loss: 0.808204  [ 1600/175341]\n",
      "loss: 0.223798  [ 3200/175341]\n",
      "loss: 0.435540  [ 4800/175341]\n",
      "loss: 0.650721  [ 6400/175341]\n",
      "loss: 0.418652  [ 8000/175341]\n",
      "loss: 0.253638  [ 9600/175341]\n",
      "loss: 0.716709  [11200/175341]\n",
      "loss: 0.385229  [12800/175341]\n",
      "loss: 0.383489  [14400/175341]\n",
      "loss: 0.305794  [16000/175341]\n",
      "loss: 0.405787  [17600/175341]\n",
      "loss: 0.309105  [19200/175341]\n",
      "loss: 0.633957  [20800/175341]\n",
      "loss: 0.729406  [22400/175341]\n",
      "loss: 0.384835  [24000/175341]\n",
      "loss: 0.208332  [25600/175341]\n",
      "loss: 0.416471  [27200/175341]\n",
      "loss: 0.457992  [28800/175341]\n",
      "loss: 0.572191  [30400/175341]\n",
      "loss: 0.601370  [32000/175341]\n",
      "loss: 0.391259  [33600/175341]\n",
      "loss: 0.163891  [35200/175341]\n",
      "loss: 0.483069  [36800/175341]\n",
      "loss: 0.508068  [38400/175341]\n",
      "loss: 0.592840  [40000/175341]\n",
      "loss: 0.264700  [41600/175341]\n",
      "loss: 0.909552  [43200/175341]\n",
      "loss: 0.601672  [44800/175341]\n",
      "loss: 0.237950  [46400/175341]\n",
      "loss: 0.610249  [48000/175341]\n",
      "loss: 0.848315  [49600/175341]\n",
      "loss: 0.549569  [51200/175341]\n",
      "loss: 0.475741  [52800/175341]\n",
      "loss: 0.379415  [54400/175341]\n",
      "loss: 0.410244  [56000/175341]\n",
      "loss: 0.178383  [57600/175341]\n",
      "loss: 0.406990  [59200/175341]\n",
      "loss: 0.267402  [60800/175341]\n",
      "loss: 0.487514  [62400/175341]\n",
      "loss: 0.471375  [64000/175341]\n",
      "loss: 0.306062  [65600/175341]\n",
      "loss: 0.482348  [67200/175341]\n",
      "loss: 0.436784  [68800/175341]\n",
      "loss: 0.459525  [70400/175341]\n",
      "loss: 0.463003  [72000/175341]\n",
      "loss: 0.409045  [73600/175341]\n",
      "loss: 0.755038  [75200/175341]\n",
      "loss: 0.402834  [76800/175341]\n",
      "loss: 0.384522  [78400/175341]\n",
      "loss: 0.293049  [80000/175341]\n",
      "loss: 0.193133  [81600/175341]\n",
      "loss: 0.519985  [83200/175341]\n",
      "loss: 0.191650  [84800/175341]\n",
      "loss: 0.123069  [86400/175341]\n",
      "loss: 0.177369  [88000/175341]\n",
      "loss: 0.294774  [89600/175341]\n",
      "loss: 0.330522  [91200/175341]\n",
      "loss: 0.374422  [92800/175341]\n",
      "loss: 0.323501  [94400/175341]\n",
      "loss: 0.376368  [96000/175341]\n",
      "loss: 0.446054  [97600/175341]\n",
      "loss: 0.636020  [99200/175341]\n",
      "loss: 0.792518  [100800/175341]\n",
      "loss: 0.424638  [102400/175341]\n",
      "loss: 0.297488  [104000/175341]\n",
      "loss: 0.376464  [105600/175341]\n",
      "loss: 0.455127  [107200/175341]\n",
      "loss: 0.174824  [108800/175341]\n",
      "loss: 0.309783  [110400/175341]\n",
      "loss: 0.654927  [112000/175341]\n",
      "loss: 0.247222  [113600/175341]\n",
      "loss: 0.583175  [115200/175341]\n",
      "loss: 0.401236  [116800/175341]\n",
      "loss: 0.315611  [118400/175341]\n",
      "loss: 0.396618  [120000/175341]\n",
      "loss: 0.333909  [121600/175341]\n",
      "loss: 0.326253  [123200/175341]\n",
      "loss: 0.324354  [124800/175341]\n",
      "loss: 0.901109  [126400/175341]\n",
      "loss: 0.460565  [128000/175341]\n",
      "loss: 0.595327  [129600/175341]\n",
      "loss: 0.345066  [131200/175341]\n",
      "loss: 0.352321  [132800/175341]\n",
      "loss: 0.185510  [134400/175341]\n",
      "loss: 0.274679  [136000/175341]\n",
      "loss: 0.459362  [137600/175341]\n",
      "loss: 0.269550  [139200/175341]\n",
      "loss: 0.583522  [140800/175341]\n",
      "loss: 0.551585  [142400/175341]\n",
      "loss: 0.847908  [144000/175341]\n",
      "loss: 0.313278  [145600/175341]\n",
      "loss: 0.559147  [147200/175341]\n",
      "loss: 0.499913  [148800/175341]\n",
      "loss: 0.305667  [150400/175341]\n",
      "loss: 0.967649  [152000/175341]\n",
      "loss: 0.432043  [153600/175341]\n",
      "loss: 0.967602  [155200/175341]\n",
      "loss: 0.415716  [156800/175341]\n",
      "loss: 0.447504  [158400/175341]\n",
      "loss: 0.489574  [160000/175341]\n",
      "loss: 0.148632  [161600/175341]\n",
      "loss: 0.349018  [163200/175341]\n",
      "loss: 0.212894  [164800/175341]\n",
      "loss: 0.682925  [166400/175341]\n",
      "loss: 0.364926  [168000/175341]\n",
      "loss: 0.150673  [169600/175341]\n",
      "loss: 0.332685  [171200/175341]\n",
      "loss: 0.500372  [172800/175341]\n",
      "loss: 0.443682  [174400/175341]\n",
      "Train Accuracy: 81.4550%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.556587, F1-score: 75.13%, Macro_F1-Score:  41.23%  \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.511891  [    0/175341]\n",
      "loss: 0.480295  [ 1600/175341]\n",
      "loss: 0.785891  [ 3200/175341]\n",
      "loss: 0.135899  [ 4800/175341]\n",
      "loss: 0.179915  [ 6400/175341]\n",
      "loss: 0.467575  [ 8000/175341]\n",
      "loss: 0.369452  [ 9600/175341]\n",
      "loss: 0.580839  [11200/175341]\n",
      "loss: 0.177963  [12800/175341]\n",
      "loss: 0.151493  [14400/175341]\n",
      "loss: 0.241516  [16000/175341]\n",
      "loss: 0.733714  [17600/175341]\n",
      "loss: 0.895177  [19200/175341]\n",
      "loss: 0.642565  [20800/175341]\n",
      "loss: 0.517524  [22400/175341]\n",
      "loss: 0.557981  [24000/175341]\n",
      "loss: 0.118572  [25600/175341]\n",
      "loss: 0.286189  [27200/175341]\n",
      "loss: 0.236575  [28800/175341]\n",
      "loss: 0.441899  [30400/175341]\n",
      "loss: 0.072350  [32000/175341]\n",
      "loss: 0.441108  [33600/175341]\n",
      "loss: 0.070984  [35200/175341]\n",
      "loss: 0.591716  [36800/175341]\n",
      "loss: 0.552825  [38400/175341]\n",
      "loss: 0.530714  [40000/175341]\n",
      "loss: 0.350518  [41600/175341]\n",
      "loss: 0.695878  [43200/175341]\n",
      "loss: 0.604695  [44800/175341]\n",
      "loss: 0.498031  [46400/175341]\n",
      "loss: 0.416643  [48000/175341]\n",
      "loss: 0.745521  [49600/175341]\n",
      "loss: 0.541169  [51200/175341]\n",
      "loss: 0.384074  [52800/175341]\n",
      "loss: 0.105033  [54400/175341]\n",
      "loss: 0.453458  [56000/175341]\n",
      "loss: 0.496314  [57600/175341]\n",
      "loss: 0.380133  [59200/175341]\n",
      "loss: 0.399422  [60800/175341]\n",
      "loss: 0.255670  [62400/175341]\n",
      "loss: 0.398055  [64000/175341]\n",
      "loss: 0.331023  [65600/175341]\n",
      "loss: 0.399501  [67200/175341]\n",
      "loss: 0.589100  [68800/175341]\n",
      "loss: 0.766173  [70400/175341]\n",
      "loss: 0.511502  [72000/175341]\n",
      "loss: 0.272005  [73600/175341]\n",
      "loss: 0.327278  [75200/175341]\n",
      "loss: 0.356925  [76800/175341]\n",
      "loss: 0.641335  [78400/175341]\n",
      "loss: 0.483446  [80000/175341]\n",
      "loss: 0.210139  [81600/175341]\n",
      "loss: 0.301149  [83200/175341]\n",
      "loss: 0.361195  [84800/175341]\n",
      "loss: 0.448003  [86400/175341]\n",
      "loss: 0.597444  [88000/175341]\n",
      "loss: 0.330536  [89600/175341]\n",
      "loss: 0.488687  [91200/175341]\n",
      "loss: 0.377515  [92800/175341]\n",
      "loss: 0.365702  [94400/175341]\n",
      "loss: 0.310051  [96000/175341]\n",
      "loss: 0.232490  [97600/175341]\n",
      "loss: 0.597303  [99200/175341]\n",
      "loss: 0.355328  [100800/175341]\n",
      "loss: 0.939917  [102400/175341]\n",
      "loss: 0.126459  [104000/175341]\n",
      "loss: 0.290938  [105600/175341]\n",
      "loss: 0.350286  [107200/175341]\n",
      "loss: 0.229783  [108800/175341]\n",
      "loss: 0.429395  [110400/175341]\n",
      "loss: 0.208981  [112000/175341]\n",
      "loss: 0.336266  [113600/175341]\n",
      "loss: 0.358840  [115200/175341]\n",
      "loss: 0.477071  [116800/175341]\n",
      "loss: 0.546357  [118400/175341]\n",
      "loss: 0.549818  [120000/175341]\n",
      "loss: 0.391301  [121600/175341]\n",
      "loss: 0.232429  [123200/175341]\n",
      "loss: 0.274550  [124800/175341]\n",
      "loss: 0.430341  [126400/175341]\n",
      "loss: 0.462858  [128000/175341]\n",
      "loss: 0.409597  [129600/175341]\n",
      "loss: 0.566795  [131200/175341]\n",
      "loss: 0.723665  [132800/175341]\n",
      "loss: 0.506156  [134400/175341]\n",
      "loss: 0.400786  [136000/175341]\n",
      "loss: 0.478376  [137600/175341]\n",
      "loss: 0.619965  [139200/175341]\n",
      "loss: 0.688112  [140800/175341]\n",
      "loss: 0.541440  [142400/175341]\n",
      "loss: 0.313235  [144000/175341]\n",
      "loss: 0.444721  [145600/175341]\n",
      "loss: 0.741738  [147200/175341]\n",
      "loss: 0.815972  [148800/175341]\n",
      "loss: 0.319818  [150400/175341]\n",
      "loss: 0.340910  [152000/175341]\n",
      "loss: 0.348902  [153600/175341]\n",
      "loss: 0.466014  [155200/175341]\n",
      "loss: 0.833182  [156800/175341]\n",
      "loss: 0.901909  [158400/175341]\n",
      "loss: 0.466270  [160000/175341]\n",
      "loss: 0.571792  [161600/175341]\n",
      "loss: 0.689243  [163200/175341]\n",
      "loss: 0.512601  [164800/175341]\n",
      "loss: 0.410211  [166400/175341]\n",
      "loss: 0.409404  [168000/175341]\n",
      "loss: 0.453497  [169600/175341]\n",
      "loss: 0.359802  [171200/175341]\n",
      "loss: 0.431636  [172800/175341]\n",
      "loss: 0.381516  [174400/175341]\n",
      "Train Accuracy: 81.4886%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.537549, F1-score: 75.75%, Macro_F1-Score:  42.76%  \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.148425  [    0/175341]\n",
      "loss: 0.241100  [ 1600/175341]\n",
      "loss: 0.251441  [ 3200/175341]\n",
      "loss: 0.423011  [ 4800/175341]\n",
      "loss: 0.173580  [ 6400/175341]\n",
      "loss: 0.588855  [ 8000/175341]\n",
      "loss: 0.398041  [ 9600/175341]\n",
      "loss: 0.541204  [11200/175341]\n",
      "loss: 0.482495  [12800/175341]\n",
      "loss: 0.570502  [14400/175341]\n",
      "loss: 0.328773  [16000/175341]\n",
      "loss: 0.306044  [17600/175341]\n",
      "loss: 0.285358  [19200/175341]\n",
      "loss: 0.218677  [20800/175341]\n",
      "loss: 0.791352  [22400/175341]\n",
      "loss: 0.447162  [24000/175341]\n",
      "loss: 0.654650  [25600/175341]\n",
      "loss: 0.475163  [27200/175341]\n",
      "loss: 0.415302  [28800/175341]\n",
      "loss: 0.403776  [30400/175341]\n",
      "loss: 0.324358  [32000/175341]\n",
      "loss: 0.640214  [33600/175341]\n",
      "loss: 0.416864  [35200/175341]\n",
      "loss: 0.419825  [36800/175341]\n",
      "loss: 0.365974  [38400/175341]\n",
      "loss: 0.303512  [40000/175341]\n",
      "loss: 0.668007  [41600/175341]\n",
      "loss: 0.666667  [43200/175341]\n",
      "loss: 0.369538  [44800/175341]\n",
      "loss: 0.326925  [46400/175341]\n",
      "loss: 0.319609  [48000/175341]\n",
      "loss: 0.686332  [49600/175341]\n",
      "loss: 0.131543  [51200/175341]\n",
      "loss: 0.660991  [52800/175341]\n",
      "loss: 0.268990  [54400/175341]\n",
      "loss: 0.626820  [56000/175341]\n",
      "loss: 0.185420  [57600/175341]\n",
      "loss: 0.609959  [59200/175341]\n",
      "loss: 0.331311  [60800/175341]\n",
      "loss: 0.389695  [62400/175341]\n",
      "loss: 0.518549  [64000/175341]\n",
      "loss: 0.790165  [65600/175341]\n",
      "loss: 0.287398  [67200/175341]\n",
      "loss: 0.697801  [68800/175341]\n",
      "loss: 0.455074  [70400/175341]\n",
      "loss: 0.614871  [72000/175341]\n",
      "loss: 0.973694  [73600/175341]\n",
      "loss: 0.905502  [75200/175341]\n",
      "loss: 0.506105  [76800/175341]\n",
      "loss: 0.434390  [78400/175341]\n",
      "loss: 0.275986  [80000/175341]\n",
      "loss: 0.469421  [81600/175341]\n",
      "loss: 0.150705  [83200/175341]\n",
      "loss: 0.554276  [84800/175341]\n",
      "loss: 0.824116  [86400/175341]\n",
      "loss: 0.556319  [88000/175341]\n",
      "loss: 0.577623  [89600/175341]\n",
      "loss: 0.284323  [91200/175341]\n",
      "loss: 0.331350  [92800/175341]\n",
      "loss: 0.257063  [94400/175341]\n",
      "loss: 0.202462  [96000/175341]\n",
      "loss: 0.632306  [97600/175341]\n",
      "loss: 0.303598  [99200/175341]\n",
      "loss: 0.170402  [100800/175341]\n",
      "loss: 0.237501  [102400/175341]\n",
      "loss: 0.484957  [104000/175341]\n",
      "loss: 0.279776  [105600/175341]\n",
      "loss: 0.354668  [107200/175341]\n",
      "loss: 0.415623  [108800/175341]\n",
      "loss: 0.691686  [110400/175341]\n",
      "loss: 0.524428  [112000/175341]\n",
      "loss: 0.919099  [113600/175341]\n",
      "loss: 0.817977  [115200/175341]\n",
      "loss: 0.160524  [116800/175341]\n",
      "loss: 0.334307  [118400/175341]\n",
      "loss: 0.265528  [120000/175341]\n",
      "loss: 0.315859  [121600/175341]\n",
      "loss: 0.315619  [123200/175341]\n",
      "loss: 0.475260  [124800/175341]\n",
      "loss: 0.337768  [126400/175341]\n",
      "loss: 0.316853  [128000/175341]\n",
      "loss: 0.572228  [129600/175341]\n",
      "loss: 0.275813  [131200/175341]\n",
      "loss: 0.210390  [132800/175341]\n",
      "loss: 0.284613  [134400/175341]\n",
      "loss: 0.272418  [136000/175341]\n",
      "loss: 0.558821  [137600/175341]\n",
      "loss: 0.314237  [139200/175341]\n",
      "loss: 0.321480  [140800/175341]\n",
      "loss: 0.262867  [142400/175341]\n",
      "loss: 0.512754  [144000/175341]\n",
      "loss: 0.537178  [145600/175341]\n",
      "loss: 0.376423  [147200/175341]\n",
      "loss: 0.532463  [148800/175341]\n",
      "loss: 0.314320  [150400/175341]\n",
      "loss: 0.266490  [152000/175341]\n",
      "loss: 0.198638  [153600/175341]\n",
      "loss: 0.305066  [155200/175341]\n",
      "loss: 0.631272  [156800/175341]\n",
      "loss: 0.730979  [158400/175341]\n",
      "loss: 0.234606  [160000/175341]\n",
      "loss: 0.498663  [161600/175341]\n",
      "loss: 0.422877  [163200/175341]\n",
      "loss: 0.549866  [164800/175341]\n",
      "loss: 0.526933  [166400/175341]\n",
      "loss: 0.326371  [168000/175341]\n",
      "loss: 1.250077  [169600/175341]\n",
      "loss: 0.872869  [171200/175341]\n",
      "loss: 0.438413  [172800/175341]\n",
      "loss: 0.985889  [174400/175341]\n",
      "Train Accuracy: 81.4704%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.559585, F1-score: 74.93%, Macro_F1-Score:  41.92%  \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.522625  [    0/175341]\n",
      "loss: 0.366270  [ 1600/175341]\n",
      "loss: 0.598080  [ 3200/175341]\n",
      "loss: 0.276038  [ 4800/175341]\n",
      "loss: 0.547325  [ 6400/175341]\n",
      "loss: 0.545743  [ 8000/175341]\n",
      "loss: 0.640483  [ 9600/175341]\n",
      "loss: 0.439890  [11200/175341]\n",
      "loss: 0.335845  [12800/175341]\n",
      "loss: 0.376844  [14400/175341]\n",
      "loss: 0.394705  [16000/175341]\n",
      "loss: 0.399973  [17600/175341]\n",
      "loss: 0.191868  [19200/175341]\n",
      "loss: 0.321538  [20800/175341]\n",
      "loss: 0.095211  [22400/175341]\n",
      "loss: 0.282922  [24000/175341]\n",
      "loss: 0.473836  [25600/175341]\n",
      "loss: 0.881944  [27200/175341]\n",
      "loss: 0.191476  [28800/175341]\n",
      "loss: 0.507226  [30400/175341]\n",
      "loss: 0.161279  [32000/175341]\n",
      "loss: 0.352964  [33600/175341]\n",
      "loss: 0.470472  [35200/175341]\n",
      "loss: 0.249226  [36800/175341]\n",
      "loss: 0.627682  [38400/175341]\n",
      "loss: 0.417316  [40000/175341]\n",
      "loss: 0.412978  [41600/175341]\n",
      "loss: 0.415598  [43200/175341]\n",
      "loss: 0.563909  [44800/175341]\n",
      "loss: 0.326616  [46400/175341]\n",
      "loss: 0.375881  [48000/175341]\n",
      "loss: 0.182065  [49600/175341]\n",
      "loss: 0.585854  [51200/175341]\n",
      "loss: 0.473139  [52800/175341]\n",
      "loss: 0.351979  [54400/175341]\n",
      "loss: 0.516931  [56000/175341]\n",
      "loss: 0.191782  [57600/175341]\n",
      "loss: 0.531908  [59200/175341]\n",
      "loss: 0.858802  [60800/175341]\n",
      "loss: 0.243800  [62400/175341]\n",
      "loss: 0.742277  [64000/175341]\n",
      "loss: 0.415738  [65600/175341]\n",
      "loss: 0.157088  [67200/175341]\n",
      "loss: 1.010107  [68800/175341]\n",
      "loss: 0.372741  [70400/175341]\n",
      "loss: 0.264132  [72000/175341]\n",
      "loss: 1.232726  [73600/175341]\n",
      "loss: 0.262295  [75200/175341]\n",
      "loss: 0.323242  [76800/175341]\n",
      "loss: 0.516460  [78400/175341]\n",
      "loss: 0.396934  [80000/175341]\n",
      "loss: 0.942484  [81600/175341]\n",
      "loss: 0.544201  [83200/175341]\n",
      "loss: 0.786337  [84800/175341]\n",
      "loss: 0.721854  [86400/175341]\n",
      "loss: 0.317775  [88000/175341]\n",
      "loss: 0.661538  [89600/175341]\n",
      "loss: 0.531081  [91200/175341]\n",
      "loss: 0.324078  [92800/175341]\n",
      "loss: 0.398239  [94400/175341]\n",
      "loss: 0.573163  [96000/175341]\n",
      "loss: 0.112080  [97600/175341]\n",
      "loss: 0.269057  [99200/175341]\n",
      "loss: 0.663944  [100800/175341]\n",
      "loss: 0.294727  [102400/175341]\n",
      "loss: 0.486347  [104000/175341]\n",
      "loss: 0.679299  [105600/175341]\n",
      "loss: 0.359756  [107200/175341]\n",
      "loss: 0.477168  [108800/175341]\n",
      "loss: 0.161957  [110400/175341]\n",
      "loss: 0.583413  [112000/175341]\n",
      "loss: 0.269483  [113600/175341]\n",
      "loss: 0.199824  [115200/175341]\n",
      "loss: 0.498637  [116800/175341]\n",
      "loss: 0.747221  [118400/175341]\n",
      "loss: 0.594269  [120000/175341]\n",
      "loss: 0.394995  [121600/175341]\n",
      "loss: 0.757388  [123200/175341]\n",
      "loss: 0.305918  [124800/175341]\n",
      "loss: 0.190964  [126400/175341]\n",
      "loss: 0.656625  [128000/175341]\n",
      "loss: 0.522133  [129600/175341]\n",
      "loss: 0.219690  [131200/175341]\n",
      "loss: 0.279157  [132800/175341]\n",
      "loss: 0.865121  [134400/175341]\n",
      "loss: 0.477589  [136000/175341]\n",
      "loss: 0.359702  [137600/175341]\n",
      "loss: 0.348629  [139200/175341]\n",
      "loss: 0.692493  [140800/175341]\n",
      "loss: 0.775509  [142400/175341]\n",
      "loss: 0.239953  [144000/175341]\n",
      "loss: 0.684033  [145600/175341]\n",
      "loss: 0.359258  [147200/175341]\n",
      "loss: 0.450889  [148800/175341]\n",
      "loss: 0.595751  [150400/175341]\n",
      "loss: 0.233685  [152000/175341]\n",
      "loss: 0.446101  [153600/175341]\n",
      "loss: 0.499733  [155200/175341]\n",
      "loss: 0.353827  [156800/175341]\n",
      "loss: 0.441903  [158400/175341]\n",
      "loss: 0.195059  [160000/175341]\n",
      "loss: 0.770921  [161600/175341]\n",
      "loss: 0.474985  [163200/175341]\n",
      "loss: 0.333181  [164800/175341]\n",
      "loss: 0.617810  [166400/175341]\n",
      "loss: 0.482608  [168000/175341]\n",
      "loss: 0.272315  [169600/175341]\n",
      "loss: 0.874745  [171200/175341]\n",
      "loss: 0.635336  [172800/175341]\n",
      "loss: 0.564548  [174400/175341]\n",
      "Train Accuracy: 81.4875%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.528615, F1-score: 76.59%, Macro_F1-Score:  42.17%  \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.087875  [    0/175341]\n",
      "loss: 0.476349  [ 1600/175341]\n",
      "loss: 0.705956  [ 3200/175341]\n",
      "loss: 0.328360  [ 4800/175341]\n",
      "loss: 0.896192  [ 6400/175341]\n",
      "loss: 0.226187  [ 8000/175341]\n",
      "loss: 0.515510  [ 9600/175341]\n",
      "loss: 0.762686  [11200/175341]\n",
      "loss: 0.581655  [12800/175341]\n",
      "loss: 0.361976  [14400/175341]\n",
      "loss: 0.796018  [16000/175341]\n",
      "loss: 0.327755  [17600/175341]\n",
      "loss: 0.222412  [19200/175341]\n",
      "loss: 0.509620  [20800/175341]\n",
      "loss: 0.588067  [22400/175341]\n",
      "loss: 0.208623  [24000/175341]\n",
      "loss: 0.249969  [25600/175341]\n",
      "loss: 0.635763  [27200/175341]\n",
      "loss: 0.379691  [28800/175341]\n",
      "loss: 0.811443  [30400/175341]\n",
      "loss: 0.319281  [32000/175341]\n",
      "loss: 0.474955  [33600/175341]\n",
      "loss: 0.661947  [35200/175341]\n",
      "loss: 0.341639  [36800/175341]\n",
      "loss: 0.468089  [38400/175341]\n",
      "loss: 0.578862  [40000/175341]\n",
      "loss: 1.162558  [41600/175341]\n",
      "loss: 0.221521  [43200/175341]\n",
      "loss: 0.510524  [44800/175341]\n",
      "loss: 0.412655  [46400/175341]\n",
      "loss: 0.373540  [48000/175341]\n",
      "loss: 0.532744  [49600/175341]\n",
      "loss: 0.198504  [51200/175341]\n",
      "loss: 0.806480  [52800/175341]\n",
      "loss: 0.642470  [54400/175341]\n",
      "loss: 0.623033  [56000/175341]\n",
      "loss: 0.410473  [57600/175341]\n",
      "loss: 0.336761  [59200/175341]\n",
      "loss: 0.405232  [60800/175341]\n",
      "loss: 0.728006  [62400/175341]\n",
      "loss: 0.338337  [64000/175341]\n",
      "loss: 0.738761  [65600/175341]\n",
      "loss: 0.480827  [67200/175341]\n",
      "loss: 0.769109  [68800/175341]\n",
      "loss: 0.299590  [70400/175341]\n",
      "loss: 0.300068  [72000/175341]\n",
      "loss: 0.369780  [73600/175341]\n",
      "loss: 0.400351  [75200/175341]\n",
      "loss: 0.768792  [76800/175341]\n",
      "loss: 0.492836  [78400/175341]\n",
      "loss: 0.141228  [80000/175341]\n",
      "loss: 0.266464  [81600/175341]\n",
      "loss: 0.495300  [83200/175341]\n",
      "loss: 0.331291  [84800/175341]\n",
      "loss: 0.616511  [86400/175341]\n",
      "loss: 0.460092  [88000/175341]\n",
      "loss: 0.214485  [89600/175341]\n",
      "loss: 0.430408  [91200/175341]\n",
      "loss: 0.258015  [92800/175341]\n",
      "loss: 0.515311  [94400/175341]\n",
      "loss: 0.442721  [96000/175341]\n",
      "loss: 0.586298  [97600/175341]\n",
      "loss: 0.258243  [99200/175341]\n",
      "loss: 0.320954  [100800/175341]\n",
      "loss: 0.417799  [102400/175341]\n",
      "loss: 0.132946  [104000/175341]\n",
      "loss: 0.369771  [105600/175341]\n",
      "loss: 0.096525  [107200/175341]\n",
      "loss: 0.381605  [108800/175341]\n",
      "loss: 0.383839  [110400/175341]\n",
      "loss: 0.629863  [112000/175341]\n",
      "loss: 0.298290  [113600/175341]\n",
      "loss: 0.372132  [115200/175341]\n",
      "loss: 0.847360  [116800/175341]\n",
      "loss: 0.206828  [118400/175341]\n",
      "loss: 0.583884  [120000/175341]\n",
      "loss: 0.289470  [121600/175341]\n",
      "loss: 0.274227  [123200/175341]\n",
      "loss: 0.389217  [124800/175341]\n",
      "loss: 0.418993  [126400/175341]\n",
      "loss: 0.160483  [128000/175341]\n",
      "loss: 0.726515  [129600/175341]\n",
      "loss: 0.189792  [131200/175341]\n",
      "loss: 0.175827  [132800/175341]\n",
      "loss: 0.490273  [134400/175341]\n",
      "loss: 0.741552  [136000/175341]\n",
      "loss: 0.882170  [137600/175341]\n",
      "loss: 0.206964  [139200/175341]\n",
      "loss: 0.290870  [140800/175341]\n",
      "loss: 0.477483  [142400/175341]\n",
      "loss: 0.688501  [144000/175341]\n",
      "loss: 0.494023  [145600/175341]\n",
      "loss: 0.445795  [147200/175341]\n",
      "loss: 0.680026  [148800/175341]\n",
      "loss: 0.462488  [150400/175341]\n",
      "loss: 0.745350  [152000/175341]\n",
      "loss: 0.499479  [153600/175341]\n",
      "loss: 0.569620  [155200/175341]\n",
      "loss: 0.925647  [156800/175341]\n",
      "loss: 0.423757  [158400/175341]\n",
      "loss: 0.639265  [160000/175341]\n",
      "loss: 0.143804  [161600/175341]\n",
      "loss: 0.684073  [163200/175341]\n",
      "loss: 0.504868  [164800/175341]\n",
      "loss: 0.865968  [166400/175341]\n",
      "loss: 0.237745  [168000/175341]\n",
      "loss: 0.434355  [169600/175341]\n",
      "loss: 0.406545  [171200/175341]\n",
      "loss: 0.707970  [172800/175341]\n",
      "loss: 0.176163  [174400/175341]\n",
      "Train Accuracy: 81.5554%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.520726, F1-score: 76.94%, Macro_F1-Score:  42.89%  \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.252009  [    0/175341]\n",
      "loss: 0.418094  [ 1600/175341]\n",
      "loss: 0.352268  [ 3200/175341]\n",
      "loss: 0.531090  [ 4800/175341]\n",
      "loss: 0.449802  [ 6400/175341]\n",
      "loss: 0.271474  [ 8000/175341]\n",
      "loss: 0.480526  [ 9600/175341]\n",
      "loss: 0.230641  [11200/175341]\n",
      "loss: 0.256540  [12800/175341]\n",
      "loss: 0.196626  [14400/175341]\n",
      "loss: 0.416061  [16000/175341]\n",
      "loss: 0.601614  [17600/175341]\n",
      "loss: 0.325165  [19200/175341]\n",
      "loss: 0.783052  [20800/175341]\n",
      "loss: 0.554590  [22400/175341]\n",
      "loss: 0.232772  [24000/175341]\n",
      "loss: 0.442114  [25600/175341]\n",
      "loss: 0.313695  [27200/175341]\n",
      "loss: 0.550271  [28800/175341]\n",
      "loss: 0.328556  [30400/175341]\n",
      "loss: 0.410183  [32000/175341]\n",
      "loss: 0.435397  [33600/175341]\n",
      "loss: 0.147145  [35200/175341]\n",
      "loss: 0.155381  [36800/175341]\n",
      "loss: 0.300958  [38400/175341]\n",
      "loss: 0.362811  [40000/175341]\n",
      "loss: 0.535122  [41600/175341]\n",
      "loss: 0.693687  [43200/175341]\n",
      "loss: 0.888740  [44800/175341]\n",
      "loss: 0.413116  [46400/175341]\n",
      "loss: 0.409971  [48000/175341]\n",
      "loss: 0.588241  [49600/175341]\n",
      "loss: 0.342609  [51200/175341]\n",
      "loss: 1.185620  [52800/175341]\n",
      "loss: 0.266780  [54400/175341]\n",
      "loss: 0.548526  [56000/175341]\n",
      "loss: 0.375283  [57600/175341]\n",
      "loss: 0.334332  [59200/175341]\n",
      "loss: 0.785332  [60800/175341]\n",
      "loss: 0.511374  [62400/175341]\n",
      "loss: 0.799798  [64000/175341]\n",
      "loss: 0.710719  [65600/175341]\n",
      "loss: 0.521996  [67200/175341]\n",
      "loss: 0.335961  [68800/175341]\n",
      "loss: 0.103642  [70400/175341]\n",
      "loss: 0.158687  [72000/175341]\n",
      "loss: 0.266147  [73600/175341]\n",
      "loss: 0.336744  [75200/175341]\n",
      "loss: 0.649648  [76800/175341]\n",
      "loss: 0.789546  [78400/175341]\n",
      "loss: 0.628158  [80000/175341]\n",
      "loss: 0.279395  [81600/175341]\n",
      "loss: 0.648105  [83200/175341]\n",
      "loss: 0.432906  [84800/175341]\n",
      "loss: 0.201863  [86400/175341]\n",
      "loss: 0.748526  [88000/175341]\n",
      "loss: 0.242440  [89600/175341]\n",
      "loss: 0.389701  [91200/175341]\n",
      "loss: 0.559836  [92800/175341]\n",
      "loss: 0.411557  [94400/175341]\n",
      "loss: 0.318449  [96000/175341]\n",
      "loss: 0.215520  [97600/175341]\n",
      "loss: 0.267443  [99200/175341]\n",
      "loss: 0.439470  [100800/175341]\n",
      "loss: 0.302053  [102400/175341]\n",
      "loss: 0.421744  [104000/175341]\n",
      "loss: 0.467458  [105600/175341]\n",
      "loss: 0.375993  [107200/175341]\n",
      "loss: 0.403980  [108800/175341]\n",
      "loss: 0.573240  [110400/175341]\n",
      "loss: 0.396955  [112000/175341]\n",
      "loss: 0.549991  [113600/175341]\n",
      "loss: 0.496200  [115200/175341]\n",
      "loss: 0.075273  [116800/175341]\n",
      "loss: 0.224013  [118400/175341]\n",
      "loss: 0.545523  [120000/175341]\n",
      "loss: 0.412315  [121600/175341]\n",
      "loss: 0.411069  [123200/175341]\n",
      "loss: 0.376735  [124800/175341]\n",
      "loss: 0.293430  [126400/175341]\n",
      "loss: 0.706234  [128000/175341]\n",
      "loss: 0.522679  [129600/175341]\n",
      "loss: 0.273142  [131200/175341]\n",
      "loss: 0.283549  [132800/175341]\n",
      "loss: 0.234166  [134400/175341]\n",
      "loss: 0.366968  [136000/175341]\n",
      "loss: 0.381798  [137600/175341]\n",
      "loss: 0.309458  [139200/175341]\n",
      "loss: 0.517797  [140800/175341]\n",
      "loss: 0.381488  [142400/175341]\n",
      "loss: 0.551533  [144000/175341]\n",
      "loss: 0.364626  [145600/175341]\n",
      "loss: 0.383163  [147200/175341]\n",
      "loss: 0.255315  [148800/175341]\n",
      "loss: 0.229572  [150400/175341]\n",
      "loss: 0.595532  [152000/175341]\n",
      "loss: 0.667139  [153600/175341]\n",
      "loss: 0.498745  [155200/175341]\n",
      "loss: 0.335016  [156800/175341]\n",
      "loss: 0.242367  [158400/175341]\n",
      "loss: 0.328528  [160000/175341]\n",
      "loss: 0.576344  [161600/175341]\n",
      "loss: 0.184740  [163200/175341]\n",
      "loss: 0.634301  [164800/175341]\n",
      "loss: 0.288634  [166400/175341]\n",
      "loss: 0.554094  [168000/175341]\n",
      "loss: 0.584805  [169600/175341]\n",
      "loss: 0.516266  [171200/175341]\n",
      "loss: 0.701100  [172800/175341]\n",
      "loss: 0.375250  [174400/175341]\n",
      "Train Accuracy: 81.4801%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.525333, F1-score: 76.65%, Macro_F1-Score:  42.66%  \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.332561  [    0/175341]\n",
      "loss: 0.233845  [ 1600/175341]\n",
      "loss: 0.427255  [ 3200/175341]\n",
      "loss: 0.909117  [ 4800/175341]\n",
      "loss: 0.247680  [ 6400/175341]\n",
      "loss: 0.948340  [ 8000/175341]\n",
      "loss: 0.543543  [ 9600/175341]\n",
      "loss: 0.719319  [11200/175341]\n",
      "loss: 0.525599  [12800/175341]\n",
      "loss: 0.497104  [14400/175341]\n",
      "loss: 0.366111  [16000/175341]\n",
      "loss: 0.665534  [17600/175341]\n",
      "loss: 0.476529  [19200/175341]\n",
      "loss: 0.337550  [20800/175341]\n",
      "loss: 0.541394  [22400/175341]\n",
      "loss: 0.211623  [24000/175341]\n",
      "loss: 0.354298  [25600/175341]\n",
      "loss: 0.827135  [27200/175341]\n",
      "loss: 0.636534  [28800/175341]\n",
      "loss: 0.521486  [30400/175341]\n",
      "loss: 0.882478  [32000/175341]\n",
      "loss: 0.604565  [33600/175341]\n",
      "loss: 0.546555  [35200/175341]\n",
      "loss: 0.504093  [36800/175341]\n",
      "loss: 0.537974  [38400/175341]\n",
      "loss: 0.408952  [40000/175341]\n",
      "loss: 0.272080  [41600/175341]\n",
      "loss: 0.679347  [43200/175341]\n",
      "loss: 0.349957  [44800/175341]\n",
      "loss: 0.954236  [46400/175341]\n",
      "loss: 0.286292  [48000/175341]\n",
      "loss: 0.618053  [49600/175341]\n",
      "loss: 0.529650  [51200/175341]\n",
      "loss: 0.436343  [52800/175341]\n",
      "loss: 0.368508  [54400/175341]\n",
      "loss: 0.567508  [56000/175341]\n",
      "loss: 0.636160  [57600/175341]\n",
      "loss: 0.503904  [59200/175341]\n",
      "loss: 0.599614  [60800/175341]\n",
      "loss: 0.425070  [62400/175341]\n",
      "loss: 0.315552  [64000/175341]\n",
      "loss: 0.224039  [65600/175341]\n",
      "loss: 0.242105  [67200/175341]\n",
      "loss: 0.441323  [68800/175341]\n",
      "loss: 0.548322  [70400/175341]\n",
      "loss: 0.602046  [72000/175341]\n",
      "loss: 0.503446  [73600/175341]\n",
      "loss: 0.452981  [75200/175341]\n",
      "loss: 0.813466  [76800/175341]\n",
      "loss: 0.362866  [78400/175341]\n",
      "loss: 0.284375  [80000/175341]\n",
      "loss: 0.421133  [81600/175341]\n",
      "loss: 0.193688  [83200/175341]\n",
      "loss: 0.714922  [84800/175341]\n",
      "loss: 0.385485  [86400/175341]\n",
      "loss: 0.407680  [88000/175341]\n",
      "loss: 0.328248  [89600/175341]\n",
      "loss: 0.540262  [91200/175341]\n",
      "loss: 0.497222  [92800/175341]\n",
      "loss: 0.287004  [94400/175341]\n",
      "loss: 0.597615  [96000/175341]\n",
      "loss: 0.257935  [97600/175341]\n",
      "loss: 0.657084  [99200/175341]\n",
      "loss: 0.565137  [100800/175341]\n",
      "loss: 0.722877  [102400/175341]\n",
      "loss: 0.554571  [104000/175341]\n",
      "loss: 0.509788  [105600/175341]\n",
      "loss: 0.726302  [107200/175341]\n",
      "loss: 0.310226  [108800/175341]\n",
      "loss: 0.220776  [110400/175341]\n",
      "loss: 0.785711  [112000/175341]\n",
      "loss: 0.435421  [113600/175341]\n",
      "loss: 0.570565  [115200/175341]\n",
      "loss: 0.288893  [116800/175341]\n",
      "loss: 0.426590  [118400/175341]\n",
      "loss: 0.450631  [120000/175341]\n",
      "loss: 0.418912  [121600/175341]\n",
      "loss: 0.493826  [123200/175341]\n",
      "loss: 0.450789  [124800/175341]\n",
      "loss: 0.489973  [126400/175341]\n",
      "loss: 0.923237  [128000/175341]\n",
      "loss: 0.371088  [129600/175341]\n",
      "loss: 0.430491  [131200/175341]\n",
      "loss: 0.562370  [132800/175341]\n",
      "loss: 0.278103  [134400/175341]\n",
      "loss: 0.337778  [136000/175341]\n",
      "loss: 0.391624  [137600/175341]\n",
      "loss: 0.660251  [139200/175341]\n",
      "loss: 0.657671  [140800/175341]\n",
      "loss: 0.800627  [142400/175341]\n",
      "loss: 0.301286  [144000/175341]\n",
      "loss: 0.554742  [145600/175341]\n",
      "loss: 0.098499  [147200/175341]\n",
      "loss: 0.522154  [148800/175341]\n",
      "loss: 0.653344  [150400/175341]\n",
      "loss: 0.282576  [152000/175341]\n",
      "loss: 0.427335  [153600/175341]\n",
      "loss: 0.330358  [155200/175341]\n",
      "loss: 0.272013  [156800/175341]\n",
      "loss: 0.258166  [158400/175341]\n",
      "loss: 0.427548  [160000/175341]\n",
      "loss: 0.241744  [161600/175341]\n",
      "loss: 0.172501  [163200/175341]\n",
      "loss: 0.355289  [164800/175341]\n",
      "loss: 0.097863  [166400/175341]\n",
      "loss: 0.347134  [168000/175341]\n",
      "loss: 0.170227  [169600/175341]\n",
      "loss: 0.525026  [171200/175341]\n",
      "loss: 0.527704  [172800/175341]\n",
      "loss: 0.943675  [174400/175341]\n",
      "Train Accuracy: 81.5497%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.533293, F1-score: 76.47%, Macro_F1-Score:  42.07%  \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.675783  [    0/175341]\n",
      "loss: 0.529229  [ 1600/175341]\n",
      "loss: 0.331962  [ 3200/175341]\n",
      "loss: 0.501840  [ 4800/175341]\n",
      "loss: 0.353235  [ 6400/175341]\n",
      "loss: 1.103908  [ 8000/175341]\n",
      "loss: 0.404305  [ 9600/175341]\n",
      "loss: 0.433816  [11200/175341]\n",
      "loss: 0.388134  [12800/175341]\n",
      "loss: 0.283698  [14400/175341]\n",
      "loss: 0.388886  [16000/175341]\n",
      "loss: 0.525492  [17600/175341]\n",
      "loss: 0.772897  [19200/175341]\n",
      "loss: 0.680176  [20800/175341]\n",
      "loss: 0.545852  [22400/175341]\n",
      "loss: 0.351951  [24000/175341]\n",
      "loss: 0.494316  [25600/175341]\n",
      "loss: 0.234728  [27200/175341]\n",
      "loss: 0.311819  [28800/175341]\n",
      "loss: 0.719714  [30400/175341]\n",
      "loss: 0.686991  [32000/175341]\n",
      "loss: 0.350645  [33600/175341]\n",
      "loss: 0.424128  [35200/175341]\n",
      "loss: 0.419507  [36800/175341]\n",
      "loss: 0.623960  [38400/175341]\n",
      "loss: 0.468068  [40000/175341]\n",
      "loss: 0.453622  [41600/175341]\n",
      "loss: 0.445538  [43200/175341]\n",
      "loss: 0.518219  [44800/175341]\n",
      "loss: 0.241673  [46400/175341]\n",
      "loss: 0.479821  [48000/175341]\n",
      "loss: 1.333156  [49600/175341]\n",
      "loss: 0.463103  [51200/175341]\n",
      "loss: 0.215099  [52800/175341]\n",
      "loss: 0.458947  [54400/175341]\n",
      "loss: 0.455353  [56000/175341]\n",
      "loss: 0.108589  [57600/175341]\n",
      "loss: 0.743045  [59200/175341]\n",
      "loss: 0.274000  [60800/175341]\n",
      "loss: 0.566258  [62400/175341]\n",
      "loss: 0.613520  [64000/175341]\n",
      "loss: 0.242925  [65600/175341]\n",
      "loss: 0.311261  [67200/175341]\n",
      "loss: 0.649949  [68800/175341]\n",
      "loss: 0.442574  [70400/175341]\n",
      "loss: 0.263046  [72000/175341]\n",
      "loss: 0.508996  [73600/175341]\n",
      "loss: 0.237729  [75200/175341]\n",
      "loss: 1.197647  [76800/175341]\n",
      "loss: 0.373685  [78400/175341]\n",
      "loss: 0.612559  [80000/175341]\n",
      "loss: 0.465276  [81600/175341]\n",
      "loss: 0.250583  [83200/175341]\n",
      "loss: 0.575977  [84800/175341]\n",
      "loss: 0.278888  [86400/175341]\n",
      "loss: 0.394480  [88000/175341]\n",
      "loss: 0.717149  [89600/175341]\n",
      "loss: 0.231549  [91200/175341]\n",
      "loss: 0.263465  [92800/175341]\n",
      "loss: 0.451968  [94400/175341]\n",
      "loss: 0.302290  [96000/175341]\n",
      "loss: 0.324771  [97600/175341]\n",
      "loss: 0.535152  [99200/175341]\n",
      "loss: 0.756802  [100800/175341]\n",
      "loss: 0.298048  [102400/175341]\n",
      "loss: 0.568378  [104000/175341]\n",
      "loss: 0.565886  [105600/175341]\n",
      "loss: 0.576847  [107200/175341]\n",
      "loss: 0.393665  [108800/175341]\n",
      "loss: 0.290363  [110400/175341]\n",
      "loss: 0.174090  [112000/175341]\n",
      "loss: 0.373263  [113600/175341]\n",
      "loss: 0.542339  [115200/175341]\n",
      "loss: 0.746839  [116800/175341]\n",
      "loss: 0.456244  [118400/175341]\n",
      "loss: 0.597016  [120000/175341]\n",
      "loss: 0.237067  [121600/175341]\n",
      "loss: 0.274181  [123200/175341]\n",
      "loss: 0.420556  [124800/175341]\n",
      "loss: 0.553029  [126400/175341]\n",
      "loss: 0.442889  [128000/175341]\n",
      "loss: 0.763905  [129600/175341]\n",
      "loss: 0.493850  [131200/175341]\n",
      "loss: 0.520552  [132800/175341]\n",
      "loss: 0.890576  [134400/175341]\n",
      "loss: 0.257673  [136000/175341]\n",
      "loss: 0.297948  [137600/175341]\n",
      "loss: 0.379654  [139200/175341]\n",
      "loss: 0.541938  [140800/175341]\n",
      "loss: 0.339873  [142400/175341]\n",
      "loss: 0.488521  [144000/175341]\n",
      "loss: 0.241293  [145600/175341]\n",
      "loss: 0.298389  [147200/175341]\n",
      "loss: 0.125797  [148800/175341]\n",
      "loss: 0.295378  [150400/175341]\n",
      "loss: 0.427642  [152000/175341]\n",
      "loss: 0.365989  [153600/175341]\n",
      "loss: 1.030102  [155200/175341]\n",
      "loss: 0.842932  [156800/175341]\n",
      "loss: 0.414095  [158400/175341]\n",
      "loss: 0.513556  [160000/175341]\n",
      "loss: 0.166628  [161600/175341]\n",
      "loss: 0.430074  [163200/175341]\n",
      "loss: 0.201284  [164800/175341]\n",
      "loss: 0.702198  [166400/175341]\n",
      "loss: 0.472145  [168000/175341]\n",
      "loss: 0.319264  [169600/175341]\n",
      "loss: 0.252100  [171200/175341]\n",
      "loss: 0.417287  [172800/175341]\n",
      "loss: 0.507483  [174400/175341]\n",
      "Train Accuracy: 81.5713%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.532192, F1-score: 76.29%, Macro_F1-Score:  43.00%  \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.306515  [    0/175341]\n",
      "loss: 0.334157  [ 1600/175341]\n",
      "loss: 0.553441  [ 3200/175341]\n",
      "loss: 0.545569  [ 4800/175341]\n",
      "loss: 0.148054  [ 6400/175341]\n",
      "loss: 0.207315  [ 8000/175341]\n",
      "loss: 0.457255  [ 9600/175341]\n",
      "loss: 0.346392  [11200/175341]\n",
      "loss: 0.612656  [12800/175341]\n",
      "loss: 0.531865  [14400/175341]\n",
      "loss: 0.438087  [16000/175341]\n",
      "loss: 0.204774  [17600/175341]\n",
      "loss: 0.320239  [19200/175341]\n",
      "loss: 0.372755  [20800/175341]\n",
      "loss: 0.367741  [22400/175341]\n",
      "loss: 0.328743  [24000/175341]\n",
      "loss: 0.418781  [25600/175341]\n",
      "loss: 0.571317  [27200/175341]\n",
      "loss: 0.461678  [28800/175341]\n",
      "loss: 0.258152  [30400/175341]\n",
      "loss: 0.563754  [32000/175341]\n",
      "loss: 0.301431  [33600/175341]\n",
      "loss: 0.533304  [35200/175341]\n",
      "loss: 0.536389  [36800/175341]\n",
      "loss: 0.388351  [38400/175341]\n",
      "loss: 0.419915  [40000/175341]\n",
      "loss: 0.625887  [41600/175341]\n",
      "loss: 0.309590  [43200/175341]\n",
      "loss: 0.641907  [44800/175341]\n",
      "loss: 0.480820  [46400/175341]\n",
      "loss: 0.543963  [48000/175341]\n",
      "loss: 0.189488  [49600/175341]\n",
      "loss: 0.578664  [51200/175341]\n",
      "loss: 0.744983  [52800/175341]\n",
      "loss: 0.864210  [54400/175341]\n",
      "loss: 0.242521  [56000/175341]\n",
      "loss: 0.723057  [57600/175341]\n",
      "loss: 0.176892  [59200/175341]\n",
      "loss: 0.361230  [60800/175341]\n",
      "loss: 0.492419  [62400/175341]\n",
      "loss: 0.176389  [64000/175341]\n",
      "loss: 0.480544  [65600/175341]\n",
      "loss: 0.537364  [67200/175341]\n",
      "loss: 0.399572  [68800/175341]\n",
      "loss: 0.328987  [70400/175341]\n",
      "loss: 0.280676  [72000/175341]\n",
      "loss: 0.546978  [73600/175341]\n",
      "loss: 0.751178  [75200/175341]\n",
      "loss: 0.554582  [76800/175341]\n",
      "loss: 0.314308  [78400/175341]\n",
      "loss: 1.208509  [80000/175341]\n",
      "loss: 0.480810  [81600/175341]\n",
      "loss: 0.250758  [83200/175341]\n",
      "loss: 0.191219  [84800/175341]\n",
      "loss: 0.205841  [86400/175341]\n",
      "loss: 0.487691  [88000/175341]\n",
      "loss: 0.568589  [89600/175341]\n",
      "loss: 0.326512  [91200/175341]\n",
      "loss: 0.772168  [92800/175341]\n",
      "loss: 0.866158  [94400/175341]\n",
      "loss: 0.291715  [96000/175341]\n",
      "loss: 0.741574  [97600/175341]\n",
      "loss: 0.611276  [99200/175341]\n",
      "loss: 0.332032  [100800/175341]\n",
      "loss: 0.591475  [102400/175341]\n",
      "loss: 0.637111  [104000/175341]\n",
      "loss: 0.613663  [105600/175341]\n",
      "loss: 0.570096  [107200/175341]\n",
      "loss: 0.356461  [108800/175341]\n",
      "loss: 0.509890  [110400/175341]\n",
      "loss: 0.468786  [112000/175341]\n",
      "loss: 0.735511  [113600/175341]\n",
      "loss: 0.282326  [115200/175341]\n",
      "loss: 0.809373  [116800/175341]\n",
      "loss: 0.574617  [118400/175341]\n",
      "loss: 0.727078  [120000/175341]\n",
      "loss: 0.381429  [121600/175341]\n",
      "loss: 0.304250  [123200/175341]\n",
      "loss: 0.343681  [124800/175341]\n",
      "loss: 0.129478  [126400/175341]\n",
      "loss: 0.906650  [128000/175341]\n",
      "loss: 0.292988  [129600/175341]\n",
      "loss: 0.157590  [131200/175341]\n",
      "loss: 0.344569  [132800/175341]\n",
      "loss: 0.378578  [134400/175341]\n",
      "loss: 0.434821  [136000/175341]\n",
      "loss: 0.330949  [137600/175341]\n",
      "loss: 0.343227  [139200/175341]\n",
      "loss: 0.209384  [140800/175341]\n",
      "loss: 0.445177  [142400/175341]\n",
      "loss: 0.608287  [144000/175341]\n",
      "loss: 0.605013  [145600/175341]\n",
      "loss: 0.308333  [147200/175341]\n",
      "loss: 0.343125  [148800/175341]\n",
      "loss: 0.148734  [150400/175341]\n",
      "loss: 0.623883  [152000/175341]\n",
      "loss: 0.667194  [153600/175341]\n",
      "loss: 0.477932  [155200/175341]\n",
      "loss: 0.478748  [156800/175341]\n",
      "loss: 0.444385  [158400/175341]\n",
      "loss: 0.417416  [160000/175341]\n",
      "loss: 0.457192  [161600/175341]\n",
      "loss: 0.260192  [163200/175341]\n",
      "loss: 0.197054  [164800/175341]\n",
      "loss: 0.584767  [166400/175341]\n",
      "loss: 0.368785  [168000/175341]\n",
      "loss: 0.110437  [169600/175341]\n",
      "loss: 0.735972  [171200/175341]\n",
      "loss: 0.524566  [172800/175341]\n",
      "loss: 0.445142  [174400/175341]\n",
      "Train Accuracy: 81.6078%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.553263, F1-score: 75.26%, Macro_F1-Score:  42.56%  \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.398115  [    0/175341]\n",
      "loss: 0.763297  [ 1600/175341]\n",
      "loss: 0.740640  [ 3200/175341]\n",
      "loss: 0.481894  [ 4800/175341]\n",
      "loss: 0.470898  [ 6400/175341]\n",
      "loss: 0.710870  [ 8000/175341]\n",
      "loss: 0.418466  [ 9600/175341]\n",
      "loss: 0.639761  [11200/175341]\n",
      "loss: 0.267289  [12800/175341]\n",
      "loss: 0.498638  [14400/175341]\n",
      "loss: 0.291682  [16000/175341]\n",
      "loss: 0.187535  [17600/175341]\n",
      "loss: 0.341944  [19200/175341]\n",
      "loss: 0.470917  [20800/175341]\n",
      "loss: 0.182825  [22400/175341]\n",
      "loss: 0.148144  [24000/175341]\n",
      "loss: 0.468619  [25600/175341]\n",
      "loss: 0.452913  [27200/175341]\n",
      "loss: 0.489256  [28800/175341]\n",
      "loss: 0.190939  [30400/175341]\n",
      "loss: 0.480911  [32000/175341]\n",
      "loss: 0.379502  [33600/175341]\n",
      "loss: 0.377961  [35200/175341]\n",
      "loss: 0.083665  [36800/175341]\n",
      "loss: 0.197214  [38400/175341]\n",
      "loss: 0.276377  [40000/175341]\n",
      "loss: 0.247840  [41600/175341]\n",
      "loss: 0.076832  [43200/175341]\n",
      "loss: 0.970798  [44800/175341]\n",
      "loss: 0.438799  [46400/175341]\n",
      "loss: 0.451832  [48000/175341]\n",
      "loss: 0.775885  [49600/175341]\n",
      "loss: 0.364409  [51200/175341]\n",
      "loss: 0.394160  [52800/175341]\n",
      "loss: 0.333311  [54400/175341]\n",
      "loss: 0.185539  [56000/175341]\n",
      "loss: 0.300639  [57600/175341]\n",
      "loss: 0.527346  [59200/175341]\n",
      "loss: 0.275402  [60800/175341]\n",
      "loss: 0.296079  [62400/175341]\n",
      "loss: 0.553356  [64000/175341]\n",
      "loss: 0.499486  [65600/175341]\n",
      "loss: 0.736287  [67200/175341]\n",
      "loss: 0.138011  [68800/175341]\n",
      "loss: 0.883000  [70400/175341]\n",
      "loss: 0.750696  [72000/175341]\n",
      "loss: 0.489348  [73600/175341]\n",
      "loss: 0.373436  [75200/175341]\n",
      "loss: 0.538428  [76800/175341]\n",
      "loss: 0.130615  [78400/175341]\n",
      "loss: 0.132963  [80000/175341]\n",
      "loss: 0.308437  [81600/175341]\n",
      "loss: 0.161776  [83200/175341]\n",
      "loss: 0.270819  [84800/175341]\n",
      "loss: 0.710411  [86400/175341]\n",
      "loss: 0.988538  [88000/175341]\n",
      "loss: 0.600653  [89600/175341]\n",
      "loss: 0.328077  [91200/175341]\n",
      "loss: 0.147378  [92800/175341]\n",
      "loss: 0.601714  [94400/175341]\n",
      "loss: 0.706891  [96000/175341]\n",
      "loss: 0.595314  [97600/175341]\n",
      "loss: 0.495619  [99200/175341]\n",
      "loss: 0.492375  [100800/175341]\n",
      "loss: 0.293987  [102400/175341]\n",
      "loss: 0.517275  [104000/175341]\n",
      "loss: 0.792138  [105600/175341]\n",
      "loss: 0.738066  [107200/175341]\n",
      "loss: 0.535115  [108800/175341]\n",
      "loss: 0.525042  [110400/175341]\n",
      "loss: 0.556352  [112000/175341]\n",
      "loss: 0.369129  [113600/175341]\n",
      "loss: 0.537052  [115200/175341]\n",
      "loss: 0.340110  [116800/175341]\n",
      "loss: 0.415936  [118400/175341]\n",
      "loss: 0.176055  [120000/175341]\n",
      "loss: 0.427244  [121600/175341]\n",
      "loss: 0.260816  [123200/175341]\n",
      "loss: 0.272726  [124800/175341]\n",
      "loss: 0.421184  [126400/175341]\n",
      "loss: 0.638386  [128000/175341]\n",
      "loss: 0.251839  [129600/175341]\n",
      "loss: 0.397364  [131200/175341]\n",
      "loss: 0.420316  [132800/175341]\n",
      "loss: 0.488880  [134400/175341]\n",
      "loss: 0.357079  [136000/175341]\n",
      "loss: 0.438056  [137600/175341]\n",
      "loss: 0.369106  [139200/175341]\n",
      "loss: 0.375962  [140800/175341]\n",
      "loss: 0.431409  [142400/175341]\n",
      "loss: 0.712725  [144000/175341]\n",
      "loss: 0.219833  [145600/175341]\n",
      "loss: 0.571798  [147200/175341]\n",
      "loss: 0.559525  [148800/175341]\n",
      "loss: 0.495150  [150400/175341]\n",
      "loss: 0.325356  [152000/175341]\n",
      "loss: 0.385728  [153600/175341]\n",
      "loss: 0.365693  [155200/175341]\n",
      "loss: 0.399107  [156800/175341]\n",
      "loss: 0.388506  [158400/175341]\n",
      "loss: 0.276505  [160000/175341]\n",
      "loss: 0.286418  [161600/175341]\n",
      "loss: 0.392713  [163200/175341]\n",
      "loss: 0.657774  [164800/175341]\n",
      "loss: 0.648728  [166400/175341]\n",
      "loss: 0.631335  [168000/175341]\n",
      "loss: 0.744359  [169600/175341]\n",
      "loss: 0.325273  [171200/175341]\n",
      "loss: 0.374319  [172800/175341]\n",
      "loss: 0.365122  [174400/175341]\n",
      "Train Accuracy: 81.6135%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.537256, F1-score: 76.23%, Macro_F1-Score:  42.24%  \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.638074  [    0/175341]\n",
      "loss: 0.696650  [ 1600/175341]\n",
      "loss: 0.299554  [ 3200/175341]\n",
      "loss: 0.400784  [ 4800/175341]\n",
      "loss: 0.204754  [ 6400/175341]\n",
      "loss: 0.343083  [ 8000/175341]\n",
      "loss: 0.558957  [ 9600/175341]\n",
      "loss: 0.554318  [11200/175341]\n",
      "loss: 0.399765  [12800/175341]\n",
      "loss: 0.196084  [14400/175341]\n",
      "loss: 0.621073  [16000/175341]\n",
      "loss: 0.381265  [17600/175341]\n",
      "loss: 0.503129  [19200/175341]\n",
      "loss: 0.365653  [20800/175341]\n",
      "loss: 0.504765  [22400/175341]\n",
      "loss: 0.369172  [24000/175341]\n",
      "loss: 0.697564  [25600/175341]\n",
      "loss: 0.375450  [27200/175341]\n",
      "loss: 0.306348  [28800/175341]\n",
      "loss: 0.520139  [30400/175341]\n",
      "loss: 0.497952  [32000/175341]\n",
      "loss: 0.452889  [33600/175341]\n",
      "loss: 0.709966  [35200/175341]\n",
      "loss: 0.320137  [36800/175341]\n",
      "loss: 0.530022  [38400/175341]\n",
      "loss: 0.610502  [40000/175341]\n",
      "loss: 0.140644  [41600/175341]\n",
      "loss: 0.342915  [43200/175341]\n",
      "loss: 0.648343  [44800/175341]\n",
      "loss: 0.495213  [46400/175341]\n",
      "loss: 0.568499  [48000/175341]\n",
      "loss: 0.186132  [49600/175341]\n",
      "loss: 0.427788  [51200/175341]\n",
      "loss: 0.863786  [52800/175341]\n",
      "loss: 0.262222  [54400/175341]\n",
      "loss: 0.193438  [56000/175341]\n",
      "loss: 0.357275  [57600/175341]\n",
      "loss: 0.331412  [59200/175341]\n",
      "loss: 0.553517  [60800/175341]\n",
      "loss: 0.281177  [62400/175341]\n",
      "loss: 0.182212  [64000/175341]\n",
      "loss: 0.098549  [65600/175341]\n",
      "loss: 0.436368  [67200/175341]\n",
      "loss: 0.347247  [68800/175341]\n",
      "loss: 0.459516  [70400/175341]\n",
      "loss: 0.373963  [72000/175341]\n",
      "loss: 0.604267  [73600/175341]\n",
      "loss: 0.180423  [75200/175341]\n",
      "loss: 0.139840  [76800/175341]\n",
      "loss: 0.492657  [78400/175341]\n",
      "loss: 0.666292  [80000/175341]\n",
      "loss: 0.329398  [81600/175341]\n",
      "loss: 0.433331  [83200/175341]\n",
      "loss: 0.530791  [84800/175341]\n",
      "loss: 0.689297  [86400/175341]\n",
      "loss: 0.331167  [88000/175341]\n",
      "loss: 0.338249  [89600/175341]\n",
      "loss: 0.419656  [91200/175341]\n",
      "loss: 0.214018  [92800/175341]\n",
      "loss: 0.725632  [94400/175341]\n",
      "loss: 0.498161  [96000/175341]\n",
      "loss: 0.214687  [97600/175341]\n",
      "loss: 0.286282  [99200/175341]\n",
      "loss: 0.361616  [100800/175341]\n",
      "loss: 0.911729  [102400/175341]\n",
      "loss: 0.366235  [104000/175341]\n",
      "loss: 0.123108  [105600/175341]\n",
      "loss: 0.248812  [107200/175341]\n",
      "loss: 0.273697  [108800/175341]\n",
      "loss: 0.330118  [110400/175341]\n",
      "loss: 0.196583  [112000/175341]\n",
      "loss: 0.231508  [113600/175341]\n",
      "loss: 0.739509  [115200/175341]\n",
      "loss: 0.155279  [116800/175341]\n",
      "loss: 0.804527  [118400/175341]\n",
      "loss: 0.411125  [120000/175341]\n",
      "loss: 0.437953  [121600/175341]\n",
      "loss: 0.448107  [123200/175341]\n",
      "loss: 0.318204  [124800/175341]\n",
      "loss: 0.091459  [126400/175341]\n",
      "loss: 0.530874  [128000/175341]\n",
      "loss: 0.443129  [129600/175341]\n",
      "loss: 0.381684  [131200/175341]\n",
      "loss: 0.419085  [132800/175341]\n",
      "loss: 0.326590  [134400/175341]\n",
      "loss: 0.780312  [136000/175341]\n",
      "loss: 0.488809  [137600/175341]\n",
      "loss: 0.127495  [139200/175341]\n",
      "loss: 0.411864  [140800/175341]\n",
      "loss: 0.484126  [142400/175341]\n",
      "loss: 0.291566  [144000/175341]\n",
      "loss: 0.268606  [145600/175341]\n",
      "loss: 0.519334  [147200/175341]\n",
      "loss: 0.433673  [148800/175341]\n",
      "loss: 0.269371  [150400/175341]\n",
      "loss: 0.449457  [152000/175341]\n",
      "loss: 0.722539  [153600/175341]\n",
      "loss: 0.818842  [155200/175341]\n",
      "loss: 0.477135  [156800/175341]\n",
      "loss: 0.211378  [158400/175341]\n",
      "loss: 0.560901  [160000/175341]\n",
      "loss: 0.285960  [161600/175341]\n",
      "loss: 0.932578  [163200/175341]\n",
      "loss: 1.295182  [164800/175341]\n",
      "loss: 0.397240  [166400/175341]\n",
      "loss: 0.430962  [168000/175341]\n",
      "loss: 0.825177  [169600/175341]\n",
      "loss: 0.466354  [171200/175341]\n",
      "loss: 0.327177  [172800/175341]\n",
      "loss: 0.447978  [174400/175341]\n",
      "Train Accuracy: 81.5884%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.533788, F1-score: 76.16%, Macro_F1-Score:  42.85%  \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.409159  [    0/175341]\n",
      "loss: 0.362067  [ 1600/175341]\n",
      "loss: 0.351678  [ 3200/175341]\n",
      "loss: 0.505048  [ 4800/175341]\n",
      "loss: 0.440395  [ 6400/175341]\n",
      "loss: 0.206989  [ 8000/175341]\n",
      "loss: 0.524631  [ 9600/175341]\n",
      "loss: 0.638134  [11200/175341]\n",
      "loss: 0.429250  [12800/175341]\n",
      "loss: 0.341080  [14400/175341]\n",
      "loss: 0.425545  [16000/175341]\n",
      "loss: 0.623511  [17600/175341]\n",
      "loss: 0.579713  [19200/175341]\n",
      "loss: 0.085203  [20800/175341]\n",
      "loss: 0.482179  [22400/175341]\n",
      "loss: 0.924683  [24000/175341]\n",
      "loss: 0.281121  [25600/175341]\n",
      "loss: 0.407601  [27200/175341]\n",
      "loss: 0.775287  [28800/175341]\n",
      "loss: 0.520385  [30400/175341]\n",
      "loss: 0.647662  [32000/175341]\n",
      "loss: 0.597826  [33600/175341]\n",
      "loss: 0.302330  [35200/175341]\n",
      "loss: 0.960030  [36800/175341]\n",
      "loss: 0.609878  [38400/175341]\n",
      "loss: 0.493738  [40000/175341]\n",
      "loss: 0.343020  [41600/175341]\n",
      "loss: 0.167070  [43200/175341]\n",
      "loss: 0.208891  [44800/175341]\n",
      "loss: 0.315582  [46400/175341]\n",
      "loss: 0.109461  [48000/175341]\n",
      "loss: 0.903134  [49600/175341]\n",
      "loss: 0.301185  [51200/175341]\n",
      "loss: 0.425978  [52800/175341]\n",
      "loss: 0.190084  [54400/175341]\n",
      "loss: 0.383583  [56000/175341]\n",
      "loss: 0.483232  [57600/175341]\n",
      "loss: 0.515701  [59200/175341]\n",
      "loss: 0.905008  [60800/175341]\n",
      "loss: 0.119623  [62400/175341]\n",
      "loss: 0.820471  [64000/175341]\n",
      "loss: 0.406635  [65600/175341]\n",
      "loss: 0.417900  [67200/175341]\n",
      "loss: 0.299350  [68800/175341]\n",
      "loss: 0.300684  [70400/175341]\n",
      "loss: 0.304109  [72000/175341]\n",
      "loss: 0.355266  [73600/175341]\n",
      "loss: 0.418440  [75200/175341]\n",
      "loss: 0.537366  [76800/175341]\n",
      "loss: 0.298627  [78400/175341]\n",
      "loss: 0.287482  [80000/175341]\n",
      "loss: 0.770453  [81600/175341]\n",
      "loss: 0.897836  [83200/175341]\n",
      "loss: 0.514857  [84800/175341]\n",
      "loss: 0.564290  [86400/175341]\n",
      "loss: 0.338522  [88000/175341]\n",
      "loss: 0.364294  [89600/175341]\n",
      "loss: 0.526668  [91200/175341]\n",
      "loss: 0.525108  [92800/175341]\n",
      "loss: 0.570786  [94400/175341]\n",
      "loss: 0.343571  [96000/175341]\n",
      "loss: 0.577262  [97600/175341]\n",
      "loss: 0.690816  [99200/175341]\n",
      "loss: 0.499887  [100800/175341]\n",
      "loss: 0.548866  [102400/175341]\n",
      "loss: 0.124490  [104000/175341]\n",
      "loss: 0.434305  [105600/175341]\n",
      "loss: 0.750225  [107200/175341]\n",
      "loss: 0.618525  [108800/175341]\n",
      "loss: 0.614968  [110400/175341]\n",
      "loss: 0.443449  [112000/175341]\n",
      "loss: 0.419001  [113600/175341]\n",
      "loss: 0.509758  [115200/175341]\n",
      "loss: 0.241028  [116800/175341]\n",
      "loss: 0.260098  [118400/175341]\n",
      "loss: 0.125631  [120000/175341]\n",
      "loss: 0.605775  [121600/175341]\n",
      "loss: 0.322559  [123200/175341]\n",
      "loss: 0.374659  [124800/175341]\n",
      "loss: 0.583858  [126400/175341]\n",
      "loss: 0.444453  [128000/175341]\n",
      "loss: 0.727024  [129600/175341]\n",
      "loss: 0.347294  [131200/175341]\n",
      "loss: 0.354511  [132800/175341]\n",
      "loss: 0.412642  [134400/175341]\n",
      "loss: 0.340584  [136000/175341]\n",
      "loss: 0.299395  [137600/175341]\n",
      "loss: 0.463432  [139200/175341]\n",
      "loss: 0.204697  [140800/175341]\n",
      "loss: 0.542007  [142400/175341]\n",
      "loss: 0.486343  [144000/175341]\n",
      "loss: 0.380526  [145600/175341]\n",
      "loss: 0.383066  [147200/175341]\n",
      "loss: 0.272988  [148800/175341]\n",
      "loss: 0.299557  [150400/175341]\n",
      "loss: 0.302974  [152000/175341]\n",
      "loss: 0.228236  [153600/175341]\n",
      "loss: 0.301247  [155200/175341]\n",
      "loss: 0.368819  [156800/175341]\n",
      "loss: 0.295206  [158400/175341]\n",
      "loss: 0.311114  [160000/175341]\n",
      "loss: 0.324888  [161600/175341]\n",
      "loss: 0.534661  [163200/175341]\n",
      "loss: 0.181137  [164800/175341]\n",
      "loss: 0.249513  [166400/175341]\n",
      "loss: 0.292388  [168000/175341]\n",
      "loss: 0.208517  [169600/175341]\n",
      "loss: 0.387425  [171200/175341]\n",
      "loss: 0.361233  [172800/175341]\n",
      "loss: 0.442606  [174400/175341]\n",
      "Train Accuracy: 81.6198%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.533781, F1-score: 76.43%, Macro_F1-Score:  42.48%  \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.476159  [    0/175341]\n",
      "loss: 0.258685  [ 1600/175341]\n",
      "loss: 0.359682  [ 3200/175341]\n",
      "loss: 0.226389  [ 4800/175341]\n",
      "loss: 0.579887  [ 6400/175341]\n",
      "loss: 0.312110  [ 8000/175341]\n",
      "loss: 0.770880  [ 9600/175341]\n",
      "loss: 0.076960  [11200/175341]\n",
      "loss: 0.310636  [12800/175341]\n",
      "loss: 1.114591  [14400/175341]\n",
      "loss: 0.543968  [16000/175341]\n",
      "loss: 0.443668  [17600/175341]\n",
      "loss: 0.526859  [19200/175341]\n",
      "loss: 0.357298  [20800/175341]\n",
      "loss: 0.325171  [22400/175341]\n",
      "loss: 0.709963  [24000/175341]\n",
      "loss: 0.359552  [25600/175341]\n",
      "loss: 0.327917  [27200/175341]\n",
      "loss: 0.297358  [28800/175341]\n",
      "loss: 0.497010  [30400/175341]\n",
      "loss: 0.290504  [32000/175341]\n",
      "loss: 0.556269  [33600/175341]\n",
      "loss: 0.585211  [35200/175341]\n",
      "loss: 0.366422  [36800/175341]\n",
      "loss: 0.222523  [38400/175341]\n",
      "loss: 0.383867  [40000/175341]\n",
      "loss: 0.411572  [41600/175341]\n",
      "loss: 0.460914  [43200/175341]\n",
      "loss: 0.471314  [44800/175341]\n",
      "loss: 1.052214  [46400/175341]\n",
      "loss: 0.468298  [48000/175341]\n",
      "loss: 0.570903  [49600/175341]\n",
      "loss: 0.650519  [51200/175341]\n",
      "loss: 0.239215  [52800/175341]\n",
      "loss: 0.205425  [54400/175341]\n",
      "loss: 0.341290  [56000/175341]\n",
      "loss: 1.051673  [57600/175341]\n",
      "loss: 0.455655  [59200/175341]\n",
      "loss: 0.692761  [60800/175341]\n",
      "loss: 0.289901  [62400/175341]\n",
      "loss: 0.272380  [64000/175341]\n",
      "loss: 0.706041  [65600/175341]\n",
      "loss: 0.441533  [67200/175341]\n",
      "loss: 0.408596  [68800/175341]\n",
      "loss: 0.194017  [70400/175341]\n",
      "loss: 0.251216  [72000/175341]\n",
      "loss: 0.282075  [73600/175341]\n",
      "loss: 0.727449  [75200/175341]\n",
      "loss: 0.484863  [76800/175341]\n",
      "loss: 0.427644  [78400/175341]\n",
      "loss: 0.223760  [80000/175341]\n",
      "loss: 0.559473  [81600/175341]\n",
      "loss: 0.745819  [83200/175341]\n",
      "loss: 0.112506  [84800/175341]\n",
      "loss: 0.383313  [86400/175341]\n",
      "loss: 0.226967  [88000/175341]\n",
      "loss: 0.589076  [89600/175341]\n",
      "loss: 0.473319  [91200/175341]\n",
      "loss: 0.580455  [92800/175341]\n",
      "loss: 0.579053  [94400/175341]\n",
      "loss: 1.130687  [96000/175341]\n",
      "loss: 0.710424  [97600/175341]\n",
      "loss: 0.511234  [99200/175341]\n",
      "loss: 0.492528  [100800/175341]\n",
      "loss: 0.588065  [102400/175341]\n",
      "loss: 0.128930  [104000/175341]\n",
      "loss: 0.173446  [105600/175341]\n",
      "loss: 0.291753  [107200/175341]\n",
      "loss: 0.149819  [108800/175341]\n",
      "loss: 0.521935  [110400/175341]\n",
      "loss: 0.517609  [112000/175341]\n",
      "loss: 0.341954  [113600/175341]\n",
      "loss: 0.861638  [115200/175341]\n",
      "loss: 0.634684  [116800/175341]\n",
      "loss: 0.613967  [118400/175341]\n",
      "loss: 0.401076  [120000/175341]\n",
      "loss: 0.156231  [121600/175341]\n",
      "loss: 0.526096  [123200/175341]\n",
      "loss: 0.402099  [124800/175341]\n",
      "loss: 0.415965  [126400/175341]\n",
      "loss: 0.676133  [128000/175341]\n",
      "loss: 0.333187  [129600/175341]\n",
      "loss: 0.384859  [131200/175341]\n",
      "loss: 0.533275  [132800/175341]\n",
      "loss: 0.388970  [134400/175341]\n",
      "loss: 0.384487  [136000/175341]\n",
      "loss: 0.700397  [137600/175341]\n",
      "loss: 0.207477  [139200/175341]\n",
      "loss: 0.349877  [140800/175341]\n",
      "loss: 0.100752  [142400/175341]\n",
      "loss: 0.401202  [144000/175341]\n",
      "loss: 0.072135  [145600/175341]\n",
      "loss: 0.476406  [147200/175341]\n",
      "loss: 0.519427  [148800/175341]\n",
      "loss: 0.193330  [150400/175341]\n",
      "loss: 0.213710  [152000/175341]\n",
      "loss: 0.630708  [153600/175341]\n",
      "loss: 0.175761  [155200/175341]\n",
      "loss: 0.705883  [156800/175341]\n",
      "loss: 0.357894  [158400/175341]\n",
      "loss: 0.233310  [160000/175341]\n",
      "loss: 0.465122  [161600/175341]\n",
      "loss: 0.329793  [163200/175341]\n",
      "loss: 0.258641  [164800/175341]\n",
      "loss: 0.671021  [166400/175341]\n",
      "loss: 0.378068  [168000/175341]\n",
      "loss: 0.317969  [169600/175341]\n",
      "loss: 0.413176  [171200/175341]\n",
      "loss: 0.221466  [172800/175341]\n",
      "loss: 0.244223  [174400/175341]\n",
      "Train Accuracy: 81.6375%\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.514668, F1-score: 78.36%, Macro_F1-Score:  43.88%  \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.453360  [    0/175341]\n",
      "loss: 0.665931  [ 1600/175341]\n",
      "loss: 0.423046  [ 3200/175341]\n",
      "loss: 0.724365  [ 4800/175341]\n",
      "loss: 0.729089  [ 6400/175341]\n",
      "loss: 0.754886  [ 8000/175341]\n",
      "loss: 0.908605  [ 9600/175341]\n",
      "loss: 0.391650  [11200/175341]\n",
      "loss: 0.723800  [12800/175341]\n",
      "loss: 0.093744  [14400/175341]\n",
      "loss: 0.359785  [16000/175341]\n",
      "loss: 0.308024  [17600/175341]\n",
      "loss: 0.309204  [19200/175341]\n",
      "loss: 0.468762  [20800/175341]\n",
      "loss: 0.787889  [22400/175341]\n",
      "loss: 0.413041  [24000/175341]\n",
      "loss: 0.377060  [25600/175341]\n",
      "loss: 0.604512  [27200/175341]\n",
      "loss: 0.505569  [28800/175341]\n",
      "loss: 0.349593  [30400/175341]\n",
      "loss: 0.412781  [32000/175341]\n",
      "loss: 0.304154  [33600/175341]\n",
      "loss: 0.588681  [35200/175341]\n",
      "loss: 0.215026  [36800/175341]\n",
      "loss: 0.404910  [38400/175341]\n",
      "loss: 0.679672  [40000/175341]\n",
      "loss: 0.445749  [41600/175341]\n",
      "loss: 1.038469  [43200/175341]\n",
      "loss: 0.330204  [44800/175341]\n",
      "loss: 0.492481  [46400/175341]\n",
      "loss: 0.165483  [48000/175341]\n",
      "loss: 0.582148  [49600/175341]\n",
      "loss: 0.294455  [51200/175341]\n",
      "loss: 0.757659  [52800/175341]\n",
      "loss: 0.463330  [54400/175341]\n",
      "loss: 0.326724  [56000/175341]\n",
      "loss: 0.262344  [57600/175341]\n",
      "loss: 0.574741  [59200/175341]\n",
      "loss: 0.378253  [60800/175341]\n",
      "loss: 0.652294  [62400/175341]\n",
      "loss: 0.479510  [64000/175341]\n",
      "loss: 0.299138  [65600/175341]\n",
      "loss: 0.340188  [67200/175341]\n",
      "loss: 0.771831  [68800/175341]\n",
      "loss: 0.337117  [70400/175341]\n",
      "loss: 0.288841  [72000/175341]\n",
      "loss: 0.402349  [73600/175341]\n",
      "loss: 0.648117  [75200/175341]\n",
      "loss: 0.117954  [76800/175341]\n",
      "loss: 0.349019  [78400/175341]\n",
      "loss: 0.378595  [80000/175341]\n",
      "loss: 0.387019  [81600/175341]\n",
      "loss: 0.298938  [83200/175341]\n",
      "loss: 0.303854  [84800/175341]\n",
      "loss: 0.486147  [86400/175341]\n",
      "loss: 0.658043  [88000/175341]\n",
      "loss: 0.488690  [89600/175341]\n",
      "loss: 0.250952  [91200/175341]\n",
      "loss: 0.528252  [92800/175341]\n",
      "loss: 0.365524  [94400/175341]\n",
      "loss: 0.684328  [96000/175341]\n",
      "loss: 0.553928  [97600/175341]\n",
      "loss: 0.337230  [99200/175341]\n",
      "loss: 0.623567  [100800/175341]\n",
      "loss: 0.357065  [102400/175341]\n",
      "loss: 0.234173  [104000/175341]\n",
      "loss: 0.424064  [105600/175341]\n",
      "loss: 0.602759  [107200/175341]\n",
      "loss: 0.369055  [108800/175341]\n",
      "loss: 0.923311  [110400/175341]\n",
      "loss: 0.405814  [112000/175341]\n",
      "loss: 0.598761  [113600/175341]\n",
      "loss: 0.525081  [115200/175341]\n",
      "loss: 0.547181  [116800/175341]\n",
      "loss: 0.580001  [118400/175341]\n",
      "loss: 0.496475  [120000/175341]\n",
      "loss: 0.627476  [121600/175341]\n",
      "loss: 0.548736  [123200/175341]\n",
      "loss: 0.250028  [124800/175341]\n",
      "loss: 0.663043  [126400/175341]\n",
      "loss: 0.208135  [128000/175341]\n",
      "loss: 0.953239  [129600/175341]\n",
      "loss: 0.527805  [131200/175341]\n",
      "loss: 0.389367  [132800/175341]\n",
      "loss: 0.170725  [134400/175341]\n",
      "loss: 0.354050  [136000/175341]\n",
      "loss: 0.519535  [137600/175341]\n",
      "loss: 0.411064  [139200/175341]\n",
      "loss: 0.592634  [140800/175341]\n",
      "loss: 0.409832  [142400/175341]\n",
      "loss: 0.488578  [144000/175341]\n",
      "loss: 0.508193  [145600/175341]\n",
      "loss: 0.511041  [147200/175341]\n",
      "loss: 0.501515  [148800/175341]\n",
      "loss: 0.455520  [150400/175341]\n",
      "loss: 0.519723  [152000/175341]\n",
      "loss: 0.238142  [153600/175341]\n",
      "loss: 0.311995  [155200/175341]\n",
      "loss: 0.383306  [156800/175341]\n",
      "loss: 0.716627  [158400/175341]\n",
      "loss: 0.623672  [160000/175341]\n",
      "loss: 0.280152  [161600/175341]\n",
      "loss: 0.160876  [163200/175341]\n",
      "loss: 0.513502  [164800/175341]\n",
      "loss: 0.629400  [166400/175341]\n",
      "loss: 0.219922  [168000/175341]\n",
      "loss: 0.401399  [169600/175341]\n",
      "loss: 0.388782  [171200/175341]\n",
      "loss: 0.650574  [172800/175341]\n",
      "loss: 0.982209  [174400/175341]\n",
      "Train Accuracy: 81.6415%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.548598, F1-score: 75.60%, Macro_F1-Score:  42.08%  \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.152167  [    0/175341]\n",
      "loss: 0.327521  [ 1600/175341]\n",
      "loss: 0.376255  [ 3200/175341]\n",
      "loss: 0.531314  [ 4800/175341]\n",
      "loss: 0.181476  [ 6400/175341]\n",
      "loss: 0.196355  [ 8000/175341]\n",
      "loss: 0.401811  [ 9600/175341]\n",
      "loss: 0.336679  [11200/175341]\n",
      "loss: 0.241549  [12800/175341]\n",
      "loss: 1.110409  [14400/175341]\n",
      "loss: 0.213861  [16000/175341]\n",
      "loss: 0.104703  [17600/175341]\n",
      "loss: 0.576186  [19200/175341]\n",
      "loss: 0.304314  [20800/175341]\n",
      "loss: 0.698054  [22400/175341]\n",
      "loss: 0.535454  [24000/175341]\n",
      "loss: 0.598766  [25600/175341]\n",
      "loss: 0.222247  [27200/175341]\n",
      "loss: 0.503759  [28800/175341]\n",
      "loss: 0.488560  [30400/175341]\n",
      "loss: 0.593243  [32000/175341]\n",
      "loss: 0.399049  [33600/175341]\n",
      "loss: 0.753745  [35200/175341]\n",
      "loss: 0.454497  [36800/175341]\n",
      "loss: 0.263831  [38400/175341]\n",
      "loss: 0.538962  [40000/175341]\n",
      "loss: 0.304129  [41600/175341]\n",
      "loss: 0.502009  [43200/175341]\n",
      "loss: 0.262357  [44800/175341]\n",
      "loss: 0.232269  [46400/175341]\n",
      "loss: 0.206076  [48000/175341]\n",
      "loss: 0.230124  [49600/175341]\n",
      "loss: 0.288090  [51200/175341]\n",
      "loss: 0.339946  [52800/175341]\n",
      "loss: 0.246177  [54400/175341]\n",
      "loss: 0.283898  [56000/175341]\n",
      "loss: 0.325770  [57600/175341]\n",
      "loss: 0.272384  [59200/175341]\n",
      "loss: 0.441566  [60800/175341]\n",
      "loss: 0.700194  [62400/175341]\n",
      "loss: 0.454457  [64000/175341]\n",
      "loss: 0.572175  [65600/175341]\n",
      "loss: 0.925831  [67200/175341]\n",
      "loss: 0.356234  [68800/175341]\n",
      "loss: 0.598290  [70400/175341]\n",
      "loss: 0.535588  [72000/175341]\n",
      "loss: 0.616822  [73600/175341]\n",
      "loss: 0.702046  [75200/175341]\n",
      "loss: 0.540624  [76800/175341]\n",
      "loss: 0.476327  [78400/175341]\n",
      "loss: 0.480216  [80000/175341]\n",
      "loss: 0.639026  [81600/175341]\n",
      "loss: 0.296821  [83200/175341]\n",
      "loss: 0.174032  [84800/175341]\n",
      "loss: 0.965371  [86400/175341]\n",
      "loss: 0.129376  [88000/175341]\n",
      "loss: 0.547981  [89600/175341]\n",
      "loss: 0.709884  [91200/175341]\n",
      "loss: 0.622883  [92800/175341]\n",
      "loss: 0.416878  [94400/175341]\n",
      "loss: 0.770066  [96000/175341]\n",
      "loss: 0.460215  [97600/175341]\n",
      "loss: 0.340715  [99200/175341]\n",
      "loss: 0.281782  [100800/175341]\n",
      "loss: 0.332275  [102400/175341]\n",
      "loss: 0.635782  [104000/175341]\n",
      "loss: 0.302280  [105600/175341]\n",
      "loss: 0.294599  [107200/175341]\n",
      "loss: 0.274015  [108800/175341]\n",
      "loss: 0.430739  [110400/175341]\n",
      "loss: 0.893212  [112000/175341]\n",
      "loss: 0.606536  [113600/175341]\n",
      "loss: 0.584267  [115200/175341]\n",
      "loss: 0.421974  [116800/175341]\n",
      "loss: 0.293212  [118400/175341]\n",
      "loss: 0.442486  [120000/175341]\n",
      "loss: 0.184348  [121600/175341]\n",
      "loss: 0.699150  [123200/175341]\n",
      "loss: 0.202154  [124800/175341]\n",
      "loss: 0.570287  [126400/175341]\n",
      "loss: 0.497789  [128000/175341]\n",
      "loss: 0.555334  [129600/175341]\n",
      "loss: 0.293071  [131200/175341]\n",
      "loss: 0.166904  [132800/175341]\n",
      "loss: 0.569220  [134400/175341]\n",
      "loss: 0.456078  [136000/175341]\n",
      "loss: 0.250495  [137600/175341]\n",
      "loss: 0.902773  [139200/175341]\n",
      "loss: 0.302772  [140800/175341]\n",
      "loss: 0.128933  [142400/175341]\n",
      "loss: 0.379372  [144000/175341]\n",
      "loss: 0.315140  [145600/175341]\n",
      "loss: 0.555938  [147200/175341]\n",
      "loss: 0.331269  [148800/175341]\n",
      "loss: 0.366703  [150400/175341]\n",
      "loss: 0.347662  [152000/175341]\n",
      "loss: 0.494877  [153600/175341]\n",
      "loss: 0.494686  [155200/175341]\n",
      "loss: 0.290428  [156800/175341]\n",
      "loss: 0.282897  [158400/175341]\n",
      "loss: 0.689451  [160000/175341]\n",
      "loss: 0.336200  [161600/175341]\n",
      "loss: 0.675325  [163200/175341]\n",
      "loss: 0.513617  [164800/175341]\n",
      "loss: 0.574272  [166400/175341]\n",
      "loss: 0.365365  [168000/175341]\n",
      "loss: 0.468942  [169600/175341]\n",
      "loss: 0.347814  [171200/175341]\n",
      "loss: 0.574551  [172800/175341]\n",
      "loss: 0.233920  [174400/175341]\n",
      "Train Accuracy: 81.6637%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.542014, F1-score: 75.84%, Macro_F1-Score:  41.93%  \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.408321  [    0/175341]\n",
      "loss: 0.415291  [ 1600/175341]\n",
      "loss: 0.401432  [ 3200/175341]\n",
      "loss: 0.327440  [ 4800/175341]\n",
      "loss: 0.583211  [ 6400/175341]\n",
      "loss: 0.428328  [ 8000/175341]\n",
      "loss: 0.216745  [ 9600/175341]\n",
      "loss: 0.427216  [11200/175341]\n",
      "loss: 0.410136  [12800/175341]\n",
      "loss: 0.462062  [14400/175341]\n",
      "loss: 0.256308  [16000/175341]\n",
      "loss: 0.518521  [17600/175341]\n",
      "loss: 1.046590  [19200/175341]\n",
      "loss: 0.359570  [20800/175341]\n",
      "loss: 0.701590  [22400/175341]\n",
      "loss: 0.581476  [24000/175341]\n",
      "loss: 0.533633  [25600/175341]\n",
      "loss: 0.435867  [27200/175341]\n",
      "loss: 0.578140  [28800/175341]\n",
      "loss: 0.506356  [30400/175341]\n",
      "loss: 0.532327  [32000/175341]\n",
      "loss: 0.251404  [33600/175341]\n",
      "loss: 0.279353  [35200/175341]\n",
      "loss: 0.723407  [36800/175341]\n",
      "loss: 0.643824  [38400/175341]\n",
      "loss: 0.646672  [40000/175341]\n",
      "loss: 0.796254  [41600/175341]\n",
      "loss: 0.113845  [43200/175341]\n",
      "loss: 0.148966  [44800/175341]\n",
      "loss: 0.659410  [46400/175341]\n",
      "loss: 0.581524  [48000/175341]\n",
      "loss: 0.355947  [49600/175341]\n",
      "loss: 0.633344  [51200/175341]\n",
      "loss: 0.462058  [52800/175341]\n",
      "loss: 0.443641  [54400/175341]\n",
      "loss: 0.234755  [56000/175341]\n",
      "loss: 0.542709  [57600/175341]\n",
      "loss: 0.295242  [59200/175341]\n",
      "loss: 0.424362  [60800/175341]\n",
      "loss: 0.346358  [62400/175341]\n",
      "loss: 0.728096  [64000/175341]\n",
      "loss: 0.187526  [65600/175341]\n",
      "loss: 0.245207  [67200/175341]\n",
      "loss: 0.652435  [68800/175341]\n",
      "loss: 0.152222  [70400/175341]\n",
      "loss: 0.268877  [72000/175341]\n",
      "loss: 0.483616  [73600/175341]\n",
      "loss: 0.223047  [75200/175341]\n",
      "loss: 0.249074  [76800/175341]\n",
      "loss: 0.139001  [78400/175341]\n",
      "loss: 0.184051  [80000/175341]\n",
      "loss: 0.523257  [81600/175341]\n",
      "loss: 0.360514  [83200/175341]\n",
      "loss: 0.698647  [84800/175341]\n",
      "loss: 0.349168  [86400/175341]\n",
      "loss: 0.348012  [88000/175341]\n",
      "loss: 0.205293  [89600/175341]\n",
      "loss: 0.634230  [91200/175341]\n",
      "loss: 0.396414  [92800/175341]\n",
      "loss: 0.476349  [94400/175341]\n",
      "loss: 0.527577  [96000/175341]\n",
      "loss: 0.322940  [97600/175341]\n",
      "loss: 0.442550  [99200/175341]\n",
      "loss: 0.566777  [100800/175341]\n",
      "loss: 0.388024  [102400/175341]\n",
      "loss: 0.324667  [104000/175341]\n",
      "loss: 0.251161  [105600/175341]\n",
      "loss: 0.238800  [107200/175341]\n",
      "loss: 0.577665  [108800/175341]\n",
      "loss: 0.351471  [110400/175341]\n",
      "loss: 0.499028  [112000/175341]\n",
      "loss: 0.290769  [113600/175341]\n",
      "loss: 0.223275  [115200/175341]\n",
      "loss: 0.389899  [116800/175341]\n",
      "loss: 0.596510  [118400/175341]\n",
      "loss: 0.287646  [120000/175341]\n",
      "loss: 0.214879  [121600/175341]\n",
      "loss: 0.535366  [123200/175341]\n",
      "loss: 0.275079  [124800/175341]\n",
      "loss: 0.436198  [126400/175341]\n",
      "loss: 0.451628  [128000/175341]\n",
      "loss: 0.409868  [129600/175341]\n",
      "loss: 0.473135  [131200/175341]\n",
      "loss: 0.286192  [132800/175341]\n",
      "loss: 0.657770  [134400/175341]\n",
      "loss: 0.294995  [136000/175341]\n",
      "loss: 0.714735  [137600/175341]\n",
      "loss: 0.693738  [139200/175341]\n",
      "loss: 0.469266  [140800/175341]\n",
      "loss: 0.562866  [142400/175341]\n",
      "loss: 0.727442  [144000/175341]\n",
      "loss: 0.363228  [145600/175341]\n",
      "loss: 0.376594  [147200/175341]\n",
      "loss: 0.206523  [148800/175341]\n",
      "loss: 0.304011  [150400/175341]\n",
      "loss: 0.516988  [152000/175341]\n",
      "loss: 0.564445  [153600/175341]\n",
      "loss: 0.337337  [155200/175341]\n",
      "loss: 0.499175  [156800/175341]\n",
      "loss: 0.308477  [158400/175341]\n",
      "loss: 0.509456  [160000/175341]\n",
      "loss: 0.529344  [161600/175341]\n",
      "loss: 0.321157  [163200/175341]\n",
      "loss: 0.822600  [164800/175341]\n",
      "loss: 0.616915  [166400/175341]\n",
      "loss: 0.409690  [168000/175341]\n",
      "loss: 0.356634  [169600/175341]\n",
      "loss: 0.525243  [171200/175341]\n",
      "loss: 0.334271  [172800/175341]\n",
      "loss: 0.392158  [174400/175341]\n",
      "Train Accuracy: 81.6592%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.529430, F1-score: 76.43%, Macro_F1-Score:  43.13%  \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.180660  [    0/175341]\n",
      "loss: 0.454355  [ 1600/175341]\n",
      "loss: 0.531164  [ 3200/175341]\n",
      "loss: 0.301668  [ 4800/175341]\n",
      "loss: 0.412876  [ 6400/175341]\n",
      "loss: 0.110061  [ 8000/175341]\n",
      "loss: 0.374268  [ 9600/175341]\n",
      "loss: 0.475251  [11200/175341]\n",
      "loss: 0.170818  [12800/175341]\n",
      "loss: 0.148857  [14400/175341]\n",
      "loss: 0.285157  [16000/175341]\n",
      "loss: 0.289139  [17600/175341]\n",
      "loss: 0.523438  [19200/175341]\n",
      "loss: 0.416532  [20800/175341]\n",
      "loss: 0.256581  [22400/175341]\n",
      "loss: 0.642935  [24000/175341]\n",
      "loss: 0.435246  [25600/175341]\n",
      "loss: 0.408934  [27200/175341]\n",
      "loss: 0.441455  [28800/175341]\n",
      "loss: 0.540312  [30400/175341]\n",
      "loss: 0.447265  [32000/175341]\n",
      "loss: 0.326046  [33600/175341]\n",
      "loss: 0.241850  [35200/175341]\n",
      "loss: 0.522794  [36800/175341]\n",
      "loss: 0.418369  [38400/175341]\n",
      "loss: 0.569357  [40000/175341]\n",
      "loss: 0.254892  [41600/175341]\n",
      "loss: 0.448326  [43200/175341]\n",
      "loss: 0.256654  [44800/175341]\n",
      "loss: 0.692094  [46400/175341]\n",
      "loss: 0.227617  [48000/175341]\n",
      "loss: 0.429215  [49600/175341]\n",
      "loss: 0.431412  [51200/175341]\n",
      "loss: 0.665177  [52800/175341]\n",
      "loss: 0.870273  [54400/175341]\n",
      "loss: 0.173977  [56000/175341]\n",
      "loss: 0.531841  [57600/175341]\n",
      "loss: 0.418970  [59200/175341]\n",
      "loss: 0.522180  [60800/175341]\n",
      "loss: 0.169475  [62400/175341]\n",
      "loss: 0.474760  [64000/175341]\n",
      "loss: 0.585606  [65600/175341]\n",
      "loss: 0.457839  [67200/175341]\n",
      "loss: 0.927959  [68800/175341]\n",
      "loss: 0.255560  [70400/175341]\n",
      "loss: 0.501778  [72000/175341]\n",
      "loss: 0.152244  [73600/175341]\n",
      "loss: 0.270670  [75200/175341]\n",
      "loss: 0.691012  [76800/175341]\n",
      "loss: 0.267073  [78400/175341]\n",
      "loss: 0.763489  [80000/175341]\n",
      "loss: 0.127055  [81600/175341]\n",
      "loss: 0.571615  [83200/175341]\n",
      "loss: 0.486833  [84800/175341]\n",
      "loss: 0.535710  [86400/175341]\n",
      "loss: 0.437520  [88000/175341]\n",
      "loss: 0.217165  [89600/175341]\n",
      "loss: 0.207022  [91200/175341]\n",
      "loss: 0.199693  [92800/175341]\n",
      "loss: 0.605713  [94400/175341]\n",
      "loss: 0.335973  [96000/175341]\n",
      "loss: 0.656723  [97600/175341]\n",
      "loss: 0.272694  [99200/175341]\n",
      "loss: 0.412291  [100800/175341]\n",
      "loss: 0.262377  [102400/175341]\n",
      "loss: 0.422615  [104000/175341]\n",
      "loss: 0.548033  [105600/175341]\n",
      "loss: 0.737228  [107200/175341]\n",
      "loss: 0.689649  [108800/175341]\n",
      "loss: 0.462670  [110400/175341]\n",
      "loss: 0.323413  [112000/175341]\n",
      "loss: 0.432862  [113600/175341]\n",
      "loss: 0.321863  [115200/175341]\n",
      "loss: 0.542218  [116800/175341]\n",
      "loss: 0.104705  [118400/175341]\n",
      "loss: 0.159988  [120000/175341]\n",
      "loss: 0.351064  [121600/175341]\n",
      "loss: 1.277697  [123200/175341]\n",
      "loss: 0.382735  [124800/175341]\n",
      "loss: 0.355540  [126400/175341]\n",
      "loss: 0.796560  [128000/175341]\n",
      "loss: 0.621415  [129600/175341]\n",
      "loss: 0.473794  [131200/175341]\n",
      "loss: 0.175120  [132800/175341]\n",
      "loss: 0.249656  [134400/175341]\n",
      "loss: 0.508674  [136000/175341]\n",
      "loss: 0.473959  [137600/175341]\n",
      "loss: 0.394532  [139200/175341]\n",
      "loss: 0.513556  [140800/175341]\n",
      "loss: 0.470203  [142400/175341]\n",
      "loss: 0.537099  [144000/175341]\n",
      "loss: 0.451460  [145600/175341]\n",
      "loss: 0.428026  [147200/175341]\n",
      "loss: 0.491805  [148800/175341]\n",
      "loss: 0.723783  [150400/175341]\n",
      "loss: 0.544224  [152000/175341]\n",
      "loss: 0.317713  [153600/175341]\n",
      "loss: 0.250692  [155200/175341]\n",
      "loss: 0.595481  [156800/175341]\n",
      "loss: 0.577367  [158400/175341]\n",
      "loss: 0.637506  [160000/175341]\n",
      "loss: 0.602491  [161600/175341]\n",
      "loss: 0.459332  [163200/175341]\n",
      "loss: 0.535461  [164800/175341]\n",
      "loss: 0.443774  [166400/175341]\n",
      "loss: 0.369318  [168000/175341]\n",
      "loss: 0.282736  [169600/175341]\n",
      "loss: 0.120123  [171200/175341]\n",
      "loss: 0.718043  [172800/175341]\n",
      "loss: 0.365074  [174400/175341]\n",
      "Train Accuracy: 81.7493%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.563085, F1-score: 74.94%, Macro_F1-Score:  42.46%  \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.588240  [    0/175341]\n",
      "loss: 0.453277  [ 1600/175341]\n",
      "loss: 0.263886  [ 3200/175341]\n",
      "loss: 0.389915  [ 4800/175341]\n",
      "loss: 0.422439  [ 6400/175341]\n",
      "loss: 0.255762  [ 8000/175341]\n",
      "loss: 0.352135  [ 9600/175341]\n",
      "loss: 1.050926  [11200/175341]\n",
      "loss: 1.040749  [12800/175341]\n",
      "loss: 0.502144  [14400/175341]\n",
      "loss: 0.772280  [16000/175341]\n",
      "loss: 0.286152  [17600/175341]\n",
      "loss: 0.273362  [19200/175341]\n",
      "loss: 0.335173  [20800/175341]\n",
      "loss: 0.613311  [22400/175341]\n",
      "loss: 0.334978  [24000/175341]\n",
      "loss: 0.431650  [25600/175341]\n",
      "loss: 0.334838  [27200/175341]\n",
      "loss: 0.329270  [28800/175341]\n",
      "loss: 0.425194  [30400/175341]\n",
      "loss: 0.412009  [32000/175341]\n",
      "loss: 0.418232  [33600/175341]\n",
      "loss: 0.400306  [35200/175341]\n",
      "loss: 0.242972  [36800/175341]\n",
      "loss: 0.464728  [38400/175341]\n",
      "loss: 0.777676  [40000/175341]\n",
      "loss: 0.308847  [41600/175341]\n",
      "loss: 0.290317  [43200/175341]\n",
      "loss: 0.398474  [44800/175341]\n",
      "loss: 0.586632  [46400/175341]\n",
      "loss: 0.324495  [48000/175341]\n",
      "loss: 0.366921  [49600/175341]\n",
      "loss: 0.368833  [51200/175341]\n",
      "loss: 0.348228  [52800/175341]\n",
      "loss: 0.446070  [54400/175341]\n",
      "loss: 0.599408  [56000/175341]\n",
      "loss: 0.725076  [57600/175341]\n",
      "loss: 0.327032  [59200/175341]\n",
      "loss: 0.591180  [60800/175341]\n",
      "loss: 0.718640  [62400/175341]\n",
      "loss: 0.345738  [64000/175341]\n",
      "loss: 0.457746  [65600/175341]\n",
      "loss: 0.930638  [67200/175341]\n",
      "loss: 0.180683  [68800/175341]\n",
      "loss: 0.371045  [70400/175341]\n",
      "loss: 0.371807  [72000/175341]\n",
      "loss: 0.412586  [73600/175341]\n",
      "loss: 0.400885  [75200/175341]\n",
      "loss: 0.738356  [76800/175341]\n",
      "loss: 0.307317  [78400/175341]\n",
      "loss: 0.357872  [80000/175341]\n",
      "loss: 0.233766  [81600/175341]\n",
      "loss: 0.332888  [83200/175341]\n",
      "loss: 1.009453  [84800/175341]\n",
      "loss: 0.373929  [86400/175341]\n",
      "loss: 0.287227  [88000/175341]\n",
      "loss: 0.073006  [89600/175341]\n",
      "loss: 0.274758  [91200/175341]\n",
      "loss: 0.187010  [92800/175341]\n",
      "loss: 0.579673  [94400/175341]\n",
      "loss: 0.269721  [96000/175341]\n",
      "loss: 0.476611  [97600/175341]\n",
      "loss: 0.701680  [99200/175341]\n",
      "loss: 0.483197  [100800/175341]\n",
      "loss: 0.174376  [102400/175341]\n",
      "loss: 0.675708  [104000/175341]\n",
      "loss: 0.126915  [105600/175341]\n",
      "loss: 0.637722  [107200/175341]\n",
      "loss: 0.188586  [108800/175341]\n",
      "loss: 0.455469  [110400/175341]\n",
      "loss: 0.546697  [112000/175341]\n",
      "loss: 0.379961  [113600/175341]\n",
      "loss: 0.071820  [115200/175341]\n",
      "loss: 0.167701  [116800/175341]\n",
      "loss: 0.972165  [118400/175341]\n",
      "loss: 0.244308  [120000/175341]\n",
      "loss: 0.305222  [121600/175341]\n",
      "loss: 0.558700  [123200/175341]\n",
      "loss: 0.655291  [124800/175341]\n",
      "loss: 0.348405  [126400/175341]\n",
      "loss: 0.397592  [128000/175341]\n",
      "loss: 0.213207  [129600/175341]\n",
      "loss: 0.868146  [131200/175341]\n",
      "loss: 0.473314  [132800/175341]\n",
      "loss: 0.384857  [134400/175341]\n",
      "loss: 0.299920  [136000/175341]\n",
      "loss: 0.415465  [137600/175341]\n",
      "loss: 0.621956  [139200/175341]\n",
      "loss: 0.328446  [140800/175341]\n",
      "loss: 0.493536  [142400/175341]\n",
      "loss: 0.435021  [144000/175341]\n",
      "loss: 0.504723  [145600/175341]\n",
      "loss: 0.495123  [147200/175341]\n",
      "loss: 0.601988  [148800/175341]\n",
      "loss: 0.465980  [150400/175341]\n",
      "loss: 0.811011  [152000/175341]\n",
      "loss: 0.303118  [153600/175341]\n",
      "loss: 0.464503  [155200/175341]\n",
      "loss: 0.111388  [156800/175341]\n",
      "loss: 0.321344  [158400/175341]\n",
      "loss: 0.570164  [160000/175341]\n",
      "loss: 0.516612  [161600/175341]\n",
      "loss: 0.799635  [163200/175341]\n",
      "loss: 0.282740  [164800/175341]\n",
      "loss: 0.423280  [166400/175341]\n",
      "loss: 0.497646  [168000/175341]\n",
      "loss: 0.420074  [169600/175341]\n",
      "loss: 0.463783  [171200/175341]\n",
      "loss: 0.501947  [172800/175341]\n",
      "loss: 0.134556  [174400/175341]\n",
      "Train Accuracy: 81.6757%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.536591, F1-score: 76.22%, Macro_F1-Score:  42.76%  \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.663892  [    0/175341]\n",
      "loss: 0.484583  [ 1600/175341]\n",
      "loss: 0.615210  [ 3200/175341]\n",
      "loss: 0.814124  [ 4800/175341]\n",
      "loss: 0.424042  [ 6400/175341]\n",
      "loss: 0.330290  [ 8000/175341]\n",
      "loss: 0.277555  [ 9600/175341]\n",
      "loss: 0.596984  [11200/175341]\n",
      "loss: 0.513633  [12800/175341]\n",
      "loss: 0.908312  [14400/175341]\n",
      "loss: 0.406876  [16000/175341]\n",
      "loss: 0.440476  [17600/175341]\n",
      "loss: 0.971255  [19200/175341]\n",
      "loss: 0.436558  [20800/175341]\n",
      "loss: 0.324384  [22400/175341]\n",
      "loss: 0.567861  [24000/175341]\n",
      "loss: 0.423616  [25600/175341]\n",
      "loss: 0.447089  [27200/175341]\n",
      "loss: 0.293719  [28800/175341]\n",
      "loss: 0.683989  [30400/175341]\n",
      "loss: 0.454953  [32000/175341]\n",
      "loss: 0.458390  [33600/175341]\n",
      "loss: 0.668960  [35200/175341]\n",
      "loss: 0.528463  [36800/175341]\n",
      "loss: 0.416432  [38400/175341]\n",
      "loss: 0.924371  [40000/175341]\n",
      "loss: 0.342448  [41600/175341]\n",
      "loss: 0.580235  [43200/175341]\n",
      "loss: 0.252767  [44800/175341]\n",
      "loss: 0.620516  [46400/175341]\n",
      "loss: 0.172428  [48000/175341]\n",
      "loss: 0.410883  [49600/175341]\n",
      "loss: 0.363082  [51200/175341]\n",
      "loss: 0.371359  [52800/175341]\n",
      "loss: 0.370573  [54400/175341]\n",
      "loss: 0.776885  [56000/175341]\n",
      "loss: 0.564552  [57600/175341]\n",
      "loss: 1.042728  [59200/175341]\n",
      "loss: 0.243391  [60800/175341]\n",
      "loss: 0.748598  [62400/175341]\n",
      "loss: 0.407762  [64000/175341]\n",
      "loss: 0.381416  [65600/175341]\n",
      "loss: 0.233370  [67200/175341]\n",
      "loss: 0.850287  [68800/175341]\n",
      "loss: 0.827497  [70400/175341]\n",
      "loss: 0.170275  [72000/175341]\n",
      "loss: 0.671155  [73600/175341]\n",
      "loss: 0.427169  [75200/175341]\n",
      "loss: 0.238158  [76800/175341]\n",
      "loss: 0.538414  [78400/175341]\n",
      "loss: 0.252203  [80000/175341]\n",
      "loss: 0.440321  [81600/175341]\n",
      "loss: 0.514620  [83200/175341]\n",
      "loss: 0.427022  [84800/175341]\n",
      "loss: 0.920605  [86400/175341]\n",
      "loss: 0.530951  [88000/175341]\n",
      "loss: 0.275940  [89600/175341]\n",
      "loss: 0.998203  [91200/175341]\n",
      "loss: 0.213461  [92800/175341]\n",
      "loss: 0.594197  [94400/175341]\n",
      "loss: 0.868323  [96000/175341]\n",
      "loss: 0.721059  [97600/175341]\n",
      "loss: 0.250823  [99200/175341]\n",
      "loss: 0.445684  [100800/175341]\n",
      "loss: 0.333947  [102400/175341]\n",
      "loss: 0.637258  [104000/175341]\n",
      "loss: 0.648685  [105600/175341]\n",
      "loss: 0.373421  [107200/175341]\n",
      "loss: 0.195335  [108800/175341]\n",
      "loss: 0.593261  [110400/175341]\n",
      "loss: 0.320430  [112000/175341]\n",
      "loss: 0.439386  [113600/175341]\n",
      "loss: 0.370289  [115200/175341]\n",
      "loss: 0.845589  [116800/175341]\n",
      "loss: 0.704224  [118400/175341]\n",
      "loss: 0.600639  [120000/175341]\n",
      "loss: 0.541044  [121600/175341]\n",
      "loss: 0.321304  [123200/175341]\n",
      "loss: 0.327508  [124800/175341]\n",
      "loss: 0.160724  [126400/175341]\n",
      "loss: 0.576304  [128000/175341]\n",
      "loss: 0.208372  [129600/175341]\n",
      "loss: 0.508929  [131200/175341]\n",
      "loss: 0.736472  [132800/175341]\n",
      "loss: 0.434394  [134400/175341]\n",
      "loss: 0.364007  [136000/175341]\n",
      "loss: 0.376385  [137600/175341]\n",
      "loss: 0.599907  [139200/175341]\n",
      "loss: 0.193809  [140800/175341]\n",
      "loss: 0.463481  [142400/175341]\n",
      "loss: 0.158348  [144000/175341]\n",
      "loss: 0.400679  [145600/175341]\n",
      "loss: 0.628263  [147200/175341]\n",
      "loss: 0.270869  [148800/175341]\n",
      "loss: 0.747038  [150400/175341]\n",
      "loss: 0.761483  [152000/175341]\n",
      "loss: 0.146549  [153600/175341]\n",
      "loss: 0.820960  [155200/175341]\n",
      "loss: 0.494695  [156800/175341]\n",
      "loss: 0.732861  [158400/175341]\n",
      "loss: 0.373390  [160000/175341]\n",
      "loss: 0.190339  [161600/175341]\n",
      "loss: 0.645158  [163200/175341]\n",
      "loss: 0.192352  [164800/175341]\n",
      "loss: 0.531079  [166400/175341]\n",
      "loss: 0.358768  [168000/175341]\n",
      "loss: 0.385985  [169600/175341]\n",
      "loss: 0.365566  [171200/175341]\n",
      "loss: 0.364320  [172800/175341]\n",
      "loss: 0.170192  [174400/175341]\n",
      "Train Accuracy: 81.7008%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.527651, F1-score: 77.20%, Macro_F1-Score:  43.77%  \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.433645  [    0/175341]\n",
      "loss: 0.460594  [ 1600/175341]\n",
      "loss: 0.120283  [ 3200/175341]\n",
      "loss: 0.295074  [ 4800/175341]\n",
      "loss: 0.905036  [ 6400/175341]\n",
      "loss: 0.943932  [ 8000/175341]\n",
      "loss: 0.413120  [ 9600/175341]\n",
      "loss: 0.533531  [11200/175341]\n",
      "loss: 0.529696  [12800/175341]\n",
      "loss: 0.195196  [14400/175341]\n",
      "loss: 0.202016  [16000/175341]\n",
      "loss: 0.291026  [17600/175341]\n",
      "loss: 0.772422  [19200/175341]\n",
      "loss: 0.409064  [20800/175341]\n",
      "loss: 0.148563  [22400/175341]\n",
      "loss: 0.766306  [24000/175341]\n",
      "loss: 0.381394  [25600/175341]\n",
      "loss: 0.716700  [27200/175341]\n",
      "loss: 0.430769  [28800/175341]\n",
      "loss: 0.539312  [30400/175341]\n",
      "loss: 0.599998  [32000/175341]\n",
      "loss: 0.766561  [33600/175341]\n",
      "loss: 0.309051  [35200/175341]\n",
      "loss: 0.315923  [36800/175341]\n",
      "loss: 0.238265  [38400/175341]\n",
      "loss: 0.312267  [40000/175341]\n",
      "loss: 0.273617  [41600/175341]\n",
      "loss: 0.124926  [43200/175341]\n",
      "loss: 0.519102  [44800/175341]\n",
      "loss: 0.340987  [46400/175341]\n",
      "loss: 0.288380  [48000/175341]\n",
      "loss: 0.317721  [49600/175341]\n",
      "loss: 0.161692  [51200/175341]\n",
      "loss: 0.424586  [52800/175341]\n",
      "loss: 0.689288  [54400/175341]\n",
      "loss: 0.493269  [56000/175341]\n",
      "loss: 0.267197  [57600/175341]\n",
      "loss: 0.231773  [59200/175341]\n",
      "loss: 0.747852  [60800/175341]\n",
      "loss: 0.465178  [62400/175341]\n",
      "loss: 0.680649  [64000/175341]\n",
      "loss: 0.758705  [65600/175341]\n",
      "loss: 0.630087  [67200/175341]\n",
      "loss: 0.298671  [68800/175341]\n",
      "loss: 0.363590  [70400/175341]\n",
      "loss: 0.573153  [72000/175341]\n",
      "loss: 0.956336  [73600/175341]\n",
      "loss: 0.359252  [75200/175341]\n",
      "loss: 0.454518  [76800/175341]\n",
      "loss: 0.328395  [78400/175341]\n",
      "loss: 0.308518  [80000/175341]\n",
      "loss: 0.273361  [81600/175341]\n",
      "loss: 0.442408  [83200/175341]\n",
      "loss: 0.253974  [84800/175341]\n",
      "loss: 0.239700  [86400/175341]\n",
      "loss: 0.528413  [88000/175341]\n",
      "loss: 0.378724  [89600/175341]\n",
      "loss: 0.142870  [91200/175341]\n",
      "loss: 0.108899  [92800/175341]\n",
      "loss: 0.368144  [94400/175341]\n",
      "loss: 0.232659  [96000/175341]\n",
      "loss: 0.205470  [97600/175341]\n",
      "loss: 0.470573  [99200/175341]\n",
      "loss: 0.425591  [100800/175341]\n",
      "loss: 0.787568  [102400/175341]\n",
      "loss: 0.557570  [104000/175341]\n",
      "loss: 0.443847  [105600/175341]\n",
      "loss: 0.349455  [107200/175341]\n",
      "loss: 0.343569  [108800/175341]\n",
      "loss: 0.140172  [110400/175341]\n",
      "loss: 0.516199  [112000/175341]\n",
      "loss: 0.476864  [113600/175341]\n",
      "loss: 0.151420  [115200/175341]\n",
      "loss: 0.455359  [116800/175341]\n",
      "loss: 0.299791  [118400/175341]\n",
      "loss: 0.227094  [120000/175341]\n",
      "loss: 0.522792  [121600/175341]\n",
      "loss: 0.425157  [123200/175341]\n",
      "loss: 0.489273  [124800/175341]\n",
      "loss: 0.309141  [126400/175341]\n",
      "loss: 0.453872  [128000/175341]\n",
      "loss: 0.533761  [129600/175341]\n",
      "loss: 0.630316  [131200/175341]\n",
      "loss: 0.340219  [132800/175341]\n",
      "loss: 0.587505  [134400/175341]\n",
      "loss: 0.272763  [136000/175341]\n",
      "loss: 0.413136  [137600/175341]\n",
      "loss: 0.424767  [139200/175341]\n",
      "loss: 0.443934  [140800/175341]\n",
      "loss: 0.685450  [142400/175341]\n",
      "loss: 0.483419  [144000/175341]\n",
      "loss: 0.362741  [145600/175341]\n",
      "loss: 0.709610  [147200/175341]\n",
      "loss: 0.323169  [148800/175341]\n",
      "loss: 0.707202  [150400/175341]\n",
      "loss: 0.682127  [152000/175341]\n",
      "loss: 0.666039  [153600/175341]\n",
      "loss: 0.398437  [155200/175341]\n",
      "loss: 0.717512  [156800/175341]\n",
      "loss: 0.883532  [158400/175341]\n",
      "loss: 0.304466  [160000/175341]\n",
      "loss: 0.357238  [161600/175341]\n",
      "loss: 0.805068  [163200/175341]\n",
      "loss: 0.255361  [164800/175341]\n",
      "loss: 0.224637  [166400/175341]\n",
      "loss: 0.153192  [168000/175341]\n",
      "loss: 0.454640  [169600/175341]\n",
      "loss: 0.296759  [171200/175341]\n",
      "loss: 0.306642  [172800/175341]\n",
      "loss: 0.264703  [174400/175341]\n",
      "Train Accuracy: 81.7339%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.540338, F1-score: 76.90%, Macro_F1-Score:  43.89%  \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.255869  [    0/175341]\n",
      "loss: 0.589877  [ 1600/175341]\n",
      "loss: 0.324443  [ 3200/175341]\n",
      "loss: 0.403068  [ 4800/175341]\n",
      "loss: 0.294569  [ 6400/175341]\n",
      "loss: 0.664372  [ 8000/175341]\n",
      "loss: 0.524239  [ 9600/175341]\n",
      "loss: 0.366931  [11200/175341]\n",
      "loss: 0.377857  [12800/175341]\n",
      "loss: 0.555026  [14400/175341]\n",
      "loss: 0.296775  [16000/175341]\n",
      "loss: 0.575198  [17600/175341]\n",
      "loss: 0.412290  [19200/175341]\n",
      "loss: 0.242811  [20800/175341]\n",
      "loss: 0.254001  [22400/175341]\n",
      "loss: 0.666530  [24000/175341]\n",
      "loss: 0.314012  [25600/175341]\n",
      "loss: 0.614259  [27200/175341]\n",
      "loss: 0.509538  [28800/175341]\n",
      "loss: 0.337772  [30400/175341]\n",
      "loss: 0.690695  [32000/175341]\n",
      "loss: 0.218286  [33600/175341]\n",
      "loss: 0.231521  [35200/175341]\n",
      "loss: 0.300511  [36800/175341]\n",
      "loss: 0.519291  [38400/175341]\n",
      "loss: 0.349392  [40000/175341]\n",
      "loss: 0.473564  [41600/175341]\n",
      "loss: 0.545021  [43200/175341]\n",
      "loss: 0.864986  [44800/175341]\n",
      "loss: 0.204314  [46400/175341]\n",
      "loss: 0.224533  [48000/175341]\n",
      "loss: 0.548746  [49600/175341]\n",
      "loss: 0.382489  [51200/175341]\n",
      "loss: 0.401823  [52800/175341]\n",
      "loss: 0.446842  [54400/175341]\n",
      "loss: 0.440761  [56000/175341]\n",
      "loss: 0.284141  [57600/175341]\n",
      "loss: 1.121984  [59200/175341]\n",
      "loss: 0.520310  [60800/175341]\n",
      "loss: 0.440150  [62400/175341]\n",
      "loss: 0.359588  [64000/175341]\n",
      "loss: 0.522199  [65600/175341]\n",
      "loss: 0.544247  [67200/175341]\n",
      "loss: 0.913495  [68800/175341]\n",
      "loss: 0.500931  [70400/175341]\n",
      "loss: 0.474865  [72000/175341]\n",
      "loss: 0.068996  [73600/175341]\n",
      "loss: 0.396828  [75200/175341]\n",
      "loss: 0.245009  [76800/175341]\n",
      "loss: 0.402832  [78400/175341]\n",
      "loss: 0.232739  [80000/175341]\n",
      "loss: 0.608062  [81600/175341]\n",
      "loss: 0.714282  [83200/175341]\n",
      "loss: 0.213479  [84800/175341]\n",
      "loss: 0.534286  [86400/175341]\n",
      "loss: 0.343035  [88000/175341]\n",
      "loss: 0.530068  [89600/175341]\n",
      "loss: 0.208951  [91200/175341]\n",
      "loss: 0.809756  [92800/175341]\n",
      "loss: 0.787268  [94400/175341]\n",
      "loss: 0.484774  [96000/175341]\n",
      "loss: 0.516243  [97600/175341]\n",
      "loss: 0.265750  [99200/175341]\n",
      "loss: 0.706318  [100800/175341]\n",
      "loss: 0.218210  [102400/175341]\n",
      "loss: 0.229417  [104000/175341]\n",
      "loss: 0.210465  [105600/175341]\n",
      "loss: 0.446357  [107200/175341]\n",
      "loss: 0.273689  [108800/175341]\n",
      "loss: 0.095711  [110400/175341]\n",
      "loss: 0.531230  [112000/175341]\n",
      "loss: 0.545894  [113600/175341]\n",
      "loss: 0.923711  [115200/175341]\n",
      "loss: 0.390891  [116800/175341]\n",
      "loss: 0.345282  [118400/175341]\n",
      "loss: 0.392343  [120000/175341]\n",
      "loss: 0.449227  [121600/175341]\n",
      "loss: 0.465541  [123200/175341]\n",
      "loss: 0.282944  [124800/175341]\n",
      "loss: 0.407649  [126400/175341]\n",
      "loss: 0.338063  [128000/175341]\n",
      "loss: 0.625786  [129600/175341]\n",
      "loss: 0.254202  [131200/175341]\n",
      "loss: 0.618515  [132800/175341]\n",
      "loss: 0.256682  [134400/175341]\n",
      "loss: 0.171205  [136000/175341]\n",
      "loss: 0.641899  [137600/175341]\n",
      "loss: 0.152674  [139200/175341]\n",
      "loss: 0.541583  [140800/175341]\n",
      "loss: 0.398706  [142400/175341]\n",
      "loss: 0.388777  [144000/175341]\n",
      "loss: 0.447087  [145600/175341]\n",
      "loss: 0.473650  [147200/175341]\n",
      "loss: 0.175722  [148800/175341]\n",
      "loss: 0.489465  [150400/175341]\n",
      "loss: 0.569972  [152000/175341]\n",
      "loss: 0.140499  [153600/175341]\n",
      "loss: 0.345424  [155200/175341]\n",
      "loss: 0.592807  [156800/175341]\n",
      "loss: 0.789202  [158400/175341]\n",
      "loss: 0.289211  [160000/175341]\n",
      "loss: 0.213760  [161600/175341]\n",
      "loss: 0.593855  [163200/175341]\n",
      "loss: 0.742485  [164800/175341]\n",
      "loss: 0.286781  [166400/175341]\n",
      "loss: 0.294891  [168000/175341]\n",
      "loss: 0.574305  [169600/175341]\n",
      "loss: 0.663786  [171200/175341]\n",
      "loss: 0.257001  [172800/175341]\n",
      "loss: 0.225451  [174400/175341]\n",
      "Train Accuracy: 81.7139%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.540349, F1-score: 76.10%, Macro_F1-Score:  43.59%  \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.223180  [    0/175341]\n",
      "loss: 0.693591  [ 1600/175341]\n",
      "loss: 0.518983  [ 3200/175341]\n",
      "loss: 0.366145  [ 4800/175341]\n",
      "loss: 0.373784  [ 6400/175341]\n",
      "loss: 0.398740  [ 8000/175341]\n",
      "loss: 0.870773  [ 9600/175341]\n",
      "loss: 0.773807  [11200/175341]\n",
      "loss: 0.270033  [12800/175341]\n",
      "loss: 0.420143  [14400/175341]\n",
      "loss: 0.605255  [16000/175341]\n",
      "loss: 1.161387  [17600/175341]\n",
      "loss: 0.725303  [19200/175341]\n",
      "loss: 0.424742  [20800/175341]\n",
      "loss: 0.123690  [22400/175341]\n",
      "loss: 0.368385  [24000/175341]\n",
      "loss: 0.268225  [25600/175341]\n",
      "loss: 0.183464  [27200/175341]\n",
      "loss: 0.362540  [28800/175341]\n",
      "loss: 0.305484  [30400/175341]\n",
      "loss: 0.607240  [32000/175341]\n",
      "loss: 0.254425  [33600/175341]\n",
      "loss: 0.557988  [35200/175341]\n",
      "loss: 0.332612  [36800/175341]\n",
      "loss: 0.457574  [38400/175341]\n",
      "loss: 0.295913  [40000/175341]\n",
      "loss: 0.469969  [41600/175341]\n",
      "loss: 0.144983  [43200/175341]\n",
      "loss: 0.157712  [44800/175341]\n",
      "loss: 0.381047  [46400/175341]\n",
      "loss: 0.366869  [48000/175341]\n",
      "loss: 0.447539  [49600/175341]\n",
      "loss: 0.555000  [51200/175341]\n",
      "loss: 0.657519  [52800/175341]\n",
      "loss: 0.411408  [54400/175341]\n",
      "loss: 0.314224  [56000/175341]\n",
      "loss: 1.061606  [57600/175341]\n",
      "loss: 0.369799  [59200/175341]\n",
      "loss: 0.743166  [60800/175341]\n",
      "loss: 0.557161  [62400/175341]\n",
      "loss: 0.471470  [64000/175341]\n",
      "loss: 0.664289  [65600/175341]\n",
      "loss: 0.406745  [67200/175341]\n",
      "loss: 0.732185  [68800/175341]\n",
      "loss: 0.672337  [70400/175341]\n",
      "loss: 0.482155  [72000/175341]\n",
      "loss: 0.737228  [73600/175341]\n",
      "loss: 0.589507  [75200/175341]\n",
      "loss: 0.519308  [76800/175341]\n",
      "loss: 0.584821  [78400/175341]\n",
      "loss: 0.154774  [80000/175341]\n",
      "loss: 0.316175  [81600/175341]\n",
      "loss: 0.532259  [83200/175341]\n",
      "loss: 0.365236  [84800/175341]\n",
      "loss: 0.681922  [86400/175341]\n",
      "loss: 0.633564  [88000/175341]\n",
      "loss: 0.533917  [89600/175341]\n",
      "loss: 0.328522  [91200/175341]\n",
      "loss: 0.331079  [92800/175341]\n",
      "loss: 0.376382  [94400/175341]\n",
      "loss: 0.133363  [96000/175341]\n",
      "loss: 0.377921  [97600/175341]\n",
      "loss: 0.135799  [99200/175341]\n",
      "loss: 0.160624  [100800/175341]\n",
      "loss: 0.353499  [102400/175341]\n",
      "loss: 0.419075  [104000/175341]\n",
      "loss: 0.382435  [105600/175341]\n",
      "loss: 0.335397  [107200/175341]\n",
      "loss: 0.463560  [108800/175341]\n",
      "loss: 0.501609  [110400/175341]\n",
      "loss: 0.865437  [112000/175341]\n",
      "loss: 0.338084  [113600/175341]\n",
      "loss: 0.608160  [115200/175341]\n",
      "loss: 0.289495  [116800/175341]\n",
      "loss: 0.395586  [118400/175341]\n",
      "loss: 0.529518  [120000/175341]\n",
      "loss: 0.337991  [121600/175341]\n",
      "loss: 0.332779  [123200/175341]\n",
      "loss: 0.385040  [124800/175341]\n",
      "loss: 0.395911  [126400/175341]\n",
      "loss: 0.233301  [128000/175341]\n",
      "loss: 0.368007  [129600/175341]\n",
      "loss: 0.439287  [131200/175341]\n",
      "loss: 0.433603  [132800/175341]\n",
      "loss: 0.364180  [134400/175341]\n",
      "loss: 0.277778  [136000/175341]\n",
      "loss: 0.302405  [137600/175341]\n",
      "loss: 0.264042  [139200/175341]\n",
      "loss: 0.380191  [140800/175341]\n",
      "loss: 0.494182  [142400/175341]\n",
      "loss: 0.299517  [144000/175341]\n",
      "loss: 0.509648  [145600/175341]\n",
      "loss: 0.820771  [147200/175341]\n",
      "loss: 0.720030  [148800/175341]\n",
      "loss: 0.461647  [150400/175341]\n",
      "loss: 0.242252  [152000/175341]\n",
      "loss: 0.688637  [153600/175341]\n",
      "loss: 0.470580  [155200/175341]\n",
      "loss: 0.324182  [156800/175341]\n",
      "loss: 0.551275  [158400/175341]\n",
      "loss: 0.415422  [160000/175341]\n",
      "loss: 0.634160  [161600/175341]\n",
      "loss: 0.446975  [163200/175341]\n",
      "loss: 0.612234  [164800/175341]\n",
      "loss: 0.450498  [166400/175341]\n",
      "loss: 0.436540  [168000/175341]\n",
      "loss: 0.682635  [169600/175341]\n",
      "loss: 0.691854  [171200/175341]\n",
      "loss: 0.338697  [172800/175341]\n",
      "loss: 0.228473  [174400/175341]\n",
      "Train Accuracy: 81.7841%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.545197, F1-score: 75.93%, Macro_F1-Score:  42.50%  \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.607710  [    0/175341]\n",
      "loss: 0.551679  [ 1600/175341]\n",
      "loss: 0.410887  [ 3200/175341]\n",
      "loss: 0.754780  [ 4800/175341]\n",
      "loss: 0.142646  [ 6400/175341]\n",
      "loss: 0.326659  [ 8000/175341]\n",
      "loss: 0.409503  [ 9600/175341]\n",
      "loss: 0.015035  [11200/175341]\n",
      "loss: 0.244612  [12800/175341]\n",
      "loss: 0.747186  [14400/175341]\n",
      "loss: 0.453415  [16000/175341]\n",
      "loss: 0.384665  [17600/175341]\n",
      "loss: 0.369435  [19200/175341]\n",
      "loss: 0.183800  [20800/175341]\n",
      "loss: 0.649805  [22400/175341]\n",
      "loss: 0.611611  [24000/175341]\n",
      "loss: 0.389273  [25600/175341]\n",
      "loss: 0.221426  [27200/175341]\n",
      "loss: 0.712016  [28800/175341]\n",
      "loss: 0.676764  [30400/175341]\n",
      "loss: 0.100374  [32000/175341]\n",
      "loss: 0.500506  [33600/175341]\n",
      "loss: 0.406688  [35200/175341]\n",
      "loss: 0.372808  [36800/175341]\n",
      "loss: 0.510236  [38400/175341]\n",
      "loss: 0.375295  [40000/175341]\n",
      "loss: 0.451053  [41600/175341]\n",
      "loss: 0.465731  [43200/175341]\n",
      "loss: 0.209443  [44800/175341]\n",
      "loss: 0.440906  [46400/175341]\n",
      "loss: 0.402655  [48000/175341]\n",
      "loss: 0.268858  [49600/175341]\n",
      "loss: 0.305648  [51200/175341]\n",
      "loss: 0.343223  [52800/175341]\n",
      "loss: 0.516291  [54400/175341]\n",
      "loss: 0.272477  [56000/175341]\n",
      "loss: 0.405309  [57600/175341]\n",
      "loss: 0.300556  [59200/175341]\n",
      "loss: 0.708640  [60800/175341]\n",
      "loss: 0.538721  [62400/175341]\n",
      "loss: 0.341759  [64000/175341]\n",
      "loss: 0.506929  [65600/175341]\n",
      "loss: 0.508260  [67200/175341]\n",
      "loss: 0.496703  [68800/175341]\n",
      "loss: 0.542199  [70400/175341]\n",
      "loss: 0.313337  [72000/175341]\n",
      "loss: 0.547713  [73600/175341]\n",
      "loss: 0.376247  [75200/175341]\n",
      "loss: 0.536501  [76800/175341]\n",
      "loss: 0.398037  [78400/175341]\n",
      "loss: 0.498477  [80000/175341]\n",
      "loss: 0.572687  [81600/175341]\n",
      "loss: 0.474825  [83200/175341]\n",
      "loss: 0.477790  [84800/175341]\n",
      "loss: 0.475122  [86400/175341]\n",
      "loss: 0.533338  [88000/175341]\n",
      "loss: 0.861598  [89600/175341]\n",
      "loss: 0.812993  [91200/175341]\n",
      "loss: 0.142938  [92800/175341]\n",
      "loss: 0.412860  [94400/175341]\n",
      "loss: 0.877393  [96000/175341]\n",
      "loss: 0.112355  [97600/175341]\n",
      "loss: 0.140060  [99200/175341]\n",
      "loss: 0.141834  [100800/175341]\n",
      "loss: 0.592392  [102400/175341]\n",
      "loss: 0.417226  [104000/175341]\n",
      "loss: 0.427340  [105600/175341]\n",
      "loss: 0.384235  [107200/175341]\n",
      "loss: 1.035598  [108800/175341]\n",
      "loss: 0.692824  [110400/175341]\n",
      "loss: 0.383524  [112000/175341]\n",
      "loss: 0.171156  [113600/175341]\n",
      "loss: 0.374721  [115200/175341]\n",
      "loss: 0.665168  [116800/175341]\n",
      "loss: 0.160026  [118400/175341]\n",
      "loss: 0.603517  [120000/175341]\n",
      "loss: 1.011950  [121600/175341]\n",
      "loss: 0.266806  [123200/175341]\n",
      "loss: 0.680016  [124800/175341]\n",
      "loss: 0.350193  [126400/175341]\n",
      "loss: 0.391907  [128000/175341]\n",
      "loss: 0.306431  [129600/175341]\n",
      "loss: 0.377821  [131200/175341]\n",
      "loss: 0.201325  [132800/175341]\n",
      "loss: 0.272822  [134400/175341]\n",
      "loss: 0.591197  [136000/175341]\n",
      "loss: 0.579086  [137600/175341]\n",
      "loss: 0.484932  [139200/175341]\n",
      "loss: 0.830863  [140800/175341]\n",
      "loss: 0.446896  [142400/175341]\n",
      "loss: 0.513259  [144000/175341]\n",
      "loss: 0.990274  [145600/175341]\n",
      "loss: 0.290328  [147200/175341]\n",
      "loss: 0.618429  [148800/175341]\n",
      "loss: 0.283245  [150400/175341]\n",
      "loss: 0.721244  [152000/175341]\n",
      "loss: 0.451303  [153600/175341]\n",
      "loss: 0.882670  [155200/175341]\n",
      "loss: 0.406637  [156800/175341]\n",
      "loss: 0.729887  [158400/175341]\n",
      "loss: 0.551123  [160000/175341]\n",
      "loss: 0.869706  [161600/175341]\n",
      "loss: 0.548658  [163200/175341]\n",
      "loss: 0.520440  [164800/175341]\n",
      "loss: 0.397061  [166400/175341]\n",
      "loss: 0.511134  [168000/175341]\n",
      "loss: 0.382072  [169600/175341]\n",
      "loss: 0.228471  [171200/175341]\n",
      "loss: 0.449422  [172800/175341]\n",
      "loss: 0.252285  [174400/175341]\n",
      "Train Accuracy: 81.7875%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.527524, F1-score: 76.27%, Macro_F1-Score:  43.36%  \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.239532  [    0/175341]\n",
      "loss: 0.982408  [ 1600/175341]\n",
      "loss: 0.618617  [ 3200/175341]\n",
      "loss: 0.251103  [ 4800/175341]\n",
      "loss: 0.597186  [ 6400/175341]\n",
      "loss: 0.309571  [ 8000/175341]\n",
      "loss: 0.755368  [ 9600/175341]\n",
      "loss: 0.562131  [11200/175341]\n",
      "loss: 0.285300  [12800/175341]\n",
      "loss: 0.406431  [14400/175341]\n",
      "loss: 0.375325  [16000/175341]\n",
      "loss: 0.552107  [17600/175341]\n",
      "loss: 0.527523  [19200/175341]\n",
      "loss: 0.588167  [20800/175341]\n",
      "loss: 0.771604  [22400/175341]\n",
      "loss: 0.555042  [24000/175341]\n",
      "loss: 0.371323  [25600/175341]\n",
      "loss: 0.768677  [27200/175341]\n",
      "loss: 0.246302  [28800/175341]\n",
      "loss: 0.413341  [30400/175341]\n",
      "loss: 0.407614  [32000/175341]\n",
      "loss: 0.596322  [33600/175341]\n",
      "loss: 0.664348  [35200/175341]\n",
      "loss: 0.373145  [36800/175341]\n",
      "loss: 0.615498  [38400/175341]\n",
      "loss: 0.546464  [40000/175341]\n",
      "loss: 0.426117  [41600/175341]\n",
      "loss: 0.135277  [43200/175341]\n",
      "loss: 0.505749  [44800/175341]\n",
      "loss: 0.439435  [46400/175341]\n",
      "loss: 0.748415  [48000/175341]\n",
      "loss: 0.405084  [49600/175341]\n",
      "loss: 0.258492  [51200/175341]\n",
      "loss: 0.787699  [52800/175341]\n",
      "loss: 0.294240  [54400/175341]\n",
      "loss: 0.391217  [56000/175341]\n",
      "loss: 0.190868  [57600/175341]\n",
      "loss: 0.379484  [59200/175341]\n",
      "loss: 0.383370  [60800/175341]\n",
      "loss: 0.448238  [62400/175341]\n",
      "loss: 0.720569  [64000/175341]\n",
      "loss: 0.286114  [65600/175341]\n",
      "loss: 0.340990  [67200/175341]\n",
      "loss: 0.271330  [68800/175341]\n",
      "loss: 0.316898  [70400/175341]\n",
      "loss: 0.523941  [72000/175341]\n",
      "loss: 0.651257  [73600/175341]\n",
      "loss: 0.272397  [75200/175341]\n",
      "loss: 0.355806  [76800/175341]\n",
      "loss: 0.510886  [78400/175341]\n",
      "loss: 0.582859  [80000/175341]\n",
      "loss: 0.672904  [81600/175341]\n",
      "loss: 0.291632  [83200/175341]\n",
      "loss: 0.466826  [84800/175341]\n",
      "loss: 0.550425  [86400/175341]\n",
      "loss: 0.311123  [88000/175341]\n",
      "loss: 0.368175  [89600/175341]\n",
      "loss: 0.674384  [91200/175341]\n",
      "loss: 0.539753  [92800/175341]\n",
      "loss: 0.205984  [94400/175341]\n",
      "loss: 0.579855  [96000/175341]\n",
      "loss: 0.276126  [97600/175341]\n",
      "loss: 0.798465  [99200/175341]\n",
      "loss: 0.661254  [100800/175341]\n",
      "loss: 0.080255  [102400/175341]\n",
      "loss: 0.657584  [104000/175341]\n",
      "loss: 0.268712  [105600/175341]\n",
      "loss: 0.388064  [107200/175341]\n",
      "loss: 0.329617  [108800/175341]\n",
      "loss: 0.415924  [110400/175341]\n",
      "loss: 0.131733  [112000/175341]\n",
      "loss: 0.629157  [113600/175341]\n",
      "loss: 0.590083  [115200/175341]\n",
      "loss: 0.169182  [116800/175341]\n",
      "loss: 0.307735  [118400/175341]\n",
      "loss: 0.688475  [120000/175341]\n",
      "loss: 0.745139  [121600/175341]\n",
      "loss: 0.260912  [123200/175341]\n",
      "loss: 0.399324  [124800/175341]\n",
      "loss: 0.336282  [126400/175341]\n",
      "loss: 0.390035  [128000/175341]\n",
      "loss: 0.618542  [129600/175341]\n",
      "loss: 0.422552  [131200/175341]\n",
      "loss: 0.386509  [132800/175341]\n",
      "loss: 0.310743  [134400/175341]\n",
      "loss: 0.461101  [136000/175341]\n",
      "loss: 0.383628  [137600/175341]\n",
      "loss: 0.253597  [139200/175341]\n",
      "loss: 0.426360  [140800/175341]\n",
      "loss: 0.357911  [142400/175341]\n",
      "loss: 0.351622  [144000/175341]\n",
      "loss: 0.466901  [145600/175341]\n",
      "loss: 0.219822  [147200/175341]\n",
      "loss: 0.166363  [148800/175341]\n",
      "loss: 0.578063  [150400/175341]\n",
      "loss: 0.279068  [152000/175341]\n",
      "loss: 0.819863  [153600/175341]\n",
      "loss: 0.269107  [155200/175341]\n",
      "loss: 0.200593  [156800/175341]\n",
      "loss: 0.772649  [158400/175341]\n",
      "loss: 0.219232  [160000/175341]\n",
      "loss: 0.763492  [161600/175341]\n",
      "loss: 0.894687  [163200/175341]\n",
      "loss: 0.164555  [164800/175341]\n",
      "loss: 0.682230  [166400/175341]\n",
      "loss: 0.196440  [168000/175341]\n",
      "loss: 0.601054  [169600/175341]\n",
      "loss: 0.323641  [171200/175341]\n",
      "loss: 0.713246  [172800/175341]\n",
      "loss: 0.869471  [174400/175341]\n",
      "Train Accuracy: 81.7949%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.550053, F1-score: 75.54%, Macro_F1-Score:  42.37%  \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.591918  [    0/175341]\n",
      "loss: 0.320882  [ 1600/175341]\n",
      "loss: 0.693070  [ 3200/175341]\n",
      "loss: 0.239461  [ 4800/175341]\n",
      "loss: 0.453412  [ 6400/175341]\n",
      "loss: 0.738822  [ 8000/175341]\n",
      "loss: 0.927189  [ 9600/175341]\n",
      "loss: 0.316799  [11200/175341]\n",
      "loss: 0.346821  [12800/175341]\n",
      "loss: 0.927853  [14400/175341]\n",
      "loss: 0.511099  [16000/175341]\n",
      "loss: 0.502488  [17600/175341]\n",
      "loss: 0.185551  [19200/175341]\n",
      "loss: 0.394645  [20800/175341]\n",
      "loss: 0.385866  [22400/175341]\n",
      "loss: 0.636536  [24000/175341]\n",
      "loss: 0.267137  [25600/175341]\n",
      "loss: 0.879070  [27200/175341]\n",
      "loss: 0.558086  [28800/175341]\n",
      "loss: 0.479753  [30400/175341]\n",
      "loss: 0.515796  [32000/175341]\n",
      "loss: 0.506517  [33600/175341]\n",
      "loss: 1.403902  [35200/175341]\n",
      "loss: 0.460947  [36800/175341]\n",
      "loss: 0.427723  [38400/175341]\n",
      "loss: 0.432546  [40000/175341]\n",
      "loss: 0.380607  [41600/175341]\n",
      "loss: 0.485606  [43200/175341]\n",
      "loss: 0.407857  [44800/175341]\n",
      "loss: 0.503675  [46400/175341]\n",
      "loss: 0.329325  [48000/175341]\n",
      "loss: 0.993238  [49600/175341]\n",
      "loss: 0.569929  [51200/175341]\n",
      "loss: 0.552098  [52800/175341]\n",
      "loss: 0.248683  [54400/175341]\n",
      "loss: 0.189554  [56000/175341]\n",
      "loss: 0.350848  [57600/175341]\n",
      "loss: 0.291772  [59200/175341]\n",
      "loss: 0.320931  [60800/175341]\n",
      "loss: 0.256208  [62400/175341]\n",
      "loss: 0.495935  [64000/175341]\n",
      "loss: 0.766780  [65600/175341]\n",
      "loss: 0.354685  [67200/175341]\n",
      "loss: 0.162272  [68800/175341]\n",
      "loss: 0.253771  [70400/175341]\n",
      "loss: 0.335507  [72000/175341]\n",
      "loss: 0.347074  [73600/175341]\n",
      "loss: 0.603755  [75200/175341]\n",
      "loss: 0.236873  [76800/175341]\n",
      "loss: 0.334101  [78400/175341]\n",
      "loss: 0.617397  [80000/175341]\n",
      "loss: 0.416241  [81600/175341]\n",
      "loss: 0.641546  [83200/175341]\n",
      "loss: 0.814142  [84800/175341]\n",
      "loss: 0.445454  [86400/175341]\n",
      "loss: 0.267896  [88000/175341]\n",
      "loss: 0.355621  [89600/175341]\n",
      "loss: 0.703433  [91200/175341]\n",
      "loss: 0.290605  [92800/175341]\n",
      "loss: 0.806034  [94400/175341]\n",
      "loss: 0.184143  [96000/175341]\n",
      "loss: 0.370090  [97600/175341]\n",
      "loss: 0.353686  [99200/175341]\n",
      "loss: 0.212216  [100800/175341]\n",
      "loss: 0.186483  [102400/175341]\n",
      "loss: 0.942290  [104000/175341]\n",
      "loss: 0.288159  [105600/175341]\n",
      "loss: 0.375629  [107200/175341]\n",
      "loss: 0.443421  [108800/175341]\n",
      "loss: 0.383356  [110400/175341]\n",
      "loss: 0.528446  [112000/175341]\n",
      "loss: 0.143163  [113600/175341]\n",
      "loss: 0.638341  [115200/175341]\n",
      "loss: 0.597812  [116800/175341]\n",
      "loss: 0.572217  [118400/175341]\n",
      "loss: 0.609534  [120000/175341]\n",
      "loss: 0.443950  [121600/175341]\n",
      "loss: 0.422267  [123200/175341]\n",
      "loss: 0.562719  [124800/175341]\n",
      "loss: 0.377080  [126400/175341]\n",
      "loss: 0.570069  [128000/175341]\n",
      "loss: 0.547885  [129600/175341]\n",
      "loss: 0.402287  [131200/175341]\n",
      "loss: 0.515697  [132800/175341]\n",
      "loss: 0.596253  [134400/175341]\n",
      "loss: 0.323695  [136000/175341]\n",
      "loss: 0.796890  [137600/175341]\n",
      "loss: 0.665973  [139200/175341]\n",
      "loss: 0.510727  [140800/175341]\n",
      "loss: 0.246980  [142400/175341]\n",
      "loss: 0.332255  [144000/175341]\n",
      "loss: 0.445991  [145600/175341]\n",
      "loss: 0.849461  [147200/175341]\n",
      "loss: 0.220809  [148800/175341]\n",
      "loss: 0.744804  [150400/175341]\n",
      "loss: 0.287878  [152000/175341]\n",
      "loss: 0.354046  [153600/175341]\n",
      "loss: 0.845957  [155200/175341]\n",
      "loss: 0.420837  [156800/175341]\n",
      "loss: 0.145699  [158400/175341]\n",
      "loss: 0.556711  [160000/175341]\n",
      "loss: 0.699446  [161600/175341]\n",
      "loss: 1.054376  [163200/175341]\n",
      "loss: 0.216049  [164800/175341]\n",
      "loss: 0.484720  [166400/175341]\n",
      "loss: 0.274255  [168000/175341]\n",
      "loss: 0.493025  [169600/175341]\n",
      "loss: 0.649275  [171200/175341]\n",
      "loss: 0.522549  [172800/175341]\n",
      "loss: 0.386237  [174400/175341]\n",
      "Train Accuracy: 81.8565%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#wider network\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 50\u001b[0m, in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m---> 50\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     52\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficient_kan\\kan.py:278\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x, update_grid)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_grid:\n\u001b[0;32m    277\u001b[0m         layer\u001b[38;5;241m.\u001b[39mupdate_grid(x)\n\u001b[1;32m--> 278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficient_kan\\kan.py:160\u001b[0m, in \u001b[0;36mKANLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features)\n\u001b[0;32m    158\u001b[0m base_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_activation(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_weight)\n\u001b[0;32m    159\u001b[0m spline_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_splines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_spline_weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m output \u001b[38;5;241m=\u001b[39m base_output \u001b[38;5;241m+\u001b[39m spline_output\n\u001b[0;32m    165\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39moriginal_shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficient_kan\\kan.py:93\u001b[0m, in \u001b[0;36mKANLinear.b_splines\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\n\u001b[0;32m     90\u001b[0m grid: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\n\u001b[0;32m     92\u001b[0m )  \u001b[38;5;66;03m# (in_features, grid_size + 2 * spline_order + 1)\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m bases \u001b[38;5;241m=\u001b[39m ((x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m grid[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m&\u001b[39m (x \u001b[38;5;241m<\u001b[39m grid[:, \u001b[38;5;241m1\u001b[39m:]))\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspline_order \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4) #using L2 regularization\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2362c3-f2fe-4d79-ab22-038e027752ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.5, scale_base=0.2, scale_spline=0.2)\n",
    "loss_over_train = []\n",
    "loss_over_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b427034-602d-4252-af52-9c7d0908f074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115f1b7-87f8-41a5-8992-af1752600f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add252-99f2-4d32-bb41-6e558870ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise scale is low (0.2)\n",
    "plt.plot(loss_over_train, label=\"Training Loss\")\n",
    "plt.plot(loss_over_test, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_data_y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
