{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb2e4ac-d59e-4e27-9f18-e9f0aa674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y_binary = train_labels_encoded[\"label\"]\n",
    "test_data_y_binary = test_labels_encoded[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b95a23d-e0ba-48a9-8e8f-a76411be9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = test_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c14778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175341, 56)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:23:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['remainder__djit', 'encoder__proto_sctp', 'remainder__sjit', 'remainder__ct_src_ltm', 'remainder__sload', 'remainder__ct_dst_ltm', 'remainder__spkts', 'remainder__response_body_len', 'encoder__service_ftp', 'remainder__dur', 'remainder__ct_src_dport_ltm', 'remainder__tcprtt', 'remainder__ct_flw_http_mthd', 'remainder__dttl', 'remainder__sinpkt', 'remainder__dloss', 'encoder__proto_ospf', 'encoder__proto_udp', 'remainder__ct_srv_src', 'remainder__ct_state_ttl', 'remainder__sloss', 'encoder__state_REQ', 'remainder__ct_dst_src_ltm', 'remainder__dbytes', 'remainder__synack', 'remainder__trans_depth', 'encoder__state_CON', 'remainder__dmean', 'encoder__service_http', 'encoder__state_ECO', 'encoder__proto_arp', 'remainder__sbytes', 'remainder__smean', 'encoder__proto_tcp', 'encoder__service_dns', 'remainder__dpkts', 'remainder__ct_dst_sport_ltm', 'encoder__proto_unas', 'remainder__ct_srv_dst', 'remainder__sttl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "top_40_features = np.argsort(feature_importances)[-45:]\n",
    "\n",
    "# Print selected features\n",
    "selected_features = train_data_X.columns[top_40_features]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_data_y.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_over_train = []\n",
    "loss_over_test = []\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    mean_loss = epoch_loss / len(dataloader) \n",
    "    loss_over_train.append(mean_loss)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    mean_loss = test_loss / num_batches  # Divide by the number of batches\n",
    "    loss_over_test.append(mean_loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "epochs = 200  # Number of training epochs per fold\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define loss function and learning parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Combine datasets (Only Train Data)\n",
    "dataset = train_dataset  # Use only training dataset\n",
    "\n",
    "# Store results across folds\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_losses = []\n",
    "\n",
    "# K-Fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\\n\" + \"-\" * 30)\n",
    "    print(\"initializing model...\")\n",
    "    model = KAN([45, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best\n",
    "    # Create data subsets for the current fold\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Re-initialize model and optimizer for each fold\n",
    "     # Replace with your model class\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training function\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        return correct / total * 100, epoch_loss / len(dataloader)\n",
    "\n",
    "    # Validation function\n",
    "    def val_loop(dataloader, model, loss_fn):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "                all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracy = correct / len(dataloader.dataset) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "        mean_loss = val_loss / len(dataloader)\n",
    "\n",
    "        return accuracy, f1, f1_macro, mean_loss\n",
    "\n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_acc, f1, f1_macro, val_loss = val_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%, F1={f1:.2f}%, Macro F1={f1_macro:.2f}%\")\n",
    "\n",
    "    # Store fold results\n",
    "    fold_accuracies.append(val_acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_f1_macro_scores.append(f1_macro)\n",
    "    fold_losses.append(val_loss)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Avg Validation Accuracy: {sum(fold_accuracies) / k_folds:.2f}%\")\n",
    "print(f\"Avg Weighted F1-score: {sum(fold_f1_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Macro F1-score: {sum(fold_f1_macro_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Validation Loss: {sum(fold_losses) / k_folds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.280684  [    0/175341]\n",
      "loss: 2.185728  [ 1600/175341]\n",
      "loss: 2.090110  [ 3200/175341]\n",
      "loss: 1.823511  [ 4800/175341]\n",
      "loss: 2.093148  [ 6400/175341]\n",
      "loss: 1.785107  [ 8000/175341]\n",
      "loss: 1.945937  [ 9600/175341]\n",
      "loss: 1.460202  [11200/175341]\n",
      "loss: 1.699281  [12800/175341]\n",
      "loss: 1.232844  [14400/175341]\n",
      "loss: 1.323537  [16000/175341]\n",
      "loss: 1.159500  [17600/175341]\n",
      "loss: 1.101543  [19200/175341]\n",
      "loss: 1.299924  [20800/175341]\n",
      "loss: 1.293118  [22400/175341]\n",
      "loss: 0.734469  [24000/175341]\n",
      "loss: 0.902815  [25600/175341]\n",
      "loss: 0.865066  [27200/175341]\n",
      "loss: 0.670279  [28800/175341]\n",
      "loss: 1.211756  [30400/175341]\n",
      "loss: 1.070600  [32000/175341]\n",
      "loss: 1.049690  [33600/175341]\n",
      "loss: 0.759300  [35200/175341]\n",
      "loss: 0.924392  [36800/175341]\n",
      "loss: 0.808965  [38400/175341]\n",
      "loss: 0.687244  [40000/175341]\n",
      "loss: 0.765689  [41600/175341]\n",
      "loss: 0.747907  [43200/175341]\n",
      "loss: 1.026137  [44800/175341]\n",
      "loss: 0.945428  [46400/175341]\n",
      "loss: 0.556581  [48000/175341]\n",
      "loss: 0.854561  [49600/175341]\n",
      "loss: 1.078586  [51200/175341]\n",
      "loss: 0.834984  [52800/175341]\n",
      "loss: 0.937431  [54400/175341]\n",
      "loss: 0.791569  [56000/175341]\n",
      "loss: 0.608835  [57600/175341]\n",
      "loss: 0.633390  [59200/175341]\n",
      "loss: 0.497090  [60800/175341]\n",
      "loss: 0.986898  [62400/175341]\n",
      "loss: 1.084007  [64000/175341]\n",
      "loss: 1.079577  [65600/175341]\n",
      "loss: 0.649904  [67200/175341]\n",
      "loss: 0.722770  [68800/175341]\n",
      "loss: 1.229551  [70400/175341]\n",
      "loss: 0.429785  [72000/175341]\n",
      "loss: 0.667221  [73600/175341]\n",
      "loss: 0.897240  [75200/175341]\n",
      "loss: 0.634496  [76800/175341]\n",
      "loss: 0.832939  [78400/175341]\n",
      "loss: 0.991930  [80000/175341]\n",
      "loss: 0.607189  [81600/175341]\n",
      "loss: 0.644613  [83200/175341]\n",
      "loss: 0.888114  [84800/175341]\n",
      "loss: 0.426370  [86400/175341]\n",
      "loss: 0.885974  [88000/175341]\n",
      "loss: 1.042270  [89600/175341]\n",
      "loss: 0.683259  [91200/175341]\n",
      "loss: 0.835621  [92800/175341]\n",
      "loss: 0.547794  [94400/175341]\n",
      "loss: 1.022570  [96000/175341]\n",
      "loss: 0.710162  [97600/175341]\n",
      "loss: 0.485926  [99200/175341]\n",
      "loss: 0.992325  [100800/175341]\n",
      "loss: 0.766351  [102400/175341]\n",
      "loss: 0.755494  [104000/175341]\n",
      "loss: 1.020814  [105600/175341]\n",
      "loss: 0.781654  [107200/175341]\n",
      "loss: 0.656482  [108800/175341]\n",
      "loss: 0.605794  [110400/175341]\n",
      "loss: 1.014969  [112000/175341]\n",
      "loss: 0.483939  [113600/175341]\n",
      "loss: 0.774377  [115200/175341]\n",
      "loss: 0.813290  [116800/175341]\n",
      "loss: 0.651833  [118400/175341]\n",
      "loss: 0.477620  [120000/175341]\n",
      "loss: 0.986286  [121600/175341]\n",
      "loss: 1.099236  [123200/175341]\n",
      "loss: 0.243805  [124800/175341]\n",
      "loss: 0.561898  [126400/175341]\n",
      "loss: 0.844038  [128000/175341]\n",
      "loss: 0.640280  [129600/175341]\n",
      "loss: 1.073289  [131200/175341]\n",
      "loss: 0.417419  [132800/175341]\n",
      "loss: 0.495079  [134400/175341]\n",
      "loss: 0.829325  [136000/175341]\n",
      "loss: 0.927432  [137600/175341]\n",
      "loss: 0.975296  [139200/175341]\n",
      "loss: 0.488284  [140800/175341]\n",
      "loss: 0.483087  [142400/175341]\n",
      "loss: 0.662355  [144000/175341]\n",
      "loss: 0.508260  [145600/175341]\n",
      "loss: 0.564523  [147200/175341]\n",
      "loss: 0.443721  [148800/175341]\n",
      "loss: 0.663946  [150400/175341]\n",
      "loss: 0.639476  [152000/175341]\n",
      "loss: 0.439585  [153600/175341]\n",
      "loss: 0.726859  [155200/175341]\n",
      "loss: 0.540024  [156800/175341]\n",
      "loss: 0.392602  [158400/175341]\n",
      "loss: 1.073069  [160000/175341]\n",
      "loss: 0.412393  [161600/175341]\n",
      "loss: 0.806998  [163200/175341]\n",
      "loss: 0.672179  [164800/175341]\n",
      "loss: 0.819031  [166400/175341]\n",
      "loss: 0.582940  [168000/175341]\n",
      "loss: 0.994648  [169600/175341]\n",
      "loss: 0.569165  [171200/175341]\n",
      "loss: 0.824902  [172800/175341]\n",
      "loss: 0.775951  [174400/175341]\n",
      "Train Accuracy: 70.5431%\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.721973, F1-score: 69.87%, Macro_F1-Score:  31.27%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.693064  [    0/175341]\n",
      "loss: 0.708079  [ 1600/175341]\n",
      "loss: 0.390193  [ 3200/175341]\n",
      "loss: 0.297562  [ 4800/175341]\n",
      "loss: 0.886206  [ 6400/175341]\n",
      "loss: 0.326797  [ 8000/175341]\n",
      "loss: 0.569515  [ 9600/175341]\n",
      "loss: 0.951313  [11200/175341]\n",
      "loss: 0.514239  [12800/175341]\n",
      "loss: 0.518063  [14400/175341]\n",
      "loss: 1.652754  [16000/175341]\n",
      "loss: 1.072667  [17600/175341]\n",
      "loss: 0.521113  [19200/175341]\n",
      "loss: 0.526309  [20800/175341]\n",
      "loss: 0.635003  [22400/175341]\n",
      "loss: 0.717398  [24000/175341]\n",
      "loss: 0.478538  [25600/175341]\n",
      "loss: 0.466509  [27200/175341]\n",
      "loss: 1.150373  [28800/175341]\n",
      "loss: 0.844577  [30400/175341]\n",
      "loss: 0.627106  [32000/175341]\n",
      "loss: 1.030238  [33600/175341]\n",
      "loss: 0.792592  [35200/175341]\n",
      "loss: 0.517528  [36800/175341]\n",
      "loss: 0.658069  [38400/175341]\n",
      "loss: 0.284873  [40000/175341]\n",
      "loss: 0.266396  [41600/175341]\n",
      "loss: 0.507392  [43200/175341]\n",
      "loss: 0.997539  [44800/175341]\n",
      "loss: 0.426500  [46400/175341]\n",
      "loss: 0.923609  [48000/175341]\n",
      "loss: 0.471875  [49600/175341]\n",
      "loss: 0.477879  [51200/175341]\n",
      "loss: 1.075049  [52800/175341]\n",
      "loss: 1.538339  [54400/175341]\n",
      "loss: 0.813281  [56000/175341]\n",
      "loss: 0.662393  [57600/175341]\n",
      "loss: 0.378961  [59200/175341]\n",
      "loss: 0.881232  [60800/175341]\n",
      "loss: 0.824285  [62400/175341]\n",
      "loss: 0.487525  [64000/175341]\n",
      "loss: 0.858561  [65600/175341]\n",
      "loss: 0.339230  [67200/175341]\n",
      "loss: 0.468345  [68800/175341]\n",
      "loss: 0.432387  [70400/175341]\n",
      "loss: 0.777882  [72000/175341]\n",
      "loss: 0.577616  [73600/175341]\n",
      "loss: 0.904454  [75200/175341]\n",
      "loss: 0.656808  [76800/175341]\n",
      "loss: 0.565332  [78400/175341]\n",
      "loss: 0.494480  [80000/175341]\n",
      "loss: 0.261851  [81600/175341]\n",
      "loss: 0.797076  [83200/175341]\n",
      "loss: 0.463146  [84800/175341]\n",
      "loss: 0.805334  [86400/175341]\n",
      "loss: 0.492425  [88000/175341]\n",
      "loss: 0.924067  [89600/175341]\n",
      "loss: 0.521589  [91200/175341]\n",
      "loss: 0.704397  [92800/175341]\n",
      "loss: 0.872389  [94400/175341]\n",
      "loss: 0.912840  [96000/175341]\n",
      "loss: 0.518517  [97600/175341]\n",
      "loss: 0.629092  [99200/175341]\n",
      "loss: 0.363852  [100800/175341]\n",
      "loss: 0.719606  [102400/175341]\n",
      "loss: 0.494363  [104000/175341]\n",
      "loss: 0.950867  [105600/175341]\n",
      "loss: 1.103822  [107200/175341]\n",
      "loss: 0.813073  [108800/175341]\n",
      "loss: 0.564317  [110400/175341]\n",
      "loss: 0.458930  [112000/175341]\n",
      "loss: 0.843381  [113600/175341]\n",
      "loss: 0.838553  [115200/175341]\n",
      "loss: 0.348615  [116800/175341]\n",
      "loss: 0.983898  [118400/175341]\n",
      "loss: 0.783283  [120000/175341]\n",
      "loss: 1.077653  [121600/175341]\n",
      "loss: 0.519338  [123200/175341]\n",
      "loss: 0.242515  [124800/175341]\n",
      "loss: 0.851330  [126400/175341]\n",
      "loss: 0.815270  [128000/175341]\n",
      "loss: 0.247847  [129600/175341]\n",
      "loss: 0.694091  [131200/175341]\n",
      "loss: 0.674121  [132800/175341]\n",
      "loss: 1.003812  [134400/175341]\n",
      "loss: 1.011485  [136000/175341]\n",
      "loss: 0.831044  [137600/175341]\n",
      "loss: 0.783461  [139200/175341]\n",
      "loss: 0.477565  [140800/175341]\n",
      "loss: 0.634682  [142400/175341]\n",
      "loss: 0.265965  [144000/175341]\n",
      "loss: 0.413109  [145600/175341]\n",
      "loss: 0.339234  [147200/175341]\n",
      "loss: 0.882420  [148800/175341]\n",
      "loss: 0.289989  [150400/175341]\n",
      "loss: 0.734569  [152000/175341]\n",
      "loss: 0.600819  [153600/175341]\n",
      "loss: 0.988799  [155200/175341]\n",
      "loss: 0.319165  [156800/175341]\n",
      "loss: 1.323223  [158400/175341]\n",
      "loss: 0.547573  [160000/175341]\n",
      "loss: 0.784941  [161600/175341]\n",
      "loss: 0.800525  [163200/175341]\n",
      "loss: 0.593232  [164800/175341]\n",
      "loss: 0.301715  [166400/175341]\n",
      "loss: 0.573317  [168000/175341]\n",
      "loss: 0.753687  [169600/175341]\n",
      "loss: 0.324145  [171200/175341]\n",
      "loss: 0.273797  [172800/175341]\n",
      "loss: 0.607265  [174400/175341]\n",
      "Train Accuracy: 77.5979%\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.689578, F1-score: 70.39%, Macro_F1-Score:  32.70%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.639955  [    0/175341]\n",
      "loss: 0.708855  [ 1600/175341]\n",
      "loss: 0.518310  [ 3200/175341]\n",
      "loss: 0.864443  [ 4800/175341]\n",
      "loss: 0.250604  [ 6400/175341]\n",
      "loss: 0.579662  [ 8000/175341]\n",
      "loss: 0.841678  [ 9600/175341]\n",
      "loss: 0.608135  [11200/175341]\n",
      "loss: 0.624890  [12800/175341]\n",
      "loss: 0.500496  [14400/175341]\n",
      "loss: 0.319996  [16000/175341]\n",
      "loss: 0.346583  [17600/175341]\n",
      "loss: 0.475380  [19200/175341]\n",
      "loss: 0.538395  [20800/175341]\n",
      "loss: 0.641310  [22400/175341]\n",
      "loss: 0.612534  [24000/175341]\n",
      "loss: 0.494853  [25600/175341]\n",
      "loss: 0.729752  [27200/175341]\n",
      "loss: 0.649299  [28800/175341]\n",
      "loss: 0.495658  [30400/175341]\n",
      "loss: 0.830912  [32000/175341]\n",
      "loss: 0.527230  [33600/175341]\n",
      "loss: 1.195467  [35200/175341]\n",
      "loss: 0.307312  [36800/175341]\n",
      "loss: 0.187629  [38400/175341]\n",
      "loss: 0.534645  [40000/175341]\n",
      "loss: 0.921572  [41600/175341]\n",
      "loss: 0.568954  [43200/175341]\n",
      "loss: 0.822371  [44800/175341]\n",
      "loss: 0.656592  [46400/175341]\n",
      "loss: 0.520456  [48000/175341]\n",
      "loss: 0.225887  [49600/175341]\n",
      "loss: 0.327636  [51200/175341]\n",
      "loss: 0.513459  [52800/175341]\n",
      "loss: 0.822947  [54400/175341]\n",
      "loss: 0.361872  [56000/175341]\n",
      "loss: 0.686163  [57600/175341]\n",
      "loss: 0.360514  [59200/175341]\n",
      "loss: 0.273204  [60800/175341]\n",
      "loss: 0.502712  [62400/175341]\n",
      "loss: 0.424488  [64000/175341]\n",
      "loss: 0.319228  [65600/175341]\n",
      "loss: 0.588103  [67200/175341]\n",
      "loss: 0.219594  [68800/175341]\n",
      "loss: 0.858841  [70400/175341]\n",
      "loss: 0.631235  [72000/175341]\n",
      "loss: 0.417713  [73600/175341]\n",
      "loss: 0.302580  [75200/175341]\n",
      "loss: 0.398840  [76800/175341]\n",
      "loss: 0.432700  [78400/175341]\n",
      "loss: 0.690737  [80000/175341]\n",
      "loss: 0.688522  [81600/175341]\n",
      "loss: 0.383146  [83200/175341]\n",
      "loss: 0.909685  [84800/175341]\n",
      "loss: 0.625620  [86400/175341]\n",
      "loss: 0.194365  [88000/175341]\n",
      "loss: 0.412428  [89600/175341]\n",
      "loss: 0.944003  [91200/175341]\n",
      "loss: 0.722112  [92800/175341]\n",
      "loss: 0.632520  [94400/175341]\n",
      "loss: 0.471125  [96000/175341]\n",
      "loss: 0.677358  [97600/175341]\n",
      "loss: 0.891877  [99200/175341]\n",
      "loss: 0.648162  [100800/175341]\n",
      "loss: 0.936036  [102400/175341]\n",
      "loss: 0.841351  [104000/175341]\n",
      "loss: 0.804573  [105600/175341]\n",
      "loss: 0.774401  [107200/175341]\n",
      "loss: 0.670162  [108800/175341]\n",
      "loss: 0.507345  [110400/175341]\n",
      "loss: 0.894903  [112000/175341]\n",
      "loss: 0.928159  [113600/175341]\n",
      "loss: 0.854636  [115200/175341]\n",
      "loss: 0.391746  [116800/175341]\n",
      "loss: 0.584269  [118400/175341]\n",
      "loss: 0.311597  [120000/175341]\n",
      "loss: 0.593234  [121600/175341]\n",
      "loss: 0.497999  [123200/175341]\n",
      "loss: 0.335434  [124800/175341]\n",
      "loss: 0.521138  [126400/175341]\n",
      "loss: 0.591513  [128000/175341]\n",
      "loss: 0.536777  [129600/175341]\n",
      "loss: 0.820168  [131200/175341]\n",
      "loss: 0.874883  [132800/175341]\n",
      "loss: 0.402714  [134400/175341]\n",
      "loss: 0.563596  [136000/175341]\n",
      "loss: 0.448578  [137600/175341]\n",
      "loss: 0.187946  [139200/175341]\n",
      "loss: 0.568622  [140800/175341]\n",
      "loss: 0.495149  [142400/175341]\n",
      "loss: 0.666574  [144000/175341]\n",
      "loss: 0.732999  [145600/175341]\n",
      "loss: 0.387142  [147200/175341]\n",
      "loss: 0.636217  [148800/175341]\n",
      "loss: 0.918358  [150400/175341]\n",
      "loss: 0.620128  [152000/175341]\n",
      "loss: 0.312606  [153600/175341]\n",
      "loss: 0.722874  [155200/175341]\n",
      "loss: 0.347894  [156800/175341]\n",
      "loss: 0.438930  [158400/175341]\n",
      "loss: 0.310766  [160000/175341]\n",
      "loss: 0.411676  [161600/175341]\n",
      "loss: 0.692470  [163200/175341]\n",
      "loss: 0.132883  [164800/175341]\n",
      "loss: 0.468080  [166400/175341]\n",
      "loss: 0.395827  [168000/175341]\n",
      "loss: 0.500475  [169600/175341]\n",
      "loss: 0.413045  [171200/175341]\n",
      "loss: 0.519772  [172800/175341]\n",
      "loss: 0.542349  [174400/175341]\n",
      "Train Accuracy: 78.5635%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.635874, F1-score: 73.48%, Macro_F1-Score:  34.00%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.613951  [    0/175341]\n",
      "loss: 0.397174  [ 1600/175341]\n",
      "loss: 0.298586  [ 3200/175341]\n",
      "loss: 0.505810  [ 4800/175341]\n",
      "loss: 0.594256  [ 6400/175341]\n",
      "loss: 0.617677  [ 8000/175341]\n",
      "loss: 0.313661  [ 9600/175341]\n",
      "loss: 0.722223  [11200/175341]\n",
      "loss: 0.558167  [12800/175341]\n",
      "loss: 0.500708  [14400/175341]\n",
      "loss: 0.781203  [16000/175341]\n",
      "loss: 0.876396  [17600/175341]\n",
      "loss: 0.879079  [19200/175341]\n",
      "loss: 0.563448  [20800/175341]\n",
      "loss: 0.324844  [22400/175341]\n",
      "loss: 0.807783  [24000/175341]\n",
      "loss: 0.464671  [25600/175341]\n",
      "loss: 0.781802  [27200/175341]\n",
      "loss: 0.700088  [28800/175341]\n",
      "loss: 0.547088  [30400/175341]\n",
      "loss: 0.384960  [32000/175341]\n",
      "loss: 0.571695  [33600/175341]\n",
      "loss: 0.450155  [35200/175341]\n",
      "loss: 0.698272  [36800/175341]\n",
      "loss: 0.473156  [38400/175341]\n",
      "loss: 0.580488  [40000/175341]\n",
      "loss: 0.591580  [41600/175341]\n",
      "loss: 0.615598  [43200/175341]\n",
      "loss: 0.465397  [44800/175341]\n",
      "loss: 0.416843  [46400/175341]\n",
      "loss: 0.363511  [48000/175341]\n",
      "loss: 0.227128  [49600/175341]\n",
      "loss: 0.423882  [51200/175341]\n",
      "loss: 0.589771  [52800/175341]\n",
      "loss: 0.844906  [54400/175341]\n",
      "loss: 0.398462  [56000/175341]\n",
      "loss: 0.762873  [57600/175341]\n",
      "loss: 0.610272  [59200/175341]\n",
      "loss: 0.426013  [60800/175341]\n",
      "loss: 0.333969  [62400/175341]\n",
      "loss: 0.649782  [64000/175341]\n",
      "loss: 0.395720  [65600/175341]\n",
      "loss: 0.453923  [67200/175341]\n",
      "loss: 0.414857  [68800/175341]\n",
      "loss: 0.292942  [70400/175341]\n",
      "loss: 0.906009  [72000/175341]\n",
      "loss: 0.263385  [73600/175341]\n",
      "loss: 1.244684  [75200/175341]\n",
      "loss: 0.498207  [76800/175341]\n",
      "loss: 1.163265  [78400/175341]\n",
      "loss: 0.853351  [80000/175341]\n",
      "loss: 0.547014  [81600/175341]\n",
      "loss: 1.014627  [83200/175341]\n",
      "loss: 0.511814  [84800/175341]\n",
      "loss: 0.693212  [86400/175341]\n",
      "loss: 0.847850  [88000/175341]\n",
      "loss: 1.229270  [89600/175341]\n",
      "loss: 0.771662  [91200/175341]\n",
      "loss: 0.516307  [92800/175341]\n",
      "loss: 0.817599  [94400/175341]\n",
      "loss: 0.654097  [96000/175341]\n",
      "loss: 0.749020  [97600/175341]\n",
      "loss: 0.168316  [99200/175341]\n",
      "loss: 1.098727  [100800/175341]\n",
      "loss: 0.122700  [102400/175341]\n",
      "loss: 0.479767  [104000/175341]\n",
      "loss: 0.217554  [105600/175341]\n",
      "loss: 1.313867  [107200/175341]\n",
      "loss: 0.443268  [108800/175341]\n",
      "loss: 0.358061  [110400/175341]\n",
      "loss: 0.774912  [112000/175341]\n",
      "loss: 0.343120  [113600/175341]\n",
      "loss: 0.519135  [115200/175341]\n",
      "loss: 0.333890  [116800/175341]\n",
      "loss: 0.771020  [118400/175341]\n",
      "loss: 0.257322  [120000/175341]\n",
      "loss: 0.902266  [121600/175341]\n",
      "loss: 0.886026  [123200/175341]\n",
      "loss: 0.543077  [124800/175341]\n",
      "loss: 0.860537  [126400/175341]\n",
      "loss: 0.653199  [128000/175341]\n",
      "loss: 0.740174  [129600/175341]\n",
      "loss: 0.412605  [131200/175341]\n",
      "loss: 0.903229  [132800/175341]\n",
      "loss: 0.565884  [134400/175341]\n",
      "loss: 0.486245  [136000/175341]\n",
      "loss: 0.443537  [137600/175341]\n",
      "loss: 0.921971  [139200/175341]\n",
      "loss: 0.361493  [140800/175341]\n",
      "loss: 1.152086  [142400/175341]\n",
      "loss: 0.741312  [144000/175341]\n",
      "loss: 0.893322  [145600/175341]\n",
      "loss: 0.985466  [147200/175341]\n",
      "loss: 0.709133  [148800/175341]\n",
      "loss: 0.161018  [150400/175341]\n",
      "loss: 0.466746  [152000/175341]\n",
      "loss: 0.591417  [153600/175341]\n",
      "loss: 0.699567  [155200/175341]\n",
      "loss: 0.298983  [156800/175341]\n",
      "loss: 0.505992  [158400/175341]\n",
      "loss: 0.426729  [160000/175341]\n",
      "loss: 0.268096  [161600/175341]\n",
      "loss: 0.664583  [163200/175341]\n",
      "loss: 0.747907  [164800/175341]\n",
      "loss: 0.478334  [166400/175341]\n",
      "loss: 0.383682  [168000/175341]\n",
      "loss: 0.676102  [169600/175341]\n",
      "loss: 0.581761  [171200/175341]\n",
      "loss: 0.431101  [172800/175341]\n",
      "loss: 0.328120  [174400/175341]\n",
      "Train Accuracy: 78.9792%\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.652875, F1-score: 71.60%, Macro_F1-Score:  33.37%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.713825  [    0/175341]\n",
      "loss: 0.449832  [ 1600/175341]\n",
      "loss: 0.489716  [ 3200/175341]\n",
      "loss: 0.762005  [ 4800/175341]\n",
      "loss: 0.555406  [ 6400/175341]\n",
      "loss: 0.408292  [ 8000/175341]\n",
      "loss: 0.513390  [ 9600/175341]\n",
      "loss: 0.705888  [11200/175341]\n",
      "loss: 0.849631  [12800/175341]\n",
      "loss: 0.259307  [14400/175341]\n",
      "loss: 0.339083  [16000/175341]\n",
      "loss: 0.952883  [17600/175341]\n",
      "loss: 0.585452  [19200/175341]\n",
      "loss: 0.642943  [20800/175341]\n",
      "loss: 0.710057  [22400/175341]\n",
      "loss: 0.474230  [24000/175341]\n",
      "loss: 0.454242  [25600/175341]\n",
      "loss: 0.426161  [27200/175341]\n",
      "loss: 0.359741  [28800/175341]\n",
      "loss: 0.463673  [30400/175341]\n",
      "loss: 0.324382  [32000/175341]\n",
      "loss: 0.458555  [33600/175341]\n",
      "loss: 0.591200  [35200/175341]\n",
      "loss: 0.596766  [36800/175341]\n",
      "loss: 0.044016  [38400/175341]\n",
      "loss: 0.744887  [40000/175341]\n",
      "loss: 0.376298  [41600/175341]\n",
      "loss: 0.499446  [43200/175341]\n",
      "loss: 0.846103  [44800/175341]\n",
      "loss: 0.882581  [46400/175341]\n",
      "loss: 0.765198  [48000/175341]\n",
      "loss: 0.704550  [49600/175341]\n",
      "loss: 0.454441  [51200/175341]\n",
      "loss: 0.488194  [52800/175341]\n",
      "loss: 0.508935  [54400/175341]\n",
      "loss: 0.332911  [56000/175341]\n",
      "loss: 0.771580  [57600/175341]\n",
      "loss: 0.228910  [59200/175341]\n",
      "loss: 0.408285  [60800/175341]\n",
      "loss: 0.596244  [62400/175341]\n",
      "loss: 0.320895  [64000/175341]\n",
      "loss: 0.359849  [65600/175341]\n",
      "loss: 1.180403  [67200/175341]\n",
      "loss: 0.516476  [68800/175341]\n",
      "loss: 0.243559  [70400/175341]\n",
      "loss: 0.600599  [72000/175341]\n",
      "loss: 1.094877  [73600/175341]\n",
      "loss: 0.474111  [75200/175341]\n",
      "loss: 0.744673  [76800/175341]\n",
      "loss: 0.568788  [78400/175341]\n",
      "loss: 0.356284  [80000/175341]\n",
      "loss: 0.520659  [81600/175341]\n",
      "loss: 0.363458  [83200/175341]\n",
      "loss: 0.782694  [84800/175341]\n",
      "loss: 0.707666  [86400/175341]\n",
      "loss: 0.278165  [88000/175341]\n",
      "loss: 0.313352  [89600/175341]\n",
      "loss: 0.700539  [91200/175341]\n",
      "loss: 0.562716  [92800/175341]\n",
      "loss: 0.850690  [94400/175341]\n",
      "loss: 0.419235  [96000/175341]\n",
      "loss: 0.628287  [97600/175341]\n",
      "loss: 0.235019  [99200/175341]\n",
      "loss: 0.808940  [100800/175341]\n",
      "loss: 0.588548  [102400/175341]\n",
      "loss: 0.496122  [104000/175341]\n",
      "loss: 0.582451  [105600/175341]\n",
      "loss: 0.835316  [107200/175341]\n",
      "loss: 0.580877  [108800/175341]\n",
      "loss: 0.252424  [110400/175341]\n",
      "loss: 0.503449  [112000/175341]\n",
      "loss: 0.522599  [113600/175341]\n",
      "loss: 0.369067  [115200/175341]\n",
      "loss: 0.626569  [116800/175341]\n",
      "loss: 0.677587  [118400/175341]\n",
      "loss: 0.622406  [120000/175341]\n",
      "loss: 0.754371  [121600/175341]\n",
      "loss: 0.447082  [123200/175341]\n",
      "loss: 0.422046  [124800/175341]\n",
      "loss: 0.436291  [126400/175341]\n",
      "loss: 0.597016  [128000/175341]\n",
      "loss: 0.636913  [129600/175341]\n",
      "loss: 0.513100  [131200/175341]\n",
      "loss: 0.350213  [132800/175341]\n",
      "loss: 0.443949  [134400/175341]\n",
      "loss: 0.609784  [136000/175341]\n",
      "loss: 0.766545  [137600/175341]\n",
      "loss: 0.352312  [139200/175341]\n",
      "loss: 0.287475  [140800/175341]\n",
      "loss: 0.610589  [142400/175341]\n",
      "loss: 0.651312  [144000/175341]\n",
      "loss: 0.588472  [145600/175341]\n",
      "loss: 0.572468  [147200/175341]\n",
      "loss: 0.946004  [148800/175341]\n",
      "loss: 0.397345  [150400/175341]\n",
      "loss: 0.509459  [152000/175341]\n",
      "loss: 0.612821  [153600/175341]\n",
      "loss: 0.429020  [155200/175341]\n",
      "loss: 0.191791  [156800/175341]\n",
      "loss: 0.356309  [158400/175341]\n",
      "loss: 0.564625  [160000/175341]\n",
      "loss: 0.350152  [161600/175341]\n",
      "loss: 0.520269  [163200/175341]\n",
      "loss: 0.593271  [164800/175341]\n",
      "loss: 0.381640  [166400/175341]\n",
      "loss: 0.497636  [168000/175341]\n",
      "loss: 0.482976  [169600/175341]\n",
      "loss: 0.988098  [171200/175341]\n",
      "loss: 0.236970  [172800/175341]\n",
      "loss: 0.269956  [174400/175341]\n",
      "Train Accuracy: 79.2633%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.623902, F1-score: 73.53%, Macro_F1-Score:  34.16%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.329200  [    0/175341]\n",
      "loss: 1.033158  [ 1600/175341]\n",
      "loss: 0.524764  [ 3200/175341]\n",
      "loss: 0.238505  [ 4800/175341]\n",
      "loss: 0.404644  [ 6400/175341]\n",
      "loss: 0.190025  [ 8000/175341]\n",
      "loss: 0.813381  [ 9600/175341]\n",
      "loss: 1.073748  [11200/175341]\n",
      "loss: 0.538180  [12800/175341]\n",
      "loss: 1.119147  [14400/175341]\n",
      "loss: 0.512203  [16000/175341]\n",
      "loss: 0.584633  [17600/175341]\n",
      "loss: 0.459783  [19200/175341]\n",
      "loss: 0.499703  [20800/175341]\n",
      "loss: 0.542291  [22400/175341]\n",
      "loss: 0.506944  [24000/175341]\n",
      "loss: 0.461272  [25600/175341]\n",
      "loss: 0.519652  [27200/175341]\n",
      "loss: 0.610430  [28800/175341]\n",
      "loss: 0.307428  [30400/175341]\n",
      "loss: 0.962704  [32000/175341]\n",
      "loss: 0.858622  [33600/175341]\n",
      "loss: 0.708364  [35200/175341]\n",
      "loss: 0.211319  [36800/175341]\n",
      "loss: 0.418595  [38400/175341]\n",
      "loss: 0.397630  [40000/175341]\n",
      "loss: 0.447715  [41600/175341]\n",
      "loss: 0.773008  [43200/175341]\n",
      "loss: 0.768417  [44800/175341]\n",
      "loss: 0.200959  [46400/175341]\n",
      "loss: 0.461011  [48000/175341]\n",
      "loss: 0.244614  [49600/175341]\n",
      "loss: 0.072618  [51200/175341]\n",
      "loss: 0.153914  [52800/175341]\n",
      "loss: 0.525680  [54400/175341]\n",
      "loss: 0.927950  [56000/175341]\n",
      "loss: 0.376133  [57600/175341]\n",
      "loss: 0.660205  [59200/175341]\n",
      "loss: 0.311102  [60800/175341]\n",
      "loss: 0.382576  [62400/175341]\n",
      "loss: 0.773869  [64000/175341]\n",
      "loss: 0.409346  [65600/175341]\n",
      "loss: 0.457821  [67200/175341]\n",
      "loss: 0.400258  [68800/175341]\n",
      "loss: 0.552101  [70400/175341]\n",
      "loss: 0.349683  [72000/175341]\n",
      "loss: 0.392260  [73600/175341]\n",
      "loss: 0.675524  [75200/175341]\n",
      "loss: 0.254926  [76800/175341]\n",
      "loss: 0.545719  [78400/175341]\n",
      "loss: 0.315990  [80000/175341]\n",
      "loss: 0.690735  [81600/175341]\n",
      "loss: 0.474261  [83200/175341]\n",
      "loss: 0.712241  [84800/175341]\n",
      "loss: 0.523791  [86400/175341]\n",
      "loss: 0.392767  [88000/175341]\n",
      "loss: 0.119327  [89600/175341]\n",
      "loss: 0.606434  [91200/175341]\n",
      "loss: 0.600460  [92800/175341]\n",
      "loss: 0.386168  [94400/175341]\n",
      "loss: 1.178183  [96000/175341]\n",
      "loss: 0.632480  [97600/175341]\n",
      "loss: 0.223089  [99200/175341]\n",
      "loss: 0.424272  [100800/175341]\n",
      "loss: 0.210689  [102400/175341]\n",
      "loss: 0.209114  [104000/175341]\n",
      "loss: 0.705929  [105600/175341]\n",
      "loss: 0.471902  [107200/175341]\n",
      "loss: 0.565624  [108800/175341]\n",
      "loss: 0.470589  [110400/175341]\n",
      "loss: 0.630845  [112000/175341]\n",
      "loss: 0.499623  [113600/175341]\n",
      "loss: 0.755267  [115200/175341]\n",
      "loss: 0.205988  [116800/175341]\n",
      "loss: 0.205185  [118400/175341]\n",
      "loss: 0.481340  [120000/175341]\n",
      "loss: 0.321587  [121600/175341]\n",
      "loss: 0.818544  [123200/175341]\n",
      "loss: 0.431296  [124800/175341]\n",
      "loss: 0.685583  [126400/175341]\n",
      "loss: 0.301232  [128000/175341]\n",
      "loss: 0.609277  [129600/175341]\n",
      "loss: 0.345420  [131200/175341]\n",
      "loss: 0.346741  [132800/175341]\n",
      "loss: 0.332791  [134400/175341]\n",
      "loss: 0.381727  [136000/175341]\n",
      "loss: 0.197334  [137600/175341]\n",
      "loss: 0.944452  [139200/175341]\n",
      "loss: 0.702490  [140800/175341]\n",
      "loss: 1.274412  [142400/175341]\n",
      "loss: 0.937271  [144000/175341]\n",
      "loss: 0.211807  [145600/175341]\n",
      "loss: 0.323716  [147200/175341]\n",
      "loss: 0.279086  [148800/175341]\n",
      "loss: 0.607514  [150400/175341]\n",
      "loss: 0.579048  [152000/175341]\n",
      "loss: 0.653498  [153600/175341]\n",
      "loss: 0.554345  [155200/175341]\n",
      "loss: 0.512500  [156800/175341]\n",
      "loss: 0.688329  [158400/175341]\n",
      "loss: 0.582617  [160000/175341]\n",
      "loss: 0.729947  [161600/175341]\n",
      "loss: 0.873195  [163200/175341]\n",
      "loss: 0.462477  [164800/175341]\n",
      "loss: 0.971175  [166400/175341]\n",
      "loss: 0.268621  [168000/175341]\n",
      "loss: 0.566018  [169600/175341]\n",
      "loss: 0.418613  [171200/175341]\n",
      "loss: 0.487173  [172800/175341]\n",
      "loss: 0.369732  [174400/175341]\n",
      "Train Accuracy: 79.5798%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.621052, F1-score: 74.06%, Macro_F1-Score:  36.30%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.229720  [    0/175341]\n",
      "loss: 0.683554  [ 1600/175341]\n",
      "loss: 0.637805  [ 3200/175341]\n",
      "loss: 0.973213  [ 4800/175341]\n",
      "loss: 0.651434  [ 6400/175341]\n",
      "loss: 0.373639  [ 8000/175341]\n",
      "loss: 0.496932  [ 9600/175341]\n",
      "loss: 0.536746  [11200/175341]\n",
      "loss: 0.320066  [12800/175341]\n",
      "loss: 0.439592  [14400/175341]\n",
      "loss: 0.558883  [16000/175341]\n",
      "loss: 0.229565  [17600/175341]\n",
      "loss: 0.421137  [19200/175341]\n",
      "loss: 0.818286  [20800/175341]\n",
      "loss: 0.293893  [22400/175341]\n",
      "loss: 0.354927  [24000/175341]\n",
      "loss: 0.657484  [25600/175341]\n",
      "loss: 1.098902  [27200/175341]\n",
      "loss: 0.333912  [28800/175341]\n",
      "loss: 0.637123  [30400/175341]\n",
      "loss: 0.416846  [32000/175341]\n",
      "loss: 0.709825  [33600/175341]\n",
      "loss: 1.002781  [35200/175341]\n",
      "loss: 0.194631  [36800/175341]\n",
      "loss: 0.216936  [38400/175341]\n",
      "loss: 1.028496  [40000/175341]\n",
      "loss: 0.644166  [41600/175341]\n",
      "loss: 0.636906  [43200/175341]\n",
      "loss: 0.544981  [44800/175341]\n",
      "loss: 0.482600  [46400/175341]\n",
      "loss: 0.502571  [48000/175341]\n",
      "loss: 0.425724  [49600/175341]\n",
      "loss: 0.684614  [51200/175341]\n",
      "loss: 0.445630  [52800/175341]\n",
      "loss: 0.411817  [54400/175341]\n",
      "loss: 0.673633  [56000/175341]\n",
      "loss: 0.337702  [57600/175341]\n",
      "loss: 0.475463  [59200/175341]\n",
      "loss: 0.392407  [60800/175341]\n",
      "loss: 0.631869  [62400/175341]\n",
      "loss: 0.686479  [64000/175341]\n",
      "loss: 0.506053  [65600/175341]\n",
      "loss: 0.646434  [67200/175341]\n",
      "loss: 0.267915  [68800/175341]\n",
      "loss: 0.411542  [70400/175341]\n",
      "loss: 0.633533  [72000/175341]\n",
      "loss: 0.576558  [73600/175341]\n",
      "loss: 0.491062  [75200/175341]\n",
      "loss: 0.747244  [76800/175341]\n",
      "loss: 0.292429  [78400/175341]\n",
      "loss: 0.514996  [80000/175341]\n",
      "loss: 0.938002  [81600/175341]\n",
      "loss: 0.355020  [83200/175341]\n",
      "loss: 0.257350  [84800/175341]\n",
      "loss: 0.353940  [86400/175341]\n",
      "loss: 0.523565  [88000/175341]\n",
      "loss: 0.451971  [89600/175341]\n",
      "loss: 0.482115  [91200/175341]\n",
      "loss: 0.404337  [92800/175341]\n",
      "loss: 0.378007  [94400/175341]\n",
      "loss: 0.317573  [96000/175341]\n",
      "loss: 0.717006  [97600/175341]\n",
      "loss: 0.403937  [99200/175341]\n",
      "loss: 0.630465  [100800/175341]\n",
      "loss: 0.796548  [102400/175341]\n",
      "loss: 0.408376  [104000/175341]\n",
      "loss: 0.690344  [105600/175341]\n",
      "loss: 0.655860  [107200/175341]\n",
      "loss: 0.344476  [108800/175341]\n",
      "loss: 0.254625  [110400/175341]\n",
      "loss: 0.168908  [112000/175341]\n",
      "loss: 0.388631  [113600/175341]\n",
      "loss: 0.372396  [115200/175341]\n",
      "loss: 0.267461  [116800/175341]\n",
      "loss: 0.473931  [118400/175341]\n",
      "loss: 0.378985  [120000/175341]\n",
      "loss: 0.674954  [121600/175341]\n",
      "loss: 0.980193  [123200/175341]\n",
      "loss: 0.590957  [124800/175341]\n",
      "loss: 1.178725  [126400/175341]\n",
      "loss: 0.521805  [128000/175341]\n",
      "loss: 0.434049  [129600/175341]\n",
      "loss: 0.557909  [131200/175341]\n",
      "loss: 0.484821  [132800/175341]\n",
      "loss: 0.403998  [134400/175341]\n",
      "loss: 0.286240  [136000/175341]\n",
      "loss: 0.476702  [137600/175341]\n",
      "loss: 0.377140  [139200/175341]\n",
      "loss: 0.182020  [140800/175341]\n",
      "loss: 0.747262  [142400/175341]\n",
      "loss: 0.408126  [144000/175341]\n",
      "loss: 0.327918  [145600/175341]\n",
      "loss: 0.247840  [147200/175341]\n",
      "loss: 0.391187  [148800/175341]\n",
      "loss: 0.524507  [150400/175341]\n",
      "loss: 0.684695  [152000/175341]\n",
      "loss: 0.629518  [153600/175341]\n",
      "loss: 0.537030  [155200/175341]\n",
      "loss: 0.358913  [156800/175341]\n",
      "loss: 0.791220  [158400/175341]\n",
      "loss: 0.610855  [160000/175341]\n",
      "loss: 0.156648  [161600/175341]\n",
      "loss: 0.726381  [163200/175341]\n",
      "loss: 0.582976  [164800/175341]\n",
      "loss: 0.596353  [166400/175341]\n",
      "loss: 0.564150  [168000/175341]\n",
      "loss: 0.882425  [169600/175341]\n",
      "loss: 0.149752  [171200/175341]\n",
      "loss: 0.456882  [172800/175341]\n",
      "loss: 0.324297  [174400/175341]\n",
      "Train Accuracy: 79.8410%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.608824, F1-score: 74.53%, Macro_F1-Score:  38.34%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.314237  [    0/175341]\n",
      "loss: 0.573785  [ 1600/175341]\n",
      "loss: 0.604274  [ 3200/175341]\n",
      "loss: 0.268313  [ 4800/175341]\n",
      "loss: 0.526045  [ 6400/175341]\n",
      "loss: 0.614883  [ 8000/175341]\n",
      "loss: 0.420612  [ 9600/175341]\n",
      "loss: 0.312418  [11200/175341]\n",
      "loss: 0.443298  [12800/175341]\n",
      "loss: 0.423798  [14400/175341]\n",
      "loss: 0.276812  [16000/175341]\n",
      "loss: 0.519356  [17600/175341]\n",
      "loss: 0.658503  [19200/175341]\n",
      "loss: 0.725699  [20800/175341]\n",
      "loss: 0.794239  [22400/175341]\n",
      "loss: 0.955257  [24000/175341]\n",
      "loss: 0.888900  [25600/175341]\n",
      "loss: 0.649777  [27200/175341]\n",
      "loss: 0.633125  [28800/175341]\n",
      "loss: 0.262459  [30400/175341]\n",
      "loss: 0.146625  [32000/175341]\n",
      "loss: 0.385560  [33600/175341]\n",
      "loss: 0.574197  [35200/175341]\n",
      "loss: 0.272640  [36800/175341]\n",
      "loss: 0.311777  [38400/175341]\n",
      "loss: 0.428405  [40000/175341]\n",
      "loss: 0.472684  [41600/175341]\n",
      "loss: 0.630975  [43200/175341]\n",
      "loss: 0.566632  [44800/175341]\n",
      "loss: 0.280908  [46400/175341]\n",
      "loss: 0.388648  [48000/175341]\n",
      "loss: 0.583870  [49600/175341]\n",
      "loss: 0.391936  [51200/175341]\n",
      "loss: 0.650655  [52800/175341]\n",
      "loss: 0.841200  [54400/175341]\n",
      "loss: 0.728122  [56000/175341]\n",
      "loss: 0.589867  [57600/175341]\n",
      "loss: 0.229048  [59200/175341]\n",
      "loss: 0.439461  [60800/175341]\n",
      "loss: 0.309626  [62400/175341]\n",
      "loss: 0.633967  [64000/175341]\n",
      "loss: 0.385098  [65600/175341]\n",
      "loss: 0.255874  [67200/175341]\n",
      "loss: 0.593155  [68800/175341]\n",
      "loss: 0.331423  [70400/175341]\n",
      "loss: 0.493189  [72000/175341]\n",
      "loss: 0.467573  [73600/175341]\n",
      "loss: 0.186862  [75200/175341]\n",
      "loss: 0.547040  [76800/175341]\n",
      "loss: 0.639179  [78400/175341]\n",
      "loss: 0.386585  [80000/175341]\n",
      "loss: 0.517776  [81600/175341]\n",
      "loss: 0.596755  [83200/175341]\n",
      "loss: 0.601355  [84800/175341]\n",
      "loss: 0.638337  [86400/175341]\n",
      "loss: 0.467886  [88000/175341]\n",
      "loss: 0.510989  [89600/175341]\n",
      "loss: 0.632661  [91200/175341]\n",
      "loss: 0.253736  [92800/175341]\n",
      "loss: 0.404572  [94400/175341]\n",
      "loss: 0.425592  [96000/175341]\n",
      "loss: 0.502033  [97600/175341]\n",
      "loss: 0.417417  [99200/175341]\n",
      "loss: 0.506220  [100800/175341]\n",
      "loss: 0.589149  [102400/175341]\n",
      "loss: 0.455365  [104000/175341]\n",
      "loss: 0.681541  [105600/175341]\n",
      "loss: 0.274230  [107200/175341]\n",
      "loss: 0.387592  [108800/175341]\n",
      "loss: 0.407906  [110400/175341]\n",
      "loss: 0.402572  [112000/175341]\n",
      "loss: 0.560429  [113600/175341]\n",
      "loss: 0.445521  [115200/175341]\n",
      "loss: 0.586713  [116800/175341]\n",
      "loss: 0.514316  [118400/175341]\n",
      "loss: 0.549215  [120000/175341]\n",
      "loss: 0.555868  [121600/175341]\n",
      "loss: 0.684978  [123200/175341]\n",
      "loss: 0.337928  [124800/175341]\n",
      "loss: 0.758995  [126400/175341]\n",
      "loss: 0.473434  [128000/175341]\n",
      "loss: 0.379190  [129600/175341]\n",
      "loss: 0.116004  [131200/175341]\n",
      "loss: 0.668821  [132800/175341]\n",
      "loss: 0.365702  [134400/175341]\n",
      "loss: 0.486236  [136000/175341]\n",
      "loss: 0.539387  [137600/175341]\n",
      "loss: 0.296677  [139200/175341]\n",
      "loss: 0.669283  [140800/175341]\n",
      "loss: 0.324251  [142400/175341]\n",
      "loss: 0.699915  [144000/175341]\n",
      "loss: 0.670565  [145600/175341]\n",
      "loss: 0.487744  [147200/175341]\n",
      "loss: 0.853772  [148800/175341]\n",
      "loss: 0.626616  [150400/175341]\n",
      "loss: 0.826456  [152000/175341]\n",
      "loss: 0.161128  [153600/175341]\n",
      "loss: 0.370152  [155200/175341]\n",
      "loss: 0.757650  [156800/175341]\n",
      "loss: 0.177538  [158400/175341]\n",
      "loss: 0.244053  [160000/175341]\n",
      "loss: 0.372907  [161600/175341]\n",
      "loss: 0.452518  [163200/175341]\n",
      "loss: 0.336203  [164800/175341]\n",
      "loss: 0.208012  [166400/175341]\n",
      "loss: 0.379727  [168000/175341]\n",
      "loss: 0.566690  [169600/175341]\n",
      "loss: 0.425122  [171200/175341]\n",
      "loss: 0.311391  [172800/175341]\n",
      "loss: 0.223419  [174400/175341]\n",
      "Train Accuracy: 80.0138%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.603578, F1-score: 74.37%, Macro_F1-Score:  38.38%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.344932  [    0/175341]\n",
      "loss: 0.315369  [ 1600/175341]\n",
      "loss: 0.789811  [ 3200/175341]\n",
      "loss: 0.469057  [ 4800/175341]\n",
      "loss: 0.879917  [ 6400/175341]\n",
      "loss: 0.163734  [ 8000/175341]\n",
      "loss: 0.487181  [ 9600/175341]\n",
      "loss: 0.411374  [11200/175341]\n",
      "loss: 0.256863  [12800/175341]\n",
      "loss: 0.690023  [14400/175341]\n",
      "loss: 0.227699  [16000/175341]\n",
      "loss: 0.546864  [17600/175341]\n",
      "loss: 0.209525  [19200/175341]\n",
      "loss: 0.860918  [20800/175341]\n",
      "loss: 0.498706  [22400/175341]\n",
      "loss: 1.054406  [24000/175341]\n",
      "loss: 0.431360  [25600/175341]\n",
      "loss: 0.608143  [27200/175341]\n",
      "loss: 0.510466  [28800/175341]\n",
      "loss: 0.936985  [30400/175341]\n",
      "loss: 0.337081  [32000/175341]\n",
      "loss: 0.258668  [33600/175341]\n",
      "loss: 0.566140  [35200/175341]\n",
      "loss: 0.349603  [36800/175341]\n",
      "loss: 0.366751  [38400/175341]\n",
      "loss: 0.249618  [40000/175341]\n",
      "loss: 0.494665  [41600/175341]\n",
      "loss: 0.297830  [43200/175341]\n",
      "loss: 0.570562  [44800/175341]\n",
      "loss: 0.397357  [46400/175341]\n",
      "loss: 0.502370  [48000/175341]\n",
      "loss: 0.401412  [49600/175341]\n",
      "loss: 0.274282  [51200/175341]\n",
      "loss: 0.376594  [52800/175341]\n",
      "loss: 0.710444  [54400/175341]\n",
      "loss: 0.537951  [56000/175341]\n",
      "loss: 0.363499  [57600/175341]\n",
      "loss: 0.702908  [59200/175341]\n",
      "loss: 0.312326  [60800/175341]\n",
      "loss: 0.432653  [62400/175341]\n",
      "loss: 0.669869  [64000/175341]\n",
      "loss: 0.274592  [65600/175341]\n",
      "loss: 0.882951  [67200/175341]\n",
      "loss: 0.510105  [68800/175341]\n",
      "loss: 0.347093  [70400/175341]\n",
      "loss: 0.558296  [72000/175341]\n",
      "loss: 0.361926  [73600/175341]\n",
      "loss: 0.290506  [75200/175341]\n",
      "loss: 0.654735  [76800/175341]\n",
      "loss: 0.658681  [78400/175341]\n",
      "loss: 0.379701  [80000/175341]\n",
      "loss: 0.271745  [81600/175341]\n",
      "loss: 0.704024  [83200/175341]\n",
      "loss: 0.518792  [84800/175341]\n",
      "loss: 0.549193  [86400/175341]\n",
      "loss: 0.449723  [88000/175341]\n",
      "loss: 0.288609  [89600/175341]\n",
      "loss: 0.800717  [91200/175341]\n",
      "loss: 0.649037  [92800/175341]\n",
      "loss: 0.289374  [94400/175341]\n",
      "loss: 0.468544  [96000/175341]\n",
      "loss: 0.637212  [97600/175341]\n",
      "loss: 0.484157  [99200/175341]\n",
      "loss: 0.601175  [100800/175341]\n",
      "loss: 0.296618  [102400/175341]\n",
      "loss: 0.460849  [104000/175341]\n",
      "loss: 0.391845  [105600/175341]\n",
      "loss: 0.298852  [107200/175341]\n",
      "loss: 0.392440  [108800/175341]\n",
      "loss: 0.426756  [110400/175341]\n",
      "loss: 0.857615  [112000/175341]\n",
      "loss: 0.576486  [113600/175341]\n",
      "loss: 0.357389  [115200/175341]\n",
      "loss: 0.195910  [116800/175341]\n",
      "loss: 0.405087  [118400/175341]\n",
      "loss: 0.473009  [120000/175341]\n",
      "loss: 0.314053  [121600/175341]\n",
      "loss: 0.341111  [123200/175341]\n",
      "loss: 0.596112  [124800/175341]\n",
      "loss: 0.298776  [126400/175341]\n",
      "loss: 0.557153  [128000/175341]\n",
      "loss: 0.195859  [129600/175341]\n",
      "loss: 0.283104  [131200/175341]\n",
      "loss: 0.223257  [132800/175341]\n",
      "loss: 0.396601  [134400/175341]\n",
      "loss: 0.498946  [136000/175341]\n",
      "loss: 0.445872  [137600/175341]\n",
      "loss: 0.723751  [139200/175341]\n",
      "loss: 0.360973  [140800/175341]\n",
      "loss: 0.341311  [142400/175341]\n",
      "loss: 1.557288  [144000/175341]\n",
      "loss: 0.305951  [145600/175341]\n",
      "loss: 0.177354  [147200/175341]\n",
      "loss: 1.249937  [148800/175341]\n",
      "loss: 0.578918  [150400/175341]\n",
      "loss: 0.207571  [152000/175341]\n",
      "loss: 0.636339  [153600/175341]\n",
      "loss: 0.367649  [155200/175341]\n",
      "loss: 0.426899  [156800/175341]\n",
      "loss: 0.952370  [158400/175341]\n",
      "loss: 0.443093  [160000/175341]\n",
      "loss: 0.675416  [161600/175341]\n",
      "loss: 0.829059  [163200/175341]\n",
      "loss: 0.528896  [164800/175341]\n",
      "loss: 0.490965  [166400/175341]\n",
      "loss: 0.163456  [168000/175341]\n",
      "loss: 0.307238  [169600/175341]\n",
      "loss: 0.393205  [171200/175341]\n",
      "loss: 0.220595  [172800/175341]\n",
      "loss: 0.639493  [174400/175341]\n",
      "Train Accuracy: 80.1279%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.602421, F1-score: 74.52%, Macro_F1-Score:  38.82%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.677248  [    0/175341]\n",
      "loss: 0.889591  [ 1600/175341]\n",
      "loss: 0.687641  [ 3200/175341]\n",
      "loss: 0.245694  [ 4800/175341]\n",
      "loss: 0.315605  [ 6400/175341]\n",
      "loss: 0.650919  [ 8000/175341]\n",
      "loss: 0.338542  [ 9600/175341]\n",
      "loss: 0.852823  [11200/175341]\n",
      "loss: 0.866969  [12800/175341]\n",
      "loss: 0.564802  [14400/175341]\n",
      "loss: 0.338040  [16000/175341]\n",
      "loss: 0.579471  [17600/175341]\n",
      "loss: 0.365070  [19200/175341]\n",
      "loss: 0.671772  [20800/175341]\n",
      "loss: 0.526508  [22400/175341]\n",
      "loss: 0.356418  [24000/175341]\n",
      "loss: 0.168690  [25600/175341]\n",
      "loss: 0.489222  [27200/175341]\n",
      "loss: 0.201021  [28800/175341]\n",
      "loss: 0.762526  [30400/175341]\n",
      "loss: 0.243614  [32000/175341]\n",
      "loss: 0.399213  [33600/175341]\n",
      "loss: 0.464688  [35200/175341]\n",
      "loss: 0.555348  [36800/175341]\n",
      "loss: 0.756382  [38400/175341]\n",
      "loss: 0.574376  [40000/175341]\n",
      "loss: 0.957766  [41600/175341]\n",
      "loss: 0.878031  [43200/175341]\n",
      "loss: 0.224150  [44800/175341]\n",
      "loss: 0.855517  [46400/175341]\n",
      "loss: 1.063734  [48000/175341]\n",
      "loss: 0.459902  [49600/175341]\n",
      "loss: 0.688413  [51200/175341]\n",
      "loss: 0.344896  [52800/175341]\n",
      "loss: 0.265390  [54400/175341]\n",
      "loss: 0.576628  [56000/175341]\n",
      "loss: 0.713021  [57600/175341]\n",
      "loss: 0.476806  [59200/175341]\n",
      "loss: 0.374742  [60800/175341]\n",
      "loss: 0.553796  [62400/175341]\n",
      "loss: 0.322478  [64000/175341]\n",
      "loss: 0.479540  [65600/175341]\n",
      "loss: 0.739062  [67200/175341]\n",
      "loss: 0.857259  [68800/175341]\n",
      "loss: 0.127578  [70400/175341]\n",
      "loss: 0.665391  [72000/175341]\n",
      "loss: 0.546672  [73600/175341]\n",
      "loss: 0.516098  [75200/175341]\n",
      "loss: 0.320399  [76800/175341]\n",
      "loss: 0.273213  [78400/175341]\n",
      "loss: 0.275613  [80000/175341]\n",
      "loss: 0.445090  [81600/175341]\n",
      "loss: 0.435636  [83200/175341]\n",
      "loss: 0.468754  [84800/175341]\n",
      "loss: 0.363402  [86400/175341]\n",
      "loss: 0.470853  [88000/175341]\n",
      "loss: 0.245068  [89600/175341]\n",
      "loss: 0.430835  [91200/175341]\n",
      "loss: 0.592354  [92800/175341]\n",
      "loss: 0.195559  [94400/175341]\n",
      "loss: 0.483712  [96000/175341]\n",
      "loss: 1.226420  [97600/175341]\n",
      "loss: 0.228742  [99200/175341]\n",
      "loss: 0.518071  [100800/175341]\n",
      "loss: 0.322437  [102400/175341]\n",
      "loss: 0.380343  [104000/175341]\n",
      "loss: 0.562270  [105600/175341]\n",
      "loss: 0.408494  [107200/175341]\n",
      "loss: 0.577383  [108800/175341]\n",
      "loss: 0.530044  [110400/175341]\n",
      "loss: 0.562689  [112000/175341]\n",
      "loss: 0.462666  [113600/175341]\n",
      "loss: 0.591968  [115200/175341]\n",
      "loss: 0.443194  [116800/175341]\n",
      "loss: 0.520087  [118400/175341]\n",
      "loss: 0.341590  [120000/175341]\n",
      "loss: 0.949272  [121600/175341]\n",
      "loss: 0.537289  [123200/175341]\n",
      "loss: 0.512374  [124800/175341]\n",
      "loss: 0.385627  [126400/175341]\n",
      "loss: 0.296963  [128000/175341]\n",
      "loss: 0.534433  [129600/175341]\n",
      "loss: 0.278287  [131200/175341]\n",
      "loss: 0.318280  [132800/175341]\n",
      "loss: 0.365839  [134400/175341]\n",
      "loss: 0.225866  [136000/175341]\n",
      "loss: 0.191897  [137600/175341]\n",
      "loss: 0.434524  [139200/175341]\n",
      "loss: 0.734077  [140800/175341]\n",
      "loss: 0.139737  [142400/175341]\n",
      "loss: 0.816710  [144000/175341]\n",
      "loss: 0.870467  [145600/175341]\n",
      "loss: 0.340503  [147200/175341]\n",
      "loss: 0.385048  [148800/175341]\n",
      "loss: 0.227465  [150400/175341]\n",
      "loss: 0.418039  [152000/175341]\n",
      "loss: 0.302941  [153600/175341]\n",
      "loss: 0.452985  [155200/175341]\n",
      "loss: 0.638806  [156800/175341]\n",
      "loss: 0.653130  [158400/175341]\n",
      "loss: 0.558409  [160000/175341]\n",
      "loss: 0.668067  [161600/175341]\n",
      "loss: 0.545452  [163200/175341]\n",
      "loss: 0.797902  [164800/175341]\n",
      "loss: 0.419771  [166400/175341]\n",
      "loss: 1.386868  [168000/175341]\n",
      "loss: 0.316720  [169600/175341]\n",
      "loss: 0.213187  [171200/175341]\n",
      "loss: 0.607411  [172800/175341]\n",
      "loss: 0.247305  [174400/175341]\n",
      "Train Accuracy: 80.1849%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.600267, F1-score: 75.51%, Macro_F1-Score:  38.88%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.523724  [    0/175341]\n",
      "loss: 0.275661  [ 1600/175341]\n",
      "loss: 0.186138  [ 3200/175341]\n",
      "loss: 0.435828  [ 4800/175341]\n",
      "loss: 0.685867  [ 6400/175341]\n",
      "loss: 0.415182  [ 8000/175341]\n",
      "loss: 0.507487  [ 9600/175341]\n",
      "loss: 1.031551  [11200/175341]\n",
      "loss: 0.633071  [12800/175341]\n",
      "loss: 0.184347  [14400/175341]\n",
      "loss: 0.328967  [16000/175341]\n",
      "loss: 0.655033  [17600/175341]\n",
      "loss: 0.390855  [19200/175341]\n",
      "loss: 0.567454  [20800/175341]\n",
      "loss: 0.559368  [22400/175341]\n",
      "loss: 0.423304  [24000/175341]\n",
      "loss: 0.222745  [25600/175341]\n",
      "loss: 0.762153  [27200/175341]\n",
      "loss: 0.191424  [28800/175341]\n",
      "loss: 0.266828  [30400/175341]\n",
      "loss: 0.952248  [32000/175341]\n",
      "loss: 0.646053  [33600/175341]\n",
      "loss: 0.389240  [35200/175341]\n",
      "loss: 0.399139  [36800/175341]\n",
      "loss: 0.699905  [38400/175341]\n",
      "loss: 0.759874  [40000/175341]\n",
      "loss: 0.653053  [41600/175341]\n",
      "loss: 0.391308  [43200/175341]\n",
      "loss: 0.251785  [44800/175341]\n",
      "loss: 0.448543  [46400/175341]\n",
      "loss: 0.611457  [48000/175341]\n",
      "loss: 0.266644  [49600/175341]\n",
      "loss: 0.322391  [51200/175341]\n",
      "loss: 0.305241  [52800/175341]\n",
      "loss: 0.447956  [54400/175341]\n",
      "loss: 0.354453  [56000/175341]\n",
      "loss: 0.770826  [57600/175341]\n",
      "loss: 0.201056  [59200/175341]\n",
      "loss: 0.975171  [60800/175341]\n",
      "loss: 0.314208  [62400/175341]\n",
      "loss: 0.666601  [64000/175341]\n",
      "loss: 0.286437  [65600/175341]\n",
      "loss: 0.498231  [67200/175341]\n",
      "loss: 0.592256  [68800/175341]\n",
      "loss: 0.222227  [70400/175341]\n",
      "loss: 0.849726  [72000/175341]\n",
      "loss: 0.549420  [73600/175341]\n",
      "loss: 0.610358  [75200/175341]\n",
      "loss: 0.295362  [76800/175341]\n",
      "loss: 0.512522  [78400/175341]\n",
      "loss: 0.348326  [80000/175341]\n",
      "loss: 0.394992  [81600/175341]\n",
      "loss: 0.740782  [83200/175341]\n",
      "loss: 0.337900  [84800/175341]\n",
      "loss: 0.338886  [86400/175341]\n",
      "loss: 0.425776  [88000/175341]\n",
      "loss: 0.409693  [89600/175341]\n",
      "loss: 0.565502  [91200/175341]\n",
      "loss: 0.249047  [92800/175341]\n",
      "loss: 0.620252  [94400/175341]\n",
      "loss: 0.433638  [96000/175341]\n",
      "loss: 0.848624  [97600/175341]\n",
      "loss: 0.570572  [99200/175341]\n",
      "loss: 0.756215  [100800/175341]\n",
      "loss: 0.786223  [102400/175341]\n",
      "loss: 0.780519  [104000/175341]\n",
      "loss: 0.378352  [105600/175341]\n",
      "loss: 0.340489  [107200/175341]\n",
      "loss: 0.186320  [108800/175341]\n",
      "loss: 0.563994  [110400/175341]\n",
      "loss: 0.623348  [112000/175341]\n",
      "loss: 0.430841  [113600/175341]\n",
      "loss: 0.798448  [115200/175341]\n",
      "loss: 0.573900  [116800/175341]\n",
      "loss: 0.262170  [118400/175341]\n",
      "loss: 0.228562  [120000/175341]\n",
      "loss: 0.643492  [121600/175341]\n",
      "loss: 0.331432  [123200/175341]\n",
      "loss: 0.477588  [124800/175341]\n",
      "loss: 0.414287  [126400/175341]\n",
      "loss: 0.513087  [128000/175341]\n",
      "loss: 0.387292  [129600/175341]\n",
      "loss: 0.372861  [131200/175341]\n",
      "loss: 0.764277  [132800/175341]\n",
      "loss: 0.626811  [134400/175341]\n",
      "loss: 0.541895  [136000/175341]\n",
      "loss: 0.564333  [137600/175341]\n",
      "loss: 0.505680  [139200/175341]\n",
      "loss: 0.441438  [140800/175341]\n",
      "loss: 0.499786  [142400/175341]\n",
      "loss: 0.227416  [144000/175341]\n",
      "loss: 0.587413  [145600/175341]\n",
      "loss: 0.154951  [147200/175341]\n",
      "loss: 0.517576  [148800/175341]\n",
      "loss: 0.874397  [150400/175341]\n",
      "loss: 0.597622  [152000/175341]\n",
      "loss: 0.462035  [153600/175341]\n",
      "loss: 0.471677  [155200/175341]\n",
      "loss: 0.265980  [156800/175341]\n",
      "loss: 0.515396  [158400/175341]\n",
      "loss: 0.525417  [160000/175341]\n",
      "loss: 0.291859  [161600/175341]\n",
      "loss: 0.956033  [163200/175341]\n",
      "loss: 0.524060  [164800/175341]\n",
      "loss: 0.786299  [166400/175341]\n",
      "loss: 0.369281  [168000/175341]\n",
      "loss: 0.470350  [169600/175341]\n",
      "loss: 0.755053  [171200/175341]\n",
      "loss: 0.519006  [172800/175341]\n",
      "loss: 0.951415  [174400/175341]\n",
      "Train Accuracy: 80.2630%\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.635639, F1-score: 73.11%, Macro_F1-Score:  38.30%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.764936  [    0/175341]\n",
      "loss: 0.584576  [ 1600/175341]\n",
      "loss: 0.225543  [ 3200/175341]\n",
      "loss: 0.043924  [ 4800/175341]\n",
      "loss: 0.263493  [ 6400/175341]\n",
      "loss: 0.422805  [ 8000/175341]\n",
      "loss: 0.442324  [ 9600/175341]\n",
      "loss: 0.339179  [11200/175341]\n",
      "loss: 0.737001  [12800/175341]\n",
      "loss: 0.446584  [14400/175341]\n",
      "loss: 0.575952  [16000/175341]\n",
      "loss: 0.490924  [17600/175341]\n",
      "loss: 0.567125  [19200/175341]\n",
      "loss: 0.860714  [20800/175341]\n",
      "loss: 0.718391  [22400/175341]\n",
      "loss: 0.546329  [24000/175341]\n",
      "loss: 0.326033  [25600/175341]\n",
      "loss: 0.685857  [27200/175341]\n",
      "loss: 0.487803  [28800/175341]\n",
      "loss: 0.124389  [30400/175341]\n",
      "loss: 0.320153  [32000/175341]\n",
      "loss: 0.490012  [33600/175341]\n",
      "loss: 0.550991  [35200/175341]\n",
      "loss: 0.498594  [36800/175341]\n",
      "loss: 0.510921  [38400/175341]\n",
      "loss: 0.946795  [40000/175341]\n",
      "loss: 0.688001  [41600/175341]\n",
      "loss: 0.597600  [43200/175341]\n",
      "loss: 0.428572  [44800/175341]\n",
      "loss: 0.767042  [46400/175341]\n",
      "loss: 0.279644  [48000/175341]\n",
      "loss: 0.560959  [49600/175341]\n",
      "loss: 0.553070  [51200/175341]\n",
      "loss: 0.113368  [52800/175341]\n",
      "loss: 0.216907  [54400/175341]\n",
      "loss: 0.686732  [56000/175341]\n",
      "loss: 0.785260  [57600/175341]\n",
      "loss: 0.601817  [59200/175341]\n",
      "loss: 0.491498  [60800/175341]\n",
      "loss: 0.549082  [62400/175341]\n",
      "loss: 0.213830  [64000/175341]\n",
      "loss: 0.944458  [65600/175341]\n",
      "loss: 0.132971  [67200/175341]\n",
      "loss: 0.225467  [68800/175341]\n",
      "loss: 0.460198  [70400/175341]\n",
      "loss: 0.383661  [72000/175341]\n",
      "loss: 0.662664  [73600/175341]\n",
      "loss: 1.006827  [75200/175341]\n",
      "loss: 0.575503  [76800/175341]\n",
      "loss: 0.249089  [78400/175341]\n",
      "loss: 0.646899  [80000/175341]\n",
      "loss: 0.587397  [81600/175341]\n",
      "loss: 0.435031  [83200/175341]\n",
      "loss: 0.362777  [84800/175341]\n",
      "loss: 0.242230  [86400/175341]\n",
      "loss: 0.858672  [88000/175341]\n",
      "loss: 0.465708  [89600/175341]\n",
      "loss: 0.471485  [91200/175341]\n",
      "loss: 0.283917  [92800/175341]\n",
      "loss: 0.768457  [94400/175341]\n",
      "loss: 0.620837  [96000/175341]\n",
      "loss: 0.321928  [97600/175341]\n",
      "loss: 0.625132  [99200/175341]\n",
      "loss: 0.618721  [100800/175341]\n",
      "loss: 0.435097  [102400/175341]\n",
      "loss: 0.396855  [104000/175341]\n",
      "loss: 0.675007  [105600/175341]\n",
      "loss: 0.416540  [107200/175341]\n",
      "loss: 0.367282  [108800/175341]\n",
      "loss: 0.781302  [110400/175341]\n",
      "loss: 0.310013  [112000/175341]\n",
      "loss: 0.585076  [113600/175341]\n",
      "loss: 0.945793  [115200/175341]\n",
      "loss: 0.480189  [116800/175341]\n",
      "loss: 0.301556  [118400/175341]\n",
      "loss: 0.438402  [120000/175341]\n",
      "loss: 0.118438  [121600/175341]\n",
      "loss: 0.538303  [123200/175341]\n",
      "loss: 0.587739  [124800/175341]\n",
      "loss: 0.349559  [126400/175341]\n",
      "loss: 0.883129  [128000/175341]\n",
      "loss: 1.043524  [129600/175341]\n",
      "loss: 0.131139  [131200/175341]\n",
      "loss: 0.465211  [132800/175341]\n",
      "loss: 0.179918  [134400/175341]\n",
      "loss: 0.416171  [136000/175341]\n",
      "loss: 0.485380  [137600/175341]\n",
      "loss: 0.523173  [139200/175341]\n",
      "loss: 0.385359  [140800/175341]\n",
      "loss: 0.550483  [142400/175341]\n",
      "loss: 0.394758  [144000/175341]\n",
      "loss: 0.372402  [145600/175341]\n",
      "loss: 1.091677  [147200/175341]\n",
      "loss: 0.675919  [148800/175341]\n",
      "loss: 0.331351  [150400/175341]\n",
      "loss: 0.196003  [152000/175341]\n",
      "loss: 0.206046  [153600/175341]\n",
      "loss: 1.242131  [155200/175341]\n",
      "loss: 0.329200  [156800/175341]\n",
      "loss: 0.434385  [158400/175341]\n",
      "loss: 0.338630  [160000/175341]\n",
      "loss: 0.735690  [161600/175341]\n",
      "loss: 0.831612  [163200/175341]\n",
      "loss: 0.180311  [164800/175341]\n",
      "loss: 0.531755  [166400/175341]\n",
      "loss: 0.347142  [168000/175341]\n",
      "loss: 0.336795  [169600/175341]\n",
      "loss: 0.426447  [171200/175341]\n",
      "loss: 0.212583  [172800/175341]\n",
      "loss: 0.678115  [174400/175341]\n",
      "Train Accuracy: 80.3440%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.588942, F1-score: 75.06%, Macro_F1-Score:  39.76%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.572804  [    0/175341]\n",
      "loss: 0.484280  [ 1600/175341]\n",
      "loss: 0.854828  [ 3200/175341]\n",
      "loss: 0.379913  [ 4800/175341]\n",
      "loss: 0.183113  [ 6400/175341]\n",
      "loss: 0.554826  [ 8000/175341]\n",
      "loss: 0.290555  [ 9600/175341]\n",
      "loss: 0.616663  [11200/175341]\n",
      "loss: 0.231552  [12800/175341]\n",
      "loss: 0.865153  [14400/175341]\n",
      "loss: 0.939271  [16000/175341]\n",
      "loss: 0.427789  [17600/175341]\n",
      "loss: 0.678147  [19200/175341]\n",
      "loss: 0.658074  [20800/175341]\n",
      "loss: 0.191356  [22400/175341]\n",
      "loss: 0.402935  [24000/175341]\n",
      "loss: 0.474746  [25600/175341]\n",
      "loss: 0.630977  [27200/175341]\n",
      "loss: 0.330241  [28800/175341]\n",
      "loss: 0.436479  [30400/175341]\n",
      "loss: 0.482450  [32000/175341]\n",
      "loss: 0.993009  [33600/175341]\n",
      "loss: 0.279558  [35200/175341]\n",
      "loss: 0.431164  [36800/175341]\n",
      "loss: 0.347163  [38400/175341]\n",
      "loss: 0.394658  [40000/175341]\n",
      "loss: 0.784896  [41600/175341]\n",
      "loss: 0.584258  [43200/175341]\n",
      "loss: 0.352609  [44800/175341]\n",
      "loss: 0.749301  [46400/175341]\n",
      "loss: 0.366908  [48000/175341]\n",
      "loss: 0.246624  [49600/175341]\n",
      "loss: 0.574406  [51200/175341]\n",
      "loss: 0.303678  [52800/175341]\n",
      "loss: 0.307790  [54400/175341]\n",
      "loss: 0.381544  [56000/175341]\n",
      "loss: 0.277015  [57600/175341]\n",
      "loss: 0.472432  [59200/175341]\n",
      "loss: 0.328294  [60800/175341]\n",
      "loss: 0.243683  [62400/175341]\n",
      "loss: 0.210554  [64000/175341]\n",
      "loss: 0.408891  [65600/175341]\n",
      "loss: 0.745585  [67200/175341]\n",
      "loss: 0.595329  [68800/175341]\n",
      "loss: 0.309600  [70400/175341]\n",
      "loss: 0.430527  [72000/175341]\n",
      "loss: 0.282453  [73600/175341]\n",
      "loss: 0.433768  [75200/175341]\n",
      "loss: 0.831392  [76800/175341]\n",
      "loss: 0.404939  [78400/175341]\n",
      "loss: 1.036383  [80000/175341]\n",
      "loss: 0.717614  [81600/175341]\n",
      "loss: 0.468460  [83200/175341]\n",
      "loss: 0.451690  [84800/175341]\n",
      "loss: 0.220319  [86400/175341]\n",
      "loss: 0.208053  [88000/175341]\n",
      "loss: 0.590985  [89600/175341]\n",
      "loss: 0.464868  [91200/175341]\n",
      "loss: 0.343735  [92800/175341]\n",
      "loss: 0.646134  [94400/175341]\n",
      "loss: 0.415001  [96000/175341]\n",
      "loss: 0.746161  [97600/175341]\n",
      "loss: 0.914481  [99200/175341]\n",
      "loss: 0.559184  [100800/175341]\n",
      "loss: 0.595402  [102400/175341]\n",
      "loss: 0.503524  [104000/175341]\n",
      "loss: 0.267737  [105600/175341]\n",
      "loss: 0.258620  [107200/175341]\n",
      "loss: 0.899413  [108800/175341]\n",
      "loss: 0.909277  [110400/175341]\n",
      "loss: 0.295835  [112000/175341]\n",
      "loss: 0.276832  [113600/175341]\n",
      "loss: 0.557377  [115200/175341]\n",
      "loss: 0.733384  [116800/175341]\n",
      "loss: 0.414249  [118400/175341]\n",
      "loss: 0.859098  [120000/175341]\n",
      "loss: 0.300468  [121600/175341]\n",
      "loss: 0.610353  [123200/175341]\n",
      "loss: 0.205056  [124800/175341]\n",
      "loss: 0.925189  [126400/175341]\n",
      "loss: 0.545183  [128000/175341]\n",
      "loss: 0.259450  [129600/175341]\n",
      "loss: 0.420352  [131200/175341]\n",
      "loss: 0.360089  [132800/175341]\n",
      "loss: 0.381141  [134400/175341]\n",
      "loss: 0.449869  [136000/175341]\n",
      "loss: 0.474497  [137600/175341]\n",
      "loss: 0.680443  [139200/175341]\n",
      "loss: 0.495286  [140800/175341]\n",
      "loss: 0.618787  [142400/175341]\n",
      "loss: 0.432927  [144000/175341]\n",
      "loss: 0.147284  [145600/175341]\n",
      "loss: 0.662805  [147200/175341]\n",
      "loss: 0.398762  [148800/175341]\n",
      "loss: 0.093513  [150400/175341]\n",
      "loss: 0.613418  [152000/175341]\n",
      "loss: 0.318752  [153600/175341]\n",
      "loss: 0.360777  [155200/175341]\n",
      "loss: 0.380920  [156800/175341]\n",
      "loss: 0.891771  [158400/175341]\n",
      "loss: 0.416650  [160000/175341]\n",
      "loss: 0.393731  [161600/175341]\n",
      "loss: 0.383251  [163200/175341]\n",
      "loss: 0.857395  [164800/175341]\n",
      "loss: 0.328166  [166400/175341]\n",
      "loss: 0.394342  [168000/175341]\n",
      "loss: 0.859860  [169600/175341]\n",
      "loss: 0.315449  [171200/175341]\n",
      "loss: 0.459558  [172800/175341]\n",
      "loss: 0.285407  [174400/175341]\n",
      "Train Accuracy: 80.4096%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.603102, F1-score: 73.65%, Macro_F1-Score:  38.43%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.778768  [    0/175341]\n",
      "loss: 0.604663  [ 1600/175341]\n",
      "loss: 0.240326  [ 3200/175341]\n",
      "loss: 0.668144  [ 4800/175341]\n",
      "loss: 0.895148  [ 6400/175341]\n",
      "loss: 0.413627  [ 8000/175341]\n",
      "loss: 0.520171  [ 9600/175341]\n",
      "loss: 0.995909  [11200/175341]\n",
      "loss: 0.429013  [12800/175341]\n",
      "loss: 0.418502  [14400/175341]\n",
      "loss: 0.930973  [16000/175341]\n",
      "loss: 0.413117  [17600/175341]\n",
      "loss: 0.702207  [19200/175341]\n",
      "loss: 0.580258  [20800/175341]\n",
      "loss: 0.715770  [22400/175341]\n",
      "loss: 0.568656  [24000/175341]\n",
      "loss: 0.528772  [25600/175341]\n",
      "loss: 0.457168  [27200/175341]\n",
      "loss: 0.487366  [28800/175341]\n",
      "loss: 0.344867  [30400/175341]\n",
      "loss: 0.753336  [32000/175341]\n",
      "loss: 0.204792  [33600/175341]\n",
      "loss: 0.377092  [35200/175341]\n",
      "loss: 0.449117  [36800/175341]\n",
      "loss: 0.779321  [38400/175341]\n",
      "loss: 0.549770  [40000/175341]\n",
      "loss: 0.772923  [41600/175341]\n",
      "loss: 0.423820  [43200/175341]\n",
      "loss: 0.671581  [44800/175341]\n",
      "loss: 0.453042  [46400/175341]\n",
      "loss: 0.474354  [48000/175341]\n",
      "loss: 0.164831  [49600/175341]\n",
      "loss: 0.561226  [51200/175341]\n",
      "loss: 0.367464  [52800/175341]\n",
      "loss: 0.628148  [54400/175341]\n",
      "loss: 0.370222  [56000/175341]\n",
      "loss: 0.424346  [57600/175341]\n",
      "loss: 0.197308  [59200/175341]\n",
      "loss: 0.205681  [60800/175341]\n",
      "loss: 0.388950  [62400/175341]\n",
      "loss: 0.444965  [64000/175341]\n",
      "loss: 0.296609  [65600/175341]\n",
      "loss: 0.370555  [67200/175341]\n",
      "loss: 0.277797  [68800/175341]\n",
      "loss: 0.241176  [70400/175341]\n",
      "loss: 0.430794  [72000/175341]\n",
      "loss: 0.737993  [73600/175341]\n",
      "loss: 0.045704  [75200/175341]\n",
      "loss: 0.391386  [76800/175341]\n",
      "loss: 0.294210  [78400/175341]\n",
      "loss: 0.406168  [80000/175341]\n",
      "loss: 0.296998  [81600/175341]\n",
      "loss: 0.235921  [83200/175341]\n",
      "loss: 0.872060  [84800/175341]\n",
      "loss: 0.533651  [86400/175341]\n",
      "loss: 0.568254  [88000/175341]\n",
      "loss: 0.526862  [89600/175341]\n",
      "loss: 0.804564  [91200/175341]\n",
      "loss: 0.403324  [92800/175341]\n",
      "loss: 0.359404  [94400/175341]\n",
      "loss: 0.510026  [96000/175341]\n",
      "loss: 0.396939  [97600/175341]\n",
      "loss: 0.498249  [99200/175341]\n",
      "loss: 0.467988  [100800/175341]\n",
      "loss: 0.550440  [102400/175341]\n",
      "loss: 0.298334  [104000/175341]\n",
      "loss: 0.465449  [105600/175341]\n",
      "loss: 0.584183  [107200/175341]\n",
      "loss: 0.402704  [108800/175341]\n",
      "loss: 0.319287  [110400/175341]\n",
      "loss: 0.346580  [112000/175341]\n",
      "loss: 0.212158  [113600/175341]\n",
      "loss: 0.638144  [115200/175341]\n",
      "loss: 0.348791  [116800/175341]\n",
      "loss: 0.123680  [118400/175341]\n",
      "loss: 0.414819  [120000/175341]\n",
      "loss: 0.543106  [121600/175341]\n",
      "loss: 0.319092  [123200/175341]\n",
      "loss: 0.906543  [124800/175341]\n",
      "loss: 0.355055  [126400/175341]\n",
      "loss: 0.614398  [128000/175341]\n",
      "loss: 0.328571  [129600/175341]\n",
      "loss: 0.361923  [131200/175341]\n",
      "loss: 0.627841  [132800/175341]\n",
      "loss: 1.148364  [134400/175341]\n",
      "loss: 0.674256  [136000/175341]\n",
      "loss: 0.458263  [137600/175341]\n",
      "loss: 0.201018  [139200/175341]\n",
      "loss: 0.304974  [140800/175341]\n",
      "loss: 0.419125  [142400/175341]\n",
      "loss: 0.262409  [144000/175341]\n",
      "loss: 0.236497  [145600/175341]\n",
      "loss: 0.457985  [147200/175341]\n",
      "loss: 0.794986  [148800/175341]\n",
      "loss: 0.547533  [150400/175341]\n",
      "loss: 0.189428  [152000/175341]\n",
      "loss: 0.326378  [153600/175341]\n",
      "loss: 0.202059  [155200/175341]\n",
      "loss: 0.578811  [156800/175341]\n",
      "loss: 0.274487  [158400/175341]\n",
      "loss: 0.931327  [160000/175341]\n",
      "loss: 0.691353  [161600/175341]\n",
      "loss: 0.512776  [163200/175341]\n",
      "loss: 0.420447  [164800/175341]\n",
      "loss: 0.495088  [166400/175341]\n",
      "loss: 0.187795  [168000/175341]\n",
      "loss: 0.425974  [169600/175341]\n",
      "loss: 0.411366  [171200/175341]\n",
      "loss: 0.996188  [172800/175341]\n",
      "loss: 0.504231  [174400/175341]\n",
      "Train Accuracy: 80.4609%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.584508, F1-score: 75.15%, Macro_F1-Score:  39.60%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.575330  [    0/175341]\n",
      "loss: 0.247824  [ 1600/175341]\n",
      "loss: 0.641335  [ 3200/175341]\n",
      "loss: 0.423025  [ 4800/175341]\n",
      "loss: 0.359537  [ 6400/175341]\n",
      "loss: 0.388003  [ 8000/175341]\n",
      "loss: 0.996907  [ 9600/175341]\n",
      "loss: 0.666296  [11200/175341]\n",
      "loss: 0.733993  [12800/175341]\n",
      "loss: 0.558580  [14400/175341]\n",
      "loss: 0.978823  [16000/175341]\n",
      "loss: 0.617468  [17600/175341]\n",
      "loss: 0.683304  [19200/175341]\n",
      "loss: 0.269256  [20800/175341]\n",
      "loss: 0.434804  [22400/175341]\n",
      "loss: 0.830755  [24000/175341]\n",
      "loss: 0.318216  [25600/175341]\n",
      "loss: 0.169441  [27200/175341]\n",
      "loss: 0.220241  [28800/175341]\n",
      "loss: 0.708910  [30400/175341]\n",
      "loss: 0.571539  [32000/175341]\n",
      "loss: 0.574960  [33600/175341]\n",
      "loss: 0.489021  [35200/175341]\n",
      "loss: 0.438738  [36800/175341]\n",
      "loss: 0.420103  [38400/175341]\n",
      "loss: 0.770886  [40000/175341]\n",
      "loss: 0.649637  [41600/175341]\n",
      "loss: 0.405266  [43200/175341]\n",
      "loss: 0.589417  [44800/175341]\n",
      "loss: 0.682529  [46400/175341]\n",
      "loss: 0.682776  [48000/175341]\n",
      "loss: 0.660742  [49600/175341]\n",
      "loss: 0.466146  [51200/175341]\n",
      "loss: 0.197596  [52800/175341]\n",
      "loss: 0.605180  [54400/175341]\n",
      "loss: 0.319931  [56000/175341]\n",
      "loss: 0.180999  [57600/175341]\n",
      "loss: 0.648782  [59200/175341]\n",
      "loss: 0.988264  [60800/175341]\n",
      "loss: 0.433553  [62400/175341]\n",
      "loss: 0.274179  [64000/175341]\n",
      "loss: 0.462050  [65600/175341]\n",
      "loss: 0.505719  [67200/175341]\n",
      "loss: 0.456359  [68800/175341]\n",
      "loss: 0.387054  [70400/175341]\n",
      "loss: 0.267667  [72000/175341]\n",
      "loss: 0.732597  [73600/175341]\n",
      "loss: 0.513629  [75200/175341]\n",
      "loss: 0.224323  [76800/175341]\n",
      "loss: 0.440813  [78400/175341]\n",
      "loss: 0.514786  [80000/175341]\n",
      "loss: 0.824267  [81600/175341]\n",
      "loss: 0.636514  [83200/175341]\n",
      "loss: 0.419214  [84800/175341]\n",
      "loss: 0.593110  [86400/175341]\n",
      "loss: 0.428969  [88000/175341]\n",
      "loss: 0.348480  [89600/175341]\n",
      "loss: 0.719888  [91200/175341]\n",
      "loss: 0.358298  [92800/175341]\n",
      "loss: 0.510265  [94400/175341]\n",
      "loss: 0.496024  [96000/175341]\n",
      "loss: 0.565560  [97600/175341]\n",
      "loss: 0.489894  [99200/175341]\n",
      "loss: 0.341265  [100800/175341]\n",
      "loss: 0.340969  [102400/175341]\n",
      "loss: 0.740847  [104000/175341]\n",
      "loss: 0.481917  [105600/175341]\n",
      "loss: 0.549099  [107200/175341]\n",
      "loss: 1.005026  [108800/175341]\n",
      "loss: 0.589065  [110400/175341]\n",
      "loss: 0.398510  [112000/175341]\n",
      "loss: 0.234331  [113600/175341]\n",
      "loss: 0.218229  [115200/175341]\n",
      "loss: 0.530221  [116800/175341]\n",
      "loss: 1.083671  [118400/175341]\n",
      "loss: 0.618525  [120000/175341]\n",
      "loss: 0.468766  [121600/175341]\n",
      "loss: 0.248420  [123200/175341]\n",
      "loss: 0.681570  [124800/175341]\n",
      "loss: 0.401022  [126400/175341]\n",
      "loss: 0.270891  [128000/175341]\n",
      "loss: 0.285458  [129600/175341]\n",
      "loss: 0.930703  [131200/175341]\n",
      "loss: 0.780123  [132800/175341]\n",
      "loss: 0.812387  [134400/175341]\n",
      "loss: 0.450465  [136000/175341]\n",
      "loss: 0.241199  [137600/175341]\n",
      "loss: 0.363026  [139200/175341]\n",
      "loss: 0.588917  [140800/175341]\n",
      "loss: 0.255521  [142400/175341]\n",
      "loss: 0.170755  [144000/175341]\n",
      "loss: 0.120365  [145600/175341]\n",
      "loss: 0.410061  [147200/175341]\n",
      "loss: 0.440105  [148800/175341]\n",
      "loss: 0.689606  [150400/175341]\n",
      "loss: 0.733730  [152000/175341]\n",
      "loss: 0.306294  [153600/175341]\n",
      "loss: 0.321162  [155200/175341]\n",
      "loss: 0.203808  [156800/175341]\n",
      "loss: 0.214829  [158400/175341]\n",
      "loss: 0.211788  [160000/175341]\n",
      "loss: 0.787546  [161600/175341]\n",
      "loss: 0.816544  [163200/175341]\n",
      "loss: 0.668172  [164800/175341]\n",
      "loss: 0.458770  [166400/175341]\n",
      "loss: 0.685457  [168000/175341]\n",
      "loss: 0.698837  [169600/175341]\n",
      "loss: 0.507163  [171200/175341]\n",
      "loss: 0.379928  [172800/175341]\n",
      "loss: 0.222542  [174400/175341]\n",
      "Train Accuracy: 80.4547%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.588101, F1-score: 75.14%, Macro_F1-Score:  38.98%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.612828  [    0/175341]\n",
      "loss: 0.245240  [ 1600/175341]\n",
      "loss: 0.261229  [ 3200/175341]\n",
      "loss: 0.507536  [ 4800/175341]\n",
      "loss: 0.241075  [ 6400/175341]\n",
      "loss: 0.318309  [ 8000/175341]\n",
      "loss: 0.492703  [ 9600/175341]\n",
      "loss: 0.666319  [11200/175341]\n",
      "loss: 0.387035  [12800/175341]\n",
      "loss: 0.257912  [14400/175341]\n",
      "loss: 0.483206  [16000/175341]\n",
      "loss: 0.139848  [17600/175341]\n",
      "loss: 0.279125  [19200/175341]\n",
      "loss: 0.435903  [20800/175341]\n",
      "loss: 0.268022  [22400/175341]\n",
      "loss: 0.340557  [24000/175341]\n",
      "loss: 0.456600  [25600/175341]\n",
      "loss: 0.334230  [27200/175341]\n",
      "loss: 0.534371  [28800/175341]\n",
      "loss: 0.439191  [30400/175341]\n",
      "loss: 0.338239  [32000/175341]\n",
      "loss: 0.099946  [33600/175341]\n",
      "loss: 0.501909  [35200/175341]\n",
      "loss: 0.760285  [36800/175341]\n",
      "loss: 0.659576  [38400/175341]\n",
      "loss: 0.523778  [40000/175341]\n",
      "loss: 0.523568  [41600/175341]\n",
      "loss: 0.520005  [43200/175341]\n",
      "loss: 0.719539  [44800/175341]\n",
      "loss: 0.693460  [46400/175341]\n",
      "loss: 0.714462  [48000/175341]\n",
      "loss: 0.388745  [49600/175341]\n",
      "loss: 0.511467  [51200/175341]\n",
      "loss: 0.419652  [52800/175341]\n",
      "loss: 0.767915  [54400/175341]\n",
      "loss: 0.592557  [56000/175341]\n",
      "loss: 0.365361  [57600/175341]\n",
      "loss: 0.559115  [59200/175341]\n",
      "loss: 0.827366  [60800/175341]\n",
      "loss: 0.503539  [62400/175341]\n",
      "loss: 0.230049  [64000/175341]\n",
      "loss: 0.531263  [65600/175341]\n",
      "loss: 0.545563  [67200/175341]\n",
      "loss: 0.526126  [68800/175341]\n",
      "loss: 0.558908  [70400/175341]\n",
      "loss: 0.281717  [72000/175341]\n",
      "loss: 0.402415  [73600/175341]\n",
      "loss: 0.483771  [75200/175341]\n",
      "loss: 0.496942  [76800/175341]\n",
      "loss: 0.552186  [78400/175341]\n",
      "loss: 0.404873  [80000/175341]\n",
      "loss: 0.358138  [81600/175341]\n",
      "loss: 0.276057  [83200/175341]\n",
      "loss: 0.516764  [84800/175341]\n",
      "loss: 0.491341  [86400/175341]\n",
      "loss: 0.157642  [88000/175341]\n",
      "loss: 0.151178  [89600/175341]\n",
      "loss: 0.822641  [91200/175341]\n",
      "loss: 0.940925  [92800/175341]\n",
      "loss: 0.382751  [94400/175341]\n",
      "loss: 0.668440  [96000/175341]\n",
      "loss: 0.442350  [97600/175341]\n",
      "loss: 0.744095  [99200/175341]\n",
      "loss: 0.363388  [100800/175341]\n",
      "loss: 0.754861  [102400/175341]\n",
      "loss: 0.424833  [104000/175341]\n",
      "loss: 0.702669  [105600/175341]\n",
      "loss: 0.192388  [107200/175341]\n",
      "loss: 0.872210  [108800/175341]\n",
      "loss: 0.645750  [110400/175341]\n",
      "loss: 0.865572  [112000/175341]\n",
      "loss: 0.291574  [113600/175341]\n",
      "loss: 0.508083  [115200/175341]\n",
      "loss: 0.515274  [116800/175341]\n",
      "loss: 0.543222  [118400/175341]\n",
      "loss: 0.856122  [120000/175341]\n",
      "loss: 1.068141  [121600/175341]\n",
      "loss: 0.565806  [123200/175341]\n",
      "loss: 0.173855  [124800/175341]\n",
      "loss: 0.948620  [126400/175341]\n",
      "loss: 0.381745  [128000/175341]\n",
      "loss: 0.167656  [129600/175341]\n",
      "loss: 0.387194  [131200/175341]\n",
      "loss: 0.678967  [132800/175341]\n",
      "loss: 0.201444  [134400/175341]\n",
      "loss: 0.605522  [136000/175341]\n",
      "loss: 0.273622  [137600/175341]\n",
      "loss: 0.743473  [139200/175341]\n",
      "loss: 0.387348  [140800/175341]\n",
      "loss: 0.293293  [142400/175341]\n",
      "loss: 0.536934  [144000/175341]\n",
      "loss: 0.691536  [145600/175341]\n",
      "loss: 0.265089  [147200/175341]\n",
      "loss: 0.718712  [148800/175341]\n",
      "loss: 0.712678  [150400/175341]\n",
      "loss: 0.475713  [152000/175341]\n",
      "loss: 0.303819  [153600/175341]\n",
      "loss: 0.488395  [155200/175341]\n",
      "loss: 0.797138  [156800/175341]\n",
      "loss: 0.470995  [158400/175341]\n",
      "loss: 0.319477  [160000/175341]\n",
      "loss: 0.965242  [161600/175341]\n",
      "loss: 0.110077  [163200/175341]\n",
      "loss: 0.261166  [164800/175341]\n",
      "loss: 0.250386  [166400/175341]\n",
      "loss: 0.336216  [168000/175341]\n",
      "loss: 0.449080  [169600/175341]\n",
      "loss: 0.497158  [171200/175341]\n",
      "loss: 0.248733  [172800/175341]\n",
      "loss: 0.443641  [174400/175341]\n",
      "Train Accuracy: 80.5431%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.585187, F1-score: 74.76%, Macro_F1-Score:  39.06%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.233422  [    0/175341]\n",
      "loss: 0.683665  [ 1600/175341]\n",
      "loss: 0.147726  [ 3200/175341]\n",
      "loss: 0.581858  [ 4800/175341]\n",
      "loss: 0.325168  [ 6400/175341]\n",
      "loss: 0.419549  [ 8000/175341]\n",
      "loss: 0.559411  [ 9600/175341]\n",
      "loss: 0.798820  [11200/175341]\n",
      "loss: 0.535886  [12800/175341]\n",
      "loss: 0.418794  [14400/175341]\n",
      "loss: 0.388809  [16000/175341]\n",
      "loss: 0.510157  [17600/175341]\n",
      "loss: 0.273630  [19200/175341]\n",
      "loss: 0.671546  [20800/175341]\n",
      "loss: 0.456840  [22400/175341]\n",
      "loss: 0.485553  [24000/175341]\n",
      "loss: 0.389665  [25600/175341]\n",
      "loss: 0.777910  [27200/175341]\n",
      "loss: 0.307589  [28800/175341]\n",
      "loss: 0.555797  [30400/175341]\n",
      "loss: 0.232700  [32000/175341]\n",
      "loss: 0.350232  [33600/175341]\n",
      "loss: 0.198204  [35200/175341]\n",
      "loss: 0.368330  [36800/175341]\n",
      "loss: 0.603118  [38400/175341]\n",
      "loss: 0.754637  [40000/175341]\n",
      "loss: 0.292028  [41600/175341]\n",
      "loss: 0.423257  [43200/175341]\n",
      "loss: 0.455377  [44800/175341]\n",
      "loss: 0.608968  [46400/175341]\n",
      "loss: 0.103346  [48000/175341]\n",
      "loss: 0.199800  [49600/175341]\n",
      "loss: 0.564042  [51200/175341]\n",
      "loss: 0.409435  [52800/175341]\n",
      "loss: 0.901757  [54400/175341]\n",
      "loss: 0.747365  [56000/175341]\n",
      "loss: 0.464132  [57600/175341]\n",
      "loss: 0.573276  [59200/175341]\n",
      "loss: 0.258390  [60800/175341]\n",
      "loss: 0.578281  [62400/175341]\n",
      "loss: 0.555207  [64000/175341]\n",
      "loss: 0.279901  [65600/175341]\n",
      "loss: 0.283744  [67200/175341]\n",
      "loss: 0.588194  [68800/175341]\n",
      "loss: 0.241682  [70400/175341]\n",
      "loss: 0.398966  [72000/175341]\n",
      "loss: 0.440444  [73600/175341]\n",
      "loss: 0.533169  [75200/175341]\n",
      "loss: 0.515742  [76800/175341]\n",
      "loss: 0.718474  [78400/175341]\n",
      "loss: 0.721211  [80000/175341]\n",
      "loss: 0.357453  [81600/175341]\n",
      "loss: 0.197277  [83200/175341]\n",
      "loss: 0.675225  [84800/175341]\n",
      "loss: 0.384294  [86400/175341]\n",
      "loss: 0.637273  [88000/175341]\n",
      "loss: 0.731343  [89600/175341]\n",
      "loss: 0.228585  [91200/175341]\n",
      "loss: 0.545729  [92800/175341]\n",
      "loss: 0.325580  [94400/175341]\n",
      "loss: 0.489024  [96000/175341]\n",
      "loss: 0.432406  [97600/175341]\n",
      "loss: 0.576224  [99200/175341]\n",
      "loss: 0.390091  [100800/175341]\n",
      "loss: 0.823466  [102400/175341]\n",
      "loss: 0.536485  [104000/175341]\n",
      "loss: 0.193902  [105600/175341]\n",
      "loss: 0.773563  [107200/175341]\n",
      "loss: 0.724708  [108800/175341]\n",
      "loss: 0.417410  [110400/175341]\n",
      "loss: 0.389222  [112000/175341]\n",
      "loss: 0.424979  [113600/175341]\n",
      "loss: 0.602999  [115200/175341]\n",
      "loss: 0.761696  [116800/175341]\n",
      "loss: 0.259644  [118400/175341]\n",
      "loss: 0.180351  [120000/175341]\n",
      "loss: 0.379091  [121600/175341]\n",
      "loss: 0.386209  [123200/175341]\n",
      "loss: 0.349849  [124800/175341]\n",
      "loss: 0.428803  [126400/175341]\n",
      "loss: 0.230541  [128000/175341]\n",
      "loss: 0.498672  [129600/175341]\n",
      "loss: 0.336670  [131200/175341]\n",
      "loss: 0.307090  [132800/175341]\n",
      "loss: 0.490989  [134400/175341]\n",
      "loss: 0.833702  [136000/175341]\n",
      "loss: 0.791532  [137600/175341]\n",
      "loss: 0.409142  [139200/175341]\n",
      "loss: 0.581284  [140800/175341]\n",
      "loss: 0.457130  [142400/175341]\n",
      "loss: 0.480697  [144000/175341]\n",
      "loss: 0.769608  [145600/175341]\n",
      "loss: 0.374523  [147200/175341]\n",
      "loss: 0.533636  [148800/175341]\n",
      "loss: 0.383391  [150400/175341]\n",
      "loss: 0.377261  [152000/175341]\n",
      "loss: 0.366944  [153600/175341]\n",
      "loss: 0.688555  [155200/175341]\n",
      "loss: 0.545219  [156800/175341]\n",
      "loss: 0.335138  [158400/175341]\n",
      "loss: 0.245166  [160000/175341]\n",
      "loss: 0.601209  [161600/175341]\n",
      "loss: 0.390354  [163200/175341]\n",
      "loss: 0.178403  [164800/175341]\n",
      "loss: 0.594249  [166400/175341]\n",
      "loss: 0.485081  [168000/175341]\n",
      "loss: 0.403585  [169600/175341]\n",
      "loss: 0.330720  [171200/175341]\n",
      "loss: 0.366327  [172800/175341]\n",
      "loss: 0.516063  [174400/175341]\n",
      "Train Accuracy: 80.5539%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.601032, F1-score: 74.16%, Macro_F1-Score:  38.54%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.529367  [    0/175341]\n",
      "loss: 0.807582  [ 1600/175341]\n",
      "loss: 0.327705  [ 3200/175341]\n",
      "loss: 0.410075  [ 4800/175341]\n",
      "loss: 0.095321  [ 6400/175341]\n",
      "loss: 0.518839  [ 8000/175341]\n",
      "loss: 0.884934  [ 9600/175341]\n",
      "loss: 0.633170  [11200/175341]\n",
      "loss: 0.572886  [12800/175341]\n",
      "loss: 0.307593  [14400/175341]\n",
      "loss: 0.548880  [16000/175341]\n",
      "loss: 0.615875  [17600/175341]\n",
      "loss: 1.003764  [19200/175341]\n",
      "loss: 0.543092  [20800/175341]\n",
      "loss: 0.223233  [22400/175341]\n",
      "loss: 0.333796  [24000/175341]\n",
      "loss: 0.463938  [25600/175341]\n",
      "loss: 0.381225  [27200/175341]\n",
      "loss: 0.929940  [28800/175341]\n",
      "loss: 0.327332  [30400/175341]\n",
      "loss: 0.288061  [32000/175341]\n",
      "loss: 0.511664  [33600/175341]\n",
      "loss: 0.821397  [35200/175341]\n",
      "loss: 0.106513  [36800/175341]\n",
      "loss: 0.646882  [38400/175341]\n",
      "loss: 0.489264  [40000/175341]\n",
      "loss: 0.424742  [41600/175341]\n",
      "loss: 0.323748  [43200/175341]\n",
      "loss: 0.451903  [44800/175341]\n",
      "loss: 0.274290  [46400/175341]\n",
      "loss: 0.805669  [48000/175341]\n",
      "loss: 0.367196  [49600/175341]\n",
      "loss: 0.234202  [51200/175341]\n",
      "loss: 0.365766  [52800/175341]\n",
      "loss: 0.346951  [54400/175341]\n",
      "loss: 0.381714  [56000/175341]\n",
      "loss: 0.265528  [57600/175341]\n",
      "loss: 0.407884  [59200/175341]\n",
      "loss: 0.363723  [60800/175341]\n",
      "loss: 0.266584  [62400/175341]\n",
      "loss: 0.453191  [64000/175341]\n",
      "loss: 0.506958  [65600/175341]\n",
      "loss: 0.359648  [67200/175341]\n",
      "loss: 0.220757  [68800/175341]\n",
      "loss: 0.261652  [70400/175341]\n",
      "loss: 0.935435  [72000/175341]\n",
      "loss: 0.362488  [73600/175341]\n",
      "loss: 0.454591  [75200/175341]\n",
      "loss: 0.468382  [76800/175341]\n",
      "loss: 0.608856  [78400/175341]\n",
      "loss: 0.645547  [80000/175341]\n",
      "loss: 0.426523  [81600/175341]\n",
      "loss: 0.347767  [83200/175341]\n",
      "loss: 0.505744  [84800/175341]\n",
      "loss: 0.436943  [86400/175341]\n",
      "loss: 0.492773  [88000/175341]\n",
      "loss: 0.657600  [89600/175341]\n",
      "loss: 0.206974  [91200/175341]\n",
      "loss: 0.312982  [92800/175341]\n",
      "loss: 0.383813  [94400/175341]\n",
      "loss: 0.729816  [96000/175341]\n",
      "loss: 0.457989  [97600/175341]\n",
      "loss: 0.762768  [99200/175341]\n",
      "loss: 0.122328  [100800/175341]\n",
      "loss: 0.564288  [102400/175341]\n",
      "loss: 0.175675  [104000/175341]\n",
      "loss: 0.335580  [105600/175341]\n",
      "loss: 0.269571  [107200/175341]\n",
      "loss: 0.723999  [108800/175341]\n",
      "loss: 0.606421  [110400/175341]\n",
      "loss: 0.375205  [112000/175341]\n",
      "loss: 0.366582  [113600/175341]\n",
      "loss: 0.299083  [115200/175341]\n",
      "loss: 0.353788  [116800/175341]\n",
      "loss: 0.596742  [118400/175341]\n",
      "loss: 0.350264  [120000/175341]\n",
      "loss: 0.828255  [121600/175341]\n",
      "loss: 0.872408  [123200/175341]\n",
      "loss: 0.465285  [124800/175341]\n",
      "loss: 0.584012  [126400/175341]\n",
      "loss: 0.240536  [128000/175341]\n",
      "loss: 0.315451  [129600/175341]\n",
      "loss: 0.656669  [131200/175341]\n",
      "loss: 0.738006  [132800/175341]\n",
      "loss: 0.169173  [134400/175341]\n",
      "loss: 0.977204  [136000/175341]\n",
      "loss: 0.116759  [137600/175341]\n",
      "loss: 0.675969  [139200/175341]\n",
      "loss: 0.383129  [140800/175341]\n",
      "loss: 0.457690  [142400/175341]\n",
      "loss: 0.380095  [144000/175341]\n",
      "loss: 0.517864  [145600/175341]\n",
      "loss: 0.791059  [147200/175341]\n",
      "loss: 0.291861  [148800/175341]\n",
      "loss: 0.389789  [150400/175341]\n",
      "loss: 0.289226  [152000/175341]\n",
      "loss: 0.329239  [153600/175341]\n",
      "loss: 0.217484  [155200/175341]\n",
      "loss: 0.514405  [156800/175341]\n",
      "loss: 0.271912  [158400/175341]\n",
      "loss: 0.184680  [160000/175341]\n",
      "loss: 0.932977  [161600/175341]\n",
      "loss: 0.583121  [163200/175341]\n",
      "loss: 0.447563  [164800/175341]\n",
      "loss: 0.302142  [166400/175341]\n",
      "loss: 0.725945  [168000/175341]\n",
      "loss: 0.386364  [169600/175341]\n",
      "loss: 0.855502  [171200/175341]\n",
      "loss: 0.474159  [172800/175341]\n",
      "loss: 0.399071  [174400/175341]\n",
      "Train Accuracy: 80.6457%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.560478, F1-score: 76.67%, Macro_F1-Score:  39.35%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.467332  [    0/175341]\n",
      "loss: 0.410256  [ 1600/175341]\n",
      "loss: 0.783562  [ 3200/175341]\n",
      "loss: 0.858194  [ 4800/175341]\n",
      "loss: 0.458451  [ 6400/175341]\n",
      "loss: 0.372299  [ 8000/175341]\n",
      "loss: 0.798586  [ 9600/175341]\n",
      "loss: 0.494391  [11200/175341]\n",
      "loss: 0.318566  [12800/175341]\n",
      "loss: 0.203148  [14400/175341]\n",
      "loss: 0.441638  [16000/175341]\n",
      "loss: 0.863663  [17600/175341]\n",
      "loss: 0.614875  [19200/175341]\n",
      "loss: 0.807354  [20800/175341]\n",
      "loss: 0.240304  [22400/175341]\n",
      "loss: 0.730591  [24000/175341]\n",
      "loss: 0.472646  [25600/175341]\n",
      "loss: 0.558826  [27200/175341]\n",
      "loss: 0.869872  [28800/175341]\n",
      "loss: 0.157463  [30400/175341]\n",
      "loss: 0.316382  [32000/175341]\n",
      "loss: 0.856330  [33600/175341]\n",
      "loss: 0.384331  [35200/175341]\n",
      "loss: 0.513837  [36800/175341]\n",
      "loss: 0.305289  [38400/175341]\n",
      "loss: 0.272816  [40000/175341]\n",
      "loss: 0.332290  [41600/175341]\n",
      "loss: 0.420016  [43200/175341]\n",
      "loss: 0.214942  [44800/175341]\n",
      "loss: 0.416455  [46400/175341]\n",
      "loss: 0.535640  [48000/175341]\n",
      "loss: 0.317153  [49600/175341]\n",
      "loss: 0.543441  [51200/175341]\n",
      "loss: 0.418092  [52800/175341]\n",
      "loss: 0.242130  [54400/175341]\n",
      "loss: 0.681426  [56000/175341]\n",
      "loss: 0.188579  [57600/175341]\n",
      "loss: 0.764214  [59200/175341]\n",
      "loss: 0.661659  [60800/175341]\n",
      "loss: 0.686244  [62400/175341]\n",
      "loss: 0.244847  [64000/175341]\n",
      "loss: 0.636235  [65600/175341]\n",
      "loss: 0.563513  [67200/175341]\n",
      "loss: 0.552237  [68800/175341]\n",
      "loss: 0.513647  [70400/175341]\n",
      "loss: 0.647094  [72000/175341]\n",
      "loss: 0.478686  [73600/175341]\n",
      "loss: 0.139992  [75200/175341]\n",
      "loss: 0.825544  [76800/175341]\n",
      "loss: 0.249074  [78400/175341]\n",
      "loss: 0.652463  [80000/175341]\n",
      "loss: 0.297242  [81600/175341]\n",
      "loss: 0.873607  [83200/175341]\n",
      "loss: 0.605250  [84800/175341]\n",
      "loss: 0.626284  [86400/175341]\n",
      "loss: 0.717043  [88000/175341]\n",
      "loss: 0.522927  [89600/175341]\n",
      "loss: 0.509326  [91200/175341]\n",
      "loss: 0.255435  [92800/175341]\n",
      "loss: 0.333519  [94400/175341]\n",
      "loss: 0.411240  [96000/175341]\n",
      "loss: 0.751098  [97600/175341]\n",
      "loss: 0.274620  [99200/175341]\n",
      "loss: 0.353259  [100800/175341]\n",
      "loss: 0.398329  [102400/175341]\n",
      "loss: 0.402714  [104000/175341]\n",
      "loss: 0.311414  [105600/175341]\n",
      "loss: 0.704204  [107200/175341]\n",
      "loss: 0.477867  [108800/175341]\n",
      "loss: 0.509342  [110400/175341]\n",
      "loss: 0.204714  [112000/175341]\n",
      "loss: 1.120346  [113600/175341]\n",
      "loss: 0.253667  [115200/175341]\n",
      "loss: 0.537748  [116800/175341]\n",
      "loss: 0.755982  [118400/175341]\n",
      "loss: 0.469034  [120000/175341]\n",
      "loss: 0.475312  [121600/175341]\n",
      "loss: 0.537204  [123200/175341]\n",
      "loss: 0.495712  [124800/175341]\n",
      "loss: 0.294187  [126400/175341]\n",
      "loss: 0.400519  [128000/175341]\n",
      "loss: 0.303948  [129600/175341]\n",
      "loss: 0.501046  [131200/175341]\n",
      "loss: 0.493291  [132800/175341]\n",
      "loss: 0.523901  [134400/175341]\n",
      "loss: 0.527866  [136000/175341]\n",
      "loss: 0.146705  [137600/175341]\n",
      "loss: 0.225246  [139200/175341]\n",
      "loss: 0.654340  [140800/175341]\n",
      "loss: 0.857043  [142400/175341]\n",
      "loss: 0.391561  [144000/175341]\n",
      "loss: 0.250112  [145600/175341]\n",
      "loss: 0.492390  [147200/175341]\n",
      "loss: 0.378392  [148800/175341]\n",
      "loss: 0.219660  [150400/175341]\n",
      "loss: 0.802051  [152000/175341]\n",
      "loss: 0.230833  [153600/175341]\n",
      "loss: 0.510017  [155200/175341]\n",
      "loss: 0.538031  [156800/175341]\n",
      "loss: 0.609443  [158400/175341]\n",
      "loss: 0.428052  [160000/175341]\n",
      "loss: 0.616257  [161600/175341]\n",
      "loss: 0.726132  [163200/175341]\n",
      "loss: 0.728440  [164800/175341]\n",
      "loss: 0.569137  [166400/175341]\n",
      "loss: 0.514260  [168000/175341]\n",
      "loss: 0.584521  [169600/175341]\n",
      "loss: 0.621790  [171200/175341]\n",
      "loss: 0.839214  [172800/175341]\n",
      "loss: 0.141826  [174400/175341]\n",
      "Train Accuracy: 80.6457%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.571588, F1-score: 75.70%, Macro_F1-Score:  39.22%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.953654  [    0/175341]\n",
      "loss: 0.526242  [ 1600/175341]\n",
      "loss: 0.511941  [ 3200/175341]\n",
      "loss: 0.819516  [ 4800/175341]\n",
      "loss: 0.400517  [ 6400/175341]\n",
      "loss: 0.660011  [ 8000/175341]\n",
      "loss: 0.673270  [ 9600/175341]\n",
      "loss: 0.305264  [11200/175341]\n",
      "loss: 0.470188  [12800/175341]\n",
      "loss: 0.334429  [14400/175341]\n",
      "loss: 0.211011  [16000/175341]\n",
      "loss: 0.209865  [17600/175341]\n",
      "loss: 0.086409  [19200/175341]\n",
      "loss: 0.331035  [20800/175341]\n",
      "loss: 0.421314  [22400/175341]\n",
      "loss: 0.873184  [24000/175341]\n",
      "loss: 0.453910  [25600/175341]\n",
      "loss: 0.657341  [27200/175341]\n",
      "loss: 0.524969  [28800/175341]\n",
      "loss: 0.658942  [30400/175341]\n",
      "loss: 0.271535  [32000/175341]\n",
      "loss: 0.281604  [33600/175341]\n",
      "loss: 0.551167  [35200/175341]\n",
      "loss: 0.693010  [36800/175341]\n",
      "loss: 0.273947  [38400/175341]\n",
      "loss: 0.615495  [40000/175341]\n",
      "loss: 0.246898  [41600/175341]\n",
      "loss: 0.256496  [43200/175341]\n",
      "loss: 0.507043  [44800/175341]\n",
      "loss: 0.749029  [46400/175341]\n",
      "loss: 0.974950  [48000/175341]\n",
      "loss: 0.387653  [49600/175341]\n",
      "loss: 0.266491  [51200/175341]\n",
      "loss: 0.546339  [52800/175341]\n",
      "loss: 0.594099  [54400/175341]\n",
      "loss: 0.273396  [56000/175341]\n",
      "loss: 0.232282  [57600/175341]\n",
      "loss: 0.267787  [59200/175341]\n",
      "loss: 0.809124  [60800/175341]\n",
      "loss: 0.449010  [62400/175341]\n",
      "loss: 0.248704  [64000/175341]\n",
      "loss: 0.559579  [65600/175341]\n",
      "loss: 0.565800  [67200/175341]\n",
      "loss: 0.733444  [68800/175341]\n",
      "loss: 0.927410  [70400/175341]\n",
      "loss: 0.414285  [72000/175341]\n",
      "loss: 0.162478  [73600/175341]\n",
      "loss: 0.137515  [75200/175341]\n",
      "loss: 0.879434  [76800/175341]\n",
      "loss: 0.305591  [78400/175341]\n",
      "loss: 0.347209  [80000/175341]\n",
      "loss: 0.654666  [81600/175341]\n",
      "loss: 0.649629  [83200/175341]\n",
      "loss: 0.445774  [84800/175341]\n",
      "loss: 0.358969  [86400/175341]\n",
      "loss: 0.484346  [88000/175341]\n",
      "loss: 0.178682  [89600/175341]\n",
      "loss: 0.389456  [91200/175341]\n",
      "loss: 0.848480  [92800/175341]\n",
      "loss: 0.204689  [94400/175341]\n",
      "loss: 0.918286  [96000/175341]\n",
      "loss: 0.356481  [97600/175341]\n",
      "loss: 0.845531  [99200/175341]\n",
      "loss: 0.353974  [100800/175341]\n",
      "loss: 0.100733  [102400/175341]\n",
      "loss: 0.454900  [104000/175341]\n",
      "loss: 0.763628  [105600/175341]\n",
      "loss: 0.467467  [107200/175341]\n",
      "loss: 0.601924  [108800/175341]\n",
      "loss: 0.450554  [110400/175341]\n",
      "loss: 0.355791  [112000/175341]\n",
      "loss: 0.730442  [113600/175341]\n",
      "loss: 0.506244  [115200/175341]\n",
      "loss: 0.630641  [116800/175341]\n",
      "loss: 0.304218  [118400/175341]\n",
      "loss: 0.667746  [120000/175341]\n",
      "loss: 0.602247  [121600/175341]\n",
      "loss: 0.505485  [123200/175341]\n",
      "loss: 0.381540  [124800/175341]\n",
      "loss: 0.521795  [126400/175341]\n",
      "loss: 0.432955  [128000/175341]\n",
      "loss: 0.423947  [129600/175341]\n",
      "loss: 0.245093  [131200/175341]\n",
      "loss: 0.635222  [132800/175341]\n",
      "loss: 0.620404  [134400/175341]\n",
      "loss: 0.748784  [136000/175341]\n",
      "loss: 0.413565  [137600/175341]\n",
      "loss: 0.550905  [139200/175341]\n",
      "loss: 0.210269  [140800/175341]\n",
      "loss: 0.253271  [142400/175341]\n",
      "loss: 0.374292  [144000/175341]\n",
      "loss: 0.432204  [145600/175341]\n",
      "loss: 0.719269  [147200/175341]\n",
      "loss: 0.853889  [148800/175341]\n",
      "loss: 0.616654  [150400/175341]\n",
      "loss: 0.271684  [152000/175341]\n",
      "loss: 0.408712  [153600/175341]\n",
      "loss: 0.741116  [155200/175341]\n",
      "loss: 0.326216  [156800/175341]\n",
      "loss: 0.125784  [158400/175341]\n",
      "loss: 0.386938  [160000/175341]\n",
      "loss: 0.158098  [161600/175341]\n",
      "loss: 0.217199  [163200/175341]\n",
      "loss: 0.445868  [164800/175341]\n",
      "loss: 0.416280  [166400/175341]\n",
      "loss: 0.915029  [168000/175341]\n",
      "loss: 0.493237  [169600/175341]\n",
      "loss: 0.310703  [171200/175341]\n",
      "loss: 0.286528  [172800/175341]\n",
      "loss: 0.626061  [174400/175341]\n",
      "Train Accuracy: 80.7181%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.572682, F1-score: 75.27%, Macro_F1-Score:  39.50%  \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.659579  [    0/175341]\n",
      "loss: 0.352138  [ 1600/175341]\n",
      "loss: 0.323329  [ 3200/175341]\n",
      "loss: 0.242209  [ 4800/175341]\n",
      "loss: 0.264368  [ 6400/175341]\n",
      "loss: 0.274018  [ 8000/175341]\n",
      "loss: 0.336563  [ 9600/175341]\n",
      "loss: 1.109594  [11200/175341]\n",
      "loss: 1.061006  [12800/175341]\n",
      "loss: 0.889784  [14400/175341]\n",
      "loss: 0.792471  [16000/175341]\n",
      "loss: 0.718109  [17600/175341]\n",
      "loss: 0.416125  [19200/175341]\n",
      "loss: 0.241305  [20800/175341]\n",
      "loss: 0.323846  [22400/175341]\n",
      "loss: 0.657352  [24000/175341]\n",
      "loss: 0.357510  [25600/175341]\n",
      "loss: 0.715418  [27200/175341]\n",
      "loss: 0.456152  [28800/175341]\n",
      "loss: 0.461983  [30400/175341]\n",
      "loss: 0.388317  [32000/175341]\n",
      "loss: 0.294436  [33600/175341]\n",
      "loss: 0.392255  [35200/175341]\n",
      "loss: 0.388821  [36800/175341]\n",
      "loss: 0.650452  [38400/175341]\n",
      "loss: 0.488452  [40000/175341]\n",
      "loss: 0.523685  [41600/175341]\n",
      "loss: 0.570210  [43200/175341]\n",
      "loss: 0.257343  [44800/175341]\n",
      "loss: 0.283364  [46400/175341]\n",
      "loss: 0.399764  [48000/175341]\n",
      "loss: 0.677634  [49600/175341]\n",
      "loss: 0.195033  [51200/175341]\n",
      "loss: 0.376551  [52800/175341]\n",
      "loss: 0.444126  [54400/175341]\n",
      "loss: 0.809815  [56000/175341]\n",
      "loss: 0.564048  [57600/175341]\n",
      "loss: 0.762485  [59200/175341]\n",
      "loss: 0.428546  [60800/175341]\n",
      "loss: 0.865129  [62400/175341]\n",
      "loss: 0.495219  [64000/175341]\n",
      "loss: 0.216690  [65600/175341]\n",
      "loss: 0.437169  [67200/175341]\n",
      "loss: 0.736033  [68800/175341]\n",
      "loss: 0.403454  [70400/175341]\n",
      "loss: 0.377089  [72000/175341]\n",
      "loss: 0.280034  [73600/175341]\n",
      "loss: 0.765037  [75200/175341]\n",
      "loss: 0.199565  [76800/175341]\n",
      "loss: 0.624982  [78400/175341]\n",
      "loss: 0.674112  [80000/175341]\n",
      "loss: 0.391697  [81600/175341]\n",
      "loss: 0.216070  [83200/175341]\n",
      "loss: 0.887436  [84800/175341]\n",
      "loss: 0.570038  [86400/175341]\n",
      "loss: 0.700390  [88000/175341]\n",
      "loss: 0.440824  [89600/175341]\n",
      "loss: 0.329901  [91200/175341]\n",
      "loss: 0.373928  [92800/175341]\n",
      "loss: 0.386744  [94400/175341]\n",
      "loss: 0.210704  [96000/175341]\n",
      "loss: 0.582207  [97600/175341]\n",
      "loss: 0.589457  [99200/175341]\n",
      "loss: 0.469517  [100800/175341]\n",
      "loss: 0.382888  [102400/175341]\n",
      "loss: 0.716373  [104000/175341]\n",
      "loss: 0.258905  [105600/175341]\n",
      "loss: 0.471471  [107200/175341]\n",
      "loss: 0.718554  [108800/175341]\n",
      "loss: 0.395352  [110400/175341]\n",
      "loss: 0.605805  [112000/175341]\n",
      "loss: 0.421102  [113600/175341]\n",
      "loss: 0.430370  [115200/175341]\n",
      "loss: 0.766381  [116800/175341]\n",
      "loss: 0.290656  [118400/175341]\n",
      "loss: 1.093586  [120000/175341]\n",
      "loss: 0.102085  [121600/175341]\n",
      "loss: 0.340489  [123200/175341]\n",
      "loss: 0.647209  [124800/175341]\n",
      "loss: 0.606421  [126400/175341]\n",
      "loss: 0.216512  [128000/175341]\n",
      "loss: 0.504979  [129600/175341]\n",
      "loss: 0.829849  [131200/175341]\n",
      "loss: 0.640255  [132800/175341]\n",
      "loss: 0.405464  [134400/175341]\n",
      "loss: 0.403761  [136000/175341]\n",
      "loss: 0.587667  [137600/175341]\n",
      "loss: 0.412332  [139200/175341]\n",
      "loss: 0.222013  [140800/175341]\n",
      "loss: 0.133470  [142400/175341]\n",
      "loss: 0.586911  [144000/175341]\n",
      "loss: 0.845401  [145600/175341]\n",
      "loss: 0.460966  [147200/175341]\n",
      "loss: 0.840686  [148800/175341]\n",
      "loss: 0.283490  [150400/175341]\n",
      "loss: 0.626037  [152000/175341]\n",
      "loss: 0.479204  [153600/175341]\n",
      "loss: 0.754455  [155200/175341]\n",
      "loss: 0.312927  [156800/175341]\n",
      "loss: 0.289580  [158400/175341]\n",
      "loss: 0.657580  [160000/175341]\n",
      "loss: 0.754303  [161600/175341]\n",
      "loss: 0.648773  [163200/175341]\n",
      "loss: 0.601027  [164800/175341]\n",
      "loss: 0.617894  [166400/175341]\n",
      "loss: 0.408546  [168000/175341]\n",
      "loss: 0.526048  [169600/175341]\n",
      "loss: 0.244454  [171200/175341]\n",
      "loss: 0.715753  [172800/175341]\n",
      "loss: 0.175070  [174400/175341]\n",
      "Train Accuracy: 80.7621%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.568601, F1-score: 75.43%, Macro_F1-Score:  39.54%  \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.796110  [    0/175341]\n",
      "loss: 0.803465  [ 1600/175341]\n",
      "loss: 0.235053  [ 3200/175341]\n",
      "loss: 0.799749  [ 4800/175341]\n",
      "loss: 0.437713  [ 6400/175341]\n",
      "loss: 0.467409  [ 8000/175341]\n",
      "loss: 0.473002  [ 9600/175341]\n",
      "loss: 0.963027  [11200/175341]\n",
      "loss: 0.341423  [12800/175341]\n",
      "loss: 0.234952  [14400/175341]\n",
      "loss: 0.647990  [16000/175341]\n",
      "loss: 0.371946  [17600/175341]\n",
      "loss: 0.372397  [19200/175341]\n",
      "loss: 0.546402  [20800/175341]\n",
      "loss: 0.518155  [22400/175341]\n",
      "loss: 0.758678  [24000/175341]\n",
      "loss: 0.705397  [25600/175341]\n",
      "loss: 0.361049  [27200/175341]\n",
      "loss: 0.559347  [28800/175341]\n",
      "loss: 0.678137  [30400/175341]\n",
      "loss: 0.565106  [32000/175341]\n",
      "loss: 0.781461  [33600/175341]\n",
      "loss: 0.750162  [35200/175341]\n",
      "loss: 0.612628  [36800/175341]\n",
      "loss: 0.645715  [38400/175341]\n",
      "loss: 0.566638  [40000/175341]\n",
      "loss: 0.389959  [41600/175341]\n",
      "loss: 0.894977  [43200/175341]\n",
      "loss: 0.732418  [44800/175341]\n",
      "loss: 0.712276  [46400/175341]\n",
      "loss: 0.293178  [48000/175341]\n",
      "loss: 0.533330  [49600/175341]\n",
      "loss: 0.695421  [51200/175341]\n",
      "loss: 0.307521  [52800/175341]\n",
      "loss: 0.289285  [54400/175341]\n",
      "loss: 0.751023  [56000/175341]\n",
      "loss: 0.796731  [57600/175341]\n",
      "loss: 0.710443  [59200/175341]\n",
      "loss: 0.433206  [60800/175341]\n",
      "loss: 0.785775  [62400/175341]\n",
      "loss: 0.886338  [64000/175341]\n",
      "loss: 0.553596  [65600/175341]\n",
      "loss: 0.424930  [67200/175341]\n",
      "loss: 0.298455  [68800/175341]\n",
      "loss: 0.691188  [70400/175341]\n",
      "loss: 0.208631  [72000/175341]\n",
      "loss: 0.332902  [73600/175341]\n",
      "loss: 0.312346  [75200/175341]\n",
      "loss: 0.572805  [76800/175341]\n",
      "loss: 0.898651  [78400/175341]\n",
      "loss: 0.590453  [80000/175341]\n",
      "loss: 0.488594  [81600/175341]\n",
      "loss: 0.129507  [83200/175341]\n",
      "loss: 0.294144  [84800/175341]\n",
      "loss: 0.562660  [86400/175341]\n",
      "loss: 0.235886  [88000/175341]\n",
      "loss: 0.478135  [89600/175341]\n",
      "loss: 0.520749  [91200/175341]\n",
      "loss: 0.353151  [92800/175341]\n",
      "loss: 0.382111  [94400/175341]\n",
      "loss: 0.492658  [96000/175341]\n",
      "loss: 0.548456  [97600/175341]\n",
      "loss: 0.606987  [99200/175341]\n",
      "loss: 0.369954  [100800/175341]\n",
      "loss: 0.633494  [102400/175341]\n",
      "loss: 0.503824  [104000/175341]\n",
      "loss: 0.567967  [105600/175341]\n",
      "loss: 0.553999  [107200/175341]\n",
      "loss: 0.155482  [108800/175341]\n",
      "loss: 0.579612  [110400/175341]\n",
      "loss: 0.143299  [112000/175341]\n",
      "loss: 0.531723  [113600/175341]\n",
      "loss: 0.111490  [115200/175341]\n",
      "loss: 0.424568  [116800/175341]\n",
      "loss: 0.408869  [118400/175341]\n",
      "loss: 0.683175  [120000/175341]\n",
      "loss: 0.538404  [121600/175341]\n",
      "loss: 0.507590  [123200/175341]\n",
      "loss: 0.664960  [124800/175341]\n",
      "loss: 0.503564  [126400/175341]\n",
      "loss: 0.627134  [128000/175341]\n",
      "loss: 0.718401  [129600/175341]\n",
      "loss: 0.233380  [131200/175341]\n",
      "loss: 0.667831  [132800/175341]\n",
      "loss: 0.878699  [134400/175341]\n",
      "loss: 0.328375  [136000/175341]\n",
      "loss: 0.280599  [137600/175341]\n",
      "loss: 0.245286  [139200/175341]\n",
      "loss: 0.648868  [140800/175341]\n",
      "loss: 0.260967  [142400/175341]\n",
      "loss: 0.426255  [144000/175341]\n",
      "loss: 0.367666  [145600/175341]\n",
      "loss: 0.299017  [147200/175341]\n",
      "loss: 0.927721  [148800/175341]\n",
      "loss: 0.553699  [150400/175341]\n",
      "loss: 0.548343  [152000/175341]\n",
      "loss: 0.468498  [153600/175341]\n",
      "loss: 0.188391  [155200/175341]\n",
      "loss: 0.523897  [156800/175341]\n",
      "loss: 0.339999  [158400/175341]\n",
      "loss: 0.189870  [160000/175341]\n",
      "loss: 0.343062  [161600/175341]\n",
      "loss: 0.478837  [163200/175341]\n",
      "loss: 0.557172  [164800/175341]\n",
      "loss: 0.615472  [166400/175341]\n",
      "loss: 0.515590  [168000/175341]\n",
      "loss: 0.304858  [169600/175341]\n",
      "loss: 0.445086  [171200/175341]\n",
      "loss: 0.240457  [172800/175341]\n",
      "loss: 0.684790  [174400/175341]\n",
      "Train Accuracy: 80.7438%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.582635, F1-score: 74.81%, Macro_F1-Score:  39.05%  \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.441640  [    0/175341]\n",
      "loss: 0.158731  [ 1600/175341]\n",
      "loss: 0.267056  [ 3200/175341]\n",
      "loss: 0.472485  [ 4800/175341]\n",
      "loss: 0.513620  [ 6400/175341]\n",
      "loss: 0.365578  [ 8000/175341]\n",
      "loss: 0.330329  [ 9600/175341]\n",
      "loss: 0.523517  [11200/175341]\n",
      "loss: 0.617794  [12800/175341]\n",
      "loss: 0.489854  [14400/175341]\n",
      "loss: 0.621994  [16000/175341]\n",
      "loss: 0.915799  [17600/175341]\n",
      "loss: 0.302341  [19200/175341]\n",
      "loss: 0.278938  [20800/175341]\n",
      "loss: 0.246464  [22400/175341]\n",
      "loss: 0.336658  [24000/175341]\n",
      "loss: 0.527197  [25600/175341]\n",
      "loss: 0.640882  [27200/175341]\n",
      "loss: 0.328824  [28800/175341]\n",
      "loss: 0.311069  [30400/175341]\n",
      "loss: 0.694996  [32000/175341]\n",
      "loss: 0.399154  [33600/175341]\n",
      "loss: 0.563191  [35200/175341]\n",
      "loss: 0.568502  [36800/175341]\n",
      "loss: 0.560788  [38400/175341]\n",
      "loss: 0.387367  [40000/175341]\n",
      "loss: 0.720241  [41600/175341]\n",
      "loss: 0.554197  [43200/175341]\n",
      "loss: 0.664013  [44800/175341]\n",
      "loss: 0.428386  [46400/175341]\n",
      "loss: 0.527526  [48000/175341]\n",
      "loss: 0.610457  [49600/175341]\n",
      "loss: 0.398678  [51200/175341]\n",
      "loss: 0.537173  [52800/175341]\n",
      "loss: 0.630045  [54400/175341]\n",
      "loss: 0.486104  [56000/175341]\n",
      "loss: 0.541668  [57600/175341]\n",
      "loss: 0.901499  [59200/175341]\n",
      "loss: 0.661065  [60800/175341]\n",
      "loss: 0.173243  [62400/175341]\n",
      "loss: 0.766643  [64000/175341]\n",
      "loss: 0.756233  [65600/175341]\n",
      "loss: 0.388628  [67200/175341]\n",
      "loss: 0.560989  [68800/175341]\n",
      "loss: 0.555587  [70400/175341]\n",
      "loss: 0.309109  [72000/175341]\n",
      "loss: 0.451864  [73600/175341]\n",
      "loss: 0.672533  [75200/175341]\n",
      "loss: 0.520392  [76800/175341]\n",
      "loss: 0.262985  [78400/175341]\n",
      "loss: 0.611450  [80000/175341]\n",
      "loss: 0.446502  [81600/175341]\n",
      "loss: 0.306015  [83200/175341]\n",
      "loss: 0.197816  [84800/175341]\n",
      "loss: 0.361677  [86400/175341]\n",
      "loss: 0.439271  [88000/175341]\n",
      "loss: 0.197383  [89600/175341]\n",
      "loss: 0.555169  [91200/175341]\n",
      "loss: 0.905813  [92800/175341]\n",
      "loss: 0.606581  [94400/175341]\n",
      "loss: 0.162987  [96000/175341]\n",
      "loss: 0.638558  [97600/175341]\n",
      "loss: 1.127200  [99200/175341]\n",
      "loss: 0.248240  [100800/175341]\n",
      "loss: 0.151274  [102400/175341]\n",
      "loss: 0.239301  [104000/175341]\n",
      "loss: 0.424422  [105600/175341]\n",
      "loss: 0.717939  [107200/175341]\n",
      "loss: 0.288745  [108800/175341]\n",
      "loss: 0.371368  [110400/175341]\n",
      "loss: 0.320195  [112000/175341]\n",
      "loss: 0.620856  [113600/175341]\n",
      "loss: 0.333251  [115200/175341]\n",
      "loss: 0.162836  [116800/175341]\n",
      "loss: 0.308384  [118400/175341]\n",
      "loss: 0.645095  [120000/175341]\n",
      "loss: 0.338143  [121600/175341]\n",
      "loss: 0.804558  [123200/175341]\n",
      "loss: 0.327761  [124800/175341]\n",
      "loss: 0.374561  [126400/175341]\n",
      "loss: 0.549574  [128000/175341]\n",
      "loss: 0.560405  [129600/175341]\n",
      "loss: 0.441470  [131200/175341]\n",
      "loss: 0.506106  [132800/175341]\n",
      "loss: 0.685547  [134400/175341]\n",
      "loss: 0.389462  [136000/175341]\n",
      "loss: 0.379254  [137600/175341]\n",
      "loss: 0.162071  [139200/175341]\n",
      "loss: 0.365153  [140800/175341]\n",
      "loss: 0.349282  [142400/175341]\n",
      "loss: 0.509112  [144000/175341]\n",
      "loss: 0.139617  [145600/175341]\n",
      "loss: 0.186938  [147200/175341]\n",
      "loss: 0.489444  [148800/175341]\n",
      "loss: 0.224733  [150400/175341]\n",
      "loss: 0.442972  [152000/175341]\n",
      "loss: 0.360784  [153600/175341]\n",
      "loss: 0.422624  [155200/175341]\n",
      "loss: 0.303046  [156800/175341]\n",
      "loss: 0.538435  [158400/175341]\n",
      "loss: 0.349680  [160000/175341]\n",
      "loss: 0.702393  [161600/175341]\n",
      "loss: 0.417086  [163200/175341]\n",
      "loss: 0.206662  [164800/175341]\n",
      "loss: 0.552812  [166400/175341]\n",
      "loss: 0.213260  [168000/175341]\n",
      "loss: 0.616619  [169600/175341]\n",
      "loss: 0.295527  [171200/175341]\n",
      "loss: 0.262119  [172800/175341]\n",
      "loss: 0.365808  [174400/175341]\n",
      "Train Accuracy: 80.8265%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.576990, F1-score: 75.30%, Macro_F1-Score:  39.01%  \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.362005  [    0/175341]\n",
      "loss: 0.466935  [ 1600/175341]\n",
      "loss: 0.126656  [ 3200/175341]\n",
      "loss: 0.290498  [ 4800/175341]\n",
      "loss: 0.722432  [ 6400/175341]\n",
      "loss: 0.731187  [ 8000/175341]\n",
      "loss: 0.369497  [ 9600/175341]\n",
      "loss: 0.291338  [11200/175341]\n",
      "loss: 0.594283  [12800/175341]\n",
      "loss: 0.323065  [14400/175341]\n",
      "loss: 0.683085  [16000/175341]\n",
      "loss: 0.535164  [17600/175341]\n",
      "loss: 0.498376  [19200/175341]\n",
      "loss: 0.725735  [20800/175341]\n",
      "loss: 0.368705  [22400/175341]\n",
      "loss: 0.419103  [24000/175341]\n",
      "loss: 0.421068  [25600/175341]\n",
      "loss: 0.694625  [27200/175341]\n",
      "loss: 0.472899  [28800/175341]\n",
      "loss: 0.510364  [30400/175341]\n",
      "loss: 0.155205  [32000/175341]\n",
      "loss: 0.164805  [33600/175341]\n",
      "loss: 0.548910  [35200/175341]\n",
      "loss: 0.211274  [36800/175341]\n",
      "loss: 1.102708  [38400/175341]\n",
      "loss: 0.297948  [40000/175341]\n",
      "loss: 0.349558  [41600/175341]\n",
      "loss: 0.331396  [43200/175341]\n",
      "loss: 0.271925  [44800/175341]\n",
      "loss: 0.430403  [46400/175341]\n",
      "loss: 0.426132  [48000/175341]\n",
      "loss: 0.435175  [49600/175341]\n",
      "loss: 0.247525  [51200/175341]\n",
      "loss: 0.248538  [52800/175341]\n",
      "loss: 0.327396  [54400/175341]\n",
      "loss: 1.028796  [56000/175341]\n",
      "loss: 0.451066  [57600/175341]\n",
      "loss: 0.523298  [59200/175341]\n",
      "loss: 0.247847  [60800/175341]\n",
      "loss: 0.634145  [62400/175341]\n",
      "loss: 0.368497  [64000/175341]\n",
      "loss: 0.343805  [65600/175341]\n",
      "loss: 0.329460  [67200/175341]\n",
      "loss: 0.688379  [68800/175341]\n",
      "loss: 0.262430  [70400/175341]\n",
      "loss: 0.224435  [72000/175341]\n",
      "loss: 0.786827  [73600/175341]\n",
      "loss: 0.261447  [75200/175341]\n",
      "loss: 0.452563  [76800/175341]\n",
      "loss: 0.437845  [78400/175341]\n",
      "loss: 0.437005  [80000/175341]\n",
      "loss: 0.214952  [81600/175341]\n",
      "loss: 0.187609  [83200/175341]\n",
      "loss: 0.373492  [84800/175341]\n",
      "loss: 0.356918  [86400/175341]\n",
      "loss: 0.929238  [88000/175341]\n",
      "loss: 0.664275  [89600/175341]\n",
      "loss: 0.393208  [91200/175341]\n",
      "loss: 0.589698  [92800/175341]\n",
      "loss: 0.667891  [94400/175341]\n",
      "loss: 0.632996  [96000/175341]\n",
      "loss: 0.232454  [97600/175341]\n",
      "loss: 0.845575  [99200/175341]\n",
      "loss: 0.478070  [100800/175341]\n",
      "loss: 0.385747  [102400/175341]\n",
      "loss: 0.522143  [104000/175341]\n",
      "loss: 0.632781  [105600/175341]\n",
      "loss: 0.696861  [107200/175341]\n",
      "loss: 0.684204  [108800/175341]\n",
      "loss: 0.339078  [110400/175341]\n",
      "loss: 0.611150  [112000/175341]\n",
      "loss: 0.861387  [113600/175341]\n",
      "loss: 0.656682  [115200/175341]\n",
      "loss: 0.353150  [116800/175341]\n",
      "loss: 0.280512  [118400/175341]\n",
      "loss: 0.607212  [120000/175341]\n",
      "loss: 0.474752  [121600/175341]\n",
      "loss: 0.940689  [123200/175341]\n",
      "loss: 0.621523  [124800/175341]\n",
      "loss: 0.651627  [126400/175341]\n",
      "loss: 0.667875  [128000/175341]\n",
      "loss: 0.679636  [129600/175341]\n",
      "loss: 0.530179  [131200/175341]\n",
      "loss: 0.275841  [132800/175341]\n",
      "loss: 0.548431  [134400/175341]\n",
      "loss: 0.238370  [136000/175341]\n",
      "loss: 0.674088  [137600/175341]\n",
      "loss: 0.673211  [139200/175341]\n",
      "loss: 0.166053  [140800/175341]\n",
      "loss: 0.367194  [142400/175341]\n",
      "loss: 0.202647  [144000/175341]\n",
      "loss: 0.266779  [145600/175341]\n",
      "loss: 0.290347  [147200/175341]\n",
      "loss: 0.429583  [148800/175341]\n",
      "loss: 0.455920  [150400/175341]\n",
      "loss: 0.521006  [152000/175341]\n",
      "loss: 0.202986  [153600/175341]\n",
      "loss: 0.659032  [155200/175341]\n",
      "loss: 0.656501  [156800/175341]\n",
      "loss: 0.517947  [158400/175341]\n",
      "loss: 0.585555  [160000/175341]\n",
      "loss: 0.397766  [161600/175341]\n",
      "loss: 0.576301  [163200/175341]\n",
      "loss: 0.280694  [164800/175341]\n",
      "loss: 0.814222  [166400/175341]\n",
      "loss: 0.526063  [168000/175341]\n",
      "loss: 0.441773  [169600/175341]\n",
      "loss: 0.326476  [171200/175341]\n",
      "loss: 0.577681  [172800/175341]\n",
      "loss: 0.470352  [174400/175341]\n",
      "Train Accuracy: 80.8465%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.582432, F1-score: 74.07%, Macro_F1-Score:  39.28%  \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.539801  [    0/175341]\n",
      "loss: 0.541225  [ 1600/175341]\n",
      "loss: 0.334405  [ 3200/175341]\n",
      "loss: 0.483866  [ 4800/175341]\n",
      "loss: 0.939253  [ 6400/175341]\n",
      "loss: 0.415797  [ 8000/175341]\n",
      "loss: 0.485743  [ 9600/175341]\n",
      "loss: 0.231481  [11200/175341]\n",
      "loss: 0.357197  [12800/175341]\n",
      "loss: 0.580132  [14400/175341]\n",
      "loss: 0.634136  [16000/175341]\n",
      "loss: 0.640617  [17600/175341]\n",
      "loss: 0.343171  [19200/175341]\n",
      "loss: 0.608602  [20800/175341]\n",
      "loss: 0.404093  [22400/175341]\n",
      "loss: 0.640939  [24000/175341]\n",
      "loss: 0.667854  [25600/175341]\n",
      "loss: 0.826497  [27200/175341]\n",
      "loss: 0.329089  [28800/175341]\n",
      "loss: 0.428374  [30400/175341]\n",
      "loss: 0.360361  [32000/175341]\n",
      "loss: 0.374462  [33600/175341]\n",
      "loss: 0.682886  [35200/175341]\n",
      "loss: 0.217586  [36800/175341]\n",
      "loss: 0.460495  [38400/175341]\n",
      "loss: 0.535906  [40000/175341]\n",
      "loss: 0.436523  [41600/175341]\n",
      "loss: 0.334099  [43200/175341]\n",
      "loss: 0.307113  [44800/175341]\n",
      "loss: 0.471725  [46400/175341]\n",
      "loss: 0.407545  [48000/175341]\n",
      "loss: 0.736867  [49600/175341]\n",
      "loss: 0.172430  [51200/175341]\n",
      "loss: 0.665874  [52800/175341]\n",
      "loss: 0.437560  [54400/175341]\n",
      "loss: 0.381721  [56000/175341]\n",
      "loss: 0.499083  [57600/175341]\n",
      "loss: 0.555641  [59200/175341]\n",
      "loss: 0.794351  [60800/175341]\n",
      "loss: 0.594380  [62400/175341]\n",
      "loss: 0.555917  [64000/175341]\n",
      "loss: 0.231367  [65600/175341]\n",
      "loss: 0.380266  [67200/175341]\n",
      "loss: 0.141799  [68800/175341]\n",
      "loss: 0.392984  [70400/175341]\n",
      "loss: 0.726473  [72000/175341]\n",
      "loss: 0.485542  [73600/175341]\n",
      "loss: 0.438358  [75200/175341]\n",
      "loss: 0.273235  [76800/175341]\n",
      "loss: 0.219777  [78400/175341]\n",
      "loss: 0.116539  [80000/175341]\n",
      "loss: 0.724730  [81600/175341]\n",
      "loss: 0.720849  [83200/175341]\n",
      "loss: 0.454275  [84800/175341]\n",
      "loss: 0.198330  [86400/175341]\n",
      "loss: 0.495511  [88000/175341]\n",
      "loss: 0.414126  [89600/175341]\n",
      "loss: 0.371862  [91200/175341]\n",
      "loss: 0.259306  [92800/175341]\n",
      "loss: 0.605625  [94400/175341]\n",
      "loss: 0.416127  [96000/175341]\n",
      "loss: 0.484288  [97600/175341]\n",
      "loss: 0.225240  [99200/175341]\n",
      "loss: 0.258074  [100800/175341]\n",
      "loss: 0.189621  [102400/175341]\n",
      "loss: 0.493970  [104000/175341]\n",
      "loss: 0.429033  [105600/175341]\n",
      "loss: 1.070507  [107200/175341]\n",
      "loss: 0.829361  [108800/175341]\n",
      "loss: 0.624566  [110400/175341]\n",
      "loss: 0.423696  [112000/175341]\n",
      "loss: 0.647256  [113600/175341]\n",
      "loss: 1.090696  [115200/175341]\n",
      "loss: 0.549702  [116800/175341]\n",
      "loss: 0.629496  [118400/175341]\n",
      "loss: 0.402468  [120000/175341]\n",
      "loss: 0.440284  [121600/175341]\n",
      "loss: 0.350121  [123200/175341]\n",
      "loss: 0.236898  [124800/175341]\n",
      "loss: 0.553222  [126400/175341]\n",
      "loss: 0.634890  [128000/175341]\n",
      "loss: 0.204286  [129600/175341]\n",
      "loss: 0.289194  [131200/175341]\n",
      "loss: 0.611258  [132800/175341]\n",
      "loss: 0.514408  [134400/175341]\n",
      "loss: 0.280232  [136000/175341]\n",
      "loss: 0.528310  [137600/175341]\n",
      "loss: 0.225058  [139200/175341]\n",
      "loss: 0.468027  [140800/175341]\n",
      "loss: 0.571315  [142400/175341]\n",
      "loss: 0.324002  [144000/175341]\n",
      "loss: 0.997610  [145600/175341]\n",
      "loss: 0.383046  [147200/175341]\n",
      "loss: 0.530412  [148800/175341]\n",
      "loss: 0.622529  [150400/175341]\n",
      "loss: 0.568760  [152000/175341]\n",
      "loss: 0.387606  [153600/175341]\n",
      "loss: 0.924772  [155200/175341]\n",
      "loss: 0.543076  [156800/175341]\n",
      "loss: 0.433145  [158400/175341]\n",
      "loss: 0.747081  [160000/175341]\n",
      "loss: 0.574588  [161600/175341]\n",
      "loss: 0.321182  [163200/175341]\n",
      "loss: 0.271419  [164800/175341]\n",
      "loss: 0.460613  [166400/175341]\n",
      "loss: 0.212413  [168000/175341]\n",
      "loss: 0.158767  [169600/175341]\n",
      "loss: 0.493398  [171200/175341]\n",
      "loss: 0.340837  [172800/175341]\n",
      "loss: 0.412609  [174400/175341]\n",
      "Train Accuracy: 80.8676%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.549872, F1-score: 76.46%, Macro_F1-Score:  40.05%  \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.913900  [    0/175341]\n",
      "loss: 0.627333  [ 1600/175341]\n",
      "loss: 0.279777  [ 3200/175341]\n",
      "loss: 0.454113  [ 4800/175341]\n",
      "loss: 0.294641  [ 6400/175341]\n",
      "loss: 0.476727  [ 8000/175341]\n",
      "loss: 0.303585  [ 9600/175341]\n",
      "loss: 0.439849  [11200/175341]\n",
      "loss: 0.503789  [12800/175341]\n",
      "loss: 0.569315  [14400/175341]\n",
      "loss: 0.181185  [16000/175341]\n",
      "loss: 0.180768  [17600/175341]\n",
      "loss: 0.328635  [19200/175341]\n",
      "loss: 0.662370  [20800/175341]\n",
      "loss: 0.300288  [22400/175341]\n",
      "loss: 0.221905  [24000/175341]\n",
      "loss: 0.659667  [25600/175341]\n",
      "loss: 0.412905  [27200/175341]\n",
      "loss: 0.452369  [28800/175341]\n",
      "loss: 0.292596  [30400/175341]\n",
      "loss: 0.307779  [32000/175341]\n",
      "loss: 0.418693  [33600/175341]\n",
      "loss: 0.305218  [35200/175341]\n",
      "loss: 0.214031  [36800/175341]\n",
      "loss: 0.190347  [38400/175341]\n",
      "loss: 0.428862  [40000/175341]\n",
      "loss: 0.323258  [41600/175341]\n",
      "loss: 0.522966  [43200/175341]\n",
      "loss: 0.420726  [44800/175341]\n",
      "loss: 0.597666  [46400/175341]\n",
      "loss: 0.364745  [48000/175341]\n",
      "loss: 0.694796  [49600/175341]\n",
      "loss: 0.366570  [51200/175341]\n",
      "loss: 0.422843  [52800/175341]\n",
      "loss: 0.798280  [54400/175341]\n",
      "loss: 0.747893  [56000/175341]\n",
      "loss: 0.589013  [57600/175341]\n",
      "loss: 0.495265  [59200/175341]\n",
      "loss: 0.628984  [60800/175341]\n",
      "loss: 0.312435  [62400/175341]\n",
      "loss: 0.223505  [64000/175341]\n",
      "loss: 0.743723  [65600/175341]\n",
      "loss: 0.194719  [67200/175341]\n",
      "loss: 0.484530  [68800/175341]\n",
      "loss: 0.228042  [70400/175341]\n",
      "loss: 0.437382  [72000/175341]\n",
      "loss: 0.506768  [73600/175341]\n",
      "loss: 0.505264  [75200/175341]\n",
      "loss: 0.590482  [76800/175341]\n",
      "loss: 1.009133  [78400/175341]\n",
      "loss: 0.439910  [80000/175341]\n",
      "loss: 0.229782  [81600/175341]\n",
      "loss: 0.167493  [83200/175341]\n",
      "loss: 0.569362  [84800/175341]\n",
      "loss: 0.422937  [86400/175341]\n",
      "loss: 0.330783  [88000/175341]\n",
      "loss: 0.475271  [89600/175341]\n",
      "loss: 0.291412  [91200/175341]\n",
      "loss: 0.547177  [92800/175341]\n",
      "loss: 0.644173  [94400/175341]\n",
      "loss: 0.624261  [96000/175341]\n",
      "loss: 0.586700  [97600/175341]\n",
      "loss: 0.299261  [99200/175341]\n",
      "loss: 0.594725  [100800/175341]\n",
      "loss: 0.520435  [102400/175341]\n",
      "loss: 0.297048  [104000/175341]\n",
      "loss: 0.580167  [105600/175341]\n",
      "loss: 0.191675  [107200/175341]\n",
      "loss: 0.390260  [108800/175341]\n",
      "loss: 0.666315  [110400/175341]\n",
      "loss: 0.364879  [112000/175341]\n",
      "loss: 0.292169  [113600/175341]\n",
      "loss: 0.254853  [115200/175341]\n",
      "loss: 1.225875  [116800/175341]\n",
      "loss: 0.673487  [118400/175341]\n",
      "loss: 0.243715  [120000/175341]\n",
      "loss: 0.486093  [121600/175341]\n",
      "loss: 0.641833  [123200/175341]\n",
      "loss: 0.666275  [124800/175341]\n",
      "loss: 0.302314  [126400/175341]\n",
      "loss: 0.321428  [128000/175341]\n",
      "loss: 0.283364  [129600/175341]\n",
      "loss: 0.765239  [131200/175341]\n",
      "loss: 0.384210  [132800/175341]\n",
      "loss: 0.574404  [134400/175341]\n",
      "loss: 0.490178  [136000/175341]\n",
      "loss: 0.285269  [137600/175341]\n",
      "loss: 0.773988  [139200/175341]\n",
      "loss: 0.413160  [140800/175341]\n",
      "loss: 0.674167  [142400/175341]\n",
      "loss: 0.592548  [144000/175341]\n",
      "loss: 0.359238  [145600/175341]\n",
      "loss: 0.562452  [147200/175341]\n",
      "loss: 0.246388  [148800/175341]\n",
      "loss: 0.448067  [150400/175341]\n",
      "loss: 0.452144  [152000/175341]\n",
      "loss: 0.376123  [153600/175341]\n",
      "loss: 0.388164  [155200/175341]\n",
      "loss: 0.370341  [156800/175341]\n",
      "loss: 0.416024  [158400/175341]\n",
      "loss: 0.874505  [160000/175341]\n",
      "loss: 0.254941  [161600/175341]\n",
      "loss: 0.730498  [163200/175341]\n",
      "loss: 0.388210  [164800/175341]\n",
      "loss: 0.546198  [166400/175341]\n",
      "loss: 0.203686  [168000/175341]\n",
      "loss: 0.850643  [169600/175341]\n",
      "loss: 0.477165  [171200/175341]\n",
      "loss: 0.402953  [172800/175341]\n",
      "loss: 0.397460  [174400/175341]\n",
      "Train Accuracy: 80.9714%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.546770, F1-score: 76.64%, Macro_F1-Score:  40.30%  \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.160982  [    0/175341]\n",
      "loss: 0.578499  [ 1600/175341]\n",
      "loss: 0.391844  [ 3200/175341]\n",
      "loss: 0.540511  [ 4800/175341]\n",
      "loss: 0.266913  [ 6400/175341]\n",
      "loss: 0.616473  [ 8000/175341]\n",
      "loss: 0.831385  [ 9600/175341]\n",
      "loss: 0.962188  [11200/175341]\n",
      "loss: 0.474269  [12800/175341]\n",
      "loss: 0.776399  [14400/175341]\n",
      "loss: 0.706139  [16000/175341]\n",
      "loss: 0.515319  [17600/175341]\n",
      "loss: 0.210446  [19200/175341]\n",
      "loss: 1.095387  [20800/175341]\n",
      "loss: 0.768437  [22400/175341]\n",
      "loss: 0.522384  [24000/175341]\n",
      "loss: 0.629722  [25600/175341]\n",
      "loss: 0.527559  [27200/175341]\n",
      "loss: 0.381876  [28800/175341]\n",
      "loss: 0.607749  [30400/175341]\n",
      "loss: 0.581053  [32000/175341]\n",
      "loss: 0.836206  [33600/175341]\n",
      "loss: 0.145305  [35200/175341]\n",
      "loss: 0.364451  [36800/175341]\n",
      "loss: 0.446315  [38400/175341]\n",
      "loss: 0.393365  [40000/175341]\n",
      "loss: 0.376103  [41600/175341]\n",
      "loss: 0.337042  [43200/175341]\n",
      "loss: 0.540148  [44800/175341]\n",
      "loss: 0.499364  [46400/175341]\n",
      "loss: 0.733350  [48000/175341]\n",
      "loss: 0.793375  [49600/175341]\n",
      "loss: 0.275570  [51200/175341]\n",
      "loss: 0.226161  [52800/175341]\n",
      "loss: 0.484675  [54400/175341]\n",
      "loss: 0.304137  [56000/175341]\n",
      "loss: 0.345866  [57600/175341]\n",
      "loss: 0.723808  [59200/175341]\n",
      "loss: 0.215921  [60800/175341]\n",
      "loss: 0.617356  [62400/175341]\n",
      "loss: 0.660784  [64000/175341]\n",
      "loss: 0.494452  [65600/175341]\n",
      "loss: 0.266561  [67200/175341]\n",
      "loss: 0.118115  [68800/175341]\n",
      "loss: 0.318558  [70400/175341]\n",
      "loss: 0.302448  [72000/175341]\n",
      "loss: 0.214777  [73600/175341]\n",
      "loss: 0.643018  [75200/175341]\n",
      "loss: 0.459168  [76800/175341]\n",
      "loss: 0.500773  [78400/175341]\n",
      "loss: 0.644058  [80000/175341]\n",
      "loss: 0.634634  [81600/175341]\n",
      "loss: 0.658850  [83200/175341]\n",
      "loss: 0.138013  [84800/175341]\n",
      "loss: 0.482145  [86400/175341]\n",
      "loss: 0.318342  [88000/175341]\n",
      "loss: 0.530714  [89600/175341]\n",
      "loss: 0.320259  [91200/175341]\n",
      "loss: 0.173896  [92800/175341]\n",
      "loss: 0.456270  [94400/175341]\n",
      "loss: 0.316119  [96000/175341]\n",
      "loss: 0.181606  [97600/175341]\n",
      "loss: 0.617515  [99200/175341]\n",
      "loss: 0.713051  [100800/175341]\n",
      "loss: 0.447766  [102400/175341]\n",
      "loss: 0.476230  [104000/175341]\n",
      "loss: 0.681186  [105600/175341]\n",
      "loss: 0.314812  [107200/175341]\n",
      "loss: 0.477901  [108800/175341]\n",
      "loss: 0.218881  [110400/175341]\n",
      "loss: 0.094744  [112000/175341]\n",
      "loss: 0.776624  [113600/175341]\n",
      "loss: 0.337032  [115200/175341]\n",
      "loss: 0.984036  [116800/175341]\n",
      "loss: 0.891751  [118400/175341]\n",
      "loss: 0.158431  [120000/175341]\n",
      "loss: 0.382230  [121600/175341]\n",
      "loss: 0.909423  [123200/175341]\n",
      "loss: 0.472637  [124800/175341]\n",
      "loss: 0.437865  [126400/175341]\n",
      "loss: 0.685829  [128000/175341]\n",
      "loss: 0.418716  [129600/175341]\n",
      "loss: 0.450126  [131200/175341]\n",
      "loss: 0.212699  [132800/175341]\n",
      "loss: 0.468844  [134400/175341]\n",
      "loss: 0.390484  [136000/175341]\n",
      "loss: 0.246859  [137600/175341]\n",
      "loss: 0.229848  [139200/175341]\n",
      "loss: 0.436993  [140800/175341]\n",
      "loss: 0.593423  [142400/175341]\n",
      "loss: 0.869853  [144000/175341]\n",
      "loss: 0.492042  [145600/175341]\n",
      "loss: 0.379082  [147200/175341]\n",
      "loss: 0.573642  [148800/175341]\n",
      "loss: 0.471001  [150400/175341]\n",
      "loss: 0.776744  [152000/175341]\n",
      "loss: 0.509738  [153600/175341]\n",
      "loss: 0.394458  [155200/175341]\n",
      "loss: 0.371762  [156800/175341]\n",
      "loss: 0.346621  [158400/175341]\n",
      "loss: 0.765873  [160000/175341]\n",
      "loss: 0.513888  [161600/175341]\n",
      "loss: 0.488799  [163200/175341]\n",
      "loss: 0.340994  [164800/175341]\n",
      "loss: 0.706578  [166400/175341]\n",
      "loss: 0.425945  [168000/175341]\n",
      "loss: 0.146042  [169600/175341]\n",
      "loss: 0.459324  [171200/175341]\n",
      "loss: 0.745870  [172800/175341]\n",
      "loss: 0.256287  [174400/175341]\n",
      "Train Accuracy: 80.9320%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.583116, F1-score: 74.59%, Macro_F1-Score:  38.93%  \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.259832  [    0/175341]\n",
      "loss: 0.327543  [ 1600/175341]\n",
      "loss: 0.964443  [ 3200/175341]\n",
      "loss: 0.379229  [ 4800/175341]\n",
      "loss: 0.815856  [ 6400/175341]\n",
      "loss: 0.458389  [ 8000/175341]\n",
      "loss: 0.549165  [ 9600/175341]\n",
      "loss: 0.656058  [11200/175341]\n",
      "loss: 0.387972  [12800/175341]\n",
      "loss: 1.092551  [14400/175341]\n",
      "loss: 0.572691  [16000/175341]\n",
      "loss: 0.452052  [17600/175341]\n",
      "loss: 0.650423  [19200/175341]\n",
      "loss: 0.858776  [20800/175341]\n",
      "loss: 0.681263  [22400/175341]\n",
      "loss: 0.256905  [24000/175341]\n",
      "loss: 0.620751  [25600/175341]\n",
      "loss: 0.246096  [27200/175341]\n",
      "loss: 0.371711  [28800/175341]\n",
      "loss: 0.240814  [30400/175341]\n",
      "loss: 0.400844  [32000/175341]\n",
      "loss: 0.624152  [33600/175341]\n",
      "loss: 0.669528  [35200/175341]\n",
      "loss: 0.697636  [36800/175341]\n",
      "loss: 0.697371  [38400/175341]\n",
      "loss: 0.162740  [40000/175341]\n",
      "loss: 0.454409  [41600/175341]\n",
      "loss: 1.248369  [43200/175341]\n",
      "loss: 0.181937  [44800/175341]\n",
      "loss: 0.548042  [46400/175341]\n",
      "loss: 0.688086  [48000/175341]\n",
      "loss: 0.844715  [49600/175341]\n",
      "loss: 0.994527  [51200/175341]\n",
      "loss: 0.417450  [52800/175341]\n",
      "loss: 0.427172  [54400/175341]\n",
      "loss: 0.587990  [56000/175341]\n",
      "loss: 0.458606  [57600/175341]\n",
      "loss: 0.533123  [59200/175341]\n",
      "loss: 0.920946  [60800/175341]\n",
      "loss: 0.564887  [62400/175341]\n",
      "loss: 0.332184  [64000/175341]\n",
      "loss: 0.541428  [65600/175341]\n",
      "loss: 0.364354  [67200/175341]\n",
      "loss: 0.139001  [68800/175341]\n",
      "loss: 0.328154  [70400/175341]\n",
      "loss: 0.741752  [72000/175341]\n",
      "loss: 0.373008  [73600/175341]\n",
      "loss: 0.848274  [75200/175341]\n",
      "loss: 0.774170  [76800/175341]\n",
      "loss: 0.556092  [78400/175341]\n",
      "loss: 0.431288  [80000/175341]\n",
      "loss: 0.646965  [81600/175341]\n",
      "loss: 0.382191  [83200/175341]\n",
      "loss: 0.224536  [84800/175341]\n",
      "loss: 0.145661  [86400/175341]\n",
      "loss: 0.335013  [88000/175341]\n",
      "loss: 0.567722  [89600/175341]\n",
      "loss: 0.351609  [91200/175341]\n",
      "loss: 0.494568  [92800/175341]\n",
      "loss: 0.772542  [94400/175341]\n",
      "loss: 0.611626  [96000/175341]\n",
      "loss: 0.598627  [97600/175341]\n",
      "loss: 0.726592  [99200/175341]\n",
      "loss: 0.420121  [100800/175341]\n",
      "loss: 0.381591  [102400/175341]\n",
      "loss: 0.352930  [104000/175341]\n",
      "loss: 0.575074  [105600/175341]\n",
      "loss: 0.376699  [107200/175341]\n",
      "loss: 0.632602  [108800/175341]\n",
      "loss: 0.482244  [110400/175341]\n",
      "loss: 0.507838  [112000/175341]\n",
      "loss: 0.762510  [113600/175341]\n",
      "loss: 0.287742  [115200/175341]\n",
      "loss: 0.853724  [116800/175341]\n",
      "loss: 0.572992  [118400/175341]\n",
      "loss: 0.834362  [120000/175341]\n",
      "loss: 0.355980  [121600/175341]\n",
      "loss: 0.422886  [123200/175341]\n",
      "loss: 0.432437  [124800/175341]\n",
      "loss: 0.456500  [126400/175341]\n",
      "loss: 0.087738  [128000/175341]\n",
      "loss: 0.705557  [129600/175341]\n",
      "loss: 0.329028  [131200/175341]\n",
      "loss: 0.464146  [132800/175341]\n",
      "loss: 0.570357  [134400/175341]\n",
      "loss: 0.153328  [136000/175341]\n",
      "loss: 0.189137  [137600/175341]\n",
      "loss: 0.379782  [139200/175341]\n",
      "loss: 0.222678  [140800/175341]\n",
      "loss: 0.194741  [142400/175341]\n",
      "loss: 0.580116  [144000/175341]\n",
      "loss: 0.234539  [145600/175341]\n",
      "loss: 0.508975  [147200/175341]\n",
      "loss: 0.740098  [148800/175341]\n",
      "loss: 0.491057  [150400/175341]\n",
      "loss: 0.573279  [152000/175341]\n",
      "loss: 0.779869  [153600/175341]\n",
      "loss: 1.184636  [155200/175341]\n",
      "loss: 0.584180  [156800/175341]\n",
      "loss: 0.274778  [158400/175341]\n",
      "loss: 0.504344  [160000/175341]\n",
      "loss: 0.569099  [161600/175341]\n",
      "loss: 0.729660  [163200/175341]\n",
      "loss: 0.658793  [164800/175341]\n",
      "loss: 0.472984  [166400/175341]\n",
      "loss: 0.510801  [168000/175341]\n",
      "loss: 0.278265  [169600/175341]\n",
      "loss: 0.504595  [171200/175341]\n",
      "loss: 0.405732  [172800/175341]\n",
      "loss: 0.421604  [174400/175341]\n",
      "Train Accuracy: 80.9685%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.583260, F1-score: 74.89%, Macro_F1-Score:  39.14%  \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.378810  [    0/175341]\n",
      "loss: 0.646734  [ 1600/175341]\n",
      "loss: 0.680906  [ 3200/175341]\n",
      "loss: 0.455400  [ 4800/175341]\n",
      "loss: 0.254763  [ 6400/175341]\n",
      "loss: 0.410817  [ 8000/175341]\n",
      "loss: 0.515374  [ 9600/175341]\n",
      "loss: 0.492235  [11200/175341]\n",
      "loss: 0.709537  [12800/175341]\n",
      "loss: 0.649080  [14400/175341]\n",
      "loss: 0.306631  [16000/175341]\n",
      "loss: 0.471700  [17600/175341]\n",
      "loss: 0.403045  [19200/175341]\n",
      "loss: 0.218063  [20800/175341]\n",
      "loss: 0.783805  [22400/175341]\n",
      "loss: 0.304137  [24000/175341]\n",
      "loss: 0.393559  [25600/175341]\n",
      "loss: 0.391202  [27200/175341]\n",
      "loss: 0.230233  [28800/175341]\n",
      "loss: 0.600691  [30400/175341]\n",
      "loss: 0.261295  [32000/175341]\n",
      "loss: 0.461515  [33600/175341]\n",
      "loss: 0.775757  [35200/175341]\n",
      "loss: 0.440764  [36800/175341]\n",
      "loss: 0.334718  [38400/175341]\n",
      "loss: 0.114639  [40000/175341]\n",
      "loss: 0.400049  [41600/175341]\n",
      "loss: 0.740250  [43200/175341]\n",
      "loss: 0.799669  [44800/175341]\n",
      "loss: 0.533400  [46400/175341]\n",
      "loss: 0.527091  [48000/175341]\n",
      "loss: 0.287015  [49600/175341]\n",
      "loss: 0.408228  [51200/175341]\n",
      "loss: 0.508057  [52800/175341]\n",
      "loss: 0.334327  [54400/175341]\n",
      "loss: 0.416597  [56000/175341]\n",
      "loss: 0.336789  [57600/175341]\n",
      "loss: 0.428942  [59200/175341]\n",
      "loss: 0.469379  [60800/175341]\n",
      "loss: 0.335765  [62400/175341]\n",
      "loss: 0.421973  [64000/175341]\n",
      "loss: 0.398359  [65600/175341]\n",
      "loss: 0.379822  [67200/175341]\n",
      "loss: 0.732098  [68800/175341]\n",
      "loss: 0.326206  [70400/175341]\n",
      "loss: 0.382406  [72000/175341]\n",
      "loss: 0.641089  [73600/175341]\n",
      "loss: 0.354501  [75200/175341]\n",
      "loss: 0.478554  [76800/175341]\n",
      "loss: 0.403736  [78400/175341]\n",
      "loss: 0.396595  [80000/175341]\n",
      "loss: 0.279585  [81600/175341]\n",
      "loss: 0.669334  [83200/175341]\n",
      "loss: 0.525949  [84800/175341]\n",
      "loss: 0.507669  [86400/175341]\n",
      "loss: 0.209862  [88000/175341]\n",
      "loss: 0.481091  [89600/175341]\n",
      "loss: 0.126724  [91200/175341]\n",
      "loss: 0.492821  [92800/175341]\n",
      "loss: 0.334601  [94400/175341]\n",
      "loss: 0.714069  [96000/175341]\n",
      "loss: 0.613312  [97600/175341]\n",
      "loss: 0.545321  [99200/175341]\n",
      "loss: 0.546257  [100800/175341]\n",
      "loss: 0.481086  [102400/175341]\n",
      "loss: 0.500945  [104000/175341]\n",
      "loss: 0.358240  [105600/175341]\n",
      "loss: 0.960097  [107200/175341]\n",
      "loss: 0.713623  [108800/175341]\n",
      "loss: 0.502502  [110400/175341]\n",
      "loss: 0.402728  [112000/175341]\n",
      "loss: 0.455504  [113600/175341]\n",
      "loss: 0.459578  [115200/175341]\n",
      "loss: 0.303601  [116800/175341]\n",
      "loss: 0.612904  [118400/175341]\n",
      "loss: 0.492086  [120000/175341]\n",
      "loss: 0.307489  [121600/175341]\n",
      "loss: 0.252226  [123200/175341]\n",
      "loss: 0.418149  [124800/175341]\n",
      "loss: 0.460528  [126400/175341]\n",
      "loss: 0.782403  [128000/175341]\n",
      "loss: 0.449297  [129600/175341]\n",
      "loss: 0.249043  [131200/175341]\n",
      "loss: 0.246756  [132800/175341]\n",
      "loss: 0.326667  [134400/175341]\n",
      "loss: 0.407342  [136000/175341]\n",
      "loss: 0.354977  [137600/175341]\n",
      "loss: 0.384001  [139200/175341]\n",
      "loss: 0.354070  [140800/175341]\n",
      "loss: 0.145798  [142400/175341]\n",
      "loss: 0.735194  [144000/175341]\n",
      "loss: 0.433137  [145600/175341]\n",
      "loss: 0.092616  [147200/175341]\n",
      "loss: 0.132965  [148800/175341]\n",
      "loss: 0.592441  [150400/175341]\n",
      "loss: 0.083712  [152000/175341]\n",
      "loss: 0.464715  [153600/175341]\n",
      "loss: 0.253984  [155200/175341]\n",
      "loss: 0.177248  [156800/175341]\n",
      "loss: 0.434832  [158400/175341]\n",
      "loss: 0.591333  [160000/175341]\n",
      "loss: 0.132017  [161600/175341]\n",
      "loss: 0.421388  [163200/175341]\n",
      "loss: 0.528760  [164800/175341]\n",
      "loss: 0.405307  [166400/175341]\n",
      "loss: 0.902360  [168000/175341]\n",
      "loss: 0.486476  [169600/175341]\n",
      "loss: 0.370545  [171200/175341]\n",
      "loss: 0.466001  [172800/175341]\n",
      "loss: 0.195725  [174400/175341]\n",
      "Train Accuracy: 81.0267%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.560332, F1-score: 76.51%, Macro_F1-Score:  39.75%  \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.329468  [    0/175341]\n",
      "loss: 0.246974  [ 1600/175341]\n",
      "loss: 0.110060  [ 3200/175341]\n",
      "loss: 0.336463  [ 4800/175341]\n",
      "loss: 0.468679  [ 6400/175341]\n",
      "loss: 0.297898  [ 8000/175341]\n",
      "loss: 0.837212  [ 9600/175341]\n",
      "loss: 0.332021  [11200/175341]\n",
      "loss: 0.472519  [12800/175341]\n",
      "loss: 0.606303  [14400/175341]\n",
      "loss: 0.220338  [16000/175341]\n",
      "loss: 0.257148  [17600/175341]\n",
      "loss: 0.256971  [19200/175341]\n",
      "loss: 0.509082  [20800/175341]\n",
      "loss: 0.812750  [22400/175341]\n",
      "loss: 0.378024  [24000/175341]\n",
      "loss: 0.385820  [25600/175341]\n",
      "loss: 0.268189  [27200/175341]\n",
      "loss: 0.488970  [28800/175341]\n",
      "loss: 0.449707  [30400/175341]\n",
      "loss: 0.320009  [32000/175341]\n",
      "loss: 0.468420  [33600/175341]\n",
      "loss: 0.572124  [35200/175341]\n",
      "loss: 0.650836  [36800/175341]\n",
      "loss: 0.346700  [38400/175341]\n",
      "loss: 0.298927  [40000/175341]\n",
      "loss: 0.459095  [41600/175341]\n",
      "loss: 0.223115  [43200/175341]\n",
      "loss: 0.477507  [44800/175341]\n",
      "loss: 0.766479  [46400/175341]\n",
      "loss: 0.948346  [48000/175341]\n",
      "loss: 0.848060  [49600/175341]\n",
      "loss: 0.502220  [51200/175341]\n",
      "loss: 0.221723  [52800/175341]\n",
      "loss: 0.659725  [54400/175341]\n",
      "loss: 0.308021  [56000/175341]\n",
      "loss: 0.538865  [57600/175341]\n",
      "loss: 0.209811  [59200/175341]\n",
      "loss: 0.165829  [60800/175341]\n",
      "loss: 0.797907  [62400/175341]\n",
      "loss: 0.490048  [64000/175341]\n",
      "loss: 0.291155  [65600/175341]\n",
      "loss: 0.302334  [67200/175341]\n",
      "loss: 0.327738  [68800/175341]\n",
      "loss: 0.225664  [70400/175341]\n",
      "loss: 0.588554  [72000/175341]\n",
      "loss: 0.644116  [73600/175341]\n",
      "loss: 0.460598  [75200/175341]\n",
      "loss: 0.756355  [76800/175341]\n",
      "loss: 0.686732  [78400/175341]\n",
      "loss: 0.656574  [80000/175341]\n",
      "loss: 0.262007  [81600/175341]\n",
      "loss: 0.823050  [83200/175341]\n",
      "loss: 0.403681  [84800/175341]\n",
      "loss: 0.615765  [86400/175341]\n",
      "loss: 0.524515  [88000/175341]\n",
      "loss: 0.432095  [89600/175341]\n",
      "loss: 0.434533  [91200/175341]\n",
      "loss: 0.429227  [92800/175341]\n",
      "loss: 0.961445  [94400/175341]\n",
      "loss: 0.517099  [96000/175341]\n",
      "loss: 0.690445  [97600/175341]\n",
      "loss: 0.412134  [99200/175341]\n",
      "loss: 0.596017  [100800/175341]\n",
      "loss: 0.476317  [102400/175341]\n",
      "loss: 0.455969  [104000/175341]\n",
      "loss: 0.201855  [105600/175341]\n",
      "loss: 0.722680  [107200/175341]\n",
      "loss: 0.473670  [108800/175341]\n",
      "loss: 0.229788  [110400/175341]\n",
      "loss: 0.400544  [112000/175341]\n",
      "loss: 0.859866  [113600/175341]\n",
      "loss: 0.549692  [115200/175341]\n",
      "loss: 0.367590  [116800/175341]\n",
      "loss: 0.685259  [118400/175341]\n",
      "loss: 0.314276  [120000/175341]\n",
      "loss: 0.349046  [121600/175341]\n",
      "loss: 0.389341  [123200/175341]\n",
      "loss: 0.567451  [124800/175341]\n",
      "loss: 0.745009  [126400/175341]\n",
      "loss: 0.570013  [128000/175341]\n",
      "loss: 0.289918  [129600/175341]\n",
      "loss: 0.439545  [131200/175341]\n",
      "loss: 0.344591  [132800/175341]\n",
      "loss: 1.266432  [134400/175341]\n",
      "loss: 0.312363  [136000/175341]\n",
      "loss: 0.361795  [137600/175341]\n",
      "loss: 0.565885  [139200/175341]\n",
      "loss: 0.272149  [140800/175341]\n",
      "loss: 0.382216  [142400/175341]\n",
      "loss: 0.684558  [144000/175341]\n",
      "loss: 0.422528  [145600/175341]\n",
      "loss: 0.597309  [147200/175341]\n",
      "loss: 0.092816  [148800/175341]\n",
      "loss: 0.526905  [150400/175341]\n",
      "loss: 0.501692  [152000/175341]\n",
      "loss: 0.654475  [153600/175341]\n",
      "loss: 0.315336  [155200/175341]\n",
      "loss: 0.595038  [156800/175341]\n",
      "loss: 0.236443  [158400/175341]\n",
      "loss: 0.246230  [160000/175341]\n",
      "loss: 0.372892  [161600/175341]\n",
      "loss: 0.281453  [163200/175341]\n",
      "loss: 0.445634  [164800/175341]\n",
      "loss: 0.600383  [166400/175341]\n",
      "loss: 0.374377  [168000/175341]\n",
      "loss: 0.347519  [169600/175341]\n",
      "loss: 0.436681  [171200/175341]\n",
      "loss: 0.391469  [172800/175341]\n",
      "loss: 0.548348  [174400/175341]\n",
      "Train Accuracy: 81.0871%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.571999, F1-score: 75.59%, Macro_F1-Score:  39.46%  \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.536272  [    0/175341]\n",
      "loss: 0.652915  [ 1600/175341]\n",
      "loss: 0.622790  [ 3200/175341]\n",
      "loss: 0.268590  [ 4800/175341]\n",
      "loss: 0.511033  [ 6400/175341]\n",
      "loss: 0.151833  [ 8000/175341]\n",
      "loss: 0.540144  [ 9600/175341]\n",
      "loss: 0.464419  [11200/175341]\n",
      "loss: 0.333331  [12800/175341]\n",
      "loss: 0.347097  [14400/175341]\n",
      "loss: 0.280921  [16000/175341]\n",
      "loss: 0.383178  [17600/175341]\n",
      "loss: 0.642437  [19200/175341]\n",
      "loss: 0.420963  [20800/175341]\n",
      "loss: 0.191463  [22400/175341]\n",
      "loss: 0.543474  [24000/175341]\n",
      "loss: 0.703353  [25600/175341]\n",
      "loss: 0.920080  [27200/175341]\n",
      "loss: 0.873789  [28800/175341]\n",
      "loss: 0.440594  [30400/175341]\n",
      "loss: 0.515177  [32000/175341]\n",
      "loss: 0.576049  [33600/175341]\n",
      "loss: 0.382483  [35200/175341]\n",
      "loss: 0.506473  [36800/175341]\n",
      "loss: 0.220143  [38400/175341]\n",
      "loss: 0.151346  [40000/175341]\n",
      "loss: 1.153319  [41600/175341]\n",
      "loss: 0.264276  [43200/175341]\n",
      "loss: 0.291865  [44800/175341]\n",
      "loss: 0.948544  [46400/175341]\n",
      "loss: 0.474992  [48000/175341]\n",
      "loss: 0.205321  [49600/175341]\n",
      "loss: 0.289813  [51200/175341]\n",
      "loss: 0.409857  [52800/175341]\n",
      "loss: 0.201685  [54400/175341]\n",
      "loss: 0.581973  [56000/175341]\n",
      "loss: 0.884653  [57600/175341]\n",
      "loss: 0.260138  [59200/175341]\n",
      "loss: 0.217155  [60800/175341]\n",
      "loss: 0.420536  [62400/175341]\n",
      "loss: 0.512558  [64000/175341]\n",
      "loss: 0.466237  [65600/175341]\n",
      "loss: 0.465589  [67200/175341]\n",
      "loss: 0.839186  [68800/175341]\n",
      "loss: 0.289119  [70400/175341]\n",
      "loss: 0.714274  [72000/175341]\n",
      "loss: 0.341740  [73600/175341]\n",
      "loss: 0.948725  [75200/175341]\n",
      "loss: 0.084306  [76800/175341]\n",
      "loss: 0.989190  [78400/175341]\n",
      "loss: 0.480704  [80000/175341]\n",
      "loss: 0.364619  [81600/175341]\n",
      "loss: 0.610864  [83200/175341]\n",
      "loss: 0.230546  [84800/175341]\n",
      "loss: 0.602735  [86400/175341]\n",
      "loss: 0.482327  [88000/175341]\n",
      "loss: 0.389093  [89600/175341]\n",
      "loss: 0.042931  [91200/175341]\n",
      "loss: 0.406649  [92800/175341]\n",
      "loss: 0.755889  [94400/175341]\n",
      "loss: 0.325586  [96000/175341]\n",
      "loss: 0.579582  [97600/175341]\n",
      "loss: 0.623312  [99200/175341]\n",
      "loss: 0.311138  [100800/175341]\n",
      "loss: 0.311153  [102400/175341]\n",
      "loss: 0.546394  [104000/175341]\n",
      "loss: 0.946079  [105600/175341]\n",
      "loss: 0.541625  [107200/175341]\n",
      "loss: 0.454758  [108800/175341]\n",
      "loss: 0.184154  [110400/175341]\n",
      "loss: 0.393737  [112000/175341]\n",
      "loss: 0.354878  [113600/175341]\n",
      "loss: 0.464199  [115200/175341]\n",
      "loss: 0.836589  [116800/175341]\n",
      "loss: 0.310476  [118400/175341]\n",
      "loss: 0.691726  [120000/175341]\n",
      "loss: 0.446291  [121600/175341]\n",
      "loss: 0.547834  [123200/175341]\n",
      "loss: 0.657198  [124800/175341]\n",
      "loss: 0.379400  [126400/175341]\n",
      "loss: 1.109103  [128000/175341]\n",
      "loss: 0.471807  [129600/175341]\n",
      "loss: 0.137467  [131200/175341]\n",
      "loss: 0.472594  [132800/175341]\n",
      "loss: 0.209973  [134400/175341]\n",
      "loss: 0.429452  [136000/175341]\n",
      "loss: 0.340676  [137600/175341]\n",
      "loss: 0.642143  [139200/175341]\n",
      "loss: 0.189997  [140800/175341]\n",
      "loss: 0.473804  [142400/175341]\n",
      "loss: 0.505220  [144000/175341]\n",
      "loss: 0.332258  [145600/175341]\n",
      "loss: 0.475602  [147200/175341]\n",
      "loss: 0.592892  [148800/175341]\n",
      "loss: 0.654204  [150400/175341]\n",
      "loss: 0.345453  [152000/175341]\n",
      "loss: 1.020980  [153600/175341]\n",
      "loss: 0.737946  [155200/175341]\n",
      "loss: 0.353956  [156800/175341]\n",
      "loss: 0.542129  [158400/175341]\n",
      "loss: 0.827375  [160000/175341]\n",
      "loss: 0.942654  [161600/175341]\n",
      "loss: 0.517358  [163200/175341]\n",
      "loss: 0.652147  [164800/175341]\n",
      "loss: 0.708431  [166400/175341]\n",
      "loss: 0.266311  [168000/175341]\n",
      "loss: 0.363132  [169600/175341]\n",
      "loss: 0.284395  [171200/175341]\n",
      "loss: 0.484178  [172800/175341]\n",
      "loss: 0.336925  [174400/175341]\n",
      "Train Accuracy: 81.1065%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.557813, F1-score: 75.59%, Macro_F1-Score:  39.74%  \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.460881  [    0/175341]\n",
      "loss: 0.345346  [ 1600/175341]\n",
      "loss: 0.250830  [ 3200/175341]\n",
      "loss: 0.392293  [ 4800/175341]\n",
      "loss: 0.331443  [ 6400/175341]\n",
      "loss: 0.605627  [ 8000/175341]\n",
      "loss: 0.490048  [ 9600/175341]\n",
      "loss: 0.588524  [11200/175341]\n",
      "loss: 0.533830  [12800/175341]\n",
      "loss: 0.595135  [14400/175341]\n",
      "loss: 0.649939  [16000/175341]\n",
      "loss: 0.076808  [17600/175341]\n",
      "loss: 0.440320  [19200/175341]\n",
      "loss: 0.369769  [20800/175341]\n",
      "loss: 0.361825  [22400/175341]\n",
      "loss: 0.672155  [24000/175341]\n",
      "loss: 0.287685  [25600/175341]\n",
      "loss: 0.810067  [27200/175341]\n",
      "loss: 0.226216  [28800/175341]\n",
      "loss: 0.225849  [30400/175341]\n",
      "loss: 0.726924  [32000/175341]\n",
      "loss: 0.649370  [33600/175341]\n",
      "loss: 0.354592  [35200/175341]\n",
      "loss: 0.509233  [36800/175341]\n",
      "loss: 0.310462  [38400/175341]\n",
      "loss: 0.426986  [40000/175341]\n",
      "loss: 0.354755  [41600/175341]\n",
      "loss: 0.395413  [43200/175341]\n",
      "loss: 0.218656  [44800/175341]\n",
      "loss: 0.229297  [46400/175341]\n",
      "loss: 0.533713  [48000/175341]\n",
      "loss: 0.135063  [49600/175341]\n",
      "loss: 0.642392  [51200/175341]\n",
      "loss: 0.638848  [52800/175341]\n",
      "loss: 0.305008  [54400/175341]\n",
      "loss: 0.827847  [56000/175341]\n",
      "loss: 0.308388  [57600/175341]\n",
      "loss: 0.442631  [59200/175341]\n",
      "loss: 0.634547  [60800/175341]\n",
      "loss: 0.423546  [62400/175341]\n",
      "loss: 0.757709  [64000/175341]\n",
      "loss: 0.459802  [65600/175341]\n",
      "loss: 0.313749  [67200/175341]\n",
      "loss: 0.351915  [68800/175341]\n",
      "loss: 0.173422  [70400/175341]\n",
      "loss: 0.623331  [72000/175341]\n",
      "loss: 0.599351  [73600/175341]\n",
      "loss: 0.542444  [75200/175341]\n",
      "loss: 0.816675  [76800/175341]\n",
      "loss: 0.173257  [78400/175341]\n",
      "loss: 0.360965  [80000/175341]\n",
      "loss: 0.207458  [81600/175341]\n",
      "loss: 0.218657  [83200/175341]\n",
      "loss: 0.530959  [84800/175341]\n",
      "loss: 0.334825  [86400/175341]\n",
      "loss: 0.270191  [88000/175341]\n",
      "loss: 0.528167  [89600/175341]\n",
      "loss: 0.492099  [91200/175341]\n",
      "loss: 0.600143  [92800/175341]\n",
      "loss: 0.235918  [94400/175341]\n",
      "loss: 0.719143  [96000/175341]\n",
      "loss: 0.202708  [97600/175341]\n",
      "loss: 0.119583  [99200/175341]\n",
      "loss: 0.107632  [100800/175341]\n",
      "loss: 0.230431  [102400/175341]\n",
      "loss: 0.301125  [104000/175341]\n",
      "loss: 0.811852  [105600/175341]\n",
      "loss: 0.297287  [107200/175341]\n",
      "loss: 0.952220  [108800/175341]\n",
      "loss: 0.345881  [110400/175341]\n",
      "loss: 0.485236  [112000/175341]\n",
      "loss: 0.403473  [113600/175341]\n",
      "loss: 0.616614  [115200/175341]\n",
      "loss: 0.254901  [116800/175341]\n",
      "loss: 0.325383  [118400/175341]\n",
      "loss: 0.623796  [120000/175341]\n",
      "loss: 0.588203  [121600/175341]\n",
      "loss: 0.382888  [123200/175341]\n",
      "loss: 0.373981  [124800/175341]\n",
      "loss: 0.572399  [126400/175341]\n",
      "loss: 0.665612  [128000/175341]\n",
      "loss: 0.573027  [129600/175341]\n",
      "loss: 0.444846  [131200/175341]\n",
      "loss: 0.640439  [132800/175341]\n",
      "loss: 0.394587  [134400/175341]\n",
      "loss: 0.600618  [136000/175341]\n",
      "loss: 0.423146  [137600/175341]\n",
      "loss: 0.619305  [139200/175341]\n",
      "loss: 0.209284  [140800/175341]\n",
      "loss: 0.352968  [142400/175341]\n",
      "loss: 0.633109  [144000/175341]\n",
      "loss: 0.260979  [145600/175341]\n",
      "loss: 0.680451  [147200/175341]\n",
      "loss: 0.345508  [148800/175341]\n",
      "loss: 0.354121  [150400/175341]\n",
      "loss: 0.405635  [152000/175341]\n",
      "loss: 0.182637  [153600/175341]\n",
      "loss: 0.781514  [155200/175341]\n",
      "loss: 0.277273  [156800/175341]\n",
      "loss: 0.472236  [158400/175341]\n",
      "loss: 0.380668  [160000/175341]\n",
      "loss: 0.358123  [161600/175341]\n",
      "loss: 0.216317  [163200/175341]\n",
      "loss: 0.146441  [164800/175341]\n",
      "loss: 0.626073  [166400/175341]\n",
      "loss: 0.465797  [168000/175341]\n",
      "loss: 0.780812  [169600/175341]\n",
      "loss: 0.734654  [171200/175341]\n",
      "loss: 0.502661  [172800/175341]\n",
      "loss: 0.536205  [174400/175341]\n",
      "Train Accuracy: 81.1293%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.548934, F1-score: 76.72%, Macro_F1-Score:  40.61%  \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.236560  [    0/175341]\n",
      "loss: 0.229680  [ 1600/175341]\n",
      "loss: 0.801955  [ 3200/175341]\n",
      "loss: 0.594248  [ 4800/175341]\n",
      "loss: 0.385437  [ 6400/175341]\n",
      "loss: 0.558505  [ 8000/175341]\n",
      "loss: 0.421677  [ 9600/175341]\n",
      "loss: 0.308658  [11200/175341]\n",
      "loss: 0.435261  [12800/175341]\n",
      "loss: 0.383712  [14400/175341]\n",
      "loss: 0.156899  [16000/175341]\n",
      "loss: 0.373569  [17600/175341]\n",
      "loss: 0.338775  [19200/175341]\n",
      "loss: 0.665214  [20800/175341]\n",
      "loss: 0.564189  [22400/175341]\n",
      "loss: 0.280295  [24000/175341]\n",
      "loss: 0.244756  [25600/175341]\n",
      "loss: 0.354115  [27200/175341]\n",
      "loss: 0.508864  [28800/175341]\n",
      "loss: 0.367636  [30400/175341]\n",
      "loss: 0.578213  [32000/175341]\n",
      "loss: 0.820162  [33600/175341]\n",
      "loss: 0.440816  [35200/175341]\n",
      "loss: 0.832409  [36800/175341]\n",
      "loss: 0.455178  [38400/175341]\n",
      "loss: 0.316804  [40000/175341]\n",
      "loss: 0.267654  [41600/175341]\n",
      "loss: 0.569941  [43200/175341]\n",
      "loss: 0.426678  [44800/175341]\n",
      "loss: 0.592293  [46400/175341]\n",
      "loss: 0.447761  [48000/175341]\n",
      "loss: 0.148118  [49600/175341]\n",
      "loss: 0.399368  [51200/175341]\n",
      "loss: 0.718202  [52800/175341]\n",
      "loss: 0.518021  [54400/175341]\n",
      "loss: 0.405657  [56000/175341]\n",
      "loss: 0.364369  [57600/175341]\n",
      "loss: 0.561843  [59200/175341]\n",
      "loss: 0.254201  [60800/175341]\n",
      "loss: 0.542800  [62400/175341]\n",
      "loss: 0.631893  [64000/175341]\n",
      "loss: 0.229525  [65600/175341]\n",
      "loss: 0.209345  [67200/175341]\n",
      "loss: 0.631717  [68800/175341]\n",
      "loss: 0.282796  [70400/175341]\n",
      "loss: 0.193160  [72000/175341]\n",
      "loss: 0.456152  [73600/175341]\n",
      "loss: 0.241186  [75200/175341]\n",
      "loss: 0.189383  [76800/175341]\n",
      "loss: 0.426290  [78400/175341]\n",
      "loss: 0.546863  [80000/175341]\n",
      "loss: 0.499260  [81600/175341]\n",
      "loss: 0.255746  [83200/175341]\n",
      "loss: 0.242962  [84800/175341]\n",
      "loss: 0.722806  [86400/175341]\n",
      "loss: 0.774710  [88000/175341]\n",
      "loss: 0.375780  [89600/175341]\n",
      "loss: 0.643026  [91200/175341]\n",
      "loss: 0.338139  [92800/175341]\n",
      "loss: 0.297805  [94400/175341]\n",
      "loss: 0.318260  [96000/175341]\n",
      "loss: 0.318322  [97600/175341]\n",
      "loss: 0.137580  [99200/175341]\n",
      "loss: 0.710773  [100800/175341]\n",
      "loss: 0.377730  [102400/175341]\n",
      "loss: 0.197582  [104000/175341]\n",
      "loss: 0.250778  [105600/175341]\n",
      "loss: 0.282760  [107200/175341]\n",
      "loss: 0.336515  [108800/175341]\n",
      "loss: 0.218314  [110400/175341]\n",
      "loss: 0.442753  [112000/175341]\n",
      "loss: 0.405287  [113600/175341]\n",
      "loss: 0.944383  [115200/175341]\n",
      "loss: 0.710232  [116800/175341]\n",
      "loss: 0.413840  [118400/175341]\n",
      "loss: 0.317578  [120000/175341]\n",
      "loss: 0.294560  [121600/175341]\n",
      "loss: 0.682069  [123200/175341]\n",
      "loss: 0.362195  [124800/175341]\n",
      "loss: 0.160817  [126400/175341]\n",
      "loss: 0.328258  [128000/175341]\n",
      "loss: 0.566382  [129600/175341]\n",
      "loss: 0.617747  [131200/175341]\n",
      "loss: 0.212052  [132800/175341]\n",
      "loss: 0.311030  [134400/175341]\n",
      "loss: 0.459696  [136000/175341]\n",
      "loss: 0.245425  [137600/175341]\n",
      "loss: 0.457871  [139200/175341]\n",
      "loss: 0.360692  [140800/175341]\n",
      "loss: 0.134518  [142400/175341]\n",
      "loss: 0.458200  [144000/175341]\n",
      "loss: 0.447094  [145600/175341]\n",
      "loss: 0.560814  [147200/175341]\n",
      "loss: 0.726277  [148800/175341]\n",
      "loss: 1.208255  [150400/175341]\n",
      "loss: 0.251969  [152000/175341]\n",
      "loss: 0.601243  [153600/175341]\n",
      "loss: 0.665727  [155200/175341]\n",
      "loss: 0.180043  [156800/175341]\n",
      "loss: 0.627688  [158400/175341]\n",
      "loss: 0.571643  [160000/175341]\n",
      "loss: 0.383226  [161600/175341]\n",
      "loss: 0.719442  [163200/175341]\n",
      "loss: 0.428898  [164800/175341]\n",
      "loss: 0.462250  [166400/175341]\n",
      "loss: 0.311587  [168000/175341]\n",
      "loss: 0.478992  [169600/175341]\n",
      "loss: 0.593755  [171200/175341]\n",
      "loss: 0.398046  [172800/175341]\n",
      "loss: 0.632071  [174400/175341]\n",
      "Train Accuracy: 81.1111%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.575108, F1-score: 75.53%, Macro_F1-Score:  40.07%  \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.236955  [    0/175341]\n",
      "loss: 0.444670  [ 1600/175341]\n",
      "loss: 0.533131  [ 3200/175341]\n",
      "loss: 0.760050  [ 4800/175341]\n",
      "loss: 0.306895  [ 6400/175341]\n",
      "loss: 0.830600  [ 8000/175341]\n",
      "loss: 0.766966  [ 9600/175341]\n",
      "loss: 0.648491  [11200/175341]\n",
      "loss: 0.266567  [12800/175341]\n",
      "loss: 0.455367  [14400/175341]\n",
      "loss: 0.389851  [16000/175341]\n",
      "loss: 0.213516  [17600/175341]\n",
      "loss: 0.116024  [19200/175341]\n",
      "loss: 0.814565  [20800/175341]\n",
      "loss: 0.657761  [22400/175341]\n",
      "loss: 0.769808  [24000/175341]\n",
      "loss: 0.405628  [25600/175341]\n",
      "loss: 0.759685  [27200/175341]\n",
      "loss: 0.296476  [28800/175341]\n",
      "loss: 0.919262  [30400/175341]\n",
      "loss: 0.377111  [32000/175341]\n",
      "loss: 0.322619  [33600/175341]\n",
      "loss: 0.561531  [35200/175341]\n",
      "loss: 0.306498  [36800/175341]\n",
      "loss: 0.174777  [38400/175341]\n",
      "loss: 0.356394  [40000/175341]\n",
      "loss: 0.544895  [41600/175341]\n",
      "loss: 0.107428  [43200/175341]\n",
      "loss: 0.606031  [44800/175341]\n",
      "loss: 0.611273  [46400/175341]\n",
      "loss: 0.266187  [48000/175341]\n",
      "loss: 0.637160  [49600/175341]\n",
      "loss: 0.089990  [51200/175341]\n",
      "loss: 0.391635  [52800/175341]\n",
      "loss: 0.475768  [54400/175341]\n",
      "loss: 0.435075  [56000/175341]\n",
      "loss: 0.311372  [57600/175341]\n",
      "loss: 0.443522  [59200/175341]\n",
      "loss: 0.933245  [60800/175341]\n",
      "loss: 0.421899  [62400/175341]\n",
      "loss: 0.372756  [64000/175341]\n",
      "loss: 0.417087  [65600/175341]\n",
      "loss: 0.827954  [67200/175341]\n",
      "loss: 0.260968  [68800/175341]\n",
      "loss: 0.435715  [70400/175341]\n",
      "loss: 0.424610  [72000/175341]\n",
      "loss: 0.886934  [73600/175341]\n",
      "loss: 0.300217  [75200/175341]\n",
      "loss: 0.692411  [76800/175341]\n",
      "loss: 0.226525  [78400/175341]\n",
      "loss: 0.342161  [80000/175341]\n",
      "loss: 0.412643  [81600/175341]\n",
      "loss: 0.383451  [83200/175341]\n",
      "loss: 0.369946  [84800/175341]\n",
      "loss: 0.173699  [86400/175341]\n",
      "loss: 0.642571  [88000/175341]\n",
      "loss: 0.767745  [89600/175341]\n",
      "loss: 0.198408  [91200/175341]\n",
      "loss: 0.649048  [92800/175341]\n",
      "loss: 0.748174  [94400/175341]\n",
      "loss: 0.639665  [96000/175341]\n",
      "loss: 0.155944  [97600/175341]\n",
      "loss: 0.660157  [99200/175341]\n",
      "loss: 0.615822  [100800/175341]\n",
      "loss: 0.362722  [102400/175341]\n",
      "loss: 0.406668  [104000/175341]\n",
      "loss: 0.284170  [105600/175341]\n",
      "loss: 0.459130  [107200/175341]\n",
      "loss: 0.560167  [108800/175341]\n",
      "loss: 0.537977  [110400/175341]\n",
      "loss: 0.271641  [112000/175341]\n",
      "loss: 0.788445  [113600/175341]\n",
      "loss: 0.637033  [115200/175341]\n",
      "loss: 0.566405  [116800/175341]\n",
      "loss: 0.628784  [118400/175341]\n",
      "loss: 0.405120  [120000/175341]\n",
      "loss: 0.253803  [121600/175341]\n",
      "loss: 0.316136  [123200/175341]\n",
      "loss: 0.637719  [124800/175341]\n",
      "loss: 0.759341  [126400/175341]\n",
      "loss: 0.176618  [128000/175341]\n",
      "loss: 0.169426  [129600/175341]\n",
      "loss: 0.238748  [131200/175341]\n",
      "loss: 0.483948  [132800/175341]\n",
      "loss: 0.496315  [134400/175341]\n",
      "loss: 0.721673  [136000/175341]\n",
      "loss: 0.339977  [137600/175341]\n",
      "loss: 0.546039  [139200/175341]\n",
      "loss: 0.480846  [140800/175341]\n",
      "loss: 0.196050  [142400/175341]\n",
      "loss: 0.253611  [144000/175341]\n",
      "loss: 0.426501  [145600/175341]\n",
      "loss: 0.561328  [147200/175341]\n",
      "loss: 0.348022  [148800/175341]\n",
      "loss: 0.633351  [150400/175341]\n",
      "loss: 0.922595  [152000/175341]\n",
      "loss: 0.373349  [153600/175341]\n",
      "loss: 0.584564  [155200/175341]\n",
      "loss: 0.610898  [156800/175341]\n",
      "loss: 0.276942  [158400/175341]\n",
      "loss: 0.636837  [160000/175341]\n",
      "loss: 0.177562  [161600/175341]\n",
      "loss: 0.436746  [163200/175341]\n",
      "loss: 0.218960  [164800/175341]\n",
      "loss: 0.685727  [166400/175341]\n",
      "loss: 0.633666  [168000/175341]\n",
      "loss: 0.770712  [169600/175341]\n",
      "loss: 0.468202  [171200/175341]\n",
      "loss: 0.372351  [172800/175341]\n",
      "loss: 0.541319  [174400/175341]\n",
      "Train Accuracy: 81.2434%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.566951, F1-score: 75.51%, Macro_F1-Score:  40.18%  \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.428835  [    0/175341]\n",
      "loss: 0.219572  [ 1600/175341]\n",
      "loss: 0.447390  [ 3200/175341]\n",
      "loss: 0.385986  [ 4800/175341]\n",
      "loss: 0.155347  [ 6400/175341]\n",
      "loss: 0.273522  [ 8000/175341]\n",
      "loss: 0.546603  [ 9600/175341]\n",
      "loss: 0.219460  [11200/175341]\n",
      "loss: 0.404472  [12800/175341]\n",
      "loss: 0.520779  [14400/175341]\n",
      "loss: 0.449890  [16000/175341]\n",
      "loss: 0.269321  [17600/175341]\n",
      "loss: 0.493510  [19200/175341]\n",
      "loss: 0.686072  [20800/175341]\n",
      "loss: 0.777933  [22400/175341]\n",
      "loss: 0.297508  [24000/175341]\n",
      "loss: 0.727765  [25600/175341]\n",
      "loss: 0.423841  [27200/175341]\n",
      "loss: 0.285219  [28800/175341]\n",
      "loss: 0.686619  [30400/175341]\n",
      "loss: 0.640338  [32000/175341]\n",
      "loss: 0.355754  [33600/175341]\n",
      "loss: 0.489940  [35200/175341]\n",
      "loss: 0.414870  [36800/175341]\n",
      "loss: 0.521889  [38400/175341]\n",
      "loss: 0.438462  [40000/175341]\n",
      "loss: 0.469894  [41600/175341]\n",
      "loss: 0.455068  [43200/175341]\n",
      "loss: 0.416718  [44800/175341]\n",
      "loss: 0.493347  [46400/175341]\n",
      "loss: 0.811939  [48000/175341]\n",
      "loss: 0.638814  [49600/175341]\n",
      "loss: 0.379083  [51200/175341]\n",
      "loss: 0.579619  [52800/175341]\n",
      "loss: 0.311847  [54400/175341]\n",
      "loss: 0.463805  [56000/175341]\n",
      "loss: 0.380533  [57600/175341]\n",
      "loss: 0.405445  [59200/175341]\n",
      "loss: 0.216734  [60800/175341]\n",
      "loss: 0.503350  [62400/175341]\n",
      "loss: 0.175076  [64000/175341]\n",
      "loss: 0.336794  [65600/175341]\n",
      "loss: 0.480137  [67200/175341]\n",
      "loss: 0.223146  [68800/175341]\n",
      "loss: 0.562132  [70400/175341]\n",
      "loss: 0.875432  [72000/175341]\n",
      "loss: 0.524481  [73600/175341]\n",
      "loss: 0.274550  [75200/175341]\n",
      "loss: 0.466955  [76800/175341]\n",
      "loss: 0.388180  [78400/175341]\n",
      "loss: 0.233915  [80000/175341]\n",
      "loss: 0.278443  [81600/175341]\n",
      "loss: 0.677583  [83200/175341]\n",
      "loss: 0.578612  [84800/175341]\n",
      "loss: 1.014689  [86400/175341]\n",
      "loss: 0.930370  [88000/175341]\n",
      "loss: 0.521642  [89600/175341]\n",
      "loss: 0.677970  [91200/175341]\n",
      "loss: 0.783325  [92800/175341]\n",
      "loss: 0.769635  [94400/175341]\n",
      "loss: 0.662877  [96000/175341]\n",
      "loss: 0.373131  [97600/175341]\n",
      "loss: 0.402560  [99200/175341]\n",
      "loss: 0.623467  [100800/175341]\n",
      "loss: 0.409374  [102400/175341]\n",
      "loss: 0.469374  [104000/175341]\n",
      "loss: 0.295849  [105600/175341]\n",
      "loss: 0.634458  [107200/175341]\n",
      "loss: 0.527436  [108800/175341]\n",
      "loss: 0.430973  [110400/175341]\n",
      "loss: 0.312873  [112000/175341]\n",
      "loss: 0.346698  [113600/175341]\n",
      "loss: 0.399209  [115200/175341]\n",
      "loss: 0.387703  [116800/175341]\n",
      "loss: 0.325281  [118400/175341]\n",
      "loss: 0.670662  [120000/175341]\n",
      "loss: 0.473628  [121600/175341]\n",
      "loss: 0.142987  [123200/175341]\n",
      "loss: 0.271192  [124800/175341]\n",
      "loss: 0.405333  [126400/175341]\n",
      "loss: 0.320305  [128000/175341]\n",
      "loss: 0.331228  [129600/175341]\n",
      "loss: 0.461745  [131200/175341]\n",
      "loss: 0.643479  [132800/175341]\n",
      "loss: 0.601565  [134400/175341]\n",
      "loss: 0.422765  [136000/175341]\n",
      "loss: 0.406664  [137600/175341]\n",
      "loss: 0.397817  [139200/175341]\n",
      "loss: 0.289150  [140800/175341]\n",
      "loss: 0.151016  [142400/175341]\n",
      "loss: 0.491626  [144000/175341]\n",
      "loss: 0.463919  [145600/175341]\n",
      "loss: 0.724337  [147200/175341]\n",
      "loss: 0.585500  [148800/175341]\n",
      "loss: 0.664860  [150400/175341]\n",
      "loss: 0.333743  [152000/175341]\n",
      "loss: 0.363437  [153600/175341]\n",
      "loss: 0.406918  [155200/175341]\n",
      "loss: 0.255055  [156800/175341]\n",
      "loss: 0.828993  [158400/175341]\n",
      "loss: 0.640213  [160000/175341]\n",
      "loss: 0.542415  [161600/175341]\n",
      "loss: 0.536266  [163200/175341]\n",
      "loss: 0.214891  [164800/175341]\n",
      "loss: 0.189591  [166400/175341]\n",
      "loss: 0.462197  [168000/175341]\n",
      "loss: 0.648726  [169600/175341]\n",
      "loss: 0.497195  [171200/175341]\n",
      "loss: 0.791437  [172800/175341]\n",
      "loss: 0.555279  [174400/175341]\n",
      "Train Accuracy: 81.2223%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.574690, F1-score: 74.87%, Macro_F1-Score:  39.82%  \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.576273  [    0/175341]\n",
      "loss: 0.433311  [ 1600/175341]\n",
      "loss: 0.886361  [ 3200/175341]\n",
      "loss: 0.406661  [ 4800/175341]\n",
      "loss: 0.373998  [ 6400/175341]\n",
      "loss: 0.142400  [ 8000/175341]\n",
      "loss: 0.434763  [ 9600/175341]\n",
      "loss: 0.442132  [11200/175341]\n",
      "loss: 0.489652  [12800/175341]\n",
      "loss: 0.119587  [14400/175341]\n",
      "loss: 0.527594  [16000/175341]\n",
      "loss: 0.522823  [17600/175341]\n",
      "loss: 0.209327  [19200/175341]\n",
      "loss: 0.397509  [20800/175341]\n",
      "loss: 0.625598  [22400/175341]\n",
      "loss: 0.630690  [24000/175341]\n",
      "loss: 0.323554  [25600/175341]\n",
      "loss: 0.577574  [27200/175341]\n",
      "loss: 0.893146  [28800/175341]\n",
      "loss: 0.851498  [30400/175341]\n",
      "loss: 0.347372  [32000/175341]\n",
      "loss: 0.283134  [33600/175341]\n",
      "loss: 0.149413  [35200/175341]\n",
      "loss: 0.207450  [36800/175341]\n",
      "loss: 0.849156  [38400/175341]\n",
      "loss: 0.381293  [40000/175341]\n",
      "loss: 0.387397  [41600/175341]\n",
      "loss: 0.226689  [43200/175341]\n",
      "loss: 0.244243  [44800/175341]\n",
      "loss: 0.353810  [46400/175341]\n",
      "loss: 0.524978  [48000/175341]\n",
      "loss: 0.555857  [49600/175341]\n",
      "loss: 0.689478  [51200/175341]\n",
      "loss: 1.071186  [52800/175341]\n",
      "loss: 0.345760  [54400/175341]\n",
      "loss: 0.326600  [56000/175341]\n",
      "loss: 0.206874  [57600/175341]\n",
      "loss: 0.735797  [59200/175341]\n",
      "loss: 0.210604  [60800/175341]\n",
      "loss: 0.653987  [62400/175341]\n",
      "loss: 0.355811  [64000/175341]\n",
      "loss: 0.312823  [65600/175341]\n",
      "loss: 0.740741  [67200/175341]\n",
      "loss: 0.260465  [68800/175341]\n",
      "loss: 0.544639  [70400/175341]\n",
      "loss: 0.602393  [72000/175341]\n",
      "loss: 0.288037  [73600/175341]\n",
      "loss: 0.760485  [75200/175341]\n",
      "loss: 0.671113  [76800/175341]\n",
      "loss: 0.737371  [78400/175341]\n",
      "loss: 0.570061  [80000/175341]\n",
      "loss: 0.562222  [81600/175341]\n",
      "loss: 0.579805  [83200/175341]\n",
      "loss: 0.491365  [84800/175341]\n",
      "loss: 0.262343  [86400/175341]\n",
      "loss: 0.415990  [88000/175341]\n",
      "loss: 0.412671  [89600/175341]\n",
      "loss: 1.006428  [91200/175341]\n",
      "loss: 0.491885  [92800/175341]\n",
      "loss: 0.210904  [94400/175341]\n",
      "loss: 0.313946  [96000/175341]\n",
      "loss: 0.804467  [97600/175341]\n",
      "loss: 0.293823  [99200/175341]\n",
      "loss: 0.645675  [100800/175341]\n",
      "loss: 0.400431  [102400/175341]\n",
      "loss: 0.223210  [104000/175341]\n",
      "loss: 0.252440  [105600/175341]\n",
      "loss: 0.281114  [107200/175341]\n",
      "loss: 0.346699  [108800/175341]\n",
      "loss: 0.241981  [110400/175341]\n",
      "loss: 0.495823  [112000/175341]\n",
      "loss: 0.683806  [113600/175341]\n",
      "loss: 0.332057  [115200/175341]\n",
      "loss: 0.238832  [116800/175341]\n",
      "loss: 0.348682  [118400/175341]\n",
      "loss: 0.402955  [120000/175341]\n",
      "loss: 0.381409  [121600/175341]\n",
      "loss: 0.381666  [123200/175341]\n",
      "loss: 0.299954  [124800/175341]\n",
      "loss: 0.606527  [126400/175341]\n",
      "loss: 0.128640  [128000/175341]\n",
      "loss: 0.632918  [129600/175341]\n",
      "loss: 0.502346  [131200/175341]\n",
      "loss: 0.621488  [132800/175341]\n",
      "loss: 0.415811  [134400/175341]\n",
      "loss: 0.738675  [136000/175341]\n",
      "loss: 0.266859  [137600/175341]\n",
      "loss: 0.474426  [139200/175341]\n",
      "loss: 0.337148  [140800/175341]\n",
      "loss: 0.770921  [142400/175341]\n",
      "loss: 0.937130  [144000/175341]\n",
      "loss: 0.245629  [145600/175341]\n",
      "loss: 0.788168  [147200/175341]\n",
      "loss: 0.686204  [148800/175341]\n",
      "loss: 0.860786  [150400/175341]\n",
      "loss: 0.583698  [152000/175341]\n",
      "loss: 0.868168  [153600/175341]\n",
      "loss: 0.393389  [155200/175341]\n",
      "loss: 0.498683  [156800/175341]\n",
      "loss: 0.215644  [158400/175341]\n",
      "loss: 1.258866  [160000/175341]\n",
      "loss: 0.247364  [161600/175341]\n",
      "loss: 0.465331  [163200/175341]\n",
      "loss: 0.252555  [164800/175341]\n",
      "loss: 0.664606  [166400/175341]\n",
      "loss: 0.430302  [168000/175341]\n",
      "loss: 0.530064  [169600/175341]\n",
      "loss: 0.271598  [171200/175341]\n",
      "loss: 0.691094  [172800/175341]\n",
      "loss: 0.407170  [174400/175341]\n",
      "Train Accuracy: 81.2594%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.586098, F1-score: 74.36%, Macro_F1-Score:  39.78%  \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.323543  [    0/175341]\n",
      "loss: 0.078889  [ 1600/175341]\n",
      "loss: 0.237565  [ 3200/175341]\n",
      "loss: 0.054277  [ 4800/175341]\n",
      "loss: 0.292286  [ 6400/175341]\n",
      "loss: 0.149403  [ 8000/175341]\n",
      "loss: 0.426460  [ 9600/175341]\n",
      "loss: 0.345472  [11200/175341]\n",
      "loss: 0.456950  [12800/175341]\n",
      "loss: 0.214409  [14400/175341]\n",
      "loss: 0.317180  [16000/175341]\n",
      "loss: 0.593153  [17600/175341]\n",
      "loss: 0.747042  [19200/175341]\n",
      "loss: 0.511106  [20800/175341]\n",
      "loss: 0.218574  [22400/175341]\n",
      "loss: 0.460213  [24000/175341]\n",
      "loss: 0.329569  [25600/175341]\n",
      "loss: 0.286150  [27200/175341]\n",
      "loss: 0.543470  [28800/175341]\n",
      "loss: 0.501188  [30400/175341]\n",
      "loss: 0.568387  [32000/175341]\n",
      "loss: 0.477398  [33600/175341]\n",
      "loss: 0.268436  [35200/175341]\n",
      "loss: 0.230570  [36800/175341]\n",
      "loss: 0.482676  [38400/175341]\n",
      "loss: 0.526276  [40000/175341]\n",
      "loss: 0.647665  [41600/175341]\n",
      "loss: 0.555587  [43200/175341]\n",
      "loss: 0.701041  [44800/175341]\n",
      "loss: 0.208251  [46400/175341]\n",
      "loss: 0.243832  [48000/175341]\n",
      "loss: 0.116658  [49600/175341]\n",
      "loss: 0.459668  [51200/175341]\n",
      "loss: 0.205198  [52800/175341]\n",
      "loss: 0.495726  [54400/175341]\n",
      "loss: 0.342611  [56000/175341]\n",
      "loss: 0.603336  [57600/175341]\n",
      "loss: 0.337569  [59200/175341]\n",
      "loss: 0.498337  [60800/175341]\n",
      "loss: 0.505501  [62400/175341]\n",
      "loss: 0.173888  [64000/175341]\n",
      "loss: 0.591331  [65600/175341]\n",
      "loss: 0.409941  [67200/175341]\n",
      "loss: 0.479614  [68800/175341]\n",
      "loss: 0.314612  [70400/175341]\n",
      "loss: 1.020918  [72000/175341]\n",
      "loss: 0.432613  [73600/175341]\n",
      "loss: 0.758028  [75200/175341]\n",
      "loss: 0.458959  [76800/175341]\n",
      "loss: 0.346355  [78400/175341]\n",
      "loss: 0.209180  [80000/175341]\n",
      "loss: 0.252433  [81600/175341]\n",
      "loss: 0.579035  [83200/175341]\n",
      "loss: 0.253375  [84800/175341]\n",
      "loss: 0.310073  [86400/175341]\n",
      "loss: 1.469226  [88000/175341]\n",
      "loss: 0.484088  [89600/175341]\n",
      "loss: 0.402137  [91200/175341]\n",
      "loss: 0.360398  [92800/175341]\n",
      "loss: 0.434552  [94400/175341]\n",
      "loss: 0.461517  [96000/175341]\n",
      "loss: 0.636815  [97600/175341]\n",
      "loss: 0.299493  [99200/175341]\n",
      "loss: 0.586064  [100800/175341]\n",
      "loss: 0.088492  [102400/175341]\n",
      "loss: 0.342419  [104000/175341]\n",
      "loss: 0.566166  [105600/175341]\n",
      "loss: 0.854475  [107200/175341]\n",
      "loss: 0.719393  [108800/175341]\n",
      "loss: 0.619654  [110400/175341]\n",
      "loss: 0.243551  [112000/175341]\n",
      "loss: 0.465917  [113600/175341]\n",
      "loss: 0.477744  [115200/175341]\n",
      "loss: 0.778011  [116800/175341]\n",
      "loss: 0.855606  [118400/175341]\n",
      "loss: 0.390299  [120000/175341]\n",
      "loss: 0.630505  [121600/175341]\n",
      "loss: 0.212945  [123200/175341]\n",
      "loss: 0.444673  [124800/175341]\n",
      "loss: 0.554690  [126400/175341]\n",
      "loss: 0.658171  [128000/175341]\n",
      "loss: 0.310278  [129600/175341]\n",
      "loss: 0.814042  [131200/175341]\n",
      "loss: 0.492431  [132800/175341]\n",
      "loss: 0.623734  [134400/175341]\n",
      "loss: 0.507801  [136000/175341]\n",
      "loss: 0.510088  [137600/175341]\n",
      "loss: 0.820074  [139200/175341]\n",
      "loss: 0.486422  [140800/175341]\n",
      "loss: 0.350641  [142400/175341]\n",
      "loss: 0.535962  [144000/175341]\n",
      "loss: 0.592987  [145600/175341]\n",
      "loss: 0.314182  [147200/175341]\n",
      "loss: 0.370069  [148800/175341]\n",
      "loss: 0.883167  [150400/175341]\n",
      "loss: 0.756349  [152000/175341]\n",
      "loss: 0.391199  [153600/175341]\n",
      "loss: 0.771173  [155200/175341]\n",
      "loss: 0.721624  [156800/175341]\n",
      "loss: 0.282622  [158400/175341]\n",
      "loss: 0.163657  [160000/175341]\n",
      "loss: 0.317134  [161600/175341]\n",
      "loss: 0.645848  [163200/175341]\n",
      "loss: 0.475735  [164800/175341]\n",
      "loss: 0.707936  [166400/175341]\n",
      "loss: 0.502728  [168000/175341]\n",
      "loss: 0.579498  [169600/175341]\n",
      "loss: 0.214518  [171200/175341]\n",
      "loss: 0.406236  [172800/175341]\n",
      "loss: 0.437054  [174400/175341]\n",
      "Train Accuracy: 81.2987%\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.582373, F1-score: 73.98%, Macro_F1-Score:  39.90%  \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.107050  [    0/175341]\n",
      "loss: 0.334478  [ 1600/175341]\n",
      "loss: 0.304787  [ 3200/175341]\n",
      "loss: 0.609735  [ 4800/175341]\n",
      "loss: 0.369306  [ 6400/175341]\n",
      "loss: 0.360168  [ 8000/175341]\n",
      "loss: 0.084165  [ 9600/175341]\n",
      "loss: 0.347461  [11200/175341]\n",
      "loss: 0.379677  [12800/175341]\n",
      "loss: 0.787666  [14400/175341]\n",
      "loss: 0.359453  [16000/175341]\n",
      "loss: 0.322061  [17600/175341]\n",
      "loss: 0.650752  [19200/175341]\n",
      "loss: 0.803180  [20800/175341]\n",
      "loss: 0.243332  [22400/175341]\n",
      "loss: 0.436648  [24000/175341]\n",
      "loss: 0.626170  [25600/175341]\n",
      "loss: 0.542385  [27200/175341]\n",
      "loss: 0.474920  [28800/175341]\n",
      "loss: 0.800106  [30400/175341]\n",
      "loss: 0.171256  [32000/175341]\n",
      "loss: 0.328932  [33600/175341]\n",
      "loss: 0.404261  [35200/175341]\n",
      "loss: 0.418056  [36800/175341]\n",
      "loss: 0.409093  [38400/175341]\n",
      "loss: 0.479704  [40000/175341]\n",
      "loss: 0.180139  [41600/175341]\n",
      "loss: 0.389706  [43200/175341]\n",
      "loss: 0.770077  [44800/175341]\n",
      "loss: 0.285461  [46400/175341]\n",
      "loss: 0.275631  [48000/175341]\n",
      "loss: 0.404843  [49600/175341]\n",
      "loss: 0.296989  [51200/175341]\n",
      "loss: 0.318748  [52800/175341]\n",
      "loss: 0.417406  [54400/175341]\n",
      "loss: 0.351301  [56000/175341]\n",
      "loss: 0.399852  [57600/175341]\n",
      "loss: 0.274188  [59200/175341]\n",
      "loss: 0.276862  [60800/175341]\n",
      "loss: 0.635174  [62400/175341]\n",
      "loss: 0.374780  [64000/175341]\n",
      "loss: 0.456998  [65600/175341]\n",
      "loss: 0.338688  [67200/175341]\n",
      "loss: 0.327759  [68800/175341]\n",
      "loss: 1.028374  [70400/175341]\n",
      "loss: 0.119457  [72000/175341]\n",
      "loss: 0.395868  [73600/175341]\n",
      "loss: 0.337194  [75200/175341]\n",
      "loss: 0.427906  [76800/175341]\n",
      "loss: 0.544941  [78400/175341]\n",
      "loss: 0.470553  [80000/175341]\n",
      "loss: 0.470070  [81600/175341]\n",
      "loss: 0.348088  [83200/175341]\n",
      "loss: 0.874299  [84800/175341]\n",
      "loss: 0.265219  [86400/175341]\n",
      "loss: 0.428965  [88000/175341]\n",
      "loss: 0.338201  [89600/175341]\n",
      "loss: 0.858244  [91200/175341]\n",
      "loss: 0.844715  [92800/175341]\n",
      "loss: 0.186958  [94400/175341]\n",
      "loss: 0.191465  [96000/175341]\n",
      "loss: 0.301584  [97600/175341]\n",
      "loss: 0.430844  [99200/175341]\n",
      "loss: 0.202639  [100800/175341]\n",
      "loss: 0.428786  [102400/175341]\n",
      "loss: 0.672685  [104000/175341]\n",
      "loss: 0.816752  [105600/175341]\n",
      "loss: 0.722229  [107200/175341]\n",
      "loss: 0.356467  [108800/175341]\n",
      "loss: 0.680924  [110400/175341]\n",
      "loss: 1.176973  [112000/175341]\n",
      "loss: 0.493063  [113600/175341]\n",
      "loss: 0.330613  [115200/175341]\n",
      "loss: 0.881694  [116800/175341]\n",
      "loss: 0.544177  [118400/175341]\n",
      "loss: 0.701367  [120000/175341]\n",
      "loss: 0.023166  [121600/175341]\n",
      "loss: 0.247233  [123200/175341]\n",
      "loss: 0.366224  [124800/175341]\n",
      "loss: 0.711228  [126400/175341]\n",
      "loss: 0.516432  [128000/175341]\n",
      "loss: 0.764810  [129600/175341]\n",
      "loss: 0.538511  [131200/175341]\n",
      "loss: 0.427976  [132800/175341]\n",
      "loss: 0.515638  [134400/175341]\n",
      "loss: 0.482075  [136000/175341]\n",
      "loss: 0.409175  [137600/175341]\n",
      "loss: 0.546070  [139200/175341]\n",
      "loss: 0.578888  [140800/175341]\n",
      "loss: 0.641029  [142400/175341]\n",
      "loss: 0.424057  [144000/175341]\n",
      "loss: 0.485850  [145600/175341]\n",
      "loss: 0.468915  [147200/175341]\n",
      "loss: 0.494183  [148800/175341]\n",
      "loss: 0.567674  [150400/175341]\n",
      "loss: 0.438387  [152000/175341]\n",
      "loss: 0.735583  [153600/175341]\n",
      "loss: 0.177675  [155200/175341]\n",
      "loss: 0.732430  [156800/175341]\n",
      "loss: 0.392782  [158400/175341]\n",
      "loss: 0.448916  [160000/175341]\n",
      "loss: 0.195678  [161600/175341]\n",
      "loss: 0.921538  [163200/175341]\n",
      "loss: 0.237209  [164800/175341]\n",
      "loss: 0.434481  [166400/175341]\n",
      "loss: 0.537002  [168000/175341]\n",
      "loss: 0.117739  [169600/175341]\n",
      "loss: 0.455349  [171200/175341]\n",
      "loss: 0.340701  [172800/175341]\n",
      "loss: 0.659118  [174400/175341]\n",
      "Train Accuracy: 81.3193%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.559255, F1-score: 75.79%, Macro_F1-Score:  40.32%  \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.337649  [    0/175341]\n",
      "loss: 0.172952  [ 1600/175341]\n",
      "loss: 0.665866  [ 3200/175341]\n",
      "loss: 0.199508  [ 4800/175341]\n",
      "loss: 0.771644  [ 6400/175341]\n",
      "loss: 0.179797  [ 8000/175341]\n",
      "loss: 0.631816  [ 9600/175341]\n",
      "loss: 0.431340  [11200/175341]\n",
      "loss: 0.287157  [12800/175341]\n",
      "loss: 0.315715  [14400/175341]\n",
      "loss: 0.399608  [16000/175341]\n",
      "loss: 0.861733  [17600/175341]\n",
      "loss: 0.776018  [19200/175341]\n",
      "loss: 0.397932  [20800/175341]\n",
      "loss: 0.297197  [22400/175341]\n",
      "loss: 0.877824  [24000/175341]\n",
      "loss: 0.217164  [25600/175341]\n",
      "loss: 0.255441  [27200/175341]\n",
      "loss: 0.369387  [28800/175341]\n",
      "loss: 0.657107  [30400/175341]\n",
      "loss: 0.366803  [32000/175341]\n",
      "loss: 0.403217  [33600/175341]\n",
      "loss: 0.584411  [35200/175341]\n",
      "loss: 0.594752  [36800/175341]\n",
      "loss: 0.921701  [38400/175341]\n",
      "loss: 0.272775  [40000/175341]\n",
      "loss: 0.525865  [41600/175341]\n",
      "loss: 0.233912  [43200/175341]\n",
      "loss: 0.494588  [44800/175341]\n",
      "loss: 0.364634  [46400/175341]\n",
      "loss: 0.670292  [48000/175341]\n",
      "loss: 0.481814  [49600/175341]\n",
      "loss: 0.679698  [51200/175341]\n",
      "loss: 0.580593  [52800/175341]\n",
      "loss: 0.673805  [54400/175341]\n",
      "loss: 0.316314  [56000/175341]\n",
      "loss: 0.263102  [57600/175341]\n",
      "loss: 0.138985  [59200/175341]\n",
      "loss: 0.664125  [60800/175341]\n",
      "loss: 0.555729  [62400/175341]\n",
      "loss: 0.502329  [64000/175341]\n",
      "loss: 0.406849  [65600/175341]\n",
      "loss: 0.363876  [67200/175341]\n",
      "loss: 0.329989  [68800/175341]\n",
      "loss: 0.439692  [70400/175341]\n",
      "loss: 0.495145  [72000/175341]\n",
      "loss: 0.500769  [73600/175341]\n",
      "loss: 0.829967  [75200/175341]\n",
      "loss: 0.365853  [76800/175341]\n",
      "loss: 0.429689  [78400/175341]\n",
      "loss: 0.444268  [80000/175341]\n",
      "loss: 0.602094  [81600/175341]\n",
      "loss: 0.542569  [83200/175341]\n",
      "loss: 0.774664  [84800/175341]\n",
      "loss: 0.200239  [86400/175341]\n",
      "loss: 0.287880  [88000/175341]\n",
      "loss: 0.275486  [89600/175341]\n",
      "loss: 0.374991  [91200/175341]\n",
      "loss: 0.313398  [92800/175341]\n",
      "loss: 0.269056  [94400/175341]\n",
      "loss: 0.384761  [96000/175341]\n",
      "loss: 0.288635  [97600/175341]\n",
      "loss: 0.468074  [99200/175341]\n",
      "loss: 0.656090  [100800/175341]\n",
      "loss: 0.290950  [102400/175341]\n",
      "loss: 0.345539  [104000/175341]\n",
      "loss: 0.122450  [105600/175341]\n",
      "loss: 0.460981  [107200/175341]\n",
      "loss: 0.610084  [108800/175341]\n",
      "loss: 0.461405  [110400/175341]\n",
      "loss: 0.324420  [112000/175341]\n",
      "loss: 0.142200  [113600/175341]\n",
      "loss: 0.501324  [115200/175341]\n",
      "loss: 0.690319  [116800/175341]\n",
      "loss: 0.274259  [118400/175341]\n",
      "loss: 0.771257  [120000/175341]\n",
      "loss: 0.328079  [121600/175341]\n",
      "loss: 0.367545  [123200/175341]\n",
      "loss: 0.460476  [124800/175341]\n",
      "loss: 0.418693  [126400/175341]\n",
      "loss: 0.454882  [128000/175341]\n",
      "loss: 0.817680  [129600/175341]\n",
      "loss: 0.489427  [131200/175341]\n",
      "loss: 0.271676  [132800/175341]\n",
      "loss: 0.757442  [134400/175341]\n",
      "loss: 0.506647  [136000/175341]\n",
      "loss: 0.348135  [137600/175341]\n",
      "loss: 0.526984  [139200/175341]\n",
      "loss: 0.393195  [140800/175341]\n",
      "loss: 0.412059  [142400/175341]\n",
      "loss: 0.156787  [144000/175341]\n",
      "loss: 0.423443  [145600/175341]\n",
      "loss: 0.583024  [147200/175341]\n",
      "loss: 0.330757  [148800/175341]\n",
      "loss: 0.651670  [150400/175341]\n",
      "loss: 0.807039  [152000/175341]\n",
      "loss: 0.346871  [153600/175341]\n",
      "loss: 0.335454  [155200/175341]\n",
      "loss: 0.483257  [156800/175341]\n",
      "loss: 0.695035  [158400/175341]\n",
      "loss: 0.613800  [160000/175341]\n",
      "loss: 0.342531  [161600/175341]\n",
      "loss: 0.420210  [163200/175341]\n",
      "loss: 0.896361  [164800/175341]\n",
      "loss: 0.686574  [166400/175341]\n",
      "loss: 0.375440  [168000/175341]\n",
      "loss: 0.348590  [169600/175341]\n",
      "loss: 0.285761  [171200/175341]\n",
      "loss: 0.550877  [172800/175341]\n",
      "loss: 0.423070  [174400/175341]\n",
      "Train Accuracy: 81.3284%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.575456, F1-score: 74.94%, Macro_F1-Score:  40.24%  \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.016415  [    0/175341]\n",
      "loss: 0.396900  [ 1600/175341]\n",
      "loss: 0.251995  [ 3200/175341]\n",
      "loss: 0.212912  [ 4800/175341]\n",
      "loss: 0.549421  [ 6400/175341]\n",
      "loss: 0.663012  [ 8000/175341]\n",
      "loss: 0.343078  [ 9600/175341]\n",
      "loss: 0.152874  [11200/175341]\n",
      "loss: 0.262192  [12800/175341]\n",
      "loss: 0.325866  [14400/175341]\n",
      "loss: 0.131777  [16000/175341]\n",
      "loss: 0.790588  [17600/175341]\n",
      "loss: 0.530235  [19200/175341]\n",
      "loss: 0.498837  [20800/175341]\n",
      "loss: 0.485152  [22400/175341]\n",
      "loss: 0.365315  [24000/175341]\n",
      "loss: 0.557366  [25600/175341]\n",
      "loss: 0.392624  [27200/175341]\n",
      "loss: 0.403943  [28800/175341]\n",
      "loss: 0.592378  [30400/175341]\n",
      "loss: 0.141354  [32000/175341]\n",
      "loss: 0.818010  [33600/175341]\n",
      "loss: 0.176645  [35200/175341]\n",
      "loss: 0.238453  [36800/175341]\n",
      "loss: 0.298594  [38400/175341]\n",
      "loss: 0.282109  [40000/175341]\n",
      "loss: 0.299027  [41600/175341]\n",
      "loss: 0.621212  [43200/175341]\n",
      "loss: 0.276761  [44800/175341]\n",
      "loss: 0.620305  [46400/175341]\n",
      "loss: 0.229234  [48000/175341]\n",
      "loss: 0.636705  [49600/175341]\n",
      "loss: 0.511870  [51200/175341]\n",
      "loss: 0.271985  [52800/175341]\n",
      "loss: 0.382216  [54400/175341]\n",
      "loss: 0.313290  [56000/175341]\n",
      "loss: 0.383062  [57600/175341]\n",
      "loss: 0.657076  [59200/175341]\n",
      "loss: 0.295412  [60800/175341]\n",
      "loss: 0.413778  [62400/175341]\n",
      "loss: 0.612106  [64000/175341]\n",
      "loss: 0.325690  [65600/175341]\n",
      "loss: 0.222113  [67200/175341]\n",
      "loss: 0.269888  [68800/175341]\n",
      "loss: 0.315794  [70400/175341]\n",
      "loss: 0.318062  [72000/175341]\n",
      "loss: 0.599931  [73600/175341]\n",
      "loss: 0.665127  [75200/175341]\n",
      "loss: 0.096901  [76800/175341]\n",
      "loss: 0.704460  [78400/175341]\n",
      "loss: 0.361022  [80000/175341]\n",
      "loss: 0.310595  [81600/175341]\n",
      "loss: 0.225868  [83200/175341]\n",
      "loss: 0.646317  [84800/175341]\n",
      "loss: 0.850514  [86400/175341]\n",
      "loss: 0.150285  [88000/175341]\n",
      "loss: 0.306854  [89600/175341]\n",
      "loss: 0.303780  [91200/175341]\n",
      "loss: 0.431204  [92800/175341]\n",
      "loss: 0.299503  [94400/175341]\n",
      "loss: 0.429138  [96000/175341]\n",
      "loss: 0.416090  [97600/175341]\n",
      "loss: 0.272000  [99200/175341]\n",
      "loss: 0.698246  [100800/175341]\n",
      "loss: 0.541670  [102400/175341]\n",
      "loss: 0.305218  [104000/175341]\n",
      "loss: 0.304535  [105600/175341]\n",
      "loss: 0.148663  [107200/175341]\n",
      "loss: 0.244970  [108800/175341]\n",
      "loss: 0.427553  [110400/175341]\n",
      "loss: 0.218694  [112000/175341]\n",
      "loss: 0.563695  [113600/175341]\n",
      "loss: 0.790495  [115200/175341]\n",
      "loss: 0.405201  [116800/175341]\n",
      "loss: 0.978673  [118400/175341]\n",
      "loss: 0.452650  [120000/175341]\n",
      "loss: 0.335430  [121600/175341]\n",
      "loss: 0.556591  [123200/175341]\n",
      "loss: 0.068281  [124800/175341]\n",
      "loss: 0.454248  [126400/175341]\n",
      "loss: 0.903191  [128000/175341]\n",
      "loss: 0.518198  [129600/175341]\n",
      "loss: 0.119254  [131200/175341]\n",
      "loss: 0.287399  [132800/175341]\n",
      "loss: 0.727646  [134400/175341]\n",
      "loss: 0.740658  [136000/175341]\n",
      "loss: 0.353515  [137600/175341]\n",
      "loss: 0.123513  [139200/175341]\n",
      "loss: 0.881469  [140800/175341]\n",
      "loss: 0.194012  [142400/175341]\n",
      "loss: 0.292779  [144000/175341]\n",
      "loss: 0.633347  [145600/175341]\n",
      "loss: 0.187584  [147200/175341]\n",
      "loss: 0.414066  [148800/175341]\n",
      "loss: 0.505632  [150400/175341]\n",
      "loss: 0.321862  [152000/175341]\n",
      "loss: 0.650961  [153600/175341]\n",
      "loss: 0.467772  [155200/175341]\n",
      "loss: 0.342410  [156800/175341]\n",
      "loss: 0.350023  [158400/175341]\n",
      "loss: 0.082954  [160000/175341]\n",
      "loss: 0.275992  [161600/175341]\n",
      "loss: 0.618742  [163200/175341]\n",
      "loss: 0.804714  [164800/175341]\n",
      "loss: 0.340714  [166400/175341]\n",
      "loss: 0.439114  [168000/175341]\n",
      "loss: 0.376744  [169600/175341]\n",
      "loss: 0.232589  [171200/175341]\n",
      "loss: 0.487577  [172800/175341]\n",
      "loss: 0.664595  [174400/175341]\n",
      "Train Accuracy: 81.3888%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.566431, F1-score: 75.36%, Macro_F1-Score:  40.58%  \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.334263  [    0/175341]\n",
      "loss: 0.492942  [ 1600/175341]\n",
      "loss: 0.461500  [ 3200/175341]\n",
      "loss: 0.183397  [ 4800/175341]\n",
      "loss: 0.287656  [ 6400/175341]\n",
      "loss: 0.609424  [ 8000/175341]\n",
      "loss: 0.082538  [ 9600/175341]\n",
      "loss: 0.859010  [11200/175341]\n",
      "loss: 0.429894  [12800/175341]\n",
      "loss: 0.316842  [14400/175341]\n",
      "loss: 0.456532  [16000/175341]\n",
      "loss: 0.278801  [17600/175341]\n",
      "loss: 0.435240  [19200/175341]\n",
      "loss: 0.402841  [20800/175341]\n",
      "loss: 0.247743  [22400/175341]\n",
      "loss: 0.359817  [24000/175341]\n",
      "loss: 0.811987  [25600/175341]\n",
      "loss: 0.308573  [27200/175341]\n",
      "loss: 0.344914  [28800/175341]\n",
      "loss: 0.385583  [30400/175341]\n",
      "loss: 0.421166  [32000/175341]\n",
      "loss: 0.479848  [33600/175341]\n",
      "loss: 0.740175  [35200/175341]\n",
      "loss: 0.521243  [36800/175341]\n",
      "loss: 0.277916  [38400/175341]\n",
      "loss: 0.338328  [40000/175341]\n",
      "loss: 0.537005  [41600/175341]\n",
      "loss: 0.167243  [43200/175341]\n",
      "loss: 0.250966  [44800/175341]\n",
      "loss: 0.437497  [46400/175341]\n",
      "loss: 0.756060  [48000/175341]\n",
      "loss: 0.292303  [49600/175341]\n",
      "loss: 0.575005  [51200/175341]\n",
      "loss: 0.620083  [52800/175341]\n",
      "loss: 0.366040  [54400/175341]\n",
      "loss: 0.133599  [56000/175341]\n",
      "loss: 0.768461  [57600/175341]\n",
      "loss: 0.397608  [59200/175341]\n",
      "loss: 0.340879  [60800/175341]\n",
      "loss: 0.466720  [62400/175341]\n",
      "loss: 0.476198  [64000/175341]\n",
      "loss: 0.552190  [65600/175341]\n",
      "loss: 0.441920  [67200/175341]\n",
      "loss: 0.228028  [68800/175341]\n",
      "loss: 0.784592  [70400/175341]\n",
      "loss: 0.536134  [72000/175341]\n",
      "loss: 0.394629  [73600/175341]\n",
      "loss: 0.689020  [75200/175341]\n",
      "loss: 0.464492  [76800/175341]\n",
      "loss: 0.314979  [78400/175341]\n",
      "loss: 0.436342  [80000/175341]\n",
      "loss: 0.174660  [81600/175341]\n",
      "loss: 0.437831  [83200/175341]\n",
      "loss: 0.526377  [84800/175341]\n",
      "loss: 0.669440  [86400/175341]\n",
      "loss: 0.692089  [88000/175341]\n",
      "loss: 0.517220  [89600/175341]\n",
      "loss: 0.447522  [91200/175341]\n",
      "loss: 0.517562  [92800/175341]\n",
      "loss: 0.524162  [94400/175341]\n",
      "loss: 0.490302  [96000/175341]\n",
      "loss: 0.470418  [97600/175341]\n",
      "loss: 0.836385  [99200/175341]\n",
      "loss: 0.371436  [100800/175341]\n",
      "loss: 0.305967  [102400/175341]\n",
      "loss: 0.601691  [104000/175341]\n",
      "loss: 0.635592  [105600/175341]\n",
      "loss: 0.750296  [107200/175341]\n",
      "loss: 0.327144  [108800/175341]\n",
      "loss: 0.427597  [110400/175341]\n",
      "loss: 0.649739  [112000/175341]\n",
      "loss: 0.324012  [113600/175341]\n",
      "loss: 0.562327  [115200/175341]\n",
      "loss: 0.748563  [116800/175341]\n",
      "loss: 0.377373  [118400/175341]\n",
      "loss: 0.215392  [120000/175341]\n",
      "loss: 0.687496  [121600/175341]\n",
      "loss: 0.432968  [123200/175341]\n",
      "loss: 0.570043  [124800/175341]\n",
      "loss: 0.365493  [126400/175341]\n",
      "loss: 0.659608  [128000/175341]\n",
      "loss: 0.546095  [129600/175341]\n",
      "loss: 0.394265  [131200/175341]\n",
      "loss: 0.326844  [132800/175341]\n",
      "loss: 0.373987  [134400/175341]\n",
      "loss: 0.740025  [136000/175341]\n",
      "loss: 0.597005  [137600/175341]\n",
      "loss: 0.688418  [139200/175341]\n",
      "loss: 0.439752  [140800/175341]\n",
      "loss: 0.575206  [142400/175341]\n",
      "loss: 0.165438  [144000/175341]\n",
      "loss: 0.483050  [145600/175341]\n",
      "loss: 0.265302  [147200/175341]\n",
      "loss: 0.440022  [148800/175341]\n",
      "loss: 0.436767  [150400/175341]\n",
      "loss: 0.165403  [152000/175341]\n",
      "loss: 0.502992  [153600/175341]\n",
      "loss: 0.504709  [155200/175341]\n",
      "loss: 0.390043  [156800/175341]\n",
      "loss: 0.460183  [158400/175341]\n",
      "loss: 0.488101  [160000/175341]\n",
      "loss: 0.428686  [161600/175341]\n",
      "loss: 0.171766  [163200/175341]\n",
      "loss: 0.485711  [164800/175341]\n",
      "loss: 0.274252  [166400/175341]\n",
      "loss: 0.420014  [168000/175341]\n",
      "loss: 0.366548  [169600/175341]\n",
      "loss: 0.554115  [171200/175341]\n",
      "loss: 0.155591  [172800/175341]\n",
      "loss: 0.590103  [174400/175341]\n",
      "Train Accuracy: 81.3672%\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.548561, F1-score: 77.40%, Macro_F1-Score:  40.41%  \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.712425  [    0/175341]\n",
      "loss: 0.304012  [ 1600/175341]\n",
      "loss: 0.765564  [ 3200/175341]\n",
      "loss: 0.380645  [ 4800/175341]\n",
      "loss: 0.094971  [ 6400/175341]\n",
      "loss: 0.567966  [ 8000/175341]\n",
      "loss: 0.588123  [ 9600/175341]\n",
      "loss: 0.640595  [11200/175341]\n",
      "loss: 0.176514  [12800/175341]\n",
      "loss: 0.361843  [14400/175341]\n",
      "loss: 0.842146  [16000/175341]\n",
      "loss: 0.404445  [17600/175341]\n",
      "loss: 0.527361  [19200/175341]\n",
      "loss: 0.742445  [20800/175341]\n",
      "loss: 0.673798  [22400/175341]\n",
      "loss: 0.225683  [24000/175341]\n",
      "loss: 0.357085  [25600/175341]\n",
      "loss: 0.264733  [27200/175341]\n",
      "loss: 0.874340  [28800/175341]\n",
      "loss: 0.507903  [30400/175341]\n",
      "loss: 0.517380  [32000/175341]\n",
      "loss: 0.427956  [33600/175341]\n",
      "loss: 0.512827  [35200/175341]\n",
      "loss: 0.920947  [36800/175341]\n",
      "loss: 0.235752  [38400/175341]\n",
      "loss: 0.292973  [40000/175341]\n",
      "loss: 0.140370  [41600/175341]\n",
      "loss: 0.862993  [43200/175341]\n",
      "loss: 0.145414  [44800/175341]\n",
      "loss: 0.180691  [46400/175341]\n",
      "loss: 0.434996  [48000/175341]\n",
      "loss: 0.386816  [49600/175341]\n",
      "loss: 0.505873  [51200/175341]\n",
      "loss: 0.636435  [52800/175341]\n",
      "loss: 0.665809  [54400/175341]\n",
      "loss: 0.905222  [56000/175341]\n",
      "loss: 0.461946  [57600/175341]\n",
      "loss: 0.214832  [59200/175341]\n",
      "loss: 0.482258  [60800/175341]\n",
      "loss: 0.339061  [62400/175341]\n",
      "loss: 0.577525  [64000/175341]\n",
      "loss: 0.393725  [65600/175341]\n",
      "loss: 0.261168  [67200/175341]\n",
      "loss: 0.322913  [68800/175341]\n",
      "loss: 0.150604  [70400/175341]\n",
      "loss: 0.346364  [72000/175341]\n",
      "loss: 0.271657  [73600/175341]\n",
      "loss: 0.332349  [75200/175341]\n",
      "loss: 0.228141  [76800/175341]\n",
      "loss: 0.953621  [78400/175341]\n",
      "loss: 0.933624  [80000/175341]\n",
      "loss: 0.357618  [81600/175341]\n",
      "loss: 0.214499  [83200/175341]\n",
      "loss: 0.896133  [84800/175341]\n",
      "loss: 0.449098  [86400/175341]\n",
      "loss: 0.144220  [88000/175341]\n",
      "loss: 0.377590  [89600/175341]\n",
      "loss: 0.686566  [91200/175341]\n",
      "loss: 0.513632  [92800/175341]\n",
      "loss: 0.289136  [94400/175341]\n",
      "loss: 0.556855  [96000/175341]\n",
      "loss: 0.280709  [97600/175341]\n",
      "loss: 0.527682  [99200/175341]\n",
      "loss: 0.466989  [100800/175341]\n",
      "loss: 0.473864  [102400/175341]\n",
      "loss: 0.617992  [104000/175341]\n",
      "loss: 0.181611  [105600/175341]\n",
      "loss: 0.426941  [107200/175341]\n",
      "loss: 0.387745  [108800/175341]\n",
      "loss: 0.588723  [110400/175341]\n",
      "loss: 0.490805  [112000/175341]\n",
      "loss: 0.323852  [113600/175341]\n",
      "loss: 0.105517  [115200/175341]\n",
      "loss: 0.302989  [116800/175341]\n",
      "loss: 0.265063  [118400/175341]\n",
      "loss: 0.393186  [120000/175341]\n",
      "loss: 0.620497  [121600/175341]\n",
      "loss: 0.412669  [123200/175341]\n",
      "loss: 0.321376  [124800/175341]\n",
      "loss: 0.349491  [126400/175341]\n",
      "loss: 0.619589  [128000/175341]\n",
      "loss: 0.352746  [129600/175341]\n",
      "loss: 1.013871  [131200/175341]\n",
      "loss: 0.762657  [132800/175341]\n",
      "loss: 0.409775  [134400/175341]\n",
      "loss: 0.486345  [136000/175341]\n",
      "loss: 0.240531  [137600/175341]\n",
      "loss: 0.602786  [139200/175341]\n",
      "loss: 0.665952  [140800/175341]\n",
      "loss: 0.599990  [142400/175341]\n",
      "loss: 1.032125  [144000/175341]\n",
      "loss: 0.136296  [145600/175341]\n",
      "loss: 0.604192  [147200/175341]\n",
      "loss: 0.454657  [148800/175341]\n",
      "loss: 0.763222  [150400/175341]\n",
      "loss: 0.329273  [152000/175341]\n",
      "loss: 0.778648  [153600/175341]\n",
      "loss: 0.561363  [155200/175341]\n",
      "loss: 0.252354  [156800/175341]\n",
      "loss: 0.320169  [158400/175341]\n",
      "loss: 0.395101  [160000/175341]\n",
      "loss: 0.278189  [161600/175341]\n",
      "loss: 0.714184  [163200/175341]\n",
      "loss: 0.438420  [164800/175341]\n",
      "loss: 0.285356  [166400/175341]\n",
      "loss: 0.077438  [168000/175341]\n",
      "loss: 0.280493  [169600/175341]\n",
      "loss: 0.616041  [171200/175341]\n",
      "loss: 0.632393  [172800/175341]\n",
      "loss: 0.786715  [174400/175341]\n",
      "Train Accuracy: 81.4134%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.556986, F1-score: 76.48%, Macro_F1-Score:  41.37%  \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.747329  [    0/175341]\n",
      "loss: 0.206822  [ 1600/175341]\n",
      "loss: 0.494165  [ 3200/175341]\n",
      "loss: 0.461156  [ 4800/175341]\n",
      "loss: 0.386999  [ 6400/175341]\n",
      "loss: 0.591583  [ 8000/175341]\n",
      "loss: 0.437971  [ 9600/175341]\n",
      "loss: 0.954987  [11200/175341]\n",
      "loss: 0.426912  [12800/175341]\n",
      "loss: 0.344656  [14400/175341]\n",
      "loss: 0.384722  [16000/175341]\n",
      "loss: 0.339953  [17600/175341]\n",
      "loss: 0.196295  [19200/175341]\n",
      "loss: 0.392046  [20800/175341]\n",
      "loss: 0.446729  [22400/175341]\n",
      "loss: 0.471270  [24000/175341]\n",
      "loss: 0.713726  [25600/175341]\n",
      "loss: 0.971232  [27200/175341]\n",
      "loss: 0.899337  [28800/175341]\n",
      "loss: 0.363776  [30400/175341]\n",
      "loss: 0.532369  [32000/175341]\n",
      "loss: 0.154584  [33600/175341]\n",
      "loss: 0.550771  [35200/175341]\n",
      "loss: 0.296338  [36800/175341]\n",
      "loss: 0.736951  [38400/175341]\n",
      "loss: 0.542216  [40000/175341]\n",
      "loss: 0.468133  [41600/175341]\n",
      "loss: 0.682893  [43200/175341]\n",
      "loss: 0.736202  [44800/175341]\n",
      "loss: 0.352507  [46400/175341]\n",
      "loss: 0.322402  [48000/175341]\n",
      "loss: 0.159207  [49600/175341]\n",
      "loss: 0.573316  [51200/175341]\n",
      "loss: 0.608850  [52800/175341]\n",
      "loss: 0.415846  [54400/175341]\n",
      "loss: 0.361935  [56000/175341]\n",
      "loss: 0.466999  [57600/175341]\n",
      "loss: 0.196086  [59200/175341]\n",
      "loss: 0.618660  [60800/175341]\n",
      "loss: 0.997729  [62400/175341]\n",
      "loss: 0.395174  [64000/175341]\n",
      "loss: 0.502296  [65600/175341]\n",
      "loss: 0.342572  [67200/175341]\n",
      "loss: 0.300237  [68800/175341]\n",
      "loss: 0.514183  [70400/175341]\n",
      "loss: 0.978626  [72000/175341]\n",
      "loss: 1.040593  [73600/175341]\n",
      "loss: 0.613406  [75200/175341]\n",
      "loss: 0.504769  [76800/175341]\n",
      "loss: 0.418631  [78400/175341]\n",
      "loss: 0.289180  [80000/175341]\n",
      "loss: 0.363543  [81600/175341]\n",
      "loss: 0.340462  [83200/175341]\n",
      "loss: 0.364016  [84800/175341]\n",
      "loss: 0.298327  [86400/175341]\n",
      "loss: 0.449627  [88000/175341]\n",
      "loss: 0.727450  [89600/175341]\n",
      "loss: 0.362744  [91200/175341]\n",
      "loss: 0.226849  [92800/175341]\n",
      "loss: 0.498503  [94400/175341]\n",
      "loss: 0.599264  [96000/175341]\n",
      "loss: 0.574795  [97600/175341]\n",
      "loss: 0.702861  [99200/175341]\n",
      "loss: 0.351700  [100800/175341]\n",
      "loss: 0.210209  [102400/175341]\n",
      "loss: 0.441809  [104000/175341]\n",
      "loss: 0.273149  [105600/175341]\n",
      "loss: 0.290918  [107200/175341]\n",
      "loss: 0.416978  [108800/175341]\n",
      "loss: 0.354096  [110400/175341]\n",
      "loss: 0.399891  [112000/175341]\n",
      "loss: 0.332415  [113600/175341]\n",
      "loss: 0.522269  [115200/175341]\n",
      "loss: 0.478477  [116800/175341]\n",
      "loss: 0.755389  [118400/175341]\n",
      "loss: 0.168622  [120000/175341]\n",
      "loss: 0.529437  [121600/175341]\n",
      "loss: 0.413388  [123200/175341]\n",
      "loss: 0.877552  [124800/175341]\n",
      "loss: 0.374981  [126400/175341]\n",
      "loss: 0.345530  [128000/175341]\n",
      "loss: 0.085074  [129600/175341]\n",
      "loss: 0.610455  [131200/175341]\n",
      "loss: 0.802795  [132800/175341]\n",
      "loss: 0.600789  [134400/175341]\n",
      "loss: 0.513227  [136000/175341]\n",
      "loss: 0.706990  [137600/175341]\n",
      "loss: 0.267622  [139200/175341]\n",
      "loss: 0.530829  [140800/175341]\n",
      "loss: 0.345217  [142400/175341]\n",
      "loss: 0.262852  [144000/175341]\n",
      "loss: 0.706481  [145600/175341]\n",
      "loss: 0.657488  [147200/175341]\n",
      "loss: 0.467230  [148800/175341]\n",
      "loss: 0.423662  [150400/175341]\n",
      "loss: 0.731902  [152000/175341]\n",
      "loss: 0.462579  [153600/175341]\n",
      "loss: 0.553739  [155200/175341]\n",
      "loss: 0.665334  [156800/175341]\n",
      "loss: 0.508847  [158400/175341]\n",
      "loss: 0.564856  [160000/175341]\n",
      "loss: 0.184981  [161600/175341]\n",
      "loss: 0.356954  [163200/175341]\n",
      "loss: 0.172760  [164800/175341]\n",
      "loss: 0.319929  [166400/175341]\n",
      "loss: 0.254381  [168000/175341]\n",
      "loss: 0.364047  [169600/175341]\n",
      "loss: 0.558260  [171200/175341]\n",
      "loss: 0.241008  [172800/175341]\n",
      "loss: 0.418211  [174400/175341]\n",
      "Train Accuracy: 81.3831%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.548499, F1-score: 76.32%, Macro_F1-Score:  40.95%  \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.328492  [    0/175341]\n",
      "loss: 0.779742  [ 1600/175341]\n",
      "loss: 0.557011  [ 3200/175341]\n",
      "loss: 0.438880  [ 4800/175341]\n",
      "loss: 0.274648  [ 6400/175341]\n",
      "loss: 0.309031  [ 8000/175341]\n",
      "loss: 0.478418  [ 9600/175341]\n",
      "loss: 0.230610  [11200/175341]\n",
      "loss: 0.315022  [12800/175341]\n",
      "loss: 0.285250  [14400/175341]\n",
      "loss: 0.442680  [16000/175341]\n",
      "loss: 0.635496  [17600/175341]\n",
      "loss: 0.135215  [19200/175341]\n",
      "loss: 0.521966  [20800/175341]\n",
      "loss: 0.429017  [22400/175341]\n",
      "loss: 0.583305  [24000/175341]\n",
      "loss: 0.550760  [25600/175341]\n",
      "loss: 0.684805  [27200/175341]\n",
      "loss: 0.169556  [28800/175341]\n",
      "loss: 0.318520  [30400/175341]\n",
      "loss: 0.290432  [32000/175341]\n",
      "loss: 0.486497  [33600/175341]\n",
      "loss: 0.402828  [35200/175341]\n",
      "loss: 0.436117  [36800/175341]\n",
      "loss: 0.325526  [38400/175341]\n",
      "loss: 0.194312  [40000/175341]\n",
      "loss: 0.655804  [41600/175341]\n",
      "loss: 0.711851  [43200/175341]\n",
      "loss: 0.303932  [44800/175341]\n",
      "loss: 0.580198  [46400/175341]\n",
      "loss: 0.174404  [48000/175341]\n",
      "loss: 0.596887  [49600/175341]\n",
      "loss: 0.486493  [51200/175341]\n",
      "loss: 0.589151  [52800/175341]\n",
      "loss: 1.000978  [54400/175341]\n",
      "loss: 0.413137  [56000/175341]\n",
      "loss: 0.160998  [57600/175341]\n",
      "loss: 0.406622  [59200/175341]\n",
      "loss: 0.866665  [60800/175341]\n",
      "loss: 0.454143  [62400/175341]\n",
      "loss: 0.674458  [64000/175341]\n",
      "loss: 0.436983  [65600/175341]\n",
      "loss: 0.354816  [67200/175341]\n",
      "loss: 0.281025  [68800/175341]\n",
      "loss: 0.506293  [70400/175341]\n",
      "loss: 0.783718  [72000/175341]\n",
      "loss: 0.394510  [73600/175341]\n",
      "loss: 0.343050  [75200/175341]\n",
      "loss: 0.311833  [76800/175341]\n",
      "loss: 0.204053  [78400/175341]\n",
      "loss: 0.665010  [80000/175341]\n",
      "loss: 0.153085  [81600/175341]\n",
      "loss: 0.503764  [83200/175341]\n",
      "loss: 0.389621  [84800/175341]\n",
      "loss: 0.301596  [86400/175341]\n",
      "loss: 0.668438  [88000/175341]\n",
      "loss: 1.042962  [89600/175341]\n",
      "loss: 0.296488  [91200/175341]\n",
      "loss: 0.395258  [92800/175341]\n",
      "loss: 0.536279  [94400/175341]\n",
      "loss: 0.351978  [96000/175341]\n",
      "loss: 0.749312  [97600/175341]\n",
      "loss: 0.323196  [99200/175341]\n",
      "loss: 0.782102  [100800/175341]\n",
      "loss: 0.468273  [102400/175341]\n",
      "loss: 0.525392  [104000/175341]\n",
      "loss: 0.398551  [105600/175341]\n",
      "loss: 0.520179  [107200/175341]\n",
      "loss: 0.209375  [108800/175341]\n",
      "loss: 0.347390  [110400/175341]\n",
      "loss: 0.385507  [112000/175341]\n",
      "loss: 0.868960  [113600/175341]\n",
      "loss: 0.459136  [115200/175341]\n",
      "loss: 0.285215  [116800/175341]\n",
      "loss: 0.516734  [118400/175341]\n",
      "loss: 0.522385  [120000/175341]\n",
      "loss: 0.367593  [121600/175341]\n",
      "loss: 0.443393  [123200/175341]\n",
      "loss: 0.822608  [124800/175341]\n",
      "loss: 0.656831  [126400/175341]\n",
      "loss: 0.484277  [128000/175341]\n",
      "loss: 0.349507  [129600/175341]\n",
      "loss: 0.230212  [131200/175341]\n",
      "loss: 0.189230  [132800/175341]\n",
      "loss: 0.397523  [134400/175341]\n",
      "loss: 0.994220  [136000/175341]\n",
      "loss: 0.702683  [137600/175341]\n",
      "loss: 0.376113  [139200/175341]\n",
      "loss: 0.521125  [140800/175341]\n",
      "loss: 0.648189  [142400/175341]\n",
      "loss: 0.542458  [144000/175341]\n",
      "loss: 0.151759  [145600/175341]\n",
      "loss: 0.489964  [147200/175341]\n",
      "loss: 0.310019  [148800/175341]\n",
      "loss: 0.283472  [150400/175341]\n",
      "loss: 0.465612  [152000/175341]\n",
      "loss: 0.061855  [153600/175341]\n",
      "loss: 0.515898  [155200/175341]\n",
      "loss: 0.235084  [156800/175341]\n",
      "loss: 0.742107  [158400/175341]\n",
      "loss: 0.621807  [160000/175341]\n",
      "loss: 0.728789  [161600/175341]\n",
      "loss: 0.298724  [163200/175341]\n",
      "loss: 0.552027  [164800/175341]\n",
      "loss: 0.240524  [166400/175341]\n",
      "loss: 0.309378  [168000/175341]\n",
      "loss: 0.535492  [169600/175341]\n",
      "loss: 0.310956  [171200/175341]\n",
      "loss: 0.493994  [172800/175341]\n",
      "loss: 0.672936  [174400/175341]\n",
      "Train Accuracy: 81.4225%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.566900, F1-score: 76.30%, Macro_F1-Score:  40.55%  \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.635766  [    0/175341]\n",
      "loss: 0.607146  [ 1600/175341]\n",
      "loss: 0.579757  [ 3200/175341]\n",
      "loss: 0.718305  [ 4800/175341]\n",
      "loss: 0.648691  [ 6400/175341]\n",
      "loss: 0.461699  [ 8000/175341]\n",
      "loss: 0.734499  [ 9600/175341]\n",
      "loss: 0.538384  [11200/175341]\n",
      "loss: 0.165041  [12800/175341]\n",
      "loss: 0.445952  [14400/175341]\n",
      "loss: 0.314241  [16000/175341]\n",
      "loss: 0.471786  [17600/175341]\n",
      "loss: 0.309517  [19200/175341]\n",
      "loss: 0.521501  [20800/175341]\n",
      "loss: 0.143338  [22400/175341]\n",
      "loss: 0.519310  [24000/175341]\n",
      "loss: 0.194040  [25600/175341]\n",
      "loss: 0.603669  [27200/175341]\n",
      "loss: 0.541750  [28800/175341]\n",
      "loss: 0.735105  [30400/175341]\n",
      "loss: 0.622851  [32000/175341]\n",
      "loss: 0.296638  [33600/175341]\n",
      "loss: 0.457858  [35200/175341]\n",
      "loss: 0.500958  [36800/175341]\n",
      "loss: 0.677941  [38400/175341]\n",
      "loss: 0.248514  [40000/175341]\n",
      "loss: 0.423062  [41600/175341]\n",
      "loss: 0.796106  [43200/175341]\n",
      "loss: 0.463316  [44800/175341]\n",
      "loss: 0.638623  [46400/175341]\n",
      "loss: 0.656835  [48000/175341]\n",
      "loss: 0.575986  [49600/175341]\n",
      "loss: 0.670415  [51200/175341]\n",
      "loss: 0.389250  [52800/175341]\n",
      "loss: 0.613399  [54400/175341]\n",
      "loss: 0.572281  [56000/175341]\n",
      "loss: 0.380903  [57600/175341]\n",
      "loss: 0.494413  [59200/175341]\n",
      "loss: 0.403025  [60800/175341]\n",
      "loss: 0.212576  [62400/175341]\n",
      "loss: 0.244733  [64000/175341]\n",
      "loss: 0.874487  [65600/175341]\n",
      "loss: 0.412737  [67200/175341]\n",
      "loss: 0.237955  [68800/175341]\n",
      "loss: 0.585865  [70400/175341]\n",
      "loss: 0.498113  [72000/175341]\n",
      "loss: 0.892654  [73600/175341]\n",
      "loss: 0.195867  [75200/175341]\n",
      "loss: 0.434249  [76800/175341]\n",
      "loss: 0.335444  [78400/175341]\n",
      "loss: 0.560287  [80000/175341]\n",
      "loss: 0.453089  [81600/175341]\n",
      "loss: 0.110900  [83200/175341]\n",
      "loss: 0.654256  [84800/175341]\n",
      "loss: 0.367380  [86400/175341]\n",
      "loss: 0.648816  [88000/175341]\n",
      "loss: 0.224582  [89600/175341]\n",
      "loss: 0.178617  [91200/175341]\n",
      "loss: 0.606632  [92800/175341]\n",
      "loss: 0.603886  [94400/175341]\n",
      "loss: 0.381339  [96000/175341]\n",
      "loss: 0.223932  [97600/175341]\n",
      "loss: 0.776589  [99200/175341]\n",
      "loss: 0.385601  [100800/175341]\n",
      "loss: 0.491088  [102400/175341]\n",
      "loss: 0.578707  [104000/175341]\n",
      "loss: 0.714613  [105600/175341]\n",
      "loss: 0.525573  [107200/175341]\n",
      "loss: 0.489509  [108800/175341]\n",
      "loss: 0.307738  [110400/175341]\n",
      "loss: 0.625064  [112000/175341]\n",
      "loss: 0.241792  [113600/175341]\n",
      "loss: 0.622959  [115200/175341]\n",
      "loss: 0.194502  [116800/175341]\n",
      "loss: 0.405012  [118400/175341]\n",
      "loss: 0.508612  [120000/175341]\n",
      "loss: 0.498436  [121600/175341]\n",
      "loss: 0.726690  [123200/175341]\n",
      "loss: 0.414244  [124800/175341]\n",
      "loss: 0.327624  [126400/175341]\n",
      "loss: 0.684594  [128000/175341]\n",
      "loss: 0.154727  [129600/175341]\n",
      "loss: 1.100502  [131200/175341]\n",
      "loss: 0.591088  [132800/175341]\n",
      "loss: 0.479001  [134400/175341]\n",
      "loss: 0.168642  [136000/175341]\n",
      "loss: 0.457152  [137600/175341]\n",
      "loss: 0.405464  [139200/175341]\n",
      "loss: 0.516432  [140800/175341]\n",
      "loss: 0.196662  [142400/175341]\n",
      "loss: 0.518157  [144000/175341]\n",
      "loss: 0.274402  [145600/175341]\n",
      "loss: 0.460536  [147200/175341]\n",
      "loss: 0.258704  [148800/175341]\n",
      "loss: 0.686437  [150400/175341]\n",
      "loss: 0.462903  [152000/175341]\n",
      "loss: 0.238119  [153600/175341]\n",
      "loss: 0.246960  [155200/175341]\n",
      "loss: 0.602851  [156800/175341]\n",
      "loss: 0.726873  [158400/175341]\n",
      "loss: 0.305667  [160000/175341]\n",
      "loss: 1.335805  [161600/175341]\n",
      "loss: 0.361816  [163200/175341]\n",
      "loss: 0.541601  [164800/175341]\n",
      "loss: 0.663031  [166400/175341]\n",
      "loss: 0.609173  [168000/175341]\n",
      "loss: 0.617293  [169600/175341]\n",
      "loss: 0.242228  [171200/175341]\n",
      "loss: 0.198111  [172800/175341]\n",
      "loss: 0.486497  [174400/175341]\n",
      "Train Accuracy: 81.4727%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.557080, F1-score: 75.76%, Macro_F1-Score:  40.61%  \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.471637  [    0/175341]\n",
      "loss: 0.530960  [ 1600/175341]\n",
      "loss: 0.384100  [ 3200/175341]\n",
      "loss: 0.495860  [ 4800/175341]\n",
      "loss: 0.367613  [ 6400/175341]\n",
      "loss: 0.334440  [ 8000/175341]\n",
      "loss: 0.634375  [ 9600/175341]\n",
      "loss: 0.485555  [11200/175341]\n",
      "loss: 0.346710  [12800/175341]\n",
      "loss: 0.410632  [14400/175341]\n",
      "loss: 0.126539  [16000/175341]\n",
      "loss: 0.121696  [17600/175341]\n",
      "loss: 0.316349  [19200/175341]\n",
      "loss: 0.491109  [20800/175341]\n",
      "loss: 0.503568  [22400/175341]\n",
      "loss: 0.347125  [24000/175341]\n",
      "loss: 0.564181  [25600/175341]\n",
      "loss: 0.128892  [27200/175341]\n",
      "loss: 0.236294  [28800/175341]\n",
      "loss: 0.264543  [30400/175341]\n",
      "loss: 0.626898  [32000/175341]\n",
      "loss: 0.488605  [33600/175341]\n",
      "loss: 0.238430  [35200/175341]\n",
      "loss: 0.222204  [36800/175341]\n",
      "loss: 0.617329  [38400/175341]\n",
      "loss: 0.057394  [40000/175341]\n",
      "loss: 0.282598  [41600/175341]\n",
      "loss: 0.503432  [43200/175341]\n",
      "loss: 0.363955  [44800/175341]\n",
      "loss: 0.375252  [46400/175341]\n",
      "loss: 0.576394  [48000/175341]\n",
      "loss: 0.203391  [49600/175341]\n",
      "loss: 0.442818  [51200/175341]\n",
      "loss: 0.352536  [52800/175341]\n",
      "loss: 0.461586  [54400/175341]\n",
      "loss: 0.406701  [56000/175341]\n",
      "loss: 0.605162  [57600/175341]\n",
      "loss: 0.661203  [59200/175341]\n",
      "loss: 0.322944  [60800/175341]\n",
      "loss: 0.191216  [62400/175341]\n",
      "loss: 0.386549  [64000/175341]\n",
      "loss: 0.139274  [65600/175341]\n",
      "loss: 0.890425  [67200/175341]\n",
      "loss: 0.489169  [68800/175341]\n",
      "loss: 0.970947  [70400/175341]\n",
      "loss: 0.229406  [72000/175341]\n",
      "loss: 0.288640  [73600/175341]\n",
      "loss: 0.207761  [75200/175341]\n",
      "loss: 0.507281  [76800/175341]\n",
      "loss: 0.167050  [78400/175341]\n",
      "loss: 0.360381  [80000/175341]\n",
      "loss: 0.179252  [81600/175341]\n",
      "loss: 0.584056  [83200/175341]\n",
      "loss: 0.697016  [84800/175341]\n",
      "loss: 0.626555  [86400/175341]\n",
      "loss: 0.329075  [88000/175341]\n",
      "loss: 0.601319  [89600/175341]\n",
      "loss: 0.518649  [91200/175341]\n",
      "loss: 0.171013  [92800/175341]\n",
      "loss: 0.242036  [94400/175341]\n",
      "loss: 0.598393  [96000/175341]\n",
      "loss: 0.295459  [97600/175341]\n",
      "loss: 0.452170  [99200/175341]\n",
      "loss: 0.352071  [100800/175341]\n",
      "loss: 0.508464  [102400/175341]\n",
      "loss: 0.590389  [104000/175341]\n",
      "loss: 0.372821  [105600/175341]\n",
      "loss: 0.602643  [107200/175341]\n",
      "loss: 0.214740  [108800/175341]\n",
      "loss: 0.753165  [110400/175341]\n",
      "loss: 0.531310  [112000/175341]\n",
      "loss: 0.630497  [113600/175341]\n",
      "loss: 0.632377  [115200/175341]\n",
      "loss: 0.650804  [116800/175341]\n",
      "loss: 0.350476  [118400/175341]\n",
      "loss: 0.332946  [120000/175341]\n",
      "loss: 0.719766  [121600/175341]\n",
      "loss: 0.801921  [123200/175341]\n",
      "loss: 0.550056  [124800/175341]\n",
      "loss: 0.325963  [126400/175341]\n",
      "loss: 0.600587  [128000/175341]\n",
      "loss: 0.471674  [129600/175341]\n",
      "loss: 0.765472  [131200/175341]\n",
      "loss: 0.455845  [132800/175341]\n",
      "loss: 0.591227  [134400/175341]\n",
      "loss: 0.238956  [136000/175341]\n",
      "loss: 0.241890  [137600/175341]\n",
      "loss: 0.278166  [139200/175341]\n",
      "loss: 0.964972  [140800/175341]\n",
      "loss: 0.597791  [142400/175341]\n",
      "loss: 0.647269  [144000/175341]\n",
      "loss: 0.334603  [145600/175341]\n",
      "loss: 0.194939  [147200/175341]\n",
      "loss: 0.608263  [148800/175341]\n",
      "loss: 1.056612  [150400/175341]\n",
      "loss: 0.227408  [152000/175341]\n",
      "loss: 0.424244  [153600/175341]\n",
      "loss: 0.755101  [155200/175341]\n",
      "loss: 0.621995  [156800/175341]\n",
      "loss: 0.396514  [158400/175341]\n",
      "loss: 0.304692  [160000/175341]\n",
      "loss: 0.489153  [161600/175341]\n",
      "loss: 0.164914  [163200/175341]\n",
      "loss: 0.361553  [164800/175341]\n",
      "loss: 0.541621  [166400/175341]\n",
      "loss: 0.489704  [168000/175341]\n",
      "loss: 0.336114  [169600/175341]\n",
      "loss: 0.580445  [171200/175341]\n",
      "loss: 0.363265  [172800/175341]\n",
      "loss: 0.728723  [174400/175341]\n",
      "Train Accuracy: 81.4972%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.562614, F1-score: 75.91%, Macro_F1-Score:  40.84%  \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.449975  [    0/175341]\n",
      "loss: 0.657063  [ 1600/175341]\n",
      "loss: 0.222160  [ 3200/175341]\n",
      "loss: 0.324559  [ 4800/175341]\n",
      "loss: 0.358308  [ 6400/175341]\n",
      "loss: 0.228205  [ 8000/175341]\n",
      "loss: 0.536793  [ 9600/175341]\n",
      "loss: 0.548779  [11200/175341]\n",
      "loss: 0.338938  [12800/175341]\n",
      "loss: 0.708041  [14400/175341]\n",
      "loss: 0.327991  [16000/175341]\n",
      "loss: 0.468989  [17600/175341]\n",
      "loss: 0.374235  [19200/175341]\n",
      "loss: 0.322630  [20800/175341]\n",
      "loss: 0.127972  [22400/175341]\n",
      "loss: 0.327602  [24000/175341]\n",
      "loss: 0.630487  [25600/175341]\n",
      "loss: 0.402407  [27200/175341]\n",
      "loss: 0.445472  [28800/175341]\n",
      "loss: 0.488459  [30400/175341]\n",
      "loss: 0.277432  [32000/175341]\n",
      "loss: 0.405048  [33600/175341]\n",
      "loss: 0.223115  [35200/175341]\n",
      "loss: 0.320361  [36800/175341]\n",
      "loss: 0.508664  [38400/175341]\n",
      "loss: 0.775098  [40000/175341]\n",
      "loss: 0.473070  [41600/175341]\n",
      "loss: 0.462536  [43200/175341]\n",
      "loss: 0.883673  [44800/175341]\n",
      "loss: 0.842235  [46400/175341]\n",
      "loss: 0.370747  [48000/175341]\n",
      "loss: 0.383845  [49600/175341]\n",
      "loss: 0.597451  [51200/175341]\n",
      "loss: 0.288190  [52800/175341]\n",
      "loss: 0.363680  [54400/175341]\n",
      "loss: 0.395535  [56000/175341]\n",
      "loss: 0.144351  [57600/175341]\n",
      "loss: 0.605966  [59200/175341]\n",
      "loss: 0.491012  [60800/175341]\n",
      "loss: 0.549644  [62400/175341]\n",
      "loss: 0.341165  [64000/175341]\n",
      "loss: 0.187007  [65600/175341]\n",
      "loss: 0.442589  [67200/175341]\n",
      "loss: 0.474667  [68800/175341]\n",
      "loss: 0.294411  [70400/175341]\n",
      "loss: 0.249018  [72000/175341]\n",
      "loss: 0.647318  [73600/175341]\n",
      "loss: 0.095312  [75200/175341]\n",
      "loss: 0.598250  [76800/175341]\n",
      "loss: 0.380887  [78400/175341]\n",
      "loss: 0.493265  [80000/175341]\n",
      "loss: 0.232949  [81600/175341]\n",
      "loss: 0.342861  [83200/175341]\n",
      "loss: 0.359601  [84800/175341]\n",
      "loss: 0.445179  [86400/175341]\n",
      "loss: 0.571496  [88000/175341]\n",
      "loss: 0.528545  [89600/175341]\n",
      "loss: 0.266303  [91200/175341]\n",
      "loss: 0.376366  [92800/175341]\n",
      "loss: 0.361288  [94400/175341]\n",
      "loss: 0.444549  [96000/175341]\n",
      "loss: 0.346622  [97600/175341]\n",
      "loss: 0.676341  [99200/175341]\n",
      "loss: 0.269461  [100800/175341]\n",
      "loss: 0.590500  [102400/175341]\n",
      "loss: 0.507647  [104000/175341]\n",
      "loss: 0.516599  [105600/175341]\n",
      "loss: 0.532999  [107200/175341]\n",
      "loss: 0.781142  [108800/175341]\n",
      "loss: 0.241724  [110400/175341]\n",
      "loss: 0.519205  [112000/175341]\n",
      "loss: 0.678611  [113600/175341]\n",
      "loss: 0.977274  [115200/175341]\n",
      "loss: 0.684570  [116800/175341]\n",
      "loss: 0.380616  [118400/175341]\n",
      "loss: 0.250174  [120000/175341]\n",
      "loss: 0.604303  [121600/175341]\n",
      "loss: 0.616372  [123200/175341]\n",
      "loss: 0.553900  [124800/175341]\n",
      "loss: 1.082894  [126400/175341]\n",
      "loss: 0.439629  [128000/175341]\n",
      "loss: 0.501924  [129600/175341]\n",
      "loss: 0.382858  [131200/175341]\n",
      "loss: 0.560511  [132800/175341]\n",
      "loss: 0.757164  [134400/175341]\n",
      "loss: 0.305654  [136000/175341]\n",
      "loss: 0.759204  [137600/175341]\n",
      "loss: 0.574805  [139200/175341]\n",
      "loss: 0.187738  [140800/175341]\n",
      "loss: 0.724347  [142400/175341]\n",
      "loss: 0.511329  [144000/175341]\n",
      "loss: 0.592825  [145600/175341]\n",
      "loss: 0.669700  [147200/175341]\n",
      "loss: 0.564462  [148800/175341]\n",
      "loss: 0.475705  [150400/175341]\n",
      "loss: 0.843517  [152000/175341]\n",
      "loss: 0.396995  [153600/175341]\n",
      "loss: 0.357384  [155200/175341]\n",
      "loss: 0.232439  [156800/175341]\n",
      "loss: 0.696298  [158400/175341]\n",
      "loss: 0.272773  [160000/175341]\n",
      "loss: 0.618100  [161600/175341]\n",
      "loss: 0.225757  [163200/175341]\n",
      "loss: 0.625632  [164800/175341]\n",
      "loss: 0.647389  [166400/175341]\n",
      "loss: 0.211231  [168000/175341]\n",
      "loss: 0.535962  [169600/175341]\n",
      "loss: 0.198560  [171200/175341]\n",
      "loss: 0.489894  [172800/175341]\n",
      "loss: 0.352067  [174400/175341]\n",
      "Train Accuracy: 81.5154%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.555463, F1-score: 76.01%, Macro_F1-Score:  40.98%  \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.562405  [    0/175341]\n",
      "loss: 0.192847  [ 1600/175341]\n",
      "loss: 0.727340  [ 3200/175341]\n",
      "loss: 0.229517  [ 4800/175341]\n",
      "loss: 0.496782  [ 6400/175341]\n",
      "loss: 0.348794  [ 8000/175341]\n",
      "loss: 0.345917  [ 9600/175341]\n",
      "loss: 0.532821  [11200/175341]\n",
      "loss: 0.175205  [12800/175341]\n",
      "loss: 1.369280  [14400/175341]\n",
      "loss: 1.022282  [16000/175341]\n",
      "loss: 0.673747  [17600/175341]\n",
      "loss: 0.479045  [19200/175341]\n",
      "loss: 0.375651  [20800/175341]\n",
      "loss: 0.531744  [22400/175341]\n",
      "loss: 0.426693  [24000/175341]\n",
      "loss: 0.890092  [25600/175341]\n",
      "loss: 0.493474  [27200/175341]\n",
      "loss: 0.382978  [28800/175341]\n",
      "loss: 0.308764  [30400/175341]\n",
      "loss: 0.353923  [32000/175341]\n",
      "loss: 0.121548  [33600/175341]\n",
      "loss: 0.248150  [35200/175341]\n",
      "loss: 0.458233  [36800/175341]\n",
      "loss: 0.222151  [38400/175341]\n",
      "loss: 0.616342  [40000/175341]\n",
      "loss: 0.739915  [41600/175341]\n",
      "loss: 0.242714  [43200/175341]\n",
      "loss: 0.645502  [44800/175341]\n",
      "loss: 0.516394  [46400/175341]\n",
      "loss: 0.473110  [48000/175341]\n",
      "loss: 0.865774  [49600/175341]\n",
      "loss: 0.506087  [51200/175341]\n",
      "loss: 0.867893  [52800/175341]\n",
      "loss: 0.429981  [54400/175341]\n",
      "loss: 0.486754  [56000/175341]\n",
      "loss: 0.508042  [57600/175341]\n",
      "loss: 0.589908  [59200/175341]\n",
      "loss: 0.504338  [60800/175341]\n",
      "loss: 0.646878  [62400/175341]\n",
      "loss: 0.732331  [64000/175341]\n",
      "loss: 0.367435  [65600/175341]\n",
      "loss: 0.420977  [67200/175341]\n",
      "loss: 0.629879  [68800/175341]\n",
      "loss: 0.345502  [70400/175341]\n",
      "loss: 0.467797  [72000/175341]\n",
      "loss: 0.683016  [73600/175341]\n",
      "loss: 0.088520  [75200/175341]\n",
      "loss: 0.162466  [76800/175341]\n",
      "loss: 0.372597  [78400/175341]\n",
      "loss: 0.487238  [80000/175341]\n",
      "loss: 0.487547  [81600/175341]\n",
      "loss: 0.609388  [83200/175341]\n",
      "loss: 0.277490  [84800/175341]\n",
      "loss: 0.203464  [86400/175341]\n",
      "loss: 0.466109  [88000/175341]\n",
      "loss: 0.441993  [89600/175341]\n",
      "loss: 0.419548  [91200/175341]\n",
      "loss: 0.521603  [92800/175341]\n",
      "loss: 0.571398  [94400/175341]\n",
      "loss: 0.465318  [96000/175341]\n",
      "loss: 0.412752  [97600/175341]\n",
      "loss: 0.337706  [99200/175341]\n",
      "loss: 0.408669  [100800/175341]\n",
      "loss: 0.454939  [102400/175341]\n",
      "loss: 0.257090  [104000/175341]\n",
      "loss: 0.424631  [105600/175341]\n",
      "loss: 1.513891  [107200/175341]\n",
      "loss: 0.078931  [108800/175341]\n",
      "loss: 0.537236  [110400/175341]\n",
      "loss: 0.563745  [112000/175341]\n",
      "loss: 0.337639  [113600/175341]\n",
      "loss: 0.560957  [115200/175341]\n",
      "loss: 0.780638  [116800/175341]\n",
      "loss: 0.448684  [118400/175341]\n",
      "loss: 0.257155  [120000/175341]\n",
      "loss: 0.403153  [121600/175341]\n",
      "loss: 0.508693  [123200/175341]\n",
      "loss: 0.477040  [124800/175341]\n",
      "loss: 0.938911  [126400/175341]\n",
      "loss: 0.175880  [128000/175341]\n",
      "loss: 0.759376  [129600/175341]\n",
      "loss: 0.082156  [131200/175341]\n",
      "loss: 0.479288  [132800/175341]\n",
      "loss: 0.560883  [134400/175341]\n",
      "loss: 0.384996  [136000/175341]\n",
      "loss: 0.651592  [137600/175341]\n",
      "loss: 0.789442  [139200/175341]\n",
      "loss: 0.505674  [140800/175341]\n",
      "loss: 0.736802  [142400/175341]\n",
      "loss: 0.661407  [144000/175341]\n",
      "loss: 0.335044  [145600/175341]\n",
      "loss: 0.344093  [147200/175341]\n",
      "loss: 0.462686  [148800/175341]\n",
      "loss: 0.557683  [150400/175341]\n",
      "loss: 0.398445  [152000/175341]\n",
      "loss: 0.560492  [153600/175341]\n",
      "loss: 0.468924  [155200/175341]\n",
      "loss: 0.603405  [156800/175341]\n",
      "loss: 0.492758  [158400/175341]\n",
      "loss: 0.409981  [160000/175341]\n",
      "loss: 0.190628  [161600/175341]\n",
      "loss: 0.574547  [163200/175341]\n",
      "loss: 0.787680  [164800/175341]\n",
      "loss: 0.894810  [166400/175341]\n",
      "loss: 0.488241  [168000/175341]\n",
      "loss: 0.516358  [169600/175341]\n",
      "loss: 1.017457  [171200/175341]\n",
      "loss: 1.143890  [172800/175341]\n",
      "loss: 0.248246  [174400/175341]\n",
      "Train Accuracy: 81.5371%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.581319, F1-score: 75.42%, Macro_F1-Score:  40.20%  \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.417154  [    0/175341]\n",
      "loss: 0.289101  [ 1600/175341]\n",
      "loss: 0.359986  [ 3200/175341]\n",
      "loss: 0.210625  [ 4800/175341]\n",
      "loss: 0.528524  [ 6400/175341]\n",
      "loss: 0.608979  [ 8000/175341]\n",
      "loss: 0.790278  [ 9600/175341]\n",
      "loss: 0.267975  [11200/175341]\n",
      "loss: 0.570645  [12800/175341]\n",
      "loss: 0.518048  [14400/175341]\n",
      "loss: 0.795999  [16000/175341]\n",
      "loss: 0.104518  [17600/175341]\n",
      "loss: 0.148781  [19200/175341]\n",
      "loss: 0.354003  [20800/175341]\n",
      "loss: 0.794324  [22400/175341]\n",
      "loss: 0.789404  [24000/175341]\n",
      "loss: 0.534064  [25600/175341]\n",
      "loss: 0.302188  [27200/175341]\n",
      "loss: 0.223520  [28800/175341]\n",
      "loss: 0.288010  [30400/175341]\n",
      "loss: 0.482758  [32000/175341]\n",
      "loss: 0.876161  [33600/175341]\n",
      "loss: 0.204523  [35200/175341]\n",
      "loss: 0.677091  [36800/175341]\n",
      "loss: 0.346916  [38400/175341]\n",
      "loss: 0.788726  [40000/175341]\n",
      "loss: 0.462684  [41600/175341]\n",
      "loss: 0.208888  [43200/175341]\n",
      "loss: 0.230835  [44800/175341]\n",
      "loss: 0.439949  [46400/175341]\n",
      "loss: 0.561092  [48000/175341]\n",
      "loss: 0.301415  [49600/175341]\n",
      "loss: 0.517151  [51200/175341]\n",
      "loss: 0.434537  [52800/175341]\n",
      "loss: 0.263097  [54400/175341]\n",
      "loss: 0.367726  [56000/175341]\n",
      "loss: 0.274231  [57600/175341]\n",
      "loss: 0.296957  [59200/175341]\n",
      "loss: 0.289636  [60800/175341]\n",
      "loss: 0.402854  [62400/175341]\n",
      "loss: 0.354530  [64000/175341]\n",
      "loss: 0.694464  [65600/175341]\n",
      "loss: 0.838180  [67200/175341]\n",
      "loss: 0.300579  [68800/175341]\n",
      "loss: 0.372672  [70400/175341]\n",
      "loss: 0.818267  [72000/175341]\n",
      "loss: 0.487800  [73600/175341]\n",
      "loss: 0.199157  [75200/175341]\n",
      "loss: 0.785900  [76800/175341]\n",
      "loss: 1.111960  [78400/175341]\n",
      "loss: 0.258599  [80000/175341]\n",
      "loss: 0.589955  [81600/175341]\n",
      "loss: 0.238335  [83200/175341]\n",
      "loss: 0.574569  [84800/175341]\n",
      "loss: 0.678378  [86400/175341]\n",
      "loss: 0.209646  [88000/175341]\n",
      "loss: 0.547837  [89600/175341]\n",
      "loss: 0.551330  [91200/175341]\n",
      "loss: 0.592246  [92800/175341]\n",
      "loss: 0.260467  [94400/175341]\n",
      "loss: 0.505725  [96000/175341]\n",
      "loss: 0.477815  [97600/175341]\n",
      "loss: 0.486561  [99200/175341]\n",
      "loss: 0.434442  [100800/175341]\n",
      "loss: 0.313517  [102400/175341]\n",
      "loss: 0.682108  [104000/175341]\n",
      "loss: 0.742236  [105600/175341]\n",
      "loss: 0.427264  [107200/175341]\n",
      "loss: 0.409095  [108800/175341]\n",
      "loss: 0.721307  [110400/175341]\n",
      "loss: 0.501574  [112000/175341]\n",
      "loss: 0.241014  [113600/175341]\n",
      "loss: 0.602570  [115200/175341]\n",
      "loss: 0.321707  [116800/175341]\n",
      "loss: 0.293070  [118400/175341]\n",
      "loss: 0.662850  [120000/175341]\n",
      "loss: 0.181706  [121600/175341]\n",
      "loss: 0.188088  [123200/175341]\n",
      "loss: 0.474044  [124800/175341]\n",
      "loss: 0.260813  [126400/175341]\n",
      "loss: 0.698304  [128000/175341]\n",
      "loss: 0.234416  [129600/175341]\n",
      "loss: 0.406181  [131200/175341]\n",
      "loss: 0.472903  [132800/175341]\n",
      "loss: 0.316744  [134400/175341]\n",
      "loss: 0.766417  [136000/175341]\n",
      "loss: 0.394566  [137600/175341]\n",
      "loss: 0.413308  [139200/175341]\n",
      "loss: 0.360102  [140800/175341]\n",
      "loss: 0.377627  [142400/175341]\n",
      "loss: 0.288425  [144000/175341]\n",
      "loss: 0.277342  [145600/175341]\n",
      "loss: 0.214742  [147200/175341]\n",
      "loss: 0.499378  [148800/175341]\n",
      "loss: 0.781957  [150400/175341]\n",
      "loss: 0.200956  [152000/175341]\n",
      "loss: 0.444429  [153600/175341]\n",
      "loss: 0.298118  [155200/175341]\n",
      "loss: 0.540483  [156800/175341]\n",
      "loss: 0.590044  [158400/175341]\n",
      "loss: 0.145170  [160000/175341]\n",
      "loss: 0.498789  [161600/175341]\n",
      "loss: 0.304493  [163200/175341]\n",
      "loss: 0.522003  [164800/175341]\n",
      "loss: 0.154415  [166400/175341]\n",
      "loss: 0.238028  [168000/175341]\n",
      "loss: 0.580627  [169600/175341]\n",
      "loss: 0.425521  [171200/175341]\n",
      "loss: 0.757375  [172800/175341]\n",
      "loss: 0.402469  [174400/175341]\n",
      "Train Accuracy: 81.5126%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.563170, F1-score: 75.48%, Macro_F1-Score:  40.57%  \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.714519  [    0/175341]\n",
      "loss: 0.202494  [ 1600/175341]\n",
      "loss: 0.565530  [ 3200/175341]\n",
      "loss: 0.501787  [ 4800/175341]\n",
      "loss: 0.365221  [ 6400/175341]\n",
      "loss: 0.475032  [ 8000/175341]\n",
      "loss: 0.762498  [ 9600/175341]\n",
      "loss: 0.164471  [11200/175341]\n",
      "loss: 0.888689  [12800/175341]\n",
      "loss: 0.715434  [14400/175341]\n",
      "loss: 0.619784  [16000/175341]\n",
      "loss: 0.487787  [17600/175341]\n",
      "loss: 0.510025  [19200/175341]\n",
      "loss: 0.472174  [20800/175341]\n",
      "loss: 0.736814  [22400/175341]\n",
      "loss: 0.449531  [24000/175341]\n",
      "loss: 0.407705  [25600/175341]\n",
      "loss: 0.668413  [27200/175341]\n",
      "loss: 0.167318  [28800/175341]\n",
      "loss: 0.275277  [30400/175341]\n",
      "loss: 0.386911  [32000/175341]\n",
      "loss: 0.635197  [33600/175341]\n",
      "loss: 0.724969  [35200/175341]\n",
      "loss: 0.429875  [36800/175341]\n",
      "loss: 0.640255  [38400/175341]\n",
      "loss: 0.201654  [40000/175341]\n",
      "loss: 0.732656  [41600/175341]\n",
      "loss: 0.306913  [43200/175341]\n",
      "loss: 0.343518  [44800/175341]\n",
      "loss: 0.559017  [46400/175341]\n",
      "loss: 0.316987  [48000/175341]\n",
      "loss: 0.318317  [49600/175341]\n",
      "loss: 0.275451  [51200/175341]\n",
      "loss: 0.717884  [52800/175341]\n",
      "loss: 0.565542  [54400/175341]\n",
      "loss: 0.423165  [56000/175341]\n",
      "loss: 0.495814  [57600/175341]\n",
      "loss: 0.384206  [59200/175341]\n",
      "loss: 0.561192  [60800/175341]\n",
      "loss: 0.425666  [62400/175341]\n",
      "loss: 0.521727  [64000/175341]\n",
      "loss: 0.512476  [65600/175341]\n",
      "loss: 0.503293  [67200/175341]\n",
      "loss: 0.401586  [68800/175341]\n",
      "loss: 0.187415  [70400/175341]\n",
      "loss: 0.310598  [72000/175341]\n",
      "loss: 0.413619  [73600/175341]\n",
      "loss: 0.672727  [75200/175341]\n",
      "loss: 0.480658  [76800/175341]\n",
      "loss: 0.275388  [78400/175341]\n",
      "loss: 0.242091  [80000/175341]\n",
      "loss: 0.402796  [81600/175341]\n",
      "loss: 0.389278  [83200/175341]\n",
      "loss: 0.421056  [84800/175341]\n",
      "loss: 0.816221  [86400/175341]\n",
      "loss: 0.379179  [88000/175341]\n",
      "loss: 0.678460  [89600/175341]\n",
      "loss: 0.277836  [91200/175341]\n",
      "loss: 0.592274  [92800/175341]\n",
      "loss: 0.360431  [94400/175341]\n",
      "loss: 0.497372  [96000/175341]\n",
      "loss: 0.715548  [97600/175341]\n",
      "loss: 0.571258  [99200/175341]\n",
      "loss: 0.291705  [100800/175341]\n",
      "loss: 0.288934  [102400/175341]\n",
      "loss: 0.337954  [104000/175341]\n",
      "loss: 0.406025  [105600/175341]\n",
      "loss: 0.363649  [107200/175341]\n",
      "loss: 0.226129  [108800/175341]\n",
      "loss: 0.321729  [110400/175341]\n",
      "loss: 0.326301  [112000/175341]\n",
      "loss: 0.446422  [113600/175341]\n",
      "loss: 0.425526  [115200/175341]\n",
      "loss: 0.537368  [116800/175341]\n",
      "loss: 0.321865  [118400/175341]\n",
      "loss: 0.344594  [120000/175341]\n",
      "loss: 0.785367  [121600/175341]\n",
      "loss: 1.145935  [123200/175341]\n",
      "loss: 0.527871  [124800/175341]\n",
      "loss: 0.816280  [126400/175341]\n",
      "loss: 0.494122  [128000/175341]\n",
      "loss: 0.813082  [129600/175341]\n",
      "loss: 0.739972  [131200/175341]\n",
      "loss: 0.404186  [132800/175341]\n",
      "loss: 0.351431  [134400/175341]\n",
      "loss: 0.584315  [136000/175341]\n",
      "loss: 0.481679  [137600/175341]\n",
      "loss: 0.449898  [139200/175341]\n",
      "loss: 0.583752  [140800/175341]\n",
      "loss: 0.615975  [142400/175341]\n",
      "loss: 0.270397  [144000/175341]\n",
      "loss: 0.188416  [145600/175341]\n",
      "loss: 0.490697  [147200/175341]\n",
      "loss: 0.551137  [148800/175341]\n",
      "loss: 0.634033  [150400/175341]\n",
      "loss: 0.495136  [152000/175341]\n",
      "loss: 0.354144  [153600/175341]\n",
      "loss: 0.361808  [155200/175341]\n",
      "loss: 0.445465  [156800/175341]\n",
      "loss: 0.681161  [158400/175341]\n",
      "loss: 0.579431  [160000/175341]\n",
      "loss: 0.412975  [161600/175341]\n",
      "loss: 0.394316  [163200/175341]\n",
      "loss: 0.515874  [164800/175341]\n",
      "loss: 0.527567  [166400/175341]\n",
      "loss: 0.312537  [168000/175341]\n",
      "loss: 0.471343  [169600/175341]\n",
      "loss: 0.503759  [171200/175341]\n",
      "loss: 0.617653  [172800/175341]\n",
      "loss: 0.146534  [174400/175341]\n",
      "Train Accuracy: 81.5388%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.571515, F1-score: 74.93%, Macro_F1-Score:  40.60%  \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.440240  [    0/175341]\n",
      "loss: 0.163119  [ 1600/175341]\n",
      "loss: 0.362383  [ 3200/175341]\n",
      "loss: 0.285969  [ 4800/175341]\n",
      "loss: 0.525324  [ 6400/175341]\n",
      "loss: 0.480972  [ 8000/175341]\n",
      "loss: 0.348585  [ 9600/175341]\n",
      "loss: 0.698559  [11200/175341]\n",
      "loss: 0.395400  [12800/175341]\n",
      "loss: 0.126671  [14400/175341]\n",
      "loss: 0.323964  [16000/175341]\n",
      "loss: 0.534435  [17600/175341]\n",
      "loss: 0.576209  [19200/175341]\n",
      "loss: 0.492746  [20800/175341]\n",
      "loss: 0.390700  [22400/175341]\n",
      "loss: 0.646591  [24000/175341]\n",
      "loss: 0.366228  [25600/175341]\n",
      "loss: 0.347654  [27200/175341]\n",
      "loss: 0.118973  [28800/175341]\n",
      "loss: 0.641207  [30400/175341]\n",
      "loss: 0.444127  [32000/175341]\n",
      "loss: 0.304152  [33600/175341]\n",
      "loss: 0.711026  [35200/175341]\n",
      "loss: 0.947550  [36800/175341]\n",
      "loss: 0.555930  [38400/175341]\n",
      "loss: 0.403098  [40000/175341]\n",
      "loss: 0.664114  [41600/175341]\n",
      "loss: 0.874290  [43200/175341]\n",
      "loss: 0.381731  [44800/175341]\n",
      "loss: 0.632076  [46400/175341]\n",
      "loss: 0.545556  [48000/175341]\n",
      "loss: 0.559742  [49600/175341]\n",
      "loss: 0.434720  [51200/175341]\n",
      "loss: 0.926689  [52800/175341]\n",
      "loss: 0.324498  [54400/175341]\n",
      "loss: 0.470871  [56000/175341]\n",
      "loss: 0.572692  [57600/175341]\n",
      "loss: 0.456570  [59200/175341]\n",
      "loss: 0.575496  [60800/175341]\n",
      "loss: 0.508672  [62400/175341]\n",
      "loss: 0.783224  [64000/175341]\n",
      "loss: 0.349606  [65600/175341]\n",
      "loss: 0.448592  [67200/175341]\n",
      "loss: 0.826351  [68800/175341]\n",
      "loss: 0.413759  [70400/175341]\n",
      "loss: 0.734442  [72000/175341]\n",
      "loss: 0.421856  [73600/175341]\n",
      "loss: 0.416052  [75200/175341]\n",
      "loss: 0.349840  [76800/175341]\n",
      "loss: 0.608764  [78400/175341]\n",
      "loss: 0.791805  [80000/175341]\n",
      "loss: 0.124660  [81600/175341]\n",
      "loss: 0.425968  [83200/175341]\n",
      "loss: 0.356094  [84800/175341]\n",
      "loss: 0.381085  [86400/175341]\n",
      "loss: 0.547326  [88000/175341]\n",
      "loss: 0.526467  [89600/175341]\n",
      "loss: 0.353650  [91200/175341]\n",
      "loss: 0.304879  [92800/175341]\n",
      "loss: 0.216371  [94400/175341]\n",
      "loss: 0.305298  [96000/175341]\n",
      "loss: 0.619483  [97600/175341]\n",
      "loss: 0.492309  [99200/175341]\n",
      "loss: 0.496905  [100800/175341]\n",
      "loss: 0.581606  [102400/175341]\n",
      "loss: 0.399447  [104000/175341]\n",
      "loss: 0.202696  [105600/175341]\n",
      "loss: 0.069899  [107200/175341]\n",
      "loss: 0.538764  [108800/175341]\n",
      "loss: 0.340384  [110400/175341]\n",
      "loss: 0.332637  [112000/175341]\n",
      "loss: 0.704694  [113600/175341]\n",
      "loss: 0.321671  [115200/175341]\n",
      "loss: 0.473974  [116800/175341]\n",
      "loss: 1.035782  [118400/175341]\n",
      "loss: 0.795905  [120000/175341]\n",
      "loss: 0.503582  [121600/175341]\n",
      "loss: 0.651327  [123200/175341]\n",
      "loss: 0.878443  [124800/175341]\n",
      "loss: 0.635898  [126400/175341]\n",
      "loss: 0.300572  [128000/175341]\n",
      "loss: 0.356878  [129600/175341]\n",
      "loss: 0.586098  [131200/175341]\n",
      "loss: 0.649189  [132800/175341]\n",
      "loss: 0.190277  [134400/175341]\n",
      "loss: 0.493779  [136000/175341]\n",
      "loss: 0.501901  [137600/175341]\n",
      "loss: 0.636385  [139200/175341]\n",
      "loss: 0.586283  [140800/175341]\n",
      "loss: 0.284889  [142400/175341]\n",
      "loss: 0.576658  [144000/175341]\n",
      "loss: 0.483190  [145600/175341]\n",
      "loss: 0.556930  [147200/175341]\n",
      "loss: 0.449176  [148800/175341]\n",
      "loss: 0.249577  [150400/175341]\n",
      "loss: 0.345759  [152000/175341]\n",
      "loss: 0.317513  [153600/175341]\n",
      "loss: 0.162506  [155200/175341]\n",
      "loss: 0.322883  [156800/175341]\n",
      "loss: 0.237216  [158400/175341]\n",
      "loss: 0.191966  [160000/175341]\n",
      "loss: 0.660149  [161600/175341]\n",
      "loss: 0.393078  [163200/175341]\n",
      "loss: 0.327097  [164800/175341]\n",
      "loss: 0.351806  [166400/175341]\n",
      "loss: 0.649948  [168000/175341]\n",
      "loss: 0.737731  [169600/175341]\n",
      "loss: 0.923114  [171200/175341]\n",
      "loss: 0.178320  [172800/175341]\n",
      "loss: 0.473750  [174400/175341]\n",
      "Train Accuracy: 81.5725%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.558448, F1-score: 76.33%, Macro_F1-Score:  40.91%  \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.389733  [    0/175341]\n",
      "loss: 0.219234  [ 1600/175341]\n",
      "loss: 0.486825  [ 3200/175341]\n",
      "loss: 0.099187  [ 4800/175341]\n",
      "loss: 0.181147  [ 6400/175341]\n",
      "loss: 0.871660  [ 8000/175341]\n",
      "loss: 0.750919  [ 9600/175341]\n",
      "loss: 0.959717  [11200/175341]\n",
      "loss: 0.180866  [12800/175341]\n",
      "loss: 0.403512  [14400/175341]\n",
      "loss: 0.711315  [16000/175341]\n",
      "loss: 0.555519  [17600/175341]\n",
      "loss: 1.109858  [19200/175341]\n",
      "loss: 0.897975  [20800/175341]\n",
      "loss: 0.516634  [22400/175341]\n",
      "loss: 0.354059  [24000/175341]\n",
      "loss: 0.434460  [25600/175341]\n",
      "loss: 0.127021  [27200/175341]\n",
      "loss: 0.544303  [28800/175341]\n",
      "loss: 0.294429  [30400/175341]\n",
      "loss: 0.152934  [32000/175341]\n",
      "loss: 0.204809  [33600/175341]\n",
      "loss: 0.492016  [35200/175341]\n",
      "loss: 0.735668  [36800/175341]\n",
      "loss: 0.640857  [38400/175341]\n",
      "loss: 0.465007  [40000/175341]\n",
      "loss: 0.615554  [41600/175341]\n",
      "loss: 0.209177  [43200/175341]\n",
      "loss: 0.652101  [44800/175341]\n",
      "loss: 0.147890  [46400/175341]\n",
      "loss: 0.578538  [48000/175341]\n",
      "loss: 0.579908  [49600/175341]\n",
      "loss: 0.911558  [51200/175341]\n",
      "loss: 0.326356  [52800/175341]\n",
      "loss: 0.419514  [54400/175341]\n",
      "loss: 0.250626  [56000/175341]\n",
      "loss: 0.370518  [57600/175341]\n",
      "loss: 0.354943  [59200/175341]\n",
      "loss: 0.407268  [60800/175341]\n",
      "loss: 0.486869  [62400/175341]\n",
      "loss: 0.317148  [64000/175341]\n",
      "loss: 0.328006  [65600/175341]\n",
      "loss: 0.606014  [67200/175341]\n",
      "loss: 0.552797  [68800/175341]\n",
      "loss: 0.331822  [70400/175341]\n",
      "loss: 1.009437  [72000/175341]\n",
      "loss: 0.510893  [73600/175341]\n",
      "loss: 0.566625  [75200/175341]\n",
      "loss: 0.275304  [76800/175341]\n",
      "loss: 0.128552  [78400/175341]\n",
      "loss: 0.323213  [80000/175341]\n",
      "loss: 0.292296  [81600/175341]\n",
      "loss: 0.151643  [83200/175341]\n",
      "loss: 0.644549  [84800/175341]\n",
      "loss: 0.408302  [86400/175341]\n",
      "loss: 0.671485  [88000/175341]\n",
      "loss: 0.110558  [89600/175341]\n",
      "loss: 0.689516  [91200/175341]\n",
      "loss: 0.665629  [92800/175341]\n",
      "loss: 0.261270  [94400/175341]\n",
      "loss: 0.505887  [96000/175341]\n",
      "loss: 0.308359  [97600/175341]\n",
      "loss: 0.438124  [99200/175341]\n",
      "loss: 0.335050  [100800/175341]\n",
      "loss: 0.386597  [102400/175341]\n",
      "loss: 0.385189  [104000/175341]\n",
      "loss: 0.266632  [105600/175341]\n",
      "loss: 0.594895  [107200/175341]\n",
      "loss: 0.244081  [108800/175341]\n",
      "loss: 0.734982  [110400/175341]\n",
      "loss: 0.623665  [112000/175341]\n",
      "loss: 0.350447  [113600/175341]\n",
      "loss: 0.189851  [115200/175341]\n",
      "loss: 0.645461  [116800/175341]\n",
      "loss: 0.297248  [118400/175341]\n",
      "loss: 0.253544  [120000/175341]\n",
      "loss: 0.401375  [121600/175341]\n",
      "loss: 0.434611  [123200/175341]\n",
      "loss: 0.343768  [124800/175341]\n",
      "loss: 0.279405  [126400/175341]\n",
      "loss: 0.435441  [128000/175341]\n",
      "loss: 0.578494  [129600/175341]\n",
      "loss: 0.258515  [131200/175341]\n",
      "loss: 0.761317  [132800/175341]\n",
      "loss: 0.249195  [134400/175341]\n",
      "loss: 0.461627  [136000/175341]\n",
      "loss: 0.394998  [137600/175341]\n",
      "loss: 0.516366  [139200/175341]\n",
      "loss: 0.500708  [140800/175341]\n",
      "loss: 0.400301  [142400/175341]\n",
      "loss: 0.642084  [144000/175341]\n",
      "loss: 0.662440  [145600/175341]\n",
      "loss: 0.184326  [147200/175341]\n",
      "loss: 0.394505  [148800/175341]\n",
      "loss: 0.429898  [150400/175341]\n",
      "loss: 0.622675  [152000/175341]\n",
      "loss: 0.464722  [153600/175341]\n",
      "loss: 0.650280  [155200/175341]\n",
      "loss: 0.676346  [156800/175341]\n",
      "loss: 0.500275  [158400/175341]\n",
      "loss: 0.675899  [160000/175341]\n",
      "loss: 0.609957  [161600/175341]\n",
      "loss: 0.812964  [163200/175341]\n",
      "loss: 0.506079  [164800/175341]\n",
      "loss: 0.748420  [166400/175341]\n",
      "loss: 0.391356  [168000/175341]\n",
      "loss: 0.549876  [169600/175341]\n",
      "loss: 0.326077  [171200/175341]\n",
      "loss: 0.358791  [172800/175341]\n",
      "loss: 0.137001  [174400/175341]\n",
      "Train Accuracy: 81.5799%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.546431, F1-score: 76.83%, Macro_F1-Score:  41.24%  \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.628281  [    0/175341]\n",
      "loss: 0.400209  [ 1600/175341]\n",
      "loss: 0.355161  [ 3200/175341]\n",
      "loss: 0.422837  [ 4800/175341]\n",
      "loss: 0.568381  [ 6400/175341]\n",
      "loss: 0.543506  [ 8000/175341]\n",
      "loss: 0.361828  [ 9600/175341]\n",
      "loss: 0.486950  [11200/175341]\n",
      "loss: 0.515179  [12800/175341]\n",
      "loss: 0.787246  [14400/175341]\n",
      "loss: 0.272577  [16000/175341]\n",
      "loss: 0.184560  [17600/175341]\n",
      "loss: 0.440443  [19200/175341]\n",
      "loss: 0.241284  [20800/175341]\n",
      "loss: 0.640299  [22400/175341]\n",
      "loss: 0.456008  [24000/175341]\n",
      "loss: 0.517670  [25600/175341]\n",
      "loss: 0.596553  [27200/175341]\n",
      "loss: 0.369893  [28800/175341]\n",
      "loss: 0.307187  [30400/175341]\n",
      "loss: 0.197307  [32000/175341]\n",
      "loss: 0.311718  [33600/175341]\n",
      "loss: 0.384065  [35200/175341]\n",
      "loss: 0.254162  [36800/175341]\n",
      "loss: 0.363400  [38400/175341]\n",
      "loss: 0.531252  [40000/175341]\n",
      "loss: 0.735992  [41600/175341]\n",
      "loss: 0.384140  [43200/175341]\n",
      "loss: 0.491517  [44800/175341]\n",
      "loss: 0.416192  [46400/175341]\n",
      "loss: 0.354834  [48000/175341]\n",
      "loss: 0.225891  [49600/175341]\n",
      "loss: 0.161144  [51200/175341]\n",
      "loss: 0.621296  [52800/175341]\n",
      "loss: 0.215068  [54400/175341]\n",
      "loss: 0.256265  [56000/175341]\n",
      "loss: 0.932546  [57600/175341]\n",
      "loss: 0.397235  [59200/175341]\n",
      "loss: 0.214605  [60800/175341]\n",
      "loss: 0.247837  [62400/175341]\n",
      "loss: 0.632295  [64000/175341]\n",
      "loss: 0.409246  [65600/175341]\n",
      "loss: 0.946800  [67200/175341]\n",
      "loss: 0.223837  [68800/175341]\n",
      "loss: 0.490772  [70400/175341]\n",
      "loss: 0.644315  [72000/175341]\n",
      "loss: 0.158601  [73600/175341]\n",
      "loss: 0.738889  [75200/175341]\n",
      "loss: 0.331504  [76800/175341]\n",
      "loss: 0.481885  [78400/175341]\n",
      "loss: 0.455352  [80000/175341]\n",
      "loss: 0.758457  [81600/175341]\n",
      "loss: 0.686603  [83200/175341]\n",
      "loss: 0.455973  [84800/175341]\n",
      "loss: 0.696846  [86400/175341]\n",
      "loss: 0.806245  [88000/175341]\n",
      "loss: 0.449883  [89600/175341]\n",
      "loss: 0.237457  [91200/175341]\n",
      "loss: 0.491866  [92800/175341]\n",
      "loss: 0.521875  [94400/175341]\n",
      "loss: 0.498347  [96000/175341]\n",
      "loss: 0.576285  [97600/175341]\n",
      "loss: 0.298144  [99200/175341]\n",
      "loss: 0.705600  [100800/175341]\n",
      "loss: 0.662429  [102400/175341]\n",
      "loss: 0.126296  [104000/175341]\n",
      "loss: 0.456689  [105600/175341]\n",
      "loss: 0.229465  [107200/175341]\n",
      "loss: 0.317454  [108800/175341]\n",
      "loss: 0.659891  [110400/175341]\n",
      "loss: 0.351994  [112000/175341]\n",
      "loss: 0.507205  [113600/175341]\n",
      "loss: 0.224764  [115200/175341]\n",
      "loss: 0.219556  [116800/175341]\n",
      "loss: 0.529359  [118400/175341]\n",
      "loss: 0.532546  [120000/175341]\n",
      "loss: 0.690151  [121600/175341]\n",
      "loss: 0.865688  [123200/175341]\n",
      "loss: 0.319843  [124800/175341]\n",
      "loss: 1.068045  [126400/175341]\n",
      "loss: 0.672067  [128000/175341]\n",
      "loss: 0.358623  [129600/175341]\n",
      "loss: 0.304006  [131200/175341]\n",
      "loss: 0.573265  [132800/175341]\n",
      "loss: 0.251084  [134400/175341]\n",
      "loss: 0.202526  [136000/175341]\n",
      "loss: 0.617626  [137600/175341]\n",
      "loss: 0.617048  [139200/175341]\n",
      "loss: 0.334634  [140800/175341]\n",
      "loss: 0.706717  [142400/175341]\n",
      "loss: 0.220408  [144000/175341]\n",
      "loss: 0.248757  [145600/175341]\n",
      "loss: 0.284716  [147200/175341]\n",
      "loss: 0.409578  [148800/175341]\n",
      "loss: 0.914450  [150400/175341]\n",
      "loss: 0.293665  [152000/175341]\n",
      "loss: 0.584653  [153600/175341]\n",
      "loss: 0.417707  [155200/175341]\n",
      "loss: 0.846226  [156800/175341]\n",
      "loss: 0.482179  [158400/175341]\n",
      "loss: 0.358630  [160000/175341]\n",
      "loss: 0.342766  [161600/175341]\n",
      "loss: 0.742496  [163200/175341]\n",
      "loss: 0.665357  [164800/175341]\n",
      "loss: 0.652552  [166400/175341]\n",
      "loss: 0.320801  [168000/175341]\n",
      "loss: 0.244921  [169600/175341]\n",
      "loss: 0.394079  [171200/175341]\n",
      "loss: 0.627320  [172800/175341]\n",
      "loss: 0.458688  [174400/175341]\n",
      "Train Accuracy: 81.6141%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.568356, F1-score: 75.42%, Macro_F1-Score:  40.74%  \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.350787  [    0/175341]\n",
      "loss: 0.317089  [ 1600/175341]\n",
      "loss: 0.407745  [ 3200/175341]\n",
      "loss: 0.249639  [ 4800/175341]\n",
      "loss: 0.225037  [ 6400/175341]\n",
      "loss: 0.417280  [ 8000/175341]\n",
      "loss: 0.255769  [ 9600/175341]\n",
      "loss: 0.418028  [11200/175341]\n",
      "loss: 0.569584  [12800/175341]\n",
      "loss: 0.760183  [14400/175341]\n",
      "loss: 0.322289  [16000/175341]\n",
      "loss: 0.548850  [17600/175341]\n",
      "loss: 0.593570  [19200/175341]\n",
      "loss: 0.599302  [20800/175341]\n",
      "loss: 0.264557  [22400/175341]\n",
      "loss: 0.328903  [24000/175341]\n",
      "loss: 0.211817  [25600/175341]\n",
      "loss: 0.393671  [27200/175341]\n",
      "loss: 0.454928  [28800/175341]\n",
      "loss: 0.305686  [30400/175341]\n",
      "loss: 0.290114  [32000/175341]\n",
      "loss: 0.591879  [33600/175341]\n",
      "loss: 0.641749  [35200/175341]\n",
      "loss: 0.500638  [36800/175341]\n",
      "loss: 0.566466  [38400/175341]\n",
      "loss: 0.298760  [40000/175341]\n",
      "loss: 0.396108  [41600/175341]\n",
      "loss: 0.350262  [43200/175341]\n",
      "loss: 0.734372  [44800/175341]\n",
      "loss: 0.190015  [46400/175341]\n",
      "loss: 0.424404  [48000/175341]\n",
      "loss: 0.463496  [49600/175341]\n",
      "loss: 0.687081  [51200/175341]\n",
      "loss: 0.421326  [52800/175341]\n",
      "loss: 0.513938  [54400/175341]\n",
      "loss: 0.325171  [56000/175341]\n",
      "loss: 0.680317  [57600/175341]\n",
      "loss: 0.168802  [59200/175341]\n",
      "loss: 0.433973  [60800/175341]\n",
      "loss: 0.592136  [62400/175341]\n",
      "loss: 0.474905  [64000/175341]\n",
      "loss: 0.293563  [65600/175341]\n",
      "loss: 0.470063  [67200/175341]\n",
      "loss: 0.436581  [68800/175341]\n",
      "loss: 0.140477  [70400/175341]\n",
      "loss: 0.588002  [72000/175341]\n",
      "loss: 0.367723  [73600/175341]\n",
      "loss: 0.819888  [75200/175341]\n",
      "loss: 0.350453  [76800/175341]\n",
      "loss: 0.304880  [78400/175341]\n",
      "loss: 0.196584  [80000/175341]\n",
      "loss: 0.457967  [81600/175341]\n",
      "loss: 0.416654  [83200/175341]\n",
      "loss: 0.460645  [84800/175341]\n",
      "loss: 0.426353  [86400/175341]\n",
      "loss: 0.372143  [88000/175341]\n",
      "loss: 0.492728  [89600/175341]\n",
      "loss: 0.288766  [91200/175341]\n",
      "loss: 0.522448  [92800/175341]\n",
      "loss: 0.268221  [94400/175341]\n",
      "loss: 0.258648  [96000/175341]\n",
      "loss: 0.602732  [97600/175341]\n",
      "loss: 0.394059  [99200/175341]\n",
      "loss: 0.528503  [100800/175341]\n",
      "loss: 0.928824  [102400/175341]\n",
      "loss: 0.340628  [104000/175341]\n",
      "loss: 0.478295  [105600/175341]\n",
      "loss: 0.565362  [107200/175341]\n",
      "loss: 0.591681  [108800/175341]\n",
      "loss: 0.552094  [110400/175341]\n",
      "loss: 0.310316  [112000/175341]\n",
      "loss: 0.171654  [113600/175341]\n",
      "loss: 0.546309  [115200/175341]\n",
      "loss: 0.639639  [116800/175341]\n",
      "loss: 0.406300  [118400/175341]\n",
      "loss: 0.428309  [120000/175341]\n",
      "loss: 0.585896  [121600/175341]\n",
      "loss: 0.631712  [123200/175341]\n",
      "loss: 0.649472  [124800/175341]\n",
      "loss: 0.412188  [126400/175341]\n",
      "loss: 0.390727  [128000/175341]\n",
      "loss: 0.558005  [129600/175341]\n",
      "loss: 0.331058  [131200/175341]\n",
      "loss: 0.121592  [132800/175341]\n",
      "loss: 0.758422  [134400/175341]\n",
      "loss: 0.323643  [136000/175341]\n",
      "loss: 0.289032  [137600/175341]\n",
      "loss: 0.412885  [139200/175341]\n",
      "loss: 0.189109  [140800/175341]\n",
      "loss: 0.786336  [142400/175341]\n",
      "loss: 0.669198  [144000/175341]\n",
      "loss: 0.415396  [145600/175341]\n",
      "loss: 0.663893  [147200/175341]\n",
      "loss: 0.393199  [148800/175341]\n",
      "loss: 0.116469  [150400/175341]\n",
      "loss: 0.627017  [152000/175341]\n",
      "loss: 0.438379  [153600/175341]\n",
      "loss: 0.468552  [155200/175341]\n",
      "loss: 0.127702  [156800/175341]\n",
      "loss: 0.333995  [158400/175341]\n",
      "loss: 0.824123  [160000/175341]\n",
      "loss: 0.935034  [161600/175341]\n",
      "loss: 0.365658  [163200/175341]\n",
      "loss: 0.360423  [164800/175341]\n",
      "loss: 0.736239  [166400/175341]\n",
      "loss: 0.684949  [168000/175341]\n",
      "loss: 0.670642  [169600/175341]\n",
      "loss: 0.812173  [171200/175341]\n",
      "loss: 0.547155  [172800/175341]\n",
      "loss: 0.334969  [174400/175341]\n",
      "Train Accuracy: 81.5753%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.572673, F1-score: 75.70%, Macro_F1-Score:  41.06%  \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.343622  [    0/175341]\n",
      "loss: 0.427043  [ 1600/175341]\n",
      "loss: 0.341458  [ 3200/175341]\n",
      "loss: 0.423943  [ 4800/175341]\n",
      "loss: 0.923575  [ 6400/175341]\n",
      "loss: 0.401706  [ 8000/175341]\n",
      "loss: 0.394783  [ 9600/175341]\n",
      "loss: 0.575309  [11200/175341]\n",
      "loss: 0.589762  [12800/175341]\n",
      "loss: 0.660932  [14400/175341]\n",
      "loss: 0.473839  [16000/175341]\n",
      "loss: 0.524940  [17600/175341]\n",
      "loss: 0.326277  [19200/175341]\n",
      "loss: 0.084198  [20800/175341]\n",
      "loss: 0.434320  [22400/175341]\n",
      "loss: 0.465927  [24000/175341]\n",
      "loss: 0.879998  [25600/175341]\n",
      "loss: 0.477718  [27200/175341]\n",
      "loss: 0.324658  [28800/175341]\n",
      "loss: 0.501104  [30400/175341]\n",
      "loss: 0.409241  [32000/175341]\n",
      "loss: 0.239284  [33600/175341]\n",
      "loss: 0.727602  [35200/175341]\n",
      "loss: 0.642740  [36800/175341]\n",
      "loss: 0.603560  [38400/175341]\n",
      "loss: 0.753537  [40000/175341]\n",
      "loss: 0.404852  [41600/175341]\n",
      "loss: 0.515187  [43200/175341]\n",
      "loss: 1.016587  [44800/175341]\n",
      "loss: 0.517253  [46400/175341]\n",
      "loss: 0.426394  [48000/175341]\n",
      "loss: 0.691336  [49600/175341]\n",
      "loss: 0.200111  [51200/175341]\n",
      "loss: 0.280080  [52800/175341]\n",
      "loss: 0.949246  [54400/175341]\n",
      "loss: 0.353666  [56000/175341]\n",
      "loss: 0.532525  [57600/175341]\n",
      "loss: 0.383016  [59200/175341]\n",
      "loss: 0.439460  [60800/175341]\n",
      "loss: 0.442515  [62400/175341]\n",
      "loss: 0.528620  [64000/175341]\n",
      "loss: 0.680487  [65600/175341]\n",
      "loss: 0.386621  [67200/175341]\n",
      "loss: 0.234042  [68800/175341]\n",
      "loss: 0.520376  [70400/175341]\n",
      "loss: 0.441678  [72000/175341]\n",
      "loss: 0.359512  [73600/175341]\n",
      "loss: 0.189390  [75200/175341]\n",
      "loss: 0.614422  [76800/175341]\n",
      "loss: 0.305512  [78400/175341]\n",
      "loss: 0.672413  [80000/175341]\n",
      "loss: 0.119799  [81600/175341]\n",
      "loss: 0.536186  [83200/175341]\n",
      "loss: 0.423452  [84800/175341]\n",
      "loss: 0.807825  [86400/175341]\n",
      "loss: 0.433152  [88000/175341]\n",
      "loss: 0.424154  [89600/175341]\n",
      "loss: 0.574869  [91200/175341]\n",
      "loss: 0.278462  [92800/175341]\n",
      "loss: 0.565885  [94400/175341]\n",
      "loss: 0.440738  [96000/175341]\n",
      "loss: 0.464052  [97600/175341]\n",
      "loss: 0.290125  [99200/175341]\n",
      "loss: 0.582502  [100800/175341]\n",
      "loss: 0.313611  [102400/175341]\n",
      "loss: 0.516932  [104000/175341]\n",
      "loss: 0.248010  [105600/175341]\n",
      "loss: 0.798642  [107200/175341]\n",
      "loss: 0.896863  [108800/175341]\n",
      "loss: 0.426147  [110400/175341]\n",
      "loss: 0.282462  [112000/175341]\n",
      "loss: 0.334571  [113600/175341]\n",
      "loss: 0.540545  [115200/175341]\n",
      "loss: 0.276168  [116800/175341]\n",
      "loss: 0.362170  [118400/175341]\n",
      "loss: 0.497330  [120000/175341]\n",
      "loss: 0.133850  [121600/175341]\n",
      "loss: 0.332557  [123200/175341]\n",
      "loss: 0.422661  [124800/175341]\n",
      "loss: 0.501999  [126400/175341]\n",
      "loss: 0.088020  [128000/175341]\n",
      "loss: 0.814570  [129600/175341]\n",
      "loss: 0.595760  [131200/175341]\n",
      "loss: 0.499706  [132800/175341]\n",
      "loss: 0.564258  [134400/175341]\n",
      "loss: 0.221556  [136000/175341]\n",
      "loss: 0.191766  [137600/175341]\n",
      "loss: 0.405205  [139200/175341]\n",
      "loss: 1.022042  [140800/175341]\n",
      "loss: 0.329820  [142400/175341]\n",
      "loss: 0.186702  [144000/175341]\n",
      "loss: 0.377299  [145600/175341]\n",
      "loss: 0.514154  [147200/175341]\n",
      "loss: 0.590504  [148800/175341]\n",
      "loss: 0.410820  [150400/175341]\n",
      "loss: 0.412838  [152000/175341]\n",
      "loss: 0.658599  [153600/175341]\n",
      "loss: 0.796596  [155200/175341]\n",
      "loss: 0.524484  [156800/175341]\n",
      "loss: 0.414652  [158400/175341]\n",
      "loss: 0.609044  [160000/175341]\n",
      "loss: 0.275841  [161600/175341]\n",
      "loss: 0.489533  [163200/175341]\n",
      "loss: 0.391141  [164800/175341]\n",
      "loss: 0.400243  [166400/175341]\n",
      "loss: 0.273320  [168000/175341]\n",
      "loss: 0.469465  [169600/175341]\n",
      "loss: 0.168199  [171200/175341]\n",
      "loss: 0.655600  [172800/175341]\n",
      "loss: 0.831736  [174400/175341]\n",
      "Train Accuracy: 81.6227%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.577497, F1-score: 75.09%, Macro_F1-Score:  40.61%  \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.691108  [    0/175341]\n",
      "loss: 0.833539  [ 1600/175341]\n",
      "loss: 0.364428  [ 3200/175341]\n",
      "loss: 0.178110  [ 4800/175341]\n",
      "loss: 0.621304  [ 6400/175341]\n",
      "loss: 0.514026  [ 8000/175341]\n",
      "loss: 0.386270  [ 9600/175341]\n",
      "loss: 0.350267  [11200/175341]\n",
      "loss: 0.493820  [12800/175341]\n",
      "loss: 0.541627  [14400/175341]\n",
      "loss: 0.581332  [16000/175341]\n",
      "loss: 0.401880  [17600/175341]\n",
      "loss: 0.394684  [19200/175341]\n",
      "loss: 0.823896  [20800/175341]\n",
      "loss: 0.268557  [22400/175341]\n",
      "loss: 0.412846  [24000/175341]\n",
      "loss: 0.281616  [25600/175341]\n",
      "loss: 0.189987  [27200/175341]\n",
      "loss: 0.395789  [28800/175341]\n",
      "loss: 0.278834  [30400/175341]\n",
      "loss: 0.590402  [32000/175341]\n",
      "loss: 0.485388  [33600/175341]\n",
      "loss: 0.719386  [35200/175341]\n",
      "loss: 0.550742  [36800/175341]\n",
      "loss: 0.627879  [38400/175341]\n",
      "loss: 0.200292  [40000/175341]\n",
      "loss: 0.360297  [41600/175341]\n",
      "loss: 0.402585  [43200/175341]\n",
      "loss: 0.382899  [44800/175341]\n",
      "loss: 0.372834  [46400/175341]\n",
      "loss: 0.292907  [48000/175341]\n",
      "loss: 0.664110  [49600/175341]\n",
      "loss: 0.540665  [51200/175341]\n",
      "loss: 0.366409  [52800/175341]\n",
      "loss: 0.897150  [54400/175341]\n",
      "loss: 0.141509  [56000/175341]\n",
      "loss: 0.684250  [57600/175341]\n",
      "loss: 0.200347  [59200/175341]\n",
      "loss: 0.267952  [60800/175341]\n",
      "loss: 0.224454  [62400/175341]\n",
      "loss: 0.126803  [64000/175341]\n",
      "loss: 0.387079  [65600/175341]\n",
      "loss: 0.384357  [67200/175341]\n",
      "loss: 0.480801  [68800/175341]\n",
      "loss: 0.606461  [70400/175341]\n",
      "loss: 0.458127  [72000/175341]\n",
      "loss: 0.179264  [73600/175341]\n",
      "loss: 0.330183  [75200/175341]\n",
      "loss: 0.383114  [76800/175341]\n",
      "loss: 0.166700  [78400/175341]\n",
      "loss: 0.244441  [80000/175341]\n",
      "loss: 0.181702  [81600/175341]\n",
      "loss: 0.239179  [83200/175341]\n",
      "loss: 0.363997  [84800/175341]\n",
      "loss: 0.208721  [86400/175341]\n",
      "loss: 0.411995  [88000/175341]\n",
      "loss: 0.207158  [89600/175341]\n",
      "loss: 0.427799  [91200/175341]\n",
      "loss: 0.588932  [92800/175341]\n",
      "loss: 0.051166  [94400/175341]\n",
      "loss: 0.797898  [96000/175341]\n",
      "loss: 0.686964  [97600/175341]\n",
      "loss: 0.656912  [99200/175341]\n",
      "loss: 0.852558  [100800/175341]\n",
      "loss: 0.244166  [102400/175341]\n",
      "loss: 0.292436  [104000/175341]\n",
      "loss: 1.073432  [105600/175341]\n",
      "loss: 0.588842  [107200/175341]\n",
      "loss: 0.383418  [108800/175341]\n",
      "loss: 0.471238  [110400/175341]\n",
      "loss: 0.329738  [112000/175341]\n",
      "loss: 0.269803  [113600/175341]\n",
      "loss: 0.516670  [115200/175341]\n",
      "loss: 0.256861  [116800/175341]\n",
      "loss: 0.536416  [118400/175341]\n",
      "loss: 0.439576  [120000/175341]\n",
      "loss: 0.299299  [121600/175341]\n",
      "loss: 0.536391  [123200/175341]\n",
      "loss: 0.727595  [124800/175341]\n",
      "loss: 0.525762  [126400/175341]\n",
      "loss: 0.659731  [128000/175341]\n",
      "loss: 0.519477  [129600/175341]\n",
      "loss: 0.728560  [131200/175341]\n",
      "loss: 0.216662  [132800/175341]\n",
      "loss: 0.541008  [134400/175341]\n",
      "loss: 0.533684  [136000/175341]\n",
      "loss: 1.059776  [137600/175341]\n",
      "loss: 0.413211  [139200/175341]\n",
      "loss: 0.661184  [140800/175341]\n",
      "loss: 0.201444  [142400/175341]\n",
      "loss: 0.402431  [144000/175341]\n",
      "loss: 0.425950  [145600/175341]\n",
      "loss: 0.751804  [147200/175341]\n",
      "loss: 0.385302  [148800/175341]\n",
      "loss: 0.473894  [150400/175341]\n",
      "loss: 0.423880  [152000/175341]\n",
      "loss: 0.213607  [153600/175341]\n",
      "loss: 0.201014  [155200/175341]\n",
      "loss: 0.525701  [156800/175341]\n",
      "loss: 0.224605  [158400/175341]\n",
      "loss: 0.319661  [160000/175341]\n",
      "loss: 0.252640  [161600/175341]\n",
      "loss: 0.412128  [163200/175341]\n",
      "loss: 0.412880  [164800/175341]\n",
      "loss: 0.407369  [166400/175341]\n",
      "loss: 0.601120  [168000/175341]\n",
      "loss: 0.212504  [169600/175341]\n",
      "loss: 0.212478  [171200/175341]\n",
      "loss: 0.709216  [172800/175341]\n",
      "loss: 0.241483  [174400/175341]\n",
      "Train Accuracy: 81.6227%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.564184, F1-score: 75.25%, Macro_F1-Score:  40.74%  \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.317699  [    0/175341]\n",
      "loss: 0.597727  [ 1600/175341]\n",
      "loss: 0.430947  [ 3200/175341]\n",
      "loss: 0.534393  [ 4800/175341]\n",
      "loss: 0.570916  [ 6400/175341]\n",
      "loss: 0.414106  [ 8000/175341]\n",
      "loss: 0.294608  [ 9600/175341]\n",
      "loss: 0.568402  [11200/175341]\n",
      "loss: 0.517491  [12800/175341]\n",
      "loss: 0.337432  [14400/175341]\n",
      "loss: 0.580459  [16000/175341]\n",
      "loss: 1.041144  [17600/175341]\n",
      "loss: 0.503702  [19200/175341]\n",
      "loss: 0.185772  [20800/175341]\n",
      "loss: 0.493487  [22400/175341]\n",
      "loss: 0.658213  [24000/175341]\n",
      "loss: 0.059588  [25600/175341]\n",
      "loss: 0.511214  [27200/175341]\n",
      "loss: 0.243759  [28800/175341]\n",
      "loss: 0.120041  [30400/175341]\n",
      "loss: 0.548352  [32000/175341]\n",
      "loss: 0.238302  [33600/175341]\n",
      "loss: 0.577775  [35200/175341]\n",
      "loss: 1.355177  [36800/175341]\n",
      "loss: 0.087939  [38400/175341]\n",
      "loss: 0.377687  [40000/175341]\n",
      "loss: 0.427541  [41600/175341]\n",
      "loss: 0.624659  [43200/175341]\n",
      "loss: 0.353191  [44800/175341]\n",
      "loss: 0.337283  [46400/175341]\n",
      "loss: 0.486969  [48000/175341]\n",
      "loss: 0.663033  [49600/175341]\n",
      "loss: 0.315438  [51200/175341]\n",
      "loss: 0.307727  [52800/175341]\n",
      "loss: 0.402858  [54400/175341]\n",
      "loss: 0.179627  [56000/175341]\n",
      "loss: 0.116023  [57600/175341]\n",
      "loss: 0.240119  [59200/175341]\n",
      "loss: 0.437256  [60800/175341]\n",
      "loss: 0.547347  [62400/175341]\n",
      "loss: 0.294277  [64000/175341]\n",
      "loss: 0.522023  [65600/175341]\n",
      "loss: 0.135735  [67200/175341]\n",
      "loss: 0.333019  [68800/175341]\n",
      "loss: 0.860894  [70400/175341]\n",
      "loss: 0.584701  [72000/175341]\n",
      "loss: 0.335839  [73600/175341]\n",
      "loss: 0.676450  [75200/175341]\n",
      "loss: 0.239925  [76800/175341]\n",
      "loss: 0.164635  [78400/175341]\n",
      "loss: 0.382845  [80000/175341]\n",
      "loss: 0.290241  [81600/175341]\n",
      "loss: 0.354320  [83200/175341]\n",
      "loss: 0.445919  [84800/175341]\n",
      "loss: 0.393626  [86400/175341]\n",
      "loss: 0.712271  [88000/175341]\n",
      "loss: 0.453865  [89600/175341]\n",
      "loss: 0.331048  [91200/175341]\n",
      "loss: 0.100374  [92800/175341]\n",
      "loss: 0.438213  [94400/175341]\n",
      "loss: 0.833148  [96000/175341]\n",
      "loss: 0.451211  [97600/175341]\n",
      "loss: 0.466532  [99200/175341]\n",
      "loss: 0.240985  [100800/175341]\n",
      "loss: 0.257007  [102400/175341]\n",
      "loss: 0.354513  [104000/175341]\n",
      "loss: 0.464447  [105600/175341]\n",
      "loss: 0.444908  [107200/175341]\n",
      "loss: 0.246573  [108800/175341]\n",
      "loss: 0.463462  [110400/175341]\n",
      "loss: 0.463900  [112000/175341]\n",
      "loss: 0.856583  [113600/175341]\n",
      "loss: 0.935335  [115200/175341]\n",
      "loss: 0.847599  [116800/175341]\n",
      "loss: 0.580521  [118400/175341]\n",
      "loss: 0.467111  [120000/175341]\n",
      "loss: 0.325703  [121600/175341]\n",
      "loss: 0.569975  [123200/175341]\n",
      "loss: 0.346678  [124800/175341]\n",
      "loss: 0.681981  [126400/175341]\n",
      "loss: 1.002930  [128000/175341]\n",
      "loss: 0.549374  [129600/175341]\n",
      "loss: 0.464159  [131200/175341]\n",
      "loss: 0.237513  [132800/175341]\n",
      "loss: 0.513777  [134400/175341]\n",
      "loss: 0.102191  [136000/175341]\n",
      "loss: 0.514135  [137600/175341]\n",
      "loss: 0.103353  [139200/175341]\n",
      "loss: 0.356158  [140800/175341]\n",
      "loss: 0.507341  [142400/175341]\n",
      "loss: 0.253618  [144000/175341]\n",
      "loss: 0.656698  [145600/175341]\n",
      "loss: 0.302972  [147200/175341]\n",
      "loss: 0.129320  [148800/175341]\n",
      "loss: 0.247773  [150400/175341]\n",
      "loss: 0.336996  [152000/175341]\n",
      "loss: 0.307847  [153600/175341]\n",
      "loss: 0.531678  [155200/175341]\n",
      "loss: 0.477728  [156800/175341]\n",
      "loss: 0.266548  [158400/175341]\n",
      "loss: 0.291785  [160000/175341]\n",
      "loss: 0.762812  [161600/175341]\n",
      "loss: 0.183977  [163200/175341]\n",
      "loss: 0.309639  [164800/175341]\n",
      "loss: 0.345819  [166400/175341]\n",
      "loss: 0.604462  [168000/175341]\n",
      "loss: 0.425038  [169600/175341]\n",
      "loss: 0.508285  [171200/175341]\n",
      "loss: 0.463846  [172800/175341]\n",
      "loss: 0.341540  [174400/175341]\n",
      "Train Accuracy: 81.6307%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.557795, F1-score: 76.08%, Macro_F1-Score:  41.10%  \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.372248  [    0/175341]\n",
      "loss: 0.540229  [ 1600/175341]\n",
      "loss: 0.507491  [ 3200/175341]\n",
      "loss: 0.441321  [ 4800/175341]\n",
      "loss: 0.645235  [ 6400/175341]\n",
      "loss: 0.254787  [ 8000/175341]\n",
      "loss: 0.541976  [ 9600/175341]\n",
      "loss: 0.256231  [11200/175341]\n",
      "loss: 0.312782  [12800/175341]\n",
      "loss: 0.378556  [14400/175341]\n",
      "loss: 0.448255  [16000/175341]\n",
      "loss: 0.243026  [17600/175341]\n",
      "loss: 0.635413  [19200/175341]\n",
      "loss: 0.676052  [20800/175341]\n",
      "loss: 0.732469  [22400/175341]\n",
      "loss: 0.971450  [24000/175341]\n",
      "loss: 0.347053  [25600/175341]\n",
      "loss: 0.441227  [27200/175341]\n",
      "loss: 0.704574  [28800/175341]\n",
      "loss: 0.105450  [30400/175341]\n",
      "loss: 0.187083  [32000/175341]\n",
      "loss: 0.543491  [33600/175341]\n",
      "loss: 0.545746  [35200/175341]\n",
      "loss: 0.602795  [36800/175341]\n",
      "loss: 0.843601  [38400/175341]\n",
      "loss: 0.324580  [40000/175341]\n",
      "loss: 0.318060  [41600/175341]\n",
      "loss: 0.522979  [43200/175341]\n",
      "loss: 0.386109  [44800/175341]\n",
      "loss: 0.302491  [46400/175341]\n",
      "loss: 0.463274  [48000/175341]\n",
      "loss: 0.316763  [49600/175341]\n",
      "loss: 0.388808  [51200/175341]\n",
      "loss: 0.599637  [52800/175341]\n",
      "loss: 0.485808  [54400/175341]\n",
      "loss: 0.613181  [56000/175341]\n",
      "loss: 0.646548  [57600/175341]\n",
      "loss: 0.171187  [59200/175341]\n",
      "loss: 0.523178  [60800/175341]\n",
      "loss: 0.376926  [62400/175341]\n",
      "loss: 0.466067  [64000/175341]\n",
      "loss: 0.319242  [65600/175341]\n",
      "loss: 0.534912  [67200/175341]\n",
      "loss: 0.395714  [68800/175341]\n",
      "loss: 0.390207  [70400/175341]\n",
      "loss: 0.366462  [72000/175341]\n",
      "loss: 0.469860  [73600/175341]\n",
      "loss: 0.853428  [75200/175341]\n",
      "loss: 0.522477  [76800/175341]\n",
      "loss: 0.810740  [78400/175341]\n",
      "loss: 0.304676  [80000/175341]\n",
      "loss: 0.103486  [81600/175341]\n",
      "loss: 0.137628  [83200/175341]\n",
      "loss: 0.392975  [84800/175341]\n",
      "loss: 0.214198  [86400/175341]\n",
      "loss: 0.267777  [88000/175341]\n",
      "loss: 0.452733  [89600/175341]\n",
      "loss: 0.773088  [91200/175341]\n",
      "loss: 0.253088  [92800/175341]\n",
      "loss: 0.478272  [94400/175341]\n",
      "loss: 0.373295  [96000/175341]\n",
      "loss: 0.418191  [97600/175341]\n",
      "loss: 0.209080  [99200/175341]\n",
      "loss: 0.435060  [100800/175341]\n",
      "loss: 0.549502  [102400/175341]\n",
      "loss: 0.722529  [104000/175341]\n",
      "loss: 0.240974  [105600/175341]\n",
      "loss: 0.398137  [107200/175341]\n",
      "loss: 0.315556  [108800/175341]\n",
      "loss: 0.255943  [110400/175341]\n",
      "loss: 0.353742  [112000/175341]\n",
      "loss: 0.351487  [113600/175341]\n",
      "loss: 0.709426  [115200/175341]\n",
      "loss: 0.311926  [116800/175341]\n",
      "loss: 0.185355  [118400/175341]\n",
      "loss: 0.214469  [120000/175341]\n",
      "loss: 0.548855  [121600/175341]\n",
      "loss: 0.787438  [123200/175341]\n",
      "loss: 0.249675  [124800/175341]\n",
      "loss: 0.850659  [126400/175341]\n",
      "loss: 0.360761  [128000/175341]\n",
      "loss: 0.359258  [129600/175341]\n",
      "loss: 0.256089  [131200/175341]\n",
      "loss: 0.512528  [132800/175341]\n",
      "loss: 0.382194  [134400/175341]\n",
      "loss: 0.717495  [136000/175341]\n",
      "loss: 0.245623  [137600/175341]\n",
      "loss: 0.334705  [139200/175341]\n",
      "loss: 0.332636  [140800/175341]\n",
      "loss: 0.481335  [142400/175341]\n",
      "loss: 0.386036  [144000/175341]\n",
      "loss: 0.413428  [145600/175341]\n",
      "loss: 0.278705  [147200/175341]\n",
      "loss: 0.147980  [148800/175341]\n",
      "loss: 0.393108  [150400/175341]\n",
      "loss: 0.248534  [152000/175341]\n",
      "loss: 0.467407  [153600/175341]\n",
      "loss: 0.435389  [155200/175341]\n",
      "loss: 0.692640  [156800/175341]\n",
      "loss: 0.200309  [158400/175341]\n",
      "loss: 0.442316  [160000/175341]\n",
      "loss: 0.583832  [161600/175341]\n",
      "loss: 0.110755  [163200/175341]\n",
      "loss: 0.762938  [164800/175341]\n",
      "loss: 0.607419  [166400/175341]\n",
      "loss: 1.104676  [168000/175341]\n",
      "loss: 0.199426  [169600/175341]\n",
      "loss: 0.199422  [171200/175341]\n",
      "loss: 0.759072  [172800/175341]\n",
      "loss: 0.407926  [174400/175341]\n",
      "Train Accuracy: 81.6381%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.581090, F1-score: 74.66%, Macro_F1-Score:  41.15%  \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.697413  [    0/175341]\n",
      "loss: 0.226876  [ 1600/175341]\n",
      "loss: 0.079350  [ 3200/175341]\n",
      "loss: 0.149604  [ 4800/175341]\n",
      "loss: 0.596735  [ 6400/175341]\n",
      "loss: 0.866381  [ 8000/175341]\n",
      "loss: 0.384306  [ 9600/175341]\n",
      "loss: 0.832510  [11200/175341]\n",
      "loss: 0.279750  [12800/175341]\n",
      "loss: 0.321533  [14400/175341]\n",
      "loss: 0.096639  [16000/175341]\n",
      "loss: 0.725559  [17600/175341]\n",
      "loss: 0.630557  [19200/175341]\n",
      "loss: 0.438494  [20800/175341]\n",
      "loss: 0.728086  [22400/175341]\n",
      "loss: 0.567316  [24000/175341]\n",
      "loss: 0.224320  [25600/175341]\n",
      "loss: 0.438115  [27200/175341]\n",
      "loss: 0.285733  [28800/175341]\n",
      "loss: 0.419880  [30400/175341]\n",
      "loss: 0.729273  [32000/175341]\n",
      "loss: 0.739761  [33600/175341]\n",
      "loss: 0.299145  [35200/175341]\n",
      "loss: 0.340236  [36800/175341]\n",
      "loss: 0.949257  [38400/175341]\n",
      "loss: 0.513457  [40000/175341]\n",
      "loss: 0.342941  [41600/175341]\n",
      "loss: 0.224439  [43200/175341]\n",
      "loss: 0.343610  [44800/175341]\n",
      "loss: 0.404523  [46400/175341]\n",
      "loss: 1.000254  [48000/175341]\n",
      "loss: 0.375965  [49600/175341]\n",
      "loss: 0.487784  [51200/175341]\n",
      "loss: 0.283184  [52800/175341]\n",
      "loss: 0.776210  [54400/175341]\n",
      "loss: 0.887747  [56000/175341]\n",
      "loss: 0.354200  [57600/175341]\n",
      "loss: 0.292998  [59200/175341]\n",
      "loss: 0.342393  [60800/175341]\n",
      "loss: 0.258279  [62400/175341]\n",
      "loss: 0.321383  [64000/175341]\n",
      "loss: 0.255898  [65600/175341]\n",
      "loss: 0.672494  [67200/175341]\n",
      "loss: 0.892060  [68800/175341]\n",
      "loss: 0.206020  [70400/175341]\n",
      "loss: 0.380051  [72000/175341]\n",
      "loss: 0.607662  [73600/175341]\n",
      "loss: 0.800372  [75200/175341]\n",
      "loss: 0.465384  [76800/175341]\n",
      "loss: 0.268027  [78400/175341]\n",
      "loss: 0.845223  [80000/175341]\n",
      "loss: 0.290507  [81600/175341]\n",
      "loss: 0.265803  [83200/175341]\n",
      "loss: 0.385791  [84800/175341]\n",
      "loss: 0.526758  [86400/175341]\n",
      "loss: 0.516686  [88000/175341]\n",
      "loss: 0.128868  [89600/175341]\n",
      "loss: 0.521124  [91200/175341]\n",
      "loss: 0.177600  [92800/175341]\n",
      "loss: 1.186394  [94400/175341]\n",
      "loss: 0.475763  [96000/175341]\n",
      "loss: 0.596224  [97600/175341]\n",
      "loss: 0.337879  [99200/175341]\n",
      "loss: 0.387132  [100800/175341]\n",
      "loss: 0.355466  [102400/175341]\n",
      "loss: 0.372513  [104000/175341]\n",
      "loss: 0.305389  [105600/175341]\n",
      "loss: 0.611861  [107200/175341]\n",
      "loss: 0.225369  [108800/175341]\n",
      "loss: 0.833736  [110400/175341]\n",
      "loss: 0.356909  [112000/175341]\n",
      "loss: 0.497149  [113600/175341]\n",
      "loss: 0.163619  [115200/175341]\n",
      "loss: 0.419362  [116800/175341]\n",
      "loss: 0.226045  [118400/175341]\n",
      "loss: 0.454005  [120000/175341]\n",
      "loss: 0.271442  [121600/175341]\n",
      "loss: 0.190780  [123200/175341]\n",
      "loss: 0.706339  [124800/175341]\n",
      "loss: 0.462639  [126400/175341]\n",
      "loss: 0.106291  [128000/175341]\n",
      "loss: 0.627393  [129600/175341]\n",
      "loss: 0.440800  [131200/175341]\n",
      "loss: 0.311184  [132800/175341]\n",
      "loss: 0.724779  [134400/175341]\n",
      "loss: 0.410016  [136000/175341]\n",
      "loss: 0.347919  [137600/175341]\n",
      "loss: 0.598668  [139200/175341]\n",
      "loss: 0.227358  [140800/175341]\n",
      "loss: 0.154229  [142400/175341]\n",
      "loss: 0.355496  [144000/175341]\n",
      "loss: 0.198147  [145600/175341]\n",
      "loss: 0.340589  [147200/175341]\n",
      "loss: 0.384347  [148800/175341]\n",
      "loss: 0.168681  [150400/175341]\n",
      "loss: 0.452494  [152000/175341]\n",
      "loss: 0.715970  [153600/175341]\n",
      "loss: 0.508088  [155200/175341]\n",
      "loss: 0.499221  [156800/175341]\n",
      "loss: 0.382036  [158400/175341]\n",
      "loss: 1.319229  [160000/175341]\n",
      "loss: 0.639927  [161600/175341]\n",
      "loss: 0.255373  [163200/175341]\n",
      "loss: 0.739458  [164800/175341]\n",
      "loss: 0.555115  [166400/175341]\n",
      "loss: 0.503665  [168000/175341]\n",
      "loss: 0.474538  [169600/175341]\n",
      "loss: 0.224123  [171200/175341]\n",
      "loss: 0.578772  [172800/175341]\n",
      "loss: 0.556587  [174400/175341]\n",
      "Train Accuracy: 81.6706%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.546803, F1-score: 77.06%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.432126  [    0/175341]\n",
      "loss: 0.339645  [ 1600/175341]\n",
      "loss: 0.571001  [ 3200/175341]\n",
      "loss: 0.400880  [ 4800/175341]\n",
      "loss: 0.457186  [ 6400/175341]\n",
      "loss: 0.493385  [ 8000/175341]\n",
      "loss: 0.456363  [ 9600/175341]\n",
      "loss: 0.673348  [11200/175341]\n",
      "loss: 0.558596  [12800/175341]\n",
      "loss: 0.474789  [14400/175341]\n",
      "loss: 0.129319  [16000/175341]\n",
      "loss: 0.395058  [17600/175341]\n",
      "loss: 0.268039  [19200/175341]\n",
      "loss: 0.201258  [20800/175341]\n",
      "loss: 0.461889  [22400/175341]\n",
      "loss: 0.293569  [24000/175341]\n",
      "loss: 0.388556  [25600/175341]\n",
      "loss: 0.392896  [27200/175341]\n",
      "loss: 0.512925  [28800/175341]\n",
      "loss: 0.210257  [30400/175341]\n",
      "loss: 0.612716  [32000/175341]\n",
      "loss: 0.544937  [33600/175341]\n",
      "loss: 0.477875  [35200/175341]\n",
      "loss: 0.463052  [36800/175341]\n",
      "loss: 0.228027  [38400/175341]\n",
      "loss: 0.100319  [40000/175341]\n",
      "loss: 0.291208  [41600/175341]\n",
      "loss: 0.352833  [43200/175341]\n",
      "loss: 0.165587  [44800/175341]\n",
      "loss: 0.441575  [46400/175341]\n",
      "loss: 0.543988  [48000/175341]\n",
      "loss: 0.524408  [49600/175341]\n",
      "loss: 0.382813  [51200/175341]\n",
      "loss: 0.572177  [52800/175341]\n",
      "loss: 0.594032  [54400/175341]\n",
      "loss: 0.372946  [56000/175341]\n",
      "loss: 0.341303  [57600/175341]\n",
      "loss: 0.400932  [59200/175341]\n",
      "loss: 0.139945  [60800/175341]\n",
      "loss: 0.087359  [62400/175341]\n",
      "loss: 0.312062  [64000/175341]\n",
      "loss: 0.273424  [65600/175341]\n",
      "loss: 0.536759  [67200/175341]\n",
      "loss: 0.499703  [68800/175341]\n",
      "loss: 0.648236  [70400/175341]\n",
      "loss: 0.180534  [72000/175341]\n",
      "loss: 0.442049  [73600/175341]\n",
      "loss: 0.916088  [75200/175341]\n",
      "loss: 0.738985  [76800/175341]\n",
      "loss: 0.281710  [78400/175341]\n",
      "loss: 0.394347  [80000/175341]\n",
      "loss: 0.355835  [81600/175341]\n",
      "loss: 0.521831  [83200/175341]\n",
      "loss: 0.176221  [84800/175341]\n",
      "loss: 0.581428  [86400/175341]\n",
      "loss: 0.349381  [88000/175341]\n",
      "loss: 0.351159  [89600/175341]\n",
      "loss: 0.507731  [91200/175341]\n",
      "loss: 0.669554  [92800/175341]\n",
      "loss: 0.071654  [94400/175341]\n",
      "loss: 0.524534  [96000/175341]\n",
      "loss: 0.564190  [97600/175341]\n",
      "loss: 0.352676  [99200/175341]\n",
      "loss: 0.327129  [100800/175341]\n",
      "loss: 0.503727  [102400/175341]\n",
      "loss: 0.430224  [104000/175341]\n",
      "loss: 0.307079  [105600/175341]\n",
      "loss: 0.389159  [107200/175341]\n",
      "loss: 0.419023  [108800/175341]\n",
      "loss: 0.375431  [110400/175341]\n",
      "loss: 0.724640  [112000/175341]\n",
      "loss: 0.514446  [113600/175341]\n",
      "loss: 0.361673  [115200/175341]\n",
      "loss: 0.273964  [116800/175341]\n",
      "loss: 0.904553  [118400/175341]\n",
      "loss: 0.751695  [120000/175341]\n",
      "loss: 0.268359  [121600/175341]\n",
      "loss: 0.343987  [123200/175341]\n",
      "loss: 0.910887  [124800/175341]\n",
      "loss: 0.499789  [126400/175341]\n",
      "loss: 0.521161  [128000/175341]\n",
      "loss: 0.184851  [129600/175341]\n",
      "loss: 1.125123  [131200/175341]\n",
      "loss: 0.171840  [132800/175341]\n",
      "loss: 0.584750  [134400/175341]\n",
      "loss: 0.488094  [136000/175341]\n",
      "loss: 0.114458  [137600/175341]\n",
      "loss: 0.579714  [139200/175341]\n",
      "loss: 0.530868  [140800/175341]\n",
      "loss: 0.729037  [142400/175341]\n",
      "loss: 0.560132  [144000/175341]\n",
      "loss: 0.201109  [145600/175341]\n",
      "loss: 0.324799  [147200/175341]\n",
      "loss: 0.297672  [148800/175341]\n",
      "loss: 0.301365  [150400/175341]\n",
      "loss: 0.376888  [152000/175341]\n",
      "loss: 0.147177  [153600/175341]\n",
      "loss: 0.132225  [155200/175341]\n",
      "loss: 0.207187  [156800/175341]\n",
      "loss: 0.634970  [158400/175341]\n",
      "loss: 0.707993  [160000/175341]\n",
      "loss: 0.293648  [161600/175341]\n",
      "loss: 0.298845  [163200/175341]\n",
      "loss: 0.519797  [164800/175341]\n",
      "loss: 0.401401  [166400/175341]\n",
      "loss: 0.936574  [168000/175341]\n",
      "loss: 0.193404  [169600/175341]\n",
      "loss: 0.401063  [171200/175341]\n",
      "loss: 0.296788  [172800/175341]\n",
      "loss: 0.546265  [174400/175341]\n",
      "Train Accuracy: 81.6295%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.562639, F1-score: 75.58%, Macro_F1-Score:  40.89%  \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.458617  [    0/175341]\n",
      "loss: 0.291112  [ 1600/175341]\n",
      "loss: 0.280342  [ 3200/175341]\n",
      "loss: 0.326928  [ 4800/175341]\n",
      "loss: 0.249745  [ 6400/175341]\n",
      "loss: 0.418387  [ 8000/175341]\n",
      "loss: 0.511523  [ 9600/175341]\n",
      "loss: 0.397099  [11200/175341]\n",
      "loss: 0.155329  [12800/175341]\n",
      "loss: 0.735271  [14400/175341]\n",
      "loss: 0.382504  [16000/175341]\n",
      "loss: 0.467963  [17600/175341]\n",
      "loss: 0.394023  [19200/175341]\n",
      "loss: 0.475732  [20800/175341]\n",
      "loss: 0.474312  [22400/175341]\n",
      "loss: 0.351425  [24000/175341]\n",
      "loss: 0.103052  [25600/175341]\n",
      "loss: 0.284466  [27200/175341]\n",
      "loss: 0.188149  [28800/175341]\n",
      "loss: 0.251711  [30400/175341]\n",
      "loss: 0.342864  [32000/175341]\n",
      "loss: 0.259681  [33600/175341]\n",
      "loss: 0.342339  [35200/175341]\n",
      "loss: 0.234203  [36800/175341]\n",
      "loss: 0.258627  [38400/175341]\n",
      "loss: 0.377745  [40000/175341]\n",
      "loss: 0.302988  [41600/175341]\n",
      "loss: 0.348343  [43200/175341]\n",
      "loss: 0.702003  [44800/175341]\n",
      "loss: 0.200647  [46400/175341]\n",
      "loss: 0.376021  [48000/175341]\n",
      "loss: 0.534391  [49600/175341]\n",
      "loss: 0.304032  [51200/175341]\n",
      "loss: 0.229230  [52800/175341]\n",
      "loss: 0.492581  [54400/175341]\n",
      "loss: 0.691724  [56000/175341]\n",
      "loss: 0.644374  [57600/175341]\n",
      "loss: 0.418696  [59200/175341]\n",
      "loss: 0.282739  [60800/175341]\n",
      "loss: 0.343708  [62400/175341]\n",
      "loss: 0.465318  [64000/175341]\n",
      "loss: 0.085499  [65600/175341]\n",
      "loss: 0.412382  [67200/175341]\n",
      "loss: 0.625367  [68800/175341]\n",
      "loss: 0.301318  [70400/175341]\n",
      "loss: 0.581195  [72000/175341]\n",
      "loss: 0.233371  [73600/175341]\n",
      "loss: 0.393897  [75200/175341]\n",
      "loss: 0.286587  [76800/175341]\n",
      "loss: 0.295957  [78400/175341]\n",
      "loss: 0.284998  [80000/175341]\n",
      "loss: 0.817332  [81600/175341]\n",
      "loss: 0.227541  [83200/175341]\n",
      "loss: 0.436555  [84800/175341]\n",
      "loss: 0.324304  [86400/175341]\n",
      "loss: 0.294027  [88000/175341]\n",
      "loss: 1.083135  [89600/175341]\n",
      "loss: 0.385864  [91200/175341]\n",
      "loss: 0.160922  [92800/175341]\n",
      "loss: 0.395498  [94400/175341]\n",
      "loss: 0.732810  [96000/175341]\n",
      "loss: 0.654728  [97600/175341]\n",
      "loss: 0.357622  [99200/175341]\n",
      "loss: 0.255787  [100800/175341]\n",
      "loss: 0.285769  [102400/175341]\n",
      "loss: 0.354740  [104000/175341]\n",
      "loss: 0.451658  [105600/175341]\n",
      "loss: 0.354087  [107200/175341]\n",
      "loss: 0.463350  [108800/175341]\n",
      "loss: 0.274613  [110400/175341]\n",
      "loss: 0.313734  [112000/175341]\n",
      "loss: 0.708543  [113600/175341]\n",
      "loss: 0.309978  [115200/175341]\n",
      "loss: 0.422163  [116800/175341]\n",
      "loss: 0.477898  [118400/175341]\n",
      "loss: 0.853671  [120000/175341]\n",
      "loss: 0.258282  [121600/175341]\n",
      "loss: 0.538020  [123200/175341]\n",
      "loss: 0.190684  [124800/175341]\n",
      "loss: 0.578874  [126400/175341]\n",
      "loss: 0.895536  [128000/175341]\n",
      "loss: 0.471731  [129600/175341]\n",
      "loss: 0.359797  [131200/175341]\n",
      "loss: 0.168198  [132800/175341]\n",
      "loss: 0.359233  [134400/175341]\n",
      "loss: 0.502845  [136000/175341]\n",
      "loss: 0.568929  [137600/175341]\n",
      "loss: 0.445876  [139200/175341]\n",
      "loss: 0.069263  [140800/175341]\n",
      "loss: 0.745318  [142400/175341]\n",
      "loss: 0.548429  [144000/175341]\n",
      "loss: 0.421952  [145600/175341]\n",
      "loss: 0.097223  [147200/175341]\n",
      "loss: 0.183589  [148800/175341]\n",
      "loss: 0.579038  [150400/175341]\n",
      "loss: 0.318453  [152000/175341]\n",
      "loss: 0.419248  [153600/175341]\n",
      "loss: 0.260530  [155200/175341]\n",
      "loss: 0.245788  [156800/175341]\n",
      "loss: 0.375226  [158400/175341]\n",
      "loss: 0.547378  [160000/175341]\n",
      "loss: 0.443561  [161600/175341]\n",
      "loss: 0.381082  [163200/175341]\n",
      "loss: 0.437145  [164800/175341]\n",
      "loss: 0.335324  [166400/175341]\n",
      "loss: 0.414790  [168000/175341]\n",
      "loss: 0.549311  [169600/175341]\n",
      "loss: 0.400998  [171200/175341]\n",
      "loss: 0.710371  [172800/175341]\n",
      "loss: 0.463688  [174400/175341]\n",
      "Train Accuracy: 81.6860%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.593511, F1-score: 74.85%, Macro_F1-Score:  40.49%  \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.598695  [    0/175341]\n",
      "loss: 0.599482  [ 1600/175341]\n",
      "loss: 0.344125  [ 3200/175341]\n",
      "loss: 0.193123  [ 4800/175341]\n",
      "loss: 0.353884  [ 6400/175341]\n",
      "loss: 0.365797  [ 8000/175341]\n",
      "loss: 0.730021  [ 9600/175341]\n",
      "loss: 0.339012  [11200/175341]\n",
      "loss: 0.698340  [12800/175341]\n",
      "loss: 0.979271  [14400/175341]\n",
      "loss: 0.938769  [16000/175341]\n",
      "loss: 0.552967  [17600/175341]\n",
      "loss: 0.385650  [19200/175341]\n",
      "loss: 0.476838  [20800/175341]\n",
      "loss: 0.625200  [22400/175341]\n",
      "loss: 0.487338  [24000/175341]\n",
      "loss: 0.377980  [25600/175341]\n",
      "loss: 0.443375  [27200/175341]\n",
      "loss: 0.383066  [28800/175341]\n",
      "loss: 0.290260  [30400/175341]\n",
      "loss: 0.589283  [32000/175341]\n",
      "loss: 0.327313  [33600/175341]\n",
      "loss: 0.371669  [35200/175341]\n",
      "loss: 0.648060  [36800/175341]\n",
      "loss: 0.072714  [38400/175341]\n",
      "loss: 0.650221  [40000/175341]\n",
      "loss: 0.215206  [41600/175341]\n",
      "loss: 0.538953  [43200/175341]\n",
      "loss: 0.321229  [44800/175341]\n",
      "loss: 0.366602  [46400/175341]\n",
      "loss: 0.480252  [48000/175341]\n",
      "loss: 0.524075  [49600/175341]\n",
      "loss: 0.439048  [51200/175341]\n",
      "loss: 0.444815  [52800/175341]\n",
      "loss: 0.599861  [54400/175341]\n",
      "loss: 0.492598  [56000/175341]\n",
      "loss: 0.315633  [57600/175341]\n",
      "loss: 0.715323  [59200/175341]\n",
      "loss: 0.386779  [60800/175341]\n",
      "loss: 0.378059  [62400/175341]\n",
      "loss: 0.424295  [64000/175341]\n",
      "loss: 0.711797  [65600/175341]\n",
      "loss: 0.378492  [67200/175341]\n",
      "loss: 0.630522  [68800/175341]\n",
      "loss: 0.573436  [70400/175341]\n",
      "loss: 0.429367  [72000/175341]\n",
      "loss: 0.502403  [73600/175341]\n",
      "loss: 0.332824  [75200/175341]\n",
      "loss: 0.732928  [76800/175341]\n",
      "loss: 0.626313  [78400/175341]\n",
      "loss: 0.436567  [80000/175341]\n",
      "loss: 1.060194  [81600/175341]\n",
      "loss: 0.742979  [83200/175341]\n",
      "loss: 0.347205  [84800/175341]\n",
      "loss: 0.717697  [86400/175341]\n",
      "loss: 0.940261  [88000/175341]\n",
      "loss: 0.719377  [89600/175341]\n",
      "loss: 0.846411  [91200/175341]\n",
      "loss: 0.441009  [92800/175341]\n",
      "loss: 0.152731  [94400/175341]\n",
      "loss: 0.551634  [96000/175341]\n",
      "loss: 0.389385  [97600/175341]\n",
      "loss: 0.208542  [99200/175341]\n",
      "loss: 0.709657  [100800/175341]\n",
      "loss: 0.737461  [102400/175341]\n",
      "loss: 0.444766  [104000/175341]\n",
      "loss: 0.566730  [105600/175341]\n",
      "loss: 0.125106  [107200/175341]\n",
      "loss: 0.566078  [108800/175341]\n",
      "loss: 0.558512  [110400/175341]\n",
      "loss: 0.533276  [112000/175341]\n",
      "loss: 0.397165  [113600/175341]\n",
      "loss: 0.589774  [115200/175341]\n",
      "loss: 0.663476  [116800/175341]\n",
      "loss: 0.123041  [118400/175341]\n",
      "loss: 0.464969  [120000/175341]\n",
      "loss: 0.423184  [121600/175341]\n",
      "loss: 0.643707  [123200/175341]\n",
      "loss: 0.490274  [124800/175341]\n",
      "loss: 0.193528  [126400/175341]\n",
      "loss: 0.407546  [128000/175341]\n",
      "loss: 0.490901  [129600/175341]\n",
      "loss: 0.651842  [131200/175341]\n",
      "loss: 0.250456  [132800/175341]\n",
      "loss: 0.305180  [134400/175341]\n",
      "loss: 0.285807  [136000/175341]\n",
      "loss: 0.695339  [137600/175341]\n",
      "loss: 0.426858  [139200/175341]\n",
      "loss: 0.256710  [140800/175341]\n",
      "loss: 0.315348  [142400/175341]\n",
      "loss: 0.484720  [144000/175341]\n",
      "loss: 0.440240  [145600/175341]\n",
      "loss: 0.573244  [147200/175341]\n",
      "loss: 0.371474  [148800/175341]\n",
      "loss: 0.791731  [150400/175341]\n",
      "loss: 0.391887  [152000/175341]\n",
      "loss: 0.354439  [153600/175341]\n",
      "loss: 0.292000  [155200/175341]\n",
      "loss: 0.774396  [156800/175341]\n",
      "loss: 0.420350  [158400/175341]\n",
      "loss: 0.453738  [160000/175341]\n",
      "loss: 0.357948  [161600/175341]\n",
      "loss: 0.264349  [163200/175341]\n",
      "loss: 0.532907  [164800/175341]\n",
      "loss: 0.389379  [166400/175341]\n",
      "loss: 0.577185  [168000/175341]\n",
      "loss: 0.607157  [169600/175341]\n",
      "loss: 0.785488  [171200/175341]\n",
      "loss: 0.277414  [172800/175341]\n",
      "loss: 0.666326  [174400/175341]\n",
      "Train Accuracy: 81.6774%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.558437, F1-score: 75.53%, Macro_F1-Score:  40.81%  \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.743132  [    0/175341]\n",
      "loss: 0.376316  [ 1600/175341]\n",
      "loss: 0.440519  [ 3200/175341]\n",
      "loss: 0.536169  [ 4800/175341]\n",
      "loss: 0.480573  [ 6400/175341]\n",
      "loss: 0.255985  [ 8000/175341]\n",
      "loss: 0.465213  [ 9600/175341]\n",
      "loss: 0.771929  [11200/175341]\n",
      "loss: 0.618865  [12800/175341]\n",
      "loss: 0.661470  [14400/175341]\n",
      "loss: 0.341688  [16000/175341]\n",
      "loss: 0.238688  [17600/175341]\n",
      "loss: 0.217606  [19200/175341]\n",
      "loss: 0.757679  [20800/175341]\n",
      "loss: 0.593394  [22400/175341]\n",
      "loss: 0.151061  [24000/175341]\n",
      "loss: 0.317321  [25600/175341]\n",
      "loss: 0.166786  [27200/175341]\n",
      "loss: 0.116708  [28800/175341]\n",
      "loss: 0.326549  [30400/175341]\n",
      "loss: 0.507997  [32000/175341]\n",
      "loss: 0.579435  [33600/175341]\n",
      "loss: 0.901803  [35200/175341]\n",
      "loss: 0.896469  [36800/175341]\n",
      "loss: 0.374032  [38400/175341]\n",
      "loss: 0.279591  [40000/175341]\n",
      "loss: 0.210117  [41600/175341]\n",
      "loss: 0.502140  [43200/175341]\n",
      "loss: 0.179963  [44800/175341]\n",
      "loss: 0.887280  [46400/175341]\n",
      "loss: 0.439500  [48000/175341]\n",
      "loss: 0.909409  [49600/175341]\n",
      "loss: 0.646142  [51200/175341]\n",
      "loss: 0.415583  [52800/175341]\n",
      "loss: 0.254546  [54400/175341]\n",
      "loss: 0.108994  [56000/175341]\n",
      "loss: 0.355065  [57600/175341]\n",
      "loss: 0.456837  [59200/175341]\n",
      "loss: 0.433703  [60800/175341]\n",
      "loss: 0.326429  [62400/175341]\n",
      "loss: 0.385355  [64000/175341]\n",
      "loss: 0.654869  [65600/175341]\n",
      "loss: 0.274277  [67200/175341]\n",
      "loss: 0.540891  [68800/175341]\n",
      "loss: 0.285198  [70400/175341]\n",
      "loss: 0.608939  [72000/175341]\n",
      "loss: 0.288931  [73600/175341]\n",
      "loss: 0.219008  [75200/175341]\n",
      "loss: 0.197605  [76800/175341]\n",
      "loss: 0.177736  [78400/175341]\n",
      "loss: 0.604130  [80000/175341]\n",
      "loss: 0.311622  [81600/175341]\n",
      "loss: 0.282126  [83200/175341]\n",
      "loss: 0.633593  [84800/175341]\n",
      "loss: 0.522949  [86400/175341]\n",
      "loss: 0.226771  [88000/175341]\n",
      "loss: 0.319035  [89600/175341]\n",
      "loss: 0.239964  [91200/175341]\n",
      "loss: 0.410570  [92800/175341]\n",
      "loss: 0.304459  [94400/175341]\n",
      "loss: 0.874777  [96000/175341]\n",
      "loss: 0.691899  [97600/175341]\n",
      "loss: 0.397014  [99200/175341]\n",
      "loss: 0.417295  [100800/175341]\n",
      "loss: 0.896848  [102400/175341]\n",
      "loss: 0.945940  [104000/175341]\n",
      "loss: 0.479452  [105600/175341]\n",
      "loss: 0.630456  [107200/175341]\n",
      "loss: 0.466305  [108800/175341]\n",
      "loss: 0.358491  [110400/175341]\n",
      "loss: 0.236936  [112000/175341]\n",
      "loss: 0.427820  [113600/175341]\n",
      "loss: 0.577960  [115200/175341]\n",
      "loss: 0.183467  [116800/175341]\n",
      "loss: 0.277884  [118400/175341]\n",
      "loss: 0.598887  [120000/175341]\n",
      "loss: 0.404307  [121600/175341]\n",
      "loss: 0.857721  [123200/175341]\n",
      "loss: 0.823442  [124800/175341]\n",
      "loss: 0.777651  [126400/175341]\n",
      "loss: 0.603236  [128000/175341]\n",
      "loss: 0.691140  [129600/175341]\n",
      "loss: 0.505789  [131200/175341]\n",
      "loss: 0.242301  [132800/175341]\n",
      "loss: 0.336897  [134400/175341]\n",
      "loss: 0.404721  [136000/175341]\n",
      "loss: 0.432097  [137600/175341]\n",
      "loss: 0.082918  [139200/175341]\n",
      "loss: 0.361193  [140800/175341]\n",
      "loss: 0.499442  [142400/175341]\n",
      "loss: 0.500221  [144000/175341]\n",
      "loss: 0.329592  [145600/175341]\n",
      "loss: 0.380918  [147200/175341]\n",
      "loss: 0.290233  [148800/175341]\n",
      "loss: 0.464384  [150400/175341]\n",
      "loss: 0.238447  [152000/175341]\n",
      "loss: 0.537639  [153600/175341]\n",
      "loss: 0.592487  [155200/175341]\n",
      "loss: 0.460177  [156800/175341]\n",
      "loss: 0.607996  [158400/175341]\n",
      "loss: 0.557061  [160000/175341]\n",
      "loss: 0.630410  [161600/175341]\n",
      "loss: 0.304727  [163200/175341]\n",
      "loss: 0.710538  [164800/175341]\n",
      "loss: 0.334687  [166400/175341]\n",
      "loss: 0.352275  [168000/175341]\n",
      "loss: 0.806465  [169600/175341]\n",
      "loss: 0.335460  [171200/175341]\n",
      "loss: 0.480775  [172800/175341]\n",
      "loss: 0.486075  [174400/175341]\n",
      "Train Accuracy: 81.6865%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.550970, F1-score: 76.82%, Macro_F1-Score:  41.48%  \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.513673  [    0/175341]\n",
      "loss: 0.622872  [ 1600/175341]\n",
      "loss: 0.235876  [ 3200/175341]\n",
      "loss: 0.437689  [ 4800/175341]\n",
      "loss: 0.410877  [ 6400/175341]\n",
      "loss: 0.682749  [ 8000/175341]\n",
      "loss: 0.785131  [ 9600/175341]\n",
      "loss: 0.225909  [11200/175341]\n",
      "loss: 0.510804  [12800/175341]\n",
      "loss: 0.466610  [14400/175341]\n",
      "loss: 0.252759  [16000/175341]\n",
      "loss: 0.374277  [17600/175341]\n",
      "loss: 0.730840  [19200/175341]\n",
      "loss: 0.465346  [20800/175341]\n",
      "loss: 0.412093  [22400/175341]\n",
      "loss: 0.614536  [24000/175341]\n",
      "loss: 0.336370  [25600/175341]\n",
      "loss: 0.320277  [27200/175341]\n",
      "loss: 0.557655  [28800/175341]\n",
      "loss: 0.392034  [30400/175341]\n",
      "loss: 0.475056  [32000/175341]\n",
      "loss: 0.261990  [33600/175341]\n",
      "loss: 0.244138  [35200/175341]\n",
      "loss: 0.347733  [36800/175341]\n",
      "loss: 0.129163  [38400/175341]\n",
      "loss: 0.454688  [40000/175341]\n",
      "loss: 0.551925  [41600/175341]\n",
      "loss: 0.554786  [43200/175341]\n",
      "loss: 0.523211  [44800/175341]\n",
      "loss: 0.288759  [46400/175341]\n",
      "loss: 0.291509  [48000/175341]\n",
      "loss: 0.342902  [49600/175341]\n",
      "loss: 0.788183  [51200/175341]\n",
      "loss: 0.661950  [52800/175341]\n",
      "loss: 0.869040  [54400/175341]\n",
      "loss: 0.408933  [56000/175341]\n",
      "loss: 0.427866  [57600/175341]\n",
      "loss: 0.445712  [59200/175341]\n",
      "loss: 0.201106  [60800/175341]\n",
      "loss: 0.349637  [62400/175341]\n",
      "loss: 0.487926  [64000/175341]\n",
      "loss: 0.482542  [65600/175341]\n",
      "loss: 0.643930  [67200/175341]\n",
      "loss: 0.105648  [68800/175341]\n",
      "loss: 0.221752  [70400/175341]\n",
      "loss: 0.745420  [72000/175341]\n",
      "loss: 0.680117  [73600/175341]\n",
      "loss: 0.250658  [75200/175341]\n",
      "loss: 0.492896  [76800/175341]\n",
      "loss: 0.578649  [78400/175341]\n",
      "loss: 0.942305  [80000/175341]\n",
      "loss: 0.298696  [81600/175341]\n",
      "loss: 0.255729  [83200/175341]\n",
      "loss: 0.493833  [84800/175341]\n",
      "loss: 0.622236  [86400/175341]\n",
      "loss: 0.497028  [88000/175341]\n",
      "loss: 0.396883  [89600/175341]\n",
      "loss: 0.503812  [91200/175341]\n",
      "loss: 0.172445  [92800/175341]\n",
      "loss: 0.184300  [94400/175341]\n",
      "loss: 0.637496  [96000/175341]\n",
      "loss: 0.464912  [97600/175341]\n",
      "loss: 0.762782  [99200/175341]\n",
      "loss: 0.608454  [100800/175341]\n",
      "loss: 0.195705  [102400/175341]\n",
      "loss: 0.548078  [104000/175341]\n",
      "loss: 0.130698  [105600/175341]\n",
      "loss: 0.422060  [107200/175341]\n",
      "loss: 0.835719  [108800/175341]\n",
      "loss: 0.557553  [110400/175341]\n",
      "loss: 0.365258  [112000/175341]\n",
      "loss: 0.490134  [113600/175341]\n",
      "loss: 0.498514  [115200/175341]\n",
      "loss: 0.402279  [116800/175341]\n",
      "loss: 0.131151  [118400/175341]\n",
      "loss: 0.397126  [120000/175341]\n",
      "loss: 0.747804  [121600/175341]\n",
      "loss: 0.438366  [123200/175341]\n",
      "loss: 0.568584  [124800/175341]\n",
      "loss: 0.485258  [126400/175341]\n",
      "loss: 0.450629  [128000/175341]\n",
      "loss: 0.581239  [129600/175341]\n",
      "loss: 0.413326  [131200/175341]\n",
      "loss: 0.558166  [132800/175341]\n",
      "loss: 0.588045  [134400/175341]\n",
      "loss: 0.504384  [136000/175341]\n",
      "loss: 0.626388  [137600/175341]\n",
      "loss: 0.459461  [139200/175341]\n",
      "loss: 0.173126  [140800/175341]\n",
      "loss: 0.034663  [142400/175341]\n",
      "loss: 0.228228  [144000/175341]\n",
      "loss: 0.211078  [145600/175341]\n",
      "loss: 0.228015  [147200/175341]\n",
      "loss: 0.373795  [148800/175341]\n",
      "loss: 0.358491  [150400/175341]\n",
      "loss: 0.407495  [152000/175341]\n",
      "loss: 0.342979  [153600/175341]\n",
      "loss: 0.654741  [155200/175341]\n",
      "loss: 0.207740  [156800/175341]\n",
      "loss: 0.463065  [158400/175341]\n",
      "loss: 0.578031  [160000/175341]\n",
      "loss: 0.364167  [161600/175341]\n",
      "loss: 0.355948  [163200/175341]\n",
      "loss: 0.479945  [164800/175341]\n",
      "loss: 0.366903  [166400/175341]\n",
      "loss: 0.456236  [168000/175341]\n",
      "loss: 0.621461  [169600/175341]\n",
      "loss: 0.521231  [171200/175341]\n",
      "loss: 0.771330  [172800/175341]\n",
      "loss: 0.449349  [174400/175341]\n",
      "Train Accuracy: 81.6951%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.563604, F1-score: 75.61%, Macro_F1-Score:  41.12%  \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.370111  [    0/175341]\n",
      "loss: 0.585208  [ 1600/175341]\n",
      "loss: 0.032858  [ 3200/175341]\n",
      "loss: 0.393177  [ 4800/175341]\n",
      "loss: 0.318333  [ 6400/175341]\n",
      "loss: 0.525509  [ 8000/175341]\n",
      "loss: 0.250035  [ 9600/175341]\n",
      "loss: 0.840509  [11200/175341]\n",
      "loss: 0.614621  [12800/175341]\n",
      "loss: 0.412081  [14400/175341]\n",
      "loss: 0.786103  [16000/175341]\n",
      "loss: 0.481899  [17600/175341]\n",
      "loss: 0.540173  [19200/175341]\n",
      "loss: 0.575150  [20800/175341]\n",
      "loss: 0.329281  [22400/175341]\n",
      "loss: 0.143327  [24000/175341]\n",
      "loss: 0.391149  [25600/175341]\n",
      "loss: 0.436083  [27200/175341]\n",
      "loss: 0.268982  [28800/175341]\n",
      "loss: 0.811099  [30400/175341]\n",
      "loss: 0.599735  [32000/175341]\n",
      "loss: 0.599259  [33600/175341]\n",
      "loss: 0.279326  [35200/175341]\n",
      "loss: 0.784554  [36800/175341]\n",
      "loss: 0.481409  [38400/175341]\n",
      "loss: 0.440624  [40000/175341]\n",
      "loss: 0.172240  [41600/175341]\n",
      "loss: 0.461311  [43200/175341]\n",
      "loss: 0.239245  [44800/175341]\n",
      "loss: 0.311813  [46400/175341]\n",
      "loss: 0.560856  [48000/175341]\n",
      "loss: 0.138423  [49600/175341]\n",
      "loss: 0.260617  [51200/175341]\n",
      "loss: 0.662220  [52800/175341]\n",
      "loss: 0.245400  [54400/175341]\n",
      "loss: 0.410009  [56000/175341]\n",
      "loss: 0.613781  [57600/175341]\n",
      "loss: 0.477190  [59200/175341]\n",
      "loss: 0.638472  [60800/175341]\n",
      "loss: 0.240374  [62400/175341]\n",
      "loss: 0.873980  [64000/175341]\n",
      "loss: 0.342623  [65600/175341]\n",
      "loss: 0.364599  [67200/175341]\n",
      "loss: 0.319085  [68800/175341]\n",
      "loss: 0.182014  [70400/175341]\n",
      "loss: 0.449378  [72000/175341]\n",
      "loss: 0.372229  [73600/175341]\n",
      "loss: 0.440233  [75200/175341]\n",
      "loss: 0.482318  [76800/175341]\n",
      "loss: 0.598383  [78400/175341]\n",
      "loss: 1.056088  [80000/175341]\n",
      "loss: 0.423151  [81600/175341]\n",
      "loss: 0.518505  [83200/175341]\n",
      "loss: 0.500127  [84800/175341]\n",
      "loss: 0.456172  [86400/175341]\n",
      "loss: 0.809455  [88000/175341]\n",
      "loss: 0.610531  [89600/175341]\n",
      "loss: 0.596289  [91200/175341]\n",
      "loss: 0.410796  [92800/175341]\n",
      "loss: 0.479219  [94400/175341]\n",
      "loss: 0.609221  [96000/175341]\n",
      "loss: 0.639616  [97600/175341]\n",
      "loss: 0.385462  [99200/175341]\n",
      "loss: 0.507357  [100800/175341]\n",
      "loss: 0.553176  [102400/175341]\n",
      "loss: 0.379840  [104000/175341]\n",
      "loss: 0.537488  [105600/175341]\n",
      "loss: 0.181341  [107200/175341]\n",
      "loss: 0.030798  [108800/175341]\n",
      "loss: 0.418457  [110400/175341]\n",
      "loss: 0.591757  [112000/175341]\n",
      "loss: 0.587469  [113600/175341]\n",
      "loss: 0.568342  [115200/175341]\n",
      "loss: 0.313928  [116800/175341]\n",
      "loss: 0.383051  [118400/175341]\n",
      "loss: 0.417685  [120000/175341]\n",
      "loss: 0.267118  [121600/175341]\n",
      "loss: 0.711902  [123200/175341]\n",
      "loss: 0.736438  [124800/175341]\n",
      "loss: 0.278957  [126400/175341]\n",
      "loss: 0.521294  [128000/175341]\n",
      "loss: 0.419621  [129600/175341]\n",
      "loss: 0.734299  [131200/175341]\n",
      "loss: 0.082938  [132800/175341]\n",
      "loss: 0.536908  [134400/175341]\n",
      "loss: 0.666278  [136000/175341]\n",
      "loss: 0.648556  [137600/175341]\n",
      "loss: 0.345548  [139200/175341]\n",
      "loss: 0.476048  [140800/175341]\n",
      "loss: 0.321973  [142400/175341]\n",
      "loss: 0.469814  [144000/175341]\n",
      "loss: 0.139733  [145600/175341]\n",
      "loss: 0.204252  [147200/175341]\n",
      "loss: 0.518159  [148800/175341]\n",
      "loss: 0.360450  [150400/175341]\n",
      "loss: 0.747704  [152000/175341]\n",
      "loss: 0.681056  [153600/175341]\n",
      "loss: 0.264627  [155200/175341]\n",
      "loss: 0.752307  [156800/175341]\n",
      "loss: 0.396236  [158400/175341]\n",
      "loss: 0.342173  [160000/175341]\n",
      "loss: 0.412917  [161600/175341]\n",
      "loss: 0.759496  [163200/175341]\n",
      "loss: 0.735032  [164800/175341]\n",
      "loss: 0.280655  [166400/175341]\n",
      "loss: 0.165709  [168000/175341]\n",
      "loss: 0.649832  [169600/175341]\n",
      "loss: 0.178221  [171200/175341]\n",
      "loss: 0.167238  [172800/175341]\n",
      "loss: 0.524661  [174400/175341]\n",
      "Train Accuracy: 81.7430%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.557086, F1-score: 76.38%, Macro_F1-Score:  41.22%  \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.329500  [    0/175341]\n",
      "loss: 0.636286  [ 1600/175341]\n",
      "loss: 0.151254  [ 3200/175341]\n",
      "loss: 0.461366  [ 4800/175341]\n",
      "loss: 0.382146  [ 6400/175341]\n",
      "loss: 0.596129  [ 8000/175341]\n",
      "loss: 0.830140  [ 9600/175341]\n",
      "loss: 0.334181  [11200/175341]\n",
      "loss: 0.494368  [12800/175341]\n",
      "loss: 0.449514  [14400/175341]\n",
      "loss: 0.342306  [16000/175341]\n",
      "loss: 0.516396  [17600/175341]\n",
      "loss: 0.698535  [19200/175341]\n",
      "loss: 0.596133  [20800/175341]\n",
      "loss: 0.668090  [22400/175341]\n",
      "loss: 0.154978  [24000/175341]\n",
      "loss: 0.264121  [25600/175341]\n",
      "loss: 0.860601  [27200/175341]\n",
      "loss: 0.568744  [28800/175341]\n",
      "loss: 0.834533  [30400/175341]\n",
      "loss: 0.443240  [32000/175341]\n",
      "loss: 0.448411  [33600/175341]\n",
      "loss: 0.232796  [35200/175341]\n",
      "loss: 0.313491  [36800/175341]\n",
      "loss: 0.526437  [38400/175341]\n",
      "loss: 0.517847  [40000/175341]\n",
      "loss: 0.632428  [41600/175341]\n",
      "loss: 0.628154  [43200/175341]\n",
      "loss: 0.521571  [44800/175341]\n",
      "loss: 0.413961  [46400/175341]\n",
      "loss: 0.442777  [48000/175341]\n",
      "loss: 0.216341  [49600/175341]\n",
      "loss: 0.435698  [51200/175341]\n",
      "loss: 0.543948  [52800/175341]\n",
      "loss: 0.450595  [54400/175341]\n",
      "loss: 0.739614  [56000/175341]\n",
      "loss: 0.272372  [57600/175341]\n",
      "loss: 0.367892  [59200/175341]\n",
      "loss: 0.406977  [60800/175341]\n",
      "loss: 0.337559  [62400/175341]\n",
      "loss: 0.249329  [64000/175341]\n",
      "loss: 0.562397  [65600/175341]\n",
      "loss: 0.343350  [67200/175341]\n",
      "loss: 0.388799  [68800/175341]\n",
      "loss: 0.543098  [70400/175341]\n",
      "loss: 0.274760  [72000/175341]\n",
      "loss: 0.487422  [73600/175341]\n",
      "loss: 0.393691  [75200/175341]\n",
      "loss: 0.776067  [76800/175341]\n",
      "loss: 0.334919  [78400/175341]\n",
      "loss: 0.284239  [80000/175341]\n",
      "loss: 0.216333  [81600/175341]\n",
      "loss: 0.294286  [83200/175341]\n",
      "loss: 0.216322  [84800/175341]\n",
      "loss: 0.720453  [86400/175341]\n",
      "loss: 0.665731  [88000/175341]\n",
      "loss: 0.655269  [89600/175341]\n",
      "loss: 0.078259  [91200/175341]\n",
      "loss: 0.555318  [92800/175341]\n",
      "loss: 0.664561  [94400/175341]\n",
      "loss: 0.394942  [96000/175341]\n",
      "loss: 0.208129  [97600/175341]\n",
      "loss: 0.687948  [99200/175341]\n",
      "loss: 0.567487  [100800/175341]\n",
      "loss: 1.042593  [102400/175341]\n",
      "loss: 0.559758  [104000/175341]\n",
      "loss: 0.284048  [105600/175341]\n",
      "loss: 0.201829  [107200/175341]\n",
      "loss: 0.817806  [108800/175341]\n",
      "loss: 0.832974  [110400/175341]\n",
      "loss: 0.211384  [112000/175341]\n",
      "loss: 0.718879  [113600/175341]\n",
      "loss: 0.370666  [115200/175341]\n",
      "loss: 0.347768  [116800/175341]\n",
      "loss: 0.279043  [118400/175341]\n",
      "loss: 0.622904  [120000/175341]\n",
      "loss: 0.457405  [121600/175341]\n",
      "loss: 0.501953  [123200/175341]\n",
      "loss: 0.657911  [124800/175341]\n",
      "loss: 0.329964  [126400/175341]\n",
      "loss: 0.216296  [128000/175341]\n",
      "loss: 0.440070  [129600/175341]\n",
      "loss: 0.176973  [131200/175341]\n",
      "loss: 0.601255  [132800/175341]\n",
      "loss: 0.974659  [134400/175341]\n",
      "loss: 0.470913  [136000/175341]\n",
      "loss: 0.189045  [137600/175341]\n",
      "loss: 0.449657  [139200/175341]\n",
      "loss: 0.360633  [140800/175341]\n",
      "loss: 0.434327  [142400/175341]\n",
      "loss: 0.796895  [144000/175341]\n",
      "loss: 0.397640  [145600/175341]\n",
      "loss: 0.286338  [147200/175341]\n",
      "loss: 0.768004  [148800/175341]\n",
      "loss: 0.244324  [150400/175341]\n",
      "loss: 0.280896  [152000/175341]\n",
      "loss: 0.293381  [153600/175341]\n",
      "loss: 0.376336  [155200/175341]\n",
      "loss: 0.178679  [156800/175341]\n",
      "loss: 0.265245  [158400/175341]\n",
      "loss: 0.456560  [160000/175341]\n",
      "loss: 0.572977  [161600/175341]\n",
      "loss: 0.272848  [163200/175341]\n",
      "loss: 0.280661  [164800/175341]\n",
      "loss: 0.413411  [166400/175341]\n",
      "loss: 0.284139  [168000/175341]\n",
      "loss: 0.550302  [169600/175341]\n",
      "loss: 0.179053  [171200/175341]\n",
      "loss: 0.433516  [172800/175341]\n",
      "loss: 0.390692  [174400/175341]\n",
      "Train Accuracy: 81.6911%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.551380, F1-score: 76.80%, Macro_F1-Score:  41.46%  \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.255224  [    0/175341]\n",
      "loss: 0.740455  [ 1600/175341]\n",
      "loss: 0.551256  [ 3200/175341]\n",
      "loss: 0.411847  [ 4800/175341]\n",
      "loss: 0.288639  [ 6400/175341]\n",
      "loss: 0.368915  [ 8000/175341]\n",
      "loss: 0.716221  [ 9600/175341]\n",
      "loss: 0.118392  [11200/175341]\n",
      "loss: 0.330405  [12800/175341]\n",
      "loss: 0.322070  [14400/175341]\n",
      "loss: 0.233218  [16000/175341]\n",
      "loss: 1.201679  [17600/175341]\n",
      "loss: 0.268631  [19200/175341]\n",
      "loss: 0.487503  [20800/175341]\n",
      "loss: 0.351291  [22400/175341]\n",
      "loss: 0.138573  [24000/175341]\n",
      "loss: 0.296436  [25600/175341]\n",
      "loss: 0.213961  [27200/175341]\n",
      "loss: 0.630354  [28800/175341]\n",
      "loss: 0.548846  [30400/175341]\n",
      "loss: 0.482072  [32000/175341]\n",
      "loss: 0.326406  [33600/175341]\n",
      "loss: 0.444653  [35200/175341]\n",
      "loss: 0.516648  [36800/175341]\n",
      "loss: 0.669886  [38400/175341]\n",
      "loss: 0.276514  [40000/175341]\n",
      "loss: 0.456015  [41600/175341]\n",
      "loss: 0.579581  [43200/175341]\n",
      "loss: 0.407201  [44800/175341]\n",
      "loss: 0.255675  [46400/175341]\n",
      "loss: 0.523194  [48000/175341]\n",
      "loss: 0.240825  [49600/175341]\n",
      "loss: 0.253066  [51200/175341]\n",
      "loss: 0.273951  [52800/175341]\n",
      "loss: 0.316389  [54400/175341]\n",
      "loss: 0.640249  [56000/175341]\n",
      "loss: 0.664853  [57600/175341]\n",
      "loss: 0.412524  [59200/175341]\n",
      "loss: 0.913134  [60800/175341]\n",
      "loss: 0.791261  [62400/175341]\n",
      "loss: 0.307136  [64000/175341]\n",
      "loss: 0.425165  [65600/175341]\n",
      "loss: 0.329906  [67200/175341]\n",
      "loss: 0.491607  [68800/175341]\n",
      "loss: 0.445742  [70400/175341]\n",
      "loss: 0.565554  [72000/175341]\n",
      "loss: 0.142876  [73600/175341]\n",
      "loss: 0.564328  [75200/175341]\n",
      "loss: 0.550657  [76800/175341]\n",
      "loss: 0.293447  [78400/175341]\n",
      "loss: 0.122057  [80000/175341]\n",
      "loss: 0.276456  [81600/175341]\n",
      "loss: 0.466014  [83200/175341]\n",
      "loss: 0.628096  [84800/175341]\n",
      "loss: 0.639961  [86400/175341]\n",
      "loss: 0.317353  [88000/175341]\n",
      "loss: 0.141285  [89600/175341]\n",
      "loss: 0.320451  [91200/175341]\n",
      "loss: 0.727536  [92800/175341]\n",
      "loss: 0.564952  [94400/175341]\n",
      "loss: 0.350073  [96000/175341]\n",
      "loss: 0.404758  [97600/175341]\n",
      "loss: 0.521096  [99200/175341]\n",
      "loss: 0.253126  [100800/175341]\n",
      "loss: 0.548888  [102400/175341]\n",
      "loss: 0.301841  [104000/175341]\n",
      "loss: 0.301672  [105600/175341]\n",
      "loss: 0.252439  [107200/175341]\n",
      "loss: 0.469044  [108800/175341]\n",
      "loss: 0.567017  [110400/175341]\n",
      "loss: 0.690545  [112000/175341]\n",
      "loss: 0.161646  [113600/175341]\n",
      "loss: 0.492211  [115200/175341]\n",
      "loss: 0.558763  [116800/175341]\n",
      "loss: 0.275265  [118400/175341]\n",
      "loss: 0.442336  [120000/175341]\n",
      "loss: 1.088595  [121600/175341]\n",
      "loss: 0.236444  [123200/175341]\n",
      "loss: 0.471447  [124800/175341]\n",
      "loss: 0.377035  [126400/175341]\n",
      "loss: 0.555442  [128000/175341]\n",
      "loss: 1.043220  [129600/175341]\n",
      "loss: 0.308883  [131200/175341]\n",
      "loss: 0.228716  [132800/175341]\n",
      "loss: 0.364749  [134400/175341]\n",
      "loss: 0.320724  [136000/175341]\n",
      "loss: 0.373516  [137600/175341]\n",
      "loss: 0.339855  [139200/175341]\n",
      "loss: 0.334390  [140800/175341]\n",
      "loss: 0.517344  [142400/175341]\n",
      "loss: 0.446938  [144000/175341]\n",
      "loss: 0.285764  [145600/175341]\n",
      "loss: 0.299656  [147200/175341]\n",
      "loss: 0.506852  [148800/175341]\n",
      "loss: 0.441168  [150400/175341]\n",
      "loss: 0.401641  [152000/175341]\n",
      "loss: 0.257755  [153600/175341]\n",
      "loss: 0.369708  [155200/175341]\n",
      "loss: 0.286230  [156800/175341]\n",
      "loss: 0.426919  [158400/175341]\n",
      "loss: 0.468122  [160000/175341]\n",
      "loss: 0.408390  [161600/175341]\n",
      "loss: 0.237763  [163200/175341]\n",
      "loss: 0.274620  [164800/175341]\n",
      "loss: 0.473194  [166400/175341]\n",
      "loss: 0.425004  [168000/175341]\n",
      "loss: 0.392605  [169600/175341]\n",
      "loss: 0.618760  [171200/175341]\n",
      "loss: 0.246879  [172800/175341]\n",
      "loss: 0.360038  [174400/175341]\n",
      "Train Accuracy: 81.7242%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.561863, F1-score: 75.68%, Macro_F1-Score:  41.19%  \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.491977  [    0/175341]\n",
      "loss: 0.388311  [ 1600/175341]\n",
      "loss: 0.458446  [ 3200/175341]\n",
      "loss: 0.199757  [ 4800/175341]\n",
      "loss: 0.583657  [ 6400/175341]\n",
      "loss: 0.547249  [ 8000/175341]\n",
      "loss: 0.624076  [ 9600/175341]\n",
      "loss: 0.735040  [11200/175341]\n",
      "loss: 0.356239  [12800/175341]\n",
      "loss: 0.439821  [14400/175341]\n",
      "loss: 0.779851  [16000/175341]\n",
      "loss: 0.352984  [17600/175341]\n",
      "loss: 0.655754  [19200/175341]\n",
      "loss: 0.254591  [20800/175341]\n",
      "loss: 0.707684  [22400/175341]\n",
      "loss: 0.267894  [24000/175341]\n",
      "loss: 0.081224  [25600/175341]\n",
      "loss: 0.839255  [27200/175341]\n",
      "loss: 0.338331  [28800/175341]\n",
      "loss: 0.533935  [30400/175341]\n",
      "loss: 0.632002  [32000/175341]\n",
      "loss: 1.060543  [33600/175341]\n",
      "loss: 0.296910  [35200/175341]\n",
      "loss: 0.434563  [36800/175341]\n",
      "loss: 0.405864  [38400/175341]\n",
      "loss: 0.521361  [40000/175341]\n",
      "loss: 0.425665  [41600/175341]\n",
      "loss: 0.634102  [43200/175341]\n",
      "loss: 0.754534  [44800/175341]\n",
      "loss: 0.432214  [46400/175341]\n",
      "loss: 0.562117  [48000/175341]\n",
      "loss: 0.480095  [49600/175341]\n",
      "loss: 0.422550  [51200/175341]\n",
      "loss: 0.227987  [52800/175341]\n",
      "loss: 0.469514  [54400/175341]\n",
      "loss: 0.607855  [56000/175341]\n",
      "loss: 0.569544  [57600/175341]\n",
      "loss: 0.402257  [59200/175341]\n",
      "loss: 0.245235  [60800/175341]\n",
      "loss: 0.224024  [62400/175341]\n",
      "loss: 0.717673  [64000/175341]\n",
      "loss: 0.731900  [65600/175341]\n",
      "loss: 0.607808  [67200/175341]\n",
      "loss: 0.382842  [68800/175341]\n",
      "loss: 0.336151  [70400/175341]\n",
      "loss: 0.422371  [72000/175341]\n",
      "loss: 0.595009  [73600/175341]\n",
      "loss: 0.177820  [75200/175341]\n",
      "loss: 0.541876  [76800/175341]\n",
      "loss: 0.523517  [78400/175341]\n",
      "loss: 0.763598  [80000/175341]\n",
      "loss: 0.475366  [81600/175341]\n",
      "loss: 0.262484  [83200/175341]\n",
      "loss: 0.686004  [84800/175341]\n",
      "loss: 0.414712  [86400/175341]\n",
      "loss: 0.160404  [88000/175341]\n",
      "loss: 0.320274  [89600/175341]\n",
      "loss: 0.366483  [91200/175341]\n",
      "loss: 0.556932  [92800/175341]\n",
      "loss: 0.403537  [94400/175341]\n",
      "loss: 0.087486  [96000/175341]\n",
      "loss: 0.308497  [97600/175341]\n",
      "loss: 0.680029  [99200/175341]\n",
      "loss: 0.260054  [100800/175341]\n",
      "loss: 0.454075  [102400/175341]\n",
      "loss: 0.415989  [104000/175341]\n",
      "loss: 0.129896  [105600/175341]\n",
      "loss: 0.232999  [107200/175341]\n",
      "loss: 0.402166  [108800/175341]\n",
      "loss: 0.581489  [110400/175341]\n",
      "loss: 0.141423  [112000/175341]\n",
      "loss: 0.676453  [113600/175341]\n",
      "loss: 0.379142  [115200/175341]\n",
      "loss: 0.531426  [116800/175341]\n",
      "loss: 0.540255  [118400/175341]\n",
      "loss: 0.258028  [120000/175341]\n",
      "loss: 0.430660  [121600/175341]\n",
      "loss: 0.399143  [123200/175341]\n",
      "loss: 0.489657  [124800/175341]\n",
      "loss: 0.447319  [126400/175341]\n",
      "loss: 0.391007  [128000/175341]\n",
      "loss: 0.753917  [129600/175341]\n",
      "loss: 1.173823  [131200/175341]\n",
      "loss: 0.555414  [132800/175341]\n",
      "loss: 1.522546  [134400/175341]\n",
      "loss: 0.284191  [136000/175341]\n",
      "loss: 0.416306  [137600/175341]\n",
      "loss: 0.354127  [139200/175341]\n",
      "loss: 0.548222  [140800/175341]\n",
      "loss: 0.502911  [142400/175341]\n",
      "loss: 0.679362  [144000/175341]\n",
      "loss: 0.657394  [145600/175341]\n",
      "loss: 0.393372  [147200/175341]\n",
      "loss: 0.260785  [148800/175341]\n",
      "loss: 0.847657  [150400/175341]\n",
      "loss: 0.474955  [152000/175341]\n",
      "loss: 0.353242  [153600/175341]\n",
      "loss: 0.772518  [155200/175341]\n",
      "loss: 0.641604  [156800/175341]\n",
      "loss: 0.171849  [158400/175341]\n",
      "loss: 0.311559  [160000/175341]\n",
      "loss: 0.221005  [161600/175341]\n",
      "loss: 0.445206  [163200/175341]\n",
      "loss: 0.240626  [164800/175341]\n",
      "loss: 0.180745  [166400/175341]\n",
      "loss: 0.363437  [168000/175341]\n",
      "loss: 0.526880  [169600/175341]\n",
      "loss: 0.211403  [171200/175341]\n",
      "loss: 0.323394  [172800/175341]\n",
      "loss: 0.757106  [174400/175341]\n",
      "Train Accuracy: 81.7373%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.562515, F1-score: 75.49%, Macro_F1-Score:  41.16%  \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.064435  [    0/175341]\n",
      "loss: 0.537027  [ 1600/175341]\n",
      "loss: 0.371781  [ 3200/175341]\n",
      "loss: 0.354162  [ 4800/175341]\n",
      "loss: 0.525723  [ 6400/175341]\n",
      "loss: 0.648515  [ 8000/175341]\n",
      "loss: 0.384214  [ 9600/175341]\n",
      "loss: 0.382905  [11200/175341]\n",
      "loss: 0.433864  [12800/175341]\n",
      "loss: 0.684411  [14400/175341]\n",
      "loss: 0.119020  [16000/175341]\n",
      "loss: 0.260221  [17600/175341]\n",
      "loss: 0.676489  [19200/175341]\n",
      "loss: 0.739985  [20800/175341]\n",
      "loss: 0.240793  [22400/175341]\n",
      "loss: 0.725952  [24000/175341]\n",
      "loss: 0.505021  [25600/175341]\n",
      "loss: 0.232389  [27200/175341]\n",
      "loss: 0.706328  [28800/175341]\n",
      "loss: 0.432565  [30400/175341]\n",
      "loss: 0.641416  [32000/175341]\n",
      "loss: 0.560070  [33600/175341]\n",
      "loss: 0.377337  [35200/175341]\n",
      "loss: 0.461578  [36800/175341]\n",
      "loss: 1.036802  [38400/175341]\n",
      "loss: 0.179221  [40000/175341]\n",
      "loss: 0.247224  [41600/175341]\n",
      "loss: 0.491164  [43200/175341]\n",
      "loss: 0.507011  [44800/175341]\n",
      "loss: 0.217679  [46400/175341]\n",
      "loss: 0.842842  [48000/175341]\n",
      "loss: 0.588981  [49600/175341]\n",
      "loss: 0.475717  [51200/175341]\n",
      "loss: 0.677579  [52800/175341]\n",
      "loss: 0.341733  [54400/175341]\n",
      "loss: 0.401961  [56000/175341]\n",
      "loss: 0.457429  [57600/175341]\n",
      "loss: 0.159766  [59200/175341]\n",
      "loss: 0.434981  [60800/175341]\n",
      "loss: 0.440087  [62400/175341]\n",
      "loss: 0.269848  [64000/175341]\n",
      "loss: 0.043480  [65600/175341]\n",
      "loss: 0.689965  [67200/175341]\n",
      "loss: 0.565584  [68800/175341]\n",
      "loss: 0.484531  [70400/175341]\n",
      "loss: 0.307943  [72000/175341]\n",
      "loss: 0.343648  [73600/175341]\n",
      "loss: 0.587830  [75200/175341]\n",
      "loss: 0.528514  [76800/175341]\n",
      "loss: 0.363516  [78400/175341]\n",
      "loss: 0.428972  [80000/175341]\n",
      "loss: 0.472464  [81600/175341]\n",
      "loss: 0.535429  [83200/175341]\n",
      "loss: 0.191806  [84800/175341]\n",
      "loss: 0.148564  [86400/175341]\n",
      "loss: 0.678278  [88000/175341]\n",
      "loss: 0.581241  [89600/175341]\n",
      "loss: 0.221556  [91200/175341]\n",
      "loss: 0.550447  [92800/175341]\n",
      "loss: 0.376472  [94400/175341]\n",
      "loss: 0.398783  [96000/175341]\n",
      "loss: 0.327121  [97600/175341]\n",
      "loss: 0.689753  [99200/175341]\n",
      "loss: 0.909558  [100800/175341]\n",
      "loss: 0.393183  [102400/175341]\n",
      "loss: 0.352080  [104000/175341]\n",
      "loss: 0.600842  [105600/175341]\n",
      "loss: 0.506408  [107200/175341]\n",
      "loss: 0.372580  [108800/175341]\n",
      "loss: 0.647147  [110400/175341]\n",
      "loss: 0.460563  [112000/175341]\n",
      "loss: 0.410609  [113600/175341]\n",
      "loss: 0.481550  [115200/175341]\n",
      "loss: 0.555141  [116800/175341]\n",
      "loss: 0.328908  [118400/175341]\n",
      "loss: 0.537093  [120000/175341]\n",
      "loss: 0.655838  [121600/175341]\n",
      "loss: 0.239285  [123200/175341]\n",
      "loss: 0.456225  [124800/175341]\n",
      "loss: 0.760196  [126400/175341]\n",
      "loss: 0.342921  [128000/175341]\n",
      "loss: 0.530654  [129600/175341]\n",
      "loss: 0.293899  [131200/175341]\n",
      "loss: 0.358053  [132800/175341]\n",
      "loss: 0.491625  [134400/175341]\n",
      "loss: 0.337725  [136000/175341]\n",
      "loss: 0.334927  [137600/175341]\n",
      "loss: 0.525503  [139200/175341]\n",
      "loss: 0.596360  [140800/175341]\n",
      "loss: 0.738331  [142400/175341]\n",
      "loss: 0.617215  [144000/175341]\n",
      "loss: 0.675565  [145600/175341]\n",
      "loss: 0.756213  [147200/175341]\n",
      "loss: 0.685070  [148800/175341]\n",
      "loss: 0.275018  [150400/175341]\n",
      "loss: 0.898496  [152000/175341]\n",
      "loss: 0.569319  [153600/175341]\n",
      "loss: 0.809019  [155200/175341]\n",
      "loss: 0.612609  [156800/175341]\n",
      "loss: 0.295082  [158400/175341]\n",
      "loss: 0.686656  [160000/175341]\n",
      "loss: 0.309540  [161600/175341]\n",
      "loss: 0.559039  [163200/175341]\n",
      "loss: 0.717664  [164800/175341]\n",
      "loss: 0.450384  [166400/175341]\n",
      "loss: 0.333791  [168000/175341]\n",
      "loss: 0.332963  [169600/175341]\n",
      "loss: 0.359141  [171200/175341]\n",
      "loss: 0.298500  [172800/175341]\n",
      "loss: 0.470522  [174400/175341]\n",
      "Train Accuracy: 81.7573%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.548986, F1-score: 76.41%, Macro_F1-Score:  41.27%  \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.471675  [    0/175341]\n",
      "loss: 0.589582  [ 1600/175341]\n",
      "loss: 0.654729  [ 3200/175341]\n",
      "loss: 0.255412  [ 4800/175341]\n",
      "loss: 0.143718  [ 6400/175341]\n",
      "loss: 0.376239  [ 8000/175341]\n",
      "loss: 0.731528  [ 9600/175341]\n",
      "loss: 0.360155  [11200/175341]\n",
      "loss: 0.284149  [12800/175341]\n",
      "loss: 0.323862  [14400/175341]\n",
      "loss: 0.435412  [16000/175341]\n",
      "loss: 0.149993  [17600/175341]\n",
      "loss: 0.762780  [19200/175341]\n",
      "loss: 0.503485  [20800/175341]\n",
      "loss: 0.560254  [22400/175341]\n",
      "loss: 0.400625  [24000/175341]\n",
      "loss: 0.498977  [25600/175341]\n",
      "loss: 0.434229  [27200/175341]\n",
      "loss: 0.848690  [28800/175341]\n",
      "loss: 0.274360  [30400/175341]\n",
      "loss: 0.555576  [32000/175341]\n",
      "loss: 0.170864  [33600/175341]\n",
      "loss: 0.770282  [35200/175341]\n",
      "loss: 0.285892  [36800/175341]\n",
      "loss: 0.528319  [38400/175341]\n",
      "loss: 0.340260  [40000/175341]\n",
      "loss: 0.334419  [41600/175341]\n",
      "loss: 0.436521  [43200/175341]\n",
      "loss: 0.266072  [44800/175341]\n",
      "loss: 0.397686  [46400/175341]\n",
      "loss: 0.505217  [48000/175341]\n",
      "loss: 0.302578  [49600/175341]\n",
      "loss: 0.157988  [51200/175341]\n",
      "loss: 0.309423  [52800/175341]\n",
      "loss: 0.317792  [54400/175341]\n",
      "loss: 0.926090  [56000/175341]\n",
      "loss: 0.395945  [57600/175341]\n",
      "loss: 0.338558  [59200/175341]\n",
      "loss: 0.400028  [60800/175341]\n",
      "loss: 0.323380  [62400/175341]\n",
      "loss: 0.413659  [64000/175341]\n",
      "loss: 0.251515  [65600/175341]\n",
      "loss: 0.330047  [67200/175341]\n",
      "loss: 0.176925  [68800/175341]\n",
      "loss: 0.509076  [70400/175341]\n",
      "loss: 0.221424  [72000/175341]\n",
      "loss: 0.251817  [73600/175341]\n",
      "loss: 0.762455  [75200/175341]\n",
      "loss: 0.248538  [76800/175341]\n",
      "loss: 0.203406  [78400/175341]\n",
      "loss: 0.327338  [80000/175341]\n",
      "loss: 0.669645  [81600/175341]\n",
      "loss: 0.524210  [83200/175341]\n",
      "loss: 0.167897  [84800/175341]\n",
      "loss: 0.502020  [86400/175341]\n",
      "loss: 0.829667  [88000/175341]\n",
      "loss: 0.387556  [89600/175341]\n",
      "loss: 0.642268  [91200/175341]\n",
      "loss: 0.541774  [92800/175341]\n",
      "loss: 0.147805  [94400/175341]\n",
      "loss: 0.431333  [96000/175341]\n",
      "loss: 0.423765  [97600/175341]\n",
      "loss: 0.703454  [99200/175341]\n",
      "loss: 0.386913  [100800/175341]\n",
      "loss: 0.564034  [102400/175341]\n",
      "loss: 0.376785  [104000/175341]\n",
      "loss: 0.332991  [105600/175341]\n",
      "loss: 0.418691  [107200/175341]\n",
      "loss: 0.294298  [108800/175341]\n",
      "loss: 0.519464  [110400/175341]\n",
      "loss: 0.960764  [112000/175341]\n",
      "loss: 0.185303  [113600/175341]\n",
      "loss: 0.509024  [115200/175341]\n",
      "loss: 0.426085  [116800/175341]\n",
      "loss: 0.318926  [118400/175341]\n",
      "loss: 0.630295  [120000/175341]\n",
      "loss: 0.664598  [121600/175341]\n",
      "loss: 0.365717  [123200/175341]\n",
      "loss: 0.283438  [124800/175341]\n",
      "loss: 0.884871  [126400/175341]\n",
      "loss: 0.584607  [128000/175341]\n",
      "loss: 1.145758  [129600/175341]\n",
      "loss: 0.537615  [131200/175341]\n",
      "loss: 0.604881  [132800/175341]\n",
      "loss: 0.423133  [134400/175341]\n",
      "loss: 0.884890  [136000/175341]\n",
      "loss: 0.272611  [137600/175341]\n",
      "loss: 0.399059  [139200/175341]\n",
      "loss: 0.558194  [140800/175341]\n",
      "loss: 0.410146  [142400/175341]\n",
      "loss: 0.479825  [144000/175341]\n",
      "loss: 0.320279  [145600/175341]\n",
      "loss: 0.483062  [147200/175341]\n",
      "loss: 0.443674  [148800/175341]\n",
      "loss: 0.252571  [150400/175341]\n",
      "loss: 0.220349  [152000/175341]\n",
      "loss: 0.591132  [153600/175341]\n",
      "loss: 0.580645  [155200/175341]\n",
      "loss: 0.337352  [156800/175341]\n",
      "loss: 0.757669  [158400/175341]\n",
      "loss: 0.105030  [160000/175341]\n",
      "loss: 0.544977  [161600/175341]\n",
      "loss: 0.427029  [163200/175341]\n",
      "loss: 0.406241  [164800/175341]\n",
      "loss: 0.441326  [166400/175341]\n",
      "loss: 0.572674  [168000/175341]\n",
      "loss: 0.718987  [169600/175341]\n",
      "loss: 0.279661  [171200/175341]\n",
      "loss: 0.176145  [172800/175341]\n",
      "loss: 0.349265  [174400/175341]\n",
      "Train Accuracy: 81.7293%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.558836, F1-score: 75.79%, Macro_F1-Score:  41.12%  \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.272155  [    0/175341]\n",
      "loss: 0.790142  [ 1600/175341]\n",
      "loss: 0.402164  [ 3200/175341]\n",
      "loss: 0.724593  [ 4800/175341]\n",
      "loss: 0.295304  [ 6400/175341]\n",
      "loss: 0.671762  [ 8000/175341]\n",
      "loss: 0.524127  [ 9600/175341]\n",
      "loss: 0.379117  [11200/175341]\n",
      "loss: 0.362574  [12800/175341]\n",
      "loss: 0.287500  [14400/175341]\n",
      "loss: 0.233988  [16000/175341]\n",
      "loss: 0.363353  [17600/175341]\n",
      "loss: 0.917947  [19200/175341]\n",
      "loss: 0.274171  [20800/175341]\n",
      "loss: 1.093726  [22400/175341]\n",
      "loss: 0.786884  [24000/175341]\n",
      "loss: 0.310979  [25600/175341]\n",
      "loss: 0.817889  [27200/175341]\n",
      "loss: 0.420237  [28800/175341]\n",
      "loss: 0.380978  [30400/175341]\n",
      "loss: 0.324207  [32000/175341]\n",
      "loss: 1.107957  [33600/175341]\n",
      "loss: 0.329084  [35200/175341]\n",
      "loss: 0.578734  [36800/175341]\n",
      "loss: 0.799798  [38400/175341]\n",
      "loss: 0.344658  [40000/175341]\n",
      "loss: 0.089061  [41600/175341]\n",
      "loss: 0.508783  [43200/175341]\n",
      "loss: 0.351341  [44800/175341]\n",
      "loss: 0.417968  [46400/175341]\n",
      "loss: 0.595970  [48000/175341]\n",
      "loss: 0.650499  [49600/175341]\n",
      "loss: 0.263478  [51200/175341]\n",
      "loss: 0.326048  [52800/175341]\n",
      "loss: 0.964314  [54400/175341]\n",
      "loss: 0.488266  [56000/175341]\n",
      "loss: 0.446830  [57600/175341]\n",
      "loss: 0.239027  [59200/175341]\n",
      "loss: 0.499501  [60800/175341]\n",
      "loss: 0.274894  [62400/175341]\n",
      "loss: 0.348198  [64000/175341]\n",
      "loss: 0.361539  [65600/175341]\n",
      "loss: 0.418103  [67200/175341]\n",
      "loss: 0.458444  [68800/175341]\n",
      "loss: 0.342144  [70400/175341]\n",
      "loss: 0.360724  [72000/175341]\n",
      "loss: 0.726521  [73600/175341]\n",
      "loss: 0.523150  [75200/175341]\n",
      "loss: 0.348650  [76800/175341]\n",
      "loss: 0.321098  [78400/175341]\n",
      "loss: 0.595655  [80000/175341]\n",
      "loss: 0.216176  [81600/175341]\n",
      "loss: 0.093465  [83200/175341]\n",
      "loss: 0.632840  [84800/175341]\n",
      "loss: 0.312708  [86400/175341]\n",
      "loss: 0.330947  [88000/175341]\n",
      "loss: 0.383947  [89600/175341]\n",
      "loss: 0.291222  [91200/175341]\n",
      "loss: 0.260250  [92800/175341]\n",
      "loss: 0.557464  [94400/175341]\n",
      "loss: 0.824205  [96000/175341]\n",
      "loss: 0.319527  [97600/175341]\n",
      "loss: 0.295775  [99200/175341]\n",
      "loss: 0.312697  [100800/175341]\n",
      "loss: 0.271241  [102400/175341]\n",
      "loss: 0.318489  [104000/175341]\n",
      "loss: 0.376488  [105600/175341]\n",
      "loss: 0.593909  [107200/175341]\n",
      "loss: 0.600718  [108800/175341]\n",
      "loss: 0.673454  [110400/175341]\n",
      "loss: 0.341337  [112000/175341]\n",
      "loss: 0.344973  [113600/175341]\n",
      "loss: 0.240682  [115200/175341]\n",
      "loss: 0.760585  [116800/175341]\n",
      "loss: 0.304390  [118400/175341]\n",
      "loss: 0.602992  [120000/175341]\n",
      "loss: 0.769972  [121600/175341]\n",
      "loss: 0.449546  [123200/175341]\n",
      "loss: 0.242824  [124800/175341]\n",
      "loss: 1.102624  [126400/175341]\n",
      "loss: 0.439288  [128000/175341]\n",
      "loss: 0.237704  [129600/175341]\n",
      "loss: 0.288375  [131200/175341]\n",
      "loss: 0.507141  [132800/175341]\n",
      "loss: 0.419055  [134400/175341]\n",
      "loss: 0.457180  [136000/175341]\n",
      "loss: 0.546576  [137600/175341]\n",
      "loss: 0.517792  [139200/175341]\n",
      "loss: 0.434369  [140800/175341]\n",
      "loss: 1.007812  [142400/175341]\n",
      "loss: 0.427851  [144000/175341]\n",
      "loss: 0.480802  [145600/175341]\n",
      "loss: 0.448158  [147200/175341]\n",
      "loss: 0.604251  [148800/175341]\n",
      "loss: 0.168784  [150400/175341]\n",
      "loss: 0.687577  [152000/175341]\n",
      "loss: 0.536022  [153600/175341]\n",
      "loss: 0.613347  [155200/175341]\n",
      "loss: 0.817991  [156800/175341]\n",
      "loss: 0.482788  [158400/175341]\n",
      "loss: 0.494156  [160000/175341]\n",
      "loss: 0.435815  [161600/175341]\n",
      "loss: 0.700855  [163200/175341]\n",
      "loss: 0.319420  [164800/175341]\n",
      "loss: 0.492396  [166400/175341]\n",
      "loss: 0.380056  [168000/175341]\n",
      "loss: 0.647535  [169600/175341]\n",
      "loss: 0.434754  [171200/175341]\n",
      "loss: 0.455545  [172800/175341]\n",
      "loss: 0.468267  [174400/175341]\n",
      "Train Accuracy: 81.7664%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.557920, F1-score: 76.15%, Macro_F1-Score:  41.17%  \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.593652  [    0/175341]\n",
      "loss: 0.383005  [ 1600/175341]\n",
      "loss: 0.414585  [ 3200/175341]\n",
      "loss: 0.206954  [ 4800/175341]\n",
      "loss: 0.312928  [ 6400/175341]\n",
      "loss: 0.215153  [ 8000/175341]\n",
      "loss: 0.430652  [ 9600/175341]\n",
      "loss: 0.382018  [11200/175341]\n",
      "loss: 0.342292  [12800/175341]\n",
      "loss: 0.567920  [14400/175341]\n",
      "loss: 0.322799  [16000/175341]\n",
      "loss: 0.617212  [17600/175341]\n",
      "loss: 0.745458  [19200/175341]\n",
      "loss: 0.555668  [20800/175341]\n",
      "loss: 0.516255  [22400/175341]\n",
      "loss: 0.359166  [24000/175341]\n",
      "loss: 0.346360  [25600/175341]\n",
      "loss: 0.406703  [27200/175341]\n",
      "loss: 0.286278  [28800/175341]\n",
      "loss: 0.223646  [30400/175341]\n",
      "loss: 0.250552  [32000/175341]\n",
      "loss: 0.684753  [33600/175341]\n",
      "loss: 0.126110  [35200/175341]\n",
      "loss: 0.425577  [36800/175341]\n",
      "loss: 0.570628  [38400/175341]\n",
      "loss: 0.599290  [40000/175341]\n",
      "loss: 0.414941  [41600/175341]\n",
      "loss: 0.242375  [43200/175341]\n",
      "loss: 0.671218  [44800/175341]\n",
      "loss: 0.608367  [46400/175341]\n",
      "loss: 0.359789  [48000/175341]\n",
      "loss: 0.833804  [49600/175341]\n",
      "loss: 0.221594  [51200/175341]\n",
      "loss: 0.174307  [52800/175341]\n",
      "loss: 0.591610  [54400/175341]\n",
      "loss: 0.795930  [56000/175341]\n",
      "loss: 0.299216  [57600/175341]\n",
      "loss: 0.305171  [59200/175341]\n",
      "loss: 0.303172  [60800/175341]\n",
      "loss: 0.549627  [62400/175341]\n",
      "loss: 0.249166  [64000/175341]\n",
      "loss: 0.271340  [65600/175341]\n",
      "loss: 0.335661  [67200/175341]\n",
      "loss: 0.237957  [68800/175341]\n",
      "loss: 0.395117  [70400/175341]\n",
      "loss: 0.421853  [72000/175341]\n",
      "loss: 0.552528  [73600/175341]\n",
      "loss: 0.675858  [75200/175341]\n",
      "loss: 0.616073  [76800/175341]\n",
      "loss: 0.442704  [78400/175341]\n",
      "loss: 0.622131  [80000/175341]\n",
      "loss: 0.521870  [81600/175341]\n",
      "loss: 0.233386  [83200/175341]\n",
      "loss: 0.562005  [84800/175341]\n",
      "loss: 0.257215  [86400/175341]\n",
      "loss: 0.713302  [88000/175341]\n",
      "loss: 0.771594  [89600/175341]\n",
      "loss: 0.231298  [91200/175341]\n",
      "loss: 0.113628  [92800/175341]\n",
      "loss: 0.376393  [94400/175341]\n",
      "loss: 0.195686  [96000/175341]\n",
      "loss: 0.560255  [97600/175341]\n",
      "loss: 0.540728  [99200/175341]\n",
      "loss: 0.316171  [100800/175341]\n",
      "loss: 0.504272  [102400/175341]\n",
      "loss: 0.293376  [104000/175341]\n",
      "loss: 0.382591  [105600/175341]\n",
      "loss: 0.160014  [107200/175341]\n",
      "loss: 0.211041  [108800/175341]\n",
      "loss: 0.267861  [110400/175341]\n",
      "loss: 0.622829  [112000/175341]\n",
      "loss: 0.312349  [113600/175341]\n",
      "loss: 0.517933  [115200/175341]\n",
      "loss: 0.476433  [116800/175341]\n",
      "loss: 0.554550  [118400/175341]\n",
      "loss: 0.534174  [120000/175341]\n",
      "loss: 0.290916  [121600/175341]\n",
      "loss: 0.492673  [123200/175341]\n",
      "loss: 0.161104  [124800/175341]\n",
      "loss: 0.381108  [126400/175341]\n",
      "loss: 0.826773  [128000/175341]\n",
      "loss: 0.129697  [129600/175341]\n",
      "loss: 0.694569  [131200/175341]\n",
      "loss: 0.460479  [132800/175341]\n",
      "loss: 0.236042  [134400/175341]\n",
      "loss: 0.229441  [136000/175341]\n",
      "loss: 0.479216  [137600/175341]\n",
      "loss: 0.113553  [139200/175341]\n",
      "loss: 0.462138  [140800/175341]\n",
      "loss: 0.450309  [142400/175341]\n",
      "loss: 0.495674  [144000/175341]\n",
      "loss: 0.686860  [145600/175341]\n",
      "loss: 0.408764  [147200/175341]\n",
      "loss: 0.346393  [148800/175341]\n",
      "loss: 0.216216  [150400/175341]\n",
      "loss: 0.372093  [152000/175341]\n",
      "loss: 0.455367  [153600/175341]\n",
      "loss: 0.271741  [155200/175341]\n",
      "loss: 0.203511  [156800/175341]\n",
      "loss: 0.624055  [158400/175341]\n",
      "loss: 0.322545  [160000/175341]\n",
      "loss: 0.449110  [161600/175341]\n",
      "loss: 0.780718  [163200/175341]\n",
      "loss: 0.744024  [164800/175341]\n",
      "loss: 0.507981  [166400/175341]\n",
      "loss: 0.772110  [168000/175341]\n",
      "loss: 0.309174  [169600/175341]\n",
      "loss: 0.676699  [171200/175341]\n",
      "loss: 0.275741  [172800/175341]\n",
      "loss: 0.372236  [174400/175341]\n",
      "Train Accuracy: 81.7886%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.530804, F1-score: 77.29%, Macro_F1-Score:  41.39%  \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.482196  [    0/175341]\n",
      "loss: 0.669532  [ 1600/175341]\n",
      "loss: 0.704320  [ 3200/175341]\n",
      "loss: 0.598538  [ 4800/175341]\n",
      "loss: 0.623640  [ 6400/175341]\n",
      "loss: 0.607693  [ 8000/175341]\n",
      "loss: 0.574793  [ 9600/175341]\n",
      "loss: 0.482441  [11200/175341]\n",
      "loss: 0.287026  [12800/175341]\n",
      "loss: 0.308923  [14400/175341]\n",
      "loss: 0.403731  [16000/175341]\n",
      "loss: 0.317199  [17600/175341]\n",
      "loss: 0.600876  [19200/175341]\n",
      "loss: 0.187392  [20800/175341]\n",
      "loss: 0.279790  [22400/175341]\n",
      "loss: 0.460408  [24000/175341]\n",
      "loss: 0.210996  [25600/175341]\n",
      "loss: 0.429028  [27200/175341]\n",
      "loss: 0.214166  [28800/175341]\n",
      "loss: 0.549531  [30400/175341]\n",
      "loss: 0.316589  [32000/175341]\n",
      "loss: 0.331564  [33600/175341]\n",
      "loss: 0.187976  [35200/175341]\n",
      "loss: 0.658835  [36800/175341]\n",
      "loss: 0.272246  [38400/175341]\n",
      "loss: 0.445842  [40000/175341]\n",
      "loss: 0.537843  [41600/175341]\n",
      "loss: 0.242555  [43200/175341]\n",
      "loss: 0.312703  [44800/175341]\n",
      "loss: 0.344367  [46400/175341]\n",
      "loss: 0.439673  [48000/175341]\n",
      "loss: 0.590304  [49600/175341]\n",
      "loss: 0.605046  [51200/175341]\n",
      "loss: 0.155867  [52800/175341]\n",
      "loss: 0.410445  [54400/175341]\n",
      "loss: 0.394031  [56000/175341]\n",
      "loss: 0.278651  [57600/175341]\n",
      "loss: 0.531093  [59200/175341]\n",
      "loss: 0.678058  [60800/175341]\n",
      "loss: 0.386186  [62400/175341]\n",
      "loss: 0.415870  [64000/175341]\n",
      "loss: 0.253590  [65600/175341]\n",
      "loss: 0.708706  [67200/175341]\n",
      "loss: 0.490342  [68800/175341]\n",
      "loss: 0.784510  [70400/175341]\n",
      "loss: 0.302227  [72000/175341]\n",
      "loss: 0.256497  [73600/175341]\n",
      "loss: 0.484423  [75200/175341]\n",
      "loss: 0.475157  [76800/175341]\n",
      "loss: 0.564298  [78400/175341]\n",
      "loss: 0.490974  [80000/175341]\n",
      "loss: 0.561455  [81600/175341]\n",
      "loss: 0.670708  [83200/175341]\n",
      "loss: 0.311821  [84800/175341]\n",
      "loss: 0.225269  [86400/175341]\n",
      "loss: 0.344090  [88000/175341]\n",
      "loss: 0.833907  [89600/175341]\n",
      "loss: 0.463408  [91200/175341]\n",
      "loss: 0.271614  [92800/175341]\n",
      "loss: 0.521561  [94400/175341]\n",
      "loss: 0.606598  [96000/175341]\n",
      "loss: 0.408482  [97600/175341]\n",
      "loss: 0.391703  [99200/175341]\n",
      "loss: 0.546465  [100800/175341]\n",
      "loss: 0.449170  [102400/175341]\n",
      "loss: 0.175283  [104000/175341]\n",
      "loss: 0.944249  [105600/175341]\n",
      "loss: 0.796373  [107200/175341]\n",
      "loss: 0.582101  [108800/175341]\n",
      "loss: 0.648091  [110400/175341]\n",
      "loss: 0.222405  [112000/175341]\n",
      "loss: 0.346047  [113600/175341]\n",
      "loss: 0.461982  [115200/175341]\n",
      "loss: 0.467195  [116800/175341]\n",
      "loss: 0.415220  [118400/175341]\n",
      "loss: 0.234586  [120000/175341]\n",
      "loss: 0.642730  [121600/175341]\n",
      "loss: 0.657895  [123200/175341]\n",
      "loss: 0.382105  [124800/175341]\n",
      "loss: 0.717943  [126400/175341]\n",
      "loss: 0.498417  [128000/175341]\n",
      "loss: 0.333006  [129600/175341]\n",
      "loss: 0.711176  [131200/175341]\n",
      "loss: 0.525228  [132800/175341]\n",
      "loss: 0.267625  [134400/175341]\n",
      "loss: 0.179196  [136000/175341]\n",
      "loss: 0.108073  [137600/175341]\n",
      "loss: 0.737388  [139200/175341]\n",
      "loss: 0.507430  [140800/175341]\n",
      "loss: 0.308384  [142400/175341]\n",
      "loss: 0.475399  [144000/175341]\n",
      "loss: 0.255493  [145600/175341]\n",
      "loss: 0.503102  [147200/175341]\n",
      "loss: 0.126803  [148800/175341]\n",
      "loss: 0.921709  [150400/175341]\n",
      "loss: 0.406218  [152000/175341]\n",
      "loss: 0.401083  [153600/175341]\n",
      "loss: 0.398664  [155200/175341]\n",
      "loss: 0.489857  [156800/175341]\n",
      "loss: 0.664656  [158400/175341]\n",
      "loss: 0.486323  [160000/175341]\n",
      "loss: 0.268843  [161600/175341]\n",
      "loss: 0.451031  [163200/175341]\n",
      "loss: 0.482836  [164800/175341]\n",
      "loss: 0.462861  [166400/175341]\n",
      "loss: 0.203466  [168000/175341]\n",
      "loss: 0.528789  [169600/175341]\n",
      "loss: 0.220129  [171200/175341]\n",
      "loss: 0.300882  [172800/175341]\n",
      "loss: 0.471921  [174400/175341]\n",
      "Train Accuracy: 81.7806%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.559111, F1-score: 75.43%, Macro_F1-Score:  41.08%  \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.263497  [    0/175341]\n",
      "loss: 0.197025  [ 1600/175341]\n",
      "loss: 0.370338  [ 3200/175341]\n",
      "loss: 0.291355  [ 4800/175341]\n",
      "loss: 0.589894  [ 6400/175341]\n",
      "loss: 0.426811  [ 8000/175341]\n",
      "loss: 0.504863  [ 9600/175341]\n",
      "loss: 0.361802  [11200/175341]\n",
      "loss: 0.185879  [12800/175341]\n",
      "loss: 0.595234  [14400/175341]\n",
      "loss: 0.630816  [16000/175341]\n",
      "loss: 0.568860  [17600/175341]\n",
      "loss: 0.624902  [19200/175341]\n",
      "loss: 0.928178  [20800/175341]\n",
      "loss: 0.410391  [22400/175341]\n",
      "loss: 0.589415  [24000/175341]\n",
      "loss: 0.322892  [25600/175341]\n",
      "loss: 0.351439  [27200/175341]\n",
      "loss: 0.302442  [28800/175341]\n",
      "loss: 0.511184  [30400/175341]\n",
      "loss: 0.278311  [32000/175341]\n",
      "loss: 0.535635  [33600/175341]\n",
      "loss: 0.772199  [35200/175341]\n",
      "loss: 0.517103  [36800/175341]\n",
      "loss: 0.513065  [38400/175341]\n",
      "loss: 0.459050  [40000/175341]\n",
      "loss: 0.193105  [41600/175341]\n",
      "loss: 0.489033  [43200/175341]\n",
      "loss: 0.359756  [44800/175341]\n",
      "loss: 0.443518  [46400/175341]\n",
      "loss: 0.313831  [48000/175341]\n",
      "loss: 0.464398  [49600/175341]\n",
      "loss: 0.436668  [51200/175341]\n",
      "loss: 0.686583  [52800/175341]\n",
      "loss: 0.269233  [54400/175341]\n",
      "loss: 0.422824  [56000/175341]\n",
      "loss: 0.551521  [57600/175341]\n",
      "loss: 0.520647  [59200/175341]\n",
      "loss: 0.439403  [60800/175341]\n",
      "loss: 0.413748  [62400/175341]\n",
      "loss: 0.378710  [64000/175341]\n",
      "loss: 0.669986  [65600/175341]\n",
      "loss: 0.580420  [67200/175341]\n",
      "loss: 0.382155  [68800/175341]\n",
      "loss: 0.484011  [70400/175341]\n",
      "loss: 0.373073  [72000/175341]\n",
      "loss: 0.504842  [73600/175341]\n",
      "loss: 0.364789  [75200/175341]\n",
      "loss: 0.357202  [76800/175341]\n",
      "loss: 0.647536  [78400/175341]\n",
      "loss: 0.220832  [80000/175341]\n",
      "loss: 0.670702  [81600/175341]\n",
      "loss: 1.155759  [83200/175341]\n",
      "loss: 0.680574  [84800/175341]\n",
      "loss: 0.555738  [86400/175341]\n",
      "loss: 0.663164  [88000/175341]\n",
      "loss: 0.559080  [89600/175341]\n",
      "loss: 0.426142  [91200/175341]\n",
      "loss: 0.789253  [92800/175341]\n",
      "loss: 0.379129  [94400/175341]\n",
      "loss: 0.276674  [96000/175341]\n",
      "loss: 0.495086  [97600/175341]\n",
      "loss: 0.666816  [99200/175341]\n",
      "loss: 0.758946  [100800/175341]\n",
      "loss: 0.515735  [102400/175341]\n",
      "loss: 0.542459  [104000/175341]\n",
      "loss: 0.457606  [105600/175341]\n",
      "loss: 0.482805  [107200/175341]\n",
      "loss: 0.369689  [108800/175341]\n",
      "loss: 0.488906  [110400/175341]\n",
      "loss: 0.215208  [112000/175341]\n",
      "loss: 0.622450  [113600/175341]\n",
      "loss: 0.769044  [115200/175341]\n",
      "loss: 0.335908  [116800/175341]\n",
      "loss: 0.684099  [118400/175341]\n",
      "loss: 0.899484  [120000/175341]\n",
      "loss: 0.420079  [121600/175341]\n",
      "loss: 0.293882  [123200/175341]\n",
      "loss: 0.515096  [124800/175341]\n",
      "loss: 0.400457  [126400/175341]\n",
      "loss: 0.604713  [128000/175341]\n",
      "loss: 0.185131  [129600/175341]\n",
      "loss: 0.447451  [131200/175341]\n",
      "loss: 0.329587  [132800/175341]\n",
      "loss: 0.572725  [134400/175341]\n",
      "loss: 0.336059  [136000/175341]\n",
      "loss: 0.200256  [137600/175341]\n",
      "loss: 0.896571  [139200/175341]\n",
      "loss: 0.253577  [140800/175341]\n",
      "loss: 0.293809  [142400/175341]\n",
      "loss: 0.694417  [144000/175341]\n",
      "loss: 0.489900  [145600/175341]\n",
      "loss: 0.561940  [147200/175341]\n",
      "loss: 0.242027  [148800/175341]\n",
      "loss: 0.274196  [150400/175341]\n",
      "loss: 0.330778  [152000/175341]\n",
      "loss: 0.530892  [153600/175341]\n",
      "loss: 0.314638  [155200/175341]\n",
      "loss: 0.243187  [156800/175341]\n",
      "loss: 1.106701  [158400/175341]\n",
      "loss: 0.454981  [160000/175341]\n",
      "loss: 0.440948  [161600/175341]\n",
      "loss: 0.462113  [163200/175341]\n",
      "loss: 0.306677  [164800/175341]\n",
      "loss: 0.330264  [166400/175341]\n",
      "loss: 0.373255  [168000/175341]\n",
      "loss: 0.260445  [169600/175341]\n",
      "loss: 0.800940  [171200/175341]\n",
      "loss: 0.819268  [172800/175341]\n",
      "loss: 0.581088  [174400/175341]\n",
      "Train Accuracy: 81.7852%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.568325, F1-score: 76.17%, Macro_F1-Score:  41.16%  \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.267371  [    0/175341]\n",
      "loss: 0.367504  [ 1600/175341]\n",
      "loss: 0.388847  [ 3200/175341]\n",
      "loss: 0.480125  [ 4800/175341]\n",
      "loss: 0.244307  [ 6400/175341]\n",
      "loss: 0.314156  [ 8000/175341]\n",
      "loss: 0.271390  [ 9600/175341]\n",
      "loss: 0.135098  [11200/175341]\n",
      "loss: 0.460090  [12800/175341]\n",
      "loss: 0.294736  [14400/175341]\n",
      "loss: 0.413586  [16000/175341]\n",
      "loss: 0.889066  [17600/175341]\n",
      "loss: 0.293352  [19200/175341]\n",
      "loss: 0.402692  [20800/175341]\n",
      "loss: 0.882280  [22400/175341]\n",
      "loss: 0.263460  [24000/175341]\n",
      "loss: 0.636771  [25600/175341]\n",
      "loss: 0.522146  [27200/175341]\n",
      "loss: 0.369762  [28800/175341]\n",
      "loss: 0.411104  [30400/175341]\n",
      "loss: 0.603109  [32000/175341]\n",
      "loss: 0.843206  [33600/175341]\n",
      "loss: 0.374216  [35200/175341]\n",
      "loss: 0.215041  [36800/175341]\n",
      "loss: 0.250523  [38400/175341]\n",
      "loss: 0.741836  [40000/175341]\n",
      "loss: 0.400313  [41600/175341]\n",
      "loss: 0.384891  [43200/175341]\n",
      "loss: 0.702912  [44800/175341]\n",
      "loss: 0.352907  [46400/175341]\n",
      "loss: 0.697630  [48000/175341]\n",
      "loss: 0.396693  [49600/175341]\n",
      "loss: 0.781058  [51200/175341]\n",
      "loss: 0.281259  [52800/175341]\n",
      "loss: 0.424164  [54400/175341]\n",
      "loss: 0.384270  [56000/175341]\n",
      "loss: 0.193390  [57600/175341]\n",
      "loss: 0.244716  [59200/175341]\n",
      "loss: 0.504809  [60800/175341]\n",
      "loss: 0.310652  [62400/175341]\n",
      "loss: 0.305369  [64000/175341]\n",
      "loss: 0.590339  [65600/175341]\n",
      "loss: 0.672770  [67200/175341]\n",
      "loss: 0.214198  [68800/175341]\n",
      "loss: 0.181463  [70400/175341]\n",
      "loss: 0.644268  [72000/175341]\n",
      "loss: 0.353519  [73600/175341]\n",
      "loss: 0.300299  [75200/175341]\n",
      "loss: 0.654901  [76800/175341]\n",
      "loss: 0.719619  [78400/175341]\n",
      "loss: 0.455490  [80000/175341]\n",
      "loss: 0.396827  [81600/175341]\n",
      "loss: 0.537866  [83200/175341]\n",
      "loss: 0.321060  [84800/175341]\n",
      "loss: 0.441220  [86400/175341]\n",
      "loss: 0.662489  [88000/175341]\n",
      "loss: 0.777553  [89600/175341]\n",
      "loss: 0.329985  [91200/175341]\n",
      "loss: 0.349858  [92800/175341]\n",
      "loss: 0.157238  [94400/175341]\n",
      "loss: 0.360847  [96000/175341]\n",
      "loss: 0.229602  [97600/175341]\n",
      "loss: 0.585604  [99200/175341]\n",
      "loss: 0.604422  [100800/175341]\n",
      "loss: 0.294862  [102400/175341]\n",
      "loss: 0.338087  [104000/175341]\n",
      "loss: 0.329576  [105600/175341]\n",
      "loss: 0.548516  [107200/175341]\n",
      "loss: 0.914331  [108800/175341]\n",
      "loss: 0.274207  [110400/175341]\n",
      "loss: 0.411985  [112000/175341]\n",
      "loss: 0.480777  [113600/175341]\n",
      "loss: 0.227948  [115200/175341]\n",
      "loss: 0.749121  [116800/175341]\n",
      "loss: 0.237770  [118400/175341]\n",
      "loss: 0.713779  [120000/175341]\n",
      "loss: 0.935220  [121600/175341]\n",
      "loss: 0.712610  [123200/175341]\n",
      "loss: 0.822693  [124800/175341]\n",
      "loss: 0.222339  [126400/175341]\n",
      "loss: 0.906952  [128000/175341]\n",
      "loss: 0.209001  [129600/175341]\n",
      "loss: 0.735043  [131200/175341]\n",
      "loss: 0.332116  [132800/175341]\n",
      "loss: 0.368125  [134400/175341]\n",
      "loss: 0.494718  [136000/175341]\n",
      "loss: 0.512582  [137600/175341]\n",
      "loss: 0.435114  [139200/175341]\n",
      "loss: 0.561038  [140800/175341]\n",
      "loss: 0.407384  [142400/175341]\n",
      "loss: 0.129104  [144000/175341]\n",
      "loss: 0.156594  [145600/175341]\n",
      "loss: 0.296669  [147200/175341]\n",
      "loss: 0.732950  [148800/175341]\n",
      "loss: 0.180079  [150400/175341]\n",
      "loss: 0.166816  [152000/175341]\n",
      "loss: 0.290950  [153600/175341]\n",
      "loss: 0.321095  [155200/175341]\n",
      "loss: 0.344339  [156800/175341]\n",
      "loss: 0.351915  [158400/175341]\n",
      "loss: 0.183114  [160000/175341]\n",
      "loss: 0.484694  [161600/175341]\n",
      "loss: 0.462988  [163200/175341]\n",
      "loss: 0.670588  [164800/175341]\n",
      "loss: 0.703623  [166400/175341]\n",
      "loss: 0.649704  [168000/175341]\n",
      "loss: 0.629037  [169600/175341]\n",
      "loss: 0.244791  [171200/175341]\n",
      "loss: 0.374696  [172800/175341]\n",
      "loss: 0.380555  [174400/175341]\n",
      "Train Accuracy: 81.7932%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.554951, F1-score: 76.05%, Macro_F1-Score:  41.49%  \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.592480  [    0/175341]\n",
      "loss: 0.415516  [ 1600/175341]\n",
      "loss: 0.798265  [ 3200/175341]\n",
      "loss: 0.426017  [ 4800/175341]\n",
      "loss: 0.658962  [ 6400/175341]\n",
      "loss: 0.668906  [ 8000/175341]\n",
      "loss: 0.528188  [ 9600/175341]\n",
      "loss: 0.709118  [11200/175341]\n",
      "loss: 0.325577  [12800/175341]\n",
      "loss: 0.583056  [14400/175341]\n",
      "loss: 0.339409  [16000/175341]\n",
      "loss: 0.714666  [17600/175341]\n",
      "loss: 0.283926  [19200/175341]\n",
      "loss: 0.207287  [20800/175341]\n",
      "loss: 0.262770  [22400/175341]\n",
      "loss: 0.657777  [24000/175341]\n",
      "loss: 0.408572  [25600/175341]\n",
      "loss: 0.494189  [27200/175341]\n",
      "loss: 0.374002  [28800/175341]\n",
      "loss: 0.258587  [30400/175341]\n",
      "loss: 0.430909  [32000/175341]\n",
      "loss: 0.402828  [33600/175341]\n",
      "loss: 0.380423  [35200/175341]\n",
      "loss: 0.365752  [36800/175341]\n",
      "loss: 0.238398  [38400/175341]\n",
      "loss: 0.351685  [40000/175341]\n",
      "loss: 0.393368  [41600/175341]\n",
      "loss: 0.430586  [43200/175341]\n",
      "loss: 0.687504  [44800/175341]\n",
      "loss: 0.503896  [46400/175341]\n",
      "loss: 0.487988  [48000/175341]\n",
      "loss: 0.551754  [49600/175341]\n",
      "loss: 1.197540  [51200/175341]\n",
      "loss: 0.284090  [52800/175341]\n",
      "loss: 0.321195  [54400/175341]\n",
      "loss: 0.260706  [56000/175341]\n",
      "loss: 0.420702  [57600/175341]\n",
      "loss: 0.494721  [59200/175341]\n",
      "loss: 0.467608  [60800/175341]\n",
      "loss: 0.298051  [62400/175341]\n",
      "loss: 0.653259  [64000/175341]\n",
      "loss: 0.120377  [65600/175341]\n",
      "loss: 0.323583  [67200/175341]\n",
      "loss: 0.336625  [68800/175341]\n",
      "loss: 0.394503  [70400/175341]\n",
      "loss: 0.605113  [72000/175341]\n",
      "loss: 0.465353  [73600/175341]\n",
      "loss: 0.459515  [75200/175341]\n",
      "loss: 0.080598  [76800/175341]\n",
      "loss: 0.358082  [78400/175341]\n",
      "loss: 0.191603  [80000/175341]\n",
      "loss: 0.467772  [81600/175341]\n",
      "loss: 0.543318  [83200/175341]\n",
      "loss: 0.528051  [84800/175341]\n",
      "loss: 0.622953  [86400/175341]\n",
      "loss: 0.503294  [88000/175341]\n",
      "loss: 0.739380  [89600/175341]\n",
      "loss: 0.432649  [91200/175341]\n",
      "loss: 0.327816  [92800/175341]\n",
      "loss: 0.408648  [94400/175341]\n",
      "loss: 0.476496  [96000/175341]\n",
      "loss: 0.586859  [97600/175341]\n",
      "loss: 0.492815  [99200/175341]\n",
      "loss: 0.290151  [100800/175341]\n",
      "loss: 0.354623  [102400/175341]\n",
      "loss: 0.671641  [104000/175341]\n",
      "loss: 0.326637  [105600/175341]\n",
      "loss: 0.581173  [107200/175341]\n",
      "loss: 0.350443  [108800/175341]\n",
      "loss: 0.576716  [110400/175341]\n",
      "loss: 0.199985  [112000/175341]\n",
      "loss: 0.373588  [113600/175341]\n",
      "loss: 0.178336  [115200/175341]\n",
      "loss: 0.516263  [116800/175341]\n",
      "loss: 0.720244  [118400/175341]\n",
      "loss: 0.298148  [120000/175341]\n",
      "loss: 0.395253  [121600/175341]\n",
      "loss: 0.447739  [123200/175341]\n",
      "loss: 0.197033  [124800/175341]\n",
      "loss: 0.925194  [126400/175341]\n",
      "loss: 0.424484  [128000/175341]\n",
      "loss: 0.292884  [129600/175341]\n",
      "loss: 0.703586  [131200/175341]\n",
      "loss: 0.362932  [132800/175341]\n",
      "loss: 0.131287  [134400/175341]\n",
      "loss: 0.388826  [136000/175341]\n",
      "loss: 0.429565  [137600/175341]\n",
      "loss: 0.830769  [139200/175341]\n",
      "loss: 0.123903  [140800/175341]\n",
      "loss: 0.460362  [142400/175341]\n",
      "loss: 0.231059  [144000/175341]\n",
      "loss: 0.290172  [145600/175341]\n",
      "loss: 0.605886  [147200/175341]\n",
      "loss: 0.542048  [148800/175341]\n",
      "loss: 0.369614  [150400/175341]\n",
      "loss: 0.488737  [152000/175341]\n",
      "loss: 0.538382  [153600/175341]\n",
      "loss: 0.394735  [155200/175341]\n",
      "loss: 0.214724  [156800/175341]\n",
      "loss: 0.269289  [158400/175341]\n",
      "loss: 0.410725  [160000/175341]\n",
      "loss: 0.230019  [161600/175341]\n",
      "loss: 0.610389  [163200/175341]\n",
      "loss: 0.263747  [164800/175341]\n",
      "loss: 0.567618  [166400/175341]\n",
      "loss: 0.400681  [168000/175341]\n",
      "loss: 0.426131  [169600/175341]\n",
      "loss: 0.355549  [171200/175341]\n",
      "loss: 0.112269  [172800/175341]\n",
      "loss: 0.421234  [174400/175341]\n",
      "Train Accuracy: 81.8417%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.565898, F1-score: 75.62%, Macro_F1-Score:  41.35%  \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.291534  [    0/175341]\n",
      "loss: 0.486396  [ 1600/175341]\n",
      "loss: 0.168857  [ 3200/175341]\n",
      "loss: 0.676626  [ 4800/175341]\n",
      "loss: 0.491588  [ 6400/175341]\n",
      "loss: 0.252812  [ 8000/175341]\n",
      "loss: 0.864938  [ 9600/175341]\n",
      "loss: 0.681869  [11200/175341]\n",
      "loss: 0.643951  [12800/175341]\n",
      "loss: 0.126113  [14400/175341]\n",
      "loss: 0.436840  [16000/175341]\n",
      "loss: 0.683711  [17600/175341]\n",
      "loss: 0.631043  [19200/175341]\n",
      "loss: 0.477904  [20800/175341]\n",
      "loss: 0.374102  [22400/175341]\n",
      "loss: 0.873973  [24000/175341]\n",
      "loss: 0.424452  [25600/175341]\n",
      "loss: 0.185697  [27200/175341]\n",
      "loss: 0.150717  [28800/175341]\n",
      "loss: 0.141172  [30400/175341]\n",
      "loss: 0.564552  [32000/175341]\n",
      "loss: 0.382102  [33600/175341]\n",
      "loss: 0.425051  [35200/175341]\n",
      "loss: 0.407164  [36800/175341]\n",
      "loss: 0.551403  [38400/175341]\n",
      "loss: 0.731024  [40000/175341]\n",
      "loss: 0.504686  [41600/175341]\n",
      "loss: 0.132843  [43200/175341]\n",
      "loss: 0.497557  [44800/175341]\n",
      "loss: 0.328188  [46400/175341]\n",
      "loss: 0.233149  [48000/175341]\n",
      "loss: 0.803229  [49600/175341]\n",
      "loss: 0.280242  [51200/175341]\n",
      "loss: 0.259764  [52800/175341]\n",
      "loss: 0.414779  [54400/175341]\n",
      "loss: 0.409218  [56000/175341]\n",
      "loss: 0.437495  [57600/175341]\n",
      "loss: 0.119863  [59200/175341]\n",
      "loss: 0.336161  [60800/175341]\n",
      "loss: 0.536851  [62400/175341]\n",
      "loss: 0.227749  [64000/175341]\n",
      "loss: 0.289238  [65600/175341]\n",
      "loss: 0.899880  [67200/175341]\n",
      "loss: 0.299804  [68800/175341]\n",
      "loss: 0.266520  [70400/175341]\n",
      "loss: 0.084302  [72000/175341]\n",
      "loss: 0.352143  [73600/175341]\n",
      "loss: 0.301833  [75200/175341]\n",
      "loss: 0.622402  [76800/175341]\n",
      "loss: 0.270506  [78400/175341]\n",
      "loss: 0.332284  [80000/175341]\n",
      "loss: 0.740447  [81600/175341]\n",
      "loss: 0.479857  [83200/175341]\n",
      "loss: 0.422476  [84800/175341]\n",
      "loss: 0.192867  [86400/175341]\n",
      "loss: 0.298539  [88000/175341]\n",
      "loss: 0.247135  [89600/175341]\n",
      "loss: 0.359604  [91200/175341]\n",
      "loss: 0.403376  [92800/175341]\n",
      "loss: 0.603211  [94400/175341]\n",
      "loss: 0.232164  [96000/175341]\n",
      "loss: 0.505364  [97600/175341]\n",
      "loss: 0.494568  [99200/175341]\n",
      "loss: 0.337675  [100800/175341]\n",
      "loss: 0.312774  [102400/175341]\n",
      "loss: 0.423365  [104000/175341]\n",
      "loss: 0.670064  [105600/175341]\n",
      "loss: 0.433050  [107200/175341]\n",
      "loss: 0.437982  [108800/175341]\n",
      "loss: 0.398647  [110400/175341]\n",
      "loss: 0.535639  [112000/175341]\n",
      "loss: 0.548543  [113600/175341]\n",
      "loss: 0.535838  [115200/175341]\n",
      "loss: 0.491502  [116800/175341]\n",
      "loss: 0.451277  [118400/175341]\n",
      "loss: 0.571543  [120000/175341]\n",
      "loss: 0.334058  [121600/175341]\n",
      "loss: 0.223282  [123200/175341]\n",
      "loss: 0.540204  [124800/175341]\n",
      "loss: 0.254115  [126400/175341]\n",
      "loss: 0.400901  [128000/175341]\n",
      "loss: 0.398260  [129600/175341]\n",
      "loss: 0.372978  [131200/175341]\n",
      "loss: 0.603394  [132800/175341]\n",
      "loss: 0.243423  [134400/175341]\n",
      "loss: 0.421660  [136000/175341]\n",
      "loss: 0.598436  [137600/175341]\n",
      "loss: 0.134621  [139200/175341]\n",
      "loss: 0.412427  [140800/175341]\n",
      "loss: 0.619416  [142400/175341]\n",
      "loss: 0.654115  [144000/175341]\n",
      "loss: 0.643402  [145600/175341]\n",
      "loss: 0.520061  [147200/175341]\n",
      "loss: 0.394766  [148800/175341]\n",
      "loss: 0.273550  [150400/175341]\n",
      "loss: 0.221219  [152000/175341]\n",
      "loss: 0.541927  [153600/175341]\n",
      "loss: 0.643261  [155200/175341]\n",
      "loss: 0.387048  [156800/175341]\n",
      "loss: 0.233126  [158400/175341]\n",
      "loss: 0.763691  [160000/175341]\n",
      "loss: 0.449510  [161600/175341]\n",
      "loss: 0.341272  [163200/175341]\n",
      "loss: 0.672954  [164800/175341]\n",
      "loss: 0.776535  [166400/175341]\n",
      "loss: 0.435512  [168000/175341]\n",
      "loss: 0.405875  [169600/175341]\n",
      "loss: 0.275256  [171200/175341]\n",
      "loss: 0.510038  [172800/175341]\n",
      "loss: 0.218106  [174400/175341]\n",
      "Train Accuracy: 81.8251%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.546753, F1-score: 77.14%, Macro_F1-Score:  41.63%  \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.141150  [    0/175341]\n",
      "loss: 0.402447  [ 1600/175341]\n",
      "loss: 0.807500  [ 3200/175341]\n",
      "loss: 0.405890  [ 4800/175341]\n",
      "loss: 0.565329  [ 6400/175341]\n",
      "loss: 0.175935  [ 8000/175341]\n",
      "loss: 0.241260  [ 9600/175341]\n",
      "loss: 0.661702  [11200/175341]\n",
      "loss: 0.459306  [12800/175341]\n",
      "loss: 0.365762  [14400/175341]\n",
      "loss: 0.367465  [16000/175341]\n",
      "loss: 0.560377  [17600/175341]\n",
      "loss: 0.418447  [19200/175341]\n",
      "loss: 0.390519  [20800/175341]\n",
      "loss: 0.568790  [22400/175341]\n",
      "loss: 0.575096  [24000/175341]\n",
      "loss: 0.069653  [25600/175341]\n",
      "loss: 0.114774  [27200/175341]\n",
      "loss: 0.581031  [28800/175341]\n",
      "loss: 0.279413  [30400/175341]\n",
      "loss: 0.540633  [32000/175341]\n",
      "loss: 0.393308  [33600/175341]\n",
      "loss: 0.227391  [35200/175341]\n",
      "loss: 0.429073  [36800/175341]\n",
      "loss: 0.086072  [38400/175341]\n",
      "loss: 0.467250  [40000/175341]\n",
      "loss: 0.402235  [41600/175341]\n",
      "loss: 0.458218  [43200/175341]\n",
      "loss: 0.434618  [44800/175341]\n",
      "loss: 0.236553  [46400/175341]\n",
      "loss: 0.663277  [48000/175341]\n",
      "loss: 0.450344  [49600/175341]\n",
      "loss: 0.135391  [51200/175341]\n",
      "loss: 0.562303  [52800/175341]\n",
      "loss: 0.539055  [54400/175341]\n",
      "loss: 0.513474  [56000/175341]\n",
      "loss: 0.435057  [57600/175341]\n",
      "loss: 0.734505  [59200/175341]\n",
      "loss: 0.752683  [60800/175341]\n",
      "loss: 0.811824  [62400/175341]\n",
      "loss: 0.783174  [64000/175341]\n",
      "loss: 0.500445  [65600/175341]\n",
      "loss: 0.138906  [67200/175341]\n",
      "loss: 0.556691  [68800/175341]\n",
      "loss: 0.515847  [70400/175341]\n",
      "loss: 0.559785  [72000/175341]\n",
      "loss: 0.513089  [73600/175341]\n",
      "loss: 0.837853  [75200/175341]\n",
      "loss: 0.298213  [76800/175341]\n",
      "loss: 0.646279  [78400/175341]\n",
      "loss: 0.199615  [80000/175341]\n",
      "loss: 0.273150  [81600/175341]\n",
      "loss: 0.238368  [83200/175341]\n",
      "loss: 0.775424  [84800/175341]\n",
      "loss: 0.356281  [86400/175341]\n",
      "loss: 0.531613  [88000/175341]\n",
      "loss: 0.591627  [89600/175341]\n",
      "loss: 0.427139  [91200/175341]\n",
      "loss: 0.408940  [92800/175341]\n",
      "loss: 0.498943  [94400/175341]\n",
      "loss: 0.184226  [96000/175341]\n",
      "loss: 0.417750  [97600/175341]\n",
      "loss: 0.509518  [99200/175341]\n",
      "loss: 0.373894  [100800/175341]\n",
      "loss: 0.480539  [102400/175341]\n",
      "loss: 0.518333  [104000/175341]\n",
      "loss: 0.359492  [105600/175341]\n",
      "loss: 0.512976  [107200/175341]\n",
      "loss: 0.274441  [108800/175341]\n",
      "loss: 0.422752  [110400/175341]\n",
      "loss: 0.559749  [112000/175341]\n",
      "loss: 1.131534  [113600/175341]\n",
      "loss: 0.391482  [115200/175341]\n",
      "loss: 0.358754  [116800/175341]\n",
      "loss: 0.558474  [118400/175341]\n",
      "loss: 0.634969  [120000/175341]\n",
      "loss: 0.513732  [121600/175341]\n",
      "loss: 0.610284  [123200/175341]\n",
      "loss: 0.529484  [124800/175341]\n",
      "loss: 1.349283  [126400/175341]\n",
      "loss: 0.320604  [128000/175341]\n",
      "loss: 0.311123  [129600/175341]\n",
      "loss: 0.592990  [131200/175341]\n",
      "loss: 0.714203  [132800/175341]\n",
      "loss: 0.319067  [134400/175341]\n",
      "loss: 0.235656  [136000/175341]\n",
      "loss: 0.527370  [137600/175341]\n",
      "loss: 0.497510  [139200/175341]\n",
      "loss: 0.384806  [140800/175341]\n",
      "loss: 0.610118  [142400/175341]\n",
      "loss: 0.815341  [144000/175341]\n",
      "loss: 0.480770  [145600/175341]\n",
      "loss: 0.730524  [147200/175341]\n",
      "loss: 0.309550  [148800/175341]\n",
      "loss: 0.268050  [150400/175341]\n",
      "loss: 0.288781  [152000/175341]\n",
      "loss: 0.474770  [153600/175341]\n",
      "loss: 0.425811  [155200/175341]\n",
      "loss: 0.479840  [156800/175341]\n",
      "loss: 0.565941  [158400/175341]\n",
      "loss: 0.571273  [160000/175341]\n",
      "loss: 0.497863  [161600/175341]\n",
      "loss: 0.378182  [163200/175341]\n",
      "loss: 0.669963  [164800/175341]\n",
      "loss: 0.592430  [166400/175341]\n",
      "loss: 0.517394  [168000/175341]\n",
      "loss: 0.585730  [169600/175341]\n",
      "loss: 1.202525  [171200/175341]\n",
      "loss: 0.466353  [172800/175341]\n",
      "loss: 0.981889  [174400/175341]\n",
      "Train Accuracy: 81.8074%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.543785, F1-score: 76.57%, Macro_F1-Score:  41.49%  \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.663112  [    0/175341]\n",
      "loss: 0.317245  [ 1600/175341]\n",
      "loss: 0.292893  [ 3200/175341]\n",
      "loss: 0.663464  [ 4800/175341]\n",
      "loss: 0.765199  [ 6400/175341]\n",
      "loss: 0.393157  [ 8000/175341]\n",
      "loss: 0.806531  [ 9600/175341]\n",
      "loss: 0.231947  [11200/175341]\n",
      "loss: 0.256742  [12800/175341]\n",
      "loss: 0.384018  [14400/175341]\n",
      "loss: 0.258252  [16000/175341]\n",
      "loss: 0.687434  [17600/175341]\n",
      "loss: 0.268802  [19200/175341]\n",
      "loss: 0.492024  [20800/175341]\n",
      "loss: 0.110772  [22400/175341]\n",
      "loss: 0.224975  [24000/175341]\n",
      "loss: 0.354786  [25600/175341]\n",
      "loss: 0.704436  [27200/175341]\n",
      "loss: 0.432747  [28800/175341]\n",
      "loss: 0.188603  [30400/175341]\n",
      "loss: 1.066053  [32000/175341]\n",
      "loss: 0.188323  [33600/175341]\n",
      "loss: 0.227855  [35200/175341]\n",
      "loss: 0.389371  [36800/175341]\n",
      "loss: 0.578765  [38400/175341]\n",
      "loss: 0.358068  [40000/175341]\n",
      "loss: 0.238125  [41600/175341]\n",
      "loss: 0.291769  [43200/175341]\n",
      "loss: 0.835976  [44800/175341]\n",
      "loss: 0.516021  [46400/175341]\n",
      "loss: 0.513875  [48000/175341]\n",
      "loss: 0.324687  [49600/175341]\n",
      "loss: 0.440388  [51200/175341]\n",
      "loss: 0.786168  [52800/175341]\n",
      "loss: 0.376260  [54400/175341]\n",
      "loss: 0.166318  [56000/175341]\n",
      "loss: 0.269167  [57600/175341]\n",
      "loss: 0.292092  [59200/175341]\n",
      "loss: 0.765909  [60800/175341]\n",
      "loss: 0.338884  [62400/175341]\n",
      "loss: 0.596965  [64000/175341]\n",
      "loss: 0.162524  [65600/175341]\n",
      "loss: 0.545105  [67200/175341]\n",
      "loss: 0.400780  [68800/175341]\n",
      "loss: 0.650819  [70400/175341]\n",
      "loss: 0.172656  [72000/175341]\n",
      "loss: 0.661517  [73600/175341]\n",
      "loss: 0.635749  [75200/175341]\n",
      "loss: 0.612265  [76800/175341]\n",
      "loss: 0.314865  [78400/175341]\n",
      "loss: 0.366996  [80000/175341]\n",
      "loss: 0.595453  [81600/175341]\n",
      "loss: 0.534005  [83200/175341]\n",
      "loss: 0.410086  [84800/175341]\n",
      "loss: 0.773107  [86400/175341]\n",
      "loss: 0.710346  [88000/175341]\n",
      "loss: 0.333869  [89600/175341]\n",
      "loss: 0.432912  [91200/175341]\n",
      "loss: 0.348657  [92800/175341]\n",
      "loss: 0.213086  [94400/175341]\n",
      "loss: 0.605779  [96000/175341]\n",
      "loss: 0.594144  [97600/175341]\n",
      "loss: 0.570275  [99200/175341]\n",
      "loss: 0.388751  [100800/175341]\n",
      "loss: 0.645866  [102400/175341]\n",
      "loss: 0.355841  [104000/175341]\n",
      "loss: 0.440232  [105600/175341]\n",
      "loss: 0.570311  [107200/175341]\n",
      "loss: 0.268226  [108800/175341]\n",
      "loss: 0.887483  [110400/175341]\n",
      "loss: 0.368617  [112000/175341]\n",
      "loss: 0.462092  [113600/175341]\n",
      "loss: 0.493637  [115200/175341]\n",
      "loss: 0.289586  [116800/175341]\n",
      "loss: 0.458855  [118400/175341]\n",
      "loss: 1.022042  [120000/175341]\n",
      "loss: 0.613965  [121600/175341]\n",
      "loss: 0.603192  [123200/175341]\n",
      "loss: 0.471408  [124800/175341]\n",
      "loss: 0.221021  [126400/175341]\n",
      "loss: 0.513547  [128000/175341]\n",
      "loss: 0.245147  [129600/175341]\n",
      "loss: 0.732329  [131200/175341]\n",
      "loss: 0.536090  [132800/175341]\n",
      "loss: 0.546345  [134400/175341]\n",
      "loss: 0.479215  [136000/175341]\n",
      "loss: 0.280207  [137600/175341]\n",
      "loss: 0.425085  [139200/175341]\n",
      "loss: 0.180781  [140800/175341]\n",
      "loss: 0.546115  [142400/175341]\n",
      "loss: 0.675991  [144000/175341]\n",
      "loss: 0.189759  [145600/175341]\n",
      "loss: 0.619173  [147200/175341]\n",
      "loss: 0.212061  [148800/175341]\n",
      "loss: 0.505730  [150400/175341]\n",
      "loss: 0.549784  [152000/175341]\n",
      "loss: 0.312840  [153600/175341]\n",
      "loss: 0.619757  [155200/175341]\n",
      "loss: 0.324763  [156800/175341]\n",
      "loss: 0.430195  [158400/175341]\n",
      "loss: 0.236000  [160000/175341]\n",
      "loss: 0.143292  [161600/175341]\n",
      "loss: 0.510209  [163200/175341]\n",
      "loss: 0.594029  [164800/175341]\n",
      "loss: 0.210137  [166400/175341]\n",
      "loss: 0.331718  [168000/175341]\n",
      "loss: 0.670963  [169600/175341]\n",
      "loss: 0.232363  [171200/175341]\n",
      "loss: 0.578186  [172800/175341]\n",
      "loss: 0.512222  [174400/175341]\n",
      "Train Accuracy: 81.7829%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.563736, F1-score: 75.70%, Macro_F1-Score:  41.15%  \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.681804  [    0/175341]\n",
      "loss: 0.415450  [ 1600/175341]\n",
      "loss: 0.781591  [ 3200/175341]\n",
      "loss: 0.505275  [ 4800/175341]\n",
      "loss: 0.118449  [ 6400/175341]\n",
      "loss: 0.619671  [ 8000/175341]\n",
      "loss: 0.210557  [ 9600/175341]\n",
      "loss: 0.659824  [11200/175341]\n",
      "loss: 0.450266  [12800/175341]\n",
      "loss: 0.394335  [14400/175341]\n",
      "loss: 0.683136  [16000/175341]\n",
      "loss: 0.578904  [17600/175341]\n",
      "loss: 0.730135  [19200/175341]\n",
      "loss: 0.335484  [20800/175341]\n",
      "loss: 0.472224  [22400/175341]\n",
      "loss: 0.627116  [24000/175341]\n",
      "loss: 0.561520  [25600/175341]\n",
      "loss: 0.598046  [27200/175341]\n",
      "loss: 0.400217  [28800/175341]\n",
      "loss: 0.530156  [30400/175341]\n",
      "loss: 0.566931  [32000/175341]\n",
      "loss: 0.532542  [33600/175341]\n",
      "loss: 0.190821  [35200/175341]\n",
      "loss: 0.704426  [36800/175341]\n",
      "loss: 0.843522  [38400/175341]\n",
      "loss: 0.172505  [40000/175341]\n",
      "loss: 0.200228  [41600/175341]\n",
      "loss: 0.442638  [43200/175341]\n",
      "loss: 0.360041  [44800/175341]\n",
      "loss: 0.291369  [46400/175341]\n",
      "loss: 0.258658  [48000/175341]\n",
      "loss: 0.375286  [49600/175341]\n",
      "loss: 0.230619  [51200/175341]\n",
      "loss: 0.355143  [52800/175341]\n",
      "loss: 0.416834  [54400/175341]\n",
      "loss: 0.668522  [56000/175341]\n",
      "loss: 0.512826  [57600/175341]\n",
      "loss: 0.469380  [59200/175341]\n",
      "loss: 0.582613  [60800/175341]\n",
      "loss: 0.230386  [62400/175341]\n",
      "loss: 0.279112  [64000/175341]\n",
      "loss: 0.845902  [65600/175341]\n",
      "loss: 0.220681  [67200/175341]\n",
      "loss: 0.847958  [68800/175341]\n",
      "loss: 0.302468  [70400/175341]\n",
      "loss: 0.388434  [72000/175341]\n",
      "loss: 0.484881  [73600/175341]\n",
      "loss: 0.349671  [75200/175341]\n",
      "loss: 0.247694  [76800/175341]\n",
      "loss: 0.442365  [78400/175341]\n",
      "loss: 0.509353  [80000/175341]\n",
      "loss: 0.332672  [81600/175341]\n",
      "loss: 0.371871  [83200/175341]\n",
      "loss: 0.744270  [84800/175341]\n",
      "loss: 0.591480  [86400/175341]\n",
      "loss: 0.285994  [88000/175341]\n",
      "loss: 0.605847  [89600/175341]\n",
      "loss: 0.370508  [91200/175341]\n",
      "loss: 0.586233  [92800/175341]\n",
      "loss: 0.346272  [94400/175341]\n",
      "loss: 0.439502  [96000/175341]\n",
      "loss: 0.602951  [97600/175341]\n",
      "loss: 0.575216  [99200/175341]\n",
      "loss: 0.402680  [100800/175341]\n",
      "loss: 0.698245  [102400/175341]\n",
      "loss: 0.577799  [104000/175341]\n",
      "loss: 0.463595  [105600/175341]\n",
      "loss: 0.182185  [107200/175341]\n",
      "loss: 0.383468  [108800/175341]\n",
      "loss: 0.231993  [110400/175341]\n",
      "loss: 0.504699  [112000/175341]\n",
      "loss: 0.590676  [113600/175341]\n",
      "loss: 0.842790  [115200/175341]\n",
      "loss: 1.130814  [116800/175341]\n",
      "loss: 0.620261  [118400/175341]\n",
      "loss: 0.537057  [120000/175341]\n",
      "loss: 0.265839  [121600/175341]\n",
      "loss: 0.725977  [123200/175341]\n",
      "loss: 0.279382  [124800/175341]\n",
      "loss: 0.324101  [126400/175341]\n",
      "loss: 0.533524  [128000/175341]\n",
      "loss: 0.581342  [129600/175341]\n",
      "loss: 0.142489  [131200/175341]\n",
      "loss: 0.849362  [132800/175341]\n",
      "loss: 0.719122  [134400/175341]\n",
      "loss: 0.401996  [136000/175341]\n",
      "loss: 0.477399  [137600/175341]\n",
      "loss: 0.816968  [139200/175341]\n",
      "loss: 0.584395  [140800/175341]\n",
      "loss: 0.499734  [142400/175341]\n",
      "loss: 0.339059  [144000/175341]\n",
      "loss: 0.368104  [145600/175341]\n",
      "loss: 0.537343  [147200/175341]\n",
      "loss: 0.729181  [148800/175341]\n",
      "loss: 0.298547  [150400/175341]\n",
      "loss: 0.507197  [152000/175341]\n",
      "loss: 0.476559  [153600/175341]\n",
      "loss: 0.270262  [155200/175341]\n",
      "loss: 0.217169  [156800/175341]\n",
      "loss: 0.480060  [158400/175341]\n",
      "loss: 0.329513  [160000/175341]\n",
      "loss: 0.954852  [161600/175341]\n",
      "loss: 0.192313  [163200/175341]\n",
      "loss: 0.447890  [164800/175341]\n",
      "loss: 0.287169  [166400/175341]\n",
      "loss: 0.452439  [168000/175341]\n",
      "loss: 0.350476  [169600/175341]\n",
      "loss: 0.351856  [171200/175341]\n",
      "loss: 0.522606  [172800/175341]\n",
      "loss: 0.479120  [174400/175341]\n",
      "Train Accuracy: 81.7966%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.548755, F1-score: 76.29%, Macro_F1-Score:  41.28%  \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.403271  [    0/175341]\n",
      "loss: 0.611811  [ 1600/175341]\n",
      "loss: 0.659900  [ 3200/175341]\n",
      "loss: 0.348440  [ 4800/175341]\n",
      "loss: 0.383690  [ 6400/175341]\n",
      "loss: 0.738694  [ 8000/175341]\n",
      "loss: 0.474163  [ 9600/175341]\n",
      "loss: 0.238009  [11200/175341]\n",
      "loss: 0.353420  [12800/175341]\n",
      "loss: 0.669286  [14400/175341]\n",
      "loss: 0.458969  [16000/175341]\n",
      "loss: 0.480898  [17600/175341]\n",
      "loss: 0.168026  [19200/175341]\n",
      "loss: 0.614407  [20800/175341]\n",
      "loss: 0.418819  [22400/175341]\n",
      "loss: 0.729450  [24000/175341]\n",
      "loss: 0.530591  [25600/175341]\n",
      "loss: 0.519746  [27200/175341]\n",
      "loss: 1.242460  [28800/175341]\n",
      "loss: 0.433025  [30400/175341]\n",
      "loss: 0.422606  [32000/175341]\n",
      "loss: 0.754399  [33600/175341]\n",
      "loss: 1.101668  [35200/175341]\n",
      "loss: 0.744403  [36800/175341]\n",
      "loss: 0.244695  [38400/175341]\n",
      "loss: 0.683541  [40000/175341]\n",
      "loss: 0.301512  [41600/175341]\n",
      "loss: 0.440208  [43200/175341]\n",
      "loss: 0.382812  [44800/175341]\n",
      "loss: 0.474540  [46400/175341]\n",
      "loss: 0.225995  [48000/175341]\n",
      "loss: 0.213750  [49600/175341]\n",
      "loss: 0.131421  [51200/175341]\n",
      "loss: 0.179447  [52800/175341]\n",
      "loss: 0.433490  [54400/175341]\n",
      "loss: 0.156392  [56000/175341]\n",
      "loss: 0.317885  [57600/175341]\n",
      "loss: 0.480291  [59200/175341]\n",
      "loss: 0.442970  [60800/175341]\n",
      "loss: 0.656266  [62400/175341]\n",
      "loss: 0.663197  [64000/175341]\n",
      "loss: 0.746308  [65600/175341]\n",
      "loss: 0.395159  [67200/175341]\n",
      "loss: 0.447241  [68800/175341]\n",
      "loss: 0.299459  [70400/175341]\n",
      "loss: 0.104770  [72000/175341]\n",
      "loss: 0.503583  [73600/175341]\n",
      "loss: 0.365468  [75200/175341]\n",
      "loss: 0.383408  [76800/175341]\n",
      "loss: 0.336042  [78400/175341]\n",
      "loss: 0.399694  [80000/175341]\n",
      "loss: 0.396999  [81600/175341]\n",
      "loss: 0.739819  [83200/175341]\n",
      "loss: 0.371667  [84800/175341]\n",
      "loss: 0.827816  [86400/175341]\n",
      "loss: 0.135021  [88000/175341]\n",
      "loss: 0.475970  [89600/175341]\n",
      "loss: 0.220604  [91200/175341]\n",
      "loss: 0.299615  [92800/175341]\n",
      "loss: 0.427291  [94400/175341]\n",
      "loss: 0.295406  [96000/175341]\n",
      "loss: 0.419067  [97600/175341]\n",
      "loss: 0.605009  [99200/175341]\n",
      "loss: 0.661870  [100800/175341]\n",
      "loss: 0.305361  [102400/175341]\n",
      "loss: 0.399197  [104000/175341]\n",
      "loss: 0.324993  [105600/175341]\n",
      "loss: 0.866336  [107200/175341]\n",
      "loss: 0.480234  [108800/175341]\n",
      "loss: 0.351369  [110400/175341]\n",
      "loss: 0.405727  [112000/175341]\n",
      "loss: 0.336447  [113600/175341]\n",
      "loss: 0.504011  [115200/175341]\n",
      "loss: 0.673271  [116800/175341]\n",
      "loss: 0.333897  [118400/175341]\n",
      "loss: 0.414490  [120000/175341]\n",
      "loss: 0.460815  [121600/175341]\n",
      "loss: 0.415789  [123200/175341]\n",
      "loss: 0.493349  [124800/175341]\n",
      "loss: 0.235359  [126400/175341]\n",
      "loss: 0.219871  [128000/175341]\n",
      "loss: 0.177980  [129600/175341]\n",
      "loss: 0.281097  [131200/175341]\n",
      "loss: 0.291290  [132800/175341]\n",
      "loss: 0.544396  [134400/175341]\n",
      "loss: 0.439495  [136000/175341]\n",
      "loss: 0.240049  [137600/175341]\n",
      "loss: 0.320853  [139200/175341]\n",
      "loss: 0.273517  [140800/175341]\n",
      "loss: 0.276300  [142400/175341]\n",
      "loss: 0.165186  [144000/175341]\n",
      "loss: 0.377741  [145600/175341]\n",
      "loss: 0.704772  [147200/175341]\n",
      "loss: 0.793819  [148800/175341]\n",
      "loss: 0.434234  [150400/175341]\n",
      "loss: 0.484480  [152000/175341]\n",
      "loss: 0.331206  [153600/175341]\n",
      "loss: 0.269708  [155200/175341]\n",
      "loss: 0.817350  [156800/175341]\n",
      "loss: 0.452282  [158400/175341]\n",
      "loss: 0.663407  [160000/175341]\n",
      "loss: 0.318160  [161600/175341]\n",
      "loss: 0.648512  [163200/175341]\n",
      "loss: 0.220882  [164800/175341]\n",
      "loss: 0.497174  [166400/175341]\n",
      "loss: 0.288618  [168000/175341]\n",
      "loss: 0.253985  [169600/175341]\n",
      "loss: 0.524326  [171200/175341]\n",
      "loss: 0.344610  [172800/175341]\n",
      "loss: 0.371763  [174400/175341]\n",
      "Train Accuracy: 81.8462%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.553714, F1-score: 76.13%, Macro_F1-Score:  41.58%  \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.321897  [    0/175341]\n",
      "loss: 0.390519  [ 1600/175341]\n",
      "loss: 0.570912  [ 3200/175341]\n",
      "loss: 0.262426  [ 4800/175341]\n",
      "loss: 0.973102  [ 6400/175341]\n",
      "loss: 0.289463  [ 8000/175341]\n",
      "loss: 0.159211  [ 9600/175341]\n",
      "loss: 0.587355  [11200/175341]\n",
      "loss: 0.528939  [12800/175341]\n",
      "loss: 0.225745  [14400/175341]\n",
      "loss: 0.439544  [16000/175341]\n",
      "loss: 0.359503  [17600/175341]\n",
      "loss: 0.597791  [19200/175341]\n",
      "loss: 0.674522  [20800/175341]\n",
      "loss: 0.340059  [22400/175341]\n",
      "loss: 0.412219  [24000/175341]\n",
      "loss: 0.180908  [25600/175341]\n",
      "loss: 0.568944  [27200/175341]\n",
      "loss: 0.516465  [28800/175341]\n",
      "loss: 0.266991  [30400/175341]\n",
      "loss: 0.281480  [32000/175341]\n",
      "loss: 0.171025  [33600/175341]\n",
      "loss: 0.407227  [35200/175341]\n",
      "loss: 0.460230  [36800/175341]\n",
      "loss: 0.183687  [38400/175341]\n",
      "loss: 0.530198  [40000/175341]\n",
      "loss: 0.382481  [41600/175341]\n",
      "loss: 0.213196  [43200/175341]\n",
      "loss: 0.310007  [44800/175341]\n",
      "loss: 0.895475  [46400/175341]\n",
      "loss: 0.716465  [48000/175341]\n",
      "loss: 0.364612  [49600/175341]\n",
      "loss: 0.418226  [51200/175341]\n",
      "loss: 0.704374  [52800/175341]\n",
      "loss: 0.323801  [54400/175341]\n",
      "loss: 0.706077  [56000/175341]\n",
      "loss: 0.464731  [57600/175341]\n",
      "loss: 0.223985  [59200/175341]\n",
      "loss: 0.869434  [60800/175341]\n",
      "loss: 0.226892  [62400/175341]\n",
      "loss: 0.563173  [64000/175341]\n",
      "loss: 0.562235  [65600/175341]\n",
      "loss: 0.693674  [67200/175341]\n",
      "loss: 0.682277  [68800/175341]\n",
      "loss: 0.360401  [70400/175341]\n",
      "loss: 0.406903  [72000/175341]\n",
      "loss: 0.475569  [73600/175341]\n",
      "loss: 0.555476  [75200/175341]\n",
      "loss: 0.313361  [76800/175341]\n",
      "loss: 0.592468  [78400/175341]\n",
      "loss: 0.567574  [80000/175341]\n",
      "loss: 0.356275  [81600/175341]\n",
      "loss: 0.239827  [83200/175341]\n",
      "loss: 0.135666  [84800/175341]\n",
      "loss: 0.236935  [86400/175341]\n",
      "loss: 0.619698  [88000/175341]\n",
      "loss: 0.493728  [89600/175341]\n",
      "loss: 0.752914  [91200/175341]\n",
      "loss: 0.599714  [92800/175341]\n",
      "loss: 0.547409  [94400/175341]\n",
      "loss: 0.172428  [96000/175341]\n",
      "loss: 0.624362  [97600/175341]\n",
      "loss: 0.485488  [99200/175341]\n",
      "loss: 0.666629  [100800/175341]\n",
      "loss: 0.618960  [102400/175341]\n",
      "loss: 0.522532  [104000/175341]\n",
      "loss: 0.761404  [105600/175341]\n",
      "loss: 0.514134  [107200/175341]\n",
      "loss: 0.417737  [108800/175341]\n",
      "loss: 0.381375  [110400/175341]\n",
      "loss: 0.329052  [112000/175341]\n",
      "loss: 0.410593  [113600/175341]\n",
      "loss: 0.356190  [115200/175341]\n",
      "loss: 0.774158  [116800/175341]\n",
      "loss: 0.557133  [118400/175341]\n",
      "loss: 0.537482  [120000/175341]\n",
      "loss: 0.544030  [121600/175341]\n",
      "loss: 0.327509  [123200/175341]\n",
      "loss: 0.362048  [124800/175341]\n",
      "loss: 0.577809  [126400/175341]\n",
      "loss: 0.655918  [128000/175341]\n",
      "loss: 0.624908  [129600/175341]\n",
      "loss: 0.600911  [131200/175341]\n",
      "loss: 0.444378  [132800/175341]\n",
      "loss: 0.156601  [134400/175341]\n",
      "loss: 0.464581  [136000/175341]\n",
      "loss: 1.034460  [137600/175341]\n",
      "loss: 0.510980  [139200/175341]\n",
      "loss: 0.524643  [140800/175341]\n",
      "loss: 0.510180  [142400/175341]\n",
      "loss: 0.173498  [144000/175341]\n",
      "loss: 0.405664  [145600/175341]\n",
      "loss: 0.759996  [147200/175341]\n",
      "loss: 0.716007  [148800/175341]\n",
      "loss: 0.368697  [150400/175341]\n",
      "loss: 0.682334  [152000/175341]\n",
      "loss: 0.581933  [153600/175341]\n",
      "loss: 1.008647  [155200/175341]\n",
      "loss: 0.429523  [156800/175341]\n",
      "loss: 0.538245  [158400/175341]\n",
      "loss: 0.705837  [160000/175341]\n",
      "loss: 0.485444  [161600/175341]\n",
      "loss: 0.680623  [163200/175341]\n",
      "loss: 0.692712  [164800/175341]\n",
      "loss: 0.313704  [166400/175341]\n",
      "loss: 0.546431  [168000/175341]\n",
      "loss: 0.323899  [169600/175341]\n",
      "loss: 0.478223  [171200/175341]\n",
      "loss: 0.985156  [172800/175341]\n",
      "loss: 0.393487  [174400/175341]\n",
      "Train Accuracy: 81.8132%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.534105, F1-score: 77.26%, Macro_F1-Score:  41.72%  \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.226162  [    0/175341]\n",
      "loss: 0.220937  [ 1600/175341]\n",
      "loss: 0.495698  [ 3200/175341]\n",
      "loss: 0.164645  [ 4800/175341]\n",
      "loss: 0.273404  [ 6400/175341]\n",
      "loss: 0.573685  [ 8000/175341]\n",
      "loss: 0.299186  [ 9600/175341]\n",
      "loss: 0.155628  [11200/175341]\n",
      "loss: 0.442787  [12800/175341]\n",
      "loss: 0.193668  [14400/175341]\n",
      "loss: 0.098252  [16000/175341]\n",
      "loss: 0.240882  [17600/175341]\n",
      "loss: 0.113642  [19200/175341]\n",
      "loss: 0.426533  [20800/175341]\n",
      "loss: 0.650445  [22400/175341]\n",
      "loss: 0.303162  [24000/175341]\n",
      "loss: 0.314629  [25600/175341]\n",
      "loss: 0.656570  [27200/175341]\n",
      "loss: 1.130007  [28800/175341]\n",
      "loss: 0.847981  [30400/175341]\n",
      "loss: 0.413951  [32000/175341]\n",
      "loss: 0.485007  [33600/175341]\n",
      "loss: 0.463893  [35200/175341]\n",
      "loss: 0.280829  [36800/175341]\n",
      "loss: 0.705980  [38400/175341]\n",
      "loss: 0.420828  [40000/175341]\n",
      "loss: 0.426419  [41600/175341]\n",
      "loss: 0.398935  [43200/175341]\n",
      "loss: 0.151576  [44800/175341]\n",
      "loss: 0.634812  [46400/175341]\n",
      "loss: 0.270229  [48000/175341]\n",
      "loss: 0.442558  [49600/175341]\n",
      "loss: 0.540757  [51200/175341]\n",
      "loss: 0.336642  [52800/175341]\n",
      "loss: 0.463568  [54400/175341]\n",
      "loss: 0.388850  [56000/175341]\n",
      "loss: 0.276636  [57600/175341]\n",
      "loss: 0.316618  [59200/175341]\n",
      "loss: 0.460601  [60800/175341]\n",
      "loss: 0.489985  [62400/175341]\n",
      "loss: 0.796571  [64000/175341]\n",
      "loss: 0.570354  [65600/175341]\n",
      "loss: 0.592765  [67200/175341]\n",
      "loss: 0.534556  [68800/175341]\n",
      "loss: 0.365415  [70400/175341]\n",
      "loss: 0.250000  [72000/175341]\n",
      "loss: 0.611112  [73600/175341]\n",
      "loss: 0.585857  [75200/175341]\n",
      "loss: 0.791438  [76800/175341]\n",
      "loss: 0.415659  [78400/175341]\n",
      "loss: 0.420014  [80000/175341]\n",
      "loss: 0.202107  [81600/175341]\n",
      "loss: 0.419707  [83200/175341]\n",
      "loss: 0.466970  [84800/175341]\n",
      "loss: 0.567479  [86400/175341]\n",
      "loss: 0.429533  [88000/175341]\n",
      "loss: 0.204677  [89600/175341]\n",
      "loss: 0.450077  [91200/175341]\n",
      "loss: 0.926952  [92800/175341]\n",
      "loss: 1.127776  [94400/175341]\n",
      "loss: 0.319191  [96000/175341]\n",
      "loss: 0.292819  [97600/175341]\n",
      "loss: 0.272753  [99200/175341]\n",
      "loss: 0.633819  [100800/175341]\n",
      "loss: 0.452393  [102400/175341]\n",
      "loss: 0.815029  [104000/175341]\n",
      "loss: 0.262537  [105600/175341]\n",
      "loss: 0.103670  [107200/175341]\n",
      "loss: 0.294045  [108800/175341]\n",
      "loss: 0.309454  [110400/175341]\n",
      "loss: 0.375888  [112000/175341]\n",
      "loss: 0.444501  [113600/175341]\n",
      "loss: 0.558700  [115200/175341]\n",
      "loss: 0.492546  [116800/175341]\n",
      "loss: 0.419712  [118400/175341]\n",
      "loss: 0.575342  [120000/175341]\n",
      "loss: 0.288558  [121600/175341]\n",
      "loss: 0.343773  [123200/175341]\n",
      "loss: 0.344247  [124800/175341]\n",
      "loss: 0.674983  [126400/175341]\n",
      "loss: 0.499524  [128000/175341]\n",
      "loss: 0.076694  [129600/175341]\n",
      "loss: 0.209073  [131200/175341]\n",
      "loss: 0.439368  [132800/175341]\n",
      "loss: 0.312103  [134400/175341]\n",
      "loss: 0.376858  [136000/175341]\n",
      "loss: 0.403714  [137600/175341]\n",
      "loss: 0.319933  [139200/175341]\n",
      "loss: 0.445929  [140800/175341]\n",
      "loss: 0.383437  [142400/175341]\n",
      "loss: 0.392960  [144000/175341]\n",
      "loss: 0.731244  [145600/175341]\n",
      "loss: 0.467332  [147200/175341]\n",
      "loss: 0.813998  [148800/175341]\n",
      "loss: 0.807739  [150400/175341]\n",
      "loss: 0.441555  [152000/175341]\n",
      "loss: 0.490660  [153600/175341]\n",
      "loss: 0.497452  [155200/175341]\n",
      "loss: 0.372561  [156800/175341]\n",
      "loss: 0.446907  [158400/175341]\n",
      "loss: 0.340812  [160000/175341]\n",
      "loss: 0.540608  [161600/175341]\n",
      "loss: 0.170441  [163200/175341]\n",
      "loss: 0.561998  [164800/175341]\n",
      "loss: 0.334883  [166400/175341]\n",
      "loss: 0.591612  [168000/175341]\n",
      "loss: 0.957709  [169600/175341]\n",
      "loss: 0.360503  [171200/175341]\n",
      "loss: 0.472053  [172800/175341]\n",
      "loss: 0.548575  [174400/175341]\n",
      "Train Accuracy: 81.8565%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.547091, F1-score: 76.54%, Macro_F1-Score:  42.02%  \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.533403  [    0/175341]\n",
      "loss: 0.477290  [ 1600/175341]\n",
      "loss: 0.407033  [ 3200/175341]\n",
      "loss: 0.301972  [ 4800/175341]\n",
      "loss: 0.337838  [ 6400/175341]\n",
      "loss: 0.374912  [ 8000/175341]\n",
      "loss: 0.416182  [ 9600/175341]\n",
      "loss: 0.424921  [11200/175341]\n",
      "loss: 0.598410  [12800/175341]\n",
      "loss: 0.279770  [14400/175341]\n",
      "loss: 0.487997  [16000/175341]\n",
      "loss: 0.512003  [17600/175341]\n",
      "loss: 0.647984  [19200/175341]\n",
      "loss: 0.450689  [20800/175341]\n",
      "loss: 0.595882  [22400/175341]\n",
      "loss: 0.769761  [24000/175341]\n",
      "loss: 0.445783  [25600/175341]\n",
      "loss: 0.212233  [27200/175341]\n",
      "loss: 0.205296  [28800/175341]\n",
      "loss: 0.504169  [30400/175341]\n",
      "loss: 0.144153  [32000/175341]\n",
      "loss: 0.288426  [33600/175341]\n",
      "loss: 0.383503  [35200/175341]\n",
      "loss: 0.633033  [36800/175341]\n",
      "loss: 0.978811  [38400/175341]\n",
      "loss: 0.249834  [40000/175341]\n",
      "loss: 0.461655  [41600/175341]\n",
      "loss: 0.269740  [43200/175341]\n",
      "loss: 0.096291  [44800/175341]\n",
      "loss: 0.955994  [46400/175341]\n",
      "loss: 0.359450  [48000/175341]\n",
      "loss: 0.680224  [49600/175341]\n",
      "loss: 0.355792  [51200/175341]\n",
      "loss: 0.433616  [52800/175341]\n",
      "loss: 0.451849  [54400/175341]\n",
      "loss: 0.450336  [56000/175341]\n",
      "loss: 0.412475  [57600/175341]\n",
      "loss: 0.891910  [59200/175341]\n",
      "loss: 0.580912  [60800/175341]\n",
      "loss: 0.348483  [62400/175341]\n",
      "loss: 0.275964  [64000/175341]\n",
      "loss: 0.690654  [65600/175341]\n",
      "loss: 0.427121  [67200/175341]\n",
      "loss: 0.975935  [68800/175341]\n",
      "loss: 0.362555  [70400/175341]\n",
      "loss: 0.368729  [72000/175341]\n",
      "loss: 0.758897  [73600/175341]\n",
      "loss: 0.563396  [75200/175341]\n",
      "loss: 0.369402  [76800/175341]\n",
      "loss: 0.749800  [78400/175341]\n",
      "loss: 0.242476  [80000/175341]\n",
      "loss: 0.776547  [81600/175341]\n",
      "loss: 0.664414  [83200/175341]\n",
      "loss: 0.282041  [84800/175341]\n",
      "loss: 0.625444  [86400/175341]\n",
      "loss: 0.493214  [88000/175341]\n",
      "loss: 0.252308  [89600/175341]\n",
      "loss: 0.194626  [91200/175341]\n",
      "loss: 0.371280  [92800/175341]\n",
      "loss: 0.723534  [94400/175341]\n",
      "loss: 0.762764  [96000/175341]\n",
      "loss: 0.363286  [97600/175341]\n",
      "loss: 0.790202  [99200/175341]\n",
      "loss: 0.493371  [100800/175341]\n",
      "loss: 0.599516  [102400/175341]\n",
      "loss: 0.575231  [104000/175341]\n",
      "loss: 0.541556  [105600/175341]\n",
      "loss: 0.199568  [107200/175341]\n",
      "loss: 0.357069  [108800/175341]\n",
      "loss: 0.635883  [110400/175341]\n",
      "loss: 0.355411  [112000/175341]\n",
      "loss: 0.277338  [113600/175341]\n",
      "loss: 0.549789  [115200/175341]\n",
      "loss: 0.557327  [116800/175341]\n",
      "loss: 0.509151  [118400/175341]\n",
      "loss: 0.450839  [120000/175341]\n",
      "loss: 0.513743  [121600/175341]\n",
      "loss: 0.502012  [123200/175341]\n",
      "loss: 0.233255  [124800/175341]\n",
      "loss: 0.403673  [126400/175341]\n",
      "loss: 0.539080  [128000/175341]\n",
      "loss: 0.370737  [129600/175341]\n",
      "loss: 0.314580  [131200/175341]\n",
      "loss: 0.305792  [132800/175341]\n",
      "loss: 0.628863  [134400/175341]\n",
      "loss: 0.479880  [136000/175341]\n",
      "loss: 0.614390  [137600/175341]\n",
      "loss: 0.305881  [139200/175341]\n",
      "loss: 0.348329  [140800/175341]\n",
      "loss: 0.600047  [142400/175341]\n",
      "loss: 0.611743  [144000/175341]\n",
      "loss: 0.422459  [145600/175341]\n",
      "loss: 0.397230  [147200/175341]\n",
      "loss: 0.346192  [148800/175341]\n",
      "loss: 0.334955  [150400/175341]\n",
      "loss: 0.409674  [152000/175341]\n",
      "loss: 0.659119  [153600/175341]\n",
      "loss: 0.428280  [155200/175341]\n",
      "loss: 0.569987  [156800/175341]\n",
      "loss: 0.543923  [158400/175341]\n",
      "loss: 0.375691  [160000/175341]\n",
      "loss: 0.410174  [161600/175341]\n",
      "loss: 0.123121  [163200/175341]\n",
      "loss: 0.345142  [164800/175341]\n",
      "loss: 0.512455  [166400/175341]\n",
      "loss: 0.750014  [168000/175341]\n",
      "loss: 0.360692  [169600/175341]\n",
      "loss: 0.357316  [171200/175341]\n",
      "loss: 0.344443  [172800/175341]\n",
      "loss: 0.189711  [174400/175341]\n",
      "Train Accuracy: 81.7972%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.574305, F1-score: 75.96%, Macro_F1-Score:  41.42%  \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.440570  [    0/175341]\n",
      "loss: 0.502658  [ 1600/175341]\n",
      "loss: 0.618766  [ 3200/175341]\n",
      "loss: 0.700118  [ 4800/175341]\n",
      "loss: 0.362371  [ 6400/175341]\n",
      "loss: 0.591410  [ 8000/175341]\n",
      "loss: 0.496251  [ 9600/175341]\n",
      "loss: 0.153107  [11200/175341]\n",
      "loss: 0.276098  [12800/175341]\n",
      "loss: 0.564616  [14400/175341]\n",
      "loss: 0.569625  [16000/175341]\n",
      "loss: 0.479142  [17600/175341]\n",
      "loss: 0.554475  [19200/175341]\n",
      "loss: 0.455721  [20800/175341]\n",
      "loss: 0.308901  [22400/175341]\n",
      "loss: 0.402579  [24000/175341]\n",
      "loss: 0.317389  [25600/175341]\n",
      "loss: 0.293306  [27200/175341]\n",
      "loss: 0.550168  [28800/175341]\n",
      "loss: 0.161366  [30400/175341]\n",
      "loss: 0.647162  [32000/175341]\n",
      "loss: 0.415211  [33600/175341]\n",
      "loss: 0.261286  [35200/175341]\n",
      "loss: 0.250605  [36800/175341]\n",
      "loss: 0.330764  [38400/175341]\n",
      "loss: 0.678571  [40000/175341]\n",
      "loss: 0.689265  [41600/175341]\n",
      "loss: 0.476415  [43200/175341]\n",
      "loss: 0.073928  [44800/175341]\n",
      "loss: 0.384969  [46400/175341]\n",
      "loss: 0.568861  [48000/175341]\n",
      "loss: 0.553557  [49600/175341]\n",
      "loss: 0.541330  [51200/175341]\n",
      "loss: 0.562146  [52800/175341]\n",
      "loss: 0.344709  [54400/175341]\n",
      "loss: 0.335864  [56000/175341]\n",
      "loss: 0.664085  [57600/175341]\n",
      "loss: 0.328060  [59200/175341]\n",
      "loss: 0.509904  [60800/175341]\n",
      "loss: 0.111172  [62400/175341]\n",
      "loss: 0.548334  [64000/175341]\n",
      "loss: 0.475269  [65600/175341]\n",
      "loss: 0.704336  [67200/175341]\n",
      "loss: 0.260422  [68800/175341]\n",
      "loss: 0.623692  [70400/175341]\n",
      "loss: 0.499225  [72000/175341]\n",
      "loss: 0.337486  [73600/175341]\n",
      "loss: 0.743832  [75200/175341]\n",
      "loss: 1.003218  [76800/175341]\n",
      "loss: 0.588477  [78400/175341]\n",
      "loss: 0.480989  [80000/175341]\n",
      "loss: 0.616495  [81600/175341]\n",
      "loss: 0.425117  [83200/175341]\n",
      "loss: 0.426714  [84800/175341]\n",
      "loss: 0.223778  [86400/175341]\n",
      "loss: 0.646322  [88000/175341]\n",
      "loss: 0.364629  [89600/175341]\n",
      "loss: 0.586716  [91200/175341]\n",
      "loss: 0.950955  [92800/175341]\n",
      "loss: 0.318143  [94400/175341]\n",
      "loss: 0.424519  [96000/175341]\n",
      "loss: 0.407246  [97600/175341]\n",
      "loss: 0.418369  [99200/175341]\n",
      "loss: 0.275219  [100800/175341]\n",
      "loss: 0.491611  [102400/175341]\n",
      "loss: 0.662536  [104000/175341]\n",
      "loss: 0.436171  [105600/175341]\n",
      "loss: 0.650866  [107200/175341]\n",
      "loss: 0.437071  [108800/175341]\n",
      "loss: 0.635334  [110400/175341]\n",
      "loss: 0.271435  [112000/175341]\n",
      "loss: 0.666556  [113600/175341]\n",
      "loss: 0.474223  [115200/175341]\n",
      "loss: 0.169855  [116800/175341]\n",
      "loss: 0.658886  [118400/175341]\n",
      "loss: 0.398627  [120000/175341]\n",
      "loss: 0.444597  [121600/175341]\n",
      "loss: 0.333192  [123200/175341]\n",
      "loss: 0.959199  [124800/175341]\n",
      "loss: 0.385303  [126400/175341]\n",
      "loss: 0.202711  [128000/175341]\n",
      "loss: 0.398035  [129600/175341]\n",
      "loss: 0.172467  [131200/175341]\n",
      "loss: 0.256532  [132800/175341]\n",
      "loss: 0.423262  [134400/175341]\n",
      "loss: 0.405448  [136000/175341]\n",
      "loss: 0.663087  [137600/175341]\n",
      "loss: 0.447499  [139200/175341]\n",
      "loss: 0.380086  [140800/175341]\n",
      "loss: 0.910986  [142400/175341]\n",
      "loss: 0.217480  [144000/175341]\n",
      "loss: 0.292183  [145600/175341]\n",
      "loss: 0.443845  [147200/175341]\n",
      "loss: 0.357891  [148800/175341]\n",
      "loss: 0.578544  [150400/175341]\n",
      "loss: 0.341149  [152000/175341]\n",
      "loss: 0.558689  [153600/175341]\n",
      "loss: 0.910724  [155200/175341]\n",
      "loss: 0.514793  [156800/175341]\n",
      "loss: 0.548131  [158400/175341]\n",
      "loss: 0.378225  [160000/175341]\n",
      "loss: 0.279471  [161600/175341]\n",
      "loss: 0.333390  [163200/175341]\n",
      "loss: 0.276718  [164800/175341]\n",
      "loss: 0.484131  [166400/175341]\n",
      "loss: 0.333616  [168000/175341]\n",
      "loss: 0.483943  [169600/175341]\n",
      "loss: 0.654695  [171200/175341]\n",
      "loss: 0.068852  [172800/175341]\n",
      "loss: 0.476071  [174400/175341]\n",
      "Train Accuracy: 81.8377%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.546565, F1-score: 76.22%, Macro_F1-Score:  41.14%  \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.469994  [    0/175341]\n",
      "loss: 0.701250  [ 1600/175341]\n",
      "loss: 0.848728  [ 3200/175341]\n",
      "loss: 0.444967  [ 4800/175341]\n",
      "loss: 0.390092  [ 6400/175341]\n",
      "loss: 0.217728  [ 8000/175341]\n",
      "loss: 0.640338  [ 9600/175341]\n",
      "loss: 0.474207  [11200/175341]\n",
      "loss: 0.924556  [12800/175341]\n",
      "loss: 0.684485  [14400/175341]\n",
      "loss: 0.438156  [16000/175341]\n",
      "loss: 0.386366  [17600/175341]\n",
      "loss: 0.312709  [19200/175341]\n",
      "loss: 0.420416  [20800/175341]\n",
      "loss: 0.329445  [22400/175341]\n",
      "loss: 0.787196  [24000/175341]\n",
      "loss: 0.421994  [25600/175341]\n",
      "loss: 0.276278  [27200/175341]\n",
      "loss: 0.380243  [28800/175341]\n",
      "loss: 0.331510  [30400/175341]\n",
      "loss: 0.371992  [32000/175341]\n",
      "loss: 0.514697  [33600/175341]\n",
      "loss: 0.130556  [35200/175341]\n",
      "loss: 0.099382  [36800/175341]\n",
      "loss: 0.125319  [38400/175341]\n",
      "loss: 0.388096  [40000/175341]\n",
      "loss: 0.422080  [41600/175341]\n",
      "loss: 0.443303  [43200/175341]\n",
      "loss: 0.597883  [44800/175341]\n",
      "loss: 0.492538  [46400/175341]\n",
      "loss: 0.333593  [48000/175341]\n",
      "loss: 0.323207  [49600/175341]\n",
      "loss: 0.279838  [51200/175341]\n",
      "loss: 0.644288  [52800/175341]\n",
      "loss: 0.248382  [54400/175341]\n",
      "loss: 0.659593  [56000/175341]\n",
      "loss: 0.804596  [57600/175341]\n",
      "loss: 0.378866  [59200/175341]\n",
      "loss: 0.434949  [60800/175341]\n",
      "loss: 0.198070  [62400/175341]\n",
      "loss: 0.761274  [64000/175341]\n",
      "loss: 0.379136  [65600/175341]\n",
      "loss: 1.026086  [67200/175341]\n",
      "loss: 0.630313  [68800/175341]\n",
      "loss: 0.349912  [70400/175341]\n",
      "loss: 0.322103  [72000/175341]\n",
      "loss: 0.434291  [73600/175341]\n",
      "loss: 0.522087  [75200/175341]\n",
      "loss: 0.109538  [76800/175341]\n",
      "loss: 0.565243  [78400/175341]\n",
      "loss: 0.428320  [80000/175341]\n",
      "loss: 0.324072  [81600/175341]\n",
      "loss: 0.235785  [83200/175341]\n",
      "loss: 0.470843  [84800/175341]\n",
      "loss: 0.438001  [86400/175341]\n",
      "loss: 0.771011  [88000/175341]\n",
      "loss: 0.502492  [89600/175341]\n",
      "loss: 0.361811  [91200/175341]\n",
      "loss: 0.439076  [92800/175341]\n",
      "loss: 0.744386  [94400/175341]\n",
      "loss: 0.386789  [96000/175341]\n",
      "loss: 0.328171  [97600/175341]\n",
      "loss: 0.319986  [99200/175341]\n",
      "loss: 0.385899  [100800/175341]\n",
      "loss: 0.296640  [102400/175341]\n",
      "loss: 0.706125  [104000/175341]\n",
      "loss: 0.752235  [105600/175341]\n",
      "loss: 0.876631  [107200/175341]\n",
      "loss: 0.350163  [108800/175341]\n",
      "loss: 0.298833  [110400/175341]\n",
      "loss: 0.249188  [112000/175341]\n",
      "loss: 0.465584  [113600/175341]\n",
      "loss: 0.382177  [115200/175341]\n",
      "loss: 0.932456  [116800/175341]\n",
      "loss: 0.150354  [118400/175341]\n",
      "loss: 0.256881  [120000/175341]\n",
      "loss: 0.456796  [121600/175341]\n",
      "loss: 0.295293  [123200/175341]\n",
      "loss: 0.200048  [124800/175341]\n",
      "loss: 0.710321  [126400/175341]\n",
      "loss: 0.460163  [128000/175341]\n",
      "loss: 0.736063  [129600/175341]\n",
      "loss: 0.223257  [131200/175341]\n",
      "loss: 0.311384  [132800/175341]\n",
      "loss: 0.427681  [134400/175341]\n",
      "loss: 0.379340  [136000/175341]\n",
      "loss: 0.739899  [137600/175341]\n",
      "loss: 0.247740  [139200/175341]\n",
      "loss: 0.418012  [140800/175341]\n",
      "loss: 0.323218  [142400/175341]\n",
      "loss: 0.552416  [144000/175341]\n",
      "loss: 0.285625  [145600/175341]\n",
      "loss: 0.630694  [147200/175341]\n",
      "loss: 0.451589  [148800/175341]\n",
      "loss: 0.298048  [150400/175341]\n",
      "loss: 0.353210  [152000/175341]\n",
      "loss: 0.521207  [153600/175341]\n",
      "loss: 0.526904  [155200/175341]\n",
      "loss: 0.479531  [156800/175341]\n",
      "loss: 0.088106  [158400/175341]\n",
      "loss: 0.342817  [160000/175341]\n",
      "loss: 0.327709  [161600/175341]\n",
      "loss: 0.544276  [163200/175341]\n",
      "loss: 0.188820  [164800/175341]\n",
      "loss: 0.259994  [166400/175341]\n",
      "loss: 0.320565  [168000/175341]\n",
      "loss: 0.249948  [169600/175341]\n",
      "loss: 0.446771  [171200/175341]\n",
      "loss: 0.487842  [172800/175341]\n",
      "loss: 0.349128  [174400/175341]\n",
      "Train Accuracy: 81.8759%\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.549525, F1-score: 77.58%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.438295  [    0/175341]\n",
      "loss: 0.378643  [ 1600/175341]\n",
      "loss: 0.478024  [ 3200/175341]\n",
      "loss: 0.291895  [ 4800/175341]\n",
      "loss: 0.576479  [ 6400/175341]\n",
      "loss: 0.169648  [ 8000/175341]\n",
      "loss: 0.123838  [ 9600/175341]\n",
      "loss: 0.528888  [11200/175341]\n",
      "loss: 0.493698  [12800/175341]\n",
      "loss: 0.375378  [14400/175341]\n",
      "loss: 0.560800  [16000/175341]\n",
      "loss: 0.665227  [17600/175341]\n",
      "loss: 0.224465  [19200/175341]\n",
      "loss: 0.043165  [20800/175341]\n",
      "loss: 0.160990  [22400/175341]\n",
      "loss: 0.109405  [24000/175341]\n",
      "loss: 0.620025  [25600/175341]\n",
      "loss: 0.682586  [27200/175341]\n",
      "loss: 0.504912  [28800/175341]\n",
      "loss: 0.737337  [30400/175341]\n",
      "loss: 0.167096  [32000/175341]\n",
      "loss: 0.662529  [33600/175341]\n",
      "loss: 0.482320  [35200/175341]\n",
      "loss: 0.429296  [36800/175341]\n",
      "loss: 0.510296  [38400/175341]\n",
      "loss: 0.367156  [40000/175341]\n",
      "loss: 1.200133  [41600/175341]\n",
      "loss: 0.313878  [43200/175341]\n",
      "loss: 0.209495  [44800/175341]\n",
      "loss: 0.670092  [46400/175341]\n",
      "loss: 0.554221  [48000/175341]\n",
      "loss: 0.402861  [49600/175341]\n",
      "loss: 0.261577  [51200/175341]\n",
      "loss: 0.861543  [52800/175341]\n",
      "loss: 0.482103  [54400/175341]\n",
      "loss: 0.575130  [56000/175341]\n",
      "loss: 0.418362  [57600/175341]\n",
      "loss: 0.142928  [59200/175341]\n",
      "loss: 0.315783  [60800/175341]\n",
      "loss: 0.281557  [62400/175341]\n",
      "loss: 0.566713  [64000/175341]\n",
      "loss: 0.338242  [65600/175341]\n",
      "loss: 0.188151  [67200/175341]\n",
      "loss: 0.367607  [68800/175341]\n",
      "loss: 0.554745  [70400/175341]\n",
      "loss: 0.451917  [72000/175341]\n",
      "loss: 0.474480  [73600/175341]\n",
      "loss: 0.177181  [75200/175341]\n",
      "loss: 0.483540  [76800/175341]\n",
      "loss: 0.265584  [78400/175341]\n",
      "loss: 0.370682  [80000/175341]\n",
      "loss: 0.498054  [81600/175341]\n",
      "loss: 0.508855  [83200/175341]\n",
      "loss: 0.828560  [84800/175341]\n",
      "loss: 0.333125  [86400/175341]\n",
      "loss: 0.581368  [88000/175341]\n",
      "loss: 0.885983  [89600/175341]\n",
      "loss: 0.709282  [91200/175341]\n",
      "loss: 0.476114  [92800/175341]\n",
      "loss: 0.693658  [94400/175341]\n",
      "loss: 0.560692  [96000/175341]\n",
      "loss: 0.258171  [97600/175341]\n",
      "loss: 0.211195  [99200/175341]\n",
      "loss: 0.308716  [100800/175341]\n",
      "loss: 0.447704  [102400/175341]\n",
      "loss: 0.398184  [104000/175341]\n",
      "loss: 0.932641  [105600/175341]\n",
      "loss: 0.127289  [107200/175341]\n",
      "loss: 0.417818  [108800/175341]\n",
      "loss: 0.403351  [110400/175341]\n",
      "loss: 0.291071  [112000/175341]\n",
      "loss: 0.755248  [113600/175341]\n",
      "loss: 0.618775  [115200/175341]\n",
      "loss: 0.379690  [116800/175341]\n",
      "loss: 0.739149  [118400/175341]\n",
      "loss: 0.547064  [120000/175341]\n",
      "loss: 0.610168  [121600/175341]\n",
      "loss: 0.510254  [123200/175341]\n",
      "loss: 0.384213  [124800/175341]\n",
      "loss: 0.371979  [126400/175341]\n",
      "loss: 0.414636  [128000/175341]\n",
      "loss: 0.411629  [129600/175341]\n",
      "loss: 0.641459  [131200/175341]\n",
      "loss: 0.958163  [132800/175341]\n",
      "loss: 0.424812  [134400/175341]\n",
      "loss: 0.619816  [136000/175341]\n",
      "loss: 0.542945  [137600/175341]\n",
      "loss: 0.273127  [139200/175341]\n",
      "loss: 0.635727  [140800/175341]\n",
      "loss: 0.357015  [142400/175341]\n",
      "loss: 0.283950  [144000/175341]\n",
      "loss: 0.228772  [145600/175341]\n",
      "loss: 0.507957  [147200/175341]\n",
      "loss: 0.591339  [148800/175341]\n",
      "loss: 0.342807  [150400/175341]\n",
      "loss: 1.117718  [152000/175341]\n",
      "loss: 0.784755  [153600/175341]\n",
      "loss: 0.384857  [155200/175341]\n",
      "loss: 0.462325  [156800/175341]\n",
      "loss: 0.124422  [158400/175341]\n",
      "loss: 0.737587  [160000/175341]\n",
      "loss: 0.294288  [161600/175341]\n",
      "loss: 0.498817  [163200/175341]\n",
      "loss: 0.818117  [164800/175341]\n",
      "loss: 0.329236  [166400/175341]\n",
      "loss: 0.472627  [168000/175341]\n",
      "loss: 0.334328  [169600/175341]\n",
      "loss: 0.209422  [171200/175341]\n",
      "loss: 0.661977  [172800/175341]\n",
      "loss: 0.857154  [174400/175341]\n",
      "Train Accuracy: 81.8314%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.568964, F1-score: 75.07%, Macro_F1-Score:  40.96%  \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.526530  [    0/175341]\n",
      "loss: 0.322750  [ 1600/175341]\n",
      "loss: 0.474636  [ 3200/175341]\n",
      "loss: 0.569856  [ 4800/175341]\n",
      "loss: 0.494485  [ 6400/175341]\n",
      "loss: 0.595826  [ 8000/175341]\n",
      "loss: 0.563672  [ 9600/175341]\n",
      "loss: 0.387992  [11200/175341]\n",
      "loss: 0.526260  [12800/175341]\n",
      "loss: 0.547589  [14400/175341]\n",
      "loss: 0.414433  [16000/175341]\n",
      "loss: 0.531512  [17600/175341]\n",
      "loss: 0.285116  [19200/175341]\n",
      "loss: 0.425229  [20800/175341]\n",
      "loss: 0.149832  [22400/175341]\n",
      "loss: 0.203246  [24000/175341]\n",
      "loss: 0.517321  [25600/175341]\n",
      "loss: 0.829552  [27200/175341]\n",
      "loss: 0.273605  [28800/175341]\n",
      "loss: 0.611610  [30400/175341]\n",
      "loss: 0.559347  [32000/175341]\n",
      "loss: 0.321216  [33600/175341]\n",
      "loss: 0.645956  [35200/175341]\n",
      "loss: 0.076893  [36800/175341]\n",
      "loss: 0.602425  [38400/175341]\n",
      "loss: 0.440428  [40000/175341]\n",
      "loss: 0.699016  [41600/175341]\n",
      "loss: 0.594691  [43200/175341]\n",
      "loss: 0.263396  [44800/175341]\n",
      "loss: 0.185166  [46400/175341]\n",
      "loss: 0.873890  [48000/175341]\n",
      "loss: 0.353839  [49600/175341]\n",
      "loss: 0.430340  [51200/175341]\n",
      "loss: 0.559135  [52800/175341]\n",
      "loss: 0.175758  [54400/175341]\n",
      "loss: 0.766402  [56000/175341]\n",
      "loss: 0.575842  [57600/175341]\n",
      "loss: 0.321120  [59200/175341]\n",
      "loss: 0.517995  [60800/175341]\n",
      "loss: 0.484134  [62400/175341]\n",
      "loss: 0.464584  [64000/175341]\n",
      "loss: 0.474184  [65600/175341]\n",
      "loss: 0.729229  [67200/175341]\n",
      "loss: 0.344066  [68800/175341]\n",
      "loss: 0.228317  [70400/175341]\n",
      "loss: 0.660675  [72000/175341]\n",
      "loss: 0.799161  [73600/175341]\n",
      "loss: 0.184935  [75200/175341]\n",
      "loss: 0.322316  [76800/175341]\n",
      "loss: 0.536759  [78400/175341]\n",
      "loss: 0.364937  [80000/175341]\n",
      "loss: 0.571498  [81600/175341]\n",
      "loss: 0.269136  [83200/175341]\n",
      "loss: 0.281237  [84800/175341]\n",
      "loss: 0.571806  [86400/175341]\n",
      "loss: 0.584425  [88000/175341]\n",
      "loss: 0.137714  [89600/175341]\n",
      "loss: 0.239875  [91200/175341]\n",
      "loss: 0.401873  [92800/175341]\n",
      "loss: 0.329669  [94400/175341]\n",
      "loss: 0.050724  [96000/175341]\n",
      "loss: 0.241057  [97600/175341]\n",
      "loss: 0.718383  [99200/175341]\n",
      "loss: 0.330879  [100800/175341]\n",
      "loss: 0.287904  [102400/175341]\n",
      "loss: 0.471172  [104000/175341]\n",
      "loss: 0.553607  [105600/175341]\n",
      "loss: 0.566441  [107200/175341]\n",
      "loss: 0.311188  [108800/175341]\n",
      "loss: 0.504967  [110400/175341]\n",
      "loss: 0.388608  [112000/175341]\n",
      "loss: 0.234683  [113600/175341]\n",
      "loss: 0.521029  [115200/175341]\n",
      "loss: 0.321065  [116800/175341]\n",
      "loss: 0.275493  [118400/175341]\n",
      "loss: 0.859761  [120000/175341]\n",
      "loss: 0.551678  [121600/175341]\n",
      "loss: 0.617640  [123200/175341]\n",
      "loss: 0.635111  [124800/175341]\n",
      "loss: 0.318473  [126400/175341]\n",
      "loss: 0.485479  [128000/175341]\n",
      "loss: 0.541083  [129600/175341]\n",
      "loss: 0.436588  [131200/175341]\n",
      "loss: 0.349389  [132800/175341]\n",
      "loss: 0.810033  [134400/175341]\n",
      "loss: 0.378094  [136000/175341]\n",
      "loss: 0.279856  [137600/175341]\n",
      "loss: 0.197781  [139200/175341]\n",
      "loss: 0.535646  [140800/175341]\n",
      "loss: 0.568653  [142400/175341]\n",
      "loss: 0.743673  [144000/175341]\n",
      "loss: 0.401486  [145600/175341]\n",
      "loss: 0.534186  [147200/175341]\n",
      "loss: 0.272054  [148800/175341]\n",
      "loss: 0.589942  [150400/175341]\n",
      "loss: 0.580663  [152000/175341]\n",
      "loss: 0.330143  [153600/175341]\n",
      "loss: 0.445646  [155200/175341]\n",
      "loss: 0.395586  [156800/175341]\n",
      "loss: 0.241899  [158400/175341]\n",
      "loss: 0.465081  [160000/175341]\n",
      "loss: 0.561476  [161600/175341]\n",
      "loss: 0.353408  [163200/175341]\n",
      "loss: 0.577258  [164800/175341]\n",
      "loss: 0.305227  [166400/175341]\n",
      "loss: 0.061169  [168000/175341]\n",
      "loss: 0.335361  [169600/175341]\n",
      "loss: 0.464022  [171200/175341]\n",
      "loss: 0.296477  [172800/175341]\n",
      "loss: 0.901488  [174400/175341]\n",
      "Train Accuracy: 81.8582%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.548679, F1-score: 76.26%, Macro_F1-Score:  41.27%  \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.751247  [    0/175341]\n",
      "loss: 0.601874  [ 1600/175341]\n",
      "loss: 0.272128  [ 3200/175341]\n",
      "loss: 0.687582  [ 4800/175341]\n",
      "loss: 0.290691  [ 6400/175341]\n",
      "loss: 0.498080  [ 8000/175341]\n",
      "loss: 0.390507  [ 9600/175341]\n",
      "loss: 0.607577  [11200/175341]\n",
      "loss: 0.702239  [12800/175341]\n",
      "loss: 0.563351  [14400/175341]\n",
      "loss: 0.332786  [16000/175341]\n",
      "loss: 0.352074  [17600/175341]\n",
      "loss: 0.411067  [19200/175341]\n",
      "loss: 0.487758  [20800/175341]\n",
      "loss: 0.298689  [22400/175341]\n",
      "loss: 0.155645  [24000/175341]\n",
      "loss: 0.380615  [25600/175341]\n",
      "loss: 0.364930  [27200/175341]\n",
      "loss: 0.498965  [28800/175341]\n",
      "loss: 0.439014  [30400/175341]\n",
      "loss: 0.283898  [32000/175341]\n",
      "loss: 0.224819  [33600/175341]\n",
      "loss: 0.308710  [35200/175341]\n",
      "loss: 0.930564  [36800/175341]\n",
      "loss: 0.411617  [38400/175341]\n",
      "loss: 0.841368  [40000/175341]\n",
      "loss: 0.462760  [41600/175341]\n",
      "loss: 0.726607  [43200/175341]\n",
      "loss: 0.545935  [44800/175341]\n",
      "loss: 0.307029  [46400/175341]\n",
      "loss: 0.175848  [48000/175341]\n",
      "loss: 0.597467  [49600/175341]\n",
      "loss: 0.415952  [51200/175341]\n",
      "loss: 1.092449  [52800/175341]\n",
      "loss: 0.679306  [54400/175341]\n",
      "loss: 0.344684  [56000/175341]\n",
      "loss: 0.390119  [57600/175341]\n",
      "loss: 0.505449  [59200/175341]\n",
      "loss: 0.453051  [60800/175341]\n",
      "loss: 0.385785  [62400/175341]\n",
      "loss: 0.908294  [64000/175341]\n",
      "loss: 0.265158  [65600/175341]\n",
      "loss: 0.473621  [67200/175341]\n",
      "loss: 0.807621  [68800/175341]\n",
      "loss: 0.503568  [70400/175341]\n",
      "loss: 0.316990  [72000/175341]\n",
      "loss: 0.576048  [73600/175341]\n",
      "loss: 0.460859  [75200/175341]\n",
      "loss: 0.706566  [76800/175341]\n",
      "loss: 0.679187  [78400/175341]\n",
      "loss: 0.239436  [80000/175341]\n",
      "loss: 0.402749  [81600/175341]\n",
      "loss: 0.373932  [83200/175341]\n",
      "loss: 0.562738  [84800/175341]\n",
      "loss: 0.693004  [86400/175341]\n",
      "loss: 0.384642  [88000/175341]\n",
      "loss: 0.341746  [89600/175341]\n",
      "loss: 0.584703  [91200/175341]\n",
      "loss: 0.169272  [92800/175341]\n",
      "loss: 0.284522  [94400/175341]\n",
      "loss: 0.281324  [96000/175341]\n",
      "loss: 0.610828  [97600/175341]\n",
      "loss: 0.305599  [99200/175341]\n",
      "loss: 0.552600  [100800/175341]\n",
      "loss: 0.345133  [102400/175341]\n",
      "loss: 0.543652  [104000/175341]\n",
      "loss: 0.340265  [105600/175341]\n",
      "loss: 0.186974  [107200/175341]\n",
      "loss: 0.283641  [108800/175341]\n",
      "loss: 0.396683  [110400/175341]\n",
      "loss: 0.350865  [112000/175341]\n",
      "loss: 0.690487  [113600/175341]\n",
      "loss: 0.238353  [115200/175341]\n",
      "loss: 0.322382  [116800/175341]\n",
      "loss: 0.611912  [118400/175341]\n",
      "loss: 0.809438  [120000/175341]\n",
      "loss: 0.335319  [121600/175341]\n",
      "loss: 0.329500  [123200/175341]\n",
      "loss: 0.666364  [124800/175341]\n",
      "loss: 0.479303  [126400/175341]\n",
      "loss: 0.379344  [128000/175341]\n",
      "loss: 0.468020  [129600/175341]\n",
      "loss: 0.363225  [131200/175341]\n",
      "loss: 0.517923  [132800/175341]\n",
      "loss: 0.193832  [134400/175341]\n",
      "loss: 0.485590  [136000/175341]\n",
      "loss: 0.364675  [137600/175341]\n",
      "loss: 0.309500  [139200/175341]\n",
      "loss: 0.448678  [140800/175341]\n",
      "loss: 0.395898  [142400/175341]\n",
      "loss: 0.243860  [144000/175341]\n",
      "loss: 0.631383  [145600/175341]\n",
      "loss: 0.226383  [147200/175341]\n",
      "loss: 0.256787  [148800/175341]\n",
      "loss: 0.351178  [150400/175341]\n",
      "loss: 0.400238  [152000/175341]\n",
      "loss: 0.376824  [153600/175341]\n",
      "loss: 0.159794  [155200/175341]\n",
      "loss: 0.714983  [156800/175341]\n",
      "loss: 0.384244  [158400/175341]\n",
      "loss: 0.667084  [160000/175341]\n",
      "loss: 0.485035  [161600/175341]\n",
      "loss: 0.526431  [163200/175341]\n",
      "loss: 0.245384  [164800/175341]\n",
      "loss: 0.091923  [166400/175341]\n",
      "loss: 0.364677  [168000/175341]\n",
      "loss: 0.551785  [169600/175341]\n",
      "loss: 0.586132  [171200/175341]\n",
      "loss: 0.258666  [172800/175341]\n",
      "loss: 0.595712  [174400/175341]\n",
      "Train Accuracy: 81.8930%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.545017, F1-score: 77.28%, Macro_F1-Score:  41.71%  \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.136298  [    0/175341]\n",
      "loss: 0.350751  [ 1600/175341]\n",
      "loss: 0.318655  [ 3200/175341]\n",
      "loss: 0.342812  [ 4800/175341]\n",
      "loss: 0.651303  [ 6400/175341]\n",
      "loss: 0.646428  [ 8000/175341]\n",
      "loss: 0.636765  [ 9600/175341]\n",
      "loss: 0.537641  [11200/175341]\n",
      "loss: 0.215382  [12800/175341]\n",
      "loss: 0.531082  [14400/175341]\n",
      "loss: 0.464109  [16000/175341]\n",
      "loss: 0.699334  [17600/175341]\n",
      "loss: 0.116316  [19200/175341]\n",
      "loss: 0.242605  [20800/175341]\n",
      "loss: 0.475263  [22400/175341]\n",
      "loss: 0.617667  [24000/175341]\n",
      "loss: 0.495907  [25600/175341]\n",
      "loss: 0.721673  [27200/175341]\n",
      "loss: 0.097550  [28800/175341]\n",
      "loss: 0.554053  [30400/175341]\n",
      "loss: 0.123905  [32000/175341]\n",
      "loss: 0.203768  [33600/175341]\n",
      "loss: 0.401737  [35200/175341]\n",
      "loss: 0.769766  [36800/175341]\n",
      "loss: 0.614725  [38400/175341]\n",
      "loss: 0.283340  [40000/175341]\n",
      "loss: 0.287058  [41600/175341]\n",
      "loss: 0.271696  [43200/175341]\n",
      "loss: 0.411560  [44800/175341]\n",
      "loss: 0.151071  [46400/175341]\n",
      "loss: 0.695977  [48000/175341]\n",
      "loss: 0.807236  [49600/175341]\n",
      "loss: 0.587735  [51200/175341]\n",
      "loss: 0.414487  [52800/175341]\n",
      "loss: 0.446029  [54400/175341]\n",
      "loss: 1.046412  [56000/175341]\n",
      "loss: 0.269585  [57600/175341]\n",
      "loss: 0.383092  [59200/175341]\n",
      "loss: 0.551530  [60800/175341]\n",
      "loss: 0.326628  [62400/175341]\n",
      "loss: 0.695757  [64000/175341]\n",
      "loss: 0.555578  [65600/175341]\n",
      "loss: 0.450803  [67200/175341]\n",
      "loss: 0.286673  [68800/175341]\n",
      "loss: 0.266781  [70400/175341]\n",
      "loss: 0.405957  [72000/175341]\n",
      "loss: 0.265651  [73600/175341]\n",
      "loss: 0.611616  [75200/175341]\n",
      "loss: 0.429760  [76800/175341]\n",
      "loss: 0.426728  [78400/175341]\n",
      "loss: 0.570834  [80000/175341]\n",
      "loss: 0.205668  [81600/175341]\n",
      "loss: 0.384509  [83200/175341]\n",
      "loss: 0.497057  [84800/175341]\n",
      "loss: 0.596734  [86400/175341]\n",
      "loss: 0.417858  [88000/175341]\n",
      "loss: 1.084520  [89600/175341]\n",
      "loss: 0.608672  [91200/175341]\n",
      "loss: 0.419534  [92800/175341]\n",
      "loss: 0.471030  [94400/175341]\n",
      "loss: 0.666454  [96000/175341]\n",
      "loss: 0.561211  [97600/175341]\n",
      "loss: 0.086665  [99200/175341]\n",
      "loss: 0.571202  [100800/175341]\n",
      "loss: 0.506208  [102400/175341]\n",
      "loss: 0.405896  [104000/175341]\n",
      "loss: 0.807697  [105600/175341]\n",
      "loss: 0.444702  [107200/175341]\n",
      "loss: 1.021442  [108800/175341]\n",
      "loss: 0.601932  [110400/175341]\n",
      "loss: 0.308831  [112000/175341]\n",
      "loss: 0.319529  [113600/175341]\n",
      "loss: 0.636525  [115200/175341]\n",
      "loss: 0.477315  [116800/175341]\n",
      "loss: 0.347920  [118400/175341]\n",
      "loss: 0.187908  [120000/175341]\n",
      "loss: 0.258603  [121600/175341]\n",
      "loss: 0.723418  [123200/175341]\n",
      "loss: 0.779574  [124800/175341]\n",
      "loss: 0.850660  [126400/175341]\n",
      "loss: 0.233460  [128000/175341]\n",
      "loss: 0.318033  [129600/175341]\n",
      "loss: 0.642091  [131200/175341]\n",
      "loss: 0.526242  [132800/175341]\n",
      "loss: 0.363758  [134400/175341]\n",
      "loss: 0.715397  [136000/175341]\n",
      "loss: 0.766113  [137600/175341]\n",
      "loss: 0.793738  [139200/175341]\n",
      "loss: 0.616084  [140800/175341]\n",
      "loss: 0.412293  [142400/175341]\n",
      "loss: 0.221298  [144000/175341]\n",
      "loss: 0.360907  [145600/175341]\n",
      "loss: 0.514179  [147200/175341]\n",
      "loss: 0.238898  [148800/175341]\n",
      "loss: 0.512632  [150400/175341]\n",
      "loss: 0.363473  [152000/175341]\n",
      "loss: 0.904221  [153600/175341]\n",
      "loss: 0.260477  [155200/175341]\n",
      "loss: 0.602747  [156800/175341]\n",
      "loss: 0.346016  [158400/175341]\n",
      "loss: 0.270071  [160000/175341]\n",
      "loss: 0.403905  [161600/175341]\n",
      "loss: 0.534435  [163200/175341]\n",
      "loss: 0.234748  [164800/175341]\n",
      "loss: 0.449908  [166400/175341]\n",
      "loss: 0.389350  [168000/175341]\n",
      "loss: 0.774424  [169600/175341]\n",
      "loss: 0.309865  [171200/175341]\n",
      "loss: 0.232155  [172800/175341]\n",
      "loss: 0.285831  [174400/175341]\n",
      "Train Accuracy: 81.8884%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.563422, F1-score: 75.64%, Macro_F1-Score:  41.20%  \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.294899  [    0/175341]\n",
      "loss: 0.775654  [ 1600/175341]\n",
      "loss: 0.285166  [ 3200/175341]\n",
      "loss: 0.391631  [ 4800/175341]\n",
      "loss: 0.729693  [ 6400/175341]\n",
      "loss: 0.370725  [ 8000/175341]\n",
      "loss: 0.508702  [ 9600/175341]\n",
      "loss: 0.563314  [11200/175341]\n",
      "loss: 0.399621  [12800/175341]\n",
      "loss: 0.478945  [14400/175341]\n",
      "loss: 0.564849  [16000/175341]\n",
      "loss: 0.636515  [17600/175341]\n",
      "loss: 0.738911  [19200/175341]\n",
      "loss: 0.437751  [20800/175341]\n",
      "loss: 0.236365  [22400/175341]\n",
      "loss: 0.527888  [24000/175341]\n",
      "loss: 0.291849  [25600/175341]\n",
      "loss: 0.730278  [27200/175341]\n",
      "loss: 0.472705  [28800/175341]\n",
      "loss: 0.792198  [30400/175341]\n",
      "loss: 0.295380  [32000/175341]\n",
      "loss: 0.263509  [33600/175341]\n",
      "loss: 0.552693  [35200/175341]\n",
      "loss: 0.285776  [36800/175341]\n",
      "loss: 0.219537  [38400/175341]\n",
      "loss: 0.059859  [40000/175341]\n",
      "loss: 0.622152  [41600/175341]\n",
      "loss: 0.680046  [43200/175341]\n",
      "loss: 0.674869  [44800/175341]\n",
      "loss: 0.122165  [46400/175341]\n",
      "loss: 0.832113  [48000/175341]\n",
      "loss: 0.542374  [49600/175341]\n",
      "loss: 0.268967  [51200/175341]\n",
      "loss: 0.457727  [52800/175341]\n",
      "loss: 1.041364  [54400/175341]\n",
      "loss: 0.343070  [56000/175341]\n",
      "loss: 0.638738  [57600/175341]\n",
      "loss: 0.505888  [59200/175341]\n",
      "loss: 0.787439  [60800/175341]\n",
      "loss: 0.247972  [62400/175341]\n",
      "loss: 0.497679  [64000/175341]\n",
      "loss: 0.260502  [65600/175341]\n",
      "loss: 0.395409  [67200/175341]\n",
      "loss: 0.510844  [68800/175341]\n",
      "loss: 0.496522  [70400/175341]\n",
      "loss: 0.581195  [72000/175341]\n",
      "loss: 0.174207  [73600/175341]\n",
      "loss: 0.276747  [75200/175341]\n",
      "loss: 0.979913  [76800/175341]\n",
      "loss: 0.697800  [78400/175341]\n",
      "loss: 0.333394  [80000/175341]\n",
      "loss: 0.798982  [81600/175341]\n",
      "loss: 0.425523  [83200/175341]\n",
      "loss: 0.342849  [84800/175341]\n",
      "loss: 0.355924  [86400/175341]\n",
      "loss: 0.587967  [88000/175341]\n",
      "loss: 0.668150  [89600/175341]\n",
      "loss: 0.136804  [91200/175341]\n",
      "loss: 0.506392  [92800/175341]\n",
      "loss: 0.438426  [94400/175341]\n",
      "loss: 0.385185  [96000/175341]\n",
      "loss: 0.514540  [97600/175341]\n",
      "loss: 0.162352  [99200/175341]\n",
      "loss: 0.620366  [100800/175341]\n",
      "loss: 0.282529  [102400/175341]\n",
      "loss: 0.845955  [104000/175341]\n",
      "loss: 0.501976  [105600/175341]\n",
      "loss: 0.749259  [107200/175341]\n",
      "loss: 0.400412  [108800/175341]\n",
      "loss: 0.308555  [110400/175341]\n",
      "loss: 0.666445  [112000/175341]\n",
      "loss: 0.410258  [113600/175341]\n",
      "loss: 0.529669  [115200/175341]\n",
      "loss: 0.399400  [116800/175341]\n",
      "loss: 0.827239  [118400/175341]\n",
      "loss: 0.458716  [120000/175341]\n",
      "loss: 0.384584  [121600/175341]\n",
      "loss: 0.321057  [123200/175341]\n",
      "loss: 0.521695  [124800/175341]\n",
      "loss: 0.317568  [126400/175341]\n",
      "loss: 0.372824  [128000/175341]\n",
      "loss: 0.422831  [129600/175341]\n",
      "loss: 0.365709  [131200/175341]\n",
      "loss: 0.671023  [132800/175341]\n",
      "loss: 0.396506  [134400/175341]\n",
      "loss: 0.679397  [136000/175341]\n",
      "loss: 0.275953  [137600/175341]\n",
      "loss: 0.222008  [139200/175341]\n",
      "loss: 0.718439  [140800/175341]\n",
      "loss: 0.855658  [142400/175341]\n",
      "loss: 0.248428  [144000/175341]\n",
      "loss: 0.368273  [145600/175341]\n",
      "loss: 0.410483  [147200/175341]\n",
      "loss: 0.375775  [148800/175341]\n",
      "loss: 0.233109  [150400/175341]\n",
      "loss: 0.273232  [152000/175341]\n",
      "loss: 0.218077  [153600/175341]\n",
      "loss: 0.384379  [155200/175341]\n",
      "loss: 0.538856  [156800/175341]\n",
      "loss: 0.094498  [158400/175341]\n",
      "loss: 0.197497  [160000/175341]\n",
      "loss: 0.475018  [161600/175341]\n",
      "loss: 0.409589  [163200/175341]\n",
      "loss: 0.600198  [164800/175341]\n",
      "loss: 0.633939  [166400/175341]\n",
      "loss: 0.353219  [168000/175341]\n",
      "loss: 0.336206  [169600/175341]\n",
      "loss: 0.838184  [171200/175341]\n",
      "loss: 0.431090  [172800/175341]\n",
      "loss: 0.989105  [174400/175341]\n",
      "Train Accuracy: 81.9141%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.555877, F1-score: 76.14%, Macro_F1-Score:  41.50%  \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.510520  [    0/175341]\n",
      "loss: 0.323575  [ 1600/175341]\n",
      "loss: 0.319883  [ 3200/175341]\n",
      "loss: 0.332961  [ 4800/175341]\n",
      "loss: 0.462301  [ 6400/175341]\n",
      "loss: 0.711112  [ 8000/175341]\n",
      "loss: 0.242504  [ 9600/175341]\n",
      "loss: 0.164398  [11200/175341]\n",
      "loss: 0.464119  [12800/175341]\n",
      "loss: 0.423051  [14400/175341]\n",
      "loss: 0.233801  [16000/175341]\n",
      "loss: 0.642918  [17600/175341]\n",
      "loss: 0.281840  [19200/175341]\n",
      "loss: 0.565544  [20800/175341]\n",
      "loss: 0.418540  [22400/175341]\n",
      "loss: 0.274039  [24000/175341]\n",
      "loss: 0.635487  [25600/175341]\n",
      "loss: 0.261628  [27200/175341]\n",
      "loss: 0.272614  [28800/175341]\n",
      "loss: 0.382299  [30400/175341]\n",
      "loss: 0.371491  [32000/175341]\n",
      "loss: 0.605464  [33600/175341]\n",
      "loss: 0.298155  [35200/175341]\n",
      "loss: 0.307995  [36800/175341]\n",
      "loss: 0.255379  [38400/175341]\n",
      "loss: 0.619100  [40000/175341]\n",
      "loss: 0.979428  [41600/175341]\n",
      "loss: 0.472281  [43200/175341]\n",
      "loss: 0.943896  [44800/175341]\n",
      "loss: 0.387869  [46400/175341]\n",
      "loss: 0.464964  [48000/175341]\n",
      "loss: 0.657058  [49600/175341]\n",
      "loss: 0.378630  [51200/175341]\n",
      "loss: 0.434274  [52800/175341]\n",
      "loss: 0.416609  [54400/175341]\n",
      "loss: 0.415910  [56000/175341]\n",
      "loss: 0.114820  [57600/175341]\n",
      "loss: 0.549831  [59200/175341]\n",
      "loss: 0.530676  [60800/175341]\n",
      "loss: 0.417677  [62400/175341]\n",
      "loss: 0.481146  [64000/175341]\n",
      "loss: 0.637674  [65600/175341]\n",
      "loss: 0.313963  [67200/175341]\n",
      "loss: 0.705569  [68800/175341]\n",
      "loss: 0.352294  [70400/175341]\n",
      "loss: 0.723168  [72000/175341]\n",
      "loss: 0.361435  [73600/175341]\n",
      "loss: 0.385471  [75200/175341]\n",
      "loss: 0.575611  [76800/175341]\n",
      "loss: 0.747985  [78400/175341]\n",
      "loss: 0.663142  [80000/175341]\n",
      "loss: 0.224252  [81600/175341]\n",
      "loss: 0.618340  [83200/175341]\n",
      "loss: 0.324218  [84800/175341]\n",
      "loss: 0.635947  [86400/175341]\n",
      "loss: 0.238946  [88000/175341]\n",
      "loss: 0.717657  [89600/175341]\n",
      "loss: 0.845519  [91200/175341]\n",
      "loss: 0.611686  [92800/175341]\n",
      "loss: 0.391672  [94400/175341]\n",
      "loss: 0.302523  [96000/175341]\n",
      "loss: 0.300161  [97600/175341]\n",
      "loss: 0.358746  [99200/175341]\n",
      "loss: 0.444504  [100800/175341]\n",
      "loss: 0.592688  [102400/175341]\n",
      "loss: 0.788896  [104000/175341]\n",
      "loss: 0.493102  [105600/175341]\n",
      "loss: 0.720610  [107200/175341]\n",
      "loss: 0.753198  [108800/175341]\n",
      "loss: 0.576900  [110400/175341]\n",
      "loss: 0.337048  [112000/175341]\n",
      "loss: 0.294194  [113600/175341]\n",
      "loss: 0.270795  [115200/175341]\n",
      "loss: 0.332036  [116800/175341]\n",
      "loss: 0.877257  [118400/175341]\n",
      "loss: 0.475939  [120000/175341]\n",
      "loss: 0.498423  [121600/175341]\n",
      "loss: 0.375882  [123200/175341]\n",
      "loss: 0.748126  [124800/175341]\n",
      "loss: 0.461298  [126400/175341]\n",
      "loss: 0.112786  [128000/175341]\n",
      "loss: 0.532909  [129600/175341]\n",
      "loss: 0.235562  [131200/175341]\n",
      "loss: 0.480478  [132800/175341]\n",
      "loss: 0.555788  [134400/175341]\n",
      "loss: 0.485075  [136000/175341]\n",
      "loss: 0.308088  [137600/175341]\n",
      "loss: 0.259675  [139200/175341]\n",
      "loss: 0.574352  [140800/175341]\n",
      "loss: 0.529537  [142400/175341]\n",
      "loss: 0.334618  [144000/175341]\n",
      "loss: 0.802500  [145600/175341]\n",
      "loss: 0.394060  [147200/175341]\n",
      "loss: 0.312351  [148800/175341]\n",
      "loss: 0.207665  [150400/175341]\n",
      "loss: 0.268643  [152000/175341]\n",
      "loss: 0.659601  [153600/175341]\n",
      "loss: 0.523876  [155200/175341]\n",
      "loss: 0.528208  [156800/175341]\n",
      "loss: 0.635610  [158400/175341]\n",
      "loss: 0.362163  [160000/175341]\n",
      "loss: 0.348718  [161600/175341]\n",
      "loss: 0.191373  [163200/175341]\n",
      "loss: 0.873553  [164800/175341]\n",
      "loss: 0.709010  [166400/175341]\n",
      "loss: 0.377358  [168000/175341]\n",
      "loss: 0.258423  [169600/175341]\n",
      "loss: 0.469271  [171200/175341]\n",
      "loss: 0.405049  [172800/175341]\n",
      "loss: 0.716019  [174400/175341]\n",
      "Train Accuracy: 81.9055%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.551090, F1-score: 76.60%, Macro_F1-Score:  41.59%  \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.187774  [    0/175341]\n",
      "loss: 0.609951  [ 1600/175341]\n",
      "loss: 0.920688  [ 3200/175341]\n",
      "loss: 0.408263  [ 4800/175341]\n",
      "loss: 0.297728  [ 6400/175341]\n",
      "loss: 0.191332  [ 8000/175341]\n",
      "loss: 0.405488  [ 9600/175341]\n",
      "loss: 0.479579  [11200/175341]\n",
      "loss: 0.403294  [12800/175341]\n",
      "loss: 0.924014  [14400/175341]\n",
      "loss: 0.628420  [16000/175341]\n",
      "loss: 0.346918  [17600/175341]\n",
      "loss: 0.343132  [19200/175341]\n",
      "loss: 0.529057  [20800/175341]\n",
      "loss: 0.165536  [22400/175341]\n",
      "loss: 0.421057  [24000/175341]\n",
      "loss: 0.460632  [25600/175341]\n",
      "loss: 0.335440  [27200/175341]\n",
      "loss: 0.557531  [28800/175341]\n",
      "loss: 0.681777  [30400/175341]\n",
      "loss: 0.220693  [32000/175341]\n",
      "loss: 0.359879  [33600/175341]\n",
      "loss: 0.189387  [35200/175341]\n",
      "loss: 0.570308  [36800/175341]\n",
      "loss: 0.331597  [38400/175341]\n",
      "loss: 0.242560  [40000/175341]\n",
      "loss: 0.368801  [41600/175341]\n",
      "loss: 0.472723  [43200/175341]\n",
      "loss: 0.284216  [44800/175341]\n",
      "loss: 0.716465  [46400/175341]\n",
      "loss: 0.306002  [48000/175341]\n",
      "loss: 0.438148  [49600/175341]\n",
      "loss: 0.341027  [51200/175341]\n",
      "loss: 0.273167  [52800/175341]\n",
      "loss: 0.472350  [54400/175341]\n",
      "loss: 0.653301  [56000/175341]\n",
      "loss: 0.590834  [57600/175341]\n",
      "loss: 0.395319  [59200/175341]\n",
      "loss: 0.346656  [60800/175341]\n",
      "loss: 0.501650  [62400/175341]\n",
      "loss: 0.499689  [64000/175341]\n",
      "loss: 0.220349  [65600/175341]\n",
      "loss: 0.168192  [67200/175341]\n",
      "loss: 0.167219  [68800/175341]\n",
      "loss: 0.727800  [70400/175341]\n",
      "loss: 0.263047  [72000/175341]\n",
      "loss: 0.392231  [73600/175341]\n",
      "loss: 0.605340  [75200/175341]\n",
      "loss: 0.552545  [76800/175341]\n",
      "loss: 0.535425  [78400/175341]\n",
      "loss: 0.365736  [80000/175341]\n",
      "loss: 0.103845  [81600/175341]\n",
      "loss: 0.419453  [83200/175341]\n",
      "loss: 0.221834  [84800/175341]\n",
      "loss: 0.451049  [86400/175341]\n",
      "loss: 0.439236  [88000/175341]\n",
      "loss: 0.534480  [89600/175341]\n",
      "loss: 1.185370  [91200/175341]\n",
      "loss: 0.205064  [92800/175341]\n",
      "loss: 0.525715  [94400/175341]\n",
      "loss: 0.419599  [96000/175341]\n",
      "loss: 0.575741  [97600/175341]\n",
      "loss: 0.481008  [99200/175341]\n",
      "loss: 0.425987  [100800/175341]\n",
      "loss: 0.647543  [102400/175341]\n",
      "loss: 0.240447  [104000/175341]\n",
      "loss: 0.464997  [105600/175341]\n",
      "loss: 0.454851  [107200/175341]\n",
      "loss: 0.233196  [108800/175341]\n",
      "loss: 0.266727  [110400/175341]\n",
      "loss: 0.378523  [112000/175341]\n",
      "loss: 0.517477  [113600/175341]\n",
      "loss: 0.679448  [115200/175341]\n",
      "loss: 0.456607  [116800/175341]\n",
      "loss: 0.648029  [118400/175341]\n",
      "loss: 0.599473  [120000/175341]\n",
      "loss: 0.275542  [121600/175341]\n",
      "loss: 0.656264  [123200/175341]\n",
      "loss: 0.462601  [124800/175341]\n",
      "loss: 0.202680  [126400/175341]\n",
      "loss: 0.779733  [128000/175341]\n",
      "loss: 0.787813  [129600/175341]\n",
      "loss: 0.236941  [131200/175341]\n",
      "loss: 0.342026  [132800/175341]\n",
      "loss: 0.421242  [134400/175341]\n",
      "loss: 0.250259  [136000/175341]\n",
      "loss: 0.291952  [137600/175341]\n",
      "loss: 0.403946  [139200/175341]\n",
      "loss: 0.375489  [140800/175341]\n",
      "loss: 0.492413  [142400/175341]\n",
      "loss: 0.208433  [144000/175341]\n",
      "loss: 0.341031  [145600/175341]\n",
      "loss: 0.625095  [147200/175341]\n",
      "loss: 0.294422  [148800/175341]\n",
      "loss: 0.287053  [150400/175341]\n",
      "loss: 0.364249  [152000/175341]\n",
      "loss: 0.222730  [153600/175341]\n",
      "loss: 0.523549  [155200/175341]\n",
      "loss: 0.302988  [156800/175341]\n",
      "loss: 0.273666  [158400/175341]\n",
      "loss: 0.501688  [160000/175341]\n",
      "loss: 0.244606  [161600/175341]\n",
      "loss: 0.499622  [163200/175341]\n",
      "loss: 0.477756  [164800/175341]\n",
      "loss: 0.743292  [166400/175341]\n",
      "loss: 0.375316  [168000/175341]\n",
      "loss: 0.583610  [169600/175341]\n",
      "loss: 0.597378  [171200/175341]\n",
      "loss: 0.303634  [172800/175341]\n",
      "loss: 0.734828  [174400/175341]\n",
      "Train Accuracy: 81.8930%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.552923, F1-score: 76.61%, Macro_F1-Score:  41.62%  \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.659144  [    0/175341]\n",
      "loss: 0.590133  [ 1600/175341]\n",
      "loss: 0.638683  [ 3200/175341]\n",
      "loss: 0.202098  [ 4800/175341]\n",
      "loss: 0.354421  [ 6400/175341]\n",
      "loss: 0.428748  [ 8000/175341]\n",
      "loss: 0.289255  [ 9600/175341]\n",
      "loss: 0.381409  [11200/175341]\n",
      "loss: 0.729478  [12800/175341]\n",
      "loss: 0.387072  [14400/175341]\n",
      "loss: 0.847373  [16000/175341]\n",
      "loss: 0.287173  [17600/175341]\n",
      "loss: 0.182884  [19200/175341]\n",
      "loss: 0.370478  [20800/175341]\n",
      "loss: 0.140611  [22400/175341]\n",
      "loss: 0.273823  [24000/175341]\n",
      "loss: 0.694717  [25600/175341]\n",
      "loss: 0.205961  [27200/175341]\n",
      "loss: 0.443273  [28800/175341]\n",
      "loss: 0.568261  [30400/175341]\n",
      "loss: 0.360707  [32000/175341]\n",
      "loss: 0.307408  [33600/175341]\n",
      "loss: 0.332480  [35200/175341]\n",
      "loss: 0.704301  [36800/175341]\n",
      "loss: 0.253433  [38400/175341]\n",
      "loss: 0.291528  [40000/175341]\n",
      "loss: 0.361292  [41600/175341]\n",
      "loss: 0.572532  [43200/175341]\n",
      "loss: 0.972376  [44800/175341]\n",
      "loss: 1.217692  [46400/175341]\n",
      "loss: 0.530098  [48000/175341]\n",
      "loss: 0.496480  [49600/175341]\n",
      "loss: 0.597354  [51200/175341]\n",
      "loss: 0.427157  [52800/175341]\n",
      "loss: 0.496827  [54400/175341]\n",
      "loss: 0.234989  [56000/175341]\n",
      "loss: 0.101472  [57600/175341]\n",
      "loss: 0.537122  [59200/175341]\n",
      "loss: 0.615321  [60800/175341]\n",
      "loss: 0.264602  [62400/175341]\n",
      "loss: 0.219099  [64000/175341]\n",
      "loss: 0.133826  [65600/175341]\n",
      "loss: 0.663815  [67200/175341]\n",
      "loss: 0.855297  [68800/175341]\n",
      "loss: 0.598578  [70400/175341]\n",
      "loss: 0.285570  [72000/175341]\n",
      "loss: 0.553990  [73600/175341]\n",
      "loss: 0.437298  [75200/175341]\n",
      "loss: 0.409722  [76800/175341]\n",
      "loss: 0.537028  [78400/175341]\n",
      "loss: 0.502605  [80000/175341]\n",
      "loss: 0.595252  [81600/175341]\n",
      "loss: 0.661448  [83200/175341]\n",
      "loss: 0.206804  [84800/175341]\n",
      "loss: 0.260988  [86400/175341]\n",
      "loss: 0.361425  [88000/175341]\n",
      "loss: 0.637169  [89600/175341]\n",
      "loss: 0.246050  [91200/175341]\n",
      "loss: 0.719428  [92800/175341]\n",
      "loss: 0.219601  [94400/175341]\n",
      "loss: 0.315619  [96000/175341]\n",
      "loss: 0.365191  [97600/175341]\n",
      "loss: 0.223205  [99200/175341]\n",
      "loss: 0.807850  [100800/175341]\n",
      "loss: 0.242983  [102400/175341]\n",
      "loss: 0.549718  [104000/175341]\n",
      "loss: 0.409643  [105600/175341]\n",
      "loss: 0.307096  [107200/175341]\n",
      "loss: 0.440301  [108800/175341]\n",
      "loss: 0.257408  [110400/175341]\n",
      "loss: 0.099328  [112000/175341]\n",
      "loss: 0.380221  [113600/175341]\n",
      "loss: 0.830955  [115200/175341]\n",
      "loss: 0.123636  [116800/175341]\n",
      "loss: 0.495444  [118400/175341]\n",
      "loss: 0.370941  [120000/175341]\n",
      "loss: 0.556790  [121600/175341]\n",
      "loss: 0.340227  [123200/175341]\n",
      "loss: 1.021920  [124800/175341]\n",
      "loss: 0.238867  [126400/175341]\n",
      "loss: 0.614779  [128000/175341]\n",
      "loss: 0.407037  [129600/175341]\n",
      "loss: 0.810828  [131200/175341]\n",
      "loss: 0.155980  [132800/175341]\n",
      "loss: 0.570207  [134400/175341]\n",
      "loss: 0.290075  [136000/175341]\n",
      "loss: 0.407242  [137600/175341]\n",
      "loss: 0.290239  [139200/175341]\n",
      "loss: 0.448803  [140800/175341]\n",
      "loss: 0.690766  [142400/175341]\n",
      "loss: 0.653132  [144000/175341]\n",
      "loss: 0.797437  [145600/175341]\n",
      "loss: 0.504940  [147200/175341]\n",
      "loss: 0.424288  [148800/175341]\n",
      "loss: 0.225644  [150400/175341]\n",
      "loss: 0.381380  [152000/175341]\n",
      "loss: 0.368015  [153600/175341]\n",
      "loss: 0.345991  [155200/175341]\n",
      "loss: 0.626507  [156800/175341]\n",
      "loss: 0.677556  [158400/175341]\n",
      "loss: 0.877463  [160000/175341]\n",
      "loss: 0.563824  [161600/175341]\n",
      "loss: 0.124411  [163200/175341]\n",
      "loss: 0.615937  [164800/175341]\n",
      "loss: 0.704023  [166400/175341]\n",
      "loss: 0.623010  [168000/175341]\n",
      "loss: 0.393347  [169600/175341]\n",
      "loss: 0.434893  [171200/175341]\n",
      "loss: 0.562756  [172800/175341]\n",
      "loss: 0.452778  [174400/175341]\n",
      "Train Accuracy: 81.9061%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.547967, F1-score: 76.95%, Macro_F1-Score:  41.57%  \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.224886  [    0/175341]\n",
      "loss: 0.250818  [ 1600/175341]\n",
      "loss: 0.321472  [ 3200/175341]\n",
      "loss: 0.164488  [ 4800/175341]\n",
      "loss: 0.451547  [ 6400/175341]\n",
      "loss: 0.258382  [ 8000/175341]\n",
      "loss: 0.483119  [ 9600/175341]\n",
      "loss: 0.330726  [11200/175341]\n",
      "loss: 0.416563  [12800/175341]\n",
      "loss: 0.528289  [14400/175341]\n",
      "loss: 0.238037  [16000/175341]\n",
      "loss: 0.880823  [17600/175341]\n",
      "loss: 0.369227  [19200/175341]\n",
      "loss: 0.360870  [20800/175341]\n",
      "loss: 0.476466  [22400/175341]\n",
      "loss: 0.379998  [24000/175341]\n",
      "loss: 0.574283  [25600/175341]\n",
      "loss: 0.413292  [27200/175341]\n",
      "loss: 0.531556  [28800/175341]\n",
      "loss: 0.277507  [30400/175341]\n",
      "loss: 0.766946  [32000/175341]\n",
      "loss: 0.341443  [33600/175341]\n",
      "loss: 0.339516  [35200/175341]\n",
      "loss: 0.149629  [36800/175341]\n",
      "loss: 0.519533  [38400/175341]\n",
      "loss: 0.250770  [40000/175341]\n",
      "loss: 0.475958  [41600/175341]\n",
      "loss: 0.395866  [43200/175341]\n",
      "loss: 0.254582  [44800/175341]\n",
      "loss: 0.177316  [46400/175341]\n",
      "loss: 0.289700  [48000/175341]\n",
      "loss: 0.502511  [49600/175341]\n",
      "loss: 0.280563  [51200/175341]\n",
      "loss: 0.444915  [52800/175341]\n",
      "loss: 0.504214  [54400/175341]\n",
      "loss: 0.338277  [56000/175341]\n",
      "loss: 0.737422  [57600/175341]\n",
      "loss: 0.573768  [59200/175341]\n",
      "loss: 0.467964  [60800/175341]\n",
      "loss: 0.301557  [62400/175341]\n",
      "loss: 0.370474  [64000/175341]\n",
      "loss: 0.348307  [65600/175341]\n",
      "loss: 0.291986  [67200/175341]\n",
      "loss: 0.734949  [68800/175341]\n",
      "loss: 0.324650  [70400/175341]\n",
      "loss: 0.480292  [72000/175341]\n",
      "loss: 0.320551  [73600/175341]\n",
      "loss: 0.133690  [75200/175341]\n",
      "loss: 0.693124  [76800/175341]\n",
      "loss: 0.408424  [78400/175341]\n",
      "loss: 0.849101  [80000/175341]\n",
      "loss: 0.198355  [81600/175341]\n",
      "loss: 0.542649  [83200/175341]\n",
      "loss: 0.483964  [84800/175341]\n",
      "loss: 0.473643  [86400/175341]\n",
      "loss: 0.311448  [88000/175341]\n",
      "loss: 0.344292  [89600/175341]\n",
      "loss: 0.422209  [91200/175341]\n",
      "loss: 0.580894  [92800/175341]\n",
      "loss: 0.303765  [94400/175341]\n",
      "loss: 0.252130  [96000/175341]\n",
      "loss: 0.139259  [97600/175341]\n",
      "loss: 0.670214  [99200/175341]\n",
      "loss: 0.556366  [100800/175341]\n",
      "loss: 0.813410  [102400/175341]\n",
      "loss: 0.271424  [104000/175341]\n",
      "loss: 0.324454  [105600/175341]\n",
      "loss: 0.182047  [107200/175341]\n",
      "loss: 0.382054  [108800/175341]\n",
      "loss: 0.578317  [110400/175341]\n",
      "loss: 0.423776  [112000/175341]\n",
      "loss: 0.667252  [113600/175341]\n",
      "loss: 0.181774  [115200/175341]\n",
      "loss: 0.599604  [116800/175341]\n",
      "loss: 0.829667  [118400/175341]\n",
      "loss: 0.363113  [120000/175341]\n",
      "loss: 0.402724  [121600/175341]\n",
      "loss: 0.397424  [123200/175341]\n",
      "loss: 0.564704  [124800/175341]\n",
      "loss: 0.288667  [126400/175341]\n",
      "loss: 0.703993  [128000/175341]\n",
      "loss: 0.867204  [129600/175341]\n",
      "loss: 0.130088  [131200/175341]\n",
      "loss: 0.466413  [132800/175341]\n",
      "loss: 0.162790  [134400/175341]\n",
      "loss: 0.514348  [136000/175341]\n",
      "loss: 0.558582  [137600/175341]\n",
      "loss: 0.468850  [139200/175341]\n",
      "loss: 0.838894  [140800/175341]\n",
      "loss: 0.612472  [142400/175341]\n",
      "loss: 0.271507  [144000/175341]\n",
      "loss: 0.652321  [145600/175341]\n",
      "loss: 0.518023  [147200/175341]\n",
      "loss: 0.279303  [148800/175341]\n",
      "loss: 0.377451  [150400/175341]\n",
      "loss: 0.295711  [152000/175341]\n",
      "loss: 0.341878  [153600/175341]\n",
      "loss: 0.274871  [155200/175341]\n",
      "loss: 0.476013  [156800/175341]\n",
      "loss: 0.504271  [158400/175341]\n",
      "loss: 0.438509  [160000/175341]\n",
      "loss: 0.306416  [161600/175341]\n",
      "loss: 0.366314  [163200/175341]\n",
      "loss: 0.424095  [164800/175341]\n",
      "loss: 0.382935  [166400/175341]\n",
      "loss: 0.515619  [168000/175341]\n",
      "loss: 0.518260  [169600/175341]\n",
      "loss: 0.351754  [171200/175341]\n",
      "loss: 0.275443  [172800/175341]\n",
      "loss: 0.177901  [174400/175341]\n",
      "Train Accuracy: 81.9090%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.544421, F1-score: 77.01%, Macro_F1-Score:  41.50%  \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.388535  [    0/175341]\n",
      "loss: 0.721754  [ 1600/175341]\n",
      "loss: 0.684605  [ 3200/175341]\n",
      "loss: 0.570454  [ 4800/175341]\n",
      "loss: 0.540403  [ 6400/175341]\n",
      "loss: 1.329278  [ 8000/175341]\n",
      "loss: 0.698322  [ 9600/175341]\n",
      "loss: 0.442246  [11200/175341]\n",
      "loss: 0.285623  [12800/175341]\n",
      "loss: 0.532277  [14400/175341]\n",
      "loss: 0.315772  [16000/175341]\n",
      "loss: 0.229300  [17600/175341]\n",
      "loss: 0.489905  [19200/175341]\n",
      "loss: 0.877523  [20800/175341]\n",
      "loss: 0.140505  [22400/175341]\n",
      "loss: 0.743846  [24000/175341]\n",
      "loss: 0.280362  [25600/175341]\n",
      "loss: 0.393368  [27200/175341]\n",
      "loss: 0.426338  [28800/175341]\n",
      "loss: 0.294140  [30400/175341]\n",
      "loss: 0.248354  [32000/175341]\n",
      "loss: 0.328098  [33600/175341]\n",
      "loss: 0.329070  [35200/175341]\n",
      "loss: 0.135115  [36800/175341]\n",
      "loss: 0.332337  [38400/175341]\n",
      "loss: 0.245026  [40000/175341]\n",
      "loss: 0.804695  [41600/175341]\n",
      "loss: 0.472335  [43200/175341]\n",
      "loss: 0.634372  [44800/175341]\n",
      "loss: 0.342462  [46400/175341]\n",
      "loss: 0.285010  [48000/175341]\n",
      "loss: 0.391078  [49600/175341]\n",
      "loss: 0.423061  [51200/175341]\n",
      "loss: 0.490794  [52800/175341]\n",
      "loss: 0.622306  [54400/175341]\n",
      "loss: 0.425327  [56000/175341]\n",
      "loss: 0.380346  [57600/175341]\n",
      "loss: 0.472504  [59200/175341]\n",
      "loss: 0.192443  [60800/175341]\n",
      "loss: 0.425081  [62400/175341]\n",
      "loss: 0.581151  [64000/175341]\n",
      "loss: 0.453717  [65600/175341]\n",
      "loss: 0.853984  [67200/175341]\n",
      "loss: 0.334537  [68800/175341]\n",
      "loss: 0.770988  [70400/175341]\n",
      "loss: 0.429317  [72000/175341]\n",
      "loss: 0.541217  [73600/175341]\n",
      "loss: 0.407474  [75200/175341]\n",
      "loss: 0.435464  [76800/175341]\n",
      "loss: 0.597573  [78400/175341]\n",
      "loss: 0.297361  [80000/175341]\n",
      "loss: 0.425339  [81600/175341]\n",
      "loss: 0.252786  [83200/175341]\n",
      "loss: 0.309723  [84800/175341]\n",
      "loss: 0.713068  [86400/175341]\n",
      "loss: 0.358555  [88000/175341]\n",
      "loss: 0.251132  [89600/175341]\n",
      "loss: 0.699207  [91200/175341]\n",
      "loss: 0.347450  [92800/175341]\n",
      "loss: 0.257834  [94400/175341]\n",
      "loss: 0.123630  [96000/175341]\n",
      "loss: 0.489234  [97600/175341]\n",
      "loss: 0.252353  [99200/175341]\n",
      "loss: 0.301840  [100800/175341]\n",
      "loss: 0.331058  [102400/175341]\n",
      "loss: 0.161369  [104000/175341]\n",
      "loss: 0.276149  [105600/175341]\n",
      "loss: 0.302467  [107200/175341]\n",
      "loss: 0.166580  [108800/175341]\n",
      "loss: 0.464034  [110400/175341]\n",
      "loss: 0.218180  [112000/175341]\n",
      "loss: 0.492787  [113600/175341]\n",
      "loss: 0.772975  [115200/175341]\n",
      "loss: 0.421224  [116800/175341]\n",
      "loss: 0.260288  [118400/175341]\n",
      "loss: 0.759235  [120000/175341]\n",
      "loss: 0.924148  [121600/175341]\n",
      "loss: 0.472713  [123200/175341]\n",
      "loss: 0.475541  [124800/175341]\n",
      "loss: 0.592501  [126400/175341]\n",
      "loss: 0.341085  [128000/175341]\n",
      "loss: 0.515571  [129600/175341]\n",
      "loss: 0.154578  [131200/175341]\n",
      "loss: 0.297615  [132800/175341]\n",
      "loss: 0.482846  [134400/175341]\n",
      "loss: 0.557339  [136000/175341]\n",
      "loss: 0.880545  [137600/175341]\n",
      "loss: 0.396578  [139200/175341]\n",
      "loss: 0.333720  [140800/175341]\n",
      "loss: 0.522498  [142400/175341]\n",
      "loss: 0.436330  [144000/175341]\n",
      "loss: 0.782283  [145600/175341]\n",
      "loss: 0.252211  [147200/175341]\n",
      "loss: 0.165528  [148800/175341]\n",
      "loss: 0.344453  [150400/175341]\n",
      "loss: 0.666821  [152000/175341]\n",
      "loss: 0.660497  [153600/175341]\n",
      "loss: 0.397032  [155200/175341]\n",
      "loss: 0.078900  [156800/175341]\n",
      "loss: 0.437284  [158400/175341]\n",
      "loss: 0.285928  [160000/175341]\n",
      "loss: 0.742297  [161600/175341]\n",
      "loss: 0.525655  [163200/175341]\n",
      "loss: 1.032518  [164800/175341]\n",
      "loss: 0.203707  [166400/175341]\n",
      "loss: 0.288592  [168000/175341]\n",
      "loss: 0.275984  [169600/175341]\n",
      "loss: 0.714402  [171200/175341]\n",
      "loss: 0.561009  [172800/175341]\n",
      "loss: 0.819358  [174400/175341]\n",
      "Train Accuracy: 81.9535%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.565516, F1-score: 75.31%, Macro_F1-Score:  41.16%  \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.511480  [    0/175341]\n",
      "loss: 0.139369  [ 1600/175341]\n",
      "loss: 0.187039  [ 3200/175341]\n",
      "loss: 0.753900  [ 4800/175341]\n",
      "loss: 0.467135  [ 6400/175341]\n",
      "loss: 0.575818  [ 8000/175341]\n",
      "loss: 0.379151  [ 9600/175341]\n",
      "loss: 0.085594  [11200/175341]\n",
      "loss: 0.521193  [12800/175341]\n",
      "loss: 0.400274  [14400/175341]\n",
      "loss: 0.831023  [16000/175341]\n",
      "loss: 0.617054  [17600/175341]\n",
      "loss: 0.373126  [19200/175341]\n",
      "loss: 0.175983  [20800/175341]\n",
      "loss: 0.414122  [22400/175341]\n",
      "loss: 0.256045  [24000/175341]\n",
      "loss: 0.392388  [25600/175341]\n",
      "loss: 0.989972  [27200/175341]\n",
      "loss: 0.487517  [28800/175341]\n",
      "loss: 0.733948  [30400/175341]\n",
      "loss: 0.331187  [32000/175341]\n",
      "loss: 0.579940  [33600/175341]\n",
      "loss: 0.210186  [35200/175341]\n",
      "loss: 0.455065  [36800/175341]\n",
      "loss: 0.376107  [38400/175341]\n",
      "loss: 0.264372  [40000/175341]\n",
      "loss: 0.636659  [41600/175341]\n",
      "loss: 0.638626  [43200/175341]\n",
      "loss: 0.338474  [44800/175341]\n",
      "loss: 0.646021  [46400/175341]\n",
      "loss: 0.144760  [48000/175341]\n",
      "loss: 0.195888  [49600/175341]\n",
      "loss: 0.399987  [51200/175341]\n",
      "loss: 0.308653  [52800/175341]\n",
      "loss: 0.297813  [54400/175341]\n",
      "loss: 0.337603  [56000/175341]\n",
      "loss: 0.362293  [57600/175341]\n",
      "loss: 0.415163  [59200/175341]\n",
      "loss: 0.529966  [60800/175341]\n",
      "loss: 0.246511  [62400/175341]\n",
      "loss: 0.483201  [64000/175341]\n",
      "loss: 0.228381  [65600/175341]\n",
      "loss: 0.313319  [67200/175341]\n",
      "loss: 0.591509  [68800/175341]\n",
      "loss: 0.220486  [70400/175341]\n",
      "loss: 0.460334  [72000/175341]\n",
      "loss: 0.420636  [73600/175341]\n",
      "loss: 0.112437  [75200/175341]\n",
      "loss: 0.411593  [76800/175341]\n",
      "loss: 0.358588  [78400/175341]\n",
      "loss: 0.568102  [80000/175341]\n",
      "loss: 0.684839  [81600/175341]\n",
      "loss: 0.655061  [83200/175341]\n",
      "loss: 0.534343  [84800/175341]\n",
      "loss: 0.635203  [86400/175341]\n",
      "loss: 0.646124  [88000/175341]\n",
      "loss: 0.323713  [89600/175341]\n",
      "loss: 0.518165  [91200/175341]\n",
      "loss: 0.649363  [92800/175341]\n",
      "loss: 0.611438  [94400/175341]\n",
      "loss: 0.612157  [96000/175341]\n",
      "loss: 0.197891  [97600/175341]\n",
      "loss: 0.593252  [99200/175341]\n",
      "loss: 0.195594  [100800/175341]\n",
      "loss: 0.397107  [102400/175341]\n",
      "loss: 0.341796  [104000/175341]\n",
      "loss: 0.372142  [105600/175341]\n",
      "loss: 0.154471  [107200/175341]\n",
      "loss: 0.494050  [108800/175341]\n",
      "loss: 0.700095  [110400/175341]\n",
      "loss: 0.412662  [112000/175341]\n",
      "loss: 0.356197  [113600/175341]\n",
      "loss: 0.788795  [115200/175341]\n",
      "loss: 0.470177  [116800/175341]\n",
      "loss: 0.413546  [118400/175341]\n",
      "loss: 0.382855  [120000/175341]\n",
      "loss: 0.575922  [121600/175341]\n",
      "loss: 0.385937  [123200/175341]\n",
      "loss: 0.511698  [124800/175341]\n",
      "loss: 0.285932  [126400/175341]\n",
      "loss: 0.862351  [128000/175341]\n",
      "loss: 0.276262  [129600/175341]\n",
      "loss: 0.452543  [131200/175341]\n",
      "loss: 0.533350  [132800/175341]\n",
      "loss: 0.664206  [134400/175341]\n",
      "loss: 0.074660  [136000/175341]\n",
      "loss: 0.277260  [137600/175341]\n",
      "loss: 0.243282  [139200/175341]\n",
      "loss: 0.342729  [140800/175341]\n",
      "loss: 0.664256  [142400/175341]\n",
      "loss: 0.351932  [144000/175341]\n",
      "loss: 0.487996  [145600/175341]\n",
      "loss: 0.287960  [147200/175341]\n",
      "loss: 0.347935  [148800/175341]\n",
      "loss: 0.491194  [150400/175341]\n",
      "loss: 0.366877  [152000/175341]\n",
      "loss: 0.301357  [153600/175341]\n",
      "loss: 0.277979  [155200/175341]\n",
      "loss: 0.231487  [156800/175341]\n",
      "loss: 0.648726  [158400/175341]\n",
      "loss: 0.315580  [160000/175341]\n",
      "loss: 0.611841  [161600/175341]\n",
      "loss: 0.409355  [163200/175341]\n",
      "loss: 0.713384  [164800/175341]\n",
      "loss: 0.322464  [166400/175341]\n",
      "loss: 0.500551  [168000/175341]\n",
      "loss: 0.381429  [169600/175341]\n",
      "loss: 0.395627  [171200/175341]\n",
      "loss: 0.329562  [172800/175341]\n",
      "loss: 0.326875  [174400/175341]\n",
      "Train Accuracy: 81.9563%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.583375, F1-score: 74.67%, Macro_F1-Score:  40.62%  \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.152899  [    0/175341]\n",
      "loss: 0.617262  [ 1600/175341]\n",
      "loss: 0.098264  [ 3200/175341]\n",
      "loss: 0.407093  [ 4800/175341]\n",
      "loss: 0.233185  [ 6400/175341]\n",
      "loss: 0.260735  [ 8000/175341]\n",
      "loss: 0.445738  [ 9600/175341]\n",
      "loss: 0.517306  [11200/175341]\n",
      "loss: 0.347198  [12800/175341]\n",
      "loss: 0.404732  [14400/175341]\n",
      "loss: 0.331757  [16000/175341]\n",
      "loss: 0.379259  [17600/175341]\n",
      "loss: 0.539699  [19200/175341]\n",
      "loss: 0.257995  [20800/175341]\n",
      "loss: 0.395302  [22400/175341]\n",
      "loss: 0.269172  [24000/175341]\n",
      "loss: 0.409730  [25600/175341]\n",
      "loss: 0.315225  [27200/175341]\n",
      "loss: 0.604191  [28800/175341]\n",
      "loss: 0.227467  [30400/175341]\n",
      "loss: 0.663501  [32000/175341]\n",
      "loss: 0.394841  [33600/175341]\n",
      "loss: 0.344392  [35200/175341]\n",
      "loss: 1.105247  [36800/175341]\n",
      "loss: 0.561994  [38400/175341]\n",
      "loss: 0.508172  [40000/175341]\n",
      "loss: 0.346689  [41600/175341]\n",
      "loss: 0.267649  [43200/175341]\n",
      "loss: 0.107774  [44800/175341]\n",
      "loss: 0.383557  [46400/175341]\n",
      "loss: 0.387269  [48000/175341]\n",
      "loss: 0.557350  [49600/175341]\n",
      "loss: 0.403677  [51200/175341]\n",
      "loss: 0.445737  [52800/175341]\n",
      "loss: 0.472668  [54400/175341]\n",
      "loss: 0.597010  [56000/175341]\n",
      "loss: 0.356436  [57600/175341]\n",
      "loss: 0.300885  [59200/175341]\n",
      "loss: 0.805725  [60800/175341]\n",
      "loss: 0.318569  [62400/175341]\n",
      "loss: 0.246999  [64000/175341]\n",
      "loss: 0.417644  [65600/175341]\n",
      "loss: 0.283430  [67200/175341]\n",
      "loss: 0.429211  [68800/175341]\n",
      "loss: 0.441594  [70400/175341]\n",
      "loss: 0.430691  [72000/175341]\n",
      "loss: 0.496513  [73600/175341]\n",
      "loss: 0.876849  [75200/175341]\n",
      "loss: 0.583852  [76800/175341]\n",
      "loss: 0.600658  [78400/175341]\n",
      "loss: 0.424491  [80000/175341]\n",
      "loss: 0.298182  [81600/175341]\n",
      "loss: 0.649758  [83200/175341]\n",
      "loss: 0.563007  [84800/175341]\n",
      "loss: 0.380130  [86400/175341]\n",
      "loss: 0.607370  [88000/175341]\n",
      "loss: 0.470170  [89600/175341]\n",
      "loss: 0.330596  [91200/175341]\n",
      "loss: 0.394376  [92800/175341]\n",
      "loss: 0.224298  [94400/175341]\n",
      "loss: 0.276792  [96000/175341]\n",
      "loss: 0.853967  [97600/175341]\n",
      "loss: 0.569117  [99200/175341]\n",
      "loss: 0.259369  [100800/175341]\n",
      "loss: 0.568987  [102400/175341]\n",
      "loss: 0.737914  [104000/175341]\n",
      "loss: 0.212742  [105600/175341]\n",
      "loss: 0.688542  [107200/175341]\n",
      "loss: 0.192918  [108800/175341]\n",
      "loss: 0.517056  [110400/175341]\n",
      "loss: 0.544843  [112000/175341]\n",
      "loss: 0.308543  [113600/175341]\n",
      "loss: 0.457970  [115200/175341]\n",
      "loss: 0.777303  [116800/175341]\n",
      "loss: 0.343767  [118400/175341]\n",
      "loss: 0.299092  [120000/175341]\n",
      "loss: 0.251081  [121600/175341]\n",
      "loss: 0.446165  [123200/175341]\n",
      "loss: 0.522875  [124800/175341]\n",
      "loss: 0.786247  [126400/175341]\n",
      "loss: 0.210453  [128000/175341]\n",
      "loss: 0.318728  [129600/175341]\n",
      "loss: 0.378811  [131200/175341]\n",
      "loss: 0.415013  [132800/175341]\n",
      "loss: 0.384015  [134400/175341]\n",
      "loss: 0.601014  [136000/175341]\n",
      "loss: 0.253555  [137600/175341]\n",
      "loss: 0.470472  [139200/175341]\n",
      "loss: 0.492997  [140800/175341]\n",
      "loss: 0.571457  [142400/175341]\n",
      "loss: 0.437561  [144000/175341]\n",
      "loss: 0.218494  [145600/175341]\n",
      "loss: 0.471216  [147200/175341]\n",
      "loss: 0.259988  [148800/175341]\n",
      "loss: 0.171398  [150400/175341]\n",
      "loss: 0.366652  [152000/175341]\n",
      "loss: 0.499863  [153600/175341]\n",
      "loss: 0.421616  [155200/175341]\n",
      "loss: 0.474137  [156800/175341]\n",
      "loss: 0.669550  [158400/175341]\n",
      "loss: 0.165355  [160000/175341]\n",
      "loss: 0.356861  [161600/175341]\n",
      "loss: 0.374272  [163200/175341]\n",
      "loss: 0.453090  [164800/175341]\n",
      "loss: 0.350189  [166400/175341]\n",
      "loss: 0.707040  [168000/175341]\n",
      "loss: 0.495050  [169600/175341]\n",
      "loss: 0.724402  [171200/175341]\n",
      "loss: 0.174453  [172800/175341]\n",
      "loss: 0.419864  [174400/175341]\n",
      "Train Accuracy: 81.9529%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.574603, F1-score: 75.76%, Macro_F1-Score:  41.52%  \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.362200  [    0/175341]\n",
      "loss: 0.262927  [ 1600/175341]\n",
      "loss: 0.477667  [ 3200/175341]\n",
      "loss: 0.172771  [ 4800/175341]\n",
      "loss: 0.323715  [ 6400/175341]\n",
      "loss: 0.340386  [ 8000/175341]\n",
      "loss: 0.264843  [ 9600/175341]\n",
      "loss: 0.546323  [11200/175341]\n",
      "loss: 0.303842  [12800/175341]\n",
      "loss: 0.334001  [14400/175341]\n",
      "loss: 0.294172  [16000/175341]\n",
      "loss: 0.533068  [17600/175341]\n",
      "loss: 0.155496  [19200/175341]\n",
      "loss: 0.384867  [20800/175341]\n",
      "loss: 0.182434  [22400/175341]\n",
      "loss: 0.724862  [24000/175341]\n",
      "loss: 0.237524  [25600/175341]\n",
      "loss: 0.141795  [27200/175341]\n",
      "loss: 0.501178  [28800/175341]\n",
      "loss: 0.172112  [30400/175341]\n",
      "loss: 0.519512  [32000/175341]\n",
      "loss: 0.386019  [33600/175341]\n",
      "loss: 0.491843  [35200/175341]\n",
      "loss: 0.406456  [36800/175341]\n",
      "loss: 0.443983  [38400/175341]\n",
      "loss: 0.315131  [40000/175341]\n",
      "loss: 0.560885  [41600/175341]\n",
      "loss: 0.223761  [43200/175341]\n",
      "loss: 0.565397  [44800/175341]\n",
      "loss: 0.173865  [46400/175341]\n",
      "loss: 0.639368  [48000/175341]\n",
      "loss: 0.430155  [49600/175341]\n",
      "loss: 0.382684  [51200/175341]\n",
      "loss: 0.221080  [52800/175341]\n",
      "loss: 0.558268  [54400/175341]\n",
      "loss: 0.633383  [56000/175341]\n",
      "loss: 0.375854  [57600/175341]\n",
      "loss: 0.483416  [59200/175341]\n",
      "loss: 0.397416  [60800/175341]\n",
      "loss: 0.338847  [62400/175341]\n",
      "loss: 0.412006  [64000/175341]\n",
      "loss: 0.331059  [65600/175341]\n",
      "loss: 0.581052  [67200/175341]\n",
      "loss: 0.305914  [68800/175341]\n",
      "loss: 0.624681  [70400/175341]\n",
      "loss: 0.491373  [72000/175341]\n",
      "loss: 0.606820  [73600/175341]\n",
      "loss: 0.584656  [75200/175341]\n",
      "loss: 0.317688  [76800/175341]\n",
      "loss: 0.308426  [78400/175341]\n",
      "loss: 0.214522  [80000/175341]\n",
      "loss: 0.389216  [81600/175341]\n",
      "loss: 0.318447  [83200/175341]\n",
      "loss: 0.293838  [84800/175341]\n",
      "loss: 0.545047  [86400/175341]\n",
      "loss: 0.173199  [88000/175341]\n",
      "loss: 0.251331  [89600/175341]\n",
      "loss: 0.425228  [91200/175341]\n",
      "loss: 0.895258  [92800/175341]\n",
      "loss: 1.029296  [94400/175341]\n",
      "loss: 0.508755  [96000/175341]\n",
      "loss: 0.635456  [97600/175341]\n",
      "loss: 0.567142  [99200/175341]\n",
      "loss: 0.334298  [100800/175341]\n",
      "loss: 0.338551  [102400/175341]\n",
      "loss: 0.417395  [104000/175341]\n",
      "loss: 0.091188  [105600/175341]\n",
      "loss: 0.600477  [107200/175341]\n",
      "loss: 0.882006  [108800/175341]\n",
      "loss: 0.625538  [110400/175341]\n",
      "loss: 0.227441  [112000/175341]\n",
      "loss: 0.275765  [113600/175341]\n",
      "loss: 0.477275  [115200/175341]\n",
      "loss: 0.697410  [116800/175341]\n",
      "loss: 0.607744  [118400/175341]\n",
      "loss: 0.302577  [120000/175341]\n",
      "loss: 0.283457  [121600/175341]\n",
      "loss: 0.319421  [123200/175341]\n",
      "loss: 0.473080  [124800/175341]\n",
      "loss: 0.624671  [126400/175341]\n",
      "loss: 0.658364  [128000/175341]\n",
      "loss: 0.453083  [129600/175341]\n",
      "loss: 0.430510  [131200/175341]\n",
      "loss: 0.363644  [132800/175341]\n",
      "loss: 0.976575  [134400/175341]\n",
      "loss: 0.778501  [136000/175341]\n",
      "loss: 0.134261  [137600/175341]\n",
      "loss: 0.911151  [139200/175341]\n",
      "loss: 0.207098  [140800/175341]\n",
      "loss: 0.727104  [142400/175341]\n",
      "loss: 0.348542  [144000/175341]\n",
      "loss: 0.547215  [145600/175341]\n",
      "loss: 0.650735  [147200/175341]\n",
      "loss: 0.179204  [148800/175341]\n",
      "loss: 0.418664  [150400/175341]\n",
      "loss: 0.419809  [152000/175341]\n",
      "loss: 0.797289  [153600/175341]\n",
      "loss: 0.535695  [155200/175341]\n",
      "loss: 0.455357  [156800/175341]\n",
      "loss: 0.392820  [158400/175341]\n",
      "loss: 0.794092  [160000/175341]\n",
      "loss: 0.487250  [161600/175341]\n",
      "loss: 0.727715  [163200/175341]\n",
      "loss: 0.281622  [164800/175341]\n",
      "loss: 0.339899  [166400/175341]\n",
      "loss: 0.174343  [168000/175341]\n",
      "loss: 0.506447  [169600/175341]\n",
      "loss: 0.576471  [171200/175341]\n",
      "loss: 0.506352  [172800/175341]\n",
      "loss: 0.396962  [174400/175341]\n",
      "Train Accuracy: 81.9483%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.563030, F1-score: 76.01%, Macro_F1-Score:  41.59%  \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.135059  [    0/175341]\n",
      "loss: 0.164483  [ 1600/175341]\n",
      "loss: 0.227461  [ 3200/175341]\n",
      "loss: 0.396662  [ 4800/175341]\n",
      "loss: 0.258073  [ 6400/175341]\n",
      "loss: 0.313915  [ 8000/175341]\n",
      "loss: 0.562123  [ 9600/175341]\n",
      "loss: 0.321847  [11200/175341]\n",
      "loss: 0.321737  [12800/175341]\n",
      "loss: 0.299557  [14400/175341]\n",
      "loss: 0.206839  [16000/175341]\n",
      "loss: 0.419427  [17600/175341]\n",
      "loss: 0.365058  [19200/175341]\n",
      "loss: 0.868216  [20800/175341]\n",
      "loss: 1.101770  [22400/175341]\n",
      "loss: 0.429378  [24000/175341]\n",
      "loss: 0.348929  [25600/175341]\n",
      "loss: 0.652957  [27200/175341]\n",
      "loss: 0.494125  [28800/175341]\n",
      "loss: 0.767331  [30400/175341]\n",
      "loss: 0.288767  [32000/175341]\n",
      "loss: 0.319858  [33600/175341]\n",
      "loss: 0.449639  [35200/175341]\n",
      "loss: 0.654220  [36800/175341]\n",
      "loss: 0.275913  [38400/175341]\n",
      "loss: 0.157508  [40000/175341]\n",
      "loss: 0.459177  [41600/175341]\n",
      "loss: 0.455861  [43200/175341]\n",
      "loss: 0.301014  [44800/175341]\n",
      "loss: 0.328460  [46400/175341]\n",
      "loss: 0.494172  [48000/175341]\n",
      "loss: 0.219973  [49600/175341]\n",
      "loss: 1.224271  [51200/175341]\n",
      "loss: 0.682036  [52800/175341]\n",
      "loss: 0.560866  [54400/175341]\n",
      "loss: 0.595642  [56000/175341]\n",
      "loss: 0.574770  [57600/175341]\n",
      "loss: 0.247567  [59200/175341]\n",
      "loss: 0.477004  [60800/175341]\n",
      "loss: 0.432636  [62400/175341]\n",
      "loss: 0.313289  [64000/175341]\n",
      "loss: 0.631613  [65600/175341]\n",
      "loss: 0.442110  [67200/175341]\n",
      "loss: 0.579963  [68800/175341]\n",
      "loss: 0.435240  [70400/175341]\n",
      "loss: 0.623838  [72000/175341]\n",
      "loss: 0.466307  [73600/175341]\n",
      "loss: 0.184771  [75200/175341]\n",
      "loss: 0.715597  [76800/175341]\n",
      "loss: 0.387086  [78400/175341]\n",
      "loss: 0.617150  [80000/175341]\n",
      "loss: 0.474029  [81600/175341]\n",
      "loss: 0.775463  [83200/175341]\n",
      "loss: 0.248670  [84800/175341]\n",
      "loss: 0.078308  [86400/175341]\n",
      "loss: 0.486424  [88000/175341]\n",
      "loss: 0.483348  [89600/175341]\n",
      "loss: 0.200482  [91200/175341]\n",
      "loss: 0.402341  [92800/175341]\n",
      "loss: 0.291664  [94400/175341]\n",
      "loss: 0.458643  [96000/175341]\n",
      "loss: 0.224693  [97600/175341]\n",
      "loss: 0.359101  [99200/175341]\n",
      "loss: 0.696911  [100800/175341]\n",
      "loss: 0.634647  [102400/175341]\n",
      "loss: 0.314872  [104000/175341]\n",
      "loss: 0.315145  [105600/175341]\n",
      "loss: 0.364031  [107200/175341]\n",
      "loss: 0.473964  [108800/175341]\n",
      "loss: 0.396295  [110400/175341]\n",
      "loss: 0.315383  [112000/175341]\n",
      "loss: 0.325303  [113600/175341]\n",
      "loss: 0.322160  [115200/175341]\n",
      "loss: 0.100377  [116800/175341]\n",
      "loss: 0.389265  [118400/175341]\n",
      "loss: 0.331149  [120000/175341]\n",
      "loss: 0.674717  [121600/175341]\n",
      "loss: 0.201106  [123200/175341]\n",
      "loss: 0.328943  [124800/175341]\n",
      "loss: 0.205351  [126400/175341]\n",
      "loss: 0.688938  [128000/175341]\n",
      "loss: 0.587972  [129600/175341]\n",
      "loss: 0.496908  [131200/175341]\n",
      "loss: 0.428380  [132800/175341]\n",
      "loss: 0.826994  [134400/175341]\n",
      "loss: 0.498849  [136000/175341]\n",
      "loss: 0.462845  [137600/175341]\n",
      "loss: 0.964946  [139200/175341]\n",
      "loss: 0.580407  [140800/175341]\n",
      "loss: 0.836257  [142400/175341]\n",
      "loss: 0.451221  [144000/175341]\n",
      "loss: 0.196275  [145600/175341]\n",
      "loss: 0.338335  [147200/175341]\n",
      "loss: 0.554393  [148800/175341]\n",
      "loss: 0.498756  [150400/175341]\n",
      "loss: 0.287933  [152000/175341]\n",
      "loss: 0.603013  [153600/175341]\n",
      "loss: 0.451584  [155200/175341]\n",
      "loss: 0.590818  [156800/175341]\n",
      "loss: 0.225159  [158400/175341]\n",
      "loss: 0.505027  [160000/175341]\n",
      "loss: 0.236193  [161600/175341]\n",
      "loss: 0.659669  [163200/175341]\n",
      "loss: 0.157790  [164800/175341]\n",
      "loss: 0.527632  [166400/175341]\n",
      "loss: 0.405633  [168000/175341]\n",
      "loss: 0.207712  [169600/175341]\n",
      "loss: 0.351982  [171200/175341]\n",
      "loss: 0.500200  [172800/175341]\n",
      "loss: 0.343679  [174400/175341]\n",
      "Train Accuracy: 81.9609%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.544675, F1-score: 76.88%, Macro_F1-Score:  41.83%  \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.730314  [    0/175341]\n",
      "loss: 0.505399  [ 1600/175341]\n",
      "loss: 0.541052  [ 3200/175341]\n",
      "loss: 0.567670  [ 4800/175341]\n",
      "loss: 0.579574  [ 6400/175341]\n",
      "loss: 0.223342  [ 8000/175341]\n",
      "loss: 0.634285  [ 9600/175341]\n",
      "loss: 0.438115  [11200/175341]\n",
      "loss: 0.477276  [12800/175341]\n",
      "loss: 0.593835  [14400/175341]\n",
      "loss: 0.434688  [16000/175341]\n",
      "loss: 0.476791  [17600/175341]\n",
      "loss: 0.427866  [19200/175341]\n",
      "loss: 0.435647  [20800/175341]\n",
      "loss: 0.566306  [22400/175341]\n",
      "loss: 0.381892  [24000/175341]\n",
      "loss: 0.248869  [25600/175341]\n",
      "loss: 0.411286  [27200/175341]\n",
      "loss: 0.216279  [28800/175341]\n",
      "loss: 0.667384  [30400/175341]\n",
      "loss: 0.285696  [32000/175341]\n",
      "loss: 0.633727  [33600/175341]\n",
      "loss: 0.524286  [35200/175341]\n",
      "loss: 0.323538  [36800/175341]\n",
      "loss: 0.349042  [38400/175341]\n",
      "loss: 0.362688  [40000/175341]\n",
      "loss: 0.268103  [41600/175341]\n",
      "loss: 0.577958  [43200/175341]\n",
      "loss: 0.732204  [44800/175341]\n",
      "loss: 0.350026  [46400/175341]\n",
      "loss: 0.680249  [48000/175341]\n",
      "loss: 0.387698  [49600/175341]\n",
      "loss: 0.238569  [51200/175341]\n",
      "loss: 0.224456  [52800/175341]\n",
      "loss: 0.407586  [54400/175341]\n",
      "loss: 0.408295  [56000/175341]\n",
      "loss: 0.199000  [57600/175341]\n",
      "loss: 0.459822  [59200/175341]\n",
      "loss: 0.168533  [60800/175341]\n",
      "loss: 0.334348  [62400/175341]\n",
      "loss: 0.379075  [64000/175341]\n",
      "loss: 0.292324  [65600/175341]\n",
      "loss: 0.562390  [67200/175341]\n",
      "loss: 0.957494  [68800/175341]\n",
      "loss: 0.233379  [70400/175341]\n",
      "loss: 0.342826  [72000/175341]\n",
      "loss: 0.477838  [73600/175341]\n",
      "loss: 0.468711  [75200/175341]\n",
      "loss: 0.515040  [76800/175341]\n",
      "loss: 0.505283  [78400/175341]\n",
      "loss: 0.567944  [80000/175341]\n",
      "loss: 0.980153  [81600/175341]\n",
      "loss: 0.283319  [83200/175341]\n",
      "loss: 0.176226  [84800/175341]\n",
      "loss: 0.280692  [86400/175341]\n",
      "loss: 0.350353  [88000/175341]\n",
      "loss: 0.637908  [89600/175341]\n",
      "loss: 0.174384  [91200/175341]\n",
      "loss: 0.415424  [92800/175341]\n",
      "loss: 0.117084  [94400/175341]\n",
      "loss: 0.271231  [96000/175341]\n",
      "loss: 0.505625  [97600/175341]\n",
      "loss: 0.769086  [99200/175341]\n",
      "loss: 0.190839  [100800/175341]\n",
      "loss: 0.925987  [102400/175341]\n",
      "loss: 0.577431  [104000/175341]\n",
      "loss: 0.230407  [105600/175341]\n",
      "loss: 0.546783  [107200/175341]\n",
      "loss: 0.252240  [108800/175341]\n",
      "loss: 0.713336  [110400/175341]\n",
      "loss: 0.678051  [112000/175341]\n",
      "loss: 0.207655  [113600/175341]\n",
      "loss: 0.540388  [115200/175341]\n",
      "loss: 0.257026  [116800/175341]\n",
      "loss: 0.489693  [118400/175341]\n",
      "loss: 0.975783  [120000/175341]\n",
      "loss: 0.456973  [121600/175341]\n",
      "loss: 0.504902  [123200/175341]\n",
      "loss: 0.663465  [124800/175341]\n",
      "loss: 0.443389  [126400/175341]\n",
      "loss: 0.607025  [128000/175341]\n",
      "loss: 0.396861  [129600/175341]\n",
      "loss: 0.304548  [131200/175341]\n",
      "loss: 0.265405  [132800/175341]\n",
      "loss: 0.365958  [134400/175341]\n",
      "loss: 0.354519  [136000/175341]\n",
      "loss: 0.346659  [137600/175341]\n",
      "loss: 0.239556  [139200/175341]\n",
      "loss: 0.298307  [140800/175341]\n",
      "loss: 0.677678  [142400/175341]\n",
      "loss: 0.251942  [144000/175341]\n",
      "loss: 0.528888  [145600/175341]\n",
      "loss: 0.402172  [147200/175341]\n",
      "loss: 0.323645  [148800/175341]\n",
      "loss: 0.312975  [150400/175341]\n",
      "loss: 0.270918  [152000/175341]\n",
      "loss: 0.519277  [153600/175341]\n",
      "loss: 0.471819  [155200/175341]\n",
      "loss: 0.421078  [156800/175341]\n",
      "loss: 0.565486  [158400/175341]\n",
      "loss: 0.609397  [160000/175341]\n",
      "loss: 0.305655  [161600/175341]\n",
      "loss: 0.869330  [163200/175341]\n",
      "loss: 0.566970  [164800/175341]\n",
      "loss: 0.987675  [166400/175341]\n",
      "loss: 0.340189  [168000/175341]\n",
      "loss: 0.481869  [169600/175341]\n",
      "loss: 0.567297  [171200/175341]\n",
      "loss: 0.291460  [172800/175341]\n",
      "loss: 0.267623  [174400/175341]\n",
      "Train Accuracy: 81.9306%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.549360, F1-score: 76.58%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.596778  [    0/175341]\n",
      "loss: 0.261722  [ 1600/175341]\n",
      "loss: 0.449264  [ 3200/175341]\n",
      "loss: 0.308767  [ 4800/175341]\n",
      "loss: 0.511069  [ 6400/175341]\n",
      "loss: 0.310010  [ 8000/175341]\n",
      "loss: 0.720627  [ 9600/175341]\n",
      "loss: 0.514863  [11200/175341]\n",
      "loss: 0.449694  [12800/175341]\n",
      "loss: 0.234199  [14400/175341]\n",
      "loss: 0.387501  [16000/175341]\n",
      "loss: 0.338823  [17600/175341]\n",
      "loss: 0.471537  [19200/175341]\n",
      "loss: 0.463864  [20800/175341]\n",
      "loss: 0.372195  [22400/175341]\n",
      "loss: 0.285058  [24000/175341]\n",
      "loss: 0.757507  [25600/175341]\n",
      "loss: 0.379306  [27200/175341]\n",
      "loss: 0.288812  [28800/175341]\n",
      "loss: 0.643546  [30400/175341]\n",
      "loss: 0.232863  [32000/175341]\n",
      "loss: 0.648927  [33600/175341]\n",
      "loss: 0.587307  [35200/175341]\n",
      "loss: 0.336269  [36800/175341]\n",
      "loss: 0.665829  [38400/175341]\n",
      "loss: 0.529382  [40000/175341]\n",
      "loss: 0.212746  [41600/175341]\n",
      "loss: 0.553957  [43200/175341]\n",
      "loss: 0.230443  [44800/175341]\n",
      "loss: 0.452940  [46400/175341]\n",
      "loss: 0.170949  [48000/175341]\n",
      "loss: 0.181443  [49600/175341]\n",
      "loss: 0.437840  [51200/175341]\n",
      "loss: 0.321023  [52800/175341]\n",
      "loss: 0.295236  [54400/175341]\n",
      "loss: 0.585137  [56000/175341]\n",
      "loss: 0.610222  [57600/175341]\n",
      "loss: 0.495803  [59200/175341]\n",
      "loss: 0.054345  [60800/175341]\n",
      "loss: 0.694997  [62400/175341]\n",
      "loss: 0.223023  [64000/175341]\n",
      "loss: 0.578453  [65600/175341]\n",
      "loss: 1.035121  [67200/175341]\n",
      "loss: 0.569468  [68800/175341]\n",
      "loss: 0.363712  [70400/175341]\n",
      "loss: 0.173381  [72000/175341]\n",
      "loss: 0.767451  [73600/175341]\n",
      "loss: 0.754247  [75200/175341]\n",
      "loss: 0.551991  [76800/175341]\n",
      "loss: 0.355235  [78400/175341]\n",
      "loss: 0.619029  [80000/175341]\n",
      "loss: 0.243866  [81600/175341]\n",
      "loss: 0.711901  [83200/175341]\n",
      "loss: 0.495646  [84800/175341]\n",
      "loss: 0.406271  [86400/175341]\n",
      "loss: 0.237632  [88000/175341]\n",
      "loss: 0.257094  [89600/175341]\n",
      "loss: 0.600029  [91200/175341]\n",
      "loss: 0.105686  [92800/175341]\n",
      "loss: 0.612749  [94400/175341]\n",
      "loss: 0.518900  [96000/175341]\n",
      "loss: 0.322848  [97600/175341]\n",
      "loss: 0.469206  [99200/175341]\n",
      "loss: 0.499073  [100800/175341]\n",
      "loss: 0.474540  [102400/175341]\n",
      "loss: 0.143440  [104000/175341]\n",
      "loss: 0.414769  [105600/175341]\n",
      "loss: 0.657957  [107200/175341]\n",
      "loss: 0.059680  [108800/175341]\n",
      "loss: 0.442976  [110400/175341]\n",
      "loss: 0.690175  [112000/175341]\n",
      "loss: 0.510679  [113600/175341]\n",
      "loss: 0.549602  [115200/175341]\n",
      "loss: 0.207700  [116800/175341]\n",
      "loss: 0.333668  [118400/175341]\n",
      "loss: 0.288128  [120000/175341]\n",
      "loss: 0.579336  [121600/175341]\n",
      "loss: 0.473784  [123200/175341]\n",
      "loss: 0.402616  [124800/175341]\n",
      "loss: 0.342005  [126400/175341]\n",
      "loss: 0.527438  [128000/175341]\n",
      "loss: 0.567551  [129600/175341]\n",
      "loss: 0.605385  [131200/175341]\n",
      "loss: 0.345381  [132800/175341]\n",
      "loss: 0.172210  [134400/175341]\n",
      "loss: 0.197258  [136000/175341]\n",
      "loss: 0.530066  [137600/175341]\n",
      "loss: 0.203074  [139200/175341]\n",
      "loss: 0.628717  [140800/175341]\n",
      "loss: 0.600534  [142400/175341]\n",
      "loss: 0.680223  [144000/175341]\n",
      "loss: 0.437431  [145600/175341]\n",
      "loss: 0.544042  [147200/175341]\n",
      "loss: 0.000133  [148800/175341]\n",
      "loss: 0.684018  [150400/175341]\n",
      "loss: 0.578978  [152000/175341]\n",
      "loss: 0.234918  [153600/175341]\n",
      "loss: 0.065808  [155200/175341]\n",
      "loss: 0.456983  [156800/175341]\n",
      "loss: 0.407575  [158400/175341]\n",
      "loss: 0.562235  [160000/175341]\n",
      "loss: 0.089626  [161600/175341]\n",
      "loss: 0.647544  [163200/175341]\n",
      "loss: 0.591488  [164800/175341]\n",
      "loss: 0.416299  [166400/175341]\n",
      "loss: 0.267799  [168000/175341]\n",
      "loss: 0.370332  [169600/175341]\n",
      "loss: 0.210074  [171200/175341]\n",
      "loss: 0.293550  [172800/175341]\n",
      "loss: 0.441036  [174400/175341]\n",
      "Train Accuracy: 81.9438%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.555365, F1-score: 76.78%, Macro_F1-Score:  41.78%  \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.524547  [    0/175341]\n",
      "loss: 0.498346  [ 1600/175341]\n",
      "loss: 0.389207  [ 3200/175341]\n",
      "loss: 0.670615  [ 4800/175341]\n",
      "loss: 0.484290  [ 6400/175341]\n",
      "loss: 0.491908  [ 8000/175341]\n",
      "loss: 0.320895  [ 9600/175341]\n",
      "loss: 0.755356  [11200/175341]\n",
      "loss: 0.310216  [12800/175341]\n",
      "loss: 0.471716  [14400/175341]\n",
      "loss: 0.642742  [16000/175341]\n",
      "loss: 0.407045  [17600/175341]\n",
      "loss: 0.529781  [19200/175341]\n",
      "loss: 0.480764  [20800/175341]\n",
      "loss: 0.250124  [22400/175341]\n",
      "loss: 0.415793  [24000/175341]\n",
      "loss: 0.411990  [25600/175341]\n",
      "loss: 0.206210  [27200/175341]\n",
      "loss: 0.412019  [28800/175341]\n",
      "loss: 1.008090  [30400/175341]\n",
      "loss: 0.524173  [32000/175341]\n",
      "loss: 0.357852  [33600/175341]\n",
      "loss: 0.359227  [35200/175341]\n",
      "loss: 0.344143  [36800/175341]\n",
      "loss: 0.299750  [38400/175341]\n",
      "loss: 0.218066  [40000/175341]\n",
      "loss: 0.204737  [41600/175341]\n",
      "loss: 0.530487  [43200/175341]\n",
      "loss: 0.530966  [44800/175341]\n",
      "loss: 0.829891  [46400/175341]\n",
      "loss: 0.533074  [48000/175341]\n",
      "loss: 0.149404  [49600/175341]\n",
      "loss: 0.300672  [51200/175341]\n",
      "loss: 0.654073  [52800/175341]\n",
      "loss: 0.494684  [54400/175341]\n",
      "loss: 0.448837  [56000/175341]\n",
      "loss: 0.485844  [57600/175341]\n",
      "loss: 0.362913  [59200/175341]\n",
      "loss: 0.454691  [60800/175341]\n",
      "loss: 0.304955  [62400/175341]\n",
      "loss: 0.964244  [64000/175341]\n",
      "loss: 0.305146  [65600/175341]\n",
      "loss: 0.647336  [67200/175341]\n",
      "loss: 0.384953  [68800/175341]\n",
      "loss: 0.410403  [70400/175341]\n",
      "loss: 0.755672  [72000/175341]\n",
      "loss: 0.312509  [73600/175341]\n",
      "loss: 0.136277  [75200/175341]\n",
      "loss: 0.432127  [76800/175341]\n",
      "loss: 0.558226  [78400/175341]\n",
      "loss: 0.334569  [80000/175341]\n",
      "loss: 0.451303  [81600/175341]\n",
      "loss: 0.341386  [83200/175341]\n",
      "loss: 0.293540  [84800/175341]\n",
      "loss: 0.430818  [86400/175341]\n",
      "loss: 0.660052  [88000/175341]\n",
      "loss: 0.225151  [89600/175341]\n",
      "loss: 0.902611  [91200/175341]\n",
      "loss: 0.660990  [92800/175341]\n",
      "loss: 0.644779  [94400/175341]\n",
      "loss: 0.288255  [96000/175341]\n",
      "loss: 0.733216  [97600/175341]\n",
      "loss: 0.443719  [99200/175341]\n",
      "loss: 0.545128  [100800/175341]\n",
      "loss: 0.412599  [102400/175341]\n",
      "loss: 0.635879  [104000/175341]\n",
      "loss: 0.664888  [105600/175341]\n",
      "loss: 0.074366  [107200/175341]\n",
      "loss: 0.231103  [108800/175341]\n",
      "loss: 0.301085  [110400/175341]\n",
      "loss: 0.963589  [112000/175341]\n",
      "loss: 0.455709  [113600/175341]\n",
      "loss: 0.663181  [115200/175341]\n",
      "loss: 0.191282  [116800/175341]\n",
      "loss: 0.256642  [118400/175341]\n",
      "loss: 0.297007  [120000/175341]\n",
      "loss: 0.815335  [121600/175341]\n",
      "loss: 0.513949  [123200/175341]\n",
      "loss: 0.367989  [124800/175341]\n",
      "loss: 0.510697  [126400/175341]\n",
      "loss: 0.293517  [128000/175341]\n",
      "loss: 0.341252  [129600/175341]\n",
      "loss: 0.182242  [131200/175341]\n",
      "loss: 0.305872  [132800/175341]\n",
      "loss: 0.639557  [134400/175341]\n",
      "loss: 0.284143  [136000/175341]\n",
      "loss: 0.445934  [137600/175341]\n",
      "loss: 0.104836  [139200/175341]\n",
      "loss: 0.508834  [140800/175341]\n",
      "loss: 0.300565  [142400/175341]\n",
      "loss: 0.811269  [144000/175341]\n",
      "loss: 0.509990  [145600/175341]\n",
      "loss: 0.587610  [147200/175341]\n",
      "loss: 0.605870  [148800/175341]\n",
      "loss: 0.449923  [150400/175341]\n",
      "loss: 0.316861  [152000/175341]\n",
      "loss: 0.785674  [153600/175341]\n",
      "loss: 0.364492  [155200/175341]\n",
      "loss: 0.550463  [156800/175341]\n",
      "loss: 0.323543  [158400/175341]\n",
      "loss: 0.432520  [160000/175341]\n",
      "loss: 0.733028  [161600/175341]\n",
      "loss: 0.325041  [163200/175341]\n",
      "loss: 0.259044  [164800/175341]\n",
      "loss: 0.483627  [166400/175341]\n",
      "loss: 0.092717  [168000/175341]\n",
      "loss: 0.579645  [169600/175341]\n",
      "loss: 0.788633  [171200/175341]\n",
      "loss: 0.211649  [172800/175341]\n",
      "loss: 0.178794  [174400/175341]\n",
      "Train Accuracy: 81.9900%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.558593, F1-score: 76.44%, Macro_F1-Score:  41.61%  \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.495325  [    0/175341]\n",
      "loss: 0.172288  [ 1600/175341]\n",
      "loss: 0.315505  [ 3200/175341]\n",
      "loss: 0.524393  [ 4800/175341]\n",
      "loss: 0.768923  [ 6400/175341]\n",
      "loss: 0.920967  [ 8000/175341]\n",
      "loss: 0.693760  [ 9600/175341]\n",
      "loss: 0.630347  [11200/175341]\n",
      "loss: 0.576040  [12800/175341]\n",
      "loss: 0.356889  [14400/175341]\n",
      "loss: 0.522834  [16000/175341]\n",
      "loss: 0.224435  [17600/175341]\n",
      "loss: 0.781203  [19200/175341]\n",
      "loss: 0.545116  [20800/175341]\n",
      "loss: 0.026356  [22400/175341]\n",
      "loss: 0.471072  [24000/175341]\n",
      "loss: 0.196202  [25600/175341]\n",
      "loss: 0.419829  [27200/175341]\n",
      "loss: 0.534491  [28800/175341]\n",
      "loss: 0.348741  [30400/175341]\n",
      "loss: 0.650863  [32000/175341]\n",
      "loss: 0.485783  [33600/175341]\n",
      "loss: 0.508404  [35200/175341]\n",
      "loss: 0.405931  [36800/175341]\n",
      "loss: 0.512404  [38400/175341]\n",
      "loss: 0.382442  [40000/175341]\n",
      "loss: 0.373829  [41600/175341]\n",
      "loss: 0.230140  [43200/175341]\n",
      "loss: 0.508793  [44800/175341]\n",
      "loss: 0.346749  [46400/175341]\n",
      "loss: 0.426317  [48000/175341]\n",
      "loss: 0.616524  [49600/175341]\n",
      "loss: 0.308712  [51200/175341]\n",
      "loss: 0.299686  [52800/175341]\n",
      "loss: 0.348156  [54400/175341]\n",
      "loss: 0.295084  [56000/175341]\n",
      "loss: 0.508163  [57600/175341]\n",
      "loss: 0.483937  [59200/175341]\n",
      "loss: 0.153939  [60800/175341]\n",
      "loss: 0.110207  [62400/175341]\n",
      "loss: 0.335376  [64000/175341]\n",
      "loss: 0.522296  [65600/175341]\n",
      "loss: 0.641913  [67200/175341]\n",
      "loss: 0.263372  [68800/175341]\n",
      "loss: 0.547230  [70400/175341]\n",
      "loss: 0.714826  [72000/175341]\n",
      "loss: 0.692536  [73600/175341]\n",
      "loss: 0.372759  [75200/175341]\n",
      "loss: 0.535933  [76800/175341]\n",
      "loss: 0.380472  [78400/175341]\n",
      "loss: 0.569917  [80000/175341]\n",
      "loss: 0.348780  [81600/175341]\n",
      "loss: 0.195294  [83200/175341]\n",
      "loss: 0.245444  [84800/175341]\n",
      "loss: 0.084353  [86400/175341]\n",
      "loss: 0.433705  [88000/175341]\n",
      "loss: 0.253085  [89600/175341]\n",
      "loss: 0.089256  [91200/175341]\n",
      "loss: 0.406022  [92800/175341]\n",
      "loss: 0.419842  [94400/175341]\n",
      "loss: 0.565850  [96000/175341]\n",
      "loss: 0.316497  [97600/175341]\n",
      "loss: 0.612016  [99200/175341]\n",
      "loss: 0.114982  [100800/175341]\n",
      "loss: 0.246025  [102400/175341]\n",
      "loss: 0.618871  [104000/175341]\n",
      "loss: 0.699524  [105600/175341]\n",
      "loss: 0.524900  [107200/175341]\n",
      "loss: 0.271760  [108800/175341]\n",
      "loss: 0.406345  [110400/175341]\n",
      "loss: 0.637450  [112000/175341]\n",
      "loss: 0.325738  [113600/175341]\n",
      "loss: 0.361632  [115200/175341]\n",
      "loss: 0.269692  [116800/175341]\n",
      "loss: 0.523931  [118400/175341]\n",
      "loss: 0.139558  [120000/175341]\n",
      "loss: 0.601302  [121600/175341]\n",
      "loss: 0.470737  [123200/175341]\n",
      "loss: 0.541243  [124800/175341]\n",
      "loss: 0.426778  [126400/175341]\n",
      "loss: 0.165142  [128000/175341]\n",
      "loss: 0.377880  [129600/175341]\n",
      "loss: 0.242922  [131200/175341]\n",
      "loss: 0.622225  [132800/175341]\n",
      "loss: 0.604574  [134400/175341]\n",
      "loss: 0.420623  [136000/175341]\n",
      "loss: 0.326290  [137600/175341]\n",
      "loss: 0.440505  [139200/175341]\n",
      "loss: 0.216060  [140800/175341]\n",
      "loss: 0.289628  [142400/175341]\n",
      "loss: 0.738692  [144000/175341]\n",
      "loss: 0.622258  [145600/175341]\n",
      "loss: 1.006347  [147200/175341]\n",
      "loss: 0.439214  [148800/175341]\n",
      "loss: 0.776150  [150400/175341]\n",
      "loss: 0.300194  [152000/175341]\n",
      "loss: 0.486109  [153600/175341]\n",
      "loss: 0.795407  [155200/175341]\n",
      "loss: 0.463063  [156800/175341]\n",
      "loss: 0.544195  [158400/175341]\n",
      "loss: 0.392824  [160000/175341]\n",
      "loss: 0.701083  [161600/175341]\n",
      "loss: 0.250149  [163200/175341]\n",
      "loss: 0.508607  [164800/175341]\n",
      "loss: 0.256134  [166400/175341]\n",
      "loss: 0.445260  [168000/175341]\n",
      "loss: 0.570892  [169600/175341]\n",
      "loss: 0.332094  [171200/175341]\n",
      "loss: 0.554435  [172800/175341]\n",
      "loss: 0.666105  [174400/175341]\n",
      "Train Accuracy: 81.9785%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.550238, F1-score: 76.75%, Macro_F1-Score:  42.00%  \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.484427  [    0/175341]\n",
      "loss: 0.272994  [ 1600/175341]\n",
      "loss: 1.064975  [ 3200/175341]\n",
      "loss: 0.370567  [ 4800/175341]\n",
      "loss: 0.415761  [ 6400/175341]\n",
      "loss: 0.489146  [ 8000/175341]\n",
      "loss: 0.240669  [ 9600/175341]\n",
      "loss: 0.852194  [11200/175341]\n",
      "loss: 0.295941  [12800/175341]\n",
      "loss: 0.258834  [14400/175341]\n",
      "loss: 0.720374  [16000/175341]\n",
      "loss: 0.515753  [17600/175341]\n",
      "loss: 0.367775  [19200/175341]\n",
      "loss: 0.574998  [20800/175341]\n",
      "loss: 0.799863  [22400/175341]\n",
      "loss: 0.587870  [24000/175341]\n",
      "loss: 0.482746  [25600/175341]\n",
      "loss: 0.203898  [27200/175341]\n",
      "loss: 0.454691  [28800/175341]\n",
      "loss: 0.351573  [30400/175341]\n",
      "loss: 0.678799  [32000/175341]\n",
      "loss: 0.736369  [33600/175341]\n",
      "loss: 0.332557  [35200/175341]\n",
      "loss: 0.288724  [36800/175341]\n",
      "loss: 0.406940  [38400/175341]\n",
      "loss: 0.162181  [40000/175341]\n",
      "loss: 0.362724  [41600/175341]\n",
      "loss: 0.224558  [43200/175341]\n",
      "loss: 0.214381  [44800/175341]\n",
      "loss: 0.393618  [46400/175341]\n",
      "loss: 0.411917  [48000/175341]\n",
      "loss: 0.752880  [49600/175341]\n",
      "loss: 0.269962  [51200/175341]\n",
      "loss: 0.357415  [52800/175341]\n",
      "loss: 0.293803  [54400/175341]\n",
      "loss: 0.406897  [56000/175341]\n",
      "loss: 0.657155  [57600/175341]\n",
      "loss: 0.172664  [59200/175341]\n",
      "loss: 0.447916  [60800/175341]\n",
      "loss: 0.768512  [62400/175341]\n",
      "loss: 0.712753  [64000/175341]\n",
      "loss: 0.804194  [65600/175341]\n",
      "loss: 0.526393  [67200/175341]\n",
      "loss: 0.658593  [68800/175341]\n",
      "loss: 0.400342  [70400/175341]\n",
      "loss: 0.215871  [72000/175341]\n",
      "loss: 0.473619  [73600/175341]\n",
      "loss: 0.412935  [75200/175341]\n",
      "loss: 0.425053  [76800/175341]\n",
      "loss: 0.155124  [78400/175341]\n",
      "loss: 0.279051  [80000/175341]\n",
      "loss: 0.389661  [81600/175341]\n",
      "loss: 0.705275  [83200/175341]\n",
      "loss: 0.722852  [84800/175341]\n",
      "loss: 0.388575  [86400/175341]\n",
      "loss: 0.617663  [88000/175341]\n",
      "loss: 0.328708  [89600/175341]\n",
      "loss: 0.409135  [91200/175341]\n",
      "loss: 0.654971  [92800/175341]\n",
      "loss: 0.070835  [94400/175341]\n",
      "loss: 0.265229  [96000/175341]\n",
      "loss: 0.409354  [97600/175341]\n",
      "loss: 0.581924  [99200/175341]\n",
      "loss: 0.188981  [100800/175341]\n",
      "loss: 0.489062  [102400/175341]\n",
      "loss: 0.900547  [104000/175341]\n",
      "loss: 0.270094  [105600/175341]\n",
      "loss: 0.447451  [107200/175341]\n",
      "loss: 0.364878  [108800/175341]\n",
      "loss: 0.213730  [110400/175341]\n",
      "loss: 0.205470  [112000/175341]\n",
      "loss: 0.398564  [113600/175341]\n",
      "loss: 0.528470  [115200/175341]\n",
      "loss: 0.510344  [116800/175341]\n",
      "loss: 0.610771  [118400/175341]\n",
      "loss: 0.591312  [120000/175341]\n",
      "loss: 0.159848  [121600/175341]\n",
      "loss: 0.621781  [123200/175341]\n",
      "loss: 0.476320  [124800/175341]\n",
      "loss: 0.372278  [126400/175341]\n",
      "loss: 0.234787  [128000/175341]\n",
      "loss: 0.633868  [129600/175341]\n",
      "loss: 0.433314  [131200/175341]\n",
      "loss: 0.225354  [132800/175341]\n",
      "loss: 0.304810  [134400/175341]\n",
      "loss: 0.964881  [136000/175341]\n",
      "loss: 0.420309  [137600/175341]\n",
      "loss: 0.243753  [139200/175341]\n",
      "loss: 0.295347  [140800/175341]\n",
      "loss: 0.746645  [142400/175341]\n",
      "loss: 0.461587  [144000/175341]\n",
      "loss: 0.196273  [145600/175341]\n",
      "loss: 0.513234  [147200/175341]\n",
      "loss: 0.340747  [148800/175341]\n",
      "loss: 0.266434  [150400/175341]\n",
      "loss: 0.368266  [152000/175341]\n",
      "loss: 0.397271  [153600/175341]\n",
      "loss: 0.521778  [155200/175341]\n",
      "loss: 0.714821  [156800/175341]\n",
      "loss: 0.655748  [158400/175341]\n",
      "loss: 0.552412  [160000/175341]\n",
      "loss: 0.365346  [161600/175341]\n",
      "loss: 0.528273  [163200/175341]\n",
      "loss: 0.554023  [164800/175341]\n",
      "loss: 0.224188  [166400/175341]\n",
      "loss: 0.680972  [168000/175341]\n",
      "loss: 0.326888  [169600/175341]\n",
      "loss: 0.604095  [171200/175341]\n",
      "loss: 0.454971  [172800/175341]\n",
      "loss: 1.032097  [174400/175341]\n",
      "Train Accuracy: 82.0253%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.574881, F1-score: 75.57%, Macro_F1-Score:  41.49%  \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.701365  [    0/175341]\n",
      "loss: 0.351784  [ 1600/175341]\n",
      "loss: 0.381498  [ 3200/175341]\n",
      "loss: 0.352425  [ 4800/175341]\n",
      "loss: 0.538958  [ 6400/175341]\n",
      "loss: 0.745564  [ 8000/175341]\n",
      "loss: 0.497026  [ 9600/175341]\n",
      "loss: 0.833334  [11200/175341]\n",
      "loss: 0.421074  [12800/175341]\n",
      "loss: 0.084161  [14400/175341]\n",
      "loss: 0.694183  [16000/175341]\n",
      "loss: 0.325703  [17600/175341]\n",
      "loss: 0.539543  [19200/175341]\n",
      "loss: 0.388864  [20800/175341]\n",
      "loss: 0.882336  [22400/175341]\n",
      "loss: 0.904922  [24000/175341]\n",
      "loss: 0.647248  [25600/175341]\n",
      "loss: 0.296213  [27200/175341]\n",
      "loss: 0.454865  [28800/175341]\n",
      "loss: 0.540563  [30400/175341]\n",
      "loss: 0.525684  [32000/175341]\n",
      "loss: 1.039379  [33600/175341]\n",
      "loss: 0.691714  [35200/175341]\n",
      "loss: 0.184839  [36800/175341]\n",
      "loss: 0.547738  [38400/175341]\n",
      "loss: 0.178751  [40000/175341]\n",
      "loss: 0.526749  [41600/175341]\n",
      "loss: 0.543861  [43200/175341]\n",
      "loss: 0.743925  [44800/175341]\n",
      "loss: 0.376465  [46400/175341]\n",
      "loss: 0.740436  [48000/175341]\n",
      "loss: 0.701451  [49600/175341]\n",
      "loss: 0.351911  [51200/175341]\n",
      "loss: 0.476918  [52800/175341]\n",
      "loss: 0.524206  [54400/175341]\n",
      "loss: 0.334112  [56000/175341]\n",
      "loss: 0.522729  [57600/175341]\n",
      "loss: 0.507342  [59200/175341]\n",
      "loss: 0.231205  [60800/175341]\n",
      "loss: 0.274730  [62400/175341]\n",
      "loss: 0.400778  [64000/175341]\n",
      "loss: 0.408862  [65600/175341]\n",
      "loss: 0.538980  [67200/175341]\n",
      "loss: 0.512505  [68800/175341]\n",
      "loss: 0.250505  [70400/175341]\n",
      "loss: 0.385619  [72000/175341]\n",
      "loss: 0.653628  [73600/175341]\n",
      "loss: 0.140693  [75200/175341]\n",
      "loss: 0.916033  [76800/175341]\n",
      "loss: 0.661956  [78400/175341]\n",
      "loss: 0.859422  [80000/175341]\n",
      "loss: 0.400247  [81600/175341]\n",
      "loss: 0.511221  [83200/175341]\n",
      "loss: 0.651566  [84800/175341]\n",
      "loss: 0.508248  [86400/175341]\n",
      "loss: 0.742545  [88000/175341]\n",
      "loss: 0.729890  [89600/175341]\n",
      "loss: 0.632365  [91200/175341]\n",
      "loss: 0.347855  [92800/175341]\n",
      "loss: 0.355313  [94400/175341]\n",
      "loss: 0.591057  [96000/175341]\n",
      "loss: 0.434079  [97600/175341]\n",
      "loss: 0.731541  [99200/175341]\n",
      "loss: 0.284760  [100800/175341]\n",
      "loss: 0.707536  [102400/175341]\n",
      "loss: 0.431883  [104000/175341]\n",
      "loss: 0.395663  [105600/175341]\n",
      "loss: 0.476364  [107200/175341]\n",
      "loss: 0.573672  [108800/175341]\n",
      "loss: 0.815022  [110400/175341]\n",
      "loss: 0.284067  [112000/175341]\n",
      "loss: 0.329849  [113600/175341]\n",
      "loss: 0.337578  [115200/175341]\n",
      "loss: 0.794348  [116800/175341]\n",
      "loss: 0.582967  [118400/175341]\n",
      "loss: 0.360378  [120000/175341]\n",
      "loss: 0.586719  [121600/175341]\n",
      "loss: 0.414610  [123200/175341]\n",
      "loss: 0.732222  [124800/175341]\n",
      "loss: 0.441500  [126400/175341]\n",
      "loss: 0.788711  [128000/175341]\n",
      "loss: 0.195156  [129600/175341]\n",
      "loss: 0.792906  [131200/175341]\n",
      "loss: 0.512955  [132800/175341]\n",
      "loss: 0.119532  [134400/175341]\n",
      "loss: 0.715642  [136000/175341]\n",
      "loss: 0.501376  [137600/175341]\n",
      "loss: 0.441295  [139200/175341]\n",
      "loss: 0.458352  [140800/175341]\n",
      "loss: 0.710989  [142400/175341]\n",
      "loss: 0.248898  [144000/175341]\n",
      "loss: 0.364452  [145600/175341]\n",
      "loss: 0.453242  [147200/175341]\n",
      "loss: 0.765835  [148800/175341]\n",
      "loss: 0.388379  [150400/175341]\n",
      "loss: 0.478544  [152000/175341]\n",
      "loss: 0.367196  [153600/175341]\n",
      "loss: 0.425681  [155200/175341]\n",
      "loss: 0.296709  [156800/175341]\n",
      "loss: 0.290279  [158400/175341]\n",
      "loss: 0.479582  [160000/175341]\n",
      "loss: 0.617073  [161600/175341]\n",
      "loss: 0.333279  [163200/175341]\n",
      "loss: 0.367548  [164800/175341]\n",
      "loss: 0.484828  [166400/175341]\n",
      "loss: 0.587143  [168000/175341]\n",
      "loss: 0.301316  [169600/175341]\n",
      "loss: 0.512658  [171200/175341]\n",
      "loss: 0.552490  [172800/175341]\n",
      "loss: 0.338106  [174400/175341]\n",
      "Train Accuracy: 81.9808%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.553554, F1-score: 76.15%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.221968  [    0/175341]\n",
      "loss: 1.207406  [ 1600/175341]\n",
      "loss: 0.335293  [ 3200/175341]\n",
      "loss: 0.474871  [ 4800/175341]\n",
      "loss: 0.270334  [ 6400/175341]\n",
      "loss: 0.431016  [ 8000/175341]\n",
      "loss: 0.315464  [ 9600/175341]\n",
      "loss: 0.614610  [11200/175341]\n",
      "loss: 0.631544  [12800/175341]\n",
      "loss: 0.383290  [14400/175341]\n",
      "loss: 0.381933  [16000/175341]\n",
      "loss: 0.358344  [17600/175341]\n",
      "loss: 0.364835  [19200/175341]\n",
      "loss: 0.462960  [20800/175341]\n",
      "loss: 0.122035  [22400/175341]\n",
      "loss: 0.444318  [24000/175341]\n",
      "loss: 0.327640  [25600/175341]\n",
      "loss: 0.665291  [27200/175341]\n",
      "loss: 0.401239  [28800/175341]\n",
      "loss: 0.633362  [30400/175341]\n",
      "loss: 0.699010  [32000/175341]\n",
      "loss: 0.528897  [33600/175341]\n",
      "loss: 0.550909  [35200/175341]\n",
      "loss: 0.342791  [36800/175341]\n",
      "loss: 0.539434  [38400/175341]\n",
      "loss: 0.312963  [40000/175341]\n",
      "loss: 0.729587  [41600/175341]\n",
      "loss: 0.459803  [43200/175341]\n",
      "loss: 0.328383  [44800/175341]\n",
      "loss: 0.375092  [46400/175341]\n",
      "loss: 0.820267  [48000/175341]\n",
      "loss: 0.207714  [49600/175341]\n",
      "loss: 0.574621  [51200/175341]\n",
      "loss: 0.341488  [52800/175341]\n",
      "loss: 0.861702  [54400/175341]\n",
      "loss: 0.498675  [56000/175341]\n",
      "loss: 0.585461  [57600/175341]\n",
      "loss: 0.125262  [59200/175341]\n",
      "loss: 0.255597  [60800/175341]\n",
      "loss: 0.816649  [62400/175341]\n",
      "loss: 0.766892  [64000/175341]\n",
      "loss: 0.197077  [65600/175341]\n",
      "loss: 0.675481  [67200/175341]\n",
      "loss: 0.543127  [68800/175341]\n",
      "loss: 0.500309  [70400/175341]\n",
      "loss: 0.238661  [72000/175341]\n",
      "loss: 0.258160  [73600/175341]\n",
      "loss: 0.400511  [75200/175341]\n",
      "loss: 0.849547  [76800/175341]\n",
      "loss: 0.309536  [78400/175341]\n",
      "loss: 0.391327  [80000/175341]\n",
      "loss: 0.277900  [81600/175341]\n",
      "loss: 0.421015  [83200/175341]\n",
      "loss: 0.534133  [84800/175341]\n",
      "loss: 0.222500  [86400/175341]\n",
      "loss: 0.394786  [88000/175341]\n",
      "loss: 0.598450  [89600/175341]\n",
      "loss: 0.453762  [91200/175341]\n",
      "loss: 0.192250  [92800/175341]\n",
      "loss: 0.324705  [94400/175341]\n",
      "loss: 0.324482  [96000/175341]\n",
      "loss: 0.476814  [97600/175341]\n",
      "loss: 0.492931  [99200/175341]\n",
      "loss: 0.529374  [100800/175341]\n",
      "loss: 0.302005  [102400/175341]\n",
      "loss: 0.265045  [104000/175341]\n",
      "loss: 0.557100  [105600/175341]\n",
      "loss: 0.307035  [107200/175341]\n",
      "loss: 0.374812  [108800/175341]\n",
      "loss: 0.666790  [110400/175341]\n",
      "loss: 0.855879  [112000/175341]\n",
      "loss: 0.315044  [113600/175341]\n",
      "loss: 0.547788  [115200/175341]\n",
      "loss: 0.687121  [116800/175341]\n",
      "loss: 0.260168  [118400/175341]\n",
      "loss: 0.563751  [120000/175341]\n",
      "loss: 0.231154  [121600/175341]\n",
      "loss: 0.363206  [123200/175341]\n",
      "loss: 0.649042  [124800/175341]\n",
      "loss: 0.283497  [126400/175341]\n",
      "loss: 0.583527  [128000/175341]\n",
      "loss: 0.257503  [129600/175341]\n",
      "loss: 0.244789  [131200/175341]\n",
      "loss: 0.470016  [132800/175341]\n",
      "loss: 0.439864  [134400/175341]\n",
      "loss: 0.569818  [136000/175341]\n",
      "loss: 0.425906  [137600/175341]\n",
      "loss: 0.788241  [139200/175341]\n",
      "loss: 0.786762  [140800/175341]\n",
      "loss: 0.194504  [142400/175341]\n",
      "loss: 0.606394  [144000/175341]\n",
      "loss: 0.236552  [145600/175341]\n",
      "loss: 0.645125  [147200/175341]\n",
      "loss: 0.323155  [148800/175341]\n",
      "loss: 0.534271  [150400/175341]\n",
      "loss: 0.301591  [152000/175341]\n",
      "loss: 0.446800  [153600/175341]\n",
      "loss: 0.229218  [155200/175341]\n",
      "loss: 0.605528  [156800/175341]\n",
      "loss: 0.984061  [158400/175341]\n",
      "loss: 0.580402  [160000/175341]\n",
      "loss: 0.496128  [161600/175341]\n",
      "loss: 0.329826  [163200/175341]\n",
      "loss: 0.291235  [164800/175341]\n",
      "loss: 0.407425  [166400/175341]\n",
      "loss: 0.213798  [168000/175341]\n",
      "loss: 0.310646  [169600/175341]\n",
      "loss: 0.949602  [171200/175341]\n",
      "loss: 0.139458  [172800/175341]\n",
      "loss: 0.393546  [174400/175341]\n",
      "Train Accuracy: 81.9472%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.542697, F1-score: 76.84%, Macro_F1-Score:  42.20%  \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.642349  [    0/175341]\n",
      "loss: 0.540468  [ 1600/175341]\n",
      "loss: 0.442480  [ 3200/175341]\n",
      "loss: 0.445350  [ 4800/175341]\n",
      "loss: 0.797128  [ 6400/175341]\n",
      "loss: 0.328810  [ 8000/175341]\n",
      "loss: 0.336557  [ 9600/175341]\n",
      "loss: 0.328539  [11200/175341]\n",
      "loss: 0.324843  [12800/175341]\n",
      "loss: 0.447404  [14400/175341]\n",
      "loss: 0.507389  [16000/175341]\n",
      "loss: 1.138459  [17600/175341]\n",
      "loss: 0.207337  [19200/175341]\n",
      "loss: 0.868395  [20800/175341]\n",
      "loss: 0.174547  [22400/175341]\n",
      "loss: 0.544226  [24000/175341]\n",
      "loss: 0.732777  [25600/175341]\n",
      "loss: 0.190705  [27200/175341]\n",
      "loss: 0.369449  [28800/175341]\n",
      "loss: 0.951319  [30400/175341]\n",
      "loss: 0.428625  [32000/175341]\n",
      "loss: 0.715634  [33600/175341]\n",
      "loss: 0.306039  [35200/175341]\n",
      "loss: 0.111978  [36800/175341]\n",
      "loss: 0.379704  [38400/175341]\n",
      "loss: 0.579157  [40000/175341]\n",
      "loss: 1.423392  [41600/175341]\n",
      "loss: 0.282649  [43200/175341]\n",
      "loss: 0.455025  [44800/175341]\n",
      "loss: 0.343957  [46400/175341]\n",
      "loss: 0.937775  [48000/175341]\n",
      "loss: 0.353567  [49600/175341]\n",
      "loss: 0.418546  [51200/175341]\n",
      "loss: 0.517143  [52800/175341]\n",
      "loss: 1.159023  [54400/175341]\n",
      "loss: 0.496924  [56000/175341]\n",
      "loss: 0.510321  [57600/175341]\n",
      "loss: 0.410663  [59200/175341]\n",
      "loss: 0.807771  [60800/175341]\n",
      "loss: 0.386795  [62400/175341]\n",
      "loss: 0.410399  [64000/175341]\n",
      "loss: 0.513889  [65600/175341]\n",
      "loss: 0.159916  [67200/175341]\n",
      "loss: 0.219253  [68800/175341]\n",
      "loss: 0.422659  [70400/175341]\n",
      "loss: 0.530862  [72000/175341]\n",
      "loss: 0.435540  [73600/175341]\n",
      "loss: 0.259299  [75200/175341]\n",
      "loss: 0.415507  [76800/175341]\n",
      "loss: 0.286567  [78400/175341]\n",
      "loss: 0.575122  [80000/175341]\n",
      "loss: 0.234275  [81600/175341]\n",
      "loss: 0.188947  [83200/175341]\n",
      "loss: 0.407588  [84800/175341]\n",
      "loss: 0.243142  [86400/175341]\n",
      "loss: 0.326829  [88000/175341]\n",
      "loss: 0.564825  [89600/175341]\n",
      "loss: 0.531202  [91200/175341]\n",
      "loss: 0.443597  [92800/175341]\n",
      "loss: 0.336395  [94400/175341]\n",
      "loss: 0.552336  [96000/175341]\n",
      "loss: 0.369381  [97600/175341]\n",
      "loss: 0.639414  [99200/175341]\n",
      "loss: 0.342002  [100800/175341]\n",
      "loss: 0.325068  [102400/175341]\n",
      "loss: 0.483917  [104000/175341]\n",
      "loss: 0.114333  [105600/175341]\n",
      "loss: 0.216289  [107200/175341]\n",
      "loss: 0.125780  [108800/175341]\n",
      "loss: 0.328761  [110400/175341]\n",
      "loss: 0.923533  [112000/175341]\n",
      "loss: 0.354385  [113600/175341]\n",
      "loss: 0.349460  [115200/175341]\n",
      "loss: 0.566824  [116800/175341]\n",
      "loss: 0.827011  [118400/175341]\n",
      "loss: 0.149158  [120000/175341]\n",
      "loss: 0.429773  [121600/175341]\n",
      "loss: 0.401284  [123200/175341]\n",
      "loss: 0.854504  [124800/175341]\n",
      "loss: 0.642631  [126400/175341]\n",
      "loss: 0.793162  [128000/175341]\n",
      "loss: 0.372554  [129600/175341]\n",
      "loss: 0.278290  [131200/175341]\n",
      "loss: 0.560816  [132800/175341]\n",
      "loss: 0.308460  [134400/175341]\n",
      "loss: 0.300595  [136000/175341]\n",
      "loss: 0.437394  [137600/175341]\n",
      "loss: 0.228493  [139200/175341]\n",
      "loss: 0.400194  [140800/175341]\n",
      "loss: 0.936694  [142400/175341]\n",
      "loss: 0.412183  [144000/175341]\n",
      "loss: 0.404714  [145600/175341]\n",
      "loss: 0.300900  [147200/175341]\n",
      "loss: 0.303090  [148800/175341]\n",
      "loss: 0.284466  [150400/175341]\n",
      "loss: 0.490676  [152000/175341]\n",
      "loss: 0.431577  [153600/175341]\n",
      "loss: 0.542380  [155200/175341]\n",
      "loss: 0.761551  [156800/175341]\n",
      "loss: 0.326274  [158400/175341]\n",
      "loss: 0.274241  [160000/175341]\n",
      "loss: 0.425892  [161600/175341]\n",
      "loss: 0.510235  [163200/175341]\n",
      "loss: 0.326079  [164800/175341]\n",
      "loss: 0.686666  [166400/175341]\n",
      "loss: 0.172593  [168000/175341]\n",
      "loss: 0.355559  [169600/175341]\n",
      "loss: 0.212478  [171200/175341]\n",
      "loss: 0.488225  [172800/175341]\n",
      "loss: 0.531257  [174400/175341]\n",
      "Train Accuracy: 82.0150%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.545090, F1-score: 77.53%, Macro_F1-Score:  42.39%  \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.438822  [    0/175341]\n",
      "loss: 0.369007  [ 1600/175341]\n",
      "loss: 0.559662  [ 3200/175341]\n",
      "loss: 0.506142  [ 4800/175341]\n",
      "loss: 0.258278  [ 6400/175341]\n",
      "loss: 0.342661  [ 8000/175341]\n",
      "loss: 0.087643  [ 9600/175341]\n",
      "loss: 0.504032  [11200/175341]\n",
      "loss: 0.076560  [12800/175341]\n",
      "loss: 0.322225  [14400/175341]\n",
      "loss: 0.243548  [16000/175341]\n",
      "loss: 0.225621  [17600/175341]\n",
      "loss: 0.534209  [19200/175341]\n",
      "loss: 0.231222  [20800/175341]\n",
      "loss: 0.416027  [22400/175341]\n",
      "loss: 0.534678  [24000/175341]\n",
      "loss: 0.457154  [25600/175341]\n",
      "loss: 0.196463  [27200/175341]\n",
      "loss: 0.674764  [28800/175341]\n",
      "loss: 0.185279  [30400/175341]\n",
      "loss: 0.261602  [32000/175341]\n",
      "loss: 0.679955  [33600/175341]\n",
      "loss: 0.357878  [35200/175341]\n",
      "loss: 0.276952  [36800/175341]\n",
      "loss: 0.780747  [38400/175341]\n",
      "loss: 0.402340  [40000/175341]\n",
      "loss: 0.566296  [41600/175341]\n",
      "loss: 0.655342  [43200/175341]\n",
      "loss: 0.180689  [44800/175341]\n",
      "loss: 0.305798  [46400/175341]\n",
      "loss: 0.779332  [48000/175341]\n",
      "loss: 0.198399  [49600/175341]\n",
      "loss: 0.244560  [51200/175341]\n",
      "loss: 0.779253  [52800/175341]\n",
      "loss: 0.166833  [54400/175341]\n",
      "loss: 0.446506  [56000/175341]\n",
      "loss: 0.268289  [57600/175341]\n",
      "loss: 0.480706  [59200/175341]\n",
      "loss: 0.603402  [60800/175341]\n",
      "loss: 0.177869  [62400/175341]\n",
      "loss: 0.476158  [64000/175341]\n",
      "loss: 0.704448  [65600/175341]\n",
      "loss: 0.449143  [67200/175341]\n",
      "loss: 0.519047  [68800/175341]\n",
      "loss: 0.431104  [70400/175341]\n",
      "loss: 0.279527  [72000/175341]\n",
      "loss: 0.470923  [73600/175341]\n",
      "loss: 0.487800  [75200/175341]\n",
      "loss: 0.352029  [76800/175341]\n",
      "loss: 0.650466  [78400/175341]\n",
      "loss: 0.345781  [80000/175341]\n",
      "loss: 0.560557  [81600/175341]\n",
      "loss: 0.451440  [83200/175341]\n",
      "loss: 0.550686  [84800/175341]\n",
      "loss: 0.560645  [86400/175341]\n",
      "loss: 0.538037  [88000/175341]\n",
      "loss: 0.295996  [89600/175341]\n",
      "loss: 0.741356  [91200/175341]\n",
      "loss: 0.272842  [92800/175341]\n",
      "loss: 0.493358  [94400/175341]\n",
      "loss: 0.210593  [96000/175341]\n",
      "loss: 0.555970  [97600/175341]\n",
      "loss: 0.250167  [99200/175341]\n",
      "loss: 0.216742  [100800/175341]\n",
      "loss: 0.703284  [102400/175341]\n",
      "loss: 0.142279  [104000/175341]\n",
      "loss: 0.694831  [105600/175341]\n",
      "loss: 0.380363  [107200/175341]\n",
      "loss: 0.699636  [108800/175341]\n",
      "loss: 0.727409  [110400/175341]\n",
      "loss: 0.974778  [112000/175341]\n",
      "loss: 0.439634  [113600/175341]\n",
      "loss: 0.360100  [115200/175341]\n",
      "loss: 0.250864  [116800/175341]\n",
      "loss: 0.144803  [118400/175341]\n",
      "loss: 0.617446  [120000/175341]\n",
      "loss: 0.689551  [121600/175341]\n",
      "loss: 0.560297  [123200/175341]\n",
      "loss: 0.431539  [124800/175341]\n",
      "loss: 0.345305  [126400/175341]\n",
      "loss: 0.581329  [128000/175341]\n",
      "loss: 0.670880  [129600/175341]\n",
      "loss: 0.316645  [131200/175341]\n",
      "loss: 0.375640  [132800/175341]\n",
      "loss: 0.329482  [134400/175341]\n",
      "loss: 0.516180  [136000/175341]\n",
      "loss: 0.489598  [137600/175341]\n",
      "loss: 0.427197  [139200/175341]\n",
      "loss: 0.487757  [140800/175341]\n",
      "loss: 0.180600  [142400/175341]\n",
      "loss: 0.784996  [144000/175341]\n",
      "loss: 0.481092  [145600/175341]\n",
      "loss: 0.711283  [147200/175341]\n",
      "loss: 0.483264  [148800/175341]\n",
      "loss: 0.150355  [150400/175341]\n",
      "loss: 0.474122  [152000/175341]\n",
      "loss: 0.575902  [153600/175341]\n",
      "loss: 0.052988  [155200/175341]\n",
      "loss: 0.235938  [156800/175341]\n",
      "loss: 0.351360  [158400/175341]\n",
      "loss: 0.087445  [160000/175341]\n",
      "loss: 0.337194  [161600/175341]\n",
      "loss: 0.472308  [163200/175341]\n",
      "loss: 0.752672  [164800/175341]\n",
      "loss: 0.695943  [166400/175341]\n",
      "loss: 0.238709  [168000/175341]\n",
      "loss: 0.636921  [169600/175341]\n",
      "loss: 0.263939  [171200/175341]\n",
      "loss: 0.779370  [172800/175341]\n",
      "loss: 0.520842  [174400/175341]\n",
      "Train Accuracy: 82.0310%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.583389, F1-score: 74.85%, Macro_F1-Score:  40.90%  \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.613201  [    0/175341]\n",
      "loss: 0.640114  [ 1600/175341]\n",
      "loss: 0.243598  [ 3200/175341]\n",
      "loss: 0.327027  [ 4800/175341]\n",
      "loss: 0.557166  [ 6400/175341]\n",
      "loss: 0.622094  [ 8000/175341]\n",
      "loss: 0.541233  [ 9600/175341]\n",
      "loss: 0.604727  [11200/175341]\n",
      "loss: 0.356169  [12800/175341]\n",
      "loss: 0.653373  [14400/175341]\n",
      "loss: 0.374964  [16000/175341]\n",
      "loss: 0.062888  [17600/175341]\n",
      "loss: 0.408184  [19200/175341]\n",
      "loss: 0.391453  [20800/175341]\n",
      "loss: 0.365950  [22400/175341]\n",
      "loss: 0.077010  [24000/175341]\n",
      "loss: 0.568825  [25600/175341]\n",
      "loss: 0.692844  [27200/175341]\n",
      "loss: 0.673931  [28800/175341]\n",
      "loss: 0.680802  [30400/175341]\n",
      "loss: 0.463552  [32000/175341]\n",
      "loss: 0.399580  [33600/175341]\n",
      "loss: 0.378001  [35200/175341]\n",
      "loss: 0.212438  [36800/175341]\n",
      "loss: 0.444431  [38400/175341]\n",
      "loss: 0.875811  [40000/175341]\n",
      "loss: 0.259182  [41600/175341]\n",
      "loss: 0.332940  [43200/175341]\n",
      "loss: 0.354455  [44800/175341]\n",
      "loss: 0.377201  [46400/175341]\n",
      "loss: 0.333341  [48000/175341]\n",
      "loss: 0.543359  [49600/175341]\n",
      "loss: 0.458441  [51200/175341]\n",
      "loss: 0.537067  [52800/175341]\n",
      "loss: 0.486682  [54400/175341]\n",
      "loss: 0.455470  [56000/175341]\n",
      "loss: 0.468197  [57600/175341]\n",
      "loss: 1.046867  [59200/175341]\n",
      "loss: 0.401211  [60800/175341]\n",
      "loss: 0.503820  [62400/175341]\n",
      "loss: 0.309094  [64000/175341]\n",
      "loss: 0.381865  [65600/175341]\n",
      "loss: 0.673616  [67200/175341]\n",
      "loss: 0.461748  [68800/175341]\n",
      "loss: 0.732797  [70400/175341]\n",
      "loss: 0.464253  [72000/175341]\n",
      "loss: 0.359001  [73600/175341]\n",
      "loss: 0.303100  [75200/175341]\n",
      "loss: 0.320534  [76800/175341]\n",
      "loss: 0.573521  [78400/175341]\n",
      "loss: 0.446455  [80000/175341]\n",
      "loss: 0.350491  [81600/175341]\n",
      "loss: 0.353046  [83200/175341]\n",
      "loss: 0.542436  [84800/175341]\n",
      "loss: 0.553488  [86400/175341]\n",
      "loss: 0.906330  [88000/175341]\n",
      "loss: 0.448929  [89600/175341]\n",
      "loss: 0.357503  [91200/175341]\n",
      "loss: 0.910640  [92800/175341]\n",
      "loss: 0.476398  [94400/175341]\n",
      "loss: 0.433649  [96000/175341]\n",
      "loss: 0.518839  [97600/175341]\n",
      "loss: 0.633151  [99200/175341]\n",
      "loss: 0.301223  [100800/175341]\n",
      "loss: 0.623822  [102400/175341]\n",
      "loss: 0.436599  [104000/175341]\n",
      "loss: 1.163497  [105600/175341]\n",
      "loss: 0.235989  [107200/175341]\n",
      "loss: 0.391378  [108800/175341]\n",
      "loss: 0.446452  [110400/175341]\n",
      "loss: 0.226730  [112000/175341]\n",
      "loss: 0.541687  [113600/175341]\n",
      "loss: 0.542426  [115200/175341]\n",
      "loss: 0.675873  [116800/175341]\n",
      "loss: 0.337680  [118400/175341]\n",
      "loss: 0.615376  [120000/175341]\n",
      "loss: 0.318482  [121600/175341]\n",
      "loss: 0.171398  [123200/175341]\n",
      "loss: 0.408742  [124800/175341]\n",
      "loss: 0.156039  [126400/175341]\n",
      "loss: 0.334550  [128000/175341]\n",
      "loss: 0.195647  [129600/175341]\n",
      "loss: 0.579148  [131200/175341]\n",
      "loss: 0.180677  [132800/175341]\n",
      "loss: 0.419525  [134400/175341]\n",
      "loss: 0.271475  [136000/175341]\n",
      "loss: 0.130999  [137600/175341]\n",
      "loss: 0.427542  [139200/175341]\n",
      "loss: 0.696028  [140800/175341]\n",
      "loss: 0.340827  [142400/175341]\n",
      "loss: 0.533756  [144000/175341]\n",
      "loss: 0.470760  [145600/175341]\n",
      "loss: 0.356576  [147200/175341]\n",
      "loss: 0.971646  [148800/175341]\n",
      "loss: 1.048856  [150400/175341]\n",
      "loss: 0.358942  [152000/175341]\n",
      "loss: 0.326353  [153600/175341]\n",
      "loss: 0.277342  [155200/175341]\n",
      "loss: 0.280146  [156800/175341]\n",
      "loss: 0.552220  [158400/175341]\n",
      "loss: 0.551491  [160000/175341]\n",
      "loss: 0.330574  [161600/175341]\n",
      "loss: 0.563278  [163200/175341]\n",
      "loss: 0.438096  [164800/175341]\n",
      "loss: 0.372946  [166400/175341]\n",
      "loss: 0.429281  [168000/175341]\n",
      "loss: 0.376432  [169600/175341]\n",
      "loss: 0.198982  [171200/175341]\n",
      "loss: 0.159755  [172800/175341]\n",
      "loss: 0.314197  [174400/175341]\n",
      "Train Accuracy: 81.9500%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.553072, F1-score: 76.37%, Macro_F1-Score:  42.13%  \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.436534  [    0/175341]\n",
      "loss: 0.368548  [ 1600/175341]\n",
      "loss: 0.750857  [ 3200/175341]\n",
      "loss: 0.586929  [ 4800/175341]\n",
      "loss: 0.312075  [ 6400/175341]\n",
      "loss: 0.615391  [ 8000/175341]\n",
      "loss: 0.487005  [ 9600/175341]\n",
      "loss: 0.434738  [11200/175341]\n",
      "loss: 0.228871  [12800/175341]\n",
      "loss: 0.450038  [14400/175341]\n",
      "loss: 0.748045  [16000/175341]\n",
      "loss: 0.328070  [17600/175341]\n",
      "loss: 0.418275  [19200/175341]\n",
      "loss: 0.434221  [20800/175341]\n",
      "loss: 0.410235  [22400/175341]\n",
      "loss: 0.338028  [24000/175341]\n",
      "loss: 0.943993  [25600/175341]\n",
      "loss: 0.763465  [27200/175341]\n",
      "loss: 0.498235  [28800/175341]\n",
      "loss: 0.645424  [30400/175341]\n",
      "loss: 0.751617  [32000/175341]\n",
      "loss: 0.333883  [33600/175341]\n",
      "loss: 0.532859  [35200/175341]\n",
      "loss: 0.149467  [36800/175341]\n",
      "loss: 0.171866  [38400/175341]\n",
      "loss: 0.233058  [40000/175341]\n",
      "loss: 0.112313  [41600/175341]\n",
      "loss: 0.619526  [43200/175341]\n",
      "loss: 0.221095  [44800/175341]\n",
      "loss: 0.581118  [46400/175341]\n",
      "loss: 0.383096  [48000/175341]\n",
      "loss: 0.734742  [49600/175341]\n",
      "loss: 0.454399  [51200/175341]\n",
      "loss: 0.405832  [52800/175341]\n",
      "loss: 0.300854  [54400/175341]\n",
      "loss: 0.660085  [56000/175341]\n",
      "loss: 0.476069  [57600/175341]\n",
      "loss: 0.304747  [59200/175341]\n",
      "loss: 0.832230  [60800/175341]\n",
      "loss: 0.319456  [62400/175341]\n",
      "loss: 0.461662  [64000/175341]\n",
      "loss: 0.167972  [65600/175341]\n",
      "loss: 0.072981  [67200/175341]\n",
      "loss: 0.482851  [68800/175341]\n",
      "loss: 0.650210  [70400/175341]\n",
      "loss: 0.335462  [72000/175341]\n",
      "loss: 0.401633  [73600/175341]\n",
      "loss: 0.333285  [75200/175341]\n",
      "loss: 0.359619  [76800/175341]\n",
      "loss: 0.781399  [78400/175341]\n",
      "loss: 0.370805  [80000/175341]\n",
      "loss: 0.675813  [81600/175341]\n",
      "loss: 0.303511  [83200/175341]\n",
      "loss: 0.559899  [84800/175341]\n",
      "loss: 0.346877  [86400/175341]\n",
      "loss: 0.261368  [88000/175341]\n",
      "loss: 0.171714  [89600/175341]\n",
      "loss: 0.285137  [91200/175341]\n",
      "loss: 0.188708  [92800/175341]\n",
      "loss: 0.338972  [94400/175341]\n",
      "loss: 0.157915  [96000/175341]\n",
      "loss: 0.113366  [97600/175341]\n",
      "loss: 0.555428  [99200/175341]\n",
      "loss: 0.439723  [100800/175341]\n",
      "loss: 0.540917  [102400/175341]\n",
      "loss: 0.123108  [104000/175341]\n",
      "loss: 0.605965  [105600/175341]\n",
      "loss: 0.581060  [107200/175341]\n",
      "loss: 0.145126  [108800/175341]\n",
      "loss: 0.673918  [110400/175341]\n",
      "loss: 0.120040  [112000/175341]\n",
      "loss: 0.338745  [113600/175341]\n",
      "loss: 0.396944  [115200/175341]\n",
      "loss: 0.311792  [116800/175341]\n",
      "loss: 0.096105  [118400/175341]\n",
      "loss: 0.166515  [120000/175341]\n",
      "loss: 0.472854  [121600/175341]\n",
      "loss: 0.285388  [123200/175341]\n",
      "loss: 0.260167  [124800/175341]\n",
      "loss: 0.368361  [126400/175341]\n",
      "loss: 0.499234  [128000/175341]\n",
      "loss: 0.146768  [129600/175341]\n",
      "loss: 0.643050  [131200/175341]\n",
      "loss: 0.306065  [132800/175341]\n",
      "loss: 0.562771  [134400/175341]\n",
      "loss: 0.438780  [136000/175341]\n",
      "loss: 0.098353  [137600/175341]\n",
      "loss: 0.507793  [139200/175341]\n",
      "loss: 0.870909  [140800/175341]\n",
      "loss: 0.601670  [142400/175341]\n",
      "loss: 0.229251  [144000/175341]\n",
      "loss: 0.581416  [145600/175341]\n",
      "loss: 0.583660  [147200/175341]\n",
      "loss: 0.727376  [148800/175341]\n",
      "loss: 0.618066  [150400/175341]\n",
      "loss: 0.434123  [152000/175341]\n",
      "loss: 0.666371  [153600/175341]\n",
      "loss: 0.566141  [155200/175341]\n",
      "loss: 0.217230  [156800/175341]\n",
      "loss: 0.921356  [158400/175341]\n",
      "loss: 0.207797  [160000/175341]\n",
      "loss: 0.477085  [161600/175341]\n",
      "loss: 0.290986  [163200/175341]\n",
      "loss: 0.264013  [164800/175341]\n",
      "loss: 0.481285  [166400/175341]\n",
      "loss: 0.421257  [168000/175341]\n",
      "loss: 0.707900  [169600/175341]\n",
      "loss: 0.454434  [171200/175341]\n",
      "loss: 0.578863  [172800/175341]\n",
      "loss: 0.573333  [174400/175341]\n",
      "Train Accuracy: 82.0259%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.556040, F1-score: 76.05%, Macro_F1-Score:  41.77%  \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.412924  [    0/175341]\n",
      "loss: 0.626748  [ 1600/175341]\n",
      "loss: 0.386558  [ 3200/175341]\n",
      "loss: 0.186035  [ 4800/175341]\n",
      "loss: 0.246778  [ 6400/175341]\n",
      "loss: 0.545581  [ 8000/175341]\n",
      "loss: 0.484798  [ 9600/175341]\n",
      "loss: 0.104011  [11200/175341]\n",
      "loss: 0.346771  [12800/175341]\n",
      "loss: 0.738246  [14400/175341]\n",
      "loss: 0.416079  [16000/175341]\n",
      "loss: 0.391224  [17600/175341]\n",
      "loss: 0.260798  [19200/175341]\n",
      "loss: 0.674462  [20800/175341]\n",
      "loss: 0.401593  [22400/175341]\n",
      "loss: 0.757415  [24000/175341]\n",
      "loss: 0.799041  [25600/175341]\n",
      "loss: 0.451441  [27200/175341]\n",
      "loss: 0.270938  [28800/175341]\n",
      "loss: 0.585141  [30400/175341]\n",
      "loss: 0.700774  [32000/175341]\n",
      "loss: 0.383978  [33600/175341]\n",
      "loss: 0.221674  [35200/175341]\n",
      "loss: 0.677432  [36800/175341]\n",
      "loss: 0.734742  [38400/175341]\n",
      "loss: 0.854374  [40000/175341]\n",
      "loss: 0.525458  [41600/175341]\n",
      "loss: 0.206718  [43200/175341]\n",
      "loss: 0.313553  [44800/175341]\n",
      "loss: 0.423356  [46400/175341]\n",
      "loss: 0.347886  [48000/175341]\n",
      "loss: 0.204098  [49600/175341]\n",
      "loss: 0.457776  [51200/175341]\n",
      "loss: 0.327799  [52800/175341]\n",
      "loss: 0.547672  [54400/175341]\n",
      "loss: 0.322330  [56000/175341]\n",
      "loss: 0.628457  [57600/175341]\n",
      "loss: 0.321869  [59200/175341]\n",
      "loss: 0.222418  [60800/175341]\n",
      "loss: 0.313735  [62400/175341]\n",
      "loss: 0.594210  [64000/175341]\n",
      "loss: 0.637099  [65600/175341]\n",
      "loss: 0.190042  [67200/175341]\n",
      "loss: 0.435052  [68800/175341]\n",
      "loss: 0.423444  [70400/175341]\n",
      "loss: 0.311150  [72000/175341]\n",
      "loss: 0.498963  [73600/175341]\n",
      "loss: 0.398185  [75200/175341]\n",
      "loss: 0.604085  [76800/175341]\n",
      "loss: 0.402552  [78400/175341]\n",
      "loss: 0.536609  [80000/175341]\n",
      "loss: 0.765603  [81600/175341]\n",
      "loss: 0.267897  [83200/175341]\n",
      "loss: 0.477038  [84800/175341]\n",
      "loss: 0.690706  [86400/175341]\n",
      "loss: 0.530638  [88000/175341]\n",
      "loss: 0.326196  [89600/175341]\n",
      "loss: 0.524509  [91200/175341]\n",
      "loss: 0.509920  [92800/175341]\n",
      "loss: 0.259130  [94400/175341]\n",
      "loss: 0.271958  [96000/175341]\n",
      "loss: 0.486269  [97600/175341]\n",
      "loss: 0.651519  [99200/175341]\n",
      "loss: 0.217391  [100800/175341]\n",
      "loss: 0.414228  [102400/175341]\n",
      "loss: 0.455874  [104000/175341]\n",
      "loss: 0.454760  [105600/175341]\n",
      "loss: 0.807257  [107200/175341]\n",
      "loss: 0.417670  [108800/175341]\n",
      "loss: 0.590306  [110400/175341]\n",
      "loss: 0.478388  [112000/175341]\n",
      "loss: 0.463912  [113600/175341]\n",
      "loss: 0.383124  [115200/175341]\n",
      "loss: 0.404629  [116800/175341]\n",
      "loss: 0.376930  [118400/175341]\n",
      "loss: 0.281368  [120000/175341]\n",
      "loss: 0.494188  [121600/175341]\n",
      "loss: 0.565031  [123200/175341]\n",
      "loss: 0.449382  [124800/175341]\n",
      "loss: 0.227114  [126400/175341]\n",
      "loss: 0.555752  [128000/175341]\n",
      "loss: 0.220102  [129600/175341]\n",
      "loss: 0.243211  [131200/175341]\n",
      "loss: 0.553565  [132800/175341]\n",
      "loss: 0.233638  [134400/175341]\n",
      "loss: 0.356554  [136000/175341]\n",
      "loss: 0.392173  [137600/175341]\n",
      "loss: 0.378179  [139200/175341]\n",
      "loss: 0.098061  [140800/175341]\n",
      "loss: 0.268011  [142400/175341]\n",
      "loss: 0.697881  [144000/175341]\n",
      "loss: 0.588687  [145600/175341]\n",
      "loss: 0.146220  [147200/175341]\n",
      "loss: 0.277509  [148800/175341]\n",
      "loss: 1.050725  [150400/175341]\n",
      "loss: 0.551632  [152000/175341]\n",
      "loss: 0.682259  [153600/175341]\n",
      "loss: 0.322533  [155200/175341]\n",
      "loss: 0.445625  [156800/175341]\n",
      "loss: 0.326295  [158400/175341]\n",
      "loss: 0.656045  [160000/175341]\n",
      "loss: 0.264444  [161600/175341]\n",
      "loss: 0.337812  [163200/175341]\n",
      "loss: 0.558732  [164800/175341]\n",
      "loss: 0.191931  [166400/175341]\n",
      "loss: 0.634028  [168000/175341]\n",
      "loss: 0.365013  [169600/175341]\n",
      "loss: 0.186644  [171200/175341]\n",
      "loss: 0.104315  [172800/175341]\n",
      "loss: 0.605082  [174400/175341]\n",
      "Train Accuracy: 82.0247%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.555484, F1-score: 76.55%, Macro_F1-Score:  42.12%  \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.986245  [    0/175341]\n",
      "loss: 0.549359  [ 1600/175341]\n",
      "loss: 0.296652  [ 3200/175341]\n",
      "loss: 0.315243  [ 4800/175341]\n",
      "loss: 0.328055  [ 6400/175341]\n",
      "loss: 0.344270  [ 8000/175341]\n",
      "loss: 0.485025  [ 9600/175341]\n",
      "loss: 0.343043  [11200/175341]\n",
      "loss: 0.487615  [12800/175341]\n",
      "loss: 0.453333  [14400/175341]\n",
      "loss: 0.363270  [16000/175341]\n",
      "loss: 0.587554  [17600/175341]\n",
      "loss: 0.163493  [19200/175341]\n",
      "loss: 0.477911  [20800/175341]\n",
      "loss: 0.627736  [22400/175341]\n",
      "loss: 0.075885  [24000/175341]\n",
      "loss: 0.397311  [25600/175341]\n",
      "loss: 0.440716  [27200/175341]\n",
      "loss: 0.372335  [28800/175341]\n",
      "loss: 0.430706  [30400/175341]\n",
      "loss: 0.771501  [32000/175341]\n",
      "loss: 0.281851  [33600/175341]\n",
      "loss: 0.401024  [35200/175341]\n",
      "loss: 0.507490  [36800/175341]\n",
      "loss: 0.431516  [38400/175341]\n",
      "loss: 0.591322  [40000/175341]\n",
      "loss: 0.258200  [41600/175341]\n",
      "loss: 0.338734  [43200/175341]\n",
      "loss: 0.190827  [44800/175341]\n",
      "loss: 0.444251  [46400/175341]\n",
      "loss: 0.613082  [48000/175341]\n",
      "loss: 0.638547  [49600/175341]\n",
      "loss: 0.308321  [51200/175341]\n",
      "loss: 0.543695  [52800/175341]\n",
      "loss: 0.349347  [54400/175341]\n",
      "loss: 0.318464  [56000/175341]\n",
      "loss: 0.307240  [57600/175341]\n",
      "loss: 0.143255  [59200/175341]\n",
      "loss: 0.648151  [60800/175341]\n",
      "loss: 0.539425  [62400/175341]\n",
      "loss: 0.469885  [64000/175341]\n",
      "loss: 0.708868  [65600/175341]\n",
      "loss: 0.587110  [67200/175341]\n",
      "loss: 0.358368  [68800/175341]\n",
      "loss: 0.736113  [70400/175341]\n",
      "loss: 0.247793  [72000/175341]\n",
      "loss: 0.312547  [73600/175341]\n",
      "loss: 0.181608  [75200/175341]\n",
      "loss: 0.064256  [76800/175341]\n",
      "loss: 0.503038  [78400/175341]\n",
      "loss: 0.439922  [80000/175341]\n",
      "loss: 0.253774  [81600/175341]\n",
      "loss: 0.104776  [83200/175341]\n",
      "loss: 0.138935  [84800/175341]\n",
      "loss: 0.257045  [86400/175341]\n",
      "loss: 0.553977  [88000/175341]\n",
      "loss: 0.595528  [89600/175341]\n",
      "loss: 0.186898  [91200/175341]\n",
      "loss: 0.197010  [92800/175341]\n",
      "loss: 0.401449  [94400/175341]\n",
      "loss: 0.163852  [96000/175341]\n",
      "loss: 0.364357  [97600/175341]\n",
      "loss: 0.465545  [99200/175341]\n",
      "loss: 0.490757  [100800/175341]\n",
      "loss: 0.414905  [102400/175341]\n",
      "loss: 0.370212  [104000/175341]\n",
      "loss: 0.537927  [105600/175341]\n",
      "loss: 0.513814  [107200/175341]\n",
      "loss: 0.281577  [108800/175341]\n",
      "loss: 0.297866  [110400/175341]\n",
      "loss: 0.373663  [112000/175341]\n",
      "loss: 0.756755  [113600/175341]\n",
      "loss: 0.171554  [115200/175341]\n",
      "loss: 0.527611  [116800/175341]\n",
      "loss: 0.767710  [118400/175341]\n",
      "loss: 0.412306  [120000/175341]\n",
      "loss: 0.639699  [121600/175341]\n",
      "loss: 0.614526  [123200/175341]\n",
      "loss: 0.219705  [124800/175341]\n",
      "loss: 0.363128  [126400/175341]\n",
      "loss: 0.316091  [128000/175341]\n",
      "loss: 0.583521  [129600/175341]\n",
      "loss: 0.399749  [131200/175341]\n",
      "loss: 0.174817  [132800/175341]\n",
      "loss: 0.465332  [134400/175341]\n",
      "loss: 0.215710  [136000/175341]\n",
      "loss: 0.177158  [137600/175341]\n",
      "loss: 0.393136  [139200/175341]\n",
      "loss: 0.441780  [140800/175341]\n",
      "loss: 0.314755  [142400/175341]\n",
      "loss: 0.707586  [144000/175341]\n",
      "loss: 0.456806  [145600/175341]\n",
      "loss: 0.975849  [147200/175341]\n",
      "loss: 0.605018  [148800/175341]\n",
      "loss: 0.165201  [150400/175341]\n",
      "loss: 0.675243  [152000/175341]\n",
      "loss: 0.332695  [153600/175341]\n",
      "loss: 0.360939  [155200/175341]\n",
      "loss: 0.272187  [156800/175341]\n",
      "loss: 0.450341  [158400/175341]\n",
      "loss: 0.524365  [160000/175341]\n",
      "loss: 0.391807  [161600/175341]\n",
      "loss: 0.374429  [163200/175341]\n",
      "loss: 0.217650  [164800/175341]\n",
      "loss: 0.316733  [166400/175341]\n",
      "loss: 0.217149  [168000/175341]\n",
      "loss: 0.654133  [169600/175341]\n",
      "loss: 0.243712  [171200/175341]\n",
      "loss: 0.337885  [172800/175341]\n",
      "loss: 0.589505  [174400/175341]\n",
      "Train Accuracy: 82.0156%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.552844, F1-score: 76.75%, Macro_F1-Score:  42.20%  \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.394913  [    0/175341]\n",
      "loss: 0.311404  [ 1600/175341]\n",
      "loss: 0.232372  [ 3200/175341]\n",
      "loss: 0.217394  [ 4800/175341]\n",
      "loss: 0.695058  [ 6400/175341]\n",
      "loss: 0.318586  [ 8000/175341]\n",
      "loss: 0.321736  [ 9600/175341]\n",
      "loss: 0.478587  [11200/175341]\n",
      "loss: 0.220997  [12800/175341]\n",
      "loss: 0.129573  [14400/175341]\n",
      "loss: 0.716789  [16000/175341]\n",
      "loss: 0.473027  [17600/175341]\n",
      "loss: 0.450957  [19200/175341]\n",
      "loss: 0.385106  [20800/175341]\n",
      "loss: 0.378402  [22400/175341]\n",
      "loss: 0.196030  [24000/175341]\n",
      "loss: 0.143668  [25600/175341]\n",
      "loss: 0.247915  [27200/175341]\n",
      "loss: 0.593875  [28800/175341]\n",
      "loss: 0.365703  [30400/175341]\n",
      "loss: 0.462352  [32000/175341]\n",
      "loss: 0.320843  [33600/175341]\n",
      "loss: 0.340590  [35200/175341]\n",
      "loss: 0.711722  [36800/175341]\n",
      "loss: 0.412068  [38400/175341]\n",
      "loss: 0.380770  [40000/175341]\n",
      "loss: 0.907685  [41600/175341]\n",
      "loss: 0.486126  [43200/175341]\n",
      "loss: 0.624920  [44800/175341]\n",
      "loss: 1.048058  [46400/175341]\n",
      "loss: 0.439884  [48000/175341]\n",
      "loss: 0.464413  [49600/175341]\n",
      "loss: 0.136841  [51200/175341]\n",
      "loss: 0.524834  [52800/175341]\n",
      "loss: 0.184060  [54400/175341]\n",
      "loss: 0.397503  [56000/175341]\n",
      "loss: 0.403019  [57600/175341]\n",
      "loss: 0.524448  [59200/175341]\n",
      "loss: 0.739007  [60800/175341]\n",
      "loss: 0.433941  [62400/175341]\n",
      "loss: 0.464053  [64000/175341]\n",
      "loss: 0.485518  [65600/175341]\n",
      "loss: 0.491855  [67200/175341]\n",
      "loss: 0.657736  [68800/175341]\n",
      "loss: 0.345229  [70400/175341]\n",
      "loss: 0.397842  [72000/175341]\n",
      "loss: 0.278617  [73600/175341]\n",
      "loss: 0.631350  [75200/175341]\n",
      "loss: 0.466963  [76800/175341]\n",
      "loss: 0.234139  [78400/175341]\n",
      "loss: 0.595973  [80000/175341]\n",
      "loss: 0.414121  [81600/175341]\n",
      "loss: 0.133207  [83200/175341]\n",
      "loss: 0.536001  [84800/175341]\n",
      "loss: 0.587900  [86400/175341]\n",
      "loss: 0.382946  [88000/175341]\n",
      "loss: 1.201435  [89600/175341]\n",
      "loss: 0.353197  [91200/175341]\n",
      "loss: 0.510785  [92800/175341]\n",
      "loss: 0.628109  [94400/175341]\n",
      "loss: 0.577926  [96000/175341]\n",
      "loss: 0.321754  [97600/175341]\n",
      "loss: 0.170712  [99200/175341]\n",
      "loss: 0.208218  [100800/175341]\n",
      "loss: 0.168946  [102400/175341]\n",
      "loss: 0.340927  [104000/175341]\n",
      "loss: 0.133484  [105600/175341]\n",
      "loss: 0.811113  [107200/175341]\n",
      "loss: 0.581287  [108800/175341]\n",
      "loss: 0.423350  [110400/175341]\n",
      "loss: 0.196922  [112000/175341]\n",
      "loss: 0.650457  [113600/175341]\n",
      "loss: 0.567341  [115200/175341]\n",
      "loss: 0.408444  [116800/175341]\n",
      "loss: 0.537983  [118400/175341]\n",
      "loss: 0.201130  [120000/175341]\n",
      "loss: 0.168718  [121600/175341]\n",
      "loss: 0.398495  [123200/175341]\n",
      "loss: 0.320370  [124800/175341]\n",
      "loss: 0.594704  [126400/175341]\n",
      "loss: 0.518447  [128000/175341]\n",
      "loss: 0.275722  [129600/175341]\n",
      "loss: 0.611538  [131200/175341]\n",
      "loss: 0.537467  [132800/175341]\n",
      "loss: 0.473199  [134400/175341]\n",
      "loss: 0.790659  [136000/175341]\n",
      "loss: 0.141761  [137600/175341]\n",
      "loss: 0.616657  [139200/175341]\n",
      "loss: 0.557827  [140800/175341]\n",
      "loss: 0.316006  [142400/175341]\n",
      "loss: 0.195378  [144000/175341]\n",
      "loss: 0.078018  [145600/175341]\n",
      "loss: 0.762469  [147200/175341]\n",
      "loss: 0.144039  [148800/175341]\n",
      "loss: 0.574407  [150400/175341]\n",
      "loss: 0.280747  [152000/175341]\n",
      "loss: 0.231466  [153600/175341]\n",
      "loss: 0.896410  [155200/175341]\n",
      "loss: 0.369582  [156800/175341]\n",
      "loss: 0.628020  [158400/175341]\n",
      "loss: 0.128689  [160000/175341]\n",
      "loss: 0.395888  [161600/175341]\n",
      "loss: 0.178444  [163200/175341]\n",
      "loss: 0.142229  [164800/175341]\n",
      "loss: 0.387603  [166400/175341]\n",
      "loss: 0.331232  [168000/175341]\n",
      "loss: 0.425172  [169600/175341]\n",
      "loss: 0.647091  [171200/175341]\n",
      "loss: 0.501883  [172800/175341]\n",
      "loss: 0.472103  [174400/175341]\n",
      "Train Accuracy: 82.0384%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.552552, F1-score: 76.10%, Macro_F1-Score:  41.71%  \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.549360  [    0/175341]\n",
      "loss: 0.259509  [ 1600/175341]\n",
      "loss: 0.409216  [ 3200/175341]\n",
      "loss: 0.493735  [ 4800/175341]\n",
      "loss: 0.507493  [ 6400/175341]\n",
      "loss: 0.486222  [ 8000/175341]\n",
      "loss: 0.193842  [ 9600/175341]\n",
      "loss: 0.671168  [11200/175341]\n",
      "loss: 0.201785  [12800/175341]\n",
      "loss: 0.193485  [14400/175341]\n",
      "loss: 0.641632  [16000/175341]\n",
      "loss: 0.197876  [17600/175341]\n",
      "loss: 0.560982  [19200/175341]\n",
      "loss: 0.285374  [20800/175341]\n",
      "loss: 0.206452  [22400/175341]\n",
      "loss: 0.630372  [24000/175341]\n",
      "loss: 0.562070  [25600/175341]\n",
      "loss: 0.362537  [27200/175341]\n",
      "loss: 0.375232  [28800/175341]\n",
      "loss: 0.458340  [30400/175341]\n",
      "loss: 0.386216  [32000/175341]\n",
      "loss: 0.622296  [33600/175341]\n",
      "loss: 0.682033  [35200/175341]\n",
      "loss: 0.269723  [36800/175341]\n",
      "loss: 0.371378  [38400/175341]\n",
      "loss: 0.323239  [40000/175341]\n",
      "loss: 0.412068  [41600/175341]\n",
      "loss: 0.366589  [43200/175341]\n",
      "loss: 0.217059  [44800/175341]\n",
      "loss: 0.271520  [46400/175341]\n",
      "loss: 0.165617  [48000/175341]\n",
      "loss: 0.427366  [49600/175341]\n",
      "loss: 0.304635  [51200/175341]\n",
      "loss: 0.576627  [52800/175341]\n",
      "loss: 0.434616  [54400/175341]\n",
      "loss: 0.522003  [56000/175341]\n",
      "loss: 0.335312  [57600/175341]\n",
      "loss: 0.424053  [59200/175341]\n",
      "loss: 0.395478  [60800/175341]\n",
      "loss: 0.267987  [62400/175341]\n",
      "loss: 0.482387  [64000/175341]\n",
      "loss: 0.300657  [65600/175341]\n",
      "loss: 0.649932  [67200/175341]\n",
      "loss: 0.315814  [68800/175341]\n",
      "loss: 0.476742  [70400/175341]\n",
      "loss: 0.662542  [72000/175341]\n",
      "loss: 0.141003  [73600/175341]\n",
      "loss: 0.438751  [75200/175341]\n",
      "loss: 0.464331  [76800/175341]\n",
      "loss: 0.310403  [78400/175341]\n",
      "loss: 0.355888  [80000/175341]\n",
      "loss: 0.571651  [81600/175341]\n",
      "loss: 0.493032  [83200/175341]\n",
      "loss: 0.719456  [84800/175341]\n",
      "loss: 0.643212  [86400/175341]\n",
      "loss: 0.119964  [88000/175341]\n",
      "loss: 0.167388  [89600/175341]\n",
      "loss: 0.357455  [91200/175341]\n",
      "loss: 0.934326  [92800/175341]\n",
      "loss: 0.491951  [94400/175341]\n",
      "loss: 0.411414  [96000/175341]\n",
      "loss: 0.645923  [97600/175341]\n",
      "loss: 0.485736  [99200/175341]\n",
      "loss: 0.319526  [100800/175341]\n",
      "loss: 0.268882  [102400/175341]\n",
      "loss: 0.464892  [104000/175341]\n",
      "loss: 0.439199  [105600/175341]\n",
      "loss: 0.574170  [107200/175341]\n",
      "loss: 0.513443  [108800/175341]\n",
      "loss: 0.665547  [110400/175341]\n",
      "loss: 0.406841  [112000/175341]\n",
      "loss: 0.553978  [113600/175341]\n",
      "loss: 0.531125  [115200/175341]\n",
      "loss: 0.571176  [116800/175341]\n",
      "loss: 0.166627  [118400/175341]\n",
      "loss: 0.220198  [120000/175341]\n",
      "loss: 0.728550  [121600/175341]\n",
      "loss: 0.453618  [123200/175341]\n",
      "loss: 0.238782  [124800/175341]\n",
      "loss: 0.579994  [126400/175341]\n",
      "loss: 0.137039  [128000/175341]\n",
      "loss: 0.449307  [129600/175341]\n",
      "loss: 0.899861  [131200/175341]\n",
      "loss: 0.363487  [132800/175341]\n",
      "loss: 0.192818  [134400/175341]\n",
      "loss: 0.432886  [136000/175341]\n",
      "loss: 0.235430  [137600/175341]\n",
      "loss: 0.327326  [139200/175341]\n",
      "loss: 0.351636  [140800/175341]\n",
      "loss: 0.574627  [142400/175341]\n",
      "loss: 0.304817  [144000/175341]\n",
      "loss: 0.403381  [145600/175341]\n",
      "loss: 0.341918  [147200/175341]\n",
      "loss: 0.332195  [148800/175341]\n",
      "loss: 0.411813  [150400/175341]\n",
      "loss: 0.403542  [152000/175341]\n",
      "loss: 0.259163  [153600/175341]\n",
      "loss: 0.193370  [155200/175341]\n",
      "loss: 0.256702  [156800/175341]\n",
      "loss: 0.411262  [158400/175341]\n",
      "loss: 0.529250  [160000/175341]\n",
      "loss: 0.638050  [161600/175341]\n",
      "loss: 0.274368  [163200/175341]\n",
      "loss: 0.313000  [164800/175341]\n",
      "loss: 0.644353  [166400/175341]\n",
      "loss: 0.115865  [168000/175341]\n",
      "loss: 0.412464  [169600/175341]\n",
      "loss: 0.437726  [171200/175341]\n",
      "loss: 0.333586  [172800/175341]\n",
      "loss: 0.579467  [174400/175341]\n",
      "Train Accuracy: 82.0173%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.568921, F1-score: 75.45%, Macro_F1-Score:  42.36%  \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.502290  [    0/175341]\n",
      "loss: 0.396187  [ 1600/175341]\n",
      "loss: 0.545238  [ 3200/175341]\n",
      "loss: 0.731073  [ 4800/175341]\n",
      "loss: 0.214163  [ 6400/175341]\n",
      "loss: 0.439471  [ 8000/175341]\n",
      "loss: 0.545911  [ 9600/175341]\n",
      "loss: 0.459518  [11200/175341]\n",
      "loss: 0.519237  [12800/175341]\n",
      "loss: 0.522983  [14400/175341]\n",
      "loss: 0.254282  [16000/175341]\n",
      "loss: 0.424797  [17600/175341]\n",
      "loss: 0.459786  [19200/175341]\n",
      "loss: 0.527428  [20800/175341]\n",
      "loss: 0.373326  [22400/175341]\n",
      "loss: 0.378001  [24000/175341]\n",
      "loss: 0.238328  [25600/175341]\n",
      "loss: 0.268050  [27200/175341]\n",
      "loss: 1.098024  [28800/175341]\n",
      "loss: 0.617190  [30400/175341]\n",
      "loss: 0.543159  [32000/175341]\n",
      "loss: 0.576014  [33600/175341]\n",
      "loss: 0.588225  [35200/175341]\n",
      "loss: 0.883316  [36800/175341]\n",
      "loss: 0.361325  [38400/175341]\n",
      "loss: 0.376654  [40000/175341]\n",
      "loss: 0.042366  [41600/175341]\n",
      "loss: 0.203329  [43200/175341]\n",
      "loss: 0.492115  [44800/175341]\n",
      "loss: 0.182862  [46400/175341]\n",
      "loss: 0.366082  [48000/175341]\n",
      "loss: 0.746631  [49600/175341]\n",
      "loss: 0.684663  [51200/175341]\n",
      "loss: 0.657769  [52800/175341]\n",
      "loss: 0.580744  [54400/175341]\n",
      "loss: 0.178441  [56000/175341]\n",
      "loss: 0.321761  [57600/175341]\n",
      "loss: 0.610085  [59200/175341]\n",
      "loss: 0.737004  [60800/175341]\n",
      "loss: 0.564192  [62400/175341]\n",
      "loss: 0.685875  [64000/175341]\n",
      "loss: 0.096990  [65600/175341]\n",
      "loss: 0.536742  [67200/175341]\n",
      "loss: 0.399700  [68800/175341]\n",
      "loss: 0.356029  [70400/175341]\n",
      "loss: 0.138384  [72000/175341]\n",
      "loss: 0.125703  [73600/175341]\n",
      "loss: 0.143539  [75200/175341]\n",
      "loss: 0.426820  [76800/175341]\n",
      "loss: 0.413676  [78400/175341]\n",
      "loss: 0.655140  [80000/175341]\n",
      "loss: 0.557320  [81600/175341]\n",
      "loss: 0.624745  [83200/175341]\n",
      "loss: 0.886694  [84800/175341]\n",
      "loss: 0.078290  [86400/175341]\n",
      "loss: 0.547241  [88000/175341]\n",
      "loss: 0.422591  [89600/175341]\n",
      "loss: 0.296565  [91200/175341]\n",
      "loss: 0.342407  [92800/175341]\n",
      "loss: 0.609015  [94400/175341]\n",
      "loss: 0.592371  [96000/175341]\n",
      "loss: 0.436466  [97600/175341]\n",
      "loss: 0.336658  [99200/175341]\n",
      "loss: 0.372164  [100800/175341]\n",
      "loss: 0.284211  [102400/175341]\n",
      "loss: 0.476890  [104000/175341]\n",
      "loss: 0.300653  [105600/175341]\n",
      "loss: 0.363503  [107200/175341]\n",
      "loss: 0.521490  [108800/175341]\n",
      "loss: 0.256445  [110400/175341]\n",
      "loss: 0.604666  [112000/175341]\n",
      "loss: 0.453550  [113600/175341]\n",
      "loss: 0.253178  [115200/175341]\n",
      "loss: 0.271715  [116800/175341]\n",
      "loss: 0.890145  [118400/175341]\n",
      "loss: 0.736508  [120000/175341]\n",
      "loss: 0.493660  [121600/175341]\n",
      "loss: 0.653224  [123200/175341]\n",
      "loss: 0.380750  [124800/175341]\n",
      "loss: 1.070699  [126400/175341]\n",
      "loss: 0.393801  [128000/175341]\n",
      "loss: 0.597828  [129600/175341]\n",
      "loss: 0.248018  [131200/175341]\n",
      "loss: 0.627553  [132800/175341]\n",
      "loss: 0.650765  [134400/175341]\n",
      "loss: 0.478656  [136000/175341]\n",
      "loss: 0.256804  [137600/175341]\n",
      "loss: 0.539607  [139200/175341]\n",
      "loss: 0.350756  [140800/175341]\n",
      "loss: 0.311641  [142400/175341]\n",
      "loss: 0.395447  [144000/175341]\n",
      "loss: 0.809105  [145600/175341]\n",
      "loss: 0.645976  [147200/175341]\n",
      "loss: 0.383705  [148800/175341]\n",
      "loss: 0.336894  [150400/175341]\n",
      "loss: 0.696491  [152000/175341]\n",
      "loss: 0.305170  [153600/175341]\n",
      "loss: 0.673547  [155200/175341]\n",
      "loss: 0.615454  [156800/175341]\n",
      "loss: 0.373917  [158400/175341]\n",
      "loss: 0.290537  [160000/175341]\n",
      "loss: 0.331407  [161600/175341]\n",
      "loss: 0.355910  [163200/175341]\n",
      "loss: 0.194884  [164800/175341]\n",
      "loss: 0.354696  [166400/175341]\n",
      "loss: 0.303645  [168000/175341]\n",
      "loss: 0.161886  [169600/175341]\n",
      "loss: 0.153212  [171200/175341]\n",
      "loss: 0.297388  [172800/175341]\n",
      "loss: 0.306667  [174400/175341]\n",
      "Train Accuracy: 82.0122%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.557733, F1-score: 76.62%, Macro_F1-Score:  42.80%  \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.640628  [    0/175341]\n",
      "loss: 0.526183  [ 1600/175341]\n",
      "loss: 0.491856  [ 3200/175341]\n",
      "loss: 0.251846  [ 4800/175341]\n",
      "loss: 0.796607  [ 6400/175341]\n",
      "loss: 0.796835  [ 8000/175341]\n",
      "loss: 0.453804  [ 9600/175341]\n",
      "loss: 0.540879  [11200/175341]\n",
      "loss: 0.108350  [12800/175341]\n",
      "loss: 0.531820  [14400/175341]\n",
      "loss: 0.185062  [16000/175341]\n",
      "loss: 0.570879  [17600/175341]\n",
      "loss: 0.496161  [19200/175341]\n",
      "loss: 0.666963  [20800/175341]\n",
      "loss: 0.552439  [22400/175341]\n",
      "loss: 0.576290  [24000/175341]\n",
      "loss: 0.347891  [25600/175341]\n",
      "loss: 0.118468  [27200/175341]\n",
      "loss: 0.371756  [28800/175341]\n",
      "loss: 0.416541  [30400/175341]\n",
      "loss: 0.710165  [32000/175341]\n",
      "loss: 0.130685  [33600/175341]\n",
      "loss: 0.150919  [35200/175341]\n",
      "loss: 0.408493  [36800/175341]\n",
      "loss: 0.288045  [38400/175341]\n",
      "loss: 0.264104  [40000/175341]\n",
      "loss: 0.217058  [41600/175341]\n",
      "loss: 0.083319  [43200/175341]\n",
      "loss: 0.460625  [44800/175341]\n",
      "loss: 0.254184  [46400/175341]\n",
      "loss: 0.569289  [48000/175341]\n",
      "loss: 0.445805  [49600/175341]\n",
      "loss: 0.416520  [51200/175341]\n",
      "loss: 0.299095  [52800/175341]\n",
      "loss: 0.719770  [54400/175341]\n",
      "loss: 0.297109  [56000/175341]\n",
      "loss: 0.436563  [57600/175341]\n",
      "loss: 0.331458  [59200/175341]\n",
      "loss: 0.379724  [60800/175341]\n",
      "loss: 0.697857  [62400/175341]\n",
      "loss: 0.564028  [64000/175341]\n",
      "loss: 0.449160  [65600/175341]\n",
      "loss: 0.544417  [67200/175341]\n",
      "loss: 0.143427  [68800/175341]\n",
      "loss: 0.985214  [70400/175341]\n",
      "loss: 0.168785  [72000/175341]\n",
      "loss: 0.487174  [73600/175341]\n",
      "loss: 0.469421  [75200/175341]\n",
      "loss: 0.625104  [76800/175341]\n",
      "loss: 0.220456  [78400/175341]\n",
      "loss: 0.737493  [80000/175341]\n",
      "loss: 0.214631  [81600/175341]\n",
      "loss: 0.122066  [83200/175341]\n",
      "loss: 0.234185  [84800/175341]\n",
      "loss: 0.321763  [86400/175341]\n",
      "loss: 0.459256  [88000/175341]\n",
      "loss: 0.310040  [89600/175341]\n",
      "loss: 0.726250  [91200/175341]\n",
      "loss: 0.499510  [92800/175341]\n",
      "loss: 0.228589  [94400/175341]\n",
      "loss: 0.094796  [96000/175341]\n",
      "loss: 0.556230  [97600/175341]\n",
      "loss: 0.292788  [99200/175341]\n",
      "loss: 0.455164  [100800/175341]\n",
      "loss: 0.445726  [102400/175341]\n",
      "loss: 0.345469  [104000/175341]\n",
      "loss: 0.771114  [105600/175341]\n",
      "loss: 0.323069  [107200/175341]\n",
      "loss: 0.322433  [108800/175341]\n",
      "loss: 0.346144  [110400/175341]\n",
      "loss: 0.365418  [112000/175341]\n",
      "loss: 0.808157  [113600/175341]\n",
      "loss: 0.431740  [115200/175341]\n",
      "loss: 0.545357  [116800/175341]\n",
      "loss: 0.173533  [118400/175341]\n",
      "loss: 0.368798  [120000/175341]\n",
      "loss: 0.598341  [121600/175341]\n",
      "loss: 0.405320  [123200/175341]\n",
      "loss: 0.351886  [124800/175341]\n",
      "loss: 0.600386  [126400/175341]\n",
      "loss: 0.724380  [128000/175341]\n",
      "loss: 1.006124  [129600/175341]\n",
      "loss: 0.285505  [131200/175341]\n",
      "loss: 0.309562  [132800/175341]\n",
      "loss: 0.459213  [134400/175341]\n",
      "loss: 0.399254  [136000/175341]\n",
      "loss: 0.115540  [137600/175341]\n",
      "loss: 0.946220  [139200/175341]\n",
      "loss: 0.236425  [140800/175341]\n",
      "loss: 0.228235  [142400/175341]\n",
      "loss: 0.531338  [144000/175341]\n",
      "loss: 0.351434  [145600/175341]\n",
      "loss: 0.140309  [147200/175341]\n",
      "loss: 0.646166  [148800/175341]\n",
      "loss: 0.545562  [150400/175341]\n",
      "loss: 0.219220  [152000/175341]\n",
      "loss: 0.560837  [153600/175341]\n",
      "loss: 0.794318  [155200/175341]\n",
      "loss: 0.209682  [156800/175341]\n",
      "loss: 0.490740  [158400/175341]\n",
      "loss: 0.535085  [160000/175341]\n",
      "loss: 0.445947  [161600/175341]\n",
      "loss: 0.200532  [163200/175341]\n",
      "loss: 0.354807  [164800/175341]\n",
      "loss: 0.285604  [166400/175341]\n",
      "loss: 0.377868  [168000/175341]\n",
      "loss: 0.350028  [169600/175341]\n",
      "loss: 0.228628  [171200/175341]\n",
      "loss: 0.794389  [172800/175341]\n",
      "loss: 0.192990  [174400/175341]\n",
      "Train Accuracy: 81.9586%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.558404, F1-score: 76.19%, Macro_F1-Score:  42.59%  \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.329717  [    0/175341]\n",
      "loss: 0.289774  [ 1600/175341]\n",
      "loss: 0.265701  [ 3200/175341]\n",
      "loss: 0.461156  [ 4800/175341]\n",
      "loss: 0.361089  [ 6400/175341]\n",
      "loss: 0.299269  [ 8000/175341]\n",
      "loss: 0.466790  [ 9600/175341]\n",
      "loss: 0.337971  [11200/175341]\n",
      "loss: 0.251001  [12800/175341]\n",
      "loss: 0.510891  [14400/175341]\n",
      "loss: 0.629847  [16000/175341]\n",
      "loss: 0.417939  [17600/175341]\n",
      "loss: 0.423393  [19200/175341]\n",
      "loss: 0.738206  [20800/175341]\n",
      "loss: 0.290994  [22400/175341]\n",
      "loss: 0.123360  [24000/175341]\n",
      "loss: 0.242058  [25600/175341]\n",
      "loss: 0.499762  [27200/175341]\n",
      "loss: 0.801680  [28800/175341]\n",
      "loss: 0.566270  [30400/175341]\n",
      "loss: 0.538253  [32000/175341]\n",
      "loss: 0.574535  [33600/175341]\n",
      "loss: 0.175997  [35200/175341]\n",
      "loss: 0.368864  [36800/175341]\n",
      "loss: 0.445282  [38400/175341]\n",
      "loss: 0.669880  [40000/175341]\n",
      "loss: 0.326495  [41600/175341]\n",
      "loss: 0.611939  [43200/175341]\n",
      "loss: 0.849154  [44800/175341]\n",
      "loss: 0.923552  [46400/175341]\n",
      "loss: 0.550225  [48000/175341]\n",
      "loss: 0.525609  [49600/175341]\n",
      "loss: 0.540271  [51200/175341]\n",
      "loss: 0.337266  [52800/175341]\n",
      "loss: 0.327442  [54400/175341]\n",
      "loss: 0.363106  [56000/175341]\n",
      "loss: 0.581384  [57600/175341]\n",
      "loss: 0.584433  [59200/175341]\n",
      "loss: 0.665670  [60800/175341]\n",
      "loss: 0.471588  [62400/175341]\n",
      "loss: 0.386565  [64000/175341]\n",
      "loss: 0.496751  [65600/175341]\n",
      "loss: 0.455942  [67200/175341]\n",
      "loss: 0.487290  [68800/175341]\n",
      "loss: 0.168995  [70400/175341]\n",
      "loss: 0.261491  [72000/175341]\n",
      "loss: 1.047550  [73600/175341]\n",
      "loss: 0.594671  [75200/175341]\n",
      "loss: 0.190356  [76800/175341]\n",
      "loss: 0.443362  [78400/175341]\n",
      "loss: 0.427031  [80000/175341]\n",
      "loss: 0.659859  [81600/175341]\n",
      "loss: 0.416524  [83200/175341]\n",
      "loss: 0.718051  [84800/175341]\n",
      "loss: 0.353068  [86400/175341]\n",
      "loss: 0.679511  [88000/175341]\n",
      "loss: 0.340002  [89600/175341]\n",
      "loss: 0.151806  [91200/175341]\n",
      "loss: 0.153943  [92800/175341]\n",
      "loss: 0.588279  [94400/175341]\n",
      "loss: 0.314677  [96000/175341]\n",
      "loss: 0.148940  [97600/175341]\n",
      "loss: 0.424903  [99200/175341]\n",
      "loss: 0.273180  [100800/175341]\n",
      "loss: 0.391198  [102400/175341]\n",
      "loss: 0.169310  [104000/175341]\n",
      "loss: 0.666586  [105600/175341]\n",
      "loss: 0.904200  [107200/175341]\n",
      "loss: 0.214114  [108800/175341]\n",
      "loss: 0.267460  [110400/175341]\n",
      "loss: 0.510580  [112000/175341]\n",
      "loss: 0.357071  [113600/175341]\n",
      "loss: 0.633446  [115200/175341]\n",
      "loss: 0.316372  [116800/175341]\n",
      "loss: 0.496786  [118400/175341]\n",
      "loss: 0.427603  [120000/175341]\n",
      "loss: 0.255438  [121600/175341]\n",
      "loss: 0.581622  [123200/175341]\n",
      "loss: 0.585059  [124800/175341]\n",
      "loss: 0.381350  [126400/175341]\n",
      "loss: 0.454837  [128000/175341]\n",
      "loss: 0.420416  [129600/175341]\n",
      "loss: 0.318136  [131200/175341]\n",
      "loss: 0.412085  [132800/175341]\n",
      "loss: 0.608175  [134400/175341]\n",
      "loss: 0.210541  [136000/175341]\n",
      "loss: 0.376683  [137600/175341]\n",
      "loss: 0.843653  [139200/175341]\n",
      "loss: 0.584700  [140800/175341]\n",
      "loss: 0.574346  [142400/175341]\n",
      "loss: 0.507843  [144000/175341]\n",
      "loss: 0.189051  [145600/175341]\n",
      "loss: 1.006660  [147200/175341]\n",
      "loss: 0.438568  [148800/175341]\n",
      "loss: 0.653511  [150400/175341]\n",
      "loss: 0.641317  [152000/175341]\n",
      "loss: 0.949332  [153600/175341]\n",
      "loss: 0.446088  [155200/175341]\n",
      "loss: 1.117891  [156800/175341]\n",
      "loss: 0.299180  [158400/175341]\n",
      "loss: 0.446433  [160000/175341]\n",
      "loss: 0.384877  [161600/175341]\n",
      "loss: 0.387287  [163200/175341]\n",
      "loss: 0.225614  [164800/175341]\n",
      "loss: 0.516968  [166400/175341]\n",
      "loss: 0.549488  [168000/175341]\n",
      "loss: 0.678875  [169600/175341]\n",
      "loss: 1.112877  [171200/175341]\n",
      "loss: 0.450381  [172800/175341]\n",
      "loss: 0.280873  [174400/175341]\n",
      "Train Accuracy: 82.0635%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.544321, F1-score: 77.20%, Macro_F1-Score:  42.00%  \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.629802  [    0/175341]\n",
      "loss: 0.291019  [ 1600/175341]\n",
      "loss: 0.439265  [ 3200/175341]\n",
      "loss: 0.695668  [ 4800/175341]\n",
      "loss: 0.395203  [ 6400/175341]\n",
      "loss: 0.497702  [ 8000/175341]\n",
      "loss: 0.744005  [ 9600/175341]\n",
      "loss: 0.242109  [11200/175341]\n",
      "loss: 0.898669  [12800/175341]\n",
      "loss: 0.697178  [14400/175341]\n",
      "loss: 0.373341  [16000/175341]\n",
      "loss: 0.255068  [17600/175341]\n",
      "loss: 0.279531  [19200/175341]\n",
      "loss: 0.246261  [20800/175341]\n",
      "loss: 0.561115  [22400/175341]\n",
      "loss: 0.375674  [24000/175341]\n",
      "loss: 0.630614  [25600/175341]\n",
      "loss: 0.233253  [27200/175341]\n",
      "loss: 0.331440  [28800/175341]\n",
      "loss: 0.189580  [30400/175341]\n",
      "loss: 0.680826  [32000/175341]\n",
      "loss: 0.699035  [33600/175341]\n",
      "loss: 0.477269  [35200/175341]\n",
      "loss: 0.165244  [36800/175341]\n",
      "loss: 0.361942  [38400/175341]\n",
      "loss: 0.341579  [40000/175341]\n",
      "loss: 0.352532  [41600/175341]\n",
      "loss: 0.446469  [43200/175341]\n",
      "loss: 0.348302  [44800/175341]\n",
      "loss: 0.580728  [46400/175341]\n",
      "loss: 0.540336  [48000/175341]\n",
      "loss: 0.309044  [49600/175341]\n",
      "loss: 0.539168  [51200/175341]\n",
      "loss: 0.640519  [52800/175341]\n",
      "loss: 0.126461  [54400/175341]\n",
      "loss: 0.175677  [56000/175341]\n",
      "loss: 0.487322  [57600/175341]\n",
      "loss: 0.642385  [59200/175341]\n",
      "loss: 0.281082  [60800/175341]\n",
      "loss: 0.513153  [62400/175341]\n",
      "loss: 0.144701  [64000/175341]\n",
      "loss: 0.440564  [65600/175341]\n",
      "loss: 0.522157  [67200/175341]\n",
      "loss: 0.381012  [68800/175341]\n",
      "loss: 0.379745  [70400/175341]\n",
      "loss: 0.569374  [72000/175341]\n",
      "loss: 0.312051  [73600/175341]\n",
      "loss: 0.340152  [75200/175341]\n",
      "loss: 0.220294  [76800/175341]\n",
      "loss: 0.297256  [78400/175341]\n",
      "loss: 0.555684  [80000/175341]\n",
      "loss: 0.359190  [81600/175341]\n",
      "loss: 0.720239  [83200/175341]\n",
      "loss: 0.144102  [84800/175341]\n",
      "loss: 0.411455  [86400/175341]\n",
      "loss: 0.357586  [88000/175341]\n",
      "loss: 0.548812  [89600/175341]\n",
      "loss: 0.306976  [91200/175341]\n",
      "loss: 0.360982  [92800/175341]\n",
      "loss: 0.567837  [94400/175341]\n",
      "loss: 0.336314  [96000/175341]\n",
      "loss: 0.109826  [97600/175341]\n",
      "loss: 0.196341  [99200/175341]\n",
      "loss: 0.245565  [100800/175341]\n",
      "loss: 0.747914  [102400/175341]\n",
      "loss: 0.486791  [104000/175341]\n",
      "loss: 0.459381  [105600/175341]\n",
      "loss: 0.293605  [107200/175341]\n",
      "loss: 0.808007  [108800/175341]\n",
      "loss: 0.449940  [110400/175341]\n",
      "loss: 0.586589  [112000/175341]\n",
      "loss: 0.470382  [113600/175341]\n",
      "loss: 0.385481  [115200/175341]\n",
      "loss: 0.423098  [116800/175341]\n",
      "loss: 0.793712  [118400/175341]\n",
      "loss: 0.873520  [120000/175341]\n",
      "loss: 0.173206  [121600/175341]\n",
      "loss: 0.313388  [123200/175341]\n",
      "loss: 0.155394  [124800/175341]\n",
      "loss: 0.332719  [126400/175341]\n",
      "loss: 0.320926  [128000/175341]\n",
      "loss: 0.701121  [129600/175341]\n",
      "loss: 0.267796  [131200/175341]\n",
      "loss: 0.867732  [132800/175341]\n",
      "loss: 0.587531  [134400/175341]\n",
      "loss: 0.402731  [136000/175341]\n",
      "loss: 0.577483  [137600/175341]\n",
      "loss: 0.617994  [139200/175341]\n",
      "loss: 0.507411  [140800/175341]\n",
      "loss: 0.480058  [142400/175341]\n",
      "loss: 0.476852  [144000/175341]\n",
      "loss: 0.397340  [145600/175341]\n",
      "loss: 0.250699  [147200/175341]\n",
      "loss: 0.145067  [148800/175341]\n",
      "loss: 0.322497  [150400/175341]\n",
      "loss: 0.398716  [152000/175341]\n",
      "loss: 0.543918  [153600/175341]\n",
      "loss: 0.275603  [155200/175341]\n",
      "loss: 0.425776  [156800/175341]\n",
      "loss: 0.335013  [158400/175341]\n",
      "loss: 0.582410  [160000/175341]\n",
      "loss: 0.592575  [161600/175341]\n",
      "loss: 0.186507  [163200/175341]\n",
      "loss: 0.347889  [164800/175341]\n",
      "loss: 0.531360  [166400/175341]\n",
      "loss: 0.438245  [168000/175341]\n",
      "loss: 0.471823  [169600/175341]\n",
      "loss: 0.215094  [171200/175341]\n",
      "loss: 0.636769  [172800/175341]\n",
      "loss: 0.436615  [174400/175341]\n",
      "Train Accuracy: 82.0053%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.572570, F1-score: 75.61%, Macro_F1-Score:  42.03%  \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.631351  [    0/175341]\n",
      "loss: 0.329531  [ 1600/175341]\n",
      "loss: 0.700634  [ 3200/175341]\n",
      "loss: 0.579945  [ 4800/175341]\n",
      "loss: 0.727560  [ 6400/175341]\n",
      "loss: 0.726015  [ 8000/175341]\n",
      "loss: 0.762036  [ 9600/175341]\n",
      "loss: 0.498487  [11200/175341]\n",
      "loss: 0.437273  [12800/175341]\n",
      "loss: 0.424059  [14400/175341]\n",
      "loss: 0.356802  [16000/175341]\n",
      "loss: 0.352634  [17600/175341]\n",
      "loss: 0.425331  [19200/175341]\n",
      "loss: 0.665797  [20800/175341]\n",
      "loss: 0.593464  [22400/175341]\n",
      "loss: 0.217187  [24000/175341]\n",
      "loss: 0.387450  [25600/175341]\n",
      "loss: 0.490432  [27200/175341]\n",
      "loss: 0.500754  [28800/175341]\n",
      "loss: 0.393122  [30400/175341]\n",
      "loss: 0.405128  [32000/175341]\n",
      "loss: 0.494534  [33600/175341]\n",
      "loss: 0.479333  [35200/175341]\n",
      "loss: 0.702877  [36800/175341]\n",
      "loss: 0.185693  [38400/175341]\n",
      "loss: 0.666685  [40000/175341]\n",
      "loss: 0.676735  [41600/175341]\n",
      "loss: 0.436975  [43200/175341]\n",
      "loss: 0.520962  [44800/175341]\n",
      "loss: 0.204101  [46400/175341]\n",
      "loss: 0.276718  [48000/175341]\n",
      "loss: 0.379408  [49600/175341]\n",
      "loss: 0.665244  [51200/175341]\n",
      "loss: 0.287169  [52800/175341]\n",
      "loss: 0.401457  [54400/175341]\n",
      "loss: 0.612594  [56000/175341]\n",
      "loss: 0.497695  [57600/175341]\n",
      "loss: 0.623258  [59200/175341]\n",
      "loss: 0.274989  [60800/175341]\n",
      "loss: 0.488100  [62400/175341]\n",
      "loss: 0.544345  [64000/175341]\n",
      "loss: 0.331632  [65600/175341]\n",
      "loss: 0.144692  [67200/175341]\n",
      "loss: 0.575854  [68800/175341]\n",
      "loss: 0.694388  [70400/175341]\n",
      "loss: 0.338368  [72000/175341]\n",
      "loss: 0.621406  [73600/175341]\n",
      "loss: 0.277608  [75200/175341]\n",
      "loss: 0.284480  [76800/175341]\n",
      "loss: 0.422647  [78400/175341]\n",
      "loss: 0.308716  [80000/175341]\n",
      "loss: 0.282114  [81600/175341]\n",
      "loss: 0.214086  [83200/175341]\n",
      "loss: 0.548558  [84800/175341]\n",
      "loss: 0.127685  [86400/175341]\n",
      "loss: 0.663086  [88000/175341]\n",
      "loss: 0.348861  [89600/175341]\n",
      "loss: 0.610644  [91200/175341]\n",
      "loss: 0.181610  [92800/175341]\n",
      "loss: 0.486937  [94400/175341]\n",
      "loss: 0.353361  [96000/175341]\n",
      "loss: 0.289159  [97600/175341]\n",
      "loss: 0.653824  [99200/175341]\n",
      "loss: 0.147849  [100800/175341]\n",
      "loss: 0.481702  [102400/175341]\n",
      "loss: 0.754172  [104000/175341]\n",
      "loss: 0.722659  [105600/175341]\n",
      "loss: 0.523516  [107200/175341]\n",
      "loss: 0.318196  [108800/175341]\n",
      "loss: 0.384800  [110400/175341]\n",
      "loss: 0.759039  [112000/175341]\n",
      "loss: 0.185764  [113600/175341]\n",
      "loss: 0.246470  [115200/175341]\n",
      "loss: 0.769714  [116800/175341]\n",
      "loss: 0.948886  [118400/175341]\n",
      "loss: 0.452968  [120000/175341]\n",
      "loss: 0.353493  [121600/175341]\n",
      "loss: 0.428092  [123200/175341]\n",
      "loss: 0.708301  [124800/175341]\n",
      "loss: 0.337566  [126400/175341]\n",
      "loss: 0.476122  [128000/175341]\n",
      "loss: 0.610677  [129600/175341]\n",
      "loss: 0.328715  [131200/175341]\n",
      "loss: 0.412588  [132800/175341]\n",
      "loss: 0.496285  [134400/175341]\n",
      "loss: 0.229327  [136000/175341]\n",
      "loss: 0.312289  [137600/175341]\n",
      "loss: 0.548213  [139200/175341]\n",
      "loss: 0.402971  [140800/175341]\n",
      "loss: 0.336742  [142400/175341]\n",
      "loss: 0.290470  [144000/175341]\n",
      "loss: 0.428878  [145600/175341]\n",
      "loss: 0.186038  [147200/175341]\n",
      "loss: 0.530627  [148800/175341]\n",
      "loss: 0.362682  [150400/175341]\n",
      "loss: 0.674639  [152000/175341]\n",
      "loss: 0.474546  [153600/175341]\n",
      "loss: 0.634362  [155200/175341]\n",
      "loss: 0.780681  [156800/175341]\n",
      "loss: 0.443415  [158400/175341]\n",
      "loss: 0.452010  [160000/175341]\n",
      "loss: 0.608087  [161600/175341]\n",
      "loss: 0.104890  [163200/175341]\n",
      "loss: 0.496854  [164800/175341]\n",
      "loss: 0.678063  [166400/175341]\n",
      "loss: 0.212129  [168000/175341]\n",
      "loss: 0.470209  [169600/175341]\n",
      "loss: 0.327733  [171200/175341]\n",
      "loss: 0.954554  [172800/175341]\n",
      "loss: 0.703674  [174400/175341]\n",
      "Train Accuracy: 82.0476%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.570431, F1-score: 75.46%, Macro_F1-Score:  41.49%  \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.488501  [    0/175341]\n",
      "loss: 0.362463  [ 1600/175341]\n",
      "loss: 0.297483  [ 3200/175341]\n",
      "loss: 0.399483  [ 4800/175341]\n",
      "loss: 0.521873  [ 6400/175341]\n",
      "loss: 0.323506  [ 8000/175341]\n",
      "loss: 0.444049  [ 9600/175341]\n",
      "loss: 0.654890  [11200/175341]\n",
      "loss: 0.670471  [12800/175341]\n",
      "loss: 0.246637  [14400/175341]\n",
      "loss: 0.632341  [16000/175341]\n",
      "loss: 0.324652  [17600/175341]\n",
      "loss: 0.457784  [19200/175341]\n",
      "loss: 0.584654  [20800/175341]\n",
      "loss: 0.254943  [22400/175341]\n",
      "loss: 0.460744  [24000/175341]\n",
      "loss: 0.500182  [25600/175341]\n",
      "loss: 0.439900  [27200/175341]\n",
      "loss: 0.376438  [28800/175341]\n",
      "loss: 0.468116  [30400/175341]\n",
      "loss: 0.289978  [32000/175341]\n",
      "loss: 0.238743  [33600/175341]\n",
      "loss: 0.510112  [35200/175341]\n",
      "loss: 0.356108  [36800/175341]\n",
      "loss: 0.478051  [38400/175341]\n",
      "loss: 0.253728  [40000/175341]\n",
      "loss: 0.768598  [41600/175341]\n",
      "loss: 0.415229  [43200/175341]\n",
      "loss: 0.437920  [44800/175341]\n",
      "loss: 0.588547  [46400/175341]\n",
      "loss: 0.298844  [48000/175341]\n",
      "loss: 0.353721  [49600/175341]\n",
      "loss: 0.065003  [51200/175341]\n",
      "loss: 0.235925  [52800/175341]\n",
      "loss: 0.212966  [54400/175341]\n",
      "loss: 0.164049  [56000/175341]\n",
      "loss: 0.635367  [57600/175341]\n",
      "loss: 0.432387  [59200/175341]\n",
      "loss: 0.604660  [60800/175341]\n",
      "loss: 0.311292  [62400/175341]\n",
      "loss: 0.748791  [64000/175341]\n",
      "loss: 0.301839  [65600/175341]\n",
      "loss: 0.234070  [67200/175341]\n",
      "loss: 0.335956  [68800/175341]\n",
      "loss: 0.340791  [70400/175341]\n",
      "loss: 0.358772  [72000/175341]\n",
      "loss: 0.381366  [73600/175341]\n",
      "loss: 0.510991  [75200/175341]\n",
      "loss: 0.493661  [76800/175341]\n",
      "loss: 0.535555  [78400/175341]\n",
      "loss: 0.584120  [80000/175341]\n",
      "loss: 0.294096  [81600/175341]\n",
      "loss: 0.514892  [83200/175341]\n",
      "loss: 0.663532  [84800/175341]\n",
      "loss: 0.301051  [86400/175341]\n",
      "loss: 0.108425  [88000/175341]\n",
      "loss: 0.397856  [89600/175341]\n",
      "loss: 0.444027  [91200/175341]\n",
      "loss: 0.390682  [92800/175341]\n",
      "loss: 0.637159  [94400/175341]\n",
      "loss: 0.332109  [96000/175341]\n",
      "loss: 0.255747  [97600/175341]\n",
      "loss: 0.186339  [99200/175341]\n",
      "loss: 0.578919  [100800/175341]\n",
      "loss: 0.278501  [102400/175341]\n",
      "loss: 0.298312  [104000/175341]\n",
      "loss: 0.373682  [105600/175341]\n",
      "loss: 0.528096  [107200/175341]\n",
      "loss: 0.161310  [108800/175341]\n",
      "loss: 0.243430  [110400/175341]\n",
      "loss: 0.362449  [112000/175341]\n",
      "loss: 0.609669  [113600/175341]\n",
      "loss: 0.408803  [115200/175341]\n",
      "loss: 0.184997  [116800/175341]\n",
      "loss: 0.805215  [118400/175341]\n",
      "loss: 0.389776  [120000/175341]\n",
      "loss: 0.297342  [121600/175341]\n",
      "loss: 0.472811  [123200/175341]\n",
      "loss: 0.176159  [124800/175341]\n",
      "loss: 0.637750  [126400/175341]\n",
      "loss: 0.583920  [128000/175341]\n",
      "loss: 0.321077  [129600/175341]\n",
      "loss: 0.376279  [131200/175341]\n",
      "loss: 0.387400  [132800/175341]\n",
      "loss: 0.366066  [134400/175341]\n",
      "loss: 0.352226  [136000/175341]\n",
      "loss: 0.554401  [137600/175341]\n",
      "loss: 0.336656  [139200/175341]\n",
      "loss: 0.250845  [140800/175341]\n",
      "loss: 0.383451  [142400/175341]\n",
      "loss: 0.272846  [144000/175341]\n",
      "loss: 0.337936  [145600/175341]\n",
      "loss: 0.319384  [147200/175341]\n",
      "loss: 0.448920  [148800/175341]\n",
      "loss: 0.275940  [150400/175341]\n",
      "loss: 0.398104  [152000/175341]\n",
      "loss: 0.259159  [153600/175341]\n",
      "loss: 0.149543  [155200/175341]\n",
      "loss: 0.223511  [156800/175341]\n",
      "loss: 0.494772  [158400/175341]\n",
      "loss: 0.314796  [160000/175341]\n",
      "loss: 0.880370  [161600/175341]\n",
      "loss: 0.459267  [163200/175341]\n",
      "loss: 0.462992  [164800/175341]\n",
      "loss: 0.241131  [166400/175341]\n",
      "loss: 0.520296  [168000/175341]\n",
      "loss: 0.616290  [169600/175341]\n",
      "loss: 0.559384  [171200/175341]\n",
      "loss: 0.204050  [172800/175341]\n",
      "loss: 0.470551  [174400/175341]\n",
      "Train Accuracy: 82.0082%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.544925, F1-score: 77.49%, Macro_F1-Score:  42.36%  \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.633450  [    0/175341]\n",
      "loss: 0.767502  [ 1600/175341]\n",
      "loss: 0.355478  [ 3200/175341]\n",
      "loss: 0.333027  [ 4800/175341]\n",
      "loss: 0.503371  [ 6400/175341]\n",
      "loss: 0.488708  [ 8000/175341]\n",
      "loss: 0.288326  [ 9600/175341]\n",
      "loss: 0.311438  [11200/175341]\n",
      "loss: 0.438811  [12800/175341]\n",
      "loss: 0.478308  [14400/175341]\n",
      "loss: 0.738655  [16000/175341]\n",
      "loss: 0.609909  [17600/175341]\n",
      "loss: 0.811367  [19200/175341]\n",
      "loss: 0.357916  [20800/175341]\n",
      "loss: 0.482669  [22400/175341]\n",
      "loss: 0.774314  [24000/175341]\n",
      "loss: 0.248315  [25600/175341]\n",
      "loss: 0.712506  [27200/175341]\n",
      "loss: 0.394452  [28800/175341]\n",
      "loss: 0.457774  [30400/175341]\n",
      "loss: 0.474622  [32000/175341]\n",
      "loss: 0.413930  [33600/175341]\n",
      "loss: 0.220333  [35200/175341]\n",
      "loss: 0.514728  [36800/175341]\n",
      "loss: 0.363758  [38400/175341]\n",
      "loss: 0.612441  [40000/175341]\n",
      "loss: 0.417892  [41600/175341]\n",
      "loss: 0.647301  [43200/175341]\n",
      "loss: 0.432384  [44800/175341]\n",
      "loss: 0.439750  [46400/175341]\n",
      "loss: 0.274951  [48000/175341]\n",
      "loss: 0.554768  [49600/175341]\n",
      "loss: 0.570222  [51200/175341]\n",
      "loss: 0.553609  [52800/175341]\n",
      "loss: 0.317407  [54400/175341]\n",
      "loss: 0.479246  [56000/175341]\n",
      "loss: 0.305836  [57600/175341]\n",
      "loss: 0.373231  [59200/175341]\n",
      "loss: 0.514173  [60800/175341]\n",
      "loss: 0.732897  [62400/175341]\n",
      "loss: 0.235398  [64000/175341]\n",
      "loss: 0.333985  [65600/175341]\n",
      "loss: 0.206351  [67200/175341]\n",
      "loss: 0.340607  [68800/175341]\n",
      "loss: 0.316571  [70400/175341]\n",
      "loss: 0.371535  [72000/175341]\n",
      "loss: 0.391612  [73600/175341]\n",
      "loss: 0.373230  [75200/175341]\n",
      "loss: 0.569646  [76800/175341]\n",
      "loss: 0.718885  [78400/175341]\n",
      "loss: 0.645794  [80000/175341]\n",
      "loss: 0.194806  [81600/175341]\n",
      "loss: 0.658689  [83200/175341]\n",
      "loss: 0.237753  [84800/175341]\n",
      "loss: 0.265297  [86400/175341]\n",
      "loss: 0.637927  [88000/175341]\n",
      "loss: 0.654874  [89600/175341]\n",
      "loss: 0.373466  [91200/175341]\n",
      "loss: 0.580228  [92800/175341]\n",
      "loss: 0.543812  [94400/175341]\n",
      "loss: 0.653830  [96000/175341]\n",
      "loss: 0.148775  [97600/175341]\n",
      "loss: 0.342508  [99200/175341]\n",
      "loss: 0.352935  [100800/175341]\n",
      "loss: 0.297960  [102400/175341]\n",
      "loss: 0.093094  [104000/175341]\n",
      "loss: 0.313202  [105600/175341]\n",
      "loss: 0.963639  [107200/175341]\n",
      "loss: 0.260412  [108800/175341]\n",
      "loss: 0.110086  [110400/175341]\n",
      "loss: 0.306881  [112000/175341]\n",
      "loss: 0.463407  [113600/175341]\n",
      "loss: 0.402935  [115200/175341]\n",
      "loss: 0.382352  [116800/175341]\n",
      "loss: 0.449391  [118400/175341]\n",
      "loss: 0.482725  [120000/175341]\n",
      "loss: 0.386331  [121600/175341]\n",
      "loss: 0.491233  [123200/175341]\n",
      "loss: 0.374640  [124800/175341]\n",
      "loss: 0.713862  [126400/175341]\n",
      "loss: 0.322907  [128000/175341]\n",
      "loss: 0.275862  [129600/175341]\n",
      "loss: 0.582341  [131200/175341]\n",
      "loss: 0.661073  [132800/175341]\n",
      "loss: 0.452729  [134400/175341]\n",
      "loss: 0.817505  [136000/175341]\n",
      "loss: 0.568363  [137600/175341]\n",
      "loss: 0.358362  [139200/175341]\n",
      "loss: 0.400150  [140800/175341]\n",
      "loss: 0.465121  [142400/175341]\n",
      "loss: 0.303871  [144000/175341]\n",
      "loss: 0.718485  [145600/175341]\n",
      "loss: 0.382087  [147200/175341]\n",
      "loss: 0.184331  [148800/175341]\n",
      "loss: 0.230278  [150400/175341]\n",
      "loss: 0.779858  [152000/175341]\n",
      "loss: 0.283349  [153600/175341]\n",
      "loss: 0.453847  [155200/175341]\n",
      "loss: 0.531637  [156800/175341]\n",
      "loss: 0.233255  [158400/175341]\n",
      "loss: 0.298784  [160000/175341]\n",
      "loss: 0.563862  [161600/175341]\n",
      "loss: 0.442783  [163200/175341]\n",
      "loss: 0.135001  [164800/175341]\n",
      "loss: 0.359350  [166400/175341]\n",
      "loss: 0.548283  [168000/175341]\n",
      "loss: 0.448044  [169600/175341]\n",
      "loss: 0.671875  [171200/175341]\n",
      "loss: 0.159238  [172800/175341]\n",
      "loss: 0.156934  [174400/175341]\n",
      "Train Accuracy: 82.1109%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.562536, F1-score: 76.42%, Macro_F1-Score:  41.92%  \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.251233  [    0/175341]\n",
      "loss: 0.266994  [ 1600/175341]\n",
      "loss: 0.548763  [ 3200/175341]\n",
      "loss: 0.503975  [ 4800/175341]\n",
      "loss: 0.465866  [ 6400/175341]\n",
      "loss: 0.174063  [ 8000/175341]\n",
      "loss: 0.607256  [ 9600/175341]\n",
      "loss: 0.312641  [11200/175341]\n",
      "loss: 0.230850  [12800/175341]\n",
      "loss: 0.453637  [14400/175341]\n",
      "loss: 0.178811  [16000/175341]\n",
      "loss: 0.236739  [17600/175341]\n",
      "loss: 0.272562  [19200/175341]\n",
      "loss: 0.523903  [20800/175341]\n",
      "loss: 0.289735  [22400/175341]\n",
      "loss: 0.690091  [24000/175341]\n",
      "loss: 0.368840  [25600/175341]\n",
      "loss: 0.614283  [27200/175341]\n",
      "loss: 0.470063  [28800/175341]\n",
      "loss: 0.332157  [30400/175341]\n",
      "loss: 0.728954  [32000/175341]\n",
      "loss: 0.248347  [33600/175341]\n",
      "loss: 0.481225  [35200/175341]\n",
      "loss: 0.290721  [36800/175341]\n",
      "loss: 0.300062  [38400/175341]\n",
      "loss: 0.373070  [40000/175341]\n",
      "loss: 0.514636  [41600/175341]\n",
      "loss: 0.315727  [43200/175341]\n",
      "loss: 0.551838  [44800/175341]\n",
      "loss: 0.404494  [46400/175341]\n",
      "loss: 0.412793  [48000/175341]\n",
      "loss: 0.842469  [49600/175341]\n",
      "loss: 0.161956  [51200/175341]\n",
      "loss: 0.401980  [52800/175341]\n",
      "loss: 0.360377  [54400/175341]\n",
      "loss: 0.554091  [56000/175341]\n",
      "loss: 0.380735  [57600/175341]\n",
      "loss: 0.408837  [59200/175341]\n",
      "loss: 0.257838  [60800/175341]\n",
      "loss: 0.590727  [62400/175341]\n",
      "loss: 0.348818  [64000/175341]\n",
      "loss: 0.284220  [65600/175341]\n",
      "loss: 0.323875  [67200/175341]\n",
      "loss: 0.463611  [68800/175341]\n",
      "loss: 0.334449  [70400/175341]\n",
      "loss: 0.428581  [72000/175341]\n",
      "loss: 0.544662  [73600/175341]\n",
      "loss: 0.406818  [75200/175341]\n",
      "loss: 0.206567  [76800/175341]\n",
      "loss: 0.670138  [78400/175341]\n",
      "loss: 0.320373  [80000/175341]\n",
      "loss: 0.596615  [81600/175341]\n",
      "loss: 0.695641  [83200/175341]\n",
      "loss: 0.238858  [84800/175341]\n",
      "loss: 0.439338  [86400/175341]\n",
      "loss: 0.505033  [88000/175341]\n",
      "loss: 0.351165  [89600/175341]\n",
      "loss: 0.321848  [91200/175341]\n",
      "loss: 0.351976  [92800/175341]\n",
      "loss: 0.399867  [94400/175341]\n",
      "loss: 0.249435  [96000/175341]\n",
      "loss: 0.487097  [97600/175341]\n",
      "loss: 0.569187  [99200/175341]\n",
      "loss: 0.286889  [100800/175341]\n",
      "loss: 0.281794  [102400/175341]\n",
      "loss: 0.311552  [104000/175341]\n",
      "loss: 0.524774  [105600/175341]\n",
      "loss: 0.470285  [107200/175341]\n",
      "loss: 0.343381  [108800/175341]\n",
      "loss: 0.411390  [110400/175341]\n",
      "loss: 0.321745  [112000/175341]\n",
      "loss: 0.340454  [113600/175341]\n",
      "loss: 0.497204  [115200/175341]\n",
      "loss: 0.362835  [116800/175341]\n",
      "loss: 0.246508  [118400/175341]\n",
      "loss: 0.400748  [120000/175341]\n",
      "loss: 0.242726  [121600/175341]\n",
      "loss: 0.369172  [123200/175341]\n",
      "loss: 0.288087  [124800/175341]\n",
      "loss: 0.478588  [126400/175341]\n",
      "loss: 0.302754  [128000/175341]\n",
      "loss: 0.379726  [129600/175341]\n",
      "loss: 0.817389  [131200/175341]\n",
      "loss: 0.096573  [132800/175341]\n",
      "loss: 0.381414  [134400/175341]\n",
      "loss: 0.892378  [136000/175341]\n",
      "loss: 0.859880  [137600/175341]\n",
      "loss: 0.059211  [139200/175341]\n",
      "loss: 0.710845  [140800/175341]\n",
      "loss: 0.566811  [142400/175341]\n",
      "loss: 0.349342  [144000/175341]\n",
      "loss: 0.145146  [145600/175341]\n",
      "loss: 0.442932  [147200/175341]\n",
      "loss: 0.403075  [148800/175341]\n",
      "loss: 0.354771  [150400/175341]\n",
      "loss: 0.720104  [152000/175341]\n",
      "loss: 0.471849  [153600/175341]\n",
      "loss: 0.504368  [155200/175341]\n",
      "loss: 0.558738  [156800/175341]\n",
      "loss: 0.345109  [158400/175341]\n",
      "loss: 0.595995  [160000/175341]\n",
      "loss: 0.323958  [161600/175341]\n",
      "loss: 0.703389  [163200/175341]\n",
      "loss: 0.314147  [164800/175341]\n",
      "loss: 0.666942  [166400/175341]\n",
      "loss: 0.264907  [168000/175341]\n",
      "loss: 0.192176  [169600/175341]\n",
      "loss: 0.213363  [171200/175341]\n",
      "loss: 0.366696  [172800/175341]\n",
      "loss: 0.295481  [174400/175341]\n",
      "Train Accuracy: 82.0784%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.553563, F1-score: 77.06%, Macro_F1-Score:  42.71%  \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.279050  [    0/175341]\n",
      "loss: 0.418214  [ 1600/175341]\n",
      "loss: 0.207733  [ 3200/175341]\n",
      "loss: 0.148564  [ 4800/175341]\n",
      "loss: 0.717704  [ 6400/175341]\n",
      "loss: 0.813423  [ 8000/175341]\n",
      "loss: 0.225883  [ 9600/175341]\n",
      "loss: 0.300820  [11200/175341]\n",
      "loss: 0.235407  [12800/175341]\n",
      "loss: 0.405682  [14400/175341]\n",
      "loss: 0.388083  [16000/175341]\n",
      "loss: 0.380380  [17600/175341]\n",
      "loss: 0.376700  [19200/175341]\n",
      "loss: 0.464112  [20800/175341]\n",
      "loss: 0.769753  [22400/175341]\n",
      "loss: 0.527305  [24000/175341]\n",
      "loss: 0.542861  [25600/175341]\n",
      "loss: 0.265083  [27200/175341]\n",
      "loss: 0.377454  [28800/175341]\n",
      "loss: 0.689416  [30400/175341]\n",
      "loss: 0.504186  [32000/175341]\n",
      "loss: 0.524147  [33600/175341]\n",
      "loss: 0.270860  [35200/175341]\n",
      "loss: 0.712947  [36800/175341]\n",
      "loss: 0.480449  [38400/175341]\n",
      "loss: 0.621353  [40000/175341]\n",
      "loss: 0.444046  [41600/175341]\n",
      "loss: 0.667572  [43200/175341]\n",
      "loss: 0.815781  [44800/175341]\n",
      "loss: 0.565471  [46400/175341]\n",
      "loss: 0.487498  [48000/175341]\n",
      "loss: 0.449191  [49600/175341]\n",
      "loss: 0.436022  [51200/175341]\n",
      "loss: 0.317891  [52800/175341]\n",
      "loss: 0.296028  [54400/175341]\n",
      "loss: 0.809712  [56000/175341]\n",
      "loss: 0.657034  [57600/175341]\n",
      "loss: 0.523147  [59200/175341]\n",
      "loss: 0.630223  [60800/175341]\n",
      "loss: 0.494272  [62400/175341]\n",
      "loss: 0.277335  [64000/175341]\n",
      "loss: 0.485898  [65600/175341]\n",
      "loss: 0.373285  [67200/175341]\n",
      "loss: 0.502418  [68800/175341]\n",
      "loss: 0.260309  [70400/175341]\n",
      "loss: 0.484690  [72000/175341]\n",
      "loss: 0.529088  [73600/175341]\n",
      "loss: 0.417065  [75200/175341]\n",
      "loss: 0.200394  [76800/175341]\n",
      "loss: 0.276017  [78400/175341]\n",
      "loss: 0.677321  [80000/175341]\n",
      "loss: 0.327264  [81600/175341]\n",
      "loss: 0.395558  [83200/175341]\n",
      "loss: 0.363121  [84800/175341]\n",
      "loss: 0.248262  [86400/175341]\n",
      "loss: 0.469640  [88000/175341]\n",
      "loss: 0.358987  [89600/175341]\n",
      "loss: 0.219733  [91200/175341]\n",
      "loss: 0.922427  [92800/175341]\n",
      "loss: 0.449656  [94400/175341]\n",
      "loss: 0.837451  [96000/175341]\n",
      "loss: 0.608345  [97600/175341]\n",
      "loss: 0.477605  [99200/175341]\n",
      "loss: 0.548774  [100800/175341]\n",
      "loss: 0.539596  [102400/175341]\n",
      "loss: 0.615564  [104000/175341]\n",
      "loss: 0.272875  [105600/175341]\n",
      "loss: 0.673794  [107200/175341]\n",
      "loss: 0.475318  [108800/175341]\n",
      "loss: 0.392676  [110400/175341]\n",
      "loss: 0.724075  [112000/175341]\n",
      "loss: 0.695627  [113600/175341]\n",
      "loss: 0.316295  [115200/175341]\n",
      "loss: 0.514977  [116800/175341]\n",
      "loss: 0.599511  [118400/175341]\n",
      "loss: 0.483894  [120000/175341]\n",
      "loss: 0.363871  [121600/175341]\n",
      "loss: 0.257969  [123200/175341]\n",
      "loss: 0.574388  [124800/175341]\n",
      "loss: 0.423253  [126400/175341]\n",
      "loss: 0.449216  [128000/175341]\n",
      "loss: 0.386730  [129600/175341]\n",
      "loss: 0.258700  [131200/175341]\n",
      "loss: 0.525429  [132800/175341]\n",
      "loss: 0.432458  [134400/175341]\n",
      "loss: 0.333584  [136000/175341]\n",
      "loss: 0.343028  [137600/175341]\n",
      "loss: 0.505140  [139200/175341]\n",
      "loss: 0.311264  [140800/175341]\n",
      "loss: 0.729311  [142400/175341]\n",
      "loss: 0.256454  [144000/175341]\n",
      "loss: 0.445407  [145600/175341]\n",
      "loss: 0.332018  [147200/175341]\n",
      "loss: 0.435876  [148800/175341]\n",
      "loss: 0.508247  [150400/175341]\n",
      "loss: 0.532300  [152000/175341]\n",
      "loss: 0.469667  [153600/175341]\n",
      "loss: 0.464639  [155200/175341]\n",
      "loss: 0.161565  [156800/175341]\n",
      "loss: 0.573338  [158400/175341]\n",
      "loss: 0.092817  [160000/175341]\n",
      "loss: 0.306878  [161600/175341]\n",
      "loss: 0.431715  [163200/175341]\n",
      "loss: 0.500921  [164800/175341]\n",
      "loss: 0.463398  [166400/175341]\n",
      "loss: 0.332353  [168000/175341]\n",
      "loss: 0.365896  [169600/175341]\n",
      "loss: 0.689303  [171200/175341]\n",
      "loss: 0.594934  [172800/175341]\n",
      "loss: 0.620669  [174400/175341]\n",
      "Train Accuracy: 82.0692%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.570843, F1-score: 75.14%, Macro_F1-Score:  41.65%  \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.140922  [    0/175341]\n",
      "loss: 0.259518  [ 1600/175341]\n",
      "loss: 0.297869  [ 3200/175341]\n",
      "loss: 0.790982  [ 4800/175341]\n",
      "loss: 0.447427  [ 6400/175341]\n",
      "loss: 0.359482  [ 8000/175341]\n",
      "loss: 0.334194  [ 9600/175341]\n",
      "loss: 0.371244  [11200/175341]\n",
      "loss: 0.407911  [12800/175341]\n",
      "loss: 0.264344  [14400/175341]\n",
      "loss: 0.385105  [16000/175341]\n",
      "loss: 0.453432  [17600/175341]\n",
      "loss: 0.174832  [19200/175341]\n",
      "loss: 0.498707  [20800/175341]\n",
      "loss: 0.370482  [22400/175341]\n",
      "loss: 0.473058  [24000/175341]\n",
      "loss: 0.316739  [25600/175341]\n",
      "loss: 0.252010  [27200/175341]\n",
      "loss: 0.592282  [28800/175341]\n",
      "loss: 0.281624  [30400/175341]\n",
      "loss: 0.348421  [32000/175341]\n",
      "loss: 0.297808  [33600/175341]\n",
      "loss: 0.435370  [35200/175341]\n",
      "loss: 0.551782  [36800/175341]\n",
      "loss: 0.296961  [38400/175341]\n",
      "loss: 0.332969  [40000/175341]\n",
      "loss: 0.750985  [41600/175341]\n",
      "loss: 0.348372  [43200/175341]\n",
      "loss: 0.548101  [44800/175341]\n",
      "loss: 0.432688  [46400/175341]\n",
      "loss: 0.419989  [48000/175341]\n",
      "loss: 1.056923  [49600/175341]\n",
      "loss: 0.581748  [51200/175341]\n",
      "loss: 0.403489  [52800/175341]\n",
      "loss: 0.675846  [54400/175341]\n",
      "loss: 0.191695  [56000/175341]\n",
      "loss: 1.069591  [57600/175341]\n",
      "loss: 0.352246  [59200/175341]\n",
      "loss: 0.241772  [60800/175341]\n",
      "loss: 0.797773  [62400/175341]\n",
      "loss: 0.680265  [64000/175341]\n",
      "loss: 0.264811  [65600/175341]\n",
      "loss: 0.233215  [67200/175341]\n",
      "loss: 0.666855  [68800/175341]\n",
      "loss: 0.636604  [70400/175341]\n",
      "loss: 0.260561  [72000/175341]\n",
      "loss: 0.467761  [73600/175341]\n",
      "loss: 0.533793  [75200/175341]\n",
      "loss: 0.219630  [76800/175341]\n",
      "loss: 0.493874  [78400/175341]\n",
      "loss: 0.212599  [80000/175341]\n",
      "loss: 0.303712  [81600/175341]\n",
      "loss: 0.144807  [83200/175341]\n",
      "loss: 0.591983  [84800/175341]\n",
      "loss: 0.339593  [86400/175341]\n",
      "loss: 0.518183  [88000/175341]\n",
      "loss: 0.566084  [89600/175341]\n",
      "loss: 0.518354  [91200/175341]\n",
      "loss: 0.399078  [92800/175341]\n",
      "loss: 0.258359  [94400/175341]\n",
      "loss: 0.451755  [96000/175341]\n",
      "loss: 0.563118  [97600/175341]\n",
      "loss: 0.538344  [99200/175341]\n",
      "loss: 0.359035  [100800/175341]\n",
      "loss: 0.475469  [102400/175341]\n",
      "loss: 0.178620  [104000/175341]\n",
      "loss: 0.552842  [105600/175341]\n",
      "loss: 0.269430  [107200/175341]\n",
      "loss: 0.260432  [108800/175341]\n",
      "loss: 0.553970  [110400/175341]\n",
      "loss: 0.315293  [112000/175341]\n",
      "loss: 0.263287  [113600/175341]\n",
      "loss: 0.153932  [115200/175341]\n",
      "loss: 1.067273  [116800/175341]\n",
      "loss: 0.445009  [118400/175341]\n",
      "loss: 0.327202  [120000/175341]\n",
      "loss: 0.250324  [121600/175341]\n",
      "loss: 0.249516  [123200/175341]\n",
      "loss: 0.719095  [124800/175341]\n",
      "loss: 0.403420  [126400/175341]\n",
      "loss: 0.510387  [128000/175341]\n",
      "loss: 0.169982  [129600/175341]\n",
      "loss: 0.688089  [131200/175341]\n",
      "loss: 0.477893  [132800/175341]\n",
      "loss: 0.213071  [134400/175341]\n",
      "loss: 0.449883  [136000/175341]\n",
      "loss: 0.365428  [137600/175341]\n",
      "loss: 0.531466  [139200/175341]\n",
      "loss: 0.867556  [140800/175341]\n",
      "loss: 0.479917  [142400/175341]\n",
      "loss: 0.832146  [144000/175341]\n",
      "loss: 0.380214  [145600/175341]\n",
      "loss: 0.210377  [147200/175341]\n",
      "loss: 0.245738  [148800/175341]\n",
      "loss: 0.380514  [150400/175341]\n",
      "loss: 0.615650  [152000/175341]\n",
      "loss: 0.900830  [153600/175341]\n",
      "loss: 0.397077  [155200/175341]\n",
      "loss: 0.582488  [156800/175341]\n",
      "loss: 0.331855  [158400/175341]\n",
      "loss: 0.284489  [160000/175341]\n",
      "loss: 0.199810  [161600/175341]\n",
      "loss: 0.242692  [163200/175341]\n",
      "loss: 0.414806  [164800/175341]\n",
      "loss: 0.286987  [166400/175341]\n",
      "loss: 0.497073  [168000/175341]\n",
      "loss: 0.483407  [169600/175341]\n",
      "loss: 0.268054  [171200/175341]\n",
      "loss: 0.298405  [172800/175341]\n",
      "loss: 0.334646  [174400/175341]\n",
      "Train Accuracy: 82.0709%\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.536741, F1-score: 77.55%, Macro_F1-Score:  42.77%  \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.681095  [    0/175341]\n",
      "loss: 0.485426  [ 1600/175341]\n",
      "loss: 0.430344  [ 3200/175341]\n",
      "loss: 0.582111  [ 4800/175341]\n",
      "loss: 0.419384  [ 6400/175341]\n",
      "loss: 0.556930  [ 8000/175341]\n",
      "loss: 0.355288  [ 9600/175341]\n",
      "loss: 0.206764  [11200/175341]\n",
      "loss: 0.511075  [12800/175341]\n",
      "loss: 0.647283  [14400/175341]\n",
      "loss: 0.274443  [16000/175341]\n",
      "loss: 0.452244  [17600/175341]\n",
      "loss: 0.826007  [19200/175341]\n",
      "loss: 0.196695  [20800/175341]\n",
      "loss: 0.467681  [22400/175341]\n",
      "loss: 0.186570  [24000/175341]\n",
      "loss: 0.438174  [25600/175341]\n",
      "loss: 0.865090  [27200/175341]\n",
      "loss: 0.427672  [28800/175341]\n",
      "loss: 0.348446  [30400/175341]\n",
      "loss: 0.488886  [32000/175341]\n",
      "loss: 0.559265  [33600/175341]\n",
      "loss: 0.536786  [35200/175341]\n",
      "loss: 0.415062  [36800/175341]\n",
      "loss: 0.220775  [38400/175341]\n",
      "loss: 0.280406  [40000/175341]\n",
      "loss: 0.399695  [41600/175341]\n",
      "loss: 0.608341  [43200/175341]\n",
      "loss: 0.705905  [44800/175341]\n",
      "loss: 0.480001  [46400/175341]\n",
      "loss: 0.495286  [48000/175341]\n",
      "loss: 0.148413  [49600/175341]\n",
      "loss: 0.465361  [51200/175341]\n",
      "loss: 0.378729  [52800/175341]\n",
      "loss: 0.468624  [54400/175341]\n",
      "loss: 0.581521  [56000/175341]\n",
      "loss: 0.833418  [57600/175341]\n",
      "loss: 0.806475  [59200/175341]\n",
      "loss: 0.786455  [60800/175341]\n",
      "loss: 0.199967  [62400/175341]\n",
      "loss: 0.254903  [64000/175341]\n",
      "loss: 0.342429  [65600/175341]\n",
      "loss: 0.281686  [67200/175341]\n",
      "loss: 0.175105  [68800/175341]\n",
      "loss: 0.440347  [70400/175341]\n",
      "loss: 0.529241  [72000/175341]\n",
      "loss: 0.251312  [73600/175341]\n",
      "loss: 0.542130  [75200/175341]\n",
      "loss: 0.415380  [76800/175341]\n",
      "loss: 0.596150  [78400/175341]\n",
      "loss: 0.255849  [80000/175341]\n",
      "loss: 0.338460  [81600/175341]\n",
      "loss: 0.395329  [83200/175341]\n",
      "loss: 0.491331  [84800/175341]\n",
      "loss: 0.475779  [86400/175341]\n",
      "loss: 0.628836  [88000/175341]\n",
      "loss: 0.436571  [89600/175341]\n",
      "loss: 0.675892  [91200/175341]\n",
      "loss: 0.834708  [92800/175341]\n",
      "loss: 0.351845  [94400/175341]\n",
      "loss: 0.634885  [96000/175341]\n",
      "loss: 0.294312  [97600/175341]\n",
      "loss: 0.182773  [99200/175341]\n",
      "loss: 0.452073  [100800/175341]\n",
      "loss: 0.604835  [102400/175341]\n",
      "loss: 0.684866  [104000/175341]\n",
      "loss: 0.666160  [105600/175341]\n",
      "loss: 0.757284  [107200/175341]\n",
      "loss: 0.529992  [108800/175341]\n",
      "loss: 0.201186  [110400/175341]\n",
      "loss: 0.566112  [112000/175341]\n",
      "loss: 0.212805  [113600/175341]\n",
      "loss: 0.734041  [115200/175341]\n",
      "loss: 0.858588  [116800/175341]\n",
      "loss: 0.444137  [118400/175341]\n",
      "loss: 0.756479  [120000/175341]\n",
      "loss: 0.719437  [121600/175341]\n",
      "loss: 0.862558  [123200/175341]\n",
      "loss: 0.359571  [124800/175341]\n",
      "loss: 0.382982  [126400/175341]\n",
      "loss: 1.050044  [128000/175341]\n",
      "loss: 0.479803  [129600/175341]\n",
      "loss: 0.269978  [131200/175341]\n",
      "loss: 0.243433  [132800/175341]\n",
      "loss: 0.180673  [134400/175341]\n",
      "loss: 0.753806  [136000/175341]\n",
      "loss: 0.387500  [137600/175341]\n",
      "loss: 0.533858  [139200/175341]\n",
      "loss: 0.253332  [140800/175341]\n",
      "loss: 0.174745  [142400/175341]\n",
      "loss: 0.194941  [144000/175341]\n",
      "loss: 0.416653  [145600/175341]\n",
      "loss: 0.936720  [147200/175341]\n",
      "loss: 0.237537  [148800/175341]\n",
      "loss: 0.638933  [150400/175341]\n",
      "loss: 0.654815  [152000/175341]\n",
      "loss: 0.505193  [153600/175341]\n",
      "loss: 0.450951  [155200/175341]\n",
      "loss: 0.535205  [156800/175341]\n",
      "loss: 0.547274  [158400/175341]\n",
      "loss: 0.180984  [160000/175341]\n",
      "loss: 0.690485  [161600/175341]\n",
      "loss: 0.074336  [163200/175341]\n",
      "loss: 0.411539  [164800/175341]\n",
      "loss: 0.606116  [166400/175341]\n",
      "loss: 0.411906  [168000/175341]\n",
      "loss: 0.230626  [169600/175341]\n",
      "loss: 0.581840  [171200/175341]\n",
      "loss: 0.280000  [172800/175341]\n",
      "loss: 0.108204  [174400/175341]\n",
      "Train Accuracy: 82.0795%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.568833, F1-score: 75.38%, Macro_F1-Score:  42.57%  \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.543768  [    0/175341]\n",
      "loss: 0.367068  [ 1600/175341]\n",
      "loss: 0.242739  [ 3200/175341]\n",
      "loss: 0.388094  [ 4800/175341]\n",
      "loss: 0.715152  [ 6400/175341]\n",
      "loss: 0.660957  [ 8000/175341]\n",
      "loss: 0.303925  [ 9600/175341]\n",
      "loss: 0.273569  [11200/175341]\n",
      "loss: 0.431437  [12800/175341]\n",
      "loss: 0.442503  [14400/175341]\n",
      "loss: 0.389243  [16000/175341]\n",
      "loss: 0.265895  [17600/175341]\n",
      "loss: 0.229507  [19200/175341]\n",
      "loss: 0.791548  [20800/175341]\n",
      "loss: 0.252813  [22400/175341]\n",
      "loss: 0.455421  [24000/175341]\n",
      "loss: 0.622515  [25600/175341]\n",
      "loss: 0.321277  [27200/175341]\n",
      "loss: 0.143003  [28800/175341]\n",
      "loss: 0.398180  [30400/175341]\n",
      "loss: 0.365752  [32000/175341]\n",
      "loss: 0.572909  [33600/175341]\n",
      "loss: 0.221294  [35200/175341]\n",
      "loss: 0.273996  [36800/175341]\n",
      "loss: 0.240305  [38400/175341]\n",
      "loss: 0.891501  [40000/175341]\n",
      "loss: 0.376394  [41600/175341]\n",
      "loss: 0.628174  [43200/175341]\n",
      "loss: 0.490489  [44800/175341]\n",
      "loss: 0.464863  [46400/175341]\n",
      "loss: 0.524507  [48000/175341]\n",
      "loss: 0.386024  [49600/175341]\n",
      "loss: 0.527601  [51200/175341]\n",
      "loss: 0.924866  [52800/175341]\n",
      "loss: 0.416537  [54400/175341]\n",
      "loss: 0.447133  [56000/175341]\n",
      "loss: 0.279184  [57600/175341]\n",
      "loss: 0.222273  [59200/175341]\n",
      "loss: 0.350455  [60800/175341]\n",
      "loss: 0.388651  [62400/175341]\n",
      "loss: 0.608679  [64000/175341]\n",
      "loss: 0.416195  [65600/175341]\n",
      "loss: 0.726711  [67200/175341]\n",
      "loss: 0.443429  [68800/175341]\n",
      "loss: 0.512656  [70400/175341]\n",
      "loss: 0.201950  [72000/175341]\n",
      "loss: 0.882435  [73600/175341]\n",
      "loss: 0.499585  [75200/175341]\n",
      "loss: 0.272217  [76800/175341]\n",
      "loss: 0.623638  [78400/175341]\n",
      "loss: 0.072604  [80000/175341]\n",
      "loss: 0.315476  [81600/175341]\n",
      "loss: 0.254007  [83200/175341]\n",
      "loss: 0.258121  [84800/175341]\n",
      "loss: 0.379162  [86400/175341]\n",
      "loss: 0.391278  [88000/175341]\n",
      "loss: 0.616981  [89600/175341]\n",
      "loss: 0.361993  [91200/175341]\n",
      "loss: 0.393808  [92800/175341]\n",
      "loss: 0.389585  [94400/175341]\n",
      "loss: 0.359578  [96000/175341]\n",
      "loss: 0.558511  [97600/175341]\n",
      "loss: 0.357376  [99200/175341]\n",
      "loss: 1.025107  [100800/175341]\n",
      "loss: 0.539024  [102400/175341]\n",
      "loss: 0.340506  [104000/175341]\n",
      "loss: 0.430601  [105600/175341]\n",
      "loss: 0.447201  [107200/175341]\n",
      "loss: 0.390294  [108800/175341]\n",
      "loss: 0.898664  [110400/175341]\n",
      "loss: 0.498715  [112000/175341]\n",
      "loss: 0.465861  [113600/175341]\n",
      "loss: 0.486142  [115200/175341]\n",
      "loss: 0.225869  [116800/175341]\n",
      "loss: 0.594080  [118400/175341]\n",
      "loss: 0.379779  [120000/175341]\n",
      "loss: 0.401557  [121600/175341]\n",
      "loss: 0.847184  [123200/175341]\n",
      "loss: 0.471487  [124800/175341]\n",
      "loss: 0.324377  [126400/175341]\n",
      "loss: 0.331641  [128000/175341]\n",
      "loss: 0.568449  [129600/175341]\n",
      "loss: 0.180159  [131200/175341]\n",
      "loss: 0.200894  [132800/175341]\n",
      "loss: 0.380851  [134400/175341]\n",
      "loss: 0.291312  [136000/175341]\n",
      "loss: 0.724546  [137600/175341]\n",
      "loss: 0.425648  [139200/175341]\n",
      "loss: 0.728576  [140800/175341]\n",
      "loss: 0.674490  [142400/175341]\n",
      "loss: 0.370939  [144000/175341]\n",
      "loss: 0.467024  [145600/175341]\n",
      "loss: 0.560804  [147200/175341]\n",
      "loss: 0.436156  [148800/175341]\n",
      "loss: 0.986131  [150400/175341]\n",
      "loss: 0.283255  [152000/175341]\n",
      "loss: 0.540518  [153600/175341]\n",
      "loss: 0.444555  [155200/175341]\n",
      "loss: 0.435527  [156800/175341]\n",
      "loss: 0.465452  [158400/175341]\n",
      "loss: 0.565436  [160000/175341]\n",
      "loss: 0.400765  [161600/175341]\n",
      "loss: 0.456717  [163200/175341]\n",
      "loss: 0.562207  [164800/175341]\n",
      "loss: 0.228561  [166400/175341]\n",
      "loss: 0.448118  [168000/175341]\n",
      "loss: 0.157979  [169600/175341]\n",
      "loss: 0.250025  [171200/175341]\n",
      "loss: 0.371481  [172800/175341]\n",
      "loss: 0.488623  [174400/175341]\n",
      "Train Accuracy: 82.0145%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.554865, F1-score: 76.54%, Macro_F1-Score:  42.07%  \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.077570  [    0/175341]\n",
      "loss: 0.295317  [ 1600/175341]\n",
      "loss: 0.306511  [ 3200/175341]\n",
      "loss: 0.476688  [ 4800/175341]\n",
      "loss: 0.459017  [ 6400/175341]\n",
      "loss: 0.588638  [ 8000/175341]\n",
      "loss: 0.583487  [ 9600/175341]\n",
      "loss: 0.126222  [11200/175341]\n",
      "loss: 0.594083  [12800/175341]\n",
      "loss: 0.477516  [14400/175341]\n",
      "loss: 0.673655  [16000/175341]\n",
      "loss: 0.240368  [17600/175341]\n",
      "loss: 0.281746  [19200/175341]\n",
      "loss: 0.609786  [20800/175341]\n",
      "loss: 0.072280  [22400/175341]\n",
      "loss: 0.554841  [24000/175341]\n",
      "loss: 0.227775  [25600/175341]\n",
      "loss: 0.450477  [27200/175341]\n",
      "loss: 0.323749  [28800/175341]\n",
      "loss: 0.495227  [30400/175341]\n",
      "loss: 0.495209  [32000/175341]\n",
      "loss: 0.549423  [33600/175341]\n",
      "loss: 0.644288  [35200/175341]\n",
      "loss: 0.214162  [36800/175341]\n",
      "loss: 0.514767  [38400/175341]\n",
      "loss: 0.938772  [40000/175341]\n",
      "loss: 0.406500  [41600/175341]\n",
      "loss: 0.300238  [43200/175341]\n",
      "loss: 0.388667  [44800/175341]\n",
      "loss: 0.875448  [46400/175341]\n",
      "loss: 0.426208  [48000/175341]\n",
      "loss: 0.653764  [49600/175341]\n",
      "loss: 0.790991  [51200/175341]\n",
      "loss: 0.349315  [52800/175341]\n",
      "loss: 0.494903  [54400/175341]\n",
      "loss: 0.444800  [56000/175341]\n",
      "loss: 0.526131  [57600/175341]\n",
      "loss: 0.277444  [59200/175341]\n",
      "loss: 0.415820  [60800/175341]\n",
      "loss: 0.392328  [62400/175341]\n",
      "loss: 0.396118  [64000/175341]\n",
      "loss: 0.357144  [65600/175341]\n",
      "loss: 1.091505  [67200/175341]\n",
      "loss: 0.666909  [68800/175341]\n",
      "loss: 0.286113  [70400/175341]\n",
      "loss: 0.350300  [72000/175341]\n",
      "loss: 0.291328  [73600/175341]\n",
      "loss: 0.616903  [75200/175341]\n",
      "loss: 0.517710  [76800/175341]\n",
      "loss: 0.328517  [78400/175341]\n",
      "loss: 0.433065  [80000/175341]\n",
      "loss: 0.408866  [81600/175341]\n",
      "loss: 0.365207  [83200/175341]\n",
      "loss: 0.862761  [84800/175341]\n",
      "loss: 0.311483  [86400/175341]\n",
      "loss: 0.292396  [88000/175341]\n",
      "loss: 0.423026  [89600/175341]\n",
      "loss: 0.335511  [91200/175341]\n",
      "loss: 0.180487  [92800/175341]\n",
      "loss: 0.058703  [94400/175341]\n",
      "loss: 0.539741  [96000/175341]\n",
      "loss: 0.501831  [97600/175341]\n",
      "loss: 0.200222  [99200/175341]\n",
      "loss: 0.740734  [100800/175341]\n",
      "loss: 0.471127  [102400/175341]\n",
      "loss: 0.301018  [104000/175341]\n",
      "loss: 0.191066  [105600/175341]\n",
      "loss: 0.490608  [107200/175341]\n",
      "loss: 0.331663  [108800/175341]\n",
      "loss: 0.431707  [110400/175341]\n",
      "loss: 0.559000  [112000/175341]\n",
      "loss: 0.291080  [113600/175341]\n",
      "loss: 0.563626  [115200/175341]\n",
      "loss: 0.265921  [116800/175341]\n",
      "loss: 0.167897  [118400/175341]\n",
      "loss: 0.702649  [120000/175341]\n",
      "loss: 0.489915  [121600/175341]\n",
      "loss: 0.443245  [123200/175341]\n",
      "loss: 0.254046  [124800/175341]\n",
      "loss: 0.605865  [126400/175341]\n",
      "loss: 0.202334  [128000/175341]\n",
      "loss: 0.255113  [129600/175341]\n",
      "loss: 0.230086  [131200/175341]\n",
      "loss: 0.276585  [132800/175341]\n",
      "loss: 0.658796  [134400/175341]\n",
      "loss: 0.159834  [136000/175341]\n",
      "loss: 0.726416  [137600/175341]\n",
      "loss: 1.285270  [139200/175341]\n",
      "loss: 0.551146  [140800/175341]\n",
      "loss: 0.434470  [142400/175341]\n",
      "loss: 0.440294  [144000/175341]\n",
      "loss: 0.340254  [145600/175341]\n",
      "loss: 0.504964  [147200/175341]\n",
      "loss: 0.305904  [148800/175341]\n",
      "loss: 0.666262  [150400/175341]\n",
      "loss: 0.460155  [152000/175341]\n",
      "loss: 0.246791  [153600/175341]\n",
      "loss: 1.145739  [155200/175341]\n",
      "loss: 0.463206  [156800/175341]\n",
      "loss: 0.301613  [158400/175341]\n",
      "loss: 0.827490  [160000/175341]\n",
      "loss: 0.488954  [161600/175341]\n",
      "loss: 0.166003  [163200/175341]\n",
      "loss: 0.323994  [164800/175341]\n",
      "loss: 0.475393  [166400/175341]\n",
      "loss: 0.338724  [168000/175341]\n",
      "loss: 0.653053  [169600/175341]\n",
      "loss: 0.483101  [171200/175341]\n",
      "loss: 0.110568  [172800/175341]\n",
      "loss: 0.256943  [174400/175341]\n",
      "Train Accuracy: 82.0784%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.546071, F1-score: 76.99%, Macro_F1-Score:  43.40%  \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.519035  [    0/175341]\n",
      "loss: 0.380785  [ 1600/175341]\n",
      "loss: 0.472804  [ 3200/175341]\n",
      "loss: 0.604932  [ 4800/175341]\n",
      "loss: 0.243827  [ 6400/175341]\n",
      "loss: 0.436429  [ 8000/175341]\n",
      "loss: 0.178219  [ 9600/175341]\n",
      "loss: 0.349319  [11200/175341]\n",
      "loss: 0.490330  [12800/175341]\n",
      "loss: 0.248296  [14400/175341]\n",
      "loss: 0.729431  [16000/175341]\n",
      "loss: 0.524312  [17600/175341]\n",
      "loss: 0.515191  [19200/175341]\n",
      "loss: 0.259279  [20800/175341]\n",
      "loss: 0.338105  [22400/175341]\n",
      "loss: 0.175841  [24000/175341]\n",
      "loss: 0.330800  [25600/175341]\n",
      "loss: 0.626171  [27200/175341]\n",
      "loss: 0.776853  [28800/175341]\n",
      "loss: 0.280746  [30400/175341]\n",
      "loss: 0.325061  [32000/175341]\n",
      "loss: 0.122848  [33600/175341]\n",
      "loss: 0.511406  [35200/175341]\n",
      "loss: 0.550073  [36800/175341]\n",
      "loss: 0.849998  [38400/175341]\n",
      "loss: 0.363773  [40000/175341]\n",
      "loss: 0.333951  [41600/175341]\n",
      "loss: 0.430842  [43200/175341]\n",
      "loss: 0.361669  [44800/175341]\n",
      "loss: 0.491648  [46400/175341]\n",
      "loss: 0.452822  [48000/175341]\n",
      "loss: 0.515706  [49600/175341]\n",
      "loss: 0.384615  [51200/175341]\n",
      "loss: 0.869794  [52800/175341]\n",
      "loss: 0.578825  [54400/175341]\n",
      "loss: 0.451495  [56000/175341]\n",
      "loss: 0.369293  [57600/175341]\n",
      "loss: 0.118351  [59200/175341]\n",
      "loss: 0.420793  [60800/175341]\n",
      "loss: 0.179615  [62400/175341]\n",
      "loss: 0.188204  [64000/175341]\n",
      "loss: 1.019975  [65600/175341]\n",
      "loss: 0.585365  [67200/175341]\n",
      "loss: 0.229357  [68800/175341]\n",
      "loss: 0.195746  [70400/175341]\n",
      "loss: 0.457339  [72000/175341]\n",
      "loss: 0.572877  [73600/175341]\n",
      "loss: 0.808752  [75200/175341]\n",
      "loss: 0.385808  [76800/175341]\n",
      "loss: 0.466780  [78400/175341]\n",
      "loss: 0.408981  [80000/175341]\n",
      "loss: 0.627694  [81600/175341]\n",
      "loss: 0.613988  [83200/175341]\n",
      "loss: 0.483455  [84800/175341]\n",
      "loss: 0.456745  [86400/175341]\n",
      "loss: 0.672895  [88000/175341]\n",
      "loss: 0.590343  [89600/175341]\n",
      "loss: 0.574764  [91200/175341]\n",
      "loss: 0.666888  [92800/175341]\n",
      "loss: 0.523801  [94400/175341]\n",
      "loss: 0.552660  [96000/175341]\n",
      "loss: 0.422387  [97600/175341]\n",
      "loss: 0.359640  [99200/175341]\n",
      "loss: 0.988826  [100800/175341]\n",
      "loss: 0.536535  [102400/175341]\n",
      "loss: 0.241068  [104000/175341]\n",
      "loss: 0.821578  [105600/175341]\n",
      "loss: 0.546369  [107200/175341]\n",
      "loss: 0.528400  [108800/175341]\n",
      "loss: 0.579115  [110400/175341]\n",
      "loss: 0.511909  [112000/175341]\n",
      "loss: 0.276695  [113600/175341]\n",
      "loss: 0.164043  [115200/175341]\n",
      "loss: 0.486652  [116800/175341]\n",
      "loss: 0.553200  [118400/175341]\n",
      "loss: 0.443923  [120000/175341]\n",
      "loss: 0.588141  [121600/175341]\n",
      "loss: 0.528270  [123200/175341]\n",
      "loss: 0.683633  [124800/175341]\n",
      "loss: 0.382473  [126400/175341]\n",
      "loss: 0.434367  [128000/175341]\n",
      "loss: 0.094282  [129600/175341]\n",
      "loss: 0.215543  [131200/175341]\n",
      "loss: 0.308768  [132800/175341]\n",
      "loss: 0.477132  [134400/175341]\n",
      "loss: 0.729137  [136000/175341]\n",
      "loss: 0.359703  [137600/175341]\n",
      "loss: 0.751076  [139200/175341]\n",
      "loss: 0.158684  [140800/175341]\n",
      "loss: 0.422153  [142400/175341]\n",
      "loss: 0.450018  [144000/175341]\n",
      "loss: 0.330610  [145600/175341]\n",
      "loss: 0.228531  [147200/175341]\n",
      "loss: 0.130309  [148800/175341]\n",
      "loss: 0.528985  [150400/175341]\n",
      "loss: 0.704922  [152000/175341]\n",
      "loss: 0.499658  [153600/175341]\n",
      "loss: 0.671695  [155200/175341]\n",
      "loss: 0.440069  [156800/175341]\n",
      "loss: 0.305647  [158400/175341]\n",
      "loss: 0.185151  [160000/175341]\n",
      "loss: 0.377028  [161600/175341]\n",
      "loss: 0.425447  [163200/175341]\n",
      "loss: 1.116180  [164800/175341]\n",
      "loss: 0.418043  [166400/175341]\n",
      "loss: 0.666121  [168000/175341]\n",
      "loss: 0.313688  [169600/175341]\n",
      "loss: 0.450256  [171200/175341]\n",
      "loss: 0.455285  [172800/175341]\n",
      "loss: 0.383227  [174400/175341]\n",
      "Train Accuracy: 82.0932%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.542051, F1-score: 77.20%, Macro_F1-Score:  42.94%  \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.710350  [    0/175341]\n",
      "loss: 0.846189  [ 1600/175341]\n",
      "loss: 0.352466  [ 3200/175341]\n",
      "loss: 0.619557  [ 4800/175341]\n",
      "loss: 0.701416  [ 6400/175341]\n",
      "loss: 0.403362  [ 8000/175341]\n",
      "loss: 0.453211  [ 9600/175341]\n",
      "loss: 0.514639  [11200/175341]\n",
      "loss: 0.586643  [12800/175341]\n",
      "loss: 0.585193  [14400/175341]\n",
      "loss: 0.186241  [16000/175341]\n",
      "loss: 0.462563  [17600/175341]\n",
      "loss: 0.407013  [19200/175341]\n",
      "loss: 0.433104  [20800/175341]\n",
      "loss: 0.838747  [22400/175341]\n",
      "loss: 0.248230  [24000/175341]\n",
      "loss: 0.357051  [25600/175341]\n",
      "loss: 0.902974  [27200/175341]\n",
      "loss: 0.700082  [28800/175341]\n",
      "loss: 1.107796  [30400/175341]\n",
      "loss: 0.235324  [32000/175341]\n",
      "loss: 0.518487  [33600/175341]\n",
      "loss: 0.329499  [35200/175341]\n",
      "loss: 0.144195  [36800/175341]\n",
      "loss: 0.480587  [38400/175341]\n",
      "loss: 0.385346  [40000/175341]\n",
      "loss: 0.352678  [41600/175341]\n",
      "loss: 0.399939  [43200/175341]\n",
      "loss: 0.269060  [44800/175341]\n",
      "loss: 0.263983  [46400/175341]\n",
      "loss: 0.676071  [48000/175341]\n",
      "loss: 0.144353  [49600/175341]\n",
      "loss: 0.495115  [51200/175341]\n",
      "loss: 0.748956  [52800/175341]\n",
      "loss: 0.643167  [54400/175341]\n",
      "loss: 0.685036  [56000/175341]\n",
      "loss: 0.332507  [57600/175341]\n",
      "loss: 0.583602  [59200/175341]\n",
      "loss: 0.350494  [60800/175341]\n",
      "loss: 0.354684  [62400/175341]\n",
      "loss: 0.229386  [64000/175341]\n",
      "loss: 0.416183  [65600/175341]\n",
      "loss: 0.323025  [67200/175341]\n",
      "loss: 0.568454  [68800/175341]\n",
      "loss: 0.227966  [70400/175341]\n",
      "loss: 0.372857  [72000/175341]\n",
      "loss: 0.471616  [73600/175341]\n",
      "loss: 0.528327  [75200/175341]\n",
      "loss: 0.439551  [76800/175341]\n",
      "loss: 0.433521  [78400/175341]\n",
      "loss: 0.256013  [80000/175341]\n",
      "loss: 0.324546  [81600/175341]\n",
      "loss: 0.495075  [83200/175341]\n",
      "loss: 0.620688  [84800/175341]\n",
      "loss: 0.374013  [86400/175341]\n",
      "loss: 0.510014  [88000/175341]\n",
      "loss: 0.505723  [89600/175341]\n",
      "loss: 0.265544  [91200/175341]\n",
      "loss: 0.151824  [92800/175341]\n",
      "loss: 0.831075  [94400/175341]\n",
      "loss: 0.463481  [96000/175341]\n",
      "loss: 0.462321  [97600/175341]\n",
      "loss: 0.160685  [99200/175341]\n",
      "loss: 0.589162  [100800/175341]\n",
      "loss: 0.327070  [102400/175341]\n",
      "loss: 0.564928  [104000/175341]\n",
      "loss: 0.325943  [105600/175341]\n",
      "loss: 0.746177  [107200/175341]\n",
      "loss: 0.576586  [108800/175341]\n",
      "loss: 0.477636  [110400/175341]\n",
      "loss: 0.141468  [112000/175341]\n",
      "loss: 0.850715  [113600/175341]\n",
      "loss: 0.288823  [115200/175341]\n",
      "loss: 0.117938  [116800/175341]\n",
      "loss: 0.690195  [118400/175341]\n",
      "loss: 0.463866  [120000/175341]\n",
      "loss: 0.623940  [121600/175341]\n",
      "loss: 0.417117  [123200/175341]\n",
      "loss: 0.703445  [124800/175341]\n",
      "loss: 0.534084  [126400/175341]\n",
      "loss: 0.457500  [128000/175341]\n",
      "loss: 0.588867  [129600/175341]\n",
      "loss: 0.444699  [131200/175341]\n",
      "loss: 0.617805  [132800/175341]\n",
      "loss: 0.424651  [134400/175341]\n",
      "loss: 0.424098  [136000/175341]\n",
      "loss: 0.698269  [137600/175341]\n",
      "loss: 0.267824  [139200/175341]\n",
      "loss: 0.477736  [140800/175341]\n",
      "loss: 0.455098  [142400/175341]\n",
      "loss: 0.548913  [144000/175341]\n",
      "loss: 0.564630  [145600/175341]\n",
      "loss: 0.331201  [147200/175341]\n",
      "loss: 0.398459  [148800/175341]\n",
      "loss: 0.364730  [150400/175341]\n",
      "loss: 0.247096  [152000/175341]\n",
      "loss: 0.246495  [153600/175341]\n",
      "loss: 0.463330  [155200/175341]\n",
      "loss: 0.467519  [156800/175341]\n",
      "loss: 0.274173  [158400/175341]\n",
      "loss: 0.806156  [160000/175341]\n",
      "loss: 0.629567  [161600/175341]\n",
      "loss: 0.397920  [163200/175341]\n",
      "loss: 0.381321  [164800/175341]\n",
      "loss: 0.157280  [166400/175341]\n",
      "loss: 0.503195  [168000/175341]\n",
      "loss: 0.558362  [169600/175341]\n",
      "loss: 0.211445  [171200/175341]\n",
      "loss: 0.323415  [172800/175341]\n",
      "loss: 0.466056  [174400/175341]\n",
      "Train Accuracy: 82.0567%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.546851, F1-score: 77.22%, Macro_F1-Score:  42.69%  \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.315977  [    0/175341]\n",
      "loss: 0.500738  [ 1600/175341]\n",
      "loss: 0.423266  [ 3200/175341]\n",
      "loss: 0.502466  [ 4800/175341]\n",
      "loss: 0.325247  [ 6400/175341]\n",
      "loss: 0.433207  [ 8000/175341]\n",
      "loss: 0.361332  [ 9600/175341]\n",
      "loss: 0.222416  [11200/175341]\n",
      "loss: 0.849562  [12800/175341]\n",
      "loss: 0.164911  [14400/175341]\n",
      "loss: 0.483413  [16000/175341]\n",
      "loss: 0.447420  [17600/175341]\n",
      "loss: 0.418443  [19200/175341]\n",
      "loss: 0.687540  [20800/175341]\n",
      "loss: 0.367296  [22400/175341]\n",
      "loss: 0.344628  [24000/175341]\n",
      "loss: 0.373574  [25600/175341]\n",
      "loss: 0.266885  [27200/175341]\n",
      "loss: 0.355660  [28800/175341]\n",
      "loss: 0.482364  [30400/175341]\n",
      "loss: 0.524888  [32000/175341]\n",
      "loss: 0.359116  [33600/175341]\n",
      "loss: 0.314520  [35200/175341]\n",
      "loss: 0.325900  [36800/175341]\n",
      "loss: 0.486362  [38400/175341]\n",
      "loss: 0.670385  [40000/175341]\n",
      "loss: 0.497305  [41600/175341]\n",
      "loss: 0.311267  [43200/175341]\n",
      "loss: 0.440638  [44800/175341]\n",
      "loss: 0.453969  [46400/175341]\n",
      "loss: 0.440190  [48000/175341]\n",
      "loss: 0.531349  [49600/175341]\n",
      "loss: 0.136365  [51200/175341]\n",
      "loss: 0.489591  [52800/175341]\n",
      "loss: 0.810815  [54400/175341]\n",
      "loss: 0.422623  [56000/175341]\n",
      "loss: 0.270482  [57600/175341]\n",
      "loss: 0.549081  [59200/175341]\n",
      "loss: 0.320813  [60800/175341]\n",
      "loss: 0.416102  [62400/175341]\n",
      "loss: 0.407586  [64000/175341]\n",
      "loss: 0.238516  [65600/175341]\n",
      "loss: 0.519209  [67200/175341]\n",
      "loss: 0.451680  [68800/175341]\n",
      "loss: 0.451849  [70400/175341]\n",
      "loss: 0.793076  [72000/175341]\n",
      "loss: 0.598257  [73600/175341]\n",
      "loss: 0.509270  [75200/175341]\n",
      "loss: 0.542677  [76800/175341]\n",
      "loss: 0.133885  [78400/175341]\n",
      "loss: 0.100046  [80000/175341]\n",
      "loss: 0.365281  [81600/175341]\n",
      "loss: 0.518566  [83200/175341]\n",
      "loss: 0.748553  [84800/175341]\n",
      "loss: 1.123373  [86400/175341]\n",
      "loss: 0.351575  [88000/175341]\n",
      "loss: 0.420114  [89600/175341]\n",
      "loss: 0.158215  [91200/175341]\n",
      "loss: 0.389121  [92800/175341]\n",
      "loss: 0.483775  [94400/175341]\n",
      "loss: 0.428705  [96000/175341]\n",
      "loss: 0.550953  [97600/175341]\n",
      "loss: 0.081577  [99200/175341]\n",
      "loss: 0.376066  [100800/175341]\n",
      "loss: 0.244735  [102400/175341]\n",
      "loss: 0.465921  [104000/175341]\n",
      "loss: 0.427896  [105600/175341]\n",
      "loss: 0.290553  [107200/175341]\n",
      "loss: 0.072002  [108800/175341]\n",
      "loss: 0.483232  [110400/175341]\n",
      "loss: 0.300365  [112000/175341]\n",
      "loss: 0.312949  [113600/175341]\n",
      "loss: 0.254724  [115200/175341]\n",
      "loss: 0.316236  [116800/175341]\n",
      "loss: 0.408302  [118400/175341]\n",
      "loss: 0.450240  [120000/175341]\n",
      "loss: 0.508460  [121600/175341]\n",
      "loss: 0.497977  [123200/175341]\n",
      "loss: 0.282978  [124800/175341]\n",
      "loss: 0.288712  [126400/175341]\n",
      "loss: 0.396179  [128000/175341]\n",
      "loss: 0.372734  [129600/175341]\n",
      "loss: 0.239928  [131200/175341]\n",
      "loss: 0.547716  [132800/175341]\n",
      "loss: 0.522418  [134400/175341]\n",
      "loss: 0.193408  [136000/175341]\n",
      "loss: 0.485096  [137600/175341]\n",
      "loss: 0.386623  [139200/175341]\n",
      "loss: 0.910309  [140800/175341]\n",
      "loss: 0.381647  [142400/175341]\n",
      "loss: 0.413356  [144000/175341]\n",
      "loss: 0.710858  [145600/175341]\n",
      "loss: 0.697947  [147200/175341]\n",
      "loss: 0.178186  [148800/175341]\n",
      "loss: 0.295757  [150400/175341]\n",
      "loss: 0.897011  [152000/175341]\n",
      "loss: 0.689095  [153600/175341]\n",
      "loss: 0.557153  [155200/175341]\n",
      "loss: 0.920224  [156800/175341]\n",
      "loss: 0.562277  [158400/175341]\n",
      "loss: 0.405282  [160000/175341]\n",
      "loss: 0.564624  [161600/175341]\n",
      "loss: 0.364074  [163200/175341]\n",
      "loss: 0.874995  [164800/175341]\n",
      "loss: 0.241430  [166400/175341]\n",
      "loss: 0.570004  [168000/175341]\n",
      "loss: 0.149765  [169600/175341]\n",
      "loss: 0.690651  [171200/175341]\n",
      "loss: 0.210300  [172800/175341]\n",
      "loss: 0.502132  [174400/175341]\n",
      "Train Accuracy: 82.0869%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.548762, F1-score: 76.85%, Macro_F1-Score:  42.17%  \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.265327  [    0/175341]\n",
      "loss: 0.792007  [ 1600/175341]\n",
      "loss: 0.486180  [ 3200/175341]\n",
      "loss: 0.250061  [ 4800/175341]\n",
      "loss: 0.600696  [ 6400/175341]\n",
      "loss: 0.391995  [ 8000/175341]\n",
      "loss: 0.364540  [ 9600/175341]\n",
      "loss: 0.493515  [11200/175341]\n",
      "loss: 0.265947  [12800/175341]\n",
      "loss: 0.160685  [14400/175341]\n",
      "loss: 0.531712  [16000/175341]\n",
      "loss: 0.521293  [17600/175341]\n",
      "loss: 0.980579  [19200/175341]\n",
      "loss: 0.495844  [20800/175341]\n",
      "loss: 0.453811  [22400/175341]\n",
      "loss: 0.357500  [24000/175341]\n",
      "loss: 0.361813  [25600/175341]\n",
      "loss: 0.671949  [27200/175341]\n",
      "loss: 0.840926  [28800/175341]\n",
      "loss: 0.366089  [30400/175341]\n",
      "loss: 0.275027  [32000/175341]\n",
      "loss: 0.531018  [33600/175341]\n",
      "loss: 0.609272  [35200/175341]\n",
      "loss: 0.417855  [36800/175341]\n",
      "loss: 0.146611  [38400/175341]\n",
      "loss: 0.357231  [40000/175341]\n",
      "loss: 0.354784  [41600/175341]\n",
      "loss: 0.676536  [43200/175341]\n",
      "loss: 0.397313  [44800/175341]\n",
      "loss: 0.532015  [46400/175341]\n",
      "loss: 0.235315  [48000/175341]\n",
      "loss: 0.722437  [49600/175341]\n",
      "loss: 0.204903  [51200/175341]\n",
      "loss: 0.229787  [52800/175341]\n",
      "loss: 0.507942  [54400/175341]\n",
      "loss: 1.059885  [56000/175341]\n",
      "loss: 0.272199  [57600/175341]\n",
      "loss: 0.251866  [59200/175341]\n",
      "loss: 0.380116  [60800/175341]\n",
      "loss: 0.449572  [62400/175341]\n",
      "loss: 0.374947  [64000/175341]\n",
      "loss: 0.229356  [65600/175341]\n",
      "loss: 0.237539  [67200/175341]\n",
      "loss: 0.465170  [68800/175341]\n",
      "loss: 0.557191  [70400/175341]\n",
      "loss: 0.475202  [72000/175341]\n",
      "loss: 0.729614  [73600/175341]\n",
      "loss: 0.226328  [75200/175341]\n",
      "loss: 0.616862  [76800/175341]\n",
      "loss: 0.228699  [78400/175341]\n",
      "loss: 0.480740  [80000/175341]\n",
      "loss: 0.359499  [81600/175341]\n",
      "loss: 0.677713  [83200/175341]\n",
      "loss: 0.170587  [84800/175341]\n",
      "loss: 0.420687  [86400/175341]\n",
      "loss: 0.355695  [88000/175341]\n",
      "loss: 0.711775  [89600/175341]\n",
      "loss: 0.600605  [91200/175341]\n",
      "loss: 0.373066  [92800/175341]\n",
      "loss: 0.279897  [94400/175341]\n",
      "loss: 0.506441  [96000/175341]\n",
      "loss: 0.324371  [97600/175341]\n",
      "loss: 0.516350  [99200/175341]\n",
      "loss: 0.471539  [100800/175341]\n",
      "loss: 0.438051  [102400/175341]\n",
      "loss: 0.943621  [104000/175341]\n",
      "loss: 0.545297  [105600/175341]\n",
      "loss: 0.668619  [107200/175341]\n",
      "loss: 0.631368  [108800/175341]\n",
      "loss: 0.650669  [110400/175341]\n",
      "loss: 0.693561  [112000/175341]\n",
      "loss: 1.492134  [113600/175341]\n",
      "loss: 0.297216  [115200/175341]\n",
      "loss: 0.374123  [116800/175341]\n",
      "loss: 0.302424  [118400/175341]\n",
      "loss: 0.428313  [120000/175341]\n",
      "loss: 0.146306  [121600/175341]\n",
      "loss: 0.595050  [123200/175341]\n",
      "loss: 0.218878  [124800/175341]\n",
      "loss: 0.518346  [126400/175341]\n",
      "loss: 0.549941  [128000/175341]\n",
      "loss: 0.533539  [129600/175341]\n",
      "loss: 0.461906  [131200/175341]\n",
      "loss: 0.766969  [132800/175341]\n",
      "loss: 0.381600  [134400/175341]\n",
      "loss: 0.127752  [136000/175341]\n",
      "loss: 0.933314  [137600/175341]\n",
      "loss: 0.210082  [139200/175341]\n",
      "loss: 0.357018  [140800/175341]\n",
      "loss: 0.376384  [142400/175341]\n",
      "loss: 0.493338  [144000/175341]\n",
      "loss: 0.677864  [145600/175341]\n",
      "loss: 0.440087  [147200/175341]\n",
      "loss: 0.369148  [148800/175341]\n",
      "loss: 0.314637  [150400/175341]\n",
      "loss: 0.457392  [152000/175341]\n",
      "loss: 0.740569  [153600/175341]\n",
      "loss: 0.452908  [155200/175341]\n",
      "loss: 0.180822  [156800/175341]\n",
      "loss: 1.073151  [158400/175341]\n",
      "loss: 0.151384  [160000/175341]\n",
      "loss: 0.301633  [161600/175341]\n",
      "loss: 0.157470  [163200/175341]\n",
      "loss: 0.492353  [164800/175341]\n",
      "loss: 0.343527  [166400/175341]\n",
      "loss: 0.558985  [168000/175341]\n",
      "loss: 0.124347  [169600/175341]\n",
      "loss: 0.392108  [171200/175341]\n",
      "loss: 0.134473  [172800/175341]\n",
      "loss: 0.610288  [174400/175341]\n",
      "Train Accuracy: 82.0761%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.555555, F1-score: 76.18%, Macro_F1-Score:  42.09%  \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.174154  [    0/175341]\n",
      "loss: 0.429728  [ 1600/175341]\n",
      "loss: 0.234551  [ 3200/175341]\n",
      "loss: 0.760915  [ 4800/175341]\n",
      "loss: 0.522062  [ 6400/175341]\n",
      "loss: 0.555807  [ 8000/175341]\n",
      "loss: 0.300293  [ 9600/175341]\n",
      "loss: 0.090354  [11200/175341]\n",
      "loss: 0.207900  [12800/175341]\n",
      "loss: 0.339133  [14400/175341]\n",
      "loss: 0.383036  [16000/175341]\n",
      "loss: 0.475590  [17600/175341]\n",
      "loss: 0.336110  [19200/175341]\n",
      "loss: 0.385671  [20800/175341]\n",
      "loss: 0.581704  [22400/175341]\n",
      "loss: 0.358287  [24000/175341]\n",
      "loss: 0.175501  [25600/175341]\n",
      "loss: 0.351347  [27200/175341]\n",
      "loss: 0.281950  [28800/175341]\n",
      "loss: 0.662279  [30400/175341]\n",
      "loss: 0.437621  [32000/175341]\n",
      "loss: 0.292479  [33600/175341]\n",
      "loss: 0.402535  [35200/175341]\n",
      "loss: 0.378373  [36800/175341]\n",
      "loss: 0.390678  [38400/175341]\n",
      "loss: 0.326105  [40000/175341]\n",
      "loss: 0.504082  [41600/175341]\n",
      "loss: 0.532689  [43200/175341]\n",
      "loss: 0.196969  [44800/175341]\n",
      "loss: 0.195538  [46400/175341]\n",
      "loss: 0.477975  [48000/175341]\n",
      "loss: 0.194974  [49600/175341]\n",
      "loss: 0.322585  [51200/175341]\n",
      "loss: 0.517221  [52800/175341]\n",
      "loss: 0.641702  [54400/175341]\n",
      "loss: 0.362053  [56000/175341]\n",
      "loss: 0.599919  [57600/175341]\n",
      "loss: 0.438421  [59200/175341]\n",
      "loss: 0.389526  [60800/175341]\n",
      "loss: 0.206396  [62400/175341]\n",
      "loss: 0.708127  [64000/175341]\n",
      "loss: 0.343555  [65600/175341]\n",
      "loss: 0.605564  [67200/175341]\n",
      "loss: 0.579890  [68800/175341]\n",
      "loss: 0.541240  [70400/175341]\n",
      "loss: 0.427873  [72000/175341]\n",
      "loss: 0.850374  [73600/175341]\n",
      "loss: 0.245052  [75200/175341]\n",
      "loss: 0.079334  [76800/175341]\n",
      "loss: 0.141575  [78400/175341]\n",
      "loss: 0.301355  [80000/175341]\n",
      "loss: 0.359044  [81600/175341]\n",
      "loss: 0.529139  [83200/175341]\n",
      "loss: 0.422042  [84800/175341]\n",
      "loss: 0.571882  [86400/175341]\n",
      "loss: 0.114494  [88000/175341]\n",
      "loss: 0.350072  [89600/175341]\n",
      "loss: 0.340154  [91200/175341]\n",
      "loss: 0.442170  [92800/175341]\n",
      "loss: 0.494606  [94400/175341]\n",
      "loss: 0.704984  [96000/175341]\n",
      "loss: 0.477012  [97600/175341]\n",
      "loss: 0.568901  [99200/175341]\n",
      "loss: 0.329746  [100800/175341]\n",
      "loss: 0.579777  [102400/175341]\n",
      "loss: 0.481426  [104000/175341]\n",
      "loss: 0.302515  [105600/175341]\n",
      "loss: 0.560332  [107200/175341]\n",
      "loss: 0.526314  [108800/175341]\n",
      "loss: 0.414046  [110400/175341]\n",
      "loss: 0.387794  [112000/175341]\n",
      "loss: 0.479862  [113600/175341]\n",
      "loss: 0.424736  [115200/175341]\n",
      "loss: 0.669782  [116800/175341]\n",
      "loss: 0.282085  [118400/175341]\n",
      "loss: 0.582653  [120000/175341]\n",
      "loss: 0.522936  [121600/175341]\n",
      "loss: 0.552846  [123200/175341]\n",
      "loss: 0.131482  [124800/175341]\n",
      "loss: 0.231153  [126400/175341]\n",
      "loss: 0.232814  [128000/175341]\n",
      "loss: 0.391248  [129600/175341]\n",
      "loss: 0.697504  [131200/175341]\n",
      "loss: 0.357867  [132800/175341]\n",
      "loss: 0.488787  [134400/175341]\n",
      "loss: 0.453596  [136000/175341]\n",
      "loss: 0.370145  [137600/175341]\n",
      "loss: 0.386778  [139200/175341]\n",
      "loss: 0.382015  [140800/175341]\n",
      "loss: 0.686861  [142400/175341]\n",
      "loss: 0.425797  [144000/175341]\n",
      "loss: 0.462977  [145600/175341]\n",
      "loss: 0.296888  [147200/175341]\n",
      "loss: 0.744355  [148800/175341]\n",
      "loss: 0.201525  [150400/175341]\n",
      "loss: 0.673559  [152000/175341]\n",
      "loss: 0.704718  [153600/175341]\n",
      "loss: 0.484316  [155200/175341]\n",
      "loss: 0.753982  [156800/175341]\n",
      "loss: 0.335153  [158400/175341]\n",
      "loss: 0.828842  [160000/175341]\n",
      "loss: 0.525667  [161600/175341]\n",
      "loss: 0.397099  [163200/175341]\n",
      "loss: 0.159603  [164800/175341]\n",
      "loss: 0.864352  [166400/175341]\n",
      "loss: 0.578529  [168000/175341]\n",
      "loss: 0.406143  [169600/175341]\n",
      "loss: 0.390804  [171200/175341]\n",
      "loss: 0.449990  [172800/175341]\n",
      "loss: 0.234130  [174400/175341]\n",
      "Train Accuracy: 82.0681%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.543624, F1-score: 77.18%, Macro_F1-Score:  43.00%  \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.428806  [    0/175341]\n",
      "loss: 0.596245  [ 1600/175341]\n",
      "loss: 0.493685  [ 3200/175341]\n",
      "loss: 0.273331  [ 4800/175341]\n",
      "loss: 0.227429  [ 6400/175341]\n",
      "loss: 0.294080  [ 8000/175341]\n",
      "loss: 0.131490  [ 9600/175341]\n",
      "loss: 0.413839  [11200/175341]\n",
      "loss: 0.800256  [12800/175341]\n",
      "loss: 0.370244  [14400/175341]\n",
      "loss: 0.542585  [16000/175341]\n",
      "loss: 0.313926  [17600/175341]\n",
      "loss: 0.569944  [19200/175341]\n",
      "loss: 0.548680  [20800/175341]\n",
      "loss: 0.430055  [22400/175341]\n",
      "loss: 0.523821  [24000/175341]\n",
      "loss: 0.502187  [25600/175341]\n",
      "loss: 0.404379  [27200/175341]\n",
      "loss: 0.667358  [28800/175341]\n",
      "loss: 0.410144  [30400/175341]\n",
      "loss: 0.276281  [32000/175341]\n",
      "loss: 0.117939  [33600/175341]\n",
      "loss: 0.229085  [35200/175341]\n",
      "loss: 0.406979  [36800/175341]\n",
      "loss: 0.543658  [38400/175341]\n",
      "loss: 0.488527  [40000/175341]\n",
      "loss: 0.448305  [41600/175341]\n",
      "loss: 0.199775  [43200/175341]\n",
      "loss: 0.639803  [44800/175341]\n",
      "loss: 0.429131  [46400/175341]\n",
      "loss: 0.031912  [48000/175341]\n",
      "loss: 0.601067  [49600/175341]\n",
      "loss: 0.350562  [51200/175341]\n",
      "loss: 0.479609  [52800/175341]\n",
      "loss: 0.603559  [54400/175341]\n",
      "loss: 0.385704  [56000/175341]\n",
      "loss: 0.327071  [57600/175341]\n",
      "loss: 0.631921  [59200/175341]\n",
      "loss: 0.304309  [60800/175341]\n",
      "loss: 0.258209  [62400/175341]\n",
      "loss: 0.545550  [64000/175341]\n",
      "loss: 0.248456  [65600/175341]\n",
      "loss: 0.839976  [67200/175341]\n",
      "loss: 0.280459  [68800/175341]\n",
      "loss: 0.742198  [70400/175341]\n",
      "loss: 0.495756  [72000/175341]\n",
      "loss: 0.500097  [73600/175341]\n",
      "loss: 0.553993  [75200/175341]\n",
      "loss: 0.422312  [76800/175341]\n",
      "loss: 0.420713  [78400/175341]\n",
      "loss: 0.336082  [80000/175341]\n",
      "loss: 0.429026  [81600/175341]\n",
      "loss: 0.665858  [83200/175341]\n",
      "loss: 0.056123  [84800/175341]\n",
      "loss: 0.957587  [86400/175341]\n",
      "loss: 0.378569  [88000/175341]\n",
      "loss: 0.456595  [89600/175341]\n",
      "loss: 0.816343  [91200/175341]\n",
      "loss: 0.669993  [92800/175341]\n",
      "loss: 0.382851  [94400/175341]\n",
      "loss: 0.904527  [96000/175341]\n",
      "loss: 0.494333  [97600/175341]\n",
      "loss: 0.122937  [99200/175341]\n",
      "loss: 0.193369  [100800/175341]\n",
      "loss: 0.538803  [102400/175341]\n",
      "loss: 0.338026  [104000/175341]\n",
      "loss: 0.659087  [105600/175341]\n",
      "loss: 0.487109  [107200/175341]\n",
      "loss: 0.906409  [108800/175341]\n",
      "loss: 0.534888  [110400/175341]\n",
      "loss: 0.357538  [112000/175341]\n",
      "loss: 0.251065  [113600/175341]\n",
      "loss: 0.211686  [115200/175341]\n",
      "loss: 0.493911  [116800/175341]\n",
      "loss: 0.233190  [118400/175341]\n",
      "loss: 0.369569  [120000/175341]\n",
      "loss: 0.396527  [121600/175341]\n",
      "loss: 0.754185  [123200/175341]\n",
      "loss: 0.653210  [124800/175341]\n",
      "loss: 0.518985  [126400/175341]\n",
      "loss: 0.703837  [128000/175341]\n",
      "loss: 0.181023  [129600/175341]\n",
      "loss: 0.374393  [131200/175341]\n",
      "loss: 0.308554  [132800/175341]\n",
      "loss: 0.428437  [134400/175341]\n",
      "loss: 0.341219  [136000/175341]\n",
      "loss: 0.409171  [137600/175341]\n",
      "loss: 0.471098  [139200/175341]\n",
      "loss: 0.743229  [140800/175341]\n",
      "loss: 0.658264  [142400/175341]\n",
      "loss: 0.315089  [144000/175341]\n",
      "loss: 0.600810  [145600/175341]\n",
      "loss: 0.718312  [147200/175341]\n",
      "loss: 0.368124  [148800/175341]\n",
      "loss: 0.257609  [150400/175341]\n",
      "loss: 0.173184  [152000/175341]\n",
      "loss: 0.452586  [153600/175341]\n",
      "loss: 0.342388  [155200/175341]\n",
      "loss: 0.661512  [156800/175341]\n",
      "loss: 0.346045  [158400/175341]\n",
      "loss: 0.261369  [160000/175341]\n",
      "loss: 0.120194  [161600/175341]\n",
      "loss: 0.835368  [163200/175341]\n",
      "loss: 0.360252  [164800/175341]\n",
      "loss: 0.340771  [166400/175341]\n",
      "loss: 0.478382  [168000/175341]\n",
      "loss: 0.318822  [169600/175341]\n",
      "loss: 0.557079  [171200/175341]\n",
      "loss: 0.298983  [172800/175341]\n",
      "loss: 0.452725  [174400/175341]\n",
      "Train Accuracy: 82.1211%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.560613, F1-score: 76.41%, Macro_F1-Score:  42.39%  \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.542807  [    0/175341]\n",
      "loss: 0.445547  [ 1600/175341]\n",
      "loss: 0.792683  [ 3200/175341]\n",
      "loss: 0.461062  [ 4800/175341]\n",
      "loss: 0.468103  [ 6400/175341]\n",
      "loss: 0.261910  [ 8000/175341]\n",
      "loss: 0.599998  [ 9600/175341]\n",
      "loss: 0.282523  [11200/175341]\n",
      "loss: 0.587703  [12800/175341]\n",
      "loss: 0.518504  [14400/175341]\n",
      "loss: 0.160742  [16000/175341]\n",
      "loss: 0.533744  [17600/175341]\n",
      "loss: 0.228058  [19200/175341]\n",
      "loss: 0.573481  [20800/175341]\n",
      "loss: 0.371294  [22400/175341]\n",
      "loss: 0.113497  [24000/175341]\n",
      "loss: 0.606243  [25600/175341]\n",
      "loss: 0.374394  [27200/175341]\n",
      "loss: 0.667644  [28800/175341]\n",
      "loss: 0.294576  [30400/175341]\n",
      "loss: 0.446005  [32000/175341]\n",
      "loss: 0.330330  [33600/175341]\n",
      "loss: 0.318366  [35200/175341]\n",
      "loss: 0.869922  [36800/175341]\n",
      "loss: 0.405127  [38400/175341]\n",
      "loss: 0.127197  [40000/175341]\n",
      "loss: 0.558414  [41600/175341]\n",
      "loss: 0.665167  [43200/175341]\n",
      "loss: 0.211988  [44800/175341]\n",
      "loss: 0.666030  [46400/175341]\n",
      "loss: 0.517196  [48000/175341]\n",
      "loss: 0.333622  [49600/175341]\n",
      "loss: 0.114867  [51200/175341]\n",
      "loss: 0.998327  [52800/175341]\n",
      "loss: 0.383778  [54400/175341]\n",
      "loss: 0.495238  [56000/175341]\n",
      "loss: 0.396664  [57600/175341]\n",
      "loss: 0.539663  [59200/175341]\n",
      "loss: 0.931684  [60800/175341]\n",
      "loss: 0.376690  [62400/175341]\n",
      "loss: 0.356605  [64000/175341]\n",
      "loss: 0.425437  [65600/175341]\n",
      "loss: 0.340850  [67200/175341]\n",
      "loss: 0.384890  [68800/175341]\n",
      "loss: 0.172165  [70400/175341]\n",
      "loss: 0.282223  [72000/175341]\n",
      "loss: 0.375509  [73600/175341]\n",
      "loss: 0.573033  [75200/175341]\n",
      "loss: 0.480274  [76800/175341]\n",
      "loss: 0.324684  [78400/175341]\n",
      "loss: 0.630136  [80000/175341]\n",
      "loss: 0.326865  [81600/175341]\n",
      "loss: 0.256435  [83200/175341]\n",
      "loss: 0.640573  [84800/175341]\n",
      "loss: 0.518706  [86400/175341]\n",
      "loss: 0.160888  [88000/175341]\n",
      "loss: 0.666658  [89600/175341]\n",
      "loss: 0.474353  [91200/175341]\n",
      "loss: 0.475218  [92800/175341]\n",
      "loss: 0.711741  [94400/175341]\n",
      "loss: 0.373885  [96000/175341]\n",
      "loss: 0.391363  [97600/175341]\n",
      "loss: 0.255520  [99200/175341]\n",
      "loss: 0.629222  [100800/175341]\n",
      "loss: 0.787422  [102400/175341]\n",
      "loss: 0.232504  [104000/175341]\n",
      "loss: 0.616283  [105600/175341]\n",
      "loss: 0.873802  [107200/175341]\n",
      "loss: 0.437808  [108800/175341]\n",
      "loss: 0.647514  [110400/175341]\n",
      "loss: 0.816641  [112000/175341]\n",
      "loss: 0.357232  [113600/175341]\n",
      "loss: 0.483041  [115200/175341]\n",
      "loss: 0.515647  [116800/175341]\n",
      "loss: 0.334842  [118400/175341]\n",
      "loss: 0.373613  [120000/175341]\n",
      "loss: 0.554380  [121600/175341]\n",
      "loss: 0.282534  [123200/175341]\n",
      "loss: 0.353884  [124800/175341]\n",
      "loss: 0.556469  [126400/175341]\n",
      "loss: 0.452241  [128000/175341]\n",
      "loss: 0.541122  [129600/175341]\n",
      "loss: 0.649210  [131200/175341]\n",
      "loss: 0.389173  [132800/175341]\n",
      "loss: 0.540784  [134400/175341]\n",
      "loss: 0.188631  [136000/175341]\n",
      "loss: 0.313577  [137600/175341]\n",
      "loss: 0.463015  [139200/175341]\n",
      "loss: 0.580202  [140800/175341]\n",
      "loss: 0.362609  [142400/175341]\n",
      "loss: 0.560213  [144000/175341]\n",
      "loss: 0.211924  [145600/175341]\n",
      "loss: 0.719820  [147200/175341]\n",
      "loss: 0.487765  [148800/175341]\n",
      "loss: 0.562835  [150400/175341]\n",
      "loss: 0.206185  [152000/175341]\n",
      "loss: 1.062706  [153600/175341]\n",
      "loss: 0.743095  [155200/175341]\n",
      "loss: 0.218853  [156800/175341]\n",
      "loss: 0.231321  [158400/175341]\n",
      "loss: 0.384745  [160000/175341]\n",
      "loss: 0.214883  [161600/175341]\n",
      "loss: 0.409297  [163200/175341]\n",
      "loss: 0.669695  [164800/175341]\n",
      "loss: 0.337057  [166400/175341]\n",
      "loss: 0.520247  [168000/175341]\n",
      "loss: 0.256032  [169600/175341]\n",
      "loss: 0.523513  [171200/175341]\n",
      "loss: 0.435778  [172800/175341]\n",
      "loss: 0.358770  [174400/175341]\n",
      "Train Accuracy: 82.1097%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.545208, F1-score: 77.06%, Macro_F1-Score:  43.11%  \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.354303  [    0/175341]\n",
      "loss: 0.047276  [ 1600/175341]\n",
      "loss: 0.360124  [ 3200/175341]\n",
      "loss: 0.160047  [ 4800/175341]\n",
      "loss: 0.378345  [ 6400/175341]\n",
      "loss: 0.563534  [ 8000/175341]\n",
      "loss: 0.402441  [ 9600/175341]\n",
      "loss: 0.324942  [11200/175341]\n",
      "loss: 0.794268  [12800/175341]\n",
      "loss: 0.157802  [14400/175341]\n",
      "loss: 0.646124  [16000/175341]\n",
      "loss: 0.349111  [17600/175341]\n",
      "loss: 0.532461  [19200/175341]\n",
      "loss: 0.271788  [20800/175341]\n",
      "loss: 0.526836  [22400/175341]\n",
      "loss: 0.724645  [24000/175341]\n",
      "loss: 0.461103  [25600/175341]\n",
      "loss: 0.209760  [27200/175341]\n",
      "loss: 0.246098  [28800/175341]\n",
      "loss: 0.616210  [30400/175341]\n",
      "loss: 0.547608  [32000/175341]\n",
      "loss: 0.518549  [33600/175341]\n",
      "loss: 0.686356  [35200/175341]\n",
      "loss: 0.571377  [36800/175341]\n",
      "loss: 0.445330  [38400/175341]\n",
      "loss: 0.068144  [40000/175341]\n",
      "loss: 0.610735  [41600/175341]\n",
      "loss: 0.127248  [43200/175341]\n",
      "loss: 0.278945  [44800/175341]\n",
      "loss: 0.366913  [46400/175341]\n",
      "loss: 0.328239  [48000/175341]\n",
      "loss: 0.559285  [49600/175341]\n",
      "loss: 0.148412  [51200/175341]\n",
      "loss: 0.161281  [52800/175341]\n",
      "loss: 0.379445  [54400/175341]\n",
      "loss: 0.283523  [56000/175341]\n",
      "loss: 0.084445  [57600/175341]\n",
      "loss: 0.878599  [59200/175341]\n",
      "loss: 0.664361  [60800/175341]\n",
      "loss: 0.775417  [62400/175341]\n",
      "loss: 0.269936  [64000/175341]\n",
      "loss: 0.489346  [65600/175341]\n",
      "loss: 0.759315  [67200/175341]\n",
      "loss: 0.281916  [68800/175341]\n",
      "loss: 0.501444  [70400/175341]\n",
      "loss: 0.453060  [72000/175341]\n",
      "loss: 0.472622  [73600/175341]\n",
      "loss: 0.778750  [75200/175341]\n",
      "loss: 0.282564  [76800/175341]\n",
      "loss: 0.530602  [78400/175341]\n",
      "loss: 0.648454  [80000/175341]\n",
      "loss: 0.340918  [81600/175341]\n",
      "loss: 0.308488  [83200/175341]\n",
      "loss: 0.517944  [84800/175341]\n",
      "loss: 0.543864  [86400/175341]\n",
      "loss: 0.291931  [88000/175341]\n",
      "loss: 0.246131  [89600/175341]\n",
      "loss: 0.149811  [91200/175341]\n",
      "loss: 0.371378  [92800/175341]\n",
      "loss: 0.461892  [94400/175341]\n",
      "loss: 0.166927  [96000/175341]\n",
      "loss: 0.238286  [97600/175341]\n",
      "loss: 0.489937  [99200/175341]\n",
      "loss: 0.427031  [100800/175341]\n",
      "loss: 0.467651  [102400/175341]\n",
      "loss: 0.241132  [104000/175341]\n",
      "loss: 0.111569  [105600/175341]\n",
      "loss: 0.560586  [107200/175341]\n",
      "loss: 0.506571  [108800/175341]\n",
      "loss: 0.566699  [110400/175341]\n",
      "loss: 0.322988  [112000/175341]\n",
      "loss: 0.565070  [113600/175341]\n",
      "loss: 0.649084  [115200/175341]\n",
      "loss: 0.593314  [116800/175341]\n",
      "loss: 0.506767  [118400/175341]\n",
      "loss: 0.273896  [120000/175341]\n",
      "loss: 0.062422  [121600/175341]\n",
      "loss: 0.259923  [123200/175341]\n",
      "loss: 0.331917  [124800/175341]\n",
      "loss: 0.535283  [126400/175341]\n",
      "loss: 0.291546  [128000/175341]\n",
      "loss: 0.317532  [129600/175341]\n",
      "loss: 0.454185  [131200/175341]\n",
      "loss: 0.631743  [132800/175341]\n",
      "loss: 0.399217  [134400/175341]\n",
      "loss: 0.595737  [136000/175341]\n",
      "loss: 0.545146  [137600/175341]\n",
      "loss: 1.009579  [139200/175341]\n",
      "loss: 0.661973  [140800/175341]\n",
      "loss: 0.844154  [142400/175341]\n",
      "loss: 0.565758  [144000/175341]\n",
      "loss: 0.300045  [145600/175341]\n",
      "loss: 0.844295  [147200/175341]\n",
      "loss: 0.423648  [148800/175341]\n",
      "loss: 0.383955  [150400/175341]\n",
      "loss: 0.602800  [152000/175341]\n",
      "loss: 0.665300  [153600/175341]\n",
      "loss: 0.701541  [155200/175341]\n",
      "loss: 0.308227  [156800/175341]\n",
      "loss: 0.556188  [158400/175341]\n",
      "loss: 0.404630  [160000/175341]\n",
      "loss: 0.445676  [161600/175341]\n",
      "loss: 0.287247  [163200/175341]\n",
      "loss: 0.244399  [164800/175341]\n",
      "loss: 0.424061  [166400/175341]\n",
      "loss: 0.413124  [168000/175341]\n",
      "loss: 0.131732  [169600/175341]\n",
      "loss: 0.607053  [171200/175341]\n",
      "loss: 0.306913  [172800/175341]\n",
      "loss: 0.344057  [174400/175341]\n",
      "Train Accuracy: 82.1428%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.547952, F1-score: 76.61%, Macro_F1-Score:  42.45%  \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.403748  [    0/175341]\n",
      "loss: 0.382161  [ 1600/175341]\n",
      "loss: 0.666120  [ 3200/175341]\n",
      "loss: 0.442007  [ 4800/175341]\n",
      "loss: 0.398766  [ 6400/175341]\n",
      "loss: 0.299384  [ 8000/175341]\n",
      "loss: 0.340096  [ 9600/175341]\n",
      "loss: 0.065888  [11200/175341]\n",
      "loss: 0.289648  [12800/175341]\n",
      "loss: 0.637917  [14400/175341]\n",
      "loss: 0.358691  [16000/175341]\n",
      "loss: 0.358208  [17600/175341]\n",
      "loss: 0.360556  [19200/175341]\n",
      "loss: 0.384413  [20800/175341]\n",
      "loss: 0.338074  [22400/175341]\n",
      "loss: 0.386452  [24000/175341]\n",
      "loss: 0.102199  [25600/175341]\n",
      "loss: 0.262544  [27200/175341]\n",
      "loss: 0.284721  [28800/175341]\n",
      "loss: 0.358149  [30400/175341]\n",
      "loss: 0.168254  [32000/175341]\n",
      "loss: 0.296890  [33600/175341]\n",
      "loss: 0.207563  [35200/175341]\n",
      "loss: 0.517988  [36800/175341]\n",
      "loss: 0.617186  [38400/175341]\n",
      "loss: 0.277310  [40000/175341]\n",
      "loss: 0.436068  [41600/175341]\n",
      "loss: 0.479432  [43200/175341]\n",
      "loss: 0.392677  [44800/175341]\n",
      "loss: 1.017361  [46400/175341]\n",
      "loss: 0.525340  [48000/175341]\n",
      "loss: 0.919294  [49600/175341]\n",
      "loss: 0.657511  [51200/175341]\n",
      "loss: 0.829644  [52800/175341]\n",
      "loss: 0.224547  [54400/175341]\n",
      "loss: 0.258128  [56000/175341]\n",
      "loss: 0.674786  [57600/175341]\n",
      "loss: 0.318631  [59200/175341]\n",
      "loss: 0.312640  [60800/175341]\n",
      "loss: 0.620807  [62400/175341]\n",
      "loss: 0.107102  [64000/175341]\n",
      "loss: 0.415978  [65600/175341]\n",
      "loss: 0.685057  [67200/175341]\n",
      "loss: 0.749346  [68800/175341]\n",
      "loss: 0.309529  [70400/175341]\n",
      "loss: 0.438527  [72000/175341]\n",
      "loss: 0.308656  [73600/175341]\n",
      "loss: 0.447903  [75200/175341]\n",
      "loss: 0.803741  [76800/175341]\n",
      "loss: 0.636735  [78400/175341]\n",
      "loss: 0.629242  [80000/175341]\n",
      "loss: 0.353117  [81600/175341]\n",
      "loss: 0.592974  [83200/175341]\n",
      "loss: 0.307087  [84800/175341]\n",
      "loss: 0.523281  [86400/175341]\n",
      "loss: 1.032631  [88000/175341]\n",
      "loss: 0.250484  [89600/175341]\n",
      "loss: 0.541435  [91200/175341]\n",
      "loss: 0.155101  [92800/175341]\n",
      "loss: 0.593806  [94400/175341]\n",
      "loss: 0.406387  [96000/175341]\n",
      "loss: 0.365906  [97600/175341]\n",
      "loss: 0.484465  [99200/175341]\n",
      "loss: 0.529956  [100800/175341]\n",
      "loss: 0.443925  [102400/175341]\n",
      "loss: 0.521156  [104000/175341]\n",
      "loss: 0.298337  [105600/175341]\n",
      "loss: 0.282100  [107200/175341]\n",
      "loss: 0.508662  [108800/175341]\n",
      "loss: 0.373221  [110400/175341]\n",
      "loss: 0.638616  [112000/175341]\n",
      "loss: 0.385197  [113600/175341]\n",
      "loss: 0.389137  [115200/175341]\n",
      "loss: 0.368459  [116800/175341]\n",
      "loss: 0.607615  [118400/175341]\n",
      "loss: 0.536812  [120000/175341]\n",
      "loss: 0.533619  [121600/175341]\n",
      "loss: 0.464652  [123200/175341]\n",
      "loss: 0.265649  [124800/175341]\n",
      "loss: 0.223507  [126400/175341]\n",
      "loss: 0.127701  [128000/175341]\n",
      "loss: 0.312531  [129600/175341]\n",
      "loss: 0.510300  [131200/175341]\n",
      "loss: 0.670444  [132800/175341]\n",
      "loss: 0.411235  [134400/175341]\n",
      "loss: 0.352152  [136000/175341]\n",
      "loss: 0.344343  [137600/175341]\n",
      "loss: 0.467404  [139200/175341]\n",
      "loss: 0.127408  [140800/175341]\n",
      "loss: 0.401614  [142400/175341]\n",
      "loss: 0.623412  [144000/175341]\n",
      "loss: 0.336661  [145600/175341]\n",
      "loss: 0.368262  [147200/175341]\n",
      "loss: 0.707812  [148800/175341]\n",
      "loss: 0.157817  [150400/175341]\n",
      "loss: 0.766544  [152000/175341]\n",
      "loss: 0.267660  [153600/175341]\n",
      "loss: 0.340255  [155200/175341]\n",
      "loss: 0.663658  [156800/175341]\n",
      "loss: 0.673608  [158400/175341]\n",
      "loss: 0.335948  [160000/175341]\n",
      "loss: 0.434406  [161600/175341]\n",
      "loss: 0.397875  [163200/175341]\n",
      "loss: 0.589415  [164800/175341]\n",
      "loss: 0.198756  [166400/175341]\n",
      "loss: 0.768773  [168000/175341]\n",
      "loss: 0.211588  [169600/175341]\n",
      "loss: 0.240259  [171200/175341]\n",
      "loss: 0.627524  [172800/175341]\n",
      "loss: 0.310234  [174400/175341]\n",
      "Train Accuracy: 82.1040%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.557722, F1-score: 76.40%, Macro_F1-Score:  42.28%  \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.405457  [    0/175341]\n",
      "loss: 0.398592  [ 1600/175341]\n",
      "loss: 0.330311  [ 3200/175341]\n",
      "loss: 0.378869  [ 4800/175341]\n",
      "loss: 0.400200  [ 6400/175341]\n",
      "loss: 0.069354  [ 8000/175341]\n",
      "loss: 0.581789  [ 9600/175341]\n",
      "loss: 0.650743  [11200/175341]\n",
      "loss: 0.579745  [12800/175341]\n",
      "loss: 0.534908  [14400/175341]\n",
      "loss: 0.331580  [16000/175341]\n",
      "loss: 0.482222  [17600/175341]\n",
      "loss: 0.158057  [19200/175341]\n",
      "loss: 0.553045  [20800/175341]\n",
      "loss: 0.591671  [22400/175341]\n",
      "loss: 0.340597  [24000/175341]\n",
      "loss: 0.678550  [25600/175341]\n",
      "loss: 0.308759  [27200/175341]\n",
      "loss: 0.514282  [28800/175341]\n",
      "loss: 0.715777  [30400/175341]\n",
      "loss: 0.479602  [32000/175341]\n",
      "loss: 0.478375  [33600/175341]\n",
      "loss: 0.583873  [35200/175341]\n",
      "loss: 0.403455  [36800/175341]\n",
      "loss: 0.214614  [38400/175341]\n",
      "loss: 0.326387  [40000/175341]\n",
      "loss: 0.105410  [41600/175341]\n",
      "loss: 0.402335  [43200/175341]\n",
      "loss: 0.230571  [44800/175341]\n",
      "loss: 0.791382  [46400/175341]\n",
      "loss: 0.586151  [48000/175341]\n",
      "loss: 0.275987  [49600/175341]\n",
      "loss: 0.464934  [51200/175341]\n",
      "loss: 0.640725  [52800/175341]\n",
      "loss: 0.453512  [54400/175341]\n",
      "loss: 0.740095  [56000/175341]\n",
      "loss: 0.629596  [57600/175341]\n",
      "loss: 0.092279  [59200/175341]\n",
      "loss: 0.346195  [60800/175341]\n",
      "loss: 0.300451  [62400/175341]\n",
      "loss: 0.313151  [64000/175341]\n",
      "loss: 0.560369  [65600/175341]\n",
      "loss: 0.133399  [67200/175341]\n",
      "loss: 0.331770  [68800/175341]\n",
      "loss: 0.255194  [70400/175341]\n",
      "loss: 0.445973  [72000/175341]\n",
      "loss: 1.093420  [73600/175341]\n",
      "loss: 0.622522  [75200/175341]\n",
      "loss: 0.483446  [76800/175341]\n",
      "loss: 0.231690  [78400/175341]\n",
      "loss: 0.497037  [80000/175341]\n",
      "loss: 0.316914  [81600/175341]\n",
      "loss: 0.521918  [83200/175341]\n",
      "loss: 0.234846  [84800/175341]\n",
      "loss: 0.291494  [86400/175341]\n",
      "loss: 0.614326  [88000/175341]\n",
      "loss: 0.614971  [89600/175341]\n",
      "loss: 0.227964  [91200/175341]\n",
      "loss: 0.186107  [92800/175341]\n",
      "loss: 0.477198  [94400/175341]\n",
      "loss: 0.402501  [96000/175341]\n",
      "loss: 0.714471  [97600/175341]\n",
      "loss: 0.977761  [99200/175341]\n",
      "loss: 0.162585  [100800/175341]\n",
      "loss: 0.479451  [102400/175341]\n",
      "loss: 0.145269  [104000/175341]\n",
      "loss: 0.483917  [105600/175341]\n",
      "loss: 0.542749  [107200/175341]\n",
      "loss: 0.409208  [108800/175341]\n",
      "loss: 0.440386  [110400/175341]\n",
      "loss: 0.739280  [112000/175341]\n",
      "loss: 0.509938  [113600/175341]\n",
      "loss: 0.424237  [115200/175341]\n",
      "loss: 0.643689  [116800/175341]\n",
      "loss: 0.852120  [118400/175341]\n",
      "loss: 0.579028  [120000/175341]\n",
      "loss: 0.588803  [121600/175341]\n",
      "loss: 0.231850  [123200/175341]\n",
      "loss: 0.239565  [124800/175341]\n",
      "loss: 0.749801  [126400/175341]\n",
      "loss: 0.345225  [128000/175341]\n",
      "loss: 0.704899  [129600/175341]\n",
      "loss: 0.488676  [131200/175341]\n",
      "loss: 0.372215  [132800/175341]\n",
      "loss: 0.478160  [134400/175341]\n",
      "loss: 1.132014  [136000/175341]\n",
      "loss: 0.334848  [137600/175341]\n",
      "loss: 0.356632  [139200/175341]\n",
      "loss: 0.745140  [140800/175341]\n",
      "loss: 0.214013  [142400/175341]\n",
      "loss: 0.109750  [144000/175341]\n",
      "loss: 0.129370  [145600/175341]\n",
      "loss: 0.458998  [147200/175341]\n",
      "loss: 0.279010  [148800/175341]\n",
      "loss: 0.408085  [150400/175341]\n",
      "loss: 0.181368  [152000/175341]\n",
      "loss: 0.596169  [153600/175341]\n",
      "loss: 0.564756  [155200/175341]\n",
      "loss: 0.602428  [156800/175341]\n",
      "loss: 0.472519  [158400/175341]\n",
      "loss: 0.334045  [160000/175341]\n",
      "loss: 0.783613  [161600/175341]\n",
      "loss: 0.757613  [163200/175341]\n",
      "loss: 0.624918  [164800/175341]\n",
      "loss: 0.402820  [166400/175341]\n",
      "loss: 0.122856  [168000/175341]\n",
      "loss: 0.722889  [169600/175341]\n",
      "loss: 0.717121  [171200/175341]\n",
      "loss: 0.451162  [172800/175341]\n",
      "loss: 0.788295  [174400/175341]\n",
      "Train Accuracy: 82.1103%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.564877, F1-score: 76.54%, Macro_F1-Score:  41.88%  \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.984169  [    0/175341]\n",
      "loss: 0.419441  [ 1600/175341]\n",
      "loss: 0.096813  [ 3200/175341]\n",
      "loss: 0.373916  [ 4800/175341]\n",
      "loss: 0.339633  [ 6400/175341]\n",
      "loss: 0.070231  [ 8000/175341]\n",
      "loss: 0.869072  [ 9600/175341]\n",
      "loss: 0.353298  [11200/175341]\n",
      "loss: 0.421342  [12800/175341]\n",
      "loss: 0.487610  [14400/175341]\n",
      "loss: 0.253389  [16000/175341]\n",
      "loss: 0.376116  [17600/175341]\n",
      "loss: 0.249782  [19200/175341]\n",
      "loss: 0.354810  [20800/175341]\n",
      "loss: 0.585256  [22400/175341]\n",
      "loss: 0.311936  [24000/175341]\n",
      "loss: 0.346600  [25600/175341]\n",
      "loss: 0.276787  [27200/175341]\n",
      "loss: 0.441392  [28800/175341]\n",
      "loss: 0.502538  [30400/175341]\n",
      "loss: 0.717549  [32000/175341]\n",
      "loss: 0.131483  [33600/175341]\n",
      "loss: 0.623936  [35200/175341]\n",
      "loss: 0.095028  [36800/175341]\n",
      "loss: 0.100395  [38400/175341]\n",
      "loss: 0.395552  [40000/175341]\n",
      "loss: 0.497846  [41600/175341]\n",
      "loss: 0.658425  [43200/175341]\n",
      "loss: 0.949217  [44800/175341]\n",
      "loss: 0.551123  [46400/175341]\n",
      "loss: 0.619429  [48000/175341]\n",
      "loss: 0.408262  [49600/175341]\n",
      "loss: 0.264319  [51200/175341]\n",
      "loss: 0.504926  [52800/175341]\n",
      "loss: 0.354084  [54400/175341]\n",
      "loss: 0.377520  [56000/175341]\n",
      "loss: 0.379328  [57600/175341]\n",
      "loss: 0.237685  [59200/175341]\n",
      "loss: 0.774812  [60800/175341]\n",
      "loss: 0.427198  [62400/175341]\n",
      "loss: 0.331834  [64000/175341]\n",
      "loss: 0.407820  [65600/175341]\n",
      "loss: 0.396265  [67200/175341]\n",
      "loss: 0.629001  [68800/175341]\n",
      "loss: 0.488378  [70400/175341]\n",
      "loss: 0.645626  [72000/175341]\n",
      "loss: 0.371546  [73600/175341]\n",
      "loss: 0.303344  [75200/175341]\n",
      "loss: 0.762493  [76800/175341]\n",
      "loss: 0.669839  [78400/175341]\n",
      "loss: 0.640733  [80000/175341]\n",
      "loss: 0.374741  [81600/175341]\n",
      "loss: 0.603324  [83200/175341]\n",
      "loss: 0.404789  [84800/175341]\n",
      "loss: 0.527290  [86400/175341]\n",
      "loss: 0.520830  [88000/175341]\n",
      "loss: 0.569624  [89600/175341]\n",
      "loss: 0.218143  [91200/175341]\n",
      "loss: 0.641737  [92800/175341]\n",
      "loss: 0.477605  [94400/175341]\n",
      "loss: 0.786546  [96000/175341]\n",
      "loss: 0.451472  [97600/175341]\n",
      "loss: 0.336716  [99200/175341]\n",
      "loss: 0.296912  [100800/175341]\n",
      "loss: 0.387706  [102400/175341]\n",
      "loss: 0.356313  [104000/175341]\n",
      "loss: 0.279077  [105600/175341]\n",
      "loss: 0.656567  [107200/175341]\n",
      "loss: 0.194835  [108800/175341]\n",
      "loss: 0.444531  [110400/175341]\n",
      "loss: 0.346420  [112000/175341]\n",
      "loss: 0.564536  [113600/175341]\n",
      "loss: 0.843468  [115200/175341]\n",
      "loss: 0.591018  [116800/175341]\n",
      "loss: 0.431045  [118400/175341]\n",
      "loss: 0.455921  [120000/175341]\n",
      "loss: 0.627406  [121600/175341]\n",
      "loss: 0.798380  [123200/175341]\n",
      "loss: 0.280998  [124800/175341]\n",
      "loss: 0.379456  [126400/175341]\n",
      "loss: 0.649309  [128000/175341]\n",
      "loss: 0.271952  [129600/175341]\n",
      "loss: 0.609639  [131200/175341]\n",
      "loss: 0.350927  [132800/175341]\n",
      "loss: 0.175781  [134400/175341]\n",
      "loss: 0.373883  [136000/175341]\n",
      "loss: 0.364453  [137600/175341]\n",
      "loss: 0.582132  [139200/175341]\n",
      "loss: 0.312511  [140800/175341]\n",
      "loss: 0.260797  [142400/175341]\n",
      "loss: 0.117093  [144000/175341]\n",
      "loss: 0.569696  [145600/175341]\n",
      "loss: 0.506607  [147200/175341]\n",
      "loss: 0.915517  [148800/175341]\n",
      "loss: 0.431817  [150400/175341]\n",
      "loss: 0.538091  [152000/175341]\n",
      "loss: 0.675107  [153600/175341]\n",
      "loss: 0.831444  [155200/175341]\n",
      "loss: 0.207282  [156800/175341]\n",
      "loss: 0.427817  [158400/175341]\n",
      "loss: 0.473683  [160000/175341]\n",
      "loss: 0.551030  [161600/175341]\n",
      "loss: 0.670919  [163200/175341]\n",
      "loss: 0.346680  [164800/175341]\n",
      "loss: 0.574078  [166400/175341]\n",
      "loss: 0.409337  [168000/175341]\n",
      "loss: 0.735959  [169600/175341]\n",
      "loss: 0.171784  [171200/175341]\n",
      "loss: 0.880087  [172800/175341]\n",
      "loss: 0.520293  [174400/175341]\n",
      "Train Accuracy: 82.1126%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.556813, F1-score: 76.66%, Macro_F1-Score:  43.80%  \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.341032  [    0/175341]\n",
      "loss: 0.026808  [ 1600/175341]\n",
      "loss: 0.182207  [ 3200/175341]\n",
      "loss: 0.356754  [ 4800/175341]\n",
      "loss: 0.375664  [ 6400/175341]\n",
      "loss: 0.375246  [ 8000/175341]\n",
      "loss: 0.433410  [ 9600/175341]\n",
      "loss: 0.628213  [11200/175341]\n",
      "loss: 0.245097  [12800/175341]\n",
      "loss: 0.592641  [14400/175341]\n",
      "loss: 0.325649  [16000/175341]\n",
      "loss: 0.483809  [17600/175341]\n",
      "loss: 0.300421  [19200/175341]\n",
      "loss: 0.370847  [20800/175341]\n",
      "loss: 0.350385  [22400/175341]\n",
      "loss: 0.103455  [24000/175341]\n",
      "loss: 0.234482  [25600/175341]\n",
      "loss: 0.668177  [27200/175341]\n",
      "loss: 0.434049  [28800/175341]\n",
      "loss: 0.814220  [30400/175341]\n",
      "loss: 0.519274  [32000/175341]\n",
      "loss: 0.243798  [33600/175341]\n",
      "loss: 0.679786  [35200/175341]\n",
      "loss: 0.195256  [36800/175341]\n",
      "loss: 0.559094  [38400/175341]\n",
      "loss: 0.549388  [40000/175341]\n",
      "loss: 0.493366  [41600/175341]\n",
      "loss: 0.409507  [43200/175341]\n",
      "loss: 0.747732  [44800/175341]\n",
      "loss: 0.241840  [46400/175341]\n",
      "loss: 0.259852  [48000/175341]\n",
      "loss: 0.421402  [49600/175341]\n",
      "loss: 0.290121  [51200/175341]\n",
      "loss: 0.427069  [52800/175341]\n",
      "loss: 0.372447  [54400/175341]\n",
      "loss: 0.807105  [56000/175341]\n",
      "loss: 0.238493  [57600/175341]\n",
      "loss: 0.488915  [59200/175341]\n",
      "loss: 1.003495  [60800/175341]\n",
      "loss: 0.800418  [62400/175341]\n",
      "loss: 0.494583  [64000/175341]\n",
      "loss: 0.772494  [65600/175341]\n",
      "loss: 0.682314  [67200/175341]\n",
      "loss: 0.481762  [68800/175341]\n",
      "loss: 0.709322  [70400/175341]\n",
      "loss: 0.507799  [72000/175341]\n",
      "loss: 0.346833  [73600/175341]\n",
      "loss: 0.427501  [75200/175341]\n",
      "loss: 0.322597  [76800/175341]\n",
      "loss: 0.357908  [78400/175341]\n",
      "loss: 0.689602  [80000/175341]\n",
      "loss: 0.688935  [81600/175341]\n",
      "loss: 0.624598  [83200/175341]\n",
      "loss: 0.703630  [84800/175341]\n",
      "loss: 0.604179  [86400/175341]\n",
      "loss: 0.695134  [88000/175341]\n",
      "loss: 0.431482  [89600/175341]\n",
      "loss: 0.430316  [91200/175341]\n",
      "loss: 0.482094  [92800/175341]\n",
      "loss: 0.651376  [94400/175341]\n",
      "loss: 0.428655  [96000/175341]\n",
      "loss: 0.454830  [97600/175341]\n",
      "loss: 0.362927  [99200/175341]\n",
      "loss: 0.686294  [100800/175341]\n",
      "loss: 0.217332  [102400/175341]\n",
      "loss: 0.209366  [104000/175341]\n",
      "loss: 0.227383  [105600/175341]\n",
      "loss: 0.173895  [107200/175341]\n",
      "loss: 0.541626  [108800/175341]\n",
      "loss: 0.231830  [110400/175341]\n",
      "loss: 0.386498  [112000/175341]\n",
      "loss: 0.382905  [113600/175341]\n",
      "loss: 0.730942  [115200/175341]\n",
      "loss: 0.442408  [116800/175341]\n",
      "loss: 0.181186  [118400/175341]\n",
      "loss: 0.685441  [120000/175341]\n",
      "loss: 0.234872  [121600/175341]\n",
      "loss: 0.274596  [123200/175341]\n",
      "loss: 0.942430  [124800/175341]\n",
      "loss: 0.238247  [126400/175341]\n",
      "loss: 0.271374  [128000/175341]\n",
      "loss: 0.627521  [129600/175341]\n",
      "loss: 0.682785  [131200/175341]\n",
      "loss: 0.757589  [132800/175341]\n",
      "loss: 0.847696  [134400/175341]\n",
      "loss: 0.488939  [136000/175341]\n",
      "loss: 0.533682  [137600/175341]\n",
      "loss: 0.341567  [139200/175341]\n",
      "loss: 0.301487  [140800/175341]\n",
      "loss: 0.386265  [142400/175341]\n",
      "loss: 0.426759  [144000/175341]\n",
      "loss: 0.258389  [145600/175341]\n",
      "loss: 0.493546  [147200/175341]\n",
      "loss: 0.254857  [148800/175341]\n",
      "loss: 0.186737  [150400/175341]\n",
      "loss: 0.636207  [152000/175341]\n",
      "loss: 0.717087  [153600/175341]\n",
      "loss: 0.605622  [155200/175341]\n",
      "loss: 0.679913  [156800/175341]\n",
      "loss: 0.515121  [158400/175341]\n",
      "loss: 0.298349  [160000/175341]\n",
      "loss: 0.776654  [161600/175341]\n",
      "loss: 0.907634  [163200/175341]\n",
      "loss: 0.415412  [164800/175341]\n",
      "loss: 0.239340  [166400/175341]\n",
      "loss: 0.196999  [168000/175341]\n",
      "loss: 0.605040  [169600/175341]\n",
      "loss: 0.165927  [171200/175341]\n",
      "loss: 0.506619  [172800/175341]\n",
      "loss: 0.483341  [174400/175341]\n",
      "Train Accuracy: 82.0920%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.541917, F1-score: 76.89%, Macro_F1-Score:  43.33%  \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.369581  [    0/175341]\n",
      "loss: 0.235695  [ 1600/175341]\n",
      "loss: 0.475151  [ 3200/175341]\n",
      "loss: 0.631928  [ 4800/175341]\n",
      "loss: 0.634070  [ 6400/175341]\n",
      "loss: 0.504239  [ 8000/175341]\n",
      "loss: 0.670049  [ 9600/175341]\n",
      "loss: 0.259081  [11200/175341]\n",
      "loss: 0.569146  [12800/175341]\n",
      "loss: 0.394701  [14400/175341]\n",
      "loss: 0.397943  [16000/175341]\n",
      "loss: 0.833449  [17600/175341]\n",
      "loss: 0.215248  [19200/175341]\n",
      "loss: 0.296445  [20800/175341]\n",
      "loss: 0.474853  [22400/175341]\n",
      "loss: 0.399405  [24000/175341]\n",
      "loss: 0.544635  [25600/175341]\n",
      "loss: 0.681265  [27200/175341]\n",
      "loss: 0.602758  [28800/175341]\n",
      "loss: 0.496801  [30400/175341]\n",
      "loss: 0.358011  [32000/175341]\n",
      "loss: 0.861747  [33600/175341]\n",
      "loss: 0.396309  [35200/175341]\n",
      "loss: 0.860079  [36800/175341]\n",
      "loss: 0.297479  [38400/175341]\n",
      "loss: 0.223360  [40000/175341]\n",
      "loss: 0.557371  [41600/175341]\n",
      "loss: 0.260507  [43200/175341]\n",
      "loss: 0.610416  [44800/175341]\n",
      "loss: 0.668058  [46400/175341]\n",
      "loss: 0.288183  [48000/175341]\n",
      "loss: 0.531217  [49600/175341]\n",
      "loss: 0.457216  [51200/175341]\n",
      "loss: 0.631857  [52800/175341]\n",
      "loss: 0.521512  [54400/175341]\n",
      "loss: 0.690366  [56000/175341]\n",
      "loss: 0.559585  [57600/175341]\n",
      "loss: 0.458471  [59200/175341]\n",
      "loss: 0.479023  [60800/175341]\n",
      "loss: 0.202868  [62400/175341]\n",
      "loss: 0.581723  [64000/175341]\n",
      "loss: 0.334533  [65600/175341]\n",
      "loss: 1.281391  [67200/175341]\n",
      "loss: 0.363661  [68800/175341]\n",
      "loss: 0.517832  [70400/175341]\n",
      "loss: 0.528009  [72000/175341]\n",
      "loss: 0.254725  [73600/175341]\n",
      "loss: 0.759748  [75200/175341]\n",
      "loss: 0.156885  [76800/175341]\n",
      "loss: 0.741766  [78400/175341]\n",
      "loss: 0.544866  [80000/175341]\n",
      "loss: 0.671281  [81600/175341]\n",
      "loss: 0.656909  [83200/175341]\n",
      "loss: 0.218221  [84800/175341]\n",
      "loss: 0.196567  [86400/175341]\n",
      "loss: 0.344005  [88000/175341]\n",
      "loss: 0.456890  [89600/175341]\n",
      "loss: 0.252886  [91200/175341]\n",
      "loss: 0.329940  [92800/175341]\n",
      "loss: 0.275901  [94400/175341]\n",
      "loss: 0.385116  [96000/175341]\n",
      "loss: 0.462688  [97600/175341]\n",
      "loss: 0.206756  [99200/175341]\n",
      "loss: 0.196238  [100800/175341]\n",
      "loss: 0.641492  [102400/175341]\n",
      "loss: 0.216545  [104000/175341]\n",
      "loss: 0.157503  [105600/175341]\n",
      "loss: 0.536669  [107200/175341]\n",
      "loss: 0.596446  [108800/175341]\n",
      "loss: 0.589329  [110400/175341]\n",
      "loss: 0.400619  [112000/175341]\n",
      "loss: 0.712161  [113600/175341]\n",
      "loss: 0.795420  [115200/175341]\n",
      "loss: 0.392326  [116800/175341]\n",
      "loss: 0.397087  [118400/175341]\n",
      "loss: 1.039097  [120000/175341]\n",
      "loss: 0.255270  [121600/175341]\n",
      "loss: 0.501687  [123200/175341]\n",
      "loss: 0.573615  [124800/175341]\n",
      "loss: 0.484918  [126400/175341]\n",
      "loss: 0.737550  [128000/175341]\n",
      "loss: 0.359482  [129600/175341]\n",
      "loss: 0.277139  [131200/175341]\n",
      "loss: 1.216021  [132800/175341]\n",
      "loss: 0.615915  [134400/175341]\n",
      "loss: 0.663093  [136000/175341]\n",
      "loss: 0.385337  [137600/175341]\n",
      "loss: 0.436557  [139200/175341]\n",
      "loss: 0.495972  [140800/175341]\n",
      "loss: 0.422418  [142400/175341]\n",
      "loss: 0.133158  [144000/175341]\n",
      "loss: 0.262221  [145600/175341]\n",
      "loss: 0.481562  [147200/175341]\n",
      "loss: 0.848648  [148800/175341]\n",
      "loss: 0.491425  [150400/175341]\n",
      "loss: 0.771649  [152000/175341]\n",
      "loss: 0.321335  [153600/175341]\n",
      "loss: 0.522697  [155200/175341]\n",
      "loss: 0.558564  [156800/175341]\n",
      "loss: 0.614728  [158400/175341]\n",
      "loss: 0.505372  [160000/175341]\n",
      "loss: 0.368271  [161600/175341]\n",
      "loss: 0.251706  [163200/175341]\n",
      "loss: 0.822137  [164800/175341]\n",
      "loss: 0.251624  [166400/175341]\n",
      "loss: 0.538135  [168000/175341]\n",
      "loss: 0.474194  [169600/175341]\n",
      "loss: 0.281763  [171200/175341]\n",
      "loss: 0.301113  [172800/175341]\n",
      "loss: 0.198513  [174400/175341]\n",
      "Train Accuracy: 82.1206%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.556859, F1-score: 76.66%, Macro_F1-Score:  42.38%  \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.547903  [    0/175341]\n",
      "loss: 0.142319  [ 1600/175341]\n",
      "loss: 0.528091  [ 3200/175341]\n",
      "loss: 0.784706  [ 4800/175341]\n",
      "loss: 0.471097  [ 6400/175341]\n",
      "loss: 0.645479  [ 8000/175341]\n",
      "loss: 0.858192  [ 9600/175341]\n",
      "loss: 0.314801  [11200/175341]\n",
      "loss: 0.248295  [12800/175341]\n",
      "loss: 0.153780  [14400/175341]\n",
      "loss: 0.569634  [16000/175341]\n",
      "loss: 0.189259  [17600/175341]\n",
      "loss: 0.334395  [19200/175341]\n",
      "loss: 0.841092  [20800/175341]\n",
      "loss: 0.375744  [22400/175341]\n",
      "loss: 0.849687  [24000/175341]\n",
      "loss: 0.395174  [25600/175341]\n",
      "loss: 0.363772  [27200/175341]\n",
      "loss: 0.922531  [28800/175341]\n",
      "loss: 0.320714  [30400/175341]\n",
      "loss: 0.665899  [32000/175341]\n",
      "loss: 0.237050  [33600/175341]\n",
      "loss: 0.225142  [35200/175341]\n",
      "loss: 0.064605  [36800/175341]\n",
      "loss: 0.512562  [38400/175341]\n",
      "loss: 0.481918  [40000/175341]\n",
      "loss: 0.778637  [41600/175341]\n",
      "loss: 0.430969  [43200/175341]\n",
      "loss: 0.968320  [44800/175341]\n",
      "loss: 0.496623  [46400/175341]\n",
      "loss: 0.295997  [48000/175341]\n",
      "loss: 0.352750  [49600/175341]\n",
      "loss: 0.537257  [51200/175341]\n",
      "loss: 0.655275  [52800/175341]\n",
      "loss: 0.538826  [54400/175341]\n",
      "loss: 0.444211  [56000/175341]\n",
      "loss: 0.303037  [57600/175341]\n",
      "loss: 0.770825  [59200/175341]\n",
      "loss: 0.588892  [60800/175341]\n",
      "loss: 0.625316  [62400/175341]\n",
      "loss: 0.556661  [64000/175341]\n",
      "loss: 0.254192  [65600/175341]\n",
      "loss: 0.312316  [67200/175341]\n",
      "loss: 0.215194  [68800/175341]\n",
      "loss: 0.230220  [70400/175341]\n",
      "loss: 0.487887  [72000/175341]\n",
      "loss: 0.592121  [73600/175341]\n",
      "loss: 0.595983  [75200/175341]\n",
      "loss: 0.479216  [76800/175341]\n",
      "loss: 0.324104  [78400/175341]\n",
      "loss: 0.631547  [80000/175341]\n",
      "loss: 0.550217  [81600/175341]\n",
      "loss: 0.455919  [83200/175341]\n",
      "loss: 0.359393  [84800/175341]\n",
      "loss: 0.420125  [86400/175341]\n",
      "loss: 0.186288  [88000/175341]\n",
      "loss: 0.254821  [89600/175341]\n",
      "loss: 0.508951  [91200/175341]\n",
      "loss: 0.709345  [92800/175341]\n",
      "loss: 0.217850  [94400/175341]\n",
      "loss: 0.331702  [96000/175341]\n",
      "loss: 0.302672  [97600/175341]\n",
      "loss: 0.492279  [99200/175341]\n",
      "loss: 0.526048  [100800/175341]\n",
      "loss: 0.128337  [102400/175341]\n",
      "loss: 0.424008  [104000/175341]\n",
      "loss: 0.455117  [105600/175341]\n",
      "loss: 0.202084  [107200/175341]\n",
      "loss: 0.260527  [108800/175341]\n",
      "loss: 0.793684  [110400/175341]\n",
      "loss: 0.250292  [112000/175341]\n",
      "loss: 0.573336  [113600/175341]\n",
      "loss: 0.431517  [115200/175341]\n",
      "loss: 0.481533  [116800/175341]\n",
      "loss: 0.442290  [118400/175341]\n",
      "loss: 0.558956  [120000/175341]\n",
      "loss: 0.716575  [121600/175341]\n",
      "loss: 0.450376  [123200/175341]\n",
      "loss: 0.107130  [124800/175341]\n",
      "loss: 0.188743  [126400/175341]\n",
      "loss: 0.896177  [128000/175341]\n",
      "loss: 0.522374  [129600/175341]\n",
      "loss: 0.506752  [131200/175341]\n",
      "loss: 0.491915  [132800/175341]\n",
      "loss: 0.539280  [134400/175341]\n",
      "loss: 0.461671  [136000/175341]\n",
      "loss: 0.706810  [137600/175341]\n",
      "loss: 0.658142  [139200/175341]\n",
      "loss: 0.080318  [140800/175341]\n",
      "loss: 0.752258  [142400/175341]\n",
      "loss: 0.213047  [144000/175341]\n",
      "loss: 0.372107  [145600/175341]\n",
      "loss: 0.708929  [147200/175341]\n",
      "loss: 0.252577  [148800/175341]\n",
      "loss: 0.440115  [150400/175341]\n",
      "loss: 0.426015  [152000/175341]\n",
      "loss: 0.367807  [153600/175341]\n",
      "loss: 0.171718  [155200/175341]\n",
      "loss: 0.542007  [156800/175341]\n",
      "loss: 0.305319  [158400/175341]\n",
      "loss: 0.238955  [160000/175341]\n",
      "loss: 0.373756  [161600/175341]\n",
      "loss: 0.376196  [163200/175341]\n",
      "loss: 0.766504  [164800/175341]\n",
      "loss: 0.285085  [166400/175341]\n",
      "loss: 0.440501  [168000/175341]\n",
      "loss: 0.619779  [169600/175341]\n",
      "loss: 0.267296  [171200/175341]\n",
      "loss: 0.596526  [172800/175341]\n",
      "loss: 0.375525  [174400/175341]\n",
      "Train Accuracy: 82.1109%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.581976, F1-score: 75.37%, Macro_F1-Score:  42.12%  \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.395997  [    0/175341]\n",
      "loss: 0.802236  [ 1600/175341]\n",
      "loss: 0.610061  [ 3200/175341]\n",
      "loss: 0.359618  [ 4800/175341]\n",
      "loss: 0.403632  [ 6400/175341]\n",
      "loss: 0.425755  [ 8000/175341]\n",
      "loss: 0.423257  [ 9600/175341]\n",
      "loss: 0.565266  [11200/175341]\n",
      "loss: 0.670223  [12800/175341]\n",
      "loss: 0.579164  [14400/175341]\n",
      "loss: 0.806264  [16000/175341]\n",
      "loss: 0.366458  [17600/175341]\n",
      "loss: 0.385024  [19200/175341]\n",
      "loss: 0.765473  [20800/175341]\n",
      "loss: 0.379203  [22400/175341]\n",
      "loss: 0.792995  [24000/175341]\n",
      "loss: 0.498649  [25600/175341]\n",
      "loss: 0.455409  [27200/175341]\n",
      "loss: 0.570168  [28800/175341]\n",
      "loss: 0.194059  [30400/175341]\n",
      "loss: 0.978517  [32000/175341]\n",
      "loss: 0.120148  [33600/175341]\n",
      "loss: 0.236926  [35200/175341]\n",
      "loss: 0.457471  [36800/175341]\n",
      "loss: 0.155914  [38400/175341]\n",
      "loss: 0.612920  [40000/175341]\n",
      "loss: 0.788887  [41600/175341]\n",
      "loss: 0.717347  [43200/175341]\n",
      "loss: 0.149593  [44800/175341]\n",
      "loss: 0.426976  [46400/175341]\n",
      "loss: 0.470949  [48000/175341]\n",
      "loss: 0.414360  [49600/175341]\n",
      "loss: 0.253646  [51200/175341]\n",
      "loss: 0.575150  [52800/175341]\n",
      "loss: 0.347675  [54400/175341]\n",
      "loss: 0.338239  [56000/175341]\n",
      "loss: 0.674882  [57600/175341]\n",
      "loss: 0.222596  [59200/175341]\n",
      "loss: 0.875425  [60800/175341]\n",
      "loss: 0.198926  [62400/175341]\n",
      "loss: 0.710342  [64000/175341]\n",
      "loss: 0.508399  [65600/175341]\n",
      "loss: 0.414437  [67200/175341]\n",
      "loss: 0.525605  [68800/175341]\n",
      "loss: 0.367823  [70400/175341]\n",
      "loss: 0.762495  [72000/175341]\n",
      "loss: 0.143870  [73600/175341]\n",
      "loss: 0.396540  [75200/175341]\n",
      "loss: 0.418036  [76800/175341]\n",
      "loss: 1.110519  [78400/175341]\n",
      "loss: 0.325759  [80000/175341]\n",
      "loss: 0.594578  [81600/175341]\n",
      "loss: 0.746394  [83200/175341]\n",
      "loss: 0.443396  [84800/175341]\n",
      "loss: 0.488140  [86400/175341]\n",
      "loss: 0.341198  [88000/175341]\n",
      "loss: 0.720038  [89600/175341]\n",
      "loss: 0.216047  [91200/175341]\n",
      "loss: 0.259735  [92800/175341]\n",
      "loss: 0.428716  [94400/175341]\n",
      "loss: 0.549042  [96000/175341]\n",
      "loss: 0.262381  [97600/175341]\n",
      "loss: 0.670774  [99200/175341]\n",
      "loss: 0.133831  [100800/175341]\n",
      "loss: 0.575055  [102400/175341]\n",
      "loss: 0.554408  [104000/175341]\n",
      "loss: 0.290348  [105600/175341]\n",
      "loss: 0.622868  [107200/175341]\n",
      "loss: 0.304690  [108800/175341]\n",
      "loss: 0.264491  [110400/175341]\n",
      "loss: 0.499880  [112000/175341]\n",
      "loss: 0.065421  [113600/175341]\n",
      "loss: 0.933385  [115200/175341]\n",
      "loss: 0.358009  [116800/175341]\n",
      "loss: 0.245750  [118400/175341]\n",
      "loss: 0.457934  [120000/175341]\n",
      "loss: 0.621095  [121600/175341]\n",
      "loss: 0.434299  [123200/175341]\n",
      "loss: 0.636530  [124800/175341]\n",
      "loss: 0.553772  [126400/175341]\n",
      "loss: 0.708425  [128000/175341]\n",
      "loss: 0.123106  [129600/175341]\n",
      "loss: 0.124282  [131200/175341]\n",
      "loss: 0.483467  [132800/175341]\n",
      "loss: 0.129629  [134400/175341]\n",
      "loss: 0.647398  [136000/175341]\n",
      "loss: 0.229661  [137600/175341]\n",
      "loss: 0.420949  [139200/175341]\n",
      "loss: 0.604706  [140800/175341]\n",
      "loss: 0.487329  [142400/175341]\n",
      "loss: 0.338086  [144000/175341]\n",
      "loss: 0.473143  [145600/175341]\n",
      "loss: 0.473230  [147200/175341]\n",
      "loss: 0.198791  [148800/175341]\n",
      "loss: 0.574981  [150400/175341]\n",
      "loss: 0.358922  [152000/175341]\n",
      "loss: 0.620746  [153600/175341]\n",
      "loss: 0.756638  [155200/175341]\n",
      "loss: 1.088477  [156800/175341]\n",
      "loss: 0.720783  [158400/175341]\n",
      "loss: 0.436482  [160000/175341]\n",
      "loss: 0.611707  [161600/175341]\n",
      "loss: 0.476066  [163200/175341]\n",
      "loss: 0.826198  [164800/175341]\n",
      "loss: 0.457642  [166400/175341]\n",
      "loss: 0.138075  [168000/175341]\n",
      "loss: 0.513920  [169600/175341]\n",
      "loss: 0.621453  [171200/175341]\n",
      "loss: 0.557305  [172800/175341]\n",
      "loss: 0.894329  [174400/175341]\n",
      "Train Accuracy: 82.1559%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.552552, F1-score: 77.34%, Macro_F1-Score:  43.38%  \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.468050  [    0/175341]\n",
      "loss: 0.629976  [ 1600/175341]\n",
      "loss: 0.220276  [ 3200/175341]\n",
      "loss: 0.252247  [ 4800/175341]\n",
      "loss: 0.385378  [ 6400/175341]\n",
      "loss: 0.270235  [ 8000/175341]\n",
      "loss: 0.122146  [ 9600/175341]\n",
      "loss: 0.268322  [11200/175341]\n",
      "loss: 0.206905  [12800/175341]\n",
      "loss: 0.700710  [14400/175341]\n",
      "loss: 0.590091  [16000/175341]\n",
      "loss: 0.720618  [17600/175341]\n",
      "loss: 0.397948  [19200/175341]\n",
      "loss: 0.808441  [20800/175341]\n",
      "loss: 0.327915  [22400/175341]\n",
      "loss: 0.515209  [24000/175341]\n",
      "loss: 0.304205  [25600/175341]\n",
      "loss: 0.571076  [27200/175341]\n",
      "loss: 0.312395  [28800/175341]\n",
      "loss: 0.273822  [30400/175341]\n",
      "loss: 0.198616  [32000/175341]\n",
      "loss: 0.247301  [33600/175341]\n",
      "loss: 0.320192  [35200/175341]\n",
      "loss: 0.713090  [36800/175341]\n",
      "loss: 0.360482  [38400/175341]\n",
      "loss: 0.429445  [40000/175341]\n",
      "loss: 0.204008  [41600/175341]\n",
      "loss: 0.276143  [43200/175341]\n",
      "loss: 0.395750  [44800/175341]\n",
      "loss: 0.239896  [46400/175341]\n",
      "loss: 0.600061  [48000/175341]\n",
      "loss: 0.727638  [49600/175341]\n",
      "loss: 0.530360  [51200/175341]\n",
      "loss: 0.173010  [52800/175341]\n",
      "loss: 0.709973  [54400/175341]\n",
      "loss: 0.673722  [56000/175341]\n",
      "loss: 0.192445  [57600/175341]\n",
      "loss: 0.334268  [59200/175341]\n",
      "loss: 0.734393  [60800/175341]\n",
      "loss: 0.465444  [62400/175341]\n",
      "loss: 0.482583  [64000/175341]\n",
      "loss: 0.666198  [65600/175341]\n",
      "loss: 0.180351  [67200/175341]\n",
      "loss: 0.469578  [68800/175341]\n",
      "loss: 0.419042  [70400/175341]\n",
      "loss: 0.437720  [72000/175341]\n",
      "loss: 0.502056  [73600/175341]\n",
      "loss: 0.588782  [75200/175341]\n",
      "loss: 0.738769  [76800/175341]\n",
      "loss: 0.103348  [78400/175341]\n",
      "loss: 0.162511  [80000/175341]\n",
      "loss: 0.322432  [81600/175341]\n",
      "loss: 0.631157  [83200/175341]\n",
      "loss: 0.271529  [84800/175341]\n",
      "loss: 0.184280  [86400/175341]\n",
      "loss: 0.196629  [88000/175341]\n",
      "loss: 0.440984  [89600/175341]\n",
      "loss: 0.526418  [91200/175341]\n",
      "loss: 0.369835  [92800/175341]\n",
      "loss: 0.212399  [94400/175341]\n",
      "loss: 0.292248  [96000/175341]\n",
      "loss: 0.220444  [97600/175341]\n",
      "loss: 0.377246  [99200/175341]\n",
      "loss: 1.109945  [100800/175341]\n",
      "loss: 0.319287  [102400/175341]\n",
      "loss: 0.385694  [104000/175341]\n",
      "loss: 0.396625  [105600/175341]\n",
      "loss: 0.786217  [107200/175341]\n",
      "loss: 0.467052  [108800/175341]\n",
      "loss: 0.217346  [110400/175341]\n",
      "loss: 0.566652  [112000/175341]\n",
      "loss: 0.592415  [113600/175341]\n",
      "loss: 0.648550  [115200/175341]\n",
      "loss: 0.215856  [116800/175341]\n",
      "loss: 0.310999  [118400/175341]\n",
      "loss: 0.423206  [120000/175341]\n",
      "loss: 0.434852  [121600/175341]\n",
      "loss: 0.284518  [123200/175341]\n",
      "loss: 0.304380  [124800/175341]\n",
      "loss: 0.565797  [126400/175341]\n",
      "loss: 0.554089  [128000/175341]\n",
      "loss: 0.655772  [129600/175341]\n",
      "loss: 0.347185  [131200/175341]\n",
      "loss: 0.149838  [132800/175341]\n",
      "loss: 0.346692  [134400/175341]\n",
      "loss: 0.802130  [136000/175341]\n",
      "loss: 0.636618  [137600/175341]\n",
      "loss: 0.285764  [139200/175341]\n",
      "loss: 0.304325  [140800/175341]\n",
      "loss: 0.535906  [142400/175341]\n",
      "loss: 0.305830  [144000/175341]\n",
      "loss: 0.756229  [145600/175341]\n",
      "loss: 0.409540  [147200/175341]\n",
      "loss: 0.229909  [148800/175341]\n",
      "loss: 0.862050  [150400/175341]\n",
      "loss: 0.519790  [152000/175341]\n",
      "loss: 0.357254  [153600/175341]\n",
      "loss: 0.562227  [155200/175341]\n",
      "loss: 0.309042  [156800/175341]\n",
      "loss: 0.331358  [158400/175341]\n",
      "loss: 0.491826  [160000/175341]\n",
      "loss: 0.316684  [161600/175341]\n",
      "loss: 0.322091  [163200/175341]\n",
      "loss: 0.552607  [164800/175341]\n",
      "loss: 0.602019  [166400/175341]\n",
      "loss: 0.709144  [168000/175341]\n",
      "loss: 0.239038  [169600/175341]\n",
      "loss: 0.309090  [171200/175341]\n",
      "loss: 0.359658  [172800/175341]\n",
      "loss: 0.591838  [174400/175341]\n",
      "Train Accuracy: 82.1645%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.590425, F1-score: 74.71%, Macro_F1-Score:  41.78%  \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.712208  [    0/175341]\n",
      "loss: 0.483248  [ 1600/175341]\n",
      "loss: 0.506210  [ 3200/175341]\n",
      "loss: 1.091230  [ 4800/175341]\n",
      "loss: 0.441011  [ 6400/175341]\n",
      "loss: 0.331811  [ 8000/175341]\n",
      "loss: 0.488861  [ 9600/175341]\n",
      "loss: 0.963138  [11200/175341]\n",
      "loss: 0.646683  [12800/175341]\n",
      "loss: 0.324350  [14400/175341]\n",
      "loss: 0.414903  [16000/175341]\n",
      "loss: 0.629372  [17600/175341]\n",
      "loss: 0.903302  [19200/175341]\n",
      "loss: 0.533924  [20800/175341]\n",
      "loss: 0.213546  [22400/175341]\n",
      "loss: 0.294046  [24000/175341]\n",
      "loss: 0.183503  [25600/175341]\n",
      "loss: 0.508307  [27200/175341]\n",
      "loss: 0.518063  [28800/175341]\n",
      "loss: 0.656996  [30400/175341]\n",
      "loss: 0.346997  [32000/175341]\n",
      "loss: 0.356256  [33600/175341]\n",
      "loss: 0.321838  [35200/175341]\n",
      "loss: 0.637540  [36800/175341]\n",
      "loss: 0.213604  [38400/175341]\n",
      "loss: 0.475778  [40000/175341]\n",
      "loss: 0.299371  [41600/175341]\n",
      "loss: 0.264092  [43200/175341]\n",
      "loss: 0.592859  [44800/175341]\n",
      "loss: 0.726447  [46400/175341]\n",
      "loss: 0.086303  [48000/175341]\n",
      "loss: 0.754805  [49600/175341]\n",
      "loss: 0.263082  [51200/175341]\n",
      "loss: 0.645610  [52800/175341]\n",
      "loss: 0.318169  [54400/175341]\n",
      "loss: 0.270739  [56000/175341]\n",
      "loss: 0.557565  [57600/175341]\n",
      "loss: 0.400468  [59200/175341]\n",
      "loss: 0.930202  [60800/175341]\n",
      "loss: 0.247527  [62400/175341]\n",
      "loss: 0.535027  [64000/175341]\n",
      "loss: 0.418470  [65600/175341]\n",
      "loss: 0.474070  [67200/175341]\n",
      "loss: 0.353956  [68800/175341]\n",
      "loss: 0.141899  [70400/175341]\n",
      "loss: 0.347788  [72000/175341]\n",
      "loss: 0.571105  [73600/175341]\n",
      "loss: 0.364159  [75200/175341]\n",
      "loss: 0.380896  [76800/175341]\n",
      "loss: 0.403485  [78400/175341]\n",
      "loss: 0.211413  [80000/175341]\n",
      "loss: 0.430740  [81600/175341]\n",
      "loss: 0.534823  [83200/175341]\n",
      "loss: 0.361806  [84800/175341]\n",
      "loss: 0.764570  [86400/175341]\n",
      "loss: 0.565770  [88000/175341]\n",
      "loss: 0.528804  [89600/175341]\n",
      "loss: 0.485766  [91200/175341]\n",
      "loss: 0.713570  [92800/175341]\n",
      "loss: 0.571635  [94400/175341]\n",
      "loss: 0.601723  [96000/175341]\n",
      "loss: 0.634447  [97600/175341]\n",
      "loss: 0.376239  [99200/175341]\n",
      "loss: 0.299816  [100800/175341]\n",
      "loss: 0.309784  [102400/175341]\n",
      "loss: 0.469902  [104000/175341]\n",
      "loss: 0.402444  [105600/175341]\n",
      "loss: 0.411168  [107200/175341]\n",
      "loss: 0.489697  [108800/175341]\n",
      "loss: 0.201771  [110400/175341]\n",
      "loss: 0.487519  [112000/175341]\n",
      "loss: 0.810296  [113600/175341]\n",
      "loss: 0.466922  [115200/175341]\n",
      "loss: 0.129902  [116800/175341]\n",
      "loss: 0.310184  [118400/175341]\n",
      "loss: 0.458208  [120000/175341]\n",
      "loss: 0.553019  [121600/175341]\n",
      "loss: 0.538661  [123200/175341]\n",
      "loss: 0.248939  [124800/175341]\n",
      "loss: 0.971619  [126400/175341]\n",
      "loss: 0.736320  [128000/175341]\n",
      "loss: 0.092444  [129600/175341]\n",
      "loss: 0.377766  [131200/175341]\n",
      "loss: 0.212939  [132800/175341]\n",
      "loss: 0.595218  [134400/175341]\n",
      "loss: 0.586553  [136000/175341]\n",
      "loss: 0.556709  [137600/175341]\n",
      "loss: 0.456574  [139200/175341]\n",
      "loss: 0.772561  [140800/175341]\n",
      "loss: 0.133240  [142400/175341]\n",
      "loss: 0.095954  [144000/175341]\n",
      "loss: 0.559916  [145600/175341]\n",
      "loss: 0.207283  [147200/175341]\n",
      "loss: 0.387906  [148800/175341]\n",
      "loss: 0.190966  [150400/175341]\n",
      "loss: 0.385226  [152000/175341]\n",
      "loss: 0.191678  [153600/175341]\n",
      "loss: 0.513800  [155200/175341]\n",
      "loss: 0.587711  [156800/175341]\n",
      "loss: 0.498064  [158400/175341]\n",
      "loss: 0.384527  [160000/175341]\n",
      "loss: 0.529432  [161600/175341]\n",
      "loss: 1.168259  [163200/175341]\n",
      "loss: 0.551904  [164800/175341]\n",
      "loss: 0.530067  [166400/175341]\n",
      "loss: 0.485145  [168000/175341]\n",
      "loss: 0.580506  [169600/175341]\n",
      "loss: 0.310221  [171200/175341]\n",
      "loss: 0.314468  [172800/175341]\n",
      "loss: 0.243223  [174400/175341]\n",
      "Train Accuracy: 82.1787%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.556261, F1-score: 76.51%, Macro_F1-Score:  42.76%  \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.498969  [    0/175341]\n",
      "loss: 0.486309  [ 1600/175341]\n",
      "loss: 0.206633  [ 3200/175341]\n",
      "loss: 0.692391  [ 4800/175341]\n",
      "loss: 0.430866  [ 6400/175341]\n",
      "loss: 0.248673  [ 8000/175341]\n",
      "loss: 0.265132  [ 9600/175341]\n",
      "loss: 0.188621  [11200/175341]\n",
      "loss: 0.167548  [12800/175341]\n",
      "loss: 0.604808  [14400/175341]\n",
      "loss: 0.271647  [16000/175341]\n",
      "loss: 0.357169  [17600/175341]\n",
      "loss: 0.822974  [19200/175341]\n",
      "loss: 0.593236  [20800/175341]\n",
      "loss: 0.459018  [22400/175341]\n",
      "loss: 0.126207  [24000/175341]\n",
      "loss: 0.291096  [25600/175341]\n",
      "loss: 0.320712  [27200/175341]\n",
      "loss: 0.543874  [28800/175341]\n",
      "loss: 0.591801  [30400/175341]\n",
      "loss: 0.684088  [32000/175341]\n",
      "loss: 0.598606  [33600/175341]\n",
      "loss: 0.480600  [35200/175341]\n",
      "loss: 0.025785  [36800/175341]\n",
      "loss: 0.279590  [38400/175341]\n",
      "loss: 0.551607  [40000/175341]\n",
      "loss: 0.479613  [41600/175341]\n",
      "loss: 0.247436  [43200/175341]\n",
      "loss: 0.487168  [44800/175341]\n",
      "loss: 0.246660  [46400/175341]\n",
      "loss: 0.246872  [48000/175341]\n",
      "loss: 0.399130  [49600/175341]\n",
      "loss: 0.760786  [51200/175341]\n",
      "loss: 0.155834  [52800/175341]\n",
      "loss: 0.328745  [54400/175341]\n",
      "loss: 0.435432  [56000/175341]\n",
      "loss: 0.576840  [57600/175341]\n",
      "loss: 0.503559  [59200/175341]\n",
      "loss: 0.370725  [60800/175341]\n",
      "loss: 0.445188  [62400/175341]\n",
      "loss: 0.160323  [64000/175341]\n",
      "loss: 0.931503  [65600/175341]\n",
      "loss: 0.489231  [67200/175341]\n",
      "loss: 0.762656  [68800/175341]\n",
      "loss: 0.325173  [70400/175341]\n",
      "loss: 0.399576  [72000/175341]\n",
      "loss: 0.270689  [73600/175341]\n",
      "loss: 0.151828  [75200/175341]\n",
      "loss: 0.432714  [76800/175341]\n",
      "loss: 0.551666  [78400/175341]\n",
      "loss: 0.245941  [80000/175341]\n",
      "loss: 0.181487  [81600/175341]\n",
      "loss: 0.238597  [83200/175341]\n",
      "loss: 0.555970  [84800/175341]\n",
      "loss: 0.335970  [86400/175341]\n",
      "loss: 0.535401  [88000/175341]\n",
      "loss: 0.344723  [89600/175341]\n",
      "loss: 0.681079  [91200/175341]\n",
      "loss: 0.659119  [92800/175341]\n",
      "loss: 0.157183  [94400/175341]\n",
      "loss: 0.263128  [96000/175341]\n",
      "loss: 0.539165  [97600/175341]\n",
      "loss: 0.179592  [99200/175341]\n",
      "loss: 0.422507  [100800/175341]\n",
      "loss: 0.518977  [102400/175341]\n",
      "loss: 0.773144  [104000/175341]\n",
      "loss: 0.256494  [105600/175341]\n",
      "loss: 0.446454  [107200/175341]\n",
      "loss: 1.364923  [108800/175341]\n",
      "loss: 0.211153  [110400/175341]\n",
      "loss: 0.459305  [112000/175341]\n",
      "loss: 0.213155  [113600/175341]\n",
      "loss: 0.280650  [115200/175341]\n",
      "loss: 0.592137  [116800/175341]\n",
      "loss: 0.673981  [118400/175341]\n",
      "loss: 0.378948  [120000/175341]\n",
      "loss: 0.505979  [121600/175341]\n",
      "loss: 0.240965  [123200/175341]\n",
      "loss: 0.498835  [124800/175341]\n",
      "loss: 0.549730  [126400/175341]\n",
      "loss: 0.336873  [128000/175341]\n",
      "loss: 0.790414  [129600/175341]\n",
      "loss: 0.158908  [131200/175341]\n",
      "loss: 0.326284  [132800/175341]\n",
      "loss: 0.280686  [134400/175341]\n",
      "loss: 0.491620  [136000/175341]\n",
      "loss: 0.756038  [137600/175341]\n",
      "loss: 0.909787  [139200/175341]\n",
      "loss: 0.274759  [140800/175341]\n",
      "loss: 0.465065  [142400/175341]\n",
      "loss: 0.533426  [144000/175341]\n",
      "loss: 0.765964  [145600/175341]\n",
      "loss: 0.394672  [147200/175341]\n",
      "loss: 0.254863  [148800/175341]\n",
      "loss: 0.588016  [150400/175341]\n",
      "loss: 0.150399  [152000/175341]\n",
      "loss: 0.199912  [153600/175341]\n",
      "loss: 0.657037  [155200/175341]\n",
      "loss: 0.227176  [156800/175341]\n",
      "loss: 0.460373  [158400/175341]\n",
      "loss: 0.346530  [160000/175341]\n",
      "loss: 0.636061  [161600/175341]\n",
      "loss: 0.452972  [163200/175341]\n",
      "loss: 0.318498  [164800/175341]\n",
      "loss: 0.404217  [166400/175341]\n",
      "loss: 0.519746  [168000/175341]\n",
      "loss: 0.180447  [169600/175341]\n",
      "loss: 0.374830  [171200/175341]\n",
      "loss: 0.239193  [172800/175341]\n",
      "loss: 0.405869  [174400/175341]\n",
      "Train Accuracy: 82.1742%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.550622, F1-score: 76.93%, Macro_F1-Score:  42.48%  \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.342143  [    0/175341]\n",
      "loss: 0.513417  [ 1600/175341]\n",
      "loss: 0.298124  [ 3200/175341]\n",
      "loss: 0.544489  [ 4800/175341]\n",
      "loss: 0.484722  [ 6400/175341]\n",
      "loss: 0.280063  [ 8000/175341]\n",
      "loss: 0.464620  [ 9600/175341]\n",
      "loss: 0.437319  [11200/175341]\n",
      "loss: 0.503376  [12800/175341]\n",
      "loss: 0.227223  [14400/175341]\n",
      "loss: 0.277953  [16000/175341]\n",
      "loss: 0.144034  [17600/175341]\n",
      "loss: 0.345802  [19200/175341]\n",
      "loss: 0.524595  [20800/175341]\n",
      "loss: 0.400672  [22400/175341]\n",
      "loss: 0.661182  [24000/175341]\n",
      "loss: 0.345713  [25600/175341]\n",
      "loss: 0.155028  [27200/175341]\n",
      "loss: 0.422605  [28800/175341]\n",
      "loss: 0.457973  [30400/175341]\n",
      "loss: 0.253958  [32000/175341]\n",
      "loss: 0.171505  [33600/175341]\n",
      "loss: 0.527370  [35200/175341]\n",
      "loss: 0.195778  [36800/175341]\n",
      "loss: 0.467507  [38400/175341]\n",
      "loss: 0.634041  [40000/175341]\n",
      "loss: 1.102775  [41600/175341]\n",
      "loss: 0.327144  [43200/175341]\n",
      "loss: 0.710494  [44800/175341]\n",
      "loss: 0.557703  [46400/175341]\n",
      "loss: 0.688916  [48000/175341]\n",
      "loss: 0.350086  [49600/175341]\n",
      "loss: 0.501636  [51200/175341]\n",
      "loss: 0.545954  [52800/175341]\n",
      "loss: 0.250526  [54400/175341]\n",
      "loss: 0.236150  [56000/175341]\n",
      "loss: 0.552673  [57600/175341]\n",
      "loss: 0.380531  [59200/175341]\n",
      "loss: 0.223490  [60800/175341]\n",
      "loss: 0.198904  [62400/175341]\n",
      "loss: 0.776873  [64000/175341]\n",
      "loss: 0.362453  [65600/175341]\n",
      "loss: 0.938464  [67200/175341]\n",
      "loss: 0.303690  [68800/175341]\n",
      "loss: 0.201452  [70400/175341]\n",
      "loss: 0.383346  [72000/175341]\n",
      "loss: 0.166060  [73600/175341]\n",
      "loss: 0.178545  [75200/175341]\n",
      "loss: 0.707456  [76800/175341]\n",
      "loss: 0.504859  [78400/175341]\n",
      "loss: 0.713264  [80000/175341]\n",
      "loss: 0.281770  [81600/175341]\n",
      "loss: 0.405550  [83200/175341]\n",
      "loss: 0.372132  [84800/175341]\n",
      "loss: 0.562621  [86400/175341]\n",
      "loss: 0.244703  [88000/175341]\n",
      "loss: 0.247731  [89600/175341]\n",
      "loss: 0.950208  [91200/175341]\n",
      "loss: 0.380404  [92800/175341]\n",
      "loss: 0.379072  [94400/175341]\n",
      "loss: 0.685240  [96000/175341]\n",
      "loss: 0.431153  [97600/175341]\n",
      "loss: 0.198740  [99200/175341]\n",
      "loss: 0.175115  [100800/175341]\n",
      "loss: 0.375398  [102400/175341]\n",
      "loss: 0.192091  [104000/175341]\n",
      "loss: 0.489426  [105600/175341]\n",
      "loss: 0.384729  [107200/175341]\n",
      "loss: 0.186484  [108800/175341]\n",
      "loss: 0.436345  [110400/175341]\n",
      "loss: 0.433333  [112000/175341]\n",
      "loss: 0.283665  [113600/175341]\n",
      "loss: 0.214860  [115200/175341]\n",
      "loss: 0.344570  [116800/175341]\n",
      "loss: 0.452846  [118400/175341]\n",
      "loss: 0.373274  [120000/175341]\n",
      "loss: 0.081645  [121600/175341]\n",
      "loss: 0.372848  [123200/175341]\n",
      "loss: 0.628148  [124800/175341]\n",
      "loss: 0.743341  [126400/175341]\n",
      "loss: 0.544110  [128000/175341]\n",
      "loss: 0.147596  [129600/175341]\n",
      "loss: 0.364244  [131200/175341]\n",
      "loss: 0.231608  [132800/175341]\n",
      "loss: 0.198809  [134400/175341]\n",
      "loss: 0.510924  [136000/175341]\n",
      "loss: 0.361881  [137600/175341]\n",
      "loss: 0.186723  [139200/175341]\n",
      "loss: 0.680755  [140800/175341]\n",
      "loss: 0.302206  [142400/175341]\n",
      "loss: 0.463989  [144000/175341]\n",
      "loss: 0.559973  [145600/175341]\n",
      "loss: 0.687968  [147200/175341]\n",
      "loss: 0.259294  [148800/175341]\n",
      "loss: 0.436005  [150400/175341]\n",
      "loss: 0.647797  [152000/175341]\n",
      "loss: 0.248493  [153600/175341]\n",
      "loss: 0.337692  [155200/175341]\n",
      "loss: 0.257825  [156800/175341]\n",
      "loss: 0.580677  [158400/175341]\n",
      "loss: 0.274991  [160000/175341]\n",
      "loss: 0.293413  [161600/175341]\n",
      "loss: 0.593457  [163200/175341]\n",
      "loss: 0.561322  [164800/175341]\n",
      "loss: 1.276736  [166400/175341]\n",
      "loss: 0.246494  [168000/175341]\n",
      "loss: 0.331125  [169600/175341]\n",
      "loss: 0.170282  [171200/175341]\n",
      "loss: 0.499449  [172800/175341]\n",
      "loss: 0.195766  [174400/175341]\n",
      "Train Accuracy: 82.1474%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.548916, F1-score: 76.66%, Macro_F1-Score:  41.88%  \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.171162  [    0/175341]\n",
      "loss: 0.515416  [ 1600/175341]\n",
      "loss: 0.380053  [ 3200/175341]\n",
      "loss: 0.316285  [ 4800/175341]\n",
      "loss: 0.512396  [ 6400/175341]\n",
      "loss: 0.390917  [ 8000/175341]\n",
      "loss: 0.647919  [ 9600/175341]\n",
      "loss: 0.464915  [11200/175341]\n",
      "loss: 0.530178  [12800/175341]\n",
      "loss: 0.606823  [14400/175341]\n",
      "loss: 0.271317  [16000/175341]\n",
      "loss: 0.505116  [17600/175341]\n",
      "loss: 0.797793  [19200/175341]\n",
      "loss: 0.370464  [20800/175341]\n",
      "loss: 0.260047  [22400/175341]\n",
      "loss: 0.226651  [24000/175341]\n",
      "loss: 0.168070  [25600/175341]\n",
      "loss: 0.355896  [27200/175341]\n",
      "loss: 0.213561  [28800/175341]\n",
      "loss: 0.396181  [30400/175341]\n",
      "loss: 0.558304  [32000/175341]\n",
      "loss: 0.631509  [33600/175341]\n",
      "loss: 1.047274  [35200/175341]\n",
      "loss: 0.334015  [36800/175341]\n",
      "loss: 0.513417  [38400/175341]\n",
      "loss: 0.365194  [40000/175341]\n",
      "loss: 0.438787  [41600/175341]\n",
      "loss: 0.276176  [43200/175341]\n",
      "loss: 0.789305  [44800/175341]\n",
      "loss: 0.854835  [46400/175341]\n",
      "loss: 0.226487  [48000/175341]\n",
      "loss: 0.315703  [49600/175341]\n",
      "loss: 0.318001  [51200/175341]\n",
      "loss: 0.590248  [52800/175341]\n",
      "loss: 0.262144  [54400/175341]\n",
      "loss: 0.279178  [56000/175341]\n",
      "loss: 0.925356  [57600/175341]\n",
      "loss: 0.254440  [59200/175341]\n",
      "loss: 0.719789  [60800/175341]\n",
      "loss: 0.519404  [62400/175341]\n",
      "loss: 0.411790  [64000/175341]\n",
      "loss: 0.482315  [65600/175341]\n",
      "loss: 0.585406  [67200/175341]\n",
      "loss: 0.385483  [68800/175341]\n",
      "loss: 0.712946  [70400/175341]\n",
      "loss: 0.572026  [72000/175341]\n",
      "loss: 0.377006  [73600/175341]\n",
      "loss: 0.529192  [75200/175341]\n",
      "loss: 0.244233  [76800/175341]\n",
      "loss: 0.534518  [78400/175341]\n",
      "loss: 0.149304  [80000/175341]\n",
      "loss: 0.082444  [81600/175341]\n",
      "loss: 0.582166  [83200/175341]\n",
      "loss: 0.598139  [84800/175341]\n",
      "loss: 0.333101  [86400/175341]\n",
      "loss: 0.668027  [88000/175341]\n",
      "loss: 0.566627  [89600/175341]\n",
      "loss: 1.130152  [91200/175341]\n",
      "loss: 0.412102  [92800/175341]\n",
      "loss: 0.111250  [94400/175341]\n",
      "loss: 0.486102  [96000/175341]\n",
      "loss: 0.476542  [97600/175341]\n",
      "loss: 0.579454  [99200/175341]\n",
      "loss: 0.171438  [100800/175341]\n",
      "loss: 0.391991  [102400/175341]\n",
      "loss: 0.088013  [104000/175341]\n",
      "loss: 0.616703  [105600/175341]\n",
      "loss: 0.299035  [107200/175341]\n",
      "loss: 0.455069  [108800/175341]\n",
      "loss: 0.471018  [110400/175341]\n",
      "loss: 0.525524  [112000/175341]\n",
      "loss: 0.483814  [113600/175341]\n",
      "loss: 0.242145  [115200/175341]\n",
      "loss: 0.453286  [116800/175341]\n",
      "loss: 0.090401  [118400/175341]\n",
      "loss: 0.215185  [120000/175341]\n",
      "loss: 0.533527  [121600/175341]\n",
      "loss: 0.639828  [123200/175341]\n",
      "loss: 0.463856  [124800/175341]\n",
      "loss: 0.337385  [126400/175341]\n",
      "loss: 0.733392  [128000/175341]\n",
      "loss: 0.430342  [129600/175341]\n",
      "loss: 0.420133  [131200/175341]\n",
      "loss: 0.312325  [132800/175341]\n",
      "loss: 0.526958  [134400/175341]\n",
      "loss: 0.306245  [136000/175341]\n",
      "loss: 0.441928  [137600/175341]\n",
      "loss: 0.145156  [139200/175341]\n",
      "loss: 0.401540  [140800/175341]\n",
      "loss: 0.826460  [142400/175341]\n",
      "loss: 0.388411  [144000/175341]\n",
      "loss: 0.286203  [145600/175341]\n",
      "loss: 0.475399  [147200/175341]\n",
      "loss: 0.376167  [148800/175341]\n",
      "loss: 0.548540  [150400/175341]\n",
      "loss: 0.590874  [152000/175341]\n",
      "loss: 0.970555  [153600/175341]\n",
      "loss: 0.486924  [155200/175341]\n",
      "loss: 0.147412  [156800/175341]\n",
      "loss: 0.821400  [158400/175341]\n",
      "loss: 0.451167  [160000/175341]\n",
      "loss: 0.257390  [161600/175341]\n",
      "loss: 0.342377  [163200/175341]\n",
      "loss: 0.359165  [164800/175341]\n",
      "loss: 0.444954  [166400/175341]\n",
      "loss: 0.197140  [168000/175341]\n",
      "loss: 0.399041  [169600/175341]\n",
      "loss: 0.468744  [171200/175341]\n",
      "loss: 0.226416  [172800/175341]\n",
      "loss: 0.222863  [174400/175341]\n",
      "Train Accuracy: 82.1496%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.546278, F1-score: 77.73%, Macro_F1-Score:  44.68%  \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.135486  [    0/175341]\n",
      "loss: 0.744323  [ 1600/175341]\n",
      "loss: 0.212322  [ 3200/175341]\n",
      "loss: 0.522353  [ 4800/175341]\n",
      "loss: 0.413052  [ 6400/175341]\n",
      "loss: 0.667901  [ 8000/175341]\n",
      "loss: 0.177168  [ 9600/175341]\n",
      "loss: 0.424207  [11200/175341]\n",
      "loss: 0.382115  [12800/175341]\n",
      "loss: 0.666409  [14400/175341]\n",
      "loss: 0.245544  [16000/175341]\n",
      "loss: 0.529752  [17600/175341]\n",
      "loss: 0.440532  [19200/175341]\n",
      "loss: 0.565376  [20800/175341]\n",
      "loss: 0.291289  [22400/175341]\n",
      "loss: 0.215558  [24000/175341]\n",
      "loss: 0.376132  [25600/175341]\n",
      "loss: 0.523884  [27200/175341]\n",
      "loss: 0.155535  [28800/175341]\n",
      "loss: 0.287251  [30400/175341]\n",
      "loss: 0.479245  [32000/175341]\n",
      "loss: 0.301631  [33600/175341]\n",
      "loss: 0.172411  [35200/175341]\n",
      "loss: 0.392347  [36800/175341]\n",
      "loss: 0.556610  [38400/175341]\n",
      "loss: 0.286358  [40000/175341]\n",
      "loss: 0.202116  [41600/175341]\n",
      "loss: 0.588770  [43200/175341]\n",
      "loss: 0.362961  [44800/175341]\n",
      "loss: 0.374319  [46400/175341]\n",
      "loss: 0.714920  [48000/175341]\n",
      "loss: 0.517773  [49600/175341]\n",
      "loss: 0.197409  [51200/175341]\n",
      "loss: 0.267597  [52800/175341]\n",
      "loss: 0.343764  [54400/175341]\n",
      "loss: 0.204562  [56000/175341]\n",
      "loss: 0.386774  [57600/175341]\n",
      "loss: 0.650256  [59200/175341]\n",
      "loss: 0.482905  [60800/175341]\n",
      "loss: 0.515326  [62400/175341]\n",
      "loss: 0.297794  [64000/175341]\n",
      "loss: 0.308734  [65600/175341]\n",
      "loss: 0.303229  [67200/175341]\n",
      "loss: 0.300484  [68800/175341]\n",
      "loss: 0.560111  [70400/175341]\n",
      "loss: 0.691024  [72000/175341]\n",
      "loss: 0.147691  [73600/175341]\n",
      "loss: 0.213442  [75200/175341]\n",
      "loss: 0.712391  [76800/175341]\n",
      "loss: 0.419252  [78400/175341]\n",
      "loss: 0.646034  [80000/175341]\n",
      "loss: 0.405473  [81600/175341]\n",
      "loss: 0.334756  [83200/175341]\n",
      "loss: 0.233168  [84800/175341]\n",
      "loss: 0.474173  [86400/175341]\n",
      "loss: 0.263187  [88000/175341]\n",
      "loss: 0.416672  [89600/175341]\n",
      "loss: 0.586518  [91200/175341]\n",
      "loss: 0.284558  [92800/175341]\n",
      "loss: 0.851011  [94400/175341]\n",
      "loss: 0.459397  [96000/175341]\n",
      "loss: 0.544637  [97600/175341]\n",
      "loss: 0.235242  [99200/175341]\n",
      "loss: 0.195633  [100800/175341]\n",
      "loss: 0.600853  [102400/175341]\n",
      "loss: 0.318642  [104000/175341]\n",
      "loss: 0.360299  [105600/175341]\n",
      "loss: 0.285510  [107200/175341]\n",
      "loss: 0.357112  [108800/175341]\n",
      "loss: 0.273933  [110400/175341]\n",
      "loss: 0.453049  [112000/175341]\n",
      "loss: 0.616066  [113600/175341]\n",
      "loss: 0.542025  [115200/175341]\n",
      "loss: 0.358092  [116800/175341]\n",
      "loss: 0.150978  [118400/175341]\n",
      "loss: 0.543014  [120000/175341]\n",
      "loss: 0.447716  [121600/175341]\n",
      "loss: 0.589598  [123200/175341]\n",
      "loss: 0.690295  [124800/175341]\n",
      "loss: 0.141231  [126400/175341]\n",
      "loss: 0.281860  [128000/175341]\n",
      "loss: 0.890411  [129600/175341]\n",
      "loss: 0.159988  [131200/175341]\n",
      "loss: 0.088166  [132800/175341]\n",
      "loss: 0.358919  [134400/175341]\n",
      "loss: 0.387270  [136000/175341]\n",
      "loss: 0.396763  [137600/175341]\n",
      "loss: 0.687145  [139200/175341]\n",
      "loss: 0.613765  [140800/175341]\n",
      "loss: 0.359423  [142400/175341]\n",
      "loss: 0.520357  [144000/175341]\n",
      "loss: 0.245915  [145600/175341]\n",
      "loss: 0.857855  [147200/175341]\n",
      "loss: 0.270034  [148800/175341]\n",
      "loss: 0.574803  [150400/175341]\n",
      "loss: 0.164353  [152000/175341]\n",
      "loss: 0.376858  [153600/175341]\n",
      "loss: 0.218973  [155200/175341]\n",
      "loss: 0.459691  [156800/175341]\n",
      "loss: 0.122028  [158400/175341]\n",
      "loss: 0.172971  [160000/175341]\n",
      "loss: 0.706052  [161600/175341]\n",
      "loss: 0.181315  [163200/175341]\n",
      "loss: 0.679880  [164800/175341]\n",
      "loss: 0.460733  [166400/175341]\n",
      "loss: 0.584415  [168000/175341]\n",
      "loss: 0.214178  [169600/175341]\n",
      "loss: 0.200847  [171200/175341]\n",
      "loss: 0.699027  [172800/175341]\n",
      "loss: 0.507000  [174400/175341]\n",
      "Train Accuracy: 82.0995%\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.534964, F1-score: 78.04%, Macro_F1-Score:  43.36%  \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.442843  [    0/175341]\n",
      "loss: 0.273114  [ 1600/175341]\n",
      "loss: 0.480738  [ 3200/175341]\n",
      "loss: 0.440737  [ 4800/175341]\n",
      "loss: 0.229597  [ 6400/175341]\n",
      "loss: 0.607884  [ 8000/175341]\n",
      "loss: 0.299352  [ 9600/175341]\n",
      "loss: 0.735384  [11200/175341]\n",
      "loss: 0.116876  [12800/175341]\n",
      "loss: 0.524352  [14400/175341]\n",
      "loss: 0.087734  [16000/175341]\n",
      "loss: 0.575336  [17600/175341]\n",
      "loss: 0.721731  [19200/175341]\n",
      "loss: 0.419234  [20800/175341]\n",
      "loss: 0.338631  [22400/175341]\n",
      "loss: 0.363243  [24000/175341]\n",
      "loss: 0.637379  [25600/175341]\n",
      "loss: 0.258947  [27200/175341]\n",
      "loss: 0.450170  [28800/175341]\n",
      "loss: 0.425104  [30400/175341]\n",
      "loss: 0.509339  [32000/175341]\n",
      "loss: 0.420300  [33600/175341]\n",
      "loss: 0.457319  [35200/175341]\n",
      "loss: 0.780298  [36800/175341]\n",
      "loss: 0.637037  [38400/175341]\n",
      "loss: 0.277208  [40000/175341]\n",
      "loss: 0.482063  [41600/175341]\n",
      "loss: 0.536213  [43200/175341]\n",
      "loss: 0.370567  [44800/175341]\n",
      "loss: 0.701910  [46400/175341]\n",
      "loss: 1.094034  [48000/175341]\n",
      "loss: 0.497611  [49600/175341]\n",
      "loss: 0.751570  [51200/175341]\n",
      "loss: 0.513692  [52800/175341]\n",
      "loss: 0.678392  [54400/175341]\n",
      "loss: 0.296063  [56000/175341]\n",
      "loss: 0.689291  [57600/175341]\n",
      "loss: 1.072763  [59200/175341]\n",
      "loss: 0.669814  [60800/175341]\n",
      "loss: 0.538765  [62400/175341]\n",
      "loss: 0.393075  [64000/175341]\n",
      "loss: 0.244460  [65600/175341]\n",
      "loss: 0.529334  [67200/175341]\n",
      "loss: 0.424034  [68800/175341]\n",
      "loss: 0.242415  [70400/175341]\n",
      "loss: 0.521333  [72000/175341]\n",
      "loss: 0.170004  [73600/175341]\n",
      "loss: 0.439278  [75200/175341]\n",
      "loss: 0.421563  [76800/175341]\n",
      "loss: 0.322852  [78400/175341]\n",
      "loss: 0.487133  [80000/175341]\n",
      "loss: 0.578612  [81600/175341]\n",
      "loss: 0.474191  [83200/175341]\n",
      "loss: 0.691630  [84800/175341]\n",
      "loss: 0.286395  [86400/175341]\n",
      "loss: 0.383590  [88000/175341]\n",
      "loss: 0.441066  [89600/175341]\n",
      "loss: 0.591404  [91200/175341]\n",
      "loss: 0.464699  [92800/175341]\n",
      "loss: 1.023638  [94400/175341]\n",
      "loss: 0.184087  [96000/175341]\n",
      "loss: 0.308218  [97600/175341]\n",
      "loss: 0.370141  [99200/175341]\n",
      "loss: 0.400646  [100800/175341]\n",
      "loss: 0.457908  [102400/175341]\n",
      "loss: 1.072469  [104000/175341]\n",
      "loss: 0.539579  [105600/175341]\n",
      "loss: 0.374020  [107200/175341]\n",
      "loss: 0.492622  [108800/175341]\n",
      "loss: 0.560928  [110400/175341]\n",
      "loss: 0.400548  [112000/175341]\n",
      "loss: 0.467309  [113600/175341]\n",
      "loss: 0.251036  [115200/175341]\n",
      "loss: 0.609589  [116800/175341]\n",
      "loss: 0.492995  [118400/175341]\n",
      "loss: 0.594545  [120000/175341]\n",
      "loss: 0.331791  [121600/175341]\n",
      "loss: 0.664587  [123200/175341]\n",
      "loss: 0.390350  [124800/175341]\n",
      "loss: 0.828967  [126400/175341]\n",
      "loss: 0.366027  [128000/175341]\n",
      "loss: 0.241167  [129600/175341]\n",
      "loss: 0.566135  [131200/175341]\n",
      "loss: 0.202816  [132800/175341]\n",
      "loss: 0.475240  [134400/175341]\n",
      "loss: 0.730337  [136000/175341]\n",
      "loss: 0.437113  [137600/175341]\n",
      "loss: 0.508688  [139200/175341]\n",
      "loss: 0.319143  [140800/175341]\n",
      "loss: 0.308886  [142400/175341]\n",
      "loss: 0.551829  [144000/175341]\n",
      "loss: 0.129123  [145600/175341]\n",
      "loss: 0.551389  [147200/175341]\n",
      "loss: 0.141061  [148800/175341]\n",
      "loss: 0.797003  [150400/175341]\n",
      "loss: 0.788603  [152000/175341]\n",
      "loss: 0.493882  [153600/175341]\n",
      "loss: 0.543638  [155200/175341]\n",
      "loss: 0.236355  [156800/175341]\n",
      "loss: 0.765429  [158400/175341]\n",
      "loss: 0.926857  [160000/175341]\n",
      "loss: 0.573469  [161600/175341]\n",
      "loss: 0.291846  [163200/175341]\n",
      "loss: 0.755570  [164800/175341]\n",
      "loss: 0.343842  [166400/175341]\n",
      "loss: 0.237870  [168000/175341]\n",
      "loss: 0.480566  [169600/175341]\n",
      "loss: 0.283771  [171200/175341]\n",
      "loss: 0.516768  [172800/175341]\n",
      "loss: 0.397914  [174400/175341]\n",
      "Train Accuracy: 82.1320%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.559388, F1-score: 76.79%, Macro_F1-Score:  42.85%  \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.160122  [    0/175341]\n",
      "loss: 0.838483  [ 1600/175341]\n",
      "loss: 0.109072  [ 3200/175341]\n",
      "loss: 0.367386  [ 4800/175341]\n",
      "loss: 0.675761  [ 6400/175341]\n",
      "loss: 1.003550  [ 8000/175341]\n",
      "loss: 0.169949  [ 9600/175341]\n",
      "loss: 0.592670  [11200/175341]\n",
      "loss: 0.339134  [12800/175341]\n",
      "loss: 0.311443  [14400/175341]\n",
      "loss: 0.692609  [16000/175341]\n",
      "loss: 0.849078  [17600/175341]\n",
      "loss: 0.129037  [19200/175341]\n",
      "loss: 0.256288  [20800/175341]\n",
      "loss: 0.513080  [22400/175341]\n",
      "loss: 0.240787  [24000/175341]\n",
      "loss: 0.404513  [25600/175341]\n",
      "loss: 0.260905  [27200/175341]\n",
      "loss: 0.296992  [28800/175341]\n",
      "loss: 0.799275  [30400/175341]\n",
      "loss: 0.407661  [32000/175341]\n",
      "loss: 0.474352  [33600/175341]\n",
      "loss: 0.503316  [35200/175341]\n",
      "loss: 0.424032  [36800/175341]\n",
      "loss: 0.341057  [38400/175341]\n",
      "loss: 0.389476  [40000/175341]\n",
      "loss: 0.733032  [41600/175341]\n",
      "loss: 0.271838  [43200/175341]\n",
      "loss: 0.439639  [44800/175341]\n",
      "loss: 0.322194  [46400/175341]\n",
      "loss: 0.190598  [48000/175341]\n",
      "loss: 0.825381  [49600/175341]\n",
      "loss: 0.512757  [51200/175341]\n",
      "loss: 0.208076  [52800/175341]\n",
      "loss: 0.180391  [54400/175341]\n",
      "loss: 0.339656  [56000/175341]\n",
      "loss: 0.498278  [57600/175341]\n",
      "loss: 0.165055  [59200/175341]\n",
      "loss: 0.438362  [60800/175341]\n",
      "loss: 0.348837  [62400/175341]\n",
      "loss: 0.551450  [64000/175341]\n",
      "loss: 0.779068  [65600/175341]\n",
      "loss: 0.496681  [67200/175341]\n",
      "loss: 0.507221  [68800/175341]\n",
      "loss: 0.516581  [70400/175341]\n",
      "loss: 0.174729  [72000/175341]\n",
      "loss: 0.464995  [73600/175341]\n",
      "loss: 0.619529  [75200/175341]\n",
      "loss: 0.756319  [76800/175341]\n",
      "loss: 0.245034  [78400/175341]\n",
      "loss: 0.152099  [80000/175341]\n",
      "loss: 0.237539  [81600/175341]\n",
      "loss: 0.291919  [83200/175341]\n",
      "loss: 0.347721  [84800/175341]\n",
      "loss: 0.347054  [86400/175341]\n",
      "loss: 0.470366  [88000/175341]\n",
      "loss: 0.355509  [89600/175341]\n",
      "loss: 0.225108  [91200/175341]\n",
      "loss: 0.135332  [92800/175341]\n",
      "loss: 0.525113  [94400/175341]\n",
      "loss: 0.291858  [96000/175341]\n",
      "loss: 0.381735  [97600/175341]\n",
      "loss: 0.176322  [99200/175341]\n",
      "loss: 0.783248  [100800/175341]\n",
      "loss: 0.687527  [102400/175341]\n",
      "loss: 0.359532  [104000/175341]\n",
      "loss: 0.237895  [105600/175341]\n",
      "loss: 0.342831  [107200/175341]\n",
      "loss: 0.555914  [108800/175341]\n",
      "loss: 0.448313  [110400/175341]\n",
      "loss: 0.295895  [112000/175341]\n",
      "loss: 0.757467  [113600/175341]\n",
      "loss: 0.157936  [115200/175341]\n",
      "loss: 0.381157  [116800/175341]\n",
      "loss: 0.606270  [118400/175341]\n",
      "loss: 0.651119  [120000/175341]\n",
      "loss: 0.645062  [121600/175341]\n",
      "loss: 0.549061  [123200/175341]\n",
      "loss: 0.196504  [124800/175341]\n",
      "loss: 0.558685  [126400/175341]\n",
      "loss: 0.817037  [128000/175341]\n",
      "loss: 0.231399  [129600/175341]\n",
      "loss: 0.498591  [131200/175341]\n",
      "loss: 0.535362  [132800/175341]\n",
      "loss: 0.398112  [134400/175341]\n",
      "loss: 0.799872  [136000/175341]\n",
      "loss: 0.294707  [137600/175341]\n",
      "loss: 0.302764  [139200/175341]\n",
      "loss: 0.630822  [140800/175341]\n",
      "loss: 0.222999  [142400/175341]\n",
      "loss: 0.346580  [144000/175341]\n",
      "loss: 0.201868  [145600/175341]\n",
      "loss: 0.472748  [147200/175341]\n",
      "loss: 0.674954  [148800/175341]\n",
      "loss: 0.558913  [150400/175341]\n",
      "loss: 0.646132  [152000/175341]\n",
      "loss: 0.381476  [153600/175341]\n",
      "loss: 0.288896  [155200/175341]\n",
      "loss: 0.623416  [156800/175341]\n",
      "loss: 0.302701  [158400/175341]\n",
      "loss: 0.402797  [160000/175341]\n",
      "loss: 0.537991  [161600/175341]\n",
      "loss: 0.594930  [163200/175341]\n",
      "loss: 0.325959  [164800/175341]\n",
      "loss: 0.665933  [166400/175341]\n",
      "loss: 0.530205  [168000/175341]\n",
      "loss: 0.524476  [169600/175341]\n",
      "loss: 0.474941  [171200/175341]\n",
      "loss: 0.264479  [172800/175341]\n",
      "loss: 0.374035  [174400/175341]\n",
      "Train Accuracy: 82.1725%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.539184, F1-score: 77.67%, Macro_F1-Score:  42.60%  \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.387763  [    0/175341]\n",
      "loss: 0.536889  [ 1600/175341]\n",
      "loss: 0.195754  [ 3200/175341]\n",
      "loss: 0.295565  [ 4800/175341]\n",
      "loss: 0.417896  [ 6400/175341]\n",
      "loss: 0.239983  [ 8000/175341]\n",
      "loss: 0.416548  [ 9600/175341]\n",
      "loss: 0.270628  [11200/175341]\n",
      "loss: 0.482291  [12800/175341]\n",
      "loss: 0.160838  [14400/175341]\n",
      "loss: 0.355564  [16000/175341]\n",
      "loss: 0.796069  [17600/175341]\n",
      "loss: 0.303406  [19200/175341]\n",
      "loss: 0.340489  [20800/175341]\n",
      "loss: 0.438504  [22400/175341]\n",
      "loss: 0.499084  [24000/175341]\n",
      "loss: 0.122122  [25600/175341]\n",
      "loss: 0.370677  [27200/175341]\n",
      "loss: 0.223943  [28800/175341]\n",
      "loss: 0.402392  [30400/175341]\n",
      "loss: 0.277556  [32000/175341]\n",
      "loss: 0.397553  [33600/175341]\n",
      "loss: 0.163892  [35200/175341]\n",
      "loss: 0.156383  [36800/175341]\n",
      "loss: 0.495337  [38400/175341]\n",
      "loss: 0.109835  [40000/175341]\n",
      "loss: 0.416333  [41600/175341]\n",
      "loss: 0.170911  [43200/175341]\n",
      "loss: 0.301454  [44800/175341]\n",
      "loss: 0.437553  [46400/175341]\n",
      "loss: 0.490952  [48000/175341]\n",
      "loss: 0.546949  [49600/175341]\n",
      "loss: 0.737487  [51200/175341]\n",
      "loss: 0.287528  [52800/175341]\n",
      "loss: 0.281034  [54400/175341]\n",
      "loss: 0.443240  [56000/175341]\n",
      "loss: 0.462210  [57600/175341]\n",
      "loss: 0.314910  [59200/175341]\n",
      "loss: 1.340474  [60800/175341]\n",
      "loss: 0.506688  [62400/175341]\n",
      "loss: 0.233411  [64000/175341]\n",
      "loss: 0.371985  [65600/175341]\n",
      "loss: 0.279690  [67200/175341]\n",
      "loss: 0.465682  [68800/175341]\n",
      "loss: 0.209057  [70400/175341]\n",
      "loss: 0.453641  [72000/175341]\n",
      "loss: 0.565183  [73600/175341]\n",
      "loss: 0.702029  [75200/175341]\n",
      "loss: 0.548360  [76800/175341]\n",
      "loss: 0.480888  [78400/175341]\n",
      "loss: 0.517688  [80000/175341]\n",
      "loss: 0.572831  [81600/175341]\n",
      "loss: 0.615655  [83200/175341]\n",
      "loss: 0.287180  [84800/175341]\n",
      "loss: 0.274059  [86400/175341]\n",
      "loss: 0.529171  [88000/175341]\n",
      "loss: 0.802845  [89600/175341]\n",
      "loss: 0.464579  [91200/175341]\n",
      "loss: 0.701441  [92800/175341]\n",
      "loss: 0.681738  [94400/175341]\n",
      "loss: 0.414852  [96000/175341]\n",
      "loss: 0.344318  [97600/175341]\n",
      "loss: 0.513115  [99200/175341]\n",
      "loss: 0.767623  [100800/175341]\n",
      "loss: 0.489867  [102400/175341]\n",
      "loss: 0.708003  [104000/175341]\n",
      "loss: 0.385877  [105600/175341]\n",
      "loss: 0.129109  [107200/175341]\n",
      "loss: 0.312825  [108800/175341]\n",
      "loss: 0.518322  [110400/175341]\n",
      "loss: 0.520038  [112000/175341]\n",
      "loss: 0.244702  [113600/175341]\n",
      "loss: 0.322169  [115200/175341]\n",
      "loss: 0.211150  [116800/175341]\n",
      "loss: 0.454617  [118400/175341]\n",
      "loss: 0.385803  [120000/175341]\n",
      "loss: 0.095990  [121600/175341]\n",
      "loss: 0.646876  [123200/175341]\n",
      "loss: 0.409717  [124800/175341]\n",
      "loss: 0.339872  [126400/175341]\n",
      "loss: 0.518103  [128000/175341]\n",
      "loss: 0.619924  [129600/175341]\n",
      "loss: 0.396053  [131200/175341]\n",
      "loss: 0.448947  [132800/175341]\n",
      "loss: 0.480917  [134400/175341]\n",
      "loss: 0.715197  [136000/175341]\n",
      "loss: 0.478659  [137600/175341]\n",
      "loss: 0.546299  [139200/175341]\n",
      "loss: 0.231307  [140800/175341]\n",
      "loss: 0.232460  [142400/175341]\n",
      "loss: 0.601817  [144000/175341]\n",
      "loss: 0.217707  [145600/175341]\n",
      "loss: 0.984717  [147200/175341]\n",
      "loss: 0.400558  [148800/175341]\n",
      "loss: 0.270786  [150400/175341]\n",
      "loss: 0.670214  [152000/175341]\n",
      "loss: 0.405119  [153600/175341]\n",
      "loss: 0.602298  [155200/175341]\n",
      "loss: 0.265254  [156800/175341]\n",
      "loss: 0.536186  [158400/175341]\n",
      "loss: 0.861554  [160000/175341]\n",
      "loss: 0.364888  [161600/175341]\n",
      "loss: 0.121642  [163200/175341]\n",
      "loss: 0.651228  [164800/175341]\n",
      "loss: 0.490918  [166400/175341]\n",
      "loss: 0.399548  [168000/175341]\n",
      "loss: 0.534914  [169600/175341]\n",
      "loss: 0.193731  [171200/175341]\n",
      "loss: 0.925120  [172800/175341]\n",
      "loss: 0.600327  [174400/175341]\n",
      "Train Accuracy: 82.1947%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.559688, F1-score: 76.31%, Macro_F1-Score:  43.25%  \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.167482  [    0/175341]\n",
      "loss: 0.195859  [ 1600/175341]\n",
      "loss: 0.996463  [ 3200/175341]\n",
      "loss: 0.419100  [ 4800/175341]\n",
      "loss: 0.334317  [ 6400/175341]\n",
      "loss: 0.311659  [ 8000/175341]\n",
      "loss: 0.723488  [ 9600/175341]\n",
      "loss: 0.960731  [11200/175341]\n",
      "loss: 0.214597  [12800/175341]\n",
      "loss: 0.122225  [14400/175341]\n",
      "loss: 0.265689  [16000/175341]\n",
      "loss: 0.371169  [17600/175341]\n",
      "loss: 0.539628  [19200/175341]\n",
      "loss: 1.038964  [20800/175341]\n",
      "loss: 0.174760  [22400/175341]\n",
      "loss: 0.758072  [24000/175341]\n",
      "loss: 0.429068  [25600/175341]\n",
      "loss: 0.251425  [27200/175341]\n",
      "loss: 0.233065  [28800/175341]\n",
      "loss: 0.588134  [30400/175341]\n",
      "loss: 0.607257  [32000/175341]\n",
      "loss: 0.193460  [33600/175341]\n",
      "loss: 0.167758  [35200/175341]\n",
      "loss: 0.859883  [36800/175341]\n",
      "loss: 0.672752  [38400/175341]\n",
      "loss: 0.365740  [40000/175341]\n",
      "loss: 0.321288  [41600/175341]\n",
      "loss: 0.498104  [43200/175341]\n",
      "loss: 0.329354  [44800/175341]\n",
      "loss: 0.258464  [46400/175341]\n",
      "loss: 0.351273  [48000/175341]\n",
      "loss: 0.581830  [49600/175341]\n",
      "loss: 0.369374  [51200/175341]\n",
      "loss: 0.326705  [52800/175341]\n",
      "loss: 0.776905  [54400/175341]\n",
      "loss: 0.552772  [56000/175341]\n",
      "loss: 0.571776  [57600/175341]\n",
      "loss: 0.421366  [59200/175341]\n",
      "loss: 0.090756  [60800/175341]\n",
      "loss: 0.531422  [62400/175341]\n",
      "loss: 0.778056  [64000/175341]\n",
      "loss: 0.580638  [65600/175341]\n",
      "loss: 0.509315  [67200/175341]\n",
      "loss: 0.300985  [68800/175341]\n",
      "loss: 0.617941  [70400/175341]\n",
      "loss: 0.619527  [72000/175341]\n",
      "loss: 0.355880  [73600/175341]\n",
      "loss: 0.344376  [75200/175341]\n",
      "loss: 0.216253  [76800/175341]\n",
      "loss: 0.285943  [78400/175341]\n",
      "loss: 0.463458  [80000/175341]\n",
      "loss: 0.380611  [81600/175341]\n",
      "loss: 0.221725  [83200/175341]\n",
      "loss: 0.600069  [84800/175341]\n",
      "loss: 0.394093  [86400/175341]\n",
      "loss: 0.373342  [88000/175341]\n",
      "loss: 0.579629  [89600/175341]\n",
      "loss: 0.582709  [91200/175341]\n",
      "loss: 0.728080  [92800/175341]\n",
      "loss: 0.850726  [94400/175341]\n",
      "loss: 0.492743  [96000/175341]\n",
      "loss: 0.534912  [97600/175341]\n",
      "loss: 0.728201  [99200/175341]\n",
      "loss: 0.417903  [100800/175341]\n",
      "loss: 0.627852  [102400/175341]\n",
      "loss: 0.368859  [104000/175341]\n",
      "loss: 0.532178  [105600/175341]\n",
      "loss: 0.654927  [107200/175341]\n",
      "loss: 0.218248  [108800/175341]\n",
      "loss: 0.385147  [110400/175341]\n",
      "loss: 0.206894  [112000/175341]\n",
      "loss: 0.567751  [113600/175341]\n",
      "loss: 0.268044  [115200/175341]\n",
      "loss: 0.703195  [116800/175341]\n",
      "loss: 0.201395  [118400/175341]\n",
      "loss: 0.523595  [120000/175341]\n",
      "loss: 0.493469  [121600/175341]\n",
      "loss: 0.313262  [123200/175341]\n",
      "loss: 0.040401  [124800/175341]\n",
      "loss: 0.524838  [126400/175341]\n",
      "loss: 0.372017  [128000/175341]\n",
      "loss: 0.288854  [129600/175341]\n",
      "loss: 0.828063  [131200/175341]\n",
      "loss: 0.510934  [132800/175341]\n",
      "loss: 0.219709  [134400/175341]\n",
      "loss: 0.819284  [136000/175341]\n",
      "loss: 0.358440  [137600/175341]\n",
      "loss: 0.272429  [139200/175341]\n",
      "loss: 0.574727  [140800/175341]\n",
      "loss: 0.790202  [142400/175341]\n",
      "loss: 0.608803  [144000/175341]\n",
      "loss: 0.518144  [145600/175341]\n",
      "loss: 0.308127  [147200/175341]\n",
      "loss: 0.242487  [148800/175341]\n",
      "loss: 0.593496  [150400/175341]\n",
      "loss: 0.109306  [152000/175341]\n",
      "loss: 0.325217  [153600/175341]\n",
      "loss: 0.528272  [155200/175341]\n",
      "loss: 0.695625  [156800/175341]\n",
      "loss: 0.717279  [158400/175341]\n",
      "loss: 0.493643  [160000/175341]\n",
      "loss: 0.573593  [161600/175341]\n",
      "loss: 0.581281  [163200/175341]\n",
      "loss: 0.335118  [164800/175341]\n",
      "loss: 0.577276  [166400/175341]\n",
      "loss: 0.438064  [168000/175341]\n",
      "loss: 0.513170  [169600/175341]\n",
      "loss: 0.199689  [171200/175341]\n",
      "loss: 0.201412  [172800/175341]\n",
      "loss: 0.562977  [174400/175341]\n",
      "Train Accuracy: 82.1725%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.554804, F1-score: 76.87%, Macro_F1-Score:  42.37%  \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.349767  [    0/175341]\n",
      "loss: 0.570958  [ 1600/175341]\n",
      "loss: 0.205765  [ 3200/175341]\n",
      "loss: 0.785165  [ 4800/175341]\n",
      "loss: 0.416840  [ 6400/175341]\n",
      "loss: 0.922309  [ 8000/175341]\n",
      "loss: 0.815508  [ 9600/175341]\n",
      "loss: 0.536750  [11200/175341]\n",
      "loss: 0.242932  [12800/175341]\n",
      "loss: 0.419889  [14400/175341]\n",
      "loss: 0.158696  [16000/175341]\n",
      "loss: 0.264675  [17600/175341]\n",
      "loss: 0.167534  [19200/175341]\n",
      "loss: 0.232199  [20800/175341]\n",
      "loss: 0.396491  [22400/175341]\n",
      "loss: 0.340303  [24000/175341]\n",
      "loss: 0.506673  [25600/175341]\n",
      "loss: 0.716897  [27200/175341]\n",
      "loss: 0.345117  [28800/175341]\n",
      "loss: 0.211529  [30400/175341]\n",
      "loss: 0.111824  [32000/175341]\n",
      "loss: 0.201675  [33600/175341]\n",
      "loss: 0.219148  [35200/175341]\n",
      "loss: 0.273507  [36800/175341]\n",
      "loss: 0.510483  [38400/175341]\n",
      "loss: 0.940223  [40000/175341]\n",
      "loss: 0.485018  [41600/175341]\n",
      "loss: 0.259996  [43200/175341]\n",
      "loss: 0.328144  [44800/175341]\n",
      "loss: 0.350646  [46400/175341]\n",
      "loss: 0.666148  [48000/175341]\n",
      "loss: 0.306131  [49600/175341]\n",
      "loss: 0.112556  [51200/175341]\n",
      "loss: 0.486229  [52800/175341]\n",
      "loss: 0.657202  [54400/175341]\n",
      "loss: 0.601367  [56000/175341]\n",
      "loss: 0.451339  [57600/175341]\n",
      "loss: 0.486098  [59200/175341]\n",
      "loss: 0.255532  [60800/175341]\n",
      "loss: 0.489722  [62400/175341]\n",
      "loss: 0.476773  [64000/175341]\n",
      "loss: 0.265815  [65600/175341]\n",
      "loss: 0.307586  [67200/175341]\n",
      "loss: 0.444816  [68800/175341]\n",
      "loss: 0.163772  [70400/175341]\n",
      "loss: 0.418624  [72000/175341]\n",
      "loss: 0.335735  [73600/175341]\n",
      "loss: 0.435741  [75200/175341]\n",
      "loss: 0.268192  [76800/175341]\n",
      "loss: 0.177015  [78400/175341]\n",
      "loss: 0.717577  [80000/175341]\n",
      "loss: 0.346036  [81600/175341]\n",
      "loss: 0.180047  [83200/175341]\n",
      "loss: 0.304491  [84800/175341]\n",
      "loss: 0.264362  [86400/175341]\n",
      "loss: 0.302487  [88000/175341]\n",
      "loss: 0.446029  [89600/175341]\n",
      "loss: 0.269469  [91200/175341]\n",
      "loss: 0.487298  [92800/175341]\n",
      "loss: 0.333880  [94400/175341]\n",
      "loss: 0.255558  [96000/175341]\n",
      "loss: 0.284556  [97600/175341]\n",
      "loss: 0.286069  [99200/175341]\n",
      "loss: 0.723428  [100800/175341]\n",
      "loss: 0.174545  [102400/175341]\n",
      "loss: 0.617101  [104000/175341]\n",
      "loss: 0.676135  [105600/175341]\n",
      "loss: 0.349618  [107200/175341]\n",
      "loss: 0.281379  [108800/175341]\n",
      "loss: 0.253693  [110400/175341]\n",
      "loss: 0.552791  [112000/175341]\n",
      "loss: 0.330859  [113600/175341]\n",
      "loss: 0.769433  [115200/175341]\n",
      "loss: 0.402021  [116800/175341]\n",
      "loss: 0.800655  [118400/175341]\n",
      "loss: 0.576547  [120000/175341]\n",
      "loss: 0.167990  [121600/175341]\n",
      "loss: 0.314384  [123200/175341]\n",
      "loss: 0.513655  [124800/175341]\n",
      "loss: 0.711927  [126400/175341]\n",
      "loss: 0.194388  [128000/175341]\n",
      "loss: 0.398148  [129600/175341]\n",
      "loss: 0.432755  [131200/175341]\n",
      "loss: 0.315016  [132800/175341]\n",
      "loss: 0.531289  [134400/175341]\n",
      "loss: 0.434617  [136000/175341]\n",
      "loss: 0.661835  [137600/175341]\n",
      "loss: 0.303164  [139200/175341]\n",
      "loss: 0.482172  [140800/175341]\n",
      "loss: 0.393541  [142400/175341]\n",
      "loss: 0.257863  [144000/175341]\n",
      "loss: 0.455577  [145600/175341]\n",
      "loss: 0.350552  [147200/175341]\n",
      "loss: 0.470660  [148800/175341]\n",
      "loss: 0.264986  [150400/175341]\n",
      "loss: 0.373738  [152000/175341]\n",
      "loss: 0.813987  [153600/175341]\n",
      "loss: 0.404427  [155200/175341]\n",
      "loss: 0.541444  [156800/175341]\n",
      "loss: 0.699292  [158400/175341]\n",
      "loss: 0.537144  [160000/175341]\n",
      "loss: 0.476913  [161600/175341]\n",
      "loss: 0.778289  [163200/175341]\n",
      "loss: 0.459765  [164800/175341]\n",
      "loss: 0.777442  [166400/175341]\n",
      "loss: 0.038115  [168000/175341]\n",
      "loss: 0.665082  [169600/175341]\n",
      "loss: 0.272112  [171200/175341]\n",
      "loss: 0.888040  [172800/175341]\n",
      "loss: 0.359591  [174400/175341]\n",
      "Train Accuracy: 82.1622%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.540466, F1-score: 77.51%, Macro_F1-Score:  43.19%  \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.593430  [    0/175341]\n",
      "loss: 0.462565  [ 1600/175341]\n",
      "loss: 0.271751  [ 3200/175341]\n",
      "loss: 0.504914  [ 4800/175341]\n",
      "loss: 0.697740  [ 6400/175341]\n",
      "loss: 0.636564  [ 8000/175341]\n",
      "loss: 0.246852  [ 9600/175341]\n",
      "loss: 0.529539  [11200/175341]\n",
      "loss: 0.360540  [12800/175341]\n",
      "loss: 0.767746  [14400/175341]\n",
      "loss: 0.553494  [16000/175341]\n",
      "loss: 0.622704  [17600/175341]\n",
      "loss: 0.860305  [19200/175341]\n",
      "loss: 0.375750  [20800/175341]\n",
      "loss: 0.567027  [22400/175341]\n",
      "loss: 0.250280  [24000/175341]\n",
      "loss: 0.599259  [25600/175341]\n",
      "loss: 0.395357  [27200/175341]\n",
      "loss: 0.362057  [28800/175341]\n",
      "loss: 0.483369  [30400/175341]\n",
      "loss: 0.790087  [32000/175341]\n",
      "loss: 0.432053  [33600/175341]\n",
      "loss: 0.368888  [35200/175341]\n",
      "loss: 0.431216  [36800/175341]\n",
      "loss: 0.592997  [38400/175341]\n",
      "loss: 0.160969  [40000/175341]\n",
      "loss: 1.124413  [41600/175341]\n",
      "loss: 0.687171  [43200/175341]\n",
      "loss: 0.292854  [44800/175341]\n",
      "loss: 0.822070  [46400/175341]\n",
      "loss: 0.837895  [48000/175341]\n",
      "loss: 0.385944  [49600/175341]\n",
      "loss: 0.448585  [51200/175341]\n",
      "loss: 0.587207  [52800/175341]\n",
      "loss: 0.270819  [54400/175341]\n",
      "loss: 0.650643  [56000/175341]\n",
      "loss: 0.235566  [57600/175341]\n",
      "loss: 0.477067  [59200/175341]\n",
      "loss: 0.402373  [60800/175341]\n",
      "loss: 0.558248  [62400/175341]\n",
      "loss: 0.759905  [64000/175341]\n",
      "loss: 0.500000  [65600/175341]\n",
      "loss: 0.352152  [67200/175341]\n",
      "loss: 0.439336  [68800/175341]\n",
      "loss: 0.370015  [70400/175341]\n",
      "loss: 0.277875  [72000/175341]\n",
      "loss: 0.174631  [73600/175341]\n",
      "loss: 0.311533  [75200/175341]\n",
      "loss: 0.983495  [76800/175341]\n",
      "loss: 0.336122  [78400/175341]\n",
      "loss: 0.199601  [80000/175341]\n",
      "loss: 0.566403  [81600/175341]\n",
      "loss: 0.998057  [83200/175341]\n",
      "loss: 0.242273  [84800/175341]\n",
      "loss: 0.560551  [86400/175341]\n",
      "loss: 0.501808  [88000/175341]\n",
      "loss: 0.153218  [89600/175341]\n",
      "loss: 0.442467  [91200/175341]\n",
      "loss: 0.335469  [92800/175341]\n",
      "loss: 0.919346  [94400/175341]\n",
      "loss: 0.497033  [96000/175341]\n",
      "loss: 0.384060  [97600/175341]\n",
      "loss: 0.237368  [99200/175341]\n",
      "loss: 0.704035  [100800/175341]\n",
      "loss: 0.383276  [102400/175341]\n",
      "loss: 0.299982  [104000/175341]\n",
      "loss: 0.545410  [105600/175341]\n",
      "loss: 0.590671  [107200/175341]\n",
      "loss: 0.906095  [108800/175341]\n",
      "loss: 0.433828  [110400/175341]\n",
      "loss: 0.766491  [112000/175341]\n",
      "loss: 0.304743  [113600/175341]\n",
      "loss: 0.478825  [115200/175341]\n",
      "loss: 0.716813  [116800/175341]\n",
      "loss: 0.116802  [118400/175341]\n",
      "loss: 0.733189  [120000/175341]\n",
      "loss: 0.699935  [121600/175341]\n",
      "loss: 0.342582  [123200/175341]\n",
      "loss: 0.436471  [124800/175341]\n",
      "loss: 0.891485  [126400/175341]\n",
      "loss: 0.535882  [128000/175341]\n",
      "loss: 0.586696  [129600/175341]\n",
      "loss: 0.391100  [131200/175341]\n",
      "loss: 0.580626  [132800/175341]\n",
      "loss: 0.827003  [134400/175341]\n",
      "loss: 0.859069  [136000/175341]\n",
      "loss: 0.194165  [137600/175341]\n",
      "loss: 0.308328  [139200/175341]\n",
      "loss: 0.301546  [140800/175341]\n",
      "loss: 0.380307  [142400/175341]\n",
      "loss: 0.227885  [144000/175341]\n",
      "loss: 0.286664  [145600/175341]\n",
      "loss: 0.828210  [147200/175341]\n",
      "loss: 0.500588  [148800/175341]\n",
      "loss: 0.230472  [150400/175341]\n",
      "loss: 0.231728  [152000/175341]\n",
      "loss: 0.544620  [153600/175341]\n",
      "loss: 0.471019  [155200/175341]\n",
      "loss: 0.762469  [156800/175341]\n",
      "loss: 0.334320  [158400/175341]\n",
      "loss: 0.303801  [160000/175341]\n",
      "loss: 0.480009  [161600/175341]\n",
      "loss: 0.210051  [163200/175341]\n",
      "loss: 0.488937  [164800/175341]\n",
      "loss: 0.519747  [166400/175341]\n",
      "loss: 0.353616  [168000/175341]\n",
      "loss: 0.946466  [169600/175341]\n",
      "loss: 0.325987  [171200/175341]\n",
      "loss: 0.471291  [172800/175341]\n",
      "loss: 0.439778  [174400/175341]\n",
      "Train Accuracy: 82.1867%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.559515, F1-score: 76.84%, Macro_F1-Score:  44.34%  \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.473429  [    0/175341]\n",
      "loss: 0.288274  [ 1600/175341]\n",
      "loss: 0.473658  [ 3200/175341]\n",
      "loss: 0.644729  [ 4800/175341]\n",
      "loss: 1.181922  [ 6400/175341]\n",
      "loss: 0.477905  [ 8000/175341]\n",
      "loss: 0.192950  [ 9600/175341]\n",
      "loss: 0.409015  [11200/175341]\n",
      "loss: 0.669205  [12800/175341]\n",
      "loss: 0.780381  [14400/175341]\n",
      "loss: 0.333060  [16000/175341]\n",
      "loss: 0.611804  [17600/175341]\n",
      "loss: 0.476014  [19200/175341]\n",
      "loss: 0.698256  [20800/175341]\n",
      "loss: 0.388494  [22400/175341]\n",
      "loss: 0.657426  [24000/175341]\n",
      "loss: 0.777154  [25600/175341]\n",
      "loss: 0.193297  [27200/175341]\n",
      "loss: 0.672123  [28800/175341]\n",
      "loss: 0.498684  [30400/175341]\n",
      "loss: 0.834332  [32000/175341]\n",
      "loss: 0.513103  [33600/175341]\n",
      "loss: 0.838966  [35200/175341]\n",
      "loss: 0.710277  [36800/175341]\n",
      "loss: 0.504841  [38400/175341]\n",
      "loss: 0.430458  [40000/175341]\n",
      "loss: 0.283433  [41600/175341]\n",
      "loss: 0.687100  [43200/175341]\n",
      "loss: 0.141439  [44800/175341]\n",
      "loss: 0.223965  [46400/175341]\n",
      "loss: 0.693164  [48000/175341]\n",
      "loss: 1.248234  [49600/175341]\n",
      "loss: 0.647315  [51200/175341]\n",
      "loss: 0.154993  [52800/175341]\n",
      "loss: 0.637156  [54400/175341]\n",
      "loss: 0.292182  [56000/175341]\n",
      "loss: 0.131418  [57600/175341]\n",
      "loss: 0.390852  [59200/175341]\n",
      "loss: 0.308591  [60800/175341]\n",
      "loss: 0.514575  [62400/175341]\n",
      "loss: 0.524512  [64000/175341]\n",
      "loss: 0.535046  [65600/175341]\n",
      "loss: 0.665114  [67200/175341]\n",
      "loss: 0.169408  [68800/175341]\n",
      "loss: 0.646387  [70400/175341]\n",
      "loss: 0.645784  [72000/175341]\n",
      "loss: 0.430951  [73600/175341]\n",
      "loss: 0.696779  [75200/175341]\n",
      "loss: 0.520432  [76800/175341]\n",
      "loss: 0.614650  [78400/175341]\n",
      "loss: 0.485868  [80000/175341]\n",
      "loss: 0.408445  [81600/175341]\n",
      "loss: 0.354988  [83200/175341]\n",
      "loss: 0.587377  [84800/175341]\n",
      "loss: 0.451513  [86400/175341]\n",
      "loss: 0.372697  [88000/175341]\n",
      "loss: 0.308089  [89600/175341]\n",
      "loss: 0.713970  [91200/175341]\n",
      "loss: 0.245117  [92800/175341]\n",
      "loss: 0.208721  [94400/175341]\n",
      "loss: 0.246906  [96000/175341]\n",
      "loss: 0.696705  [97600/175341]\n",
      "loss: 0.939790  [99200/175341]\n",
      "loss: 0.709779  [100800/175341]\n",
      "loss: 1.414961  [102400/175341]\n",
      "loss: 0.261158  [104000/175341]\n",
      "loss: 0.617139  [105600/175341]\n",
      "loss: 0.565427  [107200/175341]\n",
      "loss: 0.698174  [108800/175341]\n",
      "loss: 0.343611  [110400/175341]\n",
      "loss: 0.678042  [112000/175341]\n",
      "loss: 0.445726  [113600/175341]\n",
      "loss: 0.157666  [115200/175341]\n",
      "loss: 0.125478  [116800/175341]\n",
      "loss: 0.437522  [118400/175341]\n",
      "loss: 0.462700  [120000/175341]\n",
      "loss: 0.758613  [121600/175341]\n",
      "loss: 1.078823  [123200/175341]\n",
      "loss: 0.567477  [124800/175341]\n",
      "loss: 0.622576  [126400/175341]\n",
      "loss: 0.321326  [128000/175341]\n",
      "loss: 0.490390  [129600/175341]\n",
      "loss: 0.422186  [131200/175341]\n",
      "loss: 0.477836  [132800/175341]\n",
      "loss: 0.254918  [134400/175341]\n",
      "loss: 0.647443  [136000/175341]\n",
      "loss: 0.285208  [137600/175341]\n",
      "loss: 0.548639  [139200/175341]\n",
      "loss: 0.515059  [140800/175341]\n",
      "loss: 0.309377  [142400/175341]\n",
      "loss: 0.356136  [144000/175341]\n",
      "loss: 0.381213  [145600/175341]\n",
      "loss: 0.235977  [147200/175341]\n",
      "loss: 0.722029  [148800/175341]\n",
      "loss: 0.479077  [150400/175341]\n",
      "loss: 0.251918  [152000/175341]\n",
      "loss: 0.474012  [153600/175341]\n",
      "loss: 0.751226  [155200/175341]\n",
      "loss: 0.341647  [156800/175341]\n",
      "loss: 0.494289  [158400/175341]\n",
      "loss: 0.798024  [160000/175341]\n",
      "loss: 0.626711  [161600/175341]\n",
      "loss: 0.441566  [163200/175341]\n",
      "loss: 0.251784  [164800/175341]\n",
      "loss: 0.673108  [166400/175341]\n",
      "loss: 0.533533  [168000/175341]\n",
      "loss: 0.604488  [169600/175341]\n",
      "loss: 0.632497  [171200/175341]\n",
      "loss: 0.256729  [172800/175341]\n",
      "loss: 0.595239  [174400/175341]\n",
      "Train Accuracy: 82.1519%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.550185, F1-score: 76.62%, Macro_F1-Score:  43.74%  \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.400623  [    0/175341]\n",
      "loss: 0.054659  [ 1600/175341]\n",
      "loss: 0.246516  [ 3200/175341]\n",
      "loss: 0.166937  [ 4800/175341]\n",
      "loss: 0.336083  [ 6400/175341]\n",
      "loss: 0.378451  [ 8000/175341]\n",
      "loss: 0.552443  [ 9600/175341]\n",
      "loss: 0.421458  [11200/175341]\n",
      "loss: 0.389539  [12800/175341]\n",
      "loss: 0.379066  [14400/175341]\n",
      "loss: 0.321907  [16000/175341]\n",
      "loss: 0.656458  [17600/175341]\n",
      "loss: 0.333452  [19200/175341]\n",
      "loss: 0.466926  [20800/175341]\n",
      "loss: 0.637132  [22400/175341]\n",
      "loss: 0.638295  [24000/175341]\n",
      "loss: 0.840328  [25600/175341]\n",
      "loss: 0.388684  [27200/175341]\n",
      "loss: 0.333305  [28800/175341]\n",
      "loss: 0.509062  [30400/175341]\n",
      "loss: 0.264320  [32000/175341]\n",
      "loss: 0.492881  [33600/175341]\n",
      "loss: 0.351494  [35200/175341]\n",
      "loss: 0.250916  [36800/175341]\n",
      "loss: 0.239284  [38400/175341]\n",
      "loss: 0.440820  [40000/175341]\n",
      "loss: 0.506799  [41600/175341]\n",
      "loss: 0.439318  [43200/175341]\n",
      "loss: 0.700710  [44800/175341]\n",
      "loss: 0.743653  [46400/175341]\n",
      "loss: 0.476305  [48000/175341]\n",
      "loss: 0.375555  [49600/175341]\n",
      "loss: 0.627698  [51200/175341]\n",
      "loss: 0.214047  [52800/175341]\n",
      "loss: 0.692014  [54400/175341]\n",
      "loss: 0.183816  [56000/175341]\n",
      "loss: 0.273070  [57600/175341]\n",
      "loss: 0.507168  [59200/175341]\n",
      "loss: 0.459246  [60800/175341]\n",
      "loss: 1.032768  [62400/175341]\n",
      "loss: 0.645011  [64000/175341]\n",
      "loss: 0.608033  [65600/175341]\n",
      "loss: 0.317568  [67200/175341]\n",
      "loss: 0.498936  [68800/175341]\n",
      "loss: 0.156852  [70400/175341]\n",
      "loss: 0.537807  [72000/175341]\n",
      "loss: 0.279532  [73600/175341]\n",
      "loss: 0.891909  [75200/175341]\n",
      "loss: 0.735930  [76800/175341]\n",
      "loss: 0.475769  [78400/175341]\n",
      "loss: 0.468150  [80000/175341]\n",
      "loss: 0.156051  [81600/175341]\n",
      "loss: 1.289327  [83200/175341]\n",
      "loss: 0.290955  [84800/175341]\n",
      "loss: 0.456371  [86400/175341]\n",
      "loss: 0.389095  [88000/175341]\n",
      "loss: 0.601223  [89600/175341]\n",
      "loss: 0.292637  [91200/175341]\n",
      "loss: 0.254553  [92800/175341]\n",
      "loss: 0.960714  [94400/175341]\n",
      "loss: 0.363240  [96000/175341]\n",
      "loss: 0.153710  [97600/175341]\n",
      "loss: 0.459065  [99200/175341]\n",
      "loss: 0.234493  [100800/175341]\n",
      "loss: 0.388514  [102400/175341]\n",
      "loss: 0.392674  [104000/175341]\n",
      "loss: 0.602477  [105600/175341]\n",
      "loss: 0.318283  [107200/175341]\n",
      "loss: 0.494828  [108800/175341]\n",
      "loss: 0.650571  [110400/175341]\n",
      "loss: 0.386955  [112000/175341]\n",
      "loss: 0.679082  [113600/175341]\n",
      "loss: 0.227268  [115200/175341]\n",
      "loss: 0.253876  [116800/175341]\n",
      "loss: 0.505893  [118400/175341]\n",
      "loss: 0.435274  [120000/175341]\n",
      "loss: 0.267485  [121600/175341]\n",
      "loss: 0.476072  [123200/175341]\n",
      "loss: 0.325772  [124800/175341]\n",
      "loss: 0.429640  [126400/175341]\n",
      "loss: 0.523357  [128000/175341]\n",
      "loss: 0.553761  [129600/175341]\n",
      "loss: 0.139298  [131200/175341]\n",
      "loss: 0.667670  [132800/175341]\n",
      "loss: 0.416050  [134400/175341]\n",
      "loss: 1.023053  [136000/175341]\n",
      "loss: 0.651311  [137600/175341]\n",
      "loss: 0.205534  [139200/175341]\n",
      "loss: 0.459594  [140800/175341]\n",
      "loss: 0.343955  [142400/175341]\n",
      "loss: 0.437218  [144000/175341]\n",
      "loss: 0.722682  [145600/175341]\n",
      "loss: 0.336293  [147200/175341]\n",
      "loss: 0.561544  [148800/175341]\n",
      "loss: 0.197961  [150400/175341]\n",
      "loss: 0.375043  [152000/175341]\n",
      "loss: 0.647808  [153600/175341]\n",
      "loss: 0.452661  [155200/175341]\n",
      "loss: 0.387632  [156800/175341]\n",
      "loss: 0.747808  [158400/175341]\n",
      "loss: 0.408665  [160000/175341]\n",
      "loss: 0.415492  [161600/175341]\n",
      "loss: 0.491972  [163200/175341]\n",
      "loss: 0.432802  [164800/175341]\n",
      "loss: 0.369054  [166400/175341]\n",
      "loss: 0.266855  [168000/175341]\n",
      "loss: 0.330616  [169600/175341]\n",
      "loss: 0.384124  [171200/175341]\n",
      "loss: 0.349915  [172800/175341]\n",
      "loss: 0.780043  [174400/175341]\n",
      "Train Accuracy: 82.2090%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.541130, F1-score: 76.67%, Macro_F1-Score:  43.25%  \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.305431  [    0/175341]\n",
      "loss: 0.547830  [ 1600/175341]\n",
      "loss: 1.083721  [ 3200/175341]\n",
      "loss: 0.407438  [ 4800/175341]\n",
      "loss: 0.400393  [ 6400/175341]\n",
      "loss: 0.215731  [ 8000/175341]\n",
      "loss: 0.661825  [ 9600/175341]\n",
      "loss: 0.193821  [11200/175341]\n",
      "loss: 0.618294  [12800/175341]\n",
      "loss: 0.183617  [14400/175341]\n",
      "loss: 0.082084  [16000/175341]\n",
      "loss: 0.395629  [17600/175341]\n",
      "loss: 0.814563  [19200/175341]\n",
      "loss: 0.328551  [20800/175341]\n",
      "loss: 0.797253  [22400/175341]\n",
      "loss: 0.136359  [24000/175341]\n",
      "loss: 0.448266  [25600/175341]\n",
      "loss: 0.506350  [27200/175341]\n",
      "loss: 0.342846  [28800/175341]\n",
      "loss: 0.270940  [30400/175341]\n",
      "loss: 0.237261  [32000/175341]\n",
      "loss: 0.306677  [33600/175341]\n",
      "loss: 0.964354  [35200/175341]\n",
      "loss: 0.637128  [36800/175341]\n",
      "loss: 0.248947  [38400/175341]\n",
      "loss: 0.397599  [40000/175341]\n",
      "loss: 0.694304  [41600/175341]\n",
      "loss: 0.275517  [43200/175341]\n",
      "loss: 0.720108  [44800/175341]\n",
      "loss: 0.572652  [46400/175341]\n",
      "loss: 0.268561  [48000/175341]\n",
      "loss: 0.480169  [49600/175341]\n",
      "loss: 0.606359  [51200/175341]\n",
      "loss: 0.242457  [52800/175341]\n",
      "loss: 0.069534  [54400/175341]\n",
      "loss: 0.480998  [56000/175341]\n",
      "loss: 0.396269  [57600/175341]\n",
      "loss: 0.449154  [59200/175341]\n",
      "loss: 0.450344  [60800/175341]\n",
      "loss: 0.379925  [62400/175341]\n",
      "loss: 0.440367  [64000/175341]\n",
      "loss: 0.625581  [65600/175341]\n",
      "loss: 0.201811  [67200/175341]\n",
      "loss: 0.516623  [68800/175341]\n",
      "loss: 0.324435  [70400/175341]\n",
      "loss: 0.113316  [72000/175341]\n",
      "loss: 0.275369  [73600/175341]\n",
      "loss: 0.621795  [75200/175341]\n",
      "loss: 0.235344  [76800/175341]\n",
      "loss: 0.152332  [78400/175341]\n",
      "loss: 0.383999  [80000/175341]\n",
      "loss: 0.330317  [81600/175341]\n",
      "loss: 0.299588  [83200/175341]\n",
      "loss: 0.229520  [84800/175341]\n",
      "loss: 0.148963  [86400/175341]\n",
      "loss: 0.658120  [88000/175341]\n",
      "loss: 0.353844  [89600/175341]\n",
      "loss: 0.681155  [91200/175341]\n",
      "loss: 0.780786  [92800/175341]\n",
      "loss: 0.591516  [94400/175341]\n",
      "loss: 0.255702  [96000/175341]\n",
      "loss: 0.135366  [97600/175341]\n",
      "loss: 0.201955  [99200/175341]\n",
      "loss: 0.117654  [100800/175341]\n",
      "loss: 0.258947  [102400/175341]\n",
      "loss: 0.481506  [104000/175341]\n",
      "loss: 0.159404  [105600/175341]\n",
      "loss: 0.582685  [107200/175341]\n",
      "loss: 0.344410  [108800/175341]\n",
      "loss: 0.468694  [110400/175341]\n",
      "loss: 0.534656  [112000/175341]\n",
      "loss: 0.828532  [113600/175341]\n",
      "loss: 0.560575  [115200/175341]\n",
      "loss: 0.126774  [116800/175341]\n",
      "loss: 0.447713  [118400/175341]\n",
      "loss: 0.451895  [120000/175341]\n",
      "loss: 0.884079  [121600/175341]\n",
      "loss: 0.199163  [123200/175341]\n",
      "loss: 0.094027  [124800/175341]\n",
      "loss: 0.628043  [126400/175341]\n",
      "loss: 0.285457  [128000/175341]\n",
      "loss: 0.612620  [129600/175341]\n",
      "loss: 0.435089  [131200/175341]\n",
      "loss: 0.353588  [132800/175341]\n",
      "loss: 0.435678  [134400/175341]\n",
      "loss: 0.408995  [136000/175341]\n",
      "loss: 0.224405  [137600/175341]\n",
      "loss: 0.636538  [139200/175341]\n",
      "loss: 0.446238  [140800/175341]\n",
      "loss: 0.188221  [142400/175341]\n",
      "loss: 0.306165  [144000/175341]\n",
      "loss: 0.342539  [145600/175341]\n",
      "loss: 0.301250  [147200/175341]\n",
      "loss: 0.640974  [148800/175341]\n",
      "loss: 0.448700  [150400/175341]\n",
      "loss: 0.379247  [152000/175341]\n",
      "loss: 0.705630  [153600/175341]\n",
      "loss: 0.255426  [155200/175341]\n",
      "loss: 0.348600  [156800/175341]\n",
      "loss: 0.359336  [158400/175341]\n",
      "loss: 0.575685  [160000/175341]\n",
      "loss: 0.648638  [161600/175341]\n",
      "loss: 0.169344  [163200/175341]\n",
      "loss: 0.430498  [164800/175341]\n",
      "loss: 0.101045  [166400/175341]\n",
      "loss: 0.788258  [168000/175341]\n",
      "loss: 0.401303  [169600/175341]\n",
      "loss: 0.363608  [171200/175341]\n",
      "loss: 0.227464  [172800/175341]\n",
      "loss: 0.212253  [174400/175341]\n",
      "Train Accuracy: 82.2032%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.552471, F1-score: 76.88%, Macro_F1-Score:  43.11%  \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.324569  [    0/175341]\n",
      "loss: 0.553308  [ 1600/175341]\n",
      "loss: 0.050247  [ 3200/175341]\n",
      "loss: 0.663210  [ 4800/175341]\n",
      "loss: 0.176683  [ 6400/175341]\n",
      "loss: 0.280926  [ 8000/175341]\n",
      "loss: 0.330101  [ 9600/175341]\n",
      "loss: 0.513449  [11200/175341]\n",
      "loss: 0.503409  [12800/175341]\n",
      "loss: 0.288220  [14400/175341]\n",
      "loss: 0.398156  [16000/175341]\n",
      "loss: 0.418134  [17600/175341]\n",
      "loss: 0.254941  [19200/175341]\n",
      "loss: 0.202721  [20800/175341]\n",
      "loss: 0.545369  [22400/175341]\n",
      "loss: 0.417916  [24000/175341]\n",
      "loss: 0.157384  [25600/175341]\n",
      "loss: 0.224842  [27200/175341]\n",
      "loss: 0.451067  [28800/175341]\n",
      "loss: 0.207325  [30400/175341]\n",
      "loss: 0.726048  [32000/175341]\n",
      "loss: 0.244159  [33600/175341]\n",
      "loss: 0.324102  [35200/175341]\n",
      "loss: 0.216200  [36800/175341]\n",
      "loss: 0.113608  [38400/175341]\n",
      "loss: 0.284736  [40000/175341]\n",
      "loss: 0.310704  [41600/175341]\n",
      "loss: 0.624781  [43200/175341]\n",
      "loss: 0.326286  [44800/175341]\n",
      "loss: 0.789842  [46400/175341]\n",
      "loss: 0.527570  [48000/175341]\n",
      "loss: 0.297136  [49600/175341]\n",
      "loss: 0.190887  [51200/175341]\n",
      "loss: 0.578579  [52800/175341]\n",
      "loss: 0.232940  [54400/175341]\n",
      "loss: 0.222556  [56000/175341]\n",
      "loss: 0.265973  [57600/175341]\n",
      "loss: 0.090770  [59200/175341]\n",
      "loss: 0.524165  [60800/175341]\n",
      "loss: 0.330476  [62400/175341]\n",
      "loss: 0.430619  [64000/175341]\n",
      "loss: 0.388789  [65600/175341]\n",
      "loss: 0.375450  [67200/175341]\n",
      "loss: 0.555650  [68800/175341]\n",
      "loss: 0.634043  [70400/175341]\n",
      "loss: 0.532182  [72000/175341]\n",
      "loss: 0.664049  [73600/175341]\n",
      "loss: 0.469112  [75200/175341]\n",
      "loss: 0.416401  [76800/175341]\n",
      "loss: 0.268288  [78400/175341]\n",
      "loss: 0.936827  [80000/175341]\n",
      "loss: 0.199877  [81600/175341]\n",
      "loss: 0.250880  [83200/175341]\n",
      "loss: 0.171532  [84800/175341]\n",
      "loss: 0.396250  [86400/175341]\n",
      "loss: 0.388609  [88000/175341]\n",
      "loss: 0.397310  [89600/175341]\n",
      "loss: 0.466959  [91200/175341]\n",
      "loss: 0.663320  [92800/175341]\n",
      "loss: 0.226673  [94400/175341]\n",
      "loss: 0.539738  [96000/175341]\n",
      "loss: 0.443722  [97600/175341]\n",
      "loss: 0.392066  [99200/175341]\n",
      "loss: 0.329709  [100800/175341]\n",
      "loss: 0.309429  [102400/175341]\n",
      "loss: 0.658709  [104000/175341]\n",
      "loss: 0.433333  [105600/175341]\n",
      "loss: 0.246047  [107200/175341]\n",
      "loss: 0.777287  [108800/175341]\n",
      "loss: 0.472364  [110400/175341]\n",
      "loss: 0.178235  [112000/175341]\n",
      "loss: 0.621920  [113600/175341]\n",
      "loss: 0.349467  [115200/175341]\n",
      "loss: 0.418385  [116800/175341]\n",
      "loss: 0.071151  [118400/175341]\n",
      "loss: 0.213568  [120000/175341]\n",
      "loss: 0.390063  [121600/175341]\n",
      "loss: 0.280352  [123200/175341]\n",
      "loss: 0.673529  [124800/175341]\n",
      "loss: 0.599332  [126400/175341]\n",
      "loss: 0.703670  [128000/175341]\n",
      "loss: 0.639512  [129600/175341]\n",
      "loss: 0.602195  [131200/175341]\n",
      "loss: 0.544500  [132800/175341]\n",
      "loss: 0.395297  [134400/175341]\n",
      "loss: 0.510488  [136000/175341]\n",
      "loss: 0.276593  [137600/175341]\n",
      "loss: 0.260110  [139200/175341]\n",
      "loss: 0.251225  [140800/175341]\n",
      "loss: 0.620662  [142400/175341]\n",
      "loss: 0.400757  [144000/175341]\n",
      "loss: 0.335361  [145600/175341]\n",
      "loss: 0.291075  [147200/175341]\n",
      "loss: 0.506345  [148800/175341]\n",
      "loss: 0.425813  [150400/175341]\n",
      "loss: 0.262695  [152000/175341]\n",
      "loss: 0.420008  [153600/175341]\n",
      "loss: 0.440046  [155200/175341]\n",
      "loss: 0.186386  [156800/175341]\n",
      "loss: 0.378112  [158400/175341]\n",
      "loss: 0.695777  [160000/175341]\n",
      "loss: 0.251049  [161600/175341]\n",
      "loss: 0.152432  [163200/175341]\n",
      "loss: 0.169898  [164800/175341]\n",
      "loss: 0.437828  [166400/175341]\n",
      "loss: 0.285593  [168000/175341]\n",
      "loss: 0.479211  [169600/175341]\n",
      "loss: 0.188679  [171200/175341]\n",
      "loss: 0.427745  [172800/175341]\n",
      "loss: 0.228379  [174400/175341]\n",
      "Train Accuracy: 82.1993%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.556092, F1-score: 75.86%, Macro_F1-Score:  42.87%  \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.788205  [    0/175341]\n",
      "loss: 0.659793  [ 1600/175341]\n",
      "loss: 0.275014  [ 3200/175341]\n",
      "loss: 0.662180  [ 4800/175341]\n",
      "loss: 0.451045  [ 6400/175341]\n",
      "loss: 0.798525  [ 8000/175341]\n",
      "loss: 0.363437  [ 9600/175341]\n",
      "loss: 0.524382  [11200/175341]\n",
      "loss: 0.414714  [12800/175341]\n",
      "loss: 0.225086  [14400/175341]\n",
      "loss: 0.248131  [16000/175341]\n",
      "loss: 0.157583  [17600/175341]\n",
      "loss: 0.390305  [19200/175341]\n",
      "loss: 0.251686  [20800/175341]\n",
      "loss: 0.590423  [22400/175341]\n",
      "loss: 0.140013  [24000/175341]\n",
      "loss: 0.668265  [25600/175341]\n",
      "loss: 0.250770  [27200/175341]\n",
      "loss: 0.280209  [28800/175341]\n",
      "loss: 0.284531  [30400/175341]\n",
      "loss: 0.134996  [32000/175341]\n",
      "loss: 0.181020  [33600/175341]\n",
      "loss: 0.212626  [35200/175341]\n",
      "loss: 0.350979  [36800/175341]\n",
      "loss: 0.413920  [38400/175341]\n",
      "loss: 0.517433  [40000/175341]\n",
      "loss: 0.225485  [41600/175341]\n",
      "loss: 0.491891  [43200/175341]\n",
      "loss: 0.319889  [44800/175341]\n",
      "loss: 0.448888  [46400/175341]\n",
      "loss: 0.238060  [48000/175341]\n",
      "loss: 0.536108  [49600/175341]\n",
      "loss: 0.290553  [51200/175341]\n",
      "loss: 0.484961  [52800/175341]\n",
      "loss: 0.419514  [54400/175341]\n",
      "loss: 0.518292  [56000/175341]\n",
      "loss: 0.293554  [57600/175341]\n",
      "loss: 0.342628  [59200/175341]\n",
      "loss: 0.453453  [60800/175341]\n",
      "loss: 0.322356  [62400/175341]\n",
      "loss: 0.381964  [64000/175341]\n",
      "loss: 0.187527  [65600/175341]\n",
      "loss: 0.237380  [67200/175341]\n",
      "loss: 0.350333  [68800/175341]\n",
      "loss: 0.648695  [70400/175341]\n",
      "loss: 0.198148  [72000/175341]\n",
      "loss: 0.461426  [73600/175341]\n",
      "loss: 0.917246  [75200/175341]\n",
      "loss: 0.617243  [76800/175341]\n",
      "loss: 0.472917  [78400/175341]\n",
      "loss: 0.576912  [80000/175341]\n",
      "loss: 0.256884  [81600/175341]\n",
      "loss: 0.434606  [83200/175341]\n",
      "loss: 0.773793  [84800/175341]\n",
      "loss: 0.177035  [86400/175341]\n",
      "loss: 0.395215  [88000/175341]\n",
      "loss: 0.441215  [89600/175341]\n",
      "loss: 0.274469  [91200/175341]\n",
      "loss: 0.564524  [92800/175341]\n",
      "loss: 0.982464  [94400/175341]\n",
      "loss: 0.691423  [96000/175341]\n",
      "loss: 0.826492  [97600/175341]\n",
      "loss: 0.302859  [99200/175341]\n",
      "loss: 0.343792  [100800/175341]\n",
      "loss: 0.565550  [102400/175341]\n",
      "loss: 0.414321  [104000/175341]\n",
      "loss: 0.222944  [105600/175341]\n",
      "loss: 0.455291  [107200/175341]\n",
      "loss: 0.677232  [108800/175341]\n",
      "loss: 0.452048  [110400/175341]\n",
      "loss: 0.564405  [112000/175341]\n",
      "loss: 0.763778  [113600/175341]\n",
      "loss: 0.207358  [115200/175341]\n",
      "loss: 0.607949  [116800/175341]\n",
      "loss: 0.430696  [118400/175341]\n",
      "loss: 0.918448  [120000/175341]\n",
      "loss: 0.459701  [121600/175341]\n",
      "loss: 0.369062  [123200/175341]\n",
      "loss: 0.342096  [124800/175341]\n",
      "loss: 0.863034  [126400/175341]\n",
      "loss: 0.535871  [128000/175341]\n",
      "loss: 0.513433  [129600/175341]\n",
      "loss: 0.527330  [131200/175341]\n",
      "loss: 0.176212  [132800/175341]\n",
      "loss: 0.595678  [134400/175341]\n",
      "loss: 0.385655  [136000/175341]\n",
      "loss: 1.273387  [137600/175341]\n",
      "loss: 0.939665  [139200/175341]\n",
      "loss: 0.617125  [140800/175341]\n",
      "loss: 0.318763  [142400/175341]\n",
      "loss: 0.297035  [144000/175341]\n",
      "loss: 0.401340  [145600/175341]\n",
      "loss: 0.449938  [147200/175341]\n",
      "loss: 0.926547  [148800/175341]\n",
      "loss: 0.527756  [150400/175341]\n",
      "loss: 0.373782  [152000/175341]\n",
      "loss: 0.686203  [153600/175341]\n",
      "loss: 0.462104  [155200/175341]\n",
      "loss: 0.126696  [156800/175341]\n",
      "loss: 0.397767  [158400/175341]\n",
      "loss: 0.589442  [160000/175341]\n",
      "loss: 0.236058  [161600/175341]\n",
      "loss: 0.430944  [163200/175341]\n",
      "loss: 0.368984  [164800/175341]\n",
      "loss: 0.323821  [166400/175341]\n",
      "loss: 0.543564  [168000/175341]\n",
      "loss: 0.752527  [169600/175341]\n",
      "loss: 0.484252  [171200/175341]\n",
      "loss: 0.901721  [172800/175341]\n",
      "loss: 0.289578  [174400/175341]\n",
      "Train Accuracy: 82.1810%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.549086, F1-score: 76.52%, Macro_F1-Score:  43.28%  \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.109714  [    0/175341]\n",
      "loss: 0.175282  [ 1600/175341]\n",
      "loss: 0.190598  [ 3200/175341]\n",
      "loss: 0.450541  [ 4800/175341]\n",
      "loss: 0.684905  [ 6400/175341]\n",
      "loss: 0.292325  [ 8000/175341]\n",
      "loss: 0.784477  [ 9600/175341]\n",
      "loss: 0.566875  [11200/175341]\n",
      "loss: 0.583226  [12800/175341]\n",
      "loss: 0.308652  [14400/175341]\n",
      "loss: 0.311466  [16000/175341]\n",
      "loss: 0.561246  [17600/175341]\n",
      "loss: 0.530433  [19200/175341]\n",
      "loss: 0.318474  [20800/175341]\n",
      "loss: 0.704029  [22400/175341]\n",
      "loss: 0.532767  [24000/175341]\n",
      "loss: 0.362397  [25600/175341]\n",
      "loss: 0.152483  [27200/175341]\n",
      "loss: 0.510713  [28800/175341]\n",
      "loss: 0.334626  [30400/175341]\n",
      "loss: 0.514793  [32000/175341]\n",
      "loss: 0.158016  [33600/175341]\n",
      "loss: 0.447776  [35200/175341]\n",
      "loss: 0.546451  [36800/175341]\n",
      "loss: 0.615627  [38400/175341]\n",
      "loss: 0.473941  [40000/175341]\n",
      "loss: 0.162011  [41600/175341]\n",
      "loss: 0.500572  [43200/175341]\n",
      "loss: 0.788333  [44800/175341]\n",
      "loss: 0.732955  [46400/175341]\n",
      "loss: 0.453462  [48000/175341]\n",
      "loss: 0.168953  [49600/175341]\n",
      "loss: 0.548115  [51200/175341]\n",
      "loss: 0.694219  [52800/175341]\n",
      "loss: 0.445034  [54400/175341]\n",
      "loss: 0.187006  [56000/175341]\n",
      "loss: 0.153612  [57600/175341]\n",
      "loss: 0.226529  [59200/175341]\n",
      "loss: 0.318024  [60800/175341]\n",
      "loss: 0.585814  [62400/175341]\n",
      "loss: 0.342887  [64000/175341]\n",
      "loss: 0.652641  [65600/175341]\n",
      "loss: 0.078288  [67200/175341]\n",
      "loss: 0.611135  [68800/175341]\n",
      "loss: 0.368204  [70400/175341]\n",
      "loss: 0.370492  [72000/175341]\n",
      "loss: 0.741922  [73600/175341]\n",
      "loss: 0.296275  [75200/175341]\n",
      "loss: 0.334877  [76800/175341]\n",
      "loss: 0.344857  [78400/175341]\n",
      "loss: 0.447345  [80000/175341]\n",
      "loss: 0.398671  [81600/175341]\n",
      "loss: 0.208195  [83200/175341]\n",
      "loss: 0.538516  [84800/175341]\n",
      "loss: 0.497579  [86400/175341]\n",
      "loss: 0.345672  [88000/175341]\n",
      "loss: 0.268450  [89600/175341]\n",
      "loss: 0.315003  [91200/175341]\n",
      "loss: 1.096266  [92800/175341]\n",
      "loss: 0.312472  [94400/175341]\n",
      "loss: 0.535444  [96000/175341]\n",
      "loss: 0.397081  [97600/175341]\n",
      "loss: 0.400457  [99200/175341]\n",
      "loss: 0.519680  [100800/175341]\n",
      "loss: 0.547916  [102400/175341]\n",
      "loss: 0.302332  [104000/175341]\n",
      "loss: 0.341619  [105600/175341]\n",
      "loss: 0.266879  [107200/175341]\n",
      "loss: 0.418338  [108800/175341]\n",
      "loss: 1.015625  [110400/175341]\n",
      "loss: 0.163786  [112000/175341]\n",
      "loss: 0.401532  [113600/175341]\n",
      "loss: 0.497836  [115200/175341]\n",
      "loss: 0.565894  [116800/175341]\n",
      "loss: 0.557276  [118400/175341]\n",
      "loss: 0.607201  [120000/175341]\n",
      "loss: 0.530167  [121600/175341]\n",
      "loss: 0.194716  [123200/175341]\n",
      "loss: 0.438621  [124800/175341]\n",
      "loss: 0.758219  [126400/175341]\n",
      "loss: 0.417609  [128000/175341]\n",
      "loss: 0.536627  [129600/175341]\n",
      "loss: 0.508029  [131200/175341]\n",
      "loss: 0.416076  [132800/175341]\n",
      "loss: 0.658356  [134400/175341]\n",
      "loss: 0.569886  [136000/175341]\n",
      "loss: 0.435559  [137600/175341]\n",
      "loss: 0.276708  [139200/175341]\n",
      "loss: 0.340372  [140800/175341]\n",
      "loss: 0.200042  [142400/175341]\n",
      "loss: 0.595077  [144000/175341]\n",
      "loss: 0.389282  [145600/175341]\n",
      "loss: 0.510718  [147200/175341]\n",
      "loss: 0.376613  [148800/175341]\n",
      "loss: 0.434656  [150400/175341]\n",
      "loss: 0.207125  [152000/175341]\n",
      "loss: 0.475952  [153600/175341]\n",
      "loss: 0.334375  [155200/175341]\n",
      "loss: 0.477433  [156800/175341]\n",
      "loss: 0.411535  [158400/175341]\n",
      "loss: 0.509272  [160000/175341]\n",
      "loss: 0.613773  [161600/175341]\n",
      "loss: 0.391258  [163200/175341]\n",
      "loss: 0.812249  [164800/175341]\n",
      "loss: 0.769315  [166400/175341]\n",
      "loss: 0.706204  [168000/175341]\n",
      "loss: 0.432355  [169600/175341]\n",
      "loss: 0.558891  [171200/175341]\n",
      "loss: 0.622939  [172800/175341]\n",
      "loss: 0.332908  [174400/175341]\n",
      "Train Accuracy: 82.2078%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.569875, F1-score: 75.53%, Macro_F1-Score:  43.55%  \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.470075  [    0/175341]\n",
      "loss: 0.497826  [ 1600/175341]\n",
      "loss: 0.261193  [ 3200/175341]\n",
      "loss: 0.279630  [ 4800/175341]\n",
      "loss: 0.333074  [ 6400/175341]\n",
      "loss: 0.507306  [ 8000/175341]\n",
      "loss: 0.425762  [ 9600/175341]\n",
      "loss: 0.508110  [11200/175341]\n",
      "loss: 0.518992  [12800/175341]\n",
      "loss: 0.404709  [14400/175341]\n",
      "loss: 0.409386  [16000/175341]\n",
      "loss: 0.536827  [17600/175341]\n",
      "loss: 0.732308  [19200/175341]\n",
      "loss: 0.453702  [20800/175341]\n",
      "loss: 0.311197  [22400/175341]\n",
      "loss: 0.266264  [24000/175341]\n",
      "loss: 0.328623  [25600/175341]\n",
      "loss: 0.598829  [27200/175341]\n",
      "loss: 0.377123  [28800/175341]\n",
      "loss: 0.345191  [30400/175341]\n",
      "loss: 0.660789  [32000/175341]\n",
      "loss: 0.544137  [33600/175341]\n",
      "loss: 0.138902  [35200/175341]\n",
      "loss: 0.365812  [36800/175341]\n",
      "loss: 0.717148  [38400/175341]\n",
      "loss: 0.400502  [40000/175341]\n",
      "loss: 0.010722  [41600/175341]\n",
      "loss: 0.780206  [43200/175341]\n",
      "loss: 0.736045  [44800/175341]\n",
      "loss: 0.429024  [46400/175341]\n",
      "loss: 0.523806  [48000/175341]\n",
      "loss: 0.513778  [49600/175341]\n",
      "loss: 0.848596  [51200/175341]\n",
      "loss: 0.073917  [52800/175341]\n",
      "loss: 0.512044  [54400/175341]\n",
      "loss: 0.616490  [56000/175341]\n",
      "loss: 0.322871  [57600/175341]\n",
      "loss: 0.522843  [59200/175341]\n",
      "loss: 0.803065  [60800/175341]\n",
      "loss: 0.804398  [62400/175341]\n",
      "loss: 0.448217  [64000/175341]\n",
      "loss: 0.324702  [65600/175341]\n",
      "loss: 0.764179  [67200/175341]\n",
      "loss: 0.457281  [68800/175341]\n",
      "loss: 0.278813  [70400/175341]\n",
      "loss: 0.365153  [72000/175341]\n",
      "loss: 0.484800  [73600/175341]\n",
      "loss: 0.491695  [75200/175341]\n",
      "loss: 0.279438  [76800/175341]\n",
      "loss: 0.423835  [78400/175341]\n",
      "loss: 0.394520  [80000/175341]\n",
      "loss: 0.332441  [81600/175341]\n",
      "loss: 0.478772  [83200/175341]\n",
      "loss: 0.886779  [84800/175341]\n",
      "loss: 0.597694  [86400/175341]\n",
      "loss: 0.465012  [88000/175341]\n",
      "loss: 0.616416  [89600/175341]\n",
      "loss: 0.296779  [91200/175341]\n",
      "loss: 0.323802  [92800/175341]\n",
      "loss: 0.362201  [94400/175341]\n",
      "loss: 0.359852  [96000/175341]\n",
      "loss: 0.582231  [97600/175341]\n",
      "loss: 0.456932  [99200/175341]\n",
      "loss: 0.225937  [100800/175341]\n",
      "loss: 0.231603  [102400/175341]\n",
      "loss: 0.598233  [104000/175341]\n",
      "loss: 0.410880  [105600/175341]\n",
      "loss: 0.571136  [107200/175341]\n",
      "loss: 0.485921  [108800/175341]\n",
      "loss: 0.780331  [110400/175341]\n",
      "loss: 0.395536  [112000/175341]\n",
      "loss: 0.525888  [113600/175341]\n",
      "loss: 0.712065  [115200/175341]\n",
      "loss: 0.529484  [116800/175341]\n",
      "loss: 0.642576  [118400/175341]\n",
      "loss: 0.264413  [120000/175341]\n",
      "loss: 0.463501  [121600/175341]\n",
      "loss: 1.007005  [123200/175341]\n",
      "loss: 0.378515  [124800/175341]\n",
      "loss: 0.236344  [126400/175341]\n",
      "loss: 0.164199  [128000/175341]\n",
      "loss: 0.245215  [129600/175341]\n",
      "loss: 0.589847  [131200/175341]\n",
      "loss: 0.695062  [132800/175341]\n",
      "loss: 0.209880  [134400/175341]\n",
      "loss: 0.802080  [136000/175341]\n",
      "loss: 0.382637  [137600/175341]\n",
      "loss: 0.343966  [139200/175341]\n",
      "loss: 0.393808  [140800/175341]\n",
      "loss: 0.168626  [142400/175341]\n",
      "loss: 0.447697  [144000/175341]\n",
      "loss: 0.385299  [145600/175341]\n",
      "loss: 0.298537  [147200/175341]\n",
      "loss: 0.460271  [148800/175341]\n",
      "loss: 0.474137  [150400/175341]\n",
      "loss: 0.827405  [152000/175341]\n",
      "loss: 0.606110  [153600/175341]\n",
      "loss: 0.823827  [155200/175341]\n",
      "loss: 0.485251  [156800/175341]\n",
      "loss: 0.923818  [158400/175341]\n",
      "loss: 0.584032  [160000/175341]\n",
      "loss: 0.355608  [161600/175341]\n",
      "loss: 0.680269  [163200/175341]\n",
      "loss: 0.259087  [164800/175341]\n",
      "loss: 0.132286  [166400/175341]\n",
      "loss: 0.983934  [168000/175341]\n",
      "loss: 0.396845  [169600/175341]\n",
      "loss: 0.285721  [171200/175341]\n",
      "loss: 0.338174  [172800/175341]\n",
      "loss: 0.509166  [174400/175341]\n",
      "Train Accuracy: 82.1844%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.555673, F1-score: 76.77%, Macro_F1-Score:  42.24%  \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.704286  [    0/175341]\n",
      "loss: 0.794040  [ 1600/175341]\n",
      "loss: 0.174374  [ 3200/175341]\n",
      "loss: 0.365242  [ 4800/175341]\n",
      "loss: 0.636465  [ 6400/175341]\n",
      "loss: 0.526956  [ 8000/175341]\n",
      "loss: 0.334922  [ 9600/175341]\n",
      "loss: 0.354230  [11200/175341]\n",
      "loss: 0.604032  [12800/175341]\n",
      "loss: 0.344275  [14400/175341]\n",
      "loss: 0.554560  [16000/175341]\n",
      "loss: 0.311598  [17600/175341]\n",
      "loss: 0.223651  [19200/175341]\n",
      "loss: 0.435276  [20800/175341]\n",
      "loss: 0.138308  [22400/175341]\n",
      "loss: 0.317875  [24000/175341]\n",
      "loss: 0.375974  [25600/175341]\n",
      "loss: 0.197753  [27200/175341]\n",
      "loss: 0.257476  [28800/175341]\n",
      "loss: 0.227437  [30400/175341]\n",
      "loss: 0.419764  [32000/175341]\n",
      "loss: 0.106227  [33600/175341]\n",
      "loss: 0.451435  [35200/175341]\n",
      "loss: 0.175604  [36800/175341]\n",
      "loss: 0.519551  [38400/175341]\n",
      "loss: 0.676668  [40000/175341]\n",
      "loss: 0.328660  [41600/175341]\n",
      "loss: 0.303284  [43200/175341]\n",
      "loss: 0.188752  [44800/175341]\n",
      "loss: 0.303957  [46400/175341]\n",
      "loss: 0.306096  [48000/175341]\n",
      "loss: 0.194169  [49600/175341]\n",
      "loss: 0.690855  [51200/175341]\n",
      "loss: 0.628749  [52800/175341]\n",
      "loss: 0.166687  [54400/175341]\n",
      "loss: 0.636162  [56000/175341]\n",
      "loss: 0.367769  [57600/175341]\n",
      "loss: 0.600195  [59200/175341]\n",
      "loss: 0.138720  [60800/175341]\n",
      "loss: 0.685878  [62400/175341]\n",
      "loss: 0.391550  [64000/175341]\n",
      "loss: 0.733396  [65600/175341]\n",
      "loss: 0.329054  [67200/175341]\n",
      "loss: 0.174580  [68800/175341]\n",
      "loss: 0.985841  [70400/175341]\n",
      "loss: 0.404888  [72000/175341]\n",
      "loss: 0.382878  [73600/175341]\n",
      "loss: 0.471881  [75200/175341]\n",
      "loss: 0.606861  [76800/175341]\n",
      "loss: 0.550619  [78400/175341]\n",
      "loss: 0.627404  [80000/175341]\n",
      "loss: 0.166132  [81600/175341]\n",
      "loss: 0.301908  [83200/175341]\n",
      "loss: 0.209140  [84800/175341]\n",
      "loss: 0.208558  [86400/175341]\n",
      "loss: 0.549991  [88000/175341]\n",
      "loss: 0.303292  [89600/175341]\n",
      "loss: 0.479188  [91200/175341]\n",
      "loss: 0.205282  [92800/175341]\n",
      "loss: 0.487065  [94400/175341]\n",
      "loss: 0.240568  [96000/175341]\n",
      "loss: 0.206558  [97600/175341]\n",
      "loss: 0.280628  [99200/175341]\n",
      "loss: 0.613140  [100800/175341]\n",
      "loss: 0.792999  [102400/175341]\n",
      "loss: 0.332233  [104000/175341]\n",
      "loss: 0.426206  [105600/175341]\n",
      "loss: 0.142233  [107200/175341]\n",
      "loss: 0.438666  [108800/175341]\n",
      "loss: 0.112333  [110400/175341]\n",
      "loss: 0.733011  [112000/175341]\n",
      "loss: 0.483172  [113600/175341]\n",
      "loss: 0.376900  [115200/175341]\n",
      "loss: 0.357874  [116800/175341]\n",
      "loss: 0.417195  [118400/175341]\n",
      "loss: 0.229878  [120000/175341]\n",
      "loss: 0.479711  [121600/175341]\n",
      "loss: 0.509861  [123200/175341]\n",
      "loss: 0.404275  [124800/175341]\n",
      "loss: 0.563331  [126400/175341]\n",
      "loss: 0.197352  [128000/175341]\n",
      "loss: 0.060290  [129600/175341]\n",
      "loss: 0.289686  [131200/175341]\n",
      "loss: 0.281723  [132800/175341]\n",
      "loss: 0.655710  [134400/175341]\n",
      "loss: 0.368464  [136000/175341]\n",
      "loss: 0.578813  [137600/175341]\n",
      "loss: 0.497578  [139200/175341]\n",
      "loss: 0.997336  [140800/175341]\n",
      "loss: 0.475434  [142400/175341]\n",
      "loss: 0.548941  [144000/175341]\n",
      "loss: 0.300834  [145600/175341]\n",
      "loss: 0.279020  [147200/175341]\n",
      "loss: 0.354590  [148800/175341]\n",
      "loss: 0.566860  [150400/175341]\n",
      "loss: 0.288355  [152000/175341]\n",
      "loss: 0.767472  [153600/175341]\n",
      "loss: 0.417782  [155200/175341]\n",
      "loss: 0.850916  [156800/175341]\n",
      "loss: 0.550129  [158400/175341]\n",
      "loss: 0.259631  [160000/175341]\n",
      "loss: 0.639872  [161600/175341]\n",
      "loss: 0.148165  [163200/175341]\n",
      "loss: 0.480426  [164800/175341]\n",
      "loss: 0.790665  [166400/175341]\n",
      "loss: 0.450110  [168000/175341]\n",
      "loss: 0.315110  [169600/175341]\n",
      "loss: 0.322785  [171200/175341]\n",
      "loss: 0.789262  [172800/175341]\n",
      "loss: 0.481417  [174400/175341]\n",
      "Train Accuracy: 82.2135%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.543875, F1-score: 77.51%, Macro_F1-Score:  43.04%  \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.494280  [    0/175341]\n",
      "loss: 0.657370  [ 1600/175341]\n",
      "loss: 0.242777  [ 3200/175341]\n",
      "loss: 0.141981  [ 4800/175341]\n",
      "loss: 0.327366  [ 6400/175341]\n",
      "loss: 0.101833  [ 8000/175341]\n",
      "loss: 0.718673  [ 9600/175341]\n",
      "loss: 0.098253  [11200/175341]\n",
      "loss: 0.290642  [12800/175341]\n",
      "loss: 0.454199  [14400/175341]\n",
      "loss: 0.517200  [16000/175341]\n",
      "loss: 0.570336  [17600/175341]\n",
      "loss: 0.748016  [19200/175341]\n",
      "loss: 0.173017  [20800/175341]\n",
      "loss: 0.579238  [22400/175341]\n",
      "loss: 0.761534  [24000/175341]\n",
      "loss: 0.458455  [25600/175341]\n",
      "loss: 0.579525  [27200/175341]\n",
      "loss: 0.380727  [28800/175341]\n",
      "loss: 0.336569  [30400/175341]\n",
      "loss: 0.728523  [32000/175341]\n",
      "loss: 0.344000  [33600/175341]\n",
      "loss: 0.544071  [35200/175341]\n",
      "loss: 0.466618  [36800/175341]\n",
      "loss: 0.177550  [38400/175341]\n",
      "loss: 0.312773  [40000/175341]\n",
      "loss: 0.486728  [41600/175341]\n",
      "loss: 0.403202  [43200/175341]\n",
      "loss: 0.562114  [44800/175341]\n",
      "loss: 0.736178  [46400/175341]\n",
      "loss: 0.333114  [48000/175341]\n",
      "loss: 0.427226  [49600/175341]\n",
      "loss: 0.143589  [51200/175341]\n",
      "loss: 0.487303  [52800/175341]\n",
      "loss: 0.471722  [54400/175341]\n",
      "loss: 0.558588  [56000/175341]\n",
      "loss: 0.485958  [57600/175341]\n",
      "loss: 0.424252  [59200/175341]\n",
      "loss: 0.655739  [60800/175341]\n",
      "loss: 0.510971  [62400/175341]\n",
      "loss: 0.271614  [64000/175341]\n",
      "loss: 0.600327  [65600/175341]\n",
      "loss: 0.612709  [67200/175341]\n",
      "loss: 0.481809  [68800/175341]\n",
      "loss: 0.595264  [70400/175341]\n",
      "loss: 0.395438  [72000/175341]\n",
      "loss: 0.495329  [73600/175341]\n",
      "loss: 0.116336  [75200/175341]\n",
      "loss: 0.334159  [76800/175341]\n",
      "loss: 0.315490  [78400/175341]\n",
      "loss: 0.376746  [80000/175341]\n",
      "loss: 0.415766  [81600/175341]\n",
      "loss: 0.666595  [83200/175341]\n",
      "loss: 0.817363  [84800/175341]\n",
      "loss: 0.223908  [86400/175341]\n",
      "loss: 0.198323  [88000/175341]\n",
      "loss: 0.608513  [89600/175341]\n",
      "loss: 0.662052  [91200/175341]\n",
      "loss: 0.542868  [92800/175341]\n",
      "loss: 0.362473  [94400/175341]\n",
      "loss: 0.205420  [96000/175341]\n",
      "loss: 0.849937  [97600/175341]\n",
      "loss: 0.412405  [99200/175341]\n",
      "loss: 0.415276  [100800/175341]\n",
      "loss: 0.268713  [102400/175341]\n",
      "loss: 0.317200  [104000/175341]\n",
      "loss: 0.500984  [105600/175341]\n",
      "loss: 0.390258  [107200/175341]\n",
      "loss: 0.330229  [108800/175341]\n",
      "loss: 0.240889  [110400/175341]\n",
      "loss: 0.435903  [112000/175341]\n",
      "loss: 0.589293  [113600/175341]\n",
      "loss: 0.689799  [115200/175341]\n",
      "loss: 1.041273  [116800/175341]\n",
      "loss: 0.470153  [118400/175341]\n",
      "loss: 0.126358  [120000/175341]\n",
      "loss: 0.662188  [121600/175341]\n",
      "loss: 0.578570  [123200/175341]\n",
      "loss: 0.550298  [124800/175341]\n",
      "loss: 0.502008  [126400/175341]\n",
      "loss: 0.285346  [128000/175341]\n",
      "loss: 0.594752  [129600/175341]\n",
      "loss: 0.677732  [131200/175341]\n",
      "loss: 0.274041  [132800/175341]\n",
      "loss: 0.498386  [134400/175341]\n",
      "loss: 0.334455  [136000/175341]\n",
      "loss: 0.549262  [137600/175341]\n",
      "loss: 0.341376  [139200/175341]\n",
      "loss: 0.514165  [140800/175341]\n",
      "loss: 0.302439  [142400/175341]\n",
      "loss: 0.606447  [144000/175341]\n",
      "loss: 0.382400  [145600/175341]\n",
      "loss: 0.565637  [147200/175341]\n",
      "loss: 0.368388  [148800/175341]\n",
      "loss: 0.531324  [150400/175341]\n",
      "loss: 0.351213  [152000/175341]\n",
      "loss: 0.323626  [153600/175341]\n",
      "loss: 0.221880  [155200/175341]\n",
      "loss: 0.507536  [156800/175341]\n",
      "loss: 0.217463  [158400/175341]\n",
      "loss: 0.319012  [160000/175341]\n",
      "loss: 0.355037  [161600/175341]\n",
      "loss: 0.580904  [163200/175341]\n",
      "loss: 0.338871  [164800/175341]\n",
      "loss: 0.635554  [166400/175341]\n",
      "loss: 0.378020  [168000/175341]\n",
      "loss: 0.296365  [169600/175341]\n",
      "loss: 0.741376  [171200/175341]\n",
      "loss: 0.237515  [172800/175341]\n",
      "loss: 0.263233  [174400/175341]\n",
      "Train Accuracy: 82.1645%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.554114, F1-score: 76.83%, Macro_F1-Score:  43.96%  \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.558274  [    0/175341]\n",
      "loss: 0.110305  [ 1600/175341]\n",
      "loss: 0.667339  [ 3200/175341]\n",
      "loss: 0.546823  [ 4800/175341]\n",
      "loss: 0.161280  [ 6400/175341]\n",
      "loss: 0.360965  [ 8000/175341]\n",
      "loss: 0.665801  [ 9600/175341]\n",
      "loss: 0.432112  [11200/175341]\n",
      "loss: 0.284623  [12800/175341]\n",
      "loss: 0.519760  [14400/175341]\n",
      "loss: 0.360482  [16000/175341]\n",
      "loss: 0.358724  [17600/175341]\n",
      "loss: 0.462438  [19200/175341]\n",
      "loss: 0.358025  [20800/175341]\n",
      "loss: 0.365191  [22400/175341]\n",
      "loss: 0.152096  [24000/175341]\n",
      "loss: 0.114180  [25600/175341]\n",
      "loss: 0.449580  [27200/175341]\n",
      "loss: 0.575443  [28800/175341]\n",
      "loss: 0.366447  [30400/175341]\n",
      "loss: 1.064170  [32000/175341]\n",
      "loss: 0.165843  [33600/175341]\n",
      "loss: 0.220298  [35200/175341]\n",
      "loss: 0.373111  [36800/175341]\n",
      "loss: 0.222685  [38400/175341]\n",
      "loss: 0.383698  [40000/175341]\n",
      "loss: 0.425307  [41600/175341]\n",
      "loss: 0.485075  [43200/175341]\n",
      "loss: 0.631304  [44800/175341]\n",
      "loss: 0.673680  [46400/175341]\n",
      "loss: 0.177600  [48000/175341]\n",
      "loss: 0.240211  [49600/175341]\n",
      "loss: 0.178170  [51200/175341]\n",
      "loss: 0.470070  [52800/175341]\n",
      "loss: 0.282621  [54400/175341]\n",
      "loss: 0.484366  [56000/175341]\n",
      "loss: 0.935632  [57600/175341]\n",
      "loss: 0.476568  [59200/175341]\n",
      "loss: 0.350369  [60800/175341]\n",
      "loss: 0.529309  [62400/175341]\n",
      "loss: 0.166335  [64000/175341]\n",
      "loss: 0.310919  [65600/175341]\n",
      "loss: 0.552648  [67200/175341]\n",
      "loss: 0.279561  [68800/175341]\n",
      "loss: 0.329136  [70400/175341]\n",
      "loss: 0.357448  [72000/175341]\n",
      "loss: 0.556914  [73600/175341]\n",
      "loss: 0.497345  [75200/175341]\n",
      "loss: 0.288803  [76800/175341]\n",
      "loss: 0.266226  [78400/175341]\n",
      "loss: 0.316091  [80000/175341]\n",
      "loss: 0.315533  [81600/175341]\n",
      "loss: 0.323804  [83200/175341]\n",
      "loss: 0.799830  [84800/175341]\n",
      "loss: 0.487621  [86400/175341]\n",
      "loss: 0.949559  [88000/175341]\n",
      "loss: 0.415374  [89600/175341]\n",
      "loss: 0.309834  [91200/175341]\n",
      "loss: 0.587693  [92800/175341]\n",
      "loss: 0.173356  [94400/175341]\n",
      "loss: 0.718021  [96000/175341]\n",
      "loss: 1.136341  [97600/175341]\n",
      "loss: 0.538956  [99200/175341]\n",
      "loss: 0.414908  [100800/175341]\n",
      "loss: 0.534210  [102400/175341]\n",
      "loss: 0.350800  [104000/175341]\n",
      "loss: 0.373140  [105600/175341]\n",
      "loss: 0.996612  [107200/175341]\n",
      "loss: 0.575246  [108800/175341]\n",
      "loss: 0.199388  [110400/175341]\n",
      "loss: 0.663093  [112000/175341]\n",
      "loss: 0.315877  [113600/175341]\n",
      "loss: 0.536645  [115200/175341]\n",
      "loss: 0.541445  [116800/175341]\n",
      "loss: 0.242907  [118400/175341]\n",
      "loss: 0.283001  [120000/175341]\n",
      "loss: 0.894215  [121600/175341]\n",
      "loss: 0.316621  [123200/175341]\n",
      "loss: 0.251753  [124800/175341]\n",
      "loss: 0.313968  [126400/175341]\n",
      "loss: 0.381707  [128000/175341]\n",
      "loss: 0.228360  [129600/175341]\n",
      "loss: 0.333566  [131200/175341]\n",
      "loss: 0.382908  [132800/175341]\n",
      "loss: 0.348265  [134400/175341]\n",
      "loss: 0.438906  [136000/175341]\n",
      "loss: 0.598600  [137600/175341]\n",
      "loss: 0.312952  [139200/175341]\n",
      "loss: 0.350663  [140800/175341]\n",
      "loss: 0.725167  [142400/175341]\n",
      "loss: 0.400342  [144000/175341]\n",
      "loss: 0.463453  [145600/175341]\n",
      "loss: 0.852517  [147200/175341]\n",
      "loss: 0.393774  [148800/175341]\n",
      "loss: 0.463241  [150400/175341]\n",
      "loss: 0.442641  [152000/175341]\n",
      "loss: 0.520533  [153600/175341]\n",
      "loss: 0.470450  [155200/175341]\n",
      "loss: 0.562835  [156800/175341]\n",
      "loss: 0.898220  [158400/175341]\n",
      "loss: 0.205730  [160000/175341]\n",
      "loss: 0.760410  [161600/175341]\n",
      "loss: 0.769416  [163200/175341]\n",
      "loss: 0.207129  [164800/175341]\n",
      "loss: 0.432191  [166400/175341]\n",
      "loss: 0.447800  [168000/175341]\n",
      "loss: 0.244272  [169600/175341]\n",
      "loss: 0.529469  [171200/175341]\n",
      "loss: 0.387823  [172800/175341]\n",
      "loss: 0.337240  [174400/175341]\n",
      "Train Accuracy: 82.1633%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.539130, F1-score: 76.97%, Macro_F1-Score:  43.20%  \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.671622  [    0/175341]\n",
      "loss: 0.262371  [ 1600/175341]\n",
      "loss: 0.365831  [ 3200/175341]\n",
      "loss: 0.683710  [ 4800/175341]\n",
      "loss: 0.331109  [ 6400/175341]\n",
      "loss: 0.349622  [ 8000/175341]\n",
      "loss: 0.934829  [ 9600/175341]\n",
      "loss: 0.821965  [11200/175341]\n",
      "loss: 0.293770  [12800/175341]\n",
      "loss: 0.775172  [14400/175341]\n",
      "loss: 0.169645  [16000/175341]\n",
      "loss: 0.058771  [17600/175341]\n",
      "loss: 1.256490  [19200/175341]\n",
      "loss: 0.338236  [20800/175341]\n",
      "loss: 0.294987  [22400/175341]\n",
      "loss: 0.264345  [24000/175341]\n",
      "loss: 0.150934  [25600/175341]\n",
      "loss: 0.492210  [27200/175341]\n",
      "loss: 0.286498  [28800/175341]\n",
      "loss: 0.254626  [30400/175341]\n",
      "loss: 1.001916  [32000/175341]\n",
      "loss: 0.869215  [33600/175341]\n",
      "loss: 0.652146  [35200/175341]\n",
      "loss: 0.509178  [36800/175341]\n",
      "loss: 0.348185  [38400/175341]\n",
      "loss: 0.591092  [40000/175341]\n",
      "loss: 0.318928  [41600/175341]\n",
      "loss: 0.718242  [43200/175341]\n",
      "loss: 0.170414  [44800/175341]\n",
      "loss: 0.357866  [46400/175341]\n",
      "loss: 0.620177  [48000/175341]\n",
      "loss: 0.335376  [49600/175341]\n",
      "loss: 0.549631  [51200/175341]\n",
      "loss: 0.382265  [52800/175341]\n",
      "loss: 0.858866  [54400/175341]\n",
      "loss: 0.282612  [56000/175341]\n",
      "loss: 0.311097  [57600/175341]\n",
      "loss: 0.320428  [59200/175341]\n",
      "loss: 0.155427  [60800/175341]\n",
      "loss: 0.488433  [62400/175341]\n",
      "loss: 0.250680  [64000/175341]\n",
      "loss: 0.323036  [65600/175341]\n",
      "loss: 0.768673  [67200/175341]\n",
      "loss: 0.519291  [68800/175341]\n",
      "loss: 0.376952  [70400/175341]\n",
      "loss: 0.296181  [72000/175341]\n",
      "loss: 0.767491  [73600/175341]\n",
      "loss: 0.487862  [75200/175341]\n",
      "loss: 0.355511  [76800/175341]\n",
      "loss: 0.606551  [78400/175341]\n",
      "loss: 0.427275  [80000/175341]\n",
      "loss: 0.275301  [81600/175341]\n",
      "loss: 0.128619  [83200/175341]\n",
      "loss: 0.687153  [84800/175341]\n",
      "loss: 0.442522  [86400/175341]\n",
      "loss: 0.378500  [88000/175341]\n",
      "loss: 0.437765  [89600/175341]\n",
      "loss: 0.284831  [91200/175341]\n",
      "loss: 0.718408  [92800/175341]\n",
      "loss: 0.227059  [94400/175341]\n",
      "loss: 0.536323  [96000/175341]\n",
      "loss: 0.408018  [97600/175341]\n",
      "loss: 0.495835  [99200/175341]\n",
      "loss: 0.313416  [100800/175341]\n",
      "loss: 0.621103  [102400/175341]\n",
      "loss: 0.130218  [104000/175341]\n",
      "loss: 0.253854  [105600/175341]\n",
      "loss: 0.427927  [107200/175341]\n",
      "loss: 0.317743  [108800/175341]\n",
      "loss: 0.282779  [110400/175341]\n",
      "loss: 0.306321  [112000/175341]\n",
      "loss: 0.421440  [113600/175341]\n",
      "loss: 0.648510  [115200/175341]\n",
      "loss: 0.207987  [116800/175341]\n",
      "loss: 0.322609  [118400/175341]\n",
      "loss: 0.375754  [120000/175341]\n",
      "loss: 0.466704  [121600/175341]\n",
      "loss: 0.522046  [123200/175341]\n",
      "loss: 0.326174  [124800/175341]\n",
      "loss: 0.463716  [126400/175341]\n",
      "loss: 0.421909  [128000/175341]\n",
      "loss: 0.341365  [129600/175341]\n",
      "loss: 0.534291  [131200/175341]\n",
      "loss: 0.985229  [132800/175341]\n",
      "loss: 0.409396  [134400/175341]\n",
      "loss: 0.539565  [136000/175341]\n",
      "loss: 0.461837  [137600/175341]\n",
      "loss: 0.325562  [139200/175341]\n",
      "loss: 0.770274  [140800/175341]\n",
      "loss: 0.440661  [142400/175341]\n",
      "loss: 0.497015  [144000/175341]\n",
      "loss: 0.682911  [145600/175341]\n",
      "loss: 0.131307  [147200/175341]\n",
      "loss: 0.334940  [148800/175341]\n",
      "loss: 0.270574  [150400/175341]\n",
      "loss: 0.218921  [152000/175341]\n",
      "loss: 0.267423  [153600/175341]\n",
      "loss: 0.259769  [155200/175341]\n",
      "loss: 0.349207  [156800/175341]\n",
      "loss: 0.444226  [158400/175341]\n",
      "loss: 0.522517  [160000/175341]\n",
      "loss: 0.424244  [161600/175341]\n",
      "loss: 0.375191  [163200/175341]\n",
      "loss: 0.346632  [164800/175341]\n",
      "loss: 0.746617  [166400/175341]\n",
      "loss: 0.741170  [168000/175341]\n",
      "loss: 0.583544  [169600/175341]\n",
      "loss: 0.805384  [171200/175341]\n",
      "loss: 0.435798  [172800/175341]\n",
      "loss: 0.820365  [174400/175341]\n",
      "Train Accuracy: 82.2420%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.539089, F1-score: 77.35%, Macro_F1-Score:  44.07%  \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.204734  [    0/175341]\n",
      "loss: 0.459881  [ 1600/175341]\n",
      "loss: 0.305229  [ 3200/175341]\n",
      "loss: 0.547838  [ 4800/175341]\n",
      "loss: 0.381754  [ 6400/175341]\n",
      "loss: 0.332553  [ 8000/175341]\n",
      "loss: 0.223567  [ 9600/175341]\n",
      "loss: 0.605113  [11200/175341]\n",
      "loss: 0.294144  [12800/175341]\n",
      "loss: 0.285185  [14400/175341]\n",
      "loss: 0.235220  [16000/175341]\n",
      "loss: 0.322202  [17600/175341]\n",
      "loss: 0.719741  [19200/175341]\n",
      "loss: 0.355345  [20800/175341]\n",
      "loss: 0.300897  [22400/175341]\n",
      "loss: 0.347766  [24000/175341]\n",
      "loss: 0.239703  [25600/175341]\n",
      "loss: 0.344465  [27200/175341]\n",
      "loss: 0.348383  [28800/175341]\n",
      "loss: 0.341805  [30400/175341]\n",
      "loss: 0.397336  [32000/175341]\n",
      "loss: 0.402866  [33600/175341]\n",
      "loss: 0.395932  [35200/175341]\n",
      "loss: 0.272314  [36800/175341]\n",
      "loss: 0.509719  [38400/175341]\n",
      "loss: 0.167043  [40000/175341]\n",
      "loss: 0.627446  [41600/175341]\n",
      "loss: 0.325787  [43200/175341]\n",
      "loss: 0.657557  [44800/175341]\n",
      "loss: 0.246841  [46400/175341]\n",
      "loss: 0.220617  [48000/175341]\n",
      "loss: 0.355423  [49600/175341]\n",
      "loss: 0.715564  [51200/175341]\n",
      "loss: 0.190947  [52800/175341]\n",
      "loss: 0.504068  [54400/175341]\n",
      "loss: 0.473566  [56000/175341]\n",
      "loss: 0.306143  [57600/175341]\n",
      "loss: 0.111354  [59200/175341]\n",
      "loss: 0.719529  [60800/175341]\n",
      "loss: 0.513237  [62400/175341]\n",
      "loss: 0.444235  [64000/175341]\n",
      "loss: 0.277962  [65600/175341]\n",
      "loss: 0.404822  [67200/175341]\n",
      "loss: 0.265723  [68800/175341]\n",
      "loss: 0.440770  [70400/175341]\n",
      "loss: 0.440075  [72000/175341]\n",
      "loss: 0.467398  [73600/175341]\n",
      "loss: 0.220046  [75200/175341]\n",
      "loss: 0.609951  [76800/175341]\n",
      "loss: 0.318238  [78400/175341]\n",
      "loss: 0.683800  [80000/175341]\n",
      "loss: 0.421352  [81600/175341]\n",
      "loss: 0.437550  [83200/175341]\n",
      "loss: 0.285682  [84800/175341]\n",
      "loss: 0.184261  [86400/175341]\n",
      "loss: 0.520411  [88000/175341]\n",
      "loss: 0.692530  [89600/175341]\n",
      "loss: 0.705972  [91200/175341]\n",
      "loss: 0.355252  [92800/175341]\n",
      "loss: 0.444843  [94400/175341]\n",
      "loss: 0.531993  [96000/175341]\n",
      "loss: 0.143716  [97600/175341]\n",
      "loss: 0.670869  [99200/175341]\n",
      "loss: 0.492567  [100800/175341]\n",
      "loss: 0.775432  [102400/175341]\n",
      "loss: 0.658871  [104000/175341]\n",
      "loss: 0.195144  [105600/175341]\n",
      "loss: 0.202144  [107200/175341]\n",
      "loss: 0.739840  [108800/175341]\n",
      "loss: 0.504149  [110400/175341]\n",
      "loss: 0.669774  [112000/175341]\n",
      "loss: 0.492894  [113600/175341]\n",
      "loss: 0.775322  [115200/175341]\n",
      "loss: 0.843714  [116800/175341]\n",
      "loss: 0.625192  [118400/175341]\n",
      "loss: 0.458948  [120000/175341]\n",
      "loss: 0.460509  [121600/175341]\n",
      "loss: 0.353723  [123200/175341]\n",
      "loss: 0.227215  [124800/175341]\n",
      "loss: 0.498424  [126400/175341]\n",
      "loss: 0.501469  [128000/175341]\n",
      "loss: 0.458866  [129600/175341]\n",
      "loss: 0.475999  [131200/175341]\n",
      "loss: 0.150539  [132800/175341]\n",
      "loss: 0.237885  [134400/175341]\n",
      "loss: 0.670604  [136000/175341]\n",
      "loss: 0.227594  [137600/175341]\n",
      "loss: 0.436622  [139200/175341]\n",
      "loss: 0.460602  [140800/175341]\n",
      "loss: 0.210132  [142400/175341]\n",
      "loss: 0.512169  [144000/175341]\n",
      "loss: 0.277041  [145600/175341]\n",
      "loss: 0.318638  [147200/175341]\n",
      "loss: 0.509668  [148800/175341]\n",
      "loss: 0.230408  [150400/175341]\n",
      "loss: 0.200897  [152000/175341]\n",
      "loss: 0.602423  [153600/175341]\n",
      "loss: 0.424681  [155200/175341]\n",
      "loss: 0.625780  [156800/175341]\n",
      "loss: 0.394644  [158400/175341]\n",
      "loss: 0.391827  [160000/175341]\n",
      "loss: 0.415084  [161600/175341]\n",
      "loss: 0.298133  [163200/175341]\n",
      "loss: 0.376998  [164800/175341]\n",
      "loss: 0.508854  [166400/175341]\n",
      "loss: 0.215336  [168000/175341]\n",
      "loss: 0.335794  [169600/175341]\n",
      "loss: 0.342168  [171200/175341]\n",
      "loss: 0.333243  [172800/175341]\n",
      "loss: 0.628407  [174400/175341]\n",
      "Train Accuracy: 82.2004%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.553464, F1-score: 76.95%, Macro_F1-Score:  43.86%  \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.637701  [    0/175341]\n",
      "loss: 0.354374  [ 1600/175341]\n",
      "loss: 0.262598  [ 3200/175341]\n",
      "loss: 0.557251  [ 4800/175341]\n",
      "loss: 0.464547  [ 6400/175341]\n",
      "loss: 0.437771  [ 8000/175341]\n",
      "loss: 0.547168  [ 9600/175341]\n",
      "loss: 0.287509  [11200/175341]\n",
      "loss: 0.312802  [12800/175341]\n",
      "loss: 0.222494  [14400/175341]\n",
      "loss: 0.701665  [16000/175341]\n",
      "loss: 0.484803  [17600/175341]\n",
      "loss: 0.232086  [19200/175341]\n",
      "loss: 0.220194  [20800/175341]\n",
      "loss: 0.143041  [22400/175341]\n",
      "loss: 0.698890  [24000/175341]\n",
      "loss: 0.531987  [25600/175341]\n",
      "loss: 0.606376  [27200/175341]\n",
      "loss: 0.304705  [28800/175341]\n",
      "loss: 0.498016  [30400/175341]\n",
      "loss: 0.548499  [32000/175341]\n",
      "loss: 0.408279  [33600/175341]\n",
      "loss: 0.295840  [35200/175341]\n",
      "loss: 0.426648  [36800/175341]\n",
      "loss: 0.308032  [38400/175341]\n",
      "loss: 0.301225  [40000/175341]\n",
      "loss: 0.727365  [41600/175341]\n",
      "loss: 0.251175  [43200/175341]\n",
      "loss: 0.252519  [44800/175341]\n",
      "loss: 0.466215  [46400/175341]\n",
      "loss: 0.505774  [48000/175341]\n",
      "loss: 0.165027  [49600/175341]\n",
      "loss: 0.388104  [51200/175341]\n",
      "loss: 0.315634  [52800/175341]\n",
      "loss: 0.470825  [54400/175341]\n",
      "loss: 0.529579  [56000/175341]\n",
      "loss: 0.554014  [57600/175341]\n",
      "loss: 0.746366  [59200/175341]\n",
      "loss: 0.829403  [60800/175341]\n",
      "loss: 0.550442  [62400/175341]\n",
      "loss: 0.306745  [64000/175341]\n",
      "loss: 0.673907  [65600/175341]\n",
      "loss: 0.600379  [67200/175341]\n",
      "loss: 0.351329  [68800/175341]\n",
      "loss: 0.687384  [70400/175341]\n",
      "loss: 0.429064  [72000/175341]\n",
      "loss: 0.739635  [73600/175341]\n",
      "loss: 0.460392  [75200/175341]\n",
      "loss: 0.199221  [76800/175341]\n",
      "loss: 0.519954  [78400/175341]\n",
      "loss: 0.472182  [80000/175341]\n",
      "loss: 0.272769  [81600/175341]\n",
      "loss: 0.645205  [83200/175341]\n",
      "loss: 0.236150  [84800/175341]\n",
      "loss: 0.661810  [86400/175341]\n",
      "loss: 0.542101  [88000/175341]\n",
      "loss: 0.554072  [89600/175341]\n",
      "loss: 0.391345  [91200/175341]\n",
      "loss: 0.309716  [92800/175341]\n",
      "loss: 0.836780  [94400/175341]\n",
      "loss: 0.520558  [96000/175341]\n",
      "loss: 0.922970  [97600/175341]\n",
      "loss: 0.596351  [99200/175341]\n",
      "loss: 0.449115  [100800/175341]\n",
      "loss: 0.493721  [102400/175341]\n",
      "loss: 0.410298  [104000/175341]\n",
      "loss: 0.520693  [105600/175341]\n",
      "loss: 0.592206  [107200/175341]\n",
      "loss: 0.569012  [108800/175341]\n",
      "loss: 0.550026  [110400/175341]\n",
      "loss: 0.437493  [112000/175341]\n",
      "loss: 0.713106  [113600/175341]\n",
      "loss: 0.133910  [115200/175341]\n",
      "loss: 0.567421  [116800/175341]\n",
      "loss: 0.140530  [118400/175341]\n",
      "loss: 0.448266  [120000/175341]\n",
      "loss: 0.426888  [121600/175341]\n",
      "loss: 0.410940  [123200/175341]\n",
      "loss: 0.553194  [124800/175341]\n",
      "loss: 0.598403  [126400/175341]\n",
      "loss: 0.714085  [128000/175341]\n",
      "loss: 0.758695  [129600/175341]\n",
      "loss: 0.581584  [131200/175341]\n",
      "loss: 0.223613  [132800/175341]\n",
      "loss: 0.937788  [134400/175341]\n",
      "loss: 0.444171  [136000/175341]\n",
      "loss: 0.306849  [137600/175341]\n",
      "loss: 0.300309  [139200/175341]\n",
      "loss: 0.774373  [140800/175341]\n",
      "loss: 0.286415  [142400/175341]\n",
      "loss: 0.368834  [144000/175341]\n",
      "loss: 0.279218  [145600/175341]\n",
      "loss: 0.492715  [147200/175341]\n",
      "loss: 0.557501  [148800/175341]\n",
      "loss: 0.256779  [150400/175341]\n",
      "loss: 0.365589  [152000/175341]\n",
      "loss: 0.675588  [153600/175341]\n",
      "loss: 0.913489  [155200/175341]\n",
      "loss: 0.388328  [156800/175341]\n",
      "loss: 0.305928  [158400/175341]\n",
      "loss: 0.761214  [160000/175341]\n",
      "loss: 0.362922  [161600/175341]\n",
      "loss: 0.280797  [163200/175341]\n",
      "loss: 0.454824  [164800/175341]\n",
      "loss: 0.399851  [166400/175341]\n",
      "loss: 0.241460  [168000/175341]\n",
      "loss: 0.510144  [169600/175341]\n",
      "loss: 0.165270  [171200/175341]\n",
      "loss: 0.761825  [172800/175341]\n",
      "loss: 0.161100  [174400/175341]\n",
      "Train Accuracy: 82.2363%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.554053, F1-score: 76.88%, Macro_F1-Score:  43.48%  \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.615767  [    0/175341]\n",
      "loss: 0.388571  [ 1600/175341]\n",
      "loss: 0.287526  [ 3200/175341]\n",
      "loss: 0.098316  [ 4800/175341]\n",
      "loss: 0.672301  [ 6400/175341]\n",
      "loss: 0.558171  [ 8000/175341]\n",
      "loss: 0.298175  [ 9600/175341]\n",
      "loss: 0.334038  [11200/175341]\n",
      "loss: 0.510584  [12800/175341]\n",
      "loss: 0.454230  [14400/175341]\n",
      "loss: 0.851089  [16000/175341]\n",
      "loss: 0.495930  [17600/175341]\n",
      "loss: 0.410618  [19200/175341]\n",
      "loss: 0.587903  [20800/175341]\n",
      "loss: 0.194957  [22400/175341]\n",
      "loss: 0.128846  [24000/175341]\n",
      "loss: 0.513636  [25600/175341]\n",
      "loss: 0.234704  [27200/175341]\n",
      "loss: 0.424954  [28800/175341]\n",
      "loss: 0.490609  [30400/175341]\n",
      "loss: 0.339901  [32000/175341]\n",
      "loss: 0.252107  [33600/175341]\n",
      "loss: 0.229108  [35200/175341]\n",
      "loss: 0.253176  [36800/175341]\n",
      "loss: 0.576339  [38400/175341]\n",
      "loss: 0.451018  [40000/175341]\n",
      "loss: 0.238283  [41600/175341]\n",
      "loss: 0.272044  [43200/175341]\n",
      "loss: 0.497956  [44800/175341]\n",
      "loss: 0.610999  [46400/175341]\n",
      "loss: 0.447566  [48000/175341]\n",
      "loss: 0.204062  [49600/175341]\n",
      "loss: 0.253701  [51200/175341]\n",
      "loss: 0.349244  [52800/175341]\n",
      "loss: 0.239613  [54400/175341]\n",
      "loss: 0.614822  [56000/175341]\n",
      "loss: 0.459250  [57600/175341]\n",
      "loss: 0.900727  [59200/175341]\n",
      "loss: 0.319271  [60800/175341]\n",
      "loss: 0.418890  [62400/175341]\n",
      "loss: 0.445949  [64000/175341]\n",
      "loss: 0.436000  [65600/175341]\n",
      "loss: 0.211626  [67200/175341]\n",
      "loss: 0.529653  [68800/175341]\n",
      "loss: 0.227247  [70400/175341]\n",
      "loss: 0.348468  [72000/175341]\n",
      "loss: 0.417806  [73600/175341]\n",
      "loss: 0.708832  [75200/175341]\n",
      "loss: 0.156564  [76800/175341]\n",
      "loss: 0.259881  [78400/175341]\n",
      "loss: 0.211922  [80000/175341]\n",
      "loss: 1.112342  [81600/175341]\n",
      "loss: 0.422423  [83200/175341]\n",
      "loss: 0.269127  [84800/175341]\n",
      "loss: 0.243285  [86400/175341]\n",
      "loss: 0.431448  [88000/175341]\n",
      "loss: 0.276363  [89600/175341]\n",
      "loss: 0.474473  [91200/175341]\n",
      "loss: 0.434778  [92800/175341]\n",
      "loss: 0.310632  [94400/175341]\n",
      "loss: 0.752961  [96000/175341]\n",
      "loss: 0.327535  [97600/175341]\n",
      "loss: 0.491556  [99200/175341]\n",
      "loss: 0.175182  [100800/175341]\n",
      "loss: 0.145958  [102400/175341]\n",
      "loss: 0.505685  [104000/175341]\n",
      "loss: 0.474063  [105600/175341]\n",
      "loss: 0.379684  [107200/175341]\n",
      "loss: 0.255898  [108800/175341]\n",
      "loss: 0.059552  [110400/175341]\n",
      "loss: 0.582238  [112000/175341]\n",
      "loss: 0.442802  [113600/175341]\n",
      "loss: 0.908403  [115200/175341]\n",
      "loss: 0.264198  [116800/175341]\n",
      "loss: 0.262556  [118400/175341]\n",
      "loss: 0.088775  [120000/175341]\n",
      "loss: 0.448593  [121600/175341]\n",
      "loss: 0.369826  [123200/175341]\n",
      "loss: 0.119315  [124800/175341]\n",
      "loss: 0.267025  [126400/175341]\n",
      "loss: 0.136099  [128000/175341]\n",
      "loss: 0.458884  [129600/175341]\n",
      "loss: 0.437126  [131200/175341]\n",
      "loss: 0.618674  [132800/175341]\n",
      "loss: 0.775584  [134400/175341]\n",
      "loss: 0.348601  [136000/175341]\n",
      "loss: 0.434310  [137600/175341]\n",
      "loss: 0.149999  [139200/175341]\n",
      "loss: 0.241246  [140800/175341]\n",
      "loss: 0.187702  [142400/175341]\n",
      "loss: 0.367447  [144000/175341]\n",
      "loss: 0.377684  [145600/175341]\n",
      "loss: 0.184583  [147200/175341]\n",
      "loss: 0.397688  [148800/175341]\n",
      "loss: 0.328868  [150400/175341]\n",
      "loss: 0.290571  [152000/175341]\n",
      "loss: 0.331017  [153600/175341]\n",
      "loss: 0.415284  [155200/175341]\n",
      "loss: 0.461303  [156800/175341]\n",
      "loss: 0.405927  [158400/175341]\n",
      "loss: 0.598853  [160000/175341]\n",
      "loss: 0.434299  [161600/175341]\n",
      "loss: 0.395962  [163200/175341]\n",
      "loss: 0.476961  [164800/175341]\n",
      "loss: 0.632235  [166400/175341]\n",
      "loss: 0.162636  [168000/175341]\n",
      "loss: 0.608777  [169600/175341]\n",
      "loss: 0.326449  [171200/175341]\n",
      "loss: 0.318757  [172800/175341]\n",
      "loss: 0.469357  [174400/175341]\n",
      "Train Accuracy: 82.1856%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.546766, F1-score: 77.55%, Macro_F1-Score:  44.04%  \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.238897  [    0/175341]\n",
      "loss: 0.332146  [ 1600/175341]\n",
      "loss: 0.656902  [ 3200/175341]\n",
      "loss: 0.343188  [ 4800/175341]\n",
      "loss: 0.357565  [ 6400/175341]\n",
      "loss: 0.369543  [ 8000/175341]\n",
      "loss: 0.565020  [ 9600/175341]\n",
      "loss: 0.922262  [11200/175341]\n",
      "loss: 0.219657  [12800/175341]\n",
      "loss: 0.330093  [14400/175341]\n",
      "loss: 0.424469  [16000/175341]\n",
      "loss: 0.473529  [17600/175341]\n",
      "loss: 0.236594  [19200/175341]\n",
      "loss: 0.292373  [20800/175341]\n",
      "loss: 0.095559  [22400/175341]\n",
      "loss: 0.732837  [24000/175341]\n",
      "loss: 0.354106  [25600/175341]\n",
      "loss: 0.174792  [27200/175341]\n",
      "loss: 0.282835  [28800/175341]\n",
      "loss: 0.684810  [30400/175341]\n",
      "loss: 0.250955  [32000/175341]\n",
      "loss: 0.390725  [33600/175341]\n",
      "loss: 0.727650  [35200/175341]\n",
      "loss: 0.336075  [36800/175341]\n",
      "loss: 0.667058  [38400/175341]\n",
      "loss: 0.534756  [40000/175341]\n",
      "loss: 0.470594  [41600/175341]\n",
      "loss: 0.201050  [43200/175341]\n",
      "loss: 0.586943  [44800/175341]\n",
      "loss: 0.920703  [46400/175341]\n",
      "loss: 0.668600  [48000/175341]\n",
      "loss: 0.205718  [49600/175341]\n",
      "loss: 0.234918  [51200/175341]\n",
      "loss: 0.534734  [52800/175341]\n",
      "loss: 0.586815  [54400/175341]\n",
      "loss: 0.506189  [56000/175341]\n",
      "loss: 0.377500  [57600/175341]\n",
      "loss: 0.714468  [59200/175341]\n",
      "loss: 0.311156  [60800/175341]\n",
      "loss: 0.366333  [62400/175341]\n",
      "loss: 1.065415  [64000/175341]\n",
      "loss: 0.530310  [65600/175341]\n",
      "loss: 0.309046  [67200/175341]\n",
      "loss: 0.351783  [68800/175341]\n",
      "loss: 0.677452  [70400/175341]\n",
      "loss: 0.275348  [72000/175341]\n",
      "loss: 0.586253  [73600/175341]\n",
      "loss: 0.397938  [75200/175341]\n",
      "loss: 0.390352  [76800/175341]\n",
      "loss: 0.283960  [78400/175341]\n",
      "loss: 0.494289  [80000/175341]\n",
      "loss: 0.513584  [81600/175341]\n",
      "loss: 0.354215  [83200/175341]\n",
      "loss: 0.974959  [84800/175341]\n",
      "loss: 0.337354  [86400/175341]\n",
      "loss: 0.324868  [88000/175341]\n",
      "loss: 0.237478  [89600/175341]\n",
      "loss: 0.840033  [91200/175341]\n",
      "loss: 0.234291  [92800/175341]\n",
      "loss: 0.608369  [94400/175341]\n",
      "loss: 0.103305  [96000/175341]\n",
      "loss: 0.244617  [97600/175341]\n",
      "loss: 0.475578  [99200/175341]\n",
      "loss: 0.245286  [100800/175341]\n",
      "loss: 0.502092  [102400/175341]\n",
      "loss: 0.223119  [104000/175341]\n",
      "loss: 0.430508  [105600/175341]\n",
      "loss: 0.179579  [107200/175341]\n",
      "loss: 0.390511  [108800/175341]\n",
      "loss: 0.424460  [110400/175341]\n",
      "loss: 0.558068  [112000/175341]\n",
      "loss: 0.416101  [113600/175341]\n",
      "loss: 0.445985  [115200/175341]\n",
      "loss: 0.296907  [116800/175341]\n",
      "loss: 0.320358  [118400/175341]\n",
      "loss: 0.664384  [120000/175341]\n",
      "loss: 0.056939  [121600/175341]\n",
      "loss: 0.339223  [123200/175341]\n",
      "loss: 0.315273  [124800/175341]\n",
      "loss: 0.592683  [126400/175341]\n",
      "loss: 0.459412  [128000/175341]\n",
      "loss: 0.282162  [129600/175341]\n",
      "loss: 0.165671  [131200/175341]\n",
      "loss: 0.549051  [132800/175341]\n",
      "loss: 0.184204  [134400/175341]\n",
      "loss: 0.409872  [136000/175341]\n",
      "loss: 0.755097  [137600/175341]\n",
      "loss: 0.445616  [139200/175341]\n",
      "loss: 0.359550  [140800/175341]\n",
      "loss: 0.430401  [142400/175341]\n",
      "loss: 0.387313  [144000/175341]\n",
      "loss: 0.456031  [145600/175341]\n",
      "loss: 0.371374  [147200/175341]\n",
      "loss: 0.348273  [148800/175341]\n",
      "loss: 0.165474  [150400/175341]\n",
      "loss: 0.370416  [152000/175341]\n",
      "loss: 0.899022  [153600/175341]\n",
      "loss: 0.579732  [155200/175341]\n",
      "loss: 0.892013  [156800/175341]\n",
      "loss: 0.173005  [158400/175341]\n",
      "loss: 0.571557  [160000/175341]\n",
      "loss: 0.400790  [161600/175341]\n",
      "loss: 0.251228  [163200/175341]\n",
      "loss: 0.518892  [164800/175341]\n",
      "loss: 0.668231  [166400/175341]\n",
      "loss: 0.524153  [168000/175341]\n",
      "loss: 0.450912  [169600/175341]\n",
      "loss: 0.446889  [171200/175341]\n",
      "loss: 0.720894  [172800/175341]\n",
      "loss: 0.195839  [174400/175341]\n",
      "Train Accuracy: 82.2494%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.568891, F1-score: 75.95%, Macro_F1-Score:  43.49%  \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.432158  [    0/175341]\n",
      "loss: 0.370354  [ 1600/175341]\n",
      "loss: 0.354562  [ 3200/175341]\n",
      "loss: 0.886859  [ 4800/175341]\n",
      "loss: 0.145950  [ 6400/175341]\n",
      "loss: 0.906898  [ 8000/175341]\n",
      "loss: 0.278154  [ 9600/175341]\n",
      "loss: 0.304380  [11200/175341]\n",
      "loss: 0.395349  [12800/175341]\n",
      "loss: 0.741064  [14400/175341]\n",
      "loss: 0.190003  [16000/175341]\n",
      "loss: 0.157593  [17600/175341]\n",
      "loss: 0.567669  [19200/175341]\n",
      "loss: 0.246178  [20800/175341]\n",
      "loss: 0.530225  [22400/175341]\n",
      "loss: 0.465197  [24000/175341]\n",
      "loss: 0.422861  [25600/175341]\n",
      "loss: 0.242414  [27200/175341]\n",
      "loss: 0.507028  [28800/175341]\n",
      "loss: 0.308626  [30400/175341]\n",
      "loss: 0.505482  [32000/175341]\n",
      "loss: 0.367451  [33600/175341]\n",
      "loss: 0.246374  [35200/175341]\n",
      "loss: 0.310655  [36800/175341]\n",
      "loss: 0.622874  [38400/175341]\n",
      "loss: 0.493903  [40000/175341]\n",
      "loss: 0.300058  [41600/175341]\n",
      "loss: 0.858701  [43200/175341]\n",
      "loss: 0.256265  [44800/175341]\n",
      "loss: 0.301599  [46400/175341]\n",
      "loss: 0.259069  [48000/175341]\n",
      "loss: 0.352782  [49600/175341]\n",
      "loss: 0.583841  [51200/175341]\n",
      "loss: 0.353539  [52800/175341]\n",
      "loss: 0.226860  [54400/175341]\n",
      "loss: 0.335815  [56000/175341]\n",
      "loss: 0.168785  [57600/175341]\n",
      "loss: 0.574474  [59200/175341]\n",
      "loss: 0.427493  [60800/175341]\n",
      "loss: 0.670967  [62400/175341]\n",
      "loss: 0.575368  [64000/175341]\n",
      "loss: 0.821039  [65600/175341]\n",
      "loss: 0.504708  [67200/175341]\n",
      "loss: 0.529515  [68800/175341]\n",
      "loss: 0.362906  [70400/175341]\n",
      "loss: 0.303778  [72000/175341]\n",
      "loss: 0.608427  [73600/175341]\n",
      "loss: 0.428126  [75200/175341]\n",
      "loss: 0.341979  [76800/175341]\n",
      "loss: 0.601607  [78400/175341]\n",
      "loss: 0.347070  [80000/175341]\n",
      "loss: 0.579455  [81600/175341]\n",
      "loss: 0.344861  [83200/175341]\n",
      "loss: 0.590515  [84800/175341]\n",
      "loss: 0.589355  [86400/175341]\n",
      "loss: 0.354086  [88000/175341]\n",
      "loss: 0.856818  [89600/175341]\n",
      "loss: 0.234733  [91200/175341]\n",
      "loss: 0.232503  [92800/175341]\n",
      "loss: 0.621662  [94400/175341]\n",
      "loss: 0.155778  [96000/175341]\n",
      "loss: 0.375063  [97600/175341]\n",
      "loss: 0.328967  [99200/175341]\n",
      "loss: 0.379493  [100800/175341]\n",
      "loss: 0.769211  [102400/175341]\n",
      "loss: 0.613441  [104000/175341]\n",
      "loss: 0.233266  [105600/175341]\n",
      "loss: 0.440066  [107200/175341]\n",
      "loss: 0.649507  [108800/175341]\n",
      "loss: 0.208283  [110400/175341]\n",
      "loss: 0.564243  [112000/175341]\n",
      "loss: 0.294978  [113600/175341]\n",
      "loss: 0.294947  [115200/175341]\n",
      "loss: 0.393064  [116800/175341]\n",
      "loss: 0.297684  [118400/175341]\n",
      "loss: 0.349984  [120000/175341]\n",
      "loss: 0.523558  [121600/175341]\n",
      "loss: 0.657062  [123200/175341]\n",
      "loss: 0.326751  [124800/175341]\n",
      "loss: 1.077742  [126400/175341]\n",
      "loss: 0.417307  [128000/175341]\n",
      "loss: 0.116738  [129600/175341]\n",
      "loss: 0.322228  [131200/175341]\n",
      "loss: 0.392448  [132800/175341]\n",
      "loss: 0.946652  [134400/175341]\n",
      "loss: 0.707063  [136000/175341]\n",
      "loss: 0.191818  [137600/175341]\n",
      "loss: 0.128912  [139200/175341]\n",
      "loss: 0.525505  [140800/175341]\n",
      "loss: 0.550724  [142400/175341]\n",
      "loss: 0.402191  [144000/175341]\n",
      "loss: 0.725566  [145600/175341]\n",
      "loss: 0.327073  [147200/175341]\n",
      "loss: 0.261604  [148800/175341]\n",
      "loss: 0.453439  [150400/175341]\n",
      "loss: 0.545948  [152000/175341]\n",
      "loss: 0.567410  [153600/175341]\n",
      "loss: 0.466639  [155200/175341]\n",
      "loss: 0.502559  [156800/175341]\n",
      "loss: 0.724191  [158400/175341]\n",
      "loss: 0.387164  [160000/175341]\n",
      "loss: 0.751415  [161600/175341]\n",
      "loss: 0.341599  [163200/175341]\n",
      "loss: 0.221236  [164800/175341]\n",
      "loss: 0.448637  [166400/175341]\n",
      "loss: 0.597964  [168000/175341]\n",
      "loss: 0.218402  [169600/175341]\n",
      "loss: 0.492726  [171200/175341]\n",
      "loss: 0.848128  [172800/175341]\n",
      "loss: 0.455160  [174400/175341]\n",
      "Train Accuracy: 82.2340%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.547832, F1-score: 76.51%, Macro_F1-Score:  43.15%  \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.564935  [    0/175341]\n",
      "loss: 0.160732  [ 1600/175341]\n",
      "loss: 0.401261  [ 3200/175341]\n",
      "loss: 0.697778  [ 4800/175341]\n",
      "loss: 0.405756  [ 6400/175341]\n",
      "loss: 0.475500  [ 8000/175341]\n",
      "loss: 0.590320  [ 9600/175341]\n",
      "loss: 0.609061  [11200/175341]\n",
      "loss: 0.716398  [12800/175341]\n",
      "loss: 0.407256  [14400/175341]\n",
      "loss: 0.722538  [16000/175341]\n",
      "loss: 0.667978  [17600/175341]\n",
      "loss: 0.158106  [19200/175341]\n",
      "loss: 0.322850  [20800/175341]\n",
      "loss: 0.363683  [22400/175341]\n",
      "loss: 0.589027  [24000/175341]\n",
      "loss: 0.447123  [25600/175341]\n",
      "loss: 0.441299  [27200/175341]\n",
      "loss: 0.349463  [28800/175341]\n",
      "loss: 0.474896  [30400/175341]\n",
      "loss: 0.201259  [32000/175341]\n",
      "loss: 0.450863  [33600/175341]\n",
      "loss: 0.165952  [35200/175341]\n",
      "loss: 0.619202  [36800/175341]\n",
      "loss: 0.213414  [38400/175341]\n",
      "loss: 0.475774  [40000/175341]\n",
      "loss: 0.266696  [41600/175341]\n",
      "loss: 0.231014  [43200/175341]\n",
      "loss: 0.631909  [44800/175341]\n",
      "loss: 0.432490  [46400/175341]\n",
      "loss: 0.313067  [48000/175341]\n",
      "loss: 0.499036  [49600/175341]\n",
      "loss: 0.307959  [51200/175341]\n",
      "loss: 0.600755  [52800/175341]\n",
      "loss: 0.515836  [54400/175341]\n",
      "loss: 0.651198  [56000/175341]\n",
      "loss: 0.371951  [57600/175341]\n",
      "loss: 0.350911  [59200/175341]\n",
      "loss: 0.347352  [60800/175341]\n",
      "loss: 0.378942  [62400/175341]\n",
      "loss: 0.482620  [64000/175341]\n",
      "loss: 0.402561  [65600/175341]\n",
      "loss: 0.634240  [67200/175341]\n",
      "loss: 0.349175  [68800/175341]\n",
      "loss: 0.327708  [70400/175341]\n",
      "loss: 0.298010  [72000/175341]\n",
      "loss: 0.307034  [73600/175341]\n",
      "loss: 0.221662  [75200/175341]\n",
      "loss: 0.369530  [76800/175341]\n",
      "loss: 0.470853  [78400/175341]\n",
      "loss: 0.562014  [80000/175341]\n",
      "loss: 0.479171  [81600/175341]\n",
      "loss: 0.592499  [83200/175341]\n",
      "loss: 0.430668  [84800/175341]\n",
      "loss: 0.283383  [86400/175341]\n",
      "loss: 0.410131  [88000/175341]\n",
      "loss: 1.064476  [89600/175341]\n",
      "loss: 0.762900  [91200/175341]\n",
      "loss: 0.173696  [92800/175341]\n",
      "loss: 0.197973  [94400/175341]\n",
      "loss: 0.059482  [96000/175341]\n",
      "loss: 0.293223  [97600/175341]\n",
      "loss: 0.202605  [99200/175341]\n",
      "loss: 0.398900  [100800/175341]\n",
      "loss: 0.649110  [102400/175341]\n",
      "loss: 0.590843  [104000/175341]\n",
      "loss: 0.140152  [105600/175341]\n",
      "loss: 0.303401  [107200/175341]\n",
      "loss: 0.494559  [108800/175341]\n",
      "loss: 0.576587  [110400/175341]\n",
      "loss: 0.824219  [112000/175341]\n",
      "loss: 0.778038  [113600/175341]\n",
      "loss: 0.681237  [115200/175341]\n",
      "loss: 0.352797  [116800/175341]\n",
      "loss: 0.329526  [118400/175341]\n",
      "loss: 0.573726  [120000/175341]\n",
      "loss: 0.307069  [121600/175341]\n",
      "loss: 0.331100  [123200/175341]\n",
      "loss: 0.209475  [124800/175341]\n",
      "loss: 0.473335  [126400/175341]\n",
      "loss: 0.236296  [128000/175341]\n",
      "loss: 0.138649  [129600/175341]\n",
      "loss: 0.282059  [131200/175341]\n",
      "loss: 0.516109  [132800/175341]\n",
      "loss: 0.491513  [134400/175341]\n",
      "loss: 0.277869  [136000/175341]\n",
      "loss: 0.388295  [137600/175341]\n",
      "loss: 0.264936  [139200/175341]\n",
      "loss: 0.673353  [140800/175341]\n",
      "loss: 0.748411  [142400/175341]\n",
      "loss: 0.650581  [144000/175341]\n",
      "loss: 0.301247  [145600/175341]\n",
      "loss: 0.259950  [147200/175341]\n",
      "loss: 0.160748  [148800/175341]\n",
      "loss: 0.325996  [150400/175341]\n",
      "loss: 0.178663  [152000/175341]\n",
      "loss: 0.868560  [153600/175341]\n",
      "loss: 0.103018  [155200/175341]\n",
      "loss: 0.339359  [156800/175341]\n",
      "loss: 0.232873  [158400/175341]\n",
      "loss: 1.025252  [160000/175341]\n",
      "loss: 0.427736  [161600/175341]\n",
      "loss: 0.295837  [163200/175341]\n",
      "loss: 0.101197  [164800/175341]\n",
      "loss: 0.643298  [166400/175341]\n",
      "loss: 0.570071  [168000/175341]\n",
      "loss: 0.115707  [169600/175341]\n",
      "loss: 0.437590  [171200/175341]\n",
      "loss: 0.984930  [172800/175341]\n",
      "loss: 0.684268  [174400/175341]\n",
      "Train Accuracy: 82.2318%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.540509, F1-score: 77.01%, Macro_F1-Score:  43.29%  \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.217582  [    0/175341]\n",
      "loss: 0.369064  [ 1600/175341]\n",
      "loss: 0.340773  [ 3200/175341]\n",
      "loss: 0.536693  [ 4800/175341]\n",
      "loss: 0.410756  [ 6400/175341]\n",
      "loss: 0.628071  [ 8000/175341]\n",
      "loss: 1.100698  [ 9600/175341]\n",
      "loss: 0.924468  [11200/175341]\n",
      "loss: 0.088001  [12800/175341]\n",
      "loss: 0.484296  [14400/175341]\n",
      "loss: 0.221525  [16000/175341]\n",
      "loss: 0.362026  [17600/175341]\n",
      "loss: 0.697421  [19200/175341]\n",
      "loss: 0.330159  [20800/175341]\n",
      "loss: 0.333040  [22400/175341]\n",
      "loss: 0.171750  [24000/175341]\n",
      "loss: 0.391270  [25600/175341]\n",
      "loss: 0.560089  [27200/175341]\n",
      "loss: 0.247501  [28800/175341]\n",
      "loss: 0.308625  [30400/175341]\n",
      "loss: 0.291197  [32000/175341]\n",
      "loss: 0.356528  [33600/175341]\n",
      "loss: 0.282520  [35200/175341]\n",
      "loss: 0.326644  [36800/175341]\n",
      "loss: 0.331229  [38400/175341]\n",
      "loss: 1.119639  [40000/175341]\n",
      "loss: 0.681400  [41600/175341]\n",
      "loss: 0.452879  [43200/175341]\n",
      "loss: 0.444334  [44800/175341]\n",
      "loss: 0.521794  [46400/175341]\n",
      "loss: 0.447919  [48000/175341]\n",
      "loss: 0.670119  [49600/175341]\n",
      "loss: 0.595126  [51200/175341]\n",
      "loss: 0.325116  [52800/175341]\n",
      "loss: 0.396432  [54400/175341]\n",
      "loss: 0.658277  [56000/175341]\n",
      "loss: 0.395728  [57600/175341]\n",
      "loss: 0.461460  [59200/175341]\n",
      "loss: 0.181230  [60800/175341]\n",
      "loss: 0.502310  [62400/175341]\n",
      "loss: 0.466904  [64000/175341]\n",
      "loss: 0.289586  [65600/175341]\n",
      "loss: 0.314741  [67200/175341]\n",
      "loss: 0.086116  [68800/175341]\n",
      "loss: 0.160727  [70400/175341]\n",
      "loss: 0.294643  [72000/175341]\n",
      "loss: 0.349459  [73600/175341]\n",
      "loss: 0.282190  [75200/175341]\n",
      "loss: 0.376830  [76800/175341]\n",
      "loss: 0.606949  [78400/175341]\n",
      "loss: 0.608930  [80000/175341]\n",
      "loss: 0.436774  [81600/175341]\n",
      "loss: 0.469440  [83200/175341]\n",
      "loss: 0.313007  [84800/175341]\n",
      "loss: 0.643276  [86400/175341]\n",
      "loss: 0.261547  [88000/175341]\n",
      "loss: 0.311705  [89600/175341]\n",
      "loss: 0.120122  [91200/175341]\n",
      "loss: 0.631701  [92800/175341]\n",
      "loss: 0.321412  [94400/175341]\n",
      "loss: 0.490494  [96000/175341]\n",
      "loss: 0.359119  [97600/175341]\n",
      "loss: 0.459829  [99200/175341]\n",
      "loss: 0.395331  [100800/175341]\n",
      "loss: 0.367005  [102400/175341]\n",
      "loss: 0.430398  [104000/175341]\n",
      "loss: 0.204764  [105600/175341]\n",
      "loss: 0.354392  [107200/175341]\n",
      "loss: 0.159258  [108800/175341]\n",
      "loss: 0.165477  [110400/175341]\n",
      "loss: 0.408504  [112000/175341]\n",
      "loss: 0.329676  [113600/175341]\n",
      "loss: 0.711421  [115200/175341]\n",
      "loss: 0.758698  [116800/175341]\n",
      "loss: 0.640379  [118400/175341]\n",
      "loss: 0.587066  [120000/175341]\n",
      "loss: 0.109855  [121600/175341]\n",
      "loss: 0.267242  [123200/175341]\n",
      "loss: 0.205050  [124800/175341]\n",
      "loss: 0.661676  [126400/175341]\n",
      "loss: 0.206588  [128000/175341]\n",
      "loss: 0.465801  [129600/175341]\n",
      "loss: 0.203431  [131200/175341]\n",
      "loss: 0.572465  [132800/175341]\n",
      "loss: 0.342896  [134400/175341]\n",
      "loss: 1.206351  [136000/175341]\n",
      "loss: 0.578072  [137600/175341]\n",
      "loss: 0.418296  [139200/175341]\n",
      "loss: 0.104380  [140800/175341]\n",
      "loss: 0.670905  [142400/175341]\n",
      "loss: 0.559164  [144000/175341]\n",
      "loss: 0.324598  [145600/175341]\n",
      "loss: 0.341397  [147200/175341]\n",
      "loss: 0.196075  [148800/175341]\n",
      "loss: 0.487848  [150400/175341]\n",
      "loss: 0.574865  [152000/175341]\n",
      "loss: 0.470421  [153600/175341]\n",
      "loss: 0.247971  [155200/175341]\n",
      "loss: 0.786647  [156800/175341]\n",
      "loss: 0.895193  [158400/175341]\n",
      "loss: 0.595851  [160000/175341]\n",
      "loss: 0.239563  [161600/175341]\n",
      "loss: 0.474855  [163200/175341]\n",
      "loss: 0.095275  [164800/175341]\n",
      "loss: 0.370769  [166400/175341]\n",
      "loss: 0.960612  [168000/175341]\n",
      "loss: 0.151599  [169600/175341]\n",
      "loss: 0.773975  [171200/175341]\n",
      "loss: 0.516287  [172800/175341]\n",
      "loss: 0.361443  [174400/175341]\n",
      "Train Accuracy: 82.2032%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.546613, F1-score: 76.66%, Macro_F1-Score:  43.03%  \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.160070  [    0/175341]\n",
      "loss: 0.246660  [ 1600/175341]\n",
      "loss: 0.657855  [ 3200/175341]\n",
      "loss: 0.241552  [ 4800/175341]\n",
      "loss: 0.658016  [ 6400/175341]\n",
      "loss: 0.396883  [ 8000/175341]\n",
      "loss: 0.398514  [ 9600/175341]\n",
      "loss: 0.707946  [11200/175341]\n",
      "loss: 0.182838  [12800/175341]\n",
      "loss: 0.237586  [14400/175341]\n",
      "loss: 0.567149  [16000/175341]\n",
      "loss: 0.279642  [17600/175341]\n",
      "loss: 0.388248  [19200/175341]\n",
      "loss: 0.269414  [20800/175341]\n",
      "loss: 0.522933  [22400/175341]\n",
      "loss: 0.481304  [24000/175341]\n",
      "loss: 0.682646  [25600/175341]\n",
      "loss: 0.414643  [27200/175341]\n",
      "loss: 0.318575  [28800/175341]\n",
      "loss: 0.700939  [30400/175341]\n",
      "loss: 0.302900  [32000/175341]\n",
      "loss: 0.571972  [33600/175341]\n",
      "loss: 0.727308  [35200/175341]\n",
      "loss: 0.204005  [36800/175341]\n",
      "loss: 0.501443  [38400/175341]\n",
      "loss: 0.322191  [40000/175341]\n",
      "loss: 0.415986  [41600/175341]\n",
      "loss: 0.460305  [43200/175341]\n",
      "loss: 0.195465  [44800/175341]\n",
      "loss: 0.796703  [46400/175341]\n",
      "loss: 0.438774  [48000/175341]\n",
      "loss: 0.618688  [49600/175341]\n",
      "loss: 0.290315  [51200/175341]\n",
      "loss: 0.760185  [52800/175341]\n",
      "loss: 0.133852  [54400/175341]\n",
      "loss: 0.403031  [56000/175341]\n",
      "loss: 0.450629  [57600/175341]\n",
      "loss: 0.325033  [59200/175341]\n",
      "loss: 0.603983  [60800/175341]\n",
      "loss: 0.251472  [62400/175341]\n",
      "loss: 0.767878  [64000/175341]\n",
      "loss: 0.408563  [65600/175341]\n",
      "loss: 0.468790  [67200/175341]\n",
      "loss: 0.529046  [68800/175341]\n",
      "loss: 0.622108  [70400/175341]\n",
      "loss: 0.340124  [72000/175341]\n",
      "loss: 0.400054  [73600/175341]\n",
      "loss: 0.508499  [75200/175341]\n",
      "loss: 0.216564  [76800/175341]\n",
      "loss: 0.584543  [78400/175341]\n",
      "loss: 0.392570  [80000/175341]\n",
      "loss: 0.899426  [81600/175341]\n",
      "loss: 0.350799  [83200/175341]\n",
      "loss: 0.288193  [84800/175341]\n",
      "loss: 0.701317  [86400/175341]\n",
      "loss: 0.337729  [88000/175341]\n",
      "loss: 0.408644  [89600/175341]\n",
      "loss: 0.238892  [91200/175341]\n",
      "loss: 0.490940  [92800/175341]\n",
      "loss: 0.794776  [94400/175341]\n",
      "loss: 0.161889  [96000/175341]\n",
      "loss: 0.545723  [97600/175341]\n",
      "loss: 0.168441  [99200/175341]\n",
      "loss: 0.360175  [100800/175341]\n",
      "loss: 0.388529  [102400/175341]\n",
      "loss: 0.268635  [104000/175341]\n",
      "loss: 0.573397  [105600/175341]\n",
      "loss: 0.712590  [107200/175341]\n",
      "loss: 0.582176  [108800/175341]\n",
      "loss: 0.141646  [110400/175341]\n",
      "loss: 0.710626  [112000/175341]\n",
      "loss: 0.698242  [113600/175341]\n",
      "loss: 0.258214  [115200/175341]\n",
      "loss: 0.513774  [116800/175341]\n",
      "loss: 0.234974  [118400/175341]\n",
      "loss: 0.662973  [120000/175341]\n",
      "loss: 0.417252  [121600/175341]\n",
      "loss: 0.404019  [123200/175341]\n",
      "loss: 0.580965  [124800/175341]\n",
      "loss: 0.257417  [126400/175341]\n",
      "loss: 0.385204  [128000/175341]\n",
      "loss: 0.584676  [129600/175341]\n",
      "loss: 0.801630  [131200/175341]\n",
      "loss: 0.087983  [132800/175341]\n",
      "loss: 0.320164  [134400/175341]\n",
      "loss: 0.224169  [136000/175341]\n",
      "loss: 0.438381  [137600/175341]\n",
      "loss: 0.674385  [139200/175341]\n",
      "loss: 0.455755  [140800/175341]\n",
      "loss: 0.950679  [142400/175341]\n",
      "loss: 0.482803  [144000/175341]\n",
      "loss: 0.682520  [145600/175341]\n",
      "loss: 0.660086  [147200/175341]\n",
      "loss: 0.448723  [148800/175341]\n",
      "loss: 0.246487  [150400/175341]\n",
      "loss: 0.483776  [152000/175341]\n",
      "loss: 0.410249  [153600/175341]\n",
      "loss: 0.584631  [155200/175341]\n",
      "loss: 0.511795  [156800/175341]\n",
      "loss: 0.274744  [158400/175341]\n",
      "loss: 0.162018  [160000/175341]\n",
      "loss: 0.486275  [161600/175341]\n",
      "loss: 0.550909  [163200/175341]\n",
      "loss: 0.653362  [164800/175341]\n",
      "loss: 0.399014  [166400/175341]\n",
      "loss: 0.271757  [168000/175341]\n",
      "loss: 0.288478  [169600/175341]\n",
      "loss: 0.216957  [171200/175341]\n",
      "loss: 0.717546  [172800/175341]\n",
      "loss: 0.359556  [174400/175341]\n",
      "Train Accuracy: 82.2004%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.547001, F1-score: 76.83%, Macro_F1-Score:  43.26%  \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.152082  [    0/175341]\n",
      "loss: 0.406508  [ 1600/175341]\n",
      "loss: 0.585965  [ 3200/175341]\n",
      "loss: 0.520261  [ 4800/175341]\n",
      "loss: 0.289575  [ 6400/175341]\n",
      "loss: 0.314163  [ 8000/175341]\n",
      "loss: 0.332217  [ 9600/175341]\n",
      "loss: 0.302952  [11200/175341]\n",
      "loss: 0.502416  [12800/175341]\n",
      "loss: 0.315019  [14400/175341]\n",
      "loss: 0.568005  [16000/175341]\n",
      "loss: 0.340382  [17600/175341]\n",
      "loss: 0.520937  [19200/175341]\n",
      "loss: 0.665846  [20800/175341]\n",
      "loss: 0.441401  [22400/175341]\n",
      "loss: 0.441015  [24000/175341]\n",
      "loss: 0.514678  [25600/175341]\n",
      "loss: 0.648715  [27200/175341]\n",
      "loss: 0.319865  [28800/175341]\n",
      "loss: 0.348039  [30400/175341]\n",
      "loss: 0.393808  [32000/175341]\n",
      "loss: 0.507362  [33600/175341]\n",
      "loss: 0.220439  [35200/175341]\n",
      "loss: 0.234969  [36800/175341]\n",
      "loss: 0.637575  [38400/175341]\n",
      "loss: 0.437784  [40000/175341]\n",
      "loss: 0.336098  [41600/175341]\n",
      "loss: 1.119725  [43200/175341]\n",
      "loss: 0.219778  [44800/175341]\n",
      "loss: 0.570198  [46400/175341]\n",
      "loss: 0.306725  [48000/175341]\n",
      "loss: 0.154626  [49600/175341]\n",
      "loss: 0.444321  [51200/175341]\n",
      "loss: 0.536000  [52800/175341]\n",
      "loss: 0.538298  [54400/175341]\n",
      "loss: 0.247338  [56000/175341]\n",
      "loss: 0.238239  [57600/175341]\n",
      "loss: 0.501580  [59200/175341]\n",
      "loss: 0.401390  [60800/175341]\n",
      "loss: 0.543285  [62400/175341]\n",
      "loss: 0.402758  [64000/175341]\n",
      "loss: 0.371101  [65600/175341]\n",
      "loss: 0.562850  [67200/175341]\n",
      "loss: 0.114224  [68800/175341]\n",
      "loss: 0.461766  [70400/175341]\n",
      "loss: 0.452932  [72000/175341]\n",
      "loss: 0.350192  [73600/175341]\n",
      "loss: 0.503947  [75200/175341]\n",
      "loss: 0.526787  [76800/175341]\n",
      "loss: 0.662937  [78400/175341]\n",
      "loss: 0.435157  [80000/175341]\n",
      "loss: 0.498090  [81600/175341]\n",
      "loss: 0.262350  [83200/175341]\n",
      "loss: 0.851216  [84800/175341]\n",
      "loss: 0.403482  [86400/175341]\n",
      "loss: 0.341288  [88000/175341]\n",
      "loss: 0.378780  [89600/175341]\n",
      "loss: 0.242886  [91200/175341]\n",
      "loss: 0.268428  [92800/175341]\n",
      "loss: 0.664525  [94400/175341]\n",
      "loss: 0.197313  [96000/175341]\n",
      "loss: 0.305598  [97600/175341]\n",
      "loss: 0.400949  [99200/175341]\n",
      "loss: 0.197828  [100800/175341]\n",
      "loss: 0.408229  [102400/175341]\n",
      "loss: 0.363675  [104000/175341]\n",
      "loss: 0.449025  [105600/175341]\n",
      "loss: 0.418173  [107200/175341]\n",
      "loss: 0.298981  [108800/175341]\n",
      "loss: 0.167475  [110400/175341]\n",
      "loss: 0.331009  [112000/175341]\n",
      "loss: 0.267464  [113600/175341]\n",
      "loss: 0.378801  [115200/175341]\n",
      "loss: 0.092216  [116800/175341]\n",
      "loss: 0.318216  [118400/175341]\n",
      "loss: 0.980780  [120000/175341]\n",
      "loss: 0.764602  [121600/175341]\n",
      "loss: 0.850992  [123200/175341]\n",
      "loss: 0.251179  [124800/175341]\n",
      "loss: 0.589867  [126400/175341]\n",
      "loss: 0.626163  [128000/175341]\n",
      "loss: 0.477938  [129600/175341]\n",
      "loss: 0.688032  [131200/175341]\n",
      "loss: 0.360030  [132800/175341]\n",
      "loss: 0.770208  [134400/175341]\n",
      "loss: 0.994667  [136000/175341]\n",
      "loss: 0.562714  [137600/175341]\n",
      "loss: 0.511009  [139200/175341]\n",
      "loss: 0.182887  [140800/175341]\n",
      "loss: 0.480299  [142400/175341]\n",
      "loss: 0.313524  [144000/175341]\n",
      "loss: 0.454785  [145600/175341]\n",
      "loss: 0.496504  [147200/175341]\n",
      "loss: 0.203529  [148800/175341]\n",
      "loss: 0.770219  [150400/175341]\n",
      "loss: 0.498562  [152000/175341]\n",
      "loss: 0.279892  [153600/175341]\n",
      "loss: 0.258124  [155200/175341]\n",
      "loss: 0.495368  [156800/175341]\n",
      "loss: 0.193343  [158400/175341]\n",
      "loss: 0.297200  [160000/175341]\n",
      "loss: 0.462174  [161600/175341]\n",
      "loss: 0.563687  [163200/175341]\n",
      "loss: 0.843984  [164800/175341]\n",
      "loss: 0.143746  [166400/175341]\n",
      "loss: 0.698367  [168000/175341]\n",
      "loss: 0.138201  [169600/175341]\n",
      "loss: 0.452192  [171200/175341]\n",
      "loss: 0.502682  [172800/175341]\n",
      "loss: 0.392518  [174400/175341]\n",
      "Train Accuracy: 82.2677%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.552166, F1-score: 76.59%, Macro_F1-Score:  43.02%  \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.487781  [    0/175341]\n",
      "loss: 0.528955  [ 1600/175341]\n",
      "loss: 0.291966  [ 3200/175341]\n",
      "loss: 0.532785  [ 4800/175341]\n",
      "loss: 0.285915  [ 6400/175341]\n",
      "loss: 0.502829  [ 8000/175341]\n",
      "loss: 0.400509  [ 9600/175341]\n",
      "loss: 0.200774  [11200/175341]\n",
      "loss: 0.154117  [12800/175341]\n",
      "loss: 0.512049  [14400/175341]\n",
      "loss: 0.534595  [16000/175341]\n",
      "loss: 0.237078  [17600/175341]\n",
      "loss: 0.323187  [19200/175341]\n",
      "loss: 0.468489  [20800/175341]\n",
      "loss: 0.143483  [22400/175341]\n",
      "loss: 0.283778  [24000/175341]\n",
      "loss: 0.354829  [25600/175341]\n",
      "loss: 0.655812  [27200/175341]\n",
      "loss: 0.291627  [28800/175341]\n",
      "loss: 0.532912  [30400/175341]\n",
      "loss: 0.567116  [32000/175341]\n",
      "loss: 0.424907  [33600/175341]\n",
      "loss: 0.548031  [35200/175341]\n",
      "loss: 0.601052  [36800/175341]\n",
      "loss: 0.210652  [38400/175341]\n",
      "loss: 0.359626  [40000/175341]\n",
      "loss: 0.288427  [41600/175341]\n",
      "loss: 0.310248  [43200/175341]\n",
      "loss: 0.628416  [44800/175341]\n",
      "loss: 0.269284  [46400/175341]\n",
      "loss: 0.324308  [48000/175341]\n",
      "loss: 0.828644  [49600/175341]\n",
      "loss: 0.452286  [51200/175341]\n",
      "loss: 0.352514  [52800/175341]\n",
      "loss: 0.846493  [54400/175341]\n",
      "loss: 0.471724  [56000/175341]\n",
      "loss: 0.376680  [57600/175341]\n",
      "loss: 0.350822  [59200/175341]\n",
      "loss: 0.548995  [60800/175341]\n",
      "loss: 0.370032  [62400/175341]\n",
      "loss: 0.276753  [64000/175341]\n",
      "loss: 0.358967  [65600/175341]\n",
      "loss: 1.071731  [67200/175341]\n",
      "loss: 0.242597  [68800/175341]\n",
      "loss: 0.410557  [70400/175341]\n",
      "loss: 0.275475  [72000/175341]\n",
      "loss: 0.518869  [73600/175341]\n",
      "loss: 0.268116  [75200/175341]\n",
      "loss: 0.467762  [76800/175341]\n",
      "loss: 0.386546  [78400/175341]\n",
      "loss: 0.242715  [80000/175341]\n",
      "loss: 0.429709  [81600/175341]\n",
      "loss: 0.458492  [83200/175341]\n",
      "loss: 0.692744  [84800/175341]\n",
      "loss: 0.644962  [86400/175341]\n",
      "loss: 0.570971  [88000/175341]\n",
      "loss: 0.361224  [89600/175341]\n",
      "loss: 0.379034  [91200/175341]\n",
      "loss: 0.849292  [92800/175341]\n",
      "loss: 0.396910  [94400/175341]\n",
      "loss: 0.324871  [96000/175341]\n",
      "loss: 0.535076  [97600/175341]\n",
      "loss: 0.182727  [99200/175341]\n",
      "loss: 0.824762  [100800/175341]\n",
      "loss: 0.451180  [102400/175341]\n",
      "loss: 0.516353  [104000/175341]\n",
      "loss: 0.613182  [105600/175341]\n",
      "loss: 0.316098  [107200/175341]\n",
      "loss: 0.405102  [108800/175341]\n",
      "loss: 0.568248  [110400/175341]\n",
      "loss: 0.480104  [112000/175341]\n",
      "loss: 0.567573  [113600/175341]\n",
      "loss: 0.213265  [115200/175341]\n",
      "loss: 0.310770  [116800/175341]\n",
      "loss: 0.351032  [118400/175341]\n",
      "loss: 0.297048  [120000/175341]\n",
      "loss: 0.192016  [121600/175341]\n",
      "loss: 0.291111  [123200/175341]\n",
      "loss: 0.142671  [124800/175341]\n",
      "loss: 0.280660  [126400/175341]\n",
      "loss: 0.955787  [128000/175341]\n",
      "loss: 0.366075  [129600/175341]\n",
      "loss: 0.262713  [131200/175341]\n",
      "loss: 0.426645  [132800/175341]\n",
      "loss: 0.359335  [134400/175341]\n",
      "loss: 0.321865  [136000/175341]\n",
      "loss: 0.189335  [137600/175341]\n",
      "loss: 0.173218  [139200/175341]\n",
      "loss: 0.364037  [140800/175341]\n",
      "loss: 0.521875  [142400/175341]\n",
      "loss: 0.403516  [144000/175341]\n",
      "loss: 0.230965  [145600/175341]\n",
      "loss: 0.346372  [147200/175341]\n",
      "loss: 0.176196  [148800/175341]\n",
      "loss: 0.671458  [150400/175341]\n",
      "loss: 0.489623  [152000/175341]\n",
      "loss: 0.636589  [153600/175341]\n",
      "loss: 0.468993  [155200/175341]\n",
      "loss: 0.669478  [156800/175341]\n",
      "loss: 0.265312  [158400/175341]\n",
      "loss: 0.174879  [160000/175341]\n",
      "loss: 0.520208  [161600/175341]\n",
      "loss: 0.323728  [163200/175341]\n",
      "loss: 0.281637  [164800/175341]\n",
      "loss: 0.373154  [166400/175341]\n",
      "loss: 0.600813  [168000/175341]\n",
      "loss: 0.905945  [169600/175341]\n",
      "loss: 0.882600  [171200/175341]\n",
      "loss: 0.357285  [172800/175341]\n",
      "loss: 0.445336  [174400/175341]\n",
      "Train Accuracy: 82.2677%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.555660, F1-score: 76.35%, Macro_F1-Score:  43.11%  \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.577442  [    0/175341]\n",
      "loss: 0.500529  [ 1600/175341]\n",
      "loss: 0.546171  [ 3200/175341]\n",
      "loss: 0.690682  [ 4800/175341]\n",
      "loss: 0.859413  [ 6400/175341]\n",
      "loss: 0.168118  [ 8000/175341]\n",
      "loss: 0.755910  [ 9600/175341]\n",
      "loss: 0.280985  [11200/175341]\n",
      "loss: 0.296280  [12800/175341]\n",
      "loss: 0.260404  [14400/175341]\n",
      "loss: 0.611273  [16000/175341]\n",
      "loss: 0.614441  [17600/175341]\n",
      "loss: 0.602670  [19200/175341]\n",
      "loss: 0.393483  [20800/175341]\n",
      "loss: 0.729051  [22400/175341]\n",
      "loss: 0.310670  [24000/175341]\n",
      "loss: 0.742792  [25600/175341]\n",
      "loss: 0.241949  [27200/175341]\n",
      "loss: 0.609047  [28800/175341]\n",
      "loss: 0.307810  [30400/175341]\n",
      "loss: 0.281879  [32000/175341]\n",
      "loss: 0.590877  [33600/175341]\n",
      "loss: 0.314779  [35200/175341]\n",
      "loss: 0.163872  [36800/175341]\n",
      "loss: 0.439296  [38400/175341]\n",
      "loss: 0.643901  [40000/175341]\n",
      "loss: 0.325743  [41600/175341]\n",
      "loss: 0.642695  [43200/175341]\n",
      "loss: 0.239474  [44800/175341]\n",
      "loss: 0.217777  [46400/175341]\n",
      "loss: 0.402700  [48000/175341]\n",
      "loss: 0.960882  [49600/175341]\n",
      "loss: 0.092814  [51200/175341]\n",
      "loss: 0.322366  [52800/175341]\n",
      "loss: 0.662510  [54400/175341]\n",
      "loss: 0.595866  [56000/175341]\n",
      "loss: 0.258761  [57600/175341]\n",
      "loss: 0.741021  [59200/175341]\n",
      "loss: 0.282430  [60800/175341]\n",
      "loss: 0.564433  [62400/175341]\n",
      "loss: 0.405117  [64000/175341]\n",
      "loss: 0.393321  [65600/175341]\n",
      "loss: 0.195199  [67200/175341]\n",
      "loss: 0.447658  [68800/175341]\n",
      "loss: 0.289772  [70400/175341]\n",
      "loss: 0.466578  [72000/175341]\n",
      "loss: 0.571316  [73600/175341]\n",
      "loss: 0.101529  [75200/175341]\n",
      "loss: 0.523083  [76800/175341]\n",
      "loss: 0.271138  [78400/175341]\n",
      "loss: 0.655897  [80000/175341]\n",
      "loss: 0.436324  [81600/175341]\n",
      "loss: 0.515525  [83200/175341]\n",
      "loss: 0.498996  [84800/175341]\n",
      "loss: 0.334295  [86400/175341]\n",
      "loss: 0.301091  [88000/175341]\n",
      "loss: 0.461484  [89600/175341]\n",
      "loss: 0.297794  [91200/175341]\n",
      "loss: 0.387496  [92800/175341]\n",
      "loss: 0.308732  [94400/175341]\n",
      "loss: 0.683411  [96000/175341]\n",
      "loss: 0.273275  [97600/175341]\n",
      "loss: 0.296776  [99200/175341]\n",
      "loss: 0.863988  [100800/175341]\n",
      "loss: 0.838941  [102400/175341]\n",
      "loss: 0.646986  [104000/175341]\n",
      "loss: 0.580798  [105600/175341]\n",
      "loss: 0.336096  [107200/175341]\n",
      "loss: 0.626919  [108800/175341]\n",
      "loss: 0.197678  [110400/175341]\n",
      "loss: 0.503338  [112000/175341]\n",
      "loss: 0.560438  [113600/175341]\n",
      "loss: 0.425536  [115200/175341]\n",
      "loss: 0.226136  [116800/175341]\n",
      "loss: 0.604775  [118400/175341]\n",
      "loss: 0.360340  [120000/175341]\n",
      "loss: 0.543870  [121600/175341]\n",
      "loss: 0.281056  [123200/175341]\n",
      "loss: 0.560905  [124800/175341]\n",
      "loss: 0.487136  [126400/175341]\n",
      "loss: 0.545408  [128000/175341]\n",
      "loss: 0.447251  [129600/175341]\n",
      "loss: 0.626529  [131200/175341]\n",
      "loss: 0.382427  [132800/175341]\n",
      "loss: 0.926368  [134400/175341]\n",
      "loss: 0.439685  [136000/175341]\n",
      "loss: 0.641210  [137600/175341]\n",
      "loss: 0.649484  [139200/175341]\n",
      "loss: 0.700643  [140800/175341]\n",
      "loss: 0.625351  [142400/175341]\n",
      "loss: 0.511665  [144000/175341]\n",
      "loss: 0.563341  [145600/175341]\n",
      "loss: 0.257111  [147200/175341]\n",
      "loss: 0.525644  [148800/175341]\n",
      "loss: 0.798020  [150400/175341]\n",
      "loss: 0.309734  [152000/175341]\n",
      "loss: 0.507301  [153600/175341]\n",
      "loss: 0.498240  [155200/175341]\n",
      "loss: 0.635460  [156800/175341]\n",
      "loss: 0.476955  [158400/175341]\n",
      "loss: 0.575829  [160000/175341]\n",
      "loss: 0.397215  [161600/175341]\n",
      "loss: 0.388423  [163200/175341]\n",
      "loss: 0.299441  [164800/175341]\n",
      "loss: 0.481321  [166400/175341]\n",
      "loss: 0.688186  [168000/175341]\n",
      "loss: 0.382823  [169600/175341]\n",
      "loss: 0.341064  [171200/175341]\n",
      "loss: 0.622970  [172800/175341]\n",
      "loss: 0.489607  [174400/175341]\n",
      "Train Accuracy: 82.2500%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.533418, F1-score: 77.57%, Macro_F1-Score:  42.80%  \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.569011  [    0/175341]\n",
      "loss: 0.805542  [ 1600/175341]\n",
      "loss: 0.900439  [ 3200/175341]\n",
      "loss: 0.726839  [ 4800/175341]\n",
      "loss: 0.252416  [ 6400/175341]\n",
      "loss: 0.624790  [ 8000/175341]\n",
      "loss: 0.503329  [ 9600/175341]\n",
      "loss: 0.581721  [11200/175341]\n",
      "loss: 0.355041  [12800/175341]\n",
      "loss: 0.454675  [14400/175341]\n",
      "loss: 0.288324  [16000/175341]\n",
      "loss: 0.474324  [17600/175341]\n",
      "loss: 0.785683  [19200/175341]\n",
      "loss: 0.367375  [20800/175341]\n",
      "loss: 0.300257  [22400/175341]\n",
      "loss: 0.690920  [24000/175341]\n",
      "loss: 0.473240  [25600/175341]\n",
      "loss: 0.406640  [27200/175341]\n",
      "loss: 0.475927  [28800/175341]\n",
      "loss: 0.803443  [30400/175341]\n",
      "loss: 0.475654  [32000/175341]\n",
      "loss: 0.244143  [33600/175341]\n",
      "loss: 0.337204  [35200/175341]\n",
      "loss: 0.321961  [36800/175341]\n",
      "loss: 0.487998  [38400/175341]\n",
      "loss: 0.214117  [40000/175341]\n",
      "loss: 0.739140  [41600/175341]\n",
      "loss: 0.572342  [43200/175341]\n",
      "loss: 0.481487  [44800/175341]\n",
      "loss: 0.776074  [46400/175341]\n",
      "loss: 0.348213  [48000/175341]\n",
      "loss: 0.677182  [49600/175341]\n",
      "loss: 0.474282  [51200/175341]\n",
      "loss: 0.322999  [52800/175341]\n",
      "loss: 0.826048  [54400/175341]\n",
      "loss: 0.306255  [56000/175341]\n",
      "loss: 0.649864  [57600/175341]\n",
      "loss: 0.798638  [59200/175341]\n",
      "loss: 0.188762  [60800/175341]\n",
      "loss: 0.426864  [62400/175341]\n",
      "loss: 0.580278  [64000/175341]\n",
      "loss: 0.145848  [65600/175341]\n",
      "loss: 0.197762  [67200/175341]\n",
      "loss: 0.403824  [68800/175341]\n",
      "loss: 0.281901  [70400/175341]\n",
      "loss: 0.567308  [72000/175341]\n",
      "loss: 0.064216  [73600/175341]\n",
      "loss: 0.790021  [75200/175341]\n",
      "loss: 0.279124  [76800/175341]\n",
      "loss: 0.333824  [78400/175341]\n",
      "loss: 0.526774  [80000/175341]\n",
      "loss: 0.230861  [81600/175341]\n",
      "loss: 0.651049  [83200/175341]\n",
      "loss: 0.541179  [84800/175341]\n",
      "loss: 0.418496  [86400/175341]\n",
      "loss: 0.735676  [88000/175341]\n",
      "loss: 0.255870  [89600/175341]\n",
      "loss: 0.153643  [91200/175341]\n",
      "loss: 0.233976  [92800/175341]\n",
      "loss: 0.392056  [94400/175341]\n",
      "loss: 0.475090  [96000/175341]\n",
      "loss: 0.410939  [97600/175341]\n",
      "loss: 0.671978  [99200/175341]\n",
      "loss: 0.209246  [100800/175341]\n",
      "loss: 0.396092  [102400/175341]\n",
      "loss: 0.527937  [104000/175341]\n",
      "loss: 0.426411  [105600/175341]\n",
      "loss: 0.403590  [107200/175341]\n",
      "loss: 0.347591  [108800/175341]\n",
      "loss: 0.164165  [110400/175341]\n",
      "loss: 0.525750  [112000/175341]\n",
      "loss: 0.167801  [113600/175341]\n",
      "loss: 0.658801  [115200/175341]\n",
      "loss: 0.551356  [116800/175341]\n",
      "loss: 0.229054  [118400/175341]\n",
      "loss: 0.349293  [120000/175341]\n",
      "loss: 0.209126  [121600/175341]\n",
      "loss: 0.381152  [123200/175341]\n",
      "loss: 0.816536  [124800/175341]\n",
      "loss: 0.539596  [126400/175341]\n",
      "loss: 0.589243  [128000/175341]\n",
      "loss: 0.283311  [129600/175341]\n",
      "loss: 0.735507  [131200/175341]\n",
      "loss: 0.498693  [132800/175341]\n",
      "loss: 0.578765  [134400/175341]\n",
      "loss: 0.351906  [136000/175341]\n",
      "loss: 0.129454  [137600/175341]\n",
      "loss: 0.619238  [139200/175341]\n",
      "loss: 0.223013  [140800/175341]\n",
      "loss: 1.167762  [142400/175341]\n",
      "loss: 0.322694  [144000/175341]\n",
      "loss: 0.458629  [145600/175341]\n",
      "loss: 0.634988  [147200/175341]\n",
      "loss: 0.408021  [148800/175341]\n",
      "loss: 0.848640  [150400/175341]\n",
      "loss: 0.269887  [152000/175341]\n",
      "loss: 0.379329  [153600/175341]\n",
      "loss: 0.489996  [155200/175341]\n",
      "loss: 0.133839  [156800/175341]\n",
      "loss: 0.794798  [158400/175341]\n",
      "loss: 0.308417  [160000/175341]\n",
      "loss: 0.308306  [161600/175341]\n",
      "loss: 0.447112  [163200/175341]\n",
      "loss: 0.617361  [164800/175341]\n",
      "loss: 0.528049  [166400/175341]\n",
      "loss: 0.379178  [168000/175341]\n",
      "loss: 0.988733  [169600/175341]\n",
      "loss: 0.290753  [171200/175341]\n",
      "loss: 0.430229  [172800/175341]\n",
      "loss: 0.294404  [174400/175341]\n",
      "Train Accuracy: 82.2968%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.548047, F1-score: 76.86%, Macro_F1-Score:  42.99%  \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.433456  [    0/175341]\n",
      "loss: 0.220250  [ 1600/175341]\n",
      "loss: 0.288567  [ 3200/175341]\n",
      "loss: 0.159418  [ 4800/175341]\n",
      "loss: 0.459074  [ 6400/175341]\n",
      "loss: 0.792152  [ 8000/175341]\n",
      "loss: 0.766794  [ 9600/175341]\n",
      "loss: 0.263689  [11200/175341]\n",
      "loss: 0.336605  [12800/175341]\n",
      "loss: 0.453262  [14400/175341]\n",
      "loss: 0.472412  [16000/175341]\n",
      "loss: 0.411437  [17600/175341]\n",
      "loss: 0.372591  [19200/175341]\n",
      "loss: 0.214312  [20800/175341]\n",
      "loss: 0.453002  [22400/175341]\n",
      "loss: 0.228864  [24000/175341]\n",
      "loss: 0.807810  [25600/175341]\n",
      "loss: 0.450630  [27200/175341]\n",
      "loss: 0.726860  [28800/175341]\n",
      "loss: 0.634771  [30400/175341]\n",
      "loss: 0.724389  [32000/175341]\n",
      "loss: 0.227613  [33600/175341]\n",
      "loss: 0.270525  [35200/175341]\n",
      "loss: 0.777038  [36800/175341]\n",
      "loss: 0.362501  [38400/175341]\n",
      "loss: 0.442894  [40000/175341]\n",
      "loss: 0.506622  [41600/175341]\n",
      "loss: 0.342137  [43200/175341]\n",
      "loss: 0.138890  [44800/175341]\n",
      "loss: 0.982939  [46400/175341]\n",
      "loss: 0.607477  [48000/175341]\n",
      "loss: 0.454351  [49600/175341]\n",
      "loss: 0.437197  [51200/175341]\n",
      "loss: 0.344150  [52800/175341]\n",
      "loss: 0.189163  [54400/175341]\n",
      "loss: 0.557970  [56000/175341]\n",
      "loss: 0.433130  [57600/175341]\n",
      "loss: 0.208647  [59200/175341]\n",
      "loss: 0.373424  [60800/175341]\n",
      "loss: 0.591858  [62400/175341]\n",
      "loss: 0.337376  [64000/175341]\n",
      "loss: 0.285301  [65600/175341]\n",
      "loss: 0.154883  [67200/175341]\n",
      "loss: 0.606856  [68800/175341]\n",
      "loss: 0.577719  [70400/175341]\n",
      "loss: 0.848225  [72000/175341]\n",
      "loss: 0.353217  [73600/175341]\n",
      "loss: 0.390588  [75200/175341]\n",
      "loss: 0.254373  [76800/175341]\n",
      "loss: 0.736272  [78400/175341]\n",
      "loss: 0.334815  [80000/175341]\n",
      "loss: 0.153620  [81600/175341]\n",
      "loss: 0.323486  [83200/175341]\n",
      "loss: 0.345111  [84800/175341]\n",
      "loss: 0.342354  [86400/175341]\n",
      "loss: 0.439329  [88000/175341]\n",
      "loss: 0.413287  [89600/175341]\n",
      "loss: 0.212372  [91200/175341]\n",
      "loss: 0.528434  [92800/175341]\n",
      "loss: 0.452361  [94400/175341]\n",
      "loss: 0.370882  [96000/175341]\n",
      "loss: 0.447166  [97600/175341]\n",
      "loss: 0.340807  [99200/175341]\n",
      "loss: 0.294634  [100800/175341]\n",
      "loss: 0.631319  [102400/175341]\n",
      "loss: 0.341920  [104000/175341]\n",
      "loss: 0.443398  [105600/175341]\n",
      "loss: 0.515999  [107200/175341]\n",
      "loss: 0.681588  [108800/175341]\n",
      "loss: 0.060413  [110400/175341]\n",
      "loss: 0.418231  [112000/175341]\n",
      "loss: 0.189100  [113600/175341]\n",
      "loss: 0.790615  [115200/175341]\n",
      "loss: 0.210915  [116800/175341]\n",
      "loss: 0.423701  [118400/175341]\n",
      "loss: 0.300101  [120000/175341]\n",
      "loss: 0.167798  [121600/175341]\n",
      "loss: 0.740985  [123200/175341]\n",
      "loss: 0.342078  [124800/175341]\n",
      "loss: 0.532468  [126400/175341]\n",
      "loss: 0.234907  [128000/175341]\n",
      "loss: 0.352654  [129600/175341]\n",
      "loss: 0.209209  [131200/175341]\n",
      "loss: 0.271833  [132800/175341]\n",
      "loss: 0.597709  [134400/175341]\n",
      "loss: 0.411669  [136000/175341]\n",
      "loss: 0.152160  [137600/175341]\n",
      "loss: 0.486288  [139200/175341]\n",
      "loss: 0.411550  [140800/175341]\n",
      "loss: 0.305111  [142400/175341]\n",
      "loss: 0.227185  [144000/175341]\n",
      "loss: 0.354503  [145600/175341]\n",
      "loss: 0.432445  [147200/175341]\n",
      "loss: 0.191978  [148800/175341]\n",
      "loss: 0.532004  [150400/175341]\n",
      "loss: 0.221851  [152000/175341]\n",
      "loss: 0.226453  [153600/175341]\n",
      "loss: 0.203183  [155200/175341]\n",
      "loss: 0.365736  [156800/175341]\n",
      "loss: 0.633701  [158400/175341]\n",
      "loss: 0.508520  [160000/175341]\n",
      "loss: 0.628238  [161600/175341]\n",
      "loss: 0.609638  [163200/175341]\n",
      "loss: 0.273591  [164800/175341]\n",
      "loss: 0.157606  [166400/175341]\n",
      "loss: 0.190656  [168000/175341]\n",
      "loss: 0.401773  [169600/175341]\n",
      "loss: 0.305419  [171200/175341]\n",
      "loss: 0.258709  [172800/175341]\n",
      "loss: 0.285479  [174400/175341]\n",
      "Train Accuracy: 82.2540%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.563942, F1-score: 76.52%, Macro_F1-Score:  43.87%  \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.463241  [    0/175341]\n",
      "loss: 0.430136  [ 1600/175341]\n",
      "loss: 0.144990  [ 3200/175341]\n",
      "loss: 0.395695  [ 4800/175341]\n",
      "loss: 0.581666  [ 6400/175341]\n",
      "loss: 0.338456  [ 8000/175341]\n",
      "loss: 0.401674  [ 9600/175341]\n",
      "loss: 0.270179  [11200/175341]\n",
      "loss: 0.529696  [12800/175341]\n",
      "loss: 0.231800  [14400/175341]\n",
      "loss: 0.404470  [16000/175341]\n",
      "loss: 0.315501  [17600/175341]\n",
      "loss: 0.460297  [19200/175341]\n",
      "loss: 0.481099  [20800/175341]\n",
      "loss: 0.363196  [22400/175341]\n",
      "loss: 0.274013  [24000/175341]\n",
      "loss: 0.251709  [25600/175341]\n",
      "loss: 0.242896  [27200/175341]\n",
      "loss: 0.375781  [28800/175341]\n",
      "loss: 0.483139  [30400/175341]\n",
      "loss: 0.573486  [32000/175341]\n",
      "loss: 0.270158  [33600/175341]\n",
      "loss: 0.268469  [35200/175341]\n",
      "loss: 0.923733  [36800/175341]\n",
      "loss: 0.394193  [38400/175341]\n",
      "loss: 0.259012  [40000/175341]\n",
      "loss: 0.922712  [41600/175341]\n",
      "loss: 0.393307  [43200/175341]\n",
      "loss: 0.190522  [44800/175341]\n",
      "loss: 0.747676  [46400/175341]\n",
      "loss: 0.134829  [48000/175341]\n",
      "loss: 0.502115  [49600/175341]\n",
      "loss: 0.907766  [51200/175341]\n",
      "loss: 0.259292  [52800/175341]\n",
      "loss: 0.362883  [54400/175341]\n",
      "loss: 0.412167  [56000/175341]\n",
      "loss: 0.286351  [57600/175341]\n",
      "loss: 0.758848  [59200/175341]\n",
      "loss: 0.336726  [60800/175341]\n",
      "loss: 0.432007  [62400/175341]\n",
      "loss: 0.511415  [64000/175341]\n",
      "loss: 0.462415  [65600/175341]\n",
      "loss: 0.720302  [67200/175341]\n",
      "loss: 0.201678  [68800/175341]\n",
      "loss: 0.159621  [70400/175341]\n",
      "loss: 0.618459  [72000/175341]\n",
      "loss: 0.479853  [73600/175341]\n",
      "loss: 0.417342  [75200/175341]\n",
      "loss: 0.416167  [76800/175341]\n",
      "loss: 0.423263  [78400/175341]\n",
      "loss: 0.090449  [80000/175341]\n",
      "loss: 0.399898  [81600/175341]\n",
      "loss: 0.638973  [83200/175341]\n",
      "loss: 0.887885  [84800/175341]\n",
      "loss: 1.179533  [86400/175341]\n",
      "loss: 0.289132  [88000/175341]\n",
      "loss: 0.878474  [89600/175341]\n",
      "loss: 0.516298  [91200/175341]\n",
      "loss: 0.476832  [92800/175341]\n",
      "loss: 0.277942  [94400/175341]\n",
      "loss: 0.398442  [96000/175341]\n",
      "loss: 0.440266  [97600/175341]\n",
      "loss: 0.459048  [99200/175341]\n",
      "loss: 0.604838  [100800/175341]\n",
      "loss: 0.270172  [102400/175341]\n",
      "loss: 0.475378  [104000/175341]\n",
      "loss: 0.413730  [105600/175341]\n",
      "loss: 0.448254  [107200/175341]\n",
      "loss: 0.529079  [108800/175341]\n",
      "loss: 0.409093  [110400/175341]\n",
      "loss: 0.610838  [112000/175341]\n",
      "loss: 0.378385  [113600/175341]\n",
      "loss: 0.440021  [115200/175341]\n",
      "loss: 0.243649  [116800/175341]\n",
      "loss: 0.307312  [118400/175341]\n",
      "loss: 0.450787  [120000/175341]\n",
      "loss: 0.577824  [121600/175341]\n",
      "loss: 0.685064  [123200/175341]\n",
      "loss: 0.586008  [124800/175341]\n",
      "loss: 0.672306  [126400/175341]\n",
      "loss: 0.258633  [128000/175341]\n",
      "loss: 0.245572  [129600/175341]\n",
      "loss: 0.117484  [131200/175341]\n",
      "loss: 0.258700  [132800/175341]\n",
      "loss: 0.901424  [134400/175341]\n",
      "loss: 0.275698  [136000/175341]\n",
      "loss: 0.650021  [137600/175341]\n",
      "loss: 0.303571  [139200/175341]\n",
      "loss: 0.544112  [140800/175341]\n",
      "loss: 1.002961  [142400/175341]\n",
      "loss: 0.509157  [144000/175341]\n",
      "loss: 0.314535  [145600/175341]\n",
      "loss: 0.523499  [147200/175341]\n",
      "loss: 0.150337  [148800/175341]\n",
      "loss: 0.270632  [150400/175341]\n",
      "loss: 0.225730  [152000/175341]\n",
      "loss: 0.451708  [153600/175341]\n",
      "loss: 0.272223  [155200/175341]\n",
      "loss: 0.753458  [156800/175341]\n",
      "loss: 0.489815  [158400/175341]\n",
      "loss: 0.525314  [160000/175341]\n",
      "loss: 0.311275  [161600/175341]\n",
      "loss: 0.183569  [163200/175341]\n",
      "loss: 0.743041  [164800/175341]\n",
      "loss: 0.365220  [166400/175341]\n",
      "loss: 0.563356  [168000/175341]\n",
      "loss: 0.581418  [169600/175341]\n",
      "loss: 0.288183  [171200/175341]\n",
      "loss: 0.236930  [172800/175341]\n",
      "loss: 0.722719  [174400/175341]\n",
      "Train Accuracy: 82.2802%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.549124, F1-score: 77.07%, Macro_F1-Score:  43.43%  \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.342145  [    0/175341]\n",
      "loss: 0.406862  [ 1600/175341]\n",
      "loss: 0.450465  [ 3200/175341]\n",
      "loss: 0.873171  [ 4800/175341]\n",
      "loss: 0.288435  [ 6400/175341]\n",
      "loss: 0.594367  [ 8000/175341]\n",
      "loss: 0.529387  [ 9600/175341]\n",
      "loss: 0.841418  [11200/175341]\n",
      "loss: 1.052064  [12800/175341]\n",
      "loss: 0.437059  [14400/175341]\n",
      "loss: 0.552890  [16000/175341]\n",
      "loss: 0.539170  [17600/175341]\n",
      "loss: 0.075992  [19200/175341]\n",
      "loss: 0.246500  [20800/175341]\n",
      "loss: 0.357538  [22400/175341]\n",
      "loss: 0.581869  [24000/175341]\n",
      "loss: 0.398459  [25600/175341]\n",
      "loss: 0.434767  [27200/175341]\n",
      "loss: 0.806170  [28800/175341]\n",
      "loss: 0.444472  [30400/175341]\n",
      "loss: 0.417202  [32000/175341]\n",
      "loss: 0.267626  [33600/175341]\n",
      "loss: 0.336151  [35200/175341]\n",
      "loss: 0.421797  [36800/175341]\n",
      "loss: 0.231060  [38400/175341]\n",
      "loss: 0.212870  [40000/175341]\n",
      "loss: 0.681703  [41600/175341]\n",
      "loss: 0.543749  [43200/175341]\n",
      "loss: 0.283559  [44800/175341]\n",
      "loss: 0.617104  [46400/175341]\n",
      "loss: 0.699428  [48000/175341]\n",
      "loss: 0.356359  [49600/175341]\n",
      "loss: 0.339378  [51200/175341]\n",
      "loss: 0.375711  [52800/175341]\n",
      "loss: 0.451231  [54400/175341]\n",
      "loss: 0.274455  [56000/175341]\n",
      "loss: 0.598534  [57600/175341]\n",
      "loss: 0.217735  [59200/175341]\n",
      "loss: 0.302940  [60800/175341]\n",
      "loss: 0.197244  [62400/175341]\n",
      "loss: 0.365011  [64000/175341]\n",
      "loss: 0.642966  [65600/175341]\n",
      "loss: 0.257819  [67200/175341]\n",
      "loss: 0.482645  [68800/175341]\n",
      "loss: 0.524365  [70400/175341]\n",
      "loss: 0.229359  [72000/175341]\n",
      "loss: 0.677663  [73600/175341]\n",
      "loss: 0.904261  [75200/175341]\n",
      "loss: 0.202929  [76800/175341]\n",
      "loss: 0.615030  [78400/175341]\n",
      "loss: 0.301570  [80000/175341]\n",
      "loss: 0.682922  [81600/175341]\n",
      "loss: 0.556072  [83200/175341]\n",
      "loss: 0.380114  [84800/175341]\n",
      "loss: 0.530533  [86400/175341]\n",
      "loss: 0.465490  [88000/175341]\n",
      "loss: 0.206653  [89600/175341]\n",
      "loss: 0.119038  [91200/175341]\n",
      "loss: 0.310901  [92800/175341]\n",
      "loss: 0.380867  [94400/175341]\n",
      "loss: 0.319118  [96000/175341]\n",
      "loss: 0.415308  [97600/175341]\n",
      "loss: 0.480100  [99200/175341]\n",
      "loss: 0.584431  [100800/175341]\n",
      "loss: 0.302768  [102400/175341]\n",
      "loss: 0.293120  [104000/175341]\n",
      "loss: 0.378165  [105600/175341]\n",
      "loss: 0.177835  [107200/175341]\n",
      "loss: 0.453359  [108800/175341]\n",
      "loss: 0.746427  [110400/175341]\n",
      "loss: 0.513436  [112000/175341]\n",
      "loss: 0.827492  [113600/175341]\n",
      "loss: 0.476787  [115200/175341]\n",
      "loss: 0.760523  [116800/175341]\n",
      "loss: 0.501474  [118400/175341]\n",
      "loss: 0.395086  [120000/175341]\n",
      "loss: 0.221963  [121600/175341]\n",
      "loss: 0.528032  [123200/175341]\n",
      "loss: 0.511240  [124800/175341]\n",
      "loss: 0.795551  [126400/175341]\n",
      "loss: 0.614247  [128000/175341]\n",
      "loss: 0.607485  [129600/175341]\n",
      "loss: 0.198946  [131200/175341]\n",
      "loss: 0.402697  [132800/175341]\n",
      "loss: 0.479897  [134400/175341]\n",
      "loss: 0.532237  [136000/175341]\n",
      "loss: 0.464874  [137600/175341]\n",
      "loss: 0.403054  [139200/175341]\n",
      "loss: 0.350408  [140800/175341]\n",
      "loss: 0.170906  [142400/175341]\n",
      "loss: 0.128436  [144000/175341]\n",
      "loss: 0.293964  [145600/175341]\n",
      "loss: 0.168036  [147200/175341]\n",
      "loss: 0.480451  [148800/175341]\n",
      "loss: 0.242771  [150400/175341]\n",
      "loss: 0.276115  [152000/175341]\n",
      "loss: 0.971948  [153600/175341]\n",
      "loss: 0.144674  [155200/175341]\n",
      "loss: 0.715999  [156800/175341]\n",
      "loss: 0.202234  [158400/175341]\n",
      "loss: 0.588641  [160000/175341]\n",
      "loss: 0.376387  [161600/175341]\n",
      "loss: 0.412519  [163200/175341]\n",
      "loss: 0.151955  [164800/175341]\n",
      "loss: 0.557028  [166400/175341]\n",
      "loss: 0.326006  [168000/175341]\n",
      "loss: 0.167463  [169600/175341]\n",
      "loss: 0.831358  [171200/175341]\n",
      "loss: 0.820275  [172800/175341]\n",
      "loss: 0.797262  [174400/175341]\n",
      "Train Accuracy: 82.2979%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.565146, F1-score: 76.40%, Macro_F1-Score:  43.22%  \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.218373  [    0/175341]\n",
      "loss: 0.319044  [ 1600/175341]\n",
      "loss: 0.633506  [ 3200/175341]\n",
      "loss: 0.388870  [ 4800/175341]\n",
      "loss: 0.470723  [ 6400/175341]\n",
      "loss: 0.526719  [ 8000/175341]\n",
      "loss: 0.605037  [ 9600/175341]\n",
      "loss: 0.371346  [11200/175341]\n",
      "loss: 0.577025  [12800/175341]\n",
      "loss: 0.604585  [14400/175341]\n",
      "loss: 0.498485  [16000/175341]\n",
      "loss: 0.265198  [17600/175341]\n",
      "loss: 0.154705  [19200/175341]\n",
      "loss: 0.814789  [20800/175341]\n",
      "loss: 0.476300  [22400/175341]\n",
      "loss: 0.164552  [24000/175341]\n",
      "loss: 0.220857  [25600/175341]\n",
      "loss: 0.371407  [27200/175341]\n",
      "loss: 0.658332  [28800/175341]\n",
      "loss: 0.347733  [30400/175341]\n",
      "loss: 0.340915  [32000/175341]\n",
      "loss: 0.680840  [33600/175341]\n",
      "loss: 0.428834  [35200/175341]\n",
      "loss: 0.879660  [36800/175341]\n",
      "loss: 0.368470  [38400/175341]\n",
      "loss: 0.483043  [40000/175341]\n",
      "loss: 0.370651  [41600/175341]\n",
      "loss: 0.262737  [43200/175341]\n",
      "loss: 0.463293  [44800/175341]\n",
      "loss: 0.481001  [46400/175341]\n",
      "loss: 0.557422  [48000/175341]\n",
      "loss: 0.229487  [49600/175341]\n",
      "loss: 0.732372  [51200/175341]\n",
      "loss: 0.455213  [52800/175341]\n",
      "loss: 0.217461  [54400/175341]\n",
      "loss: 0.519500  [56000/175341]\n",
      "loss: 0.194343  [57600/175341]\n",
      "loss: 0.224654  [59200/175341]\n",
      "loss: 0.531632  [60800/175341]\n",
      "loss: 0.257477  [62400/175341]\n",
      "loss: 0.497870  [64000/175341]\n",
      "loss: 0.541157  [65600/175341]\n",
      "loss: 0.061107  [67200/175341]\n",
      "loss: 0.190772  [68800/175341]\n",
      "loss: 0.545649  [70400/175341]\n",
      "loss: 0.520928  [72000/175341]\n",
      "loss: 0.195352  [73600/175341]\n",
      "loss: 0.771596  [75200/175341]\n",
      "loss: 0.558136  [76800/175341]\n",
      "loss: 0.241043  [78400/175341]\n",
      "loss: 0.535545  [80000/175341]\n",
      "loss: 0.294692  [81600/175341]\n",
      "loss: 0.483896  [83200/175341]\n",
      "loss: 0.609774  [84800/175341]\n",
      "loss: 0.188172  [86400/175341]\n",
      "loss: 0.433435  [88000/175341]\n",
      "loss: 0.229014  [89600/175341]\n",
      "loss: 0.553036  [91200/175341]\n",
      "loss: 0.278051  [92800/175341]\n",
      "loss: 0.430635  [94400/175341]\n",
      "loss: 0.228993  [96000/175341]\n",
      "loss: 0.251490  [97600/175341]\n",
      "loss: 0.744391  [99200/175341]\n",
      "loss: 0.309111  [100800/175341]\n",
      "loss: 0.344014  [102400/175341]\n",
      "loss: 0.134443  [104000/175341]\n",
      "loss: 0.510802  [105600/175341]\n",
      "loss: 0.524164  [107200/175341]\n",
      "loss: 0.406127  [108800/175341]\n",
      "loss: 0.504453  [110400/175341]\n",
      "loss: 0.707971  [112000/175341]\n",
      "loss: 0.149561  [113600/175341]\n",
      "loss: 0.875879  [115200/175341]\n",
      "loss: 0.612272  [116800/175341]\n",
      "loss: 0.817477  [118400/175341]\n",
      "loss: 0.609954  [120000/175341]\n",
      "loss: 0.399740  [121600/175341]\n",
      "loss: 0.332659  [123200/175341]\n",
      "loss: 0.390369  [124800/175341]\n",
      "loss: 0.491020  [126400/175341]\n",
      "loss: 0.290705  [128000/175341]\n",
      "loss: 0.224503  [129600/175341]\n",
      "loss: 0.369546  [131200/175341]\n",
      "loss: 0.213601  [132800/175341]\n",
      "loss: 0.246426  [134400/175341]\n",
      "loss: 0.787137  [136000/175341]\n",
      "loss: 0.222714  [137600/175341]\n",
      "loss: 0.249773  [139200/175341]\n",
      "loss: 0.431614  [140800/175341]\n",
      "loss: 0.727552  [142400/175341]\n",
      "loss: 0.282550  [144000/175341]\n",
      "loss: 0.314755  [145600/175341]\n",
      "loss: 0.425449  [147200/175341]\n",
      "loss: 0.758152  [148800/175341]\n",
      "loss: 0.743119  [150400/175341]\n",
      "loss: 0.321740  [152000/175341]\n",
      "loss: 0.565610  [153600/175341]\n",
      "loss: 0.044302  [155200/175341]\n",
      "loss: 0.136969  [156800/175341]\n",
      "loss: 0.411065  [158400/175341]\n",
      "loss: 0.606530  [160000/175341]\n",
      "loss: 0.901744  [161600/175341]\n",
      "loss: 0.485439  [163200/175341]\n",
      "loss: 0.586142  [164800/175341]\n",
      "loss: 0.400662  [166400/175341]\n",
      "loss: 0.311387  [168000/175341]\n",
      "loss: 0.140313  [169600/175341]\n",
      "loss: 0.204628  [171200/175341]\n",
      "loss: 0.444207  [172800/175341]\n",
      "loss: 0.139572  [174400/175341]\n",
      "Train Accuracy: 82.2563%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.557716, F1-score: 76.54%, Macro_F1-Score:  43.15%  \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.785779  [    0/175341]\n",
      "loss: 0.407263  [ 1600/175341]\n",
      "loss: 0.549074  [ 3200/175341]\n",
      "loss: 0.738142  [ 4800/175341]\n",
      "loss: 0.280586  [ 6400/175341]\n",
      "loss: 0.420850  [ 8000/175341]\n",
      "loss: 0.608645  [ 9600/175341]\n",
      "loss: 0.331135  [11200/175341]\n",
      "loss: 0.369466  [12800/175341]\n",
      "loss: 0.367572  [14400/175341]\n",
      "loss: 0.425295  [16000/175341]\n",
      "loss: 0.651249  [17600/175341]\n",
      "loss: 0.377369  [19200/175341]\n",
      "loss: 0.225789  [20800/175341]\n",
      "loss: 0.186185  [22400/175341]\n",
      "loss: 0.302266  [24000/175341]\n",
      "loss: 0.281849  [25600/175341]\n",
      "loss: 0.369081  [27200/175341]\n",
      "loss: 1.149537  [28800/175341]\n",
      "loss: 0.028278  [30400/175341]\n",
      "loss: 0.367874  [32000/175341]\n",
      "loss: 0.654349  [33600/175341]\n",
      "loss: 0.513534  [35200/175341]\n",
      "loss: 0.383138  [36800/175341]\n",
      "loss: 0.477055  [38400/175341]\n",
      "loss: 0.516863  [40000/175341]\n",
      "loss: 0.501277  [41600/175341]\n",
      "loss: 0.298895  [43200/175341]\n",
      "loss: 0.737766  [44800/175341]\n",
      "loss: 0.694756  [46400/175341]\n",
      "loss: 0.398152  [48000/175341]\n",
      "loss: 0.329887  [49600/175341]\n",
      "loss: 0.356361  [51200/175341]\n",
      "loss: 0.895822  [52800/175341]\n",
      "loss: 0.495130  [54400/175341]\n",
      "loss: 0.632835  [56000/175341]\n",
      "loss: 0.820168  [57600/175341]\n",
      "loss: 0.225436  [59200/175341]\n",
      "loss: 0.439676  [60800/175341]\n",
      "loss: 0.523554  [62400/175341]\n",
      "loss: 0.728444  [64000/175341]\n",
      "loss: 0.362204  [65600/175341]\n",
      "loss: 0.382307  [67200/175341]\n",
      "loss: 0.224334  [68800/175341]\n",
      "loss: 0.475315  [70400/175341]\n",
      "loss: 0.218493  [72000/175341]\n",
      "loss: 0.209066  [73600/175341]\n",
      "loss: 0.645103  [75200/175341]\n",
      "loss: 0.145842  [76800/175341]\n",
      "loss: 0.352085  [78400/175341]\n",
      "loss: 0.151152  [80000/175341]\n",
      "loss: 0.330485  [81600/175341]\n",
      "loss: 0.442667  [83200/175341]\n",
      "loss: 0.464891  [84800/175341]\n",
      "loss: 0.235166  [86400/175341]\n",
      "loss: 0.947156  [88000/175341]\n",
      "loss: 0.320472  [89600/175341]\n",
      "loss: 0.187687  [91200/175341]\n",
      "loss: 0.242479  [92800/175341]\n",
      "loss: 0.389846  [94400/175341]\n",
      "loss: 0.266673  [96000/175341]\n",
      "loss: 0.642007  [97600/175341]\n",
      "loss: 0.310421  [99200/175341]\n",
      "loss: 0.190830  [100800/175341]\n",
      "loss: 0.296920  [102400/175341]\n",
      "loss: 0.685033  [104000/175341]\n",
      "loss: 0.563135  [105600/175341]\n",
      "loss: 0.421944  [107200/175341]\n",
      "loss: 0.478780  [108800/175341]\n",
      "loss: 0.689137  [110400/175341]\n",
      "loss: 0.277641  [112000/175341]\n",
      "loss: 0.582998  [113600/175341]\n",
      "loss: 0.399311  [115200/175341]\n",
      "loss: 0.192182  [116800/175341]\n",
      "loss: 0.528421  [118400/175341]\n",
      "loss: 0.348459  [120000/175341]\n",
      "loss: 0.187627  [121600/175341]\n",
      "loss: 0.314811  [123200/175341]\n",
      "loss: 0.672217  [124800/175341]\n",
      "loss: 0.519026  [126400/175341]\n",
      "loss: 0.614159  [128000/175341]\n",
      "loss: 0.217364  [129600/175341]\n",
      "loss: 0.792115  [131200/175341]\n",
      "loss: 0.498436  [132800/175341]\n",
      "loss: 0.253903  [134400/175341]\n",
      "loss: 0.456154  [136000/175341]\n",
      "loss: 0.449870  [137600/175341]\n",
      "loss: 0.113459  [139200/175341]\n",
      "loss: 0.436242  [140800/175341]\n",
      "loss: 0.390907  [142400/175341]\n",
      "loss: 0.657474  [144000/175341]\n",
      "loss: 0.352414  [145600/175341]\n",
      "loss: 0.437472  [147200/175341]\n",
      "loss: 0.203733  [148800/175341]\n",
      "loss: 0.202794  [150400/175341]\n",
      "loss: 0.744791  [152000/175341]\n",
      "loss: 0.374828  [153600/175341]\n",
      "loss: 0.739713  [155200/175341]\n",
      "loss: 0.279994  [156800/175341]\n",
      "loss: 0.414681  [158400/175341]\n",
      "loss: 0.434266  [160000/175341]\n",
      "loss: 0.322075  [161600/175341]\n",
      "loss: 0.776581  [163200/175341]\n",
      "loss: 0.680443  [164800/175341]\n",
      "loss: 0.528701  [166400/175341]\n",
      "loss: 0.510231  [168000/175341]\n",
      "loss: 0.532955  [169600/175341]\n",
      "loss: 0.374125  [171200/175341]\n",
      "loss: 0.518944  [172800/175341]\n",
      "loss: 0.304383  [174400/175341]\n",
      "Train Accuracy: 82.2757%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.561029, F1-score: 76.36%, Macro_F1-Score:  43.31%  \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.913516  [    0/175341]\n",
      "loss: 0.206665  [ 1600/175341]\n",
      "loss: 0.419754  [ 3200/175341]\n",
      "loss: 0.638736  [ 4800/175341]\n",
      "loss: 0.453707  [ 6400/175341]\n",
      "loss: 0.770835  [ 8000/175341]\n",
      "loss: 0.246035  [ 9600/175341]\n",
      "loss: 1.107529  [11200/175341]\n",
      "loss: 1.031063  [12800/175341]\n",
      "loss: 0.477268  [14400/175341]\n",
      "loss: 0.546597  [16000/175341]\n",
      "loss: 0.597137  [17600/175341]\n",
      "loss: 0.865707  [19200/175341]\n",
      "loss: 0.636129  [20800/175341]\n",
      "loss: 0.348134  [22400/175341]\n",
      "loss: 0.445574  [24000/175341]\n",
      "loss: 0.765434  [25600/175341]\n",
      "loss: 0.218485  [27200/175341]\n",
      "loss: 0.649786  [28800/175341]\n",
      "loss: 0.625896  [30400/175341]\n",
      "loss: 0.850606  [32000/175341]\n",
      "loss: 0.223650  [33600/175341]\n",
      "loss: 0.460286  [35200/175341]\n",
      "loss: 0.536896  [36800/175341]\n",
      "loss: 0.545478  [38400/175341]\n",
      "loss: 0.307092  [40000/175341]\n",
      "loss: 0.850657  [41600/175341]\n",
      "loss: 0.339790  [43200/175341]\n",
      "loss: 0.260132  [44800/175341]\n",
      "loss: 0.430556  [46400/175341]\n",
      "loss: 0.624046  [48000/175341]\n",
      "loss: 0.446270  [49600/175341]\n",
      "loss: 0.190795  [51200/175341]\n",
      "loss: 0.280865  [52800/175341]\n",
      "loss: 0.708024  [54400/175341]\n",
      "loss: 0.409048  [56000/175341]\n",
      "loss: 0.288759  [57600/175341]\n",
      "loss: 0.645389  [59200/175341]\n",
      "loss: 0.257636  [60800/175341]\n",
      "loss: 0.479778  [62400/175341]\n",
      "loss: 0.295659  [64000/175341]\n",
      "loss: 0.579150  [65600/175341]\n",
      "loss: 0.371361  [67200/175341]\n",
      "loss: 0.353475  [68800/175341]\n",
      "loss: 0.355143  [70400/175341]\n",
      "loss: 0.445894  [72000/175341]\n",
      "loss: 0.765924  [73600/175341]\n",
      "loss: 0.492921  [75200/175341]\n",
      "loss: 0.108159  [76800/175341]\n",
      "loss: 0.006027  [78400/175341]\n",
      "loss: 0.598552  [80000/175341]\n",
      "loss: 0.605912  [81600/175341]\n",
      "loss: 0.579341  [83200/175341]\n",
      "loss: 0.203496  [84800/175341]\n",
      "loss: 0.583713  [86400/175341]\n",
      "loss: 0.267015  [88000/175341]\n",
      "loss: 0.498864  [89600/175341]\n",
      "loss: 0.604150  [91200/175341]\n",
      "loss: 0.588302  [92800/175341]\n",
      "loss: 0.526854  [94400/175341]\n",
      "loss: 0.647022  [96000/175341]\n",
      "loss: 0.342589  [97600/175341]\n",
      "loss: 0.427275  [99200/175341]\n",
      "loss: 0.231426  [100800/175341]\n",
      "loss: 0.753655  [102400/175341]\n",
      "loss: 0.487823  [104000/175341]\n",
      "loss: 0.522963  [105600/175341]\n",
      "loss: 0.192485  [107200/175341]\n",
      "loss: 0.502129  [108800/175341]\n",
      "loss: 0.392548  [110400/175341]\n",
      "loss: 0.341555  [112000/175341]\n",
      "loss: 0.520903  [113600/175341]\n",
      "loss: 0.727144  [115200/175341]\n",
      "loss: 0.643993  [116800/175341]\n",
      "loss: 1.121066  [118400/175341]\n",
      "loss: 0.664906  [120000/175341]\n",
      "loss: 0.588170  [121600/175341]\n",
      "loss: 0.385462  [123200/175341]\n",
      "loss: 0.318524  [124800/175341]\n",
      "loss: 0.331955  [126400/175341]\n",
      "loss: 0.602554  [128000/175341]\n",
      "loss: 0.304341  [129600/175341]\n",
      "loss: 0.703070  [131200/175341]\n",
      "loss: 0.234457  [132800/175341]\n",
      "loss: 0.452155  [134400/175341]\n",
      "loss: 0.772667  [136000/175341]\n",
      "loss: 0.496952  [137600/175341]\n",
      "loss: 0.330484  [139200/175341]\n",
      "loss: 0.491856  [140800/175341]\n",
      "loss: 0.883059  [142400/175341]\n",
      "loss: 0.289148  [144000/175341]\n",
      "loss: 0.275213  [145600/175341]\n",
      "loss: 0.376943  [147200/175341]\n",
      "loss: 0.466949  [148800/175341]\n",
      "loss: 0.660955  [150400/175341]\n",
      "loss: 1.028310  [152000/175341]\n",
      "loss: 0.454001  [153600/175341]\n",
      "loss: 0.703864  [155200/175341]\n",
      "loss: 0.529087  [156800/175341]\n",
      "loss: 0.225227  [158400/175341]\n",
      "loss: 0.445261  [160000/175341]\n",
      "loss: 0.392275  [161600/175341]\n",
      "loss: 0.259738  [163200/175341]\n",
      "loss: 0.285234  [164800/175341]\n",
      "loss: 0.205143  [166400/175341]\n",
      "loss: 0.696975  [168000/175341]\n",
      "loss: 0.692219  [169600/175341]\n",
      "loss: 0.488124  [171200/175341]\n",
      "loss: 0.567279  [172800/175341]\n",
      "loss: 0.551269  [174400/175341]\n",
      "Train Accuracy: 82.2711%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.557365, F1-score: 76.63%, Macro_F1-Score:  43.36%  \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.360516  [    0/175341]\n",
      "loss: 0.358580  [ 1600/175341]\n",
      "loss: 0.193709  [ 3200/175341]\n",
      "loss: 0.431186  [ 4800/175341]\n",
      "loss: 0.786806  [ 6400/175341]\n",
      "loss: 0.365567  [ 8000/175341]\n",
      "loss: 0.443935  [ 9600/175341]\n",
      "loss: 0.459155  [11200/175341]\n",
      "loss: 0.680849  [12800/175341]\n",
      "loss: 0.405617  [14400/175341]\n",
      "loss: 0.436861  [16000/175341]\n",
      "loss: 0.363441  [17600/175341]\n",
      "loss: 0.266093  [19200/175341]\n",
      "loss: 0.181385  [20800/175341]\n",
      "loss: 0.261950  [22400/175341]\n",
      "loss: 0.518006  [24000/175341]\n",
      "loss: 0.479797  [25600/175341]\n",
      "loss: 0.487291  [27200/175341]\n",
      "loss: 0.655820  [28800/175341]\n",
      "loss: 0.200399  [30400/175341]\n",
      "loss: 0.615623  [32000/175341]\n",
      "loss: 0.315113  [33600/175341]\n",
      "loss: 0.369349  [35200/175341]\n",
      "loss: 0.227853  [36800/175341]\n",
      "loss: 0.541129  [38400/175341]\n",
      "loss: 0.606375  [40000/175341]\n",
      "loss: 0.665574  [41600/175341]\n",
      "loss: 0.516609  [43200/175341]\n",
      "loss: 0.494756  [44800/175341]\n",
      "loss: 0.534763  [46400/175341]\n",
      "loss: 0.482849  [48000/175341]\n",
      "loss: 0.632401  [49600/175341]\n",
      "loss: 0.397475  [51200/175341]\n",
      "loss: 0.461697  [52800/175341]\n",
      "loss: 0.336544  [54400/175341]\n",
      "loss: 0.377196  [56000/175341]\n",
      "loss: 0.282123  [57600/175341]\n",
      "loss: 0.877273  [59200/175341]\n",
      "loss: 0.587997  [60800/175341]\n",
      "loss: 0.790388  [62400/175341]\n",
      "loss: 0.268460  [64000/175341]\n",
      "loss: 0.477134  [65600/175341]\n",
      "loss: 0.186717  [67200/175341]\n",
      "loss: 0.224855  [68800/175341]\n",
      "loss: 0.345288  [70400/175341]\n",
      "loss: 0.320587  [72000/175341]\n",
      "loss: 0.787471  [73600/175341]\n",
      "loss: 0.810110  [75200/175341]\n",
      "loss: 0.380942  [76800/175341]\n",
      "loss: 0.589098  [78400/175341]\n",
      "loss: 0.655501  [80000/175341]\n",
      "loss: 0.149629  [81600/175341]\n",
      "loss: 0.361174  [83200/175341]\n",
      "loss: 0.377146  [84800/175341]\n",
      "loss: 0.310975  [86400/175341]\n",
      "loss: 0.228143  [88000/175341]\n",
      "loss: 0.684661  [89600/175341]\n",
      "loss: 0.600623  [91200/175341]\n",
      "loss: 0.406281  [92800/175341]\n",
      "loss: 0.727868  [94400/175341]\n",
      "loss: 0.450454  [96000/175341]\n",
      "loss: 0.072927  [97600/175341]\n",
      "loss: 0.402300  [99200/175341]\n",
      "loss: 0.315943  [100800/175341]\n",
      "loss: 0.592669  [102400/175341]\n",
      "loss: 0.325411  [104000/175341]\n",
      "loss: 0.342429  [105600/175341]\n",
      "loss: 0.351302  [107200/175341]\n",
      "loss: 0.487873  [108800/175341]\n",
      "loss: 0.498241  [110400/175341]\n",
      "loss: 0.287874  [112000/175341]\n",
      "loss: 0.263023  [113600/175341]\n",
      "loss: 0.873519  [115200/175341]\n",
      "loss: 0.600643  [116800/175341]\n",
      "loss: 0.535513  [118400/175341]\n",
      "loss: 0.236852  [120000/175341]\n",
      "loss: 0.308178  [121600/175341]\n",
      "loss: 0.587536  [123200/175341]\n",
      "loss: 0.207149  [124800/175341]\n",
      "loss: 0.601394  [126400/175341]\n",
      "loss: 0.256247  [128000/175341]\n",
      "loss: 0.072951  [129600/175341]\n",
      "loss: 0.421418  [131200/175341]\n",
      "loss: 0.558274  [132800/175341]\n",
      "loss: 0.236496  [134400/175341]\n",
      "loss: 0.557361  [136000/175341]\n",
      "loss: 0.637447  [137600/175341]\n",
      "loss: 0.339032  [139200/175341]\n",
      "loss: 0.535602  [140800/175341]\n",
      "loss: 0.251955  [142400/175341]\n",
      "loss: 0.544956  [144000/175341]\n",
      "loss: 0.983921  [145600/175341]\n",
      "loss: 0.739975  [147200/175341]\n",
      "loss: 0.129422  [148800/175341]\n",
      "loss: 0.898806  [150400/175341]\n",
      "loss: 0.414748  [152000/175341]\n",
      "loss: 0.685531  [153600/175341]\n",
      "loss: 0.495938  [155200/175341]\n",
      "loss: 0.456140  [156800/175341]\n",
      "loss: 0.750002  [158400/175341]\n",
      "loss: 0.641373  [160000/175341]\n",
      "loss: 0.325390  [161600/175341]\n",
      "loss: 0.493707  [163200/175341]\n",
      "loss: 0.635646  [164800/175341]\n",
      "loss: 0.397663  [166400/175341]\n",
      "loss: 0.183061  [168000/175341]\n",
      "loss: 0.263664  [169600/175341]\n",
      "loss: 0.404293  [171200/175341]\n",
      "loss: 0.392778  [172800/175341]\n",
      "loss: 0.425527  [174400/175341]\n",
      "Train Accuracy: 82.2928%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.538946, F1-score: 77.57%, Macro_F1-Score:  43.95%  \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.185855  [    0/175341]\n",
      "loss: 0.610851  [ 1600/175341]\n",
      "loss: 0.207986  [ 3200/175341]\n",
      "loss: 0.351566  [ 4800/175341]\n",
      "loss: 0.375847  [ 6400/175341]\n",
      "loss: 0.446604  [ 8000/175341]\n",
      "loss: 0.425783  [ 9600/175341]\n",
      "loss: 0.130214  [11200/175341]\n",
      "loss: 0.437440  [12800/175341]\n",
      "loss: 0.657561  [14400/175341]\n",
      "loss: 0.710409  [16000/175341]\n",
      "loss: 0.472043  [17600/175341]\n",
      "loss: 0.525992  [19200/175341]\n",
      "loss: 0.742302  [20800/175341]\n",
      "loss: 0.358749  [22400/175341]\n",
      "loss: 0.157322  [24000/175341]\n",
      "loss: 0.453928  [25600/175341]\n",
      "loss: 0.182932  [27200/175341]\n",
      "loss: 0.391841  [28800/175341]\n",
      "loss: 0.567126  [30400/175341]\n",
      "loss: 0.436434  [32000/175341]\n",
      "loss: 0.456972  [33600/175341]\n",
      "loss: 0.542469  [35200/175341]\n",
      "loss: 0.456012  [36800/175341]\n",
      "loss: 0.171117  [38400/175341]\n",
      "loss: 0.294132  [40000/175341]\n",
      "loss: 0.486209  [41600/175341]\n",
      "loss: 0.431346  [43200/175341]\n",
      "loss: 0.363453  [44800/175341]\n",
      "loss: 0.358593  [46400/175341]\n",
      "loss: 0.600571  [48000/175341]\n",
      "loss: 0.316257  [49600/175341]\n",
      "loss: 0.543589  [51200/175341]\n",
      "loss: 0.337772  [52800/175341]\n",
      "loss: 0.596962  [54400/175341]\n",
      "loss: 0.128267  [56000/175341]\n",
      "loss: 0.480817  [57600/175341]\n",
      "loss: 0.159695  [59200/175341]\n",
      "loss: 0.491235  [60800/175341]\n",
      "loss: 0.372341  [62400/175341]\n",
      "loss: 0.342201  [64000/175341]\n",
      "loss: 0.305824  [65600/175341]\n",
      "loss: 0.496970  [67200/175341]\n",
      "loss: 0.129039  [68800/175341]\n",
      "loss: 0.243787  [70400/175341]\n",
      "loss: 0.313378  [72000/175341]\n",
      "loss: 0.341860  [73600/175341]\n",
      "loss: 0.222004  [75200/175341]\n",
      "loss: 0.637269  [76800/175341]\n",
      "loss: 0.500247  [78400/175341]\n",
      "loss: 0.738860  [80000/175341]\n",
      "loss: 0.530279  [81600/175341]\n",
      "loss: 0.443965  [83200/175341]\n",
      "loss: 0.452562  [84800/175341]\n",
      "loss: 0.910435  [86400/175341]\n",
      "loss: 0.262944  [88000/175341]\n",
      "loss: 0.203542  [89600/175341]\n",
      "loss: 0.256944  [91200/175341]\n",
      "loss: 0.276484  [92800/175341]\n",
      "loss: 0.511942  [94400/175341]\n",
      "loss: 0.749077  [96000/175341]\n",
      "loss: 0.273956  [97600/175341]\n",
      "loss: 0.324448  [99200/175341]\n",
      "loss: 0.473999  [100800/175341]\n",
      "loss: 0.327303  [102400/175341]\n",
      "loss: 0.367931  [104000/175341]\n",
      "loss: 0.671948  [105600/175341]\n",
      "loss: 0.427307  [107200/175341]\n",
      "loss: 0.441315  [108800/175341]\n",
      "loss: 0.689326  [110400/175341]\n",
      "loss: 0.397772  [112000/175341]\n",
      "loss: 0.322570  [113600/175341]\n",
      "loss: 0.374773  [115200/175341]\n",
      "loss: 0.441466  [116800/175341]\n",
      "loss: 0.166191  [118400/175341]\n",
      "loss: 0.508657  [120000/175341]\n",
      "loss: 0.431404  [121600/175341]\n",
      "loss: 0.405481  [123200/175341]\n",
      "loss: 0.191609  [124800/175341]\n",
      "loss: 0.293143  [126400/175341]\n",
      "loss: 0.549475  [128000/175341]\n",
      "loss: 0.417969  [129600/175341]\n",
      "loss: 0.150024  [131200/175341]\n",
      "loss: 0.651409  [132800/175341]\n",
      "loss: 1.044533  [134400/175341]\n",
      "loss: 0.246433  [136000/175341]\n",
      "loss: 0.590376  [137600/175341]\n",
      "loss: 0.304477  [139200/175341]\n",
      "loss: 0.172943  [140800/175341]\n",
      "loss: 0.363906  [142400/175341]\n",
      "loss: 0.821431  [144000/175341]\n",
      "loss: 0.195913  [145600/175341]\n",
      "loss: 0.477082  [147200/175341]\n",
      "loss: 0.399256  [148800/175341]\n",
      "loss: 0.409967  [150400/175341]\n",
      "loss: 0.237769  [152000/175341]\n",
      "loss: 0.167741  [153600/175341]\n",
      "loss: 0.394611  [155200/175341]\n",
      "loss: 0.554552  [156800/175341]\n",
      "loss: 0.558991  [158400/175341]\n",
      "loss: 0.491939  [160000/175341]\n",
      "loss: 0.499988  [161600/175341]\n",
      "loss: 0.385901  [163200/175341]\n",
      "loss: 0.402021  [164800/175341]\n",
      "loss: 0.316535  [166400/175341]\n",
      "loss: 0.299674  [168000/175341]\n",
      "loss: 0.186463  [169600/175341]\n",
      "loss: 0.296023  [171200/175341]\n",
      "loss: 0.338242  [172800/175341]\n",
      "loss: 0.277116  [174400/175341]\n",
      "Train Accuracy: 82.3310%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.567180, F1-score: 75.85%, Macro_F1-Score:  43.05%  \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.233640  [    0/175341]\n",
      "loss: 0.666531  [ 1600/175341]\n",
      "loss: 0.521278  [ 3200/175341]\n",
      "loss: 0.180902  [ 4800/175341]\n",
      "loss: 0.325330  [ 6400/175341]\n",
      "loss: 0.295253  [ 8000/175341]\n",
      "loss: 0.353561  [ 9600/175341]\n",
      "loss: 0.137711  [11200/175341]\n",
      "loss: 0.315226  [12800/175341]\n",
      "loss: 0.231226  [14400/175341]\n",
      "loss: 0.524475  [16000/175341]\n",
      "loss: 0.513860  [17600/175341]\n",
      "loss: 0.524825  [19200/175341]\n",
      "loss: 0.854523  [20800/175341]\n",
      "loss: 0.603829  [22400/175341]\n",
      "loss: 0.413794  [24000/175341]\n",
      "loss: 0.736991  [25600/175341]\n",
      "loss: 0.406622  [27200/175341]\n",
      "loss: 0.355793  [28800/175341]\n",
      "loss: 0.314313  [30400/175341]\n",
      "loss: 0.198094  [32000/175341]\n",
      "loss: 0.238224  [33600/175341]\n",
      "loss: 0.450260  [35200/175341]\n",
      "loss: 0.477363  [36800/175341]\n",
      "loss: 0.419151  [38400/175341]\n",
      "loss: 0.346235  [40000/175341]\n",
      "loss: 0.575559  [41600/175341]\n",
      "loss: 0.700015  [43200/175341]\n",
      "loss: 0.724266  [44800/175341]\n",
      "loss: 0.236999  [46400/175341]\n",
      "loss: 0.197322  [48000/175341]\n",
      "loss: 0.368542  [49600/175341]\n",
      "loss: 0.226180  [51200/175341]\n",
      "loss: 0.203542  [52800/175341]\n",
      "loss: 0.388158  [54400/175341]\n",
      "loss: 0.297897  [56000/175341]\n",
      "loss: 0.397613  [57600/175341]\n",
      "loss: 0.446568  [59200/175341]\n",
      "loss: 0.502680  [60800/175341]\n",
      "loss: 0.396887  [62400/175341]\n",
      "loss: 0.331767  [64000/175341]\n",
      "loss: 0.127486  [65600/175341]\n",
      "loss: 0.644109  [67200/175341]\n",
      "loss: 0.409567  [68800/175341]\n",
      "loss: 0.412528  [70400/175341]\n",
      "loss: 0.306756  [72000/175341]\n",
      "loss: 0.332534  [73600/175341]\n",
      "loss: 0.865525  [75200/175341]\n",
      "loss: 0.133234  [76800/175341]\n",
      "loss: 0.453576  [78400/175341]\n",
      "loss: 0.354287  [80000/175341]\n",
      "loss: 0.378180  [81600/175341]\n",
      "loss: 0.406857  [83200/175341]\n",
      "loss: 0.427420  [84800/175341]\n",
      "loss: 0.484930  [86400/175341]\n",
      "loss: 0.448081  [88000/175341]\n",
      "loss: 0.552292  [89600/175341]\n",
      "loss: 0.218760  [91200/175341]\n",
      "loss: 0.425337  [92800/175341]\n",
      "loss: 0.857895  [94400/175341]\n",
      "loss: 0.170275  [96000/175341]\n",
      "loss: 0.355646  [97600/175341]\n",
      "loss: 1.133259  [99200/175341]\n",
      "loss: 0.372921  [100800/175341]\n",
      "loss: 0.531593  [102400/175341]\n",
      "loss: 0.578290  [104000/175341]\n",
      "loss: 0.448689  [105600/175341]\n",
      "loss: 0.193268  [107200/175341]\n",
      "loss: 0.424056  [108800/175341]\n",
      "loss: 0.501703  [110400/175341]\n",
      "loss: 0.328134  [112000/175341]\n",
      "loss: 0.377171  [113600/175341]\n",
      "loss: 0.496492  [115200/175341]\n",
      "loss: 0.541317  [116800/175341]\n",
      "loss: 0.490431  [118400/175341]\n",
      "loss: 0.581990  [120000/175341]\n",
      "loss: 0.511104  [121600/175341]\n",
      "loss: 0.243084  [123200/175341]\n",
      "loss: 0.536338  [124800/175341]\n",
      "loss: 0.391270  [126400/175341]\n",
      "loss: 0.459042  [128000/175341]\n",
      "loss: 0.295051  [129600/175341]\n",
      "loss: 0.640798  [131200/175341]\n",
      "loss: 0.420732  [132800/175341]\n",
      "loss: 0.434541  [134400/175341]\n",
      "loss: 0.519890  [136000/175341]\n",
      "loss: 0.440323  [137600/175341]\n",
      "loss: 0.609690  [139200/175341]\n",
      "loss: 0.360643  [140800/175341]\n",
      "loss: 0.443621  [142400/175341]\n",
      "loss: 0.728268  [144000/175341]\n",
      "loss: 0.459080  [145600/175341]\n",
      "loss: 0.356959  [147200/175341]\n",
      "loss: 0.554364  [148800/175341]\n",
      "loss: 0.429142  [150400/175341]\n",
      "loss: 0.426638  [152000/175341]\n",
      "loss: 0.316548  [153600/175341]\n",
      "loss: 0.402881  [155200/175341]\n",
      "loss: 0.352583  [156800/175341]\n",
      "loss: 0.672413  [158400/175341]\n",
      "loss: 0.428851  [160000/175341]\n",
      "loss: 0.153379  [161600/175341]\n",
      "loss: 0.611007  [163200/175341]\n",
      "loss: 0.542412  [164800/175341]\n",
      "loss: 0.357235  [166400/175341]\n",
      "loss: 0.457707  [168000/175341]\n",
      "loss: 0.714278  [169600/175341]\n",
      "loss: 0.756144  [171200/175341]\n",
      "loss: 0.578063  [172800/175341]\n",
      "loss: 0.460049  [174400/175341]\n",
      "Train Accuracy: 82.2734%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.551182, F1-score: 77.15%, Macro_F1-Score:  43.62%  \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.479943  [    0/175341]\n",
      "loss: 0.580731  [ 1600/175341]\n",
      "loss: 0.397051  [ 3200/175341]\n",
      "loss: 0.541419  [ 4800/175341]\n",
      "loss: 0.609916  [ 6400/175341]\n",
      "loss: 0.283543  [ 8000/175341]\n",
      "loss: 0.659763  [ 9600/175341]\n",
      "loss: 0.474718  [11200/175341]\n",
      "loss: 0.743789  [12800/175341]\n",
      "loss: 0.371551  [14400/175341]\n",
      "loss: 0.432782  [16000/175341]\n",
      "loss: 0.251526  [17600/175341]\n",
      "loss: 0.598404  [19200/175341]\n",
      "loss: 0.403395  [20800/175341]\n",
      "loss: 0.360986  [22400/175341]\n",
      "loss: 0.592998  [24000/175341]\n",
      "loss: 0.186265  [25600/175341]\n",
      "loss: 0.317528  [27200/175341]\n",
      "loss: 0.041915  [28800/175341]\n",
      "loss: 0.815705  [30400/175341]\n",
      "loss: 0.589219  [32000/175341]\n",
      "loss: 0.428414  [33600/175341]\n",
      "loss: 0.271419  [35200/175341]\n",
      "loss: 0.503777  [36800/175341]\n",
      "loss: 0.400703  [38400/175341]\n",
      "loss: 0.434340  [40000/175341]\n",
      "loss: 0.200650  [41600/175341]\n",
      "loss: 0.508149  [43200/175341]\n",
      "loss: 0.449182  [44800/175341]\n",
      "loss: 0.739592  [46400/175341]\n",
      "loss: 0.491262  [48000/175341]\n",
      "loss: 0.582282  [49600/175341]\n",
      "loss: 0.522237  [51200/175341]\n",
      "loss: 0.463665  [52800/175341]\n",
      "loss: 0.512122  [54400/175341]\n",
      "loss: 0.470667  [56000/175341]\n",
      "loss: 0.286937  [57600/175341]\n",
      "loss: 0.425432  [59200/175341]\n",
      "loss: 0.559108  [60800/175341]\n",
      "loss: 0.416954  [62400/175341]\n",
      "loss: 0.522041  [64000/175341]\n",
      "loss: 0.297577  [65600/175341]\n",
      "loss: 0.601004  [67200/175341]\n",
      "loss: 0.213470  [68800/175341]\n",
      "loss: 0.372476  [70400/175341]\n",
      "loss: 0.469009  [72000/175341]\n",
      "loss: 0.221265  [73600/175341]\n",
      "loss: 0.692299  [75200/175341]\n",
      "loss: 0.363792  [76800/175341]\n",
      "loss: 0.166901  [78400/175341]\n",
      "loss: 0.373979  [80000/175341]\n",
      "loss: 0.356197  [81600/175341]\n",
      "loss: 0.714448  [83200/175341]\n",
      "loss: 0.385935  [84800/175341]\n",
      "loss: 0.917281  [86400/175341]\n",
      "loss: 0.973551  [88000/175341]\n",
      "loss: 0.177059  [89600/175341]\n",
      "loss: 0.458506  [91200/175341]\n",
      "loss: 0.329763  [92800/175341]\n",
      "loss: 0.256135  [94400/175341]\n",
      "loss: 0.513297  [96000/175341]\n",
      "loss: 0.700624  [97600/175341]\n",
      "loss: 0.497776  [99200/175341]\n",
      "loss: 0.204160  [100800/175341]\n",
      "loss: 1.138152  [102400/175341]\n",
      "loss: 0.601045  [104000/175341]\n",
      "loss: 0.564040  [105600/175341]\n",
      "loss: 0.505637  [107200/175341]\n",
      "loss: 0.492872  [108800/175341]\n",
      "loss: 0.441298  [110400/175341]\n",
      "loss: 0.658578  [112000/175341]\n",
      "loss: 0.588562  [113600/175341]\n",
      "loss: 0.944033  [115200/175341]\n",
      "loss: 0.488474  [116800/175341]\n",
      "loss: 0.586160  [118400/175341]\n",
      "loss: 0.648241  [120000/175341]\n",
      "loss: 0.345649  [121600/175341]\n",
      "loss: 0.556350  [123200/175341]\n",
      "loss: 0.604324  [124800/175341]\n",
      "loss: 0.376772  [126400/175341]\n",
      "loss: 0.171287  [128000/175341]\n",
      "loss: 0.254548  [129600/175341]\n",
      "loss: 0.360302  [131200/175341]\n",
      "loss: 0.464488  [132800/175341]\n",
      "loss: 0.702372  [134400/175341]\n",
      "loss: 0.321718  [136000/175341]\n",
      "loss: 0.394532  [137600/175341]\n",
      "loss: 0.557857  [139200/175341]\n",
      "loss: 0.486154  [140800/175341]\n",
      "loss: 0.318162  [142400/175341]\n",
      "loss: 0.144288  [144000/175341]\n",
      "loss: 0.505312  [145600/175341]\n",
      "loss: 0.522814  [147200/175341]\n",
      "loss: 0.592850  [148800/175341]\n",
      "loss: 0.313341  [150400/175341]\n",
      "loss: 0.441020  [152000/175341]\n",
      "loss: 0.389065  [153600/175341]\n",
      "loss: 0.414291  [155200/175341]\n",
      "loss: 0.370020  [156800/175341]\n",
      "loss: 0.413484  [158400/175341]\n",
      "loss: 0.562639  [160000/175341]\n",
      "loss: 0.253447  [161600/175341]\n",
      "loss: 0.152949  [163200/175341]\n",
      "loss: 0.890781  [164800/175341]\n",
      "loss: 0.307285  [166400/175341]\n",
      "loss: 0.193073  [168000/175341]\n",
      "loss: 0.447876  [169600/175341]\n",
      "loss: 0.498670  [171200/175341]\n",
      "loss: 0.593911  [172800/175341]\n",
      "loss: 0.184315  [174400/175341]\n",
      "Train Accuracy: 82.2928%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.561380, F1-score: 76.88%, Macro_F1-Score:  43.40%  \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.118849  [    0/175341]\n",
      "loss: 0.169741  [ 1600/175341]\n",
      "loss: 0.069360  [ 3200/175341]\n",
      "loss: 0.401988  [ 4800/175341]\n",
      "loss: 0.129406  [ 6400/175341]\n",
      "loss: 0.644161  [ 8000/175341]\n",
      "loss: 0.508299  [ 9600/175341]\n",
      "loss: 0.366456  [11200/175341]\n",
      "loss: 0.579384  [12800/175341]\n",
      "loss: 0.629645  [14400/175341]\n",
      "loss: 0.306805  [16000/175341]\n",
      "loss: 0.552465  [17600/175341]\n",
      "loss: 0.348211  [19200/175341]\n",
      "loss: 0.140841  [20800/175341]\n",
      "loss: 0.422121  [22400/175341]\n",
      "loss: 0.777134  [24000/175341]\n",
      "loss: 0.088458  [25600/175341]\n",
      "loss: 0.375587  [27200/175341]\n",
      "loss: 0.356067  [28800/175341]\n",
      "loss: 0.637504  [30400/175341]\n",
      "loss: 0.263757  [32000/175341]\n",
      "loss: 0.487976  [33600/175341]\n",
      "loss: 0.261955  [35200/175341]\n",
      "loss: 0.357270  [36800/175341]\n",
      "loss: 0.399568  [38400/175341]\n",
      "loss: 0.564497  [40000/175341]\n",
      "loss: 0.694360  [41600/175341]\n",
      "loss: 0.452353  [43200/175341]\n",
      "loss: 0.192156  [44800/175341]\n",
      "loss: 0.369826  [46400/175341]\n",
      "loss: 0.569645  [48000/175341]\n",
      "loss: 0.469939  [49600/175341]\n",
      "loss: 0.287379  [51200/175341]\n",
      "loss: 0.626896  [52800/175341]\n",
      "loss: 0.796132  [54400/175341]\n",
      "loss: 0.219854  [56000/175341]\n",
      "loss: 0.469948  [57600/175341]\n",
      "loss: 0.144691  [59200/175341]\n",
      "loss: 0.748546  [60800/175341]\n",
      "loss: 0.528204  [62400/175341]\n",
      "loss: 0.605566  [64000/175341]\n",
      "loss: 0.202067  [65600/175341]\n",
      "loss: 0.384907  [67200/175341]\n",
      "loss: 0.297849  [68800/175341]\n",
      "loss: 0.389261  [70400/175341]\n",
      "loss: 0.424474  [72000/175341]\n",
      "loss: 0.370037  [73600/175341]\n",
      "loss: 0.133719  [75200/175341]\n",
      "loss: 0.245773  [76800/175341]\n",
      "loss: 0.390943  [78400/175341]\n",
      "loss: 0.448026  [80000/175341]\n",
      "loss: 0.291285  [81600/175341]\n",
      "loss: 0.276318  [83200/175341]\n",
      "loss: 0.399743  [84800/175341]\n",
      "loss: 0.332697  [86400/175341]\n",
      "loss: 0.462250  [88000/175341]\n",
      "loss: 0.113038  [89600/175341]\n",
      "loss: 0.587891  [91200/175341]\n",
      "loss: 0.383314  [92800/175341]\n",
      "loss: 0.392549  [94400/175341]\n",
      "loss: 0.473500  [96000/175341]\n",
      "loss: 0.459626  [97600/175341]\n",
      "loss: 0.732564  [99200/175341]\n",
      "loss: 0.482233  [100800/175341]\n",
      "loss: 0.521664  [102400/175341]\n",
      "loss: 0.208273  [104000/175341]\n",
      "loss: 0.409127  [105600/175341]\n",
      "loss: 0.372004  [107200/175341]\n",
      "loss: 0.445048  [108800/175341]\n",
      "loss: 0.456953  [110400/175341]\n",
      "loss: 0.850378  [112000/175341]\n",
      "loss: 0.393137  [113600/175341]\n",
      "loss: 0.707080  [115200/175341]\n",
      "loss: 0.444923  [116800/175341]\n",
      "loss: 0.069303  [118400/175341]\n",
      "loss: 0.870715  [120000/175341]\n",
      "loss: 0.439594  [121600/175341]\n",
      "loss: 0.394480  [123200/175341]\n",
      "loss: 0.999228  [124800/175341]\n",
      "loss: 0.129010  [126400/175341]\n",
      "loss: 0.315790  [128000/175341]\n",
      "loss: 0.182881  [129600/175341]\n",
      "loss: 0.203102  [131200/175341]\n",
      "loss: 0.333012  [132800/175341]\n",
      "loss: 0.598797  [134400/175341]\n",
      "loss: 0.903765  [136000/175341]\n",
      "loss: 0.287580  [137600/175341]\n",
      "loss: 0.457396  [139200/175341]\n",
      "loss: 0.236554  [140800/175341]\n",
      "loss: 0.247840  [142400/175341]\n",
      "loss: 0.099431  [144000/175341]\n",
      "loss: 0.522209  [145600/175341]\n",
      "loss: 0.431520  [147200/175341]\n",
      "loss: 0.088905  [148800/175341]\n",
      "loss: 0.406322  [150400/175341]\n",
      "loss: 0.449338  [152000/175341]\n",
      "loss: 0.400262  [153600/175341]\n",
      "loss: 0.728251  [155200/175341]\n",
      "loss: 0.906010  [156800/175341]\n",
      "loss: 0.521013  [158400/175341]\n",
      "loss: 0.383192  [160000/175341]\n",
      "loss: 0.344287  [161600/175341]\n",
      "loss: 0.543393  [163200/175341]\n",
      "loss: 0.374280  [164800/175341]\n",
      "loss: 0.299189  [166400/175341]\n",
      "loss: 0.383012  [168000/175341]\n",
      "loss: 0.567672  [169600/175341]\n",
      "loss: 0.179708  [171200/175341]\n",
      "loss: 0.338379  [172800/175341]\n",
      "loss: 0.192031  [174400/175341]\n",
      "Train Accuracy: 82.3196%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.540033, F1-score: 77.41%, Macro_F1-Score:  43.56%  \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.401469  [    0/175341]\n",
      "loss: 0.769449  [ 1600/175341]\n",
      "loss: 0.324144  [ 3200/175341]\n",
      "loss: 0.258458  [ 4800/175341]\n",
      "loss: 0.381476  [ 6400/175341]\n",
      "loss: 0.253448  [ 8000/175341]\n",
      "loss: 0.620678  [ 9600/175341]\n",
      "loss: 0.358065  [11200/175341]\n",
      "loss: 0.326165  [12800/175341]\n",
      "loss: 0.318467  [14400/175341]\n",
      "loss: 0.293698  [16000/175341]\n",
      "loss: 0.179637  [17600/175341]\n",
      "loss: 0.432912  [19200/175341]\n",
      "loss: 0.136194  [20800/175341]\n",
      "loss: 0.311798  [22400/175341]\n",
      "loss: 0.757308  [24000/175341]\n",
      "loss: 0.285816  [25600/175341]\n",
      "loss: 0.403401  [27200/175341]\n",
      "loss: 0.617486  [28800/175341]\n",
      "loss: 0.248604  [30400/175341]\n",
      "loss: 0.453004  [32000/175341]\n",
      "loss: 0.523291  [33600/175341]\n",
      "loss: 0.159702  [35200/175341]\n",
      "loss: 0.379297  [36800/175341]\n",
      "loss: 0.199206  [38400/175341]\n",
      "loss: 0.390129  [40000/175341]\n",
      "loss: 0.278690  [41600/175341]\n",
      "loss: 0.791548  [43200/175341]\n",
      "loss: 0.642207  [44800/175341]\n",
      "loss: 0.486484  [46400/175341]\n",
      "loss: 0.304042  [48000/175341]\n",
      "loss: 0.635916  [49600/175341]\n",
      "loss: 0.317729  [51200/175341]\n",
      "loss: 0.271967  [52800/175341]\n",
      "loss: 0.621548  [54400/175341]\n",
      "loss: 0.426933  [56000/175341]\n",
      "loss: 0.411790  [57600/175341]\n",
      "loss: 0.308684  [59200/175341]\n",
      "loss: 0.360755  [60800/175341]\n",
      "loss: 0.564496  [62400/175341]\n",
      "loss: 0.287196  [64000/175341]\n",
      "loss: 0.206973  [65600/175341]\n",
      "loss: 0.551884  [67200/175341]\n",
      "loss: 0.255693  [68800/175341]\n",
      "loss: 0.812156  [70400/175341]\n",
      "loss: 0.257195  [72000/175341]\n",
      "loss: 0.335971  [73600/175341]\n",
      "loss: 0.223746  [75200/175341]\n",
      "loss: 0.445060  [76800/175341]\n",
      "loss: 0.315672  [78400/175341]\n",
      "loss: 0.424488  [80000/175341]\n",
      "loss: 0.225637  [81600/175341]\n",
      "loss: 0.356781  [83200/175341]\n",
      "loss: 0.332110  [84800/175341]\n",
      "loss: 0.656022  [86400/175341]\n",
      "loss: 0.256033  [88000/175341]\n",
      "loss: 0.668442  [89600/175341]\n",
      "loss: 0.327935  [91200/175341]\n",
      "loss: 0.297383  [92800/175341]\n",
      "loss: 0.449052  [94400/175341]\n",
      "loss: 0.971457  [96000/175341]\n",
      "loss: 0.316545  [97600/175341]\n",
      "loss: 0.262022  [99200/175341]\n",
      "loss: 0.528567  [100800/175341]\n",
      "loss: 0.609310  [102400/175341]\n",
      "loss: 0.339992  [104000/175341]\n",
      "loss: 0.630505  [105600/175341]\n",
      "loss: 0.332891  [107200/175341]\n",
      "loss: 0.407916  [108800/175341]\n",
      "loss: 0.284706  [110400/175341]\n",
      "loss: 0.230553  [112000/175341]\n",
      "loss: 0.921816  [113600/175341]\n",
      "loss: 0.246592  [115200/175341]\n",
      "loss: 0.337637  [116800/175341]\n",
      "loss: 0.187418  [118400/175341]\n",
      "loss: 0.305585  [120000/175341]\n",
      "loss: 0.484705  [121600/175341]\n",
      "loss: 0.459738  [123200/175341]\n",
      "loss: 0.437917  [124800/175341]\n",
      "loss: 0.474955  [126400/175341]\n",
      "loss: 0.269305  [128000/175341]\n",
      "loss: 0.383631  [129600/175341]\n",
      "loss: 0.456833  [131200/175341]\n",
      "loss: 0.258171  [132800/175341]\n",
      "loss: 0.728049  [134400/175341]\n",
      "loss: 0.450552  [136000/175341]\n",
      "loss: 0.424418  [137600/175341]\n",
      "loss: 0.371203  [139200/175341]\n",
      "loss: 0.432406  [140800/175341]\n",
      "loss: 0.379100  [142400/175341]\n",
      "loss: 0.427609  [144000/175341]\n",
      "loss: 0.368880  [145600/175341]\n",
      "loss: 0.315466  [147200/175341]\n",
      "loss: 0.138444  [148800/175341]\n",
      "loss: 0.453018  [150400/175341]\n",
      "loss: 0.553515  [152000/175341]\n",
      "loss: 0.374806  [153600/175341]\n",
      "loss: 0.640122  [155200/175341]\n",
      "loss: 0.623930  [156800/175341]\n",
      "loss: 0.835139  [158400/175341]\n",
      "loss: 0.384581  [160000/175341]\n",
      "loss: 0.369445  [161600/175341]\n",
      "loss: 0.471538  [163200/175341]\n",
      "loss: 0.297925  [164800/175341]\n",
      "loss: 0.372730  [166400/175341]\n",
      "loss: 0.348755  [168000/175341]\n",
      "loss: 0.691659  [169600/175341]\n",
      "loss: 0.240201  [171200/175341]\n",
      "loss: 0.263910  [172800/175341]\n",
      "loss: 0.321021  [174400/175341]\n",
      "Train Accuracy: 82.3042%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.564111, F1-score: 76.28%, Macro_F1-Score:  43.21%  \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.231453  [    0/175341]\n",
      "loss: 0.440085  [ 1600/175341]\n",
      "loss: 0.496299  [ 3200/175341]\n",
      "loss: 0.258746  [ 4800/175341]\n",
      "loss: 0.528589  [ 6400/175341]\n",
      "loss: 0.243628  [ 8000/175341]\n",
      "loss: 0.298159  [ 9600/175341]\n",
      "loss: 0.338137  [11200/175341]\n",
      "loss: 0.274554  [12800/175341]\n",
      "loss: 0.595842  [14400/175341]\n",
      "loss: 0.595358  [16000/175341]\n",
      "loss: 0.535811  [17600/175341]\n",
      "loss: 0.738414  [19200/175341]\n",
      "loss: 0.434960  [20800/175341]\n",
      "loss: 0.563191  [22400/175341]\n",
      "loss: 0.153617  [24000/175341]\n",
      "loss: 0.262765  [25600/175341]\n",
      "loss: 0.491513  [27200/175341]\n",
      "loss: 0.425613  [28800/175341]\n",
      "loss: 0.608381  [30400/175341]\n",
      "loss: 0.369371  [32000/175341]\n",
      "loss: 0.265598  [33600/175341]\n",
      "loss: 0.262238  [35200/175341]\n",
      "loss: 0.503715  [36800/175341]\n",
      "loss: 0.674208  [38400/175341]\n",
      "loss: 0.316728  [40000/175341]\n",
      "loss: 0.210723  [41600/175341]\n",
      "loss: 0.711330  [43200/175341]\n",
      "loss: 0.587012  [44800/175341]\n",
      "loss: 0.389820  [46400/175341]\n",
      "loss: 0.105451  [48000/175341]\n",
      "loss: 0.625849  [49600/175341]\n",
      "loss: 0.182114  [51200/175341]\n",
      "loss: 0.379704  [52800/175341]\n",
      "loss: 0.277231  [54400/175341]\n",
      "loss: 0.286839  [56000/175341]\n",
      "loss: 0.470596  [57600/175341]\n",
      "loss: 0.485227  [59200/175341]\n",
      "loss: 0.302455  [60800/175341]\n",
      "loss: 0.374141  [62400/175341]\n",
      "loss: 0.317182  [64000/175341]\n",
      "loss: 0.233365  [65600/175341]\n",
      "loss: 0.397018  [67200/175341]\n",
      "loss: 0.539397  [68800/175341]\n",
      "loss: 0.937079  [70400/175341]\n",
      "loss: 0.985626  [72000/175341]\n",
      "loss: 0.151252  [73600/175341]\n",
      "loss: 0.598208  [75200/175341]\n",
      "loss: 0.410958  [76800/175341]\n",
      "loss: 0.819196  [78400/175341]\n",
      "loss: 0.327323  [80000/175341]\n",
      "loss: 0.361874  [81600/175341]\n",
      "loss: 0.299488  [83200/175341]\n",
      "loss: 0.443515  [84800/175341]\n",
      "loss: 0.516870  [86400/175341]\n",
      "loss: 0.409831  [88000/175341]\n",
      "loss: 0.396932  [89600/175341]\n",
      "loss: 0.409499  [91200/175341]\n",
      "loss: 0.416024  [92800/175341]\n",
      "loss: 0.450088  [94400/175341]\n",
      "loss: 0.603422  [96000/175341]\n",
      "loss: 0.743593  [97600/175341]\n",
      "loss: 0.410254  [99200/175341]\n",
      "loss: 0.708337  [100800/175341]\n",
      "loss: 0.434056  [102400/175341]\n",
      "loss: 0.443028  [104000/175341]\n",
      "loss: 0.642173  [105600/175341]\n",
      "loss: 0.282083  [107200/175341]\n",
      "loss: 0.405187  [108800/175341]\n",
      "loss: 0.478671  [110400/175341]\n",
      "loss: 0.379428  [112000/175341]\n",
      "loss: 0.840172  [113600/175341]\n",
      "loss: 0.291474  [115200/175341]\n",
      "loss: 0.582145  [116800/175341]\n",
      "loss: 0.394190  [118400/175341]\n",
      "loss: 0.735128  [120000/175341]\n",
      "loss: 0.691830  [121600/175341]\n",
      "loss: 0.491583  [123200/175341]\n",
      "loss: 0.739916  [124800/175341]\n",
      "loss: 0.355343  [126400/175341]\n",
      "loss: 0.749218  [128000/175341]\n",
      "loss: 0.395295  [129600/175341]\n",
      "loss: 0.654677  [131200/175341]\n",
      "loss: 0.776617  [132800/175341]\n",
      "loss: 0.589858  [134400/175341]\n",
      "loss: 0.263012  [136000/175341]\n",
      "loss: 0.505917  [137600/175341]\n",
      "loss: 0.768612  [139200/175341]\n",
      "loss: 0.364353  [140800/175341]\n",
      "loss: 0.507828  [142400/175341]\n",
      "loss: 0.455643  [144000/175341]\n",
      "loss: 0.247489  [145600/175341]\n",
      "loss: 0.247643  [147200/175341]\n",
      "loss: 0.271491  [148800/175341]\n",
      "loss: 0.618460  [150400/175341]\n",
      "loss: 0.633373  [152000/175341]\n",
      "loss: 0.525433  [153600/175341]\n",
      "loss: 0.141798  [155200/175341]\n",
      "loss: 0.719618  [156800/175341]\n",
      "loss: 0.645122  [158400/175341]\n",
      "loss: 0.101098  [160000/175341]\n",
      "loss: 0.314611  [161600/175341]\n",
      "loss: 0.387750  [163200/175341]\n",
      "loss: 0.460173  [164800/175341]\n",
      "loss: 0.645884  [166400/175341]\n",
      "loss: 0.451515  [168000/175341]\n",
      "loss: 0.548186  [169600/175341]\n",
      "loss: 0.270348  [171200/175341]\n",
      "loss: 0.698650  [172800/175341]\n",
      "loss: 0.483799  [174400/175341]\n",
      "Train Accuracy: 82.3145%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.548518, F1-score: 77.01%, Macro_F1-Score:  43.37%  \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.221683  [    0/175341]\n",
      "loss: 0.148999  [ 1600/175341]\n",
      "loss: 0.334391  [ 3200/175341]\n",
      "loss: 0.172819  [ 4800/175341]\n",
      "loss: 0.532319  [ 6400/175341]\n",
      "loss: 0.231394  [ 8000/175341]\n",
      "loss: 0.901525  [ 9600/175341]\n",
      "loss: 0.724413  [11200/175341]\n",
      "loss: 0.761427  [12800/175341]\n",
      "loss: 0.339550  [14400/175341]\n",
      "loss: 0.365722  [16000/175341]\n",
      "loss: 0.218975  [17600/175341]\n",
      "loss: 0.197746  [19200/175341]\n",
      "loss: 0.256202  [20800/175341]\n",
      "loss: 0.025884  [22400/175341]\n",
      "loss: 0.675518  [24000/175341]\n",
      "loss: 0.428467  [25600/175341]\n",
      "loss: 0.867939  [27200/175341]\n",
      "loss: 0.537574  [28800/175341]\n",
      "loss: 0.472308  [30400/175341]\n",
      "loss: 0.319947  [32000/175341]\n",
      "loss: 0.268747  [33600/175341]\n",
      "loss: 0.292087  [35200/175341]\n",
      "loss: 0.685227  [36800/175341]\n",
      "loss: 0.479566  [38400/175341]\n",
      "loss: 0.517637  [40000/175341]\n",
      "loss: 0.278718  [41600/175341]\n",
      "loss: 0.385863  [43200/175341]\n",
      "loss: 0.315202  [44800/175341]\n",
      "loss: 0.548336  [46400/175341]\n",
      "loss: 0.314805  [48000/175341]\n",
      "loss: 0.639405  [49600/175341]\n",
      "loss: 0.295387  [51200/175341]\n",
      "loss: 0.230766  [52800/175341]\n",
      "loss: 0.935704  [54400/175341]\n",
      "loss: 0.105156  [56000/175341]\n",
      "loss: 0.594563  [57600/175341]\n",
      "loss: 0.324302  [59200/175341]\n",
      "loss: 0.372663  [60800/175341]\n",
      "loss: 0.231089  [62400/175341]\n",
      "loss: 0.318628  [64000/175341]\n",
      "loss: 0.297119  [65600/175341]\n",
      "loss: 0.307755  [67200/175341]\n",
      "loss: 0.639872  [68800/175341]\n",
      "loss: 0.640631  [70400/175341]\n",
      "loss: 0.226563  [72000/175341]\n",
      "loss: 0.161842  [73600/175341]\n",
      "loss: 0.477148  [75200/175341]\n",
      "loss: 0.888979  [76800/175341]\n",
      "loss: 0.416627  [78400/175341]\n",
      "loss: 0.612524  [80000/175341]\n",
      "loss: 0.357903  [81600/175341]\n",
      "loss: 0.639061  [83200/175341]\n",
      "loss: 0.247719  [84800/175341]\n",
      "loss: 0.500162  [86400/175341]\n",
      "loss: 0.451277  [88000/175341]\n",
      "loss: 0.548706  [89600/175341]\n",
      "loss: 0.143440  [91200/175341]\n",
      "loss: 0.572361  [92800/175341]\n",
      "loss: 0.849565  [94400/175341]\n",
      "loss: 0.219006  [96000/175341]\n",
      "loss: 0.755702  [97600/175341]\n",
      "loss: 0.250793  [99200/175341]\n",
      "loss: 0.178911  [100800/175341]\n",
      "loss: 0.440645  [102400/175341]\n",
      "loss: 0.130155  [104000/175341]\n",
      "loss: 0.409946  [105600/175341]\n",
      "loss: 0.493185  [107200/175341]\n",
      "loss: 0.555270  [108800/175341]\n",
      "loss: 0.331994  [110400/175341]\n",
      "loss: 0.540875  [112000/175341]\n",
      "loss: 0.253962  [113600/175341]\n",
      "loss: 0.360499  [115200/175341]\n",
      "loss: 0.964019  [116800/175341]\n",
      "loss: 0.289927  [118400/175341]\n",
      "loss: 0.595246  [120000/175341]\n",
      "loss: 0.466631  [121600/175341]\n",
      "loss: 0.696230  [123200/175341]\n",
      "loss: 0.266239  [124800/175341]\n",
      "loss: 0.684169  [126400/175341]\n",
      "loss: 0.380909  [128000/175341]\n",
      "loss: 0.756448  [129600/175341]\n",
      "loss: 0.300318  [131200/175341]\n",
      "loss: 0.583694  [132800/175341]\n",
      "loss: 0.676939  [134400/175341]\n",
      "loss: 0.581231  [136000/175341]\n",
      "loss: 0.128874  [137600/175341]\n",
      "loss: 0.189420  [139200/175341]\n",
      "loss: 0.477357  [140800/175341]\n",
      "loss: 0.777549  [142400/175341]\n",
      "loss: 0.496080  [144000/175341]\n",
      "loss: 0.534919  [145600/175341]\n",
      "loss: 0.369023  [147200/175341]\n",
      "loss: 0.546379  [148800/175341]\n",
      "loss: 0.565432  [150400/175341]\n",
      "loss: 0.555892  [152000/175341]\n",
      "loss: 0.837642  [153600/175341]\n",
      "loss: 0.415080  [155200/175341]\n",
      "loss: 0.548789  [156800/175341]\n",
      "loss: 0.544206  [158400/175341]\n",
      "loss: 0.925455  [160000/175341]\n",
      "loss: 0.250106  [161600/175341]\n",
      "loss: 0.511605  [163200/175341]\n",
      "loss: 0.277221  [164800/175341]\n",
      "loss: 0.239064  [166400/175341]\n",
      "loss: 0.531529  [168000/175341]\n",
      "loss: 0.323805  [169600/175341]\n",
      "loss: 0.233268  [171200/175341]\n",
      "loss: 0.325070  [172800/175341]\n",
      "loss: 0.736141  [174400/175341]\n",
      "Train Accuracy: 82.3059%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.544315, F1-score: 77.30%, Macro_F1-Score:  43.04%  \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.308849  [    0/175341]\n",
      "loss: 0.330712  [ 1600/175341]\n",
      "loss: 0.220152  [ 3200/175341]\n",
      "loss: 0.192922  [ 4800/175341]\n",
      "loss: 0.466850  [ 6400/175341]\n",
      "loss: 0.548177  [ 8000/175341]\n",
      "loss: 0.560153  [ 9600/175341]\n",
      "loss: 0.525412  [11200/175341]\n",
      "loss: 0.167554  [12800/175341]\n",
      "loss: 0.516970  [14400/175341]\n",
      "loss: 0.762397  [16000/175341]\n",
      "loss: 0.135837  [17600/175341]\n",
      "loss: 0.338687  [19200/175341]\n",
      "loss: 0.581944  [20800/175341]\n",
      "loss: 0.910626  [22400/175341]\n",
      "loss: 0.474647  [24000/175341]\n",
      "loss: 0.630700  [25600/175341]\n",
      "loss: 1.044210  [27200/175341]\n",
      "loss: 0.478215  [28800/175341]\n",
      "loss: 0.509192  [30400/175341]\n",
      "loss: 0.510216  [32000/175341]\n",
      "loss: 0.412136  [33600/175341]\n",
      "loss: 0.264117  [35200/175341]\n",
      "loss: 0.654350  [36800/175341]\n",
      "loss: 0.528137  [38400/175341]\n",
      "loss: 0.566303  [40000/175341]\n",
      "loss: 0.713783  [41600/175341]\n",
      "loss: 0.482097  [43200/175341]\n",
      "loss: 0.320216  [44800/175341]\n",
      "loss: 0.352771  [46400/175341]\n",
      "loss: 0.447138  [48000/175341]\n",
      "loss: 0.446552  [49600/175341]\n",
      "loss: 0.532947  [51200/175341]\n",
      "loss: 0.215941  [52800/175341]\n",
      "loss: 0.350010  [54400/175341]\n",
      "loss: 0.328236  [56000/175341]\n",
      "loss: 0.585060  [57600/175341]\n",
      "loss: 0.374509  [59200/175341]\n",
      "loss: 0.176192  [60800/175341]\n",
      "loss: 0.665339  [62400/175341]\n",
      "loss: 0.500845  [64000/175341]\n",
      "loss: 0.625197  [65600/175341]\n",
      "loss: 0.429853  [67200/175341]\n",
      "loss: 0.106451  [68800/175341]\n",
      "loss: 0.342132  [70400/175341]\n",
      "loss: 0.386131  [72000/175341]\n",
      "loss: 0.407115  [73600/175341]\n",
      "loss: 0.706118  [75200/175341]\n",
      "loss: 0.303154  [76800/175341]\n",
      "loss: 0.470509  [78400/175341]\n",
      "loss: 0.359575  [80000/175341]\n",
      "loss: 0.405227  [81600/175341]\n",
      "loss: 0.330024  [83200/175341]\n",
      "loss: 0.652571  [84800/175341]\n",
      "loss: 0.289732  [86400/175341]\n",
      "loss: 0.695397  [88000/175341]\n",
      "loss: 0.156105  [89600/175341]\n",
      "loss: 0.603291  [91200/175341]\n",
      "loss: 0.399634  [92800/175341]\n",
      "loss: 0.417897  [94400/175341]\n",
      "loss: 0.401740  [96000/175341]\n",
      "loss: 0.118478  [97600/175341]\n",
      "loss: 0.542871  [99200/175341]\n",
      "loss: 0.301350  [100800/175341]\n",
      "loss: 0.578582  [102400/175341]\n",
      "loss: 0.683053  [104000/175341]\n",
      "loss: 0.208172  [105600/175341]\n",
      "loss: 0.259397  [107200/175341]\n",
      "loss: 0.357172  [108800/175341]\n",
      "loss: 0.588779  [110400/175341]\n",
      "loss: 0.492884  [112000/175341]\n",
      "loss: 0.178891  [113600/175341]\n",
      "loss: 0.239138  [115200/175341]\n",
      "loss: 0.135956  [116800/175341]\n",
      "loss: 0.445563  [118400/175341]\n",
      "loss: 0.428633  [120000/175341]\n",
      "loss: 0.453109  [121600/175341]\n",
      "loss: 0.463777  [123200/175341]\n",
      "loss: 0.440036  [124800/175341]\n",
      "loss: 0.561158  [126400/175341]\n",
      "loss: 0.486007  [128000/175341]\n",
      "loss: 0.434135  [129600/175341]\n",
      "loss: 0.457226  [131200/175341]\n",
      "loss: 0.226861  [132800/175341]\n",
      "loss: 0.290833  [134400/175341]\n",
      "loss: 0.508424  [136000/175341]\n",
      "loss: 0.488091  [137600/175341]\n",
      "loss: 0.604115  [139200/175341]\n",
      "loss: 0.585438  [140800/175341]\n",
      "loss: 0.494780  [142400/175341]\n",
      "loss: 0.390883  [144000/175341]\n",
      "loss: 0.816044  [145600/175341]\n",
      "loss: 0.665940  [147200/175341]\n",
      "loss: 0.483535  [148800/175341]\n",
      "loss: 0.537284  [150400/175341]\n",
      "loss: 0.664146  [152000/175341]\n",
      "loss: 0.241884  [153600/175341]\n",
      "loss: 0.494195  [155200/175341]\n",
      "loss: 0.164267  [156800/175341]\n",
      "loss: 0.605320  [158400/175341]\n",
      "loss: 0.314763  [160000/175341]\n",
      "loss: 0.198911  [161600/175341]\n",
      "loss: 0.293417  [163200/175341]\n",
      "loss: 0.469951  [164800/175341]\n",
      "loss: 0.438406  [166400/175341]\n",
      "loss: 0.938121  [168000/175341]\n",
      "loss: 0.178822  [169600/175341]\n",
      "loss: 0.272338  [171200/175341]\n",
      "loss: 0.456233  [172800/175341]\n",
      "loss: 0.489647  [174400/175341]\n",
      "Train Accuracy: 82.3019%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.586745, F1-score: 74.78%, Macro_F1-Score:  42.17%  \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.564777  [    0/175341]\n",
      "loss: 0.203616  [ 1600/175341]\n",
      "loss: 0.659872  [ 3200/175341]\n",
      "loss: 0.780738  [ 4800/175341]\n",
      "loss: 0.306380  [ 6400/175341]\n",
      "loss: 0.815811  [ 8000/175341]\n",
      "loss: 0.279309  [ 9600/175341]\n",
      "loss: 0.363374  [11200/175341]\n",
      "loss: 0.351256  [12800/175341]\n",
      "loss: 0.261773  [14400/175341]\n",
      "loss: 0.389539  [16000/175341]\n",
      "loss: 0.745992  [17600/175341]\n",
      "loss: 0.298443  [19200/175341]\n",
      "loss: 0.445645  [20800/175341]\n",
      "loss: 0.210208  [22400/175341]\n",
      "loss: 0.460497  [24000/175341]\n",
      "loss: 0.152834  [25600/175341]\n",
      "loss: 0.758302  [27200/175341]\n",
      "loss: 0.208588  [28800/175341]\n",
      "loss: 0.749076  [30400/175341]\n",
      "loss: 0.304535  [32000/175341]\n",
      "loss: 0.214271  [33600/175341]\n",
      "loss: 0.275228  [35200/175341]\n",
      "loss: 0.290726  [36800/175341]\n",
      "loss: 0.895370  [38400/175341]\n",
      "loss: 0.713205  [40000/175341]\n",
      "loss: 0.175915  [41600/175341]\n",
      "loss: 0.386113  [43200/175341]\n",
      "loss: 0.396134  [44800/175341]\n",
      "loss: 0.431478  [46400/175341]\n",
      "loss: 0.481969  [48000/175341]\n",
      "loss: 0.442052  [49600/175341]\n",
      "loss: 0.386855  [51200/175341]\n",
      "loss: 0.708104  [52800/175341]\n",
      "loss: 0.279523  [54400/175341]\n",
      "loss: 0.182928  [56000/175341]\n",
      "loss: 0.201598  [57600/175341]\n",
      "loss: 0.292728  [59200/175341]\n",
      "loss: 1.171336  [60800/175341]\n",
      "loss: 0.257944  [62400/175341]\n",
      "loss: 0.325824  [64000/175341]\n",
      "loss: 0.339096  [65600/175341]\n",
      "loss: 0.288595  [67200/175341]\n",
      "loss: 0.144251  [68800/175341]\n",
      "loss: 0.642438  [70400/175341]\n",
      "loss: 0.534638  [72000/175341]\n",
      "loss: 0.100358  [73600/175341]\n",
      "loss: 0.624379  [75200/175341]\n",
      "loss: 0.472319  [76800/175341]\n",
      "loss: 0.369596  [78400/175341]\n",
      "loss: 0.444927  [80000/175341]\n",
      "loss: 0.345925  [81600/175341]\n",
      "loss: 0.764539  [83200/175341]\n",
      "loss: 0.555845  [84800/175341]\n",
      "loss: 1.148817  [86400/175341]\n",
      "loss: 0.488329  [88000/175341]\n",
      "loss: 0.264373  [89600/175341]\n",
      "loss: 0.389441  [91200/175341]\n",
      "loss: 0.333145  [92800/175341]\n",
      "loss: 0.444560  [94400/175341]\n",
      "loss: 0.648717  [96000/175341]\n",
      "loss: 0.706774  [97600/175341]\n",
      "loss: 0.326840  [99200/175341]\n",
      "loss: 0.351301  [100800/175341]\n",
      "loss: 0.280436  [102400/175341]\n",
      "loss: 0.555566  [104000/175341]\n",
      "loss: 0.761724  [105600/175341]\n",
      "loss: 0.208089  [107200/175341]\n",
      "loss: 0.548300  [108800/175341]\n",
      "loss: 0.466510  [110400/175341]\n",
      "loss: 0.553572  [112000/175341]\n",
      "loss: 0.698256  [113600/175341]\n",
      "loss: 0.288499  [115200/175341]\n",
      "loss: 0.280379  [116800/175341]\n",
      "loss: 1.075318  [118400/175341]\n",
      "loss: 0.253020  [120000/175341]\n",
      "loss: 0.524667  [121600/175341]\n",
      "loss: 0.617646  [123200/175341]\n",
      "loss: 0.547280  [124800/175341]\n",
      "loss: 0.785730  [126400/175341]\n",
      "loss: 0.270014  [128000/175341]\n",
      "loss: 0.293682  [129600/175341]\n",
      "loss: 0.295931  [131200/175341]\n",
      "loss: 0.573812  [132800/175341]\n",
      "loss: 0.801643  [134400/175341]\n",
      "loss: 0.590777  [136000/175341]\n",
      "loss: 0.559844  [137600/175341]\n",
      "loss: 0.388647  [139200/175341]\n",
      "loss: 0.319303  [140800/175341]\n",
      "loss: 0.197724  [142400/175341]\n",
      "loss: 0.302926  [144000/175341]\n",
      "loss: 0.404886  [145600/175341]\n",
      "loss: 0.335594  [147200/175341]\n",
      "loss: 0.431134  [148800/175341]\n",
      "loss: 0.199727  [150400/175341]\n",
      "loss: 0.667160  [152000/175341]\n",
      "loss: 0.584764  [153600/175341]\n",
      "loss: 0.518911  [155200/175341]\n",
      "loss: 0.153773  [156800/175341]\n",
      "loss: 0.380500  [158400/175341]\n",
      "loss: 0.390886  [160000/175341]\n",
      "loss: 1.017631  [161600/175341]\n",
      "loss: 0.329463  [163200/175341]\n",
      "loss: 0.375121  [164800/175341]\n",
      "loss: 0.443374  [166400/175341]\n",
      "loss: 0.750044  [168000/175341]\n",
      "loss: 0.268387  [169600/175341]\n",
      "loss: 0.501626  [171200/175341]\n",
      "loss: 0.409415  [172800/175341]\n",
      "loss: 0.578563  [174400/175341]\n",
      "Train Accuracy: 82.3612%\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.537312, F1-score: 77.53%, Macro_F1-Score:  44.29%  \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 1.020283  [    0/175341]\n",
      "loss: 0.115397  [ 1600/175341]\n",
      "loss: 0.211796  [ 3200/175341]\n",
      "loss: 0.481298  [ 4800/175341]\n",
      "loss: 0.560060  [ 6400/175341]\n",
      "loss: 0.772476  [ 8000/175341]\n",
      "loss: 0.294616  [ 9600/175341]\n",
      "loss: 0.831825  [11200/175341]\n",
      "loss: 0.205177  [12800/175341]\n",
      "loss: 0.214517  [14400/175341]\n",
      "loss: 0.202908  [16000/175341]\n",
      "loss: 0.282410  [17600/175341]\n",
      "loss: 0.271407  [19200/175341]\n",
      "loss: 0.837123  [20800/175341]\n",
      "loss: 0.726884  [22400/175341]\n",
      "loss: 0.213771  [24000/175341]\n",
      "loss: 0.625766  [25600/175341]\n",
      "loss: 0.308215  [27200/175341]\n",
      "loss: 0.321751  [28800/175341]\n",
      "loss: 1.079105  [30400/175341]\n",
      "loss: 0.517963  [32000/175341]\n",
      "loss: 0.321668  [33600/175341]\n",
      "loss: 0.492471  [35200/175341]\n",
      "loss: 0.356074  [36800/175341]\n",
      "loss: 0.637802  [38400/175341]\n",
      "loss: 0.483416  [40000/175341]\n",
      "loss: 0.215787  [41600/175341]\n",
      "loss: 0.461917  [43200/175341]\n",
      "loss: 0.737937  [44800/175341]\n",
      "loss: 0.944119  [46400/175341]\n",
      "loss: 0.699329  [48000/175341]\n",
      "loss: 0.209301  [49600/175341]\n",
      "loss: 0.432620  [51200/175341]\n",
      "loss: 0.460024  [52800/175341]\n",
      "loss: 0.335638  [54400/175341]\n",
      "loss: 0.267100  [56000/175341]\n",
      "loss: 0.424891  [57600/175341]\n",
      "loss: 0.717254  [59200/175341]\n",
      "loss: 0.456440  [60800/175341]\n",
      "loss: 0.273845  [62400/175341]\n",
      "loss: 0.880767  [64000/175341]\n",
      "loss: 0.448247  [65600/175341]\n",
      "loss: 0.347713  [67200/175341]\n",
      "loss: 0.176526  [68800/175341]\n",
      "loss: 0.312937  [70400/175341]\n",
      "loss: 0.208873  [72000/175341]\n",
      "loss: 0.253875  [73600/175341]\n",
      "loss: 0.205606  [75200/175341]\n",
      "loss: 0.386453  [76800/175341]\n",
      "loss: 0.588126  [78400/175341]\n",
      "loss: 0.518116  [80000/175341]\n",
      "loss: 0.296783  [81600/175341]\n",
      "loss: 0.518530  [83200/175341]\n",
      "loss: 0.366545  [84800/175341]\n",
      "loss: 0.489568  [86400/175341]\n",
      "loss: 0.558629  [88000/175341]\n",
      "loss: 0.238745  [89600/175341]\n",
      "loss: 0.632797  [91200/175341]\n",
      "loss: 0.399233  [92800/175341]\n",
      "loss: 0.384605  [94400/175341]\n",
      "loss: 0.472109  [96000/175341]\n",
      "loss: 0.358783  [97600/175341]\n",
      "loss: 0.428305  [99200/175341]\n",
      "loss: 0.869270  [100800/175341]\n",
      "loss: 0.171279  [102400/175341]\n",
      "loss: 0.354141  [104000/175341]\n",
      "loss: 0.286967  [105600/175341]\n",
      "loss: 0.353903  [107200/175341]\n",
      "loss: 0.369440  [108800/175341]\n",
      "loss: 0.550946  [110400/175341]\n",
      "loss: 0.123989  [112000/175341]\n",
      "loss: 0.464624  [113600/175341]\n",
      "loss: 0.267222  [115200/175341]\n",
      "loss: 0.468982  [116800/175341]\n",
      "loss: 0.323791  [118400/175341]\n",
      "loss: 0.192608  [120000/175341]\n",
      "loss: 0.343419  [121600/175341]\n",
      "loss: 0.748263  [123200/175341]\n",
      "loss: 0.817582  [124800/175341]\n",
      "loss: 0.126502  [126400/175341]\n",
      "loss: 0.361767  [128000/175341]\n",
      "loss: 0.810618  [129600/175341]\n",
      "loss: 0.618746  [131200/175341]\n",
      "loss: 0.227880  [132800/175341]\n",
      "loss: 0.509212  [134400/175341]\n",
      "loss: 0.563863  [136000/175341]\n",
      "loss: 0.633408  [137600/175341]\n",
      "loss: 0.577163  [139200/175341]\n",
      "loss: 0.656830  [140800/175341]\n",
      "loss: 0.320939  [142400/175341]\n",
      "loss: 0.580269  [144000/175341]\n",
      "loss: 1.040196  [145600/175341]\n",
      "loss: 0.613701  [147200/175341]\n",
      "loss: 0.372787  [148800/175341]\n",
      "loss: 0.887608  [150400/175341]\n",
      "loss: 0.421678  [152000/175341]\n",
      "loss: 0.615504  [153600/175341]\n",
      "loss: 0.381251  [155200/175341]\n",
      "loss: 0.247229  [156800/175341]\n",
      "loss: 0.447981  [158400/175341]\n",
      "loss: 0.534377  [160000/175341]\n",
      "loss: 0.780323  [161600/175341]\n",
      "loss: 0.151555  [163200/175341]\n",
      "loss: 0.369498  [164800/175341]\n",
      "loss: 0.364149  [166400/175341]\n",
      "loss: 0.273606  [168000/175341]\n",
      "loss: 0.269065  [169600/175341]\n",
      "loss: 0.107947  [171200/175341]\n",
      "loss: 0.773014  [172800/175341]\n",
      "loss: 0.390775  [174400/175341]\n",
      "Train Accuracy: 82.2882%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.534638, F1-score: 77.50%, Macro_F1-Score:  43.21%  \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.126394  [    0/175341]\n",
      "loss: 0.388235  [ 1600/175341]\n",
      "loss: 0.396156  [ 3200/175341]\n",
      "loss: 0.774978  [ 4800/175341]\n",
      "loss: 0.966430  [ 6400/175341]\n",
      "loss: 0.430764  [ 8000/175341]\n",
      "loss: 0.262989  [ 9600/175341]\n",
      "loss: 0.571550  [11200/175341]\n",
      "loss: 0.646774  [12800/175341]\n",
      "loss: 0.671455  [14400/175341]\n",
      "loss: 0.464917  [16000/175341]\n",
      "loss: 0.186897  [17600/175341]\n",
      "loss: 0.232369  [19200/175341]\n",
      "loss: 0.724179  [20800/175341]\n",
      "loss: 0.336775  [22400/175341]\n",
      "loss: 0.540834  [24000/175341]\n",
      "loss: 0.372935  [25600/175341]\n",
      "loss: 0.306400  [27200/175341]\n",
      "loss: 0.426092  [28800/175341]\n",
      "loss: 0.409943  [30400/175341]\n",
      "loss: 0.430605  [32000/175341]\n",
      "loss: 0.599700  [33600/175341]\n",
      "loss: 0.466692  [35200/175341]\n",
      "loss: 0.642420  [36800/175341]\n",
      "loss: 0.879642  [38400/175341]\n",
      "loss: 0.611785  [40000/175341]\n",
      "loss: 0.127725  [41600/175341]\n",
      "loss: 0.150061  [43200/175341]\n",
      "loss: 0.375562  [44800/175341]\n",
      "loss: 0.487836  [46400/175341]\n",
      "loss: 0.040980  [48000/175341]\n",
      "loss: 0.592719  [49600/175341]\n",
      "loss: 0.383629  [51200/175341]\n",
      "loss: 0.291676  [52800/175341]\n",
      "loss: 0.482173  [54400/175341]\n",
      "loss: 0.184866  [56000/175341]\n",
      "loss: 0.742438  [57600/175341]\n",
      "loss: 0.650536  [59200/175341]\n",
      "loss: 0.377133  [60800/175341]\n",
      "loss: 0.538282  [62400/175341]\n",
      "loss: 0.451216  [64000/175341]\n",
      "loss: 0.753874  [65600/175341]\n",
      "loss: 0.703163  [67200/175341]\n",
      "loss: 0.746469  [68800/175341]\n",
      "loss: 0.411940  [70400/175341]\n",
      "loss: 0.233884  [72000/175341]\n",
      "loss: 0.545334  [73600/175341]\n",
      "loss: 0.422901  [75200/175341]\n",
      "loss: 0.206009  [76800/175341]\n",
      "loss: 0.757483  [78400/175341]\n",
      "loss: 0.736295  [80000/175341]\n",
      "loss: 0.324861  [81600/175341]\n",
      "loss: 0.297151  [83200/175341]\n",
      "loss: 0.294513  [84800/175341]\n",
      "loss: 0.263567  [86400/175341]\n",
      "loss: 0.853000  [88000/175341]\n",
      "loss: 0.289698  [89600/175341]\n",
      "loss: 0.328405  [91200/175341]\n",
      "loss: 0.346268  [92800/175341]\n",
      "loss: 0.439097  [94400/175341]\n",
      "loss: 0.277667  [96000/175341]\n",
      "loss: 0.530687  [97600/175341]\n",
      "loss: 0.413627  [99200/175341]\n",
      "loss: 0.375631  [100800/175341]\n",
      "loss: 0.431457  [102400/175341]\n",
      "loss: 0.513614  [104000/175341]\n",
      "loss: 0.514202  [105600/175341]\n",
      "loss: 0.170738  [107200/175341]\n",
      "loss: 0.405574  [108800/175341]\n",
      "loss: 0.789947  [110400/175341]\n",
      "loss: 0.564836  [112000/175341]\n",
      "loss: 0.694736  [113600/175341]\n",
      "loss: 0.232052  [115200/175341]\n",
      "loss: 0.252236  [116800/175341]\n",
      "loss: 0.364360  [118400/175341]\n",
      "loss: 0.222162  [120000/175341]\n",
      "loss: 0.625983  [121600/175341]\n",
      "loss: 0.167216  [123200/175341]\n",
      "loss: 0.197636  [124800/175341]\n",
      "loss: 0.382142  [126400/175341]\n",
      "loss: 0.530996  [128000/175341]\n",
      "loss: 0.303536  [129600/175341]\n",
      "loss: 0.344173  [131200/175341]\n",
      "loss: 0.612010  [132800/175341]\n",
      "loss: 0.342364  [134400/175341]\n",
      "loss: 0.462865  [136000/175341]\n",
      "loss: 0.439526  [137600/175341]\n",
      "loss: 0.413088  [139200/175341]\n",
      "loss: 0.718957  [140800/175341]\n",
      "loss: 0.737047  [142400/175341]\n",
      "loss: 0.289169  [144000/175341]\n",
      "loss: 0.642818  [145600/175341]\n",
      "loss: 0.445487  [147200/175341]\n",
      "loss: 0.370383  [148800/175341]\n",
      "loss: 0.818252  [150400/175341]\n",
      "loss: 0.430099  [152000/175341]\n",
      "loss: 0.423168  [153600/175341]\n",
      "loss: 0.625171  [155200/175341]\n",
      "loss: 0.666116  [156800/175341]\n",
      "loss: 0.293576  [158400/175341]\n",
      "loss: 0.826574  [160000/175341]\n",
      "loss: 0.965496  [161600/175341]\n",
      "loss: 0.217341  [163200/175341]\n",
      "loss: 0.229007  [164800/175341]\n",
      "loss: 0.487627  [166400/175341]\n",
      "loss: 0.236536  [168000/175341]\n",
      "loss: 0.340238  [169600/175341]\n",
      "loss: 1.033750  [171200/175341]\n",
      "loss: 0.707082  [172800/175341]\n",
      "loss: 0.535329  [174400/175341]\n",
      "Train Accuracy: 82.3692%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.546651, F1-score: 76.98%, Macro_F1-Score:  43.05%  \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.207036  [    0/175341]\n",
      "loss: 0.338495  [ 1600/175341]\n",
      "loss: 0.467260  [ 3200/175341]\n",
      "loss: 0.324879  [ 4800/175341]\n",
      "loss: 0.628521  [ 6400/175341]\n",
      "loss: 0.289870  [ 8000/175341]\n",
      "loss: 0.452007  [ 9600/175341]\n",
      "loss: 0.103469  [11200/175341]\n",
      "loss: 0.398565  [12800/175341]\n",
      "loss: 0.158924  [14400/175341]\n",
      "loss: 0.316294  [16000/175341]\n",
      "loss: 0.430599  [17600/175341]\n",
      "loss: 0.924726  [19200/175341]\n",
      "loss: 0.498962  [20800/175341]\n",
      "loss: 0.561524  [22400/175341]\n",
      "loss: 0.863108  [24000/175341]\n",
      "loss: 0.639527  [25600/175341]\n",
      "loss: 0.301292  [27200/175341]\n",
      "loss: 0.412720  [28800/175341]\n",
      "loss: 0.292640  [30400/175341]\n",
      "loss: 0.463190  [32000/175341]\n",
      "loss: 0.230034  [33600/175341]\n",
      "loss: 0.677281  [35200/175341]\n",
      "loss: 0.192164  [36800/175341]\n",
      "loss: 0.440030  [38400/175341]\n",
      "loss: 0.107348  [40000/175341]\n",
      "loss: 0.749727  [41600/175341]\n",
      "loss: 0.276731  [43200/175341]\n",
      "loss: 0.425124  [44800/175341]\n",
      "loss: 0.237277  [46400/175341]\n",
      "loss: 0.266295  [48000/175341]\n",
      "loss: 0.225074  [49600/175341]\n",
      "loss: 0.482070  [51200/175341]\n",
      "loss: 0.340652  [52800/175341]\n",
      "loss: 0.812454  [54400/175341]\n",
      "loss: 0.426919  [56000/175341]\n",
      "loss: 0.247538  [57600/175341]\n",
      "loss: 0.724080  [59200/175341]\n",
      "loss: 0.626952  [60800/175341]\n",
      "loss: 0.347080  [62400/175341]\n",
      "loss: 0.548942  [64000/175341]\n",
      "loss: 0.457860  [65600/175341]\n",
      "loss: 0.373450  [67200/175341]\n",
      "loss: 0.194201  [68800/175341]\n",
      "loss: 0.071266  [70400/175341]\n",
      "loss: 0.185118  [72000/175341]\n",
      "loss: 0.802732  [73600/175341]\n",
      "loss: 0.267281  [75200/175341]\n",
      "loss: 0.681906  [76800/175341]\n",
      "loss: 0.262289  [78400/175341]\n",
      "loss: 0.309468  [80000/175341]\n",
      "loss: 0.456871  [81600/175341]\n",
      "loss: 0.298831  [83200/175341]\n",
      "loss: 0.287986  [84800/175341]\n",
      "loss: 0.333527  [86400/175341]\n",
      "loss: 0.872624  [88000/175341]\n",
      "loss: 0.144925  [89600/175341]\n",
      "loss: 0.671299  [91200/175341]\n",
      "loss: 0.832920  [92800/175341]\n",
      "loss: 0.544831  [94400/175341]\n",
      "loss: 0.504391  [96000/175341]\n",
      "loss: 0.680318  [97600/175341]\n",
      "loss: 0.316182  [99200/175341]\n",
      "loss: 0.154796  [100800/175341]\n",
      "loss: 0.791026  [102400/175341]\n",
      "loss: 0.650016  [104000/175341]\n",
      "loss: 0.423956  [105600/175341]\n",
      "loss: 1.285855  [107200/175341]\n",
      "loss: 0.227840  [108800/175341]\n",
      "loss: 0.438942  [110400/175341]\n",
      "loss: 0.218494  [112000/175341]\n",
      "loss: 0.303523  [113600/175341]\n",
      "loss: 0.108855  [115200/175341]\n",
      "loss: 0.309279  [116800/175341]\n",
      "loss: 0.421073  [118400/175341]\n",
      "loss: 0.396582  [120000/175341]\n",
      "loss: 0.214153  [121600/175341]\n",
      "loss: 0.308765  [123200/175341]\n",
      "loss: 0.400324  [124800/175341]\n",
      "loss: 0.182972  [126400/175341]\n",
      "loss: 0.369873  [128000/175341]\n",
      "loss: 0.404850  [129600/175341]\n",
      "loss: 0.275992  [131200/175341]\n",
      "loss: 0.595384  [132800/175341]\n",
      "loss: 0.468576  [134400/175341]\n",
      "loss: 0.312458  [136000/175341]\n",
      "loss: 0.218317  [137600/175341]\n",
      "loss: 0.465257  [139200/175341]\n",
      "loss: 0.444045  [140800/175341]\n",
      "loss: 0.789860  [142400/175341]\n",
      "loss: 0.628654  [144000/175341]\n",
      "loss: 0.622553  [145600/175341]\n",
      "loss: 0.421757  [147200/175341]\n",
      "loss: 0.324380  [148800/175341]\n",
      "loss: 0.383879  [150400/175341]\n",
      "loss: 0.522519  [152000/175341]\n",
      "loss: 0.719271  [153600/175341]\n",
      "loss: 0.212790  [155200/175341]\n",
      "loss: 0.206010  [156800/175341]\n",
      "loss: 0.112619  [158400/175341]\n",
      "loss: 0.508052  [160000/175341]\n",
      "loss: 0.647078  [161600/175341]\n",
      "loss: 1.061094  [163200/175341]\n",
      "loss: 0.380978  [164800/175341]\n",
      "loss: 0.332713  [166400/175341]\n",
      "loss: 0.208151  [168000/175341]\n",
      "loss: 0.290255  [169600/175341]\n",
      "loss: 0.306341  [171200/175341]\n",
      "loss: 0.385118  [172800/175341]\n",
      "loss: 0.427010  [174400/175341]\n",
      "Train Accuracy: 82.2648%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.555653, F1-score: 76.82%, Macro_F1-Score:  43.19%  \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.943590  [    0/175341]\n",
      "loss: 0.701572  [ 1600/175341]\n",
      "loss: 0.497484  [ 3200/175341]\n",
      "loss: 0.537424  [ 4800/175341]\n",
      "loss: 0.176631  [ 6400/175341]\n",
      "loss: 0.628755  [ 8000/175341]\n",
      "loss: 0.725315  [ 9600/175341]\n",
      "loss: 0.309960  [11200/175341]\n",
      "loss: 0.413825  [12800/175341]\n",
      "loss: 0.432193  [14400/175341]\n",
      "loss: 0.475943  [16000/175341]\n",
      "loss: 0.352765  [17600/175341]\n",
      "loss: 0.782405  [19200/175341]\n",
      "loss: 0.440470  [20800/175341]\n",
      "loss: 0.470005  [22400/175341]\n",
      "loss: 0.351928  [24000/175341]\n",
      "loss: 0.262385  [25600/175341]\n",
      "loss: 0.490643  [27200/175341]\n",
      "loss: 0.504338  [28800/175341]\n",
      "loss: 0.527576  [30400/175341]\n",
      "loss: 0.534129  [32000/175341]\n",
      "loss: 0.988150  [33600/175341]\n",
      "loss: 0.510621  [35200/175341]\n",
      "loss: 0.406157  [36800/175341]\n",
      "loss: 0.499865  [38400/175341]\n",
      "loss: 0.535249  [40000/175341]\n",
      "loss: 0.461831  [41600/175341]\n",
      "loss: 0.373843  [43200/175341]\n",
      "loss: 0.686157  [44800/175341]\n",
      "loss: 0.246764  [46400/175341]\n",
      "loss: 0.344049  [48000/175341]\n",
      "loss: 0.255433  [49600/175341]\n",
      "loss: 0.362560  [51200/175341]\n",
      "loss: 0.342667  [52800/175341]\n",
      "loss: 0.662333  [54400/175341]\n",
      "loss: 0.440290  [56000/175341]\n",
      "loss: 0.654903  [57600/175341]\n",
      "loss: 0.325986  [59200/175341]\n",
      "loss: 0.249996  [60800/175341]\n",
      "loss: 0.478728  [62400/175341]\n",
      "loss: 0.286410  [64000/175341]\n",
      "loss: 0.433369  [65600/175341]\n",
      "loss: 0.691502  [67200/175341]\n",
      "loss: 0.680714  [68800/175341]\n",
      "loss: 0.409764  [70400/175341]\n",
      "loss: 0.353875  [72000/175341]\n",
      "loss: 0.412015  [73600/175341]\n",
      "loss: 0.387394  [75200/175341]\n",
      "loss: 0.373168  [76800/175341]\n",
      "loss: 0.410667  [78400/175341]\n",
      "loss: 0.245992  [80000/175341]\n",
      "loss: 0.404541  [81600/175341]\n",
      "loss: 0.362479  [83200/175341]\n",
      "loss: 1.087596  [84800/175341]\n",
      "loss: 0.707153  [86400/175341]\n",
      "loss: 0.460876  [88000/175341]\n",
      "loss: 0.110464  [89600/175341]\n",
      "loss: 0.758800  [91200/175341]\n",
      "loss: 0.327137  [92800/175341]\n",
      "loss: 0.426995  [94400/175341]\n",
      "loss: 0.852540  [96000/175341]\n",
      "loss: 0.601927  [97600/175341]\n",
      "loss: 0.503527  [99200/175341]\n",
      "loss: 0.495858  [100800/175341]\n",
      "loss: 0.501211  [102400/175341]\n",
      "loss: 0.493086  [104000/175341]\n",
      "loss: 0.458419  [105600/175341]\n",
      "loss: 0.254739  [107200/175341]\n",
      "loss: 0.410685  [108800/175341]\n",
      "loss: 0.167278  [110400/175341]\n",
      "loss: 0.177068  [112000/175341]\n",
      "loss: 0.656114  [113600/175341]\n",
      "loss: 0.381701  [115200/175341]\n",
      "loss: 0.670958  [116800/175341]\n",
      "loss: 0.185268  [118400/175341]\n",
      "loss: 0.461063  [120000/175341]\n",
      "loss: 0.639951  [121600/175341]\n",
      "loss: 0.369503  [123200/175341]\n",
      "loss: 0.630697  [124800/175341]\n",
      "loss: 0.566545  [126400/175341]\n",
      "loss: 0.477997  [128000/175341]\n",
      "loss: 0.356240  [129600/175341]\n",
      "loss: 0.403414  [131200/175341]\n",
      "loss: 0.479490  [132800/175341]\n",
      "loss: 0.951868  [134400/175341]\n",
      "loss: 0.314057  [136000/175341]\n",
      "loss: 0.332912  [137600/175341]\n",
      "loss: 0.476517  [139200/175341]\n",
      "loss: 0.183037  [140800/175341]\n",
      "loss: 0.427594  [142400/175341]\n",
      "loss: 0.648583  [144000/175341]\n",
      "loss: 0.659442  [145600/175341]\n",
      "loss: 0.389907  [147200/175341]\n",
      "loss: 0.475733  [148800/175341]\n",
      "loss: 0.319467  [150400/175341]\n",
      "loss: 0.153790  [152000/175341]\n",
      "loss: 0.465256  [153600/175341]\n",
      "loss: 0.294927  [155200/175341]\n",
      "loss: 0.376653  [156800/175341]\n",
      "loss: 0.491699  [158400/175341]\n",
      "loss: 0.255668  [160000/175341]\n",
      "loss: 0.720169  [161600/175341]\n",
      "loss: 0.773869  [163200/175341]\n",
      "loss: 0.573966  [164800/175341]\n",
      "loss: 0.297417  [166400/175341]\n",
      "loss: 0.210987  [168000/175341]\n",
      "loss: 0.341230  [169600/175341]\n",
      "loss: 0.650078  [171200/175341]\n",
      "loss: 0.211165  [172800/175341]\n",
      "loss: 0.320668  [174400/175341]\n",
      "Train Accuracy: 82.3339%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.562280, F1-score: 76.46%, Macro_F1-Score:  42.43%  \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.555058  [    0/175341]\n",
      "loss: 0.277203  [ 1600/175341]\n",
      "loss: 0.356787  [ 3200/175341]\n",
      "loss: 0.377886  [ 4800/175341]\n",
      "loss: 0.091252  [ 6400/175341]\n",
      "loss: 0.437137  [ 8000/175341]\n",
      "loss: 0.643700  [ 9600/175341]\n",
      "loss: 0.265843  [11200/175341]\n",
      "loss: 0.618535  [12800/175341]\n",
      "loss: 0.985316  [14400/175341]\n",
      "loss: 0.423340  [16000/175341]\n",
      "loss: 0.561115  [17600/175341]\n",
      "loss: 0.387457  [19200/175341]\n",
      "loss: 1.436653  [20800/175341]\n",
      "loss: 0.548072  [22400/175341]\n",
      "loss: 0.620081  [24000/175341]\n",
      "loss: 0.304771  [25600/175341]\n",
      "loss: 0.374501  [27200/175341]\n",
      "loss: 0.163724  [28800/175341]\n",
      "loss: 0.346457  [30400/175341]\n",
      "loss: 0.633111  [32000/175341]\n",
      "loss: 0.346172  [33600/175341]\n",
      "loss: 0.582147  [35200/175341]\n",
      "loss: 0.414670  [36800/175341]\n",
      "loss: 0.567291  [38400/175341]\n",
      "loss: 0.352983  [40000/175341]\n",
      "loss: 0.368478  [41600/175341]\n",
      "loss: 0.465487  [43200/175341]\n",
      "loss: 0.505770  [44800/175341]\n",
      "loss: 0.401229  [46400/175341]\n",
      "loss: 0.154966  [48000/175341]\n",
      "loss: 0.239390  [49600/175341]\n",
      "loss: 0.398987  [51200/175341]\n",
      "loss: 0.406378  [52800/175341]\n",
      "loss: 0.513924  [54400/175341]\n",
      "loss: 0.206544  [56000/175341]\n",
      "loss: 0.299732  [57600/175341]\n",
      "loss: 0.225255  [59200/175341]\n",
      "loss: 0.604704  [60800/175341]\n",
      "loss: 0.442342  [62400/175341]\n",
      "loss: 1.102494  [64000/175341]\n",
      "loss: 0.702363  [65600/175341]\n",
      "loss: 0.352758  [67200/175341]\n",
      "loss: 0.176202  [68800/175341]\n",
      "loss: 0.822650  [70400/175341]\n",
      "loss: 0.776367  [72000/175341]\n",
      "loss: 0.272366  [73600/175341]\n",
      "loss: 0.826422  [75200/175341]\n",
      "loss: 0.293383  [76800/175341]\n",
      "loss: 0.374396  [78400/175341]\n",
      "loss: 0.244543  [80000/175341]\n",
      "loss: 0.406864  [81600/175341]\n",
      "loss: 0.403105  [83200/175341]\n",
      "loss: 0.400799  [84800/175341]\n",
      "loss: 0.330816  [86400/175341]\n",
      "loss: 0.488479  [88000/175341]\n",
      "loss: 0.148783  [89600/175341]\n",
      "loss: 0.670518  [91200/175341]\n",
      "loss: 0.285435  [92800/175341]\n",
      "loss: 0.310285  [94400/175341]\n",
      "loss: 0.454407  [96000/175341]\n",
      "loss: 0.354627  [97600/175341]\n",
      "loss: 0.396026  [99200/175341]\n",
      "loss: 0.926081  [100800/175341]\n",
      "loss: 0.360543  [102400/175341]\n",
      "loss: 0.343813  [104000/175341]\n",
      "loss: 0.157600  [105600/175341]\n",
      "loss: 1.005749  [107200/175341]\n",
      "loss: 0.404002  [108800/175341]\n",
      "loss: 0.571432  [110400/175341]\n",
      "loss: 0.861347  [112000/175341]\n",
      "loss: 0.746484  [113600/175341]\n",
      "loss: 0.283819  [115200/175341]\n",
      "loss: 0.559946  [116800/175341]\n",
      "loss: 0.400960  [118400/175341]\n",
      "loss: 0.472498  [120000/175341]\n",
      "loss: 0.312877  [121600/175341]\n",
      "loss: 0.451998  [123200/175341]\n",
      "loss: 0.391654  [124800/175341]\n",
      "loss: 0.703260  [126400/175341]\n",
      "loss: 0.385065  [128000/175341]\n",
      "loss: 0.143624  [129600/175341]\n",
      "loss: 0.099504  [131200/175341]\n",
      "loss: 0.233655  [132800/175341]\n",
      "loss: 0.504742  [134400/175341]\n",
      "loss: 0.313723  [136000/175341]\n",
      "loss: 0.354528  [137600/175341]\n",
      "loss: 0.750188  [139200/175341]\n",
      "loss: 0.617005  [140800/175341]\n",
      "loss: 0.409723  [142400/175341]\n",
      "loss: 0.586581  [144000/175341]\n",
      "loss: 0.866468  [145600/175341]\n",
      "loss: 0.416246  [147200/175341]\n",
      "loss: 0.421372  [148800/175341]\n",
      "loss: 0.337792  [150400/175341]\n",
      "loss: 0.496984  [152000/175341]\n",
      "loss: 0.703348  [153600/175341]\n",
      "loss: 0.377749  [155200/175341]\n",
      "loss: 0.375012  [156800/175341]\n",
      "loss: 0.910915  [158400/175341]\n",
      "loss: 0.543634  [160000/175341]\n",
      "loss: 0.410435  [161600/175341]\n",
      "loss: 0.381349  [163200/175341]\n",
      "loss: 0.414342  [164800/175341]\n",
      "loss: 0.385950  [166400/175341]\n",
      "loss: 0.389772  [168000/175341]\n",
      "loss: 0.614283  [169600/175341]\n",
      "loss: 0.659470  [171200/175341]\n",
      "loss: 1.540282  [172800/175341]\n",
      "loss: 0.613315  [174400/175341]\n",
      "Train Accuracy: 82.2922%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.559835, F1-score: 76.57%, Macro_F1-Score:  42.99%  \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.238131  [    0/175341]\n",
      "loss: 0.341252  [ 1600/175341]\n",
      "loss: 0.379514  [ 3200/175341]\n",
      "loss: 0.451621  [ 4800/175341]\n",
      "loss: 0.167828  [ 6400/175341]\n",
      "loss: 0.183079  [ 8000/175341]\n",
      "loss: 0.373847  [ 9600/175341]\n",
      "loss: 0.514530  [11200/175341]\n",
      "loss: 0.304354  [12800/175341]\n",
      "loss: 0.319196  [14400/175341]\n",
      "loss: 0.385984  [16000/175341]\n",
      "loss: 0.640502  [17600/175341]\n",
      "loss: 0.402124  [19200/175341]\n",
      "loss: 0.391074  [20800/175341]\n",
      "loss: 0.595411  [22400/175341]\n",
      "loss: 0.450527  [24000/175341]\n",
      "loss: 0.498905  [25600/175341]\n",
      "loss: 0.238528  [27200/175341]\n",
      "loss: 0.479098  [28800/175341]\n",
      "loss: 0.362144  [30400/175341]\n",
      "loss: 0.497903  [32000/175341]\n",
      "loss: 0.783661  [33600/175341]\n",
      "loss: 0.717258  [35200/175341]\n",
      "loss: 0.159559  [36800/175341]\n",
      "loss: 0.954057  [38400/175341]\n",
      "loss: 0.698539  [40000/175341]\n",
      "loss: 0.458550  [41600/175341]\n",
      "loss: 0.544082  [43200/175341]\n",
      "loss: 0.294655  [44800/175341]\n",
      "loss: 0.345233  [46400/175341]\n",
      "loss: 0.401989  [48000/175341]\n",
      "loss: 0.195436  [49600/175341]\n",
      "loss: 0.278453  [51200/175341]\n",
      "loss: 0.495195  [52800/175341]\n",
      "loss: 0.358064  [54400/175341]\n",
      "loss: 0.438054  [56000/175341]\n",
      "loss: 0.376689  [57600/175341]\n",
      "loss: 0.227303  [59200/175341]\n",
      "loss: 0.173385  [60800/175341]\n",
      "loss: 0.545984  [62400/175341]\n",
      "loss: 0.292120  [64000/175341]\n",
      "loss: 0.665530  [65600/175341]\n",
      "loss: 0.797319  [67200/175341]\n",
      "loss: 0.203018  [68800/175341]\n",
      "loss: 0.259355  [70400/175341]\n",
      "loss: 0.145338  [72000/175341]\n",
      "loss: 0.331858  [73600/175341]\n",
      "loss: 0.419917  [75200/175341]\n",
      "loss: 0.299996  [76800/175341]\n",
      "loss: 0.372883  [78400/175341]\n",
      "loss: 0.384920  [80000/175341]\n",
      "loss: 0.317313  [81600/175341]\n",
      "loss: 0.376765  [83200/175341]\n",
      "loss: 0.410477  [84800/175341]\n",
      "loss: 0.541910  [86400/175341]\n",
      "loss: 0.608843  [88000/175341]\n",
      "loss: 0.599781  [89600/175341]\n",
      "loss: 0.769542  [91200/175341]\n",
      "loss: 0.618158  [92800/175341]\n",
      "loss: 0.359803  [94400/175341]\n",
      "loss: 0.373862  [96000/175341]\n",
      "loss: 0.147109  [97600/175341]\n",
      "loss: 0.544180  [99200/175341]\n",
      "loss: 0.862367  [100800/175341]\n",
      "loss: 0.401747  [102400/175341]\n",
      "loss: 0.924470  [104000/175341]\n",
      "loss: 0.403864  [105600/175341]\n",
      "loss: 0.269150  [107200/175341]\n",
      "loss: 0.583447  [108800/175341]\n",
      "loss: 1.079599  [110400/175341]\n",
      "loss: 0.426447  [112000/175341]\n",
      "loss: 0.397199  [113600/175341]\n",
      "loss: 0.312321  [115200/175341]\n",
      "loss: 0.160168  [116800/175341]\n",
      "loss: 0.126071  [118400/175341]\n",
      "loss: 0.555947  [120000/175341]\n",
      "loss: 0.159100  [121600/175341]\n",
      "loss: 0.925411  [123200/175341]\n",
      "loss: 0.474694  [124800/175341]\n",
      "loss: 0.641782  [126400/175341]\n",
      "loss: 0.188254  [128000/175341]\n",
      "loss: 0.509722  [129600/175341]\n",
      "loss: 0.502650  [131200/175341]\n",
      "loss: 0.368270  [132800/175341]\n",
      "loss: 0.539503  [134400/175341]\n",
      "loss: 0.791797  [136000/175341]\n",
      "loss: 0.920995  [137600/175341]\n",
      "loss: 0.668861  [139200/175341]\n",
      "loss: 0.240927  [140800/175341]\n",
      "loss: 0.080716  [142400/175341]\n",
      "loss: 0.790208  [144000/175341]\n",
      "loss: 0.773757  [145600/175341]\n",
      "loss: 0.488296  [147200/175341]\n",
      "loss: 0.410482  [148800/175341]\n",
      "loss: 0.438593  [150400/175341]\n",
      "loss: 0.460265  [152000/175341]\n",
      "loss: 0.456474  [153600/175341]\n",
      "loss: 0.230457  [155200/175341]\n",
      "loss: 0.531739  [156800/175341]\n",
      "loss: 0.247348  [158400/175341]\n",
      "loss: 0.404657  [160000/175341]\n",
      "loss: 0.659396  [161600/175341]\n",
      "loss: 0.531877  [163200/175341]\n",
      "loss: 1.020277  [164800/175341]\n",
      "loss: 0.292722  [166400/175341]\n",
      "loss: 0.261676  [168000/175341]\n",
      "loss: 0.303197  [169600/175341]\n",
      "loss: 0.698919  [171200/175341]\n",
      "loss: 0.358150  [172800/175341]\n",
      "loss: 0.322440  [174400/175341]\n",
      "Train Accuracy: 82.3167%\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.534403, F1-score: 77.83%, Macro_F1-Score:  43.19%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4) #using L2 regularization\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6add252-99f2-4d32-bb41-6e558870ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noise scale is low (0.2)\n",
    "plt.plot(loss_over_train, label=\"Training Loss\")\n",
    "plt.plot(loss_over_test, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_data_y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999998"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n\u001b[0;32m      3\u001b[0m label_binarizer \u001b[38;5;241m=\u001b[39m LabelBinarizer()\u001b[38;5;241m.\u001b[39mfit(train_data_y)\n\u001b[1;32m----> 4\u001b[0m y_onehot_test \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_binarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_onehot_test\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# (n_samples, n_classes)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:363\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_is_multilabel \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_type_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe object was not fitted with multilabel input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlabel_binarize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:530\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    528\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m y_type:\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultioutput target data is not supported with label binarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of target data is not known\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c691d",
   "metadata": {},
   "source": [
    "## Feature Selection using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb313fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "xgb_model.fit(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "top_40_features = np.argsort(feature_importances)[-40:]\n",
    "\n",
    "# Print selected features\n",
    "selected_features = train_data_X.columns[top_40_features]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
