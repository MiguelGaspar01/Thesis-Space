{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpypalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_hex\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from pypalettes import get_hex\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\TESTER\\OneDrive\\Documentos\\GitHub\\Projects_Zone\")\n",
    "\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(r\"\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_testing-set.csv\"\")\n",
    "train_data = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_training-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4fc6f-5f2b-4a23-940a-c67baa5aa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[\"proto\"].unique()), len(test_data[\"proto\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed6f0e-91c3-495f-af48-05bd68d96ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = train_data.select_dtypes(include=\"number\").drop(columns=\"id\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117f765-2b1f-4a7d-be31-0fd7a920609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfa7a1-44a4-40cf-bef1-02c4bbd9265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = [\n",
    "    col for col in columns_numeric\n",
    "    if not len(train_data[col].value_counts()) < 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb8512-1c45-445b-8697-96927ba3a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca2383-4965-47ae-86d6-b03bfc13fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba62952-362d-4233-b35c-2052726b2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac6c23-b9e6-4b62-94c3-e9b7259e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 120)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09aa3d-fd42-4598-bb3e-3e2a20e3dc56",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b710-6197-484a-93e8-abd2a6587929",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb095de-80f2-4667-9666-d3dbb6d83b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141f1c2-3e0e-4494-b4c5-5bff0c88c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class specific\n",
    "\n",
    "def treat_outliers(dataframe, columns_numeric, isTest=False):\n",
    "    \"\"\"\n",
    "    Treats outliers in the dataset for the specified numeric columns.\n",
    "\n",
    "    On the training set:\n",
    "        - Identifies outliers using the IQR method for each column grouped by \"attack_cat\".\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "        - Stores the IQR bounds for each column and class.\n",
    "\n",
    "    On the test set:\n",
    "        - Uses the stored IQR bounds from training for each column and class.\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The dataset to process.\n",
    "        columns_numeric (list): List of numeric columns to focus on for outlier treatment.\n",
    "        isTest (bool): Whether the function is applied to the test set.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset with outliers treated.\n",
    "    \"\"\"\n",
    "    if not hasattr(treat_outliers, \"iqr_bounds\"):\n",
    "        treat_outliers.iqr_bounds = {}  # Store IQR bounds for training (keyed by column and class)\n",
    "\n",
    "    if isTest and not treat_outliers.iqr_bounds:\n",
    "        raise ValueError(\"IQR bounds are not set. Train the function on the training data first.\")\n",
    "\n",
    "    print(\"Treating outliers...\")\n",
    "    \n",
    "    # Ensure all numeric columns are cast to float to handle outliers consistently\n",
    "    dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "    # Process each numeric column\n",
    "    for col in columns_numeric:\n",
    "        if not isTest:\n",
    "            # Initialize dictionary for column if not already present\n",
    "            treat_outliers.iqr_bounds[col] = {}\n",
    "\n",
    "            # Group by \"attack_cat\" to calculate class-specific IQR bounds\n",
    "            for attack_cat, group in dataframe.groupby(\"attack_cat\"):\n",
    "                q1 = group[col].quantile(0.25)\n",
    "                q3 = group[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                treat_outliers.iqr_bounds[col][attack_cat] = (lower_bound, upper_bound)\n",
    "\n",
    "                # Identify outliers for this class\n",
    "                outliers = (group[col] < lower_bound) | (group[col] > upper_bound)\n",
    "\n",
    "                # Replace outliers with the class mean\n",
    "                replacement_mean = group[col].mean()\n",
    "                dataframe.loc[group[outliers].index, col] = replacement_mean\n",
    "        else:\n",
    "            # Use stored IQR bounds for test set\n",
    "            for attack_cat, bounds in treat_outliers.iqr_bounds[col].items():\n",
    "                lower_bound, upper_bound = bounds\n",
    "        \n",
    "                # Get test data rows for this attack_cat\n",
    "                class_rows = dataframe[dataframe[\"attack_cat\"] == attack_cat]\n",
    "        \n",
    "                # Identify outliers\n",
    "                outliers = (class_rows[col] < lower_bound) | (class_rows[col] > upper_bound)\n",
    "        \n",
    "                # Replace outliers with the training class mean\n",
    "                replacement_mean = dataframe[dataframe[\"attack_cat\"] == attack_cat][col].mean()\n",
    "                dataframe.loc[class_rows[outliers].index, col] = replacement_mean\n",
    "\n",
    "\n",
    "    print(\"Outlier treatment completed.\")\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed45c8-a3ba-4ffb-a065-ccfa3de5eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = treat_outliers(train_data, columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67678e7e-d83c-4d5a-91b1-63da92b1f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = treat_outliers(test_data, columns_numeric,isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba1d4f-2e0e-4a35-aa77-2c5eea7171cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcab144-8fd9-4407-b3ea-51359b1cc28b",
   "metadata": {},
   "source": [
    "A lot more quantiles are displayed now !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab53e0-79a2-453a-9792-1b758967e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "train_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23f0c7-76e5-4eac-982b-ac547a0a9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce categories in categorical columns while preserving the target column\n",
    "def reduce_categories(train_data, test_data, target_column, threshold=6, debug=False):\n",
    "    # Select categorical columns in train and test data, excluding the target column\n",
    "    train_cat = train_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    test_cat = test_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    \n",
    "    # Iterate through categorical columns\n",
    "    for feature in train_cat.columns:\n",
    "        if debug:\n",
    "            print(f\"Processing feature: {feature}\")\n",
    "            print(f\"Number of unique categories before reduction: {train_cat[feature].nunique()}\")\n",
    "            print('----------------------------------------------------')\n",
    "        \n",
    "        # Check if the number of unique categories exceeds the threshold\n",
    "        if train_cat[feature].nunique() > threshold:\n",
    "            # Identify the top categories in the train set\n",
    "            top_categories = train_data[feature].value_counts().head(threshold).index\n",
    "            \n",
    "            # Reduce train and test data to these top categories, replacing others with '-'\n",
    "            train_data[feature] = np.where(train_data[feature].isin(top_categories), train_data[feature], '-')\n",
    "            test_data[feature] = np.where(test_data[feature].isin(top_categories), test_data[feature], '-')\n",
    "    \n",
    "    # Return the modified train and test datasets\n",
    "    return train_data, test_data\n",
    "\n",
    "# Apply the function to your train and test datasets\n",
    "train_data, test_data = reduce_categories(train_data, test_data, target_column='attack_cat', threshold=6, debug=False)\n",
    "\n",
    "# Check the resulting categorical columns\n",
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "test_cat = test_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "print(\"Train categorical summary after reduction:\")\n",
    "print(train_cat.describe(include='all'))\n",
    "\n",
    "print(\"\\nTest categorical summary after reduction:\")\n",
    "print(test_cat.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9250b9-da54-49ea-823c-70d2c65101bc",
   "metadata": {},
   "source": [
    "## Normalizing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb003f-055e-441c-9e29-69e0880e29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc7eb1-09f6-4301-a811-adcf13a479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train_data.select_dtypes(include=\"number\").columns\n",
    "\n",
    "normalizer = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "r_scaler = RobustScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "train_data[numeric] = r_scaler.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = r_scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = scaler.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = normalizer.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = normalizer.transform(test_data[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f57f7-4b70-4b50-b7cb-3af10c44bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237a713-240f-4f46-82d4-0c55bdf1d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[\"state\"].unique()), len(test_data[\"state\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f098c9-1990-49e0-9d8a-1047acdeb567",
   "metadata": {},
   "source": [
    "## Treating Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3c56b-3f26-4db7-9fcf-1421a44e91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set skewness threshold (e.g., |skewness| > 1 is considered highly skewed)\n",
    "skew_threshold = 1\n",
    "\n",
    "numeric_cols = train_data.select_dtypes(include = \"number\").columns\n",
    "\n",
    "# Select numeric columns\n",
    "# Calculate skewness for numeric columns\n",
    "skewness = train_data[numeric_cols].skew()\n",
    "\n",
    "# Identify columns with skewness above the threshold\n",
    "skewed_cols = skewness[skewness.abs() > skew_threshold].index\n",
    "print(\"Skewed columns:\", skewed_cols)\n",
    "\n",
    "# Apply log transformation to skewed columns\n",
    "for col in skewed_cols:\n",
    "    # Add 1 to avoid issues with log(0)\n",
    "    train_data[col] = np.log1p(train_data[col])\n",
    "    test_data[col] = np.log1p(test_data[col])  # Apply same transformation to test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f467fa9-3cd3-4393-8b49-165534f19504",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdc915-e753-4a4a-9f19-977ce4c395c9",
   "metadata": {},
   "source": [
    "## OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09567e29-cacc-4d54-abf8-474f88fcedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "train_data_X = train_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "train_data_y = train_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "test_data_X = test_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "test_data_y = test_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = train_data_X.select_dtypes(include=\"object\").columns\n",
    "print(categorical_cols)  # Verify the categorical columns\n",
    "\n",
    "# Use OneHotEncoder with handle_unknown='ignore' to ensure consistency\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define and fit the encoder on training data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', drop='first')  # Drop first category to avoid dummy variable trap\n",
    "ct = ColumnTransformer(transformers=[('encoder', encoder, categorical_cols)], remainder='passthrough')\n",
    "\n",
    "# Fit the encoder on train data\n",
    "train_data_X_encoded = ct.fit_transform(train_data_X)\n",
    "\n",
    "# Transform the test data using the same encoder\n",
    "test_data_X_encoded = ct.transform(test_data_X)\n",
    "\n",
    "# Convert the encoded data back to DataFrame for alignment\n",
    "import pandas as pd\n",
    "train_data_X_encoded = pd.DataFrame(train_data_X_encoded, columns=ct.get_feature_names_out(), index=train_data_X.index)\n",
    "test_data_X_encoded = pd.DataFrame(test_data_X_encoded, columns=ct.get_feature_names_out(), index=test_data_X.index)\n",
    "\n",
    "# Ensure consistent columns between train and test\n",
    "train_data_X, test_data_X = train_data_X_encoded.align(test_data_X_encoded, join=\"outer\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc749ff-5d8c-42fb-bd0f-8632dc76be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, column_name in enumerate(test_data_X.columns):\n",
    "    print(index, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Ensure you work with copies of the DataFrames\n",
    "train_data_X = train_data_X.copy()\n",
    "test_data_X = test_data_X.copy()\n",
    "\n",
    "# Filter only numeric columns for scaling\n",
    "numeric_cols_train = train_data_X.select_dtypes(include=['number']).columns\n",
    "numeric_cols_test = test_data_X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Ensure numeric columns are cast to float before scaling\n",
    "train_data_X[numeric_cols_train] = train_data_X[numeric_cols_train].astype(float)\n",
    "test_data_X[numeric_cols_test] = test_data_X[numeric_cols_test].astype(float)\n",
    "\n",
    "# Initialize scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply StandardScaler to train_data\n",
    "train_data_scaled = standard_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply StandardScaler to test_data\n",
    "test_data_scaled = standard_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to train_data\n",
    "train_data_scaled = minmax_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to test_data\n",
    "test_data_scaled = minmax_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51262960-8085-401f-a7da-44c9b682258d",
   "metadata": {},
   "source": [
    "### Converting data to required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8a649-8f38-4489-a686-caa97f61f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data_X = pd.DataFrame(scaler.fit_transform(train_data_X), columns=train_data_X.columns)\n",
    "test_data_X = pd.DataFrame(scaler.transform(test_data_X), columns=test_data_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eb3fd-f663-480d-b324-df1bdc58a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = pd.DataFrame(label_encoder.fit_transform(train_data_y[\"attack_cat\"]))\n",
    "test_labels_encoded = pd.DataFrame(label_encoder.transform(test_data_y[\"attack_cat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad226e-b843-4273-82ee-9809c5d04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X.to_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_X_data.csv\", index=False)\n",
    "test_data_X.to_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_X_data.csv\", index=False)\n",
    "train_labels_encoded.to_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_y_data.csv\", index=False)\n",
    "test_labels_encoded.to_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_y_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491aeaf5-7cdd-4e18-ae14-043b15453f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_class_labels = label_encoder.classes_\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming you have a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder (if not already fitted)\n",
    "label_encoder.fit(['class1', 'class2', 'class3'])\n",
    "\n",
    "# Save the LabelEncoder object to a file\n",
    "with open(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save the original class labels to a CSV file (optional)\n",
    "original_class_labels = label_encoder.classes_\n",
    "original_class_labels.to_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\original_class_labels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
