{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c04eae1c-c343-40d8-a4a0-4eb4999ab427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83a82c1a-1495-4649-8933-55554a6c723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\n\\ntest_data_path = os.getenv(\"TEST_DATA_PATH\", \"data/UNSW_NB15_testing-set.csv\")\\ntrain_data_path = os.getenv(\"TRAIN_DATA_PATH\", \"data/UNSW_NB15_training-set.csv\")\\n\\ntest_data = pd.read_csv(test_data_path)\\ntrain_data = pd.read_csv(train_data_path)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "\n",
    "test_data_path = os.getenv(\"TEST_DATA_PATH\", \"data/UNSW_NB15_testing-set.csv\")\n",
    "train_data_path = os.getenv(\"TRAIN_DATA_PATH\", \"data/UNSW_NB15_training-set.csv\")\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "train_data = pd.read_csv(train_data_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4b8c325-66b7-446a-96bb-1c7a7a4081d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(r\"UNSW_NB15_testing-set.csv\")\n",
    "train_data = pd.read_csv(r\"UNSW_NB15_training-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "540ee571-e16a-4fb5-9196-90a5bec3c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(columns = [\"id\",\"attack_cat\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e846d0f5-56d7-4f9e-99c6-0510803f48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns = [\"id\",\"attack_cat\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dd4a003-96ee-443f-8565-af1fb09339a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59a0e620-6aaf-46e9-90d8-c11f9970e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b7c5c69-2f13-4a0c-85a9-31b766627069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class OutlierTreatmentTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the transformer.\n",
    "        \"\"\"\n",
    "        self.iqr_bounds = {}  # Store IQR bounds for each numeric column\n",
    "        self.columns_numeric = []  # Store numeric columns to process\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute IQR bounds for each numeric column.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The training data (feature matrix).\n",
    "            y (ignored): Not used, present for compatibility.\n",
    "\n",
    "        Returns:\n",
    "            self (OutlierTreatmentTransformer): The fitted transformer.\n",
    "        \"\"\"\n",
    "        print(\"Fitting outlier treatment...\")\n",
    "\n",
    "        # Identify numeric columns (excluding 'id' and columns with fewer than 3 unique values)\n",
    "        self.columns_numeric = X.select_dtypes(include=\"number\").drop(columns=\"id\", errors=\"ignore\").columns\n",
    "        self.columns_numeric = [\n",
    "            col for col in self.columns_numeric\n",
    "            if X[col].nunique() >= 3  # Ensure column has at least 3 unique values\n",
    "        ]\n",
    "\n",
    "        # Ensure all numeric columns are cast to float\n",
    "        for col in self.columns_numeric:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype(float)\n",
    "            else:\n",
    "                raise ValueError(f\"Column '{col}' not found in the input data.\")\n",
    "\n",
    "        # Process each numeric column\n",
    "        for col in self.columns_numeric:\n",
    "            # Calculate IQR bounds for the column\n",
    "            q1 = X[col].quantile(0.25)\n",
    "            q3 = X[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.iqr_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Treat outliers in the dataset using the stored IQR bounds.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The data to transform (feature matrix).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data with outliers treated.\n",
    "        \"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "\n",
    "        # Ensure all numeric columns are cast to float\n",
    "        for col in self.columns_numeric:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype(float)\n",
    "            else:\n",
    "                raise ValueError(f\"Column '{col}' not found in the input data.\")\n",
    "\n",
    "        # Process each numeric column\n",
    "        for col in self.columns_numeric:\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "\n",
    "            # Identify outliers\n",
    "            outliers = (X[col] < lower_bound) | (X[col] > upper_bound)\n",
    "\n",
    "            # Replace outliers with the column mean\n",
    "            replacement_mean = X[col].mean()\n",
    "            X.loc[outliers, col] = replacement_mean\n",
    "\n",
    "        print(\"Transformation completed.\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ff474af-5c06-498b-98ff-0ee62bfb011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CategoryPruner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=6, debug=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            threshold (int): The maximum number of categories to keep for each feature.\n",
    "            debug (bool): Whether to print debug information.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.debug = debug\n",
    "        self.top_categories = {}  # Store top categories for each feature\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Identify the top categories for each categorical feature in the training data.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The training data (feature matrix).\n",
    "            y (ignored): Not used, present for compatibility.\n",
    "\n",
    "        Returns:\n",
    "            self (CategoryPruner): The fitted transformer.\n",
    "        \"\"\"\n",
    "        # Select categorical columns\n",
    "        categorical_columns = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "        # Iterate through categorical columns\n",
    "        for feature in categorical_columns:\n",
    "            if self.debug:\n",
    "                print(f\"Processing feature: {feature}\")\n",
    "                print(f\"Number of unique categories before reduction: {X[feature].nunique()}\")\n",
    "                print('----------------------------------------------------')\n",
    "\n",
    "            # Check if the number of unique categories exceeds the threshold\n",
    "            if X[feature].nunique() > self.threshold:\n",
    "                # Identify the top categories in the training data\n",
    "                self.top_categories[feature] = X[feature].value_counts().head(self.threshold).index\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Reduce categories in the dataset to the top categories identified during fitting.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The data to transform (feature matrix).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed data with reduced categories.\n",
    "        \"\"\"\n",
    "        # Select categorical columns\n",
    "        categorical_columns = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "        # Iterate through categorical columns\n",
    "        for feature in categorical_columns:\n",
    "            if feature in self.top_categories:\n",
    "                # Reduce data to the top categories, replacing others with '-'\n",
    "                X[feature] = np.where(X[feature].isin(self.top_categories[feature]), X[feature], '-')\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4a9b651-5eea-4383-82f0-ab53f5a0bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SkewnessLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, skew_threshold=1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            skew_threshold (float): Threshold for identifying skewed columns.\n",
    "        \"\"\"\n",
    "        self.skew_threshold = skew_threshold\n",
    "        self.skewed_columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        numeric_cols = X.select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # Calculate skewness for numeric columns\n",
    "        skewness = X[numeric_cols].skew()\n",
    "\n",
    "        # Identify columns with skewness above the threshold\n",
    "        self.skewed_cols = skewness[skewness.abs() > self.skew_threshold].index\n",
    "        print(\"Skewed columns identified during fit:\", self.skewed_cols)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "\n",
    "        for col in self.skewed_cols:\n",
    "            if col in X.columns:\n",
    "                X[col] = np.log1p(X[col])  # log1p to avoid log(0)\n",
    "            else:\n",
    "                raise ValueError(f\"Column '{col}' not found in the input data.\")\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b48e2dbd-0f70-4582-b290-47b2d28dd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Create a ColumnTransformer to apply both scalers\n",
    "scaler_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('standard', StandardScaler(), numeric_cols),\n",
    "        ('minmax', MinMaxScaler(), numeric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep non-numeric columns if any\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e7cb2e4-264c-45a3-928a-b9f42fb26bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "class CategoricalColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.categorical_cols = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify categorical columns dynamically\n",
    "        self.categorical_cols = X.select_dtypes(include=\"object\").columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Return only the categorical columns\n",
    "        return X[self.categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3684c72d-eda4-4010-903e-0762dad1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through non-categorical columns unchanged\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0e30a4f-e00d-4c56-84fa-e2feef3da2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficient_kan import KAN\n",
    "\n",
    "class KANWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, architecture=[40, 20, 6, 3, 10], grid_size=3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2,\n",
    "                 epochs=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Wrapper for KAN to make it compatible with scikit-learn.\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.grid_size = grid_size\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None  # Placeholder for KAN model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the KAN model.\n",
    "        \"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)  # Assuming classification task\n",
    "\n",
    "        # Initialize KAN model\n",
    "        self.model = KAN(self.architecture, grid_size=self.grid_size, scale_noise=self.scale_noise,\n",
    "                         scale_base=self.scale_base, scale_spline=self.scale_spline)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # Change this if it's a regression task\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate predictions using the trained KAN model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The KAN model has not been trained yet.\")\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute accuracy score.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba1bdfc0-f5dc-4c19-84a8-34cc0f28b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n",
      "Transformation completed.\n",
      "Skewed columns identified during fit: Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
      "       'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
      "       'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n",
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n",
      "Transformation completed.\n",
      "Skewed columns identified during fit: Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
      "       'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
      "       'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n",
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n",
      "Transformation completed.\n",
      "Skewed columns identified during fit: Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
      "       'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
      "       'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n",
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n",
      "Transformation completed.\n",
      "Skewed columns identified during fit: Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
      "       'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
      "       'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd',\n",
      "       'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n",
      "Fitting outlier treatment...\n",
      "Fitting completed.\n",
      "Transforming data...\n",
      "Transformation completed.\n",
      "Skewed columns identified during fit: Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'dttl', 'sload',\n",
      "       'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb',\n",
      "       'dtcpb', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
      "       'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd',\n",
      "       'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"C:\\Users\\TESTER\\AppData\\Local\\Temp\\ipykernel_26904\\3885420240.py\", line 27, in fit\n    X_tensor = torch.tensor(X, dtype=torch.float32)\nTypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 28\u001b[0m\n\u001b[0;32m     22\u001b[0m full_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     23\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessing),\n\u001b[0;32m     24\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, KANWrapper())\n\u001b[0;32m     25\u001b[0m ])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform Cross-Validation\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\TESTER\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"C:\\Users\\TESTER\\AppData\\Local\\Temp\\ipykernel_26904\\3885420240.py\", line 27, in fit\n    X_tensor = torch.tensor(X, dtype=torch.float32)\nTypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Define Preprocessing Pipeline\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('outlier_treatment', OutlierTreatmentTransformer(), numeric_cols),\n",
    "        ('category_pruning', CategoryPruner(), X_train.select_dtypes(exclude=[np.number]).columns.tolist()),\n",
    "        ('skewness_log', SkewnessLogTransformer(), numeric_cols),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'), X_train.select_dtypes(exclude=[np.number]).columns.tolist()),\n",
    "        ('scaling', Pipeline([\n",
    "            ('standard', StandardScaler()),\n",
    "            ('minmax', MinMaxScaler())\n",
    "        ]), numeric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep non-transformed columns\n",
    ")\n",
    "\n",
    "# Create Full Pipeline with Model\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', KANWrapper())\n",
    "])\n",
    "\n",
    "# Perform Cross-Validation\n",
    "cv_scores = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3ba5e-71b3-4a69-80a0-f2628fc2537f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
