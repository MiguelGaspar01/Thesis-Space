{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbdef64-5462-455b-96fa-863d4b36bdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2c096b-e87b-4597-a60e-d432d11ad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = {\n",
    "    0: 20000,  # Class 0 should have 56000 samples\n",
    "    1: 20000,  # Class 1 should have 56000 samples\n",
    "    2: 30000,  # Class 2 should have 56000 samples\n",
    "    3: 34000,  # Class 3 should have 56000 samples\n",
    "    4: 30000,  # Class 4 should have 56000 samples\n",
    "    5: 41000,  # Class 5 should have 56000 samples\n",
    "    6: 56000,  # Class 6 should have 56000 samples\n",
    "    7: 30000,  # Class 7 should have 56000 samples\n",
    "    8: 20000,  # Class 8 should have 56000 samples\n",
    "    9: 10000,  # Class 9 should have 56000 samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bae611-6e41-45ec-93de-712ad2cefa5a",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45addaf4-c17e-4cb2-94b3-a5c6629a8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 6\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96863f09-6fdd-47a1-aa2a-0c75a380e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Current class distribution\n",
    "unique, counts = np.unique(train_labels_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "majority_class = 5\n",
    "majority_target_size = class_distribution[majority_class] // 3\n",
    "\n",
    "# Define the target size for the minority classes (e.g., match the majority target size)\n",
    "target_distribution = {cls: majority_target_size for cls in unique}\n",
    "\n",
    "# Step 1: Under-sample the majority class\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class: majority_target_size}, random_state=42)\n",
    "#train_data_X, train_labels_encoded = under_sampler.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "011c575f-d547-4b41-84c0-1af4ba290d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901db99f-954c-4b86-bcd8-af1845f20941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c45b32-8a1e-4e15-a328-d2da3f718461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "#ENN reduces the majority class by keeping only well separated instances\n",
    "enn = EditedNearestNeighbours(sampling_strategy = \"majority\", n_neighbors = 3)\n",
    "#train_data_X, train_data_y = enn.fit_resample(train_data_X, train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af5f4e3-48a5-4a60-b951-1cb02855acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102d84fa-c8e2-425c-84f8-25b8a6272498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy = \"minority\", random_state=42, n_neighbors= 3)\n",
    "#train_data_X,train_data_y = adasyn.fit_resample(train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da2d9fc-1ea8-4570-8c1c-073cd9455e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Combine SMOTE (oversampling) and ENN (cleaning majority class)\n",
    "#smote_enn = SMOTEENN(sampling_strategy=\"not majority\", enn = enn, random_state=42,n_neighbors = 5)\n",
    "\n",
    "# Apply SMOTEENN\n",
    "#train_data_X, train_data_y = smote_enn.fit_resample(train_data_X, train_data_y)\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "from collections import Counter\n",
    "#print(f\"Class distribution after resampling: {Counter(train_data_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52f54-b0d2-4d50-aad1-43bcbf2637fb",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486b73a0-a8b5-459b-acca-6894afffdd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(random_state=15)\n",
    "numeric_cols = train_data_X.select_dtypes(include = \"number\")\n",
    "rf_classifier.fit(numeric_cols, train_data_y.values.ravel())\n",
    "\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=numeric_cols.columns)\n",
    "\n",
    "def plot_importance(coef, name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind=\"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(feature_importances, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e9a23a-0d26-4f7f-bdc9-a59255c48132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder__proto_arp              0.002281\n",
       "encoder__proto_ospf             0.000773\n",
       "encoder__proto_sctp             0.000764\n",
       "encoder__proto_tcp              0.001968\n",
       "encoder__proto_udp              0.025074\n",
       "encoder__proto_unas             0.001264\n",
       "encoder__service_dns            0.048003\n",
       "encoder__service_ftp            0.000401\n",
       "encoder__service_ftp-data       0.000518\n",
       "encoder__service_http           0.004659\n",
       "encoder__service_smtp           0.000852\n",
       "encoder__state_CON              0.002821\n",
       "encoder__state_ECO              0.000027\n",
       "encoder__state_FIN              0.000631\n",
       "encoder__state_INT              0.007125\n",
       "encoder__state_REQ              0.000591\n",
       "encoder__state_RST              0.000016\n",
       "remainder__dur                  0.021013\n",
       "remainder__spkts                0.009779\n",
       "remainder__dpkts                0.009410\n",
       "remainder__sbytes               0.072537\n",
       "remainder__dbytes               0.022361\n",
       "remainder__rate                 0.024772\n",
       "remainder__sttl                 0.094229\n",
       "remainder__dttl                 0.012379\n",
       "remainder__sload                0.034418\n",
       "remainder__dload                0.033188\n",
       "remainder__sloss                0.007464\n",
       "remainder__dloss                0.010462\n",
       "remainder__sinpkt               0.017468\n",
       "remainder__dinpkt               0.014231\n",
       "remainder__sjit                 0.014147\n",
       "remainder__djit                 0.013969\n",
       "remainder__swin                 0.001737\n",
       "remainder__stcpb                0.007795\n",
       "remainder__dtcpb                0.008838\n",
       "remainder__dwin                 0.000471\n",
       "remainder__tcprtt               0.024287\n",
       "remainder__synack               0.014221\n",
       "remainder__ackdat               0.017959\n",
       "remainder__smean                0.051397\n",
       "remainder__dmean                0.026446\n",
       "remainder__trans_depth          0.002569\n",
       "remainder__response_body_len    0.002858\n",
       "remainder__ct_srv_src           0.034628\n",
       "remainder__ct_state_ttl         0.046522\n",
       "remainder__ct_dst_ltm           0.022535\n",
       "remainder__ct_src_dport_ltm     0.047388\n",
       "remainder__ct_dst_sport_ltm     0.047940\n",
       "remainder__ct_dst_src_ltm       0.044700\n",
       "remainder__is_ftp_login         0.000116\n",
       "remainder__ct_ftp_cmd           0.000211\n",
       "remainder__ct_flw_http_mthd     0.003114\n",
       "remainder__ct_src_ltm           0.018852\n",
       "remainder__ct_srv_dst           0.063816\n",
       "remainder__is_sm_ips_ports      0.002004\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f599f8b-d01b-4450-9d58-f774daf505af",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_features = [\n",
    "    \"sttl\",\n",
    "    \"PC1\",\n",
    "    \"dttl\",\n",
    "    \"ct_dst_src_ltm\",\n",
    "    \"ct_dst_ltm\",\n",
    "    \"ct_dst_sport_ltm\",\n",
    "    \"ct_state_ttl\",\n",
    "    \"ct_src_dport_ltm\",\n",
    "    \"ct_src_ltm\",\n",
    "    \"ct_srv_dst\",\n",
    "    \"ct_srv_src\",\n",
    "    \"ackdat\",\n",
    "    \"tcprtt\",\n",
    "    \"sbytes\",\n",
    "    \"smean\",\n",
    "    \"dload\",\n",
    "    \"rate\",\n",
    "    \"dmean\",\n",
    "    \"dur\",\n",
    "    \"PC3\",\n",
    "    \"PC10\",\n",
    "    \"dbytes\",\n",
    "    \"synack\",\n",
    "    \"PC2\",\n",
    "    \"ct_flw_http_mthd\",\n",
    "    \"trans_depth\",\n",
    "    \"PC4\",\n",
    "    \"sload\",\n",
    "    \"PC5\",\n",
    "    \"sinpkt\",\n",
    "    \"PC8\",\n",
    "    \"spkts\",\n",
    "    \"dpkts\",\n",
    "    \"PC9\",\n",
    "    \"sloss\",\n",
    "    \"sjit\",\n",
    "    \"PC7\",\n",
    "    \"dloss\",\n",
    "    \"dwin\",\n",
    "    \"swin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72805ac9-20ef-4b74-be70-1e6bdf82cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif #computes ANOVA \n",
    "from sklearn.feature_selection import SelectKBest  #Orders f statistics and selects the Kbest ones\n",
    "\n",
    "def anova_test():\n",
    "\n",
    "    \n",
    "    X_vars = train_data_X[train_data_X.select_dtypes(include='number').columns]\n",
    "    \n",
    "    y_var = train_data_y\n",
    "    \n",
    "    anova = SelectKBest(f_classif, k=40) #we choose to keep the 10 best ones, try different numbers\n",
    "    \n",
    "    \n",
    "    X_anova = anova.fit_transform(X_vars, y_var)\n",
    "    \n",
    "    anova_results = pd.DataFrame({'Feature': X_vars.columns, \n",
    "                                  'F-value': anova.scores_,\n",
    "                                  'p-value': anova.pvalues_})\n",
    "    \n",
    "    anova_results.sort_values(by='F-value', ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    selected_features = pd.Series(anova.get_support(), index = X_vars.columns)\n",
    "    features_to_use = [feature for feature, keep in selected_features.items() if keep]\n",
    "\n",
    "    features_to_use_str = '\", \"'.join(features_to_use)\n",
    "    \n",
    "    return selected_features, anova_results, features_to_use_str,features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274cf472-2ba7-457e-9b57-19cbec027843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "selected_features, anova_results, features_to_use_str,features_to_use = anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d62f5e-a44a-4930-9c89-85e4d3f84270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[features_to_use].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_labels_encoded.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,10, 4,10],grid_size = 4, spline_order = 2, scale_noise=0.1, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80abf3db-1a4d-4fd2-82fb-31725d75010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KAN([40, 5,4, 2,10],grid_size = 4, spline_order = 2, scale_noise=0.1, scale_base=0.2, scale_spline=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.321861  [    0/175341]\n",
      "loss: 2.287719  [ 1600/175341]\n",
      "loss: 2.280857  [ 3200/175341]\n",
      "loss: 2.179298  [ 4800/175341]\n",
      "loss: 2.156520  [ 6400/175341]\n",
      "loss: 2.056359  [ 8000/175341]\n",
      "loss: 2.011769  [ 9600/175341]\n",
      "loss: 1.982725  [11200/175341]\n",
      "loss: 1.840623  [12800/175341]\n",
      "loss: 1.830455  [14400/175341]\n",
      "loss: 1.692145  [16000/175341]\n",
      "loss: 1.526426  [17600/175341]\n",
      "loss: 1.398679  [19200/175341]\n",
      "loss: 1.539719  [20800/175341]\n",
      "loss: 1.638390  [22400/175341]\n",
      "loss: 1.660813  [24000/175341]\n",
      "loss: 1.495997  [25600/175341]\n",
      "loss: 1.840998  [27200/175341]\n",
      "loss: 1.707479  [28800/175341]\n",
      "loss: 1.519224  [30400/175341]\n",
      "loss: 1.470459  [32000/175341]\n",
      "loss: 1.403647  [33600/175341]\n",
      "loss: 1.109772  [35200/175341]\n",
      "loss: 1.307041  [36800/175341]\n",
      "loss: 1.423590  [38400/175341]\n",
      "loss: 1.037879  [40000/175341]\n",
      "loss: 1.325720  [41600/175341]\n",
      "loss: 1.119735  [43200/175341]\n",
      "loss: 1.045804  [44800/175341]\n",
      "loss: 1.081082  [46400/175341]\n",
      "loss: 0.773812  [48000/175341]\n",
      "loss: 1.068745  [49600/175341]\n",
      "loss: 1.162005  [51200/175341]\n",
      "loss: 1.052585  [52800/175341]\n",
      "loss: 0.939288  [54400/175341]\n",
      "loss: 0.998235  [56000/175341]\n",
      "loss: 0.987823  [57600/175341]\n",
      "loss: 1.084047  [59200/175341]\n",
      "loss: 1.288470  [60800/175341]\n",
      "loss: 1.482021  [62400/175341]\n",
      "loss: 0.775682  [64000/175341]\n",
      "loss: 1.200204  [65600/175341]\n",
      "loss: 0.769942  [67200/175341]\n",
      "loss: 0.615020  [68800/175341]\n",
      "loss: 1.354110  [70400/175341]\n",
      "loss: 0.944894  [72000/175341]\n",
      "loss: 0.648647  [73600/175341]\n",
      "loss: 0.951277  [75200/175341]\n",
      "loss: 1.079141  [76800/175341]\n",
      "loss: 1.569946  [78400/175341]\n",
      "loss: 1.307383  [80000/175341]\n",
      "loss: 0.824239  [81600/175341]\n",
      "loss: 0.644678  [83200/175341]\n",
      "loss: 1.163375  [84800/175341]\n",
      "loss: 0.960937  [86400/175341]\n",
      "loss: 0.699753  [88000/175341]\n",
      "loss: 0.916314  [89600/175341]\n",
      "loss: 0.984541  [91200/175341]\n",
      "loss: 0.346099  [92800/175341]\n",
      "loss: 1.116586  [94400/175341]\n",
      "loss: 0.948013  [96000/175341]\n",
      "loss: 0.959476  [97600/175341]\n",
      "loss: 0.371398  [99200/175341]\n",
      "loss: 0.585990  [100800/175341]\n",
      "loss: 1.333230  [102400/175341]\n",
      "loss: 0.830664  [104000/175341]\n",
      "loss: 1.201070  [105600/175341]\n",
      "loss: 1.107875  [107200/175341]\n",
      "loss: 0.842295  [108800/175341]\n",
      "loss: 0.984762  [110400/175341]\n",
      "loss: 0.509224  [112000/175341]\n",
      "loss: 0.779115  [113600/175341]\n",
      "loss: 0.645461  [115200/175341]\n",
      "loss: 0.874520  [116800/175341]\n",
      "loss: 1.106812  [118400/175341]\n",
      "loss: 0.381072  [120000/175341]\n",
      "loss: 0.856270  [121600/175341]\n",
      "loss: 0.705187  [123200/175341]\n",
      "loss: 1.328852  [124800/175341]\n",
      "loss: 0.679435  [126400/175341]\n",
      "loss: 0.836728  [128000/175341]\n",
      "loss: 0.704372  [129600/175341]\n",
      "loss: 0.578135  [131200/175341]\n",
      "loss: 0.702889  [132800/175341]\n",
      "loss: 0.457278  [134400/175341]\n",
      "loss: 0.946959  [136000/175341]\n",
      "loss: 1.083757  [137600/175341]\n",
      "loss: 0.327266  [139200/175341]\n",
      "loss: 0.517934  [140800/175341]\n",
      "loss: 0.517705  [142400/175341]\n",
      "loss: 0.594722  [144000/175341]\n",
      "loss: 1.139671  [145600/175341]\n",
      "loss: 1.029279  [147200/175341]\n",
      "loss: 0.878579  [148800/175341]\n",
      "loss: 1.184160  [150400/175341]\n",
      "loss: 0.993129  [152000/175341]\n",
      "loss: 0.878672  [153600/175341]\n",
      "loss: 0.745934  [155200/175341]\n",
      "loss: 0.529004  [156800/175341]\n",
      "loss: 0.345346  [158400/175341]\n",
      "loss: 0.874312  [160000/175341]\n",
      "loss: 0.974094  [161600/175341]\n",
      "loss: 0.739467  [163200/175341]\n",
      "loss: 0.657994  [164800/175341]\n",
      "loss: 0.648584  [166400/175341]\n",
      "loss: 0.363014  [168000/175341]\n",
      "loss: 0.761068  [169600/175341]\n",
      "loss: 0.836394  [171200/175341]\n",
      "loss: 0.723837  [172800/175341]\n",
      "loss: 1.054403  [174400/175341]\n",
      "Train Accuracy: 64.6084%\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.771001, F1-score: 68.72%, Macro_F1-Score:  26.35%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.019836  [    0/175341]\n",
      "loss: 0.513218  [ 1600/175341]\n",
      "loss: 1.023278  [ 3200/175341]\n",
      "loss: 0.815722  [ 4800/175341]\n",
      "loss: 1.019519  [ 6400/175341]\n",
      "loss: 0.861518  [ 8000/175341]\n",
      "loss: 0.565698  [ 9600/175341]\n",
      "loss: 0.919521  [11200/175341]\n",
      "loss: 0.422310  [12800/175341]\n",
      "loss: 0.530552  [14400/175341]\n",
      "loss: 0.742448  [16000/175341]\n",
      "loss: 0.448853  [17600/175341]\n",
      "loss: 0.498027  [19200/175341]\n",
      "loss: 0.507860  [20800/175341]\n",
      "loss: 0.487291  [22400/175341]\n",
      "loss: 0.998721  [24000/175341]\n",
      "loss: 0.496252  [25600/175341]\n",
      "loss: 0.845399  [27200/175341]\n",
      "loss: 0.446123  [28800/175341]\n",
      "loss: 1.346181  [30400/175341]\n",
      "loss: 0.788789  [32000/175341]\n",
      "loss: 0.897256  [33600/175341]\n",
      "loss: 0.488128  [35200/175341]\n",
      "loss: 0.273269  [36800/175341]\n",
      "loss: 1.201961  [38400/175341]\n",
      "loss: 0.593223  [40000/175341]\n",
      "loss: 0.547396  [41600/175341]\n",
      "loss: 0.404025  [43200/175341]\n",
      "loss: 0.788842  [44800/175341]\n",
      "loss: 0.556286  [46400/175341]\n",
      "loss: 1.126787  [48000/175341]\n",
      "loss: 0.852534  [49600/175341]\n",
      "loss: 0.477687  [51200/175341]\n",
      "loss: 0.561855  [52800/175341]\n",
      "loss: 0.473238  [54400/175341]\n",
      "loss: 0.814228  [56000/175341]\n",
      "loss: 0.785649  [57600/175341]\n",
      "loss: 1.080532  [59200/175341]\n",
      "loss: 0.915118  [60800/175341]\n",
      "loss: 1.095720  [62400/175341]\n",
      "loss: 0.661864  [64000/175341]\n",
      "loss: 0.681082  [65600/175341]\n",
      "loss: 0.737081  [67200/175341]\n",
      "loss: 0.445820  [68800/175341]\n",
      "loss: 1.132033  [70400/175341]\n",
      "loss: 0.738105  [72000/175341]\n",
      "loss: 0.458540  [73600/175341]\n",
      "loss: 0.714234  [75200/175341]\n",
      "loss: 1.108133  [76800/175341]\n",
      "loss: 0.823127  [78400/175341]\n",
      "loss: 1.288044  [80000/175341]\n",
      "loss: 0.438523  [81600/175341]\n",
      "loss: 1.241183  [83200/175341]\n",
      "loss: 0.688479  [84800/175341]\n",
      "loss: 0.561899  [86400/175341]\n",
      "loss: 0.697008  [88000/175341]\n",
      "loss: 0.294661  [89600/175341]\n",
      "loss: 0.791865  [91200/175341]\n",
      "loss: 0.837995  [92800/175341]\n",
      "loss: 0.958932  [94400/175341]\n",
      "loss: 0.571650  [96000/175341]\n",
      "loss: 0.620170  [97600/175341]\n",
      "loss: 0.434109  [99200/175341]\n",
      "loss: 0.626322  [100800/175341]\n",
      "loss: 0.627915  [102400/175341]\n",
      "loss: 0.313767  [104000/175341]\n",
      "loss: 0.454829  [105600/175341]\n",
      "loss: 0.682746  [107200/175341]\n",
      "loss: 0.302912  [108800/175341]\n",
      "loss: 0.749275  [110400/175341]\n",
      "loss: 0.481515  [112000/175341]\n",
      "loss: 0.505038  [113600/175341]\n",
      "loss: 1.197983  [115200/175341]\n",
      "loss: 0.430296  [116800/175341]\n",
      "loss: 0.655879  [118400/175341]\n",
      "loss: 0.499136  [120000/175341]\n",
      "loss: 0.408130  [121600/175341]\n",
      "loss: 0.587426  [123200/175341]\n",
      "loss: 0.202544  [124800/175341]\n",
      "loss: 0.441701  [126400/175341]\n",
      "loss: 0.888198  [128000/175341]\n",
      "loss: 0.781201  [129600/175341]\n",
      "loss: 0.425290  [131200/175341]\n",
      "loss: 0.695611  [132800/175341]\n",
      "loss: 0.728746  [134400/175341]\n",
      "loss: 0.551547  [136000/175341]\n",
      "loss: 0.742859  [137600/175341]\n",
      "loss: 0.852040  [139200/175341]\n",
      "loss: 0.807979  [140800/175341]\n",
      "loss: 0.547306  [142400/175341]\n",
      "loss: 0.418516  [144000/175341]\n",
      "loss: 0.595656  [145600/175341]\n",
      "loss: 0.482200  [147200/175341]\n",
      "loss: 0.426329  [148800/175341]\n",
      "loss: 0.688377  [150400/175341]\n",
      "loss: 0.907668  [152000/175341]\n",
      "loss: 0.628991  [153600/175341]\n",
      "loss: 0.491413  [155200/175341]\n",
      "loss: 0.652973  [156800/175341]\n",
      "loss: 0.881435  [158400/175341]\n",
      "loss: 0.777660  [160000/175341]\n",
      "loss: 0.376539  [161600/175341]\n",
      "loss: 0.559458  [163200/175341]\n",
      "loss: 0.681344  [164800/175341]\n",
      "loss: 0.416148  [166400/175341]\n",
      "loss: 0.637165  [168000/175341]\n",
      "loss: 0.680397  [169600/175341]\n",
      "loss: 0.821778  [171200/175341]\n",
      "loss: 0.717304  [172800/175341]\n",
      "loss: 0.835049  [174400/175341]\n",
      "Train Accuracy: 75.1262%\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.711236, F1-score: 69.87%, Macro_F1-Score:  31.82%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.967825  [    0/175341]\n",
      "loss: 0.355384  [ 1600/175341]\n",
      "loss: 0.397606  [ 3200/175341]\n",
      "loss: 0.512436  [ 4800/175341]\n",
      "loss: 0.847877  [ 6400/175341]\n",
      "loss: 0.425965  [ 8000/175341]\n",
      "loss: 0.408636  [ 9600/175341]\n",
      "loss: 0.833856  [11200/175341]\n",
      "loss: 0.584560  [12800/175341]\n",
      "loss: 0.968453  [14400/175341]\n",
      "loss: 1.017195  [16000/175341]\n",
      "loss: 0.723019  [17600/175341]\n",
      "loss: 0.727596  [19200/175341]\n",
      "loss: 0.859256  [20800/175341]\n",
      "loss: 0.921876  [22400/175341]\n",
      "loss: 0.921401  [24000/175341]\n",
      "loss: 0.681460  [25600/175341]\n",
      "loss: 0.591739  [27200/175341]\n",
      "loss: 0.512301  [28800/175341]\n",
      "loss: 0.344453  [30400/175341]\n",
      "loss: 0.810862  [32000/175341]\n",
      "loss: 0.643479  [33600/175341]\n",
      "loss: 0.477070  [35200/175341]\n",
      "loss: 0.623442  [36800/175341]\n",
      "loss: 0.995997  [38400/175341]\n",
      "loss: 1.038901  [40000/175341]\n",
      "loss: 0.642761  [41600/175341]\n",
      "loss: 0.393840  [43200/175341]\n",
      "loss: 0.434242  [44800/175341]\n",
      "loss: 0.451012  [46400/175341]\n",
      "loss: 0.834885  [48000/175341]\n",
      "loss: 0.465881  [49600/175341]\n",
      "loss: 0.903177  [51200/175341]\n",
      "loss: 0.581849  [52800/175341]\n",
      "loss: 0.487479  [54400/175341]\n",
      "loss: 0.968385  [56000/175341]\n",
      "loss: 0.974950  [57600/175341]\n",
      "loss: 0.718629  [59200/175341]\n",
      "loss: 0.760296  [60800/175341]\n",
      "loss: 0.366623  [62400/175341]\n",
      "loss: 0.520064  [64000/175341]\n",
      "loss: 0.283762  [65600/175341]\n",
      "loss: 0.934317  [67200/175341]\n",
      "loss: 0.589854  [68800/175341]\n",
      "loss: 0.365466  [70400/175341]\n",
      "loss: 0.547004  [72000/175341]\n",
      "loss: 0.425345  [73600/175341]\n",
      "loss: 0.518809  [75200/175341]\n",
      "loss: 0.495548  [76800/175341]\n",
      "loss: 0.480120  [78400/175341]\n",
      "loss: 0.408754  [80000/175341]\n",
      "loss: 0.462952  [81600/175341]\n",
      "loss: 0.923405  [83200/175341]\n",
      "loss: 0.643519  [84800/175341]\n",
      "loss: 0.338786  [86400/175341]\n",
      "loss: 0.419502  [88000/175341]\n",
      "loss: 0.448167  [89600/175341]\n",
      "loss: 0.559199  [91200/175341]\n",
      "loss: 0.998313  [92800/175341]\n",
      "loss: 0.962785  [94400/175341]\n",
      "loss: 0.733977  [96000/175341]\n",
      "loss: 0.723473  [97600/175341]\n",
      "loss: 0.310239  [99200/175341]\n",
      "loss: 1.128760  [100800/175341]\n",
      "loss: 0.450394  [102400/175341]\n",
      "loss: 0.485273  [104000/175341]\n",
      "loss: 0.661543  [105600/175341]\n",
      "loss: 0.345070  [107200/175341]\n",
      "loss: 1.015448  [108800/175341]\n",
      "loss: 1.877501  [110400/175341]\n",
      "loss: 0.458780  [112000/175341]\n",
      "loss: 0.705890  [113600/175341]\n",
      "loss: 0.566817  [115200/175341]\n",
      "loss: 0.447579  [116800/175341]\n",
      "loss: 0.602298  [118400/175341]\n",
      "loss: 0.399327  [120000/175341]\n",
      "loss: 0.848634  [121600/175341]\n",
      "loss: 0.624954  [123200/175341]\n",
      "loss: 0.648670  [124800/175341]\n",
      "loss: 0.585008  [126400/175341]\n",
      "loss: 0.334216  [128000/175341]\n",
      "loss: 0.605779  [129600/175341]\n",
      "loss: 0.188076  [131200/175341]\n",
      "loss: 0.745077  [132800/175341]\n",
      "loss: 0.569722  [134400/175341]\n",
      "loss: 0.251360  [136000/175341]\n",
      "loss: 0.759520  [137600/175341]\n",
      "loss: 0.210929  [139200/175341]\n",
      "loss: 0.337242  [140800/175341]\n",
      "loss: 0.392888  [142400/175341]\n",
      "loss: 0.482956  [144000/175341]\n",
      "loss: 0.859627  [145600/175341]\n",
      "loss: 0.483723  [147200/175341]\n",
      "loss: 0.484597  [148800/175341]\n",
      "loss: 0.518354  [150400/175341]\n",
      "loss: 0.411583  [152000/175341]\n",
      "loss: 0.669396  [153600/175341]\n",
      "loss: 0.633270  [155200/175341]\n",
      "loss: 0.532892  [156800/175341]\n",
      "loss: 0.357976  [158400/175341]\n",
      "loss: 0.375476  [160000/175341]\n",
      "loss: 0.470625  [161600/175341]\n",
      "loss: 0.736451  [163200/175341]\n",
      "loss: 0.553272  [164800/175341]\n",
      "loss: 0.562513  [166400/175341]\n",
      "loss: 0.691166  [168000/175341]\n",
      "loss: 0.361492  [169600/175341]\n",
      "loss: 0.496383  [171200/175341]\n",
      "loss: 0.611006  [172800/175341]\n",
      "loss: 0.430084  [174400/175341]\n",
      "Train Accuracy: 76.4995%\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.685279, F1-score: 70.39%, Macro_F1-Score:  32.34%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.570721  [    0/175341]\n",
      "loss: 0.533193  [ 1600/175341]\n",
      "loss: 0.355521  [ 3200/175341]\n",
      "loss: 0.624411  [ 4800/175341]\n",
      "loss: 0.318620  [ 6400/175341]\n",
      "loss: 0.520891  [ 8000/175341]\n",
      "loss: 0.354348  [ 9600/175341]\n",
      "loss: 0.460201  [11200/175341]\n",
      "loss: 0.438762  [12800/175341]\n",
      "loss: 1.104442  [14400/175341]\n",
      "loss: 0.656331  [16000/175341]\n",
      "loss: 0.602913  [17600/175341]\n",
      "loss: 0.227893  [19200/175341]\n",
      "loss: 0.626884  [20800/175341]\n",
      "loss: 0.775196  [22400/175341]\n",
      "loss: 0.628765  [24000/175341]\n",
      "loss: 0.903085  [25600/175341]\n",
      "loss: 0.338755  [27200/175341]\n",
      "loss: 0.393198  [28800/175341]\n",
      "loss: 0.489203  [30400/175341]\n",
      "loss: 0.874393  [32000/175341]\n",
      "loss: 0.226344  [33600/175341]\n",
      "loss: 0.480452  [35200/175341]\n",
      "loss: 0.443135  [36800/175341]\n",
      "loss: 0.436912  [38400/175341]\n",
      "loss: 0.636645  [40000/175341]\n",
      "loss: 0.432784  [41600/175341]\n",
      "loss: 0.994600  [43200/175341]\n",
      "loss: 0.248676  [44800/175341]\n",
      "loss: 0.406423  [46400/175341]\n",
      "loss: 0.442569  [48000/175341]\n",
      "loss: 0.932238  [49600/175341]\n",
      "loss: 0.584084  [51200/175341]\n",
      "loss: 0.358874  [52800/175341]\n",
      "loss: 0.376008  [54400/175341]\n",
      "loss: 0.433516  [56000/175341]\n",
      "loss: 0.443255  [57600/175341]\n",
      "loss: 0.556000  [59200/175341]\n",
      "loss: 0.900685  [60800/175341]\n",
      "loss: 0.487181  [62400/175341]\n",
      "loss: 0.841792  [64000/175341]\n",
      "loss: 0.540883  [65600/175341]\n",
      "loss: 0.592718  [67200/175341]\n",
      "loss: 1.036740  [68800/175341]\n",
      "loss: 0.965799  [70400/175341]\n",
      "loss: 0.472789  [72000/175341]\n",
      "loss: 0.715057  [73600/175341]\n",
      "loss: 0.417283  [75200/175341]\n",
      "loss: 0.730029  [76800/175341]\n",
      "loss: 0.796277  [78400/175341]\n",
      "loss: 1.110435  [80000/175341]\n",
      "loss: 0.580601  [81600/175341]\n",
      "loss: 0.332063  [83200/175341]\n",
      "loss: 0.857264  [84800/175341]\n",
      "loss: 0.352702  [86400/175341]\n",
      "loss: 0.628017  [88000/175341]\n",
      "loss: 0.560547  [89600/175341]\n",
      "loss: 1.055433  [91200/175341]\n",
      "loss: 0.289238  [92800/175341]\n",
      "loss: 0.573515  [94400/175341]\n",
      "loss: 0.353187  [96000/175341]\n",
      "loss: 0.276964  [97600/175341]\n",
      "loss: 0.437917  [99200/175341]\n",
      "loss: 0.452900  [100800/175341]\n",
      "loss: 0.730360  [102400/175341]\n",
      "loss: 0.263326  [104000/175341]\n",
      "loss: 0.655552  [105600/175341]\n",
      "loss: 0.486604  [107200/175341]\n",
      "loss: 0.914913  [108800/175341]\n",
      "loss: 0.282185  [110400/175341]\n",
      "loss: 0.878998  [112000/175341]\n",
      "loss: 0.190181  [113600/175341]\n",
      "loss: 0.361882  [115200/175341]\n",
      "loss: 0.982401  [116800/175341]\n",
      "loss: 0.892113  [118400/175341]\n",
      "loss: 0.681988  [120000/175341]\n",
      "loss: 0.598145  [121600/175341]\n",
      "loss: 0.460212  [123200/175341]\n",
      "loss: 0.565764  [124800/175341]\n",
      "loss: 0.842332  [126400/175341]\n",
      "loss: 0.504945  [128000/175341]\n",
      "loss: 0.636058  [129600/175341]\n",
      "loss: 0.580154  [131200/175341]\n",
      "loss: 0.439827  [132800/175341]\n",
      "loss: 0.568794  [134400/175341]\n",
      "loss: 0.821760  [136000/175341]\n",
      "loss: 0.432430  [137600/175341]\n",
      "loss: 0.599723  [139200/175341]\n",
      "loss: 0.769365  [140800/175341]\n",
      "loss: 0.688380  [142400/175341]\n",
      "loss: 0.586008  [144000/175341]\n",
      "loss: 0.428755  [145600/175341]\n",
      "loss: 0.684033  [147200/175341]\n",
      "loss: 0.501860  [148800/175341]\n",
      "loss: 0.677959  [150400/175341]\n",
      "loss: 0.110985  [152000/175341]\n",
      "loss: 0.352540  [153600/175341]\n",
      "loss: 0.517442  [155200/175341]\n",
      "loss: 0.568645  [156800/175341]\n",
      "loss: 0.412287  [158400/175341]\n",
      "loss: 0.370122  [160000/175341]\n",
      "loss: 0.832443  [161600/175341]\n",
      "loss: 0.935244  [163200/175341]\n",
      "loss: 0.783169  [164800/175341]\n",
      "loss: 0.551859  [166400/175341]\n",
      "loss: 0.294909  [168000/175341]\n",
      "loss: 0.818024  [169600/175341]\n",
      "loss: 0.618003  [171200/175341]\n",
      "loss: 0.786102  [172800/175341]\n",
      "loss: 0.641811  [174400/175341]\n",
      "Train Accuracy: 77.1018%\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.673197, F1-score: 70.23%, Macro_F1-Score:  32.29%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.768574  [    0/175341]\n",
      "loss: 0.586677  [ 1600/175341]\n",
      "loss: 0.683338  [ 3200/175341]\n",
      "loss: 0.455198  [ 4800/175341]\n",
      "loss: 0.878384  [ 6400/175341]\n",
      "loss: 0.526130  [ 8000/175341]\n",
      "loss: 0.472410  [ 9600/175341]\n",
      "loss: 0.884864  [11200/175341]\n",
      "loss: 0.306982  [12800/175341]\n",
      "loss: 0.575125  [14400/175341]\n",
      "loss: 0.052149  [16000/175341]\n",
      "loss: 0.794142  [17600/175341]\n",
      "loss: 0.972705  [19200/175341]\n",
      "loss: 1.099782  [20800/175341]\n",
      "loss: 0.496282  [22400/175341]\n",
      "loss: 0.531339  [24000/175341]\n",
      "loss: 0.371902  [25600/175341]\n",
      "loss: 0.678564  [27200/175341]\n",
      "loss: 0.627024  [28800/175341]\n",
      "loss: 0.727215  [30400/175341]\n",
      "loss: 0.864471  [32000/175341]\n",
      "loss: 0.625413  [33600/175341]\n",
      "loss: 0.427936  [35200/175341]\n",
      "loss: 0.293837  [36800/175341]\n",
      "loss: 0.653330  [38400/175341]\n",
      "loss: 0.384536  [40000/175341]\n",
      "loss: 0.532781  [41600/175341]\n",
      "loss: 0.905962  [43200/175341]\n",
      "loss: 0.679925  [44800/175341]\n",
      "loss: 0.071728  [46400/175341]\n",
      "loss: 0.659548  [48000/175341]\n",
      "loss: 0.872159  [49600/175341]\n",
      "loss: 0.625546  [51200/175341]\n",
      "loss: 0.345127  [52800/175341]\n",
      "loss: 0.401553  [54400/175341]\n",
      "loss: 0.958691  [56000/175341]\n",
      "loss: 0.962942  [57600/175341]\n",
      "loss: 0.383205  [59200/175341]\n",
      "loss: 0.336989  [60800/175341]\n",
      "loss: 0.532048  [62400/175341]\n",
      "loss: 0.860393  [64000/175341]\n",
      "loss: 0.578693  [65600/175341]\n",
      "loss: 0.573277  [67200/175341]\n",
      "loss: 0.294466  [68800/175341]\n",
      "loss: 0.225350  [70400/175341]\n",
      "loss: 0.671893  [72000/175341]\n",
      "loss: 0.562441  [73600/175341]\n",
      "loss: 1.241138  [75200/175341]\n",
      "loss: 0.734735  [76800/175341]\n",
      "loss: 0.594414  [78400/175341]\n",
      "loss: 0.739343  [80000/175341]\n",
      "loss: 0.402382  [81600/175341]\n",
      "loss: 0.385837  [83200/175341]\n",
      "loss: 0.540693  [84800/175341]\n",
      "loss: 0.580932  [86400/175341]\n",
      "loss: 0.194705  [88000/175341]\n",
      "loss: 0.465926  [89600/175341]\n",
      "loss: 0.800143  [91200/175341]\n",
      "loss: 0.648643  [92800/175341]\n",
      "loss: 0.623631  [94400/175341]\n",
      "loss: 0.212751  [96000/175341]\n",
      "loss: 0.938738  [97600/175341]\n",
      "loss: 0.540345  [99200/175341]\n",
      "loss: 0.511641  [100800/175341]\n",
      "loss: 0.467638  [102400/175341]\n",
      "loss: 0.529045  [104000/175341]\n",
      "loss: 0.551070  [105600/175341]\n",
      "loss: 0.663386  [107200/175341]\n",
      "loss: 0.659856  [108800/175341]\n",
      "loss: 0.599218  [110400/175341]\n",
      "loss: 0.643857  [112000/175341]\n",
      "loss: 0.668712  [113600/175341]\n",
      "loss: 0.448679  [115200/175341]\n",
      "loss: 0.479638  [116800/175341]\n",
      "loss: 0.655249  [118400/175341]\n",
      "loss: 0.810046  [120000/175341]\n",
      "loss: 0.957880  [121600/175341]\n",
      "loss: 0.730751  [123200/175341]\n",
      "loss: 0.714966  [124800/175341]\n",
      "loss: 0.439905  [126400/175341]\n",
      "loss: 1.192004  [128000/175341]\n",
      "loss: 0.686021  [129600/175341]\n",
      "loss: 0.649987  [131200/175341]\n",
      "loss: 0.862881  [132800/175341]\n",
      "loss: 0.356725  [134400/175341]\n",
      "loss: 0.513245  [136000/175341]\n",
      "loss: 0.579915  [137600/175341]\n",
      "loss: 0.499389  [139200/175341]\n",
      "loss: 0.798096  [140800/175341]\n",
      "loss: 0.581990  [142400/175341]\n",
      "loss: 0.798789  [144000/175341]\n",
      "loss: 0.152611  [145600/175341]\n",
      "loss: 0.454745  [147200/175341]\n",
      "loss: 0.824713  [148800/175341]\n",
      "loss: 0.890761  [150400/175341]\n",
      "loss: 0.646414  [152000/175341]\n",
      "loss: 0.262096  [153600/175341]\n",
      "loss: 0.459758  [155200/175341]\n",
      "loss: 0.932059  [156800/175341]\n",
      "loss: 0.575688  [158400/175341]\n",
      "loss: 0.792120  [160000/175341]\n",
      "loss: 0.490880  [161600/175341]\n",
      "loss: 0.537584  [163200/175341]\n",
      "loss: 0.455633  [164800/175341]\n",
      "loss: 0.525698  [166400/175341]\n",
      "loss: 0.538688  [168000/175341]\n",
      "loss: 0.897991  [169600/175341]\n",
      "loss: 0.670444  [171200/175341]\n",
      "loss: 0.980050  [172800/175341]\n",
      "loss: 0.547523  [174400/175341]\n",
      "Train Accuracy: 77.4143%\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.659706, F1-score: 70.67%, Macro_F1-Score:  32.61%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.442581  [    0/175341]\n",
      "loss: 0.802300  [ 1600/175341]\n",
      "loss: 0.292977  [ 3200/175341]\n",
      "loss: 0.135367  [ 4800/175341]\n",
      "loss: 0.378209  [ 6400/175341]\n",
      "loss: 0.467781  [ 8000/175341]\n",
      "loss: 0.292848  [ 9600/175341]\n",
      "loss: 0.276643  [11200/175341]\n",
      "loss: 0.301988  [12800/175341]\n",
      "loss: 0.715886  [14400/175341]\n",
      "loss: 0.492049  [16000/175341]\n",
      "loss: 0.198132  [17600/175341]\n",
      "loss: 0.812277  [19200/175341]\n",
      "loss: 0.611679  [20800/175341]\n",
      "loss: 0.972329  [22400/175341]\n",
      "loss: 0.909051  [24000/175341]\n",
      "loss: 0.433250  [25600/175341]\n",
      "loss: 0.552311  [27200/175341]\n",
      "loss: 0.664881  [28800/175341]\n",
      "loss: 0.918334  [30400/175341]\n",
      "loss: 0.496482  [32000/175341]\n",
      "loss: 0.933791  [33600/175341]\n",
      "loss: 0.422659  [35200/175341]\n",
      "loss: 0.552056  [36800/175341]\n",
      "loss: 0.520775  [38400/175341]\n",
      "loss: 0.672792  [40000/175341]\n",
      "loss: 0.522547  [41600/175341]\n",
      "loss: 0.770935  [43200/175341]\n",
      "loss: 0.565696  [44800/175341]\n",
      "loss: 1.109427  [46400/175341]\n",
      "loss: 0.427756  [48000/175341]\n",
      "loss: 0.642147  [49600/175341]\n",
      "loss: 0.868721  [51200/175341]\n",
      "loss: 0.499443  [52800/175341]\n",
      "loss: 0.724208  [54400/175341]\n",
      "loss: 0.890359  [56000/175341]\n",
      "loss: 0.418176  [57600/175341]\n",
      "loss: 0.343266  [59200/175341]\n",
      "loss: 0.395732  [60800/175341]\n",
      "loss: 0.274654  [62400/175341]\n",
      "loss: 0.658579  [64000/175341]\n",
      "loss: 0.611396  [65600/175341]\n",
      "loss: 0.600371  [67200/175341]\n",
      "loss: 0.460301  [68800/175341]\n",
      "loss: 0.775973  [70400/175341]\n",
      "loss: 0.684354  [72000/175341]\n",
      "loss: 0.553574  [73600/175341]\n",
      "loss: 0.961302  [75200/175341]\n",
      "loss: 0.312643  [76800/175341]\n",
      "loss: 0.470488  [78400/175341]\n",
      "loss: 0.353566  [80000/175341]\n",
      "loss: 0.319825  [81600/175341]\n",
      "loss: 0.590853  [83200/175341]\n",
      "loss: 0.592886  [84800/175341]\n",
      "loss: 0.691246  [86400/175341]\n",
      "loss: 0.354260  [88000/175341]\n",
      "loss: 0.701553  [89600/175341]\n",
      "loss: 0.634697  [91200/175341]\n",
      "loss: 0.355889  [92800/175341]\n",
      "loss: 0.528463  [94400/175341]\n",
      "loss: 0.207347  [96000/175341]\n",
      "loss: 0.371314  [97600/175341]\n",
      "loss: 0.554212  [99200/175341]\n",
      "loss: 0.624293  [100800/175341]\n",
      "loss: 0.929646  [102400/175341]\n",
      "loss: 1.113899  [104000/175341]\n",
      "loss: 0.635081  [105600/175341]\n",
      "loss: 0.468459  [107200/175341]\n",
      "loss: 0.524168  [108800/175341]\n",
      "loss: 0.712769  [110400/175341]\n",
      "loss: 0.726669  [112000/175341]\n",
      "loss: 0.639151  [113600/175341]\n",
      "loss: 0.462795  [115200/175341]\n",
      "loss: 0.868867  [116800/175341]\n",
      "loss: 0.188813  [118400/175341]\n",
      "loss: 0.726831  [120000/175341]\n",
      "loss: 0.397194  [121600/175341]\n",
      "loss: 0.302495  [123200/175341]\n",
      "loss: 0.405966  [124800/175341]\n",
      "loss: 0.822319  [126400/175341]\n",
      "loss: 0.271898  [128000/175341]\n",
      "loss: 0.599004  [129600/175341]\n",
      "loss: 0.646636  [131200/175341]\n",
      "loss: 0.850165  [132800/175341]\n",
      "loss: 0.586087  [134400/175341]\n",
      "loss: 0.378850  [136000/175341]\n",
      "loss: 0.929093  [137600/175341]\n",
      "loss: 0.479998  [139200/175341]\n",
      "loss: 0.482040  [140800/175341]\n",
      "loss: 0.572598  [142400/175341]\n",
      "loss: 0.264836  [144000/175341]\n",
      "loss: 0.941748  [145600/175341]\n",
      "loss: 0.462016  [147200/175341]\n",
      "loss: 0.290638  [148800/175341]\n",
      "loss: 0.420101  [150400/175341]\n",
      "loss: 0.455268  [152000/175341]\n",
      "loss: 0.574245  [153600/175341]\n",
      "loss: 0.497360  [155200/175341]\n",
      "loss: 0.340209  [156800/175341]\n",
      "loss: 0.381505  [158400/175341]\n",
      "loss: 0.824801  [160000/175341]\n",
      "loss: 0.393405  [161600/175341]\n",
      "loss: 0.902288  [163200/175341]\n",
      "loss: 0.594478  [164800/175341]\n",
      "loss: 0.650596  [166400/175341]\n",
      "loss: 0.489920  [168000/175341]\n",
      "loss: 0.571483  [169600/175341]\n",
      "loss: 1.192779  [171200/175341]\n",
      "loss: 0.573496  [172800/175341]\n",
      "loss: 0.579233  [174400/175341]\n",
      "Train Accuracy: 77.6379%\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.664077, F1-score: 69.98%, Macro_F1-Score:  32.72%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.854784  [    0/175341]\n",
      "loss: 0.421537  [ 1600/175341]\n",
      "loss: 0.684662  [ 3200/175341]\n",
      "loss: 0.191501  [ 4800/175341]\n",
      "loss: 0.904504  [ 6400/175341]\n",
      "loss: 0.253533  [ 8000/175341]\n",
      "loss: 0.589743  [ 9600/175341]\n",
      "loss: 0.594530  [11200/175341]\n",
      "loss: 0.766846  [12800/175341]\n",
      "loss: 0.409377  [14400/175341]\n",
      "loss: 0.636940  [16000/175341]\n",
      "loss: 0.559131  [17600/175341]\n",
      "loss: 0.559910  [19200/175341]\n",
      "loss: 0.954565  [20800/175341]\n",
      "loss: 0.896941  [22400/175341]\n",
      "loss: 0.888567  [24000/175341]\n",
      "loss: 0.389744  [25600/175341]\n",
      "loss: 0.548781  [27200/175341]\n",
      "loss: 0.526298  [28800/175341]\n",
      "loss: 0.613261  [30400/175341]\n",
      "loss: 0.895233  [32000/175341]\n",
      "loss: 0.487857  [33600/175341]\n",
      "loss: 0.331931  [35200/175341]\n",
      "loss: 0.435779  [36800/175341]\n",
      "loss: 0.727056  [38400/175341]\n",
      "loss: 0.577675  [40000/175341]\n",
      "loss: 0.656693  [41600/175341]\n",
      "loss: 0.354617  [43200/175341]\n",
      "loss: 0.799413  [44800/175341]\n",
      "loss: 0.597384  [46400/175341]\n",
      "loss: 0.815277  [48000/175341]\n",
      "loss: 0.909021  [49600/175341]\n",
      "loss: 0.402588  [51200/175341]\n",
      "loss: 0.969760  [52800/175341]\n",
      "loss: 0.671860  [54400/175341]\n",
      "loss: 0.702100  [56000/175341]\n",
      "loss: 0.565651  [57600/175341]\n",
      "loss: 0.451628  [59200/175341]\n",
      "loss: 0.724413  [60800/175341]\n",
      "loss: 0.484945  [62400/175341]\n",
      "loss: 0.695075  [64000/175341]\n",
      "loss: 1.088310  [65600/175341]\n",
      "loss: 0.697460  [67200/175341]\n",
      "loss: 0.573083  [68800/175341]\n",
      "loss: 0.275820  [70400/175341]\n",
      "loss: 0.751191  [72000/175341]\n",
      "loss: 0.420773  [73600/175341]\n",
      "loss: 0.540965  [75200/175341]\n",
      "loss: 0.613622  [76800/175341]\n",
      "loss: 0.645308  [78400/175341]\n",
      "loss: 0.261911  [80000/175341]\n",
      "loss: 0.514300  [81600/175341]\n",
      "loss: 0.307166  [83200/175341]\n",
      "loss: 0.346603  [84800/175341]\n",
      "loss: 0.378083  [86400/175341]\n",
      "loss: 0.430626  [88000/175341]\n",
      "loss: 0.785512  [89600/175341]\n",
      "loss: 0.435867  [91200/175341]\n",
      "loss: 0.639286  [92800/175341]\n",
      "loss: 1.081372  [94400/175341]\n",
      "loss: 0.738210  [96000/175341]\n",
      "loss: 0.790911  [97600/175341]\n",
      "loss: 0.458432  [99200/175341]\n",
      "loss: 0.521101  [100800/175341]\n",
      "loss: 0.502365  [102400/175341]\n",
      "loss: 0.863350  [104000/175341]\n",
      "loss: 0.209357  [105600/175341]\n",
      "loss: 0.425234  [107200/175341]\n",
      "loss: 0.876544  [108800/175341]\n",
      "loss: 0.396370  [110400/175341]\n",
      "loss: 0.298897  [112000/175341]\n",
      "loss: 0.438527  [113600/175341]\n",
      "loss: 0.497679  [115200/175341]\n",
      "loss: 0.478131  [116800/175341]\n",
      "loss: 0.236428  [118400/175341]\n",
      "loss: 0.560061  [120000/175341]\n",
      "loss: 0.567429  [121600/175341]\n",
      "loss: 0.699981  [123200/175341]\n",
      "loss: 0.599019  [124800/175341]\n",
      "loss: 0.353384  [126400/175341]\n",
      "loss: 0.472657  [128000/175341]\n",
      "loss: 0.496194  [129600/175341]\n",
      "loss: 0.362889  [131200/175341]\n",
      "loss: 0.537019  [132800/175341]\n",
      "loss: 0.421682  [134400/175341]\n",
      "loss: 0.340360  [136000/175341]\n",
      "loss: 0.903153  [137600/175341]\n",
      "loss: 0.421036  [139200/175341]\n",
      "loss: 0.500235  [140800/175341]\n",
      "loss: 0.664302  [142400/175341]\n",
      "loss: 0.960375  [144000/175341]\n",
      "loss: 0.558385  [145600/175341]\n",
      "loss: 0.610747  [147200/175341]\n",
      "loss: 0.559116  [148800/175341]\n",
      "loss: 0.955921  [150400/175341]\n",
      "loss: 0.309365  [152000/175341]\n",
      "loss: 0.204565  [153600/175341]\n",
      "loss: 0.345785  [155200/175341]\n",
      "loss: 0.668936  [156800/175341]\n",
      "loss: 0.792226  [158400/175341]\n",
      "loss: 0.664442  [160000/175341]\n",
      "loss: 0.260928  [161600/175341]\n",
      "loss: 0.512416  [163200/175341]\n",
      "loss: 0.510187  [164800/175341]\n",
      "loss: 0.290345  [166400/175341]\n",
      "loss: 0.204461  [168000/175341]\n",
      "loss: 0.554082  [169600/175341]\n",
      "loss: 0.437552  [171200/175341]\n",
      "loss: 0.788644  [172800/175341]\n",
      "loss: 0.462910  [174400/175341]\n",
      "Train Accuracy: 77.9487%\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.646642, F1-score: 70.79%, Macro_F1-Score:  33.11%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.230178  [    0/175341]\n",
      "loss: 0.469853  [ 1600/175341]\n",
      "loss: 0.504462  [ 3200/175341]\n",
      "loss: 0.452513  [ 4800/175341]\n",
      "loss: 0.676578  [ 6400/175341]\n",
      "loss: 0.703348  [ 8000/175341]\n",
      "loss: 0.480056  [ 9600/175341]\n",
      "loss: 0.391406  [11200/175341]\n",
      "loss: 0.541807  [12800/175341]\n",
      "loss: 0.593300  [14400/175341]\n",
      "loss: 0.481697  [16000/175341]\n",
      "loss: 0.815120  [17600/175341]\n",
      "loss: 0.451795  [19200/175341]\n",
      "loss: 0.880345  [20800/175341]\n",
      "loss: 0.654097  [22400/175341]\n",
      "loss: 1.301386  [24000/175341]\n",
      "loss: 0.949443  [25600/175341]\n",
      "loss: 0.229786  [27200/175341]\n",
      "loss: 0.362088  [28800/175341]\n",
      "loss: 0.315172  [30400/175341]\n",
      "loss: 0.745299  [32000/175341]\n",
      "loss: 0.975508  [33600/175341]\n",
      "loss: 0.717765  [35200/175341]\n",
      "loss: 0.695597  [36800/175341]\n",
      "loss: 0.541972  [38400/175341]\n",
      "loss: 0.359536  [40000/175341]\n",
      "loss: 0.422008  [41600/175341]\n",
      "loss: 0.565061  [43200/175341]\n",
      "loss: 0.626728  [44800/175341]\n",
      "loss: 0.288762  [46400/175341]\n",
      "loss: 0.327274  [48000/175341]\n",
      "loss: 0.844653  [49600/175341]\n",
      "loss: 0.384907  [51200/175341]\n",
      "loss: 0.393133  [52800/175341]\n",
      "loss: 0.493081  [54400/175341]\n",
      "loss: 1.111368  [56000/175341]\n",
      "loss: 1.119930  [57600/175341]\n",
      "loss: 0.464281  [59200/175341]\n",
      "loss: 0.671935  [60800/175341]\n",
      "loss: 0.869880  [62400/175341]\n",
      "loss: 0.799544  [64000/175341]\n",
      "loss: 0.592001  [65600/175341]\n",
      "loss: 0.247522  [67200/175341]\n",
      "loss: 0.422545  [68800/175341]\n",
      "loss: 0.301520  [70400/175341]\n",
      "loss: 0.600628  [72000/175341]\n",
      "loss: 0.282826  [73600/175341]\n",
      "loss: 0.280257  [75200/175341]\n",
      "loss: 0.299823  [76800/175341]\n",
      "loss: 0.512518  [78400/175341]\n",
      "loss: 1.002723  [80000/175341]\n",
      "loss: 0.249228  [81600/175341]\n",
      "loss: 0.614487  [83200/175341]\n",
      "loss: 0.531469  [84800/175341]\n",
      "loss: 0.410628  [86400/175341]\n",
      "loss: 0.526556  [88000/175341]\n",
      "loss: 0.591284  [89600/175341]\n",
      "loss: 0.777981  [91200/175341]\n",
      "loss: 0.583489  [92800/175341]\n",
      "loss: 0.931352  [94400/175341]\n",
      "loss: 0.537980  [96000/175341]\n",
      "loss: 0.629611  [97600/175341]\n",
      "loss: 0.628898  [99200/175341]\n",
      "loss: 0.566393  [100800/175341]\n",
      "loss: 0.487569  [102400/175341]\n",
      "loss: 0.293019  [104000/175341]\n",
      "loss: 0.412931  [105600/175341]\n",
      "loss: 0.837148  [107200/175341]\n",
      "loss: 0.503284  [108800/175341]\n",
      "loss: 0.261307  [110400/175341]\n",
      "loss: 0.316767  [112000/175341]\n",
      "loss: 0.737163  [113600/175341]\n",
      "loss: 0.509024  [115200/175341]\n",
      "loss: 0.679844  [116800/175341]\n",
      "loss: 0.575745  [118400/175341]\n",
      "loss: 0.271494  [120000/175341]\n",
      "loss: 0.469490  [121600/175341]\n",
      "loss: 0.676256  [123200/175341]\n",
      "loss: 0.577058  [124800/175341]\n",
      "loss: 0.512158  [126400/175341]\n",
      "loss: 0.285774  [128000/175341]\n",
      "loss: 0.437408  [129600/175341]\n",
      "loss: 0.578427  [131200/175341]\n",
      "loss: 0.422955  [132800/175341]\n",
      "loss: 0.582834  [134400/175341]\n",
      "loss: 0.486810  [136000/175341]\n",
      "loss: 0.654586  [137600/175341]\n",
      "loss: 0.581711  [139200/175341]\n",
      "loss: 0.490836  [140800/175341]\n",
      "loss: 0.426163  [142400/175341]\n",
      "loss: 0.715281  [144000/175341]\n",
      "loss: 0.458495  [145600/175341]\n",
      "loss: 0.439013  [147200/175341]\n",
      "loss: 0.597769  [148800/175341]\n",
      "loss: 0.464534  [150400/175341]\n",
      "loss: 0.637907  [152000/175341]\n",
      "loss: 0.480292  [153600/175341]\n",
      "loss: 0.480632  [155200/175341]\n",
      "loss: 0.576428  [156800/175341]\n",
      "loss: 0.366263  [158400/175341]\n",
      "loss: 0.405599  [160000/175341]\n",
      "loss: 0.358387  [161600/175341]\n",
      "loss: 0.760122  [163200/175341]\n",
      "loss: 0.294434  [164800/175341]\n",
      "loss: 0.276878  [166400/175341]\n",
      "loss: 0.234380  [168000/175341]\n",
      "loss: 0.576697  [169600/175341]\n",
      "loss: 0.443392  [171200/175341]\n",
      "loss: 0.519638  [172800/175341]\n",
      "loss: 0.438439  [174400/175341]\n",
      "Train Accuracy: 78.1506%\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.658669, F1-score: 70.73%, Macro_F1-Score:  33.06%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.835591  [    0/175341]\n",
      "loss: 0.656448  [ 1600/175341]\n",
      "loss: 0.456704  [ 3200/175341]\n",
      "loss: 1.439584  [ 4800/175341]\n",
      "loss: 0.604229  [ 6400/175341]\n",
      "loss: 0.642940  [ 8000/175341]\n",
      "loss: 0.758097  [ 9600/175341]\n",
      "loss: 0.863587  [11200/175341]\n",
      "loss: 0.878342  [12800/175341]\n",
      "loss: 0.503013  [14400/175341]\n",
      "loss: 0.430793  [16000/175341]\n",
      "loss: 0.449131  [17600/175341]\n",
      "loss: 0.311884  [19200/175341]\n",
      "loss: 0.480278  [20800/175341]\n",
      "loss: 0.352243  [22400/175341]\n",
      "loss: 0.580932  [24000/175341]\n",
      "loss: 0.194496  [25600/175341]\n",
      "loss: 0.331978  [27200/175341]\n",
      "loss: 0.413564  [28800/175341]\n",
      "loss: 0.905593  [30400/175341]\n",
      "loss: 0.900396  [32000/175341]\n",
      "loss: 1.100523  [33600/175341]\n",
      "loss: 0.666354  [35200/175341]\n",
      "loss: 0.558871  [36800/175341]\n",
      "loss: 0.577507  [38400/175341]\n",
      "loss: 0.531305  [40000/175341]\n",
      "loss: 0.580286  [41600/175341]\n",
      "loss: 0.373698  [43200/175341]\n",
      "loss: 0.830853  [44800/175341]\n",
      "loss: 0.357783  [46400/175341]\n",
      "loss: 0.410956  [48000/175341]\n",
      "loss: 0.518618  [49600/175341]\n",
      "loss: 0.408285  [51200/175341]\n",
      "loss: 0.598195  [52800/175341]\n",
      "loss: 0.442051  [54400/175341]\n",
      "loss: 0.574129  [56000/175341]\n",
      "loss: 0.545056  [57600/175341]\n",
      "loss: 0.387122  [59200/175341]\n",
      "loss: 0.269897  [60800/175341]\n",
      "loss: 0.797695  [62400/175341]\n",
      "loss: 0.623777  [64000/175341]\n",
      "loss: 1.007966  [65600/175341]\n",
      "loss: 0.662306  [67200/175341]\n",
      "loss: 0.270586  [68800/175341]\n",
      "loss: 0.327283  [70400/175341]\n",
      "loss: 0.440034  [72000/175341]\n",
      "loss: 0.485660  [73600/175341]\n",
      "loss: 0.532333  [75200/175341]\n",
      "loss: 0.465217  [76800/175341]\n",
      "loss: 0.802240  [78400/175341]\n",
      "loss: 0.410309  [80000/175341]\n",
      "loss: 0.625174  [81600/175341]\n",
      "loss: 0.359639  [83200/175341]\n",
      "loss: 0.728277  [84800/175341]\n",
      "loss: 0.792648  [86400/175341]\n",
      "loss: 0.782510  [88000/175341]\n",
      "loss: 0.734688  [89600/175341]\n",
      "loss: 0.650795  [91200/175341]\n",
      "loss: 0.345605  [92800/175341]\n",
      "loss: 0.297943  [94400/175341]\n",
      "loss: 0.404695  [96000/175341]\n",
      "loss: 0.609959  [97600/175341]\n",
      "loss: 0.523851  [99200/175341]\n",
      "loss: 0.768444  [100800/175341]\n",
      "loss: 0.884588  [102400/175341]\n",
      "loss: 0.383827  [104000/175341]\n",
      "loss: 0.838980  [105600/175341]\n",
      "loss: 0.423040  [107200/175341]\n",
      "loss: 0.709093  [108800/175341]\n",
      "loss: 0.568955  [110400/175341]\n",
      "loss: 0.744728  [112000/175341]\n",
      "loss: 0.795916  [113600/175341]\n",
      "loss: 0.688996  [115200/175341]\n",
      "loss: 0.434353  [116800/175341]\n",
      "loss: 0.661332  [118400/175341]\n",
      "loss: 0.619983  [120000/175341]\n",
      "loss: 0.640124  [121600/175341]\n",
      "loss: 0.376699  [123200/175341]\n",
      "loss: 0.445466  [124800/175341]\n",
      "loss: 0.395310  [126400/175341]\n",
      "loss: 0.491361  [128000/175341]\n",
      "loss: 0.573049  [129600/175341]\n",
      "loss: 0.255905  [131200/175341]\n",
      "loss: 0.672636  [132800/175341]\n",
      "loss: 0.237361  [134400/175341]\n",
      "loss: 0.642532  [136000/175341]\n",
      "loss: 1.181160  [137600/175341]\n",
      "loss: 0.861479  [139200/175341]\n",
      "loss: 0.819049  [140800/175341]\n",
      "loss: 0.730917  [142400/175341]\n",
      "loss: 0.730370  [144000/175341]\n",
      "loss: 0.509641  [145600/175341]\n",
      "loss: 0.305535  [147200/175341]\n",
      "loss: 0.576457  [148800/175341]\n",
      "loss: 0.744750  [150400/175341]\n",
      "loss: 0.754834  [152000/175341]\n",
      "loss: 0.887421  [153600/175341]\n",
      "loss: 0.377364  [155200/175341]\n",
      "loss: 0.597084  [156800/175341]\n",
      "loss: 0.384019  [158400/175341]\n",
      "loss: 0.478305  [160000/175341]\n",
      "loss: 0.441324  [161600/175341]\n",
      "loss: 0.752606  [163200/175341]\n",
      "loss: 0.566355  [164800/175341]\n",
      "loss: 0.686306  [166400/175341]\n",
      "loss: 0.361730  [168000/175341]\n",
      "loss: 0.208304  [169600/175341]\n",
      "loss: 0.809793  [171200/175341]\n",
      "loss: 1.039839  [172800/175341]\n",
      "loss: 0.561660  [174400/175341]\n",
      "Train Accuracy: 78.3451%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.653771, F1-score: 71.30%, Macro_F1-Score:  33.13%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.153274  [    0/175341]\n",
      "loss: 0.810816  [ 1600/175341]\n",
      "loss: 0.915536  [ 3200/175341]\n",
      "loss: 0.432352  [ 4800/175341]\n",
      "loss: 0.413310  [ 6400/175341]\n",
      "loss: 1.066899  [ 8000/175341]\n",
      "loss: 0.259074  [ 9600/175341]\n",
      "loss: 0.314788  [11200/175341]\n",
      "loss: 0.525400  [12800/175341]\n",
      "loss: 0.551394  [14400/175341]\n",
      "loss: 0.375815  [16000/175341]\n",
      "loss: 0.358898  [17600/175341]\n",
      "loss: 0.608550  [19200/175341]\n",
      "loss: 0.846861  [20800/175341]\n",
      "loss: 0.308989  [22400/175341]\n",
      "loss: 1.428403  [24000/175341]\n",
      "loss: 0.380444  [25600/175341]\n",
      "loss: 0.776977  [27200/175341]\n",
      "loss: 0.420345  [28800/175341]\n",
      "loss: 0.250176  [30400/175341]\n",
      "loss: 0.237909  [32000/175341]\n",
      "loss: 0.456076  [33600/175341]\n",
      "loss: 0.309523  [35200/175341]\n",
      "loss: 0.889118  [36800/175341]\n",
      "loss: 0.110369  [38400/175341]\n",
      "loss: 0.640731  [40000/175341]\n",
      "loss: 0.251914  [41600/175341]\n",
      "loss: 0.836421  [43200/175341]\n",
      "loss: 0.692086  [44800/175341]\n",
      "loss: 0.469404  [46400/175341]\n",
      "loss: 0.612445  [48000/175341]\n",
      "loss: 0.644786  [49600/175341]\n",
      "loss: 0.606560  [51200/175341]\n",
      "loss: 0.634807  [52800/175341]\n",
      "loss: 0.580412  [54400/175341]\n",
      "loss: 0.291270  [56000/175341]\n",
      "loss: 0.684114  [57600/175341]\n",
      "loss: 0.449235  [59200/175341]\n",
      "loss: 0.654471  [60800/175341]\n",
      "loss: 0.173666  [62400/175341]\n",
      "loss: 0.229131  [64000/175341]\n",
      "loss: 0.532015  [65600/175341]\n",
      "loss: 0.515429  [67200/175341]\n",
      "loss: 0.444525  [68800/175341]\n",
      "loss: 0.415512  [70400/175341]\n",
      "loss: 0.610143  [72000/175341]\n",
      "loss: 0.583506  [73600/175341]\n",
      "loss: 0.769216  [75200/175341]\n",
      "loss: 0.332305  [76800/175341]\n",
      "loss: 0.263250  [78400/175341]\n",
      "loss: 0.472716  [80000/175341]\n",
      "loss: 0.604971  [81600/175341]\n",
      "loss: 0.890798  [83200/175341]\n",
      "loss: 0.439782  [84800/175341]\n",
      "loss: 0.328037  [86400/175341]\n",
      "loss: 0.457799  [88000/175341]\n",
      "loss: 0.499669  [89600/175341]\n",
      "loss: 0.568039  [91200/175341]\n",
      "loss: 0.299307  [92800/175341]\n",
      "loss: 0.859892  [94400/175341]\n",
      "loss: 0.551374  [96000/175341]\n",
      "loss: 0.269502  [97600/175341]\n",
      "loss: 0.358371  [99200/175341]\n",
      "loss: 0.462999  [100800/175341]\n",
      "loss: 0.375076  [102400/175341]\n",
      "loss: 0.413665  [104000/175341]\n",
      "loss: 0.555737  [105600/175341]\n",
      "loss: 0.513754  [107200/175341]\n",
      "loss: 1.023162  [108800/175341]\n",
      "loss: 0.501006  [110400/175341]\n",
      "loss: 0.487123  [112000/175341]\n",
      "loss: 0.560377  [113600/175341]\n",
      "loss: 0.392708  [115200/175341]\n",
      "loss: 0.211623  [116800/175341]\n",
      "loss: 0.684823  [118400/175341]\n",
      "loss: 0.980404  [120000/175341]\n",
      "loss: 0.339129  [121600/175341]\n",
      "loss: 0.448726  [123200/175341]\n",
      "loss: 0.577994  [124800/175341]\n",
      "loss: 0.587429  [126400/175341]\n",
      "loss: 0.636024  [128000/175341]\n",
      "loss: 0.942961  [129600/175341]\n",
      "loss: 0.689392  [131200/175341]\n",
      "loss: 0.714500  [132800/175341]\n",
      "loss: 0.828333  [134400/175341]\n",
      "loss: 0.571398  [136000/175341]\n",
      "loss: 0.232827  [137600/175341]\n",
      "loss: 0.657017  [139200/175341]\n",
      "loss: 0.487780  [140800/175341]\n",
      "loss: 0.355908  [142400/175341]\n",
      "loss: 0.664342  [144000/175341]\n",
      "loss: 1.065313  [145600/175341]\n",
      "loss: 0.702280  [147200/175341]\n",
      "loss: 0.303400  [148800/175341]\n",
      "loss: 0.721645  [150400/175341]\n",
      "loss: 0.763628  [152000/175341]\n",
      "loss: 0.724441  [153600/175341]\n",
      "loss: 0.380589  [155200/175341]\n",
      "loss: 0.774462  [156800/175341]\n",
      "loss: 0.364305  [158400/175341]\n",
      "loss: 0.568635  [160000/175341]\n",
      "loss: 0.702002  [161600/175341]\n",
      "loss: 0.550203  [163200/175341]\n",
      "loss: 0.248924  [164800/175341]\n",
      "loss: 0.551436  [166400/175341]\n",
      "loss: 0.411435  [168000/175341]\n",
      "loss: 0.290862  [169600/175341]\n",
      "loss: 0.334102  [171200/175341]\n",
      "loss: 0.840172  [172800/175341]\n",
      "loss: 0.192671  [174400/175341]\n",
      "Train Accuracy: 78.5464%\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.628954, F1-score: 72.79%, Macro_F1-Score:  33.89%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.444667  [    0/175341]\n",
      "loss: 0.241647  [ 1600/175341]\n",
      "loss: 0.415298  [ 3200/175341]\n",
      "loss: 0.944658  [ 4800/175341]\n",
      "loss: 0.262717  [ 6400/175341]\n",
      "loss: 0.688487  [ 8000/175341]\n",
      "loss: 0.865776  [ 9600/175341]\n",
      "loss: 0.539137  [11200/175341]\n",
      "loss: 0.791504  [12800/175341]\n",
      "loss: 0.561650  [14400/175341]\n",
      "loss: 0.627537  [16000/175341]\n",
      "loss: 0.378244  [17600/175341]\n",
      "loss: 0.671385  [19200/175341]\n",
      "loss: 0.364354  [20800/175341]\n",
      "loss: 0.166477  [22400/175341]\n",
      "loss: 0.640180  [24000/175341]\n",
      "loss: 0.221627  [25600/175341]\n",
      "loss: 0.964110  [27200/175341]\n",
      "loss: 0.306951  [28800/175341]\n",
      "loss: 0.614178  [30400/175341]\n",
      "loss: 0.837485  [32000/175341]\n",
      "loss: 0.812237  [33600/175341]\n",
      "loss: 0.312254  [35200/175341]\n",
      "loss: 0.492636  [36800/175341]\n",
      "loss: 0.461009  [38400/175341]\n",
      "loss: 0.338961  [40000/175341]\n",
      "loss: 0.742382  [41600/175341]\n",
      "loss: 0.291617  [43200/175341]\n",
      "loss: 0.918950  [44800/175341]\n",
      "loss: 0.561389  [46400/175341]\n",
      "loss: 0.693224  [48000/175341]\n",
      "loss: 0.558394  [49600/175341]\n",
      "loss: 0.606615  [51200/175341]\n",
      "loss: 0.796755  [52800/175341]\n",
      "loss: 0.518969  [54400/175341]\n",
      "loss: 0.193796  [56000/175341]\n",
      "loss: 0.618588  [57600/175341]\n",
      "loss: 0.435950  [59200/175341]\n",
      "loss: 0.526552  [60800/175341]\n",
      "loss: 0.706534  [62400/175341]\n",
      "loss: 0.675799  [64000/175341]\n",
      "loss: 0.530189  [65600/175341]\n",
      "loss: 0.574342  [67200/175341]\n",
      "loss: 0.768901  [68800/175341]\n",
      "loss: 1.099302  [70400/175341]\n",
      "loss: 0.465686  [72000/175341]\n",
      "loss: 0.583707  [73600/175341]\n",
      "loss: 0.582044  [75200/175341]\n",
      "loss: 0.286300  [76800/175341]\n",
      "loss: 0.561586  [78400/175341]\n",
      "loss: 0.459438  [80000/175341]\n",
      "loss: 0.385458  [81600/175341]\n",
      "loss: 0.155273  [83200/175341]\n",
      "loss: 0.245161  [84800/175341]\n",
      "loss: 0.322352  [86400/175341]\n",
      "loss: 0.587603  [88000/175341]\n",
      "loss: 0.622651  [89600/175341]\n",
      "loss: 0.597270  [91200/175341]\n",
      "loss: 0.787484  [92800/175341]\n",
      "loss: 0.660015  [94400/175341]\n",
      "loss: 0.759574  [96000/175341]\n",
      "loss: 0.600661  [97600/175341]\n",
      "loss: 0.503381  [99200/175341]\n",
      "loss: 0.955370  [100800/175341]\n",
      "loss: 0.564432  [102400/175341]\n",
      "loss: 0.411849  [104000/175341]\n",
      "loss: 0.414456  [105600/175341]\n",
      "loss: 0.710262  [107200/175341]\n",
      "loss: 0.183603  [108800/175341]\n",
      "loss: 0.774624  [110400/175341]\n",
      "loss: 0.640695  [112000/175341]\n",
      "loss: 0.510359  [113600/175341]\n",
      "loss: 0.415964  [115200/175341]\n",
      "loss: 0.287062  [116800/175341]\n",
      "loss: 0.183965  [118400/175341]\n",
      "loss: 0.700923  [120000/175341]\n",
      "loss: 0.755850  [121600/175341]\n",
      "loss: 0.325349  [123200/175341]\n",
      "loss: 0.406570  [124800/175341]\n",
      "loss: 0.644546  [126400/175341]\n",
      "loss: 0.512618  [128000/175341]\n",
      "loss: 0.340799  [129600/175341]\n",
      "loss: 0.413299  [131200/175341]\n",
      "loss: 0.394469  [132800/175341]\n",
      "loss: 1.142683  [134400/175341]\n",
      "loss: 0.500383  [136000/175341]\n",
      "loss: 0.385068  [137600/175341]\n",
      "loss: 0.348834  [139200/175341]\n",
      "loss: 0.217110  [140800/175341]\n",
      "loss: 0.564933  [142400/175341]\n",
      "loss: 0.476939  [144000/175341]\n",
      "loss: 0.671486  [145600/175341]\n",
      "loss: 0.463668  [147200/175341]\n",
      "loss: 0.748847  [148800/175341]\n",
      "loss: 0.392413  [150400/175341]\n",
      "loss: 0.505950  [152000/175341]\n",
      "loss: 0.657082  [153600/175341]\n",
      "loss: 0.243661  [155200/175341]\n",
      "loss: 0.561060  [156800/175341]\n",
      "loss: 0.576064  [158400/175341]\n",
      "loss: 1.285362  [160000/175341]\n",
      "loss: 0.836652  [161600/175341]\n",
      "loss: 0.511944  [163200/175341]\n",
      "loss: 0.231680  [164800/175341]\n",
      "loss: 0.713175  [166400/175341]\n",
      "loss: 0.456068  [168000/175341]\n",
      "loss: 0.452597  [169600/175341]\n",
      "loss: 0.469064  [171200/175341]\n",
      "loss: 0.634639  [172800/175341]\n",
      "loss: 0.279502  [174400/175341]\n",
      "Train Accuracy: 78.7163%\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.633484, F1-score: 71.94%, Macro_F1-Score:  33.50%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.396564  [    0/175341]\n",
      "loss: 0.601184  [ 1600/175341]\n",
      "loss: 0.714912  [ 3200/175341]\n",
      "loss: 0.547997  [ 4800/175341]\n",
      "loss: 0.287056  [ 6400/175341]\n",
      "loss: 0.289851  [ 8000/175341]\n",
      "loss: 0.664479  [ 9600/175341]\n",
      "loss: 0.594298  [11200/175341]\n",
      "loss: 0.455495  [12800/175341]\n",
      "loss: 0.627083  [14400/175341]\n",
      "loss: 0.494922  [16000/175341]\n",
      "loss: 0.295287  [17600/175341]\n",
      "loss: 0.829536  [19200/175341]\n",
      "loss: 0.813531  [20800/175341]\n",
      "loss: 0.708814  [22400/175341]\n",
      "loss: 0.543613  [24000/175341]\n",
      "loss: 0.720559  [25600/175341]\n",
      "loss: 0.659423  [27200/175341]\n",
      "loss: 0.456978  [28800/175341]\n",
      "loss: 0.365426  [30400/175341]\n",
      "loss: 0.452336  [32000/175341]\n",
      "loss: 0.840219  [33600/175341]\n",
      "loss: 0.798445  [35200/175341]\n",
      "loss: 0.492569  [36800/175341]\n",
      "loss: 0.587340  [38400/175341]\n",
      "loss: 0.509173  [40000/175341]\n",
      "loss: 0.936746  [41600/175341]\n",
      "loss: 0.746831  [43200/175341]\n",
      "loss: 0.645654  [44800/175341]\n",
      "loss: 0.958425  [46400/175341]\n",
      "loss: 0.568104  [48000/175341]\n",
      "loss: 0.376445  [49600/175341]\n",
      "loss: 0.288624  [51200/175341]\n",
      "loss: 0.587769  [52800/175341]\n",
      "loss: 0.373617  [54400/175341]\n",
      "loss: 0.562950  [56000/175341]\n",
      "loss: 0.431407  [57600/175341]\n",
      "loss: 0.328077  [59200/175341]\n",
      "loss: 0.914302  [60800/175341]\n",
      "loss: 0.474511  [62400/175341]\n",
      "loss: 0.320953  [64000/175341]\n",
      "loss: 0.683692  [65600/175341]\n",
      "loss: 0.338267  [67200/175341]\n",
      "loss: 0.300414  [68800/175341]\n",
      "loss: 0.504260  [70400/175341]\n",
      "loss: 0.527601  [72000/175341]\n",
      "loss: 0.522270  [73600/175341]\n",
      "loss: 0.299649  [75200/175341]\n",
      "loss: 0.304168  [76800/175341]\n",
      "loss: 0.271152  [78400/175341]\n",
      "loss: 0.412255  [80000/175341]\n",
      "loss: 0.891156  [81600/175341]\n",
      "loss: 0.515333  [83200/175341]\n",
      "loss: 0.778181  [84800/175341]\n",
      "loss: 0.224646  [86400/175341]\n",
      "loss: 0.844467  [88000/175341]\n",
      "loss: 0.317787  [89600/175341]\n",
      "loss: 0.442500  [91200/175341]\n",
      "loss: 0.672203  [92800/175341]\n",
      "loss: 0.462837  [94400/175341]\n",
      "loss: 0.700413  [96000/175341]\n",
      "loss: 0.317783  [97600/175341]\n",
      "loss: 0.561621  [99200/175341]\n",
      "loss: 0.525589  [100800/175341]\n",
      "loss: 0.618830  [102400/175341]\n",
      "loss: 0.135759  [104000/175341]\n",
      "loss: 0.324558  [105600/175341]\n",
      "loss: 0.616529  [107200/175341]\n",
      "loss: 0.539720  [108800/175341]\n",
      "loss: 0.400531  [110400/175341]\n",
      "loss: 0.300069  [112000/175341]\n",
      "loss: 0.702669  [113600/175341]\n",
      "loss: 0.700417  [115200/175341]\n",
      "loss: 0.306117  [116800/175341]\n",
      "loss: 0.418056  [118400/175341]\n",
      "loss: 0.471736  [120000/175341]\n",
      "loss: 0.408602  [121600/175341]\n",
      "loss: 0.507241  [123200/175341]\n",
      "loss: 0.734871  [124800/175341]\n",
      "loss: 0.492193  [126400/175341]\n",
      "loss: 0.805452  [128000/175341]\n",
      "loss: 0.752865  [129600/175341]\n",
      "loss: 0.188888  [131200/175341]\n",
      "loss: 0.909719  [132800/175341]\n",
      "loss: 0.601801  [134400/175341]\n",
      "loss: 0.371929  [136000/175341]\n",
      "loss: 0.373065  [137600/175341]\n",
      "loss: 0.633985  [139200/175341]\n",
      "loss: 0.417824  [140800/175341]\n",
      "loss: 0.216500  [142400/175341]\n",
      "loss: 0.322620  [144000/175341]\n",
      "loss: 0.374717  [145600/175341]\n",
      "loss: 0.558140  [147200/175341]\n",
      "loss: 0.523929  [148800/175341]\n",
      "loss: 0.894574  [150400/175341]\n",
      "loss: 0.599118  [152000/175341]\n",
      "loss: 0.551537  [153600/175341]\n",
      "loss: 0.384072  [155200/175341]\n",
      "loss: 0.339835  [156800/175341]\n",
      "loss: 0.418248  [158400/175341]\n",
      "loss: 0.484403  [160000/175341]\n",
      "loss: 0.548005  [161600/175341]\n",
      "loss: 0.388600  [163200/175341]\n",
      "loss: 0.394917  [164800/175341]\n",
      "loss: 0.690357  [166400/175341]\n",
      "loss: 0.621775  [168000/175341]\n",
      "loss: 0.639621  [169600/175341]\n",
      "loss: 0.370751  [171200/175341]\n",
      "loss: 1.081906  [172800/175341]\n",
      "loss: 0.436195  [174400/175341]\n",
      "Train Accuracy: 78.8783%\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.624813, F1-score: 72.01%, Macro_F1-Score:  33.56%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.280033  [    0/175341]\n",
      "loss: 0.238834  [ 1600/175341]\n",
      "loss: 0.460473  [ 3200/175341]\n",
      "loss: 0.569460  [ 4800/175341]\n",
      "loss: 0.460981  [ 6400/175341]\n",
      "loss: 0.503355  [ 8000/175341]\n",
      "loss: 0.562799  [ 9600/175341]\n",
      "loss: 0.356118  [11200/175341]\n",
      "loss: 0.463672  [12800/175341]\n",
      "loss: 0.316205  [14400/175341]\n",
      "loss: 0.570509  [16000/175341]\n",
      "loss: 0.665747  [17600/175341]\n",
      "loss: 0.679553  [19200/175341]\n",
      "loss: 0.401843  [20800/175341]\n",
      "loss: 0.935565  [22400/175341]\n",
      "loss: 0.260801  [24000/175341]\n",
      "loss: 0.351297  [25600/175341]\n",
      "loss: 0.601279  [27200/175341]\n",
      "loss: 0.428700  [28800/175341]\n",
      "loss: 0.790598  [30400/175341]\n",
      "loss: 0.875316  [32000/175341]\n",
      "loss: 0.829543  [33600/175341]\n",
      "loss: 0.495767  [35200/175341]\n",
      "loss: 0.379533  [36800/175341]\n",
      "loss: 0.407277  [38400/175341]\n",
      "loss: 0.537145  [40000/175341]\n",
      "loss: 0.423441  [41600/175341]\n",
      "loss: 0.471157  [43200/175341]\n",
      "loss: 0.488346  [44800/175341]\n",
      "loss: 0.688234  [46400/175341]\n",
      "loss: 0.560772  [48000/175341]\n",
      "loss: 0.587773  [49600/175341]\n",
      "loss: 0.893698  [51200/175341]\n",
      "loss: 0.356678  [52800/175341]\n",
      "loss: 0.465753  [54400/175341]\n",
      "loss: 0.480249  [56000/175341]\n",
      "loss: 0.618377  [57600/175341]\n",
      "loss: 0.290917  [59200/175341]\n",
      "loss: 0.629666  [60800/175341]\n",
      "loss: 0.222341  [62400/175341]\n",
      "loss: 0.646529  [64000/175341]\n",
      "loss: 0.564532  [65600/175341]\n",
      "loss: 0.508272  [67200/175341]\n",
      "loss: 0.295773  [68800/175341]\n",
      "loss: 0.567206  [70400/175341]\n",
      "loss: 0.777748  [72000/175341]\n",
      "loss: 0.501131  [73600/175341]\n",
      "loss: 0.469031  [75200/175341]\n",
      "loss: 0.501918  [76800/175341]\n",
      "loss: 0.504165  [78400/175341]\n",
      "loss: 0.589611  [80000/175341]\n",
      "loss: 1.010492  [81600/175341]\n",
      "loss: 0.886375  [83200/175341]\n",
      "loss: 0.520091  [84800/175341]\n",
      "loss: 0.413619  [86400/175341]\n",
      "loss: 0.524476  [88000/175341]\n",
      "loss: 0.686910  [89600/175341]\n",
      "loss: 0.599531  [91200/175341]\n",
      "loss: 0.586915  [92800/175341]\n",
      "loss: 0.441090  [94400/175341]\n",
      "loss: 0.314100  [96000/175341]\n",
      "loss: 0.458238  [97600/175341]\n",
      "loss: 0.949400  [99200/175341]\n",
      "loss: 0.465397  [100800/175341]\n",
      "loss: 0.562741  [102400/175341]\n",
      "loss: 0.409233  [104000/175341]\n",
      "loss: 0.400585  [105600/175341]\n",
      "loss: 0.243897  [107200/175341]\n",
      "loss: 0.589798  [108800/175341]\n",
      "loss: 0.739881  [110400/175341]\n",
      "loss: 0.439061  [112000/175341]\n",
      "loss: 0.524319  [113600/175341]\n",
      "loss: 0.549242  [115200/175341]\n",
      "loss: 0.999045  [116800/175341]\n",
      "loss: 0.802654  [118400/175341]\n",
      "loss: 0.216992  [120000/175341]\n",
      "loss: 0.391016  [121600/175341]\n",
      "loss: 0.378192  [123200/175341]\n",
      "loss: 0.315296  [124800/175341]\n",
      "loss: 0.360185  [126400/175341]\n",
      "loss: 0.327313  [128000/175341]\n",
      "loss: 0.402035  [129600/175341]\n",
      "loss: 0.201044  [131200/175341]\n",
      "loss: 0.455586  [132800/175341]\n",
      "loss: 0.751506  [134400/175341]\n",
      "loss: 0.990345  [136000/175341]\n",
      "loss: 0.698805  [137600/175341]\n",
      "loss: 0.195599  [139200/175341]\n",
      "loss: 0.342421  [140800/175341]\n",
      "loss: 0.470324  [142400/175341]\n",
      "loss: 0.679374  [144000/175341]\n",
      "loss: 0.715001  [145600/175341]\n",
      "loss: 0.951862  [147200/175341]\n",
      "loss: 0.853983  [148800/175341]\n",
      "loss: 0.335716  [150400/175341]\n",
      "loss: 0.665039  [152000/175341]\n",
      "loss: 1.093404  [153600/175341]\n",
      "loss: 0.840046  [155200/175341]\n",
      "loss: 0.297198  [156800/175341]\n",
      "loss: 1.003803  [158400/175341]\n",
      "loss: 0.341850  [160000/175341]\n",
      "loss: 0.939884  [161600/175341]\n",
      "loss: 0.520381  [163200/175341]\n",
      "loss: 0.500781  [164800/175341]\n",
      "loss: 0.429768  [166400/175341]\n",
      "loss: 0.756974  [168000/175341]\n",
      "loss: 0.476209  [169600/175341]\n",
      "loss: 0.749159  [171200/175341]\n",
      "loss: 0.430166  [172800/175341]\n",
      "loss: 0.309717  [174400/175341]\n",
      "Train Accuracy: 78.9753%\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.633952, F1-score: 71.66%, Macro_F1-Score:  33.30%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.411318  [    0/175341]\n",
      "loss: 0.549051  [ 1600/175341]\n",
      "loss: 0.841984  [ 3200/175341]\n",
      "loss: 0.779654  [ 4800/175341]\n",
      "loss: 0.956592  [ 6400/175341]\n",
      "loss: 0.616358  [ 8000/175341]\n",
      "loss: 0.387252  [ 9600/175341]\n",
      "loss: 0.213809  [11200/175341]\n",
      "loss: 0.569727  [12800/175341]\n",
      "loss: 0.646151  [14400/175341]\n",
      "loss: 0.238287  [16000/175341]\n",
      "loss: 0.424998  [17600/175341]\n",
      "loss: 0.414834  [19200/175341]\n",
      "loss: 0.526802  [20800/175341]\n",
      "loss: 0.429767  [22400/175341]\n",
      "loss: 0.255254  [24000/175341]\n",
      "loss: 0.144946  [25600/175341]\n",
      "loss: 0.526593  [27200/175341]\n",
      "loss: 0.375515  [28800/175341]\n",
      "loss: 0.739100  [30400/175341]\n",
      "loss: 0.489543  [32000/175341]\n",
      "loss: 1.018723  [33600/175341]\n",
      "loss: 0.585709  [35200/175341]\n",
      "loss: 0.344130  [36800/175341]\n",
      "loss: 0.458481  [38400/175341]\n",
      "loss: 0.583524  [40000/175341]\n",
      "loss: 0.263269  [41600/175341]\n",
      "loss: 0.494080  [43200/175341]\n",
      "loss: 0.652991  [44800/175341]\n",
      "loss: 0.433223  [46400/175341]\n",
      "loss: 0.459170  [48000/175341]\n",
      "loss: 0.547569  [49600/175341]\n",
      "loss: 0.688479  [51200/175341]\n",
      "loss: 1.075477  [52800/175341]\n",
      "loss: 0.698474  [54400/175341]\n",
      "loss: 0.342887  [56000/175341]\n",
      "loss: 0.489783  [57600/175341]\n",
      "loss: 0.621304  [59200/175341]\n",
      "loss: 0.663308  [60800/175341]\n",
      "loss: 0.939298  [62400/175341]\n",
      "loss: 0.324453  [64000/175341]\n",
      "loss: 0.669037  [65600/175341]\n",
      "loss: 0.493724  [67200/175341]\n",
      "loss: 0.888104  [68800/175341]\n",
      "loss: 0.895791  [70400/175341]\n",
      "loss: 0.650931  [72000/175341]\n",
      "loss: 0.332378  [73600/175341]\n",
      "loss: 0.366518  [75200/175341]\n",
      "loss: 0.341139  [76800/175341]\n",
      "loss: 0.638533  [78400/175341]\n",
      "loss: 0.617495  [80000/175341]\n",
      "loss: 0.718583  [81600/175341]\n",
      "loss: 0.479776  [83200/175341]\n",
      "loss: 0.542012  [84800/175341]\n",
      "loss: 0.517469  [86400/175341]\n",
      "loss: 0.542149  [88000/175341]\n",
      "loss: 0.360337  [89600/175341]\n",
      "loss: 0.582155  [91200/175341]\n",
      "loss: 0.469934  [92800/175341]\n",
      "loss: 0.371600  [94400/175341]\n",
      "loss: 0.345649  [96000/175341]\n",
      "loss: 1.015083  [97600/175341]\n",
      "loss: 0.586079  [99200/175341]\n",
      "loss: 0.843036  [100800/175341]\n",
      "loss: 0.805746  [102400/175341]\n",
      "loss: 0.574201  [104000/175341]\n",
      "loss: 0.812122  [105600/175341]\n",
      "loss: 0.352757  [107200/175341]\n",
      "loss: 0.289405  [108800/175341]\n",
      "loss: 0.894890  [110400/175341]\n",
      "loss: 0.693231  [112000/175341]\n",
      "loss: 0.408383  [113600/175341]\n",
      "loss: 0.401438  [115200/175341]\n",
      "loss: 0.472329  [116800/175341]\n",
      "loss: 0.309550  [118400/175341]\n",
      "loss: 0.774953  [120000/175341]\n",
      "loss: 0.562683  [121600/175341]\n",
      "loss: 0.478803  [123200/175341]\n",
      "loss: 0.670563  [124800/175341]\n",
      "loss: 0.333199  [126400/175341]\n",
      "loss: 0.883007  [128000/175341]\n",
      "loss: 1.028870  [129600/175341]\n",
      "loss: 0.810061  [131200/175341]\n",
      "loss: 0.461791  [132800/175341]\n",
      "loss: 0.332981  [134400/175341]\n",
      "loss: 0.665753  [136000/175341]\n",
      "loss: 0.737149  [137600/175341]\n",
      "loss: 0.579153  [139200/175341]\n",
      "loss: 0.623393  [140800/175341]\n",
      "loss: 1.105624  [142400/175341]\n",
      "loss: 0.508670  [144000/175341]\n",
      "loss: 0.494775  [145600/175341]\n",
      "loss: 0.725080  [147200/175341]\n",
      "loss: 0.416390  [148800/175341]\n",
      "loss: 0.888099  [150400/175341]\n",
      "loss: 0.529144  [152000/175341]\n",
      "loss: 0.414368  [153600/175341]\n",
      "loss: 0.908695  [155200/175341]\n",
      "loss: 0.200693  [156800/175341]\n",
      "loss: 0.613664  [158400/175341]\n",
      "loss: 0.367492  [160000/175341]\n",
      "loss: 0.543997  [161600/175341]\n",
      "loss: 0.313096  [163200/175341]\n",
      "loss: 0.252790  [164800/175341]\n",
      "loss: 0.380329  [166400/175341]\n",
      "loss: 0.600979  [168000/175341]\n",
      "loss: 0.421839  [169600/175341]\n",
      "loss: 0.562938  [171200/175341]\n",
      "loss: 0.303298  [172800/175341]\n",
      "loss: 0.311940  [174400/175341]\n",
      "Train Accuracy: 79.0368%\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.631689, F1-score: 72.25%, Macro_F1-Score:  33.31%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.387243  [    0/175341]\n",
      "loss: 0.478868  [ 1600/175341]\n",
      "loss: 0.426784  [ 3200/175341]\n",
      "loss: 0.384835  [ 4800/175341]\n",
      "loss: 0.662826  [ 6400/175341]\n",
      "loss: 0.944625  [ 8000/175341]\n",
      "loss: 0.255554  [ 9600/175341]\n",
      "loss: 0.339453  [11200/175341]\n",
      "loss: 0.359650  [12800/175341]\n",
      "loss: 0.386805  [14400/175341]\n",
      "loss: 0.425610  [16000/175341]\n",
      "loss: 0.791374  [17600/175341]\n",
      "loss: 0.604256  [19200/175341]\n",
      "loss: 0.536358  [20800/175341]\n",
      "loss: 0.134895  [22400/175341]\n",
      "loss: 0.083309  [24000/175341]\n",
      "loss: 0.550731  [25600/175341]\n",
      "loss: 0.470067  [27200/175341]\n",
      "loss: 0.468009  [28800/175341]\n",
      "loss: 0.293891  [30400/175341]\n",
      "loss: 0.260279  [32000/175341]\n",
      "loss: 0.210913  [33600/175341]\n",
      "loss: 0.575185  [35200/175341]\n",
      "loss: 0.270223  [36800/175341]\n",
      "loss: 0.579987  [38400/175341]\n",
      "loss: 0.125386  [40000/175341]\n",
      "loss: 0.348906  [41600/175341]\n",
      "loss: 0.696763  [43200/175341]\n",
      "loss: 0.549061  [44800/175341]\n",
      "loss: 0.607132  [46400/175341]\n",
      "loss: 0.396295  [48000/175341]\n",
      "loss: 0.242488  [49600/175341]\n",
      "loss: 0.657530  [51200/175341]\n",
      "loss: 0.837573  [52800/175341]\n",
      "loss: 0.331792  [54400/175341]\n",
      "loss: 0.395513  [56000/175341]\n",
      "loss: 0.544216  [57600/175341]\n",
      "loss: 0.356010  [59200/175341]\n",
      "loss: 0.533891  [60800/175341]\n",
      "loss: 0.708761  [62400/175341]\n",
      "loss: 0.448953  [64000/175341]\n",
      "loss: 0.613073  [65600/175341]\n",
      "loss: 0.349390  [67200/175341]\n",
      "loss: 0.342065  [68800/175341]\n",
      "loss: 0.315577  [70400/175341]\n",
      "loss: 0.602444  [72000/175341]\n",
      "loss: 0.682240  [73600/175341]\n",
      "loss: 0.775974  [75200/175341]\n",
      "loss: 0.453584  [76800/175341]\n",
      "loss: 0.162175  [78400/175341]\n",
      "loss: 0.404914  [80000/175341]\n",
      "loss: 0.387882  [81600/175341]\n",
      "loss: 0.898175  [83200/175341]\n",
      "loss: 0.491883  [84800/175341]\n",
      "loss: 0.822408  [86400/175341]\n",
      "loss: 0.682004  [88000/175341]\n",
      "loss: 0.404985  [89600/175341]\n",
      "loss: 0.238619  [91200/175341]\n",
      "loss: 0.388511  [92800/175341]\n",
      "loss: 0.541107  [94400/175341]\n",
      "loss: 1.122379  [96000/175341]\n",
      "loss: 0.903295  [97600/175341]\n",
      "loss: 0.774997  [99200/175341]\n",
      "loss: 0.225033  [100800/175341]\n",
      "loss: 0.592673  [102400/175341]\n",
      "loss: 0.834942  [104000/175341]\n",
      "loss: 0.538565  [105600/175341]\n",
      "loss: 0.602982  [107200/175341]\n",
      "loss: 0.371803  [108800/175341]\n",
      "loss: 0.805424  [110400/175341]\n",
      "loss: 0.387672  [112000/175341]\n",
      "loss: 0.290576  [113600/175341]\n",
      "loss: 0.620978  [115200/175341]\n",
      "loss: 0.570426  [116800/175341]\n",
      "loss: 0.654687  [118400/175341]\n",
      "loss: 0.386833  [120000/175341]\n",
      "loss: 0.615712  [121600/175341]\n",
      "loss: 0.341283  [123200/175341]\n",
      "loss: 0.294324  [124800/175341]\n",
      "loss: 0.580308  [126400/175341]\n",
      "loss: 0.440385  [128000/175341]\n",
      "loss: 0.223759  [129600/175341]\n",
      "loss: 0.859112  [131200/175341]\n",
      "loss: 0.081729  [132800/175341]\n",
      "loss: 0.636623  [134400/175341]\n",
      "loss: 0.292845  [136000/175341]\n",
      "loss: 0.668339  [137600/175341]\n",
      "loss: 0.474376  [139200/175341]\n",
      "loss: 0.649410  [140800/175341]\n",
      "loss: 0.313157  [142400/175341]\n",
      "loss: 0.435844  [144000/175341]\n",
      "loss: 0.762485  [145600/175341]\n",
      "loss: 0.695695  [147200/175341]\n",
      "loss: 0.484977  [148800/175341]\n",
      "loss: 0.306927  [150400/175341]\n",
      "loss: 0.336814  [152000/175341]\n",
      "loss: 0.482448  [153600/175341]\n",
      "loss: 0.507225  [155200/175341]\n",
      "loss: 0.289321  [156800/175341]\n",
      "loss: 0.753654  [158400/175341]\n",
      "loss: 0.293835  [160000/175341]\n",
      "loss: 0.804272  [161600/175341]\n",
      "loss: 0.309826  [163200/175341]\n",
      "loss: 0.401012  [164800/175341]\n",
      "loss: 0.458289  [166400/175341]\n",
      "loss: 0.498843  [168000/175341]\n",
      "loss: 0.496111  [169600/175341]\n",
      "loss: 0.506663  [171200/175341]\n",
      "loss: 0.334804  [172800/175341]\n",
      "loss: 0.712218  [174400/175341]\n",
      "Train Accuracy: 79.1349%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.604429, F1-score: 73.82%, Macro_F1-Score:  33.98%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.537990  [    0/175341]\n",
      "loss: 0.643606  [ 1600/175341]\n",
      "loss: 0.249795  [ 3200/175341]\n",
      "loss: 0.577413  [ 4800/175341]\n",
      "loss: 0.578896  [ 6400/175341]\n",
      "loss: 0.833501  [ 8000/175341]\n",
      "loss: 0.570687  [ 9600/175341]\n",
      "loss: 0.681452  [11200/175341]\n",
      "loss: 0.152562  [12800/175341]\n",
      "loss: 0.200755  [14400/175341]\n",
      "loss: 0.720257  [16000/175341]\n",
      "loss: 1.286013  [17600/175341]\n",
      "loss: 0.454064  [19200/175341]\n",
      "loss: 0.132588  [20800/175341]\n",
      "loss: 0.563469  [22400/175341]\n",
      "loss: 0.247719  [24000/175341]\n",
      "loss: 0.613304  [25600/175341]\n",
      "loss: 1.140889  [27200/175341]\n",
      "loss: 0.045935  [28800/175341]\n",
      "loss: 0.389561  [30400/175341]\n",
      "loss: 0.707963  [32000/175341]\n",
      "loss: 0.524297  [33600/175341]\n",
      "loss: 0.461447  [35200/175341]\n",
      "loss: 0.604897  [36800/175341]\n",
      "loss: 0.506108  [38400/175341]\n",
      "loss: 0.786162  [40000/175341]\n",
      "loss: 0.690097  [41600/175341]\n",
      "loss: 0.670318  [43200/175341]\n",
      "loss: 0.547363  [44800/175341]\n",
      "loss: 0.476812  [46400/175341]\n",
      "loss: 0.901586  [48000/175341]\n",
      "loss: 0.515115  [49600/175341]\n",
      "loss: 0.378141  [51200/175341]\n",
      "loss: 0.287535  [52800/175341]\n",
      "loss: 0.402410  [54400/175341]\n",
      "loss: 0.278029  [56000/175341]\n",
      "loss: 0.341113  [57600/175341]\n",
      "loss: 0.483018  [59200/175341]\n",
      "loss: 0.843745  [60800/175341]\n",
      "loss: 0.648864  [62400/175341]\n",
      "loss: 0.595919  [64000/175341]\n",
      "loss: 0.978542  [65600/175341]\n",
      "loss: 0.531668  [67200/175341]\n",
      "loss: 0.910357  [68800/175341]\n",
      "loss: 0.428934  [70400/175341]\n",
      "loss: 0.502924  [72000/175341]\n",
      "loss: 0.583662  [73600/175341]\n",
      "loss: 0.411076  [75200/175341]\n",
      "loss: 0.575675  [76800/175341]\n",
      "loss: 0.449277  [78400/175341]\n",
      "loss: 0.349988  [80000/175341]\n",
      "loss: 0.500585  [81600/175341]\n",
      "loss: 0.187464  [83200/175341]\n",
      "loss: 0.453573  [84800/175341]\n",
      "loss: 0.717115  [86400/175341]\n",
      "loss: 0.390075  [88000/175341]\n",
      "loss: 0.287826  [89600/175341]\n",
      "loss: 0.434677  [91200/175341]\n",
      "loss: 0.477532  [92800/175341]\n",
      "loss: 0.296994  [94400/175341]\n",
      "loss: 0.711712  [96000/175341]\n",
      "loss: 0.780227  [97600/175341]\n",
      "loss: 0.503652  [99200/175341]\n",
      "loss: 0.724672  [100800/175341]\n",
      "loss: 0.426642  [102400/175341]\n",
      "loss: 0.497680  [104000/175341]\n",
      "loss: 0.944602  [105600/175341]\n",
      "loss: 0.612285  [107200/175341]\n",
      "loss: 0.432977  [108800/175341]\n",
      "loss: 0.444051  [110400/175341]\n",
      "loss: 0.195468  [112000/175341]\n",
      "loss: 0.378314  [113600/175341]\n",
      "loss: 0.219767  [115200/175341]\n",
      "loss: 0.565936  [116800/175341]\n",
      "loss: 0.279586  [118400/175341]\n",
      "loss: 0.677446  [120000/175341]\n",
      "loss: 0.276195  [121600/175341]\n",
      "loss: 0.656235  [123200/175341]\n",
      "loss: 0.269886  [124800/175341]\n",
      "loss: 0.263577  [126400/175341]\n",
      "loss: 0.207645  [128000/175341]\n",
      "loss: 0.723197  [129600/175341]\n",
      "loss: 0.526653  [131200/175341]\n",
      "loss: 0.599039  [132800/175341]\n",
      "loss: 0.521779  [134400/175341]\n",
      "loss: 0.570885  [136000/175341]\n",
      "loss: 0.309264  [137600/175341]\n",
      "loss: 0.635530  [139200/175341]\n",
      "loss: 0.413606  [140800/175341]\n",
      "loss: 0.341900  [142400/175341]\n",
      "loss: 0.516771  [144000/175341]\n",
      "loss: 0.483974  [145600/175341]\n",
      "loss: 1.144629  [147200/175341]\n",
      "loss: 0.711743  [148800/175341]\n",
      "loss: 0.700402  [150400/175341]\n",
      "loss: 0.598879  [152000/175341]\n",
      "loss: 0.299343  [153600/175341]\n",
      "loss: 0.364336  [155200/175341]\n",
      "loss: 0.819081  [156800/175341]\n",
      "loss: 0.241508  [158400/175341]\n",
      "loss: 0.286601  [160000/175341]\n",
      "loss: 0.273411  [161600/175341]\n",
      "loss: 0.312545  [163200/175341]\n",
      "loss: 0.261805  [164800/175341]\n",
      "loss: 0.693898  [166400/175341]\n",
      "loss: 0.703494  [168000/175341]\n",
      "loss: 0.668055  [169600/175341]\n",
      "loss: 0.548140  [171200/175341]\n",
      "loss: 0.697347  [172800/175341]\n",
      "loss: 0.314604  [174400/175341]\n",
      "Train Accuracy: 79.2108%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.619857, F1-score: 73.77%, Macro_F1-Score:  33.78%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.341246  [    0/175341]\n",
      "loss: 0.632740  [ 1600/175341]\n",
      "loss: 0.490444  [ 3200/175341]\n",
      "loss: 1.139331  [ 4800/175341]\n",
      "loss: 1.012681  [ 6400/175341]\n",
      "loss: 0.380977  [ 8000/175341]\n",
      "loss: 0.408569  [ 9600/175341]\n",
      "loss: 0.672977  [11200/175341]\n",
      "loss: 0.565417  [12800/175341]\n",
      "loss: 0.606714  [14400/175341]\n",
      "loss: 0.374250  [16000/175341]\n",
      "loss: 0.239188  [17600/175341]\n",
      "loss: 0.727483  [19200/175341]\n",
      "loss: 0.645767  [20800/175341]\n",
      "loss: 0.459323  [22400/175341]\n",
      "loss: 0.359851  [24000/175341]\n",
      "loss: 0.800253  [25600/175341]\n",
      "loss: 0.369559  [27200/175341]\n",
      "loss: 0.652200  [28800/175341]\n",
      "loss: 0.428101  [30400/175341]\n",
      "loss: 0.573539  [32000/175341]\n",
      "loss: 0.156295  [33600/175341]\n",
      "loss: 0.855868  [35200/175341]\n",
      "loss: 0.356100  [36800/175341]\n",
      "loss: 0.764479  [38400/175341]\n",
      "loss: 0.143707  [40000/175341]\n",
      "loss: 0.500613  [41600/175341]\n",
      "loss: 0.555247  [43200/175341]\n",
      "loss: 0.635510  [44800/175341]\n",
      "loss: 0.375607  [46400/175341]\n",
      "loss: 0.496715  [48000/175341]\n",
      "loss: 0.370269  [49600/175341]\n",
      "loss: 0.340479  [51200/175341]\n",
      "loss: 0.481346  [52800/175341]\n",
      "loss: 0.698859  [54400/175341]\n",
      "loss: 0.516085  [56000/175341]\n",
      "loss: 0.916243  [57600/175341]\n",
      "loss: 0.666129  [59200/175341]\n",
      "loss: 0.290392  [60800/175341]\n",
      "loss: 0.557562  [62400/175341]\n",
      "loss: 0.797706  [64000/175341]\n",
      "loss: 1.067084  [65600/175341]\n",
      "loss: 0.840719  [67200/175341]\n",
      "loss: 0.400589  [68800/175341]\n",
      "loss: 0.300481  [70400/175341]\n",
      "loss: 0.223266  [72000/175341]\n",
      "loss: 0.161714  [73600/175341]\n",
      "loss: 0.579796  [75200/175341]\n",
      "loss: 0.366722  [76800/175341]\n",
      "loss: 0.680486  [78400/175341]\n",
      "loss: 0.752378  [80000/175341]\n",
      "loss: 0.275645  [81600/175341]\n",
      "loss: 0.500524  [83200/175341]\n",
      "loss: 0.274147  [84800/175341]\n",
      "loss: 0.334765  [86400/175341]\n",
      "loss: 0.373856  [88000/175341]\n",
      "loss: 0.371510  [89600/175341]\n",
      "loss: 0.325856  [91200/175341]\n",
      "loss: 0.310644  [92800/175341]\n",
      "loss: 0.575130  [94400/175341]\n",
      "loss: 0.310237  [96000/175341]\n",
      "loss: 0.137998  [97600/175341]\n",
      "loss: 0.177201  [99200/175341]\n",
      "loss: 0.306369  [100800/175341]\n",
      "loss: 0.889669  [102400/175341]\n",
      "loss: 0.410328  [104000/175341]\n",
      "loss: 0.406014  [105600/175341]\n",
      "loss: 0.749643  [107200/175341]\n",
      "loss: 0.349929  [108800/175341]\n",
      "loss: 0.427306  [110400/175341]\n",
      "loss: 0.709916  [112000/175341]\n",
      "loss: 0.717869  [113600/175341]\n",
      "loss: 0.458528  [115200/175341]\n",
      "loss: 0.502609  [116800/175341]\n",
      "loss: 0.662574  [118400/175341]\n",
      "loss: 0.679105  [120000/175341]\n",
      "loss: 0.389379  [121600/175341]\n",
      "loss: 0.221653  [123200/175341]\n",
      "loss: 0.906339  [124800/175341]\n",
      "loss: 0.405590  [126400/175341]\n",
      "loss: 0.486034  [128000/175341]\n",
      "loss: 0.478168  [129600/175341]\n",
      "loss: 0.507753  [131200/175341]\n",
      "loss: 0.384976  [132800/175341]\n",
      "loss: 1.427850  [134400/175341]\n",
      "loss: 0.375464  [136000/175341]\n",
      "loss: 0.586009  [137600/175341]\n",
      "loss: 0.836731  [139200/175341]\n",
      "loss: 0.635049  [140800/175341]\n",
      "loss: 0.550300  [142400/175341]\n",
      "loss: 0.802414  [144000/175341]\n",
      "loss: 0.681275  [145600/175341]\n",
      "loss: 0.533359  [147200/175341]\n",
      "loss: 0.539679  [148800/175341]\n",
      "loss: 0.459032  [150400/175341]\n",
      "loss: 0.615901  [152000/175341]\n",
      "loss: 0.581241  [153600/175341]\n",
      "loss: 0.768759  [155200/175341]\n",
      "loss: 0.220233  [156800/175341]\n",
      "loss: 0.664283  [158400/175341]\n",
      "loss: 0.261419  [160000/175341]\n",
      "loss: 0.243491  [161600/175341]\n",
      "loss: 0.624607  [163200/175341]\n",
      "loss: 0.394587  [164800/175341]\n",
      "loss: 0.469321  [166400/175341]\n",
      "loss: 0.291306  [168000/175341]\n",
      "loss: 0.675336  [169600/175341]\n",
      "loss: 0.346715  [171200/175341]\n",
      "loss: 0.155633  [172800/175341]\n",
      "loss: 0.443663  [174400/175341]\n",
      "Train Accuracy: 79.2239%\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.633976, F1-score: 73.62%, Macro_F1-Score:  33.85%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.617111  [    0/175341]\n",
      "loss: 0.447287  [ 1600/175341]\n",
      "loss: 0.385633  [ 3200/175341]\n",
      "loss: 0.658526  [ 4800/175341]\n",
      "loss: 0.534933  [ 6400/175341]\n",
      "loss: 0.525136  [ 8000/175341]\n",
      "loss: 0.413627  [ 9600/175341]\n",
      "loss: 0.540483  [11200/175341]\n",
      "loss: 0.415968  [12800/175341]\n",
      "loss: 0.380437  [14400/175341]\n",
      "loss: 0.856798  [16000/175341]\n",
      "loss: 0.258117  [17600/175341]\n",
      "loss: 0.800470  [19200/175341]\n",
      "loss: 0.460859  [20800/175341]\n",
      "loss: 0.671285  [22400/175341]\n",
      "loss: 0.828735  [24000/175341]\n",
      "loss: 0.558203  [25600/175341]\n",
      "loss: 0.184549  [27200/175341]\n",
      "loss: 0.515487  [28800/175341]\n",
      "loss: 0.983049  [30400/175341]\n",
      "loss: 0.189783  [32000/175341]\n",
      "loss: 0.500458  [33600/175341]\n",
      "loss: 0.349761  [35200/175341]\n",
      "loss: 0.473866  [36800/175341]\n",
      "loss: 1.334880  [38400/175341]\n",
      "loss: 0.657987  [40000/175341]\n",
      "loss: 0.447839  [41600/175341]\n",
      "loss: 0.759388  [43200/175341]\n",
      "loss: 0.423271  [44800/175341]\n",
      "loss: 0.349013  [46400/175341]\n",
      "loss: 0.795700  [48000/175341]\n",
      "loss: 0.466069  [49600/175341]\n",
      "loss: 0.731238  [51200/175341]\n",
      "loss: 0.450522  [52800/175341]\n",
      "loss: 0.655252  [54400/175341]\n",
      "loss: 0.423237  [56000/175341]\n",
      "loss: 0.428083  [57600/175341]\n",
      "loss: 0.039458  [59200/175341]\n",
      "loss: 0.372844  [60800/175341]\n",
      "loss: 0.157292  [62400/175341]\n",
      "loss: 0.464162  [64000/175341]\n",
      "loss: 0.674744  [65600/175341]\n",
      "loss: 0.452588  [67200/175341]\n",
      "loss: 0.543470  [68800/175341]\n",
      "loss: 0.508034  [70400/175341]\n",
      "loss: 0.770006  [72000/175341]\n",
      "loss: 0.656669  [73600/175341]\n",
      "loss: 0.462669  [75200/175341]\n",
      "loss: 0.430782  [76800/175341]\n",
      "loss: 0.458180  [78400/175341]\n",
      "loss: 0.522030  [80000/175341]\n",
      "loss: 0.615770  [81600/175341]\n",
      "loss: 0.610571  [83200/175341]\n",
      "loss: 0.273940  [84800/175341]\n",
      "loss: 0.309904  [86400/175341]\n",
      "loss: 0.335944  [88000/175341]\n",
      "loss: 0.413150  [89600/175341]\n",
      "loss: 0.462196  [91200/175341]\n",
      "loss: 0.574012  [92800/175341]\n",
      "loss: 0.327395  [94400/175341]\n",
      "loss: 0.473569  [96000/175341]\n",
      "loss: 0.386253  [97600/175341]\n",
      "loss: 0.509376  [99200/175341]\n",
      "loss: 0.673997  [100800/175341]\n",
      "loss: 1.492386  [102400/175341]\n",
      "loss: 0.359561  [104000/175341]\n",
      "loss: 0.353401  [105600/175341]\n",
      "loss: 0.210957  [107200/175341]\n",
      "loss: 0.334399  [108800/175341]\n",
      "loss: 0.382381  [110400/175341]\n",
      "loss: 0.378449  [112000/175341]\n",
      "loss: 0.301107  [113600/175341]\n",
      "loss: 0.344603  [115200/175341]\n",
      "loss: 0.236509  [116800/175341]\n",
      "loss: 0.571848  [118400/175341]\n",
      "loss: 0.604391  [120000/175341]\n",
      "loss: 0.441696  [121600/175341]\n",
      "loss: 0.300075  [123200/175341]\n",
      "loss: 0.509081  [124800/175341]\n",
      "loss: 0.189943  [126400/175341]\n",
      "loss: 0.412240  [128000/175341]\n",
      "loss: 0.591676  [129600/175341]\n",
      "loss: 0.364218  [131200/175341]\n",
      "loss: 0.380771  [132800/175341]\n",
      "loss: 0.674800  [134400/175341]\n",
      "loss: 0.750293  [136000/175341]\n",
      "loss: 0.479898  [137600/175341]\n",
      "loss: 0.375307  [139200/175341]\n",
      "loss: 0.137606  [140800/175341]\n",
      "loss: 0.250574  [142400/175341]\n",
      "loss: 0.706902  [144000/175341]\n",
      "loss: 0.963645  [145600/175341]\n",
      "loss: 0.659659  [147200/175341]\n",
      "loss: 0.715821  [148800/175341]\n",
      "loss: 0.502578  [150400/175341]\n",
      "loss: 0.280934  [152000/175341]\n",
      "loss: 0.357972  [153600/175341]\n",
      "loss: 0.652602  [155200/175341]\n",
      "loss: 0.149608  [156800/175341]\n",
      "loss: 0.243434  [158400/175341]\n",
      "loss: 0.731226  [160000/175341]\n",
      "loss: 0.507042  [161600/175341]\n",
      "loss: 0.335556  [163200/175341]\n",
      "loss: 0.384014  [164800/175341]\n",
      "loss: 0.280066  [166400/175341]\n",
      "loss: 0.779625  [168000/175341]\n",
      "loss: 0.580938  [169600/175341]\n",
      "loss: 0.864635  [171200/175341]\n",
      "loss: 0.804254  [172800/175341]\n",
      "loss: 0.552652  [174400/175341]\n",
      "Train Accuracy: 79.3283%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.617285, F1-score: 73.68%, Macro_F1-Score:  34.35%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.739378  [    0/175341]\n",
      "loss: 0.397373  [ 1600/175341]\n",
      "loss: 0.844990  [ 3200/175341]\n",
      "loss: 0.468227  [ 4800/175341]\n",
      "loss: 0.323172  [ 6400/175341]\n",
      "loss: 0.451577  [ 8000/175341]\n",
      "loss: 0.355240  [ 9600/175341]\n",
      "loss: 0.603610  [11200/175341]\n",
      "loss: 0.153737  [12800/175341]\n",
      "loss: 0.569962  [14400/175341]\n",
      "loss: 0.387070  [16000/175341]\n",
      "loss: 0.671992  [17600/175341]\n",
      "loss: 0.215384  [19200/175341]\n",
      "loss: 0.498981  [20800/175341]\n",
      "loss: 0.288805  [22400/175341]\n",
      "loss: 0.902258  [24000/175341]\n",
      "loss: 0.479619  [25600/175341]\n",
      "loss: 0.492112  [27200/175341]\n",
      "loss: 0.739011  [28800/175341]\n",
      "loss: 0.354476  [30400/175341]\n",
      "loss: 0.654645  [32000/175341]\n",
      "loss: 0.795093  [33600/175341]\n",
      "loss: 0.329790  [35200/175341]\n",
      "loss: 0.410561  [36800/175341]\n",
      "loss: 0.905052  [38400/175341]\n",
      "loss: 0.494342  [40000/175341]\n",
      "loss: 0.299581  [41600/175341]\n",
      "loss: 0.269424  [43200/175341]\n",
      "loss: 0.440697  [44800/175341]\n",
      "loss: 0.446320  [46400/175341]\n",
      "loss: 0.545919  [48000/175341]\n",
      "loss: 0.448443  [49600/175341]\n",
      "loss: 0.553411  [51200/175341]\n",
      "loss: 0.301928  [52800/175341]\n",
      "loss: 0.372773  [54400/175341]\n",
      "loss: 0.608856  [56000/175341]\n",
      "loss: 0.478035  [57600/175341]\n",
      "loss: 0.355167  [59200/175341]\n",
      "loss: 0.254452  [60800/175341]\n",
      "loss: 0.442311  [62400/175341]\n",
      "loss: 0.690528  [64000/175341]\n",
      "loss: 0.441841  [65600/175341]\n",
      "loss: 0.561813  [67200/175341]\n",
      "loss: 0.418784  [68800/175341]\n",
      "loss: 0.655493  [70400/175341]\n",
      "loss: 0.278044  [72000/175341]\n",
      "loss: 0.590902  [73600/175341]\n",
      "loss: 0.879927  [75200/175341]\n",
      "loss: 0.577573  [76800/175341]\n",
      "loss: 0.407813  [78400/175341]\n",
      "loss: 0.597414  [80000/175341]\n",
      "loss: 0.659986  [81600/175341]\n",
      "loss: 0.712851  [83200/175341]\n",
      "loss: 0.540162  [84800/175341]\n",
      "loss: 0.356885  [86400/175341]\n",
      "loss: 0.475353  [88000/175341]\n",
      "loss: 0.188990  [89600/175341]\n",
      "loss: 0.241706  [91200/175341]\n",
      "loss: 0.551913  [92800/175341]\n",
      "loss: 0.693025  [94400/175341]\n",
      "loss: 0.631653  [96000/175341]\n",
      "loss: 0.977746  [97600/175341]\n",
      "loss: 0.487473  [99200/175341]\n",
      "loss: 0.514414  [100800/175341]\n",
      "loss: 0.290538  [102400/175341]\n",
      "loss: 0.813467  [104000/175341]\n",
      "loss: 1.079203  [105600/175341]\n",
      "loss: 0.520980  [107200/175341]\n",
      "loss: 0.498862  [108800/175341]\n",
      "loss: 0.128885  [110400/175341]\n",
      "loss: 0.867863  [112000/175341]\n",
      "loss: 0.653877  [113600/175341]\n",
      "loss: 0.436511  [115200/175341]\n",
      "loss: 0.493381  [116800/175341]\n",
      "loss: 0.460740  [118400/175341]\n",
      "loss: 0.255351  [120000/175341]\n",
      "loss: 0.367343  [121600/175341]\n",
      "loss: 0.386205  [123200/175341]\n",
      "loss: 0.481433  [124800/175341]\n",
      "loss: 0.909622  [126400/175341]\n",
      "loss: 0.373367  [128000/175341]\n",
      "loss: 0.300959  [129600/175341]\n",
      "loss: 0.629946  [131200/175341]\n",
      "loss: 0.290237  [132800/175341]\n",
      "loss: 0.871916  [134400/175341]\n",
      "loss: 0.568541  [136000/175341]\n",
      "loss: 0.674254  [137600/175341]\n",
      "loss: 1.241621  [139200/175341]\n",
      "loss: 0.417577  [140800/175341]\n",
      "loss: 0.602525  [142400/175341]\n",
      "loss: 0.727760  [144000/175341]\n",
      "loss: 0.298957  [145600/175341]\n",
      "loss: 0.489393  [147200/175341]\n",
      "loss: 0.575192  [148800/175341]\n",
      "loss: 0.989894  [150400/175341]\n",
      "loss: 0.581180  [152000/175341]\n",
      "loss: 0.542317  [153600/175341]\n",
      "loss: 0.290585  [155200/175341]\n",
      "loss: 0.542114  [156800/175341]\n",
      "loss: 0.364327  [158400/175341]\n",
      "loss: 0.268254  [160000/175341]\n",
      "loss: 0.395375  [161600/175341]\n",
      "loss: 0.333163  [163200/175341]\n",
      "loss: 0.349448  [164800/175341]\n",
      "loss: 0.756875  [166400/175341]\n",
      "loss: 0.669040  [168000/175341]\n",
      "loss: 0.872302  [169600/175341]\n",
      "loss: 0.588361  [171200/175341]\n",
      "loss: 0.421818  [172800/175341]\n",
      "loss: 0.603615  [174400/175341]\n",
      "Train Accuracy: 79.4036%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.609658, F1-score: 73.91%, Macro_F1-Score:  34.41%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.610520  [    0/175341]\n",
      "loss: 0.681330  [ 1600/175341]\n",
      "loss: 0.278945  [ 3200/175341]\n",
      "loss: 0.425498  [ 4800/175341]\n",
      "loss: 0.658856  [ 6400/175341]\n",
      "loss: 0.257626  [ 8000/175341]\n",
      "loss: 0.263444  [ 9600/175341]\n",
      "loss: 0.250808  [11200/175341]\n",
      "loss: 0.478090  [12800/175341]\n",
      "loss: 0.586554  [14400/175341]\n",
      "loss: 0.227603  [16000/175341]\n",
      "loss: 0.252688  [17600/175341]\n",
      "loss: 0.546313  [19200/175341]\n",
      "loss: 0.919069  [20800/175341]\n",
      "loss: 0.980935  [22400/175341]\n",
      "loss: 0.410502  [24000/175341]\n",
      "loss: 0.285379  [25600/175341]\n",
      "loss: 0.518175  [27200/175341]\n",
      "loss: 0.532231  [28800/175341]\n",
      "loss: 1.029064  [30400/175341]\n",
      "loss: 0.755093  [32000/175341]\n",
      "loss: 0.570513  [33600/175341]\n",
      "loss: 0.260074  [35200/175341]\n",
      "loss: 0.641887  [36800/175341]\n",
      "loss: 0.603556  [38400/175341]\n",
      "loss: 0.305967  [40000/175341]\n",
      "loss: 0.421843  [41600/175341]\n",
      "loss: 0.270022  [43200/175341]\n",
      "loss: 0.579382  [44800/175341]\n",
      "loss: 0.591679  [46400/175341]\n",
      "loss: 0.316430  [48000/175341]\n",
      "loss: 0.232894  [49600/175341]\n",
      "loss: 0.546157  [51200/175341]\n",
      "loss: 0.302919  [52800/175341]\n",
      "loss: 0.301985  [54400/175341]\n",
      "loss: 0.767131  [56000/175341]\n",
      "loss: 0.711247  [57600/175341]\n",
      "loss: 0.337393  [59200/175341]\n",
      "loss: 0.555395  [60800/175341]\n",
      "loss: 0.727176  [62400/175341]\n",
      "loss: 0.304524  [64000/175341]\n",
      "loss: 0.698686  [65600/175341]\n",
      "loss: 0.438262  [67200/175341]\n",
      "loss: 0.457512  [68800/175341]\n",
      "loss: 0.676240  [70400/175341]\n",
      "loss: 0.901263  [72000/175341]\n",
      "loss: 0.773667  [73600/175341]\n",
      "loss: 0.477539  [75200/175341]\n",
      "loss: 0.816614  [76800/175341]\n",
      "loss: 0.309302  [78400/175341]\n",
      "loss: 0.651920  [80000/175341]\n",
      "loss: 0.716442  [81600/175341]\n",
      "loss: 0.308633  [83200/175341]\n",
      "loss: 0.607602  [84800/175341]\n",
      "loss: 0.238792  [86400/175341]\n",
      "loss: 0.380233  [88000/175341]\n",
      "loss: 0.552697  [89600/175341]\n",
      "loss: 0.767987  [91200/175341]\n",
      "loss: 0.872743  [92800/175341]\n",
      "loss: 0.665794  [94400/175341]\n",
      "loss: 0.409633  [96000/175341]\n",
      "loss: 0.686054  [97600/175341]\n",
      "loss: 0.928153  [99200/175341]\n",
      "loss: 0.354253  [100800/175341]\n",
      "loss: 0.211815  [102400/175341]\n",
      "loss: 0.955873  [104000/175341]\n",
      "loss: 0.336897  [105600/175341]\n",
      "loss: 0.136969  [107200/175341]\n",
      "loss: 0.705154  [108800/175341]\n",
      "loss: 0.748164  [110400/175341]\n",
      "loss: 0.502671  [112000/175341]\n",
      "loss: 0.534577  [113600/175341]\n",
      "loss: 0.365374  [115200/175341]\n",
      "loss: 0.532123  [116800/175341]\n",
      "loss: 0.236297  [118400/175341]\n",
      "loss: 0.641313  [120000/175341]\n",
      "loss: 0.308936  [121600/175341]\n",
      "loss: 0.557824  [123200/175341]\n",
      "loss: 0.913077  [124800/175341]\n",
      "loss: 0.414319  [126400/175341]\n",
      "loss: 0.346539  [128000/175341]\n",
      "loss: 0.492672  [129600/175341]\n",
      "loss: 0.193492  [131200/175341]\n",
      "loss: 0.593291  [132800/175341]\n",
      "loss: 0.706939  [134400/175341]\n",
      "loss: 0.414967  [136000/175341]\n",
      "loss: 0.648550  [137600/175341]\n",
      "loss: 0.433831  [139200/175341]\n",
      "loss: 0.701345  [140800/175341]\n",
      "loss: 0.312414  [142400/175341]\n",
      "loss: 0.337792  [144000/175341]\n",
      "loss: 0.499217  [145600/175341]\n",
      "loss: 0.344517  [147200/175341]\n",
      "loss: 0.476103  [148800/175341]\n",
      "loss: 0.387527  [150400/175341]\n",
      "loss: 0.650397  [152000/175341]\n",
      "loss: 0.663411  [153600/175341]\n",
      "loss: 0.179031  [155200/175341]\n",
      "loss: 0.307307  [156800/175341]\n",
      "loss: 0.836560  [158400/175341]\n",
      "loss: 0.368290  [160000/175341]\n",
      "loss: 0.409037  [161600/175341]\n",
      "loss: 0.528726  [163200/175341]\n",
      "loss: 0.559287  [164800/175341]\n",
      "loss: 0.860803  [166400/175341]\n",
      "loss: 0.943088  [168000/175341]\n",
      "loss: 0.326237  [169600/175341]\n",
      "loss: 0.476042  [171200/175341]\n",
      "loss: 0.878957  [172800/175341]\n",
      "loss: 0.279137  [174400/175341]\n",
      "Train Accuracy: 79.4828%\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.618265, F1-score: 72.35%, Macro_F1-Score:  34.51%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdafbf2b-d151-40e6-b9d2-35b65c2b9781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.647921  [    0/175341]\n",
      "loss: 0.747021  [ 1600/175341]\n",
      "loss: 0.367443  [ 3200/175341]\n",
      "loss: 0.667875  [ 4800/175341]\n",
      "loss: 0.423390  [ 6400/175341]\n",
      "loss: 0.280544  [ 8000/175341]\n",
      "loss: 0.829443  [ 9600/175341]\n",
      "loss: 0.585493  [11200/175341]\n",
      "loss: 0.194600  [12800/175341]\n",
      "loss: 0.290210  [14400/175341]\n",
      "loss: 0.279880  [16000/175341]\n",
      "loss: 0.434381  [17600/175341]\n",
      "loss: 0.568370  [19200/175341]\n",
      "loss: 0.558152  [20800/175341]\n",
      "loss: 0.351839  [22400/175341]\n",
      "loss: 0.258956  [24000/175341]\n",
      "loss: 0.692052  [25600/175341]\n",
      "loss: 0.532199  [27200/175341]\n",
      "loss: 0.654831  [28800/175341]\n",
      "loss: 0.401729  [30400/175341]\n",
      "loss: 0.879043  [32000/175341]\n",
      "loss: 0.674484  [33600/175341]\n",
      "loss: 0.378330  [35200/175341]\n",
      "loss: 0.116435  [36800/175341]\n",
      "loss: 0.327877  [38400/175341]\n",
      "loss: 0.653554  [40000/175341]\n",
      "loss: 0.264903  [41600/175341]\n",
      "loss: 0.406363  [43200/175341]\n",
      "loss: 0.324255  [44800/175341]\n",
      "loss: 0.286960  [46400/175341]\n",
      "loss: 0.394339  [48000/175341]\n",
      "loss: 0.418765  [49600/175341]\n",
      "loss: 0.469002  [51200/175341]\n",
      "loss: 0.581007  [52800/175341]\n",
      "loss: 0.515539  [54400/175341]\n",
      "loss: 0.635815  [56000/175341]\n",
      "loss: 0.436609  [57600/175341]\n",
      "loss: 0.723287  [59200/175341]\n",
      "loss: 0.551160  [60800/175341]\n",
      "loss: 0.341959  [62400/175341]\n",
      "loss: 0.515452  [64000/175341]\n",
      "loss: 0.827367  [65600/175341]\n",
      "loss: 0.811686  [67200/175341]\n",
      "loss: 0.316256  [68800/175341]\n",
      "loss: 0.532341  [70400/175341]\n",
      "loss: 0.298597  [72000/175341]\n",
      "loss: 0.492303  [73600/175341]\n",
      "loss: 0.415550  [75200/175341]\n",
      "loss: 0.817258  [76800/175341]\n",
      "loss: 0.240796  [78400/175341]\n",
      "loss: 0.579517  [80000/175341]\n",
      "loss: 0.788629  [81600/175341]\n",
      "loss: 0.554331  [83200/175341]\n",
      "loss: 0.820523  [84800/175341]\n",
      "loss: 0.212223  [86400/175341]\n",
      "loss: 0.362215  [88000/175341]\n",
      "loss: 0.296182  [89600/175341]\n",
      "loss: 0.416366  [91200/175341]\n",
      "loss: 0.479898  [92800/175341]\n",
      "loss: 0.276091  [94400/175341]\n",
      "loss: 0.509701  [96000/175341]\n",
      "loss: 0.207878  [97600/175341]\n",
      "loss: 0.383740  [99200/175341]\n",
      "loss: 0.983999  [100800/175341]\n",
      "loss: 0.757217  [102400/175341]\n",
      "loss: 0.746938  [104000/175341]\n",
      "loss: 0.585125  [105600/175341]\n",
      "loss: 0.728608  [107200/175341]\n",
      "loss: 0.443227  [108800/175341]\n",
      "loss: 0.268024  [110400/175341]\n",
      "loss: 0.423857  [112000/175341]\n",
      "loss: 0.398776  [113600/175341]\n",
      "loss: 0.697598  [115200/175341]\n",
      "loss: 0.198951  [116800/175341]\n",
      "loss: 0.363715  [118400/175341]\n",
      "loss: 0.352985  [120000/175341]\n",
      "loss: 0.290547  [121600/175341]\n",
      "loss: 0.882457  [123200/175341]\n",
      "loss: 0.587782  [124800/175341]\n",
      "loss: 0.377302  [126400/175341]\n",
      "loss: 0.582989  [128000/175341]\n",
      "loss: 0.556604  [129600/175341]\n",
      "loss: 0.719700  [131200/175341]\n",
      "loss: 0.648139  [132800/175341]\n",
      "loss: 0.454524  [134400/175341]\n",
      "loss: 0.683903  [136000/175341]\n",
      "loss: 0.195799  [137600/175341]\n",
      "loss: 0.503968  [139200/175341]\n",
      "loss: 0.932448  [140800/175341]\n",
      "loss: 0.395488  [142400/175341]\n",
      "loss: 0.568969  [144000/175341]\n",
      "loss: 0.621721  [145600/175341]\n",
      "loss: 0.491006  [147200/175341]\n",
      "loss: 0.309538  [148800/175341]\n",
      "loss: 0.266995  [150400/175341]\n",
      "loss: 1.139382  [152000/175341]\n",
      "loss: 0.427470  [153600/175341]\n",
      "loss: 0.197876  [155200/175341]\n",
      "loss: 0.475073  [156800/175341]\n",
      "loss: 0.721829  [158400/175341]\n",
      "loss: 0.429089  [160000/175341]\n",
      "loss: 0.696076  [161600/175341]\n",
      "loss: 0.613799  [163200/175341]\n",
      "loss: 0.670604  [164800/175341]\n",
      "loss: 0.717285  [166400/175341]\n",
      "loss: 0.251787  [168000/175341]\n",
      "loss: 0.703561  [169600/175341]\n",
      "loss: 0.742455  [171200/175341]\n",
      "loss: 0.726095  [172800/175341]\n",
      "loss: 0.874832  [174400/175341]\n",
      "Train Accuracy: 79.5125%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.601051, F1-score: 74.61%, Macro_F1-Score:  35.81%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.321303  [    0/175341]\n",
      "loss: 0.414818  [ 1600/175341]\n",
      "loss: 0.344995  [ 3200/175341]\n",
      "loss: 0.585030  [ 4800/175341]\n",
      "loss: 0.326412  [ 6400/175341]\n",
      "loss: 1.195418  [ 8000/175341]\n",
      "loss: 0.293508  [ 9600/175341]\n",
      "loss: 0.409054  [11200/175341]\n",
      "loss: 0.660430  [12800/175341]\n",
      "loss: 0.677153  [14400/175341]\n",
      "loss: 0.673303  [16000/175341]\n",
      "loss: 0.327480  [17600/175341]\n",
      "loss: 0.390248  [19200/175341]\n",
      "loss: 0.501876  [20800/175341]\n",
      "loss: 0.405103  [22400/175341]\n",
      "loss: 0.454751  [24000/175341]\n",
      "loss: 0.459912  [25600/175341]\n",
      "loss: 0.627210  [27200/175341]\n",
      "loss: 0.260311  [28800/175341]\n",
      "loss: 0.417643  [30400/175341]\n",
      "loss: 1.184226  [32000/175341]\n",
      "loss: 0.807370  [33600/175341]\n",
      "loss: 0.437571  [35200/175341]\n",
      "loss: 0.588588  [36800/175341]\n",
      "loss: 0.298950  [38400/175341]\n",
      "loss: 0.366761  [40000/175341]\n",
      "loss: 0.616549  [41600/175341]\n",
      "loss: 0.642389  [43200/175341]\n",
      "loss: 0.135714  [44800/175341]\n",
      "loss: 0.313258  [46400/175341]\n",
      "loss: 0.624100  [48000/175341]\n",
      "loss: 0.276526  [49600/175341]\n",
      "loss: 0.771235  [51200/175341]\n",
      "loss: 0.294332  [52800/175341]\n",
      "loss: 0.273656  [54400/175341]\n",
      "loss: 1.065168  [56000/175341]\n",
      "loss: 0.334217  [57600/175341]\n",
      "loss: 0.731468  [59200/175341]\n",
      "loss: 0.245197  [60800/175341]\n",
      "loss: 0.982245  [62400/175341]\n",
      "loss: 0.628338  [64000/175341]\n",
      "loss: 0.372748  [65600/175341]\n",
      "loss: 0.264720  [67200/175341]\n",
      "loss: 0.239384  [68800/175341]\n",
      "loss: 0.640697  [70400/175341]\n",
      "loss: 0.441606  [72000/175341]\n",
      "loss: 0.560082  [73600/175341]\n",
      "loss: 0.259833  [75200/175341]\n",
      "loss: 0.676996  [76800/175341]\n",
      "loss: 0.540183  [78400/175341]\n",
      "loss: 0.632224  [80000/175341]\n",
      "loss: 0.492743  [81600/175341]\n",
      "loss: 1.055537  [83200/175341]\n",
      "loss: 0.568833  [84800/175341]\n",
      "loss: 0.483649  [86400/175341]\n",
      "loss: 0.292523  [88000/175341]\n",
      "loss: 0.604197  [89600/175341]\n",
      "loss: 0.638196  [91200/175341]\n",
      "loss: 0.976278  [92800/175341]\n",
      "loss: 0.726343  [94400/175341]\n",
      "loss: 0.714039  [96000/175341]\n",
      "loss: 0.517781  [97600/175341]\n",
      "loss: 0.419453  [99200/175341]\n",
      "loss: 0.634927  [100800/175341]\n",
      "loss: 0.088602  [102400/175341]\n",
      "loss: 0.916011  [104000/175341]\n",
      "loss: 0.751652  [105600/175341]\n",
      "loss: 0.434898  [107200/175341]\n",
      "loss: 0.463970  [108800/175341]\n",
      "loss: 1.068153  [110400/175341]\n",
      "loss: 0.332282  [112000/175341]\n",
      "loss: 0.340047  [113600/175341]\n",
      "loss: 0.168379  [115200/175341]\n",
      "loss: 0.449522  [116800/175341]\n",
      "loss: 0.525003  [118400/175341]\n",
      "loss: 0.562084  [120000/175341]\n",
      "loss: 0.323613  [121600/175341]\n",
      "loss: 0.490505  [123200/175341]\n",
      "loss: 0.388470  [124800/175341]\n",
      "loss: 0.488061  [126400/175341]\n",
      "loss: 0.252779  [128000/175341]\n",
      "loss: 0.473431  [129600/175341]\n",
      "loss: 0.398627  [131200/175341]\n",
      "loss: 0.392201  [132800/175341]\n",
      "loss: 0.628094  [134400/175341]\n",
      "loss: 0.799019  [136000/175341]\n",
      "loss: 0.276177  [137600/175341]\n",
      "loss: 0.236879  [139200/175341]\n",
      "loss: 0.633074  [140800/175341]\n",
      "loss: 0.785983  [142400/175341]\n",
      "loss: 0.467324  [144000/175341]\n",
      "loss: 0.803989  [145600/175341]\n",
      "loss: 0.560640  [147200/175341]\n",
      "loss: 0.448442  [148800/175341]\n",
      "loss: 0.216213  [150400/175341]\n",
      "loss: 0.692415  [152000/175341]\n",
      "loss: 0.428358  [153600/175341]\n",
      "loss: 0.462897  [155200/175341]\n",
      "loss: 0.363406  [156800/175341]\n",
      "loss: 0.828293  [158400/175341]\n",
      "loss: 0.698932  [160000/175341]\n",
      "loss: 0.379747  [161600/175341]\n",
      "loss: 0.589731  [163200/175341]\n",
      "loss: 0.325819  [164800/175341]\n",
      "loss: 0.875972  [166400/175341]\n",
      "loss: 0.182281  [168000/175341]\n",
      "loss: 0.473983  [169600/175341]\n",
      "loss: 0.759809  [171200/175341]\n",
      "loss: 0.766955  [172800/175341]\n",
      "loss: 0.425097  [174400/175341]\n",
      "Train Accuracy: 79.5667%\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.616661, F1-score: 72.50%, Macro_F1-Score:  35.60%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.552634  [    0/175341]\n",
      "loss: 0.720466  [ 1600/175341]\n",
      "loss: 0.560455  [ 3200/175341]\n",
      "loss: 0.576995  [ 4800/175341]\n",
      "loss: 0.363245  [ 6400/175341]\n",
      "loss: 0.403551  [ 8000/175341]\n",
      "loss: 0.233901  [ 9600/175341]\n",
      "loss: 0.317249  [11200/175341]\n",
      "loss: 0.501081  [12800/175341]\n",
      "loss: 0.489727  [14400/175341]\n",
      "loss: 0.398391  [16000/175341]\n",
      "loss: 0.291429  [17600/175341]\n",
      "loss: 0.255832  [19200/175341]\n",
      "loss: 0.350617  [20800/175341]\n",
      "loss: 0.353125  [22400/175341]\n",
      "loss: 0.872487  [24000/175341]\n",
      "loss: 0.455668  [25600/175341]\n",
      "loss: 0.232546  [27200/175341]\n",
      "loss: 0.613897  [28800/175341]\n",
      "loss: 0.493097  [30400/175341]\n",
      "loss: 0.507807  [32000/175341]\n",
      "loss: 0.379050  [33600/175341]\n",
      "loss: 0.787364  [35200/175341]\n",
      "loss: 0.333423  [36800/175341]\n",
      "loss: 0.368834  [38400/175341]\n",
      "loss: 0.381410  [40000/175341]\n",
      "loss: 0.769690  [41600/175341]\n",
      "loss: 0.152903  [43200/175341]\n",
      "loss: 0.473931  [44800/175341]\n",
      "loss: 0.227277  [46400/175341]\n",
      "loss: 0.421315  [48000/175341]\n",
      "loss: 0.676918  [49600/175341]\n",
      "loss: 0.444523  [51200/175341]\n",
      "loss: 0.452487  [52800/175341]\n",
      "loss: 0.224091  [54400/175341]\n",
      "loss: 0.184680  [56000/175341]\n",
      "loss: 0.502525  [57600/175341]\n",
      "loss: 0.702549  [59200/175341]\n",
      "loss: 0.532231  [60800/175341]\n",
      "loss: 0.189962  [62400/175341]\n",
      "loss: 0.851582  [64000/175341]\n",
      "loss: 0.958555  [65600/175341]\n",
      "loss: 0.468900  [67200/175341]\n",
      "loss: 0.485343  [68800/175341]\n",
      "loss: 0.241949  [70400/175341]\n",
      "loss: 0.465240  [72000/175341]\n",
      "loss: 0.453570  [73600/175341]\n",
      "loss: 0.422582  [75200/175341]\n",
      "loss: 0.773319  [76800/175341]\n",
      "loss: 0.257337  [78400/175341]\n",
      "loss: 0.655658  [80000/175341]\n",
      "loss: 0.293051  [81600/175341]\n",
      "loss: 0.293090  [83200/175341]\n",
      "loss: 0.899952  [84800/175341]\n",
      "loss: 0.371352  [86400/175341]\n",
      "loss: 0.209709  [88000/175341]\n",
      "loss: 0.698625  [89600/175341]\n",
      "loss: 0.365084  [91200/175341]\n",
      "loss: 0.797075  [92800/175341]\n",
      "loss: 0.498805  [94400/175341]\n",
      "loss: 0.199129  [96000/175341]\n",
      "loss: 0.648168  [97600/175341]\n",
      "loss: 0.698405  [99200/175341]\n",
      "loss: 0.309207  [100800/175341]\n",
      "loss: 0.483379  [102400/175341]\n",
      "loss: 0.196021  [104000/175341]\n",
      "loss: 0.490784  [105600/175341]\n",
      "loss: 0.603609  [107200/175341]\n",
      "loss: 0.778036  [108800/175341]\n",
      "loss: 0.901680  [110400/175341]\n",
      "loss: 0.554053  [112000/175341]\n",
      "loss: 0.169874  [113600/175341]\n",
      "loss: 0.363744  [115200/175341]\n",
      "loss: 0.786970  [116800/175341]\n",
      "loss: 0.777388  [118400/175341]\n",
      "loss: 0.727617  [120000/175341]\n",
      "loss: 0.501684  [121600/175341]\n",
      "loss: 0.616214  [123200/175341]\n",
      "loss: 0.159581  [124800/175341]\n",
      "loss: 0.503593  [126400/175341]\n",
      "loss: 0.439057  [128000/175341]\n",
      "loss: 0.255463  [129600/175341]\n",
      "loss: 0.288600  [131200/175341]\n",
      "loss: 0.595865  [132800/175341]\n",
      "loss: 0.408312  [134400/175341]\n",
      "loss: 0.728134  [136000/175341]\n",
      "loss: 0.211930  [137600/175341]\n",
      "loss: 0.809412  [139200/175341]\n",
      "loss: 0.208244  [140800/175341]\n",
      "loss: 0.477005  [142400/175341]\n",
      "loss: 0.757324  [144000/175341]\n",
      "loss: 0.579524  [145600/175341]\n",
      "loss: 0.897658  [147200/175341]\n",
      "loss: 0.418618  [148800/175341]\n",
      "loss: 0.823569  [150400/175341]\n",
      "loss: 0.694197  [152000/175341]\n",
      "loss: 0.771505  [153600/175341]\n",
      "loss: 0.756262  [155200/175341]\n",
      "loss: 0.976069  [156800/175341]\n",
      "loss: 0.802914  [158400/175341]\n",
      "loss: 0.432641  [160000/175341]\n",
      "loss: 0.142629  [161600/175341]\n",
      "loss: 0.750702  [163200/175341]\n",
      "loss: 0.668009  [164800/175341]\n",
      "loss: 0.228967  [166400/175341]\n",
      "loss: 0.157187  [168000/175341]\n",
      "loss: 0.453957  [169600/175341]\n",
      "loss: 0.311251  [171200/175341]\n",
      "loss: 0.437183  [172800/175341]\n",
      "loss: 0.478063  [174400/175341]\n",
      "Train Accuracy: 79.5969%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.613059, F1-score: 73.63%, Macro_F1-Score:  35.59%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.324002  [    0/175341]\n",
      "loss: 0.514329  [ 1600/175341]\n",
      "loss: 0.114458  [ 3200/175341]\n",
      "loss: 0.812491  [ 4800/175341]\n",
      "loss: 0.500251  [ 6400/175341]\n",
      "loss: 0.455424  [ 8000/175341]\n",
      "loss: 0.885114  [ 9600/175341]\n",
      "loss: 0.457640  [11200/175341]\n",
      "loss: 0.507346  [12800/175341]\n",
      "loss: 0.877118  [14400/175341]\n",
      "loss: 0.765506  [16000/175341]\n",
      "loss: 0.703152  [17600/175341]\n",
      "loss: 0.515262  [19200/175341]\n",
      "loss: 0.522019  [20800/175341]\n",
      "loss: 0.410684  [22400/175341]\n",
      "loss: 0.586883  [24000/175341]\n",
      "loss: 0.444228  [25600/175341]\n",
      "loss: 0.578509  [27200/175341]\n",
      "loss: 0.197326  [28800/175341]\n",
      "loss: 0.427213  [30400/175341]\n",
      "loss: 0.605663  [32000/175341]\n",
      "loss: 0.533206  [33600/175341]\n",
      "loss: 0.345966  [35200/175341]\n",
      "loss: 0.516381  [36800/175341]\n",
      "loss: 0.351402  [38400/175341]\n",
      "loss: 0.288366  [40000/175341]\n",
      "loss: 0.420125  [41600/175341]\n",
      "loss: 0.447042  [43200/175341]\n",
      "loss: 0.309565  [44800/175341]\n",
      "loss: 0.390932  [46400/175341]\n",
      "loss: 0.353007  [48000/175341]\n",
      "loss: 0.448953  [49600/175341]\n",
      "loss: 0.375522  [51200/175341]\n",
      "loss: 0.333976  [52800/175341]\n",
      "loss: 0.831960  [54400/175341]\n",
      "loss: 0.661059  [56000/175341]\n",
      "loss: 0.602859  [57600/175341]\n",
      "loss: 0.409615  [59200/175341]\n",
      "loss: 0.712370  [60800/175341]\n",
      "loss: 0.455790  [62400/175341]\n",
      "loss: 0.391534  [64000/175341]\n",
      "loss: 0.560867  [65600/175341]\n",
      "loss: 0.369437  [67200/175341]\n",
      "loss: 0.451656  [68800/175341]\n",
      "loss: 0.492422  [70400/175341]\n",
      "loss: 0.439989  [72000/175341]\n",
      "loss: 0.532986  [73600/175341]\n",
      "loss: 0.540311  [75200/175341]\n",
      "loss: 0.488430  [76800/175341]\n",
      "loss: 0.280436  [78400/175341]\n",
      "loss: 0.273898  [80000/175341]\n",
      "loss: 0.327546  [81600/175341]\n",
      "loss: 0.412907  [83200/175341]\n",
      "loss: 0.868128  [84800/175341]\n",
      "loss: 0.715083  [86400/175341]\n",
      "loss: 1.040465  [88000/175341]\n",
      "loss: 0.375587  [89600/175341]\n",
      "loss: 0.528533  [91200/175341]\n",
      "loss: 0.447644  [92800/175341]\n",
      "loss: 0.330303  [94400/175341]\n",
      "loss: 0.459260  [96000/175341]\n",
      "loss: 0.715725  [97600/175341]\n",
      "loss: 0.362781  [99200/175341]\n",
      "loss: 0.393303  [100800/175341]\n",
      "loss: 0.574783  [102400/175341]\n",
      "loss: 0.671589  [104000/175341]\n",
      "loss: 0.788783  [105600/175341]\n",
      "loss: 0.370379  [107200/175341]\n",
      "loss: 0.150347  [108800/175341]\n",
      "loss: 0.236522  [110400/175341]\n",
      "loss: 0.464239  [112000/175341]\n",
      "loss: 0.543146  [113600/175341]\n",
      "loss: 0.460048  [115200/175341]\n",
      "loss: 0.150248  [116800/175341]\n",
      "loss: 0.307900  [118400/175341]\n",
      "loss: 0.074432  [120000/175341]\n",
      "loss: 0.558609  [121600/175341]\n",
      "loss: 0.733644  [123200/175341]\n",
      "loss: 0.646632  [124800/175341]\n",
      "loss: 0.560140  [126400/175341]\n",
      "loss: 0.757041  [128000/175341]\n",
      "loss: 0.975910  [129600/175341]\n",
      "loss: 0.645391  [131200/175341]\n",
      "loss: 0.596731  [132800/175341]\n",
      "loss: 0.699717  [134400/175341]\n",
      "loss: 0.463135  [136000/175341]\n",
      "loss: 0.413161  [137600/175341]\n",
      "loss: 0.381148  [139200/175341]\n",
      "loss: 0.638365  [140800/175341]\n",
      "loss: 0.445415  [142400/175341]\n",
      "loss: 0.893542  [144000/175341]\n",
      "loss: 0.581580  [145600/175341]\n",
      "loss: 0.677264  [147200/175341]\n",
      "loss: 0.454677  [148800/175341]\n",
      "loss: 0.481405  [150400/175341]\n",
      "loss: 0.130921  [152000/175341]\n",
      "loss: 0.977559  [153600/175341]\n",
      "loss: 0.621179  [155200/175341]\n",
      "loss: 0.903035  [156800/175341]\n",
      "loss: 0.371600  [158400/175341]\n",
      "loss: 0.700416  [160000/175341]\n",
      "loss: 0.249060  [161600/175341]\n",
      "loss: 0.563552  [163200/175341]\n",
      "loss: 0.563502  [164800/175341]\n",
      "loss: 0.387522  [166400/175341]\n",
      "loss: 0.483766  [168000/175341]\n",
      "loss: 0.821556  [169600/175341]\n",
      "loss: 0.544151  [171200/175341]\n",
      "loss: 0.688614  [172800/175341]\n",
      "loss: 0.595001  [174400/175341]\n",
      "Train Accuracy: 79.6465%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.596999, F1-score: 73.96%, Macro_F1-Score:  36.22%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.346364  [    0/175341]\n",
      "loss: 0.884166  [ 1600/175341]\n",
      "loss: 0.132016  [ 3200/175341]\n",
      "loss: 1.108904  [ 4800/175341]\n",
      "loss: 0.705585  [ 6400/175341]\n",
      "loss: 0.329929  [ 8000/175341]\n",
      "loss: 0.388379  [ 9600/175341]\n",
      "loss: 0.449526  [11200/175341]\n",
      "loss: 0.403413  [12800/175341]\n",
      "loss: 0.814828  [14400/175341]\n",
      "loss: 0.470051  [16000/175341]\n",
      "loss: 0.340441  [17600/175341]\n",
      "loss: 0.483161  [19200/175341]\n",
      "loss: 0.740348  [20800/175341]\n",
      "loss: 0.679875  [22400/175341]\n",
      "loss: 0.541159  [24000/175341]\n",
      "loss: 0.467321  [25600/175341]\n",
      "loss: 0.785273  [27200/175341]\n",
      "loss: 1.001030  [28800/175341]\n",
      "loss: 0.216378  [30400/175341]\n",
      "loss: 0.555718  [32000/175341]\n",
      "loss: 0.315255  [33600/175341]\n",
      "loss: 0.176577  [35200/175341]\n",
      "loss: 0.536097  [36800/175341]\n",
      "loss: 0.419960  [38400/175341]\n",
      "loss: 0.513522  [40000/175341]\n",
      "loss: 0.072981  [41600/175341]\n",
      "loss: 0.798761  [43200/175341]\n",
      "loss: 0.703716  [44800/175341]\n",
      "loss: 0.382210  [46400/175341]\n",
      "loss: 0.399731  [48000/175341]\n",
      "loss: 0.617721  [49600/175341]\n",
      "loss: 0.436943  [51200/175341]\n",
      "loss: 0.319966  [52800/175341]\n",
      "loss: 0.484200  [54400/175341]\n",
      "loss: 0.633137  [56000/175341]\n",
      "loss: 0.277003  [57600/175341]\n",
      "loss: 0.449063  [59200/175341]\n",
      "loss: 0.563619  [60800/175341]\n",
      "loss: 0.531737  [62400/175341]\n",
      "loss: 0.646280  [64000/175341]\n",
      "loss: 0.671900  [65600/175341]\n",
      "loss: 0.273020  [67200/175341]\n",
      "loss: 0.826226  [68800/175341]\n",
      "loss: 0.377820  [70400/175341]\n",
      "loss: 1.026630  [72000/175341]\n",
      "loss: 0.565026  [73600/175341]\n",
      "loss: 0.133926  [75200/175341]\n",
      "loss: 0.524795  [76800/175341]\n",
      "loss: 0.882192  [78400/175341]\n",
      "loss: 0.740467  [80000/175341]\n",
      "loss: 0.624464  [81600/175341]\n",
      "loss: 0.537220  [83200/175341]\n",
      "loss: 0.654136  [84800/175341]\n",
      "loss: 0.164484  [86400/175341]\n",
      "loss: 0.476174  [88000/175341]\n",
      "loss: 0.459436  [89600/175341]\n",
      "loss: 0.614121  [91200/175341]\n",
      "loss: 0.854201  [92800/175341]\n",
      "loss: 0.563974  [94400/175341]\n",
      "loss: 0.660864  [96000/175341]\n",
      "loss: 0.892357  [97600/175341]\n",
      "loss: 0.584676  [99200/175341]\n",
      "loss: 0.319364  [100800/175341]\n",
      "loss: 0.480301  [102400/175341]\n",
      "loss: 0.534829  [104000/175341]\n",
      "loss: 0.578615  [105600/175341]\n",
      "loss: 0.676162  [107200/175341]\n",
      "loss: 0.405794  [108800/175341]\n",
      "loss: 0.327262  [110400/175341]\n",
      "loss: 0.292647  [112000/175341]\n",
      "loss: 0.362051  [113600/175341]\n",
      "loss: 0.396894  [115200/175341]\n",
      "loss: 0.463263  [116800/175341]\n",
      "loss: 0.407332  [118400/175341]\n",
      "loss: 0.531754  [120000/175341]\n",
      "loss: 0.749863  [121600/175341]\n",
      "loss: 0.562577  [123200/175341]\n",
      "loss: 0.476590  [124800/175341]\n",
      "loss: 0.713720  [126400/175341]\n",
      "loss: 0.550964  [128000/175341]\n",
      "loss: 0.405809  [129600/175341]\n",
      "loss: 0.443566  [131200/175341]\n",
      "loss: 0.421188  [132800/175341]\n",
      "loss: 0.481192  [134400/175341]\n",
      "loss: 0.731675  [136000/175341]\n",
      "loss: 0.103739  [137600/175341]\n",
      "loss: 0.342401  [139200/175341]\n",
      "loss: 0.318324  [140800/175341]\n",
      "loss: 0.334767  [142400/175341]\n",
      "loss: 0.376374  [144000/175341]\n",
      "loss: 0.684770  [145600/175341]\n",
      "loss: 0.568954  [147200/175341]\n",
      "loss: 0.169166  [148800/175341]\n",
      "loss: 0.699433  [150400/175341]\n",
      "loss: 0.267204  [152000/175341]\n",
      "loss: 0.499187  [153600/175341]\n",
      "loss: 0.684150  [155200/175341]\n",
      "loss: 0.314963  [156800/175341]\n",
      "loss: 0.277137  [158400/175341]\n",
      "loss: 0.197487  [160000/175341]\n",
      "loss: 0.576068  [161600/175341]\n",
      "loss: 0.556430  [163200/175341]\n",
      "loss: 0.718864  [164800/175341]\n",
      "loss: 0.772430  [166400/175341]\n",
      "loss: 0.792858  [168000/175341]\n",
      "loss: 0.608253  [169600/175341]\n",
      "loss: 0.877580  [171200/175341]\n",
      "loss: 0.851504  [172800/175341]\n",
      "loss: 0.219858  [174400/175341]\n",
      "Train Accuracy: 79.7492%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.588486, F1-score: 74.79%, Macro_F1-Score:  37.32%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.518783  [    0/175341]\n",
      "loss: 0.368742  [ 1600/175341]\n",
      "loss: 0.667104  [ 3200/175341]\n",
      "loss: 0.494367  [ 4800/175341]\n",
      "loss: 0.343536  [ 6400/175341]\n",
      "loss: 0.582943  [ 8000/175341]\n",
      "loss: 0.433641  [ 9600/175341]\n",
      "loss: 0.772413  [11200/175341]\n",
      "loss: 0.537048  [12800/175341]\n",
      "loss: 0.443310  [14400/175341]\n",
      "loss: 0.517452  [16000/175341]\n",
      "loss: 0.764871  [17600/175341]\n",
      "loss: 0.517495  [19200/175341]\n",
      "loss: 0.214532  [20800/175341]\n",
      "loss: 0.467533  [22400/175341]\n",
      "loss: 0.474243  [24000/175341]\n",
      "loss: 0.439585  [25600/175341]\n",
      "loss: 0.496284  [27200/175341]\n",
      "loss: 0.172527  [28800/175341]\n",
      "loss: 0.672211  [30400/175341]\n",
      "loss: 0.784574  [32000/175341]\n",
      "loss: 0.464356  [33600/175341]\n",
      "loss: 0.485817  [35200/175341]\n",
      "loss: 0.557244  [36800/175341]\n",
      "loss: 0.380108  [38400/175341]\n",
      "loss: 0.750735  [40000/175341]\n",
      "loss: 0.454149  [41600/175341]\n",
      "loss: 0.412897  [43200/175341]\n",
      "loss: 0.457261  [44800/175341]\n",
      "loss: 0.505328  [46400/175341]\n",
      "loss: 0.828882  [48000/175341]\n",
      "loss: 0.626612  [49600/175341]\n",
      "loss: 0.649877  [51200/175341]\n",
      "loss: 0.586595  [52800/175341]\n",
      "loss: 0.752773  [54400/175341]\n",
      "loss: 0.230442  [56000/175341]\n",
      "loss: 0.524960  [57600/175341]\n",
      "loss: 1.342983  [59200/175341]\n",
      "loss: 0.738893  [60800/175341]\n",
      "loss: 0.675099  [62400/175341]\n",
      "loss: 0.184241  [64000/175341]\n",
      "loss: 0.435353  [65600/175341]\n",
      "loss: 0.541159  [67200/175341]\n",
      "loss: 0.377779  [68800/175341]\n",
      "loss: 0.730795  [70400/175341]\n",
      "loss: 0.722870  [72000/175341]\n",
      "loss: 0.371215  [73600/175341]\n",
      "loss: 0.377783  [75200/175341]\n",
      "loss: 0.064078  [76800/175341]\n",
      "loss: 0.207105  [78400/175341]\n",
      "loss: 0.187017  [80000/175341]\n",
      "loss: 0.376106  [81600/175341]\n",
      "loss: 0.676057  [83200/175341]\n",
      "loss: 0.849411  [84800/175341]\n",
      "loss: 0.941445  [86400/175341]\n",
      "loss: 0.163411  [88000/175341]\n",
      "loss: 0.370463  [89600/175341]\n",
      "loss: 0.657443  [91200/175341]\n",
      "loss: 0.511666  [92800/175341]\n",
      "loss: 0.785514  [94400/175341]\n",
      "loss: 0.644486  [96000/175341]\n",
      "loss: 0.350994  [97600/175341]\n",
      "loss: 0.744364  [99200/175341]\n",
      "loss: 0.581370  [100800/175341]\n",
      "loss: 0.462143  [102400/175341]\n",
      "loss: 0.100929  [104000/175341]\n",
      "loss: 0.287968  [105600/175341]\n",
      "loss: 0.258000  [107200/175341]\n",
      "loss: 0.383788  [108800/175341]\n",
      "loss: 0.596690  [110400/175341]\n",
      "loss: 0.817607  [112000/175341]\n",
      "loss: 0.470352  [113600/175341]\n",
      "loss: 0.677373  [115200/175341]\n",
      "loss: 0.831354  [116800/175341]\n",
      "loss: 0.886528  [118400/175341]\n",
      "loss: 0.820947  [120000/175341]\n",
      "loss: 0.353785  [121600/175341]\n",
      "loss: 0.801397  [123200/175341]\n",
      "loss: 0.239500  [124800/175341]\n",
      "loss: 0.559268  [126400/175341]\n",
      "loss: 0.621695  [128000/175341]\n",
      "loss: 0.958071  [129600/175341]\n",
      "loss: 0.363098  [131200/175341]\n",
      "loss: 0.585774  [132800/175341]\n",
      "loss: 0.329432  [134400/175341]\n",
      "loss: 0.888366  [136000/175341]\n",
      "loss: 0.466027  [137600/175341]\n",
      "loss: 0.714663  [139200/175341]\n",
      "loss: 0.365400  [140800/175341]\n",
      "loss: 0.732186  [142400/175341]\n",
      "loss: 0.520478  [144000/175341]\n",
      "loss: 0.238927  [145600/175341]\n",
      "loss: 0.778007  [147200/175341]\n",
      "loss: 1.392594  [148800/175341]\n",
      "loss: 0.775158  [150400/175341]\n",
      "loss: 0.314566  [152000/175341]\n",
      "loss: 0.390547  [153600/175341]\n",
      "loss: 0.480880  [155200/175341]\n",
      "loss: 0.703645  [156800/175341]\n",
      "loss: 0.272122  [158400/175341]\n",
      "loss: 0.158362  [160000/175341]\n",
      "loss: 0.440339  [161600/175341]\n",
      "loss: 0.374912  [163200/175341]\n",
      "loss: 0.517677  [164800/175341]\n",
      "loss: 0.474964  [166400/175341]\n",
      "loss: 0.643072  [168000/175341]\n",
      "loss: 0.240447  [169600/175341]\n",
      "loss: 0.485916  [171200/175341]\n",
      "loss: 0.661250  [172800/175341]\n",
      "loss: 0.394952  [174400/175341]\n",
      "Train Accuracy: 79.8290%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.589976, F1-score: 74.91%, Macro_F1-Score:  36.84%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.626506  [    0/175341]\n",
      "loss: 0.773992  [ 1600/175341]\n",
      "loss: 0.576775  [ 3200/175341]\n",
      "loss: 0.426058  [ 4800/175341]\n",
      "loss: 0.535397  [ 6400/175341]\n",
      "loss: 0.232186  [ 8000/175341]\n",
      "loss: 0.167969  [ 9600/175341]\n",
      "loss: 1.123819  [11200/175341]\n",
      "loss: 0.571706  [12800/175341]\n",
      "loss: 0.646902  [14400/175341]\n",
      "loss: 0.705947  [16000/175341]\n",
      "loss: 0.410226  [17600/175341]\n",
      "loss: 0.343957  [19200/175341]\n",
      "loss: 0.732719  [20800/175341]\n",
      "loss: 0.621965  [22400/175341]\n",
      "loss: 0.909375  [24000/175341]\n",
      "loss: 0.763756  [25600/175341]\n",
      "loss: 0.824591  [27200/175341]\n",
      "loss: 0.471642  [28800/175341]\n",
      "loss: 0.680970  [30400/175341]\n",
      "loss: 0.615689  [32000/175341]\n",
      "loss: 0.436064  [33600/175341]\n",
      "loss: 0.722343  [35200/175341]\n",
      "loss: 0.570260  [36800/175341]\n",
      "loss: 0.488037  [38400/175341]\n",
      "loss: 1.051395  [40000/175341]\n",
      "loss: 0.425978  [41600/175341]\n",
      "loss: 0.337755  [43200/175341]\n",
      "loss: 0.704574  [44800/175341]\n",
      "loss: 0.577369  [46400/175341]\n",
      "loss: 0.783837  [48000/175341]\n",
      "loss: 0.517069  [49600/175341]\n",
      "loss: 0.260418  [51200/175341]\n",
      "loss: 0.388461  [52800/175341]\n",
      "loss: 0.434969  [54400/175341]\n",
      "loss: 0.571952  [56000/175341]\n",
      "loss: 0.559157  [57600/175341]\n",
      "loss: 0.398321  [59200/175341]\n",
      "loss: 0.200390  [60800/175341]\n",
      "loss: 0.405684  [62400/175341]\n",
      "loss: 0.547680  [64000/175341]\n",
      "loss: 0.515736  [65600/175341]\n",
      "loss: 0.466595  [67200/175341]\n",
      "loss: 0.594001  [68800/175341]\n",
      "loss: 0.833630  [70400/175341]\n",
      "loss: 1.240937  [72000/175341]\n",
      "loss: 0.344848  [73600/175341]\n",
      "loss: 0.281555  [75200/175341]\n",
      "loss: 0.263251  [76800/175341]\n",
      "loss: 0.656315  [78400/175341]\n",
      "loss: 0.443478  [80000/175341]\n",
      "loss: 0.718331  [81600/175341]\n",
      "loss: 0.526850  [83200/175341]\n",
      "loss: 0.243799  [84800/175341]\n",
      "loss: 0.809429  [86400/175341]\n",
      "loss: 0.502883  [88000/175341]\n",
      "loss: 0.339185  [89600/175341]\n",
      "loss: 0.476988  [91200/175341]\n",
      "loss: 0.472607  [92800/175341]\n",
      "loss: 0.891765  [94400/175341]\n",
      "loss: 0.868885  [96000/175341]\n",
      "loss: 0.477721  [97600/175341]\n",
      "loss: 0.341964  [99200/175341]\n",
      "loss: 0.527141  [100800/175341]\n",
      "loss: 0.409397  [102400/175341]\n",
      "loss: 0.842817  [104000/175341]\n",
      "loss: 0.545415  [105600/175341]\n",
      "loss: 0.532325  [107200/175341]\n",
      "loss: 1.131005  [108800/175341]\n",
      "loss: 0.748030  [110400/175341]\n",
      "loss: 0.891336  [112000/175341]\n",
      "loss: 0.587081  [113600/175341]\n",
      "loss: 0.410624  [115200/175341]\n",
      "loss: 0.499959  [116800/175341]\n",
      "loss: 0.224298  [118400/175341]\n",
      "loss: 0.856868  [120000/175341]\n",
      "loss: 0.294923  [121600/175341]\n",
      "loss: 0.710971  [123200/175341]\n",
      "loss: 0.368006  [124800/175341]\n",
      "loss: 0.328518  [126400/175341]\n",
      "loss: 0.210244  [128000/175341]\n",
      "loss: 0.829782  [129600/175341]\n",
      "loss: 0.482251  [131200/175341]\n",
      "loss: 0.437100  [132800/175341]\n",
      "loss: 0.535828  [134400/175341]\n",
      "loss: 0.551789  [136000/175341]\n",
      "loss: 0.338863  [137600/175341]\n",
      "loss: 0.422349  [139200/175341]\n",
      "loss: 0.362489  [140800/175341]\n",
      "loss: 0.700329  [142400/175341]\n",
      "loss: 0.694554  [144000/175341]\n",
      "loss: 0.448119  [145600/175341]\n",
      "loss: 0.556203  [147200/175341]\n",
      "loss: 0.916861  [148800/175341]\n",
      "loss: 0.348650  [150400/175341]\n",
      "loss: 0.538562  [152000/175341]\n",
      "loss: 0.494849  [153600/175341]\n",
      "loss: 0.219208  [155200/175341]\n",
      "loss: 0.553571  [156800/175341]\n",
      "loss: 0.185826  [158400/175341]\n",
      "loss: 0.400686  [160000/175341]\n",
      "loss: 0.491529  [161600/175341]\n",
      "loss: 0.588843  [163200/175341]\n",
      "loss: 0.535695  [164800/175341]\n",
      "loss: 0.661882  [166400/175341]\n",
      "loss: 0.439030  [168000/175341]\n",
      "loss: 0.323968  [169600/175341]\n",
      "loss: 0.629806  [171200/175341]\n",
      "loss: 0.375102  [172800/175341]\n",
      "loss: 0.409493  [174400/175341]\n",
      "Train Accuracy: 79.8410%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.590322, F1-score: 75.25%, Macro_F1-Score:  36.08%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.395128  [    0/175341]\n",
      "loss: 0.746589  [ 1600/175341]\n",
      "loss: 0.239314  [ 3200/175341]\n",
      "loss: 0.612119  [ 4800/175341]\n",
      "loss: 0.800779  [ 6400/175341]\n",
      "loss: 0.459652  [ 8000/175341]\n",
      "loss: 1.029542  [ 9600/175341]\n",
      "loss: 0.764847  [11200/175341]\n",
      "loss: 0.677494  [12800/175341]\n",
      "loss: 0.386963  [14400/175341]\n",
      "loss: 0.312929  [16000/175341]\n",
      "loss: 0.871023  [17600/175341]\n",
      "loss: 0.435450  [19200/175341]\n",
      "loss: 0.480142  [20800/175341]\n",
      "loss: 0.766339  [22400/175341]\n",
      "loss: 0.389151  [24000/175341]\n",
      "loss: 0.069243  [25600/175341]\n",
      "loss: 1.026858  [27200/175341]\n",
      "loss: 0.376627  [28800/175341]\n",
      "loss: 0.209597  [30400/175341]\n",
      "loss: 1.148476  [32000/175341]\n",
      "loss: 0.717548  [33600/175341]\n",
      "loss: 0.215983  [35200/175341]\n",
      "loss: 0.450480  [36800/175341]\n",
      "loss: 0.665648  [38400/175341]\n",
      "loss: 0.501241  [40000/175341]\n",
      "loss: 0.633645  [41600/175341]\n",
      "loss: 0.420599  [43200/175341]\n",
      "loss: 0.473019  [44800/175341]\n",
      "loss: 0.920854  [46400/175341]\n",
      "loss: 0.338155  [48000/175341]\n",
      "loss: 0.346012  [49600/175341]\n",
      "loss: 0.377266  [51200/175341]\n",
      "loss: 0.455413  [52800/175341]\n",
      "loss: 0.416799  [54400/175341]\n",
      "loss: 0.438116  [56000/175341]\n",
      "loss: 0.254411  [57600/175341]\n",
      "loss: 0.753095  [59200/175341]\n",
      "loss: 0.591074  [60800/175341]\n",
      "loss: 0.322439  [62400/175341]\n",
      "loss: 0.910559  [64000/175341]\n",
      "loss: 0.197462  [65600/175341]\n",
      "loss: 0.297534  [67200/175341]\n",
      "loss: 0.340120  [68800/175341]\n",
      "loss: 0.620579  [70400/175341]\n",
      "loss: 0.072665  [72000/175341]\n",
      "loss: 0.798894  [73600/175341]\n",
      "loss: 0.349986  [75200/175341]\n",
      "loss: 0.410724  [76800/175341]\n",
      "loss: 0.879361  [78400/175341]\n",
      "loss: 0.518248  [80000/175341]\n",
      "loss: 0.554360  [81600/175341]\n",
      "loss: 0.116172  [83200/175341]\n",
      "loss: 0.548697  [84800/175341]\n",
      "loss: 0.650096  [86400/175341]\n",
      "loss: 0.521396  [88000/175341]\n",
      "loss: 0.523368  [89600/175341]\n",
      "loss: 0.285672  [91200/175341]\n",
      "loss: 0.467466  [92800/175341]\n",
      "loss: 0.305029  [94400/175341]\n",
      "loss: 0.885601  [96000/175341]\n",
      "loss: 0.698618  [97600/175341]\n",
      "loss: 0.411461  [99200/175341]\n",
      "loss: 0.632097  [100800/175341]\n",
      "loss: 0.504188  [102400/175341]\n",
      "loss: 0.546748  [104000/175341]\n",
      "loss: 0.383980  [105600/175341]\n",
      "loss: 0.164298  [107200/175341]\n",
      "loss: 0.545601  [108800/175341]\n",
      "loss: 0.385803  [110400/175341]\n",
      "loss: 0.764305  [112000/175341]\n",
      "loss: 0.366655  [113600/175341]\n",
      "loss: 0.642892  [115200/175341]\n",
      "loss: 0.515221  [116800/175341]\n",
      "loss: 0.347265  [118400/175341]\n",
      "loss: 0.671404  [120000/175341]\n",
      "loss: 0.590745  [121600/175341]\n",
      "loss: 0.306035  [123200/175341]\n",
      "loss: 0.159756  [124800/175341]\n",
      "loss: 0.398946  [126400/175341]\n",
      "loss: 0.555080  [128000/175341]\n",
      "loss: 0.589279  [129600/175341]\n",
      "loss: 0.600586  [131200/175341]\n",
      "loss: 0.254132  [132800/175341]\n",
      "loss: 0.285661  [134400/175341]\n",
      "loss: 0.810182  [136000/175341]\n",
      "loss: 0.951255  [137600/175341]\n",
      "loss: 0.641912  [139200/175341]\n",
      "loss: 0.354228  [140800/175341]\n",
      "loss: 0.626508  [142400/175341]\n",
      "loss: 0.394447  [144000/175341]\n",
      "loss: 0.645870  [145600/175341]\n",
      "loss: 0.846116  [147200/175341]\n",
      "loss: 0.693401  [148800/175341]\n",
      "loss: 0.722624  [150400/175341]\n",
      "loss: 0.513415  [152000/175341]\n",
      "loss: 0.422913  [153600/175341]\n",
      "loss: 0.457949  [155200/175341]\n",
      "loss: 0.684512  [156800/175341]\n",
      "loss: 0.730717  [158400/175341]\n",
      "loss: 0.509096  [160000/175341]\n",
      "loss: 0.568942  [161600/175341]\n",
      "loss: 0.710095  [163200/175341]\n",
      "loss: 0.782000  [164800/175341]\n",
      "loss: 0.821627  [166400/175341]\n",
      "loss: 0.672519  [168000/175341]\n",
      "loss: 0.321523  [169600/175341]\n",
      "loss: 0.265520  [171200/175341]\n",
      "loss: 0.481132  [172800/175341]\n",
      "loss: 0.500299  [174400/175341]\n",
      "Train Accuracy: 79.8986%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.606015, F1-score: 75.15%, Macro_F1-Score:  36.66%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.180858  [    0/175341]\n",
      "loss: 0.451083  [ 1600/175341]\n",
      "loss: 0.349237  [ 3200/175341]\n",
      "loss: 0.612906  [ 4800/175341]\n",
      "loss: 0.291816  [ 6400/175341]\n",
      "loss: 0.476668  [ 8000/175341]\n",
      "loss: 0.263961  [ 9600/175341]\n",
      "loss: 0.380013  [11200/175341]\n",
      "loss: 0.466636  [12800/175341]\n",
      "loss: 0.134543  [14400/175341]\n",
      "loss: 0.300656  [16000/175341]\n",
      "loss: 0.663420  [17600/175341]\n",
      "loss: 0.554337  [19200/175341]\n",
      "loss: 0.413611  [20800/175341]\n",
      "loss: 0.682598  [22400/175341]\n",
      "loss: 1.035550  [24000/175341]\n",
      "loss: 0.572329  [25600/175341]\n",
      "loss: 0.555720  [27200/175341]\n",
      "loss: 0.188425  [28800/175341]\n",
      "loss: 0.203950  [30400/175341]\n",
      "loss: 0.164427  [32000/175341]\n",
      "loss: 0.548770  [33600/175341]\n",
      "loss: 0.440265  [35200/175341]\n",
      "loss: 0.171902  [36800/175341]\n",
      "loss: 0.776617  [38400/175341]\n",
      "loss: 0.134078  [40000/175341]\n",
      "loss: 0.409166  [41600/175341]\n",
      "loss: 0.735219  [43200/175341]\n",
      "loss: 0.212568  [44800/175341]\n",
      "loss: 0.667298  [46400/175341]\n",
      "loss: 0.268410  [48000/175341]\n",
      "loss: 0.191433  [49600/175341]\n",
      "loss: 0.346068  [51200/175341]\n",
      "loss: 0.610701  [52800/175341]\n",
      "loss: 0.570426  [54400/175341]\n",
      "loss: 0.692991  [56000/175341]\n",
      "loss: 0.483167  [57600/175341]\n",
      "loss: 0.691545  [59200/175341]\n",
      "loss: 0.466189  [60800/175341]\n",
      "loss: 0.316758  [62400/175341]\n",
      "loss: 0.438796  [64000/175341]\n",
      "loss: 0.519402  [65600/175341]\n",
      "loss: 0.683177  [67200/175341]\n",
      "loss: 0.257060  [68800/175341]\n",
      "loss: 0.363260  [70400/175341]\n",
      "loss: 0.319617  [72000/175341]\n",
      "loss: 0.662730  [73600/175341]\n",
      "loss: 0.662768  [75200/175341]\n",
      "loss: 0.325865  [76800/175341]\n",
      "loss: 0.514038  [78400/175341]\n",
      "loss: 0.435405  [80000/175341]\n",
      "loss: 0.402999  [81600/175341]\n",
      "loss: 0.211584  [83200/175341]\n",
      "loss: 0.204485  [84800/175341]\n",
      "loss: 0.481814  [86400/175341]\n",
      "loss: 0.266053  [88000/175341]\n",
      "loss: 0.457563  [89600/175341]\n",
      "loss: 0.471188  [91200/175341]\n",
      "loss: 0.309865  [92800/175341]\n",
      "loss: 0.736834  [94400/175341]\n",
      "loss: 0.447286  [96000/175341]\n",
      "loss: 0.775293  [97600/175341]\n",
      "loss: 0.545680  [99200/175341]\n",
      "loss: 0.355594  [100800/175341]\n",
      "loss: 0.446503  [102400/175341]\n",
      "loss: 0.548476  [104000/175341]\n",
      "loss: 0.493626  [105600/175341]\n",
      "loss: 0.620059  [107200/175341]\n",
      "loss: 0.502476  [108800/175341]\n",
      "loss: 0.238591  [110400/175341]\n",
      "loss: 0.516181  [112000/175341]\n",
      "loss: 0.893612  [113600/175341]\n",
      "loss: 0.584442  [115200/175341]\n",
      "loss: 0.383226  [116800/175341]\n",
      "loss: 0.377120  [118400/175341]\n",
      "loss: 0.523971  [120000/175341]\n",
      "loss: 0.158018  [121600/175341]\n",
      "loss: 0.227532  [123200/175341]\n",
      "loss: 0.291453  [124800/175341]\n",
      "loss: 0.569103  [126400/175341]\n",
      "loss: 0.847107  [128000/175341]\n",
      "loss: 0.493949  [129600/175341]\n",
      "loss: 0.378123  [131200/175341]\n",
      "loss: 0.895625  [132800/175341]\n",
      "loss: 0.342970  [134400/175341]\n",
      "loss: 0.246267  [136000/175341]\n",
      "loss: 0.434611  [137600/175341]\n",
      "loss: 0.700531  [139200/175341]\n",
      "loss: 0.514834  [140800/175341]\n",
      "loss: 0.171407  [142400/175341]\n",
      "loss: 0.271997  [144000/175341]\n",
      "loss: 0.327783  [145600/175341]\n",
      "loss: 0.829813  [147200/175341]\n",
      "loss: 0.482990  [148800/175341]\n",
      "loss: 0.569314  [150400/175341]\n",
      "loss: 0.451302  [152000/175341]\n",
      "loss: 0.652615  [153600/175341]\n",
      "loss: 0.835572  [155200/175341]\n",
      "loss: 0.596543  [156800/175341]\n",
      "loss: 0.546230  [158400/175341]\n",
      "loss: 0.625711  [160000/175341]\n",
      "loss: 0.927780  [161600/175341]\n",
      "loss: 0.463127  [163200/175341]\n",
      "loss: 0.576954  [164800/175341]\n",
      "loss: 0.550204  [166400/175341]\n",
      "loss: 0.159069  [168000/175341]\n",
      "loss: 0.825123  [169600/175341]\n",
      "loss: 1.014154  [171200/175341]\n",
      "loss: 0.304973  [172800/175341]\n",
      "loss: 0.533692  [174400/175341]\n",
      "Train Accuracy: 79.9180%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.592717, F1-score: 74.29%, Macro_F1-Score:  37.96%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.543053  [    0/175341]\n",
      "loss: 0.117782  [ 1600/175341]\n",
      "loss: 0.382483  [ 3200/175341]\n",
      "loss: 0.584477  [ 4800/175341]\n",
      "loss: 0.585848  [ 6400/175341]\n",
      "loss: 0.531510  [ 8000/175341]\n",
      "loss: 0.417459  [ 9600/175341]\n",
      "loss: 0.343729  [11200/175341]\n",
      "loss: 0.730514  [12800/175341]\n",
      "loss: 0.386736  [14400/175341]\n",
      "loss: 0.295609  [16000/175341]\n",
      "loss: 1.054767  [17600/175341]\n",
      "loss: 0.371790  [19200/175341]\n",
      "loss: 0.482111  [20800/175341]\n",
      "loss: 0.467241  [22400/175341]\n",
      "loss: 0.409516  [24000/175341]\n",
      "loss: 0.285781  [25600/175341]\n",
      "loss: 0.857707  [27200/175341]\n",
      "loss: 0.756318  [28800/175341]\n",
      "loss: 0.411116  [30400/175341]\n",
      "loss: 0.944407  [32000/175341]\n",
      "loss: 0.365805  [33600/175341]\n",
      "loss: 0.523221  [35200/175341]\n",
      "loss: 0.443527  [36800/175341]\n",
      "loss: 0.601986  [38400/175341]\n",
      "loss: 0.422325  [40000/175341]\n",
      "loss: 0.383754  [41600/175341]\n",
      "loss: 0.512195  [43200/175341]\n",
      "loss: 0.263580  [44800/175341]\n",
      "loss: 1.002459  [46400/175341]\n",
      "loss: 0.416164  [48000/175341]\n",
      "loss: 0.723767  [49600/175341]\n",
      "loss: 0.768147  [51200/175341]\n",
      "loss: 0.324925  [52800/175341]\n",
      "loss: 0.832053  [54400/175341]\n",
      "loss: 0.571897  [56000/175341]\n",
      "loss: 0.499867  [57600/175341]\n",
      "loss: 0.494619  [59200/175341]\n",
      "loss: 0.710793  [60800/175341]\n",
      "loss: 1.414140  [62400/175341]\n",
      "loss: 0.856018  [64000/175341]\n",
      "loss: 0.827244  [65600/175341]\n",
      "loss: 0.501980  [67200/175341]\n",
      "loss: 0.671977  [68800/175341]\n",
      "loss: 0.406578  [70400/175341]\n",
      "loss: 0.596546  [72000/175341]\n",
      "loss: 0.795543  [73600/175341]\n",
      "loss: 0.422921  [75200/175341]\n",
      "loss: 0.508129  [76800/175341]\n",
      "loss: 1.000395  [78400/175341]\n",
      "loss: 0.318327  [80000/175341]\n",
      "loss: 0.620103  [81600/175341]\n",
      "loss: 0.522578  [83200/175341]\n",
      "loss: 0.372021  [84800/175341]\n",
      "loss: 0.722921  [86400/175341]\n",
      "loss: 0.786037  [88000/175341]\n",
      "loss: 0.310151  [89600/175341]\n",
      "loss: 0.485262  [91200/175341]\n",
      "loss: 0.760562  [92800/175341]\n",
      "loss: 0.382580  [94400/175341]\n",
      "loss: 0.601184  [96000/175341]\n",
      "loss: 0.553938  [97600/175341]\n",
      "loss: 0.585629  [99200/175341]\n",
      "loss: 0.451481  [100800/175341]\n",
      "loss: 0.216758  [102400/175341]\n",
      "loss: 0.613152  [104000/175341]\n",
      "loss: 0.411778  [105600/175341]\n",
      "loss: 0.532716  [107200/175341]\n",
      "loss: 0.550664  [108800/175341]\n",
      "loss: 0.447649  [110400/175341]\n",
      "loss: 0.588092  [112000/175341]\n",
      "loss: 0.446487  [113600/175341]\n",
      "loss: 0.529464  [115200/175341]\n",
      "loss: 0.286786  [116800/175341]\n",
      "loss: 0.698816  [118400/175341]\n",
      "loss: 0.688916  [120000/175341]\n",
      "loss: 0.395993  [121600/175341]\n",
      "loss: 0.384367  [123200/175341]\n",
      "loss: 1.038608  [124800/175341]\n",
      "loss: 0.822140  [126400/175341]\n",
      "loss: 0.434147  [128000/175341]\n",
      "loss: 0.789243  [129600/175341]\n",
      "loss: 0.632875  [131200/175341]\n",
      "loss: 0.443437  [132800/175341]\n",
      "loss: 0.400330  [134400/175341]\n",
      "loss: 0.462477  [136000/175341]\n",
      "loss: 0.609146  [137600/175341]\n",
      "loss: 0.889949  [139200/175341]\n",
      "loss: 0.321634  [140800/175341]\n",
      "loss: 0.319715  [142400/175341]\n",
      "loss: 0.467057  [144000/175341]\n",
      "loss: 0.618011  [145600/175341]\n",
      "loss: 0.277519  [147200/175341]\n",
      "loss: 0.441219  [148800/175341]\n",
      "loss: 0.527633  [150400/175341]\n",
      "loss: 0.726521  [152000/175341]\n",
      "loss: 0.689648  [153600/175341]\n",
      "loss: 0.464513  [155200/175341]\n",
      "loss: 0.557222  [156800/175341]\n",
      "loss: 0.558519  [158400/175341]\n",
      "loss: 0.315719  [160000/175341]\n",
      "loss: 0.526559  [161600/175341]\n",
      "loss: 0.876968  [163200/175341]\n",
      "loss: 0.404856  [164800/175341]\n",
      "loss: 0.418261  [166400/175341]\n",
      "loss: 0.232608  [168000/175341]\n",
      "loss: 0.999119  [169600/175341]\n",
      "loss: 0.654808  [171200/175341]\n",
      "loss: 0.383300  [172800/175341]\n",
      "loss: 0.259591  [174400/175341]\n",
      "Train Accuracy: 79.9739%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.589558, F1-score: 75.04%, Macro_F1-Score:  37.82%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.578269  [    0/175341]\n",
      "loss: 0.384607  [ 1600/175341]\n",
      "loss: 0.808621  [ 3200/175341]\n",
      "loss: 0.294359  [ 4800/175341]\n",
      "loss: 0.400417  [ 6400/175341]\n",
      "loss: 0.375997  [ 8000/175341]\n",
      "loss: 0.533140  [ 9600/175341]\n",
      "loss: 0.406166  [11200/175341]\n",
      "loss: 0.410101  [12800/175341]\n",
      "loss: 0.780289  [14400/175341]\n",
      "loss: 0.542600  [16000/175341]\n",
      "loss: 0.214917  [17600/175341]\n",
      "loss: 0.660749  [19200/175341]\n",
      "loss: 0.507477  [20800/175341]\n",
      "loss: 0.854510  [22400/175341]\n",
      "loss: 0.211578  [24000/175341]\n",
      "loss: 0.293107  [25600/175341]\n",
      "loss: 0.630937  [27200/175341]\n",
      "loss: 0.720328  [28800/175341]\n",
      "loss: 0.940222  [30400/175341]\n",
      "loss: 0.916152  [32000/175341]\n",
      "loss: 0.416372  [33600/175341]\n",
      "loss: 0.301145  [35200/175341]\n",
      "loss: 0.501380  [36800/175341]\n",
      "loss: 0.509543  [38400/175341]\n",
      "loss: 0.429594  [40000/175341]\n",
      "loss: 0.393612  [41600/175341]\n",
      "loss: 0.789306  [43200/175341]\n",
      "loss: 0.630022  [44800/175341]\n",
      "loss: 0.515817  [46400/175341]\n",
      "loss: 0.634724  [48000/175341]\n",
      "loss: 0.353792  [49600/175341]\n",
      "loss: 0.533343  [51200/175341]\n",
      "loss: 0.320352  [52800/175341]\n",
      "loss: 0.418189  [54400/175341]\n",
      "loss: 0.197659  [56000/175341]\n",
      "loss: 0.830513  [57600/175341]\n",
      "loss: 0.765962  [59200/175341]\n",
      "loss: 0.534716  [60800/175341]\n",
      "loss: 0.273844  [62400/175341]\n",
      "loss: 0.156727  [64000/175341]\n",
      "loss: 0.832274  [65600/175341]\n",
      "loss: 0.347215  [67200/175341]\n",
      "loss: 0.156568  [68800/175341]\n",
      "loss: 0.513968  [70400/175341]\n",
      "loss: 0.953373  [72000/175341]\n",
      "loss: 0.514203  [73600/175341]\n",
      "loss: 0.385013  [75200/175341]\n",
      "loss: 0.450653  [76800/175341]\n",
      "loss: 0.938079  [78400/175341]\n",
      "loss: 0.454689  [80000/175341]\n",
      "loss: 0.238768  [81600/175341]\n",
      "loss: 0.535163  [83200/175341]\n",
      "loss: 0.137387  [84800/175341]\n",
      "loss: 0.346194  [86400/175341]\n",
      "loss: 0.154023  [88000/175341]\n",
      "loss: 0.553367  [89600/175341]\n",
      "loss: 0.574245  [91200/175341]\n",
      "loss: 0.987299  [92800/175341]\n",
      "loss: 0.243682  [94400/175341]\n",
      "loss: 0.411628  [96000/175341]\n",
      "loss: 0.422308  [97600/175341]\n",
      "loss: 0.431629  [99200/175341]\n",
      "loss: 0.620811  [100800/175341]\n",
      "loss: 0.265765  [102400/175341]\n",
      "loss: 0.387632  [104000/175341]\n",
      "loss: 0.491627  [105600/175341]\n",
      "loss: 0.617540  [107200/175341]\n",
      "loss: 0.580972  [108800/175341]\n",
      "loss: 0.604717  [110400/175341]\n",
      "loss: 0.262386  [112000/175341]\n",
      "loss: 0.280303  [113600/175341]\n",
      "loss: 0.353605  [115200/175341]\n",
      "loss: 0.449512  [116800/175341]\n",
      "loss: 0.541197  [118400/175341]\n",
      "loss: 0.482408  [120000/175341]\n",
      "loss: 0.167300  [121600/175341]\n",
      "loss: 0.546459  [123200/175341]\n",
      "loss: 0.246328  [124800/175341]\n",
      "loss: 0.585365  [126400/175341]\n",
      "loss: 0.440810  [128000/175341]\n",
      "loss: 0.774849  [129600/175341]\n",
      "loss: 0.447191  [131200/175341]\n",
      "loss: 0.420764  [132800/175341]\n",
      "loss: 0.222847  [134400/175341]\n",
      "loss: 0.792793  [136000/175341]\n",
      "loss: 0.183596  [137600/175341]\n",
      "loss: 0.965402  [139200/175341]\n",
      "loss: 0.680072  [140800/175341]\n",
      "loss: 0.263563  [142400/175341]\n",
      "loss: 0.545123  [144000/175341]\n",
      "loss: 0.077070  [145600/175341]\n",
      "loss: 0.251613  [147200/175341]\n",
      "loss: 0.577844  [148800/175341]\n",
      "loss: 0.445793  [150400/175341]\n",
      "loss: 0.228015  [152000/175341]\n",
      "loss: 0.647448  [153600/175341]\n",
      "loss: 0.166670  [155200/175341]\n",
      "loss: 0.525748  [156800/175341]\n",
      "loss: 1.112336  [158400/175341]\n",
      "loss: 0.416470  [160000/175341]\n",
      "loss: 0.324021  [161600/175341]\n",
      "loss: 0.181507  [163200/175341]\n",
      "loss: 0.554097  [164800/175341]\n",
      "loss: 0.380167  [166400/175341]\n",
      "loss: 0.312381  [168000/175341]\n",
      "loss: 0.749996  [169600/175341]\n",
      "loss: 0.722925  [171200/175341]\n",
      "loss: 0.619414  [172800/175341]\n",
      "loss: 0.728339  [174400/175341]\n",
      "Train Accuracy: 80.0269%\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.608049, F1-score: 73.73%, Macro_F1-Score:  37.44%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.294396  [    0/175341]\n",
      "loss: 0.561059  [ 1600/175341]\n",
      "loss: 0.344638  [ 3200/175341]\n",
      "loss: 0.822094  [ 4800/175341]\n",
      "loss: 0.401030  [ 6400/175341]\n",
      "loss: 0.601739  [ 8000/175341]\n",
      "loss: 0.203050  [ 9600/175341]\n",
      "loss: 0.451313  [11200/175341]\n",
      "loss: 0.579537  [12800/175341]\n",
      "loss: 0.238677  [14400/175341]\n",
      "loss: 0.241578  [16000/175341]\n",
      "loss: 0.417976  [17600/175341]\n",
      "loss: 0.398371  [19200/175341]\n",
      "loss: 0.445696  [20800/175341]\n",
      "loss: 0.504891  [22400/175341]\n",
      "loss: 0.619286  [24000/175341]\n",
      "loss: 0.634300  [25600/175341]\n",
      "loss: 0.798311  [27200/175341]\n",
      "loss: 0.355609  [28800/175341]\n",
      "loss: 0.421634  [30400/175341]\n",
      "loss: 0.273318  [32000/175341]\n",
      "loss: 0.574678  [33600/175341]\n",
      "loss: 0.423639  [35200/175341]\n",
      "loss: 0.409575  [36800/175341]\n",
      "loss: 0.614643  [38400/175341]\n",
      "loss: 0.213013  [40000/175341]\n",
      "loss: 0.326628  [41600/175341]\n",
      "loss: 0.778592  [43200/175341]\n",
      "loss: 0.893784  [44800/175341]\n",
      "loss: 0.465833  [46400/175341]\n",
      "loss: 0.528480  [48000/175341]\n",
      "loss: 0.586249  [49600/175341]\n",
      "loss: 0.593691  [51200/175341]\n",
      "loss: 0.668141  [52800/175341]\n",
      "loss: 0.388379  [54400/175341]\n",
      "loss: 0.465274  [56000/175341]\n",
      "loss: 0.314430  [57600/175341]\n",
      "loss: 0.540143  [59200/175341]\n",
      "loss: 0.929019  [60800/175341]\n",
      "loss: 0.424549  [62400/175341]\n",
      "loss: 0.275522  [64000/175341]\n",
      "loss: 0.503966  [65600/175341]\n",
      "loss: 0.632324  [67200/175341]\n",
      "loss: 0.478582  [68800/175341]\n",
      "loss: 0.670943  [70400/175341]\n",
      "loss: 0.596885  [72000/175341]\n",
      "loss: 0.599451  [73600/175341]\n",
      "loss: 0.453148  [75200/175341]\n",
      "loss: 0.368105  [76800/175341]\n",
      "loss: 0.586505  [78400/175341]\n",
      "loss: 0.229748  [80000/175341]\n",
      "loss: 0.279547  [81600/175341]\n",
      "loss: 0.905370  [83200/175341]\n",
      "loss: 0.784335  [84800/175341]\n",
      "loss: 0.460064  [86400/175341]\n",
      "loss: 0.166407  [88000/175341]\n",
      "loss: 0.788488  [89600/175341]\n",
      "loss: 0.487747  [91200/175341]\n",
      "loss: 1.209316  [92800/175341]\n",
      "loss: 0.361777  [94400/175341]\n",
      "loss: 0.235433  [96000/175341]\n",
      "loss: 0.214243  [97600/175341]\n",
      "loss: 0.545646  [99200/175341]\n",
      "loss: 0.293189  [100800/175341]\n",
      "loss: 0.760359  [102400/175341]\n",
      "loss: 0.440472  [104000/175341]\n",
      "loss: 0.345074  [105600/175341]\n",
      "loss: 1.071086  [107200/175341]\n",
      "loss: 0.732852  [108800/175341]\n",
      "loss: 0.195298  [110400/175341]\n",
      "loss: 0.522784  [112000/175341]\n",
      "loss: 0.289372  [113600/175341]\n",
      "loss: 0.553027  [115200/175341]\n",
      "loss: 0.309726  [116800/175341]\n",
      "loss: 0.320971  [118400/175341]\n",
      "loss: 0.495348  [120000/175341]\n",
      "loss: 0.471831  [121600/175341]\n",
      "loss: 1.018110  [123200/175341]\n",
      "loss: 0.665632  [124800/175341]\n",
      "loss: 0.680420  [126400/175341]\n",
      "loss: 0.407936  [128000/175341]\n",
      "loss: 0.331959  [129600/175341]\n",
      "loss: 0.490122  [131200/175341]\n",
      "loss: 1.004429  [132800/175341]\n",
      "loss: 0.410947  [134400/175341]\n",
      "loss: 0.590357  [136000/175341]\n",
      "loss: 0.328727  [137600/175341]\n",
      "loss: 0.565133  [139200/175341]\n",
      "loss: 0.490941  [140800/175341]\n",
      "loss: 0.636136  [142400/175341]\n",
      "loss: 0.163377  [144000/175341]\n",
      "loss: 0.490935  [145600/175341]\n",
      "loss: 0.692272  [147200/175341]\n",
      "loss: 0.567694  [148800/175341]\n",
      "loss: 0.653281  [150400/175341]\n",
      "loss: 0.723067  [152000/175341]\n",
      "loss: 0.330232  [153600/175341]\n",
      "loss: 0.309564  [155200/175341]\n",
      "loss: 0.498504  [156800/175341]\n",
      "loss: 0.702192  [158400/175341]\n",
      "loss: 1.002337  [160000/175341]\n",
      "loss: 0.709521  [161600/175341]\n",
      "loss: 0.344901  [163200/175341]\n",
      "loss: 0.514806  [164800/175341]\n",
      "loss: 0.667153  [166400/175341]\n",
      "loss: 0.202841  [168000/175341]\n",
      "loss: 0.785801  [169600/175341]\n",
      "loss: 0.294775  [171200/175341]\n",
      "loss: 0.447220  [172800/175341]\n",
      "loss: 0.847696  [174400/175341]\n",
      "Train Accuracy: 80.0720%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.597254, F1-score: 74.44%, Macro_F1-Score:  38.00%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.101364  [    0/175341]\n",
      "loss: 0.347234  [ 1600/175341]\n",
      "loss: 0.293459  [ 3200/175341]\n",
      "loss: 0.733825  [ 4800/175341]\n",
      "loss: 0.508582  [ 6400/175341]\n",
      "loss: 0.533171  [ 8000/175341]\n",
      "loss: 0.755041  [ 9600/175341]\n",
      "loss: 0.268425  [11200/175341]\n",
      "loss: 0.507036  [12800/175341]\n",
      "loss: 0.238774  [14400/175341]\n",
      "loss: 0.446354  [16000/175341]\n",
      "loss: 0.385059  [17600/175341]\n",
      "loss: 0.290888  [19200/175341]\n",
      "loss: 0.417633  [20800/175341]\n",
      "loss: 0.203009  [22400/175341]\n",
      "loss: 0.666504  [24000/175341]\n",
      "loss: 0.352425  [25600/175341]\n",
      "loss: 0.297548  [27200/175341]\n",
      "loss: 0.313846  [28800/175341]\n",
      "loss: 0.263687  [30400/175341]\n",
      "loss: 0.827773  [32000/175341]\n",
      "loss: 0.222213  [33600/175341]\n",
      "loss: 0.671398  [35200/175341]\n",
      "loss: 0.597183  [36800/175341]\n",
      "loss: 0.320588  [38400/175341]\n",
      "loss: 0.728468  [40000/175341]\n",
      "loss: 0.320893  [41600/175341]\n",
      "loss: 0.182556  [43200/175341]\n",
      "loss: 0.537969  [44800/175341]\n",
      "loss: 0.668815  [46400/175341]\n",
      "loss: 0.546541  [48000/175341]\n",
      "loss: 0.535589  [49600/175341]\n",
      "loss: 0.847938  [51200/175341]\n",
      "loss: 0.530109  [52800/175341]\n",
      "loss: 0.469545  [54400/175341]\n",
      "loss: 0.909515  [56000/175341]\n",
      "loss: 0.682158  [57600/175341]\n",
      "loss: 0.365765  [59200/175341]\n",
      "loss: 0.661927  [60800/175341]\n",
      "loss: 0.780943  [62400/175341]\n",
      "loss: 0.432610  [64000/175341]\n",
      "loss: 0.564974  [65600/175341]\n",
      "loss: 0.792145  [67200/175341]\n",
      "loss: 0.394845  [68800/175341]\n",
      "loss: 0.814101  [70400/175341]\n",
      "loss: 0.465300  [72000/175341]\n",
      "loss: 0.633875  [73600/175341]\n",
      "loss: 0.502714  [75200/175341]\n",
      "loss: 0.555822  [76800/175341]\n",
      "loss: 0.468415  [78400/175341]\n",
      "loss: 0.347269  [80000/175341]\n",
      "loss: 0.459230  [81600/175341]\n",
      "loss: 0.362173  [83200/175341]\n",
      "loss: 0.427451  [84800/175341]\n",
      "loss: 0.372669  [86400/175341]\n",
      "loss: 0.548640  [88000/175341]\n",
      "loss: 0.174389  [89600/175341]\n",
      "loss: 1.273867  [91200/175341]\n",
      "loss: 0.369588  [92800/175341]\n",
      "loss: 0.229071  [94400/175341]\n",
      "loss: 0.615777  [96000/175341]\n",
      "loss: 0.638263  [97600/175341]\n",
      "loss: 0.344863  [99200/175341]\n",
      "loss: 0.803269  [100800/175341]\n",
      "loss: 0.165633  [102400/175341]\n",
      "loss: 0.541455  [104000/175341]\n",
      "loss: 0.330024  [105600/175341]\n",
      "loss: 0.989762  [107200/175341]\n",
      "loss: 0.287814  [108800/175341]\n",
      "loss: 0.633006  [110400/175341]\n",
      "loss: 0.191691  [112000/175341]\n",
      "loss: 0.209613  [113600/175341]\n",
      "loss: 0.384623  [115200/175341]\n",
      "loss: 0.564299  [116800/175341]\n",
      "loss: 0.411126  [118400/175341]\n",
      "loss: 0.399827  [120000/175341]\n",
      "loss: 0.842699  [121600/175341]\n",
      "loss: 0.330514  [123200/175341]\n",
      "loss: 0.148016  [124800/175341]\n",
      "loss: 0.229279  [126400/175341]\n",
      "loss: 0.109585  [128000/175341]\n",
      "loss: 0.356691  [129600/175341]\n",
      "loss: 0.563115  [131200/175341]\n",
      "loss: 0.730647  [132800/175341]\n",
      "loss: 0.414279  [134400/175341]\n",
      "loss: 0.614947  [136000/175341]\n",
      "loss: 0.178622  [137600/175341]\n",
      "loss: 0.118396  [139200/175341]\n",
      "loss: 0.564957  [140800/175341]\n",
      "loss: 0.188236  [142400/175341]\n",
      "loss: 0.352700  [144000/175341]\n",
      "loss: 0.662024  [145600/175341]\n",
      "loss: 0.527197  [147200/175341]\n",
      "loss: 0.308700  [148800/175341]\n",
      "loss: 0.203437  [150400/175341]\n",
      "loss: 0.683093  [152000/175341]\n",
      "loss: 0.203240  [153600/175341]\n",
      "loss: 0.164015  [155200/175341]\n",
      "loss: 0.846429  [156800/175341]\n",
      "loss: 0.783133  [158400/175341]\n",
      "loss: 0.269044  [160000/175341]\n",
      "loss: 0.313370  [161600/175341]\n",
      "loss: 0.713303  [163200/175341]\n",
      "loss: 0.513171  [164800/175341]\n",
      "loss: 0.300569  [166400/175341]\n",
      "loss: 0.441622  [168000/175341]\n",
      "loss: 0.471471  [169600/175341]\n",
      "loss: 0.461730  [171200/175341]\n",
      "loss: 0.855584  [172800/175341]\n",
      "loss: 0.249694  [174400/175341]\n",
      "Train Accuracy: 80.0822%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.595449, F1-score: 74.86%, Macro_F1-Score:  37.92%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.057375  [    0/175341]\n",
      "loss: 0.726023  [ 1600/175341]\n",
      "loss: 0.763785  [ 3200/175341]\n",
      "loss: 0.641139  [ 4800/175341]\n",
      "loss: 0.361154  [ 6400/175341]\n",
      "loss: 0.705621  [ 8000/175341]\n",
      "loss: 0.301243  [ 9600/175341]\n",
      "loss: 0.352639  [11200/175341]\n",
      "loss: 0.436688  [12800/175341]\n",
      "loss: 0.410594  [14400/175341]\n",
      "loss: 0.467476  [16000/175341]\n",
      "loss: 0.226419  [17600/175341]\n",
      "loss: 0.744815  [19200/175341]\n",
      "loss: 0.472457  [20800/175341]\n",
      "loss: 0.397545  [22400/175341]\n",
      "loss: 0.247320  [24000/175341]\n",
      "loss: 0.297956  [25600/175341]\n",
      "loss: 0.321143  [27200/175341]\n",
      "loss: 0.280071  [28800/175341]\n",
      "loss: 0.427423  [30400/175341]\n",
      "loss: 0.560926  [32000/175341]\n",
      "loss: 0.776717  [33600/175341]\n",
      "loss: 0.486097  [35200/175341]\n",
      "loss: 0.314839  [36800/175341]\n",
      "loss: 0.120804  [38400/175341]\n",
      "loss: 0.301232  [40000/175341]\n",
      "loss: 0.774460  [41600/175341]\n",
      "loss: 0.280243  [43200/175341]\n",
      "loss: 0.437464  [44800/175341]\n",
      "loss: 0.635006  [46400/175341]\n",
      "loss: 0.501741  [48000/175341]\n",
      "loss: 0.500465  [49600/175341]\n",
      "loss: 0.443896  [51200/175341]\n",
      "loss: 0.240975  [52800/175341]\n",
      "loss: 0.745267  [54400/175341]\n",
      "loss: 0.372261  [56000/175341]\n",
      "loss: 0.347093  [57600/175341]\n",
      "loss: 0.615665  [59200/175341]\n",
      "loss: 0.436270  [60800/175341]\n",
      "loss: 0.708121  [62400/175341]\n",
      "loss: 0.216549  [64000/175341]\n",
      "loss: 0.536025  [65600/175341]\n",
      "loss: 0.390751  [67200/175341]\n",
      "loss: 0.388689  [68800/175341]\n",
      "loss: 0.474240  [70400/175341]\n",
      "loss: 0.202161  [72000/175341]\n",
      "loss: 0.540861  [73600/175341]\n",
      "loss: 0.532525  [75200/175341]\n",
      "loss: 0.884368  [76800/175341]\n",
      "loss: 1.052608  [78400/175341]\n",
      "loss: 0.599289  [80000/175341]\n",
      "loss: 0.364925  [81600/175341]\n",
      "loss: 0.544400  [83200/175341]\n",
      "loss: 0.617091  [84800/175341]\n",
      "loss: 0.669476  [86400/175341]\n",
      "loss: 0.153815  [88000/175341]\n",
      "loss: 0.464924  [89600/175341]\n",
      "loss: 0.545561  [91200/175341]\n",
      "loss: 0.143395  [92800/175341]\n",
      "loss: 0.360085  [94400/175341]\n",
      "loss: 0.684086  [96000/175341]\n",
      "loss: 1.014928  [97600/175341]\n",
      "loss: 0.090800  [99200/175341]\n",
      "loss: 0.206925  [100800/175341]\n",
      "loss: 0.372966  [102400/175341]\n",
      "loss: 0.481406  [104000/175341]\n",
      "loss: 0.285072  [105600/175341]\n",
      "loss: 0.919116  [107200/175341]\n",
      "loss: 0.428740  [108800/175341]\n",
      "loss: 0.382826  [110400/175341]\n",
      "loss: 0.483476  [112000/175341]\n",
      "loss: 0.568499  [113600/175341]\n",
      "loss: 0.872911  [115200/175341]\n",
      "loss: 0.186812  [116800/175341]\n",
      "loss: 0.447817  [118400/175341]\n",
      "loss: 0.505724  [120000/175341]\n",
      "loss: 0.406783  [121600/175341]\n",
      "loss: 0.836034  [123200/175341]\n",
      "loss: 0.393760  [124800/175341]\n",
      "loss: 0.435206  [126400/175341]\n",
      "loss: 0.438250  [128000/175341]\n",
      "loss: 0.374842  [129600/175341]\n",
      "loss: 0.271957  [131200/175341]\n",
      "loss: 0.473273  [132800/175341]\n",
      "loss: 0.383998  [134400/175341]\n",
      "loss: 0.764581  [136000/175341]\n",
      "loss: 0.511613  [137600/175341]\n",
      "loss: 0.227244  [139200/175341]\n",
      "loss: 0.523965  [140800/175341]\n",
      "loss: 0.314539  [142400/175341]\n",
      "loss: 0.352386  [144000/175341]\n",
      "loss: 0.312993  [145600/175341]\n",
      "loss: 0.639119  [147200/175341]\n",
      "loss: 0.640343  [148800/175341]\n",
      "loss: 0.387768  [150400/175341]\n",
      "loss: 0.305407  [152000/175341]\n",
      "loss: 0.551569  [153600/175341]\n",
      "loss: 0.224136  [155200/175341]\n",
      "loss: 0.192345  [156800/175341]\n",
      "loss: 0.887173  [158400/175341]\n",
      "loss: 0.295808  [160000/175341]\n",
      "loss: 0.451093  [161600/175341]\n",
      "loss: 0.502772  [163200/175341]\n",
      "loss: 0.490260  [164800/175341]\n",
      "loss: 0.596573  [166400/175341]\n",
      "loss: 0.512131  [168000/175341]\n",
      "loss: 0.461618  [169600/175341]\n",
      "loss: 0.600981  [171200/175341]\n",
      "loss: 0.424860  [172800/175341]\n",
      "loss: 0.591177  [174400/175341]\n",
      "Train Accuracy: 80.1062%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.587040, F1-score: 75.22%, Macro_F1-Score:  38.20%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.391085  [    0/175341]\n",
      "loss: 0.574484  [ 1600/175341]\n",
      "loss: 0.702638  [ 3200/175341]\n",
      "loss: 0.372971  [ 4800/175341]\n",
      "loss: 0.481533  [ 6400/175341]\n",
      "loss: 0.471058  [ 8000/175341]\n",
      "loss: 0.431486  [ 9600/175341]\n",
      "loss: 0.370950  [11200/175341]\n",
      "loss: 0.568579  [12800/175341]\n",
      "loss: 0.318520  [14400/175341]\n",
      "loss: 0.612632  [16000/175341]\n",
      "loss: 0.167633  [17600/175341]\n",
      "loss: 0.603994  [19200/175341]\n",
      "loss: 0.450205  [20800/175341]\n",
      "loss: 0.792639  [22400/175341]\n",
      "loss: 0.566614  [24000/175341]\n",
      "loss: 0.263912  [25600/175341]\n",
      "loss: 0.626966  [27200/175341]\n",
      "loss: 0.735302  [28800/175341]\n",
      "loss: 0.218366  [30400/175341]\n",
      "loss: 0.403127  [32000/175341]\n",
      "loss: 0.476511  [33600/175341]\n",
      "loss: 0.357784  [35200/175341]\n",
      "loss: 0.463442  [36800/175341]\n",
      "loss: 0.154182  [38400/175341]\n",
      "loss: 0.550009  [40000/175341]\n",
      "loss: 0.411706  [41600/175341]\n",
      "loss: 0.682246  [43200/175341]\n",
      "loss: 0.356641  [44800/175341]\n",
      "loss: 0.357859  [46400/175341]\n",
      "loss: 0.496694  [48000/175341]\n",
      "loss: 0.296971  [49600/175341]\n",
      "loss: 0.324521  [51200/175341]\n",
      "loss: 0.502090  [52800/175341]\n",
      "loss: 0.830004  [54400/175341]\n",
      "loss: 0.712633  [56000/175341]\n",
      "loss: 0.358583  [57600/175341]\n",
      "loss: 0.433368  [59200/175341]\n",
      "loss: 0.551484  [60800/175341]\n",
      "loss: 1.123875  [62400/175341]\n",
      "loss: 1.041720  [64000/175341]\n",
      "loss: 0.176057  [65600/175341]\n",
      "loss: 0.785014  [67200/175341]\n",
      "loss: 0.319824  [68800/175341]\n",
      "loss: 0.677524  [70400/175341]\n",
      "loss: 0.685166  [72000/175341]\n",
      "loss: 0.507488  [73600/175341]\n",
      "loss: 0.450928  [75200/175341]\n",
      "loss: 0.275528  [76800/175341]\n",
      "loss: 0.646083  [78400/175341]\n",
      "loss: 0.313711  [80000/175341]\n",
      "loss: 0.332649  [81600/175341]\n",
      "loss: 0.705837  [83200/175341]\n",
      "loss: 0.555459  [84800/175341]\n",
      "loss: 0.363635  [86400/175341]\n",
      "loss: 0.594921  [88000/175341]\n",
      "loss: 0.640553  [89600/175341]\n",
      "loss: 0.501325  [91200/175341]\n",
      "loss: 0.369160  [92800/175341]\n",
      "loss: 0.415488  [94400/175341]\n",
      "loss: 0.274920  [96000/175341]\n",
      "loss: 0.347104  [97600/175341]\n",
      "loss: 0.434623  [99200/175341]\n",
      "loss: 0.557014  [100800/175341]\n",
      "loss: 0.605728  [102400/175341]\n",
      "loss: 0.565943  [104000/175341]\n",
      "loss: 0.525196  [105600/175341]\n",
      "loss: 0.471395  [107200/175341]\n",
      "loss: 0.232522  [108800/175341]\n",
      "loss: 0.595059  [110400/175341]\n",
      "loss: 0.582114  [112000/175341]\n",
      "loss: 0.291399  [113600/175341]\n",
      "loss: 0.416936  [115200/175341]\n",
      "loss: 1.012418  [116800/175341]\n",
      "loss: 0.633103  [118400/175341]\n",
      "loss: 0.865938  [120000/175341]\n",
      "loss: 0.416083  [121600/175341]\n",
      "loss: 0.704186  [123200/175341]\n",
      "loss: 0.712197  [124800/175341]\n",
      "loss: 0.449615  [126400/175341]\n",
      "loss: 0.853030  [128000/175341]\n",
      "loss: 0.358207  [129600/175341]\n",
      "loss: 0.703181  [131200/175341]\n",
      "loss: 0.306584  [132800/175341]\n",
      "loss: 0.665548  [134400/175341]\n",
      "loss: 0.470605  [136000/175341]\n",
      "loss: 0.851578  [137600/175341]\n",
      "loss: 0.424526  [139200/175341]\n",
      "loss: 0.415299  [140800/175341]\n",
      "loss: 0.204121  [142400/175341]\n",
      "loss: 0.487845  [144000/175341]\n",
      "loss: 0.276497  [145600/175341]\n",
      "loss: 0.531810  [147200/175341]\n",
      "loss: 0.432331  [148800/175341]\n",
      "loss: 0.439591  [150400/175341]\n",
      "loss: 0.782618  [152000/175341]\n",
      "loss: 0.541422  [153600/175341]\n",
      "loss: 0.650572  [155200/175341]\n",
      "loss: 0.720886  [156800/175341]\n",
      "loss: 0.387513  [158400/175341]\n",
      "loss: 0.569946  [160000/175341]\n",
      "loss: 0.380991  [161600/175341]\n",
      "loss: 0.160394  [163200/175341]\n",
      "loss: 0.763865  [164800/175341]\n",
      "loss: 0.576224  [166400/175341]\n",
      "loss: 0.423878  [168000/175341]\n",
      "loss: 0.389570  [169600/175341]\n",
      "loss: 0.457214  [171200/175341]\n",
      "loss: 0.834682  [172800/175341]\n",
      "loss: 0.547890  [174400/175341]\n",
      "Train Accuracy: 80.0862%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.603540, F1-score: 74.27%, Macro_F1-Score:  37.75%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.609475  [    0/175341]\n",
      "loss: 0.559357  [ 1600/175341]\n",
      "loss: 0.317074  [ 3200/175341]\n",
      "loss: 0.395336  [ 4800/175341]\n",
      "loss: 0.451359  [ 6400/175341]\n",
      "loss: 0.866131  [ 8000/175341]\n",
      "loss: 0.682227  [ 9600/175341]\n",
      "loss: 0.698416  [11200/175341]\n",
      "loss: 0.356334  [12800/175341]\n",
      "loss: 0.355922  [14400/175341]\n",
      "loss: 0.536704  [16000/175341]\n",
      "loss: 0.213110  [17600/175341]\n",
      "loss: 0.459524  [19200/175341]\n",
      "loss: 0.483777  [20800/175341]\n",
      "loss: 0.476271  [22400/175341]\n",
      "loss: 0.260752  [24000/175341]\n",
      "loss: 0.303384  [25600/175341]\n",
      "loss: 0.224561  [27200/175341]\n",
      "loss: 0.385745  [28800/175341]\n",
      "loss: 0.855496  [30400/175341]\n",
      "loss: 0.591546  [32000/175341]\n",
      "loss: 0.720977  [33600/175341]\n",
      "loss: 0.525340  [35200/175341]\n",
      "loss: 0.594903  [36800/175341]\n",
      "loss: 0.504426  [38400/175341]\n",
      "loss: 0.833731  [40000/175341]\n",
      "loss: 1.036609  [41600/175341]\n",
      "loss: 0.571894  [43200/175341]\n",
      "loss: 0.494381  [44800/175341]\n",
      "loss: 0.431281  [46400/175341]\n",
      "loss: 0.958605  [48000/175341]\n",
      "loss: 0.620588  [49600/175341]\n",
      "loss: 0.274747  [51200/175341]\n",
      "loss: 0.417507  [52800/175341]\n",
      "loss: 0.951175  [54400/175341]\n",
      "loss: 0.521300  [56000/175341]\n",
      "loss: 0.321840  [57600/175341]\n",
      "loss: 0.396157  [59200/175341]\n",
      "loss: 0.331074  [60800/175341]\n",
      "loss: 0.395108  [62400/175341]\n",
      "loss: 0.944399  [64000/175341]\n",
      "loss: 0.714167  [65600/175341]\n",
      "loss: 0.438660  [67200/175341]\n",
      "loss: 0.516680  [68800/175341]\n",
      "loss: 0.335748  [70400/175341]\n",
      "loss: 0.784811  [72000/175341]\n",
      "loss: 0.464420  [73600/175341]\n",
      "loss: 0.917946  [75200/175341]\n",
      "loss: 0.806119  [76800/175341]\n",
      "loss: 0.817262  [78400/175341]\n",
      "loss: 0.660839  [80000/175341]\n",
      "loss: 0.367560  [81600/175341]\n",
      "loss: 0.461496  [83200/175341]\n",
      "loss: 0.642573  [84800/175341]\n",
      "loss: 0.437971  [86400/175341]\n",
      "loss: 0.550243  [88000/175341]\n",
      "loss: 0.523259  [89600/175341]\n",
      "loss: 0.806213  [91200/175341]\n",
      "loss: 0.394670  [92800/175341]\n",
      "loss: 0.245751  [94400/175341]\n",
      "loss: 0.853810  [96000/175341]\n",
      "loss: 0.628622  [97600/175341]\n",
      "loss: 0.414248  [99200/175341]\n",
      "loss: 0.474527  [100800/175341]\n",
      "loss: 0.450284  [102400/175341]\n",
      "loss: 0.140490  [104000/175341]\n",
      "loss: 0.113082  [105600/175341]\n",
      "loss: 0.588872  [107200/175341]\n",
      "loss: 0.336689  [108800/175341]\n",
      "loss: 0.875447  [110400/175341]\n",
      "loss: 0.842308  [112000/175341]\n",
      "loss: 1.066607  [113600/175341]\n",
      "loss: 0.551391  [115200/175341]\n",
      "loss: 0.553378  [116800/175341]\n",
      "loss: 0.754860  [118400/175341]\n",
      "loss: 0.204560  [120000/175341]\n",
      "loss: 0.778242  [121600/175341]\n",
      "loss: 0.799492  [123200/175341]\n",
      "loss: 0.896205  [124800/175341]\n",
      "loss: 0.321389  [126400/175341]\n",
      "loss: 0.561421  [128000/175341]\n",
      "loss: 0.559494  [129600/175341]\n",
      "loss: 0.829359  [131200/175341]\n",
      "loss: 0.575518  [132800/175341]\n",
      "loss: 0.301871  [134400/175341]\n",
      "loss: 0.202640  [136000/175341]\n",
      "loss: 0.347210  [137600/175341]\n",
      "loss: 0.671112  [139200/175341]\n",
      "loss: 0.321666  [140800/175341]\n",
      "loss: 0.772166  [142400/175341]\n",
      "loss: 0.361544  [144000/175341]\n",
      "loss: 0.303732  [145600/175341]\n",
      "loss: 0.399090  [147200/175341]\n",
      "loss: 0.535959  [148800/175341]\n",
      "loss: 0.403980  [150400/175341]\n",
      "loss: 0.267591  [152000/175341]\n",
      "loss: 0.670414  [153600/175341]\n",
      "loss: 0.434050  [155200/175341]\n",
      "loss: 0.695384  [156800/175341]\n",
      "loss: 0.394199  [158400/175341]\n",
      "loss: 0.588988  [160000/175341]\n",
      "loss: 0.794663  [161600/175341]\n",
      "loss: 0.525725  [163200/175341]\n",
      "loss: 0.191125  [164800/175341]\n",
      "loss: 0.429880  [166400/175341]\n",
      "loss: 0.176293  [168000/175341]\n",
      "loss: 0.489740  [169600/175341]\n",
      "loss: 0.481764  [171200/175341]\n",
      "loss: 0.277205  [172800/175341]\n",
      "loss: 0.480620  [174400/175341]\n",
      "Train Accuracy: 80.1028%\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.660678, F1-score: 72.88%, Macro_F1-Score:  36.43%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.348775  [    0/175341]\n",
      "loss: 0.500571  [ 1600/175341]\n",
      "loss: 0.432402  [ 3200/175341]\n",
      "loss: 0.785235  [ 4800/175341]\n",
      "loss: 0.550161  [ 6400/175341]\n",
      "loss: 0.786166  [ 8000/175341]\n",
      "loss: 1.040869  [ 9600/175341]\n",
      "loss: 0.066606  [11200/175341]\n",
      "loss: 0.167673  [12800/175341]\n",
      "loss: 0.251520  [14400/175341]\n",
      "loss: 0.448390  [16000/175341]\n",
      "loss: 0.457134  [17600/175341]\n",
      "loss: 0.323296  [19200/175341]\n",
      "loss: 0.697341  [20800/175341]\n",
      "loss: 0.517947  [22400/175341]\n",
      "loss: 0.731835  [24000/175341]\n",
      "loss: 0.308664  [25600/175341]\n",
      "loss: 0.510312  [27200/175341]\n",
      "loss: 0.369148  [28800/175341]\n",
      "loss: 0.350528  [30400/175341]\n",
      "loss: 0.404012  [32000/175341]\n",
      "loss: 0.790814  [33600/175341]\n",
      "loss: 0.619501  [35200/175341]\n",
      "loss: 0.446118  [36800/175341]\n",
      "loss: 0.198711  [38400/175341]\n",
      "loss: 0.794373  [40000/175341]\n",
      "loss: 0.321632  [41600/175341]\n",
      "loss: 0.245420  [43200/175341]\n",
      "loss: 0.623881  [44800/175341]\n",
      "loss: 0.432683  [46400/175341]\n",
      "loss: 0.830415  [48000/175341]\n",
      "loss: 0.167474  [49600/175341]\n",
      "loss: 0.302726  [51200/175341]\n",
      "loss: 0.623258  [52800/175341]\n",
      "loss: 0.387722  [54400/175341]\n",
      "loss: 0.464660  [56000/175341]\n",
      "loss: 0.703039  [57600/175341]\n",
      "loss: 0.377945  [59200/175341]\n",
      "loss: 0.468918  [60800/175341]\n",
      "loss: 0.413254  [62400/175341]\n",
      "loss: 0.200829  [64000/175341]\n",
      "loss: 0.483192  [65600/175341]\n",
      "loss: 0.678553  [67200/175341]\n",
      "loss: 0.952292  [68800/175341]\n",
      "loss: 0.728360  [70400/175341]\n",
      "loss: 0.649097  [72000/175341]\n",
      "loss: 0.387613  [73600/175341]\n",
      "loss: 0.789271  [75200/175341]\n",
      "loss: 0.493547  [76800/175341]\n",
      "loss: 0.903708  [78400/175341]\n",
      "loss: 0.399875  [80000/175341]\n",
      "loss: 0.485345  [81600/175341]\n",
      "loss: 0.511717  [83200/175341]\n",
      "loss: 0.361579  [84800/175341]\n",
      "loss: 0.268993  [86400/175341]\n",
      "loss: 0.425305  [88000/175341]\n",
      "loss: 0.697117  [89600/175341]\n",
      "loss: 0.591254  [91200/175341]\n",
      "loss: 0.246355  [92800/175341]\n",
      "loss: 0.439749  [94400/175341]\n",
      "loss: 0.507184  [96000/175341]\n",
      "loss: 0.027901  [97600/175341]\n",
      "loss: 0.833285  [99200/175341]\n",
      "loss: 0.361100  [100800/175341]\n",
      "loss: 0.249580  [102400/175341]\n",
      "loss: 0.483774  [104000/175341]\n",
      "loss: 0.369930  [105600/175341]\n",
      "loss: 0.206661  [107200/175341]\n",
      "loss: 0.523162  [108800/175341]\n",
      "loss: 0.708494  [110400/175341]\n",
      "loss: 0.463753  [112000/175341]\n",
      "loss: 0.318095  [113600/175341]\n",
      "loss: 0.393394  [115200/175341]\n",
      "loss: 0.701031  [116800/175341]\n",
      "loss: 0.284391  [118400/175341]\n",
      "loss: 0.555810  [120000/175341]\n",
      "loss: 0.499133  [121600/175341]\n",
      "loss: 0.387807  [123200/175341]\n",
      "loss: 0.337035  [124800/175341]\n",
      "loss: 0.472850  [126400/175341]\n",
      "loss: 0.770614  [128000/175341]\n",
      "loss: 0.526087  [129600/175341]\n",
      "loss: 0.710640  [131200/175341]\n",
      "loss: 0.220140  [132800/175341]\n",
      "loss: 0.740320  [134400/175341]\n",
      "loss: 0.717774  [136000/175341]\n",
      "loss: 0.282424  [137600/175341]\n",
      "loss: 0.438091  [139200/175341]\n",
      "loss: 0.257009  [140800/175341]\n",
      "loss: 0.772606  [142400/175341]\n",
      "loss: 0.267759  [144000/175341]\n",
      "loss: 0.170792  [145600/175341]\n",
      "loss: 0.612149  [147200/175341]\n",
      "loss: 0.250962  [148800/175341]\n",
      "loss: 0.513146  [150400/175341]\n",
      "loss: 0.455213  [152000/175341]\n",
      "loss: 0.501773  [153600/175341]\n",
      "loss: 0.694088  [155200/175341]\n",
      "loss: 0.632769  [156800/175341]\n",
      "loss: 0.442886  [158400/175341]\n",
      "loss: 0.705641  [160000/175341]\n",
      "loss: 0.757883  [161600/175341]\n",
      "loss: 0.686153  [163200/175341]\n",
      "loss: 0.928154  [164800/175341]\n",
      "loss: 0.381466  [166400/175341]\n",
      "loss: 0.546808  [168000/175341]\n",
      "loss: 0.597004  [169600/175341]\n",
      "loss: 0.276931  [171200/175341]\n",
      "loss: 0.656853  [172800/175341]\n",
      "loss: 0.387258  [174400/175341]\n",
      "Train Accuracy: 80.1604%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.618293, F1-score: 73.72%, Macro_F1-Score:  37.65%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.580515  [    0/175341]\n",
      "loss: 0.300115  [ 1600/175341]\n",
      "loss: 0.763359  [ 3200/175341]\n",
      "loss: 0.411432  [ 4800/175341]\n",
      "loss: 0.559986  [ 6400/175341]\n",
      "loss: 0.440195  [ 8000/175341]\n",
      "loss: 0.535889  [ 9600/175341]\n",
      "loss: 0.522819  [11200/175341]\n",
      "loss: 0.312020  [12800/175341]\n",
      "loss: 1.370660  [14400/175341]\n",
      "loss: 0.356622  [16000/175341]\n",
      "loss: 0.298305  [17600/175341]\n",
      "loss: 0.412861  [19200/175341]\n",
      "loss: 0.464563  [20800/175341]\n",
      "loss: 0.466345  [22400/175341]\n",
      "loss: 0.272761  [24000/175341]\n",
      "loss: 0.395306  [25600/175341]\n",
      "loss: 1.030209  [27200/175341]\n",
      "loss: 0.528199  [28800/175341]\n",
      "loss: 0.529595  [30400/175341]\n",
      "loss: 0.789877  [32000/175341]\n",
      "loss: 0.144916  [33600/175341]\n",
      "loss: 0.284942  [35200/175341]\n",
      "loss: 0.685668  [36800/175341]\n",
      "loss: 0.824026  [38400/175341]\n",
      "loss: 1.058202  [40000/175341]\n",
      "loss: 0.775133  [41600/175341]\n",
      "loss: 0.274972  [43200/175341]\n",
      "loss: 0.545235  [44800/175341]\n",
      "loss: 0.620066  [46400/175341]\n",
      "loss: 0.724789  [48000/175341]\n",
      "loss: 0.552973  [49600/175341]\n",
      "loss: 0.549181  [51200/175341]\n",
      "loss: 0.753155  [52800/175341]\n",
      "loss: 0.641817  [54400/175341]\n",
      "loss: 0.836956  [56000/175341]\n",
      "loss: 0.597283  [57600/175341]\n",
      "loss: 0.353012  [59200/175341]\n",
      "loss: 0.638294  [60800/175341]\n",
      "loss: 0.424088  [62400/175341]\n",
      "loss: 0.223412  [64000/175341]\n",
      "loss: 0.622935  [65600/175341]\n",
      "loss: 0.365074  [67200/175341]\n",
      "loss: 0.628811  [68800/175341]\n",
      "loss: 0.552357  [70400/175341]\n",
      "loss: 0.454559  [72000/175341]\n",
      "loss: 0.684930  [73600/175341]\n",
      "loss: 0.855178  [75200/175341]\n",
      "loss: 0.570974  [76800/175341]\n",
      "loss: 0.603194  [78400/175341]\n",
      "loss: 0.482106  [80000/175341]\n",
      "loss: 0.532975  [81600/175341]\n",
      "loss: 0.544341  [83200/175341]\n",
      "loss: 0.356328  [84800/175341]\n",
      "loss: 0.428079  [86400/175341]\n",
      "loss: 0.461642  [88000/175341]\n",
      "loss: 0.455979  [89600/175341]\n",
      "loss: 0.671898  [91200/175341]\n",
      "loss: 0.467732  [92800/175341]\n",
      "loss: 0.594728  [94400/175341]\n",
      "loss: 0.337623  [96000/175341]\n",
      "loss: 0.372671  [97600/175341]\n",
      "loss: 0.686325  [99200/175341]\n",
      "loss: 0.437320  [100800/175341]\n",
      "loss: 0.459946  [102400/175341]\n",
      "loss: 0.476688  [104000/175341]\n",
      "loss: 0.427050  [105600/175341]\n",
      "loss: 0.679563  [107200/175341]\n",
      "loss: 0.411624  [108800/175341]\n",
      "loss: 0.525826  [110400/175341]\n",
      "loss: 0.511454  [112000/175341]\n",
      "loss: 0.406151  [113600/175341]\n",
      "loss: 0.691908  [115200/175341]\n",
      "loss: 0.392032  [116800/175341]\n",
      "loss: 0.548180  [118400/175341]\n",
      "loss: 0.406708  [120000/175341]\n",
      "loss: 0.843894  [121600/175341]\n",
      "loss: 0.508306  [123200/175341]\n",
      "loss: 0.484652  [124800/175341]\n",
      "loss: 0.370049  [126400/175341]\n",
      "loss: 1.165095  [128000/175341]\n",
      "loss: 0.444782  [129600/175341]\n",
      "loss: 0.356663  [131200/175341]\n",
      "loss: 0.844852  [132800/175341]\n",
      "loss: 0.362482  [134400/175341]\n",
      "loss: 0.353961  [136000/175341]\n",
      "loss: 0.523201  [137600/175341]\n",
      "loss: 0.605525  [139200/175341]\n",
      "loss: 0.682998  [140800/175341]\n",
      "loss: 0.938506  [142400/175341]\n",
      "loss: 0.722903  [144000/175341]\n",
      "loss: 1.138429  [145600/175341]\n",
      "loss: 0.463747  [147200/175341]\n",
      "loss: 0.471140  [148800/175341]\n",
      "loss: 0.461527  [150400/175341]\n",
      "loss: 0.431261  [152000/175341]\n",
      "loss: 0.444258  [153600/175341]\n",
      "loss: 0.272902  [155200/175341]\n",
      "loss: 0.130867  [156800/175341]\n",
      "loss: 0.117258  [158400/175341]\n",
      "loss: 0.408471  [160000/175341]\n",
      "loss: 0.781018  [161600/175341]\n",
      "loss: 0.822353  [163200/175341]\n",
      "loss: 0.598782  [164800/175341]\n",
      "loss: 0.321127  [166400/175341]\n",
      "loss: 0.361112  [168000/175341]\n",
      "loss: 0.369834  [169600/175341]\n",
      "loss: 0.656698  [171200/175341]\n",
      "loss: 0.751547  [172800/175341]\n",
      "loss: 0.233082  [174400/175341]\n",
      "Train Accuracy: 80.1524%\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.632919, F1-score: 72.84%, Macro_F1-Score:  37.13%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.261318  [    0/175341]\n",
      "loss: 0.359962  [ 1600/175341]\n",
      "loss: 0.408188  [ 3200/175341]\n",
      "loss: 0.359895  [ 4800/175341]\n",
      "loss: 0.316187  [ 6400/175341]\n",
      "loss: 0.234949  [ 8000/175341]\n",
      "loss: 0.348105  [ 9600/175341]\n",
      "loss: 0.843950  [11200/175341]\n",
      "loss: 0.677603  [12800/175341]\n",
      "loss: 0.195107  [14400/175341]\n",
      "loss: 0.460991  [16000/175341]\n",
      "loss: 0.486264  [17600/175341]\n",
      "loss: 0.580283  [19200/175341]\n",
      "loss: 0.460050  [20800/175341]\n",
      "loss: 0.375302  [22400/175341]\n",
      "loss: 0.653230  [24000/175341]\n",
      "loss: 0.485445  [25600/175341]\n",
      "loss: 0.534845  [27200/175341]\n",
      "loss: 0.745963  [28800/175341]\n",
      "loss: 0.792779  [30400/175341]\n",
      "loss: 0.705298  [32000/175341]\n",
      "loss: 0.333809  [33600/175341]\n",
      "loss: 0.425123  [35200/175341]\n",
      "loss: 0.394089  [36800/175341]\n",
      "loss: 0.520814  [38400/175341]\n",
      "loss: 0.091992  [40000/175341]\n",
      "loss: 0.627279  [41600/175341]\n",
      "loss: 0.285895  [43200/175341]\n",
      "loss: 0.302266  [44800/175341]\n",
      "loss: 0.352664  [46400/175341]\n",
      "loss: 0.413686  [48000/175341]\n",
      "loss: 0.605560  [49600/175341]\n",
      "loss: 0.526563  [51200/175341]\n",
      "loss: 0.534577  [52800/175341]\n",
      "loss: 0.670641  [54400/175341]\n",
      "loss: 0.486347  [56000/175341]\n",
      "loss: 0.558799  [57600/175341]\n",
      "loss: 0.326896  [59200/175341]\n",
      "loss: 0.499322  [60800/175341]\n",
      "loss: 0.555509  [62400/175341]\n",
      "loss: 0.227434  [64000/175341]\n",
      "loss: 0.347795  [65600/175341]\n",
      "loss: 0.542178  [67200/175341]\n",
      "loss: 0.369127  [68800/175341]\n",
      "loss: 0.285694  [70400/175341]\n",
      "loss: 0.978160  [72000/175341]\n",
      "loss: 0.664992  [73600/175341]\n",
      "loss: 0.736267  [75200/175341]\n",
      "loss: 0.460969  [76800/175341]\n",
      "loss: 0.122661  [78400/175341]\n",
      "loss: 0.577583  [80000/175341]\n",
      "loss: 0.146427  [81600/175341]\n",
      "loss: 0.852068  [83200/175341]\n",
      "loss: 0.309462  [84800/175341]\n",
      "loss: 0.361535  [86400/175341]\n",
      "loss: 0.716111  [88000/175341]\n",
      "loss: 0.343878  [89600/175341]\n",
      "loss: 0.397904  [91200/175341]\n",
      "loss: 1.160067  [92800/175341]\n",
      "loss: 0.503581  [94400/175341]\n",
      "loss: 0.547602  [96000/175341]\n",
      "loss: 0.299042  [97600/175341]\n",
      "loss: 0.806195  [99200/175341]\n",
      "loss: 0.705314  [100800/175341]\n",
      "loss: 0.552142  [102400/175341]\n",
      "loss: 0.440668  [104000/175341]\n",
      "loss: 0.175855  [105600/175341]\n",
      "loss: 0.369410  [107200/175341]\n",
      "loss: 0.540732  [108800/175341]\n",
      "loss: 0.284639  [110400/175341]\n",
      "loss: 0.419170  [112000/175341]\n",
      "loss: 0.740817  [113600/175341]\n",
      "loss: 0.590167  [115200/175341]\n",
      "loss: 0.332188  [116800/175341]\n",
      "loss: 0.328727  [118400/175341]\n",
      "loss: 0.325774  [120000/175341]\n",
      "loss: 0.365189  [121600/175341]\n",
      "loss: 0.381746  [123200/175341]\n",
      "loss: 0.684912  [124800/175341]\n",
      "loss: 0.441224  [126400/175341]\n",
      "loss: 0.404823  [128000/175341]\n",
      "loss: 0.474143  [129600/175341]\n",
      "loss: 0.598329  [131200/175341]\n",
      "loss: 0.089412  [132800/175341]\n",
      "loss: 0.520683  [134400/175341]\n",
      "loss: 0.521537  [136000/175341]\n",
      "loss: 0.414510  [137600/175341]\n",
      "loss: 0.191545  [139200/175341]\n",
      "loss: 0.225568  [140800/175341]\n",
      "loss: 0.827929  [142400/175341]\n",
      "loss: 0.415571  [144000/175341]\n",
      "loss: 0.767713  [145600/175341]\n",
      "loss: 0.428237  [147200/175341]\n",
      "loss: 0.327735  [148800/175341]\n",
      "loss: 0.459663  [150400/175341]\n",
      "loss: 0.572902  [152000/175341]\n",
      "loss: 0.577464  [153600/175341]\n",
      "loss: 0.165659  [155200/175341]\n",
      "loss: 0.430157  [156800/175341]\n",
      "loss: 0.652397  [158400/175341]\n",
      "loss: 0.162856  [160000/175341]\n",
      "loss: 0.563878  [161600/175341]\n",
      "loss: 0.390285  [163200/175341]\n",
      "loss: 0.536116  [164800/175341]\n",
      "loss: 0.080965  [166400/175341]\n",
      "loss: 0.564491  [168000/175341]\n",
      "loss: 0.355721  [169600/175341]\n",
      "loss: 1.099965  [171200/175341]\n",
      "loss: 0.633574  [172800/175341]\n",
      "loss: 0.902752  [174400/175341]\n",
      "Train Accuracy: 80.2089%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.615288, F1-score: 74.18%, Macro_F1-Score:  37.55%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.821339  [    0/175341]\n",
      "loss: 0.506045  [ 1600/175341]\n",
      "loss: 0.450262  [ 3200/175341]\n",
      "loss: 0.457179  [ 4800/175341]\n",
      "loss: 1.030097  [ 6400/175341]\n",
      "loss: 0.370213  [ 8000/175341]\n",
      "loss: 0.432724  [ 9600/175341]\n",
      "loss: 0.536080  [11200/175341]\n",
      "loss: 0.574247  [12800/175341]\n",
      "loss: 0.659672  [14400/175341]\n",
      "loss: 0.575058  [16000/175341]\n",
      "loss: 0.583127  [17600/175341]\n",
      "loss: 0.289116  [19200/175341]\n",
      "loss: 0.500683  [20800/175341]\n",
      "loss: 0.407130  [22400/175341]\n",
      "loss: 0.253133  [24000/175341]\n",
      "loss: 0.364853  [25600/175341]\n",
      "loss: 0.743133  [27200/175341]\n",
      "loss: 0.550351  [28800/175341]\n",
      "loss: 0.166808  [30400/175341]\n",
      "loss: 0.435271  [32000/175341]\n",
      "loss: 0.663130  [33600/175341]\n",
      "loss: 0.288800  [35200/175341]\n",
      "loss: 0.616943  [36800/175341]\n",
      "loss: 0.161099  [38400/175341]\n",
      "loss: 0.261896  [40000/175341]\n",
      "loss: 0.199888  [41600/175341]\n",
      "loss: 0.606952  [43200/175341]\n",
      "loss: 0.721416  [44800/175341]\n",
      "loss: 0.477046  [46400/175341]\n",
      "loss: 0.752984  [48000/175341]\n",
      "loss: 0.658855  [49600/175341]\n",
      "loss: 0.834653  [51200/175341]\n",
      "loss: 0.266921  [52800/175341]\n",
      "loss: 0.524302  [54400/175341]\n",
      "loss: 0.510425  [56000/175341]\n",
      "loss: 0.201771  [57600/175341]\n",
      "loss: 0.405653  [59200/175341]\n",
      "loss: 0.442660  [60800/175341]\n",
      "loss: 0.390284  [62400/175341]\n",
      "loss: 0.612853  [64000/175341]\n",
      "loss: 0.391511  [65600/175341]\n",
      "loss: 0.206666  [67200/175341]\n",
      "loss: 0.741814  [68800/175341]\n",
      "loss: 1.149959  [70400/175341]\n",
      "loss: 0.386371  [72000/175341]\n",
      "loss: 0.421901  [73600/175341]\n",
      "loss: 0.377823  [75200/175341]\n",
      "loss: 0.243179  [76800/175341]\n",
      "loss: 0.403759  [78400/175341]\n",
      "loss: 0.384868  [80000/175341]\n",
      "loss: 0.427176  [81600/175341]\n",
      "loss: 0.474191  [83200/175341]\n",
      "loss: 0.521968  [84800/175341]\n",
      "loss: 0.418915  [86400/175341]\n",
      "loss: 0.638027  [88000/175341]\n",
      "loss: 0.264613  [89600/175341]\n",
      "loss: 0.690949  [91200/175341]\n",
      "loss: 0.488499  [92800/175341]\n",
      "loss: 0.819620  [94400/175341]\n",
      "loss: 0.349667  [96000/175341]\n",
      "loss: 0.402216  [97600/175341]\n",
      "loss: 0.429847  [99200/175341]\n",
      "loss: 0.546934  [100800/175341]\n",
      "loss: 0.889377  [102400/175341]\n",
      "loss: 0.614850  [104000/175341]\n",
      "loss: 0.225300  [105600/175341]\n",
      "loss: 1.280908  [107200/175341]\n",
      "loss: 0.266566  [108800/175341]\n",
      "loss: 0.261174  [110400/175341]\n",
      "loss: 0.568150  [112000/175341]\n",
      "loss: 0.231764  [113600/175341]\n",
      "loss: 0.556202  [115200/175341]\n",
      "loss: 0.405076  [116800/175341]\n",
      "loss: 0.395637  [118400/175341]\n",
      "loss: 0.784501  [120000/175341]\n",
      "loss: 0.832302  [121600/175341]\n",
      "loss: 0.386416  [123200/175341]\n",
      "loss: 0.448853  [124800/175341]\n",
      "loss: 0.579253  [126400/175341]\n",
      "loss: 0.482550  [128000/175341]\n",
      "loss: 0.209518  [129600/175341]\n",
      "loss: 0.932027  [131200/175341]\n",
      "loss: 1.024026  [132800/175341]\n",
      "loss: 0.500470  [134400/175341]\n",
      "loss: 0.366765  [136000/175341]\n",
      "loss: 1.124889  [137600/175341]\n",
      "loss: 0.501198  [139200/175341]\n",
      "loss: 0.664608  [140800/175341]\n",
      "loss: 0.485613  [142400/175341]\n",
      "loss: 0.430367  [144000/175341]\n",
      "loss: 0.654483  [145600/175341]\n",
      "loss: 0.835036  [147200/175341]\n",
      "loss: 0.799880  [148800/175341]\n",
      "loss: 0.651031  [150400/175341]\n",
      "loss: 0.199662  [152000/175341]\n",
      "loss: 0.481545  [153600/175341]\n",
      "loss: 0.498459  [155200/175341]\n",
      "loss: 0.871225  [156800/175341]\n",
      "loss: 0.132939  [158400/175341]\n",
      "loss: 1.078834  [160000/175341]\n",
      "loss: 0.468553  [161600/175341]\n",
      "loss: 0.561284  [163200/175341]\n",
      "loss: 0.602134  [164800/175341]\n",
      "loss: 0.282130  [166400/175341]\n",
      "loss: 0.623967  [168000/175341]\n",
      "loss: 0.425871  [169600/175341]\n",
      "loss: 0.184092  [171200/175341]\n",
      "loss: 0.787116  [172800/175341]\n",
      "loss: 0.408272  [174400/175341]\n",
      "Train Accuracy: 80.1935%\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.630549, F1-score: 73.60%, Macro_F1-Score:  37.73%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b427034-602d-4252-af52-9c7d0908f074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.501613  [    0/175341]\n",
      "loss: 0.374448  [ 1600/175341]\n",
      "loss: 0.557692  [ 3200/175341]\n",
      "loss: 0.594048  [ 4800/175341]\n",
      "loss: 0.315406  [ 6400/175341]\n",
      "loss: 0.701515  [ 8000/175341]\n",
      "loss: 0.713072  [ 9600/175341]\n",
      "loss: 0.276991  [11200/175341]\n",
      "loss: 0.545927  [12800/175341]\n",
      "loss: 0.958105  [14400/175341]\n",
      "loss: 0.411914  [16000/175341]\n",
      "loss: 0.326948  [17600/175341]\n",
      "loss: 0.516545  [19200/175341]\n",
      "loss: 0.765352  [20800/175341]\n",
      "loss: 0.362010  [22400/175341]\n",
      "loss: 0.161243  [24000/175341]\n",
      "loss: 0.581813  [25600/175341]\n",
      "loss: 0.314990  [27200/175341]\n",
      "loss: 0.332266  [28800/175341]\n",
      "loss: 0.605031  [30400/175341]\n",
      "loss: 0.195770  [32000/175341]\n",
      "loss: 0.858500  [33600/175341]\n",
      "loss: 0.491555  [35200/175341]\n",
      "loss: 0.500268  [36800/175341]\n",
      "loss: 0.197153  [38400/175341]\n",
      "loss: 0.346912  [40000/175341]\n",
      "loss: 0.700093  [41600/175341]\n",
      "loss: 0.239347  [43200/175341]\n",
      "loss: 0.205047  [44800/175341]\n",
      "loss: 0.216106  [46400/175341]\n",
      "loss: 0.554284  [48000/175341]\n",
      "loss: 0.572139  [49600/175341]\n",
      "loss: 0.534251  [51200/175341]\n",
      "loss: 0.419904  [52800/175341]\n",
      "loss: 0.126459  [54400/175341]\n",
      "loss: 0.302574  [56000/175341]\n",
      "loss: 0.510164  [57600/175341]\n",
      "loss: 0.331187  [59200/175341]\n",
      "loss: 0.509556  [60800/175341]\n",
      "loss: 0.299789  [62400/175341]\n",
      "loss: 0.669453  [64000/175341]\n",
      "loss: 0.385675  [65600/175341]\n",
      "loss: 0.206699  [67200/175341]\n",
      "loss: 0.282336  [68800/175341]\n",
      "loss: 0.613931  [70400/175341]\n",
      "loss: 1.035658  [72000/175341]\n",
      "loss: 0.357866  [73600/175341]\n",
      "loss: 0.646664  [75200/175341]\n",
      "loss: 0.308747  [76800/175341]\n",
      "loss: 0.366732  [78400/175341]\n",
      "loss: 0.728247  [80000/175341]\n",
      "loss: 0.495347  [81600/175341]\n",
      "loss: 0.544414  [83200/175341]\n",
      "loss: 0.648377  [84800/175341]\n",
      "loss: 0.597029  [86400/175341]\n",
      "loss: 0.456499  [88000/175341]\n",
      "loss: 0.652067  [89600/175341]\n",
      "loss: 0.235448  [91200/175341]\n",
      "loss: 0.732539  [92800/175341]\n",
      "loss: 0.746295  [94400/175341]\n",
      "loss: 0.569480  [96000/175341]\n",
      "loss: 0.547959  [97600/175341]\n",
      "loss: 0.521125  [99200/175341]\n",
      "loss: 0.261196  [100800/175341]\n",
      "loss: 0.329574  [102400/175341]\n",
      "loss: 0.454888  [104000/175341]\n",
      "loss: 0.611599  [105600/175341]\n",
      "loss: 0.586529  [107200/175341]\n",
      "loss: 0.427438  [108800/175341]\n",
      "loss: 0.911143  [110400/175341]\n",
      "loss: 0.763231  [112000/175341]\n",
      "loss: 0.272345  [113600/175341]\n",
      "loss: 0.517570  [115200/175341]\n",
      "loss: 0.197664  [116800/175341]\n",
      "loss: 0.453036  [118400/175341]\n",
      "loss: 0.748844  [120000/175341]\n",
      "loss: 0.596378  [121600/175341]\n",
      "loss: 0.140825  [123200/175341]\n",
      "loss: 0.364199  [124800/175341]\n",
      "loss: 0.605875  [126400/175341]\n",
      "loss: 0.487799  [128000/175341]\n",
      "loss: 0.518087  [129600/175341]\n",
      "loss: 0.707433  [131200/175341]\n",
      "loss: 0.697615  [132800/175341]\n",
      "loss: 0.534464  [134400/175341]\n",
      "loss: 0.323729  [136000/175341]\n",
      "loss: 0.544630  [137600/175341]\n",
      "loss: 0.553702  [139200/175341]\n",
      "loss: 0.604192  [140800/175341]\n",
      "loss: 0.310712  [142400/175341]\n",
      "loss: 0.460756  [144000/175341]\n",
      "loss: 0.438168  [145600/175341]\n",
      "loss: 0.873069  [147200/175341]\n",
      "loss: 0.252649  [148800/175341]\n",
      "loss: 0.284153  [150400/175341]\n",
      "loss: 0.288830  [152000/175341]\n",
      "loss: 0.389315  [153600/175341]\n",
      "loss: 0.857786  [155200/175341]\n",
      "loss: 0.556594  [156800/175341]\n",
      "loss: 0.167034  [158400/175341]\n",
      "loss: 0.333450  [160000/175341]\n",
      "loss: 0.128650  [161600/175341]\n",
      "loss: 0.445379  [163200/175341]\n",
      "loss: 0.307916  [164800/175341]\n",
      "loss: 0.205113  [166400/175341]\n",
      "loss: 0.287674  [168000/175341]\n",
      "loss: 0.349065  [169600/175341]\n",
      "loss: 0.282545  [171200/175341]\n",
      "loss: 0.325985  [172800/175341]\n",
      "loss: 0.200801  [174400/175341]\n",
      "Train Accuracy: 80.2351%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.589452, F1-score: 75.79%, Macro_F1-Score:  38.52%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.151103  [    0/175341]\n",
      "loss: 0.742079  [ 1600/175341]\n",
      "loss: 0.338086  [ 3200/175341]\n",
      "loss: 0.572461  [ 4800/175341]\n",
      "loss: 0.536275  [ 6400/175341]\n",
      "loss: 0.670179  [ 8000/175341]\n",
      "loss: 0.919057  [ 9600/175341]\n",
      "loss: 0.564921  [11200/175341]\n",
      "loss: 0.530010  [12800/175341]\n",
      "loss: 0.458537  [14400/175341]\n",
      "loss: 0.269369  [16000/175341]\n",
      "loss: 0.538683  [17600/175341]\n",
      "loss: 0.566008  [19200/175341]\n",
      "loss: 0.487896  [20800/175341]\n",
      "loss: 0.861629  [22400/175341]\n",
      "loss: 0.308121  [24000/175341]\n",
      "loss: 0.384806  [25600/175341]\n",
      "loss: 0.704356  [27200/175341]\n",
      "loss: 0.255536  [28800/175341]\n",
      "loss: 0.523369  [30400/175341]\n",
      "loss: 0.349598  [32000/175341]\n",
      "loss: 0.659490  [33600/175341]\n",
      "loss: 0.425523  [35200/175341]\n",
      "loss: 1.023197  [36800/175341]\n",
      "loss: 0.593358  [38400/175341]\n",
      "loss: 0.262948  [40000/175341]\n",
      "loss: 0.879606  [41600/175341]\n",
      "loss: 0.578215  [43200/175341]\n",
      "loss: 0.588818  [44800/175341]\n",
      "loss: 0.928887  [46400/175341]\n",
      "loss: 0.575533  [48000/175341]\n",
      "loss: 0.492925  [49600/175341]\n",
      "loss: 0.609447  [51200/175341]\n",
      "loss: 0.327044  [52800/175341]\n",
      "loss: 0.436653  [54400/175341]\n",
      "loss: 0.442566  [56000/175341]\n",
      "loss: 0.949718  [57600/175341]\n",
      "loss: 0.555037  [59200/175341]\n",
      "loss: 0.562501  [60800/175341]\n",
      "loss: 0.557621  [62400/175341]\n",
      "loss: 0.681923  [64000/175341]\n",
      "loss: 0.500352  [65600/175341]\n",
      "loss: 0.666714  [67200/175341]\n",
      "loss: 0.401623  [68800/175341]\n",
      "loss: 0.440295  [70400/175341]\n",
      "loss: 0.940305  [72000/175341]\n",
      "loss: 0.915922  [73600/175341]\n",
      "loss: 0.403502  [75200/175341]\n",
      "loss: 0.932325  [76800/175341]\n",
      "loss: 0.588219  [78400/175341]\n",
      "loss: 0.738712  [80000/175341]\n",
      "loss: 0.683524  [81600/175341]\n",
      "loss: 0.435593  [83200/175341]\n",
      "loss: 0.308406  [84800/175341]\n",
      "loss: 0.647768  [86400/175341]\n",
      "loss: 0.971820  [88000/175341]\n",
      "loss: 0.305529  [89600/175341]\n",
      "loss: 0.467336  [91200/175341]\n",
      "loss: 0.219875  [92800/175341]\n",
      "loss: 0.533672  [94400/175341]\n",
      "loss: 0.767256  [96000/175341]\n",
      "loss: 0.513795  [97600/175341]\n",
      "loss: 0.464433  [99200/175341]\n",
      "loss: 0.818153  [100800/175341]\n",
      "loss: 0.689595  [102400/175341]\n",
      "loss: 0.862876  [104000/175341]\n",
      "loss: 0.348586  [105600/175341]\n",
      "loss: 0.496977  [107200/175341]\n",
      "loss: 0.707251  [108800/175341]\n",
      "loss: 0.400997  [110400/175341]\n",
      "loss: 0.728141  [112000/175341]\n",
      "loss: 0.372212  [113600/175341]\n",
      "loss: 0.727251  [115200/175341]\n",
      "loss: 0.674265  [116800/175341]\n",
      "loss: 0.752393  [118400/175341]\n",
      "loss: 0.253168  [120000/175341]\n",
      "loss: 0.380410  [121600/175341]\n",
      "loss: 0.176826  [123200/175341]\n",
      "loss: 0.563390  [124800/175341]\n",
      "loss: 0.286673  [126400/175341]\n",
      "loss: 0.776603  [128000/175341]\n",
      "loss: 0.536560  [129600/175341]\n",
      "loss: 0.383503  [131200/175341]\n",
      "loss: 0.386861  [132800/175341]\n",
      "loss: 0.372762  [134400/175341]\n",
      "loss: 0.525058  [136000/175341]\n",
      "loss: 0.497201  [137600/175341]\n",
      "loss: 0.635782  [139200/175341]\n",
      "loss: 0.646878  [140800/175341]\n",
      "loss: 0.700039  [142400/175341]\n",
      "loss: 0.419624  [144000/175341]\n",
      "loss: 0.518014  [145600/175341]\n",
      "loss: 0.250276  [147200/175341]\n",
      "loss: 0.465996  [148800/175341]\n",
      "loss: 0.455168  [150400/175341]\n",
      "loss: 0.422021  [152000/175341]\n",
      "loss: 0.287442  [153600/175341]\n",
      "loss: 0.776209  [155200/175341]\n",
      "loss: 0.166627  [156800/175341]\n",
      "loss: 0.364041  [158400/175341]\n",
      "loss: 0.602172  [160000/175341]\n",
      "loss: 0.746536  [161600/175341]\n",
      "loss: 0.664753  [163200/175341]\n",
      "loss: 0.297596  [164800/175341]\n",
      "loss: 0.327041  [166400/175341]\n",
      "loss: 0.464798  [168000/175341]\n",
      "loss: 0.503125  [169600/175341]\n",
      "loss: 0.417582  [171200/175341]\n",
      "loss: 0.823776  [172800/175341]\n",
      "loss: 0.653818  [174400/175341]\n",
      "Train Accuracy: 80.2049%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.604727, F1-score: 74.40%, Macro_F1-Score:  38.42%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.545502  [    0/175341]\n",
      "loss: 0.355834  [ 1600/175341]\n",
      "loss: 0.634134  [ 3200/175341]\n",
      "loss: 0.145847  [ 4800/175341]\n",
      "loss: 0.783238  [ 6400/175341]\n",
      "loss: 0.230167  [ 8000/175341]\n",
      "loss: 0.679116  [ 9600/175341]\n",
      "loss: 0.883839  [11200/175341]\n",
      "loss: 0.795090  [12800/175341]\n",
      "loss: 0.205580  [14400/175341]\n",
      "loss: 0.384574  [16000/175341]\n",
      "loss: 0.682074  [17600/175341]\n",
      "loss: 0.694124  [19200/175341]\n",
      "loss: 0.836638  [20800/175341]\n",
      "loss: 0.445084  [22400/175341]\n",
      "loss: 0.718958  [24000/175341]\n",
      "loss: 0.258259  [25600/175341]\n",
      "loss: 0.399552  [27200/175341]\n",
      "loss: 0.179195  [28800/175341]\n",
      "loss: 0.298833  [30400/175341]\n",
      "loss: 0.876373  [32000/175341]\n",
      "loss: 0.265914  [33600/175341]\n",
      "loss: 0.420915  [35200/175341]\n",
      "loss: 0.410014  [36800/175341]\n",
      "loss: 0.816957  [38400/175341]\n",
      "loss: 0.834332  [40000/175341]\n",
      "loss: 0.625859  [41600/175341]\n",
      "loss: 0.760275  [43200/175341]\n",
      "loss: 0.840780  [44800/175341]\n",
      "loss: 0.818797  [46400/175341]\n",
      "loss: 0.499378  [48000/175341]\n",
      "loss: 1.047378  [49600/175341]\n",
      "loss: 0.527958  [51200/175341]\n",
      "loss: 0.677142  [52800/175341]\n",
      "loss: 0.738108  [54400/175341]\n",
      "loss: 0.315615  [56000/175341]\n",
      "loss: 0.583514  [57600/175341]\n",
      "loss: 0.440623  [59200/175341]\n",
      "loss: 0.413809  [60800/175341]\n",
      "loss: 0.506207  [62400/175341]\n",
      "loss: 0.485982  [64000/175341]\n",
      "loss: 0.291631  [65600/175341]\n",
      "loss: 0.430526  [67200/175341]\n",
      "loss: 0.924523  [68800/175341]\n",
      "loss: 0.635384  [70400/175341]\n",
      "loss: 0.628068  [72000/175341]\n",
      "loss: 0.407119  [73600/175341]\n",
      "loss: 0.408495  [75200/175341]\n",
      "loss: 0.297125  [76800/175341]\n",
      "loss: 0.138549  [78400/175341]\n",
      "loss: 0.447502  [80000/175341]\n",
      "loss: 0.511061  [81600/175341]\n",
      "loss: 0.721814  [83200/175341]\n",
      "loss: 0.597684  [84800/175341]\n",
      "loss: 0.776872  [86400/175341]\n",
      "loss: 0.454769  [88000/175341]\n",
      "loss: 0.442177  [89600/175341]\n",
      "loss: 0.140392  [91200/175341]\n",
      "loss: 0.607933  [92800/175341]\n",
      "loss: 0.641285  [94400/175341]\n",
      "loss: 0.458532  [96000/175341]\n",
      "loss: 0.790120  [97600/175341]\n",
      "loss: 0.554400  [99200/175341]\n",
      "loss: 0.357969  [100800/175341]\n",
      "loss: 0.717049  [102400/175341]\n",
      "loss: 0.480792  [104000/175341]\n",
      "loss: 0.692028  [105600/175341]\n",
      "loss: 0.304658  [107200/175341]\n",
      "loss: 0.399197  [108800/175341]\n",
      "loss: 0.720681  [110400/175341]\n",
      "loss: 1.136453  [112000/175341]\n",
      "loss: 0.608533  [113600/175341]\n",
      "loss: 0.284607  [115200/175341]\n",
      "loss: 0.314375  [116800/175341]\n",
      "loss: 0.505757  [118400/175341]\n",
      "loss: 0.482899  [120000/175341]\n",
      "loss: 0.552650  [121600/175341]\n",
      "loss: 0.402316  [123200/175341]\n",
      "loss: 0.493216  [124800/175341]\n",
      "loss: 0.246263  [126400/175341]\n",
      "loss: 0.397690  [128000/175341]\n",
      "loss: 0.153745  [129600/175341]\n",
      "loss: 0.583326  [131200/175341]\n",
      "loss: 0.248148  [132800/175341]\n",
      "loss: 0.342615  [134400/175341]\n",
      "loss: 0.644073  [136000/175341]\n",
      "loss: 0.329098  [137600/175341]\n",
      "loss: 0.233144  [139200/175341]\n",
      "loss: 0.381611  [140800/175341]\n",
      "loss: 0.452685  [142400/175341]\n",
      "loss: 0.248390  [144000/175341]\n",
      "loss: 0.567953  [145600/175341]\n",
      "loss: 0.745660  [147200/175341]\n",
      "loss: 0.536562  [148800/175341]\n",
      "loss: 0.183976  [150400/175341]\n",
      "loss: 0.425024  [152000/175341]\n",
      "loss: 0.315782  [153600/175341]\n",
      "loss: 0.497142  [155200/175341]\n",
      "loss: 0.462754  [156800/175341]\n",
      "loss: 0.486055  [158400/175341]\n",
      "loss: 0.550613  [160000/175341]\n",
      "loss: 0.278429  [161600/175341]\n",
      "loss: 0.890343  [163200/175341]\n",
      "loss: 0.460872  [164800/175341]\n",
      "loss: 0.334660  [166400/175341]\n",
      "loss: 0.773487  [168000/175341]\n",
      "loss: 0.717153  [169600/175341]\n",
      "loss: 0.343413  [171200/175341]\n",
      "loss: 0.591385  [172800/175341]\n",
      "loss: 0.624499  [174400/175341]\n",
      "Train Accuracy: 80.2550%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.598392, F1-score: 74.81%, Macro_F1-Score:  38.79%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.502405  [    0/175341]\n",
      "loss: 0.210564  [ 1600/175341]\n",
      "loss: 1.097502  [ 3200/175341]\n",
      "loss: 0.548432  [ 4800/175341]\n",
      "loss: 0.950035  [ 6400/175341]\n",
      "loss: 0.535168  [ 8000/175341]\n",
      "loss: 0.586842  [ 9600/175341]\n",
      "loss: 0.153171  [11200/175341]\n",
      "loss: 0.343496  [12800/175341]\n",
      "loss: 0.387628  [14400/175341]\n",
      "loss: 0.758007  [16000/175341]\n",
      "loss: 0.351102  [17600/175341]\n",
      "loss: 0.725116  [19200/175341]\n",
      "loss: 0.455035  [20800/175341]\n",
      "loss: 0.398807  [22400/175341]\n",
      "loss: 0.600778  [24000/175341]\n",
      "loss: 0.713940  [25600/175341]\n",
      "loss: 0.496037  [27200/175341]\n",
      "loss: 0.547884  [28800/175341]\n",
      "loss: 0.373589  [30400/175341]\n",
      "loss: 0.824341  [32000/175341]\n",
      "loss: 0.520831  [33600/175341]\n",
      "loss: 0.702663  [35200/175341]\n",
      "loss: 0.341350  [36800/175341]\n",
      "loss: 0.379200  [38400/175341]\n",
      "loss: 0.714476  [40000/175341]\n",
      "loss: 0.583048  [41600/175341]\n",
      "loss: 0.178323  [43200/175341]\n",
      "loss: 0.478905  [44800/175341]\n",
      "loss: 0.452623  [46400/175341]\n",
      "loss: 0.196486  [48000/175341]\n",
      "loss: 0.201848  [49600/175341]\n",
      "loss: 0.252545  [51200/175341]\n",
      "loss: 0.671400  [52800/175341]\n",
      "loss: 1.123659  [54400/175341]\n",
      "loss: 0.251059  [56000/175341]\n",
      "loss: 0.401894  [57600/175341]\n",
      "loss: 0.201247  [59200/175341]\n",
      "loss: 0.426108  [60800/175341]\n",
      "loss: 0.308175  [62400/175341]\n",
      "loss: 0.352923  [64000/175341]\n",
      "loss: 0.318454  [65600/175341]\n",
      "loss: 0.521907  [67200/175341]\n",
      "loss: 0.275678  [68800/175341]\n",
      "loss: 0.807857  [70400/175341]\n",
      "loss: 0.564023  [72000/175341]\n",
      "loss: 0.443581  [73600/175341]\n",
      "loss: 0.628975  [75200/175341]\n",
      "loss: 0.645257  [76800/175341]\n",
      "loss: 0.481774  [78400/175341]\n",
      "loss: 0.638367  [80000/175341]\n",
      "loss: 0.349255  [81600/175341]\n",
      "loss: 0.565406  [83200/175341]\n",
      "loss: 0.561338  [84800/175341]\n",
      "loss: 0.427324  [86400/175341]\n",
      "loss: 0.510457  [88000/175341]\n",
      "loss: 0.450484  [89600/175341]\n",
      "loss: 0.299733  [91200/175341]\n",
      "loss: 0.521065  [92800/175341]\n",
      "loss: 0.280213  [94400/175341]\n",
      "loss: 0.225442  [96000/175341]\n",
      "loss: 0.978658  [97600/175341]\n",
      "loss: 0.177785  [99200/175341]\n",
      "loss: 0.276216  [100800/175341]\n",
      "loss: 0.880024  [102400/175341]\n",
      "loss: 0.408695  [104000/175341]\n",
      "loss: 0.208304  [105600/175341]\n",
      "loss: 0.442821  [107200/175341]\n",
      "loss: 0.301431  [108800/175341]\n",
      "loss: 1.058386  [110400/175341]\n",
      "loss: 0.550580  [112000/175341]\n",
      "loss: 0.721317  [113600/175341]\n",
      "loss: 0.272973  [115200/175341]\n",
      "loss: 0.693106  [116800/175341]\n",
      "loss: 0.352965  [118400/175341]\n",
      "loss: 0.275750  [120000/175341]\n",
      "loss: 0.708849  [121600/175341]\n",
      "loss: 0.958176  [123200/175341]\n",
      "loss: 0.404707  [124800/175341]\n",
      "loss: 0.226565  [126400/175341]\n",
      "loss: 0.770604  [128000/175341]\n",
      "loss: 0.280131  [129600/175341]\n",
      "loss: 0.536708  [131200/175341]\n",
      "loss: 0.309399  [132800/175341]\n",
      "loss: 0.445984  [134400/175341]\n",
      "loss: 0.317478  [136000/175341]\n",
      "loss: 0.442881  [137600/175341]\n",
      "loss: 0.403979  [139200/175341]\n",
      "loss: 0.211682  [140800/175341]\n",
      "loss: 0.418841  [142400/175341]\n",
      "loss: 0.781073  [144000/175341]\n",
      "loss: 0.869564  [145600/175341]\n",
      "loss: 0.288330  [147200/175341]\n",
      "loss: 0.513991  [148800/175341]\n",
      "loss: 0.375810  [150400/175341]\n",
      "loss: 0.436910  [152000/175341]\n",
      "loss: 0.492702  [153600/175341]\n",
      "loss: 0.281959  [155200/175341]\n",
      "loss: 0.371850  [156800/175341]\n",
      "loss: 0.589805  [158400/175341]\n",
      "loss: 0.370479  [160000/175341]\n",
      "loss: 0.422826  [161600/175341]\n",
      "loss: 0.884892  [163200/175341]\n",
      "loss: 0.386884  [164800/175341]\n",
      "loss: 0.535660  [166400/175341]\n",
      "loss: 0.780289  [168000/175341]\n",
      "loss: 0.295139  [169600/175341]\n",
      "loss: 0.484145  [171200/175341]\n",
      "loss: 0.661604  [172800/175341]\n",
      "loss: 0.364139  [174400/175341]\n",
      "Train Accuracy: 80.2904%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.614317, F1-score: 74.23%, Macro_F1-Score:  38.16%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.567911  [    0/175341]\n",
      "loss: 0.203413  [ 1600/175341]\n",
      "loss: 0.897378  [ 3200/175341]\n",
      "loss: 0.589716  [ 4800/175341]\n",
      "loss: 0.485432  [ 6400/175341]\n",
      "loss: 0.857718  [ 8000/175341]\n",
      "loss: 0.251921  [ 9600/175341]\n",
      "loss: 0.413399  [11200/175341]\n",
      "loss: 0.791120  [12800/175341]\n",
      "loss: 0.383389  [14400/175341]\n",
      "loss: 0.310497  [16000/175341]\n",
      "loss: 0.567230  [17600/175341]\n",
      "loss: 0.358958  [19200/175341]\n",
      "loss: 0.284615  [20800/175341]\n",
      "loss: 0.692557  [22400/175341]\n",
      "loss: 0.548605  [24000/175341]\n",
      "loss: 0.309588  [25600/175341]\n",
      "loss: 0.539186  [27200/175341]\n",
      "loss: 0.646719  [28800/175341]\n",
      "loss: 0.862110  [30400/175341]\n",
      "loss: 0.703344  [32000/175341]\n",
      "loss: 0.476821  [33600/175341]\n",
      "loss: 0.291934  [35200/175341]\n",
      "loss: 0.461361  [36800/175341]\n",
      "loss: 0.261618  [38400/175341]\n",
      "loss: 0.593087  [40000/175341]\n",
      "loss: 0.779974  [41600/175341]\n",
      "loss: 0.231848  [43200/175341]\n",
      "loss: 0.354551  [44800/175341]\n",
      "loss: 0.558972  [46400/175341]\n",
      "loss: 0.698086  [48000/175341]\n",
      "loss: 1.107346  [49600/175341]\n",
      "loss: 0.352678  [51200/175341]\n",
      "loss: 0.192757  [52800/175341]\n",
      "loss: 0.657891  [54400/175341]\n",
      "loss: 0.227792  [56000/175341]\n",
      "loss: 0.369912  [57600/175341]\n",
      "loss: 0.339888  [59200/175341]\n",
      "loss: 0.611108  [60800/175341]\n",
      "loss: 0.906114  [62400/175341]\n",
      "loss: 0.272361  [64000/175341]\n",
      "loss: 0.979618  [65600/175341]\n",
      "loss: 0.586806  [67200/175341]\n",
      "loss: 0.616155  [68800/175341]\n",
      "loss: 0.417043  [70400/175341]\n",
      "loss: 0.683184  [72000/175341]\n",
      "loss: 0.349316  [73600/175341]\n",
      "loss: 0.541085  [75200/175341]\n",
      "loss: 0.527245  [76800/175341]\n",
      "loss: 0.672253  [78400/175341]\n",
      "loss: 0.104415  [80000/175341]\n",
      "loss: 0.624483  [81600/175341]\n",
      "loss: 0.592002  [83200/175341]\n",
      "loss: 0.428406  [84800/175341]\n",
      "loss: 0.836544  [86400/175341]\n",
      "loss: 1.024992  [88000/175341]\n",
      "loss: 0.629874  [89600/175341]\n",
      "loss: 0.865549  [91200/175341]\n",
      "loss: 0.278357  [92800/175341]\n",
      "loss: 0.248412  [94400/175341]\n",
      "loss: 0.512213  [96000/175341]\n",
      "loss: 0.421674  [97600/175341]\n",
      "loss: 0.675149  [99200/175341]\n",
      "loss: 0.790740  [100800/175341]\n",
      "loss: 0.277586  [102400/175341]\n",
      "loss: 0.440384  [104000/175341]\n",
      "loss: 0.315457  [105600/175341]\n",
      "loss: 0.576506  [107200/175341]\n",
      "loss: 0.308214  [108800/175341]\n",
      "loss: 0.252189  [110400/175341]\n",
      "loss: 0.371398  [112000/175341]\n",
      "loss: 0.720633  [113600/175341]\n",
      "loss: 1.076975  [115200/175341]\n",
      "loss: 0.423388  [116800/175341]\n",
      "loss: 0.407130  [118400/175341]\n",
      "loss: 0.263777  [120000/175341]\n",
      "loss: 0.583589  [121600/175341]\n",
      "loss: 0.755952  [123200/175341]\n",
      "loss: 0.630032  [124800/175341]\n",
      "loss: 0.527839  [126400/175341]\n",
      "loss: 0.726982  [128000/175341]\n",
      "loss: 0.410622  [129600/175341]\n",
      "loss: 0.390564  [131200/175341]\n",
      "loss: 0.427744  [132800/175341]\n",
      "loss: 0.244938  [134400/175341]\n",
      "loss: 0.756500  [136000/175341]\n",
      "loss: 0.521835  [137600/175341]\n",
      "loss: 0.451770  [139200/175341]\n",
      "loss: 1.117563  [140800/175341]\n",
      "loss: 0.614480  [142400/175341]\n",
      "loss: 0.198055  [144000/175341]\n",
      "loss: 0.839096  [145600/175341]\n",
      "loss: 0.546787  [147200/175341]\n",
      "loss: 0.618551  [148800/175341]\n",
      "loss: 0.342803  [150400/175341]\n",
      "loss: 0.386302  [152000/175341]\n",
      "loss: 0.239799  [153600/175341]\n",
      "loss: 0.671352  [155200/175341]\n",
      "loss: 0.422771  [156800/175341]\n",
      "loss: 0.435946  [158400/175341]\n",
      "loss: 0.761949  [160000/175341]\n",
      "loss: 0.693182  [161600/175341]\n",
      "loss: 0.744350  [163200/175341]\n",
      "loss: 0.602651  [164800/175341]\n",
      "loss: 0.768502  [166400/175341]\n",
      "loss: 0.687496  [168000/175341]\n",
      "loss: 0.784945  [169600/175341]\n",
      "loss: 0.443746  [171200/175341]\n",
      "loss: 0.711932  [172800/175341]\n",
      "loss: 0.555587  [174400/175341]\n",
      "Train Accuracy: 80.2847%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.580735, F1-score: 75.22%, Macro_F1-Score:  38.67%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.380428  [    0/175341]\n",
      "loss: 0.628274  [ 1600/175341]\n",
      "loss: 0.391325  [ 3200/175341]\n",
      "loss: 0.534225  [ 4800/175341]\n",
      "loss: 0.403304  [ 6400/175341]\n",
      "loss: 0.544570  [ 8000/175341]\n",
      "loss: 0.525612  [ 9600/175341]\n",
      "loss: 0.486432  [11200/175341]\n",
      "loss: 0.415441  [12800/175341]\n",
      "loss: 0.419327  [14400/175341]\n",
      "loss: 0.445674  [16000/175341]\n",
      "loss: 0.804347  [17600/175341]\n",
      "loss: 0.274106  [19200/175341]\n",
      "loss: 0.582962  [20800/175341]\n",
      "loss: 0.356433  [22400/175341]\n",
      "loss: 0.374009  [24000/175341]\n",
      "loss: 0.472122  [25600/175341]\n",
      "loss: 0.453048  [27200/175341]\n",
      "loss: 0.586634  [28800/175341]\n",
      "loss: 0.029448  [30400/175341]\n",
      "loss: 0.561359  [32000/175341]\n",
      "loss: 0.352087  [33600/175341]\n",
      "loss: 0.692386  [35200/175341]\n",
      "loss: 0.558204  [36800/175341]\n",
      "loss: 0.745825  [38400/175341]\n",
      "loss: 0.299896  [40000/175341]\n",
      "loss: 0.550645  [41600/175341]\n",
      "loss: 0.629190  [43200/175341]\n",
      "loss: 0.390204  [44800/175341]\n",
      "loss: 0.611455  [46400/175341]\n",
      "loss: 0.660743  [48000/175341]\n",
      "loss: 0.765054  [49600/175341]\n",
      "loss: 0.381357  [51200/175341]\n",
      "loss: 0.893448  [52800/175341]\n",
      "loss: 0.350225  [54400/175341]\n",
      "loss: 0.132600  [56000/175341]\n",
      "loss: 0.791192  [57600/175341]\n",
      "loss: 0.543529  [59200/175341]\n",
      "loss: 0.464400  [60800/175341]\n",
      "loss: 0.188537  [62400/175341]\n",
      "loss: 0.328424  [64000/175341]\n",
      "loss: 0.408564  [65600/175341]\n",
      "loss: 0.538204  [67200/175341]\n",
      "loss: 0.425164  [68800/175341]\n",
      "loss: 0.352043  [70400/175341]\n",
      "loss: 0.247184  [72000/175341]\n",
      "loss: 0.824789  [73600/175341]\n",
      "loss: 0.483107  [75200/175341]\n",
      "loss: 0.569412  [76800/175341]\n",
      "loss: 0.211913  [78400/175341]\n",
      "loss: 0.593896  [80000/175341]\n",
      "loss: 0.927046  [81600/175341]\n",
      "loss: 0.314656  [83200/175341]\n",
      "loss: 0.267434  [84800/175341]\n",
      "loss: 0.616361  [86400/175341]\n",
      "loss: 0.377967  [88000/175341]\n",
      "loss: 0.374333  [89600/175341]\n",
      "loss: 0.627528  [91200/175341]\n",
      "loss: 0.840283  [92800/175341]\n",
      "loss: 0.469209  [94400/175341]\n",
      "loss: 0.669434  [96000/175341]\n",
      "loss: 0.504739  [97600/175341]\n",
      "loss: 0.373773  [99200/175341]\n",
      "loss: 0.541617  [100800/175341]\n",
      "loss: 0.385420  [102400/175341]\n",
      "loss: 0.364138  [104000/175341]\n",
      "loss: 0.480381  [105600/175341]\n",
      "loss: 0.367419  [107200/175341]\n",
      "loss: 0.495227  [108800/175341]\n",
      "loss: 0.612460  [110400/175341]\n",
      "loss: 0.271119  [112000/175341]\n",
      "loss: 0.538138  [113600/175341]\n",
      "loss: 0.363605  [115200/175341]\n",
      "loss: 0.288554  [116800/175341]\n",
      "loss: 0.775537  [118400/175341]\n",
      "loss: 0.334152  [120000/175341]\n",
      "loss: 0.485015  [121600/175341]\n",
      "loss: 0.361383  [123200/175341]\n",
      "loss: 0.627498  [124800/175341]\n",
      "loss: 0.729891  [126400/175341]\n",
      "loss: 0.715757  [128000/175341]\n",
      "loss: 0.524436  [129600/175341]\n",
      "loss: 0.465362  [131200/175341]\n",
      "loss: 0.758470  [132800/175341]\n",
      "loss: 0.621080  [134400/175341]\n",
      "loss: 0.381971  [136000/175341]\n",
      "loss: 0.257718  [137600/175341]\n",
      "loss: 0.413280  [139200/175341]\n",
      "loss: 0.343158  [140800/175341]\n",
      "loss: 0.630802  [142400/175341]\n",
      "loss: 0.703024  [144000/175341]\n",
      "loss: 0.493809  [145600/175341]\n",
      "loss: 0.634399  [147200/175341]\n",
      "loss: 0.660035  [148800/175341]\n",
      "loss: 0.529375  [150400/175341]\n",
      "loss: 0.974055  [152000/175341]\n",
      "loss: 0.587482  [153600/175341]\n",
      "loss: 0.590142  [155200/175341]\n",
      "loss: 0.782207  [156800/175341]\n",
      "loss: 1.099753  [158400/175341]\n",
      "loss: 0.260355  [160000/175341]\n",
      "loss: 0.780911  [161600/175341]\n",
      "loss: 1.030363  [163200/175341]\n",
      "loss: 0.177123  [164800/175341]\n",
      "loss: 0.126504  [166400/175341]\n",
      "loss: 0.765598  [168000/175341]\n",
      "loss: 0.518424  [169600/175341]\n",
      "loss: 0.198598  [171200/175341]\n",
      "loss: 0.406986  [172800/175341]\n",
      "loss: 0.192551  [174400/175341]\n",
      "Train Accuracy: 80.3149%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.608561, F1-score: 74.48%, Macro_F1-Score:  38.95%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.515336  [    0/175341]\n",
      "loss: 0.105709  [ 1600/175341]\n",
      "loss: 0.347742  [ 3200/175341]\n",
      "loss: 0.345364  [ 4800/175341]\n",
      "loss: 0.396279  [ 6400/175341]\n",
      "loss: 0.637792  [ 8000/175341]\n",
      "loss: 0.709804  [ 9600/175341]\n",
      "loss: 0.268522  [11200/175341]\n",
      "loss: 0.351416  [12800/175341]\n",
      "loss: 0.583969  [14400/175341]\n",
      "loss: 0.333446  [16000/175341]\n",
      "loss: 0.771697  [17600/175341]\n",
      "loss: 1.063128  [19200/175341]\n",
      "loss: 0.636452  [20800/175341]\n",
      "loss: 0.332360  [22400/175341]\n",
      "loss: 0.485951  [24000/175341]\n",
      "loss: 0.496860  [25600/175341]\n",
      "loss: 0.508480  [27200/175341]\n",
      "loss: 1.110556  [28800/175341]\n",
      "loss: 0.445344  [30400/175341]\n",
      "loss: 0.998013  [32000/175341]\n",
      "loss: 0.281202  [33600/175341]\n",
      "loss: 0.310069  [35200/175341]\n",
      "loss: 0.411174  [36800/175341]\n",
      "loss: 0.451894  [38400/175341]\n",
      "loss: 0.487132  [40000/175341]\n",
      "loss: 0.344706  [41600/175341]\n",
      "loss: 0.600643  [43200/175341]\n",
      "loss: 0.515787  [44800/175341]\n",
      "loss: 0.541563  [46400/175341]\n",
      "loss: 0.301504  [48000/175341]\n",
      "loss: 0.531914  [49600/175341]\n",
      "loss: 0.754039  [51200/175341]\n",
      "loss: 0.280753  [52800/175341]\n",
      "loss: 0.374108  [54400/175341]\n",
      "loss: 0.554064  [56000/175341]\n",
      "loss: 0.815332  [57600/175341]\n",
      "loss: 0.494160  [59200/175341]\n",
      "loss: 0.344982  [60800/175341]\n",
      "loss: 0.407239  [62400/175341]\n",
      "loss: 0.727434  [64000/175341]\n",
      "loss: 0.728292  [65600/175341]\n",
      "loss: 1.192264  [67200/175341]\n",
      "loss: 0.835014  [68800/175341]\n",
      "loss: 0.759431  [70400/175341]\n",
      "loss: 0.804252  [72000/175341]\n",
      "loss: 0.401172  [73600/175341]\n",
      "loss: 0.700606  [75200/175341]\n",
      "loss: 0.649863  [76800/175341]\n",
      "loss: 0.581403  [78400/175341]\n",
      "loss: 0.638987  [80000/175341]\n",
      "loss: 0.291959  [81600/175341]\n",
      "loss: 0.694012  [83200/175341]\n",
      "loss: 0.503006  [84800/175341]\n",
      "loss: 0.616147  [86400/175341]\n",
      "loss: 0.546076  [88000/175341]\n",
      "loss: 0.132798  [89600/175341]\n",
      "loss: 0.223862  [91200/175341]\n",
      "loss: 0.070815  [92800/175341]\n",
      "loss: 0.441653  [94400/175341]\n",
      "loss: 0.619486  [96000/175341]\n",
      "loss: 0.650081  [97600/175341]\n",
      "loss: 0.498610  [99200/175341]\n",
      "loss: 0.424672  [100800/175341]\n",
      "loss: 0.281723  [102400/175341]\n",
      "loss: 1.110952  [104000/175341]\n",
      "loss: 0.351696  [105600/175341]\n",
      "loss: 0.527126  [107200/175341]\n",
      "loss: 0.468721  [108800/175341]\n",
      "loss: 0.124793  [110400/175341]\n",
      "loss: 0.337231  [112000/175341]\n",
      "loss: 0.301378  [113600/175341]\n",
      "loss: 0.061269  [115200/175341]\n",
      "loss: 0.684057  [116800/175341]\n",
      "loss: 0.665269  [118400/175341]\n",
      "loss: 0.500355  [120000/175341]\n",
      "loss: 0.342400  [121600/175341]\n",
      "loss: 1.053539  [123200/175341]\n",
      "loss: 0.880286  [124800/175341]\n",
      "loss: 0.358461  [126400/175341]\n",
      "loss: 0.593120  [128000/175341]\n",
      "loss: 0.235980  [129600/175341]\n",
      "loss: 0.707377  [131200/175341]\n",
      "loss: 0.590902  [132800/175341]\n",
      "loss: 0.389455  [134400/175341]\n",
      "loss: 0.206922  [136000/175341]\n",
      "loss: 0.429329  [137600/175341]\n",
      "loss: 0.323328  [139200/175341]\n",
      "loss: 0.684973  [140800/175341]\n",
      "loss: 0.639840  [142400/175341]\n",
      "loss: 0.850279  [144000/175341]\n",
      "loss: 0.434160  [145600/175341]\n",
      "loss: 0.633621  [147200/175341]\n",
      "loss: 0.690863  [148800/175341]\n",
      "loss: 0.459970  [150400/175341]\n",
      "loss: 0.876536  [152000/175341]\n",
      "loss: 0.396285  [153600/175341]\n",
      "loss: 0.418131  [155200/175341]\n",
      "loss: 0.637311  [156800/175341]\n",
      "loss: 0.409336  [158400/175341]\n",
      "loss: 0.524704  [160000/175341]\n",
      "loss: 0.703254  [161600/175341]\n",
      "loss: 0.603524  [163200/175341]\n",
      "loss: 0.471884  [164800/175341]\n",
      "loss: 0.222037  [166400/175341]\n",
      "loss: 0.373110  [168000/175341]\n",
      "loss: 0.598460  [169600/175341]\n",
      "loss: 0.488300  [171200/175341]\n",
      "loss: 0.416198  [172800/175341]\n",
      "loss: 0.262258  [174400/175341]\n",
      "Train Accuracy: 80.2955%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.592664, F1-score: 74.49%, Macro_F1-Score:  39.30%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.139575  [    0/175341]\n",
      "loss: 0.698353  [ 1600/175341]\n",
      "loss: 0.568322  [ 3200/175341]\n",
      "loss: 0.240291  [ 4800/175341]\n",
      "loss: 0.519689  [ 6400/175341]\n",
      "loss: 0.921733  [ 8000/175341]\n",
      "loss: 0.513352  [ 9600/175341]\n",
      "loss: 0.675874  [11200/175341]\n",
      "loss: 0.125237  [12800/175341]\n",
      "loss: 0.807036  [14400/175341]\n",
      "loss: 0.394103  [16000/175341]\n",
      "loss: 0.502318  [17600/175341]\n",
      "loss: 0.695398  [19200/175341]\n",
      "loss: 0.424618  [20800/175341]\n",
      "loss: 0.325173  [22400/175341]\n",
      "loss: 0.283727  [24000/175341]\n",
      "loss: 0.319363  [25600/175341]\n",
      "loss: 1.179091  [27200/175341]\n",
      "loss: 0.298842  [28800/175341]\n",
      "loss: 0.368024  [30400/175341]\n",
      "loss: 0.389192  [32000/175341]\n",
      "loss: 0.558410  [33600/175341]\n",
      "loss: 0.228957  [35200/175341]\n",
      "loss: 0.276796  [36800/175341]\n",
      "loss: 0.508950  [38400/175341]\n",
      "loss: 0.419667  [40000/175341]\n",
      "loss: 0.403864  [41600/175341]\n",
      "loss: 0.387119  [43200/175341]\n",
      "loss: 0.788363  [44800/175341]\n",
      "loss: 0.376465  [46400/175341]\n",
      "loss: 0.511512  [48000/175341]\n",
      "loss: 0.308933  [49600/175341]\n",
      "loss: 0.721825  [51200/175341]\n",
      "loss: 0.354992  [52800/175341]\n",
      "loss: 0.771920  [54400/175341]\n",
      "loss: 0.367332  [56000/175341]\n",
      "loss: 0.499007  [57600/175341]\n",
      "loss: 0.343062  [59200/175341]\n",
      "loss: 0.418315  [60800/175341]\n",
      "loss: 0.637621  [62400/175341]\n",
      "loss: 1.046348  [64000/175341]\n",
      "loss: 0.496217  [65600/175341]\n",
      "loss: 0.619902  [67200/175341]\n",
      "loss: 0.351041  [68800/175341]\n",
      "loss: 0.190530  [70400/175341]\n",
      "loss: 0.638122  [72000/175341]\n",
      "loss: 0.447026  [73600/175341]\n",
      "loss: 0.277733  [75200/175341]\n",
      "loss: 0.471639  [76800/175341]\n",
      "loss: 0.624728  [78400/175341]\n",
      "loss: 0.161807  [80000/175341]\n",
      "loss: 0.513750  [81600/175341]\n",
      "loss: 0.878234  [83200/175341]\n",
      "loss: 0.483432  [84800/175341]\n",
      "loss: 0.113827  [86400/175341]\n",
      "loss: 0.459385  [88000/175341]\n",
      "loss: 0.206382  [89600/175341]\n",
      "loss: 0.184710  [91200/175341]\n",
      "loss: 0.381388  [92800/175341]\n",
      "loss: 0.348062  [94400/175341]\n",
      "loss: 1.056035  [96000/175341]\n",
      "loss: 0.073896  [97600/175341]\n",
      "loss: 0.209309  [99200/175341]\n",
      "loss: 0.276943  [100800/175341]\n",
      "loss: 0.448367  [102400/175341]\n",
      "loss: 0.348040  [104000/175341]\n",
      "loss: 0.369685  [105600/175341]\n",
      "loss: 0.368926  [107200/175341]\n",
      "loss: 0.419361  [108800/175341]\n",
      "loss: 0.353638  [110400/175341]\n",
      "loss: 0.449245  [112000/175341]\n",
      "loss: 0.286747  [113600/175341]\n",
      "loss: 0.290941  [115200/175341]\n",
      "loss: 0.172755  [116800/175341]\n",
      "loss: 0.627296  [118400/175341]\n",
      "loss: 0.364466  [120000/175341]\n",
      "loss: 0.652872  [121600/175341]\n",
      "loss: 0.358982  [123200/175341]\n",
      "loss: 0.719387  [124800/175341]\n",
      "loss: 0.223852  [126400/175341]\n",
      "loss: 0.503622  [128000/175341]\n",
      "loss: 0.329726  [129600/175341]\n",
      "loss: 0.823699  [131200/175341]\n",
      "loss: 0.728222  [132800/175341]\n",
      "loss: 0.325236  [134400/175341]\n",
      "loss: 0.435635  [136000/175341]\n",
      "loss: 0.311884  [137600/175341]\n",
      "loss: 0.082875  [139200/175341]\n",
      "loss: 0.662470  [140800/175341]\n",
      "loss: 0.360259  [142400/175341]\n",
      "loss: 0.340494  [144000/175341]\n",
      "loss: 0.303007  [145600/175341]\n",
      "loss: 0.235883  [147200/175341]\n",
      "loss: 0.679093  [148800/175341]\n",
      "loss: 0.570039  [150400/175341]\n",
      "loss: 0.184930  [152000/175341]\n",
      "loss: 0.490428  [153600/175341]\n",
      "loss: 0.613833  [155200/175341]\n",
      "loss: 0.202188  [156800/175341]\n",
      "loss: 0.662099  [158400/175341]\n",
      "loss: 0.780420  [160000/175341]\n",
      "loss: 0.428033  [161600/175341]\n",
      "loss: 0.910773  [163200/175341]\n",
      "loss: 0.335081  [164800/175341]\n",
      "loss: 0.232678  [166400/175341]\n",
      "loss: 0.428641  [168000/175341]\n",
      "loss: 0.804456  [169600/175341]\n",
      "loss: 0.244652  [171200/175341]\n",
      "loss: 0.288343  [172800/175341]\n",
      "loss: 0.381208  [174400/175341]\n",
      "Train Accuracy: 80.3286%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.575004, F1-score: 76.28%, Macro_F1-Score:  39.92%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.193327  [    0/175341]\n",
      "loss: 0.551786  [ 1600/175341]\n",
      "loss: 0.550601  [ 3200/175341]\n",
      "loss: 0.275972  [ 4800/175341]\n",
      "loss: 0.662196  [ 6400/175341]\n",
      "loss: 0.636418  [ 8000/175341]\n",
      "loss: 0.815656  [ 9600/175341]\n",
      "loss: 0.143279  [11200/175341]\n",
      "loss: 0.604111  [12800/175341]\n",
      "loss: 0.395363  [14400/175341]\n",
      "loss: 0.606469  [16000/175341]\n",
      "loss: 0.503392  [17600/175341]\n",
      "loss: 0.190474  [19200/175341]\n",
      "loss: 0.270436  [20800/175341]\n",
      "loss: 0.486531  [22400/175341]\n",
      "loss: 0.523105  [24000/175341]\n",
      "loss: 0.360466  [25600/175341]\n",
      "loss: 0.490688  [27200/175341]\n",
      "loss: 0.674741  [28800/175341]\n",
      "loss: 0.221357  [30400/175341]\n",
      "loss: 0.321657  [32000/175341]\n",
      "loss: 0.493121  [33600/175341]\n",
      "loss: 0.451519  [35200/175341]\n",
      "loss: 0.458331  [36800/175341]\n",
      "loss: 0.590611  [38400/175341]\n",
      "loss: 0.591287  [40000/175341]\n",
      "loss: 0.582385  [41600/175341]\n",
      "loss: 0.340284  [43200/175341]\n",
      "loss: 0.321798  [44800/175341]\n",
      "loss: 0.496487  [46400/175341]\n",
      "loss: 0.314976  [48000/175341]\n",
      "loss: 0.287178  [49600/175341]\n",
      "loss: 0.462487  [51200/175341]\n",
      "loss: 0.343152  [52800/175341]\n",
      "loss: 1.245946  [54400/175341]\n",
      "loss: 0.287989  [56000/175341]\n",
      "loss: 0.460705  [57600/175341]\n",
      "loss: 0.441558  [59200/175341]\n",
      "loss: 0.563238  [60800/175341]\n",
      "loss: 0.538363  [62400/175341]\n",
      "loss: 0.385451  [64000/175341]\n",
      "loss: 0.344097  [65600/175341]\n",
      "loss: 0.651384  [67200/175341]\n",
      "loss: 0.525317  [68800/175341]\n",
      "loss: 0.440080  [70400/175341]\n",
      "loss: 0.695131  [72000/175341]\n",
      "loss: 0.500357  [73600/175341]\n",
      "loss: 0.256621  [75200/175341]\n",
      "loss: 0.537176  [76800/175341]\n",
      "loss: 0.512010  [78400/175341]\n",
      "loss: 0.567432  [80000/175341]\n",
      "loss: 0.229336  [81600/175341]\n",
      "loss: 0.379646  [83200/175341]\n",
      "loss: 0.694196  [84800/175341]\n",
      "loss: 0.225959  [86400/175341]\n",
      "loss: 0.152091  [88000/175341]\n",
      "loss: 0.818127  [89600/175341]\n",
      "loss: 0.252227  [91200/175341]\n",
      "loss: 0.553476  [92800/175341]\n",
      "loss: 0.668568  [94400/175341]\n",
      "loss: 0.640907  [96000/175341]\n",
      "loss: 0.388236  [97600/175341]\n",
      "loss: 0.400095  [99200/175341]\n",
      "loss: 0.839187  [100800/175341]\n",
      "loss: 0.437957  [102400/175341]\n",
      "loss: 0.523437  [104000/175341]\n",
      "loss: 0.416030  [105600/175341]\n",
      "loss: 0.850341  [107200/175341]\n",
      "loss: 0.422550  [108800/175341]\n",
      "loss: 0.575771  [110400/175341]\n",
      "loss: 0.271632  [112000/175341]\n",
      "loss: 0.506797  [113600/175341]\n",
      "loss: 0.173396  [115200/175341]\n",
      "loss: 0.508560  [116800/175341]\n",
      "loss: 0.619233  [118400/175341]\n",
      "loss: 0.574342  [120000/175341]\n",
      "loss: 0.695977  [121600/175341]\n",
      "loss: 0.177050  [123200/175341]\n",
      "loss: 0.519048  [124800/175341]\n",
      "loss: 0.716245  [126400/175341]\n",
      "loss: 0.844164  [128000/175341]\n",
      "loss: 0.643381  [129600/175341]\n",
      "loss: 0.346801  [131200/175341]\n",
      "loss: 0.302383  [132800/175341]\n",
      "loss: 0.533230  [134400/175341]\n",
      "loss: 0.620033  [136000/175341]\n",
      "loss: 0.195643  [137600/175341]\n",
      "loss: 0.299830  [139200/175341]\n",
      "loss: 0.472065  [140800/175341]\n",
      "loss: 0.517784  [142400/175341]\n",
      "loss: 0.795375  [144000/175341]\n",
      "loss: 0.647350  [145600/175341]\n",
      "loss: 0.420031  [147200/175341]\n",
      "loss: 0.386158  [148800/175341]\n",
      "loss: 0.398016  [150400/175341]\n",
      "loss: 0.302585  [152000/175341]\n",
      "loss: 0.560430  [153600/175341]\n",
      "loss: 0.414465  [155200/175341]\n",
      "loss: 0.382546  [156800/175341]\n",
      "loss: 1.026519  [158400/175341]\n",
      "loss: 0.627616  [160000/175341]\n",
      "loss: 0.727211  [161600/175341]\n",
      "loss: 0.482941  [163200/175341]\n",
      "loss: 0.534441  [164800/175341]\n",
      "loss: 0.574799  [166400/175341]\n",
      "loss: 0.647675  [168000/175341]\n",
      "loss: 0.604808  [169600/175341]\n",
      "loss: 0.413941  [171200/175341]\n",
      "loss: 0.483926  [172800/175341]\n",
      "loss: 0.744470  [174400/175341]\n",
      "Train Accuracy: 80.3725%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.592045, F1-score: 74.42%, Macro_F1-Score:  39.06%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.870683  [    0/175341]\n",
      "loss: 0.904693  [ 1600/175341]\n",
      "loss: 0.625041  [ 3200/175341]\n",
      "loss: 1.044301  [ 4800/175341]\n",
      "loss: 0.190885  [ 6400/175341]\n",
      "loss: 0.683283  [ 8000/175341]\n",
      "loss: 0.397410  [ 9600/175341]\n",
      "loss: 0.242099  [11200/175341]\n",
      "loss: 0.514139  [12800/175341]\n",
      "loss: 0.606985  [14400/175341]\n",
      "loss: 0.320231  [16000/175341]\n",
      "loss: 0.472512  [17600/175341]\n",
      "loss: 0.284698  [19200/175341]\n",
      "loss: 0.588673  [20800/175341]\n",
      "loss: 0.243558  [22400/175341]\n",
      "loss: 0.585201  [24000/175341]\n",
      "loss: 0.322029  [25600/175341]\n",
      "loss: 0.590695  [27200/175341]\n",
      "loss: 0.452849  [28800/175341]\n",
      "loss: 0.561214  [30400/175341]\n",
      "loss: 0.344104  [32000/175341]\n",
      "loss: 0.537313  [33600/175341]\n",
      "loss: 0.405739  [35200/175341]\n",
      "loss: 0.335646  [36800/175341]\n",
      "loss: 0.585509  [38400/175341]\n",
      "loss: 0.439336  [40000/175341]\n",
      "loss: 0.893307  [41600/175341]\n",
      "loss: 0.616804  [43200/175341]\n",
      "loss: 0.761866  [44800/175341]\n",
      "loss: 0.345057  [46400/175341]\n",
      "loss: 0.375732  [48000/175341]\n",
      "loss: 0.432968  [49600/175341]\n",
      "loss: 0.260964  [51200/175341]\n",
      "loss: 0.327860  [52800/175341]\n",
      "loss: 0.307009  [54400/175341]\n",
      "loss: 0.470103  [56000/175341]\n",
      "loss: 0.349268  [57600/175341]\n",
      "loss: 1.118607  [59200/175341]\n",
      "loss: 0.204460  [60800/175341]\n",
      "loss: 0.492430  [62400/175341]\n",
      "loss: 0.502375  [64000/175341]\n",
      "loss: 0.421579  [65600/175341]\n",
      "loss: 0.350552  [67200/175341]\n",
      "loss: 0.523733  [68800/175341]\n",
      "loss: 0.297174  [70400/175341]\n",
      "loss: 0.302054  [72000/175341]\n",
      "loss: 0.691700  [73600/175341]\n",
      "loss: 0.350880  [75200/175341]\n",
      "loss: 0.622937  [76800/175341]\n",
      "loss: 0.904046  [78400/175341]\n",
      "loss: 0.283718  [80000/175341]\n",
      "loss: 0.226170  [81600/175341]\n",
      "loss: 0.361012  [83200/175341]\n",
      "loss: 0.506122  [84800/175341]\n",
      "loss: 0.321415  [86400/175341]\n",
      "loss: 0.369831  [88000/175341]\n",
      "loss: 0.932573  [89600/175341]\n",
      "loss: 0.498897  [91200/175341]\n",
      "loss: 0.354764  [92800/175341]\n",
      "loss: 0.353270  [94400/175341]\n",
      "loss: 0.699274  [96000/175341]\n",
      "loss: 0.868723  [97600/175341]\n",
      "loss: 0.414817  [99200/175341]\n",
      "loss: 0.231366  [100800/175341]\n",
      "loss: 0.291172  [102400/175341]\n",
      "loss: 0.463384  [104000/175341]\n",
      "loss: 0.235121  [105600/175341]\n",
      "loss: 0.166276  [107200/175341]\n",
      "loss: 0.531842  [108800/175341]\n",
      "loss: 0.644560  [110400/175341]\n",
      "loss: 0.766068  [112000/175341]\n",
      "loss: 0.488666  [113600/175341]\n",
      "loss: 0.542001  [115200/175341]\n",
      "loss: 0.307835  [116800/175341]\n",
      "loss: 0.541468  [118400/175341]\n",
      "loss: 0.983941  [120000/175341]\n",
      "loss: 0.376253  [121600/175341]\n",
      "loss: 0.242654  [123200/175341]\n",
      "loss: 0.555986  [124800/175341]\n",
      "loss: 0.779718  [126400/175341]\n",
      "loss: 0.651190  [128000/175341]\n",
      "loss: 0.343523  [129600/175341]\n",
      "loss: 0.215319  [131200/175341]\n",
      "loss: 0.433699  [132800/175341]\n",
      "loss: 0.460747  [134400/175341]\n",
      "loss: 0.209679  [136000/175341]\n",
      "loss: 0.249919  [137600/175341]\n",
      "loss: 0.418884  [139200/175341]\n",
      "loss: 0.371632  [140800/175341]\n",
      "loss: 0.247986  [142400/175341]\n",
      "loss: 0.845049  [144000/175341]\n",
      "loss: 0.348863  [145600/175341]\n",
      "loss: 0.360958  [147200/175341]\n",
      "loss: 0.370090  [148800/175341]\n",
      "loss: 0.530422  [150400/175341]\n",
      "loss: 0.337377  [152000/175341]\n",
      "loss: 0.309732  [153600/175341]\n",
      "loss: 0.485517  [155200/175341]\n",
      "loss: 0.511167  [156800/175341]\n",
      "loss: 0.342458  [158400/175341]\n",
      "loss: 0.502402  [160000/175341]\n",
      "loss: 0.139612  [161600/175341]\n",
      "loss: 0.680505  [163200/175341]\n",
      "loss: 0.350022  [164800/175341]\n",
      "loss: 0.438912  [166400/175341]\n",
      "loss: 0.525148  [168000/175341]\n",
      "loss: 0.648950  [169600/175341]\n",
      "loss: 0.410367  [171200/175341]\n",
      "loss: 0.463822  [172800/175341]\n",
      "loss: 0.540673  [174400/175341]\n",
      "Train Accuracy: 80.3349%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.571747, F1-score: 75.18%, Macro_F1-Score:  39.28%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.784532  [    0/175341]\n",
      "loss: 0.687681  [ 1600/175341]\n",
      "loss: 0.675524  [ 3200/175341]\n",
      "loss: 0.593363  [ 4800/175341]\n",
      "loss: 0.300638  [ 6400/175341]\n",
      "loss: 0.319700  [ 8000/175341]\n",
      "loss: 0.346637  [ 9600/175341]\n",
      "loss: 0.185803  [11200/175341]\n",
      "loss: 0.285732  [12800/175341]\n",
      "loss: 0.573466  [14400/175341]\n",
      "loss: 0.495737  [16000/175341]\n",
      "loss: 0.685043  [17600/175341]\n",
      "loss: 0.562825  [19200/175341]\n",
      "loss: 0.362451  [20800/175341]\n",
      "loss: 0.548913  [22400/175341]\n",
      "loss: 0.195815  [24000/175341]\n",
      "loss: 0.584285  [25600/175341]\n",
      "loss: 0.476738  [27200/175341]\n",
      "loss: 0.676506  [28800/175341]\n",
      "loss: 0.624534  [30400/175341]\n",
      "loss: 0.498452  [32000/175341]\n",
      "loss: 0.341582  [33600/175341]\n",
      "loss: 0.432358  [35200/175341]\n",
      "loss: 0.664595  [36800/175341]\n",
      "loss: 0.388142  [38400/175341]\n",
      "loss: 0.880870  [40000/175341]\n",
      "loss: 0.379573  [41600/175341]\n",
      "loss: 0.376666  [43200/175341]\n",
      "loss: 0.551749  [44800/175341]\n",
      "loss: 0.115688  [46400/175341]\n",
      "loss: 0.347093  [48000/175341]\n",
      "loss: 0.671772  [49600/175341]\n",
      "loss: 0.320307  [51200/175341]\n",
      "loss: 0.296892  [52800/175341]\n",
      "loss: 0.220606  [54400/175341]\n",
      "loss: 0.580360  [56000/175341]\n",
      "loss: 0.464354  [57600/175341]\n",
      "loss: 0.466280  [59200/175341]\n",
      "loss: 1.201675  [60800/175341]\n",
      "loss: 0.291389  [62400/175341]\n",
      "loss: 1.182905  [64000/175341]\n",
      "loss: 0.181959  [65600/175341]\n",
      "loss: 0.697064  [67200/175341]\n",
      "loss: 0.578390  [68800/175341]\n",
      "loss: 0.689597  [70400/175341]\n",
      "loss: 0.409507  [72000/175341]\n",
      "loss: 0.235067  [73600/175341]\n",
      "loss: 0.614272  [75200/175341]\n",
      "loss: 0.660682  [76800/175341]\n",
      "loss: 0.454925  [78400/175341]\n",
      "loss: 0.832436  [80000/175341]\n",
      "loss: 0.418396  [81600/175341]\n",
      "loss: 0.486183  [83200/175341]\n",
      "loss: 0.536804  [84800/175341]\n",
      "loss: 0.399092  [86400/175341]\n",
      "loss: 0.812007  [88000/175341]\n",
      "loss: 0.332085  [89600/175341]\n",
      "loss: 0.250925  [91200/175341]\n",
      "loss: 0.656170  [92800/175341]\n",
      "loss: 0.501248  [94400/175341]\n",
      "loss: 0.202441  [96000/175341]\n",
      "loss: 0.119609  [97600/175341]\n",
      "loss: 0.487613  [99200/175341]\n",
      "loss: 0.222409  [100800/175341]\n",
      "loss: 0.436609  [102400/175341]\n",
      "loss: 0.593606  [104000/175341]\n",
      "loss: 0.758913  [105600/175341]\n",
      "loss: 0.467707  [107200/175341]\n",
      "loss: 0.575329  [108800/175341]\n",
      "loss: 0.827738  [110400/175341]\n",
      "loss: 0.331119  [112000/175341]\n",
      "loss: 0.402398  [113600/175341]\n",
      "loss: 0.681114  [115200/175341]\n",
      "loss: 0.624403  [116800/175341]\n",
      "loss: 0.552903  [118400/175341]\n",
      "loss: 0.706057  [120000/175341]\n",
      "loss: 0.593432  [121600/175341]\n",
      "loss: 0.393921  [123200/175341]\n",
      "loss: 0.142814  [124800/175341]\n",
      "loss: 0.579919  [126400/175341]\n",
      "loss: 0.319697  [128000/175341]\n",
      "loss: 0.303219  [129600/175341]\n",
      "loss: 0.521537  [131200/175341]\n",
      "loss: 0.255201  [132800/175341]\n",
      "loss: 0.384725  [134400/175341]\n",
      "loss: 0.754459  [136000/175341]\n",
      "loss: 0.262950  [137600/175341]\n",
      "loss: 0.274550  [139200/175341]\n",
      "loss: 0.408738  [140800/175341]\n",
      "loss: 0.379982  [142400/175341]\n",
      "loss: 0.645049  [144000/175341]\n",
      "loss: 0.645177  [145600/175341]\n",
      "loss: 0.438640  [147200/175341]\n",
      "loss: 0.547344  [148800/175341]\n",
      "loss: 0.260640  [150400/175341]\n",
      "loss: 0.769012  [152000/175341]\n",
      "loss: 0.443576  [153600/175341]\n",
      "loss: 0.137746  [155200/175341]\n",
      "loss: 0.475042  [156800/175341]\n",
      "loss: 0.630399  [158400/175341]\n",
      "loss: 0.324837  [160000/175341]\n",
      "loss: 0.362134  [161600/175341]\n",
      "loss: 0.487577  [163200/175341]\n",
      "loss: 0.638798  [164800/175341]\n",
      "loss: 0.402836  [166400/175341]\n",
      "loss: 0.856435  [168000/175341]\n",
      "loss: 0.630111  [169600/175341]\n",
      "loss: 0.569073  [171200/175341]\n",
      "loss: 0.610583  [172800/175341]\n",
      "loss: 0.704700  [174400/175341]\n",
      "Train Accuracy: 80.3760%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.596127, F1-score: 74.80%, Macro_F1-Score:  39.53%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.215220  [    0/175341]\n",
      "loss: 0.190540  [ 1600/175341]\n",
      "loss: 0.627921  [ 3200/175341]\n",
      "loss: 0.804721  [ 4800/175341]\n",
      "loss: 0.597742  [ 6400/175341]\n",
      "loss: 0.498116  [ 8000/175341]\n",
      "loss: 0.379832  [ 9600/175341]\n",
      "loss: 0.444535  [11200/175341]\n",
      "loss: 0.238194  [12800/175341]\n",
      "loss: 0.785031  [14400/175341]\n",
      "loss: 0.415350  [16000/175341]\n",
      "loss: 0.552155  [17600/175341]\n",
      "loss: 0.479131  [19200/175341]\n",
      "loss: 0.325685  [20800/175341]\n",
      "loss: 0.200056  [22400/175341]\n",
      "loss: 0.687197  [24000/175341]\n",
      "loss: 0.606658  [25600/175341]\n",
      "loss: 0.679998  [27200/175341]\n",
      "loss: 0.260478  [28800/175341]\n",
      "loss: 0.654866  [30400/175341]\n",
      "loss: 0.398321  [32000/175341]\n",
      "loss: 1.207051  [33600/175341]\n",
      "loss: 0.511099  [35200/175341]\n",
      "loss: 0.419158  [36800/175341]\n",
      "loss: 0.419733  [38400/175341]\n",
      "loss: 0.704629  [40000/175341]\n",
      "loss: 0.407271  [41600/175341]\n",
      "loss: 0.300941  [43200/175341]\n",
      "loss: 0.417328  [44800/175341]\n",
      "loss: 0.896190  [46400/175341]\n",
      "loss: 0.631373  [48000/175341]\n",
      "loss: 0.419647  [49600/175341]\n",
      "loss: 0.441845  [51200/175341]\n",
      "loss: 0.691584  [52800/175341]\n",
      "loss: 0.354162  [54400/175341]\n",
      "loss: 0.392482  [56000/175341]\n",
      "loss: 0.215021  [57600/175341]\n",
      "loss: 0.682015  [59200/175341]\n",
      "loss: 0.678190  [60800/175341]\n",
      "loss: 0.138610  [62400/175341]\n",
      "loss: 0.836008  [64000/175341]\n",
      "loss: 0.383197  [65600/175341]\n",
      "loss: 0.384492  [67200/175341]\n",
      "loss: 0.396948  [68800/175341]\n",
      "loss: 0.328367  [70400/175341]\n",
      "loss: 0.456652  [72000/175341]\n",
      "loss: 0.477539  [73600/175341]\n",
      "loss: 0.404721  [75200/175341]\n",
      "loss: 0.423618  [76800/175341]\n",
      "loss: 0.742857  [78400/175341]\n",
      "loss: 0.575756  [80000/175341]\n",
      "loss: 0.351969  [81600/175341]\n",
      "loss: 0.585595  [83200/175341]\n",
      "loss: 0.480482  [84800/175341]\n",
      "loss: 0.231650  [86400/175341]\n",
      "loss: 0.305660  [88000/175341]\n",
      "loss: 0.520200  [89600/175341]\n",
      "loss: 0.138666  [91200/175341]\n",
      "loss: 0.687981  [92800/175341]\n",
      "loss: 0.613271  [94400/175341]\n",
      "loss: 0.289888  [96000/175341]\n",
      "loss: 0.253778  [97600/175341]\n",
      "loss: 0.446687  [99200/175341]\n",
      "loss: 0.308885  [100800/175341]\n",
      "loss: 0.419256  [102400/175341]\n",
      "loss: 0.505460  [104000/175341]\n",
      "loss: 0.687615  [105600/175341]\n",
      "loss: 0.282961  [107200/175341]\n",
      "loss: 0.296131  [108800/175341]\n",
      "loss: 0.301006  [110400/175341]\n",
      "loss: 0.435298  [112000/175341]\n",
      "loss: 0.406465  [113600/175341]\n",
      "loss: 0.851768  [115200/175341]\n",
      "loss: 0.153416  [116800/175341]\n",
      "loss: 0.588073  [118400/175341]\n",
      "loss: 0.380382  [120000/175341]\n",
      "loss: 0.215627  [121600/175341]\n",
      "loss: 0.649736  [123200/175341]\n",
      "loss: 0.462450  [124800/175341]\n",
      "loss: 0.721479  [126400/175341]\n",
      "loss: 0.323602  [128000/175341]\n",
      "loss: 0.157458  [129600/175341]\n",
      "loss: 0.431371  [131200/175341]\n",
      "loss: 0.078214  [132800/175341]\n",
      "loss: 0.598373  [134400/175341]\n",
      "loss: 0.438275  [136000/175341]\n",
      "loss: 0.270306  [137600/175341]\n",
      "loss: 0.422543  [139200/175341]\n",
      "loss: 0.785267  [140800/175341]\n",
      "loss: 0.557011  [142400/175341]\n",
      "loss: 0.462451  [144000/175341]\n",
      "loss: 0.464338  [145600/175341]\n",
      "loss: 0.589831  [147200/175341]\n",
      "loss: 0.956156  [148800/175341]\n",
      "loss: 0.256812  [150400/175341]\n",
      "loss: 0.693570  [152000/175341]\n",
      "loss: 0.490975  [153600/175341]\n",
      "loss: 0.361533  [155200/175341]\n",
      "loss: 0.542711  [156800/175341]\n",
      "loss: 0.448627  [158400/175341]\n",
      "loss: 0.742498  [160000/175341]\n",
      "loss: 0.790589  [161600/175341]\n",
      "loss: 0.264685  [163200/175341]\n",
      "loss: 0.422342  [164800/175341]\n",
      "loss: 0.228966  [166400/175341]\n",
      "loss: 0.335655  [168000/175341]\n",
      "loss: 0.401113  [169600/175341]\n",
      "loss: 0.518362  [171200/175341]\n",
      "loss: 0.316296  [172800/175341]\n",
      "loss: 1.002015  [174400/175341]\n",
      "Train Accuracy: 80.3896%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.589282, F1-score: 75.13%, Macro_F1-Score:  39.18%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.423800  [    0/175341]\n",
      "loss: 0.858389  [ 1600/175341]\n",
      "loss: 0.340093  [ 3200/175341]\n",
      "loss: 0.361854  [ 4800/175341]\n",
      "loss: 0.275955  [ 6400/175341]\n",
      "loss: 0.451334  [ 8000/175341]\n",
      "loss: 0.640273  [ 9600/175341]\n",
      "loss: 0.922997  [11200/175341]\n",
      "loss: 0.498883  [12800/175341]\n",
      "loss: 0.399040  [14400/175341]\n",
      "loss: 0.348225  [16000/175341]\n",
      "loss: 0.888456  [17600/175341]\n",
      "loss: 0.516375  [19200/175341]\n",
      "loss: 0.231390  [20800/175341]\n",
      "loss: 0.382352  [22400/175341]\n",
      "loss: 0.371875  [24000/175341]\n",
      "loss: 0.479420  [25600/175341]\n",
      "loss: 0.319783  [27200/175341]\n",
      "loss: 0.308117  [28800/175341]\n",
      "loss: 0.705026  [30400/175341]\n",
      "loss: 0.760507  [32000/175341]\n",
      "loss: 0.479406  [33600/175341]\n",
      "loss: 0.570740  [35200/175341]\n",
      "loss: 0.829220  [36800/175341]\n",
      "loss: 0.980156  [38400/175341]\n",
      "loss: 0.390002  [40000/175341]\n",
      "loss: 0.418606  [41600/175341]\n",
      "loss: 0.782037  [43200/175341]\n",
      "loss: 1.204744  [44800/175341]\n",
      "loss: 0.558415  [46400/175341]\n",
      "loss: 0.417673  [48000/175341]\n",
      "loss: 0.360263  [49600/175341]\n",
      "loss: 0.404239  [51200/175341]\n",
      "loss: 0.615393  [52800/175341]\n",
      "loss: 0.552995  [54400/175341]\n",
      "loss: 0.649580  [56000/175341]\n",
      "loss: 0.556372  [57600/175341]\n",
      "loss: 0.201869  [59200/175341]\n",
      "loss: 0.205295  [60800/175341]\n",
      "loss: 0.365939  [62400/175341]\n",
      "loss: 0.155633  [64000/175341]\n",
      "loss: 0.505376  [65600/175341]\n",
      "loss: 0.629151  [67200/175341]\n",
      "loss: 0.377351  [68800/175341]\n",
      "loss: 0.594194  [70400/175341]\n",
      "loss: 0.460645  [72000/175341]\n",
      "loss: 0.438441  [73600/175341]\n",
      "loss: 0.359458  [75200/175341]\n",
      "loss: 0.462770  [76800/175341]\n",
      "loss: 0.582154  [78400/175341]\n",
      "loss: 0.418637  [80000/175341]\n",
      "loss: 0.596425  [81600/175341]\n",
      "loss: 0.311493  [83200/175341]\n",
      "loss: 0.379642  [84800/175341]\n",
      "loss: 0.760344  [86400/175341]\n",
      "loss: 0.736396  [88000/175341]\n",
      "loss: 0.414991  [89600/175341]\n",
      "loss: 0.608781  [91200/175341]\n",
      "loss: 0.645525  [92800/175341]\n",
      "loss: 0.428562  [94400/175341]\n",
      "loss: 0.248129  [96000/175341]\n",
      "loss: 0.241305  [97600/175341]\n",
      "loss: 0.324120  [99200/175341]\n",
      "loss: 0.385024  [100800/175341]\n",
      "loss: 0.570839  [102400/175341]\n",
      "loss: 0.501640  [104000/175341]\n",
      "loss: 0.589548  [105600/175341]\n",
      "loss: 0.384818  [107200/175341]\n",
      "loss: 0.500824  [108800/175341]\n",
      "loss: 0.762652  [110400/175341]\n",
      "loss: 0.601501  [112000/175341]\n",
      "loss: 0.426726  [113600/175341]\n",
      "loss: 0.335758  [115200/175341]\n",
      "loss: 0.188739  [116800/175341]\n",
      "loss: 0.282177  [118400/175341]\n",
      "loss: 0.383064  [120000/175341]\n",
      "loss: 0.412257  [121600/175341]\n",
      "loss: 0.763415  [123200/175341]\n",
      "loss: 0.550576  [124800/175341]\n",
      "loss: 0.610513  [126400/175341]\n",
      "loss: 0.406207  [128000/175341]\n",
      "loss: 0.274097  [129600/175341]\n",
      "loss: 0.577444  [131200/175341]\n",
      "loss: 0.453405  [132800/175341]\n",
      "loss: 0.695912  [134400/175341]\n",
      "loss: 0.641591  [136000/175341]\n",
      "loss: 0.847217  [137600/175341]\n",
      "loss: 0.462614  [139200/175341]\n",
      "loss: 0.590754  [140800/175341]\n",
      "loss: 0.226770  [142400/175341]\n",
      "loss: 0.629498  [144000/175341]\n",
      "loss: 0.772888  [145600/175341]\n",
      "loss: 0.761809  [147200/175341]\n",
      "loss: 0.289522  [148800/175341]\n",
      "loss: 0.276681  [150400/175341]\n",
      "loss: 0.285838  [152000/175341]\n",
      "loss: 0.432958  [153600/175341]\n",
      "loss: 0.398826  [155200/175341]\n",
      "loss: 0.462868  [156800/175341]\n",
      "loss: 1.011858  [158400/175341]\n",
      "loss: 0.436390  [160000/175341]\n",
      "loss: 0.298121  [161600/175341]\n",
      "loss: 0.255491  [163200/175341]\n",
      "loss: 0.375097  [164800/175341]\n",
      "loss: 0.320027  [166400/175341]\n",
      "loss: 0.210054  [168000/175341]\n",
      "loss: 0.608243  [169600/175341]\n",
      "loss: 0.552538  [171200/175341]\n",
      "loss: 0.894094  [172800/175341]\n",
      "loss: 0.888760  [174400/175341]\n",
      "Train Accuracy: 80.3896%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.589084, F1-score: 75.23%, Macro_F1-Score:  39.39%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.237407  [    0/175341]\n",
      "loss: 0.179870  [ 1600/175341]\n",
      "loss: 0.321544  [ 3200/175341]\n",
      "loss: 0.457882  [ 4800/175341]\n",
      "loss: 0.283125  [ 6400/175341]\n",
      "loss: 0.789266  [ 8000/175341]\n",
      "loss: 0.525726  [ 9600/175341]\n",
      "loss: 0.543012  [11200/175341]\n",
      "loss: 0.427584  [12800/175341]\n",
      "loss: 0.366278  [14400/175341]\n",
      "loss: 0.367523  [16000/175341]\n",
      "loss: 0.573700  [17600/175341]\n",
      "loss: 0.274686  [19200/175341]\n",
      "loss: 0.325684  [20800/175341]\n",
      "loss: 0.765224  [22400/175341]\n",
      "loss: 0.840123  [24000/175341]\n",
      "loss: 0.426743  [25600/175341]\n",
      "loss: 0.583543  [27200/175341]\n",
      "loss: 1.022972  [28800/175341]\n",
      "loss: 0.727396  [30400/175341]\n",
      "loss: 0.374787  [32000/175341]\n",
      "loss: 0.555035  [33600/175341]\n",
      "loss: 0.359242  [35200/175341]\n",
      "loss: 0.590781  [36800/175341]\n",
      "loss: 0.663904  [38400/175341]\n",
      "loss: 0.404589  [40000/175341]\n",
      "loss: 0.185613  [41600/175341]\n",
      "loss: 0.449254  [43200/175341]\n",
      "loss: 0.581862  [44800/175341]\n",
      "loss: 0.238550  [46400/175341]\n",
      "loss: 0.482929  [48000/175341]\n",
      "loss: 0.770779  [49600/175341]\n",
      "loss: 0.437172  [51200/175341]\n",
      "loss: 0.305386  [52800/175341]\n",
      "loss: 0.879938  [54400/175341]\n",
      "loss: 0.463573  [56000/175341]\n",
      "loss: 0.319697  [57600/175341]\n",
      "loss: 0.410634  [59200/175341]\n",
      "loss: 0.294621  [60800/175341]\n",
      "loss: 0.640075  [62400/175341]\n",
      "loss: 0.402389  [64000/175341]\n",
      "loss: 0.904709  [65600/175341]\n",
      "loss: 0.387569  [67200/175341]\n",
      "loss: 0.544219  [68800/175341]\n",
      "loss: 0.403151  [70400/175341]\n",
      "loss: 0.124554  [72000/175341]\n",
      "loss: 0.580524  [73600/175341]\n",
      "loss: 0.292182  [75200/175341]\n",
      "loss: 0.341650  [76800/175341]\n",
      "loss: 0.518362  [78400/175341]\n",
      "loss: 0.214650  [80000/175341]\n",
      "loss: 0.285186  [81600/175341]\n",
      "loss: 0.682911  [83200/175341]\n",
      "loss: 0.309113  [84800/175341]\n",
      "loss: 0.482150  [86400/175341]\n",
      "loss: 0.583779  [88000/175341]\n",
      "loss: 0.400557  [89600/175341]\n",
      "loss: 0.385700  [91200/175341]\n",
      "loss: 0.568544  [92800/175341]\n",
      "loss: 0.489907  [94400/175341]\n",
      "loss: 0.450463  [96000/175341]\n",
      "loss: 0.535301  [97600/175341]\n",
      "loss: 0.510633  [99200/175341]\n",
      "loss: 0.671688  [100800/175341]\n",
      "loss: 0.206652  [102400/175341]\n",
      "loss: 0.228830  [104000/175341]\n",
      "loss: 0.515670  [105600/175341]\n",
      "loss: 0.472729  [107200/175341]\n",
      "loss: 0.658156  [108800/175341]\n",
      "loss: 0.788309  [110400/175341]\n",
      "loss: 0.618828  [112000/175341]\n",
      "loss: 0.350988  [113600/175341]\n",
      "loss: 0.502463  [115200/175341]\n",
      "loss: 0.421429  [116800/175341]\n",
      "loss: 0.718880  [118400/175341]\n",
      "loss: 0.592640  [120000/175341]\n",
      "loss: 0.411518  [121600/175341]\n",
      "loss: 0.666199  [123200/175341]\n",
      "loss: 0.386721  [124800/175341]\n",
      "loss: 0.502221  [126400/175341]\n",
      "loss: 0.533593  [128000/175341]\n",
      "loss: 0.412717  [129600/175341]\n",
      "loss: 0.234526  [131200/175341]\n",
      "loss: 0.169555  [132800/175341]\n",
      "loss: 0.412419  [134400/175341]\n",
      "loss: 0.296507  [136000/175341]\n",
      "loss: 0.627259  [137600/175341]\n",
      "loss: 0.332966  [139200/175341]\n",
      "loss: 0.772229  [140800/175341]\n",
      "loss: 0.866126  [142400/175341]\n",
      "loss: 0.564034  [144000/175341]\n",
      "loss: 0.371935  [145600/175341]\n",
      "loss: 0.479453  [147200/175341]\n",
      "loss: 0.632974  [148800/175341]\n",
      "loss: 0.405456  [150400/175341]\n",
      "loss: 0.269725  [152000/175341]\n",
      "loss: 0.425878  [153600/175341]\n",
      "loss: 0.369587  [155200/175341]\n",
      "loss: 0.411001  [156800/175341]\n",
      "loss: 0.580826  [158400/175341]\n",
      "loss: 0.562118  [160000/175341]\n",
      "loss: 0.189132  [161600/175341]\n",
      "loss: 0.880682  [163200/175341]\n",
      "loss: 0.475045  [164800/175341]\n",
      "loss: 0.425731  [166400/175341]\n",
      "loss: 0.428387  [168000/175341]\n",
      "loss: 0.451370  [169600/175341]\n",
      "loss: 0.797195  [171200/175341]\n",
      "loss: 0.551301  [172800/175341]\n",
      "loss: 0.490359  [174400/175341]\n",
      "Train Accuracy: 80.3765%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.573247, F1-score: 76.38%, Macro_F1-Score:  39.68%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.352472  [    0/175341]\n",
      "loss: 0.675490  [ 1600/175341]\n",
      "loss: 0.668187  [ 3200/175341]\n",
      "loss: 0.376686  [ 4800/175341]\n",
      "loss: 0.314895  [ 6400/175341]\n",
      "loss: 0.660560  [ 8000/175341]\n",
      "loss: 0.377800  [ 9600/175341]\n",
      "loss: 0.197933  [11200/175341]\n",
      "loss: 0.320766  [12800/175341]\n",
      "loss: 0.394871  [14400/175341]\n",
      "loss: 0.201780  [16000/175341]\n",
      "loss: 0.254478  [17600/175341]\n",
      "loss: 0.405080  [19200/175341]\n",
      "loss: 0.837210  [20800/175341]\n",
      "loss: 0.528022  [22400/175341]\n",
      "loss: 0.692087  [24000/175341]\n",
      "loss: 0.312752  [25600/175341]\n",
      "loss: 0.544946  [27200/175341]\n",
      "loss: 0.481270  [28800/175341]\n",
      "loss: 0.260034  [30400/175341]\n",
      "loss: 0.366988  [32000/175341]\n",
      "loss: 0.485106  [33600/175341]\n",
      "loss: 0.800989  [35200/175341]\n",
      "loss: 0.590314  [36800/175341]\n",
      "loss: 0.543231  [38400/175341]\n",
      "loss: 0.574745  [40000/175341]\n",
      "loss: 0.653545  [41600/175341]\n",
      "loss: 0.401275  [43200/175341]\n",
      "loss: 0.586764  [44800/175341]\n",
      "loss: 0.537309  [46400/175341]\n",
      "loss: 0.203038  [48000/175341]\n",
      "loss: 0.284862  [49600/175341]\n",
      "loss: 0.673377  [51200/175341]\n",
      "loss: 0.603799  [52800/175341]\n",
      "loss: 0.285327  [54400/175341]\n",
      "loss: 0.654959  [56000/175341]\n",
      "loss: 0.117229  [57600/175341]\n",
      "loss: 0.169438  [59200/175341]\n",
      "loss: 0.444256  [60800/175341]\n",
      "loss: 0.209028  [62400/175341]\n",
      "loss: 0.524362  [64000/175341]\n",
      "loss: 0.449046  [65600/175341]\n",
      "loss: 0.290672  [67200/175341]\n",
      "loss: 0.592294  [68800/175341]\n",
      "loss: 0.223054  [70400/175341]\n",
      "loss: 0.194666  [72000/175341]\n",
      "loss: 0.559523  [73600/175341]\n",
      "loss: 0.468644  [75200/175341]\n",
      "loss: 0.735614  [76800/175341]\n",
      "loss: 0.471255  [78400/175341]\n",
      "loss: 0.394877  [80000/175341]\n",
      "loss: 0.847204  [81600/175341]\n",
      "loss: 0.545750  [83200/175341]\n",
      "loss: 0.244554  [84800/175341]\n",
      "loss: 0.220078  [86400/175341]\n",
      "loss: 0.395261  [88000/175341]\n",
      "loss: 0.445271  [89600/175341]\n",
      "loss: 0.345204  [91200/175341]\n",
      "loss: 0.312582  [92800/175341]\n",
      "loss: 0.545129  [94400/175341]\n",
      "loss: 0.929505  [96000/175341]\n",
      "loss: 0.480640  [97600/175341]\n",
      "loss: 0.187322  [99200/175341]\n",
      "loss: 0.168308  [100800/175341]\n",
      "loss: 0.278602  [102400/175341]\n",
      "loss: 0.835962  [104000/175341]\n",
      "loss: 0.323226  [105600/175341]\n",
      "loss: 0.372092  [107200/175341]\n",
      "loss: 0.239139  [108800/175341]\n",
      "loss: 0.835832  [110400/175341]\n",
      "loss: 0.468823  [112000/175341]\n",
      "loss: 0.614435  [113600/175341]\n",
      "loss: 0.310150  [115200/175341]\n",
      "loss: 0.445073  [116800/175341]\n",
      "loss: 0.553060  [118400/175341]\n",
      "loss: 0.546661  [120000/175341]\n",
      "loss: 0.410715  [121600/175341]\n",
      "loss: 0.612167  [123200/175341]\n",
      "loss: 1.086383  [124800/175341]\n",
      "loss: 0.301792  [126400/175341]\n",
      "loss: 0.357520  [128000/175341]\n",
      "loss: 0.314299  [129600/175341]\n",
      "loss: 0.424234  [131200/175341]\n",
      "loss: 0.359817  [132800/175341]\n",
      "loss: 0.341248  [134400/175341]\n",
      "loss: 0.572083  [136000/175341]\n",
      "loss: 0.524626  [137600/175341]\n",
      "loss: 0.264363  [139200/175341]\n",
      "loss: 0.394575  [140800/175341]\n",
      "loss: 0.427440  [142400/175341]\n",
      "loss: 0.474938  [144000/175341]\n",
      "loss: 0.290969  [145600/175341]\n",
      "loss: 0.369923  [147200/175341]\n",
      "loss: 0.503044  [148800/175341]\n",
      "loss: 0.501913  [150400/175341]\n",
      "loss: 0.551101  [152000/175341]\n",
      "loss: 0.730020  [153600/175341]\n",
      "loss: 0.383887  [155200/175341]\n",
      "loss: 0.298112  [156800/175341]\n",
      "loss: 0.779625  [158400/175341]\n",
      "loss: 0.337560  [160000/175341]\n",
      "loss: 0.352953  [161600/175341]\n",
      "loss: 0.769093  [163200/175341]\n",
      "loss: 0.184316  [164800/175341]\n",
      "loss: 0.887930  [166400/175341]\n",
      "loss: 0.655555  [168000/175341]\n",
      "loss: 0.718928  [169600/175341]\n",
      "loss: 0.454456  [171200/175341]\n",
      "loss: 0.482856  [172800/175341]\n",
      "loss: 0.080832  [174400/175341]\n",
      "Train Accuracy: 80.4085%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.600115, F1-score: 74.89%, Macro_F1-Score:  38.86%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.383017  [    0/175341]\n",
      "loss: 0.863644  [ 1600/175341]\n",
      "loss: 0.151911  [ 3200/175341]\n",
      "loss: 0.163957  [ 4800/175341]\n",
      "loss: 0.341183  [ 6400/175341]\n",
      "loss: 0.545136  [ 8000/175341]\n",
      "loss: 0.305167  [ 9600/175341]\n",
      "loss: 0.390525  [11200/175341]\n",
      "loss: 0.189380  [12800/175341]\n",
      "loss: 0.757279  [14400/175341]\n",
      "loss: 0.648354  [16000/175341]\n",
      "loss: 0.549094  [17600/175341]\n",
      "loss: 0.411412  [19200/175341]\n",
      "loss: 0.921189  [20800/175341]\n",
      "loss: 0.691943  [22400/175341]\n",
      "loss: 0.469614  [24000/175341]\n",
      "loss: 0.440649  [25600/175341]\n",
      "loss: 0.771383  [27200/175341]\n",
      "loss: 0.393841  [28800/175341]\n",
      "loss: 0.769180  [30400/175341]\n",
      "loss: 0.255598  [32000/175341]\n",
      "loss: 0.750166  [33600/175341]\n",
      "loss: 0.345459  [35200/175341]\n",
      "loss: 0.436855  [36800/175341]\n",
      "loss: 0.309449  [38400/175341]\n",
      "loss: 0.689469  [40000/175341]\n",
      "loss: 0.684779  [41600/175341]\n",
      "loss: 0.670880  [43200/175341]\n",
      "loss: 0.212634  [44800/175341]\n",
      "loss: 0.412065  [46400/175341]\n",
      "loss: 0.342776  [48000/175341]\n",
      "loss: 0.301003  [49600/175341]\n",
      "loss: 0.432117  [51200/175341]\n",
      "loss: 0.509890  [52800/175341]\n",
      "loss: 0.657631  [54400/175341]\n",
      "loss: 0.489071  [56000/175341]\n",
      "loss: 0.375936  [57600/175341]\n",
      "loss: 0.302019  [59200/175341]\n",
      "loss: 0.765085  [60800/175341]\n",
      "loss: 0.320709  [62400/175341]\n",
      "loss: 0.571038  [64000/175341]\n",
      "loss: 0.483467  [65600/175341]\n",
      "loss: 0.368067  [67200/175341]\n",
      "loss: 0.345273  [68800/175341]\n",
      "loss: 0.433834  [70400/175341]\n",
      "loss: 0.304019  [72000/175341]\n",
      "loss: 0.813740  [73600/175341]\n",
      "loss: 0.314302  [75200/175341]\n",
      "loss: 0.260819  [76800/175341]\n",
      "loss: 0.598277  [78400/175341]\n",
      "loss: 0.374284  [80000/175341]\n",
      "loss: 0.195153  [81600/175341]\n",
      "loss: 0.352706  [83200/175341]\n",
      "loss: 0.285833  [84800/175341]\n",
      "loss: 0.133641  [86400/175341]\n",
      "loss: 0.640500  [88000/175341]\n",
      "loss: 0.580669  [89600/175341]\n",
      "loss: 0.364758  [91200/175341]\n",
      "loss: 0.450907  [92800/175341]\n",
      "loss: 0.335530  [94400/175341]\n",
      "loss: 0.863675  [96000/175341]\n",
      "loss: 0.522630  [97600/175341]\n",
      "loss: 0.342266  [99200/175341]\n",
      "loss: 0.254408  [100800/175341]\n",
      "loss: 0.445969  [102400/175341]\n",
      "loss: 0.290349  [104000/175341]\n",
      "loss: 0.252943  [105600/175341]\n",
      "loss: 0.494036  [107200/175341]\n",
      "loss: 0.559721  [108800/175341]\n",
      "loss: 0.610802  [110400/175341]\n",
      "loss: 0.444816  [112000/175341]\n",
      "loss: 0.614131  [113600/175341]\n",
      "loss: 0.540330  [115200/175341]\n",
      "loss: 0.689992  [116800/175341]\n",
      "loss: 0.119415  [118400/175341]\n",
      "loss: 0.402610  [120000/175341]\n",
      "loss: 0.351942  [121600/175341]\n",
      "loss: 0.442100  [123200/175341]\n",
      "loss: 0.596739  [124800/175341]\n",
      "loss: 0.805036  [126400/175341]\n",
      "loss: 0.344407  [128000/175341]\n",
      "loss: 0.346087  [129600/175341]\n",
      "loss: 0.515294  [131200/175341]\n",
      "loss: 0.627236  [132800/175341]\n",
      "loss: 0.374678  [134400/175341]\n",
      "loss: 0.302513  [136000/175341]\n",
      "loss: 0.848723  [137600/175341]\n",
      "loss: 0.436950  [139200/175341]\n",
      "loss: 0.531372  [140800/175341]\n",
      "loss: 0.342793  [142400/175341]\n",
      "loss: 0.786940  [144000/175341]\n",
      "loss: 0.280794  [145600/175341]\n",
      "loss: 0.742385  [147200/175341]\n",
      "loss: 0.210118  [148800/175341]\n",
      "loss: 0.446460  [150400/175341]\n",
      "loss: 0.660058  [152000/175341]\n",
      "loss: 0.618216  [153600/175341]\n",
      "loss: 0.910224  [155200/175341]\n",
      "loss: 0.397805  [156800/175341]\n",
      "loss: 0.359235  [158400/175341]\n",
      "loss: 0.438750  [160000/175341]\n",
      "loss: 0.921274  [161600/175341]\n",
      "loss: 0.724668  [163200/175341]\n",
      "loss: 0.313517  [164800/175341]\n",
      "loss: 0.529540  [166400/175341]\n",
      "loss: 0.426113  [168000/175341]\n",
      "loss: 0.561304  [169600/175341]\n",
      "loss: 0.485914  [171200/175341]\n",
      "loss: 0.320677  [172800/175341]\n",
      "loss: 0.615846  [174400/175341]\n",
      "Train Accuracy: 80.4712%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.578142, F1-score: 75.72%, Macro_F1-Score:  39.94%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.323386  [    0/175341]\n",
      "loss: 0.325697  [ 1600/175341]\n",
      "loss: 0.402453  [ 3200/175341]\n",
      "loss: 0.365178  [ 4800/175341]\n",
      "loss: 0.436750  [ 6400/175341]\n",
      "loss: 0.254863  [ 8000/175341]\n",
      "loss: 0.804979  [ 9600/175341]\n",
      "loss: 0.766924  [11200/175341]\n",
      "loss: 0.241080  [12800/175341]\n",
      "loss: 0.422377  [14400/175341]\n",
      "loss: 0.452080  [16000/175341]\n",
      "loss: 0.511531  [17600/175341]\n",
      "loss: 0.429091  [19200/175341]\n",
      "loss: 0.504332  [20800/175341]\n",
      "loss: 0.189330  [22400/175341]\n",
      "loss: 0.351302  [24000/175341]\n",
      "loss: 0.457576  [25600/175341]\n",
      "loss: 0.582555  [27200/175341]\n",
      "loss: 0.731862  [28800/175341]\n",
      "loss: 0.463823  [30400/175341]\n",
      "loss: 0.798359  [32000/175341]\n",
      "loss: 0.424306  [33600/175341]\n",
      "loss: 0.512565  [35200/175341]\n",
      "loss: 0.447317  [36800/175341]\n",
      "loss: 0.375529  [38400/175341]\n",
      "loss: 0.808209  [40000/175341]\n",
      "loss: 0.351766  [41600/175341]\n",
      "loss: 0.628827  [43200/175341]\n",
      "loss: 0.213202  [44800/175341]\n",
      "loss: 0.477045  [46400/175341]\n",
      "loss: 0.572454  [48000/175341]\n",
      "loss: 0.758628  [49600/175341]\n",
      "loss: 0.595521  [51200/175341]\n",
      "loss: 0.144889  [52800/175341]\n",
      "loss: 0.345831  [54400/175341]\n",
      "loss: 0.101744  [56000/175341]\n",
      "loss: 0.669330  [57600/175341]\n",
      "loss: 0.343193  [59200/175341]\n",
      "loss: 0.343552  [60800/175341]\n",
      "loss: 0.651538  [62400/175341]\n",
      "loss: 0.625637  [64000/175341]\n",
      "loss: 0.071720  [65600/175341]\n",
      "loss: 0.810971  [67200/175341]\n",
      "loss: 0.150306  [68800/175341]\n",
      "loss: 0.352771  [70400/175341]\n",
      "loss: 0.645048  [72000/175341]\n",
      "loss: 0.213343  [73600/175341]\n",
      "loss: 0.352756  [75200/175341]\n",
      "loss: 0.629413  [76800/175341]\n",
      "loss: 0.551557  [78400/175341]\n",
      "loss: 0.532435  [80000/175341]\n",
      "loss: 0.626710  [81600/175341]\n",
      "loss: 0.147431  [83200/175341]\n",
      "loss: 0.368592  [84800/175341]\n",
      "loss: 0.539737  [86400/175341]\n",
      "loss: 0.368596  [88000/175341]\n",
      "loss: 1.508588  [89600/175341]\n",
      "loss: 0.627457  [91200/175341]\n",
      "loss: 0.318327  [92800/175341]\n",
      "loss: 0.729106  [94400/175341]\n",
      "loss: 0.268421  [96000/175341]\n",
      "loss: 0.506614  [97600/175341]\n",
      "loss: 0.567052  [99200/175341]\n",
      "loss: 0.437483  [100800/175341]\n",
      "loss: 0.274483  [102400/175341]\n",
      "loss: 0.557480  [104000/175341]\n",
      "loss: 0.594270  [105600/175341]\n",
      "loss: 0.756045  [107200/175341]\n",
      "loss: 1.111161  [108800/175341]\n",
      "loss: 0.375647  [110400/175341]\n",
      "loss: 0.456212  [112000/175341]\n",
      "loss: 0.556703  [113600/175341]\n",
      "loss: 0.324314  [115200/175341]\n",
      "loss: 0.129431  [116800/175341]\n",
      "loss: 0.903676  [118400/175341]\n",
      "loss: 0.265442  [120000/175341]\n",
      "loss: 0.375299  [121600/175341]\n",
      "loss: 0.639074  [123200/175341]\n",
      "loss: 0.645827  [124800/175341]\n",
      "loss: 0.548273  [126400/175341]\n",
      "loss: 0.611462  [128000/175341]\n",
      "loss: 1.034669  [129600/175341]\n",
      "loss: 0.507550  [131200/175341]\n",
      "loss: 0.437489  [132800/175341]\n",
      "loss: 1.002759  [134400/175341]\n",
      "loss: 0.545431  [136000/175341]\n",
      "loss: 0.833329  [137600/175341]\n",
      "loss: 0.400505  [139200/175341]\n",
      "loss: 0.502785  [140800/175341]\n",
      "loss: 0.388104  [142400/175341]\n",
      "loss: 0.411016  [144000/175341]\n",
      "loss: 0.627332  [145600/175341]\n",
      "loss: 0.785545  [147200/175341]\n",
      "loss: 1.356125  [148800/175341]\n",
      "loss: 0.667929  [150400/175341]\n",
      "loss: 0.257253  [152000/175341]\n",
      "loss: 0.285664  [153600/175341]\n",
      "loss: 0.895992  [155200/175341]\n",
      "loss: 0.397729  [156800/175341]\n",
      "loss: 0.234149  [158400/175341]\n",
      "loss: 0.273740  [160000/175341]\n",
      "loss: 0.482869  [161600/175341]\n",
      "loss: 0.305897  [163200/175341]\n",
      "loss: 0.519800  [164800/175341]\n",
      "loss: 0.637208  [166400/175341]\n",
      "loss: 0.209945  [168000/175341]\n",
      "loss: 0.506482  [169600/175341]\n",
      "loss: 0.412577  [171200/175341]\n",
      "loss: 0.426141  [172800/175341]\n",
      "loss: 0.566608  [174400/175341]\n",
      "Train Accuracy: 80.4547%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.594306, F1-score: 75.10%, Macro_F1-Score:  39.35%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.309786  [    0/175341]\n",
      "loss: 0.326533  [ 1600/175341]\n",
      "loss: 0.685630  [ 3200/175341]\n",
      "loss: 0.247721  [ 4800/175341]\n",
      "loss: 0.381282  [ 6400/175341]\n",
      "loss: 0.331958  [ 8000/175341]\n",
      "loss: 0.149875  [ 9600/175341]\n",
      "loss: 0.218693  [11200/175341]\n",
      "loss: 0.293509  [12800/175341]\n",
      "loss: 0.890289  [14400/175341]\n",
      "loss: 0.221921  [16000/175341]\n",
      "loss: 0.418274  [17600/175341]\n",
      "loss: 0.633426  [19200/175341]\n",
      "loss: 0.800784  [20800/175341]\n",
      "loss: 0.304390  [22400/175341]\n",
      "loss: 0.311828  [24000/175341]\n",
      "loss: 0.290143  [25600/175341]\n",
      "loss: 0.514053  [27200/175341]\n",
      "loss: 0.518406  [28800/175341]\n",
      "loss: 0.732777  [30400/175341]\n",
      "loss: 0.532428  [32000/175341]\n",
      "loss: 0.236012  [33600/175341]\n",
      "loss: 0.266102  [35200/175341]\n",
      "loss: 0.259091  [36800/175341]\n",
      "loss: 0.662540  [38400/175341]\n",
      "loss: 0.160059  [40000/175341]\n",
      "loss: 0.677246  [41600/175341]\n",
      "loss: 0.748965  [43200/175341]\n",
      "loss: 0.380762  [44800/175341]\n",
      "loss: 0.339512  [46400/175341]\n",
      "loss: 0.344528  [48000/175341]\n",
      "loss: 0.163760  [49600/175341]\n",
      "loss: 0.308547  [51200/175341]\n",
      "loss: 0.280045  [52800/175341]\n",
      "loss: 0.498301  [54400/175341]\n",
      "loss: 0.806547  [56000/175341]\n",
      "loss: 0.346457  [57600/175341]\n",
      "loss: 0.572559  [59200/175341]\n",
      "loss: 0.636532  [60800/175341]\n",
      "loss: 0.671220  [62400/175341]\n",
      "loss: 0.326989  [64000/175341]\n",
      "loss: 0.415389  [65600/175341]\n",
      "loss: 0.782090  [67200/175341]\n",
      "loss: 0.707235  [68800/175341]\n",
      "loss: 0.482827  [70400/175341]\n",
      "loss: 0.566799  [72000/175341]\n",
      "loss: 0.238090  [73600/175341]\n",
      "loss: 0.815576  [75200/175341]\n",
      "loss: 0.713014  [76800/175341]\n",
      "loss: 0.744667  [78400/175341]\n",
      "loss: 0.215811  [80000/175341]\n",
      "loss: 0.351388  [81600/175341]\n",
      "loss: 0.432561  [83200/175341]\n",
      "loss: 0.360397  [84800/175341]\n",
      "loss: 0.300966  [86400/175341]\n",
      "loss: 0.100173  [88000/175341]\n",
      "loss: 1.091786  [89600/175341]\n",
      "loss: 0.750380  [91200/175341]\n",
      "loss: 0.645466  [92800/175341]\n",
      "loss: 0.594756  [94400/175341]\n",
      "loss: 0.170558  [96000/175341]\n",
      "loss: 0.585140  [97600/175341]\n",
      "loss: 0.579266  [99200/175341]\n",
      "loss: 0.423210  [100800/175341]\n",
      "loss: 0.688135  [102400/175341]\n",
      "loss: 0.298585  [104000/175341]\n",
      "loss: 0.387917  [105600/175341]\n",
      "loss: 0.487327  [107200/175341]\n",
      "loss: 0.571102  [108800/175341]\n",
      "loss: 0.410742  [110400/175341]\n",
      "loss: 0.613413  [112000/175341]\n",
      "loss: 0.527101  [113600/175341]\n",
      "loss: 0.122236  [115200/175341]\n",
      "loss: 0.377839  [116800/175341]\n",
      "loss: 0.356684  [118400/175341]\n",
      "loss: 0.633459  [120000/175341]\n",
      "loss: 0.530004  [121600/175341]\n",
      "loss: 0.703289  [123200/175341]\n",
      "loss: 0.714929  [124800/175341]\n",
      "loss: 0.487853  [126400/175341]\n",
      "loss: 0.578991  [128000/175341]\n",
      "loss: 0.587888  [129600/175341]\n",
      "loss: 0.744677  [131200/175341]\n",
      "loss: 0.213828  [132800/175341]\n",
      "loss: 0.261005  [134400/175341]\n",
      "loss: 0.405225  [136000/175341]\n",
      "loss: 0.435962  [137600/175341]\n",
      "loss: 0.306388  [139200/175341]\n",
      "loss: 0.883322  [140800/175341]\n",
      "loss: 0.583333  [142400/175341]\n",
      "loss: 0.498600  [144000/175341]\n",
      "loss: 0.551820  [145600/175341]\n",
      "loss: 0.328421  [147200/175341]\n",
      "loss: 0.256233  [148800/175341]\n",
      "loss: 0.288524  [150400/175341]\n",
      "loss: 0.678156  [152000/175341]\n",
      "loss: 0.435038  [153600/175341]\n",
      "loss: 0.319633  [155200/175341]\n",
      "loss: 0.648514  [156800/175341]\n",
      "loss: 0.364326  [158400/175341]\n",
      "loss: 0.198315  [160000/175341]\n",
      "loss: 0.508792  [161600/175341]\n",
      "loss: 0.383182  [163200/175341]\n",
      "loss: 0.664206  [164800/175341]\n",
      "loss: 0.370663  [166400/175341]\n",
      "loss: 0.812302  [168000/175341]\n",
      "loss: 0.451288  [169600/175341]\n",
      "loss: 0.654587  [171200/175341]\n",
      "loss: 0.817796  [172800/175341]\n",
      "loss: 0.227883  [174400/175341]\n",
      "Train Accuracy: 80.4632%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.582090, F1-score: 75.03%, Macro_F1-Score:  39.66%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.336281  [    0/175341]\n",
      "loss: 0.559916  [ 1600/175341]\n",
      "loss: 0.083190  [ 3200/175341]\n",
      "loss: 0.576025  [ 4800/175341]\n",
      "loss: 0.550857  [ 6400/175341]\n",
      "loss: 0.576720  [ 8000/175341]\n",
      "loss: 0.813167  [ 9600/175341]\n",
      "loss: 0.609886  [11200/175341]\n",
      "loss: 0.798021  [12800/175341]\n",
      "loss: 0.211398  [14400/175341]\n",
      "loss: 0.097029  [16000/175341]\n",
      "loss: 0.263577  [17600/175341]\n",
      "loss: 0.612361  [19200/175341]\n",
      "loss: 0.595507  [20800/175341]\n",
      "loss: 0.314156  [22400/175341]\n",
      "loss: 0.271634  [24000/175341]\n",
      "loss: 0.634434  [25600/175341]\n",
      "loss: 0.673396  [27200/175341]\n",
      "loss: 0.282268  [28800/175341]\n",
      "loss: 0.711634  [30400/175341]\n",
      "loss: 0.407908  [32000/175341]\n",
      "loss: 0.463037  [33600/175341]\n",
      "loss: 0.302526  [35200/175341]\n",
      "loss: 0.255108  [36800/175341]\n",
      "loss: 0.448867  [38400/175341]\n",
      "loss: 0.843637  [40000/175341]\n",
      "loss: 0.338957  [41600/175341]\n",
      "loss: 0.826212  [43200/175341]\n",
      "loss: 0.502911  [44800/175341]\n",
      "loss: 0.402346  [46400/175341]\n",
      "loss: 0.361394  [48000/175341]\n",
      "loss: 0.437518  [49600/175341]\n",
      "loss: 0.697483  [51200/175341]\n",
      "loss: 0.521548  [52800/175341]\n",
      "loss: 0.566653  [54400/175341]\n",
      "loss: 0.558792  [56000/175341]\n",
      "loss: 0.307939  [57600/175341]\n",
      "loss: 0.213589  [59200/175341]\n",
      "loss: 0.228539  [60800/175341]\n",
      "loss: 0.626647  [62400/175341]\n",
      "loss: 0.797270  [64000/175341]\n",
      "loss: 0.624486  [65600/175341]\n",
      "loss: 0.280452  [67200/175341]\n",
      "loss: 0.285277  [68800/175341]\n",
      "loss: 0.613131  [70400/175341]\n",
      "loss: 0.843800  [72000/175341]\n",
      "loss: 0.485718  [73600/175341]\n",
      "loss: 0.557733  [75200/175341]\n",
      "loss: 0.171163  [76800/175341]\n",
      "loss: 0.761449  [78400/175341]\n",
      "loss: 0.540730  [80000/175341]\n",
      "loss: 0.375630  [81600/175341]\n",
      "loss: 0.481714  [83200/175341]\n",
      "loss: 0.759792  [84800/175341]\n",
      "loss: 0.601787  [86400/175341]\n",
      "loss: 0.559970  [88000/175341]\n",
      "loss: 0.182028  [89600/175341]\n",
      "loss: 0.647699  [91200/175341]\n",
      "loss: 0.299207  [92800/175341]\n",
      "loss: 0.417441  [94400/175341]\n",
      "loss: 0.306085  [96000/175341]\n",
      "loss: 0.673456  [97600/175341]\n",
      "loss: 0.801674  [99200/175341]\n",
      "loss: 0.601868  [100800/175341]\n",
      "loss: 0.837582  [102400/175341]\n",
      "loss: 0.662955  [104000/175341]\n",
      "loss: 0.326226  [105600/175341]\n",
      "loss: 0.453218  [107200/175341]\n",
      "loss: 0.764168  [108800/175341]\n",
      "loss: 0.436904  [110400/175341]\n",
      "loss: 0.542637  [112000/175341]\n",
      "loss: 0.299581  [113600/175341]\n",
      "loss: 0.872617  [115200/175341]\n",
      "loss: 0.452307  [116800/175341]\n",
      "loss: 0.363591  [118400/175341]\n",
      "loss: 0.567101  [120000/175341]\n",
      "loss: 0.231441  [121600/175341]\n",
      "loss: 0.417066  [123200/175341]\n",
      "loss: 0.398621  [124800/175341]\n",
      "loss: 0.136751  [126400/175341]\n",
      "loss: 0.529052  [128000/175341]\n",
      "loss: 0.306902  [129600/175341]\n",
      "loss: 0.474232  [131200/175341]\n",
      "loss: 0.582438  [132800/175341]\n",
      "loss: 0.688457  [134400/175341]\n",
      "loss: 0.639120  [136000/175341]\n",
      "loss: 0.392603  [137600/175341]\n",
      "loss: 1.087940  [139200/175341]\n",
      "loss: 0.505773  [140800/175341]\n",
      "loss: 0.512684  [142400/175341]\n",
      "loss: 0.607975  [144000/175341]\n",
      "loss: 0.405355  [145600/175341]\n",
      "loss: 0.182691  [147200/175341]\n",
      "loss: 0.413181  [148800/175341]\n",
      "loss: 0.391890  [150400/175341]\n",
      "loss: 0.539094  [152000/175341]\n",
      "loss: 0.468134  [153600/175341]\n",
      "loss: 0.439851  [155200/175341]\n",
      "loss: 0.986778  [156800/175341]\n",
      "loss: 0.208509  [158400/175341]\n",
      "loss: 0.362528  [160000/175341]\n",
      "loss: 0.288554  [161600/175341]\n",
      "loss: 0.570099  [163200/175341]\n",
      "loss: 0.660195  [164800/175341]\n",
      "loss: 0.286681  [166400/175341]\n",
      "loss: 0.328610  [168000/175341]\n",
      "loss: 0.736431  [169600/175341]\n",
      "loss: 0.554091  [171200/175341]\n",
      "loss: 0.847274  [172800/175341]\n",
      "loss: 0.377735  [174400/175341]\n",
      "Train Accuracy: 80.4872%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.581097, F1-score: 75.54%, Macro_F1-Score:  39.53%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.548984  [    0/175341]\n",
      "loss: 0.133077  [ 1600/175341]\n",
      "loss: 0.738077  [ 3200/175341]\n",
      "loss: 0.283377  [ 4800/175341]\n",
      "loss: 0.497601  [ 6400/175341]\n",
      "loss: 0.280714  [ 8000/175341]\n",
      "loss: 0.360039  [ 9600/175341]\n",
      "loss: 0.376822  [11200/175341]\n",
      "loss: 0.600173  [12800/175341]\n",
      "loss: 0.891482  [14400/175341]\n",
      "loss: 0.588875  [16000/175341]\n",
      "loss: 0.202328  [17600/175341]\n",
      "loss: 0.600646  [19200/175341]\n",
      "loss: 0.485131  [20800/175341]\n",
      "loss: 0.576359  [22400/175341]\n",
      "loss: 0.310288  [24000/175341]\n",
      "loss: 0.202682  [25600/175341]\n",
      "loss: 0.707741  [27200/175341]\n",
      "loss: 0.488588  [28800/175341]\n",
      "loss: 0.564952  [30400/175341]\n",
      "loss: 0.851466  [32000/175341]\n",
      "loss: 0.620468  [33600/175341]\n",
      "loss: 0.435232  [35200/175341]\n",
      "loss: 0.256324  [36800/175341]\n",
      "loss: 0.724850  [38400/175341]\n",
      "loss: 0.393823  [40000/175341]\n",
      "loss: 0.291336  [41600/175341]\n",
      "loss: 0.401437  [43200/175341]\n",
      "loss: 0.898040  [44800/175341]\n",
      "loss: 0.421658  [46400/175341]\n",
      "loss: 0.400663  [48000/175341]\n",
      "loss: 0.414433  [49600/175341]\n",
      "loss: 0.315204  [51200/175341]\n",
      "loss: 0.433697  [52800/175341]\n",
      "loss: 0.319500  [54400/175341]\n",
      "loss: 0.522611  [56000/175341]\n",
      "loss: 0.816988  [57600/175341]\n",
      "loss: 0.693521  [59200/175341]\n",
      "loss: 0.527188  [60800/175341]\n",
      "loss: 0.404041  [62400/175341]\n",
      "loss: 0.709525  [64000/175341]\n",
      "loss: 0.726740  [65600/175341]\n",
      "loss: 0.329464  [67200/175341]\n",
      "loss: 0.285129  [68800/175341]\n",
      "loss: 0.273032  [70400/175341]\n",
      "loss: 0.912602  [72000/175341]\n",
      "loss: 0.792562  [73600/175341]\n",
      "loss: 0.551553  [75200/175341]\n",
      "loss: 0.440123  [76800/175341]\n",
      "loss: 0.638376  [78400/175341]\n",
      "loss: 0.449607  [80000/175341]\n",
      "loss: 0.978526  [81600/175341]\n",
      "loss: 0.554746  [83200/175341]\n",
      "loss: 0.906920  [84800/175341]\n",
      "loss: 0.364439  [86400/175341]\n",
      "loss: 0.443689  [88000/175341]\n",
      "loss: 0.263682  [89600/175341]\n",
      "loss: 0.806029  [91200/175341]\n",
      "loss: 0.372604  [92800/175341]\n",
      "loss: 0.730901  [94400/175341]\n",
      "loss: 0.428409  [96000/175341]\n",
      "loss: 0.542921  [97600/175341]\n",
      "loss: 0.425323  [99200/175341]\n",
      "loss: 0.498715  [100800/175341]\n",
      "loss: 0.327565  [102400/175341]\n",
      "loss: 0.339056  [104000/175341]\n",
      "loss: 0.812385  [105600/175341]\n",
      "loss: 0.724956  [107200/175341]\n",
      "loss: 0.862613  [108800/175341]\n",
      "loss: 0.480819  [110400/175341]\n",
      "loss: 0.603170  [112000/175341]\n",
      "loss: 0.473049  [113600/175341]\n",
      "loss: 0.719069  [115200/175341]\n",
      "loss: 0.366156  [116800/175341]\n",
      "loss: 0.798218  [118400/175341]\n",
      "loss: 0.782787  [120000/175341]\n",
      "loss: 0.443195  [121600/175341]\n",
      "loss: 0.451068  [123200/175341]\n",
      "loss: 0.153844  [124800/175341]\n",
      "loss: 0.260570  [126400/175341]\n",
      "loss: 0.242827  [128000/175341]\n",
      "loss: 0.273341  [129600/175341]\n",
      "loss: 0.969523  [131200/175341]\n",
      "loss: 0.386694  [132800/175341]\n",
      "loss: 0.528123  [134400/175341]\n",
      "loss: 0.310760  [136000/175341]\n",
      "loss: 0.523131  [137600/175341]\n",
      "loss: 0.787530  [139200/175341]\n",
      "loss: 0.257094  [140800/175341]\n",
      "loss: 0.459607  [142400/175341]\n",
      "loss: 0.220927  [144000/175341]\n",
      "loss: 0.318871  [145600/175341]\n",
      "loss: 0.417424  [147200/175341]\n",
      "loss: 0.578573  [148800/175341]\n",
      "loss: 0.176287  [150400/175341]\n",
      "loss: 0.489031  [152000/175341]\n",
      "loss: 0.544962  [153600/175341]\n",
      "loss: 0.535688  [155200/175341]\n",
      "loss: 0.429060  [156800/175341]\n",
      "loss: 0.323141  [158400/175341]\n",
      "loss: 0.442056  [160000/175341]\n",
      "loss: 0.750769  [161600/175341]\n",
      "loss: 0.642460  [163200/175341]\n",
      "loss: 0.476327  [164800/175341]\n",
      "loss: 0.222791  [166400/175341]\n",
      "loss: 0.324868  [168000/175341]\n",
      "loss: 0.820577  [169600/175341]\n",
      "loss: 0.427611  [171200/175341]\n",
      "loss: 0.995405  [172800/175341]\n",
      "loss: 0.481161  [174400/175341]\n",
      "Train Accuracy: 80.4997%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.596738, F1-score: 75.72%, Macro_F1-Score:  38.98%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3115f1b7-87f8-41a5-8992-af1752600f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.265904  [    0/175341]\n",
      "loss: 0.220231  [ 1600/175341]\n",
      "loss: 0.716738  [ 3200/175341]\n",
      "loss: 0.953175  [ 4800/175341]\n",
      "loss: 0.757273  [ 6400/175341]\n",
      "loss: 0.369116  [ 8000/175341]\n",
      "loss: 0.439431  [ 9600/175341]\n",
      "loss: 0.507688  [11200/175341]\n",
      "loss: 0.615434  [12800/175341]\n",
      "loss: 0.553683  [14400/175341]\n",
      "loss: 0.209890  [16000/175341]\n",
      "loss: 0.746601  [17600/175341]\n",
      "loss: 0.557011  [19200/175341]\n",
      "loss: 0.579098  [20800/175341]\n",
      "loss: 0.519776  [22400/175341]\n",
      "loss: 0.370063  [24000/175341]\n",
      "loss: 0.368407  [25600/175341]\n",
      "loss: 0.410100  [27200/175341]\n",
      "loss: 0.333413  [28800/175341]\n",
      "loss: 0.110811  [30400/175341]\n",
      "loss: 0.401365  [32000/175341]\n",
      "loss: 0.230599  [33600/175341]\n",
      "loss: 0.737424  [35200/175341]\n",
      "loss: 0.612982  [36800/175341]\n",
      "loss: 0.230564  [38400/175341]\n",
      "loss: 0.281764  [40000/175341]\n",
      "loss: 0.633771  [41600/175341]\n",
      "loss: 0.396632  [43200/175341]\n",
      "loss: 0.585689  [44800/175341]\n",
      "loss: 0.791954  [46400/175341]\n",
      "loss: 0.306199  [48000/175341]\n",
      "loss: 0.887998  [49600/175341]\n",
      "loss: 0.507139  [51200/175341]\n",
      "loss: 0.787447  [52800/175341]\n",
      "loss: 0.135977  [54400/175341]\n",
      "loss: 0.568271  [56000/175341]\n",
      "loss: 0.774629  [57600/175341]\n",
      "loss: 0.644183  [59200/175341]\n",
      "loss: 0.378755  [60800/175341]\n",
      "loss: 0.255696  [62400/175341]\n",
      "loss: 0.444159  [64000/175341]\n",
      "loss: 0.610361  [65600/175341]\n",
      "loss: 0.487687  [67200/175341]\n",
      "loss: 0.280968  [68800/175341]\n",
      "loss: 0.240959  [70400/175341]\n",
      "loss: 0.656466  [72000/175341]\n",
      "loss: 0.612019  [73600/175341]\n",
      "loss: 0.595789  [75200/175341]\n",
      "loss: 0.679839  [76800/175341]\n",
      "loss: 0.699708  [78400/175341]\n",
      "loss: 0.417796  [80000/175341]\n",
      "loss: 0.652807  [81600/175341]\n",
      "loss: 0.268770  [83200/175341]\n",
      "loss: 0.342137  [84800/175341]\n",
      "loss: 0.264399  [86400/175341]\n",
      "loss: 0.273222  [88000/175341]\n",
      "loss: 0.636959  [89600/175341]\n",
      "loss: 0.731118  [91200/175341]\n",
      "loss: 0.293992  [92800/175341]\n",
      "loss: 0.669159  [94400/175341]\n",
      "loss: 0.522582  [96000/175341]\n",
      "loss: 0.307789  [97600/175341]\n",
      "loss: 0.547278  [99200/175341]\n",
      "loss: 0.377656  [100800/175341]\n",
      "loss: 0.632046  [102400/175341]\n",
      "loss: 0.384903  [104000/175341]\n",
      "loss: 0.492506  [105600/175341]\n",
      "loss: 0.941622  [107200/175341]\n",
      "loss: 0.412336  [108800/175341]\n",
      "loss: 0.322226  [110400/175341]\n",
      "loss: 0.521660  [112000/175341]\n",
      "loss: 0.638991  [113600/175341]\n",
      "loss: 0.608959  [115200/175341]\n",
      "loss: 0.392523  [116800/175341]\n",
      "loss: 0.304281  [118400/175341]\n",
      "loss: 0.163123  [120000/175341]\n",
      "loss: 0.629723  [121600/175341]\n",
      "loss: 0.438379  [123200/175341]\n",
      "loss: 0.818296  [124800/175341]\n",
      "loss: 0.235655  [126400/175341]\n",
      "loss: 0.541661  [128000/175341]\n",
      "loss: 0.194775  [129600/175341]\n",
      "loss: 0.262117  [131200/175341]\n",
      "loss: 0.703201  [132800/175341]\n",
      "loss: 0.615835  [134400/175341]\n",
      "loss: 0.382435  [136000/175341]\n",
      "loss: 1.019744  [137600/175341]\n",
      "loss: 0.343443  [139200/175341]\n",
      "loss: 0.254721  [140800/175341]\n",
      "loss: 0.230705  [142400/175341]\n",
      "loss: 0.263016  [144000/175341]\n",
      "loss: 0.609637  [145600/175341]\n",
      "loss: 0.509595  [147200/175341]\n",
      "loss: 0.216741  [148800/175341]\n",
      "loss: 0.172267  [150400/175341]\n",
      "loss: 0.431767  [152000/175341]\n",
      "loss: 0.365758  [153600/175341]\n",
      "loss: 0.221958  [155200/175341]\n",
      "loss: 0.208824  [156800/175341]\n",
      "loss: 0.328418  [158400/175341]\n",
      "loss: 0.636847  [160000/175341]\n",
      "loss: 0.753218  [161600/175341]\n",
      "loss: 0.219527  [163200/175341]\n",
      "loss: 0.721448  [164800/175341]\n",
      "loss: 0.635411  [166400/175341]\n",
      "loss: 0.469602  [168000/175341]\n",
      "loss: 0.170942  [169600/175341]\n",
      "loss: 0.162335  [171200/175341]\n",
      "loss: 0.591814  [172800/175341]\n",
      "loss: 0.473927  [174400/175341]\n",
      "Train Accuracy: 80.4723%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.599903, F1-score: 75.60%, Macro_F1-Score:  39.59%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.383223  [    0/175341]\n",
      "loss: 0.600945  [ 1600/175341]\n",
      "loss: 0.345897  [ 3200/175341]\n",
      "loss: 0.519149  [ 4800/175341]\n",
      "loss: 0.274806  [ 6400/175341]\n",
      "loss: 0.152957  [ 8000/175341]\n",
      "loss: 0.141670  [ 9600/175341]\n",
      "loss: 0.547374  [11200/175341]\n",
      "loss: 0.897194  [12800/175341]\n",
      "loss: 0.423978  [14400/175341]\n",
      "loss: 0.265460  [16000/175341]\n",
      "loss: 0.532338  [17600/175341]\n",
      "loss: 0.109731  [19200/175341]\n",
      "loss: 0.510483  [20800/175341]\n",
      "loss: 0.323030  [22400/175341]\n",
      "loss: 0.157233  [24000/175341]\n",
      "loss: 0.517386  [25600/175341]\n",
      "loss: 0.163887  [27200/175341]\n",
      "loss: 0.230319  [28800/175341]\n",
      "loss: 0.259458  [30400/175341]\n",
      "loss: 0.443314  [32000/175341]\n",
      "loss: 0.699790  [33600/175341]\n",
      "loss: 0.649226  [35200/175341]\n",
      "loss: 0.816605  [36800/175341]\n",
      "loss: 0.574753  [38400/175341]\n",
      "loss: 0.907527  [40000/175341]\n",
      "loss: 0.601639  [41600/175341]\n",
      "loss: 0.523662  [43200/175341]\n",
      "loss: 0.704784  [44800/175341]\n",
      "loss: 0.382390  [46400/175341]\n",
      "loss: 0.894729  [48000/175341]\n",
      "loss: 0.339600  [49600/175341]\n",
      "loss: 0.157783  [51200/175341]\n",
      "loss: 0.119511  [52800/175341]\n",
      "loss: 0.527518  [54400/175341]\n",
      "loss: 0.706516  [56000/175341]\n",
      "loss: 0.278747  [57600/175341]\n",
      "loss: 0.409078  [59200/175341]\n",
      "loss: 0.169241  [60800/175341]\n",
      "loss: 0.934457  [62400/175341]\n",
      "loss: 0.268874  [64000/175341]\n",
      "loss: 0.533175  [65600/175341]\n",
      "loss: 0.546808  [67200/175341]\n",
      "loss: 0.131142  [68800/175341]\n",
      "loss: 0.323311  [70400/175341]\n",
      "loss: 0.567123  [72000/175341]\n",
      "loss: 0.307193  [73600/175341]\n",
      "loss: 0.719151  [75200/175341]\n",
      "loss: 0.203622  [76800/175341]\n",
      "loss: 0.435048  [78400/175341]\n",
      "loss: 0.328861  [80000/175341]\n",
      "loss: 0.631673  [81600/175341]\n",
      "loss: 0.434961  [83200/175341]\n",
      "loss: 0.907125  [84800/175341]\n",
      "loss: 0.197960  [86400/175341]\n",
      "loss: 0.277130  [88000/175341]\n",
      "loss: 0.763180  [89600/175341]\n",
      "loss: 0.229265  [91200/175341]\n",
      "loss: 0.385627  [92800/175341]\n",
      "loss: 0.306910  [94400/175341]\n",
      "loss: 0.875317  [96000/175341]\n",
      "loss: 0.388700  [97600/175341]\n",
      "loss: 0.269606  [99200/175341]\n",
      "loss: 0.226849  [100800/175341]\n",
      "loss: 0.225745  [102400/175341]\n",
      "loss: 0.453328  [104000/175341]\n",
      "loss: 0.611872  [105600/175341]\n",
      "loss: 0.505904  [107200/175341]\n",
      "loss: 0.409175  [108800/175341]\n",
      "loss: 0.735214  [110400/175341]\n",
      "loss: 0.345994  [112000/175341]\n",
      "loss: 0.648097  [113600/175341]\n",
      "loss: 0.433720  [115200/175341]\n",
      "loss: 0.575057  [116800/175341]\n",
      "loss: 1.521567  [118400/175341]\n",
      "loss: 0.741063  [120000/175341]\n",
      "loss: 0.244763  [121600/175341]\n",
      "loss: 0.463638  [123200/175341]\n",
      "loss: 0.898735  [124800/175341]\n",
      "loss: 0.745099  [126400/175341]\n",
      "loss: 0.122314  [128000/175341]\n",
      "loss: 0.549343  [129600/175341]\n",
      "loss: 0.598827  [131200/175341]\n",
      "loss: 0.254278  [132800/175341]\n",
      "loss: 0.398714  [134400/175341]\n",
      "loss: 0.213767  [136000/175341]\n",
      "loss: 0.790423  [137600/175341]\n",
      "loss: 0.343238  [139200/175341]\n",
      "loss: 0.353406  [140800/175341]\n",
      "loss: 0.542300  [142400/175341]\n",
      "loss: 0.268820  [144000/175341]\n",
      "loss: 0.396546  [145600/175341]\n",
      "loss: 0.258056  [147200/175341]\n",
      "loss: 0.662869  [148800/175341]\n",
      "loss: 0.742908  [150400/175341]\n",
      "loss: 0.505952  [152000/175341]\n",
      "loss: 0.580388  [153600/175341]\n",
      "loss: 0.547031  [155200/175341]\n",
      "loss: 0.423628  [156800/175341]\n",
      "loss: 0.784683  [158400/175341]\n",
      "loss: 0.465819  [160000/175341]\n",
      "loss: 0.217792  [161600/175341]\n",
      "loss: 0.535703  [163200/175341]\n",
      "loss: 0.354931  [164800/175341]\n",
      "loss: 0.551487  [166400/175341]\n",
      "loss: 0.280568  [168000/175341]\n",
      "loss: 0.711232  [169600/175341]\n",
      "loss: 0.548665  [171200/175341]\n",
      "loss: 0.376721  [172800/175341]\n",
      "loss: 0.527151  [174400/175341]\n",
      "Train Accuracy: 80.5168%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.574278, F1-score: 75.71%, Macro_F1-Score:  40.40%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.548207  [    0/175341]\n",
      "loss: 0.449828  [ 1600/175341]\n",
      "loss: 0.286696  [ 3200/175341]\n",
      "loss: 0.588739  [ 4800/175341]\n",
      "loss: 0.240122  [ 6400/175341]\n",
      "loss: 0.398917  [ 8000/175341]\n",
      "loss: 0.294275  [ 9600/175341]\n",
      "loss: 0.927896  [11200/175341]\n",
      "loss: 0.409133  [12800/175341]\n",
      "loss: 0.140035  [14400/175341]\n",
      "loss: 0.403090  [16000/175341]\n",
      "loss: 0.467521  [17600/175341]\n",
      "loss: 0.594774  [19200/175341]\n",
      "loss: 0.494656  [20800/175341]\n",
      "loss: 0.383744  [22400/175341]\n",
      "loss: 0.616513  [24000/175341]\n",
      "loss: 0.401974  [25600/175341]\n",
      "loss: 0.585527  [27200/175341]\n",
      "loss: 0.349841  [28800/175341]\n",
      "loss: 0.397461  [30400/175341]\n",
      "loss: 0.518593  [32000/175341]\n",
      "loss: 0.341524  [33600/175341]\n",
      "loss: 0.409767  [35200/175341]\n",
      "loss: 0.596595  [36800/175341]\n",
      "loss: 0.616467  [38400/175341]\n",
      "loss: 0.601829  [40000/175341]\n",
      "loss: 0.653591  [41600/175341]\n",
      "loss: 0.522650  [43200/175341]\n",
      "loss: 0.544492  [44800/175341]\n",
      "loss: 0.554371  [46400/175341]\n",
      "loss: 0.679943  [48000/175341]\n",
      "loss: 0.393696  [49600/175341]\n",
      "loss: 0.311609  [51200/175341]\n",
      "loss: 0.487717  [52800/175341]\n",
      "loss: 0.318610  [54400/175341]\n",
      "loss: 0.434397  [56000/175341]\n",
      "loss: 0.425852  [57600/175341]\n",
      "loss: 0.651372  [59200/175341]\n",
      "loss: 0.509098  [60800/175341]\n",
      "loss: 0.531883  [62400/175341]\n",
      "loss: 0.640108  [64000/175341]\n",
      "loss: 0.736255  [65600/175341]\n",
      "loss: 0.501995  [67200/175341]\n",
      "loss: 0.478348  [68800/175341]\n",
      "loss: 0.571402  [70400/175341]\n",
      "loss: 0.546539  [72000/175341]\n",
      "loss: 0.422288  [73600/175341]\n",
      "loss: 0.489941  [75200/175341]\n",
      "loss: 0.462637  [76800/175341]\n",
      "loss: 0.366656  [78400/175341]\n",
      "loss: 0.455430  [80000/175341]\n",
      "loss: 0.578384  [81600/175341]\n",
      "loss: 0.406139  [83200/175341]\n",
      "loss: 0.247259  [84800/175341]\n",
      "loss: 0.328313  [86400/175341]\n",
      "loss: 0.633009  [88000/175341]\n",
      "loss: 0.377303  [89600/175341]\n",
      "loss: 0.512194  [91200/175341]\n",
      "loss: 0.297606  [92800/175341]\n",
      "loss: 0.552199  [94400/175341]\n",
      "loss: 0.354260  [96000/175341]\n",
      "loss: 0.442908  [97600/175341]\n",
      "loss: 0.543859  [99200/175341]\n",
      "loss: 0.502495  [100800/175341]\n",
      "loss: 0.179045  [102400/175341]\n",
      "loss: 0.713799  [104000/175341]\n",
      "loss: 0.405728  [105600/175341]\n",
      "loss: 0.687113  [107200/175341]\n",
      "loss: 0.317771  [108800/175341]\n",
      "loss: 0.501778  [110400/175341]\n",
      "loss: 0.441543  [112000/175341]\n",
      "loss: 0.593880  [113600/175341]\n",
      "loss: 0.270192  [115200/175341]\n",
      "loss: 0.447048  [116800/175341]\n",
      "loss: 0.678097  [118400/175341]\n",
      "loss: 0.380430  [120000/175341]\n",
      "loss: 1.014060  [121600/175341]\n",
      "loss: 0.398075  [123200/175341]\n",
      "loss: 0.472060  [124800/175341]\n",
      "loss: 0.956378  [126400/175341]\n",
      "loss: 0.126434  [128000/175341]\n",
      "loss: 1.041094  [129600/175341]\n",
      "loss: 0.755076  [131200/175341]\n",
      "loss: 0.548031  [132800/175341]\n",
      "loss: 0.480672  [134400/175341]\n",
      "loss: 0.491963  [136000/175341]\n",
      "loss: 0.971422  [137600/175341]\n",
      "loss: 0.322766  [139200/175341]\n",
      "loss: 0.652077  [140800/175341]\n",
      "loss: 0.615616  [142400/175341]\n",
      "loss: 0.444130  [144000/175341]\n",
      "loss: 0.348055  [145600/175341]\n",
      "loss: 0.685633  [147200/175341]\n",
      "loss: 0.569466  [148800/175341]\n",
      "loss: 0.663378  [150400/175341]\n",
      "loss: 0.432268  [152000/175341]\n",
      "loss: 0.598982  [153600/175341]\n",
      "loss: 0.688712  [155200/175341]\n",
      "loss: 0.373509  [156800/175341]\n",
      "loss: 0.497089  [158400/175341]\n",
      "loss: 0.803425  [160000/175341]\n",
      "loss: 1.076210  [161600/175341]\n",
      "loss: 0.112301  [163200/175341]\n",
      "loss: 0.299558  [164800/175341]\n",
      "loss: 0.997757  [166400/175341]\n",
      "loss: 0.276383  [168000/175341]\n",
      "loss: 0.827986  [169600/175341]\n",
      "loss: 0.771286  [171200/175341]\n",
      "loss: 0.276529  [172800/175341]\n",
      "loss: 0.261299  [174400/175341]\n",
      "Train Accuracy: 80.5761%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.573442, F1-score: 75.70%, Macro_F1-Score:  40.27%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.424959  [    0/175341]\n",
      "loss: 0.602122  [ 1600/175341]\n",
      "loss: 0.395315  [ 3200/175341]\n",
      "loss: 0.595101  [ 4800/175341]\n",
      "loss: 0.234356  [ 6400/175341]\n",
      "loss: 0.337312  [ 8000/175341]\n",
      "loss: 0.827958  [ 9600/175341]\n",
      "loss: 0.431899  [11200/175341]\n",
      "loss: 0.572326  [12800/175341]\n",
      "loss: 0.579395  [14400/175341]\n",
      "loss: 0.448496  [16000/175341]\n",
      "loss: 0.441848  [17600/175341]\n",
      "loss: 0.707426  [19200/175341]\n",
      "loss: 0.300350  [20800/175341]\n",
      "loss: 0.508068  [22400/175341]\n",
      "loss: 0.322257  [24000/175341]\n",
      "loss: 0.238549  [25600/175341]\n",
      "loss: 0.499462  [27200/175341]\n",
      "loss: 0.467928  [28800/175341]\n",
      "loss: 0.341450  [30400/175341]\n",
      "loss: 0.302337  [32000/175341]\n",
      "loss: 0.615719  [33600/175341]\n",
      "loss: 0.440584  [35200/175341]\n",
      "loss: 0.392810  [36800/175341]\n",
      "loss: 0.641581  [38400/175341]\n",
      "loss: 0.929989  [40000/175341]\n",
      "loss: 0.754303  [41600/175341]\n",
      "loss: 0.576604  [43200/175341]\n",
      "loss: 0.486266  [44800/175341]\n",
      "loss: 0.562135  [46400/175341]\n",
      "loss: 0.272490  [48000/175341]\n",
      "loss: 0.203942  [49600/175341]\n",
      "loss: 0.471492  [51200/175341]\n",
      "loss: 0.426035  [52800/175341]\n",
      "loss: 0.469844  [54400/175341]\n",
      "loss: 0.351035  [56000/175341]\n",
      "loss: 0.521822  [57600/175341]\n",
      "loss: 0.501668  [59200/175341]\n",
      "loss: 0.314018  [60800/175341]\n",
      "loss: 0.490575  [62400/175341]\n",
      "loss: 0.330354  [64000/175341]\n",
      "loss: 0.349656  [65600/175341]\n",
      "loss: 0.406551  [67200/175341]\n",
      "loss: 0.746894  [68800/175341]\n",
      "loss: 1.008123  [70400/175341]\n",
      "loss: 0.676849  [72000/175341]\n",
      "loss: 0.753961  [73600/175341]\n",
      "loss: 0.529918  [75200/175341]\n",
      "loss: 0.436418  [76800/175341]\n",
      "loss: 0.539109  [78400/175341]\n",
      "loss: 0.496523  [80000/175341]\n",
      "loss: 0.394472  [81600/175341]\n",
      "loss: 0.429237  [83200/175341]\n",
      "loss: 0.615633  [84800/175341]\n",
      "loss: 0.347470  [86400/175341]\n",
      "loss: 0.616847  [88000/175341]\n",
      "loss: 0.565888  [89600/175341]\n",
      "loss: 0.539798  [91200/175341]\n",
      "loss: 0.753605  [92800/175341]\n",
      "loss: 0.502289  [94400/175341]\n",
      "loss: 0.401912  [96000/175341]\n",
      "loss: 0.267486  [97600/175341]\n",
      "loss: 0.233845  [99200/175341]\n",
      "loss: 0.970439  [100800/175341]\n",
      "loss: 0.213991  [102400/175341]\n",
      "loss: 0.563689  [104000/175341]\n",
      "loss: 0.401623  [105600/175341]\n",
      "loss: 0.524556  [107200/175341]\n",
      "loss: 0.267047  [108800/175341]\n",
      "loss: 0.157863  [110400/175341]\n",
      "loss: 0.550101  [112000/175341]\n",
      "loss: 0.250413  [113600/175341]\n",
      "loss: 0.620693  [115200/175341]\n",
      "loss: 0.520595  [116800/175341]\n",
      "loss: 0.646567  [118400/175341]\n",
      "loss: 0.271704  [120000/175341]\n",
      "loss: 0.500350  [121600/175341]\n",
      "loss: 0.009713  [123200/175341]\n",
      "loss: 0.511338  [124800/175341]\n",
      "loss: 0.649114  [126400/175341]\n",
      "loss: 0.355472  [128000/175341]\n",
      "loss: 0.639482  [129600/175341]\n",
      "loss: 0.376466  [131200/175341]\n",
      "loss: 0.692573  [132800/175341]\n",
      "loss: 1.016008  [134400/175341]\n",
      "loss: 0.419088  [136000/175341]\n",
      "loss: 0.346330  [137600/175341]\n",
      "loss: 0.187941  [139200/175341]\n",
      "loss: 0.442114  [140800/175341]\n",
      "loss: 0.597036  [142400/175341]\n",
      "loss: 0.361279  [144000/175341]\n",
      "loss: 0.633522  [145600/175341]\n",
      "loss: 0.079595  [147200/175341]\n",
      "loss: 0.446277  [148800/175341]\n",
      "loss: 0.202191  [150400/175341]\n",
      "loss: 0.693872  [152000/175341]\n",
      "loss: 0.919948  [153600/175341]\n",
      "loss: 0.338501  [155200/175341]\n",
      "loss: 0.248740  [156800/175341]\n",
      "loss: 0.335897  [158400/175341]\n",
      "loss: 0.302835  [160000/175341]\n",
      "loss: 0.336406  [161600/175341]\n",
      "loss: 0.465006  [163200/175341]\n",
      "loss: 0.604982  [164800/175341]\n",
      "loss: 0.911568  [166400/175341]\n",
      "loss: 0.228319  [168000/175341]\n",
      "loss: 0.187391  [169600/175341]\n",
      "loss: 0.354540  [171200/175341]\n",
      "loss: 0.275625  [172800/175341]\n",
      "loss: 0.470211  [174400/175341]\n",
      "Train Accuracy: 80.5693%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.593134, F1-score: 75.28%, Macro_F1-Score:  40.45%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.582336  [    0/175341]\n",
      "loss: 0.781758  [ 1600/175341]\n",
      "loss: 0.794087  [ 3200/175341]\n",
      "loss: 0.351231  [ 4800/175341]\n",
      "loss: 0.399735  [ 6400/175341]\n",
      "loss: 0.520453  [ 8000/175341]\n",
      "loss: 0.279178  [ 9600/175341]\n",
      "loss: 0.528099  [11200/175341]\n",
      "loss: 0.389079  [12800/175341]\n",
      "loss: 0.285527  [14400/175341]\n",
      "loss: 0.389581  [16000/175341]\n",
      "loss: 0.416247  [17600/175341]\n",
      "loss: 0.618332  [19200/175341]\n",
      "loss: 0.618015  [20800/175341]\n",
      "loss: 0.703677  [22400/175341]\n",
      "loss: 0.249708  [24000/175341]\n",
      "loss: 0.460473  [25600/175341]\n",
      "loss: 0.382842  [27200/175341]\n",
      "loss: 0.542601  [28800/175341]\n",
      "loss: 0.531424  [30400/175341]\n",
      "loss: 0.344822  [32000/175341]\n",
      "loss: 0.463836  [33600/175341]\n",
      "loss: 0.649516  [35200/175341]\n",
      "loss: 0.551959  [36800/175341]\n",
      "loss: 0.363155  [38400/175341]\n",
      "loss: 0.622005  [40000/175341]\n",
      "loss: 0.957744  [41600/175341]\n",
      "loss: 0.493194  [43200/175341]\n",
      "loss: 0.442240  [44800/175341]\n",
      "loss: 0.718202  [46400/175341]\n",
      "loss: 0.389491  [48000/175341]\n",
      "loss: 0.344112  [49600/175341]\n",
      "loss: 0.324386  [51200/175341]\n",
      "loss: 0.355029  [52800/175341]\n",
      "loss: 0.238711  [54400/175341]\n",
      "loss: 0.449360  [56000/175341]\n",
      "loss: 0.187764  [57600/175341]\n",
      "loss: 0.493505  [59200/175341]\n",
      "loss: 0.570276  [60800/175341]\n",
      "loss: 0.704966  [62400/175341]\n",
      "loss: 0.864558  [64000/175341]\n",
      "loss: 0.437125  [65600/175341]\n",
      "loss: 0.503196  [67200/175341]\n",
      "loss: 0.661932  [68800/175341]\n",
      "loss: 0.496497  [70400/175341]\n",
      "loss: 0.350668  [72000/175341]\n",
      "loss: 0.493374  [73600/175341]\n",
      "loss: 0.670561  [75200/175341]\n",
      "loss: 0.549071  [76800/175341]\n",
      "loss: 1.054089  [78400/175341]\n",
      "loss: 0.209372  [80000/175341]\n",
      "loss: 0.715894  [81600/175341]\n",
      "loss: 0.611174  [83200/175341]\n",
      "loss: 0.545059  [84800/175341]\n",
      "loss: 0.435915  [86400/175341]\n",
      "loss: 0.385351  [88000/175341]\n",
      "loss: 0.484532  [89600/175341]\n",
      "loss: 0.753152  [91200/175341]\n",
      "loss: 0.297122  [92800/175341]\n",
      "loss: 0.633281  [94400/175341]\n",
      "loss: 0.741546  [96000/175341]\n",
      "loss: 0.637294  [97600/175341]\n",
      "loss: 0.493033  [99200/175341]\n",
      "loss: 0.757160  [100800/175341]\n",
      "loss: 0.475385  [102400/175341]\n",
      "loss: 0.175165  [104000/175341]\n",
      "loss: 0.791768  [105600/175341]\n",
      "loss: 0.377013  [107200/175341]\n",
      "loss: 0.373183  [108800/175341]\n",
      "loss: 0.351607  [110400/175341]\n",
      "loss: 0.670814  [112000/175341]\n",
      "loss: 0.152749  [113600/175341]\n",
      "loss: 1.134549  [115200/175341]\n",
      "loss: 0.471338  [116800/175341]\n",
      "loss: 0.311197  [118400/175341]\n",
      "loss: 0.725654  [120000/175341]\n",
      "loss: 0.315130  [121600/175341]\n",
      "loss: 0.514299  [123200/175341]\n",
      "loss: 0.580139  [124800/175341]\n",
      "loss: 0.241880  [126400/175341]\n",
      "loss: 0.491295  [128000/175341]\n",
      "loss: 0.406054  [129600/175341]\n",
      "loss: 0.500193  [131200/175341]\n",
      "loss: 0.481548  [132800/175341]\n",
      "loss: 0.549761  [134400/175341]\n",
      "loss: 0.484304  [136000/175341]\n",
      "loss: 0.552269  [137600/175341]\n",
      "loss: 0.776699  [139200/175341]\n",
      "loss: 0.239279  [140800/175341]\n",
      "loss: 0.673421  [142400/175341]\n",
      "loss: 0.918077  [144000/175341]\n",
      "loss: 0.909987  [145600/175341]\n",
      "loss: 0.345862  [147200/175341]\n",
      "loss: 0.444683  [148800/175341]\n",
      "loss: 0.489652  [150400/175341]\n",
      "loss: 0.255404  [152000/175341]\n",
      "loss: 0.525296  [153600/175341]\n",
      "loss: 0.430407  [155200/175341]\n",
      "loss: 0.708179  [156800/175341]\n",
      "loss: 0.475398  [158400/175341]\n",
      "loss: 0.559588  [160000/175341]\n",
      "loss: 0.574101  [161600/175341]\n",
      "loss: 1.031180  [163200/175341]\n",
      "loss: 0.467092  [164800/175341]\n",
      "loss: 0.779296  [166400/175341]\n",
      "loss: 0.289620  [168000/175341]\n",
      "loss: 0.881790  [169600/175341]\n",
      "loss: 0.848497  [171200/175341]\n",
      "loss: 0.416378  [172800/175341]\n",
      "loss: 0.214787  [174400/175341]\n",
      "Train Accuracy: 80.5693%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.584146, F1-score: 75.50%, Macro_F1-Score:  39.85%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.653626  [    0/175341]\n",
      "loss: 0.191755  [ 1600/175341]\n",
      "loss: 0.340708  [ 3200/175341]\n",
      "loss: 0.344874  [ 4800/175341]\n",
      "loss: 0.529866  [ 6400/175341]\n",
      "loss: 0.383286  [ 8000/175341]\n",
      "loss: 0.568478  [ 9600/175341]\n",
      "loss: 0.484876  [11200/175341]\n",
      "loss: 0.203388  [12800/175341]\n",
      "loss: 0.278429  [14400/175341]\n",
      "loss: 0.575610  [16000/175341]\n",
      "loss: 0.189593  [17600/175341]\n",
      "loss: 0.210469  [19200/175341]\n",
      "loss: 0.873269  [20800/175341]\n",
      "loss: 0.817100  [22400/175341]\n",
      "loss: 0.847156  [24000/175341]\n",
      "loss: 0.184703  [25600/175341]\n",
      "loss: 0.495193  [27200/175341]\n",
      "loss: 0.877707  [28800/175341]\n",
      "loss: 0.412162  [30400/175341]\n",
      "loss: 0.529028  [32000/175341]\n",
      "loss: 0.467831  [33600/175341]\n",
      "loss: 0.434265  [35200/175341]\n",
      "loss: 0.335353  [36800/175341]\n",
      "loss: 0.462105  [38400/175341]\n",
      "loss: 0.329644  [40000/175341]\n",
      "loss: 0.260543  [41600/175341]\n",
      "loss: 0.181017  [43200/175341]\n",
      "loss: 0.418612  [44800/175341]\n",
      "loss: 1.208724  [46400/175341]\n",
      "loss: 0.730893  [48000/175341]\n",
      "loss: 0.529270  [49600/175341]\n",
      "loss: 0.388085  [51200/175341]\n",
      "loss: 0.692565  [52800/175341]\n",
      "loss: 0.494691  [54400/175341]\n",
      "loss: 0.708918  [56000/175341]\n",
      "loss: 0.234745  [57600/175341]\n",
      "loss: 0.255648  [59200/175341]\n",
      "loss: 0.498651  [60800/175341]\n",
      "loss: 0.437507  [62400/175341]\n",
      "loss: 0.290460  [64000/175341]\n",
      "loss: 1.133893  [65600/175341]\n",
      "loss: 0.178096  [67200/175341]\n",
      "loss: 0.531970  [68800/175341]\n",
      "loss: 0.530381  [70400/175341]\n",
      "loss: 0.355861  [72000/175341]\n",
      "loss: 0.274766  [73600/175341]\n",
      "loss: 0.722143  [75200/175341]\n",
      "loss: 0.577540  [76800/175341]\n",
      "loss: 0.431744  [78400/175341]\n",
      "loss: 0.257905  [80000/175341]\n",
      "loss: 0.283163  [81600/175341]\n",
      "loss: 0.187739  [83200/175341]\n",
      "loss: 0.368752  [84800/175341]\n",
      "loss: 0.619873  [86400/175341]\n",
      "loss: 0.314365  [88000/175341]\n",
      "loss: 1.160208  [89600/175341]\n",
      "loss: 0.309039  [91200/175341]\n",
      "loss: 0.305407  [92800/175341]\n",
      "loss: 0.574986  [94400/175341]\n",
      "loss: 0.660928  [96000/175341]\n",
      "loss: 0.471130  [97600/175341]\n",
      "loss: 0.462145  [99200/175341]\n",
      "loss: 0.389129  [100800/175341]\n",
      "loss: 0.528739  [102400/175341]\n",
      "loss: 0.366122  [104000/175341]\n",
      "loss: 0.551770  [105600/175341]\n",
      "loss: 0.331098  [107200/175341]\n",
      "loss: 0.315455  [108800/175341]\n",
      "loss: 0.760522  [110400/175341]\n",
      "loss: 0.436468  [112000/175341]\n",
      "loss: 0.533947  [113600/175341]\n",
      "loss: 0.596651  [115200/175341]\n",
      "loss: 0.548377  [116800/175341]\n",
      "loss: 0.123140  [118400/175341]\n",
      "loss: 0.314182  [120000/175341]\n",
      "loss: 0.356944  [121600/175341]\n",
      "loss: 0.560576  [123200/175341]\n",
      "loss: 0.415936  [124800/175341]\n",
      "loss: 0.554261  [126400/175341]\n",
      "loss: 0.054983  [128000/175341]\n",
      "loss: 0.642441  [129600/175341]\n",
      "loss: 0.407969  [131200/175341]\n",
      "loss: 0.341978  [132800/175341]\n",
      "loss: 0.163011  [134400/175341]\n",
      "loss: 0.549890  [136000/175341]\n",
      "loss: 0.357410  [137600/175341]\n",
      "loss: 0.465353  [139200/175341]\n",
      "loss: 0.392758  [140800/175341]\n",
      "loss: 0.537205  [142400/175341]\n",
      "loss: 0.330977  [144000/175341]\n",
      "loss: 0.198277  [145600/175341]\n",
      "loss: 0.750670  [147200/175341]\n",
      "loss: 0.736518  [148800/175341]\n",
      "loss: 0.658319  [150400/175341]\n",
      "loss: 0.637504  [152000/175341]\n",
      "loss: 0.480072  [153600/175341]\n",
      "loss: 0.472681  [155200/175341]\n",
      "loss: 0.417178  [156800/175341]\n",
      "loss: 0.528411  [158400/175341]\n",
      "loss: 0.647689  [160000/175341]\n",
      "loss: 0.445602  [161600/175341]\n",
      "loss: 0.738852  [163200/175341]\n",
      "loss: 0.212231  [164800/175341]\n",
      "loss: 0.662271  [166400/175341]\n",
      "loss: 0.300481  [168000/175341]\n",
      "loss: 0.625481  [169600/175341]\n",
      "loss: 0.508551  [171200/175341]\n",
      "loss: 0.522403  [172800/175341]\n",
      "loss: 0.451030  [174400/175341]\n",
      "Train Accuracy: 80.5898%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.601207, F1-score: 74.47%, Macro_F1-Score:  39.59%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.404174  [    0/175341]\n",
      "loss: 0.979248  [ 1600/175341]\n",
      "loss: 0.585814  [ 3200/175341]\n",
      "loss: 0.321131  [ 4800/175341]\n",
      "loss: 0.230840  [ 6400/175341]\n",
      "loss: 0.511280  [ 8000/175341]\n",
      "loss: 0.313880  [ 9600/175341]\n",
      "loss: 1.062160  [11200/175341]\n",
      "loss: 0.368083  [12800/175341]\n",
      "loss: 0.778863  [14400/175341]\n",
      "loss: 0.325345  [16000/175341]\n",
      "loss: 0.787627  [17600/175341]\n",
      "loss: 0.727545  [19200/175341]\n",
      "loss: 0.638126  [20800/175341]\n",
      "loss: 0.522265  [22400/175341]\n",
      "loss: 0.243860  [24000/175341]\n",
      "loss: 0.190802  [25600/175341]\n",
      "loss: 1.002619  [27200/175341]\n",
      "loss: 0.515113  [28800/175341]\n",
      "loss: 0.718539  [30400/175341]\n",
      "loss: 0.491522  [32000/175341]\n",
      "loss: 0.417726  [33600/175341]\n",
      "loss: 0.304006  [35200/175341]\n",
      "loss: 0.322430  [36800/175341]\n",
      "loss: 0.674550  [38400/175341]\n",
      "loss: 0.550408  [40000/175341]\n",
      "loss: 0.780518  [41600/175341]\n",
      "loss: 0.568893  [43200/175341]\n",
      "loss: 0.338901  [44800/175341]\n",
      "loss: 0.289161  [46400/175341]\n",
      "loss: 0.415474  [48000/175341]\n",
      "loss: 0.440900  [49600/175341]\n",
      "loss: 0.502101  [51200/175341]\n",
      "loss: 0.344002  [52800/175341]\n",
      "loss: 0.194979  [54400/175341]\n",
      "loss: 0.659542  [56000/175341]\n",
      "loss: 0.429348  [57600/175341]\n",
      "loss: 0.714843  [59200/175341]\n",
      "loss: 0.354310  [60800/175341]\n",
      "loss: 0.431563  [62400/175341]\n",
      "loss: 0.714689  [64000/175341]\n",
      "loss: 0.267611  [65600/175341]\n",
      "loss: 0.424869  [67200/175341]\n",
      "loss: 0.424984  [68800/175341]\n",
      "loss: 0.366176  [70400/175341]\n",
      "loss: 0.586882  [72000/175341]\n",
      "loss: 0.512335  [73600/175341]\n",
      "loss: 0.956251  [75200/175341]\n",
      "loss: 0.445368  [76800/175341]\n",
      "loss: 0.346458  [78400/175341]\n",
      "loss: 0.180432  [80000/175341]\n",
      "loss: 0.410871  [81600/175341]\n",
      "loss: 0.511935  [83200/175341]\n",
      "loss: 0.178504  [84800/175341]\n",
      "loss: 0.609183  [86400/175341]\n",
      "loss: 0.261922  [88000/175341]\n",
      "loss: 0.504541  [89600/175341]\n",
      "loss: 0.490163  [91200/175341]\n",
      "loss: 0.483000  [92800/175341]\n",
      "loss: 0.352308  [94400/175341]\n",
      "loss: 0.180047  [96000/175341]\n",
      "loss: 0.413367  [97600/175341]\n",
      "loss: 0.342490  [99200/175341]\n",
      "loss: 0.461042  [100800/175341]\n",
      "loss: 0.530123  [102400/175341]\n",
      "loss: 0.500549  [104000/175341]\n",
      "loss: 0.562022  [105600/175341]\n",
      "loss: 0.480341  [107200/175341]\n",
      "loss: 0.532089  [108800/175341]\n",
      "loss: 0.531719  [110400/175341]\n",
      "loss: 0.403251  [112000/175341]\n",
      "loss: 0.702530  [113600/175341]\n",
      "loss: 0.061307  [115200/175341]\n",
      "loss: 0.743979  [116800/175341]\n",
      "loss: 0.270300  [118400/175341]\n",
      "loss: 0.262043  [120000/175341]\n",
      "loss: 0.547764  [121600/175341]\n",
      "loss: 0.309630  [123200/175341]\n",
      "loss: 0.320842  [124800/175341]\n",
      "loss: 0.948641  [126400/175341]\n",
      "loss: 0.343236  [128000/175341]\n",
      "loss: 0.765448  [129600/175341]\n",
      "loss: 0.376832  [131200/175341]\n",
      "loss: 0.481591  [132800/175341]\n",
      "loss: 0.405303  [134400/175341]\n",
      "loss: 0.433700  [136000/175341]\n",
      "loss: 0.585427  [137600/175341]\n",
      "loss: 0.658222  [139200/175341]\n",
      "loss: 0.372773  [140800/175341]\n",
      "loss: 0.488988  [142400/175341]\n",
      "loss: 0.702612  [144000/175341]\n",
      "loss: 0.497295  [145600/175341]\n",
      "loss: 0.373118  [147200/175341]\n",
      "loss: 0.571508  [148800/175341]\n",
      "loss: 0.266421  [150400/175341]\n",
      "loss: 0.333534  [152000/175341]\n",
      "loss: 0.469557  [153600/175341]\n",
      "loss: 0.117173  [155200/175341]\n",
      "loss: 0.382028  [156800/175341]\n",
      "loss: 0.674783  [158400/175341]\n",
      "loss: 0.401537  [160000/175341]\n",
      "loss: 0.422942  [161600/175341]\n",
      "loss: 0.566840  [163200/175341]\n",
      "loss: 0.299919  [164800/175341]\n",
      "loss: 0.444321  [166400/175341]\n",
      "loss: 0.312093  [168000/175341]\n",
      "loss: 0.572430  [169600/175341]\n",
      "loss: 0.612024  [171200/175341]\n",
      "loss: 0.897059  [172800/175341]\n",
      "loss: 0.526432  [174400/175341]\n",
      "Train Accuracy: 80.5972%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.599763, F1-score: 74.51%, Macro_F1-Score:  39.22%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.650490  [    0/175341]\n",
      "loss: 0.628965  [ 1600/175341]\n",
      "loss: 0.399882  [ 3200/175341]\n",
      "loss: 0.623874  [ 4800/175341]\n",
      "loss: 0.123910  [ 6400/175341]\n",
      "loss: 0.423552  [ 8000/175341]\n",
      "loss: 0.192877  [ 9600/175341]\n",
      "loss: 0.539989  [11200/175341]\n",
      "loss: 0.301901  [12800/175341]\n",
      "loss: 0.536576  [14400/175341]\n",
      "loss: 0.238003  [16000/175341]\n",
      "loss: 0.249073  [17600/175341]\n",
      "loss: 0.859311  [19200/175341]\n",
      "loss: 1.011436  [20800/175341]\n",
      "loss: 0.356380  [22400/175341]\n",
      "loss: 0.524560  [24000/175341]\n",
      "loss: 0.272850  [25600/175341]\n",
      "loss: 0.320274  [27200/175341]\n",
      "loss: 0.174123  [28800/175341]\n",
      "loss: 0.863934  [30400/175341]\n",
      "loss: 0.409993  [32000/175341]\n",
      "loss: 0.414372  [33600/175341]\n",
      "loss: 0.404622  [35200/175341]\n",
      "loss: 0.891613  [36800/175341]\n",
      "loss: 0.325952  [38400/175341]\n",
      "loss: 0.511169  [40000/175341]\n",
      "loss: 0.489306  [41600/175341]\n",
      "loss: 0.380405  [43200/175341]\n",
      "loss: 1.093260  [44800/175341]\n",
      "loss: 0.846429  [46400/175341]\n",
      "loss: 0.455307  [48000/175341]\n",
      "loss: 0.652804  [49600/175341]\n",
      "loss: 0.413875  [51200/175341]\n",
      "loss: 0.839222  [52800/175341]\n",
      "loss: 0.512545  [54400/175341]\n",
      "loss: 0.708117  [56000/175341]\n",
      "loss: 0.816383  [57600/175341]\n",
      "loss: 0.534839  [59200/175341]\n",
      "loss: 0.608897  [60800/175341]\n",
      "loss: 0.906407  [62400/175341]\n",
      "loss: 0.201299  [64000/175341]\n",
      "loss: 0.451398  [65600/175341]\n",
      "loss: 0.309884  [67200/175341]\n",
      "loss: 0.625521  [68800/175341]\n",
      "loss: 0.564729  [70400/175341]\n",
      "loss: 0.412689  [72000/175341]\n",
      "loss: 0.696199  [73600/175341]\n",
      "loss: 0.499787  [75200/175341]\n",
      "loss: 0.537485  [76800/175341]\n",
      "loss: 0.280158  [78400/175341]\n",
      "loss: 0.668979  [80000/175341]\n",
      "loss: 0.381560  [81600/175341]\n",
      "loss: 0.447662  [83200/175341]\n",
      "loss: 0.466884  [84800/175341]\n",
      "loss: 0.339414  [86400/175341]\n",
      "loss: 0.552074  [88000/175341]\n",
      "loss: 0.757707  [89600/175341]\n",
      "loss: 0.814989  [91200/175341]\n",
      "loss: 0.220409  [92800/175341]\n",
      "loss: 0.607355  [94400/175341]\n",
      "loss: 0.184002  [96000/175341]\n",
      "loss: 0.559230  [97600/175341]\n",
      "loss: 0.501100  [99200/175341]\n",
      "loss: 0.763116  [100800/175341]\n",
      "loss: 0.671443  [102400/175341]\n",
      "loss: 0.744774  [104000/175341]\n",
      "loss: 0.206342  [105600/175341]\n",
      "loss: 0.255512  [107200/175341]\n",
      "loss: 0.469662  [108800/175341]\n",
      "loss: 0.487477  [110400/175341]\n",
      "loss: 0.326452  [112000/175341]\n",
      "loss: 0.497429  [113600/175341]\n",
      "loss: 0.776859  [115200/175341]\n",
      "loss: 0.539280  [116800/175341]\n",
      "loss: 0.397167  [118400/175341]\n",
      "loss: 0.389996  [120000/175341]\n",
      "loss: 0.218986  [121600/175341]\n",
      "loss: 0.615743  [123200/175341]\n",
      "loss: 0.542012  [124800/175341]\n",
      "loss: 0.450351  [126400/175341]\n",
      "loss: 0.440248  [128000/175341]\n",
      "loss: 0.556439  [129600/175341]\n",
      "loss: 0.423708  [131200/175341]\n",
      "loss: 0.348280  [132800/175341]\n",
      "loss: 0.485670  [134400/175341]\n",
      "loss: 0.339251  [136000/175341]\n",
      "loss: 0.729410  [137600/175341]\n",
      "loss: 0.523992  [139200/175341]\n",
      "loss: 0.862911  [140800/175341]\n",
      "loss: 0.753980  [142400/175341]\n",
      "loss: 0.641201  [144000/175341]\n",
      "loss: 0.544691  [145600/175341]\n",
      "loss: 0.904933  [147200/175341]\n",
      "loss: 0.421494  [148800/175341]\n",
      "loss: 0.796194  [150400/175341]\n",
      "loss: 0.194739  [152000/175341]\n",
      "loss: 0.407900  [153600/175341]\n",
      "loss: 0.148014  [155200/175341]\n",
      "loss: 0.647689  [156800/175341]\n",
      "loss: 0.422288  [158400/175341]\n",
      "loss: 0.314708  [160000/175341]\n",
      "loss: 0.594308  [161600/175341]\n",
      "loss: 0.862545  [163200/175341]\n",
      "loss: 0.527929  [164800/175341]\n",
      "loss: 0.283235  [166400/175341]\n",
      "loss: 0.379929  [168000/175341]\n",
      "loss: 0.981177  [169600/175341]\n",
      "loss: 0.409948  [171200/175341]\n",
      "loss: 0.451790  [172800/175341]\n",
      "loss: 0.176426  [174400/175341]\n",
      "Train Accuracy: 80.6258%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.576213, F1-score: 75.03%, Macro_F1-Score:  40.03%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.372595  [    0/175341]\n",
      "loss: 0.588399  [ 1600/175341]\n",
      "loss: 0.694426  [ 3200/175341]\n",
      "loss: 0.797233  [ 4800/175341]\n",
      "loss: 0.175649  [ 6400/175341]\n",
      "loss: 0.313459  [ 8000/175341]\n",
      "loss: 0.335527  [ 9600/175341]\n",
      "loss: 0.543472  [11200/175341]\n",
      "loss: 0.351648  [12800/175341]\n",
      "loss: 0.204432  [14400/175341]\n",
      "loss: 0.956342  [16000/175341]\n",
      "loss: 0.654095  [17600/175341]\n",
      "loss: 0.437274  [19200/175341]\n",
      "loss: 0.197282  [20800/175341]\n",
      "loss: 0.571697  [22400/175341]\n",
      "loss: 0.489011  [24000/175341]\n",
      "loss: 0.990519  [25600/175341]\n",
      "loss: 0.456710  [27200/175341]\n",
      "loss: 0.681286  [28800/175341]\n",
      "loss: 0.521907  [30400/175341]\n",
      "loss: 0.647690  [32000/175341]\n",
      "loss: 0.546941  [33600/175341]\n",
      "loss: 0.363216  [35200/175341]\n",
      "loss: 0.247750  [36800/175341]\n",
      "loss: 0.251752  [38400/175341]\n",
      "loss: 0.326877  [40000/175341]\n",
      "loss: 0.520077  [41600/175341]\n",
      "loss: 0.544537  [43200/175341]\n",
      "loss: 0.310915  [44800/175341]\n",
      "loss: 0.615576  [46400/175341]\n",
      "loss: 0.432359  [48000/175341]\n",
      "loss: 0.533793  [49600/175341]\n",
      "loss: 0.388160  [51200/175341]\n",
      "loss: 0.630542  [52800/175341]\n",
      "loss: 0.664524  [54400/175341]\n",
      "loss: 0.684482  [56000/175341]\n",
      "loss: 0.473172  [57600/175341]\n",
      "loss: 0.379022  [59200/175341]\n",
      "loss: 0.142675  [60800/175341]\n",
      "loss: 0.120353  [62400/175341]\n",
      "loss: 0.178972  [64000/175341]\n",
      "loss: 0.337283  [65600/175341]\n",
      "loss: 0.336363  [67200/175341]\n",
      "loss: 0.640240  [68800/175341]\n",
      "loss: 0.482336  [70400/175341]\n",
      "loss: 0.597950  [72000/175341]\n",
      "loss: 0.622698  [73600/175341]\n",
      "loss: 0.271729  [75200/175341]\n",
      "loss: 0.601369  [76800/175341]\n",
      "loss: 0.271049  [78400/175341]\n",
      "loss: 0.570447  [80000/175341]\n",
      "loss: 0.218079  [81600/175341]\n",
      "loss: 0.237244  [83200/175341]\n",
      "loss: 0.617666  [84800/175341]\n",
      "loss: 0.264440  [86400/175341]\n",
      "loss: 0.501205  [88000/175341]\n",
      "loss: 0.922616  [89600/175341]\n",
      "loss: 0.094910  [91200/175341]\n",
      "loss: 0.234829  [92800/175341]\n",
      "loss: 0.676645  [94400/175341]\n",
      "loss: 0.536944  [96000/175341]\n",
      "loss: 0.178953  [97600/175341]\n",
      "loss: 0.674008  [99200/175341]\n",
      "loss: 0.932895  [100800/175341]\n",
      "loss: 0.322088  [102400/175341]\n",
      "loss: 0.123424  [104000/175341]\n",
      "loss: 0.751833  [105600/175341]\n",
      "loss: 0.295115  [107200/175341]\n",
      "loss: 0.375899  [108800/175341]\n",
      "loss: 0.146212  [110400/175341]\n",
      "loss: 0.464446  [112000/175341]\n",
      "loss: 0.203080  [113600/175341]\n",
      "loss: 0.084273  [115200/175341]\n",
      "loss: 0.296214  [116800/175341]\n",
      "loss: 0.421271  [118400/175341]\n",
      "loss: 0.335018  [120000/175341]\n",
      "loss: 0.551175  [121600/175341]\n",
      "loss: 0.430554  [123200/175341]\n",
      "loss: 0.203201  [124800/175341]\n",
      "loss: 0.932236  [126400/175341]\n",
      "loss: 0.785920  [128000/175341]\n",
      "loss: 0.727340  [129600/175341]\n",
      "loss: 0.382102  [131200/175341]\n",
      "loss: 0.500600  [132800/175341]\n",
      "loss: 0.283596  [134400/175341]\n",
      "loss: 0.467549  [136000/175341]\n",
      "loss: 0.377777  [137600/175341]\n",
      "loss: 0.966850  [139200/175341]\n",
      "loss: 0.178673  [140800/175341]\n",
      "loss: 0.411631  [142400/175341]\n",
      "loss: 0.491454  [144000/175341]\n",
      "loss: 0.454924  [145600/175341]\n",
      "loss: 0.792183  [147200/175341]\n",
      "loss: 0.617211  [148800/175341]\n",
      "loss: 0.287841  [150400/175341]\n",
      "loss: 0.251842  [152000/175341]\n",
      "loss: 0.559340  [153600/175341]\n",
      "loss: 0.540030  [155200/175341]\n",
      "loss: 0.130010  [156800/175341]\n",
      "loss: 0.507787  [158400/175341]\n",
      "loss: 0.259010  [160000/175341]\n",
      "loss: 0.165930  [161600/175341]\n",
      "loss: 0.615217  [163200/175341]\n",
      "loss: 0.392844  [164800/175341]\n",
      "loss: 0.464348  [166400/175341]\n",
      "loss: 0.385286  [168000/175341]\n",
      "loss: 1.059396  [169600/175341]\n",
      "loss: 0.557555  [171200/175341]\n",
      "loss: 0.415604  [172800/175341]\n",
      "loss: 0.741095  [174400/175341]\n",
      "Train Accuracy: 80.6623%\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.609423, F1-score: 73.76%, Macro_F1-Score:  39.40%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.611901  [    0/175341]\n",
      "loss: 0.344472  [ 1600/175341]\n",
      "loss: 0.463689  [ 3200/175341]\n",
      "loss: 0.484011  [ 4800/175341]\n",
      "loss: 0.202748  [ 6400/175341]\n",
      "loss: 0.391463  [ 8000/175341]\n",
      "loss: 0.433961  [ 9600/175341]\n",
      "loss: 0.274957  [11200/175341]\n",
      "loss: 0.241413  [12800/175341]\n",
      "loss: 0.541642  [14400/175341]\n",
      "loss: 0.357227  [16000/175341]\n",
      "loss: 0.332632  [17600/175341]\n",
      "loss: 0.660676  [19200/175341]\n",
      "loss: 0.714504  [20800/175341]\n",
      "loss: 0.453129  [22400/175341]\n",
      "loss: 0.093691  [24000/175341]\n",
      "loss: 0.927977  [25600/175341]\n",
      "loss: 0.464816  [27200/175341]\n",
      "loss: 0.362314  [28800/175341]\n",
      "loss: 0.335998  [30400/175341]\n",
      "loss: 0.608855  [32000/175341]\n",
      "loss: 0.298924  [33600/175341]\n",
      "loss: 0.352866  [35200/175341]\n",
      "loss: 0.933384  [36800/175341]\n",
      "loss: 0.467025  [38400/175341]\n",
      "loss: 0.552706  [40000/175341]\n",
      "loss: 0.315000  [41600/175341]\n",
      "loss: 0.448356  [43200/175341]\n",
      "loss: 0.590687  [44800/175341]\n",
      "loss: 0.746378  [46400/175341]\n",
      "loss: 0.778936  [48000/175341]\n",
      "loss: 0.186114  [49600/175341]\n",
      "loss: 0.325902  [51200/175341]\n",
      "loss: 0.392555  [52800/175341]\n",
      "loss: 0.328537  [54400/175341]\n",
      "loss: 0.617204  [56000/175341]\n",
      "loss: 0.344761  [57600/175341]\n",
      "loss: 0.675794  [59200/175341]\n",
      "loss: 0.848969  [60800/175341]\n",
      "loss: 0.323544  [62400/175341]\n",
      "loss: 0.384736  [64000/175341]\n",
      "loss: 0.552822  [65600/175341]\n",
      "loss: 0.439998  [67200/175341]\n",
      "loss: 0.716190  [68800/175341]\n",
      "loss: 0.528600  [70400/175341]\n",
      "loss: 0.387349  [72000/175341]\n",
      "loss: 0.408288  [73600/175341]\n",
      "loss: 0.507760  [75200/175341]\n",
      "loss: 0.299447  [76800/175341]\n",
      "loss: 0.258870  [78400/175341]\n",
      "loss: 0.345974  [80000/175341]\n",
      "loss: 0.707225  [81600/175341]\n",
      "loss: 0.419620  [83200/175341]\n",
      "loss: 0.408289  [84800/175341]\n",
      "loss: 0.478650  [86400/175341]\n",
      "loss: 0.490475  [88000/175341]\n",
      "loss: 0.488483  [89600/175341]\n",
      "loss: 0.411487  [91200/175341]\n",
      "loss: 0.296549  [92800/175341]\n",
      "loss: 0.510859  [94400/175341]\n",
      "loss: 0.161812  [96000/175341]\n",
      "loss: 0.685015  [97600/175341]\n",
      "loss: 0.398150  [99200/175341]\n",
      "loss: 0.640020  [100800/175341]\n",
      "loss: 0.504627  [102400/175341]\n",
      "loss: 0.720149  [104000/175341]\n",
      "loss: 0.804760  [105600/175341]\n",
      "loss: 0.852780  [107200/175341]\n",
      "loss: 0.695211  [108800/175341]\n",
      "loss: 0.266311  [110400/175341]\n",
      "loss: 0.456738  [112000/175341]\n",
      "loss: 0.517007  [113600/175341]\n",
      "loss: 0.113702  [115200/175341]\n",
      "loss: 0.469050  [116800/175341]\n",
      "loss: 0.563793  [118400/175341]\n",
      "loss: 0.431664  [120000/175341]\n",
      "loss: 0.451894  [121600/175341]\n",
      "loss: 0.484489  [123200/175341]\n",
      "loss: 0.626611  [124800/175341]\n",
      "loss: 0.279663  [126400/175341]\n",
      "loss: 0.601409  [128000/175341]\n",
      "loss: 0.314919  [129600/175341]\n",
      "loss: 0.205059  [131200/175341]\n",
      "loss: 0.835222  [132800/175341]\n",
      "loss: 0.806366  [134400/175341]\n",
      "loss: 0.345470  [136000/175341]\n",
      "loss: 0.394005  [137600/175341]\n",
      "loss: 0.780992  [139200/175341]\n",
      "loss: 0.228396  [140800/175341]\n",
      "loss: 0.242889  [142400/175341]\n",
      "loss: 0.425754  [144000/175341]\n",
      "loss: 0.746367  [145600/175341]\n",
      "loss: 0.301946  [147200/175341]\n",
      "loss: 0.417756  [148800/175341]\n",
      "loss: 0.562808  [150400/175341]\n",
      "loss: 0.679240  [152000/175341]\n",
      "loss: 0.558277  [153600/175341]\n",
      "loss: 0.326701  [155200/175341]\n",
      "loss: 0.446204  [156800/175341]\n",
      "loss: 0.910837  [158400/175341]\n",
      "loss: 0.289584  [160000/175341]\n",
      "loss: 0.666767  [161600/175341]\n",
      "loss: 0.754153  [163200/175341]\n",
      "loss: 0.195574  [164800/175341]\n",
      "loss: 0.400375  [166400/175341]\n",
      "loss: 0.631475  [168000/175341]\n",
      "loss: 0.370917  [169600/175341]\n",
      "loss: 0.464393  [171200/175341]\n",
      "loss: 0.154926  [172800/175341]\n",
      "loss: 0.694856  [174400/175341]\n",
      "Train Accuracy: 80.6309%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.576649, F1-score: 75.83%, Macro_F1-Score:  40.70%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.650238  [    0/175341]\n",
      "loss: 0.605892  [ 1600/175341]\n",
      "loss: 0.141682  [ 3200/175341]\n",
      "loss: 0.395624  [ 4800/175341]\n",
      "loss: 1.139436  [ 6400/175341]\n",
      "loss: 0.769748  [ 8000/175341]\n",
      "loss: 0.285657  [ 9600/175341]\n",
      "loss: 0.491416  [11200/175341]\n",
      "loss: 0.590411  [12800/175341]\n",
      "loss: 0.542951  [14400/175341]\n",
      "loss: 0.432074  [16000/175341]\n",
      "loss: 0.398565  [17600/175341]\n",
      "loss: 0.400766  [19200/175341]\n",
      "loss: 0.087336  [20800/175341]\n",
      "loss: 0.347358  [22400/175341]\n",
      "loss: 0.502509  [24000/175341]\n",
      "loss: 0.688199  [25600/175341]\n",
      "loss: 0.465203  [27200/175341]\n",
      "loss: 0.392002  [28800/175341]\n",
      "loss: 0.655871  [30400/175341]\n",
      "loss: 0.322715  [32000/175341]\n",
      "loss: 0.714268  [33600/175341]\n",
      "loss: 0.244999  [35200/175341]\n",
      "loss: 0.695412  [36800/175341]\n",
      "loss: 0.611374  [38400/175341]\n",
      "loss: 0.140847  [40000/175341]\n",
      "loss: 0.290870  [41600/175341]\n",
      "loss: 0.673216  [43200/175341]\n",
      "loss: 0.447902  [44800/175341]\n",
      "loss: 0.887336  [46400/175341]\n",
      "loss: 1.150798  [48000/175341]\n",
      "loss: 0.959972  [49600/175341]\n",
      "loss: 0.267587  [51200/175341]\n",
      "loss: 0.181865  [52800/175341]\n",
      "loss: 0.566627  [54400/175341]\n",
      "loss: 0.510620  [56000/175341]\n",
      "loss: 0.438142  [57600/175341]\n",
      "loss: 0.678231  [59200/175341]\n",
      "loss: 0.546933  [60800/175341]\n",
      "loss: 1.226220  [62400/175341]\n",
      "loss: 0.485198  [64000/175341]\n",
      "loss: 0.427493  [65600/175341]\n",
      "loss: 0.121349  [67200/175341]\n",
      "loss: 0.621272  [68800/175341]\n",
      "loss: 0.283227  [70400/175341]\n",
      "loss: 0.571396  [72000/175341]\n",
      "loss: 0.337029  [73600/175341]\n",
      "loss: 0.606491  [75200/175341]\n",
      "loss: 0.644468  [76800/175341]\n",
      "loss: 0.439010  [78400/175341]\n",
      "loss: 0.956053  [80000/175341]\n",
      "loss: 0.323598  [81600/175341]\n",
      "loss: 0.386973  [83200/175341]\n",
      "loss: 0.494681  [84800/175341]\n",
      "loss: 0.187452  [86400/175341]\n",
      "loss: 0.346408  [88000/175341]\n",
      "loss: 0.434771  [89600/175341]\n",
      "loss: 0.746533  [91200/175341]\n",
      "loss: 0.716336  [92800/175341]\n",
      "loss: 0.407943  [94400/175341]\n",
      "loss: 0.382861  [96000/175341]\n",
      "loss: 0.232615  [97600/175341]\n",
      "loss: 0.461213  [99200/175341]\n",
      "loss: 0.573285  [100800/175341]\n",
      "loss: 0.215976  [102400/175341]\n",
      "loss: 0.286887  [104000/175341]\n",
      "loss: 0.605877  [105600/175341]\n",
      "loss: 0.411156  [107200/175341]\n",
      "loss: 0.603032  [108800/175341]\n",
      "loss: 0.242943  [110400/175341]\n",
      "loss: 0.360967  [112000/175341]\n",
      "loss: 1.033083  [113600/175341]\n",
      "loss: 0.327964  [115200/175341]\n",
      "loss: 0.576147  [116800/175341]\n",
      "loss: 0.645644  [118400/175341]\n",
      "loss: 0.323082  [120000/175341]\n",
      "loss: 0.581772  [121600/175341]\n",
      "loss: 0.427893  [123200/175341]\n",
      "loss: 0.338416  [124800/175341]\n",
      "loss: 0.665095  [126400/175341]\n",
      "loss: 0.213506  [128000/175341]\n",
      "loss: 0.529455  [129600/175341]\n",
      "loss: 0.383643  [131200/175341]\n",
      "loss: 0.381529  [132800/175341]\n",
      "loss: 0.732265  [134400/175341]\n",
      "loss: 0.302105  [136000/175341]\n",
      "loss: 0.856524  [137600/175341]\n",
      "loss: 0.444592  [139200/175341]\n",
      "loss: 0.559892  [140800/175341]\n",
      "loss: 0.590027  [142400/175341]\n",
      "loss: 0.851555  [144000/175341]\n",
      "loss: 0.755569  [145600/175341]\n",
      "loss: 0.643047  [147200/175341]\n",
      "loss: 0.252627  [148800/175341]\n",
      "loss: 0.077579  [150400/175341]\n",
      "loss: 0.570810  [152000/175341]\n",
      "loss: 0.222783  [153600/175341]\n",
      "loss: 0.345604  [155200/175341]\n",
      "loss: 0.541818  [156800/175341]\n",
      "loss: 0.674513  [158400/175341]\n",
      "loss: 1.075194  [160000/175341]\n",
      "loss: 0.353123  [161600/175341]\n",
      "loss: 0.535599  [163200/175341]\n",
      "loss: 0.661794  [164800/175341]\n",
      "loss: 0.434520  [166400/175341]\n",
      "loss: 0.704494  [168000/175341]\n",
      "loss: 0.403230  [169600/175341]\n",
      "loss: 0.747709  [171200/175341]\n",
      "loss: 0.498336  [172800/175341]\n",
      "loss: 0.444639  [174400/175341]\n",
      "Train Accuracy: 80.6828%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.577109, F1-score: 75.14%, Macro_F1-Score:  40.36%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.249919  [    0/175341]\n",
      "loss: 0.410053  [ 1600/175341]\n",
      "loss: 0.682608  [ 3200/175341]\n",
      "loss: 0.239950  [ 4800/175341]\n",
      "loss: 0.475624  [ 6400/175341]\n",
      "loss: 0.358427  [ 8000/175341]\n",
      "loss: 0.471897  [ 9600/175341]\n",
      "loss: 0.232290  [11200/175341]\n",
      "loss: 0.658492  [12800/175341]\n",
      "loss: 0.441776  [14400/175341]\n",
      "loss: 0.574925  [16000/175341]\n",
      "loss: 0.184527  [17600/175341]\n",
      "loss: 0.472827  [19200/175341]\n",
      "loss: 0.241998  [20800/175341]\n",
      "loss: 0.480560  [22400/175341]\n",
      "loss: 0.351474  [24000/175341]\n",
      "loss: 0.452939  [25600/175341]\n",
      "loss: 0.645777  [27200/175341]\n",
      "loss: 0.225828  [28800/175341]\n",
      "loss: 0.664411  [30400/175341]\n",
      "loss: 0.339646  [32000/175341]\n",
      "loss: 0.633647  [33600/175341]\n",
      "loss: 0.674968  [35200/175341]\n",
      "loss: 0.372563  [36800/175341]\n",
      "loss: 0.566004  [38400/175341]\n",
      "loss: 0.712004  [40000/175341]\n",
      "loss: 0.492135  [41600/175341]\n",
      "loss: 0.659134  [43200/175341]\n",
      "loss: 0.533446  [44800/175341]\n",
      "loss: 0.457210  [46400/175341]\n",
      "loss: 0.680055  [48000/175341]\n",
      "loss: 0.939022  [49600/175341]\n",
      "loss: 0.670494  [51200/175341]\n",
      "loss: 0.330469  [52800/175341]\n",
      "loss: 0.588784  [54400/175341]\n",
      "loss: 0.238971  [56000/175341]\n",
      "loss: 0.526424  [57600/175341]\n",
      "loss: 0.412874  [59200/175341]\n",
      "loss: 0.493598  [60800/175341]\n",
      "loss: 0.291470  [62400/175341]\n",
      "loss: 0.775531  [64000/175341]\n",
      "loss: 0.484061  [65600/175341]\n",
      "loss: 0.403773  [67200/175341]\n",
      "loss: 0.352539  [68800/175341]\n",
      "loss: 0.273064  [70400/175341]\n",
      "loss: 0.186101  [72000/175341]\n",
      "loss: 0.468067  [73600/175341]\n",
      "loss: 0.546255  [75200/175341]\n",
      "loss: 0.258702  [76800/175341]\n",
      "loss: 0.256200  [78400/175341]\n",
      "loss: 0.341489  [80000/175341]\n",
      "loss: 0.420067  [81600/175341]\n",
      "loss: 0.638440  [83200/175341]\n",
      "loss: 0.439263  [84800/175341]\n",
      "loss: 0.357343  [86400/175341]\n",
      "loss: 0.419982  [88000/175341]\n",
      "loss: 0.402465  [89600/175341]\n",
      "loss: 0.526569  [91200/175341]\n",
      "loss: 0.380130  [92800/175341]\n",
      "loss: 0.432493  [94400/175341]\n",
      "loss: 0.726816  [96000/175341]\n",
      "loss: 0.290678  [97600/175341]\n",
      "loss: 0.294394  [99200/175341]\n",
      "loss: 0.079597  [100800/175341]\n",
      "loss: 0.428912  [102400/175341]\n",
      "loss: 0.738008  [104000/175341]\n",
      "loss: 0.280631  [105600/175341]\n",
      "loss: 0.567775  [107200/175341]\n",
      "loss: 0.209075  [108800/175341]\n",
      "loss: 0.606532  [110400/175341]\n",
      "loss: 0.703961  [112000/175341]\n",
      "loss: 0.373145  [113600/175341]\n",
      "loss: 0.271352  [115200/175341]\n",
      "loss: 0.791617  [116800/175341]\n",
      "loss: 0.650002  [118400/175341]\n",
      "loss: 0.225926  [120000/175341]\n",
      "loss: 0.886816  [121600/175341]\n",
      "loss: 0.604660  [123200/175341]\n",
      "loss: 0.470177  [124800/175341]\n",
      "loss: 0.260580  [126400/175341]\n",
      "loss: 0.471819  [128000/175341]\n",
      "loss: 0.374527  [129600/175341]\n",
      "loss: 0.717962  [131200/175341]\n",
      "loss: 0.361150  [132800/175341]\n",
      "loss: 0.560709  [134400/175341]\n",
      "loss: 0.304913  [136000/175341]\n",
      "loss: 0.562360  [137600/175341]\n",
      "loss: 0.629420  [139200/175341]\n",
      "loss: 0.429449  [140800/175341]\n",
      "loss: 0.378658  [142400/175341]\n",
      "loss: 0.431715  [144000/175341]\n",
      "loss: 0.778917  [145600/175341]\n",
      "loss: 0.543119  [147200/175341]\n",
      "loss: 0.498269  [148800/175341]\n",
      "loss: 0.303539  [150400/175341]\n",
      "loss: 0.398249  [152000/175341]\n",
      "loss: 0.303470  [153600/175341]\n",
      "loss: 0.362585  [155200/175341]\n",
      "loss: 0.431536  [156800/175341]\n",
      "loss: 0.739818  [158400/175341]\n",
      "loss: 0.567728  [160000/175341]\n",
      "loss: 0.232092  [161600/175341]\n",
      "loss: 0.494276  [163200/175341]\n",
      "loss: 0.928412  [164800/175341]\n",
      "loss: 0.526149  [166400/175341]\n",
      "loss: 0.618138  [168000/175341]\n",
      "loss: 0.216039  [169600/175341]\n",
      "loss: 0.642283  [171200/175341]\n",
      "loss: 0.491167  [172800/175341]\n",
      "loss: 0.352967  [174400/175341]\n",
      "Train Accuracy: 80.7347%\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.601789, F1-score: 74.35%, Macro_F1-Score:  40.47%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.295686  [    0/175341]\n",
      "loss: 0.415195  [ 1600/175341]\n",
      "loss: 0.434989  [ 3200/175341]\n",
      "loss: 0.553889  [ 4800/175341]\n",
      "loss: 0.267080  [ 6400/175341]\n",
      "loss: 0.424229  [ 8000/175341]\n",
      "loss: 0.143482  [ 9600/175341]\n",
      "loss: 0.302225  [11200/175341]\n",
      "loss: 0.362172  [12800/175341]\n",
      "loss: 0.380215  [14400/175341]\n",
      "loss: 0.375363  [16000/175341]\n",
      "loss: 0.189820  [17600/175341]\n",
      "loss: 0.419096  [19200/175341]\n",
      "loss: 0.842319  [20800/175341]\n",
      "loss: 0.500683  [22400/175341]\n",
      "loss: 0.636395  [24000/175341]\n",
      "loss: 0.588406  [25600/175341]\n",
      "loss: 0.322849  [27200/175341]\n",
      "loss: 0.797876  [28800/175341]\n",
      "loss: 0.530669  [30400/175341]\n",
      "loss: 0.728011  [32000/175341]\n",
      "loss: 0.521432  [33600/175341]\n",
      "loss: 0.740915  [35200/175341]\n",
      "loss: 0.395806  [36800/175341]\n",
      "loss: 0.425241  [38400/175341]\n",
      "loss: 0.710782  [40000/175341]\n",
      "loss: 0.325742  [41600/175341]\n",
      "loss: 0.127317  [43200/175341]\n",
      "loss: 0.486409  [44800/175341]\n",
      "loss: 0.448063  [46400/175341]\n",
      "loss: 1.115738  [48000/175341]\n",
      "loss: 0.729291  [49600/175341]\n",
      "loss: 0.428154  [51200/175341]\n",
      "loss: 0.409391  [52800/175341]\n",
      "loss: 0.362334  [54400/175341]\n",
      "loss: 0.461110  [56000/175341]\n",
      "loss: 0.230531  [57600/175341]\n",
      "loss: 0.636476  [59200/175341]\n",
      "loss: 0.342142  [60800/175341]\n",
      "loss: 0.431639  [62400/175341]\n",
      "loss: 0.685319  [64000/175341]\n",
      "loss: 0.141714  [65600/175341]\n",
      "loss: 0.423908  [67200/175341]\n",
      "loss: 0.632434  [68800/175341]\n",
      "loss: 0.596091  [70400/175341]\n",
      "loss: 0.405753  [72000/175341]\n",
      "loss: 0.441627  [73600/175341]\n",
      "loss: 0.590275  [75200/175341]\n",
      "loss: 0.345958  [76800/175341]\n",
      "loss: 0.445736  [78400/175341]\n",
      "loss: 0.481958  [80000/175341]\n",
      "loss: 0.198086  [81600/175341]\n",
      "loss: 0.485977  [83200/175341]\n",
      "loss: 0.280000  [84800/175341]\n",
      "loss: 0.198157  [86400/175341]\n",
      "loss: 0.869294  [88000/175341]\n",
      "loss: 0.505162  [89600/175341]\n",
      "loss: 0.420781  [91200/175341]\n",
      "loss: 0.533330  [92800/175341]\n",
      "loss: 0.671247  [94400/175341]\n",
      "loss: 0.804531  [96000/175341]\n",
      "loss: 0.618369  [97600/175341]\n",
      "loss: 0.740731  [99200/175341]\n",
      "loss: 0.214688  [100800/175341]\n",
      "loss: 0.750694  [102400/175341]\n",
      "loss: 0.609239  [104000/175341]\n",
      "loss: 0.339041  [105600/175341]\n",
      "loss: 0.577367  [107200/175341]\n",
      "loss: 0.878551  [108800/175341]\n",
      "loss: 0.217098  [110400/175341]\n",
      "loss: 0.759488  [112000/175341]\n",
      "loss: 0.722595  [113600/175341]\n",
      "loss: 0.403014  [115200/175341]\n",
      "loss: 0.322900  [116800/175341]\n",
      "loss: 0.338635  [118400/175341]\n",
      "loss: 0.566918  [120000/175341]\n",
      "loss: 0.292962  [121600/175341]\n",
      "loss: 0.290194  [123200/175341]\n",
      "loss: 0.379240  [124800/175341]\n",
      "loss: 0.749641  [126400/175341]\n",
      "loss: 0.262669  [128000/175341]\n",
      "loss: 0.497908  [129600/175341]\n",
      "loss: 0.508706  [131200/175341]\n",
      "loss: 0.610155  [132800/175341]\n",
      "loss: 0.678373  [134400/175341]\n",
      "loss: 0.427709  [136000/175341]\n",
      "loss: 0.944731  [137600/175341]\n",
      "loss: 0.390131  [139200/175341]\n",
      "loss: 0.719972  [140800/175341]\n",
      "loss: 1.083188  [142400/175341]\n",
      "loss: 0.600340  [144000/175341]\n",
      "loss: 0.724245  [145600/175341]\n",
      "loss: 0.693460  [147200/175341]\n",
      "loss: 0.441181  [148800/175341]\n",
      "loss: 1.069390  [150400/175341]\n",
      "loss: 0.635935  [152000/175341]\n",
      "loss: 0.885802  [153600/175341]\n",
      "loss: 0.255926  [155200/175341]\n",
      "loss: 0.364991  [156800/175341]\n",
      "loss: 0.324468  [158400/175341]\n",
      "loss: 0.724695  [160000/175341]\n",
      "loss: 0.250295  [161600/175341]\n",
      "loss: 0.456306  [163200/175341]\n",
      "loss: 0.413240  [164800/175341]\n",
      "loss: 0.505483  [166400/175341]\n",
      "loss: 0.910778  [168000/175341]\n",
      "loss: 0.585830  [169600/175341]\n",
      "loss: 0.537079  [171200/175341]\n",
      "loss: 0.398993  [172800/175341]\n",
      "loss: 0.429153  [174400/175341]\n",
      "Train Accuracy: 80.7261%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.591420, F1-score: 74.81%, Macro_F1-Score:  39.97%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.509425  [    0/175341]\n",
      "loss: 0.604124  [ 1600/175341]\n",
      "loss: 0.292861  [ 3200/175341]\n",
      "loss: 0.382754  [ 4800/175341]\n",
      "loss: 0.524892  [ 6400/175341]\n",
      "loss: 0.837733  [ 8000/175341]\n",
      "loss: 0.488974  [ 9600/175341]\n",
      "loss: 0.846548  [11200/175341]\n",
      "loss: 0.418764  [12800/175341]\n",
      "loss: 0.192627  [14400/175341]\n",
      "loss: 0.411592  [16000/175341]\n",
      "loss: 0.294064  [17600/175341]\n",
      "loss: 0.508760  [19200/175341]\n",
      "loss: 0.587745  [20800/175341]\n",
      "loss: 0.191927  [22400/175341]\n",
      "loss: 0.288465  [24000/175341]\n",
      "loss: 0.747001  [25600/175341]\n",
      "loss: 0.474911  [27200/175341]\n",
      "loss: 0.402515  [28800/175341]\n",
      "loss: 0.525742  [30400/175341]\n",
      "loss: 0.666920  [32000/175341]\n",
      "loss: 0.607834  [33600/175341]\n",
      "loss: 0.985644  [35200/175341]\n",
      "loss: 0.574899  [36800/175341]\n",
      "loss: 0.254614  [38400/175341]\n",
      "loss: 0.698073  [40000/175341]\n",
      "loss: 0.537930  [41600/175341]\n",
      "loss: 0.890115  [43200/175341]\n",
      "loss: 0.742487  [44800/175341]\n",
      "loss: 0.425784  [46400/175341]\n",
      "loss: 0.262330  [48000/175341]\n",
      "loss: 0.209382  [49600/175341]\n",
      "loss: 0.561750  [51200/175341]\n",
      "loss: 0.510305  [52800/175341]\n",
      "loss: 0.509511  [54400/175341]\n",
      "loss: 0.481853  [56000/175341]\n",
      "loss: 0.612268  [57600/175341]\n",
      "loss: 0.372643  [59200/175341]\n",
      "loss: 0.663640  [60800/175341]\n",
      "loss: 0.453698  [62400/175341]\n",
      "loss: 0.455056  [64000/175341]\n",
      "loss: 0.245720  [65600/175341]\n",
      "loss: 0.390407  [67200/175341]\n",
      "loss: 0.524852  [68800/175341]\n",
      "loss: 0.820047  [70400/175341]\n",
      "loss: 0.491897  [72000/175341]\n",
      "loss: 0.594535  [73600/175341]\n",
      "loss: 0.240630  [75200/175341]\n",
      "loss: 0.199712  [76800/175341]\n",
      "loss: 0.395177  [78400/175341]\n",
      "loss: 0.377924  [80000/175341]\n",
      "loss: 0.402527  [81600/175341]\n",
      "loss: 0.151013  [83200/175341]\n",
      "loss: 0.706994  [84800/175341]\n",
      "loss: 0.779986  [86400/175341]\n",
      "loss: 0.411511  [88000/175341]\n",
      "loss: 1.094452  [89600/175341]\n",
      "loss: 0.241247  [91200/175341]\n",
      "loss: 0.497394  [92800/175341]\n",
      "loss: 0.396544  [94400/175341]\n",
      "loss: 0.312276  [96000/175341]\n",
      "loss: 0.279226  [97600/175341]\n",
      "loss: 0.649133  [99200/175341]\n",
      "loss: 0.554912  [100800/175341]\n",
      "loss: 0.293728  [102400/175341]\n",
      "loss: 0.439907  [104000/175341]\n",
      "loss: 0.438791  [105600/175341]\n",
      "loss: 0.822527  [107200/175341]\n",
      "loss: 0.547245  [108800/175341]\n",
      "loss: 0.188102  [110400/175341]\n",
      "loss: 0.343406  [112000/175341]\n",
      "loss: 0.393527  [113600/175341]\n",
      "loss: 0.659822  [115200/175341]\n",
      "loss: 0.214525  [116800/175341]\n",
      "loss: 0.451989  [118400/175341]\n",
      "loss: 0.464945  [120000/175341]\n",
      "loss: 0.307917  [121600/175341]\n",
      "loss: 0.120516  [123200/175341]\n",
      "loss: 0.555541  [124800/175341]\n",
      "loss: 0.642651  [126400/175341]\n",
      "loss: 0.254359  [128000/175341]\n",
      "loss: 0.629172  [129600/175341]\n",
      "loss: 1.110265  [131200/175341]\n",
      "loss: 0.643257  [132800/175341]\n",
      "loss: 0.375894  [134400/175341]\n",
      "loss: 0.685956  [136000/175341]\n",
      "loss: 0.147911  [137600/175341]\n",
      "loss: 0.325734  [139200/175341]\n",
      "loss: 0.343538  [140800/175341]\n",
      "loss: 0.397218  [142400/175341]\n",
      "loss: 0.400279  [144000/175341]\n",
      "loss: 0.594586  [145600/175341]\n",
      "loss: 0.175115  [147200/175341]\n",
      "loss: 0.940839  [148800/175341]\n",
      "loss: 0.442393  [150400/175341]\n",
      "loss: 0.545528  [152000/175341]\n",
      "loss: 0.553060  [153600/175341]\n",
      "loss: 0.334176  [155200/175341]\n",
      "loss: 0.515168  [156800/175341]\n",
      "loss: 0.256055  [158400/175341]\n",
      "loss: 0.129298  [160000/175341]\n",
      "loss: 0.171892  [161600/175341]\n",
      "loss: 0.579486  [163200/175341]\n",
      "loss: 0.580031  [164800/175341]\n",
      "loss: 0.421676  [166400/175341]\n",
      "loss: 0.674466  [168000/175341]\n",
      "loss: 0.821893  [169600/175341]\n",
      "loss: 0.507491  [171200/175341]\n",
      "loss: 0.245523  [172800/175341]\n",
      "loss: 0.249387  [174400/175341]\n",
      "Train Accuracy: 80.7769%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.573798, F1-score: 76.10%, Macro_F1-Score:  40.44%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.703311  [    0/175341]\n",
      "loss: 0.557841  [ 1600/175341]\n",
      "loss: 0.844564  [ 3200/175341]\n",
      "loss: 0.609788  [ 4800/175341]\n",
      "loss: 0.709757  [ 6400/175341]\n",
      "loss: 0.544308  [ 8000/175341]\n",
      "loss: 0.282936  [ 9600/175341]\n",
      "loss: 0.328641  [11200/175341]\n",
      "loss: 0.494307  [12800/175341]\n",
      "loss: 0.908169  [14400/175341]\n",
      "loss: 0.448724  [16000/175341]\n",
      "loss: 0.547558  [17600/175341]\n",
      "loss: 0.409177  [19200/175341]\n",
      "loss: 0.625961  [20800/175341]\n",
      "loss: 0.361066  [22400/175341]\n",
      "loss: 0.871916  [24000/175341]\n",
      "loss: 0.138202  [25600/175341]\n",
      "loss: 0.454032  [27200/175341]\n",
      "loss: 0.710797  [28800/175341]\n",
      "loss: 0.478738  [30400/175341]\n",
      "loss: 0.505445  [32000/175341]\n",
      "loss: 0.135230  [33600/175341]\n",
      "loss: 0.464422  [35200/175341]\n",
      "loss: 0.534752  [36800/175341]\n",
      "loss: 0.541714  [38400/175341]\n",
      "loss: 0.581589  [40000/175341]\n",
      "loss: 0.414633  [41600/175341]\n",
      "loss: 0.417380  [43200/175341]\n",
      "loss: 0.238806  [44800/175341]\n",
      "loss: 1.323125  [46400/175341]\n",
      "loss: 0.413163  [48000/175341]\n",
      "loss: 0.499269  [49600/175341]\n",
      "loss: 0.353257  [51200/175341]\n",
      "loss: 0.321405  [52800/175341]\n",
      "loss: 0.416792  [54400/175341]\n",
      "loss: 0.591973  [56000/175341]\n",
      "loss: 0.445537  [57600/175341]\n",
      "loss: 0.296253  [59200/175341]\n",
      "loss: 0.109606  [60800/175341]\n",
      "loss: 0.678669  [62400/175341]\n",
      "loss: 0.421333  [64000/175341]\n",
      "loss: 0.941464  [65600/175341]\n",
      "loss: 0.674815  [67200/175341]\n",
      "loss: 0.383933  [68800/175341]\n",
      "loss: 0.186355  [70400/175341]\n",
      "loss: 0.365549  [72000/175341]\n",
      "loss: 0.550601  [73600/175341]\n",
      "loss: 0.609375  [75200/175341]\n",
      "loss: 0.270748  [76800/175341]\n",
      "loss: 0.342407  [78400/175341]\n",
      "loss: 0.411143  [80000/175341]\n",
      "loss: 0.550366  [81600/175341]\n",
      "loss: 0.331018  [83200/175341]\n",
      "loss: 0.697570  [84800/175341]\n",
      "loss: 0.705122  [86400/175341]\n",
      "loss: 0.398210  [88000/175341]\n",
      "loss: 0.722046  [89600/175341]\n",
      "loss: 0.688378  [91200/175341]\n",
      "loss: 0.622283  [92800/175341]\n",
      "loss: 0.517697  [94400/175341]\n",
      "loss: 0.685640  [96000/175341]\n",
      "loss: 0.512148  [97600/175341]\n",
      "loss: 0.362189  [99200/175341]\n",
      "loss: 0.430845  [100800/175341]\n",
      "loss: 0.325379  [102400/175341]\n",
      "loss: 0.386571  [104000/175341]\n",
      "loss: 0.034270  [105600/175341]\n",
      "loss: 0.068203  [107200/175341]\n",
      "loss: 0.409817  [108800/175341]\n",
      "loss: 0.497621  [110400/175341]\n",
      "loss: 0.604745  [112000/175341]\n",
      "loss: 0.285538  [113600/175341]\n",
      "loss: 0.803073  [115200/175341]\n",
      "loss: 0.320183  [116800/175341]\n",
      "loss: 0.623484  [118400/175341]\n",
      "loss: 0.391113  [120000/175341]\n",
      "loss: 0.274384  [121600/175341]\n",
      "loss: 0.325195  [123200/175341]\n",
      "loss: 0.821985  [124800/175341]\n",
      "loss: 0.416282  [126400/175341]\n",
      "loss: 0.587027  [128000/175341]\n",
      "loss: 0.408377  [129600/175341]\n",
      "loss: 0.591870  [131200/175341]\n",
      "loss: 0.532593  [132800/175341]\n",
      "loss: 0.393719  [134400/175341]\n",
      "loss: 0.615593  [136000/175341]\n",
      "loss: 0.975910  [137600/175341]\n",
      "loss: 0.355070  [139200/175341]\n",
      "loss: 0.709649  [140800/175341]\n",
      "loss: 0.457738  [142400/175341]\n",
      "loss: 0.440631  [144000/175341]\n",
      "loss: 0.214261  [145600/175341]\n",
      "loss: 0.636594  [147200/175341]\n",
      "loss: 0.356214  [148800/175341]\n",
      "loss: 0.871667  [150400/175341]\n",
      "loss: 0.549579  [152000/175341]\n",
      "loss: 0.419638  [153600/175341]\n",
      "loss: 0.575403  [155200/175341]\n",
      "loss: 0.285172  [156800/175341]\n",
      "loss: 0.338293  [158400/175341]\n",
      "loss: 0.370838  [160000/175341]\n",
      "loss: 0.395051  [161600/175341]\n",
      "loss: 0.881874  [163200/175341]\n",
      "loss: 0.545853  [164800/175341]\n",
      "loss: 0.628000  [166400/175341]\n",
      "loss: 0.825117  [168000/175341]\n",
      "loss: 0.452375  [169600/175341]\n",
      "loss: 0.719708  [171200/175341]\n",
      "loss: 1.273292  [172800/175341]\n",
      "loss: 0.797063  [174400/175341]\n",
      "Train Accuracy: 80.7872%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.584819, F1-score: 75.40%, Macro_F1-Score:  40.03%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.144170  [    0/175341]\n",
      "loss: 0.448080  [ 1600/175341]\n",
      "loss: 0.658359  [ 3200/175341]\n",
      "loss: 0.183656  [ 4800/175341]\n",
      "loss: 0.259571  [ 6400/175341]\n",
      "loss: 0.643368  [ 8000/175341]\n",
      "loss: 0.684416  [ 9600/175341]\n",
      "loss: 0.590064  [11200/175341]\n",
      "loss: 0.351802  [12800/175341]\n",
      "loss: 0.436799  [14400/175341]\n",
      "loss: 0.221146  [16000/175341]\n",
      "loss: 0.235756  [17600/175341]\n",
      "loss: 0.351196  [19200/175341]\n",
      "loss: 0.445472  [20800/175341]\n",
      "loss: 0.432655  [22400/175341]\n",
      "loss: 0.761536  [24000/175341]\n",
      "loss: 0.407576  [25600/175341]\n",
      "loss: 0.551378  [27200/175341]\n",
      "loss: 0.376196  [28800/175341]\n",
      "loss: 0.572208  [30400/175341]\n",
      "loss: 0.708575  [32000/175341]\n",
      "loss: 0.700091  [33600/175341]\n",
      "loss: 0.663960  [35200/175341]\n",
      "loss: 0.562752  [36800/175341]\n",
      "loss: 0.849856  [38400/175341]\n",
      "loss: 0.174372  [40000/175341]\n",
      "loss: 0.323384  [41600/175341]\n",
      "loss: 0.527207  [43200/175341]\n",
      "loss: 0.270427  [44800/175341]\n",
      "loss: 1.202700  [46400/175341]\n",
      "loss: 0.329332  [48000/175341]\n",
      "loss: 0.635521  [49600/175341]\n",
      "loss: 0.313947  [51200/175341]\n",
      "loss: 0.566203  [52800/175341]\n",
      "loss: 0.551171  [54400/175341]\n",
      "loss: 0.545156  [56000/175341]\n",
      "loss: 0.288347  [57600/175341]\n",
      "loss: 0.163991  [59200/175341]\n",
      "loss: 0.524912  [60800/175341]\n",
      "loss: 0.318393  [62400/175341]\n",
      "loss: 0.585013  [64000/175341]\n",
      "loss: 0.772033  [65600/175341]\n",
      "loss: 0.799795  [67200/175341]\n",
      "loss: 0.612265  [68800/175341]\n",
      "loss: 0.619438  [70400/175341]\n",
      "loss: 0.666580  [72000/175341]\n",
      "loss: 0.277210  [73600/175341]\n",
      "loss: 0.309835  [75200/175341]\n",
      "loss: 0.549154  [76800/175341]\n",
      "loss: 0.422775  [78400/175341]\n",
      "loss: 0.273798  [80000/175341]\n",
      "loss: 1.023370  [81600/175341]\n",
      "loss: 0.614366  [83200/175341]\n",
      "loss: 0.457308  [84800/175341]\n",
      "loss: 0.222044  [86400/175341]\n",
      "loss: 0.113875  [88000/175341]\n",
      "loss: 0.261172  [89600/175341]\n",
      "loss: 0.356420  [91200/175341]\n",
      "loss: 0.213168  [92800/175341]\n",
      "loss: 0.423123  [94400/175341]\n",
      "loss: 0.367673  [96000/175341]\n",
      "loss: 0.563132  [97600/175341]\n",
      "loss: 0.433045  [99200/175341]\n",
      "loss: 0.226491  [100800/175341]\n",
      "loss: 0.713799  [102400/175341]\n",
      "loss: 0.516333  [104000/175341]\n",
      "loss: 0.575916  [105600/175341]\n",
      "loss: 0.243403  [107200/175341]\n",
      "loss: 0.486193  [108800/175341]\n",
      "loss: 1.002681  [110400/175341]\n",
      "loss: 0.349289  [112000/175341]\n",
      "loss: 0.453383  [113600/175341]\n",
      "loss: 0.535281  [115200/175341]\n",
      "loss: 0.751173  [116800/175341]\n",
      "loss: 0.896344  [118400/175341]\n",
      "loss: 0.726114  [120000/175341]\n",
      "loss: 0.313685  [121600/175341]\n",
      "loss: 0.526483  [123200/175341]\n",
      "loss: 0.425210  [124800/175341]\n",
      "loss: 0.394450  [126400/175341]\n",
      "loss: 0.105535  [128000/175341]\n",
      "loss: 0.437899  [129600/175341]\n",
      "loss: 0.438523  [131200/175341]\n",
      "loss: 0.290156  [132800/175341]\n",
      "loss: 0.420976  [134400/175341]\n",
      "loss: 0.741418  [136000/175341]\n",
      "loss: 0.429225  [137600/175341]\n",
      "loss: 0.576563  [139200/175341]\n",
      "loss: 0.523908  [140800/175341]\n",
      "loss: 0.625802  [142400/175341]\n",
      "loss: 1.044471  [144000/175341]\n",
      "loss: 0.508607  [145600/175341]\n",
      "loss: 0.176341  [147200/175341]\n",
      "loss: 0.426486  [148800/175341]\n",
      "loss: 0.702968  [150400/175341]\n",
      "loss: 0.335712  [152000/175341]\n",
      "loss: 0.568891  [153600/175341]\n",
      "loss: 0.197733  [155200/175341]\n",
      "loss: 0.203883  [156800/175341]\n",
      "loss: 0.610102  [158400/175341]\n",
      "loss: 0.471583  [160000/175341]\n",
      "loss: 0.509634  [161600/175341]\n",
      "loss: 0.142380  [163200/175341]\n",
      "loss: 0.368891  [164800/175341]\n",
      "loss: 0.687696  [166400/175341]\n",
      "loss: 0.568230  [168000/175341]\n",
      "loss: 0.484115  [169600/175341]\n",
      "loss: 1.221105  [171200/175341]\n",
      "loss: 0.352683  [172800/175341]\n",
      "loss: 0.777297  [174400/175341]\n",
      "Train Accuracy: 80.7575%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.578199, F1-score: 74.81%, Macro_F1-Score:  39.49%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.513665  [    0/175341]\n",
      "loss: 0.797804  [ 1600/175341]\n",
      "loss: 0.359143  [ 3200/175341]\n",
      "loss: 0.415510  [ 4800/175341]\n",
      "loss: 0.463723  [ 6400/175341]\n",
      "loss: 0.370262  [ 8000/175341]\n",
      "loss: 0.394384  [ 9600/175341]\n",
      "loss: 0.478163  [11200/175341]\n",
      "loss: 0.393687  [12800/175341]\n",
      "loss: 0.463675  [14400/175341]\n",
      "loss: 0.418579  [16000/175341]\n",
      "loss: 1.040989  [17600/175341]\n",
      "loss: 0.458350  [19200/175341]\n",
      "loss: 0.331490  [20800/175341]\n",
      "loss: 0.420084  [22400/175341]\n",
      "loss: 0.384007  [24000/175341]\n",
      "loss: 0.809628  [25600/175341]\n",
      "loss: 0.726975  [27200/175341]\n",
      "loss: 0.498659  [28800/175341]\n",
      "loss: 0.324803  [30400/175341]\n",
      "loss: 1.099424  [32000/175341]\n",
      "loss: 1.385867  [33600/175341]\n",
      "loss: 0.587400  [35200/175341]\n",
      "loss: 0.134872  [36800/175341]\n",
      "loss: 0.226653  [38400/175341]\n",
      "loss: 0.299617  [40000/175341]\n",
      "loss: 0.433865  [41600/175341]\n",
      "loss: 0.602820  [43200/175341]\n",
      "loss: 0.773518  [44800/175341]\n",
      "loss: 0.446484  [46400/175341]\n",
      "loss: 0.361048  [48000/175341]\n",
      "loss: 0.341814  [49600/175341]\n",
      "loss: 0.531946  [51200/175341]\n",
      "loss: 0.577962  [52800/175341]\n",
      "loss: 0.695226  [54400/175341]\n",
      "loss: 0.362762  [56000/175341]\n",
      "loss: 0.692881  [57600/175341]\n",
      "loss: 0.342065  [59200/175341]\n",
      "loss: 0.503911  [60800/175341]\n",
      "loss: 0.329271  [62400/175341]\n",
      "loss: 0.972814  [64000/175341]\n",
      "loss: 0.479016  [65600/175341]\n",
      "loss: 0.741969  [67200/175341]\n",
      "loss: 0.304744  [68800/175341]\n",
      "loss: 0.487564  [70400/175341]\n",
      "loss: 0.323140  [72000/175341]\n",
      "loss: 0.286431  [73600/175341]\n",
      "loss: 0.736025  [75200/175341]\n",
      "loss: 0.273073  [76800/175341]\n",
      "loss: 0.404342  [78400/175341]\n",
      "loss: 0.697766  [80000/175341]\n",
      "loss: 0.441260  [81600/175341]\n",
      "loss: 0.517597  [83200/175341]\n",
      "loss: 0.673683  [84800/175341]\n",
      "loss: 0.529555  [86400/175341]\n",
      "loss: 0.671248  [88000/175341]\n",
      "loss: 0.283396  [89600/175341]\n",
      "loss: 0.599458  [91200/175341]\n",
      "loss: 0.474597  [92800/175341]\n",
      "loss: 0.505928  [94400/175341]\n",
      "loss: 0.456244  [96000/175341]\n",
      "loss: 0.692328  [97600/175341]\n",
      "loss: 0.489459  [99200/175341]\n",
      "loss: 0.879908  [100800/175341]\n",
      "loss: 0.433678  [102400/175341]\n",
      "loss: 0.526538  [104000/175341]\n",
      "loss: 0.239208  [105600/175341]\n",
      "loss: 0.240245  [107200/175341]\n",
      "loss: 0.366817  [108800/175341]\n",
      "loss: 0.432406  [110400/175341]\n",
      "loss: 0.601997  [112000/175341]\n",
      "loss: 0.615669  [113600/175341]\n",
      "loss: 0.380870  [115200/175341]\n",
      "loss: 0.322618  [116800/175341]\n",
      "loss: 0.212960  [118400/175341]\n",
      "loss: 0.256116  [120000/175341]\n",
      "loss: 0.311324  [121600/175341]\n",
      "loss: 0.490684  [123200/175341]\n",
      "loss: 0.215364  [124800/175341]\n",
      "loss: 0.359457  [126400/175341]\n",
      "loss: 0.698891  [128000/175341]\n",
      "loss: 0.362693  [129600/175341]\n",
      "loss: 0.693934  [131200/175341]\n",
      "loss: 0.360332  [132800/175341]\n",
      "loss: 0.956724  [134400/175341]\n",
      "loss: 0.871235  [136000/175341]\n",
      "loss: 0.545934  [137600/175341]\n",
      "loss: 0.910523  [139200/175341]\n",
      "loss: 0.128696  [140800/175341]\n",
      "loss: 0.711027  [142400/175341]\n",
      "loss: 0.856482  [144000/175341]\n",
      "loss: 0.521453  [145600/175341]\n",
      "loss: 0.706449  [147200/175341]\n",
      "loss: 0.455371  [148800/175341]\n",
      "loss: 0.167582  [150400/175341]\n",
      "loss: 0.458023  [152000/175341]\n",
      "loss: 0.493716  [153600/175341]\n",
      "loss: 0.286208  [155200/175341]\n",
      "loss: 0.408815  [156800/175341]\n",
      "loss: 0.659868  [158400/175341]\n",
      "loss: 0.341775  [160000/175341]\n",
      "loss: 0.345886  [161600/175341]\n",
      "loss: 0.814650  [163200/175341]\n",
      "loss: 0.505332  [164800/175341]\n",
      "loss: 0.182935  [166400/175341]\n",
      "loss: 0.734288  [168000/175341]\n",
      "loss: 0.472639  [169600/175341]\n",
      "loss: 0.527325  [171200/175341]\n",
      "loss: 0.400240  [172800/175341]\n",
      "loss: 0.187229  [174400/175341]\n",
      "Train Accuracy: 80.7501%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.575625, F1-score: 75.97%, Macro_F1-Score:  39.78%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.445142  [    0/175341]\n",
      "loss: 0.316252  [ 1600/175341]\n",
      "loss: 0.437495  [ 3200/175341]\n",
      "loss: 0.474314  [ 4800/175341]\n",
      "loss: 0.451871  [ 6400/175341]\n",
      "loss: 0.316130  [ 8000/175341]\n",
      "loss: 0.107501  [ 9600/175341]\n",
      "loss: 0.625613  [11200/175341]\n",
      "loss: 0.624540  [12800/175341]\n",
      "loss: 0.621246  [14400/175341]\n",
      "loss: 0.335923  [16000/175341]\n",
      "loss: 0.681922  [17600/175341]\n",
      "loss: 0.488759  [19200/175341]\n",
      "loss: 0.489669  [20800/175341]\n",
      "loss: 0.445999  [22400/175341]\n",
      "loss: 0.512690  [24000/175341]\n",
      "loss: 0.697083  [25600/175341]\n",
      "loss: 0.392707  [27200/175341]\n",
      "loss: 0.586070  [28800/175341]\n",
      "loss: 0.700587  [30400/175341]\n",
      "loss: 0.374231  [32000/175341]\n",
      "loss: 0.337493  [33600/175341]\n",
      "loss: 0.210705  [35200/175341]\n",
      "loss: 0.452430  [36800/175341]\n",
      "loss: 0.565892  [38400/175341]\n",
      "loss: 0.943251  [40000/175341]\n",
      "loss: 0.600516  [41600/175341]\n",
      "loss: 0.453108  [43200/175341]\n",
      "loss: 0.256328  [44800/175341]\n",
      "loss: 0.387423  [46400/175341]\n",
      "loss: 0.539001  [48000/175341]\n",
      "loss: 0.840873  [49600/175341]\n",
      "loss: 0.122097  [51200/175341]\n",
      "loss: 0.306580  [52800/175341]\n",
      "loss: 0.590714  [54400/175341]\n",
      "loss: 0.435936  [56000/175341]\n",
      "loss: 0.363332  [57600/175341]\n",
      "loss: 0.530724  [59200/175341]\n",
      "loss: 0.299103  [60800/175341]\n",
      "loss: 0.687323  [62400/175341]\n",
      "loss: 0.421577  [64000/175341]\n",
      "loss: 0.483196  [65600/175341]\n",
      "loss: 0.588384  [67200/175341]\n",
      "loss: 0.582354  [68800/175341]\n",
      "loss: 0.691507  [70400/175341]\n",
      "loss: 0.453597  [72000/175341]\n",
      "loss: 0.399346  [73600/175341]\n",
      "loss: 0.639589  [75200/175341]\n",
      "loss: 0.546190  [76800/175341]\n",
      "loss: 0.397642  [78400/175341]\n",
      "loss: 0.828683  [80000/175341]\n",
      "loss: 0.650706  [81600/175341]\n",
      "loss: 0.317247  [83200/175341]\n",
      "loss: 0.494264  [84800/175341]\n",
      "loss: 0.526697  [86400/175341]\n",
      "loss: 0.359466  [88000/175341]\n",
      "loss: 0.652616  [89600/175341]\n",
      "loss: 0.407879  [91200/175341]\n",
      "loss: 0.791769  [92800/175341]\n",
      "loss: 0.353251  [94400/175341]\n",
      "loss: 0.316237  [96000/175341]\n",
      "loss: 0.573370  [97600/175341]\n",
      "loss: 0.611625  [99200/175341]\n",
      "loss: 0.392535  [100800/175341]\n",
      "loss: 0.347492  [102400/175341]\n",
      "loss: 0.367182  [104000/175341]\n",
      "loss: 0.392894  [105600/175341]\n",
      "loss: 0.520340  [107200/175341]\n",
      "loss: 0.485887  [108800/175341]\n",
      "loss: 0.612946  [110400/175341]\n",
      "loss: 0.417558  [112000/175341]\n",
      "loss: 0.508147  [113600/175341]\n",
      "loss: 0.561356  [115200/175341]\n",
      "loss: 0.404425  [116800/175341]\n",
      "loss: 0.862944  [118400/175341]\n",
      "loss: 0.560152  [120000/175341]\n",
      "loss: 0.699344  [121600/175341]\n",
      "loss: 0.567279  [123200/175341]\n",
      "loss: 0.246838  [124800/175341]\n",
      "loss: 0.725212  [126400/175341]\n",
      "loss: 0.174843  [128000/175341]\n",
      "loss: 0.471387  [129600/175341]\n",
      "loss: 0.492405  [131200/175341]\n",
      "loss: 0.155907  [132800/175341]\n",
      "loss: 0.590342  [134400/175341]\n",
      "loss: 0.556255  [136000/175341]\n",
      "loss: 0.424593  [137600/175341]\n",
      "loss: 0.487959  [139200/175341]\n",
      "loss: 0.438801  [140800/175341]\n",
      "loss: 0.735324  [142400/175341]\n",
      "loss: 0.305225  [144000/175341]\n",
      "loss: 0.509349  [145600/175341]\n",
      "loss: 0.334599  [147200/175341]\n",
      "loss: 0.419131  [148800/175341]\n",
      "loss: 0.477200  [150400/175341]\n",
      "loss: 0.655594  [152000/175341]\n",
      "loss: 0.391567  [153600/175341]\n",
      "loss: 0.396635  [155200/175341]\n",
      "loss: 0.629387  [156800/175341]\n",
      "loss: 0.676328  [158400/175341]\n",
      "loss: 0.386483  [160000/175341]\n",
      "loss: 0.146046  [161600/175341]\n",
      "loss: 0.388602  [163200/175341]\n",
      "loss: 0.504013  [164800/175341]\n",
      "loss: 0.695009  [166400/175341]\n",
      "loss: 0.364509  [168000/175341]\n",
      "loss: 0.314429  [169600/175341]\n",
      "loss: 0.342512  [171200/175341]\n",
      "loss: 0.650655  [172800/175341]\n",
      "loss: 0.343723  [174400/175341]\n",
      "Train Accuracy: 80.7723%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.571552, F1-score: 76.10%, Macro_F1-Score:  40.40%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.144508  [    0/175341]\n",
      "loss: 0.587328  [ 1600/175341]\n",
      "loss: 0.455347  [ 3200/175341]\n",
      "loss: 0.627280  [ 4800/175341]\n",
      "loss: 0.500791  [ 6400/175341]\n",
      "loss: 0.390256  [ 8000/175341]\n",
      "loss: 0.416273  [ 9600/175341]\n",
      "loss: 0.702330  [11200/175341]\n",
      "loss: 0.506362  [12800/175341]\n",
      "loss: 0.796096  [14400/175341]\n",
      "loss: 0.259652  [16000/175341]\n",
      "loss: 0.602488  [17600/175341]\n",
      "loss: 0.473862  [19200/175341]\n",
      "loss: 0.537914  [20800/175341]\n",
      "loss: 0.380553  [22400/175341]\n",
      "loss: 0.675092  [24000/175341]\n",
      "loss: 0.211381  [25600/175341]\n",
      "loss: 0.371230  [27200/175341]\n",
      "loss: 0.528106  [28800/175341]\n",
      "loss: 0.437375  [30400/175341]\n",
      "loss: 0.375602  [32000/175341]\n",
      "loss: 0.289281  [33600/175341]\n",
      "loss: 0.427895  [35200/175341]\n",
      "loss: 0.706098  [36800/175341]\n",
      "loss: 0.424975  [38400/175341]\n",
      "loss: 0.360396  [40000/175341]\n",
      "loss: 0.444822  [41600/175341]\n",
      "loss: 0.463131  [43200/175341]\n",
      "loss: 0.729943  [44800/175341]\n",
      "loss: 0.420272  [46400/175341]\n",
      "loss: 0.476814  [48000/175341]\n",
      "loss: 0.498613  [49600/175341]\n",
      "loss: 0.549573  [51200/175341]\n",
      "loss: 0.452578  [52800/175341]\n",
      "loss: 0.418321  [54400/175341]\n",
      "loss: 0.751218  [56000/175341]\n",
      "loss: 0.569262  [57600/175341]\n",
      "loss: 0.345044  [59200/175341]\n",
      "loss: 0.223962  [60800/175341]\n",
      "loss: 0.696994  [62400/175341]\n",
      "loss: 0.617746  [64000/175341]\n",
      "loss: 0.299253  [65600/175341]\n",
      "loss: 0.818557  [67200/175341]\n",
      "loss: 0.540549  [68800/175341]\n",
      "loss: 0.389549  [70400/175341]\n",
      "loss: 0.519271  [72000/175341]\n",
      "loss: 0.194467  [73600/175341]\n",
      "loss: 0.369063  [75200/175341]\n",
      "loss: 0.486879  [76800/175341]\n",
      "loss: 0.974088  [78400/175341]\n",
      "loss: 0.381642  [80000/175341]\n",
      "loss: 0.284143  [81600/175341]\n",
      "loss: 0.630397  [83200/175341]\n",
      "loss: 0.258772  [84800/175341]\n",
      "loss: 0.582756  [86400/175341]\n",
      "loss: 0.577082  [88000/175341]\n",
      "loss: 0.346841  [89600/175341]\n",
      "loss: 0.514336  [91200/175341]\n",
      "loss: 0.203458  [92800/175341]\n",
      "loss: 0.481190  [94400/175341]\n",
      "loss: 0.411806  [96000/175341]\n",
      "loss: 0.352029  [97600/175341]\n",
      "loss: 0.452140  [99200/175341]\n",
      "loss: 0.202725  [100800/175341]\n",
      "loss: 0.487769  [102400/175341]\n",
      "loss: 0.640968  [104000/175341]\n",
      "loss: 0.380677  [105600/175341]\n",
      "loss: 0.923656  [107200/175341]\n",
      "loss: 0.412965  [108800/175341]\n",
      "loss: 0.755430  [110400/175341]\n",
      "loss: 0.677242  [112000/175341]\n",
      "loss: 0.197264  [113600/175341]\n",
      "loss: 0.461178  [115200/175341]\n",
      "loss: 0.337963  [116800/175341]\n",
      "loss: 0.834776  [118400/175341]\n",
      "loss: 0.703511  [120000/175341]\n",
      "loss: 0.331909  [121600/175341]\n",
      "loss: 0.514432  [123200/175341]\n",
      "loss: 0.313463  [124800/175341]\n",
      "loss: 0.479143  [126400/175341]\n",
      "loss: 0.668943  [128000/175341]\n",
      "loss: 0.500077  [129600/175341]\n",
      "loss: 0.586608  [131200/175341]\n",
      "loss: 0.470890  [132800/175341]\n",
      "loss: 0.502763  [134400/175341]\n",
      "loss: 0.980720  [136000/175341]\n",
      "loss: 0.421068  [137600/175341]\n",
      "loss: 0.668889  [139200/175341]\n",
      "loss: 0.501737  [140800/175341]\n",
      "loss: 0.209334  [142400/175341]\n",
      "loss: 0.759978  [144000/175341]\n",
      "loss: 1.300114  [145600/175341]\n",
      "loss: 0.431148  [147200/175341]\n",
      "loss: 0.791564  [148800/175341]\n",
      "loss: 0.296771  [150400/175341]\n",
      "loss: 0.402179  [152000/175341]\n",
      "loss: 0.459513  [153600/175341]\n",
      "loss: 0.750216  [155200/175341]\n",
      "loss: 0.485695  [156800/175341]\n",
      "loss: 0.344166  [158400/175341]\n",
      "loss: 0.142002  [160000/175341]\n",
      "loss: 0.532779  [161600/175341]\n",
      "loss: 0.679576  [163200/175341]\n",
      "loss: 0.674647  [164800/175341]\n",
      "loss: 0.165036  [166400/175341]\n",
      "loss: 0.142345  [168000/175341]\n",
      "loss: 0.508774  [169600/175341]\n",
      "loss: 0.559300  [171200/175341]\n",
      "loss: 0.189819  [172800/175341]\n",
      "loss: 0.331466  [174400/175341]\n",
      "Train Accuracy: 80.7746%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.577115, F1-score: 75.85%, Macro_F1-Score:  40.19%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.395115  [    0/175341]\n",
      "loss: 0.994734  [ 1600/175341]\n",
      "loss: 0.349279  [ 3200/175341]\n",
      "loss: 0.148387  [ 4800/175341]\n",
      "loss: 0.802391  [ 6400/175341]\n",
      "loss: 0.427498  [ 8000/175341]\n",
      "loss: 0.570308  [ 9600/175341]\n",
      "loss: 0.476960  [11200/175341]\n",
      "loss: 0.451484  [12800/175341]\n",
      "loss: 0.642013  [14400/175341]\n",
      "loss: 0.628872  [16000/175341]\n",
      "loss: 0.426576  [17600/175341]\n",
      "loss: 0.796928  [19200/175341]\n",
      "loss: 0.608716  [20800/175341]\n",
      "loss: 0.384131  [22400/175341]\n",
      "loss: 0.497057  [24000/175341]\n",
      "loss: 0.351758  [25600/175341]\n",
      "loss: 0.308947  [27200/175341]\n",
      "loss: 0.345510  [28800/175341]\n",
      "loss: 0.526376  [30400/175341]\n",
      "loss: 0.604007  [32000/175341]\n",
      "loss: 0.930076  [33600/175341]\n",
      "loss: 0.609910  [35200/175341]\n",
      "loss: 0.392088  [36800/175341]\n",
      "loss: 0.233267  [38400/175341]\n",
      "loss: 0.335846  [40000/175341]\n",
      "loss: 0.490897  [41600/175341]\n",
      "loss: 0.414856  [43200/175341]\n",
      "loss: 0.344711  [44800/175341]\n",
      "loss: 0.928025  [46400/175341]\n",
      "loss: 0.329498  [48000/175341]\n",
      "loss: 0.202006  [49600/175341]\n",
      "loss: 0.494650  [51200/175341]\n",
      "loss: 0.474784  [52800/175341]\n",
      "loss: 0.423088  [54400/175341]\n",
      "loss: 0.980742  [56000/175341]\n",
      "loss: 0.504065  [57600/175341]\n",
      "loss: 0.290846  [59200/175341]\n",
      "loss: 0.225779  [60800/175341]\n",
      "loss: 0.676835  [62400/175341]\n",
      "loss: 0.503936  [64000/175341]\n",
      "loss: 0.316188  [65600/175341]\n",
      "loss: 0.919541  [67200/175341]\n",
      "loss: 0.099609  [68800/175341]\n",
      "loss: 0.425836  [70400/175341]\n",
      "loss: 0.502989  [72000/175341]\n",
      "loss: 0.186971  [73600/175341]\n",
      "loss: 0.497275  [75200/175341]\n",
      "loss: 0.298582  [76800/175341]\n",
      "loss: 0.359502  [78400/175341]\n",
      "loss: 0.590709  [80000/175341]\n",
      "loss: 0.458822  [81600/175341]\n",
      "loss: 0.383991  [83200/175341]\n",
      "loss: 0.403376  [84800/175341]\n",
      "loss: 0.585910  [86400/175341]\n",
      "loss: 0.530655  [88000/175341]\n",
      "loss: 0.586344  [89600/175341]\n",
      "loss: 0.362951  [91200/175341]\n",
      "loss: 0.219489  [92800/175341]\n",
      "loss: 0.257823  [94400/175341]\n",
      "loss: 0.524853  [96000/175341]\n",
      "loss: 0.523293  [97600/175341]\n",
      "loss: 0.409238  [99200/175341]\n",
      "loss: 0.405997  [100800/175341]\n",
      "loss: 0.411376  [102400/175341]\n",
      "loss: 0.338301  [104000/175341]\n",
      "loss: 0.350855  [105600/175341]\n",
      "loss: 0.709874  [107200/175341]\n",
      "loss: 0.408085  [108800/175341]\n",
      "loss: 0.672988  [110400/175341]\n",
      "loss: 0.297127  [112000/175341]\n",
      "loss: 0.464622  [113600/175341]\n",
      "loss: 0.283387  [115200/175341]\n",
      "loss: 0.548410  [116800/175341]\n",
      "loss: 0.293558  [118400/175341]\n",
      "loss: 0.309317  [120000/175341]\n",
      "loss: 0.186216  [121600/175341]\n",
      "loss: 0.192461  [123200/175341]\n",
      "loss: 0.753841  [124800/175341]\n",
      "loss: 0.210509  [126400/175341]\n",
      "loss: 0.501127  [128000/175341]\n",
      "loss: 0.411112  [129600/175341]\n",
      "loss: 0.330080  [131200/175341]\n",
      "loss: 0.533694  [132800/175341]\n",
      "loss: 0.413765  [134400/175341]\n",
      "loss: 0.467379  [136000/175341]\n",
      "loss: 0.412813  [137600/175341]\n",
      "loss: 0.376845  [139200/175341]\n",
      "loss: 0.513931  [140800/175341]\n",
      "loss: 0.502561  [142400/175341]\n",
      "loss: 0.484912  [144000/175341]\n",
      "loss: 0.410056  [145600/175341]\n",
      "loss: 0.993717  [147200/175341]\n",
      "loss: 0.391599  [148800/175341]\n",
      "loss: 0.786131  [150400/175341]\n",
      "loss: 0.529767  [152000/175341]\n",
      "loss: 0.665931  [153600/175341]\n",
      "loss: 0.436000  [155200/175341]\n",
      "loss: 0.994374  [156800/175341]\n",
      "loss: 0.244737  [158400/175341]\n",
      "loss: 0.310279  [160000/175341]\n",
      "loss: 0.319022  [161600/175341]\n",
      "loss: 1.005513  [163200/175341]\n",
      "loss: 0.496933  [164800/175341]\n",
      "loss: 0.663318  [166400/175341]\n",
      "loss: 0.669010  [168000/175341]\n",
      "loss: 0.308452  [169600/175341]\n",
      "loss: 0.592393  [171200/175341]\n",
      "loss: 0.576656  [172800/175341]\n",
      "loss: 0.351869  [174400/175341]\n",
      "Train Accuracy: 80.8402%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.584386, F1-score: 75.26%, Macro_F1-Score:  39.93%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d14137f6-f30f-4334-b08b-83c562404f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}% \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef6d3b8c-ed93-47e2-ae7c-367c66fcfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,6, 3,10],grid_size = 3, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785bff68-72f6-4b56-9691-bb96c34fcbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306468  [    0/175341]\n",
      "loss: 2.296594  [ 1600/175341]\n",
      "loss: 2.292445  [ 3200/175341]\n",
      "loss: 2.275135  [ 4800/175341]\n",
      "loss: 2.264726  [ 6400/175341]\n",
      "loss: 2.251397  [ 8000/175341]\n",
      "loss: 2.211859  [ 9600/175341]\n",
      "loss: 2.210035  [11200/175341]\n",
      "loss: 2.230525  [12800/175341]\n",
      "loss: 2.149140  [14400/175341]\n",
      "loss: 2.073521  [16000/175341]\n",
      "loss: 2.137946  [17600/175341]\n",
      "loss: 2.066867  [19200/175341]\n",
      "loss: 2.048447  [20800/175341]\n",
      "loss: 2.040245  [22400/175341]\n",
      "loss: 2.028798  [24000/175341]\n",
      "loss: 1.947196  [25600/175341]\n",
      "loss: 1.977051  [27200/175341]\n",
      "loss: 1.877445  [28800/175341]\n",
      "loss: 1.897017  [30400/175341]\n",
      "loss: 1.938502  [32000/175341]\n",
      "loss: 1.800509  [33600/175341]\n",
      "loss: 1.838214  [35200/175341]\n",
      "loss: 1.848493  [36800/175341]\n",
      "loss: 1.783910  [38400/175341]\n",
      "loss: 1.956271  [40000/175341]\n",
      "loss: 1.980239  [41600/175341]\n",
      "loss: 1.841219  [43200/175341]\n",
      "loss: 1.929523  [44800/175341]\n",
      "loss: 1.889105  [46400/175341]\n",
      "loss: 2.144696  [48000/175341]\n",
      "loss: 1.886568  [49600/175341]\n",
      "loss: 1.993718  [51200/175341]\n",
      "loss: 1.710444  [52800/175341]\n",
      "loss: 1.769324  [54400/175341]\n",
      "loss: 1.622118  [56000/175341]\n",
      "loss: 1.566690  [57600/175341]\n",
      "loss: 1.530423  [59200/175341]\n",
      "loss: 1.630447  [60800/175341]\n",
      "loss: 1.484675  [62400/175341]\n",
      "loss: 1.516940  [64000/175341]\n",
      "loss: 1.513113  [65600/175341]\n",
      "loss: 1.470653  [67200/175341]\n",
      "loss: 1.553785  [68800/175341]\n",
      "loss: 1.209860  [70400/175341]\n",
      "loss: 1.556330  [72000/175341]\n",
      "loss: 1.867370  [73600/175341]\n",
      "loss: 1.499171  [75200/175341]\n",
      "loss: 1.607597  [76800/175341]\n",
      "loss: 1.725203  [78400/175341]\n",
      "loss: 1.266748  [80000/175341]\n",
      "loss: 1.302746  [81600/175341]\n",
      "loss: 1.370436  [83200/175341]\n",
      "loss: 1.363736  [84800/175341]\n",
      "loss: 1.266092  [86400/175341]\n",
      "loss: 1.566590  [88000/175341]\n",
      "loss: 1.439835  [89600/175341]\n",
      "loss: 1.546349  [91200/175341]\n",
      "loss: 1.144584  [92800/175341]\n",
      "loss: 1.083741  [94400/175341]\n",
      "loss: 1.276338  [96000/175341]\n",
      "loss: 1.322290  [97600/175341]\n",
      "loss: 1.237569  [99200/175341]\n",
      "loss: 1.097888  [100800/175341]\n",
      "loss: 1.274207  [102400/175341]\n",
      "loss: 1.130642  [104000/175341]\n",
      "loss: 1.158306  [105600/175341]\n",
      "loss: 1.027766  [107200/175341]\n",
      "loss: 1.317227  [108800/175341]\n",
      "loss: 1.198155  [110400/175341]\n",
      "loss: 1.303587  [112000/175341]\n",
      "loss: 1.018399  [113600/175341]\n",
      "loss: 1.049282  [115200/175341]\n",
      "loss: 0.882520  [116800/175341]\n",
      "loss: 1.051107  [118400/175341]\n",
      "loss: 1.192746  [120000/175341]\n",
      "loss: 0.981979  [121600/175341]\n",
      "loss: 0.961764  [123200/175341]\n",
      "loss: 1.129085  [124800/175341]\n",
      "loss: 1.214821  [126400/175341]\n",
      "loss: 0.895582  [128000/175341]\n",
      "loss: 1.053772  [129600/175341]\n",
      "loss: 0.772145  [131200/175341]\n",
      "loss: 0.979934  [132800/175341]\n",
      "loss: 1.102188  [134400/175341]\n",
      "loss: 0.474291  [136000/175341]\n",
      "loss: 0.611167  [137600/175341]\n",
      "loss: 1.348451  [139200/175341]\n",
      "loss: 0.778006  [140800/175341]\n",
      "loss: 1.047246  [142400/175341]\n",
      "loss: 0.765597  [144000/175341]\n",
      "loss: 0.765757  [145600/175341]\n",
      "loss: 0.558694  [147200/175341]\n",
      "loss: 1.020440  [148800/175341]\n",
      "loss: 0.842065  [150400/175341]\n",
      "loss: 0.762934  [152000/175341]\n",
      "loss: 1.257337  [153600/175341]\n",
      "loss: 0.934023  [155200/175341]\n",
      "loss: 0.769493  [156800/175341]\n",
      "loss: 0.640495  [158400/175341]\n",
      "loss: 0.694651  [160000/175341]\n",
      "loss: 0.753363  [161600/175341]\n",
      "loss: 0.959778  [163200/175341]\n",
      "loss: 1.025244  [164800/175341]\n",
      "loss: 0.645202  [166400/175341]\n",
      "loss: 0.930731  [168000/175341]\n",
      "loss: 0.689440  [169600/175341]\n",
      "loss: 0.765955  [171200/175341]\n",
      "loss: 1.314920  [172800/175341]\n",
      "loss: 0.872201  [174400/175341]\n",
      "Train Accuracy: 56.8498%\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.853616, F1-score: 67.99% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.013231  [    0/175341]\n",
      "loss: 0.500041  [ 1600/175341]\n",
      "loss: 0.663652  [ 3200/175341]\n",
      "loss: 0.858961  [ 4800/175341]\n",
      "loss: 1.138291  [ 6400/175341]\n",
      "loss: 0.971109  [ 8000/175341]\n",
      "loss: 0.357000  [ 9600/175341]\n",
      "loss: 0.339038  [11200/175341]\n",
      "loss: 0.740112  [12800/175341]\n",
      "loss: 0.829605  [14400/175341]\n",
      "loss: 0.682803  [16000/175341]\n",
      "loss: 0.786461  [17600/175341]\n",
      "loss: 0.395553  [19200/175341]\n",
      "loss: 0.902092  [20800/175341]\n",
      "loss: 0.744941  [22400/175341]\n",
      "loss: 1.124781  [24000/175341]\n",
      "loss: 0.974368  [25600/175341]\n",
      "loss: 1.517810  [27200/175341]\n",
      "loss: 0.823540  [28800/175341]\n",
      "loss: 0.924025  [30400/175341]\n",
      "loss: 0.873550  [32000/175341]\n",
      "loss: 0.640876  [33600/175341]\n",
      "loss: 0.817534  [35200/175341]\n",
      "loss: 0.707426  [36800/175341]\n",
      "loss: 0.595827  [38400/175341]\n",
      "loss: 0.471292  [40000/175341]\n",
      "loss: 0.720784  [41600/175341]\n",
      "loss: 0.759479  [43200/175341]\n",
      "loss: 0.537409  [44800/175341]\n",
      "loss: 0.676516  [46400/175341]\n",
      "loss: 0.953800  [48000/175341]\n",
      "loss: 1.442204  [49600/175341]\n",
      "loss: 0.700269  [51200/175341]\n",
      "loss: 0.541742  [52800/175341]\n",
      "loss: 0.675789  [54400/175341]\n",
      "loss: 0.576168  [56000/175341]\n",
      "loss: 0.638794  [57600/175341]\n",
      "loss: 0.705421  [59200/175341]\n",
      "loss: 0.513502  [60800/175341]\n",
      "loss: 1.073824  [62400/175341]\n",
      "loss: 0.364829  [64000/175341]\n",
      "loss: 0.787705  [65600/175341]\n",
      "loss: 0.458401  [67200/175341]\n",
      "loss: 0.646604  [68800/175341]\n",
      "loss: 0.287138  [70400/175341]\n",
      "loss: 1.393505  [72000/175341]\n",
      "loss: 0.591807  [73600/175341]\n",
      "loss: 0.469331  [75200/175341]\n",
      "loss: 0.490620  [76800/175341]\n",
      "loss: 1.049292  [78400/175341]\n",
      "loss: 0.570803  [80000/175341]\n",
      "loss: 0.963489  [81600/175341]\n",
      "loss: 0.421529  [83200/175341]\n",
      "loss: 1.001295  [84800/175341]\n",
      "loss: 0.318092  [86400/175341]\n",
      "loss: 0.424510  [88000/175341]\n",
      "loss: 0.989182  [89600/175341]\n",
      "loss: 0.902411  [91200/175341]\n",
      "loss: 0.394886  [92800/175341]\n",
      "loss: 0.983279  [94400/175341]\n",
      "loss: 0.538750  [96000/175341]\n",
      "loss: 0.223943  [97600/175341]\n",
      "loss: 0.875018  [99200/175341]\n",
      "loss: 0.577932  [100800/175341]\n",
      "loss: 0.491336  [102400/175341]\n",
      "loss: 0.235076  [104000/175341]\n",
      "loss: 0.572102  [105600/175341]\n",
      "loss: 0.596594  [107200/175341]\n",
      "loss: 0.322052  [108800/175341]\n",
      "loss: 0.293611  [110400/175341]\n",
      "loss: 0.496520  [112000/175341]\n",
      "loss: 0.407559  [113600/175341]\n",
      "loss: 0.623889  [115200/175341]\n",
      "loss: 0.981764  [116800/175341]\n",
      "loss: 0.680437  [118400/175341]\n",
      "loss: 0.942941  [120000/175341]\n",
      "loss: 0.661099  [121600/175341]\n",
      "loss: 0.473212  [123200/175341]\n",
      "loss: 0.635888  [124800/175341]\n",
      "loss: 0.645833  [126400/175341]\n",
      "loss: 0.308398  [128000/175341]\n",
      "loss: 0.833122  [129600/175341]\n",
      "loss: 0.994203  [131200/175341]\n",
      "loss: 0.405722  [132800/175341]\n",
      "loss: 0.582497  [134400/175341]\n",
      "loss: 1.265426  [136000/175341]\n",
      "loss: 0.516663  [137600/175341]\n",
      "loss: 0.816315  [139200/175341]\n",
      "loss: 0.233021  [140800/175341]\n",
      "loss: 1.275930  [142400/175341]\n",
      "loss: 0.805334  [144000/175341]\n",
      "loss: 0.795259  [145600/175341]\n",
      "loss: 0.400717  [147200/175341]\n",
      "loss: 0.672224  [148800/175341]\n",
      "loss: 1.779822  [150400/175341]\n",
      "loss: 0.580100  [152000/175341]\n",
      "loss: 0.564305  [153600/175341]\n",
      "loss: 1.114215  [155200/175341]\n",
      "loss: 1.444613  [156800/175341]\n",
      "loss: 0.250103  [158400/175341]\n",
      "loss: 0.591619  [160000/175341]\n",
      "loss: 0.642141  [161600/175341]\n",
      "loss: 0.271245  [163200/175341]\n",
      "loss: 0.770537  [164800/175341]\n",
      "loss: 0.709475  [166400/175341]\n",
      "loss: 0.539387  [168000/175341]\n",
      "loss: 0.352588  [169600/175341]\n",
      "loss: 0.392602  [171200/175341]\n",
      "loss: 0.847702  [172800/175341]\n",
      "loss: 0.709494  [174400/175341]\n",
      "Train Accuracy: 75.3948%\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.741038, F1-score: 69.71% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.275295  [    0/175341]\n",
      "loss: 0.912725  [ 1600/175341]\n",
      "loss: 0.589403  [ 3200/175341]\n",
      "loss: 0.818697  [ 4800/175341]\n",
      "loss: 0.324169  [ 6400/175341]\n",
      "loss: 0.424974  [ 8000/175341]\n",
      "loss: 0.266689  [ 9600/175341]\n",
      "loss: 0.529490  [11200/175341]\n",
      "loss: 0.878461  [12800/175341]\n",
      "loss: 0.487464  [14400/175341]\n",
      "loss: 0.398102  [16000/175341]\n",
      "loss: 0.715884  [17600/175341]\n",
      "loss: 0.768880  [19200/175341]\n",
      "loss: 0.881735  [20800/175341]\n",
      "loss: 0.766791  [22400/175341]\n",
      "loss: 0.937075  [24000/175341]\n",
      "loss: 0.331816  [25600/175341]\n",
      "loss: 0.657176  [27200/175341]\n",
      "loss: 0.393183  [28800/175341]\n",
      "loss: 0.257473  [30400/175341]\n",
      "loss: 0.394779  [32000/175341]\n",
      "loss: 0.446011  [33600/175341]\n",
      "loss: 0.616269  [35200/175341]\n",
      "loss: 1.066690  [36800/175341]\n",
      "loss: 0.403239  [38400/175341]\n",
      "loss: 0.595729  [40000/175341]\n",
      "loss: 0.853446  [41600/175341]\n",
      "loss: 0.776065  [43200/175341]\n",
      "loss: 0.283711  [44800/175341]\n",
      "loss: 0.421481  [46400/175341]\n",
      "loss: 0.782315  [48000/175341]\n",
      "loss: 0.899825  [49600/175341]\n",
      "loss: 0.625981  [51200/175341]\n",
      "loss: 0.484761  [52800/175341]\n",
      "loss: 0.253582  [54400/175341]\n",
      "loss: 0.420744  [56000/175341]\n",
      "loss: 0.484251  [57600/175341]\n",
      "loss: 0.499863  [59200/175341]\n",
      "loss: 0.766872  [60800/175341]\n",
      "loss: 0.648171  [62400/175341]\n",
      "loss: 0.794177  [64000/175341]\n",
      "loss: 0.396774  [65600/175341]\n",
      "loss: 0.664900  [67200/175341]\n",
      "loss: 0.641495  [68800/175341]\n",
      "loss: 0.442654  [70400/175341]\n",
      "loss: 0.854650  [72000/175341]\n",
      "loss: 0.638895  [73600/175341]\n",
      "loss: 0.938785  [75200/175341]\n",
      "loss: 0.750835  [76800/175341]\n",
      "loss: 0.852433  [78400/175341]\n",
      "loss: 0.605647  [80000/175341]\n",
      "loss: 0.514421  [81600/175341]\n",
      "loss: 0.605077  [83200/175341]\n",
      "loss: 0.408954  [84800/175341]\n",
      "loss: 0.699584  [86400/175341]\n",
      "loss: 0.352989  [88000/175341]\n",
      "loss: 0.473324  [89600/175341]\n",
      "loss: 0.372581  [91200/175341]\n",
      "loss: 0.700048  [92800/175341]\n",
      "loss: 0.853241  [94400/175341]\n",
      "loss: 1.186826  [96000/175341]\n",
      "loss: 0.368454  [97600/175341]\n",
      "loss: 0.571549  [99200/175341]\n",
      "loss: 0.340450  [100800/175341]\n",
      "loss: 0.480797  [102400/175341]\n",
      "loss: 0.306414  [104000/175341]\n",
      "loss: 0.564774  [105600/175341]\n",
      "loss: 0.442520  [107200/175341]\n",
      "loss: 0.485269  [108800/175341]\n",
      "loss: 0.281980  [110400/175341]\n",
      "loss: 0.434746  [112000/175341]\n",
      "loss: 0.388719  [113600/175341]\n",
      "loss: 0.846366  [115200/175341]\n",
      "loss: 0.362509  [116800/175341]\n",
      "loss: 0.738644  [118400/175341]\n",
      "loss: 0.668918  [120000/175341]\n",
      "loss: 0.585381  [121600/175341]\n",
      "loss: 0.883091  [123200/175341]\n",
      "loss: 0.584260  [124800/175341]\n",
      "loss: 0.655985  [126400/175341]\n",
      "loss: 0.913155  [128000/175341]\n",
      "loss: 0.498610  [129600/175341]\n",
      "loss: 0.485282  [131200/175341]\n",
      "loss: 0.815082  [132800/175341]\n",
      "loss: 1.000270  [134400/175341]\n",
      "loss: 1.017361  [136000/175341]\n",
      "loss: 0.923106  [137600/175341]\n",
      "loss: 0.191026  [139200/175341]\n",
      "loss: 0.371550  [140800/175341]\n",
      "loss: 0.662349  [142400/175341]\n",
      "loss: 0.256791  [144000/175341]\n",
      "loss: 0.641764  [145600/175341]\n",
      "loss: 0.618944  [147200/175341]\n",
      "loss: 0.475756  [148800/175341]\n",
      "loss: 0.316192  [150400/175341]\n",
      "loss: 0.875559  [152000/175341]\n",
      "loss: 0.515301  [153600/175341]\n",
      "loss: 0.580784  [155200/175341]\n",
      "loss: 0.272859  [156800/175341]\n",
      "loss: 0.355377  [158400/175341]\n",
      "loss: 0.716224  [160000/175341]\n",
      "loss: 0.301860  [161600/175341]\n",
      "loss: 1.099571  [163200/175341]\n",
      "loss: 0.593285  [164800/175341]\n",
      "loss: 0.703416  [166400/175341]\n",
      "loss: 0.968440  [168000/175341]\n",
      "loss: 0.748691  [169600/175341]\n",
      "loss: 0.448018  [171200/175341]\n",
      "loss: 0.822030  [172800/175341]\n",
      "loss: 0.538828  [174400/175341]\n",
      "Train Accuracy: 76.2155%\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.718306, F1-score: 69.91% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.558797  [    0/175341]\n",
      "loss: 0.477260  [ 1600/175341]\n",
      "loss: 0.655329  [ 3200/175341]\n",
      "loss: 0.644490  [ 4800/175341]\n",
      "loss: 0.571165  [ 6400/175341]\n",
      "loss: 0.594225  [ 8000/175341]\n",
      "loss: 0.515999  [ 9600/175341]\n",
      "loss: 1.017671  [11200/175341]\n",
      "loss: 0.688766  [12800/175341]\n",
      "loss: 0.633752  [14400/175341]\n",
      "loss: 0.774324  [16000/175341]\n",
      "loss: 0.779909  [17600/175341]\n",
      "loss: 0.860616  [19200/175341]\n",
      "loss: 0.428965  [20800/175341]\n",
      "loss: 0.767079  [22400/175341]\n",
      "loss: 0.331395  [24000/175341]\n",
      "loss: 0.653449  [25600/175341]\n",
      "loss: 0.531128  [27200/175341]\n",
      "loss: 0.529494  [28800/175341]\n",
      "loss: 0.706935  [30400/175341]\n",
      "loss: 0.608627  [32000/175341]\n",
      "loss: 0.399803  [33600/175341]\n",
      "loss: 0.685899  [35200/175341]\n",
      "loss: 0.692013  [36800/175341]\n",
      "loss: 0.921064  [38400/175341]\n",
      "loss: 0.707801  [40000/175341]\n",
      "loss: 0.532746  [41600/175341]\n",
      "loss: 0.440449  [43200/175341]\n",
      "loss: 0.713368  [44800/175341]\n",
      "loss: 0.460746  [46400/175341]\n",
      "loss: 0.710223  [48000/175341]\n",
      "loss: 0.900987  [49600/175341]\n",
      "loss: 0.523835  [51200/175341]\n",
      "loss: 0.629814  [52800/175341]\n",
      "loss: 0.614195  [54400/175341]\n",
      "loss: 0.522510  [56000/175341]\n",
      "loss: 0.459699  [57600/175341]\n",
      "loss: 0.402273  [59200/175341]\n",
      "loss: 0.323693  [60800/175341]\n",
      "loss: 0.746866  [62400/175341]\n",
      "loss: 0.484585  [64000/175341]\n",
      "loss: 0.622436  [65600/175341]\n",
      "loss: 0.733851  [67200/175341]\n",
      "loss: 0.596335  [68800/175341]\n",
      "loss: 0.588500  [70400/175341]\n",
      "loss: 0.803203  [72000/175341]\n",
      "loss: 0.831336  [73600/175341]\n",
      "loss: 0.890177  [75200/175341]\n",
      "loss: 0.588697  [76800/175341]\n",
      "loss: 0.747196  [78400/175341]\n",
      "loss: 0.714916  [80000/175341]\n",
      "loss: 0.687723  [81600/175341]\n",
      "loss: 0.655802  [83200/175341]\n",
      "loss: 0.568074  [84800/175341]\n",
      "loss: 0.558699  [86400/175341]\n",
      "loss: 0.525821  [88000/175341]\n",
      "loss: 0.556092  [89600/175341]\n",
      "loss: 0.440241  [91200/175341]\n",
      "loss: 0.935535  [92800/175341]\n",
      "loss: 0.263411  [94400/175341]\n",
      "loss: 0.891992  [96000/175341]\n",
      "loss: 0.674383  [97600/175341]\n",
      "loss: 0.219631  [99200/175341]\n",
      "loss: 0.558723  [100800/175341]\n",
      "loss: 0.455553  [102400/175341]\n",
      "loss: 0.405518  [104000/175341]\n",
      "loss: 0.442418  [105600/175341]\n",
      "loss: 0.879594  [107200/175341]\n",
      "loss: 0.317717  [108800/175341]\n",
      "loss: 0.411643  [110400/175341]\n",
      "loss: 0.337355  [112000/175341]\n",
      "loss: 0.725429  [113600/175341]\n",
      "loss: 0.677086  [115200/175341]\n",
      "loss: 0.879694  [116800/175341]\n",
      "loss: 0.560206  [118400/175341]\n",
      "loss: 0.730340  [120000/175341]\n",
      "loss: 0.777089  [121600/175341]\n",
      "loss: 0.550399  [123200/175341]\n",
      "loss: 0.422650  [124800/175341]\n",
      "loss: 0.689112  [126400/175341]\n",
      "loss: 0.539529  [128000/175341]\n",
      "loss: 1.053347  [129600/175341]\n",
      "loss: 1.108193  [131200/175341]\n",
      "loss: 0.150393  [132800/175341]\n",
      "loss: 0.932560  [134400/175341]\n",
      "loss: 0.620331  [136000/175341]\n",
      "loss: 0.463726  [137600/175341]\n",
      "loss: 0.765123  [139200/175341]\n",
      "loss: 0.534698  [140800/175341]\n",
      "loss: 1.066033  [142400/175341]\n",
      "loss: 0.367321  [144000/175341]\n",
      "loss: 0.796358  [145600/175341]\n",
      "loss: 0.757495  [147200/175341]\n",
      "loss: 0.473096  [148800/175341]\n",
      "loss: 0.707818  [150400/175341]\n",
      "loss: 0.879795  [152000/175341]\n",
      "loss: 0.773981  [153600/175341]\n",
      "loss: 0.725060  [155200/175341]\n",
      "loss: 0.466398  [156800/175341]\n",
      "loss: 0.345852  [158400/175341]\n",
      "loss: 0.463084  [160000/175341]\n",
      "loss: 0.563792  [161600/175341]\n",
      "loss: 0.407902  [163200/175341]\n",
      "loss: 0.526771  [164800/175341]\n",
      "loss: 0.253117  [166400/175341]\n",
      "loss: 0.382942  [168000/175341]\n",
      "loss: 0.521744  [169600/175341]\n",
      "loss: 0.585869  [171200/175341]\n",
      "loss: 0.770171  [172800/175341]\n",
      "loss: 0.579960  [174400/175341]\n",
      "Train Accuracy: 76.5286%\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.690860, F1-score: 70.02% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.649801  [    0/175341]\n",
      "loss: 0.743952  [ 1600/175341]\n",
      "loss: 0.621141  [ 3200/175341]\n",
      "loss: 0.520985  [ 4800/175341]\n",
      "loss: 0.274876  [ 6400/175341]\n",
      "loss: 0.505584  [ 8000/175341]\n",
      "loss: 0.500815  [ 9600/175341]\n",
      "loss: 0.500298  [11200/175341]\n",
      "loss: 0.834554  [12800/175341]\n",
      "loss: 0.191099  [14400/175341]\n",
      "loss: 0.813269  [16000/175341]\n",
      "loss: 0.406923  [17600/175341]\n",
      "loss: 0.523594  [19200/175341]\n",
      "loss: 0.952279  [20800/175341]\n",
      "loss: 1.345094  [22400/175341]\n",
      "loss: 0.505021  [24000/175341]\n",
      "loss: 0.499709  [25600/175341]\n",
      "loss: 0.296863  [27200/175341]\n",
      "loss: 0.744629  [28800/175341]\n",
      "loss: 0.898899  [30400/175341]\n",
      "loss: 0.493789  [32000/175341]\n",
      "loss: 0.476470  [33600/175341]\n",
      "loss: 0.610703  [35200/175341]\n",
      "loss: 0.222651  [36800/175341]\n",
      "loss: 0.486071  [38400/175341]\n",
      "loss: 1.017606  [40000/175341]\n",
      "loss: 0.556964  [41600/175341]\n",
      "loss: 0.659780  [43200/175341]\n",
      "loss: 0.624049  [44800/175341]\n",
      "loss: 0.254837  [46400/175341]\n",
      "loss: 0.809586  [48000/175341]\n",
      "loss: 0.347502  [49600/175341]\n",
      "loss: 0.624124  [51200/175341]\n",
      "loss: 0.948290  [52800/175341]\n",
      "loss: 0.519057  [54400/175341]\n",
      "loss: 0.633249  [56000/175341]\n",
      "loss: 0.574165  [57600/175341]\n",
      "loss: 0.730101  [59200/175341]\n",
      "loss: 0.646058  [60800/175341]\n",
      "loss: 0.405247  [62400/175341]\n",
      "loss: 0.752078  [64000/175341]\n",
      "loss: 0.432992  [65600/175341]\n",
      "loss: 0.823387  [67200/175341]\n",
      "loss: 0.798703  [68800/175341]\n",
      "loss: 0.681490  [70400/175341]\n",
      "loss: 0.654340  [72000/175341]\n",
      "loss: 0.935502  [73600/175341]\n",
      "loss: 0.665787  [75200/175341]\n",
      "loss: 0.922282  [76800/175341]\n",
      "loss: 0.550663  [78400/175341]\n",
      "loss: 0.485383  [80000/175341]\n",
      "loss: 0.473194  [81600/175341]\n",
      "loss: 0.204375  [83200/175341]\n",
      "loss: 0.438275  [84800/175341]\n",
      "loss: 1.024750  [86400/175341]\n",
      "loss: 0.868732  [88000/175341]\n",
      "loss: 0.435046  [89600/175341]\n",
      "loss: 0.505511  [91200/175341]\n",
      "loss: 0.723379  [92800/175341]\n",
      "loss: 0.456525  [94400/175341]\n",
      "loss: 0.119214  [96000/175341]\n",
      "loss: 0.570932  [97600/175341]\n",
      "loss: 0.874679  [99200/175341]\n",
      "loss: 1.021459  [100800/175341]\n",
      "loss: 0.573640  [102400/175341]\n",
      "loss: 0.683986  [104000/175341]\n",
      "loss: 0.640435  [105600/175341]\n",
      "loss: 0.356476  [107200/175341]\n",
      "loss: 0.337992  [108800/175341]\n",
      "loss: 0.731564  [110400/175341]\n",
      "loss: 0.537754  [112000/175341]\n",
      "loss: 0.552969  [113600/175341]\n",
      "loss: 1.090245  [115200/175341]\n",
      "loss: 0.467513  [116800/175341]\n",
      "loss: 0.408183  [118400/175341]\n",
      "loss: 0.431426  [120000/175341]\n",
      "loss: 0.537560  [121600/175341]\n",
      "loss: 0.801368  [123200/175341]\n",
      "loss: 0.924877  [124800/175341]\n",
      "loss: 0.572501  [126400/175341]\n",
      "loss: 0.695247  [128000/175341]\n",
      "loss: 0.520349  [129600/175341]\n",
      "loss: 1.501671  [131200/175341]\n",
      "loss: 0.895112  [132800/175341]\n",
      "loss: 0.727267  [134400/175341]\n",
      "loss: 0.667694  [136000/175341]\n",
      "loss: 0.446151  [137600/175341]\n",
      "loss: 0.766457  [139200/175341]\n",
      "loss: 0.713017  [140800/175341]\n",
      "loss: 0.663902  [142400/175341]\n",
      "loss: 0.557948  [144000/175341]\n",
      "loss: 0.490044  [145600/175341]\n",
      "loss: 1.145362  [147200/175341]\n",
      "loss: 0.529451  [148800/175341]\n",
      "loss: 0.504587  [150400/175341]\n",
      "loss: 0.558947  [152000/175341]\n",
      "loss: 0.666491  [153600/175341]\n",
      "loss: 0.815990  [155200/175341]\n",
      "loss: 0.809214  [156800/175341]\n",
      "loss: 0.499425  [158400/175341]\n",
      "loss: 0.546959  [160000/175341]\n",
      "loss: 0.437646  [161600/175341]\n",
      "loss: 0.431062  [163200/175341]\n",
      "loss: 0.761856  [164800/175341]\n",
      "loss: 0.300647  [166400/175341]\n",
      "loss: 0.656441  [168000/175341]\n",
      "loss: 0.323880  [169600/175341]\n",
      "loss: 0.850255  [171200/175341]\n",
      "loss: 0.412700  [172800/175341]\n",
      "loss: 0.646600  [174400/175341]\n",
      "Train Accuracy: 76.7932%\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.660173, F1-score: 70.74% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.654694  [    0/175341]\n",
      "loss: 0.520614  [ 1600/175341]\n",
      "loss: 0.615315  [ 3200/175341]\n",
      "loss: 0.862536  [ 4800/175341]\n",
      "loss: 0.479685  [ 6400/175341]\n",
      "loss: 0.673994  [ 8000/175341]\n",
      "loss: 0.500376  [ 9600/175341]\n",
      "loss: 0.538555  [11200/175341]\n",
      "loss: 0.404684  [12800/175341]\n",
      "loss: 0.387693  [14400/175341]\n",
      "loss: 0.535927  [16000/175341]\n",
      "loss: 0.680038  [17600/175341]\n",
      "loss: 0.953972  [19200/175341]\n",
      "loss: 1.296275  [20800/175341]\n",
      "loss: 0.737791  [22400/175341]\n",
      "loss: 0.636644  [24000/175341]\n",
      "loss: 0.326121  [25600/175341]\n",
      "loss: 0.344435  [27200/175341]\n",
      "loss: 0.247189  [28800/175341]\n",
      "loss: 0.618226  [30400/175341]\n",
      "loss: 0.256777  [32000/175341]\n",
      "loss: 0.761535  [33600/175341]\n",
      "loss: 0.694967  [35200/175341]\n",
      "loss: 0.215142  [36800/175341]\n",
      "loss: 0.894335  [38400/175341]\n",
      "loss: 0.417604  [40000/175341]\n",
      "loss: 1.093622  [41600/175341]\n",
      "loss: 0.304869  [43200/175341]\n",
      "loss: 1.072890  [44800/175341]\n",
      "loss: 0.677277  [46400/175341]\n",
      "loss: 0.830985  [48000/175341]\n",
      "loss: 0.619081  [49600/175341]\n",
      "loss: 0.416771  [51200/175341]\n",
      "loss: 0.580033  [52800/175341]\n",
      "loss: 0.647846  [54400/175341]\n",
      "loss: 0.728243  [56000/175341]\n",
      "loss: 0.173476  [57600/175341]\n",
      "loss: 1.067714  [59200/175341]\n",
      "loss: 0.825949  [60800/175341]\n",
      "loss: 0.535708  [62400/175341]\n",
      "loss: 0.491927  [64000/175341]\n",
      "loss: 0.499544  [65600/175341]\n",
      "loss: 0.365677  [67200/175341]\n",
      "loss: 0.318187  [68800/175341]\n",
      "loss: 0.481836  [70400/175341]\n",
      "loss: 0.901841  [72000/175341]\n",
      "loss: 0.798432  [73600/175341]\n",
      "loss: 0.735044  [75200/175341]\n",
      "loss: 0.263343  [76800/175341]\n",
      "loss: 0.317105  [78400/175341]\n",
      "loss: 0.487265  [80000/175341]\n",
      "loss: 0.747460  [81600/175341]\n",
      "loss: 0.244334  [83200/175341]\n",
      "loss: 0.312851  [84800/175341]\n",
      "loss: 0.777580  [86400/175341]\n",
      "loss: 0.519560  [88000/175341]\n",
      "loss: 0.456079  [89600/175341]\n",
      "loss: 0.224047  [91200/175341]\n",
      "loss: 0.501884  [92800/175341]\n",
      "loss: 1.064922  [94400/175341]\n",
      "loss: 0.965604  [96000/175341]\n",
      "loss: 0.579372  [97600/175341]\n",
      "loss: 0.273396  [99200/175341]\n",
      "loss: 0.818637  [100800/175341]\n",
      "loss: 0.723199  [102400/175341]\n",
      "loss: 0.749465  [104000/175341]\n",
      "loss: 0.614318  [105600/175341]\n",
      "loss: 0.318291  [107200/175341]\n",
      "loss: 0.369154  [108800/175341]\n",
      "loss: 0.253237  [110400/175341]\n",
      "loss: 0.590262  [112000/175341]\n",
      "loss: 0.460618  [113600/175341]\n",
      "loss: 1.050426  [115200/175341]\n",
      "loss: 0.553644  [116800/175341]\n",
      "loss: 0.543951  [118400/175341]\n",
      "loss: 0.567036  [120000/175341]\n",
      "loss: 0.632941  [121600/175341]\n",
      "loss: 0.421408  [123200/175341]\n",
      "loss: 0.810889  [124800/175341]\n",
      "loss: 0.593570  [126400/175341]\n",
      "loss: 0.641419  [128000/175341]\n",
      "loss: 0.687972  [129600/175341]\n",
      "loss: 0.274548  [131200/175341]\n",
      "loss: 0.440794  [132800/175341]\n",
      "loss: 0.365836  [134400/175341]\n",
      "loss: 0.389702  [136000/175341]\n",
      "loss: 0.615669  [137600/175341]\n",
      "loss: 0.808846  [139200/175341]\n",
      "loss: 0.710752  [140800/175341]\n",
      "loss: 0.399612  [142400/175341]\n",
      "loss: 0.918737  [144000/175341]\n",
      "loss: 0.656342  [145600/175341]\n",
      "loss: 0.947640  [147200/175341]\n",
      "loss: 1.579941  [148800/175341]\n",
      "loss: 0.524462  [150400/175341]\n",
      "loss: 0.846083  [152000/175341]\n",
      "loss: 0.462211  [153600/175341]\n",
      "loss: 0.362040  [155200/175341]\n",
      "loss: 0.309288  [156800/175341]\n",
      "loss: 0.537542  [158400/175341]\n",
      "loss: 0.668224  [160000/175341]\n",
      "loss: 0.278131  [161600/175341]\n",
      "loss: 0.288756  [163200/175341]\n",
      "loss: 0.363419  [164800/175341]\n",
      "loss: 0.305726  [166400/175341]\n",
      "loss: 0.659388  [168000/175341]\n",
      "loss: 0.568667  [169600/175341]\n",
      "loss: 0.194130  [171200/175341]\n",
      "loss: 0.784495  [172800/175341]\n",
      "loss: 0.718398  [174400/175341]\n",
      "Train Accuracy: 77.0499%\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.647765, F1-score: 71.44% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.799701  [    0/175341]\n",
      "loss: 0.615654  [ 1600/175341]\n",
      "loss: 0.248954  [ 3200/175341]\n",
      "loss: 0.518109  [ 4800/175341]\n",
      "loss: 0.279368  [ 6400/175341]\n",
      "loss: 0.475903  [ 8000/175341]\n",
      "loss: 0.660869  [ 9600/175341]\n",
      "loss: 0.366174  [11200/175341]\n",
      "loss: 0.325234  [12800/175341]\n",
      "loss: 0.435347  [14400/175341]\n",
      "loss: 0.858975  [16000/175341]\n",
      "loss: 0.528799  [17600/175341]\n",
      "loss: 0.653654  [19200/175341]\n",
      "loss: 0.265960  [20800/175341]\n",
      "loss: 0.369007  [22400/175341]\n",
      "loss: 0.304517  [24000/175341]\n",
      "loss: 0.432743  [25600/175341]\n",
      "loss: 0.417126  [27200/175341]\n",
      "loss: 1.065994  [28800/175341]\n",
      "loss: 0.393811  [30400/175341]\n",
      "loss: 0.426260  [32000/175341]\n",
      "loss: 0.182829  [33600/175341]\n",
      "loss: 0.261176  [35200/175341]\n",
      "loss: 0.641409  [36800/175341]\n",
      "loss: 0.794316  [38400/175341]\n",
      "loss: 0.393774  [40000/175341]\n",
      "loss: 0.394725  [41600/175341]\n",
      "loss: 0.556182  [43200/175341]\n",
      "loss: 0.363571  [44800/175341]\n",
      "loss: 0.403688  [46400/175341]\n",
      "loss: 0.833301  [48000/175341]\n",
      "loss: 0.982815  [49600/175341]\n",
      "loss: 0.584924  [51200/175341]\n",
      "loss: 0.803274  [52800/175341]\n",
      "loss: 0.637334  [54400/175341]\n",
      "loss: 0.439776  [56000/175341]\n",
      "loss: 0.932591  [57600/175341]\n",
      "loss: 0.558227  [59200/175341]\n",
      "loss: 0.926188  [60800/175341]\n",
      "loss: 0.375861  [62400/175341]\n",
      "loss: 1.016605  [64000/175341]\n",
      "loss: 0.433071  [65600/175341]\n",
      "loss: 0.553256  [67200/175341]\n",
      "loss: 0.589136  [68800/175341]\n",
      "loss: 0.481687  [70400/175341]\n",
      "loss: 0.524885  [72000/175341]\n",
      "loss: 0.857574  [73600/175341]\n",
      "loss: 0.666273  [75200/175341]\n",
      "loss: 0.612182  [76800/175341]\n",
      "loss: 0.508014  [78400/175341]\n",
      "loss: 0.279711  [80000/175341]\n",
      "loss: 0.653712  [81600/175341]\n",
      "loss: 0.643925  [83200/175341]\n",
      "loss: 0.343235  [84800/175341]\n",
      "loss: 0.447289  [86400/175341]\n",
      "loss: 0.359454  [88000/175341]\n",
      "loss: 0.520114  [89600/175341]\n",
      "loss: 0.883131  [91200/175341]\n",
      "loss: 0.623268  [92800/175341]\n",
      "loss: 0.358384  [94400/175341]\n",
      "loss: 0.736152  [96000/175341]\n",
      "loss: 0.486682  [97600/175341]\n",
      "loss: 0.539673  [99200/175341]\n",
      "loss: 0.978532  [100800/175341]\n",
      "loss: 0.442321  [102400/175341]\n",
      "loss: 0.229275  [104000/175341]\n",
      "loss: 0.154157  [105600/175341]\n",
      "loss: 0.653520  [107200/175341]\n",
      "loss: 0.392206  [108800/175341]\n",
      "loss: 0.599306  [110400/175341]\n",
      "loss: 0.624208  [112000/175341]\n",
      "loss: 0.660980  [113600/175341]\n",
      "loss: 0.467319  [115200/175341]\n",
      "loss: 0.446401  [116800/175341]\n",
      "loss: 0.284800  [118400/175341]\n",
      "loss: 0.506456  [120000/175341]\n",
      "loss: 0.299313  [121600/175341]\n",
      "loss: 0.725231  [123200/175341]\n",
      "loss: 0.421977  [124800/175341]\n",
      "loss: 0.358119  [126400/175341]\n",
      "loss: 0.283894  [128000/175341]\n",
      "loss: 0.389565  [129600/175341]\n",
      "loss: 1.218937  [131200/175341]\n",
      "loss: 0.848898  [132800/175341]\n",
      "loss: 0.287015  [134400/175341]\n",
      "loss: 0.512271  [136000/175341]\n",
      "loss: 0.419024  [137600/175341]\n",
      "loss: 0.497869  [139200/175341]\n",
      "loss: 0.420189  [140800/175341]\n",
      "loss: 0.557339  [142400/175341]\n",
      "loss: 0.754975  [144000/175341]\n",
      "loss: 0.759188  [145600/175341]\n",
      "loss: 0.728877  [147200/175341]\n",
      "loss: 0.590901  [148800/175341]\n",
      "loss: 0.751020  [150400/175341]\n",
      "loss: 0.999364  [152000/175341]\n",
      "loss: 0.548320  [153600/175341]\n",
      "loss: 0.431948  [155200/175341]\n",
      "loss: 0.417763  [156800/175341]\n",
      "loss: 0.864238  [158400/175341]\n",
      "loss: 0.404524  [160000/175341]\n",
      "loss: 0.784283  [161600/175341]\n",
      "loss: 0.705782  [163200/175341]\n",
      "loss: 0.937970  [164800/175341]\n",
      "loss: 0.484951  [166400/175341]\n",
      "loss: 0.574037  [168000/175341]\n",
      "loss: 0.628295  [169600/175341]\n",
      "loss: 0.871862  [171200/175341]\n",
      "loss: 0.623741  [172800/175341]\n",
      "loss: 0.395427  [174400/175341]\n",
      "Train Accuracy: 77.3498%\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.644565, F1-score: 71.49% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.618815  [    0/175341]\n",
      "loss: 0.595139  [ 1600/175341]\n",
      "loss: 0.658589  [ 3200/175341]\n",
      "loss: 0.893740  [ 4800/175341]\n",
      "loss: 0.793445  [ 6400/175341]\n",
      "loss: 0.148206  [ 8000/175341]\n",
      "loss: 0.289861  [ 9600/175341]\n",
      "loss: 0.890070  [11200/175341]\n",
      "loss: 0.936936  [12800/175341]\n",
      "loss: 1.202535  [14400/175341]\n",
      "loss: 0.707581  [16000/175341]\n",
      "loss: 0.724703  [17600/175341]\n",
      "loss: 0.498363  [19200/175341]\n",
      "loss: 0.405401  [20800/175341]\n",
      "loss: 0.331558  [22400/175341]\n",
      "loss: 0.467680  [24000/175341]\n",
      "loss: 0.942636  [25600/175341]\n",
      "loss: 0.655427  [27200/175341]\n",
      "loss: 0.495975  [28800/175341]\n",
      "loss: 1.674361  [30400/175341]\n",
      "loss: 0.739093  [32000/175341]\n",
      "loss: 0.374513  [33600/175341]\n",
      "loss: 0.495582  [35200/175341]\n",
      "loss: 0.869466  [36800/175341]\n",
      "loss: 0.369145  [38400/175341]\n",
      "loss: 0.495682  [40000/175341]\n",
      "loss: 0.557295  [41600/175341]\n",
      "loss: 0.733834  [43200/175341]\n",
      "loss: 0.869958  [44800/175341]\n",
      "loss: 0.415279  [46400/175341]\n",
      "loss: 0.555431  [48000/175341]\n",
      "loss: 0.169438  [49600/175341]\n",
      "loss: 0.364508  [51200/175341]\n",
      "loss: 0.158233  [52800/175341]\n",
      "loss: 0.453317  [54400/175341]\n",
      "loss: 0.388639  [56000/175341]\n",
      "loss: 0.518958  [57600/175341]\n",
      "loss: 0.962305  [59200/175341]\n",
      "loss: 0.565581  [60800/175341]\n",
      "loss: 0.670480  [62400/175341]\n",
      "loss: 0.510083  [64000/175341]\n",
      "loss: 0.662966  [65600/175341]\n",
      "loss: 0.308099  [67200/175341]\n",
      "loss: 0.527740  [68800/175341]\n",
      "loss: 0.673953  [70400/175341]\n",
      "loss: 0.645377  [72000/175341]\n",
      "loss: 0.629273  [73600/175341]\n",
      "loss: 0.280676  [75200/175341]\n",
      "loss: 0.928202  [76800/175341]\n",
      "loss: 0.765029  [78400/175341]\n",
      "loss: 0.791821  [80000/175341]\n",
      "loss: 0.600739  [81600/175341]\n",
      "loss: 0.784180  [83200/175341]\n",
      "loss: 0.983746  [84800/175341]\n",
      "loss: 1.368787  [86400/175341]\n",
      "loss: 0.783916  [88000/175341]\n",
      "loss: 0.695809  [89600/175341]\n",
      "loss: 0.623862  [91200/175341]\n",
      "loss: 0.706526  [92800/175341]\n",
      "loss: 0.353144  [94400/175341]\n",
      "loss: 0.473764  [96000/175341]\n",
      "loss: 0.250686  [97600/175341]\n",
      "loss: 0.425583  [99200/175341]\n",
      "loss: 0.588859  [100800/175341]\n",
      "loss: 0.339206  [102400/175341]\n",
      "loss: 1.030689  [104000/175341]\n",
      "loss: 0.415793  [105600/175341]\n",
      "loss: 0.778702  [107200/175341]\n",
      "loss: 0.241949  [108800/175341]\n",
      "loss: 0.327301  [110400/175341]\n",
      "loss: 0.579507  [112000/175341]\n",
      "loss: 0.333779  [113600/175341]\n",
      "loss: 0.773492  [115200/175341]\n",
      "loss: 0.467670  [116800/175341]\n",
      "loss: 0.651861  [118400/175341]\n",
      "loss: 0.699953  [120000/175341]\n",
      "loss: 0.496165  [121600/175341]\n",
      "loss: 0.551255  [123200/175341]\n",
      "loss: 0.671505  [124800/175341]\n",
      "loss: 1.036300  [126400/175341]\n",
      "loss: 0.270473  [128000/175341]\n",
      "loss: 0.740829  [129600/175341]\n",
      "loss: 0.697483  [131200/175341]\n",
      "loss: 0.652605  [132800/175341]\n",
      "loss: 0.658260  [134400/175341]\n",
      "loss: 0.719173  [136000/175341]\n",
      "loss: 0.522345  [137600/175341]\n",
      "loss: 0.507485  [139200/175341]\n",
      "loss: 0.990380  [140800/175341]\n",
      "loss: 0.666471  [142400/175341]\n",
      "loss: 0.646137  [144000/175341]\n",
      "loss: 0.400307  [145600/175341]\n",
      "loss: 0.323020  [147200/175341]\n",
      "loss: 0.736873  [148800/175341]\n",
      "loss: 0.643881  [150400/175341]\n",
      "loss: 0.479877  [152000/175341]\n",
      "loss: 0.310613  [153600/175341]\n",
      "loss: 0.537784  [155200/175341]\n",
      "loss: 1.176061  [156800/175341]\n",
      "loss: 0.322067  [158400/175341]\n",
      "loss: 0.492602  [160000/175341]\n",
      "loss: 0.525796  [161600/175341]\n",
      "loss: 1.347924  [163200/175341]\n",
      "loss: 0.465411  [164800/175341]\n",
      "loss: 0.489065  [166400/175341]\n",
      "loss: 0.397236  [168000/175341]\n",
      "loss: 0.418375  [169600/175341]\n",
      "loss: 0.289114  [171200/175341]\n",
      "loss: 0.407035  [172800/175341]\n",
      "loss: 0.566739  [174400/175341]\n",
      "Train Accuracy: 77.6339%\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.659349, F1-score: 71.16% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.768787  [    0/175341]\n",
      "loss: 0.524992  [ 1600/175341]\n",
      "loss: 0.377948  [ 3200/175341]\n",
      "loss: 0.618824  [ 4800/175341]\n",
      "loss: 0.912909  [ 6400/175341]\n",
      "loss: 1.127318  [ 8000/175341]\n",
      "loss: 0.618428  [ 9600/175341]\n",
      "loss: 0.415209  [11200/175341]\n",
      "loss: 1.378195  [12800/175341]\n",
      "loss: 0.737499  [14400/175341]\n",
      "loss: 0.534192  [16000/175341]\n",
      "loss: 0.730254  [17600/175341]\n",
      "loss: 0.431252  [19200/175341]\n",
      "loss: 0.278758  [20800/175341]\n",
      "loss: 0.328151  [22400/175341]\n",
      "loss: 0.396845  [24000/175341]\n",
      "loss: 0.732205  [25600/175341]\n",
      "loss: 0.648254  [27200/175341]\n",
      "loss: 0.334032  [28800/175341]\n",
      "loss: 0.360255  [30400/175341]\n",
      "loss: 0.527480  [32000/175341]\n",
      "loss: 0.605399  [33600/175341]\n",
      "loss: 0.762487  [35200/175341]\n",
      "loss: 0.390906  [36800/175341]\n",
      "loss: 0.479306  [38400/175341]\n",
      "loss: 0.403995  [40000/175341]\n",
      "loss: 0.706615  [41600/175341]\n",
      "loss: 0.512243  [43200/175341]\n",
      "loss: 0.723041  [44800/175341]\n",
      "loss: 0.485528  [46400/175341]\n",
      "loss: 0.301633  [48000/175341]\n",
      "loss: 0.640883  [49600/175341]\n",
      "loss: 0.261355  [51200/175341]\n",
      "loss: 1.344630  [52800/175341]\n",
      "loss: 0.529969  [54400/175341]\n",
      "loss: 0.497938  [56000/175341]\n",
      "loss: 0.537689  [57600/175341]\n",
      "loss: 0.557036  [59200/175341]\n",
      "loss: 0.703125  [60800/175341]\n",
      "loss: 0.298637  [62400/175341]\n",
      "loss: 0.727285  [64000/175341]\n",
      "loss: 0.719144  [65600/175341]\n",
      "loss: 0.625213  [67200/175341]\n",
      "loss: 0.548903  [68800/175341]\n",
      "loss: 0.308341  [70400/175341]\n",
      "loss: 0.392892  [72000/175341]\n",
      "loss: 0.832385  [73600/175341]\n",
      "loss: 0.425207  [75200/175341]\n",
      "loss: 0.635862  [76800/175341]\n",
      "loss: 0.957370  [78400/175341]\n",
      "loss: 0.479644  [80000/175341]\n",
      "loss: 0.455049  [81600/175341]\n",
      "loss: 0.684770  [83200/175341]\n",
      "loss: 0.738111  [84800/175341]\n",
      "loss: 0.991791  [86400/175341]\n",
      "loss: 0.480204  [88000/175341]\n",
      "loss: 0.594721  [89600/175341]\n",
      "loss: 0.494039  [91200/175341]\n",
      "loss: 0.689363  [92800/175341]\n",
      "loss: 1.023409  [94400/175341]\n",
      "loss: 0.690373  [96000/175341]\n",
      "loss: 0.614790  [97600/175341]\n",
      "loss: 0.689135  [99200/175341]\n",
      "loss: 0.627778  [100800/175341]\n",
      "loss: 0.798089  [102400/175341]\n",
      "loss: 0.576018  [104000/175341]\n",
      "loss: 0.617361  [105600/175341]\n",
      "loss: 0.633546  [107200/175341]\n",
      "loss: 0.535252  [108800/175341]\n",
      "loss: 0.634156  [110400/175341]\n",
      "loss: 1.049870  [112000/175341]\n",
      "loss: 0.276682  [113600/175341]\n",
      "loss: 0.350018  [115200/175341]\n",
      "loss: 0.905577  [116800/175341]\n",
      "loss: 1.093914  [118400/175341]\n",
      "loss: 0.382210  [120000/175341]\n",
      "loss: 0.881418  [121600/175341]\n",
      "loss: 0.423231  [123200/175341]\n",
      "loss: 0.496433  [124800/175341]\n",
      "loss: 0.624402  [126400/175341]\n",
      "loss: 0.361898  [128000/175341]\n",
      "loss: 0.346129  [129600/175341]\n",
      "loss: 0.834544  [131200/175341]\n",
      "loss: 0.624023  [132800/175341]\n",
      "loss: 0.841379  [134400/175341]\n",
      "loss: 0.650969  [136000/175341]\n",
      "loss: 0.535146  [137600/175341]\n",
      "loss: 0.414795  [139200/175341]\n",
      "loss: 0.786316  [140800/175341]\n",
      "loss: 0.217691  [142400/175341]\n",
      "loss: 0.729682  [144000/175341]\n",
      "loss: 0.491013  [145600/175341]\n",
      "loss: 1.095497  [147200/175341]\n",
      "loss: 0.745949  [148800/175341]\n",
      "loss: 0.507769  [150400/175341]\n",
      "loss: 0.856758  [152000/175341]\n",
      "loss: 0.509631  [153600/175341]\n",
      "loss: 0.522985  [155200/175341]\n",
      "loss: 0.350919  [156800/175341]\n",
      "loss: 0.642025  [158400/175341]\n",
      "loss: 0.496762  [160000/175341]\n",
      "loss: 1.152629  [161600/175341]\n",
      "loss: 0.654783  [163200/175341]\n",
      "loss: 0.452480  [164800/175341]\n",
      "loss: 0.749564  [166400/175341]\n",
      "loss: 0.726657  [168000/175341]\n",
      "loss: 0.303491  [169600/175341]\n",
      "loss: 0.651501  [171200/175341]\n",
      "loss: 0.820618  [172800/175341]\n",
      "loss: 0.759975  [174400/175341]\n",
      "Train Accuracy: 77.7525%\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.651495, F1-score: 71.41% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.222334  [    0/175341]\n",
      "loss: 0.332324  [ 1600/175341]\n",
      "loss: 0.892382  [ 3200/175341]\n",
      "loss: 0.810954  [ 4800/175341]\n",
      "loss: 1.247538  [ 6400/175341]\n",
      "loss: 0.672105  [ 8000/175341]\n",
      "loss: 0.607490  [ 9600/175341]\n",
      "loss: 1.005394  [11200/175341]\n",
      "loss: 0.820588  [12800/175341]\n",
      "loss: 0.861584  [14400/175341]\n",
      "loss: 0.428441  [16000/175341]\n",
      "loss: 0.689861  [17600/175341]\n",
      "loss: 0.503572  [19200/175341]\n",
      "loss: 1.116415  [20800/175341]\n",
      "loss: 0.506224  [22400/175341]\n",
      "loss: 0.338644  [24000/175341]\n",
      "loss: 0.441051  [25600/175341]\n",
      "loss: 0.661246  [27200/175341]\n",
      "loss: 0.274870  [28800/175341]\n",
      "loss: 0.570747  [30400/175341]\n",
      "loss: 0.799221  [32000/175341]\n",
      "loss: 0.675474  [33600/175341]\n",
      "loss: 0.544744  [35200/175341]\n",
      "loss: 0.499730  [36800/175341]\n",
      "loss: 0.812630  [38400/175341]\n",
      "loss: 0.607889  [40000/175341]\n",
      "loss: 0.564328  [41600/175341]\n",
      "loss: 0.981140  [43200/175341]\n",
      "loss: 0.212453  [44800/175341]\n",
      "loss: 0.482955  [46400/175341]\n",
      "loss: 0.477071  [48000/175341]\n",
      "loss: 0.211518  [49600/175341]\n",
      "loss: 0.373370  [51200/175341]\n",
      "loss: 1.016692  [52800/175341]\n",
      "loss: 0.766415  [54400/175341]\n",
      "loss: 0.358222  [56000/175341]\n",
      "loss: 0.478271  [57600/175341]\n",
      "loss: 0.987790  [59200/175341]\n",
      "loss: 0.369191  [60800/175341]\n",
      "loss: 1.203104  [62400/175341]\n",
      "loss: 0.365195  [64000/175341]\n",
      "loss: 0.661470  [65600/175341]\n",
      "loss: 0.234232  [67200/175341]\n",
      "loss: 0.482290  [68800/175341]\n",
      "loss: 0.467947  [70400/175341]\n",
      "loss: 0.395672  [72000/175341]\n",
      "loss: 0.512933  [73600/175341]\n",
      "loss: 0.259664  [75200/175341]\n",
      "loss: 0.946792  [76800/175341]\n",
      "loss: 0.564956  [78400/175341]\n",
      "loss: 0.494962  [80000/175341]\n",
      "loss: 0.488316  [81600/175341]\n",
      "loss: 0.401328  [83200/175341]\n",
      "loss: 0.581950  [84800/175341]\n",
      "loss: 0.690945  [86400/175341]\n",
      "loss: 0.852829  [88000/175341]\n",
      "loss: 0.898775  [89600/175341]\n",
      "loss: 0.414887  [91200/175341]\n",
      "loss: 0.426163  [92800/175341]\n",
      "loss: 0.883878  [94400/175341]\n",
      "loss: 1.177566  [96000/175341]\n",
      "loss: 0.846348  [97600/175341]\n",
      "loss: 0.438145  [99200/175341]\n",
      "loss: 0.461835  [100800/175341]\n",
      "loss: 0.558038  [102400/175341]\n",
      "loss: 0.884030  [104000/175341]\n",
      "loss: 0.806161  [105600/175341]\n",
      "loss: 0.465440  [107200/175341]\n",
      "loss: 0.688069  [108800/175341]\n",
      "loss: 0.310261  [110400/175341]\n",
      "loss: 0.483168  [112000/175341]\n",
      "loss: 0.645815  [113600/175341]\n",
      "loss: 0.522111  [115200/175341]\n",
      "loss: 0.319647  [116800/175341]\n",
      "loss: 0.483262  [118400/175341]\n",
      "loss: 0.989763  [120000/175341]\n",
      "loss: 0.629573  [121600/175341]\n",
      "loss: 0.964017  [123200/175341]\n",
      "loss: 0.257169  [124800/175341]\n",
      "loss: 0.524611  [126400/175341]\n",
      "loss: 0.389430  [128000/175341]\n",
      "loss: 0.934792  [129600/175341]\n",
      "loss: 0.525670  [131200/175341]\n",
      "loss: 0.719621  [132800/175341]\n",
      "loss: 0.380634  [134400/175341]\n",
      "loss: 0.279658  [136000/175341]\n",
      "loss: 0.842023  [137600/175341]\n",
      "loss: 0.578243  [139200/175341]\n",
      "loss: 0.413742  [140800/175341]\n",
      "loss: 0.700242  [142400/175341]\n",
      "loss: 0.350934  [144000/175341]\n",
      "loss: 0.289607  [145600/175341]\n",
      "loss: 0.857955  [147200/175341]\n",
      "loss: 0.449251  [148800/175341]\n",
      "loss: 0.948530  [150400/175341]\n",
      "loss: 0.428642  [152000/175341]\n",
      "loss: 0.568668  [153600/175341]\n",
      "loss: 0.527413  [155200/175341]\n",
      "loss: 0.536299  [156800/175341]\n",
      "loss: 0.749767  [158400/175341]\n",
      "loss: 0.517072  [160000/175341]\n",
      "loss: 0.487665  [161600/175341]\n",
      "loss: 0.767596  [163200/175341]\n",
      "loss: 0.337167  [164800/175341]\n",
      "loss: 0.627334  [166400/175341]\n",
      "loss: 0.620465  [168000/175341]\n",
      "loss: 0.514024  [169600/175341]\n",
      "loss: 0.245411  [171200/175341]\n",
      "loss: 0.102385  [172800/175341]\n",
      "loss: 0.444379  [174400/175341]\n",
      "Train Accuracy: 77.9224%\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.651250, F1-score: 71.58% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.233532  [    0/175341]\n",
      "loss: 0.433709  [ 1600/175341]\n",
      "loss: 0.956323  [ 3200/175341]\n",
      "loss: 1.129884  [ 4800/175341]\n",
      "loss: 0.669306  [ 6400/175341]\n",
      "loss: 0.265008  [ 8000/175341]\n",
      "loss: 1.272198  [ 9600/175341]\n",
      "loss: 0.413177  [11200/175341]\n",
      "loss: 0.223118  [12800/175341]\n",
      "loss: 0.576574  [14400/175341]\n",
      "loss: 0.707117  [16000/175341]\n",
      "loss: 0.296261  [17600/175341]\n",
      "loss: 0.616691  [19200/175341]\n",
      "loss: 0.535230  [20800/175341]\n",
      "loss: 0.507989  [22400/175341]\n",
      "loss: 0.605137  [24000/175341]\n",
      "loss: 0.226335  [25600/175341]\n",
      "loss: 0.585849  [27200/175341]\n",
      "loss: 0.958865  [28800/175341]\n",
      "loss: 0.353992  [30400/175341]\n",
      "loss: 0.885109  [32000/175341]\n",
      "loss: 0.715518  [33600/175341]\n",
      "loss: 0.673434  [35200/175341]\n",
      "loss: 0.822682  [36800/175341]\n",
      "loss: 0.743684  [38400/175341]\n",
      "loss: 0.998905  [40000/175341]\n",
      "loss: 0.529892  [41600/175341]\n",
      "loss: 0.533611  [43200/175341]\n",
      "loss: 0.933759  [44800/175341]\n",
      "loss: 0.756479  [46400/175341]\n",
      "loss: 0.385140  [48000/175341]\n",
      "loss: 0.187214  [49600/175341]\n",
      "loss: 0.408998  [51200/175341]\n",
      "loss: 0.619532  [52800/175341]\n",
      "loss: 1.187195  [54400/175341]\n",
      "loss: 0.499013  [56000/175341]\n",
      "loss: 0.501316  [57600/175341]\n",
      "loss: 0.644558  [59200/175341]\n",
      "loss: 0.275503  [60800/175341]\n",
      "loss: 0.056826  [62400/175341]\n",
      "loss: 1.106962  [64000/175341]\n",
      "loss: 0.630535  [65600/175341]\n",
      "loss: 0.680562  [67200/175341]\n",
      "loss: 1.110609  [68800/175341]\n",
      "loss: 0.286230  [70400/175341]\n",
      "loss: 1.072028  [72000/175341]\n",
      "loss: 0.410766  [73600/175341]\n",
      "loss: 0.823955  [75200/175341]\n",
      "loss: 0.579189  [76800/175341]\n",
      "loss: 0.331608  [78400/175341]\n",
      "loss: 0.539628  [80000/175341]\n",
      "loss: 0.538455  [81600/175341]\n",
      "loss: 0.628091  [83200/175341]\n",
      "loss: 0.738484  [84800/175341]\n",
      "loss: 1.152279  [86400/175341]\n",
      "loss: 0.693684  [88000/175341]\n",
      "loss: 0.225207  [89600/175341]\n",
      "loss: 0.395869  [91200/175341]\n",
      "loss: 0.478003  [92800/175341]\n",
      "loss: 0.957735  [94400/175341]\n",
      "loss: 1.017709  [96000/175341]\n",
      "loss: 0.514545  [97600/175341]\n",
      "loss: 0.521364  [99200/175341]\n",
      "loss: 0.684421  [100800/175341]\n",
      "loss: 0.458122  [102400/175341]\n",
      "loss: 0.552448  [104000/175341]\n",
      "loss: 0.359825  [105600/175341]\n",
      "loss: 0.608935  [107200/175341]\n",
      "loss: 0.627840  [108800/175341]\n",
      "loss: 0.348931  [110400/175341]\n",
      "loss: 0.299751  [112000/175341]\n",
      "loss: 0.568059  [113600/175341]\n",
      "loss: 0.446923  [115200/175341]\n",
      "loss: 0.702362  [116800/175341]\n",
      "loss: 1.009168  [118400/175341]\n",
      "loss: 0.583404  [120000/175341]\n",
      "loss: 0.569687  [121600/175341]\n",
      "loss: 0.624561  [123200/175341]\n",
      "loss: 0.884966  [124800/175341]\n",
      "loss: 0.640184  [126400/175341]\n",
      "loss: 0.382147  [128000/175341]\n",
      "loss: 0.712440  [129600/175341]\n",
      "loss: 0.271866  [131200/175341]\n",
      "loss: 0.723242  [132800/175341]\n",
      "loss: 0.391865  [134400/175341]\n",
      "loss: 0.540311  [136000/175341]\n",
      "loss: 0.219842  [137600/175341]\n",
      "loss: 0.381737  [139200/175341]\n",
      "loss: 0.557799  [140800/175341]\n",
      "loss: 0.526020  [142400/175341]\n",
      "loss: 0.651378  [144000/175341]\n",
      "loss: 0.679700  [145600/175341]\n",
      "loss: 0.424726  [147200/175341]\n",
      "loss: 0.376103  [148800/175341]\n",
      "loss: 0.739311  [150400/175341]\n",
      "loss: 0.465559  [152000/175341]\n",
      "loss: 0.966907  [153600/175341]\n",
      "loss: 0.504847  [155200/175341]\n",
      "loss: 0.661658  [156800/175341]\n",
      "loss: 0.501191  [158400/175341]\n",
      "loss: 0.486367  [160000/175341]\n",
      "loss: 0.673838  [161600/175341]\n",
      "loss: 0.335471  [163200/175341]\n",
      "loss: 0.547796  [164800/175341]\n",
      "loss: 0.620367  [166400/175341]\n",
      "loss: 0.509180  [168000/175341]\n",
      "loss: 0.602944  [169600/175341]\n",
      "loss: 0.566391  [171200/175341]\n",
      "loss: 0.343727  [172800/175341]\n",
      "loss: 0.443258  [174400/175341]\n",
      "Train Accuracy: 78.0759%\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.622328, F1-score: 71.80% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.693763  [    0/175341]\n",
      "loss: 0.732730  [ 1600/175341]\n",
      "loss: 0.952141  [ 3200/175341]\n",
      "loss: 0.266849  [ 4800/175341]\n",
      "loss: 0.306027  [ 6400/175341]\n",
      "loss: 0.588422  [ 8000/175341]\n",
      "loss: 0.574955  [ 9600/175341]\n",
      "loss: 0.538852  [11200/175341]\n",
      "loss: 0.632896  [12800/175341]\n",
      "loss: 0.468946  [14400/175341]\n",
      "loss: 0.540330  [16000/175341]\n",
      "loss: 0.274185  [17600/175341]\n",
      "loss: 0.509597  [19200/175341]\n",
      "loss: 0.714319  [20800/175341]\n",
      "loss: 0.607063  [22400/175341]\n",
      "loss: 0.585283  [24000/175341]\n",
      "loss: 0.589862  [25600/175341]\n",
      "loss: 0.548405  [27200/175341]\n",
      "loss: 0.667934  [28800/175341]\n",
      "loss: 0.519784  [30400/175341]\n",
      "loss: 0.447867  [32000/175341]\n",
      "loss: 0.372801  [33600/175341]\n",
      "loss: 0.490225  [35200/175341]\n",
      "loss: 0.478192  [36800/175341]\n",
      "loss: 0.246261  [38400/175341]\n",
      "loss: 0.414820  [40000/175341]\n",
      "loss: 0.387017  [41600/175341]\n",
      "loss: 0.591579  [43200/175341]\n",
      "loss: 0.469259  [44800/175341]\n",
      "loss: 0.275945  [46400/175341]\n",
      "loss: 0.264737  [48000/175341]\n",
      "loss: 0.278822  [49600/175341]\n",
      "loss: 0.665363  [51200/175341]\n",
      "loss: 0.842323  [52800/175341]\n",
      "loss: 0.738573  [54400/175341]\n",
      "loss: 0.665116  [56000/175341]\n",
      "loss: 0.869643  [57600/175341]\n",
      "loss: 0.423973  [59200/175341]\n",
      "loss: 0.437357  [60800/175341]\n",
      "loss: 0.899536  [62400/175341]\n",
      "loss: 0.334333  [64000/175341]\n",
      "loss: 0.469891  [65600/175341]\n",
      "loss: 0.317136  [67200/175341]\n",
      "loss: 0.531697  [68800/175341]\n",
      "loss: 0.468706  [70400/175341]\n",
      "loss: 0.357100  [72000/175341]\n",
      "loss: 0.684804  [73600/175341]\n",
      "loss: 0.408999  [75200/175341]\n",
      "loss: 0.330634  [76800/175341]\n",
      "loss: 0.634226  [78400/175341]\n",
      "loss: 0.428257  [80000/175341]\n",
      "loss: 0.645719  [81600/175341]\n",
      "loss: 0.460255  [83200/175341]\n",
      "loss: 0.440054  [84800/175341]\n",
      "loss: 0.567195  [86400/175341]\n",
      "loss: 0.712748  [88000/175341]\n",
      "loss: 0.643763  [89600/175341]\n",
      "loss: 0.552156  [91200/175341]\n",
      "loss: 0.203319  [92800/175341]\n",
      "loss: 0.241394  [94400/175341]\n",
      "loss: 0.609092  [96000/175341]\n",
      "loss: 0.586371  [97600/175341]\n",
      "loss: 0.633362  [99200/175341]\n",
      "loss: 0.709869  [100800/175341]\n",
      "loss: 0.695294  [102400/175341]\n",
      "loss: 0.633514  [104000/175341]\n",
      "loss: 0.532518  [105600/175341]\n",
      "loss: 0.293425  [107200/175341]\n",
      "loss: 0.412405  [108800/175341]\n",
      "loss: 0.507599  [110400/175341]\n",
      "loss: 0.808762  [112000/175341]\n",
      "loss: 0.319942  [113600/175341]\n",
      "loss: 0.620164  [115200/175341]\n",
      "loss: 0.802607  [116800/175341]\n",
      "loss: 0.308542  [118400/175341]\n",
      "loss: 0.636883  [120000/175341]\n",
      "loss: 0.616931  [121600/175341]\n",
      "loss: 0.579973  [123200/175341]\n",
      "loss: 0.454797  [124800/175341]\n",
      "loss: 0.485453  [126400/175341]\n",
      "loss: 0.306091  [128000/175341]\n",
      "loss: 0.907782  [129600/175341]\n",
      "loss: 0.722826  [131200/175341]\n",
      "loss: 0.500364  [132800/175341]\n",
      "loss: 0.428082  [134400/175341]\n",
      "loss: 0.784395  [136000/175341]\n",
      "loss: 0.430772  [137600/175341]\n",
      "loss: 0.263335  [139200/175341]\n",
      "loss: 0.476808  [140800/175341]\n",
      "loss: 0.695631  [142400/175341]\n",
      "loss: 1.200465  [144000/175341]\n",
      "loss: 0.668256  [145600/175341]\n",
      "loss: 0.674906  [147200/175341]\n",
      "loss: 0.701118  [148800/175341]\n",
      "loss: 0.406604  [150400/175341]\n",
      "loss: 0.866000  [152000/175341]\n",
      "loss: 0.416794  [153600/175341]\n",
      "loss: 0.497374  [155200/175341]\n",
      "loss: 0.958885  [156800/175341]\n",
      "loss: 0.487963  [158400/175341]\n",
      "loss: 0.690553  [160000/175341]\n",
      "loss: 0.806450  [161600/175341]\n",
      "loss: 0.238086  [163200/175341]\n",
      "loss: 0.500971  [164800/175341]\n",
      "loss: 0.755605  [166400/175341]\n",
      "loss: 0.965227  [168000/175341]\n",
      "loss: 0.615961  [169600/175341]\n",
      "loss: 0.500555  [171200/175341]\n",
      "loss: 0.610589  [172800/175341]\n",
      "loss: 0.582229  [174400/175341]\n",
      "Train Accuracy: 78.2356%\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.630614, F1-score: 72.38% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.785980  [    0/175341]\n",
      "loss: 0.762720  [ 1600/175341]\n",
      "loss: 0.643233  [ 3200/175341]\n",
      "loss: 0.318662  [ 4800/175341]\n",
      "loss: 0.648910  [ 6400/175341]\n",
      "loss: 0.423505  [ 8000/175341]\n",
      "loss: 0.846449  [ 9600/175341]\n",
      "loss: 0.542031  [11200/175341]\n",
      "loss: 0.433809  [12800/175341]\n",
      "loss: 0.837699  [14400/175341]\n",
      "loss: 0.462227  [16000/175341]\n",
      "loss: 0.799026  [17600/175341]\n",
      "loss: 0.541861  [19200/175341]\n",
      "loss: 0.295273  [20800/175341]\n",
      "loss: 0.644985  [22400/175341]\n",
      "loss: 0.763872  [24000/175341]\n",
      "loss: 1.027089  [25600/175341]\n",
      "loss: 0.447909  [27200/175341]\n",
      "loss: 0.369347  [28800/175341]\n",
      "loss: 0.327235  [30400/175341]\n",
      "loss: 0.545288  [32000/175341]\n",
      "loss: 0.361307  [33600/175341]\n",
      "loss: 0.657130  [35200/175341]\n",
      "loss: 0.755476  [36800/175341]\n",
      "loss: 0.699243  [38400/175341]\n",
      "loss: 0.616957  [40000/175341]\n",
      "loss: 0.380245  [41600/175341]\n",
      "loss: 0.632840  [43200/175341]\n",
      "loss: 0.734928  [44800/175341]\n",
      "loss: 0.632777  [46400/175341]\n",
      "loss: 0.660222  [48000/175341]\n",
      "loss: 0.503369  [49600/175341]\n",
      "loss: 0.498048  [51200/175341]\n",
      "loss: 0.264918  [52800/175341]\n",
      "loss: 0.368414  [54400/175341]\n",
      "loss: 0.767576  [56000/175341]\n",
      "loss: 0.614117  [57600/175341]\n",
      "loss: 0.368698  [59200/175341]\n",
      "loss: 0.752148  [60800/175341]\n",
      "loss: 0.548765  [62400/175341]\n",
      "loss: 0.540491  [64000/175341]\n",
      "loss: 0.330687  [65600/175341]\n",
      "loss: 0.522216  [67200/175341]\n",
      "loss: 0.114806  [68800/175341]\n",
      "loss: 0.472633  [70400/175341]\n",
      "loss: 0.860361  [72000/175341]\n",
      "loss: 0.299980  [73600/175341]\n",
      "loss: 0.276563  [75200/175341]\n",
      "loss: 0.892175  [76800/175341]\n",
      "loss: 0.588095  [78400/175341]\n",
      "loss: 0.248724  [80000/175341]\n",
      "loss: 0.318163  [81600/175341]\n",
      "loss: 0.817490  [83200/175341]\n",
      "loss: 0.819029  [84800/175341]\n",
      "loss: 0.385965  [86400/175341]\n",
      "loss: 0.292317  [88000/175341]\n",
      "loss: 0.651257  [89600/175341]\n",
      "loss: 0.685926  [91200/175341]\n",
      "loss: 0.786400  [92800/175341]\n",
      "loss: 0.863260  [94400/175341]\n",
      "loss: 0.754937  [96000/175341]\n",
      "loss: 0.597641  [97600/175341]\n",
      "loss: 0.387574  [99200/175341]\n",
      "loss: 0.647186  [100800/175341]\n",
      "loss: 0.525427  [102400/175341]\n",
      "loss: 0.678865  [104000/175341]\n",
      "loss: 0.973323  [105600/175341]\n",
      "loss: 0.620377  [107200/175341]\n",
      "loss: 0.591743  [108800/175341]\n",
      "loss: 0.610202  [110400/175341]\n",
      "loss: 0.679725  [112000/175341]\n",
      "loss: 0.601163  [113600/175341]\n",
      "loss: 0.356426  [115200/175341]\n",
      "loss: 0.244087  [116800/175341]\n",
      "loss: 0.676166  [118400/175341]\n",
      "loss: 0.333916  [120000/175341]\n",
      "loss: 1.023635  [121600/175341]\n",
      "loss: 0.193611  [123200/175341]\n",
      "loss: 1.007988  [124800/175341]\n",
      "loss: 0.937296  [126400/175341]\n",
      "loss: 0.550915  [128000/175341]\n",
      "loss: 0.972291  [129600/175341]\n",
      "loss: 0.662600  [131200/175341]\n",
      "loss: 0.774977  [132800/175341]\n",
      "loss: 0.621793  [134400/175341]\n",
      "loss: 0.807803  [136000/175341]\n",
      "loss: 1.420560  [137600/175341]\n",
      "loss: 0.626326  [139200/175341]\n",
      "loss: 0.287951  [140800/175341]\n",
      "loss: 0.643817  [142400/175341]\n",
      "loss: 0.268236  [144000/175341]\n",
      "loss: 0.473392  [145600/175341]\n",
      "loss: 0.617158  [147200/175341]\n",
      "loss: 0.358591  [148800/175341]\n",
      "loss: 0.676762  [150400/175341]\n",
      "loss: 0.323350  [152000/175341]\n",
      "loss: 0.489389  [153600/175341]\n",
      "loss: 0.237575  [155200/175341]\n",
      "loss: 0.647363  [156800/175341]\n",
      "loss: 0.730275  [158400/175341]\n",
      "loss: 0.486592  [160000/175341]\n",
      "loss: 0.709272  [161600/175341]\n",
      "loss: 0.633740  [163200/175341]\n",
      "loss: 0.558950  [164800/175341]\n",
      "loss: 0.807892  [166400/175341]\n",
      "loss: 0.409185  [168000/175341]\n",
      "loss: 0.350888  [169600/175341]\n",
      "loss: 0.472907  [171200/175341]\n",
      "loss: 0.646584  [172800/175341]\n",
      "loss: 0.340115  [174400/175341]\n",
      "Train Accuracy: 78.4648%\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.631385, F1-score: 72.49% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.512735  [    0/175341]\n",
      "loss: 0.155975  [ 1600/175341]\n",
      "loss: 0.417616  [ 3200/175341]\n",
      "loss: 0.239353  [ 4800/175341]\n",
      "loss: 0.791150  [ 6400/175341]\n",
      "loss: 0.703350  [ 8000/175341]\n",
      "loss: 0.296520  [ 9600/175341]\n",
      "loss: 0.390948  [11200/175341]\n",
      "loss: 0.666850  [12800/175341]\n",
      "loss: 0.346475  [14400/175341]\n",
      "loss: 0.706798  [16000/175341]\n",
      "loss: 0.650947  [17600/175341]\n",
      "loss: 0.438475  [19200/175341]\n",
      "loss: 0.826886  [20800/175341]\n",
      "loss: 0.503379  [22400/175341]\n",
      "loss: 0.543915  [24000/175341]\n",
      "loss: 0.684127  [25600/175341]\n",
      "loss: 0.430287  [27200/175341]\n",
      "loss: 0.164813  [28800/175341]\n",
      "loss: 0.832377  [30400/175341]\n",
      "loss: 0.321493  [32000/175341]\n",
      "loss: 0.625546  [33600/175341]\n",
      "loss: 0.385436  [35200/175341]\n",
      "loss: 0.378935  [36800/175341]\n",
      "loss: 0.666262  [38400/175341]\n",
      "loss: 0.514358  [40000/175341]\n",
      "loss: 0.785845  [41600/175341]\n",
      "loss: 0.204678  [43200/175341]\n",
      "loss: 0.328268  [44800/175341]\n",
      "loss: 0.617377  [46400/175341]\n",
      "loss: 0.228646  [48000/175341]\n",
      "loss: 0.328818  [49600/175341]\n",
      "loss: 0.582471  [51200/175341]\n",
      "loss: 0.468365  [52800/175341]\n",
      "loss: 0.359693  [54400/175341]\n",
      "loss: 0.576542  [56000/175341]\n",
      "loss: 0.303838  [57600/175341]\n",
      "loss: 0.447306  [59200/175341]\n",
      "loss: 0.318293  [60800/175341]\n",
      "loss: 0.944531  [62400/175341]\n",
      "loss: 0.529048  [64000/175341]\n",
      "loss: 0.637561  [65600/175341]\n",
      "loss: 0.584255  [67200/175341]\n",
      "loss: 0.634773  [68800/175341]\n",
      "loss: 0.572643  [70400/175341]\n",
      "loss: 0.996646  [72000/175341]\n",
      "loss: 0.472324  [73600/175341]\n",
      "loss: 0.381653  [75200/175341]\n",
      "loss: 0.728921  [76800/175341]\n",
      "loss: 0.344690  [78400/175341]\n",
      "loss: 0.465711  [80000/175341]\n",
      "loss: 0.983072  [81600/175341]\n",
      "loss: 0.792498  [83200/175341]\n",
      "loss: 0.624795  [84800/175341]\n",
      "loss: 0.477695  [86400/175341]\n",
      "loss: 0.695410  [88000/175341]\n",
      "loss: 0.194204  [89600/175341]\n",
      "loss: 0.470971  [91200/175341]\n",
      "loss: 0.634601  [92800/175341]\n",
      "loss: 0.902589  [94400/175341]\n",
      "loss: 0.489826  [96000/175341]\n",
      "loss: 0.730771  [97600/175341]\n",
      "loss: 0.438440  [99200/175341]\n",
      "loss: 0.568085  [100800/175341]\n",
      "loss: 0.271125  [102400/175341]\n",
      "loss: 0.503066  [104000/175341]\n",
      "loss: 0.182925  [105600/175341]\n",
      "loss: 0.822300  [107200/175341]\n",
      "loss: 0.585011  [108800/175341]\n",
      "loss: 0.410882  [110400/175341]\n",
      "loss: 0.699572  [112000/175341]\n",
      "loss: 0.501117  [113600/175341]\n",
      "loss: 0.369023  [115200/175341]\n",
      "loss: 0.514480  [116800/175341]\n",
      "loss: 0.576713  [118400/175341]\n",
      "loss: 0.508058  [120000/175341]\n",
      "loss: 0.816715  [121600/175341]\n",
      "loss: 0.580052  [123200/175341]\n",
      "loss: 0.629848  [124800/175341]\n",
      "loss: 0.912828  [126400/175341]\n",
      "loss: 0.422122  [128000/175341]\n",
      "loss: 0.596822  [129600/175341]\n",
      "loss: 0.813087  [131200/175341]\n",
      "loss: 0.332769  [132800/175341]\n",
      "loss: 0.507604  [134400/175341]\n",
      "loss: 0.451840  [136000/175341]\n",
      "loss: 0.495050  [137600/175341]\n",
      "loss: 0.428078  [139200/175341]\n",
      "loss: 0.200305  [140800/175341]\n",
      "loss: 0.228583  [142400/175341]\n",
      "loss: 0.691611  [144000/175341]\n",
      "loss: 0.449699  [145600/175341]\n",
      "loss: 0.409950  [147200/175341]\n",
      "loss: 1.099246  [148800/175341]\n",
      "loss: 0.258768  [150400/175341]\n",
      "loss: 0.545728  [152000/175341]\n",
      "loss: 0.900701  [153600/175341]\n",
      "loss: 0.558741  [155200/175341]\n",
      "loss: 0.542600  [156800/175341]\n",
      "loss: 0.939015  [158400/175341]\n",
      "loss: 0.329818  [160000/175341]\n",
      "loss: 0.457397  [161600/175341]\n",
      "loss: 0.469841  [163200/175341]\n",
      "loss: 0.894056  [164800/175341]\n",
      "loss: 0.741184  [166400/175341]\n",
      "loss: 0.658241  [168000/175341]\n",
      "loss: 0.503206  [169600/175341]\n",
      "loss: 0.491885  [171200/175341]\n",
      "loss: 0.267746  [172800/175341]\n",
      "loss: 0.778814  [174400/175341]\n",
      "Train Accuracy: 78.5760%\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.615281, F1-score: 73.29% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.485730  [    0/175341]\n",
      "loss: 0.389363  [ 1600/175341]\n",
      "loss: 0.775316  [ 3200/175341]\n",
      "loss: 0.597717  [ 4800/175341]\n",
      "loss: 0.506149  [ 6400/175341]\n",
      "loss: 0.654437  [ 8000/175341]\n",
      "loss: 0.361782  [ 9600/175341]\n",
      "loss: 0.486662  [11200/175341]\n",
      "loss: 0.368273  [12800/175341]\n",
      "loss: 1.136940  [14400/175341]\n",
      "loss: 0.348357  [16000/175341]\n",
      "loss: 0.908327  [17600/175341]\n",
      "loss: 0.681865  [19200/175341]\n",
      "loss: 0.575661  [20800/175341]\n",
      "loss: 0.346640  [22400/175341]\n",
      "loss: 0.482309  [24000/175341]\n",
      "loss: 0.596393  [25600/175341]\n",
      "loss: 0.471677  [27200/175341]\n",
      "loss: 0.467137  [28800/175341]\n",
      "loss: 0.934880  [30400/175341]\n",
      "loss: 0.203722  [32000/175341]\n",
      "loss: 0.604818  [33600/175341]\n",
      "loss: 0.856589  [35200/175341]\n",
      "loss: 0.730299  [36800/175341]\n",
      "loss: 0.356299  [38400/175341]\n",
      "loss: 0.525089  [40000/175341]\n",
      "loss: 0.695728  [41600/175341]\n",
      "loss: 0.194351  [43200/175341]\n",
      "loss: 0.925131  [44800/175341]\n",
      "loss: 0.384060  [46400/175341]\n",
      "loss: 0.328121  [48000/175341]\n",
      "loss: 0.646994  [49600/175341]\n",
      "loss: 0.536320  [51200/175341]\n",
      "loss: 0.842080  [52800/175341]\n",
      "loss: 0.526023  [54400/175341]\n",
      "loss: 0.583708  [56000/175341]\n",
      "loss: 0.557258  [57600/175341]\n",
      "loss: 0.802775  [59200/175341]\n",
      "loss: 0.303164  [60800/175341]\n",
      "loss: 0.925192  [62400/175341]\n",
      "loss: 0.673355  [64000/175341]\n",
      "loss: 0.812133  [65600/175341]\n",
      "loss: 0.689792  [67200/175341]\n",
      "loss: 0.229135  [68800/175341]\n",
      "loss: 0.419187  [70400/175341]\n",
      "loss: 0.660556  [72000/175341]\n",
      "loss: 0.291845  [73600/175341]\n",
      "loss: 1.307129  [75200/175341]\n",
      "loss: 0.516907  [76800/175341]\n",
      "loss: 0.214921  [78400/175341]\n",
      "loss: 0.482507  [80000/175341]\n",
      "loss: 0.504611  [81600/175341]\n",
      "loss: 0.467034  [83200/175341]\n",
      "loss: 0.363608  [84800/175341]\n",
      "loss: 0.213622  [86400/175341]\n",
      "loss: 0.582367  [88000/175341]\n",
      "loss: 0.834932  [89600/175341]\n",
      "loss: 0.570649  [91200/175341]\n",
      "loss: 0.406101  [92800/175341]\n",
      "loss: 0.479121  [94400/175341]\n",
      "loss: 0.439943  [96000/175341]\n",
      "loss: 0.381993  [97600/175341]\n",
      "loss: 0.443099  [99200/175341]\n",
      "loss: 0.782370  [100800/175341]\n",
      "loss: 0.307413  [102400/175341]\n",
      "loss: 0.779263  [104000/175341]\n",
      "loss: 0.166290  [105600/175341]\n",
      "loss: 0.477972  [107200/175341]\n",
      "loss: 0.462326  [108800/175341]\n",
      "loss: 0.844232  [110400/175341]\n",
      "loss: 0.653885  [112000/175341]\n",
      "loss: 0.627470  [113600/175341]\n",
      "loss: 0.287900  [115200/175341]\n",
      "loss: 0.420843  [116800/175341]\n",
      "loss: 0.597760  [118400/175341]\n",
      "loss: 0.189551  [120000/175341]\n",
      "loss: 0.325033  [121600/175341]\n",
      "loss: 0.669950  [123200/175341]\n",
      "loss: 0.738925  [124800/175341]\n",
      "loss: 0.725109  [126400/175341]\n",
      "loss: 0.720576  [128000/175341]\n",
      "loss: 0.363019  [129600/175341]\n",
      "loss: 0.316778  [131200/175341]\n",
      "loss: 0.464674  [132800/175341]\n",
      "loss: 0.524639  [134400/175341]\n",
      "loss: 0.609056  [136000/175341]\n",
      "loss: 0.329235  [137600/175341]\n",
      "loss: 0.341310  [139200/175341]\n",
      "loss: 0.672855  [140800/175341]\n",
      "loss: 0.268117  [142400/175341]\n",
      "loss: 0.299587  [144000/175341]\n",
      "loss: 0.626773  [145600/175341]\n",
      "loss: 0.520612  [147200/175341]\n",
      "loss: 0.438896  [148800/175341]\n",
      "loss: 0.226261  [150400/175341]\n",
      "loss: 0.396269  [152000/175341]\n",
      "loss: 0.539018  [153600/175341]\n",
      "loss: 0.714322  [155200/175341]\n",
      "loss: 0.307298  [156800/175341]\n",
      "loss: 0.662572  [158400/175341]\n",
      "loss: 0.867175  [160000/175341]\n",
      "loss: 0.826132  [161600/175341]\n",
      "loss: 0.516532  [163200/175341]\n",
      "loss: 0.389128  [164800/175341]\n",
      "loss: 0.172575  [166400/175341]\n",
      "loss: 0.819633  [168000/175341]\n",
      "loss: 0.253709  [169600/175341]\n",
      "loss: 0.504352  [171200/175341]\n",
      "loss: 0.813153  [172800/175341]\n",
      "loss: 0.616737  [174400/175341]\n",
      "Train Accuracy: 78.6844%\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.629039, F1-score: 73.14% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.404244  [    0/175341]\n",
      "loss: 0.459756  [ 1600/175341]\n",
      "loss: 0.349597  [ 3200/175341]\n",
      "loss: 0.215738  [ 4800/175341]\n",
      "loss: 0.308823  [ 6400/175341]\n",
      "loss: 0.919737  [ 8000/175341]\n",
      "loss: 0.479138  [ 9600/175341]\n",
      "loss: 0.324171  [11200/175341]\n",
      "loss: 0.404839  [12800/175341]\n",
      "loss: 0.322358  [14400/175341]\n",
      "loss: 0.477528  [16000/175341]\n",
      "loss: 0.612134  [17600/175341]\n",
      "loss: 0.780514  [19200/175341]\n",
      "loss: 0.272740  [20800/175341]\n",
      "loss: 0.501726  [22400/175341]\n",
      "loss: 0.851872  [24000/175341]\n",
      "loss: 0.709088  [25600/175341]\n",
      "loss: 0.601458  [27200/175341]\n",
      "loss: 0.276071  [28800/175341]\n",
      "loss: 0.606353  [30400/175341]\n",
      "loss: 0.592667  [32000/175341]\n",
      "loss: 0.581976  [33600/175341]\n",
      "loss: 0.442152  [35200/175341]\n",
      "loss: 0.598086  [36800/175341]\n",
      "loss: 0.181902  [38400/175341]\n",
      "loss: 0.165094  [40000/175341]\n",
      "loss: 0.318878  [41600/175341]\n",
      "loss: 0.446586  [43200/175341]\n",
      "loss: 0.441908  [44800/175341]\n",
      "loss: 0.182472  [46400/175341]\n",
      "loss: 0.499567  [48000/175341]\n",
      "loss: 0.392021  [49600/175341]\n",
      "loss: 0.476669  [51200/175341]\n",
      "loss: 0.864439  [52800/175341]\n",
      "loss: 0.628545  [54400/175341]\n",
      "loss: 0.666668  [56000/175341]\n",
      "loss: 0.468716  [57600/175341]\n",
      "loss: 0.350922  [59200/175341]\n",
      "loss: 0.624634  [60800/175341]\n",
      "loss: 0.439814  [62400/175341]\n",
      "loss: 0.636159  [64000/175341]\n",
      "loss: 0.306181  [65600/175341]\n",
      "loss: 0.457795  [67200/175341]\n",
      "loss: 0.765197  [68800/175341]\n",
      "loss: 0.467749  [70400/175341]\n",
      "loss: 0.325669  [72000/175341]\n",
      "loss: 0.448407  [73600/175341]\n",
      "loss: 0.603503  [75200/175341]\n",
      "loss: 0.509282  [76800/175341]\n",
      "loss: 0.750616  [78400/175341]\n",
      "loss: 0.732415  [80000/175341]\n",
      "loss: 0.571403  [81600/175341]\n",
      "loss: 0.888451  [83200/175341]\n",
      "loss: 0.543171  [84800/175341]\n",
      "loss: 0.410093  [86400/175341]\n",
      "loss: 0.491820  [88000/175341]\n",
      "loss: 0.285760  [89600/175341]\n",
      "loss: 0.789660  [91200/175341]\n",
      "loss: 0.686783  [92800/175341]\n",
      "loss: 0.499986  [94400/175341]\n",
      "loss: 0.528674  [96000/175341]\n",
      "loss: 0.488427  [97600/175341]\n",
      "loss: 0.570926  [99200/175341]\n",
      "loss: 0.328741  [100800/175341]\n",
      "loss: 0.765769  [102400/175341]\n",
      "loss: 0.277214  [104000/175341]\n",
      "loss: 0.455931  [105600/175341]\n",
      "loss: 0.455454  [107200/175341]\n",
      "loss: 0.211181  [108800/175341]\n",
      "loss: 0.770622  [110400/175341]\n",
      "loss: 0.783690  [112000/175341]\n",
      "loss: 1.249356  [113600/175341]\n",
      "loss: 0.254497  [115200/175341]\n",
      "loss: 0.449006  [116800/175341]\n",
      "loss: 0.412847  [118400/175341]\n",
      "loss: 0.664561  [120000/175341]\n",
      "loss: 0.662261  [121600/175341]\n",
      "loss: 0.628163  [123200/175341]\n",
      "loss: 0.551916  [124800/175341]\n",
      "loss: 1.406630  [126400/175341]\n",
      "loss: 0.572748  [128000/175341]\n",
      "loss: 0.331763  [129600/175341]\n",
      "loss: 0.228182  [131200/175341]\n",
      "loss: 0.737423  [132800/175341]\n",
      "loss: 0.171207  [134400/175341]\n",
      "loss: 1.123578  [136000/175341]\n",
      "loss: 0.386385  [137600/175341]\n",
      "loss: 0.567810  [139200/175341]\n",
      "loss: 0.495458  [140800/175341]\n",
      "loss: 0.359365  [142400/175341]\n",
      "loss: 1.255936  [144000/175341]\n",
      "loss: 0.855536  [145600/175341]\n",
      "loss: 0.649552  [147200/175341]\n",
      "loss: 0.499538  [148800/175341]\n",
      "loss: 0.661827  [150400/175341]\n",
      "loss: 0.300439  [152000/175341]\n",
      "loss: 0.530204  [153600/175341]\n",
      "loss: 0.407181  [155200/175341]\n",
      "loss: 0.777487  [156800/175341]\n",
      "loss: 0.667840  [158400/175341]\n",
      "loss: 0.397029  [160000/175341]\n",
      "loss: 0.961556  [161600/175341]\n",
      "loss: 0.755924  [163200/175341]\n",
      "loss: 0.240868  [164800/175341]\n",
      "loss: 0.347501  [166400/175341]\n",
      "loss: 0.817214  [168000/175341]\n",
      "loss: 0.363663  [169600/175341]\n",
      "loss: 0.603021  [171200/175341]\n",
      "loss: 0.817309  [172800/175341]\n",
      "loss: 0.625239  [174400/175341]\n",
      "Train Accuracy: 78.7825%\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.613088, F1-score: 73.20% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.507236  [    0/175341]\n",
      "loss: 0.572248  [ 1600/175341]\n",
      "loss: 0.874103  [ 3200/175341]\n",
      "loss: 0.604645  [ 4800/175341]\n",
      "loss: 0.599647  [ 6400/175341]\n",
      "loss: 0.643470  [ 8000/175341]\n",
      "loss: 0.756997  [ 9600/175341]\n",
      "loss: 0.334796  [11200/175341]\n",
      "loss: 0.784115  [12800/175341]\n",
      "loss: 0.202772  [14400/175341]\n",
      "loss: 0.757314  [16000/175341]\n",
      "loss: 0.235881  [17600/175341]\n",
      "loss: 0.500688  [19200/175341]\n",
      "loss: 0.472784  [20800/175341]\n",
      "loss: 0.770939  [22400/175341]\n",
      "loss: 0.459344  [24000/175341]\n",
      "loss: 0.352863  [25600/175341]\n",
      "loss: 0.641927  [27200/175341]\n",
      "loss: 0.612177  [28800/175341]\n",
      "loss: 0.430551  [30400/175341]\n",
      "loss: 0.552150  [32000/175341]\n",
      "loss: 0.471393  [33600/175341]\n",
      "loss: 0.461029  [35200/175341]\n",
      "loss: 0.580979  [36800/175341]\n",
      "loss: 0.322589  [38400/175341]\n",
      "loss: 0.454618  [40000/175341]\n",
      "loss: 0.742650  [41600/175341]\n",
      "loss: 0.497319  [43200/175341]\n",
      "loss: 1.115167  [44800/175341]\n",
      "loss: 0.463502  [46400/175341]\n",
      "loss: 0.656078  [48000/175341]\n",
      "loss: 0.586330  [49600/175341]\n",
      "loss: 0.329937  [51200/175341]\n",
      "loss: 0.841946  [52800/175341]\n",
      "loss: 0.202489  [54400/175341]\n",
      "loss: 0.424446  [56000/175341]\n",
      "loss: 0.348043  [57600/175341]\n",
      "loss: 0.472048  [59200/175341]\n",
      "loss: 0.261904  [60800/175341]\n",
      "loss: 0.783604  [62400/175341]\n",
      "loss: 0.507433  [64000/175341]\n",
      "loss: 0.483661  [65600/175341]\n",
      "loss: 0.494966  [67200/175341]\n",
      "loss: 0.924853  [68800/175341]\n",
      "loss: 0.769256  [70400/175341]\n",
      "loss: 0.482421  [72000/175341]\n",
      "loss: 0.551331  [73600/175341]\n",
      "loss: 0.702455  [75200/175341]\n",
      "loss: 0.291196  [76800/175341]\n",
      "loss: 0.667356  [78400/175341]\n",
      "loss: 0.375433  [80000/175341]\n",
      "loss: 0.279042  [81600/175341]\n",
      "loss: 0.687811  [83200/175341]\n",
      "loss: 0.160347  [84800/175341]\n",
      "loss: 0.764010  [86400/175341]\n",
      "loss: 0.196152  [88000/175341]\n",
      "loss: 0.384439  [89600/175341]\n",
      "loss: 0.352152  [91200/175341]\n",
      "loss: 0.869420  [92800/175341]\n",
      "loss: 0.600599  [94400/175341]\n",
      "loss: 0.380019  [96000/175341]\n",
      "loss: 0.151448  [97600/175341]\n",
      "loss: 0.605635  [99200/175341]\n",
      "loss: 0.661399  [100800/175341]\n",
      "loss: 0.473536  [102400/175341]\n",
      "loss: 0.555605  [104000/175341]\n",
      "loss: 0.338267  [105600/175341]\n",
      "loss: 0.383678  [107200/175341]\n",
      "loss: 0.488300  [108800/175341]\n",
      "loss: 0.195378  [110400/175341]\n",
      "loss: 0.372378  [112000/175341]\n",
      "loss: 0.320719  [113600/175341]\n",
      "loss: 0.378548  [115200/175341]\n",
      "loss: 0.760049  [116800/175341]\n",
      "loss: 0.236418  [118400/175341]\n",
      "loss: 0.577713  [120000/175341]\n",
      "loss: 0.904322  [121600/175341]\n",
      "loss: 0.670923  [123200/175341]\n",
      "loss: 0.343762  [124800/175341]\n",
      "loss: 0.438032  [126400/175341]\n",
      "loss: 0.396508  [128000/175341]\n",
      "loss: 0.489524  [129600/175341]\n",
      "loss: 0.621143  [131200/175341]\n",
      "loss: 0.173533  [132800/175341]\n",
      "loss: 0.987178  [134400/175341]\n",
      "loss: 0.751562  [136000/175341]\n",
      "loss: 0.480108  [137600/175341]\n",
      "loss: 0.763979  [139200/175341]\n",
      "loss: 0.593011  [140800/175341]\n",
      "loss: 0.672507  [142400/175341]\n",
      "loss: 0.490397  [144000/175341]\n",
      "loss: 0.302379  [145600/175341]\n",
      "loss: 0.745288  [147200/175341]\n",
      "loss: 0.587148  [148800/175341]\n",
      "loss: 0.821702  [150400/175341]\n",
      "loss: 0.818780  [152000/175341]\n",
      "loss: 0.675250  [153600/175341]\n",
      "loss: 0.052343  [155200/175341]\n",
      "loss: 1.248082  [156800/175341]\n",
      "loss: 0.396995  [158400/175341]\n",
      "loss: 0.650326  [160000/175341]\n",
      "loss: 0.440643  [161600/175341]\n",
      "loss: 0.309969  [163200/175341]\n",
      "loss: 0.685203  [164800/175341]\n",
      "loss: 0.628377  [166400/175341]\n",
      "loss: 0.743897  [168000/175341]\n",
      "loss: 0.432979  [169600/175341]\n",
      "loss: 0.469312  [171200/175341]\n",
      "loss: 0.251447  [172800/175341]\n",
      "loss: 0.480385  [174400/175341]\n",
      "Train Accuracy: 78.8367%\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.629585, F1-score: 73.22% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.414731  [    0/175341]\n",
      "loss: 0.444993  [ 1600/175341]\n",
      "loss: 0.260034  [ 3200/175341]\n",
      "loss: 0.741652  [ 4800/175341]\n",
      "loss: 0.722494  [ 6400/175341]\n",
      "loss: 0.573365  [ 8000/175341]\n",
      "loss: 0.495183  [ 9600/175341]\n",
      "loss: 0.816911  [11200/175341]\n",
      "loss: 0.535984  [12800/175341]\n",
      "loss: 0.880296  [14400/175341]\n",
      "loss: 0.558125  [16000/175341]\n",
      "loss: 0.237700  [17600/175341]\n",
      "loss: 0.814274  [19200/175341]\n",
      "loss: 0.586673  [20800/175341]\n",
      "loss: 0.994214  [22400/175341]\n",
      "loss: 0.502915  [24000/175341]\n",
      "loss: 0.362081  [25600/175341]\n",
      "loss: 0.209115  [27200/175341]\n",
      "loss: 0.927605  [28800/175341]\n",
      "loss: 0.673345  [30400/175341]\n",
      "loss: 0.579052  [32000/175341]\n",
      "loss: 0.604313  [33600/175341]\n",
      "loss: 0.447563  [35200/175341]\n",
      "loss: 0.610827  [36800/175341]\n",
      "loss: 0.336247  [38400/175341]\n",
      "loss: 0.747693  [40000/175341]\n",
      "loss: 0.410869  [41600/175341]\n",
      "loss: 1.056676  [43200/175341]\n",
      "loss: 0.515472  [44800/175341]\n",
      "loss: 0.459162  [46400/175341]\n",
      "loss: 0.300287  [48000/175341]\n",
      "loss: 0.196077  [49600/175341]\n",
      "loss: 0.400192  [51200/175341]\n",
      "loss: 0.331636  [52800/175341]\n",
      "loss: 0.395422  [54400/175341]\n",
      "loss: 0.758088  [56000/175341]\n",
      "loss: 1.089131  [57600/175341]\n",
      "loss: 0.533798  [59200/175341]\n",
      "loss: 0.931730  [60800/175341]\n",
      "loss: 0.752215  [62400/175341]\n",
      "loss: 0.185775  [64000/175341]\n",
      "loss: 0.563810  [65600/175341]\n",
      "loss: 0.414680  [67200/175341]\n",
      "loss: 0.464921  [68800/175341]\n",
      "loss: 0.319059  [70400/175341]\n",
      "loss: 0.470271  [72000/175341]\n",
      "loss: 0.281048  [73600/175341]\n",
      "loss: 0.869719  [75200/175341]\n",
      "loss: 0.641397  [76800/175341]\n",
      "loss: 0.282788  [78400/175341]\n",
      "loss: 0.364236  [80000/175341]\n",
      "loss: 0.382729  [81600/175341]\n",
      "loss: 0.573371  [83200/175341]\n",
      "loss: 0.626105  [84800/175341]\n",
      "loss: 0.823085  [86400/175341]\n",
      "loss: 0.521401  [88000/175341]\n",
      "loss: 0.629113  [89600/175341]\n",
      "loss: 0.449508  [91200/175341]\n",
      "loss: 0.354918  [92800/175341]\n",
      "loss: 0.564387  [94400/175341]\n",
      "loss: 0.670906  [96000/175341]\n",
      "loss: 0.358312  [97600/175341]\n",
      "loss: 0.512583  [99200/175341]\n",
      "loss: 0.538505  [100800/175341]\n",
      "loss: 0.379436  [102400/175341]\n",
      "loss: 0.389576  [104000/175341]\n",
      "loss: 1.213902  [105600/175341]\n",
      "loss: 0.763333  [107200/175341]\n",
      "loss: 0.771060  [108800/175341]\n",
      "loss: 0.644137  [110400/175341]\n",
      "loss: 0.429878  [112000/175341]\n",
      "loss: 0.591298  [113600/175341]\n",
      "loss: 0.516669  [115200/175341]\n",
      "loss: 0.237153  [116800/175341]\n",
      "loss: 0.530263  [118400/175341]\n",
      "loss: 0.507048  [120000/175341]\n",
      "loss: 0.388541  [121600/175341]\n",
      "loss: 0.703278  [123200/175341]\n",
      "loss: 0.216121  [124800/175341]\n",
      "loss: 0.726222  [126400/175341]\n",
      "loss: 0.994807  [128000/175341]\n",
      "loss: 0.450294  [129600/175341]\n",
      "loss: 0.429900  [131200/175341]\n",
      "loss: 0.591819  [132800/175341]\n",
      "loss: 0.596425  [134400/175341]\n",
      "loss: 0.638819  [136000/175341]\n",
      "loss: 1.370906  [137600/175341]\n",
      "loss: 0.580048  [139200/175341]\n",
      "loss: 0.431924  [140800/175341]\n",
      "loss: 0.295673  [142400/175341]\n",
      "loss: 0.256721  [144000/175341]\n",
      "loss: 0.649197  [145600/175341]\n",
      "loss: 0.285821  [147200/175341]\n",
      "loss: 0.520777  [148800/175341]\n",
      "loss: 0.382162  [150400/175341]\n",
      "loss: 0.488131  [152000/175341]\n",
      "loss: 0.606992  [153600/175341]\n",
      "loss: 0.579828  [155200/175341]\n",
      "loss: 0.558246  [156800/175341]\n",
      "loss: 0.817428  [158400/175341]\n",
      "loss: 0.843950  [160000/175341]\n",
      "loss: 1.161696  [161600/175341]\n",
      "loss: 0.487393  [163200/175341]\n",
      "loss: 0.341384  [164800/175341]\n",
      "loss: 0.635405  [166400/175341]\n",
      "loss: 0.316904  [168000/175341]\n",
      "loss: 0.807724  [169600/175341]\n",
      "loss: 0.227903  [171200/175341]\n",
      "loss: 0.632659  [172800/175341]\n",
      "loss: 0.448538  [174400/175341]\n",
      "Train Accuracy: 78.9359%\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.614328, F1-score: 73.16% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.772533  [    0/175341]\n",
      "loss: 0.492759  [ 1600/175341]\n",
      "loss: 0.642637  [ 3200/175341]\n",
      "loss: 0.433159  [ 4800/175341]\n",
      "loss: 0.329447  [ 6400/175341]\n",
      "loss: 0.499351  [ 8000/175341]\n",
      "loss: 0.434550  [ 9600/175341]\n",
      "loss: 0.532926  [11200/175341]\n",
      "loss: 0.811715  [12800/175341]\n",
      "loss: 0.724872  [14400/175341]\n",
      "loss: 0.616899  [16000/175341]\n",
      "loss: 0.600606  [17600/175341]\n",
      "loss: 0.414504  [19200/175341]\n",
      "loss: 0.416770  [20800/175341]\n",
      "loss: 0.596002  [22400/175341]\n",
      "loss: 0.315326  [24000/175341]\n",
      "loss: 0.912930  [25600/175341]\n",
      "loss: 0.875436  [27200/175341]\n",
      "loss: 0.582194  [28800/175341]\n",
      "loss: 0.905068  [30400/175341]\n",
      "loss: 0.790625  [32000/175341]\n",
      "loss: 0.563996  [33600/175341]\n",
      "loss: 0.642717  [35200/175341]\n",
      "loss: 0.896372  [36800/175341]\n",
      "loss: 0.427933  [38400/175341]\n",
      "loss: 0.484766  [40000/175341]\n",
      "loss: 1.096318  [41600/175341]\n",
      "loss: 0.644201  [43200/175341]\n",
      "loss: 0.219070  [44800/175341]\n",
      "loss: 0.350919  [46400/175341]\n",
      "loss: 0.544451  [48000/175341]\n",
      "loss: 0.400005  [49600/175341]\n",
      "loss: 0.601096  [51200/175341]\n",
      "loss: 0.400172  [52800/175341]\n",
      "loss: 0.385898  [54400/175341]\n",
      "loss: 0.645767  [56000/175341]\n",
      "loss: 0.454765  [57600/175341]\n",
      "loss: 0.211447  [59200/175341]\n",
      "loss: 0.410162  [60800/175341]\n",
      "loss: 0.790387  [62400/175341]\n",
      "loss: 0.279983  [64000/175341]\n",
      "loss: 0.681088  [65600/175341]\n",
      "loss: 1.387970  [67200/175341]\n",
      "loss: 0.759942  [68800/175341]\n",
      "loss: 0.536710  [70400/175341]\n",
      "loss: 0.868077  [72000/175341]\n",
      "loss: 0.639289  [73600/175341]\n",
      "loss: 0.771871  [75200/175341]\n",
      "loss: 0.605919  [76800/175341]\n",
      "loss: 1.014636  [78400/175341]\n",
      "loss: 0.851185  [80000/175341]\n",
      "loss: 0.371812  [81600/175341]\n",
      "loss: 0.774433  [83200/175341]\n",
      "loss: 0.694671  [84800/175341]\n",
      "loss: 0.408872  [86400/175341]\n",
      "loss: 0.933949  [88000/175341]\n",
      "loss: 0.586413  [89600/175341]\n",
      "loss: 0.256892  [91200/175341]\n",
      "loss: 0.474858  [92800/175341]\n",
      "loss: 0.430990  [94400/175341]\n",
      "loss: 0.203016  [96000/175341]\n",
      "loss: 0.744847  [97600/175341]\n",
      "loss: 0.430417  [99200/175341]\n",
      "loss: 0.568649  [100800/175341]\n",
      "loss: 0.464414  [102400/175341]\n",
      "loss: 0.325468  [104000/175341]\n",
      "loss: 0.815993  [105600/175341]\n",
      "loss: 0.399917  [107200/175341]\n",
      "loss: 1.085137  [108800/175341]\n",
      "loss: 1.095283  [110400/175341]\n",
      "loss: 0.450497  [112000/175341]\n",
      "loss: 0.698701  [113600/175341]\n",
      "loss: 0.324938  [115200/175341]\n",
      "loss: 0.270576  [116800/175341]\n",
      "loss: 0.480917  [118400/175341]\n",
      "loss: 0.724065  [120000/175341]\n",
      "loss: 0.656329  [121600/175341]\n",
      "loss: 1.026304  [123200/175341]\n",
      "loss: 0.396591  [124800/175341]\n",
      "loss: 0.896498  [126400/175341]\n",
      "loss: 0.994439  [128000/175341]\n",
      "loss: 0.556958  [129600/175341]\n",
      "loss: 0.669863  [131200/175341]\n",
      "loss: 0.340351  [132800/175341]\n",
      "loss: 0.521122  [134400/175341]\n",
      "loss: 0.458088  [136000/175341]\n",
      "loss: 0.657944  [137600/175341]\n",
      "loss: 0.229440  [139200/175341]\n",
      "loss: 0.196509  [140800/175341]\n",
      "loss: 0.433111  [142400/175341]\n",
      "loss: 0.963026  [144000/175341]\n",
      "loss: 0.631161  [145600/175341]\n",
      "loss: 0.297425  [147200/175341]\n",
      "loss: 0.754235  [148800/175341]\n",
      "loss: 0.725526  [150400/175341]\n",
      "loss: 1.048265  [152000/175341]\n",
      "loss: 0.813308  [153600/175341]\n",
      "loss: 0.350639  [155200/175341]\n",
      "loss: 0.473093  [156800/175341]\n",
      "loss: 0.327720  [158400/175341]\n",
      "loss: 0.782276  [160000/175341]\n",
      "loss: 0.756750  [161600/175341]\n",
      "loss: 0.383761  [163200/175341]\n",
      "loss: 0.593099  [164800/175341]\n",
      "loss: 0.368614  [166400/175341]\n",
      "loss: 0.511165  [168000/175341]\n",
      "loss: 0.695935  [169600/175341]\n",
      "loss: 0.799882  [171200/175341]\n",
      "loss: 0.412708  [172800/175341]\n",
      "loss: 0.445363  [174400/175341]\n",
      "Train Accuracy: 79.0346%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.620098, F1-score: 73.81% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.777870  [    0/175341]\n",
      "loss: 0.315823  [ 1600/175341]\n",
      "loss: 0.764530  [ 3200/175341]\n",
      "loss: 0.432992  [ 4800/175341]\n",
      "loss: 0.973934  [ 6400/175341]\n",
      "loss: 0.436700  [ 8000/175341]\n",
      "loss: 0.451154  [ 9600/175341]\n",
      "loss: 0.474351  [11200/175341]\n",
      "loss: 0.249943  [12800/175341]\n",
      "loss: 0.247556  [14400/175341]\n",
      "loss: 0.511619  [16000/175341]\n",
      "loss: 0.661033  [17600/175341]\n",
      "loss: 0.463141  [19200/175341]\n",
      "loss: 0.807823  [20800/175341]\n",
      "loss: 0.708814  [22400/175341]\n",
      "loss: 0.535197  [24000/175341]\n",
      "loss: 0.644063  [25600/175341]\n",
      "loss: 0.787362  [27200/175341]\n",
      "loss: 0.719239  [28800/175341]\n",
      "loss: 0.879156  [30400/175341]\n",
      "loss: 0.700650  [32000/175341]\n",
      "loss: 0.733968  [33600/175341]\n",
      "loss: 0.756663  [35200/175341]\n",
      "loss: 0.359184  [36800/175341]\n",
      "loss: 0.517594  [38400/175341]\n",
      "loss: 0.467314  [40000/175341]\n",
      "loss: 0.432077  [41600/175341]\n",
      "loss: 0.611197  [43200/175341]\n",
      "loss: 0.385476  [44800/175341]\n",
      "loss: 0.322557  [46400/175341]\n",
      "loss: 0.548291  [48000/175341]\n",
      "loss: 0.600591  [49600/175341]\n",
      "loss: 1.054314  [51200/175341]\n",
      "loss: 0.523879  [52800/175341]\n",
      "loss: 0.382894  [54400/175341]\n",
      "loss: 0.644380  [56000/175341]\n",
      "loss: 0.363779  [57600/175341]\n",
      "loss: 0.127145  [59200/175341]\n",
      "loss: 0.159648  [60800/175341]\n",
      "loss: 0.606252  [62400/175341]\n",
      "loss: 0.487900  [64000/175341]\n",
      "loss: 0.427033  [65600/175341]\n",
      "loss: 0.376305  [67200/175341]\n",
      "loss: 0.372807  [68800/175341]\n",
      "loss: 0.586619  [70400/175341]\n",
      "loss: 0.931807  [72000/175341]\n",
      "loss: 0.560673  [73600/175341]\n",
      "loss: 0.439371  [75200/175341]\n",
      "loss: 0.745646  [76800/175341]\n",
      "loss: 0.757500  [78400/175341]\n",
      "loss: 0.407189  [80000/175341]\n",
      "loss: 0.546149  [81600/175341]\n",
      "loss: 0.426625  [83200/175341]\n",
      "loss: 0.380546  [84800/175341]\n",
      "loss: 0.289416  [86400/175341]\n",
      "loss: 0.510501  [88000/175341]\n",
      "loss: 0.975621  [89600/175341]\n",
      "loss: 0.856461  [91200/175341]\n",
      "loss: 0.635647  [92800/175341]\n",
      "loss: 0.383960  [94400/175341]\n",
      "loss: 0.651541  [96000/175341]\n",
      "loss: 0.505981  [97600/175341]\n",
      "loss: 0.683779  [99200/175341]\n",
      "loss: 0.464806  [100800/175341]\n",
      "loss: 0.909844  [102400/175341]\n",
      "loss: 0.396546  [104000/175341]\n",
      "loss: 0.724773  [105600/175341]\n",
      "loss: 0.430734  [107200/175341]\n",
      "loss: 0.685481  [108800/175341]\n",
      "loss: 0.774221  [110400/175341]\n",
      "loss: 0.491015  [112000/175341]\n",
      "loss: 0.748081  [113600/175341]\n",
      "loss: 0.507305  [115200/175341]\n",
      "loss: 0.808503  [116800/175341]\n",
      "loss: 0.267805  [118400/175341]\n",
      "loss: 0.220125  [120000/175341]\n",
      "loss: 0.465518  [121600/175341]\n",
      "loss: 0.514417  [123200/175341]\n",
      "loss: 0.475979  [124800/175341]\n",
      "loss: 0.580847  [126400/175341]\n",
      "loss: 0.478380  [128000/175341]\n",
      "loss: 0.582980  [129600/175341]\n",
      "loss: 0.821085  [131200/175341]\n",
      "loss: 0.236328  [132800/175341]\n",
      "loss: 0.502854  [134400/175341]\n",
      "loss: 0.540450  [136000/175341]\n",
      "loss: 0.517891  [137600/175341]\n",
      "loss: 0.275082  [139200/175341]\n",
      "loss: 0.364923  [140800/175341]\n",
      "loss: 1.043848  [142400/175341]\n",
      "loss: 0.758093  [144000/175341]\n",
      "loss: 0.411789  [145600/175341]\n",
      "loss: 0.339530  [147200/175341]\n",
      "loss: 0.558219  [148800/175341]\n",
      "loss: 0.659276  [150400/175341]\n",
      "loss: 0.248639  [152000/175341]\n",
      "loss: 0.419369  [153600/175341]\n",
      "loss: 0.333537  [155200/175341]\n",
      "loss: 0.538235  [156800/175341]\n",
      "loss: 0.765993  [158400/175341]\n",
      "loss: 0.487536  [160000/175341]\n",
      "loss: 0.706511  [161600/175341]\n",
      "loss: 0.273045  [163200/175341]\n",
      "loss: 0.402456  [164800/175341]\n",
      "loss: 0.633524  [166400/175341]\n",
      "loss: 0.713529  [168000/175341]\n",
      "loss: 0.332376  [169600/175341]\n",
      "loss: 0.202678  [171200/175341]\n",
      "loss: 0.510093  [172800/175341]\n",
      "loss: 0.687267  [174400/175341]\n",
      "Train Accuracy: 79.0813%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.601770, F1-score: 74.01% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a37a33c4-8087-4b5c-80cf-c10334d2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 15,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ada289ca-73cc-4ac6-8f1c-89c10d83b914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304635  [    0/175341]\n",
      "loss: 2.295389  [ 1600/175341]\n",
      "loss: 2.283682  [ 3200/175341]\n",
      "loss: 2.281820  [ 4800/175341]\n",
      "loss: 2.252292  [ 6400/175341]\n",
      "loss: 2.232892  [ 8000/175341]\n",
      "loss: 2.215794  [ 9600/175341]\n",
      "loss: 2.175154  [11200/175341]\n",
      "loss: 2.142858  [12800/175341]\n",
      "loss: 2.131863  [14400/175341]\n",
      "loss: 2.071232  [16000/175341]\n",
      "loss: 2.078533  [17600/175341]\n",
      "loss: 1.994570  [19200/175341]\n",
      "loss: 1.920398  [20800/175341]\n",
      "loss: 1.708588  [22400/175341]\n",
      "loss: 1.940716  [24000/175341]\n",
      "loss: 1.849657  [25600/175341]\n",
      "loss: 1.904210  [27200/175341]\n",
      "loss: 1.994698  [28800/175341]\n",
      "loss: 1.602659  [30400/175341]\n",
      "loss: 1.618429  [32000/175341]\n",
      "loss: 1.801138  [33600/175341]\n",
      "loss: 1.572621  [35200/175341]\n",
      "loss: 1.751022  [36800/175341]\n",
      "loss: 1.797438  [38400/175341]\n",
      "loss: 0.968760  [40000/175341]\n",
      "loss: 1.469869  [41600/175341]\n",
      "loss: 1.309126  [43200/175341]\n",
      "loss: 1.052799  [44800/175341]\n",
      "loss: 1.726899  [46400/175341]\n",
      "loss: 1.161793  [48000/175341]\n",
      "loss: 1.542810  [49600/175341]\n",
      "loss: 1.161829  [51200/175341]\n",
      "loss: 0.946349  [52800/175341]\n",
      "loss: 0.961583  [54400/175341]\n",
      "loss: 1.333777  [56000/175341]\n",
      "loss: 1.561288  [57600/175341]\n",
      "loss: 1.439697  [59200/175341]\n",
      "loss: 1.249424  [60800/175341]\n",
      "loss: 1.303138  [62400/175341]\n",
      "loss: 0.838728  [64000/175341]\n",
      "loss: 1.128253  [65600/175341]\n",
      "loss: 0.742924  [67200/175341]\n",
      "loss: 1.106381  [68800/175341]\n",
      "loss: 0.666351  [70400/175341]\n",
      "loss: 1.018179  [72000/175341]\n",
      "loss: 1.209355  [73600/175341]\n",
      "loss: 1.212142  [75200/175341]\n",
      "loss: 0.946880  [76800/175341]\n",
      "loss: 1.351505  [78400/175341]\n",
      "loss: 0.739475  [80000/175341]\n",
      "loss: 1.030746  [81600/175341]\n",
      "loss: 0.791612  [83200/175341]\n",
      "loss: 0.773152  [84800/175341]\n",
      "loss: 1.178716  [86400/175341]\n",
      "loss: 0.977961  [88000/175341]\n",
      "loss: 1.008232  [89600/175341]\n",
      "loss: 0.908288  [91200/175341]\n",
      "loss: 1.063008  [92800/175341]\n",
      "loss: 0.869397  [94400/175341]\n",
      "loss: 1.078305  [96000/175341]\n",
      "loss: 0.604075  [97600/175341]\n",
      "loss: 1.288245  [99200/175341]\n",
      "loss: 0.763553  [100800/175341]\n",
      "loss: 0.817265  [102400/175341]\n",
      "loss: 0.801701  [104000/175341]\n",
      "loss: 0.933767  [105600/175341]\n",
      "loss: 0.712896  [107200/175341]\n",
      "loss: 1.398925  [108800/175341]\n",
      "loss: 1.526954  [110400/175341]\n",
      "loss: 0.625214  [112000/175341]\n",
      "loss: 0.518878  [113600/175341]\n",
      "loss: 1.291527  [115200/175341]\n",
      "loss: 0.708499  [116800/175341]\n",
      "loss: 0.612006  [118400/175341]\n",
      "loss: 0.575231  [120000/175341]\n",
      "loss: 0.981893  [121600/175341]\n",
      "loss: 0.895092  [123200/175341]\n",
      "loss: 0.568390  [124800/175341]\n",
      "loss: 0.986021  [126400/175341]\n",
      "loss: 0.487550  [128000/175341]\n",
      "loss: 0.637281  [129600/175341]\n",
      "loss: 0.545757  [131200/175341]\n",
      "loss: 0.850251  [132800/175341]\n",
      "loss: 0.963239  [134400/175341]\n",
      "loss: 0.778735  [136000/175341]\n",
      "loss: 1.033687  [137600/175341]\n",
      "loss: 0.784711  [139200/175341]\n",
      "loss: 0.576049  [140800/175341]\n",
      "loss: 0.629599  [142400/175341]\n",
      "loss: 0.858908  [144000/175341]\n",
      "loss: 0.816086  [145600/175341]\n",
      "loss: 0.918269  [147200/175341]\n",
      "loss: 0.645194  [148800/175341]\n",
      "loss: 0.715139  [150400/175341]\n",
      "loss: 1.070896  [152000/175341]\n",
      "loss: 0.775970  [153600/175341]\n",
      "loss: 0.876438  [155200/175341]\n",
      "loss: 0.920704  [156800/175341]\n",
      "loss: 0.749484  [158400/175341]\n",
      "loss: 0.828901  [160000/175341]\n",
      "loss: 1.115963  [161600/175341]\n",
      "loss: 0.832351  [163200/175341]\n",
      "loss: 0.655522  [164800/175341]\n",
      "loss: 1.284922  [166400/175341]\n",
      "loss: 0.902594  [168000/175341]\n",
      "loss: 0.596401  [169600/175341]\n",
      "loss: 1.126420  [171200/175341]\n",
      "loss: 0.753586  [172800/175341]\n",
      "loss: 0.861611  [174400/175341]\n",
      "Train Accuracy: 61.1802%\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.820928, F1-score: 68.27% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.004673  [    0/175341]\n",
      "loss: 0.880631  [ 1600/175341]\n",
      "loss: 0.855270  [ 3200/175341]\n",
      "loss: 0.631677  [ 4800/175341]\n",
      "loss: 1.273568  [ 6400/175341]\n",
      "loss: 1.113665  [ 8000/175341]\n",
      "loss: 0.851238  [ 9600/175341]\n",
      "loss: 1.168052  [11200/175341]\n",
      "loss: 0.841660  [12800/175341]\n",
      "loss: 0.863699  [14400/175341]\n",
      "loss: 0.788106  [16000/175341]\n",
      "loss: 0.379931  [17600/175341]\n",
      "loss: 0.919524  [19200/175341]\n",
      "loss: 0.823959  [20800/175341]\n",
      "loss: 0.742103  [22400/175341]\n",
      "loss: 0.705211  [24000/175341]\n",
      "loss: 0.502607  [25600/175341]\n",
      "loss: 0.880442  [27200/175341]\n",
      "loss: 1.235651  [28800/175341]\n",
      "loss: 0.804167  [30400/175341]\n",
      "loss: 0.339610  [32000/175341]\n",
      "loss: 0.857801  [33600/175341]\n",
      "loss: 0.711285  [35200/175341]\n",
      "loss: 0.529952  [36800/175341]\n",
      "loss: 0.664609  [38400/175341]\n",
      "loss: 0.460113  [40000/175341]\n",
      "loss: 0.561803  [41600/175341]\n",
      "loss: 0.721076  [43200/175341]\n",
      "loss: 0.814720  [44800/175341]\n",
      "loss: 0.450640  [46400/175341]\n",
      "loss: 0.658016  [48000/175341]\n",
      "loss: 0.791391  [49600/175341]\n",
      "loss: 0.737774  [51200/175341]\n",
      "loss: 0.781329  [52800/175341]\n",
      "loss: 0.686302  [54400/175341]\n",
      "loss: 1.201871  [56000/175341]\n",
      "loss: 0.822749  [57600/175341]\n",
      "loss: 0.560738  [59200/175341]\n",
      "loss: 0.804114  [60800/175341]\n",
      "loss: 0.721140  [62400/175341]\n",
      "loss: 0.333047  [64000/175341]\n",
      "loss: 1.187338  [65600/175341]\n",
      "loss: 0.753489  [67200/175341]\n",
      "loss: 0.756878  [68800/175341]\n",
      "loss: 0.617653  [70400/175341]\n",
      "loss: 0.546834  [72000/175341]\n",
      "loss: 1.257138  [73600/175341]\n",
      "loss: 0.659928  [75200/175341]\n",
      "loss: 0.649780  [76800/175341]\n",
      "loss: 0.363788  [78400/175341]\n",
      "loss: 0.567827  [80000/175341]\n",
      "loss: 0.819114  [81600/175341]\n",
      "loss: 0.617140  [83200/175341]\n",
      "loss: 0.672243  [84800/175341]\n",
      "loss: 0.755461  [86400/175341]\n",
      "loss: 0.647731  [88000/175341]\n",
      "loss: 1.237569  [89600/175341]\n",
      "loss: 0.904393  [91200/175341]\n",
      "loss: 0.682676  [92800/175341]\n",
      "loss: 0.746892  [94400/175341]\n",
      "loss: 0.517127  [96000/175341]\n",
      "loss: 0.662675  [97600/175341]\n",
      "loss: 0.259870  [99200/175341]\n",
      "loss: 0.674753  [100800/175341]\n",
      "loss: 0.486808  [102400/175341]\n",
      "loss: 0.295807  [104000/175341]\n",
      "loss: 0.798268  [105600/175341]\n",
      "loss: 0.567365  [107200/175341]\n",
      "loss: 1.141097  [108800/175341]\n",
      "loss: 0.559348  [110400/175341]\n",
      "loss: 0.900715  [112000/175341]\n",
      "loss: 1.367448  [113600/175341]\n",
      "loss: 0.398110  [115200/175341]\n",
      "loss: 1.147122  [116800/175341]\n",
      "loss: 1.031071  [118400/175341]\n",
      "loss: 0.858225  [120000/175341]\n",
      "loss: 0.545417  [121600/175341]\n",
      "loss: 0.960145  [123200/175341]\n",
      "loss: 0.653916  [124800/175341]\n",
      "loss: 0.593085  [126400/175341]\n",
      "loss: 0.691429  [128000/175341]\n",
      "loss: 0.680198  [129600/175341]\n",
      "loss: 0.677166  [131200/175341]\n",
      "loss: 0.272980  [132800/175341]\n",
      "loss: 0.820265  [134400/175341]\n",
      "loss: 0.843879  [136000/175341]\n",
      "loss: 0.781323  [137600/175341]\n",
      "loss: 0.228785  [139200/175341]\n",
      "loss: 0.219509  [140800/175341]\n",
      "loss: 0.901767  [142400/175341]\n",
      "loss: 0.497737  [144000/175341]\n",
      "loss: 0.458769  [145600/175341]\n",
      "loss: 0.633666  [147200/175341]\n",
      "loss: 0.481077  [148800/175341]\n",
      "loss: 0.833804  [150400/175341]\n",
      "loss: 0.639711  [152000/175341]\n",
      "loss: 0.793028  [153600/175341]\n",
      "loss: 0.800409  [155200/175341]\n",
      "loss: 0.700194  [156800/175341]\n",
      "loss: 0.873198  [158400/175341]\n",
      "loss: 0.550961  [160000/175341]\n",
      "loss: 0.440285  [161600/175341]\n",
      "loss: 0.372966  [163200/175341]\n",
      "loss: 0.802440  [164800/175341]\n",
      "loss: 0.368360  [166400/175341]\n",
      "loss: 0.419112  [168000/175341]\n",
      "loss: 0.596656  [169600/175341]\n",
      "loss: 0.625676  [171200/175341]\n",
      "loss: 1.045690  [172800/175341]\n",
      "loss: 0.803928  [174400/175341]\n",
      "Train Accuracy: 74.7133%\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.748537, F1-score: 69.60% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.810176  [    0/175341]\n",
      "loss: 0.531894  [ 1600/175341]\n",
      "loss: 1.167361  [ 3200/175341]\n",
      "loss: 0.727478  [ 4800/175341]\n",
      "loss: 0.373564  [ 6400/175341]\n",
      "loss: 0.799174  [ 8000/175341]\n",
      "loss: 0.617619  [ 9600/175341]\n",
      "loss: 0.939701  [11200/175341]\n",
      "loss: 0.912041  [12800/175341]\n",
      "loss: 0.700673  [14400/175341]\n",
      "loss: 0.616188  [16000/175341]\n",
      "loss: 0.635767  [17600/175341]\n",
      "loss: 0.603933  [19200/175341]\n",
      "loss: 0.361240  [20800/175341]\n",
      "loss: 0.491077  [22400/175341]\n",
      "loss: 0.717823  [24000/175341]\n",
      "loss: 0.521099  [25600/175341]\n",
      "loss: 0.582423  [27200/175341]\n",
      "loss: 0.264652  [28800/175341]\n",
      "loss: 0.505535  [30400/175341]\n",
      "loss: 0.465573  [32000/175341]\n",
      "loss: 1.250353  [33600/175341]\n",
      "loss: 0.773258  [35200/175341]\n",
      "loss: 0.533383  [36800/175341]\n",
      "loss: 0.180918  [38400/175341]\n",
      "loss: 0.498782  [40000/175341]\n",
      "loss: 0.672815  [41600/175341]\n",
      "loss: 0.755348  [43200/175341]\n",
      "loss: 0.637576  [44800/175341]\n",
      "loss: 0.529073  [46400/175341]\n",
      "loss: 0.587234  [48000/175341]\n",
      "loss: 0.867056  [49600/175341]\n",
      "loss: 0.540713  [51200/175341]\n",
      "loss: 0.448517  [52800/175341]\n",
      "loss: 0.820196  [54400/175341]\n",
      "loss: 0.457462  [56000/175341]\n",
      "loss: 0.817030  [57600/175341]\n",
      "loss: 0.674252  [59200/175341]\n",
      "loss: 0.727073  [60800/175341]\n",
      "loss: 0.673227  [62400/175341]\n",
      "loss: 0.835712  [64000/175341]\n",
      "loss: 0.753337  [65600/175341]\n",
      "loss: 0.230865  [67200/175341]\n",
      "loss: 0.779208  [68800/175341]\n",
      "loss: 0.245697  [70400/175341]\n",
      "loss: 1.211264  [72000/175341]\n",
      "loss: 0.491059  [73600/175341]\n",
      "loss: 0.504824  [75200/175341]\n",
      "loss: 0.851275  [76800/175341]\n",
      "loss: 0.302320  [78400/175341]\n",
      "loss: 0.315395  [80000/175341]\n",
      "loss: 0.927946  [81600/175341]\n",
      "loss: 0.548447  [83200/175341]\n",
      "loss: 0.584421  [84800/175341]\n",
      "loss: 0.675473  [86400/175341]\n",
      "loss: 0.522077  [88000/175341]\n",
      "loss: 0.796962  [89600/175341]\n",
      "loss: 0.200034  [91200/175341]\n",
      "loss: 0.635531  [92800/175341]\n",
      "loss: 0.412747  [94400/175341]\n",
      "loss: 1.130715  [96000/175341]\n",
      "loss: 0.612152  [97600/175341]\n",
      "loss: 0.946080  [99200/175341]\n",
      "loss: 0.861011  [100800/175341]\n",
      "loss: 0.812616  [102400/175341]\n",
      "loss: 0.592744  [104000/175341]\n",
      "loss: 0.600221  [105600/175341]\n",
      "loss: 0.690930  [107200/175341]\n",
      "loss: 0.557579  [108800/175341]\n",
      "loss: 0.639437  [110400/175341]\n",
      "loss: 1.041964  [112000/175341]\n",
      "loss: 0.625586  [113600/175341]\n",
      "loss: 0.713399  [115200/175341]\n",
      "loss: 0.973286  [116800/175341]\n",
      "loss: 0.211120  [118400/175341]\n",
      "loss: 1.265764  [120000/175341]\n",
      "loss: 1.611829  [121600/175341]\n",
      "loss: 0.973944  [123200/175341]\n",
      "loss: 0.229204  [124800/175341]\n",
      "loss: 0.544473  [126400/175341]\n",
      "loss: 0.914076  [128000/175341]\n",
      "loss: 0.960589  [129600/175341]\n",
      "loss: 0.733141  [131200/175341]\n",
      "loss: 0.757700  [132800/175341]\n",
      "loss: 0.564161  [134400/175341]\n",
      "loss: 0.458652  [136000/175341]\n",
      "loss: 1.087877  [137600/175341]\n",
      "loss: 0.868476  [139200/175341]\n",
      "loss: 0.357851  [140800/175341]\n",
      "loss: 0.544692  [142400/175341]\n",
      "loss: 0.499112  [144000/175341]\n",
      "loss: 0.712240  [145600/175341]\n",
      "loss: 0.473428  [147200/175341]\n",
      "loss: 0.412086  [148800/175341]\n",
      "loss: 0.512899  [150400/175341]\n",
      "loss: 0.668562  [152000/175341]\n",
      "loss: 1.150797  [153600/175341]\n",
      "loss: 0.509522  [155200/175341]\n",
      "loss: 0.370797  [156800/175341]\n",
      "loss: 0.348024  [158400/175341]\n",
      "loss: 0.443358  [160000/175341]\n",
      "loss: 0.568208  [161600/175341]\n",
      "loss: 0.559522  [163200/175341]\n",
      "loss: 1.275314  [164800/175341]\n",
      "loss: 0.593516  [166400/175341]\n",
      "loss: 0.353343  [168000/175341]\n",
      "loss: 0.315704  [169600/175341]\n",
      "loss: 0.722267  [171200/175341]\n",
      "loss: 0.198587  [172800/175341]\n",
      "loss: 0.543220  [174400/175341]\n",
      "Train Accuracy: 76.1864%\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.722306, F1-score: 70.37% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.814969  [    0/175341]\n",
      "loss: 0.528400  [ 1600/175341]\n",
      "loss: 0.437420  [ 3200/175341]\n",
      "loss: 0.275066  [ 4800/175341]\n",
      "loss: 0.496519  [ 6400/175341]\n",
      "loss: 0.595648  [ 8000/175341]\n",
      "loss: 0.947697  [ 9600/175341]\n",
      "loss: 0.257450  [11200/175341]\n",
      "loss: 0.388477  [12800/175341]\n",
      "loss: 0.638529  [14400/175341]\n",
      "loss: 1.286451  [16000/175341]\n",
      "loss: 0.251956  [17600/175341]\n",
      "loss: 0.673219  [19200/175341]\n",
      "loss: 0.702526  [20800/175341]\n",
      "loss: 0.413559  [22400/175341]\n",
      "loss: 0.582976  [24000/175341]\n",
      "loss: 0.675475  [25600/175341]\n",
      "loss: 0.438160  [27200/175341]\n",
      "loss: 0.569193  [28800/175341]\n",
      "loss: 0.669563  [30400/175341]\n",
      "loss: 0.512501  [32000/175341]\n",
      "loss: 0.527688  [33600/175341]\n",
      "loss: 0.418047  [35200/175341]\n",
      "loss: 0.320323  [36800/175341]\n",
      "loss: 0.869503  [38400/175341]\n",
      "loss: 0.776210  [40000/175341]\n",
      "loss: 0.912646  [41600/175341]\n",
      "loss: 0.609839  [43200/175341]\n",
      "loss: 0.740712  [44800/175341]\n",
      "loss: 0.583043  [46400/175341]\n",
      "loss: 0.366983  [48000/175341]\n",
      "loss: 0.362069  [49600/175341]\n",
      "loss: 0.721537  [51200/175341]\n",
      "loss: 0.241929  [52800/175341]\n",
      "loss: 0.563172  [54400/175341]\n",
      "loss: 0.554860  [56000/175341]\n",
      "loss: 0.637866  [57600/175341]\n",
      "loss: 0.579726  [59200/175341]\n",
      "loss: 0.727645  [60800/175341]\n",
      "loss: 0.689759  [62400/175341]\n",
      "loss: 0.879266  [64000/175341]\n",
      "loss: 0.716319  [65600/175341]\n",
      "loss: 0.435708  [67200/175341]\n",
      "loss: 0.557278  [68800/175341]\n",
      "loss: 0.496734  [70400/175341]\n",
      "loss: 0.692893  [72000/175341]\n",
      "loss: 0.780689  [73600/175341]\n",
      "loss: 0.559430  [75200/175341]\n",
      "loss: 1.004013  [76800/175341]\n",
      "loss: 1.132812  [78400/175341]\n",
      "loss: 0.473387  [80000/175341]\n",
      "loss: 0.316536  [81600/175341]\n",
      "loss: 0.958174  [83200/175341]\n",
      "loss: 0.541001  [84800/175341]\n",
      "loss: 0.823255  [86400/175341]\n",
      "loss: 0.606981  [88000/175341]\n",
      "loss: 0.772307  [89600/175341]\n",
      "loss: 0.283461  [91200/175341]\n",
      "loss: 0.592345  [92800/175341]\n",
      "loss: 0.783697  [94400/175341]\n",
      "loss: 0.933588  [96000/175341]\n",
      "loss: 0.697664  [97600/175341]\n",
      "loss: 0.660377  [99200/175341]\n",
      "loss: 0.211523  [100800/175341]\n",
      "loss: 0.733606  [102400/175341]\n",
      "loss: 0.377159  [104000/175341]\n",
      "loss: 0.662251  [105600/175341]\n",
      "loss: 0.805121  [107200/175341]\n",
      "loss: 0.373509  [108800/175341]\n",
      "loss: 0.737719  [110400/175341]\n",
      "loss: 0.406422  [112000/175341]\n",
      "loss: 0.947297  [113600/175341]\n",
      "loss: 0.398443  [115200/175341]\n",
      "loss: 0.315315  [116800/175341]\n",
      "loss: 0.867577  [118400/175341]\n",
      "loss: 0.207028  [120000/175341]\n",
      "loss: 1.265242  [121600/175341]\n",
      "loss: 0.935346  [123200/175341]\n",
      "loss: 0.566036  [124800/175341]\n",
      "loss: 0.237235  [126400/175341]\n",
      "loss: 0.355411  [128000/175341]\n",
      "loss: 0.786924  [129600/175341]\n",
      "loss: 0.383971  [131200/175341]\n",
      "loss: 0.539030  [132800/175341]\n",
      "loss: 0.689721  [134400/175341]\n",
      "loss: 0.711425  [136000/175341]\n",
      "loss: 0.513484  [137600/175341]\n",
      "loss: 0.438466  [139200/175341]\n",
      "loss: 0.681324  [140800/175341]\n",
      "loss: 0.539610  [142400/175341]\n",
      "loss: 0.851780  [144000/175341]\n",
      "loss: 0.445152  [145600/175341]\n",
      "loss: 0.407362  [147200/175341]\n",
      "loss: 0.665168  [148800/175341]\n",
      "loss: 0.496700  [150400/175341]\n",
      "loss: 0.744009  [152000/175341]\n",
      "loss: 0.549107  [153600/175341]\n",
      "loss: 0.507919  [155200/175341]\n",
      "loss: 0.666319  [156800/175341]\n",
      "loss: 0.559963  [158400/175341]\n",
      "loss: 0.870648  [160000/175341]\n",
      "loss: 1.083509  [161600/175341]\n",
      "loss: 0.780102  [163200/175341]\n",
      "loss: 0.507156  [164800/175341]\n",
      "loss: 0.710870  [166400/175341]\n",
      "loss: 1.362763  [168000/175341]\n",
      "loss: 1.268121  [169600/175341]\n",
      "loss: 0.838875  [171200/175341]\n",
      "loss: 0.724636  [172800/175341]\n",
      "loss: 0.404438  [174400/175341]\n",
      "Train Accuracy: 76.7265%\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.699901, F1-score: 70.38% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.832516  [    0/175341]\n",
      "loss: 0.637235  [ 1600/175341]\n",
      "loss: 0.524705  [ 3200/175341]\n",
      "loss: 1.099352  [ 4800/175341]\n",
      "loss: 1.051046  [ 6400/175341]\n",
      "loss: 0.707357  [ 8000/175341]\n",
      "loss: 0.339666  [ 9600/175341]\n",
      "loss: 0.952291  [11200/175341]\n",
      "loss: 0.898608  [12800/175341]\n",
      "loss: 0.424207  [14400/175341]\n",
      "loss: 1.072948  [16000/175341]\n",
      "loss: 0.624526  [17600/175341]\n",
      "loss: 0.560356  [19200/175341]\n",
      "loss: 0.754483  [20800/175341]\n",
      "loss: 0.555057  [22400/175341]\n",
      "loss: 0.649751  [24000/175341]\n",
      "loss: 0.501399  [25600/175341]\n",
      "loss: 0.391691  [27200/175341]\n",
      "loss: 0.617335  [28800/175341]\n",
      "loss: 0.683712  [30400/175341]\n",
      "loss: 0.322959  [32000/175341]\n",
      "loss: 0.634622  [33600/175341]\n",
      "loss: 0.495344  [35200/175341]\n",
      "loss: 1.045118  [36800/175341]\n",
      "loss: 1.326677  [38400/175341]\n",
      "loss: 1.042772  [40000/175341]\n",
      "loss: 0.574820  [41600/175341]\n",
      "loss: 0.484377  [43200/175341]\n",
      "loss: 0.520438  [44800/175341]\n",
      "loss: 0.638879  [46400/175341]\n",
      "loss: 0.648002  [48000/175341]\n",
      "loss: 0.760457  [49600/175341]\n",
      "loss: 0.851222  [51200/175341]\n",
      "loss: 0.381333  [52800/175341]\n",
      "loss: 0.775410  [54400/175341]\n",
      "loss: 0.840290  [56000/175341]\n",
      "loss: 0.527581  [57600/175341]\n",
      "loss: 0.626806  [59200/175341]\n",
      "loss: 0.764887  [60800/175341]\n",
      "loss: 0.479163  [62400/175341]\n",
      "loss: 0.934115  [64000/175341]\n",
      "loss: 0.397667  [65600/175341]\n",
      "loss: 0.710985  [67200/175341]\n",
      "loss: 0.513028  [68800/175341]\n",
      "loss: 0.367443  [70400/175341]\n",
      "loss: 0.746534  [72000/175341]\n",
      "loss: 0.575693  [73600/175341]\n",
      "loss: 0.545700  [75200/175341]\n",
      "loss: 0.461744  [76800/175341]\n",
      "loss: 0.783492  [78400/175341]\n",
      "loss: 0.509079  [80000/175341]\n",
      "loss: 0.709413  [81600/175341]\n",
      "loss: 1.180036  [83200/175341]\n",
      "loss: 0.576096  [84800/175341]\n",
      "loss: 0.782216  [86400/175341]\n",
      "loss: 0.789268  [88000/175341]\n",
      "loss: 0.733711  [89600/175341]\n",
      "loss: 1.065883  [91200/175341]\n",
      "loss: 0.425031  [92800/175341]\n",
      "loss: 0.607101  [94400/175341]\n",
      "loss: 0.829312  [96000/175341]\n",
      "loss: 1.027218  [97600/175341]\n",
      "loss: 0.265982  [99200/175341]\n",
      "loss: 0.613327  [100800/175341]\n",
      "loss: 0.837147  [102400/175341]\n",
      "loss: 0.566984  [104000/175341]\n",
      "loss: 1.564174  [105600/175341]\n",
      "loss: 0.605879  [107200/175341]\n",
      "loss: 0.337501  [108800/175341]\n",
      "loss: 0.700597  [110400/175341]\n",
      "loss: 0.898670  [112000/175341]\n",
      "loss: 0.404575  [113600/175341]\n",
      "loss: 0.912120  [115200/175341]\n",
      "loss: 0.533274  [116800/175341]\n",
      "loss: 0.500973  [118400/175341]\n",
      "loss: 0.348202  [120000/175341]\n",
      "loss: 0.584200  [121600/175341]\n",
      "loss: 1.101916  [123200/175341]\n",
      "loss: 0.643458  [124800/175341]\n",
      "loss: 0.378965  [126400/175341]\n",
      "loss: 1.032395  [128000/175341]\n",
      "loss: 0.686843  [129600/175341]\n",
      "loss: 0.508229  [131200/175341]\n",
      "loss: 0.449739  [132800/175341]\n",
      "loss: 0.594004  [134400/175341]\n",
      "loss: 0.523938  [136000/175341]\n",
      "loss: 0.695195  [137600/175341]\n",
      "loss: 0.950912  [139200/175341]\n",
      "loss: 0.345881  [140800/175341]\n",
      "loss: 0.721956  [142400/175341]\n",
      "loss: 0.691595  [144000/175341]\n",
      "loss: 0.928393  [145600/175341]\n",
      "loss: 1.244781  [147200/175341]\n",
      "loss: 0.514389  [148800/175341]\n",
      "loss: 0.871045  [150400/175341]\n",
      "loss: 0.471620  [152000/175341]\n",
      "loss: 0.647677  [153600/175341]\n",
      "loss: 0.550968  [155200/175341]\n",
      "loss: 0.644066  [156800/175341]\n",
      "loss: 0.438873  [158400/175341]\n",
      "loss: 0.476434  [160000/175341]\n",
      "loss: 0.320291  [161600/175341]\n",
      "loss: 0.414776  [163200/175341]\n",
      "loss: 0.896761  [164800/175341]\n",
      "loss: 1.343463  [166400/175341]\n",
      "loss: 0.656487  [168000/175341]\n",
      "loss: 0.500580  [169600/175341]\n",
      "loss: 0.833876  [171200/175341]\n",
      "loss: 0.420174  [172800/175341]\n",
      "loss: 0.777168  [174400/175341]\n",
      "Train Accuracy: 76.9027%\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.672079, F1-score: 71.87% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.494615  [    0/175341]\n",
      "loss: 0.544918  [ 1600/175341]\n",
      "loss: 0.775598  [ 3200/175341]\n",
      "loss: 0.416766  [ 4800/175341]\n",
      "loss: 0.925954  [ 6400/175341]\n",
      "loss: 0.821067  [ 8000/175341]\n",
      "loss: 0.450341  [ 9600/175341]\n",
      "loss: 0.660942  [11200/175341]\n",
      "loss: 0.362089  [12800/175341]\n",
      "loss: 0.242845  [14400/175341]\n",
      "loss: 0.726789  [16000/175341]\n",
      "loss: 0.414568  [17600/175341]\n",
      "loss: 0.537278  [19200/175341]\n",
      "loss: 0.602775  [20800/175341]\n",
      "loss: 0.732735  [22400/175341]\n",
      "loss: 0.459349  [24000/175341]\n",
      "loss: 1.181976  [25600/175341]\n",
      "loss: 0.406003  [27200/175341]\n",
      "loss: 0.957201  [28800/175341]\n",
      "loss: 0.868344  [30400/175341]\n",
      "loss: 0.488395  [32000/175341]\n",
      "loss: 0.453458  [33600/175341]\n",
      "loss: 0.473387  [35200/175341]\n",
      "loss: 0.522424  [36800/175341]\n",
      "loss: 0.852628  [38400/175341]\n",
      "loss: 0.461619  [40000/175341]\n",
      "loss: 0.801088  [41600/175341]\n",
      "loss: 0.553032  [43200/175341]\n",
      "loss: 0.400853  [44800/175341]\n",
      "loss: 0.620632  [46400/175341]\n",
      "loss: 0.648412  [48000/175341]\n",
      "loss: 0.342217  [49600/175341]\n",
      "loss: 0.595803  [51200/175341]\n",
      "loss: 0.531706  [52800/175341]\n",
      "loss: 0.538199  [54400/175341]\n",
      "loss: 0.589743  [56000/175341]\n",
      "loss: 0.606463  [57600/175341]\n",
      "loss: 0.213666  [59200/175341]\n",
      "loss: 0.762862  [60800/175341]\n",
      "loss: 0.522630  [62400/175341]\n",
      "loss: 0.591088  [64000/175341]\n",
      "loss: 0.560411  [65600/175341]\n",
      "loss: 0.343186  [67200/175341]\n",
      "loss: 0.489427  [68800/175341]\n",
      "loss: 0.540756  [70400/175341]\n",
      "loss: 0.353684  [72000/175341]\n",
      "loss: 0.642696  [73600/175341]\n",
      "loss: 0.565000  [75200/175341]\n",
      "loss: 0.418031  [76800/175341]\n",
      "loss: 0.377141  [78400/175341]\n",
      "loss: 0.542474  [80000/175341]\n",
      "loss: 0.444498  [81600/175341]\n",
      "loss: 0.947572  [83200/175341]\n",
      "loss: 0.407633  [84800/175341]\n",
      "loss: 0.894732  [86400/175341]\n",
      "loss: 0.604939  [88000/175341]\n",
      "loss: 0.660358  [89600/175341]\n",
      "loss: 0.724766  [91200/175341]\n",
      "loss: 0.772678  [92800/175341]\n",
      "loss: 0.786820  [94400/175341]\n",
      "loss: 0.898902  [96000/175341]\n",
      "loss: 0.725424  [97600/175341]\n",
      "loss: 0.556454  [99200/175341]\n",
      "loss: 0.629402  [100800/175341]\n",
      "loss: 1.054950  [102400/175341]\n",
      "loss: 0.859109  [104000/175341]\n",
      "loss: 1.014587  [105600/175341]\n",
      "loss: 0.456694  [107200/175341]\n",
      "loss: 0.600370  [108800/175341]\n",
      "loss: 0.489797  [110400/175341]\n",
      "loss: 0.515637  [112000/175341]\n",
      "loss: 0.698093  [113600/175341]\n",
      "loss: 0.934369  [115200/175341]\n",
      "loss: 0.868822  [116800/175341]\n",
      "loss: 0.707414  [118400/175341]\n",
      "loss: 0.699040  [120000/175341]\n",
      "loss: 0.898400  [121600/175341]\n",
      "loss: 0.434392  [123200/175341]\n",
      "loss: 0.579348  [124800/175341]\n",
      "loss: 0.664260  [126400/175341]\n",
      "loss: 0.914780  [128000/175341]\n",
      "loss: 0.716302  [129600/175341]\n",
      "loss: 0.569992  [131200/175341]\n",
      "loss: 0.692780  [132800/175341]\n",
      "loss: 0.481065  [134400/175341]\n",
      "loss: 0.891278  [136000/175341]\n",
      "loss: 0.544043  [137600/175341]\n",
      "loss: 0.944170  [139200/175341]\n",
      "loss: 0.679942  [140800/175341]\n",
      "loss: 0.565902  [142400/175341]\n",
      "loss: 0.561850  [144000/175341]\n",
      "loss: 0.408465  [145600/175341]\n",
      "loss: 0.979249  [147200/175341]\n",
      "loss: 0.369808  [148800/175341]\n",
      "loss: 0.394310  [150400/175341]\n",
      "loss: 0.828653  [152000/175341]\n",
      "loss: 0.488954  [153600/175341]\n",
      "loss: 0.630269  [155200/175341]\n",
      "loss: 0.314078  [156800/175341]\n",
      "loss: 0.130147  [158400/175341]\n",
      "loss: 0.554556  [160000/175341]\n",
      "loss: 0.350739  [161600/175341]\n",
      "loss: 0.615213  [163200/175341]\n",
      "loss: 0.591113  [164800/175341]\n",
      "loss: 0.697731  [166400/175341]\n",
      "loss: 0.744841  [168000/175341]\n",
      "loss: 0.717923  [169600/175341]\n",
      "loss: 0.521562  [171200/175341]\n",
      "loss: 0.773907  [172800/175341]\n",
      "loss: 0.708977  [174400/175341]\n",
      "Train Accuracy: 77.1308%\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.651602, F1-score: 71.70% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.799200  [    0/175341]\n",
      "loss: 0.592024  [ 1600/175341]\n",
      "loss: 0.909786  [ 3200/175341]\n",
      "loss: 0.322582  [ 4800/175341]\n",
      "loss: 0.334309  [ 6400/175341]\n",
      "loss: 0.476873  [ 8000/175341]\n",
      "loss: 0.442755  [ 9600/175341]\n",
      "loss: 0.635025  [11200/175341]\n",
      "loss: 1.203631  [12800/175341]\n",
      "loss: 0.563090  [14400/175341]\n",
      "loss: 0.634936  [16000/175341]\n",
      "loss: 0.701346  [17600/175341]\n",
      "loss: 0.766719  [19200/175341]\n",
      "loss: 0.907725  [20800/175341]\n",
      "loss: 0.536740  [22400/175341]\n",
      "loss: 0.637017  [24000/175341]\n",
      "loss: 0.654286  [25600/175341]\n",
      "loss: 0.933539  [27200/175341]\n",
      "loss: 0.851802  [28800/175341]\n",
      "loss: 0.297352  [30400/175341]\n",
      "loss: 0.338609  [32000/175341]\n",
      "loss: 0.526075  [33600/175341]\n",
      "loss: 0.459101  [35200/175341]\n",
      "loss: 0.472565  [36800/175341]\n",
      "loss: 0.222888  [38400/175341]\n",
      "loss: 0.470374  [40000/175341]\n",
      "loss: 0.637599  [41600/175341]\n",
      "loss: 0.682674  [43200/175341]\n",
      "loss: 0.905080  [44800/175341]\n",
      "loss: 0.700878  [46400/175341]\n",
      "loss: 0.544205  [48000/175341]\n",
      "loss: 0.567278  [49600/175341]\n",
      "loss: 0.347505  [51200/175341]\n",
      "loss: 0.663552  [52800/175341]\n",
      "loss: 0.878699  [54400/175341]\n",
      "loss: 0.712413  [56000/175341]\n",
      "loss: 0.504362  [57600/175341]\n",
      "loss: 1.288146  [59200/175341]\n",
      "loss: 0.461151  [60800/175341]\n",
      "loss: 0.779286  [62400/175341]\n",
      "loss: 0.482388  [64000/175341]\n",
      "loss: 0.397698  [65600/175341]\n",
      "loss: 0.511717  [67200/175341]\n",
      "loss: 0.565175  [68800/175341]\n",
      "loss: 0.690359  [70400/175341]\n",
      "loss: 0.720414  [72000/175341]\n",
      "loss: 0.634112  [73600/175341]\n",
      "loss: 0.825050  [75200/175341]\n",
      "loss: 0.911751  [76800/175341]\n",
      "loss: 0.398404  [78400/175341]\n",
      "loss: 1.200545  [80000/175341]\n",
      "loss: 0.439675  [81600/175341]\n",
      "loss: 0.987983  [83200/175341]\n",
      "loss: 0.527304  [84800/175341]\n",
      "loss: 1.130182  [86400/175341]\n",
      "loss: 0.514284  [88000/175341]\n",
      "loss: 1.140692  [89600/175341]\n",
      "loss: 0.893105  [91200/175341]\n",
      "loss: 0.424815  [92800/175341]\n",
      "loss: 0.564522  [94400/175341]\n",
      "loss: 0.133858  [96000/175341]\n",
      "loss: 0.268194  [97600/175341]\n",
      "loss: 0.344224  [99200/175341]\n",
      "loss: 0.797549  [100800/175341]\n",
      "loss: 0.130727  [102400/175341]\n",
      "loss: 0.430279  [104000/175341]\n",
      "loss: 0.713209  [105600/175341]\n",
      "loss: 0.672237  [107200/175341]\n",
      "loss: 0.645294  [108800/175341]\n",
      "loss: 0.491804  [110400/175341]\n",
      "loss: 0.469924  [112000/175341]\n",
      "loss: 0.458217  [113600/175341]\n",
      "loss: 0.534509  [115200/175341]\n",
      "loss: 0.409014  [116800/175341]\n",
      "loss: 0.677510  [118400/175341]\n",
      "loss: 0.875888  [120000/175341]\n",
      "loss: 0.288880  [121600/175341]\n",
      "loss: 0.621373  [123200/175341]\n",
      "loss: 0.628489  [124800/175341]\n",
      "loss: 0.772885  [126400/175341]\n",
      "loss: 0.797883  [128000/175341]\n",
      "loss: 0.540462  [129600/175341]\n",
      "loss: 0.458819  [131200/175341]\n",
      "loss: 0.718453  [132800/175341]\n",
      "loss: 0.768247  [134400/175341]\n",
      "loss: 0.946050  [136000/175341]\n",
      "loss: 0.580321  [137600/175341]\n",
      "loss: 0.461827  [139200/175341]\n",
      "loss: 0.502215  [140800/175341]\n",
      "loss: 0.423871  [142400/175341]\n",
      "loss: 1.129621  [144000/175341]\n",
      "loss: 0.926116  [145600/175341]\n",
      "loss: 0.490955  [147200/175341]\n",
      "loss: 0.409155  [148800/175341]\n",
      "loss: 0.282427  [150400/175341]\n",
      "loss: 0.259793  [152000/175341]\n",
      "loss: 0.374703  [153600/175341]\n",
      "loss: 1.010289  [155200/175341]\n",
      "loss: 0.451556  [156800/175341]\n",
      "loss: 0.672956  [158400/175341]\n",
      "loss: 0.442824  [160000/175341]\n",
      "loss: 0.399317  [161600/175341]\n",
      "loss: 0.164606  [163200/175341]\n",
      "loss: 0.529473  [164800/175341]\n",
      "loss: 0.576539  [166400/175341]\n",
      "loss: 0.545885  [168000/175341]\n",
      "loss: 0.492410  [169600/175341]\n",
      "loss: 0.691487  [171200/175341]\n",
      "loss: 0.442143  [172800/175341]\n",
      "loss: 0.649705  [174400/175341]\n",
      "Train Accuracy: 77.3898%\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.639048, F1-score: 72.17% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.783408  [    0/175341]\n",
      "loss: 0.502952  [ 1600/175341]\n",
      "loss: 0.628889  [ 3200/175341]\n",
      "loss: 0.632769  [ 4800/175341]\n",
      "loss: 0.400546  [ 6400/175341]\n",
      "loss: 0.619711  [ 8000/175341]\n",
      "loss: 0.394877  [ 9600/175341]\n",
      "loss: 0.501343  [11200/175341]\n",
      "loss: 0.301385  [12800/175341]\n",
      "loss: 0.460470  [14400/175341]\n",
      "loss: 0.658381  [16000/175341]\n",
      "loss: 0.714897  [17600/175341]\n",
      "loss: 0.476244  [19200/175341]\n",
      "loss: 0.785119  [20800/175341]\n",
      "loss: 0.445464  [22400/175341]\n",
      "loss: 0.397406  [24000/175341]\n",
      "loss: 0.897131  [25600/175341]\n",
      "loss: 1.027044  [27200/175341]\n",
      "loss: 0.469071  [28800/175341]\n",
      "loss: 0.894034  [30400/175341]\n",
      "loss: 0.337790  [32000/175341]\n",
      "loss: 0.395028  [33600/175341]\n",
      "loss: 0.518294  [35200/175341]\n",
      "loss: 0.454193  [36800/175341]\n",
      "loss: 0.781517  [38400/175341]\n",
      "loss: 0.712119  [40000/175341]\n",
      "loss: 0.634538  [41600/175341]\n",
      "loss: 0.222866  [43200/175341]\n",
      "loss: 0.525856  [44800/175341]\n",
      "loss: 0.434756  [46400/175341]\n",
      "loss: 0.224153  [48000/175341]\n",
      "loss: 0.928403  [49600/175341]\n",
      "loss: 0.188530  [51200/175341]\n",
      "loss: 0.560399  [52800/175341]\n",
      "loss: 0.706155  [54400/175341]\n",
      "loss: 0.652588  [56000/175341]\n",
      "loss: 0.603145  [57600/175341]\n",
      "loss: 0.655318  [59200/175341]\n",
      "loss: 0.359753  [60800/175341]\n",
      "loss: 0.583516  [62400/175341]\n",
      "loss: 0.482248  [64000/175341]\n",
      "loss: 0.589001  [65600/175341]\n",
      "loss: 0.416434  [67200/175341]\n",
      "loss: 0.121853  [68800/175341]\n",
      "loss: 0.401200  [70400/175341]\n",
      "loss: 0.593150  [72000/175341]\n",
      "loss: 1.881120  [73600/175341]\n",
      "loss: 0.513397  [75200/175341]\n",
      "loss: 0.141981  [76800/175341]\n",
      "loss: 0.768296  [78400/175341]\n",
      "loss: 0.705213  [80000/175341]\n",
      "loss: 0.775596  [81600/175341]\n",
      "loss: 0.510386  [83200/175341]\n",
      "loss: 0.452183  [84800/175341]\n",
      "loss: 0.554839  [86400/175341]\n",
      "loss: 0.518324  [88000/175341]\n",
      "loss: 0.336529  [89600/175341]\n",
      "loss: 0.443210  [91200/175341]\n",
      "loss: 0.534536  [92800/175341]\n",
      "loss: 0.436616  [94400/175341]\n",
      "loss: 0.336741  [96000/175341]\n",
      "loss: 0.459016  [97600/175341]\n",
      "loss: 0.421247  [99200/175341]\n",
      "loss: 0.384739  [100800/175341]\n",
      "loss: 0.306895  [102400/175341]\n",
      "loss: 0.430972  [104000/175341]\n",
      "loss: 0.427531  [105600/175341]\n",
      "loss: 0.707402  [107200/175341]\n",
      "loss: 0.501749  [108800/175341]\n",
      "loss: 0.601774  [110400/175341]\n",
      "loss: 0.316502  [112000/175341]\n",
      "loss: 0.385799  [113600/175341]\n",
      "loss: 0.740443  [115200/175341]\n",
      "loss: 0.449884  [116800/175341]\n",
      "loss: 0.456297  [118400/175341]\n",
      "loss: 0.353877  [120000/175341]\n",
      "loss: 0.440373  [121600/175341]\n",
      "loss: 0.465456  [123200/175341]\n",
      "loss: 0.606317  [124800/175341]\n",
      "loss: 0.708381  [126400/175341]\n",
      "loss: 0.342461  [128000/175341]\n",
      "loss: 0.638629  [129600/175341]\n",
      "loss: 0.832066  [131200/175341]\n",
      "loss: 0.476974  [132800/175341]\n",
      "loss: 0.578366  [134400/175341]\n",
      "loss: 0.099357  [136000/175341]\n",
      "loss: 0.672362  [137600/175341]\n",
      "loss: 0.467732  [139200/175341]\n",
      "loss: 0.359365  [140800/175341]\n",
      "loss: 1.177081  [142400/175341]\n",
      "loss: 1.247606  [144000/175341]\n",
      "loss: 0.403469  [145600/175341]\n",
      "loss: 0.748158  [147200/175341]\n",
      "loss: 0.324972  [148800/175341]\n",
      "loss: 0.411033  [150400/175341]\n",
      "loss: 0.597813  [152000/175341]\n",
      "loss: 0.563944  [153600/175341]\n",
      "loss: 0.749690  [155200/175341]\n",
      "loss: 0.439148  [156800/175341]\n",
      "loss: 0.295317  [158400/175341]\n",
      "loss: 0.598937  [160000/175341]\n",
      "loss: 0.659577  [161600/175341]\n",
      "loss: 0.281522  [163200/175341]\n",
      "loss: 0.431468  [164800/175341]\n",
      "loss: 0.508126  [166400/175341]\n",
      "loss: 0.218755  [168000/175341]\n",
      "loss: 0.554562  [169600/175341]\n",
      "loss: 0.505264  [171200/175341]\n",
      "loss: 0.601312  [172800/175341]\n",
      "loss: 0.834700  [174400/175341]\n",
      "Train Accuracy: 77.6150%\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.635941, F1-score: 72.69% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.914017  [    0/175341]\n",
      "loss: 0.467430  [ 1600/175341]\n",
      "loss: 0.319799  [ 3200/175341]\n",
      "loss: 0.738062  [ 4800/175341]\n",
      "loss: 0.574228  [ 6400/175341]\n",
      "loss: 0.473619  [ 8000/175341]\n",
      "loss: 0.740533  [ 9600/175341]\n",
      "loss: 0.484368  [11200/175341]\n",
      "loss: 0.698378  [12800/175341]\n",
      "loss: 0.533253  [14400/175341]\n",
      "loss: 0.752252  [16000/175341]\n",
      "loss: 0.533243  [17600/175341]\n",
      "loss: 0.458319  [19200/175341]\n",
      "loss: 0.567097  [20800/175341]\n",
      "loss: 0.740114  [22400/175341]\n",
      "loss: 1.026672  [24000/175341]\n",
      "loss: 0.158283  [25600/175341]\n",
      "loss: 0.407575  [27200/175341]\n",
      "loss: 0.325731  [28800/175341]\n",
      "loss: 0.359879  [30400/175341]\n",
      "loss: 0.467982  [32000/175341]\n",
      "loss: 0.700859  [33600/175341]\n",
      "loss: 0.404812  [35200/175341]\n",
      "loss: 0.396837  [36800/175341]\n",
      "loss: 0.959170  [38400/175341]\n",
      "loss: 0.884253  [40000/175341]\n",
      "loss: 0.642056  [41600/175341]\n",
      "loss: 0.543586  [43200/175341]\n",
      "loss: 0.487178  [44800/175341]\n",
      "loss: 0.730670  [46400/175341]\n",
      "loss: 0.286305  [48000/175341]\n",
      "loss: 0.896296  [49600/175341]\n",
      "loss: 0.568284  [51200/175341]\n",
      "loss: 0.390454  [52800/175341]\n",
      "loss: 0.572657  [54400/175341]\n",
      "loss: 0.315698  [56000/175341]\n",
      "loss: 0.715205  [57600/175341]\n",
      "loss: 0.510254  [59200/175341]\n",
      "loss: 0.399607  [60800/175341]\n",
      "loss: 0.505522  [62400/175341]\n",
      "loss: 0.440719  [64000/175341]\n",
      "loss: 0.464799  [65600/175341]\n",
      "loss: 0.605996  [67200/175341]\n",
      "loss: 0.631340  [68800/175341]\n",
      "loss: 0.681484  [70400/175341]\n",
      "loss: 0.437878  [72000/175341]\n",
      "loss: 0.257468  [73600/175341]\n",
      "loss: 0.870232  [75200/175341]\n",
      "loss: 0.894001  [76800/175341]\n",
      "loss: 0.514614  [78400/175341]\n",
      "loss: 0.353077  [80000/175341]\n",
      "loss: 0.585600  [81600/175341]\n",
      "loss: 0.589845  [83200/175341]\n",
      "loss: 0.471101  [84800/175341]\n",
      "loss: 0.572216  [86400/175341]\n",
      "loss: 0.711960  [88000/175341]\n",
      "loss: 0.422615  [89600/175341]\n",
      "loss: 0.283214  [91200/175341]\n",
      "loss: 0.518709  [92800/175341]\n",
      "loss: 0.404607  [94400/175341]\n",
      "loss: 0.470711  [96000/175341]\n",
      "loss: 0.260312  [97600/175341]\n",
      "loss: 0.744321  [99200/175341]\n",
      "loss: 0.631856  [100800/175341]\n",
      "loss: 0.418748  [102400/175341]\n",
      "loss: 0.418055  [104000/175341]\n",
      "loss: 0.487139  [105600/175341]\n",
      "loss: 0.591851  [107200/175341]\n",
      "loss: 0.586089  [108800/175341]\n",
      "loss: 1.000760  [110400/175341]\n",
      "loss: 0.645750  [112000/175341]\n",
      "loss: 0.615652  [113600/175341]\n",
      "loss: 0.238925  [115200/175341]\n",
      "loss: 0.772803  [116800/175341]\n",
      "loss: 0.558147  [118400/175341]\n",
      "loss: 0.832114  [120000/175341]\n",
      "loss: 0.842511  [121600/175341]\n",
      "loss: 0.399457  [123200/175341]\n",
      "loss: 0.828375  [124800/175341]\n",
      "loss: 0.334873  [126400/175341]\n",
      "loss: 1.082328  [128000/175341]\n",
      "loss: 0.470012  [129600/175341]\n",
      "loss: 0.740200  [131200/175341]\n",
      "loss: 0.376278  [132800/175341]\n",
      "loss: 0.229909  [134400/175341]\n",
      "loss: 0.387872  [136000/175341]\n",
      "loss: 1.621100  [137600/175341]\n",
      "loss: 0.514073  [139200/175341]\n",
      "loss: 0.441146  [140800/175341]\n",
      "loss: 0.760783  [142400/175341]\n",
      "loss: 1.000593  [144000/175341]\n",
      "loss: 0.313506  [145600/175341]\n",
      "loss: 0.785336  [147200/175341]\n",
      "loss: 0.369716  [148800/175341]\n",
      "loss: 0.490795  [150400/175341]\n",
      "loss: 0.353908  [152000/175341]\n",
      "loss: 0.438869  [153600/175341]\n",
      "loss: 0.456913  [155200/175341]\n",
      "loss: 0.842633  [156800/175341]\n",
      "loss: 0.659523  [158400/175341]\n",
      "loss: 0.824991  [160000/175341]\n",
      "loss: 0.646433  [161600/175341]\n",
      "loss: 0.866840  [163200/175341]\n",
      "loss: 0.583090  [164800/175341]\n",
      "loss: 0.555938  [166400/175341]\n",
      "loss: 0.249093  [168000/175341]\n",
      "loss: 0.460196  [169600/175341]\n",
      "loss: 0.759275  [171200/175341]\n",
      "loss: 0.844071  [172800/175341]\n",
      "loss: 0.660798  [174400/175341]\n",
      "Train Accuracy: 77.8010%\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.643736, F1-score: 71.87% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.450432  [    0/175341]\n",
      "loss: 0.322177  [ 1600/175341]\n",
      "loss: 0.697475  [ 3200/175341]\n",
      "loss: 0.638260  [ 4800/175341]\n",
      "loss: 0.711066  [ 6400/175341]\n",
      "loss: 0.615277  [ 8000/175341]\n",
      "loss: 0.459177  [ 9600/175341]\n",
      "loss: 0.393365  [11200/175341]\n",
      "loss: 1.024825  [12800/175341]\n",
      "loss: 0.407376  [14400/175341]\n",
      "loss: 0.485486  [16000/175341]\n",
      "loss: 0.424111  [17600/175341]\n",
      "loss: 0.188141  [19200/175341]\n",
      "loss: 0.577965  [20800/175341]\n",
      "loss: 0.306988  [22400/175341]\n",
      "loss: 0.447828  [24000/175341]\n",
      "loss: 0.391734  [25600/175341]\n",
      "loss: 0.782868  [27200/175341]\n",
      "loss: 0.762309  [28800/175341]\n",
      "loss: 1.034432  [30400/175341]\n",
      "loss: 0.259825  [32000/175341]\n",
      "loss: 0.334059  [33600/175341]\n",
      "loss: 0.410964  [35200/175341]\n",
      "loss: 0.347536  [36800/175341]\n",
      "loss: 0.554294  [38400/175341]\n",
      "loss: 0.650161  [40000/175341]\n",
      "loss: 0.483071  [41600/175341]\n",
      "loss: 1.022739  [43200/175341]\n",
      "loss: 0.701624  [44800/175341]\n",
      "loss: 0.477429  [46400/175341]\n",
      "loss: 0.745617  [48000/175341]\n",
      "loss: 0.353631  [49600/175341]\n",
      "loss: 0.410218  [51200/175341]\n",
      "loss: 0.342874  [52800/175341]\n",
      "loss: 0.818860  [54400/175341]\n",
      "loss: 0.899785  [56000/175341]\n",
      "loss: 0.633418  [57600/175341]\n",
      "loss: 0.520895  [59200/175341]\n",
      "loss: 0.967997  [60800/175341]\n",
      "loss: 0.564803  [62400/175341]\n",
      "loss: 0.577885  [64000/175341]\n",
      "loss: 0.238074  [65600/175341]\n",
      "loss: 0.608870  [67200/175341]\n",
      "loss: 0.479489  [68800/175341]\n",
      "loss: 0.626693  [70400/175341]\n",
      "loss: 0.835020  [72000/175341]\n",
      "loss: 0.504189  [73600/175341]\n",
      "loss: 0.495374  [75200/175341]\n",
      "loss: 0.599189  [76800/175341]\n",
      "loss: 0.470907  [78400/175341]\n",
      "loss: 0.301522  [80000/175341]\n",
      "loss: 0.514283  [81600/175341]\n",
      "loss: 0.410116  [83200/175341]\n",
      "loss: 0.300342  [84800/175341]\n",
      "loss: 0.571763  [86400/175341]\n",
      "loss: 0.858540  [88000/175341]\n",
      "loss: 0.581112  [89600/175341]\n",
      "loss: 0.666441  [91200/175341]\n",
      "loss: 0.931765  [92800/175341]\n",
      "loss: 0.142160  [94400/175341]\n",
      "loss: 0.999120  [96000/175341]\n",
      "loss: 0.670803  [97600/175341]\n",
      "loss: 0.345368  [99200/175341]\n",
      "loss: 0.794413  [100800/175341]\n",
      "loss: 0.470471  [102400/175341]\n",
      "loss: 0.536508  [104000/175341]\n",
      "loss: 0.060102  [105600/175341]\n",
      "loss: 0.448505  [107200/175341]\n",
      "loss: 0.568018  [108800/175341]\n",
      "loss: 0.499622  [110400/175341]\n",
      "loss: 0.483678  [112000/175341]\n",
      "loss: 0.319448  [113600/175341]\n",
      "loss: 0.595327  [115200/175341]\n",
      "loss: 1.075915  [116800/175341]\n",
      "loss: 0.469120  [118400/175341]\n",
      "loss: 0.695465  [120000/175341]\n",
      "loss: 0.527652  [121600/175341]\n",
      "loss: 0.285331  [123200/175341]\n",
      "loss: 0.893470  [124800/175341]\n",
      "loss: 1.093381  [126400/175341]\n",
      "loss: 0.606910  [128000/175341]\n",
      "loss: 0.359057  [129600/175341]\n",
      "loss: 0.377788  [131200/175341]\n",
      "loss: 0.447708  [132800/175341]\n",
      "loss: 0.654828  [134400/175341]\n",
      "loss: 0.605488  [136000/175341]\n",
      "loss: 0.248398  [137600/175341]\n",
      "loss: 0.329159  [139200/175341]\n",
      "loss: 0.811725  [140800/175341]\n",
      "loss: 0.291378  [142400/175341]\n",
      "loss: 0.589428  [144000/175341]\n",
      "loss: 0.592637  [145600/175341]\n",
      "loss: 0.394996  [147200/175341]\n",
      "loss: 0.747834  [148800/175341]\n",
      "loss: 0.415504  [150400/175341]\n",
      "loss: 0.699911  [152000/175341]\n",
      "loss: 0.807255  [153600/175341]\n",
      "loss: 0.775475  [155200/175341]\n",
      "loss: 0.831428  [156800/175341]\n",
      "loss: 0.715717  [158400/175341]\n",
      "loss: 0.625221  [160000/175341]\n",
      "loss: 0.415342  [161600/175341]\n",
      "loss: 0.209500  [163200/175341]\n",
      "loss: 0.924088  [164800/175341]\n",
      "loss: 0.397258  [166400/175341]\n",
      "loss: 0.405043  [168000/175341]\n",
      "loss: 0.614468  [169600/175341]\n",
      "loss: 0.691820  [171200/175341]\n",
      "loss: 0.449965  [172800/175341]\n",
      "loss: 0.903087  [174400/175341]\n",
      "Train Accuracy: 77.9253%\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.638229, F1-score: 72.31% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.485042  [    0/175341]\n",
      "loss: 0.489549  [ 1600/175341]\n",
      "loss: 0.897424  [ 3200/175341]\n",
      "loss: 0.306230  [ 4800/175341]\n",
      "loss: 0.975315  [ 6400/175341]\n",
      "loss: 0.798393  [ 8000/175341]\n",
      "loss: 0.136299  [ 9600/175341]\n",
      "loss: 0.927721  [11200/175341]\n",
      "loss: 0.797888  [12800/175341]\n",
      "loss: 0.659467  [14400/175341]\n",
      "loss: 0.244234  [16000/175341]\n",
      "loss: 0.789888  [17600/175341]\n",
      "loss: 0.251704  [19200/175341]\n",
      "loss: 0.664859  [20800/175341]\n",
      "loss: 0.717066  [22400/175341]\n",
      "loss: 0.271103  [24000/175341]\n",
      "loss: 0.561615  [25600/175341]\n",
      "loss: 0.867956  [27200/175341]\n",
      "loss: 0.578535  [28800/175341]\n",
      "loss: 0.589529  [30400/175341]\n",
      "loss: 0.477167  [32000/175341]\n",
      "loss: 0.441763  [33600/175341]\n",
      "loss: 0.535347  [35200/175341]\n",
      "loss: 0.281476  [36800/175341]\n",
      "loss: 0.761850  [38400/175341]\n",
      "loss: 0.258323  [40000/175341]\n",
      "loss: 0.579728  [41600/175341]\n",
      "loss: 0.812692  [43200/175341]\n",
      "loss: 0.396241  [44800/175341]\n",
      "loss: 0.233057  [46400/175341]\n",
      "loss: 0.598823  [48000/175341]\n",
      "loss: 0.586930  [49600/175341]\n",
      "loss: 1.024388  [51200/175341]\n",
      "loss: 0.312185  [52800/175341]\n",
      "loss: 0.809023  [54400/175341]\n",
      "loss: 0.610413  [56000/175341]\n",
      "loss: 0.852199  [57600/175341]\n",
      "loss: 0.294236  [59200/175341]\n",
      "loss: 0.646672  [60800/175341]\n",
      "loss: 1.017627  [62400/175341]\n",
      "loss: 0.780439  [64000/175341]\n",
      "loss: 0.560020  [65600/175341]\n",
      "loss: 0.629432  [67200/175341]\n",
      "loss: 0.758637  [68800/175341]\n",
      "loss: 0.327835  [70400/175341]\n",
      "loss: 0.470654  [72000/175341]\n",
      "loss: 0.447249  [73600/175341]\n",
      "loss: 0.362232  [75200/175341]\n",
      "loss: 0.599007  [76800/175341]\n",
      "loss: 0.953276  [78400/175341]\n",
      "loss: 0.159895  [80000/175341]\n",
      "loss: 0.122063  [81600/175341]\n",
      "loss: 0.614224  [83200/175341]\n",
      "loss: 0.834725  [84800/175341]\n",
      "loss: 0.261642  [86400/175341]\n",
      "loss: 0.269529  [88000/175341]\n",
      "loss: 0.570708  [89600/175341]\n",
      "loss: 0.739071  [91200/175341]\n",
      "loss: 0.491866  [92800/175341]\n",
      "loss: 0.498740  [94400/175341]\n",
      "loss: 0.607738  [96000/175341]\n",
      "loss: 1.102021  [97600/175341]\n",
      "loss: 0.663555  [99200/175341]\n",
      "loss: 0.339941  [100800/175341]\n",
      "loss: 0.513092  [102400/175341]\n",
      "loss: 0.589072  [104000/175341]\n",
      "loss: 0.353232  [105600/175341]\n",
      "loss: 0.384808  [107200/175341]\n",
      "loss: 0.450107  [108800/175341]\n",
      "loss: 0.671131  [110400/175341]\n",
      "loss: 0.965224  [112000/175341]\n",
      "loss: 1.059525  [113600/175341]\n",
      "loss: 0.326428  [115200/175341]\n",
      "loss: 0.758955  [116800/175341]\n",
      "loss: 0.486902  [118400/175341]\n",
      "loss: 0.549188  [120000/175341]\n",
      "loss: 0.588020  [121600/175341]\n",
      "loss: 0.449896  [123200/175341]\n",
      "loss: 0.198841  [124800/175341]\n",
      "loss: 0.586918  [126400/175341]\n",
      "loss: 0.699610  [128000/175341]\n",
      "loss: 0.439715  [129600/175341]\n",
      "loss: 0.464788  [131200/175341]\n",
      "loss: 0.997142  [132800/175341]\n",
      "loss: 0.526644  [134400/175341]\n",
      "loss: 1.265746  [136000/175341]\n",
      "loss: 0.581281  [137600/175341]\n",
      "loss: 0.876888  [139200/175341]\n",
      "loss: 0.863937  [140800/175341]\n",
      "loss: 0.313695  [142400/175341]\n",
      "loss: 0.283794  [144000/175341]\n",
      "loss: 0.625490  [145600/175341]\n",
      "loss: 0.644311  [147200/175341]\n",
      "loss: 0.496869  [148800/175341]\n",
      "loss: 0.933126  [150400/175341]\n",
      "loss: 0.478110  [152000/175341]\n",
      "loss: 0.894628  [153600/175341]\n",
      "loss: 0.372007  [155200/175341]\n",
      "loss: 0.503099  [156800/175341]\n",
      "loss: 0.267356  [158400/175341]\n",
      "loss: 1.200104  [160000/175341]\n",
      "loss: 0.344498  [161600/175341]\n",
      "loss: 0.842223  [163200/175341]\n",
      "loss: 0.330033  [164800/175341]\n",
      "loss: 0.498935  [166400/175341]\n",
      "loss: 0.439654  [168000/175341]\n",
      "loss: 0.483281  [169600/175341]\n",
      "loss: 0.880432  [171200/175341]\n",
      "loss: 0.443251  [172800/175341]\n",
      "loss: 0.852669  [174400/175341]\n",
      "Train Accuracy: 78.0262%\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.648132, F1-score: 71.90% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.805902  [    0/175341]\n",
      "loss: 1.130859  [ 1600/175341]\n",
      "loss: 0.701921  [ 3200/175341]\n",
      "loss: 0.389120  [ 4800/175341]\n",
      "loss: 0.398639  [ 6400/175341]\n",
      "loss: 0.301960  [ 8000/175341]\n",
      "loss: 0.936529  [ 9600/175341]\n",
      "loss: 0.461408  [11200/175341]\n",
      "loss: 0.295135  [12800/175341]\n",
      "loss: 0.194074  [14400/175341]\n",
      "loss: 0.970083  [16000/175341]\n",
      "loss: 0.320054  [17600/175341]\n",
      "loss: 0.374348  [19200/175341]\n",
      "loss: 0.289555  [20800/175341]\n",
      "loss: 0.647420  [22400/175341]\n",
      "loss: 0.439415  [24000/175341]\n",
      "loss: 0.851499  [25600/175341]\n",
      "loss: 0.724261  [27200/175341]\n",
      "loss: 0.585761  [28800/175341]\n",
      "loss: 0.223027  [30400/175341]\n",
      "loss: 0.815835  [32000/175341]\n",
      "loss: 1.061678  [33600/175341]\n",
      "loss: 0.432288  [35200/175341]\n",
      "loss: 0.309845  [36800/175341]\n",
      "loss: 0.384146  [38400/175341]\n",
      "loss: 0.289764  [40000/175341]\n",
      "loss: 0.244267  [41600/175341]\n",
      "loss: 0.844736  [43200/175341]\n",
      "loss: 0.650259  [44800/175341]\n",
      "loss: 0.474783  [46400/175341]\n",
      "loss: 0.419225  [48000/175341]\n",
      "loss: 0.550674  [49600/175341]\n",
      "loss: 0.463540  [51200/175341]\n",
      "loss: 0.737264  [52800/175341]\n",
      "loss: 0.282060  [54400/175341]\n",
      "loss: 1.147881  [56000/175341]\n",
      "loss: 0.927229  [57600/175341]\n",
      "loss: 0.702823  [59200/175341]\n",
      "loss: 0.547148  [60800/175341]\n",
      "loss: 0.496781  [62400/175341]\n",
      "loss: 0.306264  [64000/175341]\n",
      "loss: 1.495345  [65600/175341]\n",
      "loss: 0.595605  [67200/175341]\n",
      "loss: 0.653604  [68800/175341]\n",
      "loss: 0.462520  [70400/175341]\n",
      "loss: 0.532102  [72000/175341]\n",
      "loss: 0.057439  [73600/175341]\n",
      "loss: 0.550125  [75200/175341]\n",
      "loss: 0.299016  [76800/175341]\n",
      "loss: 0.443682  [78400/175341]\n",
      "loss: 0.896080  [80000/175341]\n",
      "loss: 0.799997  [81600/175341]\n",
      "loss: 0.737286  [83200/175341]\n",
      "loss: 0.351832  [84800/175341]\n",
      "loss: 1.378956  [86400/175341]\n",
      "loss: 0.805114  [88000/175341]\n",
      "loss: 0.353603  [89600/175341]\n",
      "loss: 0.377111  [91200/175341]\n",
      "loss: 0.512872  [92800/175341]\n",
      "loss: 0.492816  [94400/175341]\n",
      "loss: 0.362304  [96000/175341]\n",
      "loss: 0.627176  [97600/175341]\n",
      "loss: 0.384904  [99200/175341]\n",
      "loss: 0.282098  [100800/175341]\n",
      "loss: 0.677902  [102400/175341]\n",
      "loss: 0.810520  [104000/175341]\n",
      "loss: 0.289941  [105600/175341]\n",
      "loss: 0.755768  [107200/175341]\n",
      "loss: 0.677393  [108800/175341]\n",
      "loss: 0.491999  [110400/175341]\n",
      "loss: 0.822027  [112000/175341]\n",
      "loss: 0.311968  [113600/175341]\n",
      "loss: 0.788578  [115200/175341]\n",
      "loss: 0.813715  [116800/175341]\n",
      "loss: 0.461277  [118400/175341]\n",
      "loss: 0.689926  [120000/175341]\n",
      "loss: 0.299560  [121600/175341]\n",
      "loss: 0.171636  [123200/175341]\n",
      "loss: 0.552954  [124800/175341]\n",
      "loss: 0.479380  [126400/175341]\n",
      "loss: 0.318525  [128000/175341]\n",
      "loss: 0.170793  [129600/175341]\n",
      "loss: 1.188234  [131200/175341]\n",
      "loss: 0.616498  [132800/175341]\n",
      "loss: 0.533563  [134400/175341]\n",
      "loss: 0.574949  [136000/175341]\n",
      "loss: 0.242895  [137600/175341]\n",
      "loss: 0.742761  [139200/175341]\n",
      "loss: 0.526896  [140800/175341]\n",
      "loss: 0.452581  [142400/175341]\n",
      "loss: 0.662624  [144000/175341]\n",
      "loss: 0.575838  [145600/175341]\n",
      "loss: 0.722430  [147200/175341]\n",
      "loss: 0.316023  [148800/175341]\n",
      "loss: 0.542339  [150400/175341]\n",
      "loss: 0.679198  [152000/175341]\n",
      "loss: 0.348322  [153600/175341]\n",
      "loss: 1.092665  [155200/175341]\n",
      "loss: 0.623136  [156800/175341]\n",
      "loss: 0.535498  [158400/175341]\n",
      "loss: 0.762302  [160000/175341]\n",
      "loss: 0.746329  [161600/175341]\n",
      "loss: 0.458748  [163200/175341]\n",
      "loss: 0.479248  [164800/175341]\n",
      "loss: 0.473195  [166400/175341]\n",
      "loss: 0.578274  [168000/175341]\n",
      "loss: 0.485969  [169600/175341]\n",
      "loss: 0.652446  [171200/175341]\n",
      "loss: 0.502945  [172800/175341]\n",
      "loss: 0.897025  [174400/175341]\n",
      "Train Accuracy: 78.2361%\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.641340, F1-score: 71.82% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.495949  [    0/175341]\n",
      "loss: 0.602557  [ 1600/175341]\n",
      "loss: 0.769369  [ 3200/175341]\n",
      "loss: 0.742669  [ 4800/175341]\n",
      "loss: 0.349846  [ 6400/175341]\n",
      "loss: 0.861787  [ 8000/175341]\n",
      "loss: 0.658580  [ 9600/175341]\n",
      "loss: 0.464166  [11200/175341]\n",
      "loss: 0.372767  [12800/175341]\n",
      "loss: 1.109898  [14400/175341]\n",
      "loss: 0.389088  [16000/175341]\n",
      "loss: 0.847386  [17600/175341]\n",
      "loss: 0.803904  [19200/175341]\n",
      "loss: 0.223570  [20800/175341]\n",
      "loss: 0.349059  [22400/175341]\n",
      "loss: 0.543591  [24000/175341]\n",
      "loss: 0.754194  [25600/175341]\n",
      "loss: 0.762642  [27200/175341]\n",
      "loss: 0.763932  [28800/175341]\n",
      "loss: 0.510410  [30400/175341]\n",
      "loss: 0.776992  [32000/175341]\n",
      "loss: 0.479761  [33600/175341]\n",
      "loss: 0.349407  [35200/175341]\n",
      "loss: 0.649396  [36800/175341]\n",
      "loss: 0.256761  [38400/175341]\n",
      "loss: 1.044539  [40000/175341]\n",
      "loss: 0.502316  [41600/175341]\n",
      "loss: 0.326342  [43200/175341]\n",
      "loss: 0.250149  [44800/175341]\n",
      "loss: 0.836505  [46400/175341]\n",
      "loss: 1.010117  [48000/175341]\n",
      "loss: 0.197763  [49600/175341]\n",
      "loss: 1.089634  [51200/175341]\n",
      "loss: 0.401668  [52800/175341]\n",
      "loss: 0.384475  [54400/175341]\n",
      "loss: 0.547727  [56000/175341]\n",
      "loss: 0.633672  [57600/175341]\n",
      "loss: 0.468306  [59200/175341]\n",
      "loss: 0.338992  [60800/175341]\n",
      "loss: 0.597699  [62400/175341]\n",
      "loss: 0.530150  [64000/175341]\n",
      "loss: 0.801348  [65600/175341]\n",
      "loss: 0.611642  [67200/175341]\n",
      "loss: 0.692090  [68800/175341]\n",
      "loss: 0.631613  [70400/175341]\n",
      "loss: 0.405547  [72000/175341]\n",
      "loss: 0.477992  [73600/175341]\n",
      "loss: 0.711519  [75200/175341]\n",
      "loss: 0.728351  [76800/175341]\n",
      "loss: 0.324405  [78400/175341]\n",
      "loss: 0.504272  [80000/175341]\n",
      "loss: 0.754487  [81600/175341]\n",
      "loss: 0.581130  [83200/175341]\n",
      "loss: 0.737438  [84800/175341]\n",
      "loss: 0.485703  [86400/175341]\n",
      "loss: 0.515406  [88000/175341]\n",
      "loss: 0.477762  [89600/175341]\n",
      "loss: 0.522267  [91200/175341]\n",
      "loss: 0.626887  [92800/175341]\n",
      "loss: 0.628872  [94400/175341]\n",
      "loss: 0.575291  [96000/175341]\n",
      "loss: 0.453823  [97600/175341]\n",
      "loss: 0.474299  [99200/175341]\n",
      "loss: 0.214101  [100800/175341]\n",
      "loss: 0.794800  [102400/175341]\n",
      "loss: 0.803795  [104000/175341]\n",
      "loss: 0.433589  [105600/175341]\n",
      "loss: 0.597668  [107200/175341]\n",
      "loss: 0.338457  [108800/175341]\n",
      "loss: 0.678581  [110400/175341]\n",
      "loss: 0.432969  [112000/175341]\n",
      "loss: 0.402764  [113600/175341]\n",
      "loss: 0.528231  [115200/175341]\n",
      "loss: 0.471726  [116800/175341]\n",
      "loss: 0.951071  [118400/175341]\n",
      "loss: 0.519021  [120000/175341]\n",
      "loss: 0.704023  [121600/175341]\n",
      "loss: 0.267468  [123200/175341]\n",
      "loss: 0.294557  [124800/175341]\n",
      "loss: 0.680492  [126400/175341]\n",
      "loss: 0.393234  [128000/175341]\n",
      "loss: 0.709614  [129600/175341]\n",
      "loss: 0.752715  [131200/175341]\n",
      "loss: 0.246674  [132800/175341]\n",
      "loss: 0.363387  [134400/175341]\n",
      "loss: 0.459128  [136000/175341]\n",
      "loss: 0.588302  [137600/175341]\n",
      "loss: 0.274706  [139200/175341]\n",
      "loss: 1.440001  [140800/175341]\n",
      "loss: 0.269777  [142400/175341]\n",
      "loss: 0.342232  [144000/175341]\n",
      "loss: 0.612591  [145600/175341]\n",
      "loss: 0.595864  [147200/175341]\n",
      "loss: 0.667776  [148800/175341]\n",
      "loss: 0.697061  [150400/175341]\n",
      "loss: 0.315078  [152000/175341]\n",
      "loss: 0.421250  [153600/175341]\n",
      "loss: 0.569640  [155200/175341]\n",
      "loss: 0.382658  [156800/175341]\n",
      "loss: 0.381658  [158400/175341]\n",
      "loss: 0.432932  [160000/175341]\n",
      "loss: 0.632110  [161600/175341]\n",
      "loss: 0.545857  [163200/175341]\n",
      "loss: 0.357567  [164800/175341]\n",
      "loss: 0.743344  [166400/175341]\n",
      "loss: 0.748374  [168000/175341]\n",
      "loss: 0.816032  [169600/175341]\n",
      "loss: 0.530133  [171200/175341]\n",
      "loss: 0.458047  [172800/175341]\n",
      "loss: 0.984183  [174400/175341]\n",
      "Train Accuracy: 78.3981%\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.664575, F1-score: 71.12% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.642930  [    0/175341]\n",
      "loss: 0.527042  [ 1600/175341]\n",
      "loss: 0.407783  [ 3200/175341]\n",
      "loss: 0.458344  [ 4800/175341]\n",
      "loss: 0.412282  [ 6400/175341]\n",
      "loss: 0.496271  [ 8000/175341]\n",
      "loss: 0.204241  [ 9600/175341]\n",
      "loss: 0.857620  [11200/175341]\n",
      "loss: 0.754517  [12800/175341]\n",
      "loss: 0.374773  [14400/175341]\n",
      "loss: 0.345663  [16000/175341]\n",
      "loss: 0.888569  [17600/175341]\n",
      "loss: 0.479132  [19200/175341]\n",
      "loss: 0.397993  [20800/175341]\n",
      "loss: 0.280562  [22400/175341]\n",
      "loss: 0.396789  [24000/175341]\n",
      "loss: 0.461834  [25600/175341]\n",
      "loss: 0.183662  [27200/175341]\n",
      "loss: 0.837358  [28800/175341]\n",
      "loss: 0.280672  [30400/175341]\n",
      "loss: 0.428137  [32000/175341]\n",
      "loss: 0.571336  [33600/175341]\n",
      "loss: 0.508611  [35200/175341]\n",
      "loss: 0.419260  [36800/175341]\n",
      "loss: 0.608574  [38400/175341]\n",
      "loss: 0.266192  [40000/175341]\n",
      "loss: 0.156634  [41600/175341]\n",
      "loss: 0.249575  [43200/175341]\n",
      "loss: 0.213535  [44800/175341]\n",
      "loss: 0.283817  [46400/175341]\n",
      "loss: 0.544735  [48000/175341]\n",
      "loss: 0.169108  [49600/175341]\n",
      "loss: 0.272541  [51200/175341]\n",
      "loss: 0.576990  [52800/175341]\n",
      "loss: 0.369078  [54400/175341]\n",
      "loss: 0.612836  [56000/175341]\n",
      "loss: 0.454992  [57600/175341]\n",
      "loss: 1.058580  [59200/175341]\n",
      "loss: 0.563723  [60800/175341]\n",
      "loss: 0.237634  [62400/175341]\n",
      "loss: 0.596517  [64000/175341]\n",
      "loss: 0.370145  [65600/175341]\n",
      "loss: 0.547179  [67200/175341]\n",
      "loss: 0.591153  [68800/175341]\n",
      "loss: 0.669276  [70400/175341]\n",
      "loss: 0.309238  [72000/175341]\n",
      "loss: 1.097840  [73600/175341]\n",
      "loss: 0.444844  [75200/175341]\n",
      "loss: 0.579549  [76800/175341]\n",
      "loss: 0.660373  [78400/175341]\n",
      "loss: 0.373252  [80000/175341]\n",
      "loss: 0.938332  [81600/175341]\n",
      "loss: 0.227063  [83200/175341]\n",
      "loss: 0.339404  [84800/175341]\n",
      "loss: 0.465377  [86400/175341]\n",
      "loss: 0.783495  [88000/175341]\n",
      "loss: 0.599789  [89600/175341]\n",
      "loss: 0.477799  [91200/175341]\n",
      "loss: 0.583291  [92800/175341]\n",
      "loss: 0.197723  [94400/175341]\n",
      "loss: 0.507169  [96000/175341]\n",
      "loss: 0.509456  [97600/175341]\n",
      "loss: 0.615762  [99200/175341]\n",
      "loss: 0.675518  [100800/175341]\n",
      "loss: 0.604716  [102400/175341]\n",
      "loss: 0.614345  [104000/175341]\n",
      "loss: 0.715995  [105600/175341]\n",
      "loss: 1.056243  [107200/175341]\n",
      "loss: 0.366080  [108800/175341]\n",
      "loss: 0.396581  [110400/175341]\n",
      "loss: 1.078987  [112000/175341]\n",
      "loss: 0.569218  [113600/175341]\n",
      "loss: 0.572023  [115200/175341]\n",
      "loss: 0.551151  [116800/175341]\n",
      "loss: 0.588501  [118400/175341]\n",
      "loss: 0.529566  [120000/175341]\n",
      "loss: 0.613807  [121600/175341]\n",
      "loss: 0.981189  [123200/175341]\n",
      "loss: 0.641832  [124800/175341]\n",
      "loss: 0.516397  [126400/175341]\n",
      "loss: 0.614707  [128000/175341]\n",
      "loss: 0.842142  [129600/175341]\n",
      "loss: 0.707739  [131200/175341]\n",
      "loss: 0.456745  [132800/175341]\n",
      "loss: 0.931843  [134400/175341]\n",
      "loss: 0.483187  [136000/175341]\n",
      "loss: 0.558510  [137600/175341]\n",
      "loss: 0.391008  [139200/175341]\n",
      "loss: 0.250231  [140800/175341]\n",
      "loss: 0.599951  [142400/175341]\n",
      "loss: 0.181806  [144000/175341]\n",
      "loss: 0.328017  [145600/175341]\n",
      "loss: 0.664584  [147200/175341]\n",
      "loss: 0.358300  [148800/175341]\n",
      "loss: 0.765665  [150400/175341]\n",
      "loss: 0.513188  [152000/175341]\n",
      "loss: 0.845521  [153600/175341]\n",
      "loss: 0.296113  [155200/175341]\n",
      "loss: 0.295242  [156800/175341]\n",
      "loss: 0.863430  [158400/175341]\n",
      "loss: 0.525779  [160000/175341]\n",
      "loss: 0.481184  [161600/175341]\n",
      "loss: 0.533954  [163200/175341]\n",
      "loss: 0.526615  [164800/175341]\n",
      "loss: 0.330662  [166400/175341]\n",
      "loss: 0.596071  [168000/175341]\n",
      "loss: 0.725236  [169600/175341]\n",
      "loss: 0.482206  [171200/175341]\n",
      "loss: 0.536598  [172800/175341]\n",
      "loss: 0.847571  [174400/175341]\n",
      "Train Accuracy: 78.4762%\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.623137, F1-score: 72.54% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.375330  [    0/175341]\n",
      "loss: 0.241335  [ 1600/175341]\n",
      "loss: 0.487295  [ 3200/175341]\n",
      "loss: 0.376672  [ 4800/175341]\n",
      "loss: 0.543812  [ 6400/175341]\n",
      "loss: 0.883872  [ 8000/175341]\n",
      "loss: 0.591578  [ 9600/175341]\n",
      "loss: 0.347653  [11200/175341]\n",
      "loss: 0.554146  [12800/175341]\n",
      "loss: 0.494705  [14400/175341]\n",
      "loss: 0.820606  [16000/175341]\n",
      "loss: 0.793730  [17600/175341]\n",
      "loss: 0.728495  [19200/175341]\n",
      "loss: 0.404068  [20800/175341]\n",
      "loss: 0.597244  [22400/175341]\n",
      "loss: 0.668128  [24000/175341]\n",
      "loss: 0.577688  [25600/175341]\n",
      "loss: 0.231143  [27200/175341]\n",
      "loss: 0.438749  [28800/175341]\n",
      "loss: 0.861727  [30400/175341]\n",
      "loss: 0.873602  [32000/175341]\n",
      "loss: 0.352892  [33600/175341]\n",
      "loss: 0.339400  [35200/175341]\n",
      "loss: 0.476815  [36800/175341]\n",
      "loss: 0.480245  [38400/175341]\n",
      "loss: 1.037747  [40000/175341]\n",
      "loss: 0.134401  [41600/175341]\n",
      "loss: 0.350893  [43200/175341]\n",
      "loss: 0.464982  [44800/175341]\n",
      "loss: 0.490037  [46400/175341]\n",
      "loss: 0.174877  [48000/175341]\n",
      "loss: 0.478487  [49600/175341]\n",
      "loss: 0.599021  [51200/175341]\n",
      "loss: 0.546564  [52800/175341]\n",
      "loss: 0.242782  [54400/175341]\n",
      "loss: 0.817910  [56000/175341]\n",
      "loss: 0.548982  [57600/175341]\n",
      "loss: 0.489614  [59200/175341]\n",
      "loss: 0.504709  [60800/175341]\n",
      "loss: 0.407431  [62400/175341]\n",
      "loss: 0.406510  [64000/175341]\n",
      "loss: 0.496557  [65600/175341]\n",
      "loss: 0.412453  [67200/175341]\n",
      "loss: 0.522928  [68800/175341]\n",
      "loss: 0.631647  [70400/175341]\n",
      "loss: 0.922864  [72000/175341]\n",
      "loss: 0.389248  [73600/175341]\n",
      "loss: 0.767458  [75200/175341]\n",
      "loss: 0.519475  [76800/175341]\n",
      "loss: 0.553628  [78400/175341]\n",
      "loss: 0.339224  [80000/175341]\n",
      "loss: 0.629471  [81600/175341]\n",
      "loss: 0.293204  [83200/175341]\n",
      "loss: 0.409860  [84800/175341]\n",
      "loss: 0.744364  [86400/175341]\n",
      "loss: 0.256429  [88000/175341]\n",
      "loss: 0.459723  [89600/175341]\n",
      "loss: 0.503064  [91200/175341]\n",
      "loss: 0.352082  [92800/175341]\n",
      "loss: 0.476618  [94400/175341]\n",
      "loss: 0.880795  [96000/175341]\n",
      "loss: 0.632977  [97600/175341]\n",
      "loss: 0.407474  [99200/175341]\n",
      "loss: 0.456419  [100800/175341]\n",
      "loss: 0.742788  [102400/175341]\n",
      "loss: 0.642067  [104000/175341]\n",
      "loss: 0.555280  [105600/175341]\n",
      "loss: 0.400250  [107200/175341]\n",
      "loss: 0.619449  [108800/175341]\n",
      "loss: 0.567619  [110400/175341]\n",
      "loss: 0.043342  [112000/175341]\n",
      "loss: 0.409809  [113600/175341]\n",
      "loss: 1.004112  [115200/175341]\n",
      "loss: 0.772097  [116800/175341]\n",
      "loss: 0.654136  [118400/175341]\n",
      "loss: 0.538971  [120000/175341]\n",
      "loss: 0.505860  [121600/175341]\n",
      "loss: 0.364600  [123200/175341]\n",
      "loss: 0.248522  [124800/175341]\n",
      "loss: 0.608929  [126400/175341]\n",
      "loss: 0.768414  [128000/175341]\n",
      "loss: 0.731568  [129600/175341]\n",
      "loss: 0.402171  [131200/175341]\n",
      "loss: 0.669507  [132800/175341]\n",
      "loss: 0.379724  [134400/175341]\n",
      "loss: 0.420614  [136000/175341]\n",
      "loss: 0.235013  [137600/175341]\n",
      "loss: 0.594296  [139200/175341]\n",
      "loss: 1.064428  [140800/175341]\n",
      "loss: 1.092534  [142400/175341]\n",
      "loss: 1.127456  [144000/175341]\n",
      "loss: 0.263559  [145600/175341]\n",
      "loss: 0.103900  [147200/175341]\n",
      "loss: 0.320480  [148800/175341]\n",
      "loss: 0.488035  [150400/175341]\n",
      "loss: 0.391318  [152000/175341]\n",
      "loss: 0.163680  [153600/175341]\n",
      "loss: 0.381609  [155200/175341]\n",
      "loss: 0.477036  [156800/175341]\n",
      "loss: 0.805494  [158400/175341]\n",
      "loss: 0.668828  [160000/175341]\n",
      "loss: 0.176242  [161600/175341]\n",
      "loss: 0.866766  [163200/175341]\n",
      "loss: 0.552278  [164800/175341]\n",
      "loss: 0.077319  [166400/175341]\n",
      "loss: 0.672619  [168000/175341]\n",
      "loss: 0.784839  [169600/175341]\n",
      "loss: 0.920691  [171200/175341]\n",
      "loss: 0.787666  [172800/175341]\n",
      "loss: 0.174604  [174400/175341]\n",
      "Train Accuracy: 78.5310%\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.629355, F1-score: 72.75% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.529465  [    0/175341]\n",
      "loss: 0.659302  [ 1600/175341]\n",
      "loss: 0.558705  [ 3200/175341]\n",
      "loss: 0.306896  [ 4800/175341]\n",
      "loss: 0.348431  [ 6400/175341]\n",
      "loss: 0.340916  [ 8000/175341]\n",
      "loss: 0.652619  [ 9600/175341]\n",
      "loss: 1.003993  [11200/175341]\n",
      "loss: 0.869613  [12800/175341]\n",
      "loss: 0.407926  [14400/175341]\n",
      "loss: 0.853846  [16000/175341]\n",
      "loss: 0.289295  [17600/175341]\n",
      "loss: 0.385525  [19200/175341]\n",
      "loss: 0.183509  [20800/175341]\n",
      "loss: 0.420249  [22400/175341]\n",
      "loss: 0.361772  [24000/175341]\n",
      "loss: 0.736482  [25600/175341]\n",
      "loss: 1.190428  [27200/175341]\n",
      "loss: 0.749790  [28800/175341]\n",
      "loss: 0.193095  [30400/175341]\n",
      "loss: 0.665026  [32000/175341]\n",
      "loss: 0.698483  [33600/175341]\n",
      "loss: 0.494512  [35200/175341]\n",
      "loss: 0.651670  [36800/175341]\n",
      "loss: 0.274664  [38400/175341]\n",
      "loss: 0.304981  [40000/175341]\n",
      "loss: 0.451168  [41600/175341]\n",
      "loss: 0.669710  [43200/175341]\n",
      "loss: 0.789156  [44800/175341]\n",
      "loss: 0.768962  [46400/175341]\n",
      "loss: 0.482148  [48000/175341]\n",
      "loss: 0.531012  [49600/175341]\n",
      "loss: 0.668916  [51200/175341]\n",
      "loss: 0.698240  [52800/175341]\n",
      "loss: 0.594889  [54400/175341]\n",
      "loss: 0.711131  [56000/175341]\n",
      "loss: 0.547294  [57600/175341]\n",
      "loss: 0.475277  [59200/175341]\n",
      "loss: 0.970481  [60800/175341]\n",
      "loss: 0.479076  [62400/175341]\n",
      "loss: 0.862065  [64000/175341]\n",
      "loss: 0.459064  [65600/175341]\n",
      "loss: 0.650202  [67200/175341]\n",
      "loss: 0.301431  [68800/175341]\n",
      "loss: 0.410417  [70400/175341]\n",
      "loss: 0.691122  [72000/175341]\n",
      "loss: 0.451574  [73600/175341]\n",
      "loss: 0.397153  [75200/175341]\n",
      "loss: 0.499173  [76800/175341]\n",
      "loss: 0.753529  [78400/175341]\n",
      "loss: 0.396995  [80000/175341]\n",
      "loss: 0.693969  [81600/175341]\n",
      "loss: 0.411105  [83200/175341]\n",
      "loss: 0.332072  [84800/175341]\n",
      "loss: 0.622927  [86400/175341]\n",
      "loss: 0.744594  [88000/175341]\n",
      "loss: 0.287259  [89600/175341]\n",
      "loss: 0.331782  [91200/175341]\n",
      "loss: 0.289193  [92800/175341]\n",
      "loss: 0.472336  [94400/175341]\n",
      "loss: 0.496142  [96000/175341]\n",
      "loss: 0.592727  [97600/175341]\n",
      "loss: 0.204125  [99200/175341]\n",
      "loss: 0.530655  [100800/175341]\n",
      "loss: 0.576122  [102400/175341]\n",
      "loss: 0.288209  [104000/175341]\n",
      "loss: 0.543002  [105600/175341]\n",
      "loss: 0.329804  [107200/175341]\n",
      "loss: 0.521330  [108800/175341]\n",
      "loss: 0.478458  [110400/175341]\n",
      "loss: 0.907562  [112000/175341]\n",
      "loss: 0.438624  [113600/175341]\n",
      "loss: 0.410107  [115200/175341]\n",
      "loss: 0.768876  [116800/175341]\n",
      "loss: 0.221332  [118400/175341]\n",
      "loss: 0.849217  [120000/175341]\n",
      "loss: 0.543085  [121600/175341]\n",
      "loss: 0.339453  [123200/175341]\n",
      "loss: 0.237412  [124800/175341]\n",
      "loss: 0.312602  [126400/175341]\n",
      "loss: 0.236671  [128000/175341]\n",
      "loss: 0.522722  [129600/175341]\n",
      "loss: 0.575738  [131200/175341]\n",
      "loss: 0.862101  [132800/175341]\n",
      "loss: 0.456394  [134400/175341]\n",
      "loss: 0.472711  [136000/175341]\n",
      "loss: 0.732422  [137600/175341]\n",
      "loss: 0.225287  [139200/175341]\n",
      "loss: 0.490391  [140800/175341]\n",
      "loss: 0.505077  [142400/175341]\n",
      "loss: 0.457037  [144000/175341]\n",
      "loss: 0.242701  [145600/175341]\n",
      "loss: 0.669283  [147200/175341]\n",
      "loss: 0.426345  [148800/175341]\n",
      "loss: 0.494374  [150400/175341]\n",
      "loss: 0.468233  [152000/175341]\n",
      "loss: 0.371806  [153600/175341]\n",
      "loss: 0.308460  [155200/175341]\n",
      "loss: 0.609937  [156800/175341]\n",
      "loss: 0.850079  [158400/175341]\n",
      "loss: 0.429300  [160000/175341]\n",
      "loss: 0.396184  [161600/175341]\n",
      "loss: 0.549000  [163200/175341]\n",
      "loss: 0.735546  [164800/175341]\n",
      "loss: 0.583315  [166400/175341]\n",
      "loss: 0.163729  [168000/175341]\n",
      "loss: 0.280787  [169600/175341]\n",
      "loss: 0.471509  [171200/175341]\n",
      "loss: 1.024118  [172800/175341]\n",
      "loss: 0.741652  [174400/175341]\n",
      "Train Accuracy: 78.7049%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.619967, F1-score: 73.71% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.444773  [    0/175341]\n",
      "loss: 0.392722  [ 1600/175341]\n",
      "loss: 0.612889  [ 3200/175341]\n",
      "loss: 0.659996  [ 4800/175341]\n",
      "loss: 0.253165  [ 6400/175341]\n",
      "loss: 0.270140  [ 8000/175341]\n",
      "loss: 0.045123  [ 9600/175341]\n",
      "loss: 0.406201  [11200/175341]\n",
      "loss: 0.197775  [12800/175341]\n",
      "loss: 0.876034  [14400/175341]\n",
      "loss: 0.167177  [16000/175341]\n",
      "loss: 0.742021  [17600/175341]\n",
      "loss: 0.685863  [19200/175341]\n",
      "loss: 0.557759  [20800/175341]\n",
      "loss: 0.674231  [22400/175341]\n",
      "loss: 0.359598  [24000/175341]\n",
      "loss: 0.697437  [25600/175341]\n",
      "loss: 0.496823  [27200/175341]\n",
      "loss: 0.287820  [28800/175341]\n",
      "loss: 0.850660  [30400/175341]\n",
      "loss: 0.491647  [32000/175341]\n",
      "loss: 0.538972  [33600/175341]\n",
      "loss: 0.322872  [35200/175341]\n",
      "loss: 0.816338  [36800/175341]\n",
      "loss: 0.696546  [38400/175341]\n",
      "loss: 0.384926  [40000/175341]\n",
      "loss: 0.602428  [41600/175341]\n",
      "loss: 0.666106  [43200/175341]\n",
      "loss: 0.235735  [44800/175341]\n",
      "loss: 0.384721  [46400/175341]\n",
      "loss: 0.668335  [48000/175341]\n",
      "loss: 0.538326  [49600/175341]\n",
      "loss: 0.469484  [51200/175341]\n",
      "loss: 0.202164  [52800/175341]\n",
      "loss: 0.507201  [54400/175341]\n",
      "loss: 0.363951  [56000/175341]\n",
      "loss: 0.662872  [57600/175341]\n",
      "loss: 0.738232  [59200/175341]\n",
      "loss: 1.209393  [60800/175341]\n",
      "loss: 0.673710  [62400/175341]\n",
      "loss: 0.653010  [64000/175341]\n",
      "loss: 0.229079  [65600/175341]\n",
      "loss: 0.700408  [67200/175341]\n",
      "loss: 0.379485  [68800/175341]\n",
      "loss: 0.906747  [70400/175341]\n",
      "loss: 0.241241  [72000/175341]\n",
      "loss: 0.277500  [73600/175341]\n",
      "loss: 0.215425  [75200/175341]\n",
      "loss: 0.834679  [76800/175341]\n",
      "loss: 0.428346  [78400/175341]\n",
      "loss: 0.378075  [80000/175341]\n",
      "loss: 0.392331  [81600/175341]\n",
      "loss: 0.506858  [83200/175341]\n",
      "loss: 0.280152  [84800/175341]\n",
      "loss: 0.675662  [86400/175341]\n",
      "loss: 0.452674  [88000/175341]\n",
      "loss: 0.601434  [89600/175341]\n",
      "loss: 0.752022  [91200/175341]\n",
      "loss: 0.421542  [92800/175341]\n",
      "loss: 0.237436  [94400/175341]\n",
      "loss: 0.660007  [96000/175341]\n",
      "loss: 0.887065  [97600/175341]\n",
      "loss: 0.682109  [99200/175341]\n",
      "loss: 0.569787  [100800/175341]\n",
      "loss: 0.471029  [102400/175341]\n",
      "loss: 0.762269  [104000/175341]\n",
      "loss: 0.658838  [105600/175341]\n",
      "loss: 0.602206  [107200/175341]\n",
      "loss: 0.749469  [108800/175341]\n",
      "loss: 0.474885  [110400/175341]\n",
      "loss: 0.696398  [112000/175341]\n",
      "loss: 0.874499  [113600/175341]\n",
      "loss: 0.535462  [115200/175341]\n",
      "loss: 0.242572  [116800/175341]\n",
      "loss: 0.890000  [118400/175341]\n",
      "loss: 0.249983  [120000/175341]\n",
      "loss: 0.478930  [121600/175341]\n",
      "loss: 0.758637  [123200/175341]\n",
      "loss: 0.283792  [124800/175341]\n",
      "loss: 0.751357  [126400/175341]\n",
      "loss: 0.496383  [128000/175341]\n",
      "loss: 0.509203  [129600/175341]\n",
      "loss: 0.873979  [131200/175341]\n",
      "loss: 0.626162  [132800/175341]\n",
      "loss: 0.547556  [134400/175341]\n",
      "loss: 0.199443  [136000/175341]\n",
      "loss: 0.741085  [137600/175341]\n",
      "loss: 0.526196  [139200/175341]\n",
      "loss: 0.980868  [140800/175341]\n",
      "loss: 0.504028  [142400/175341]\n",
      "loss: 0.450655  [144000/175341]\n",
      "loss: 0.894976  [145600/175341]\n",
      "loss: 0.509308  [147200/175341]\n",
      "loss: 0.718108  [148800/175341]\n",
      "loss: 0.643993  [150400/175341]\n",
      "loss: 0.551294  [152000/175341]\n",
      "loss: 0.563981  [153600/175341]\n",
      "loss: 0.394617  [155200/175341]\n",
      "loss: 0.750764  [156800/175341]\n",
      "loss: 0.311587  [158400/175341]\n",
      "loss: 0.736908  [160000/175341]\n",
      "loss: 0.961734  [161600/175341]\n",
      "loss: 0.484054  [163200/175341]\n",
      "loss: 0.438372  [164800/175341]\n",
      "loss: 0.659148  [166400/175341]\n",
      "loss: 0.920335  [168000/175341]\n",
      "loss: 0.457626  [169600/175341]\n",
      "loss: 0.249826  [171200/175341]\n",
      "loss: 0.771624  [172800/175341]\n",
      "loss: 0.285098  [174400/175341]\n",
      "Train Accuracy: 78.8070%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.607689, F1-score: 74.36% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.576259  [    0/175341]\n",
      "loss: 0.653448  [ 1600/175341]\n",
      "loss: 0.228103  [ 3200/175341]\n",
      "loss: 0.825026  [ 4800/175341]\n",
      "loss: 0.489600  [ 6400/175341]\n",
      "loss: 0.285862  [ 8000/175341]\n",
      "loss: 0.270586  [ 9600/175341]\n",
      "loss: 0.426741  [11200/175341]\n",
      "loss: 0.683899  [12800/175341]\n",
      "loss: 0.766517  [14400/175341]\n",
      "loss: 0.418795  [16000/175341]\n",
      "loss: 0.657320  [17600/175341]\n",
      "loss: 0.166061  [19200/175341]\n",
      "loss: 0.674876  [20800/175341]\n",
      "loss: 0.749714  [22400/175341]\n",
      "loss: 0.237085  [24000/175341]\n",
      "loss: 0.359109  [25600/175341]\n",
      "loss: 0.437645  [27200/175341]\n",
      "loss: 0.573372  [28800/175341]\n",
      "loss: 0.474269  [30400/175341]\n",
      "loss: 0.665413  [32000/175341]\n",
      "loss: 0.374294  [33600/175341]\n",
      "loss: 0.382938  [35200/175341]\n",
      "loss: 0.642584  [36800/175341]\n",
      "loss: 0.324396  [38400/175341]\n",
      "loss: 0.396909  [40000/175341]\n",
      "loss: 1.274856  [41600/175341]\n",
      "loss: 0.325402  [43200/175341]\n",
      "loss: 0.698330  [44800/175341]\n",
      "loss: 0.604459  [46400/175341]\n",
      "loss: 0.822404  [48000/175341]\n",
      "loss: 0.447203  [49600/175341]\n",
      "loss: 0.582191  [51200/175341]\n",
      "loss: 0.655167  [52800/175341]\n",
      "loss: 0.343029  [54400/175341]\n",
      "loss: 0.409924  [56000/175341]\n",
      "loss: 0.708366  [57600/175341]\n",
      "loss: 0.406339  [59200/175341]\n",
      "loss: 0.983350  [60800/175341]\n",
      "loss: 0.629102  [62400/175341]\n",
      "loss: 0.647005  [64000/175341]\n",
      "loss: 0.602468  [65600/175341]\n",
      "loss: 0.691623  [67200/175341]\n",
      "loss: 0.731160  [68800/175341]\n",
      "loss: 0.399909  [70400/175341]\n",
      "loss: 0.746388  [72000/175341]\n",
      "loss: 0.183014  [73600/175341]\n",
      "loss: 0.459524  [75200/175341]\n",
      "loss: 0.592663  [76800/175341]\n",
      "loss: 0.432364  [78400/175341]\n",
      "loss: 0.305055  [80000/175341]\n",
      "loss: 0.209055  [81600/175341]\n",
      "loss: 0.112430  [83200/175341]\n",
      "loss: 0.485923  [84800/175341]\n",
      "loss: 0.795583  [86400/175341]\n",
      "loss: 0.318987  [88000/175341]\n",
      "loss: 0.355804  [89600/175341]\n",
      "loss: 0.308356  [91200/175341]\n",
      "loss: 0.553517  [92800/175341]\n",
      "loss: 0.353546  [94400/175341]\n",
      "loss: 1.062623  [96000/175341]\n",
      "loss: 0.868729  [97600/175341]\n",
      "loss: 0.287799  [99200/175341]\n",
      "loss: 0.772153  [100800/175341]\n",
      "loss: 0.630991  [102400/175341]\n",
      "loss: 0.284913  [104000/175341]\n",
      "loss: 0.290156  [105600/175341]\n",
      "loss: 0.572116  [107200/175341]\n",
      "loss: 0.800720  [108800/175341]\n",
      "loss: 0.822280  [110400/175341]\n",
      "loss: 0.349595  [112000/175341]\n",
      "loss: 0.526763  [113600/175341]\n",
      "loss: 0.509404  [115200/175341]\n",
      "loss: 0.697114  [116800/175341]\n",
      "loss: 0.267541  [118400/175341]\n",
      "loss: 0.798316  [120000/175341]\n",
      "loss: 0.145114  [121600/175341]\n",
      "loss: 0.204278  [123200/175341]\n",
      "loss: 1.085840  [124800/175341]\n",
      "loss: 0.209634  [126400/175341]\n",
      "loss: 0.669880  [128000/175341]\n",
      "loss: 0.473826  [129600/175341]\n",
      "loss: 0.705673  [131200/175341]\n",
      "loss: 0.549130  [132800/175341]\n",
      "loss: 0.664762  [134400/175341]\n",
      "loss: 0.459966  [136000/175341]\n",
      "loss: 0.716667  [137600/175341]\n",
      "loss: 0.516995  [139200/175341]\n",
      "loss: 0.410999  [140800/175341]\n",
      "loss: 0.547192  [142400/175341]\n",
      "loss: 0.639325  [144000/175341]\n",
      "loss: 0.297456  [145600/175341]\n",
      "loss: 0.442670  [147200/175341]\n",
      "loss: 0.613983  [148800/175341]\n",
      "loss: 0.569549  [150400/175341]\n",
      "loss: 0.337366  [152000/175341]\n",
      "loss: 0.420437  [153600/175341]\n",
      "loss: 0.630121  [155200/175341]\n",
      "loss: 0.824980  [156800/175341]\n",
      "loss: 0.477056  [158400/175341]\n",
      "loss: 0.363490  [160000/175341]\n",
      "loss: 0.430009  [161600/175341]\n",
      "loss: 0.377811  [163200/175341]\n",
      "loss: 0.964727  [164800/175341]\n",
      "loss: 0.421134  [166400/175341]\n",
      "loss: 0.555637  [168000/175341]\n",
      "loss: 0.943058  [169600/175341]\n",
      "loss: 0.255294  [171200/175341]\n",
      "loss: 0.785663  [172800/175341]\n",
      "loss: 0.536801  [174400/175341]\n",
      "Train Accuracy: 78.9616%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.614401, F1-score: 73.86% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.604006  [    0/175341]\n",
      "loss: 0.516453  [ 1600/175341]\n",
      "loss: 0.557756  [ 3200/175341]\n",
      "loss: 0.861623  [ 4800/175341]\n",
      "loss: 0.630692  [ 6400/175341]\n",
      "loss: 0.609211  [ 8000/175341]\n",
      "loss: 0.794058  [ 9600/175341]\n",
      "loss: 0.340889  [11200/175341]\n",
      "loss: 0.326061  [12800/175341]\n",
      "loss: 0.717844  [14400/175341]\n",
      "loss: 0.757242  [16000/175341]\n",
      "loss: 0.915075  [17600/175341]\n",
      "loss: 0.619475  [19200/175341]\n",
      "loss: 0.363317  [20800/175341]\n",
      "loss: 0.343825  [22400/175341]\n",
      "loss: 0.521456  [24000/175341]\n",
      "loss: 0.301255  [25600/175341]\n",
      "loss: 1.121897  [27200/175341]\n",
      "loss: 0.625709  [28800/175341]\n",
      "loss: 0.416661  [30400/175341]\n",
      "loss: 0.472409  [32000/175341]\n",
      "loss: 0.811062  [33600/175341]\n",
      "loss: 0.604725  [35200/175341]\n",
      "loss: 0.825069  [36800/175341]\n",
      "loss: 0.446985  [38400/175341]\n",
      "loss: 0.856252  [40000/175341]\n",
      "loss: 0.476374  [41600/175341]\n",
      "loss: 0.539848  [43200/175341]\n",
      "loss: 0.835033  [44800/175341]\n",
      "loss: 0.416324  [46400/175341]\n",
      "loss: 0.495168  [48000/175341]\n",
      "loss: 0.785425  [49600/175341]\n",
      "loss: 0.715957  [51200/175341]\n",
      "loss: 0.482651  [52800/175341]\n",
      "loss: 0.455400  [54400/175341]\n",
      "loss: 0.386182  [56000/175341]\n",
      "loss: 0.851009  [57600/175341]\n",
      "loss: 0.332831  [59200/175341]\n",
      "loss: 0.505466  [60800/175341]\n",
      "loss: 0.896833  [62400/175341]\n",
      "loss: 0.625662  [64000/175341]\n",
      "loss: 0.468453  [65600/175341]\n",
      "loss: 0.439366  [67200/175341]\n",
      "loss: 0.406812  [68800/175341]\n",
      "loss: 0.318837  [70400/175341]\n",
      "loss: 0.675442  [72000/175341]\n",
      "loss: 0.424202  [73600/175341]\n",
      "loss: 0.763861  [75200/175341]\n",
      "loss: 0.121790  [76800/175341]\n",
      "loss: 0.319842  [78400/175341]\n",
      "loss: 0.469390  [80000/175341]\n",
      "loss: 0.435742  [81600/175341]\n",
      "loss: 0.425423  [83200/175341]\n",
      "loss: 0.482825  [84800/175341]\n",
      "loss: 0.528178  [86400/175341]\n",
      "loss: 0.476151  [88000/175341]\n",
      "loss: 0.587671  [89600/175341]\n",
      "loss: 0.506459  [91200/175341]\n",
      "loss: 0.823893  [92800/175341]\n",
      "loss: 0.801725  [94400/175341]\n",
      "loss: 0.761796  [96000/175341]\n",
      "loss: 0.570520  [97600/175341]\n",
      "loss: 0.233131  [99200/175341]\n",
      "loss: 0.541485  [100800/175341]\n",
      "loss: 1.051505  [102400/175341]\n",
      "loss: 0.567384  [104000/175341]\n",
      "loss: 0.553076  [105600/175341]\n",
      "loss: 0.633865  [107200/175341]\n",
      "loss: 0.862769  [108800/175341]\n",
      "loss: 0.571032  [110400/175341]\n",
      "loss: 0.245461  [112000/175341]\n",
      "loss: 0.248234  [113600/175341]\n",
      "loss: 0.280727  [115200/175341]\n",
      "loss: 0.177413  [116800/175341]\n",
      "loss: 0.760122  [118400/175341]\n",
      "loss: 0.243214  [120000/175341]\n",
      "loss: 1.079273  [121600/175341]\n",
      "loss: 0.340064  [123200/175341]\n",
      "loss: 0.234490  [124800/175341]\n",
      "loss: 1.157660  [126400/175341]\n",
      "loss: 0.648328  [128000/175341]\n",
      "loss: 0.693695  [129600/175341]\n",
      "loss: 0.452359  [131200/175341]\n",
      "loss: 0.334958  [132800/175341]\n",
      "loss: 0.647731  [134400/175341]\n",
      "loss: 0.360005  [136000/175341]\n",
      "loss: 0.216337  [137600/175341]\n",
      "loss: 0.632911  [139200/175341]\n",
      "loss: 0.539308  [140800/175341]\n",
      "loss: 0.445950  [142400/175341]\n",
      "loss: 0.492494  [144000/175341]\n",
      "loss: 0.715912  [145600/175341]\n",
      "loss: 0.613683  [147200/175341]\n",
      "loss: 0.450007  [148800/175341]\n",
      "loss: 0.759995  [150400/175341]\n",
      "loss: 0.686017  [152000/175341]\n",
      "loss: 0.464176  [153600/175341]\n",
      "loss: 0.351249  [155200/175341]\n",
      "loss: 0.910162  [156800/175341]\n",
      "loss: 0.397000  [158400/175341]\n",
      "loss: 0.415700  [160000/175341]\n",
      "loss: 0.505850  [161600/175341]\n",
      "loss: 0.846441  [163200/175341]\n",
      "loss: 0.373447  [164800/175341]\n",
      "loss: 0.429437  [166400/175341]\n",
      "loss: 0.229625  [168000/175341]\n",
      "loss: 0.578530  [169600/175341]\n",
      "loss: 0.341981  [171200/175341]\n",
      "loss: 1.156596  [172800/175341]\n",
      "loss: 1.021761  [174400/175341]\n",
      "Train Accuracy: 79.0192%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.608526, F1-score: 73.97% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.697744  [    0/175341]\n",
      "loss: 0.295811  [ 1600/175341]\n",
      "loss: 0.365843  [ 3200/175341]\n",
      "loss: 0.327314  [ 4800/175341]\n",
      "loss: 0.484525  [ 6400/175341]\n",
      "loss: 0.495260  [ 8000/175341]\n",
      "loss: 0.837003  [ 9600/175341]\n",
      "loss: 0.449329  [11200/175341]\n",
      "loss: 0.371968  [12800/175341]\n",
      "loss: 0.516700  [14400/175341]\n",
      "loss: 0.758237  [16000/175341]\n",
      "loss: 0.660168  [17600/175341]\n",
      "loss: 0.283810  [19200/175341]\n",
      "loss: 0.753493  [20800/175341]\n",
      "loss: 0.302632  [22400/175341]\n",
      "loss: 0.695043  [24000/175341]\n",
      "loss: 0.335383  [25600/175341]\n",
      "loss: 0.289241  [27200/175341]\n",
      "loss: 0.178400  [28800/175341]\n",
      "loss: 0.767912  [30400/175341]\n",
      "loss: 0.439435  [32000/175341]\n",
      "loss: 0.725332  [33600/175341]\n",
      "loss: 0.585618  [35200/175341]\n",
      "loss: 0.687075  [36800/175341]\n",
      "loss: 0.280085  [38400/175341]\n",
      "loss: 0.732926  [40000/175341]\n",
      "loss: 0.191353  [41600/175341]\n",
      "loss: 0.438173  [43200/175341]\n",
      "loss: 1.036668  [44800/175341]\n",
      "loss: 0.470738  [46400/175341]\n",
      "loss: 0.345747  [48000/175341]\n",
      "loss: 0.224154  [49600/175341]\n",
      "loss: 0.387016  [51200/175341]\n",
      "loss: 0.716644  [52800/175341]\n",
      "loss: 0.999524  [54400/175341]\n",
      "loss: 0.318652  [56000/175341]\n",
      "loss: 0.474474  [57600/175341]\n",
      "loss: 0.775374  [59200/175341]\n",
      "loss: 0.675795  [60800/175341]\n",
      "loss: 0.497237  [62400/175341]\n",
      "loss: 0.449410  [64000/175341]\n",
      "loss: 0.716466  [65600/175341]\n",
      "loss: 0.369140  [67200/175341]\n",
      "loss: 0.516249  [68800/175341]\n",
      "loss: 0.446907  [70400/175341]\n",
      "loss: 1.186566  [72000/175341]\n",
      "loss: 0.381259  [73600/175341]\n",
      "loss: 0.455731  [75200/175341]\n",
      "loss: 0.532165  [76800/175341]\n",
      "loss: 0.252979  [78400/175341]\n",
      "loss: 0.234003  [80000/175341]\n",
      "loss: 0.606881  [81600/175341]\n",
      "loss: 0.642975  [83200/175341]\n",
      "loss: 0.676573  [84800/175341]\n",
      "loss: 0.408881  [86400/175341]\n",
      "loss: 0.583572  [88000/175341]\n",
      "loss: 0.688571  [89600/175341]\n",
      "loss: 0.470674  [91200/175341]\n",
      "loss: 0.591960  [92800/175341]\n",
      "loss: 0.719947  [94400/175341]\n",
      "loss: 0.689996  [96000/175341]\n",
      "loss: 0.642709  [97600/175341]\n",
      "loss: 0.399267  [99200/175341]\n",
      "loss: 0.825637  [100800/175341]\n",
      "loss: 0.854448  [102400/175341]\n",
      "loss: 0.383381  [104000/175341]\n",
      "loss: 0.802992  [105600/175341]\n",
      "loss: 0.118067  [107200/175341]\n",
      "loss: 0.665040  [108800/175341]\n",
      "loss: 0.500341  [110400/175341]\n",
      "loss: 0.368841  [112000/175341]\n",
      "loss: 0.254619  [113600/175341]\n",
      "loss: 0.146086  [115200/175341]\n",
      "loss: 0.819740  [116800/175341]\n",
      "loss: 0.471269  [118400/175341]\n",
      "loss: 0.797789  [120000/175341]\n",
      "loss: 0.881632  [121600/175341]\n",
      "loss: 0.864458  [123200/175341]\n",
      "loss: 0.439527  [124800/175341]\n",
      "loss: 0.402863  [126400/175341]\n",
      "loss: 0.652461  [128000/175341]\n",
      "loss: 0.570597  [129600/175341]\n",
      "loss: 0.480219  [131200/175341]\n",
      "loss: 0.749505  [132800/175341]\n",
      "loss: 0.774654  [134400/175341]\n",
      "loss: 0.621264  [136000/175341]\n",
      "loss: 0.422753  [137600/175341]\n",
      "loss: 0.307549  [139200/175341]\n",
      "loss: 0.193635  [140800/175341]\n",
      "loss: 0.298163  [142400/175341]\n",
      "loss: 0.571976  [144000/175341]\n",
      "loss: 0.786833  [145600/175341]\n",
      "loss: 0.383368  [147200/175341]\n",
      "loss: 0.764380  [148800/175341]\n",
      "loss: 0.379044  [150400/175341]\n",
      "loss: 0.811836  [152000/175341]\n",
      "loss: 0.437071  [153600/175341]\n",
      "loss: 0.698894  [155200/175341]\n",
      "loss: 0.611952  [156800/175341]\n",
      "loss: 0.339618  [158400/175341]\n",
      "loss: 0.547861  [160000/175341]\n",
      "loss: 0.385338  [161600/175341]\n",
      "loss: 0.507879  [163200/175341]\n",
      "loss: 1.151585  [164800/175341]\n",
      "loss: 0.459916  [166400/175341]\n",
      "loss: 0.498117  [168000/175341]\n",
      "loss: 0.470661  [169600/175341]\n",
      "loss: 0.589918  [171200/175341]\n",
      "loss: 0.823550  [172800/175341]\n",
      "loss: 0.893313  [174400/175341]\n",
      "Train Accuracy: 79.0853%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.603292, F1-score: 74.32% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "431a6d93-0220-4085-bb3f-8965b97240b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 10,6, 3,10],grid_size = 4, scale_noise=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25e34344-c96e-42c9-b318-96c5e291cfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305355  [    0/175341]\n",
      "loss: 2.295147  [ 1600/175341]\n",
      "loss: 2.295293  [ 3200/175341]\n",
      "loss: 2.244799  [ 4800/175341]\n",
      "loss: 2.234281  [ 6400/175341]\n",
      "loss: 2.091908  [ 8000/175341]\n",
      "loss: 2.050698  [ 9600/175341]\n",
      "loss: 1.993330  [11200/175341]\n",
      "loss: 1.877544  [12800/175341]\n",
      "loss: 1.842996  [14400/175341]\n",
      "loss: 1.902210  [16000/175341]\n",
      "loss: 1.648370  [17600/175341]\n",
      "loss: 1.304614  [19200/175341]\n",
      "loss: 1.627516  [20800/175341]\n",
      "loss: 1.299440  [22400/175341]\n",
      "loss: 1.631766  [24000/175341]\n",
      "loss: 1.441162  [25600/175341]\n",
      "loss: 1.148322  [27200/175341]\n",
      "loss: 1.208263  [28800/175341]\n",
      "loss: 1.029948  [30400/175341]\n",
      "loss: 1.095951  [32000/175341]\n",
      "loss: 1.404565  [33600/175341]\n",
      "loss: 1.353548  [35200/175341]\n",
      "loss: 1.063082  [36800/175341]\n",
      "loss: 1.024352  [38400/175341]\n",
      "loss: 1.312292  [40000/175341]\n",
      "loss: 1.407327  [41600/175341]\n",
      "loss: 1.450266  [43200/175341]\n",
      "loss: 1.133116  [44800/175341]\n",
      "loss: 0.909590  [46400/175341]\n",
      "loss: 0.896582  [48000/175341]\n",
      "loss: 0.963833  [49600/175341]\n",
      "loss: 0.870078  [51200/175341]\n",
      "loss: 1.149349  [52800/175341]\n",
      "loss: 1.218885  [54400/175341]\n",
      "loss: 0.973309  [56000/175341]\n",
      "loss: 1.144078  [57600/175341]\n",
      "loss: 0.952325  [59200/175341]\n",
      "loss: 1.203187  [60800/175341]\n",
      "loss: 0.964388  [62400/175341]\n",
      "loss: 0.992573  [64000/175341]\n",
      "loss: 0.901069  [65600/175341]\n",
      "loss: 0.916472  [67200/175341]\n",
      "loss: 0.866159  [68800/175341]\n",
      "loss: 1.075860  [70400/175341]\n",
      "loss: 1.266222  [72000/175341]\n",
      "loss: 0.932123  [73600/175341]\n",
      "loss: 0.922604  [75200/175341]\n",
      "loss: 1.307122  [76800/175341]\n",
      "loss: 0.932803  [78400/175341]\n",
      "loss: 1.215658  [80000/175341]\n",
      "loss: 0.493176  [81600/175341]\n",
      "loss: 1.210622  [83200/175341]\n",
      "loss: 1.099006  [84800/175341]\n",
      "loss: 0.855170  [86400/175341]\n",
      "loss: 1.216489  [88000/175341]\n",
      "loss: 1.290713  [89600/175341]\n",
      "loss: 0.757212  [91200/175341]\n",
      "loss: 1.145688  [92800/175341]\n",
      "loss: 0.678313  [94400/175341]\n",
      "loss: 0.994398  [96000/175341]\n",
      "loss: 0.618392  [97600/175341]\n",
      "loss: 1.263895  [99200/175341]\n",
      "loss: 1.025869  [100800/175341]\n",
      "loss: 0.628850  [102400/175341]\n",
      "loss: 1.294367  [104000/175341]\n",
      "loss: 0.729800  [105600/175341]\n",
      "loss: 1.058376  [107200/175341]\n",
      "loss: 1.134709  [108800/175341]\n",
      "loss: 0.729937  [110400/175341]\n",
      "loss: 0.680687  [112000/175341]\n",
      "loss: 0.801465  [113600/175341]\n",
      "loss: 0.561248  [115200/175341]\n",
      "loss: 1.475825  [116800/175341]\n",
      "loss: 0.866460  [118400/175341]\n",
      "loss: 0.605051  [120000/175341]\n",
      "loss: 0.604315  [121600/175341]\n",
      "loss: 0.765720  [123200/175341]\n",
      "loss: 1.112231  [124800/175341]\n",
      "loss: 0.883298  [126400/175341]\n",
      "loss: 0.954485  [128000/175341]\n",
      "loss: 1.100399  [129600/175341]\n",
      "loss: 0.721735  [131200/175341]\n",
      "loss: 0.781331  [132800/175341]\n",
      "loss: 0.944073  [134400/175341]\n",
      "loss: 0.606570  [136000/175341]\n",
      "loss: 0.797582  [137600/175341]\n",
      "loss: 0.810915  [139200/175341]\n",
      "loss: 0.359923  [140800/175341]\n",
      "loss: 0.591965  [142400/175341]\n",
      "loss: 1.030687  [144000/175341]\n",
      "loss: 1.311736  [145600/175341]\n",
      "loss: 0.563610  [147200/175341]\n",
      "loss: 0.763523  [148800/175341]\n",
      "loss: 0.889665  [150400/175341]\n",
      "loss: 0.758358  [152000/175341]\n",
      "loss: 0.811005  [153600/175341]\n",
      "loss: 0.878748  [155200/175341]\n",
      "loss: 0.454063  [156800/175341]\n",
      "loss: 0.951118  [158400/175341]\n",
      "loss: 0.685272  [160000/175341]\n",
      "loss: 1.013682  [161600/175341]\n",
      "loss: 0.578965  [163200/175341]\n",
      "loss: 0.544194  [164800/175341]\n",
      "loss: 0.795202  [166400/175341]\n",
      "loss: 1.369504  [168000/175341]\n",
      "loss: 0.589190  [169600/175341]\n",
      "loss: 0.795532  [171200/175341]\n",
      "loss: 1.044638  [172800/175341]\n",
      "loss: 0.399523  [174400/175341]\n",
      "Train Accuracy: 61.4095%\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.837164, F1-score: 68.06% \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.871174  [    0/175341]\n",
      "loss: 0.810540  [ 1600/175341]\n",
      "loss: 0.633032  [ 3200/175341]\n",
      "loss: 0.541146  [ 4800/175341]\n",
      "loss: 0.827571  [ 6400/175341]\n",
      "loss: 0.308756  [ 8000/175341]\n",
      "loss: 0.751813  [ 9600/175341]\n",
      "loss: 0.842154  [11200/175341]\n",
      "loss: 0.579051  [12800/175341]\n",
      "loss: 0.893700  [14400/175341]\n",
      "loss: 0.655886  [16000/175341]\n",
      "loss: 0.637512  [17600/175341]\n",
      "loss: 0.631006  [19200/175341]\n",
      "loss: 1.074050  [20800/175341]\n",
      "loss: 0.524660  [22400/175341]\n",
      "loss: 1.266686  [24000/175341]\n",
      "loss: 0.991339  [25600/175341]\n",
      "loss: 1.416522  [27200/175341]\n",
      "loss: 1.076646  [28800/175341]\n",
      "loss: 0.858556  [30400/175341]\n",
      "loss: 0.903809  [32000/175341]\n",
      "loss: 0.557128  [33600/175341]\n",
      "loss: 0.580838  [35200/175341]\n",
      "loss: 0.633779  [36800/175341]\n",
      "loss: 0.966608  [38400/175341]\n",
      "loss: 0.268349  [40000/175341]\n",
      "loss: 0.639069  [41600/175341]\n",
      "loss: 0.868700  [43200/175341]\n",
      "loss: 0.769074  [44800/175341]\n",
      "loss: 0.734707  [46400/175341]\n",
      "loss: 0.262179  [48000/175341]\n",
      "loss: 0.760405  [49600/175341]\n",
      "loss: 0.809722  [51200/175341]\n",
      "loss: 0.519610  [52800/175341]\n",
      "loss: 0.551108  [54400/175341]\n",
      "loss: 0.383360  [56000/175341]\n",
      "loss: 0.526741  [57600/175341]\n",
      "loss: 0.911087  [59200/175341]\n",
      "loss: 0.437901  [60800/175341]\n",
      "loss: 0.273882  [62400/175341]\n",
      "loss: 0.602212  [64000/175341]\n",
      "loss: 0.758250  [65600/175341]\n",
      "loss: 0.579904  [67200/175341]\n",
      "loss: 0.872794  [68800/175341]\n",
      "loss: 0.925947  [70400/175341]\n",
      "loss: 0.647310  [72000/175341]\n",
      "loss: 0.314383  [73600/175341]\n",
      "loss: 0.467020  [75200/175341]\n",
      "loss: 0.707884  [76800/175341]\n",
      "loss: 0.789399  [78400/175341]\n",
      "loss: 0.626708  [80000/175341]\n",
      "loss: 0.424750  [81600/175341]\n",
      "loss: 0.586229  [83200/175341]\n",
      "loss: 0.498556  [84800/175341]\n",
      "loss: 0.616205  [86400/175341]\n",
      "loss: 0.693849  [88000/175341]\n",
      "loss: 0.384489  [89600/175341]\n",
      "loss: 0.667048  [91200/175341]\n",
      "loss: 0.961684  [92800/175341]\n",
      "loss: 0.651676  [94400/175341]\n",
      "loss: 0.833202  [96000/175341]\n",
      "loss: 0.799403  [97600/175341]\n",
      "loss: 0.639120  [99200/175341]\n",
      "loss: 0.743900  [100800/175341]\n",
      "loss: 0.702657  [102400/175341]\n",
      "loss: 1.036388  [104000/175341]\n",
      "loss: 0.915520  [105600/175341]\n",
      "loss: 0.733863  [107200/175341]\n",
      "loss: 0.467738  [108800/175341]\n",
      "loss: 0.607255  [110400/175341]\n",
      "loss: 0.915869  [112000/175341]\n",
      "loss: 0.559445  [113600/175341]\n",
      "loss: 0.729805  [115200/175341]\n",
      "loss: 0.752283  [116800/175341]\n",
      "loss: 1.113525  [118400/175341]\n",
      "loss: 0.633351  [120000/175341]\n",
      "loss: 0.409405  [121600/175341]\n",
      "loss: 1.266112  [123200/175341]\n",
      "loss: 0.722665  [124800/175341]\n",
      "loss: 0.377452  [126400/175341]\n",
      "loss: 1.067797  [128000/175341]\n",
      "loss: 0.655353  [129600/175341]\n",
      "loss: 0.696195  [131200/175341]\n",
      "loss: 0.426694  [132800/175341]\n",
      "loss: 0.723839  [134400/175341]\n",
      "loss: 0.927359  [136000/175341]\n",
      "loss: 0.649427  [137600/175341]\n",
      "loss: 0.456707  [139200/175341]\n",
      "loss: 0.441038  [140800/175341]\n",
      "loss: 0.568256  [142400/175341]\n",
      "loss: 0.237725  [144000/175341]\n",
      "loss: 0.617047  [145600/175341]\n",
      "loss: 0.464921  [147200/175341]\n",
      "loss: 0.814498  [148800/175341]\n",
      "loss: 0.237686  [150400/175341]\n",
      "loss: 0.727443  [152000/175341]\n",
      "loss: 0.650647  [153600/175341]\n",
      "loss: 0.384014  [155200/175341]\n",
      "loss: 0.751571  [156800/175341]\n",
      "loss: 0.760911  [158400/175341]\n",
      "loss: 1.013199  [160000/175341]\n",
      "loss: 0.341647  [161600/175341]\n",
      "loss: 0.347143  [163200/175341]\n",
      "loss: 0.381625  [164800/175341]\n",
      "loss: 0.553545  [166400/175341]\n",
      "loss: 0.372121  [168000/175341]\n",
      "loss: 0.846437  [169600/175341]\n",
      "loss: 0.632622  [171200/175341]\n",
      "loss: 0.583775  [172800/175341]\n",
      "loss: 0.628704  [174400/175341]\n",
      "Train Accuracy: 74.7954%\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.746285, F1-score: 70.26% \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.494937  [    0/175341]\n",
      "loss: 0.247906  [ 1600/175341]\n",
      "loss: 0.677085  [ 3200/175341]\n",
      "loss: 0.601932  [ 4800/175341]\n",
      "loss: 0.318514  [ 6400/175341]\n",
      "loss: 0.706243  [ 8000/175341]\n",
      "loss: 0.970237  [ 9600/175341]\n",
      "loss: 0.609090  [11200/175341]\n",
      "loss: 1.170650  [12800/175341]\n",
      "loss: 0.540046  [14400/175341]\n",
      "loss: 0.262952  [16000/175341]\n",
      "loss: 0.780844  [17600/175341]\n",
      "loss: 0.596724  [19200/175341]\n",
      "loss: 0.655920  [20800/175341]\n",
      "loss: 0.531406  [22400/175341]\n",
      "loss: 0.488016  [24000/175341]\n",
      "loss: 0.789074  [25600/175341]\n",
      "loss: 0.512934  [27200/175341]\n",
      "loss: 0.963735  [28800/175341]\n",
      "loss: 0.537944  [30400/175341]\n",
      "loss: 0.820704  [32000/175341]\n",
      "loss: 0.359507  [33600/175341]\n",
      "loss: 0.456863  [35200/175341]\n",
      "loss: 0.708262  [36800/175341]\n",
      "loss: 0.425385  [38400/175341]\n",
      "loss: 0.684417  [40000/175341]\n",
      "loss: 0.565306  [41600/175341]\n",
      "loss: 0.548398  [43200/175341]\n",
      "loss: 0.307583  [44800/175341]\n",
      "loss: 0.590997  [46400/175341]\n",
      "loss: 0.464340  [48000/175341]\n",
      "loss: 0.521796  [49600/175341]\n",
      "loss: 0.653303  [51200/175341]\n",
      "loss: 0.974331  [52800/175341]\n",
      "loss: 0.673786  [54400/175341]\n",
      "loss: 1.043700  [56000/175341]\n",
      "loss: 0.730573  [57600/175341]\n",
      "loss: 0.342719  [59200/175341]\n",
      "loss: 0.638662  [60800/175341]\n",
      "loss: 0.518506  [62400/175341]\n",
      "loss: 0.428681  [64000/175341]\n",
      "loss: 0.489141  [65600/175341]\n",
      "loss: 0.530565  [67200/175341]\n",
      "loss: 0.711498  [68800/175341]\n",
      "loss: 0.888598  [70400/175341]\n",
      "loss: 0.321316  [72000/175341]\n",
      "loss: 0.440926  [73600/175341]\n",
      "loss: 0.347636  [75200/175341]\n",
      "loss: 0.649304  [76800/175341]\n",
      "loss: 0.431020  [78400/175341]\n",
      "loss: 0.566519  [80000/175341]\n",
      "loss: 0.552488  [81600/175341]\n",
      "loss: 0.455940  [83200/175341]\n",
      "loss: 0.799339  [84800/175341]\n",
      "loss: 0.398246  [86400/175341]\n",
      "loss: 0.908596  [88000/175341]\n",
      "loss: 0.610638  [89600/175341]\n",
      "loss: 0.737936  [91200/175341]\n",
      "loss: 0.877096  [92800/175341]\n",
      "loss: 0.134792  [94400/175341]\n",
      "loss: 0.323961  [96000/175341]\n",
      "loss: 0.406323  [97600/175341]\n",
      "loss: 0.489892  [99200/175341]\n",
      "loss: 0.737902  [100800/175341]\n",
      "loss: 0.358715  [102400/175341]\n",
      "loss: 0.658532  [104000/175341]\n",
      "loss: 0.429815  [105600/175341]\n",
      "loss: 0.676504  [107200/175341]\n",
      "loss: 0.762999  [108800/175341]\n",
      "loss: 0.467325  [110400/175341]\n",
      "loss: 0.195129  [112000/175341]\n",
      "loss: 0.553523  [113600/175341]\n",
      "loss: 0.543085  [115200/175341]\n",
      "loss: 0.650979  [116800/175341]\n",
      "loss: 0.318023  [118400/175341]\n",
      "loss: 0.400029  [120000/175341]\n",
      "loss: 0.741419  [121600/175341]\n",
      "loss: 0.754972  [123200/175341]\n",
      "loss: 0.874242  [124800/175341]\n",
      "loss: 0.633496  [126400/175341]\n",
      "loss: 0.621243  [128000/175341]\n",
      "loss: 0.824603  [129600/175341]\n",
      "loss: 0.713779  [131200/175341]\n",
      "loss: 0.617485  [132800/175341]\n",
      "loss: 0.647461  [134400/175341]\n",
      "loss: 0.621967  [136000/175341]\n",
      "loss: 0.934692  [137600/175341]\n",
      "loss: 0.502109  [139200/175341]\n",
      "loss: 0.509326  [140800/175341]\n",
      "loss: 0.413712  [142400/175341]\n",
      "loss: 0.835719  [144000/175341]\n",
      "loss: 0.542994  [145600/175341]\n",
      "loss: 0.576835  [147200/175341]\n",
      "loss: 0.204920  [148800/175341]\n",
      "loss: 0.405026  [150400/175341]\n",
      "loss: 0.517256  [152000/175341]\n",
      "loss: 0.608721  [153600/175341]\n",
      "loss: 0.622057  [155200/175341]\n",
      "loss: 0.629135  [156800/175341]\n",
      "loss: 0.792045  [158400/175341]\n",
      "loss: 0.559411  [160000/175341]\n",
      "loss: 0.510265  [161600/175341]\n",
      "loss: 0.324186  [163200/175341]\n",
      "loss: 0.288174  [164800/175341]\n",
      "loss: 0.476271  [166400/175341]\n",
      "loss: 0.482655  [168000/175341]\n",
      "loss: 1.001891  [169600/175341]\n",
      "loss: 0.596122  [171200/175341]\n",
      "loss: 0.839419  [172800/175341]\n",
      "loss: 0.652809  [174400/175341]\n",
      "Train Accuracy: 76.3107%\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.710353, F1-score: 70.11% \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.430084  [    0/175341]\n",
      "loss: 0.472624  [ 1600/175341]\n",
      "loss: 0.640509  [ 3200/175341]\n",
      "loss: 0.327620  [ 4800/175341]\n",
      "loss: 0.412719  [ 6400/175341]\n",
      "loss: 0.488753  [ 8000/175341]\n",
      "loss: 0.846363  [ 9600/175341]\n",
      "loss: 0.291751  [11200/175341]\n",
      "loss: 0.509642  [12800/175341]\n",
      "loss: 0.323727  [14400/175341]\n",
      "loss: 0.718240  [16000/175341]\n",
      "loss: 0.654034  [17600/175341]\n",
      "loss: 0.293244  [19200/175341]\n",
      "loss: 0.461664  [20800/175341]\n",
      "loss: 0.543609  [22400/175341]\n",
      "loss: 0.372717  [24000/175341]\n",
      "loss: 0.534487  [25600/175341]\n",
      "loss: 0.895768  [27200/175341]\n",
      "loss: 0.760037  [28800/175341]\n",
      "loss: 0.860068  [30400/175341]\n",
      "loss: 0.683691  [32000/175341]\n",
      "loss: 0.140599  [33600/175341]\n",
      "loss: 0.828986  [35200/175341]\n",
      "loss: 0.874171  [36800/175341]\n",
      "loss: 0.519823  [38400/175341]\n",
      "loss: 0.256602  [40000/175341]\n",
      "loss: 0.399187  [41600/175341]\n",
      "loss: 0.692214  [43200/175341]\n",
      "loss: 0.348847  [44800/175341]\n",
      "loss: 0.831875  [46400/175341]\n",
      "loss: 0.678493  [48000/175341]\n",
      "loss: 0.705504  [49600/175341]\n",
      "loss: 0.961031  [51200/175341]\n",
      "loss: 0.694488  [52800/175341]\n",
      "loss: 0.565360  [54400/175341]\n",
      "loss: 0.610247  [56000/175341]\n",
      "loss: 0.655586  [57600/175341]\n",
      "loss: 0.363743  [59200/175341]\n",
      "loss: 0.394052  [60800/175341]\n",
      "loss: 0.524488  [62400/175341]\n",
      "loss: 0.534625  [64000/175341]\n",
      "loss: 0.488107  [65600/175341]\n",
      "loss: 0.725308  [67200/175341]\n",
      "loss: 0.347608  [68800/175341]\n",
      "loss: 0.329040  [70400/175341]\n",
      "loss: 0.373409  [72000/175341]\n",
      "loss: 1.025820  [73600/175341]\n",
      "loss: 0.775716  [75200/175341]\n",
      "loss: 0.301092  [76800/175341]\n",
      "loss: 0.301694  [78400/175341]\n",
      "loss: 0.448837  [80000/175341]\n",
      "loss: 0.391632  [81600/175341]\n",
      "loss: 0.469672  [83200/175341]\n",
      "loss: 0.389515  [84800/175341]\n",
      "loss: 0.729136  [86400/175341]\n",
      "loss: 0.164014  [88000/175341]\n",
      "loss: 0.724999  [89600/175341]\n",
      "loss: 0.363032  [91200/175341]\n",
      "loss: 1.338124  [92800/175341]\n",
      "loss: 0.739768  [94400/175341]\n",
      "loss: 0.112461  [96000/175341]\n",
      "loss: 0.735009  [97600/175341]\n",
      "loss: 0.589676  [99200/175341]\n",
      "loss: 0.845910  [100800/175341]\n",
      "loss: 0.577567  [102400/175341]\n",
      "loss: 0.867249  [104000/175341]\n",
      "loss: 0.504686  [105600/175341]\n",
      "loss: 0.767650  [107200/175341]\n",
      "loss: 0.231787  [108800/175341]\n",
      "loss: 0.619588  [110400/175341]\n",
      "loss: 0.476676  [112000/175341]\n",
      "loss: 0.920463  [113600/175341]\n",
      "loss: 0.941899  [115200/175341]\n",
      "loss: 0.261075  [116800/175341]\n",
      "loss: 0.677886  [118400/175341]\n",
      "loss: 0.372532  [120000/175341]\n",
      "loss: 0.616019  [121600/175341]\n",
      "loss: 0.585849  [123200/175341]\n",
      "loss: 0.459098  [124800/175341]\n",
      "loss: 0.877675  [126400/175341]\n",
      "loss: 1.007358  [128000/175341]\n",
      "loss: 0.645837  [129600/175341]\n",
      "loss: 0.227698  [131200/175341]\n",
      "loss: 0.572755  [132800/175341]\n",
      "loss: 0.185081  [134400/175341]\n",
      "loss: 0.544676  [136000/175341]\n",
      "loss: 0.618654  [137600/175341]\n",
      "loss: 0.470697  [139200/175341]\n",
      "loss: 0.459528  [140800/175341]\n",
      "loss: 0.672519  [142400/175341]\n",
      "loss: 1.140196  [144000/175341]\n",
      "loss: 0.354119  [145600/175341]\n",
      "loss: 0.667216  [147200/175341]\n",
      "loss: 0.835194  [148800/175341]\n",
      "loss: 0.817268  [150400/175341]\n",
      "loss: 0.592033  [152000/175341]\n",
      "loss: 0.265581  [153600/175341]\n",
      "loss: 0.314020  [155200/175341]\n",
      "loss: 0.465557  [156800/175341]\n",
      "loss: 0.839484  [158400/175341]\n",
      "loss: 0.447548  [160000/175341]\n",
      "loss: 0.462532  [161600/175341]\n",
      "loss: 0.526557  [163200/175341]\n",
      "loss: 0.523313  [164800/175341]\n",
      "loss: 0.225482  [166400/175341]\n",
      "loss: 0.641633  [168000/175341]\n",
      "loss: 0.397643  [169600/175341]\n",
      "loss: 0.825047  [171200/175341]\n",
      "loss: 0.822738  [172800/175341]\n",
      "loss: 0.255676  [174400/175341]\n",
      "Train Accuracy: 76.7989%\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.703647, F1-score: 69.81% \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.950665  [    0/175341]\n",
      "loss: 1.149290  [ 1600/175341]\n",
      "loss: 0.309785  [ 3200/175341]\n",
      "loss: 0.659434  [ 4800/175341]\n",
      "loss: 1.071801  [ 6400/175341]\n",
      "loss: 0.624316  [ 8000/175341]\n",
      "loss: 1.005376  [ 9600/175341]\n",
      "loss: 0.501320  [11200/175341]\n",
      "loss: 1.005539  [12800/175341]\n",
      "loss: 0.812735  [14400/175341]\n",
      "loss: 0.523855  [16000/175341]\n",
      "loss: 0.190769  [17600/175341]\n",
      "loss: 1.081635  [19200/175341]\n",
      "loss: 0.604922  [20800/175341]\n",
      "loss: 0.489247  [22400/175341]\n",
      "loss: 0.581356  [24000/175341]\n",
      "loss: 0.592230  [25600/175341]\n",
      "loss: 0.607273  [27200/175341]\n",
      "loss: 0.312655  [28800/175341]\n",
      "loss: 0.314900  [30400/175341]\n",
      "loss: 0.575361  [32000/175341]\n",
      "loss: 0.741490  [33600/175341]\n",
      "loss: 0.262783  [35200/175341]\n",
      "loss: 0.746366  [36800/175341]\n",
      "loss: 0.505098  [38400/175341]\n",
      "loss: 0.395266  [40000/175341]\n",
      "loss: 0.884163  [41600/175341]\n",
      "loss: 0.807164  [43200/175341]\n",
      "loss: 0.664138  [44800/175341]\n",
      "loss: 0.530623  [46400/175341]\n",
      "loss: 0.571721  [48000/175341]\n",
      "loss: 0.264263  [49600/175341]\n",
      "loss: 0.367523  [51200/175341]\n",
      "loss: 1.000267  [52800/175341]\n",
      "loss: 0.854611  [54400/175341]\n",
      "loss: 0.729742  [56000/175341]\n",
      "loss: 0.909636  [57600/175341]\n",
      "loss: 0.380583  [59200/175341]\n",
      "loss: 0.888469  [60800/175341]\n",
      "loss: 0.601890  [62400/175341]\n",
      "loss: 0.470740  [64000/175341]\n",
      "loss: 0.510819  [65600/175341]\n",
      "loss: 0.552000  [67200/175341]\n",
      "loss: 0.524111  [68800/175341]\n",
      "loss: 0.628203  [70400/175341]\n",
      "loss: 0.156552  [72000/175341]\n",
      "loss: 0.303146  [73600/175341]\n",
      "loss: 0.342545  [75200/175341]\n",
      "loss: 0.613942  [76800/175341]\n",
      "loss: 0.513045  [78400/175341]\n",
      "loss: 0.505448  [80000/175341]\n",
      "loss: 0.812959  [81600/175341]\n",
      "loss: 0.359023  [83200/175341]\n",
      "loss: 0.813687  [84800/175341]\n",
      "loss: 1.034947  [86400/175341]\n",
      "loss: 0.582341  [88000/175341]\n",
      "loss: 0.978903  [89600/175341]\n",
      "loss: 0.807754  [91200/175341]\n",
      "loss: 0.508694  [92800/175341]\n",
      "loss: 0.949268  [94400/175341]\n",
      "loss: 0.612000  [96000/175341]\n",
      "loss: 0.637327  [97600/175341]\n",
      "loss: 0.701195  [99200/175341]\n",
      "loss: 0.362788  [100800/175341]\n",
      "loss: 0.670544  [102400/175341]\n",
      "loss: 0.858919  [104000/175341]\n",
      "loss: 0.552839  [105600/175341]\n",
      "loss: 0.625868  [107200/175341]\n",
      "loss: 0.445181  [108800/175341]\n",
      "loss: 0.923829  [110400/175341]\n",
      "loss: 0.388483  [112000/175341]\n",
      "loss: 0.399124  [113600/175341]\n",
      "loss: 0.635450  [115200/175341]\n",
      "loss: 1.548055  [116800/175341]\n",
      "loss: 0.768800  [118400/175341]\n",
      "loss: 0.594549  [120000/175341]\n",
      "loss: 0.585656  [121600/175341]\n",
      "loss: 0.568063  [123200/175341]\n",
      "loss: 1.290660  [124800/175341]\n",
      "loss: 0.339330  [126400/175341]\n",
      "loss: 0.818232  [128000/175341]\n",
      "loss: 0.440984  [129600/175341]\n",
      "loss: 0.556282  [131200/175341]\n",
      "loss: 0.448410  [132800/175341]\n",
      "loss: 0.746336  [134400/175341]\n",
      "loss: 0.924631  [136000/175341]\n",
      "loss: 0.534811  [137600/175341]\n",
      "loss: 0.537774  [139200/175341]\n",
      "loss: 1.200336  [140800/175341]\n",
      "loss: 1.020222  [142400/175341]\n",
      "loss: 0.653216  [144000/175341]\n",
      "loss: 0.285815  [145600/175341]\n",
      "loss: 0.531169  [147200/175341]\n",
      "loss: 0.392806  [148800/175341]\n",
      "loss: 0.456624  [150400/175341]\n",
      "loss: 0.591879  [152000/175341]\n",
      "loss: 0.523444  [153600/175341]\n",
      "loss: 0.360801  [155200/175341]\n",
      "loss: 0.691019  [156800/175341]\n",
      "loss: 0.287039  [158400/175341]\n",
      "loss: 0.215864  [160000/175341]\n",
      "loss: 0.418360  [161600/175341]\n",
      "loss: 0.897220  [163200/175341]\n",
      "loss: 0.661978  [164800/175341]\n",
      "loss: 0.533652  [166400/175341]\n",
      "loss: 0.410867  [168000/175341]\n",
      "loss: 0.408000  [169600/175341]\n",
      "loss: 0.639540  [171200/175341]\n",
      "loss: 0.831930  [172800/175341]\n",
      "loss: 0.623044  [174400/175341]\n",
      "Train Accuracy: 77.2700%\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.656935, F1-score: 70.99% \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.509123  [    0/175341]\n",
      "loss: 0.297210  [ 1600/175341]\n",
      "loss: 0.794370  [ 3200/175341]\n",
      "loss: 0.565868  [ 4800/175341]\n",
      "loss: 0.665758  [ 6400/175341]\n",
      "loss: 0.448923  [ 8000/175341]\n",
      "loss: 0.253227  [ 9600/175341]\n",
      "loss: 1.096707  [11200/175341]\n",
      "loss: 0.650009  [12800/175341]\n",
      "loss: 0.666412  [14400/175341]\n",
      "loss: 0.596677  [16000/175341]\n",
      "loss: 0.719831  [17600/175341]\n",
      "loss: 0.643136  [19200/175341]\n",
      "loss: 0.594702  [20800/175341]\n",
      "loss: 0.475216  [22400/175341]\n",
      "loss: 0.824273  [24000/175341]\n",
      "loss: 0.756999  [25600/175341]\n",
      "loss: 0.615730  [27200/175341]\n",
      "loss: 0.511374  [28800/175341]\n",
      "loss: 0.518535  [30400/175341]\n",
      "loss: 0.313721  [32000/175341]\n",
      "loss: 0.518297  [33600/175341]\n",
      "loss: 0.529487  [35200/175341]\n",
      "loss: 0.334434  [36800/175341]\n",
      "loss: 0.798002  [38400/175341]\n",
      "loss: 0.570469  [40000/175341]\n",
      "loss: 0.737665  [41600/175341]\n",
      "loss: 0.272081  [43200/175341]\n",
      "loss: 0.590356  [44800/175341]\n",
      "loss: 0.885682  [46400/175341]\n",
      "loss: 0.600929  [48000/175341]\n",
      "loss: 0.727687  [49600/175341]\n",
      "loss: 0.656930  [51200/175341]\n",
      "loss: 0.701102  [52800/175341]\n",
      "loss: 0.541393  [54400/175341]\n",
      "loss: 0.578147  [56000/175341]\n",
      "loss: 0.593569  [57600/175341]\n",
      "loss: 0.252412  [59200/175341]\n",
      "loss: 0.619828  [60800/175341]\n",
      "loss: 0.152686  [62400/175341]\n",
      "loss: 0.222276  [64000/175341]\n",
      "loss: 0.505625  [65600/175341]\n",
      "loss: 0.432830  [67200/175341]\n",
      "loss: 0.542615  [68800/175341]\n",
      "loss: 0.500758  [70400/175341]\n",
      "loss: 0.654520  [72000/175341]\n",
      "loss: 0.781261  [73600/175341]\n",
      "loss: 0.834994  [75200/175341]\n",
      "loss: 0.581979  [76800/175341]\n",
      "loss: 0.652719  [78400/175341]\n",
      "loss: 0.471366  [80000/175341]\n",
      "loss: 0.704003  [81600/175341]\n",
      "loss: 0.906572  [83200/175341]\n",
      "loss: 1.080153  [84800/175341]\n",
      "loss: 0.488335  [86400/175341]\n",
      "loss: 0.447335  [88000/175341]\n",
      "loss: 0.265761  [89600/175341]\n",
      "loss: 0.519986  [91200/175341]\n",
      "loss: 1.061643  [92800/175341]\n",
      "loss: 0.250625  [94400/175341]\n",
      "loss: 0.700538  [96000/175341]\n",
      "loss: 1.003377  [97600/175341]\n",
      "loss: 0.717925  [99200/175341]\n",
      "loss: 0.559810  [100800/175341]\n",
      "loss: 0.832613  [102400/175341]\n",
      "loss: 0.665879  [104000/175341]\n",
      "loss: 0.939483  [105600/175341]\n",
      "loss: 0.599624  [107200/175341]\n",
      "loss: 1.009609  [108800/175341]\n",
      "loss: 0.814005  [110400/175341]\n",
      "loss: 0.820970  [112000/175341]\n",
      "loss: 0.465497  [113600/175341]\n",
      "loss: 1.171420  [115200/175341]\n",
      "loss: 0.299386  [116800/175341]\n",
      "loss: 0.854237  [118400/175341]\n",
      "loss: 0.832118  [120000/175341]\n",
      "loss: 0.508363  [121600/175341]\n",
      "loss: 0.316527  [123200/175341]\n",
      "loss: 0.583283  [124800/175341]\n",
      "loss: 0.427519  [126400/175341]\n",
      "loss: 0.964498  [128000/175341]\n",
      "loss: 0.761217  [129600/175341]\n",
      "loss: 0.374191  [131200/175341]\n",
      "loss: 0.470384  [132800/175341]\n",
      "loss: 0.490967  [134400/175341]\n",
      "loss: 0.983703  [136000/175341]\n",
      "loss: 0.708307  [137600/175341]\n",
      "loss: 0.715088  [139200/175341]\n",
      "loss: 0.804189  [140800/175341]\n",
      "loss: 0.408641  [142400/175341]\n",
      "loss: 0.872153  [144000/175341]\n",
      "loss: 0.798668  [145600/175341]\n",
      "loss: 0.422769  [147200/175341]\n",
      "loss: 0.140381  [148800/175341]\n",
      "loss: 0.553442  [150400/175341]\n",
      "loss: 0.321744  [152000/175341]\n",
      "loss: 0.551127  [153600/175341]\n",
      "loss: 0.386869  [155200/175341]\n",
      "loss: 0.748668  [156800/175341]\n",
      "loss: 0.850877  [158400/175341]\n",
      "loss: 0.542530  [160000/175341]\n",
      "loss: 0.842952  [161600/175341]\n",
      "loss: 0.386428  [163200/175341]\n",
      "loss: 0.448442  [164800/175341]\n",
      "loss: 0.859128  [166400/175341]\n",
      "loss: 0.743875  [168000/175341]\n",
      "loss: 1.139701  [169600/175341]\n",
      "loss: 0.607125  [171200/175341]\n",
      "loss: 0.643507  [172800/175341]\n",
      "loss: 0.674721  [174400/175341]\n",
      "Train Accuracy: 77.5438%\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.643194, F1-score: 71.15% \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.783112  [    0/175341]\n",
      "loss: 0.602574  [ 1600/175341]\n",
      "loss: 0.378877  [ 3200/175341]\n",
      "loss: 0.809718  [ 4800/175341]\n",
      "loss: 0.333510  [ 6400/175341]\n",
      "loss: 0.814143  [ 8000/175341]\n",
      "loss: 0.652437  [ 9600/175341]\n",
      "loss: 0.573204  [11200/175341]\n",
      "loss: 0.940382  [12800/175341]\n",
      "loss: 0.551826  [14400/175341]\n",
      "loss: 0.723143  [16000/175341]\n",
      "loss: 0.409775  [17600/175341]\n",
      "loss: 0.716138  [19200/175341]\n",
      "loss: 0.314528  [20800/175341]\n",
      "loss: 0.318453  [22400/175341]\n",
      "loss: 0.878115  [24000/175341]\n",
      "loss: 0.185785  [25600/175341]\n",
      "loss: 0.669377  [27200/175341]\n",
      "loss: 0.358734  [28800/175341]\n",
      "loss: 0.377271  [30400/175341]\n",
      "loss: 0.316380  [32000/175341]\n",
      "loss: 0.478280  [33600/175341]\n",
      "loss: 0.312471  [35200/175341]\n",
      "loss: 0.445006  [36800/175341]\n",
      "loss: 0.636854  [38400/175341]\n",
      "loss: 0.595289  [40000/175341]\n",
      "loss: 0.642547  [41600/175341]\n",
      "loss: 0.616012  [43200/175341]\n",
      "loss: 0.340557  [44800/175341]\n",
      "loss: 0.419146  [46400/175341]\n",
      "loss: 0.626323  [48000/175341]\n",
      "loss: 0.400404  [49600/175341]\n",
      "loss: 0.997821  [51200/175341]\n",
      "loss: 0.623061  [52800/175341]\n",
      "loss: 0.591104  [54400/175341]\n",
      "loss: 0.522707  [56000/175341]\n",
      "loss: 0.662128  [57600/175341]\n",
      "loss: 0.422607  [59200/175341]\n",
      "loss: 0.654238  [60800/175341]\n",
      "loss: 0.628961  [62400/175341]\n",
      "loss: 0.749927  [64000/175341]\n",
      "loss: 0.295095  [65600/175341]\n",
      "loss: 0.444973  [67200/175341]\n",
      "loss: 0.746305  [68800/175341]\n",
      "loss: 0.840245  [70400/175341]\n",
      "loss: 0.844021  [72000/175341]\n",
      "loss: 0.667739  [73600/175341]\n",
      "loss: 0.510958  [75200/175341]\n",
      "loss: 0.830841  [76800/175341]\n",
      "loss: 1.029956  [78400/175341]\n",
      "loss: 0.614561  [80000/175341]\n",
      "loss: 0.883694  [81600/175341]\n",
      "loss: 0.860550  [83200/175341]\n",
      "loss: 0.487244  [84800/175341]\n",
      "loss: 0.699430  [86400/175341]\n",
      "loss: 0.341895  [88000/175341]\n",
      "loss: 0.568509  [89600/175341]\n",
      "loss: 0.618446  [91200/175341]\n",
      "loss: 0.890675  [92800/175341]\n",
      "loss: 0.374483  [94400/175341]\n",
      "loss: 0.260382  [96000/175341]\n",
      "loss: 0.346771  [97600/175341]\n",
      "loss: 0.312562  [99200/175341]\n",
      "loss: 0.421377  [100800/175341]\n",
      "loss: 0.745212  [102400/175341]\n",
      "loss: 0.548494  [104000/175341]\n",
      "loss: 0.437469  [105600/175341]\n",
      "loss: 0.411894  [107200/175341]\n",
      "loss: 0.822081  [108800/175341]\n",
      "loss: 0.801618  [110400/175341]\n",
      "loss: 0.415664  [112000/175341]\n",
      "loss: 0.538204  [113600/175341]\n",
      "loss: 0.328374  [115200/175341]\n",
      "loss: 0.412357  [116800/175341]\n",
      "loss: 0.405062  [118400/175341]\n",
      "loss: 0.360809  [120000/175341]\n",
      "loss: 0.380197  [121600/175341]\n",
      "loss: 0.253095  [123200/175341]\n",
      "loss: 0.373735  [124800/175341]\n",
      "loss: 0.254437  [126400/175341]\n",
      "loss: 0.522861  [128000/175341]\n",
      "loss: 0.466934  [129600/175341]\n",
      "loss: 0.487902  [131200/175341]\n",
      "loss: 0.422158  [132800/175341]\n",
      "loss: 0.788843  [134400/175341]\n",
      "loss: 0.495256  [136000/175341]\n",
      "loss: 0.495707  [137600/175341]\n",
      "loss: 0.866671  [139200/175341]\n",
      "loss: 0.468327  [140800/175341]\n",
      "loss: 0.403506  [142400/175341]\n",
      "loss: 0.630171  [144000/175341]\n",
      "loss: 0.751127  [145600/175341]\n",
      "loss: 0.752538  [147200/175341]\n",
      "loss: 0.821726  [148800/175341]\n",
      "loss: 0.554390  [150400/175341]\n",
      "loss: 0.608391  [152000/175341]\n",
      "loss: 0.627639  [153600/175341]\n",
      "loss: 0.249213  [155200/175341]\n",
      "loss: 0.655500  [156800/175341]\n",
      "loss: 0.211749  [158400/175341]\n",
      "loss: 0.538384  [160000/175341]\n",
      "loss: 0.779680  [161600/175341]\n",
      "loss: 0.487718  [163200/175341]\n",
      "loss: 0.950729  [164800/175341]\n",
      "loss: 0.854957  [166400/175341]\n",
      "loss: 0.293787  [168000/175341]\n",
      "loss: 0.883185  [169600/175341]\n",
      "loss: 0.435230  [171200/175341]\n",
      "loss: 0.649002  [172800/175341]\n",
      "loss: 0.483754  [174400/175341]\n",
      "Train Accuracy: 77.7742%\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.650727, F1-score: 71.24% \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.705939  [    0/175341]\n",
      "loss: 0.950593  [ 1600/175341]\n",
      "loss: 0.482856  [ 3200/175341]\n",
      "loss: 0.495393  [ 4800/175341]\n",
      "loss: 0.478953  [ 6400/175341]\n",
      "loss: 0.724918  [ 8000/175341]\n",
      "loss: 0.354600  [ 9600/175341]\n",
      "loss: 0.492612  [11200/175341]\n",
      "loss: 0.852524  [12800/175341]\n",
      "loss: 0.378984  [14400/175341]\n",
      "loss: 1.267022  [16000/175341]\n",
      "loss: 0.810765  [17600/175341]\n",
      "loss: 0.524702  [19200/175341]\n",
      "loss: 0.550969  [20800/175341]\n",
      "loss: 0.523518  [22400/175341]\n",
      "loss: 0.523332  [24000/175341]\n",
      "loss: 0.563934  [25600/175341]\n",
      "loss: 0.282766  [27200/175341]\n",
      "loss: 0.470304  [28800/175341]\n",
      "loss: 0.300268  [30400/175341]\n",
      "loss: 0.713007  [32000/175341]\n",
      "loss: 0.482175  [33600/175341]\n",
      "loss: 0.248610  [35200/175341]\n",
      "loss: 0.627112  [36800/175341]\n",
      "loss: 0.271534  [38400/175341]\n",
      "loss: 0.611946  [40000/175341]\n",
      "loss: 0.521772  [41600/175341]\n",
      "loss: 0.229909  [43200/175341]\n",
      "loss: 0.473161  [44800/175341]\n",
      "loss: 0.632009  [46400/175341]\n",
      "loss: 0.309835  [48000/175341]\n",
      "loss: 0.962584  [49600/175341]\n",
      "loss: 0.410954  [51200/175341]\n",
      "loss: 0.393128  [52800/175341]\n",
      "loss: 0.565312  [54400/175341]\n",
      "loss: 0.668473  [56000/175341]\n",
      "loss: 0.647117  [57600/175341]\n",
      "loss: 0.637522  [59200/175341]\n",
      "loss: 0.477068  [60800/175341]\n",
      "loss: 1.060989  [62400/175341]\n",
      "loss: 1.053933  [64000/175341]\n",
      "loss: 0.905893  [65600/175341]\n",
      "loss: 0.593945  [67200/175341]\n",
      "loss: 0.578409  [68800/175341]\n",
      "loss: 0.386146  [70400/175341]\n",
      "loss: 0.580860  [72000/175341]\n",
      "loss: 0.372992  [73600/175341]\n",
      "loss: 0.326397  [75200/175341]\n",
      "loss: 0.362099  [76800/175341]\n",
      "loss: 0.527227  [78400/175341]\n",
      "loss: 0.407310  [80000/175341]\n",
      "loss: 0.683533  [81600/175341]\n",
      "loss: 0.193017  [83200/175341]\n",
      "loss: 0.392283  [84800/175341]\n",
      "loss: 0.634068  [86400/175341]\n",
      "loss: 0.760301  [88000/175341]\n",
      "loss: 0.269688  [89600/175341]\n",
      "loss: 0.590271  [91200/175341]\n",
      "loss: 1.024832  [92800/175341]\n",
      "loss: 0.909687  [94400/175341]\n",
      "loss: 0.494487  [96000/175341]\n",
      "loss: 0.555367  [97600/175341]\n",
      "loss: 0.277099  [99200/175341]\n",
      "loss: 0.765431  [100800/175341]\n",
      "loss: 0.321180  [102400/175341]\n",
      "loss: 0.445217  [104000/175341]\n",
      "loss: 0.725647  [105600/175341]\n",
      "loss: 0.478315  [107200/175341]\n",
      "loss: 0.295642  [108800/175341]\n",
      "loss: 0.533183  [110400/175341]\n",
      "loss: 0.626105  [112000/175341]\n",
      "loss: 0.417602  [113600/175341]\n",
      "loss: 0.326858  [115200/175341]\n",
      "loss: 0.881674  [116800/175341]\n",
      "loss: 0.306407  [118400/175341]\n",
      "loss: 0.568138  [120000/175341]\n",
      "loss: 0.451677  [121600/175341]\n",
      "loss: 0.478660  [123200/175341]\n",
      "loss: 0.581547  [124800/175341]\n",
      "loss: 0.403586  [126400/175341]\n",
      "loss: 0.718480  [128000/175341]\n",
      "loss: 0.453262  [129600/175341]\n",
      "loss: 0.750201  [131200/175341]\n",
      "loss: 0.250997  [132800/175341]\n",
      "loss: 0.546851  [134400/175341]\n",
      "loss: 0.283150  [136000/175341]\n",
      "loss: 0.280278  [137600/175341]\n",
      "loss: 0.531506  [139200/175341]\n",
      "loss: 0.637304  [140800/175341]\n",
      "loss: 0.945511  [142400/175341]\n",
      "loss: 0.250940  [144000/175341]\n",
      "loss: 0.420545  [145600/175341]\n",
      "loss: 0.584557  [147200/175341]\n",
      "loss: 0.762836  [148800/175341]\n",
      "loss: 0.770753  [150400/175341]\n",
      "loss: 0.864546  [152000/175341]\n",
      "loss: 0.974231  [153600/175341]\n",
      "loss: 0.480298  [155200/175341]\n",
      "loss: 0.716403  [156800/175341]\n",
      "loss: 0.455034  [158400/175341]\n",
      "loss: 0.701949  [160000/175341]\n",
      "loss: 0.523186  [161600/175341]\n",
      "loss: 0.175200  [163200/175341]\n",
      "loss: 0.208320  [164800/175341]\n",
      "loss: 0.685941  [166400/175341]\n",
      "loss: 0.575214  [168000/175341]\n",
      "loss: 0.348798  [169600/175341]\n",
      "loss: 0.748481  [171200/175341]\n",
      "loss: 0.644291  [172800/175341]\n",
      "loss: 0.461066  [174400/175341]\n",
      "Train Accuracy: 77.9823%\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.628924, F1-score: 72.20% \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.421874  [    0/175341]\n",
      "loss: 0.218726  [ 1600/175341]\n",
      "loss: 0.282982  [ 3200/175341]\n",
      "loss: 0.316527  [ 4800/175341]\n",
      "loss: 0.303691  [ 6400/175341]\n",
      "loss: 0.451794  [ 8000/175341]\n",
      "loss: 0.679515  [ 9600/175341]\n",
      "loss: 0.132319  [11200/175341]\n",
      "loss: 0.321851  [12800/175341]\n",
      "loss: 0.533605  [14400/175341]\n",
      "loss: 0.852626  [16000/175341]\n",
      "loss: 0.707025  [17600/175341]\n",
      "loss: 0.880964  [19200/175341]\n",
      "loss: 0.740745  [20800/175341]\n",
      "loss: 0.316868  [22400/175341]\n",
      "loss: 0.666014  [24000/175341]\n",
      "loss: 0.710679  [25600/175341]\n",
      "loss: 0.661567  [27200/175341]\n",
      "loss: 0.378311  [28800/175341]\n",
      "loss: 0.972722  [30400/175341]\n",
      "loss: 0.781489  [32000/175341]\n",
      "loss: 0.545952  [33600/175341]\n",
      "loss: 0.740734  [35200/175341]\n",
      "loss: 0.669228  [36800/175341]\n",
      "loss: 0.388789  [38400/175341]\n",
      "loss: 0.450493  [40000/175341]\n",
      "loss: 0.299028  [41600/175341]\n",
      "loss: 0.578519  [43200/175341]\n",
      "loss: 0.632909  [44800/175341]\n",
      "loss: 0.663597  [46400/175341]\n",
      "loss: 0.389911  [48000/175341]\n",
      "loss: 0.446510  [49600/175341]\n",
      "loss: 0.552179  [51200/175341]\n",
      "loss: 0.629571  [52800/175341]\n",
      "loss: 0.523165  [54400/175341]\n",
      "loss: 0.510955  [56000/175341]\n",
      "loss: 0.773704  [57600/175341]\n",
      "loss: 0.341160  [59200/175341]\n",
      "loss: 0.688000  [60800/175341]\n",
      "loss: 0.435806  [62400/175341]\n",
      "loss: 0.785943  [64000/175341]\n",
      "loss: 0.194467  [65600/175341]\n",
      "loss: 0.540479  [67200/175341]\n",
      "loss: 0.963136  [68800/175341]\n",
      "loss: 0.513717  [70400/175341]\n",
      "loss: 1.360808  [72000/175341]\n",
      "loss: 0.614192  [73600/175341]\n",
      "loss: 1.022585  [75200/175341]\n",
      "loss: 0.387512  [76800/175341]\n",
      "loss: 0.354505  [78400/175341]\n",
      "loss: 0.245835  [80000/175341]\n",
      "loss: 0.770717  [81600/175341]\n",
      "loss: 0.740776  [83200/175341]\n",
      "loss: 0.901090  [84800/175341]\n",
      "loss: 0.537077  [86400/175341]\n",
      "loss: 0.608735  [88000/175341]\n",
      "loss: 0.439221  [89600/175341]\n",
      "loss: 0.380736  [91200/175341]\n",
      "loss: 0.914475  [92800/175341]\n",
      "loss: 0.677786  [94400/175341]\n",
      "loss: 0.528886  [96000/175341]\n",
      "loss: 0.767333  [97600/175341]\n",
      "loss: 0.681947  [99200/175341]\n",
      "loss: 0.320539  [100800/175341]\n",
      "loss: 0.509834  [102400/175341]\n",
      "loss: 0.500798  [104000/175341]\n",
      "loss: 1.185870  [105600/175341]\n",
      "loss: 0.296718  [107200/175341]\n",
      "loss: 0.355653  [108800/175341]\n",
      "loss: 0.208132  [110400/175341]\n",
      "loss: 0.729408  [112000/175341]\n",
      "loss: 0.480154  [113600/175341]\n",
      "loss: 0.305944  [115200/175341]\n",
      "loss: 0.535080  [116800/175341]\n",
      "loss: 0.848060  [118400/175341]\n",
      "loss: 0.501365  [120000/175341]\n",
      "loss: 0.248674  [121600/175341]\n",
      "loss: 0.487646  [123200/175341]\n",
      "loss: 0.498449  [124800/175341]\n",
      "loss: 0.476811  [126400/175341]\n",
      "loss: 0.527744  [128000/175341]\n",
      "loss: 0.224617  [129600/175341]\n",
      "loss: 0.626547  [131200/175341]\n",
      "loss: 0.778328  [132800/175341]\n",
      "loss: 0.272435  [134400/175341]\n",
      "loss: 0.720775  [136000/175341]\n",
      "loss: 0.364525  [137600/175341]\n",
      "loss: 0.659510  [139200/175341]\n",
      "loss: 0.742968  [140800/175341]\n",
      "loss: 0.476765  [142400/175341]\n",
      "loss: 0.232959  [144000/175341]\n",
      "loss: 0.411546  [145600/175341]\n",
      "loss: 0.328451  [147200/175341]\n",
      "loss: 0.722959  [148800/175341]\n",
      "loss: 0.925763  [150400/175341]\n",
      "loss: 0.570677  [152000/175341]\n",
      "loss: 0.944336  [153600/175341]\n",
      "loss: 0.480636  [155200/175341]\n",
      "loss: 0.440021  [156800/175341]\n",
      "loss: 0.793718  [158400/175341]\n",
      "loss: 1.150124  [160000/175341]\n",
      "loss: 0.876658  [161600/175341]\n",
      "loss: 0.233997  [163200/175341]\n",
      "loss: 0.637617  [164800/175341]\n",
      "loss: 0.539423  [166400/175341]\n",
      "loss: 0.670044  [168000/175341]\n",
      "loss: 0.410824  [169600/175341]\n",
      "loss: 0.566911  [171200/175341]\n",
      "loss: 0.476211  [172800/175341]\n",
      "loss: 0.565404  [174400/175341]\n",
      "Train Accuracy: 78.2641%\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.646145, F1-score: 71.86% \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.490912  [    0/175341]\n",
      "loss: 0.576493  [ 1600/175341]\n",
      "loss: 0.867757  [ 3200/175341]\n",
      "loss: 0.389749  [ 4800/175341]\n",
      "loss: 0.786218  [ 6400/175341]\n",
      "loss: 0.929607  [ 8000/175341]\n",
      "loss: 0.565563  [ 9600/175341]\n",
      "loss: 0.739805  [11200/175341]\n",
      "loss: 0.465152  [12800/175341]\n",
      "loss: 0.557324  [14400/175341]\n",
      "loss: 0.468088  [16000/175341]\n",
      "loss: 0.395997  [17600/175341]\n",
      "loss: 0.692691  [19200/175341]\n",
      "loss: 0.671479  [20800/175341]\n",
      "loss: 0.487343  [22400/175341]\n",
      "loss: 0.266610  [24000/175341]\n",
      "loss: 0.340222  [25600/175341]\n",
      "loss: 0.250220  [27200/175341]\n",
      "loss: 0.580655  [28800/175341]\n",
      "loss: 0.442087  [30400/175341]\n",
      "loss: 0.604841  [32000/175341]\n",
      "loss: 0.709625  [33600/175341]\n",
      "loss: 0.602260  [35200/175341]\n",
      "loss: 0.631478  [36800/175341]\n",
      "loss: 0.632327  [38400/175341]\n",
      "loss: 0.255235  [40000/175341]\n",
      "loss: 0.730994  [41600/175341]\n",
      "loss: 0.690591  [43200/175341]\n",
      "loss: 0.729787  [44800/175341]\n",
      "loss: 0.408222  [46400/175341]\n",
      "loss: 0.456799  [48000/175341]\n",
      "loss: 0.254711  [49600/175341]\n",
      "loss: 0.762651  [51200/175341]\n",
      "loss: 0.810708  [52800/175341]\n",
      "loss: 0.907178  [54400/175341]\n",
      "loss: 0.436594  [56000/175341]\n",
      "loss: 0.901609  [57600/175341]\n",
      "loss: 0.448736  [59200/175341]\n",
      "loss: 0.677306  [60800/175341]\n",
      "loss: 0.170421  [62400/175341]\n",
      "loss: 0.701391  [64000/175341]\n",
      "loss: 0.542871  [65600/175341]\n",
      "loss: 0.492083  [67200/175341]\n",
      "loss: 0.828076  [68800/175341]\n",
      "loss: 0.736480  [70400/175341]\n",
      "loss: 0.316179  [72000/175341]\n",
      "loss: 0.745243  [73600/175341]\n",
      "loss: 0.784637  [75200/175341]\n",
      "loss: 0.520934  [76800/175341]\n",
      "loss: 0.209536  [78400/175341]\n",
      "loss: 0.426067  [80000/175341]\n",
      "loss: 0.718982  [81600/175341]\n",
      "loss: 0.631320  [83200/175341]\n",
      "loss: 0.421200  [84800/175341]\n",
      "loss: 0.415440  [86400/175341]\n",
      "loss: 0.549926  [88000/175341]\n",
      "loss: 0.961438  [89600/175341]\n",
      "loss: 0.721568  [91200/175341]\n",
      "loss: 0.260745  [92800/175341]\n",
      "loss: 0.687064  [94400/175341]\n",
      "loss: 0.565992  [96000/175341]\n",
      "loss: 0.446259  [97600/175341]\n",
      "loss: 0.076784  [99200/175341]\n",
      "loss: 0.517187  [100800/175341]\n",
      "loss: 0.715129  [102400/175341]\n",
      "loss: 0.607883  [104000/175341]\n",
      "loss: 0.700952  [105600/175341]\n",
      "loss: 0.533033  [107200/175341]\n",
      "loss: 0.705999  [108800/175341]\n",
      "loss: 0.711617  [110400/175341]\n",
      "loss: 0.713799  [112000/175341]\n",
      "loss: 0.520205  [113600/175341]\n",
      "loss: 0.975601  [115200/175341]\n",
      "loss: 0.934441  [116800/175341]\n",
      "loss: 0.447130  [118400/175341]\n",
      "loss: 0.318245  [120000/175341]\n",
      "loss: 0.620711  [121600/175341]\n",
      "loss: 0.575632  [123200/175341]\n",
      "loss: 0.548902  [124800/175341]\n",
      "loss: 0.433231  [126400/175341]\n",
      "loss: 0.468010  [128000/175341]\n",
      "loss: 0.690430  [129600/175341]\n",
      "loss: 0.742626  [131200/175341]\n",
      "loss: 0.558597  [132800/175341]\n",
      "loss: 0.357549  [134400/175341]\n",
      "loss: 0.604005  [136000/175341]\n",
      "loss: 0.998126  [137600/175341]\n",
      "loss: 0.273991  [139200/175341]\n",
      "loss: 0.637729  [140800/175341]\n",
      "loss: 0.129911  [142400/175341]\n",
      "loss: 0.543870  [144000/175341]\n",
      "loss: 0.268051  [145600/175341]\n",
      "loss: 0.374179  [147200/175341]\n",
      "loss: 1.124667  [148800/175341]\n",
      "loss: 0.744338  [150400/175341]\n",
      "loss: 0.186156  [152000/175341]\n",
      "loss: 0.527609  [153600/175341]\n",
      "loss: 0.702120  [155200/175341]\n",
      "loss: 0.762793  [156800/175341]\n",
      "loss: 0.383919  [158400/175341]\n",
      "loss: 0.366126  [160000/175341]\n",
      "loss: 0.714037  [161600/175341]\n",
      "loss: 0.859493  [163200/175341]\n",
      "loss: 0.621495  [164800/175341]\n",
      "loss: 0.677598  [166400/175341]\n",
      "loss: 0.404143  [168000/175341]\n",
      "loss: 0.791673  [169600/175341]\n",
      "loss: 0.356681  [171200/175341]\n",
      "loss: 0.739694  [172800/175341]\n",
      "loss: 0.781475  [174400/175341]\n",
      "Train Accuracy: 78.4603%\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.625626, F1-score: 73.20% \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.430061  [    0/175341]\n",
      "loss: 0.622707  [ 1600/175341]\n",
      "loss: 0.664136  [ 3200/175341]\n",
      "loss: 0.668704  [ 4800/175341]\n",
      "loss: 0.484851  [ 6400/175341]\n",
      "loss: 0.485619  [ 8000/175341]\n",
      "loss: 0.744832  [ 9600/175341]\n",
      "loss: 1.024463  [11200/175341]\n",
      "loss: 0.714465  [12800/175341]\n",
      "loss: 1.118078  [14400/175341]\n",
      "loss: 0.412776  [16000/175341]\n",
      "loss: 0.247151  [17600/175341]\n",
      "loss: 0.943564  [19200/175341]\n",
      "loss: 0.565198  [20800/175341]\n",
      "loss: 0.309408  [22400/175341]\n",
      "loss: 0.367217  [24000/175341]\n",
      "loss: 0.513991  [25600/175341]\n",
      "loss: 0.840165  [27200/175341]\n",
      "loss: 0.620065  [28800/175341]\n",
      "loss: 0.484295  [30400/175341]\n",
      "loss: 0.611984  [32000/175341]\n",
      "loss: 0.219979  [33600/175341]\n",
      "loss: 0.646760  [35200/175341]\n",
      "loss: 0.661877  [36800/175341]\n",
      "loss: 0.466786  [38400/175341]\n",
      "loss: 0.779835  [40000/175341]\n",
      "loss: 0.349249  [41600/175341]\n",
      "loss: 0.406664  [43200/175341]\n",
      "loss: 0.303124  [44800/175341]\n",
      "loss: 0.332006  [46400/175341]\n",
      "loss: 0.953471  [48000/175341]\n",
      "loss: 0.579940  [49600/175341]\n",
      "loss: 0.767048  [51200/175341]\n",
      "loss: 0.385200  [52800/175341]\n",
      "loss: 0.243540  [54400/175341]\n",
      "loss: 0.807318  [56000/175341]\n",
      "loss: 0.814450  [57600/175341]\n",
      "loss: 1.032232  [59200/175341]\n",
      "loss: 0.245797  [60800/175341]\n",
      "loss: 0.429058  [62400/175341]\n",
      "loss: 0.245779  [64000/175341]\n",
      "loss: 0.730740  [65600/175341]\n",
      "loss: 0.438287  [67200/175341]\n",
      "loss: 0.510627  [68800/175341]\n",
      "loss: 0.499744  [70400/175341]\n",
      "loss: 0.391324  [72000/175341]\n",
      "loss: 0.847385  [73600/175341]\n",
      "loss: 0.483721  [75200/175341]\n",
      "loss: 0.756173  [76800/175341]\n",
      "loss: 0.162424  [78400/175341]\n",
      "loss: 0.369750  [80000/175341]\n",
      "loss: 0.295341  [81600/175341]\n",
      "loss: 0.662055  [83200/175341]\n",
      "loss: 1.003590  [84800/175341]\n",
      "loss: 0.268154  [86400/175341]\n",
      "loss: 0.926294  [88000/175341]\n",
      "loss: 0.570503  [89600/175341]\n",
      "loss: 0.973835  [91200/175341]\n",
      "loss: 0.280504  [92800/175341]\n",
      "loss: 0.360715  [94400/175341]\n",
      "loss: 0.744223  [96000/175341]\n",
      "loss: 0.303263  [97600/175341]\n",
      "loss: 0.895120  [99200/175341]\n",
      "loss: 0.414542  [100800/175341]\n",
      "loss: 0.272349  [102400/175341]\n",
      "loss: 0.685227  [104000/175341]\n",
      "loss: 0.763163  [105600/175341]\n",
      "loss: 0.606875  [107200/175341]\n",
      "loss: 0.480406  [108800/175341]\n",
      "loss: 0.696231  [110400/175341]\n",
      "loss: 0.581685  [112000/175341]\n",
      "loss: 0.692008  [113600/175341]\n",
      "loss: 0.767822  [115200/175341]\n",
      "loss: 0.476293  [116800/175341]\n",
      "loss: 0.676507  [118400/175341]\n",
      "loss: 0.468772  [120000/175341]\n",
      "loss: 0.645934  [121600/175341]\n",
      "loss: 0.963062  [123200/175341]\n",
      "loss: 0.866179  [124800/175341]\n",
      "loss: 0.607232  [126400/175341]\n",
      "loss: 0.769122  [128000/175341]\n",
      "loss: 1.017804  [129600/175341]\n",
      "loss: 0.364162  [131200/175341]\n",
      "loss: 0.370277  [132800/175341]\n",
      "loss: 0.697059  [134400/175341]\n",
      "loss: 0.291973  [136000/175341]\n",
      "loss: 0.244360  [137600/175341]\n",
      "loss: 0.937109  [139200/175341]\n",
      "loss: 0.469807  [140800/175341]\n",
      "loss: 0.825633  [142400/175341]\n",
      "loss: 0.396865  [144000/175341]\n",
      "loss: 0.329420  [145600/175341]\n",
      "loss: 0.584457  [147200/175341]\n",
      "loss: 0.651006  [148800/175341]\n",
      "loss: 0.474173  [150400/175341]\n",
      "loss: 0.509105  [152000/175341]\n",
      "loss: 0.556423  [153600/175341]\n",
      "loss: 0.338565  [155200/175341]\n",
      "loss: 0.332404  [156800/175341]\n",
      "loss: 0.352354  [158400/175341]\n",
      "loss: 0.437850  [160000/175341]\n",
      "loss: 0.162582  [161600/175341]\n",
      "loss: 0.791866  [163200/175341]\n",
      "loss: 0.516997  [164800/175341]\n",
      "loss: 0.237021  [166400/175341]\n",
      "loss: 0.704255  [168000/175341]\n",
      "loss: 0.930575  [169600/175341]\n",
      "loss: 0.688601  [171200/175341]\n",
      "loss: 0.392646  [172800/175341]\n",
      "loss: 0.528700  [174400/175341]\n",
      "Train Accuracy: 78.6063%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.610169, F1-score: 74.00% \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.475128  [    0/175341]\n",
      "loss: 0.300426  [ 1600/175341]\n",
      "loss: 0.979395  [ 3200/175341]\n",
      "loss: 0.365260  [ 4800/175341]\n",
      "loss: 0.445220  [ 6400/175341]\n",
      "loss: 0.480380  [ 8000/175341]\n",
      "loss: 0.737890  [ 9600/175341]\n",
      "loss: 0.521527  [11200/175341]\n",
      "loss: 0.542206  [12800/175341]\n",
      "loss: 0.764114  [14400/175341]\n",
      "loss: 0.706769  [16000/175341]\n",
      "loss: 0.556451  [17600/175341]\n",
      "loss: 0.692852  [19200/175341]\n",
      "loss: 0.666219  [20800/175341]\n",
      "loss: 0.294434  [22400/175341]\n",
      "loss: 0.783037  [24000/175341]\n",
      "loss: 0.994025  [25600/175341]\n",
      "loss: 0.526654  [27200/175341]\n",
      "loss: 0.778230  [28800/175341]\n",
      "loss: 0.478809  [30400/175341]\n",
      "loss: 0.322173  [32000/175341]\n",
      "loss: 0.671660  [33600/175341]\n",
      "loss: 0.578115  [35200/175341]\n",
      "loss: 0.330312  [36800/175341]\n",
      "loss: 0.809869  [38400/175341]\n",
      "loss: 0.623577  [40000/175341]\n",
      "loss: 0.361392  [41600/175341]\n",
      "loss: 0.557455  [43200/175341]\n",
      "loss: 0.396649  [44800/175341]\n",
      "loss: 0.753007  [46400/175341]\n",
      "loss: 0.481783  [48000/175341]\n",
      "loss: 0.515927  [49600/175341]\n",
      "loss: 0.418019  [51200/175341]\n",
      "loss: 0.900533  [52800/175341]\n",
      "loss: 0.249405  [54400/175341]\n",
      "loss: 0.476387  [56000/175341]\n",
      "loss: 0.786523  [57600/175341]\n",
      "loss: 0.805099  [59200/175341]\n",
      "loss: 0.536132  [60800/175341]\n",
      "loss: 0.844515  [62400/175341]\n",
      "loss: 0.208674  [64000/175341]\n",
      "loss: 0.567637  [65600/175341]\n",
      "loss: 0.912265  [67200/175341]\n",
      "loss: 0.433829  [68800/175341]\n",
      "loss: 0.366099  [70400/175341]\n",
      "loss: 0.935535  [72000/175341]\n",
      "loss: 0.419688  [73600/175341]\n",
      "loss: 0.574183  [75200/175341]\n",
      "loss: 0.304434  [76800/175341]\n",
      "loss: 0.535210  [78400/175341]\n",
      "loss: 0.358630  [80000/175341]\n",
      "loss: 0.704168  [81600/175341]\n",
      "loss: 0.203146  [83200/175341]\n",
      "loss: 0.528336  [84800/175341]\n",
      "loss: 0.890059  [86400/175341]\n",
      "loss: 0.519333  [88000/175341]\n",
      "loss: 0.600593  [89600/175341]\n",
      "loss: 0.188073  [91200/175341]\n",
      "loss: 1.110351  [92800/175341]\n",
      "loss: 0.416333  [94400/175341]\n",
      "loss: 0.459455  [96000/175341]\n",
      "loss: 0.367535  [97600/175341]\n",
      "loss: 0.370183  [99200/175341]\n",
      "loss: 0.166530  [100800/175341]\n",
      "loss: 0.610356  [102400/175341]\n",
      "loss: 0.389990  [104000/175341]\n",
      "loss: 0.453985  [105600/175341]\n",
      "loss: 0.413526  [107200/175341]\n",
      "loss: 0.746649  [108800/175341]\n",
      "loss: 0.530690  [110400/175341]\n",
      "loss: 0.450650  [112000/175341]\n",
      "loss: 0.524739  [113600/175341]\n",
      "loss: 0.374528  [115200/175341]\n",
      "loss: 0.395465  [116800/175341]\n",
      "loss: 0.703987  [118400/175341]\n",
      "loss: 0.564768  [120000/175341]\n",
      "loss: 0.519505  [121600/175341]\n",
      "loss: 0.530851  [123200/175341]\n",
      "loss: 0.346869  [124800/175341]\n",
      "loss: 0.663307  [126400/175341]\n",
      "loss: 0.434392  [128000/175341]\n",
      "loss: 0.743295  [129600/175341]\n",
      "loss: 0.666819  [131200/175341]\n",
      "loss: 0.744481  [132800/175341]\n",
      "loss: 0.257548  [134400/175341]\n",
      "loss: 0.561854  [136000/175341]\n",
      "loss: 0.454068  [137600/175341]\n",
      "loss: 0.614069  [139200/175341]\n",
      "loss: 0.925512  [140800/175341]\n",
      "loss: 0.446668  [142400/175341]\n",
      "loss: 0.940459  [144000/175341]\n",
      "loss: 0.303979  [145600/175341]\n",
      "loss: 0.609828  [147200/175341]\n",
      "loss: 0.500992  [148800/175341]\n",
      "loss: 0.442942  [150400/175341]\n",
      "loss: 0.824970  [152000/175341]\n",
      "loss: 0.520381  [153600/175341]\n",
      "loss: 0.576056  [155200/175341]\n",
      "loss: 0.557825  [156800/175341]\n",
      "loss: 0.526118  [158400/175341]\n",
      "loss: 0.601273  [160000/175341]\n",
      "loss: 1.173911  [161600/175341]\n",
      "loss: 0.997927  [163200/175341]\n",
      "loss: 0.348814  [164800/175341]\n",
      "loss: 0.428521  [166400/175341]\n",
      "loss: 0.156328  [168000/175341]\n",
      "loss: 0.604070  [169600/175341]\n",
      "loss: 0.424742  [171200/175341]\n",
      "loss: 0.642508  [172800/175341]\n",
      "loss: 0.234519  [174400/175341]\n",
      "Train Accuracy: 78.6547%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.618956, F1-score: 74.03% \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.488748  [    0/175341]\n",
      "loss: 0.578457  [ 1600/175341]\n",
      "loss: 0.349158  [ 3200/175341]\n",
      "loss: 0.668521  [ 4800/175341]\n",
      "loss: 0.358958  [ 6400/175341]\n",
      "loss: 0.527538  [ 8000/175341]\n",
      "loss: 0.337115  [ 9600/175341]\n",
      "loss: 1.079227  [11200/175341]\n",
      "loss: 0.598730  [12800/175341]\n",
      "loss: 0.673297  [14400/175341]\n",
      "loss: 0.538853  [16000/175341]\n",
      "loss: 0.353447  [17600/175341]\n",
      "loss: 0.477912  [19200/175341]\n",
      "loss: 0.489118  [20800/175341]\n",
      "loss: 0.465122  [22400/175341]\n",
      "loss: 1.127076  [24000/175341]\n",
      "loss: 0.557444  [25600/175341]\n",
      "loss: 0.869929  [27200/175341]\n",
      "loss: 0.700391  [28800/175341]\n",
      "loss: 0.503161  [30400/175341]\n",
      "loss: 0.851481  [32000/175341]\n",
      "loss: 0.997044  [33600/175341]\n",
      "loss: 0.401244  [35200/175341]\n",
      "loss: 0.638369  [36800/175341]\n",
      "loss: 0.427078  [38400/175341]\n",
      "loss: 0.562807  [40000/175341]\n",
      "loss: 0.737395  [41600/175341]\n",
      "loss: 0.766849  [43200/175341]\n",
      "loss: 0.381845  [44800/175341]\n",
      "loss: 0.530277  [46400/175341]\n",
      "loss: 0.651328  [48000/175341]\n",
      "loss: 0.704843  [49600/175341]\n",
      "loss: 0.533357  [51200/175341]\n",
      "loss: 0.789964  [52800/175341]\n",
      "loss: 0.328507  [54400/175341]\n",
      "loss: 0.501606  [56000/175341]\n",
      "loss: 0.462549  [57600/175341]\n",
      "loss: 0.620317  [59200/175341]\n",
      "loss: 0.764845  [60800/175341]\n",
      "loss: 1.009455  [62400/175341]\n",
      "loss: 0.641120  [64000/175341]\n",
      "loss: 0.375717  [65600/175341]\n",
      "loss: 1.093410  [67200/175341]\n",
      "loss: 0.663870  [68800/175341]\n",
      "loss: 0.768811  [70400/175341]\n",
      "loss: 0.465775  [72000/175341]\n",
      "loss: 0.491622  [73600/175341]\n",
      "loss: 0.469110  [75200/175341]\n",
      "loss: 0.587172  [76800/175341]\n",
      "loss: 0.401254  [78400/175341]\n",
      "loss: 0.840613  [80000/175341]\n",
      "loss: 0.548081  [81600/175341]\n",
      "loss: 0.618805  [83200/175341]\n",
      "loss: 0.513560  [84800/175341]\n",
      "loss: 0.389608  [86400/175341]\n",
      "loss: 0.947630  [88000/175341]\n",
      "loss: 0.172865  [89600/175341]\n",
      "loss: 0.742444  [91200/175341]\n",
      "loss: 0.752283  [92800/175341]\n",
      "loss: 0.519266  [94400/175341]\n",
      "loss: 0.453711  [96000/175341]\n",
      "loss: 0.740765  [97600/175341]\n",
      "loss: 0.603849  [99200/175341]\n",
      "loss: 0.567081  [100800/175341]\n",
      "loss: 0.702641  [102400/175341]\n",
      "loss: 0.544458  [104000/175341]\n",
      "loss: 0.610416  [105600/175341]\n",
      "loss: 0.695814  [107200/175341]\n",
      "loss: 0.764240  [108800/175341]\n",
      "loss: 0.570468  [110400/175341]\n",
      "loss: 0.312038  [112000/175341]\n",
      "loss: 0.477349  [113600/175341]\n",
      "loss: 0.544263  [115200/175341]\n",
      "loss: 0.371588  [116800/175341]\n",
      "loss: 0.366876  [118400/175341]\n",
      "loss: 0.370655  [120000/175341]\n",
      "loss: 0.495140  [121600/175341]\n",
      "loss: 0.356444  [123200/175341]\n",
      "loss: 0.363723  [124800/175341]\n",
      "loss: 0.155409  [126400/175341]\n",
      "loss: 0.413015  [128000/175341]\n",
      "loss: 0.428213  [129600/175341]\n",
      "loss: 0.522977  [131200/175341]\n",
      "loss: 0.900359  [132800/175341]\n",
      "loss: 0.507817  [134400/175341]\n",
      "loss: 0.501015  [136000/175341]\n",
      "loss: 0.871412  [137600/175341]\n",
      "loss: 0.410172  [139200/175341]\n",
      "loss: 0.632697  [140800/175341]\n",
      "loss: 0.820283  [142400/175341]\n",
      "loss: 0.701303  [144000/175341]\n",
      "loss: 0.466649  [145600/175341]\n",
      "loss: 0.901095  [147200/175341]\n",
      "loss: 0.394680  [148800/175341]\n",
      "loss: 0.231672  [150400/175341]\n",
      "loss: 0.475455  [152000/175341]\n",
      "loss: 0.428655  [153600/175341]\n",
      "loss: 0.434706  [155200/175341]\n",
      "loss: 0.913590  [156800/175341]\n",
      "loss: 0.136374  [158400/175341]\n",
      "loss: 0.881316  [160000/175341]\n",
      "loss: 0.548142  [161600/175341]\n",
      "loss: 0.895261  [163200/175341]\n",
      "loss: 0.526224  [164800/175341]\n",
      "loss: 0.732325  [166400/175341]\n",
      "loss: 0.750915  [168000/175341]\n",
      "loss: 0.377757  [169600/175341]\n",
      "loss: 0.430487  [171200/175341]\n",
      "loss: 0.317479  [172800/175341]\n",
      "loss: 0.549887  [174400/175341]\n",
      "Train Accuracy: 78.7705%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.616192, F1-score: 73.95% \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.526076  [    0/175341]\n",
      "loss: 0.301461  [ 1600/175341]\n",
      "loss: 0.410059  [ 3200/175341]\n",
      "loss: 0.619423  [ 4800/175341]\n",
      "loss: 0.508328  [ 6400/175341]\n",
      "loss: 0.524184  [ 8000/175341]\n",
      "loss: 0.503236  [ 9600/175341]\n",
      "loss: 0.624645  [11200/175341]\n",
      "loss: 0.323829  [12800/175341]\n",
      "loss: 0.591629  [14400/175341]\n",
      "loss: 0.384798  [16000/175341]\n",
      "loss: 0.810496  [17600/175341]\n",
      "loss: 0.247894  [19200/175341]\n",
      "loss: 0.401769  [20800/175341]\n",
      "loss: 0.732153  [22400/175341]\n",
      "loss: 0.462426  [24000/175341]\n",
      "loss: 0.861169  [25600/175341]\n",
      "loss: 0.967086  [27200/175341]\n",
      "loss: 0.705949  [28800/175341]\n",
      "loss: 0.317509  [30400/175341]\n",
      "loss: 1.045211  [32000/175341]\n",
      "loss: 0.294502  [33600/175341]\n",
      "loss: 0.660175  [35200/175341]\n",
      "loss: 0.727108  [36800/175341]\n",
      "loss: 0.213494  [38400/175341]\n",
      "loss: 0.854152  [40000/175341]\n",
      "loss: 0.368848  [41600/175341]\n",
      "loss: 0.376970  [43200/175341]\n",
      "loss: 0.561649  [44800/175341]\n",
      "loss: 0.767260  [46400/175341]\n",
      "loss: 0.332778  [48000/175341]\n",
      "loss: 0.611819  [49600/175341]\n",
      "loss: 0.198573  [51200/175341]\n",
      "loss: 0.268192  [52800/175341]\n",
      "loss: 0.582399  [54400/175341]\n",
      "loss: 0.313110  [56000/175341]\n",
      "loss: 0.712018  [57600/175341]\n",
      "loss: 0.710185  [59200/175341]\n",
      "loss: 0.616108  [60800/175341]\n",
      "loss: 1.020271  [62400/175341]\n",
      "loss: 0.655233  [64000/175341]\n",
      "loss: 0.362605  [65600/175341]\n",
      "loss: 0.639947  [67200/175341]\n",
      "loss: 0.543770  [68800/175341]\n",
      "loss: 0.426502  [70400/175341]\n",
      "loss: 0.622641  [72000/175341]\n",
      "loss: 0.255675  [73600/175341]\n",
      "loss: 0.513188  [75200/175341]\n",
      "loss: 0.463783  [76800/175341]\n",
      "loss: 0.336806  [78400/175341]\n",
      "loss: 0.308861  [80000/175341]\n",
      "loss: 0.279217  [81600/175341]\n",
      "loss: 0.220350  [83200/175341]\n",
      "loss: 0.425137  [84800/175341]\n",
      "loss: 0.336359  [86400/175341]\n",
      "loss: 0.446461  [88000/175341]\n",
      "loss: 0.772980  [89600/175341]\n",
      "loss: 0.422681  [91200/175341]\n",
      "loss: 0.737180  [92800/175341]\n",
      "loss: 0.432256  [94400/175341]\n",
      "loss: 0.544995  [96000/175341]\n",
      "loss: 0.488812  [97600/175341]\n",
      "loss: 0.291729  [99200/175341]\n",
      "loss: 0.406516  [100800/175341]\n",
      "loss: 0.593048  [102400/175341]\n",
      "loss: 0.601332  [104000/175341]\n",
      "loss: 0.348973  [105600/175341]\n",
      "loss: 0.182568  [107200/175341]\n",
      "loss: 0.417421  [108800/175341]\n",
      "loss: 0.720637  [110400/175341]\n",
      "loss: 0.415962  [112000/175341]\n",
      "loss: 0.602525  [113600/175341]\n",
      "loss: 0.581792  [115200/175341]\n",
      "loss: 0.427755  [116800/175341]\n",
      "loss: 1.089983  [118400/175341]\n",
      "loss: 0.617767  [120000/175341]\n",
      "loss: 0.329069  [121600/175341]\n",
      "loss: 0.723496  [123200/175341]\n",
      "loss: 0.849959  [124800/175341]\n",
      "loss: 0.593672  [126400/175341]\n",
      "loss: 0.653918  [128000/175341]\n",
      "loss: 0.507823  [129600/175341]\n",
      "loss: 0.542521  [131200/175341]\n",
      "loss: 0.410931  [132800/175341]\n",
      "loss: 0.480337  [134400/175341]\n",
      "loss: 0.456387  [136000/175341]\n",
      "loss: 0.552631  [137600/175341]\n",
      "loss: 0.442835  [139200/175341]\n",
      "loss: 0.687590  [140800/175341]\n",
      "loss: 0.387762  [142400/175341]\n",
      "loss: 0.287935  [144000/175341]\n",
      "loss: 0.734205  [145600/175341]\n",
      "loss: 0.220300  [147200/175341]\n",
      "loss: 0.844624  [148800/175341]\n",
      "loss: 0.247522  [150400/175341]\n",
      "loss: 0.686360  [152000/175341]\n",
      "loss: 0.412626  [153600/175341]\n",
      "loss: 1.150191  [155200/175341]\n",
      "loss: 0.680225  [156800/175341]\n",
      "loss: 1.131783  [158400/175341]\n",
      "loss: 0.668674  [160000/175341]\n",
      "loss: 0.548744  [161600/175341]\n",
      "loss: 0.707846  [163200/175341]\n",
      "loss: 0.298323  [164800/175341]\n",
      "loss: 0.219918  [166400/175341]\n",
      "loss: 0.496767  [168000/175341]\n",
      "loss: 0.530813  [169600/175341]\n",
      "loss: 0.510782  [171200/175341]\n",
      "loss: 0.520066  [172800/175341]\n",
      "loss: 0.369550  [174400/175341]\n",
      "Train Accuracy: 78.7859%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.613041, F1-score: 73.97% \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.303819  [    0/175341]\n",
      "loss: 0.488798  [ 1600/175341]\n",
      "loss: 0.638565  [ 3200/175341]\n",
      "loss: 0.857331  [ 4800/175341]\n",
      "loss: 0.487281  [ 6400/175341]\n",
      "loss: 0.586957  [ 8000/175341]\n",
      "loss: 0.292265  [ 9600/175341]\n",
      "loss: 0.510812  [11200/175341]\n",
      "loss: 0.336547  [12800/175341]\n",
      "loss: 0.326950  [14400/175341]\n",
      "loss: 0.189347  [16000/175341]\n",
      "loss: 0.477489  [17600/175341]\n",
      "loss: 0.333539  [19200/175341]\n",
      "loss: 0.636228  [20800/175341]\n",
      "loss: 0.415357  [22400/175341]\n",
      "loss: 0.229612  [24000/175341]\n",
      "loss: 0.813587  [25600/175341]\n",
      "loss: 0.408144  [27200/175341]\n",
      "loss: 0.436206  [28800/175341]\n",
      "loss: 0.410295  [30400/175341]\n",
      "loss: 0.449227  [32000/175341]\n",
      "loss: 0.235264  [33600/175341]\n",
      "loss: 0.997141  [35200/175341]\n",
      "loss: 0.636096  [36800/175341]\n",
      "loss: 0.816782  [38400/175341]\n",
      "loss: 0.445203  [40000/175341]\n",
      "loss: 0.381547  [41600/175341]\n",
      "loss: 0.211874  [43200/175341]\n",
      "loss: 0.681251  [44800/175341]\n",
      "loss: 0.213191  [46400/175341]\n",
      "loss: 1.102547  [48000/175341]\n",
      "loss: 1.327129  [49600/175341]\n",
      "loss: 0.282477  [51200/175341]\n",
      "loss: 1.142502  [52800/175341]\n",
      "loss: 0.813979  [54400/175341]\n",
      "loss: 0.703987  [56000/175341]\n",
      "loss: 0.780755  [57600/175341]\n",
      "loss: 0.500049  [59200/175341]\n",
      "loss: 0.915438  [60800/175341]\n",
      "loss: 0.883847  [62400/175341]\n",
      "loss: 0.726667  [64000/175341]\n",
      "loss: 0.397701  [65600/175341]\n",
      "loss: 0.690764  [67200/175341]\n",
      "loss: 0.665782  [68800/175341]\n",
      "loss: 0.523954  [70400/175341]\n",
      "loss: 1.036492  [72000/175341]\n",
      "loss: 1.116065  [73600/175341]\n",
      "loss: 0.224116  [75200/175341]\n",
      "loss: 0.485350  [76800/175341]\n",
      "loss: 0.263700  [78400/175341]\n",
      "loss: 0.443332  [80000/175341]\n",
      "loss: 0.521514  [81600/175341]\n",
      "loss: 0.265238  [83200/175341]\n",
      "loss: 0.314238  [84800/175341]\n",
      "loss: 0.357163  [86400/175341]\n",
      "loss: 0.971566  [88000/175341]\n",
      "loss: 0.874596  [89600/175341]\n",
      "loss: 0.254542  [91200/175341]\n",
      "loss: 0.436695  [92800/175341]\n",
      "loss: 1.073324  [94400/175341]\n",
      "loss: 0.684158  [96000/175341]\n",
      "loss: 0.797514  [97600/175341]\n",
      "loss: 0.392886  [99200/175341]\n",
      "loss: 0.535539  [100800/175341]\n",
      "loss: 0.562693  [102400/175341]\n",
      "loss: 0.237972  [104000/175341]\n",
      "loss: 0.197706  [105600/175341]\n",
      "loss: 0.739047  [107200/175341]\n",
      "loss: 0.548038  [108800/175341]\n",
      "loss: 0.172779  [110400/175341]\n",
      "loss: 0.348962  [112000/175341]\n",
      "loss: 0.926338  [113600/175341]\n",
      "loss: 0.286163  [115200/175341]\n",
      "loss: 0.700433  [116800/175341]\n",
      "loss: 0.701707  [118400/175341]\n",
      "loss: 0.351763  [120000/175341]\n",
      "loss: 0.433205  [121600/175341]\n",
      "loss: 0.639187  [123200/175341]\n",
      "loss: 0.667875  [124800/175341]\n",
      "loss: 0.421253  [126400/175341]\n",
      "loss: 0.447933  [128000/175341]\n",
      "loss: 0.917244  [129600/175341]\n",
      "loss: 1.165630  [131200/175341]\n",
      "loss: 0.402515  [132800/175341]\n",
      "loss: 0.792554  [134400/175341]\n",
      "loss: 0.436148  [136000/175341]\n",
      "loss: 0.687564  [137600/175341]\n",
      "loss: 0.648334  [139200/175341]\n",
      "loss: 0.614763  [140800/175341]\n",
      "loss: 0.439001  [142400/175341]\n",
      "loss: 0.623083  [144000/175341]\n",
      "loss: 0.517026  [145600/175341]\n",
      "loss: 0.889790  [147200/175341]\n",
      "loss: 0.305809  [148800/175341]\n",
      "loss: 0.482772  [150400/175341]\n",
      "loss: 0.827207  [152000/175341]\n",
      "loss: 0.816212  [153600/175341]\n",
      "loss: 0.414803  [155200/175341]\n",
      "loss: 0.620314  [156800/175341]\n",
      "loss: 0.365726  [158400/175341]\n",
      "loss: 0.227889  [160000/175341]\n",
      "loss: 0.601726  [161600/175341]\n",
      "loss: 0.558700  [163200/175341]\n",
      "loss: 0.736985  [164800/175341]\n",
      "loss: 0.247890  [166400/175341]\n",
      "loss: 0.419369  [168000/175341]\n",
      "loss: 0.804878  [169600/175341]\n",
      "loss: 0.415599  [171200/175341]\n",
      "loss: 0.307602  [172800/175341]\n",
      "loss: 0.476230  [174400/175341]\n",
      "Train Accuracy: 78.8789%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.621658, F1-score: 73.88% \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.684963  [    0/175341]\n",
      "loss: 0.620536  [ 1600/175341]\n",
      "loss: 0.293475  [ 3200/175341]\n",
      "loss: 0.399808  [ 4800/175341]\n",
      "loss: 0.491553  [ 6400/175341]\n",
      "loss: 0.660778  [ 8000/175341]\n",
      "loss: 0.305100  [ 9600/175341]\n",
      "loss: 0.833154  [11200/175341]\n",
      "loss: 0.579051  [12800/175341]\n",
      "loss: 0.539310  [14400/175341]\n",
      "loss: 0.982639  [16000/175341]\n",
      "loss: 0.203347  [17600/175341]\n",
      "loss: 0.368589  [19200/175341]\n",
      "loss: 0.370792  [20800/175341]\n",
      "loss: 0.433961  [22400/175341]\n",
      "loss: 0.527278  [24000/175341]\n",
      "loss: 0.596937  [25600/175341]\n",
      "loss: 0.445878  [27200/175341]\n",
      "loss: 0.875594  [28800/175341]\n",
      "loss: 0.451020  [30400/175341]\n",
      "loss: 0.887134  [32000/175341]\n",
      "loss: 0.522956  [33600/175341]\n",
      "loss: 0.620946  [35200/175341]\n",
      "loss: 0.222569  [36800/175341]\n",
      "loss: 0.359403  [38400/175341]\n",
      "loss: 0.431216  [40000/175341]\n",
      "loss: 0.414010  [41600/175341]\n",
      "loss: 0.381186  [43200/175341]\n",
      "loss: 0.157711  [44800/175341]\n",
      "loss: 0.268484  [46400/175341]\n",
      "loss: 0.596283  [48000/175341]\n",
      "loss: 0.540940  [49600/175341]\n",
      "loss: 0.346110  [51200/175341]\n",
      "loss: 0.433314  [52800/175341]\n",
      "loss: 0.706034  [54400/175341]\n",
      "loss: 0.511282  [56000/175341]\n",
      "loss: 0.746171  [57600/175341]\n",
      "loss: 0.421152  [59200/175341]\n",
      "loss: 0.625241  [60800/175341]\n",
      "loss: 0.662451  [62400/175341]\n",
      "loss: 0.321752  [64000/175341]\n",
      "loss: 0.585719  [65600/175341]\n",
      "loss: 0.831204  [67200/175341]\n",
      "loss: 0.628757  [68800/175341]\n",
      "loss: 0.425378  [70400/175341]\n",
      "loss: 0.573817  [72000/175341]\n",
      "loss: 0.221415  [73600/175341]\n",
      "loss: 0.643405  [75200/175341]\n",
      "loss: 0.402710  [76800/175341]\n",
      "loss: 0.813326  [78400/175341]\n",
      "loss: 0.649601  [80000/175341]\n",
      "loss: 0.429650  [81600/175341]\n",
      "loss: 0.935547  [83200/175341]\n",
      "loss: 0.748147  [84800/175341]\n",
      "loss: 0.232224  [86400/175341]\n",
      "loss: 0.696754  [88000/175341]\n",
      "loss: 0.814460  [89600/175341]\n",
      "loss: 0.389707  [91200/175341]\n",
      "loss: 0.407185  [92800/175341]\n",
      "loss: 0.256788  [94400/175341]\n",
      "loss: 0.191302  [96000/175341]\n",
      "loss: 0.487800  [97600/175341]\n",
      "loss: 0.557536  [99200/175341]\n",
      "loss: 0.769532  [100800/175341]\n",
      "loss: 0.442449  [102400/175341]\n",
      "loss: 0.211008  [104000/175341]\n",
      "loss: 0.370852  [105600/175341]\n",
      "loss: 0.464347  [107200/175341]\n",
      "loss: 0.365551  [108800/175341]\n",
      "loss: 0.494273  [110400/175341]\n",
      "loss: 0.763913  [112000/175341]\n",
      "loss: 0.780973  [113600/175341]\n",
      "loss: 0.563931  [115200/175341]\n",
      "loss: 0.663401  [116800/175341]\n",
      "loss: 0.353498  [118400/175341]\n",
      "loss: 0.496382  [120000/175341]\n",
      "loss: 1.081900  [121600/175341]\n",
      "loss: 0.375375  [123200/175341]\n",
      "loss: 0.273716  [124800/175341]\n",
      "loss: 0.665120  [126400/175341]\n",
      "loss: 0.555982  [128000/175341]\n",
      "loss: 0.473233  [129600/175341]\n",
      "loss: 0.480609  [131200/175341]\n",
      "loss: 0.673792  [132800/175341]\n",
      "loss: 0.355015  [134400/175341]\n",
      "loss: 0.207293  [136000/175341]\n",
      "loss: 0.046316  [137600/175341]\n",
      "loss: 0.433412  [139200/175341]\n",
      "loss: 0.737270  [140800/175341]\n",
      "loss: 0.443928  [142400/175341]\n",
      "loss: 0.479873  [144000/175341]\n",
      "loss: 0.128339  [145600/175341]\n",
      "loss: 0.420812  [147200/175341]\n",
      "loss: 0.826528  [148800/175341]\n",
      "loss: 0.753214  [150400/175341]\n",
      "loss: 0.306988  [152000/175341]\n",
      "loss: 0.409541  [153600/175341]\n",
      "loss: 0.892330  [155200/175341]\n",
      "loss: 0.283637  [156800/175341]\n",
      "loss: 0.592911  [158400/175341]\n",
      "loss: 0.619617  [160000/175341]\n",
      "loss: 0.451803  [161600/175341]\n",
      "loss: 0.492853  [163200/175341]\n",
      "loss: 0.997539  [164800/175341]\n",
      "loss: 0.617809  [166400/175341]\n",
      "loss: 0.262049  [168000/175341]\n",
      "loss: 0.536674  [169600/175341]\n",
      "loss: 0.770666  [171200/175341]\n",
      "loss: 0.570772  [172800/175341]\n",
      "loss: 0.348990  [174400/175341]\n",
      "Train Accuracy: 78.9696%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.621675, F1-score: 73.27% \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.528630  [    0/175341]\n",
      "loss: 0.739273  [ 1600/175341]\n",
      "loss: 0.778875  [ 3200/175341]\n",
      "loss: 0.981576  [ 4800/175341]\n",
      "loss: 0.492522  [ 6400/175341]\n",
      "loss: 0.690369  [ 8000/175341]\n",
      "loss: 0.382872  [ 9600/175341]\n",
      "loss: 0.836182  [11200/175341]\n",
      "loss: 0.529391  [12800/175341]\n",
      "loss: 0.302256  [14400/175341]\n",
      "loss: 0.323105  [16000/175341]\n",
      "loss: 0.257480  [17600/175341]\n",
      "loss: 0.573613  [19200/175341]\n",
      "loss: 0.491087  [20800/175341]\n",
      "loss: 0.520757  [22400/175341]\n",
      "loss: 0.501951  [24000/175341]\n",
      "loss: 0.348058  [25600/175341]\n",
      "loss: 0.524703  [27200/175341]\n",
      "loss: 0.497852  [28800/175341]\n",
      "loss: 0.417805  [30400/175341]\n",
      "loss: 0.578565  [32000/175341]\n",
      "loss: 0.345421  [33600/175341]\n",
      "loss: 0.577187  [35200/175341]\n",
      "loss: 0.488787  [36800/175341]\n",
      "loss: 0.425885  [38400/175341]\n",
      "loss: 0.325614  [40000/175341]\n",
      "loss: 0.569864  [41600/175341]\n",
      "loss: 0.664500  [43200/175341]\n",
      "loss: 0.396928  [44800/175341]\n",
      "loss: 0.572090  [46400/175341]\n",
      "loss: 0.489584  [48000/175341]\n",
      "loss: 0.562832  [49600/175341]\n",
      "loss: 1.150541  [51200/175341]\n",
      "loss: 1.021733  [52800/175341]\n",
      "loss: 0.617341  [54400/175341]\n",
      "loss: 0.282177  [56000/175341]\n",
      "loss: 0.450850  [57600/175341]\n",
      "loss: 0.523200  [59200/175341]\n",
      "loss: 0.586844  [60800/175341]\n",
      "loss: 0.798663  [62400/175341]\n",
      "loss: 0.600979  [64000/175341]\n",
      "loss: 0.483547  [65600/175341]\n",
      "loss: 0.384904  [67200/175341]\n",
      "loss: 0.621631  [68800/175341]\n",
      "loss: 0.538448  [70400/175341]\n",
      "loss: 0.512071  [72000/175341]\n",
      "loss: 0.492681  [73600/175341]\n",
      "loss: 0.309155  [75200/175341]\n",
      "loss: 1.202474  [76800/175341]\n",
      "loss: 0.586659  [78400/175341]\n",
      "loss: 0.175975  [80000/175341]\n",
      "loss: 0.428499  [81600/175341]\n",
      "loss: 0.284188  [83200/175341]\n",
      "loss: 0.402382  [84800/175341]\n",
      "loss: 0.665017  [86400/175341]\n",
      "loss: 0.607178  [88000/175341]\n",
      "loss: 0.223391  [89600/175341]\n",
      "loss: 0.674536  [91200/175341]\n",
      "loss: 0.536786  [92800/175341]\n",
      "loss: 0.446895  [94400/175341]\n",
      "loss: 0.577144  [96000/175341]\n",
      "loss: 0.656832  [97600/175341]\n",
      "loss: 0.356012  [99200/175341]\n",
      "loss: 0.195476  [100800/175341]\n",
      "loss: 0.462647  [102400/175341]\n",
      "loss: 0.455483  [104000/175341]\n",
      "loss: 0.269235  [105600/175341]\n",
      "loss: 0.599986  [107200/175341]\n",
      "loss: 0.548494  [108800/175341]\n",
      "loss: 0.175690  [110400/175341]\n",
      "loss: 0.327693  [112000/175341]\n",
      "loss: 0.632386  [113600/175341]\n",
      "loss: 0.433263  [115200/175341]\n",
      "loss: 0.448894  [116800/175341]\n",
      "loss: 0.432232  [118400/175341]\n",
      "loss: 0.339026  [120000/175341]\n",
      "loss: 0.803424  [121600/175341]\n",
      "loss: 0.533407  [123200/175341]\n",
      "loss: 0.478317  [124800/175341]\n",
      "loss: 0.552828  [126400/175341]\n",
      "loss: 0.926218  [128000/175341]\n",
      "loss: 0.522713  [129600/175341]\n",
      "loss: 0.543526  [131200/175341]\n",
      "loss: 0.567010  [132800/175341]\n",
      "loss: 0.381853  [134400/175341]\n",
      "loss: 0.233247  [136000/175341]\n",
      "loss: 0.674771  [137600/175341]\n",
      "loss: 1.004103  [139200/175341]\n",
      "loss: 0.451184  [140800/175341]\n",
      "loss: 0.590510  [142400/175341]\n",
      "loss: 0.219603  [144000/175341]\n",
      "loss: 0.270226  [145600/175341]\n",
      "loss: 0.327791  [147200/175341]\n",
      "loss: 0.553936  [148800/175341]\n",
      "loss: 0.207660  [150400/175341]\n",
      "loss: 0.330340  [152000/175341]\n",
      "loss: 0.515526  [153600/175341]\n",
      "loss: 0.521693  [155200/175341]\n",
      "loss: 0.707803  [156800/175341]\n",
      "loss: 0.747801  [158400/175341]\n",
      "loss: 0.506608  [160000/175341]\n",
      "loss: 0.345332  [161600/175341]\n",
      "loss: 0.476780  [163200/175341]\n",
      "loss: 0.659995  [164800/175341]\n",
      "loss: 0.398214  [166400/175341]\n",
      "loss: 0.572422  [168000/175341]\n",
      "loss: 0.441496  [169600/175341]\n",
      "loss: 1.007966  [171200/175341]\n",
      "loss: 0.780368  [172800/175341]\n",
      "loss: 0.598220  [174400/175341]\n",
      "Train Accuracy: 78.9964%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.606099, F1-score: 74.40% \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.246164  [    0/175341]\n",
      "loss: 0.625144  [ 1600/175341]\n",
      "loss: 0.755026  [ 3200/175341]\n",
      "loss: 0.284306  [ 4800/175341]\n",
      "loss: 0.253309  [ 6400/175341]\n",
      "loss: 0.415220  [ 8000/175341]\n",
      "loss: 0.428995  [ 9600/175341]\n",
      "loss: 0.601109  [11200/175341]\n",
      "loss: 0.608499  [12800/175341]\n",
      "loss: 0.479335  [14400/175341]\n",
      "loss: 0.365228  [16000/175341]\n",
      "loss: 0.608346  [17600/175341]\n",
      "loss: 0.853959  [19200/175341]\n",
      "loss: 0.629592  [20800/175341]\n",
      "loss: 0.838010  [22400/175341]\n",
      "loss: 0.729177  [24000/175341]\n",
      "loss: 0.529949  [25600/175341]\n",
      "loss: 0.550389  [27200/175341]\n",
      "loss: 0.369090  [28800/175341]\n",
      "loss: 0.440501  [30400/175341]\n",
      "loss: 0.896152  [32000/175341]\n",
      "loss: 0.579757  [33600/175341]\n",
      "loss: 1.505104  [35200/175341]\n",
      "loss: 0.164477  [36800/175341]\n",
      "loss: 0.403544  [38400/175341]\n",
      "loss: 0.587021  [40000/175341]\n",
      "loss: 0.558064  [41600/175341]\n",
      "loss: 0.352765  [43200/175341]\n",
      "loss: 0.225109  [44800/175341]\n",
      "loss: 0.277753  [46400/175341]\n",
      "loss: 0.860397  [48000/175341]\n",
      "loss: 0.240097  [49600/175341]\n",
      "loss: 0.778814  [51200/175341]\n",
      "loss: 0.579908  [52800/175341]\n",
      "loss: 0.244027  [54400/175341]\n",
      "loss: 0.567368  [56000/175341]\n",
      "loss: 0.397441  [57600/175341]\n",
      "loss: 0.561117  [59200/175341]\n",
      "loss: 0.363335  [60800/175341]\n",
      "loss: 0.315152  [62400/175341]\n",
      "loss: 0.588601  [64000/175341]\n",
      "loss: 0.476651  [65600/175341]\n",
      "loss: 0.563902  [67200/175341]\n",
      "loss: 0.810679  [68800/175341]\n",
      "loss: 0.593832  [70400/175341]\n",
      "loss: 0.203143  [72000/175341]\n",
      "loss: 0.486762  [73600/175341]\n",
      "loss: 0.594309  [75200/175341]\n",
      "loss: 0.445601  [76800/175341]\n",
      "loss: 0.410645  [78400/175341]\n",
      "loss: 0.555413  [80000/175341]\n",
      "loss: 0.387828  [81600/175341]\n",
      "loss: 0.419999  [83200/175341]\n",
      "loss: 0.518587  [84800/175341]\n",
      "loss: 0.879470  [86400/175341]\n",
      "loss: 0.534476  [88000/175341]\n",
      "loss: 0.494571  [89600/175341]\n",
      "loss: 0.685250  [91200/175341]\n",
      "loss: 0.572156  [92800/175341]\n",
      "loss: 1.206688  [94400/175341]\n",
      "loss: 0.223153  [96000/175341]\n",
      "loss: 0.374494  [97600/175341]\n",
      "loss: 0.367059  [99200/175341]\n",
      "loss: 0.609686  [100800/175341]\n",
      "loss: 0.423754  [102400/175341]\n",
      "loss: 0.613295  [104000/175341]\n",
      "loss: 0.252634  [105600/175341]\n",
      "loss: 0.329170  [107200/175341]\n",
      "loss: 0.279707  [108800/175341]\n",
      "loss: 0.511566  [110400/175341]\n",
      "loss: 0.689196  [112000/175341]\n",
      "loss: 0.586325  [113600/175341]\n",
      "loss: 0.375492  [115200/175341]\n",
      "loss: 0.471700  [116800/175341]\n",
      "loss: 0.779645  [118400/175341]\n",
      "loss: 0.482798  [120000/175341]\n",
      "loss: 0.316385  [121600/175341]\n",
      "loss: 0.808354  [123200/175341]\n",
      "loss: 0.964992  [124800/175341]\n",
      "loss: 0.450660  [126400/175341]\n",
      "loss: 0.719233  [128000/175341]\n",
      "loss: 0.214784  [129600/175341]\n",
      "loss: 0.485652  [131200/175341]\n",
      "loss: 0.292287  [132800/175341]\n",
      "loss: 0.411376  [134400/175341]\n",
      "loss: 0.668760  [136000/175341]\n",
      "loss: 0.560206  [137600/175341]\n",
      "loss: 0.517418  [139200/175341]\n",
      "loss: 0.516674  [140800/175341]\n",
      "loss: 0.447581  [142400/175341]\n",
      "loss: 0.271800  [144000/175341]\n",
      "loss: 0.726149  [145600/175341]\n",
      "loss: 0.625955  [147200/175341]\n",
      "loss: 0.348204  [148800/175341]\n",
      "loss: 0.854140  [150400/175341]\n",
      "loss: 0.385690  [152000/175341]\n",
      "loss: 0.760430  [153600/175341]\n",
      "loss: 0.398056  [155200/175341]\n",
      "loss: 0.633958  [156800/175341]\n",
      "loss: 0.149804  [158400/175341]\n",
      "loss: 0.520591  [160000/175341]\n",
      "loss: 0.694294  [161600/175341]\n",
      "loss: 1.136571  [163200/175341]\n",
      "loss: 0.581711  [164800/175341]\n",
      "loss: 0.830442  [166400/175341]\n",
      "loss: 0.681386  [168000/175341]\n",
      "loss: 0.731147  [169600/175341]\n",
      "loss: 0.614332  [171200/175341]\n",
      "loss: 0.564948  [172800/175341]\n",
      "loss: 0.900099  [174400/175341]\n",
      "Train Accuracy: 79.0363%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.615434, F1-score: 74.00% \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.562488  [    0/175341]\n",
      "loss: 0.620836  [ 1600/175341]\n",
      "loss: 0.410330  [ 3200/175341]\n",
      "loss: 0.549458  [ 4800/175341]\n",
      "loss: 0.288197  [ 6400/175341]\n",
      "loss: 1.306234  [ 8000/175341]\n",
      "loss: 0.300519  [ 9600/175341]\n",
      "loss: 0.515618  [11200/175341]\n",
      "loss: 0.328703  [12800/175341]\n",
      "loss: 0.487806  [14400/175341]\n",
      "loss: 0.268533  [16000/175341]\n",
      "loss: 0.418588  [17600/175341]\n",
      "loss: 0.210472  [19200/175341]\n",
      "loss: 0.721650  [20800/175341]\n",
      "loss: 0.529002  [22400/175341]\n",
      "loss: 0.543816  [24000/175341]\n",
      "loss: 0.934546  [25600/175341]\n",
      "loss: 0.459644  [27200/175341]\n",
      "loss: 0.159505  [28800/175341]\n",
      "loss: 0.845262  [30400/175341]\n",
      "loss: 0.780886  [32000/175341]\n",
      "loss: 0.614831  [33600/175341]\n",
      "loss: 0.176995  [35200/175341]\n",
      "loss: 1.209508  [36800/175341]\n",
      "loss: 0.482914  [38400/175341]\n",
      "loss: 0.362803  [40000/175341]\n",
      "loss: 0.697177  [41600/175341]\n",
      "loss: 0.525855  [43200/175341]\n",
      "loss: 0.340151  [44800/175341]\n",
      "loss: 0.268285  [46400/175341]\n",
      "loss: 0.549689  [48000/175341]\n",
      "loss: 0.491676  [49600/175341]\n",
      "loss: 0.778524  [51200/175341]\n",
      "loss: 0.980621  [52800/175341]\n",
      "loss: 0.383303  [54400/175341]\n",
      "loss: 0.274790  [56000/175341]\n",
      "loss: 0.802888  [57600/175341]\n",
      "loss: 0.600483  [59200/175341]\n",
      "loss: 0.402424  [60800/175341]\n",
      "loss: 0.361619  [62400/175341]\n",
      "loss: 0.397091  [64000/175341]\n",
      "loss: 0.417809  [65600/175341]\n",
      "loss: 0.367167  [67200/175341]\n",
      "loss: 0.757063  [68800/175341]\n",
      "loss: 0.515888  [70400/175341]\n",
      "loss: 0.571867  [72000/175341]\n",
      "loss: 0.496739  [73600/175341]\n",
      "loss: 0.183813  [75200/175341]\n",
      "loss: 0.693662  [76800/175341]\n",
      "loss: 0.518387  [78400/175341]\n",
      "loss: 0.740006  [80000/175341]\n",
      "loss: 0.396158  [81600/175341]\n",
      "loss: 0.186309  [83200/175341]\n",
      "loss: 0.455842  [84800/175341]\n",
      "loss: 0.448462  [86400/175341]\n",
      "loss: 0.270715  [88000/175341]\n",
      "loss: 0.535920  [89600/175341]\n",
      "loss: 0.780269  [91200/175341]\n",
      "loss: 0.405520  [92800/175341]\n",
      "loss: 0.216941  [94400/175341]\n",
      "loss: 0.587348  [96000/175341]\n",
      "loss: 0.673788  [97600/175341]\n",
      "loss: 0.290691  [99200/175341]\n",
      "loss: 0.213589  [100800/175341]\n",
      "loss: 0.821472  [102400/175341]\n",
      "loss: 0.401124  [104000/175341]\n",
      "loss: 0.479248  [105600/175341]\n",
      "loss: 0.983140  [107200/175341]\n",
      "loss: 0.610146  [108800/175341]\n",
      "loss: 0.310976  [110400/175341]\n",
      "loss: 0.284981  [112000/175341]\n",
      "loss: 0.385470  [113600/175341]\n",
      "loss: 0.275235  [115200/175341]\n",
      "loss: 0.437382  [116800/175341]\n",
      "loss: 0.171123  [118400/175341]\n",
      "loss: 0.508048  [120000/175341]\n",
      "loss: 0.409996  [121600/175341]\n",
      "loss: 0.410489  [123200/175341]\n",
      "loss: 0.238733  [124800/175341]\n",
      "loss: 0.378885  [126400/175341]\n",
      "loss: 0.403411  [128000/175341]\n",
      "loss: 0.498217  [129600/175341]\n",
      "loss: 0.251110  [131200/175341]\n",
      "loss: 0.404306  [132800/175341]\n",
      "loss: 0.669776  [134400/175341]\n",
      "loss: 0.575209  [136000/175341]\n",
      "loss: 0.420123  [137600/175341]\n",
      "loss: 0.544557  [139200/175341]\n",
      "loss: 0.369983  [140800/175341]\n",
      "loss: 0.546594  [142400/175341]\n",
      "loss: 0.303554  [144000/175341]\n",
      "loss: 0.052268  [145600/175341]\n",
      "loss: 0.538742  [147200/175341]\n",
      "loss: 0.748592  [148800/175341]\n",
      "loss: 0.933905  [150400/175341]\n",
      "loss: 0.329130  [152000/175341]\n",
      "loss: 0.345878  [153600/175341]\n",
      "loss: 1.060787  [155200/175341]\n",
      "loss: 0.393182  [156800/175341]\n",
      "loss: 0.630419  [158400/175341]\n",
      "loss: 0.391648  [160000/175341]\n",
      "loss: 0.996282  [161600/175341]\n",
      "loss: 0.261009  [163200/175341]\n",
      "loss: 0.338115  [164800/175341]\n",
      "loss: 0.834159  [166400/175341]\n",
      "loss: 0.517238  [168000/175341]\n",
      "loss: 0.488207  [169600/175341]\n",
      "loss: 0.715477  [171200/175341]\n",
      "loss: 0.402786  [172800/175341]\n",
      "loss: 0.580211  [174400/175341]\n",
      "Train Accuracy: 79.0374%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.611738, F1-score: 74.45% \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.000655  [    0/175341]\n",
      "loss: 0.778350  [ 1600/175341]\n",
      "loss: 0.609538  [ 3200/175341]\n",
      "loss: 0.814653  [ 4800/175341]\n",
      "loss: 0.390809  [ 6400/175341]\n",
      "loss: 0.689371  [ 8000/175341]\n",
      "loss: 0.467250  [ 9600/175341]\n",
      "loss: 0.587253  [11200/175341]\n",
      "loss: 0.664442  [12800/175341]\n",
      "loss: 0.923860  [14400/175341]\n",
      "loss: 0.398792  [16000/175341]\n",
      "loss: 0.541661  [17600/175341]\n",
      "loss: 0.270902  [19200/175341]\n",
      "loss: 0.701764  [20800/175341]\n",
      "loss: 0.619039  [22400/175341]\n",
      "loss: 0.572450  [24000/175341]\n",
      "loss: 0.364696  [25600/175341]\n",
      "loss: 0.590673  [27200/175341]\n",
      "loss: 0.588394  [28800/175341]\n",
      "loss: 0.588607  [30400/175341]\n",
      "loss: 0.682494  [32000/175341]\n",
      "loss: 0.307043  [33600/175341]\n",
      "loss: 0.433469  [35200/175341]\n",
      "loss: 0.394059  [36800/175341]\n",
      "loss: 0.128199  [38400/175341]\n",
      "loss: 0.303979  [40000/175341]\n",
      "loss: 0.309309  [41600/175341]\n",
      "loss: 0.139086  [43200/175341]\n",
      "loss: 0.254799  [44800/175341]\n",
      "loss: 0.367526  [46400/175341]\n",
      "loss: 0.917475  [48000/175341]\n",
      "loss: 0.271582  [49600/175341]\n",
      "loss: 0.340009  [51200/175341]\n",
      "loss: 0.381620  [52800/175341]\n",
      "loss: 0.526542  [54400/175341]\n",
      "loss: 0.517182  [56000/175341]\n",
      "loss: 0.583223  [57600/175341]\n",
      "loss: 0.458240  [59200/175341]\n",
      "loss: 0.391956  [60800/175341]\n",
      "loss: 0.580886  [62400/175341]\n",
      "loss: 0.478987  [64000/175341]\n",
      "loss: 0.676804  [65600/175341]\n",
      "loss: 0.324111  [67200/175341]\n",
      "loss: 0.984760  [68800/175341]\n",
      "loss: 0.756287  [70400/175341]\n",
      "loss: 0.647999  [72000/175341]\n",
      "loss: 0.731343  [73600/175341]\n",
      "loss: 0.999721  [75200/175341]\n",
      "loss: 0.313859  [76800/175341]\n",
      "loss: 0.654560  [78400/175341]\n",
      "loss: 0.275334  [80000/175341]\n",
      "loss: 1.136499  [81600/175341]\n",
      "loss: 0.378158  [83200/175341]\n",
      "loss: 0.493162  [84800/175341]\n",
      "loss: 0.398606  [86400/175341]\n",
      "loss: 0.338547  [88000/175341]\n",
      "loss: 0.422492  [89600/175341]\n",
      "loss: 0.564752  [91200/175341]\n",
      "loss: 0.628192  [92800/175341]\n",
      "loss: 0.732688  [94400/175341]\n",
      "loss: 0.386612  [96000/175341]\n",
      "loss: 0.509883  [97600/175341]\n",
      "loss: 1.025717  [99200/175341]\n",
      "loss: 0.688055  [100800/175341]\n",
      "loss: 0.757412  [102400/175341]\n",
      "loss: 0.677419  [104000/175341]\n",
      "loss: 0.740329  [105600/175341]\n",
      "loss: 0.603382  [107200/175341]\n",
      "loss: 0.160924  [108800/175341]\n",
      "loss: 0.419946  [110400/175341]\n",
      "loss: 0.489782  [112000/175341]\n",
      "loss: 0.561848  [113600/175341]\n",
      "loss: 0.307219  [115200/175341]\n",
      "loss: 0.485593  [116800/175341]\n",
      "loss: 0.183102  [118400/175341]\n",
      "loss: 0.451907  [120000/175341]\n",
      "loss: 0.412016  [121600/175341]\n",
      "loss: 0.862568  [123200/175341]\n",
      "loss: 0.386934  [124800/175341]\n",
      "loss: 0.897617  [126400/175341]\n",
      "loss: 0.810450  [128000/175341]\n",
      "loss: 0.560136  [129600/175341]\n",
      "loss: 0.691661  [131200/175341]\n",
      "loss: 0.668384  [132800/175341]\n",
      "loss: 1.155333  [134400/175341]\n",
      "loss: 0.607657  [136000/175341]\n",
      "loss: 0.387648  [137600/175341]\n",
      "loss: 0.673399  [139200/175341]\n",
      "loss: 0.322085  [140800/175341]\n",
      "loss: 0.577643  [142400/175341]\n",
      "loss: 0.890086  [144000/175341]\n",
      "loss: 0.662933  [145600/175341]\n",
      "loss: 0.389758  [147200/175341]\n",
      "loss: 0.526390  [148800/175341]\n",
      "loss: 0.356090  [150400/175341]\n",
      "loss: 0.579975  [152000/175341]\n",
      "loss: 0.814012  [153600/175341]\n",
      "loss: 0.545949  [155200/175341]\n",
      "loss: 0.769341  [156800/175341]\n",
      "loss: 0.745612  [158400/175341]\n",
      "loss: 0.524111  [160000/175341]\n",
      "loss: 0.430762  [161600/175341]\n",
      "loss: 0.253796  [163200/175341]\n",
      "loss: 0.486451  [164800/175341]\n",
      "loss: 0.390079  [166400/175341]\n",
      "loss: 0.594450  [168000/175341]\n",
      "loss: 0.551069  [169600/175341]\n",
      "loss: 0.531228  [171200/175341]\n",
      "loss: 0.529480  [172800/175341]\n",
      "loss: 0.401860  [174400/175341]\n",
      "Train Accuracy: 79.1138%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.604066, F1-score: 74.80% \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "237f0ad0-0334-4f18-b8a8-2e653a66b477",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Smooth and plot training loss for each fold\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, losses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mall_losses\u001b[49m):\n\u001b[0;32m     17\u001b[0m     smoothed_loss \u001b[38;5;241m=\u001b[39m smooth_loss(losses, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Adjust window_size as needed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(smoothed_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Smoothing function using a moving average\n",
    "def smooth_loss(losses, window_size=100):\n",
    "    \"\"\"\n",
    "    Smooth the loss values using a moving average.\n",
    "    :param losses: List of loss values.\n",
    "    :param window_size: Size of the moving window.\n",
    "    :return: Smoothed loss values.\n",
    "    \"\"\"\n",
    "    smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')\n",
    "    return smoothed_losses\n",
    "\n",
    "# Plot the training loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Smooth and plot training loss for each fold\n",
    "for fold, losses in enumerate(all_losses):\n",
    "    smoothed_loss = smooth_loss(losses, window_size=100)  # Adjust window_size as needed\n",
    "    plt.plot(smoothed_loss, label=f'Fold {fold + 1}')\n",
    "\n",
    "# Calculate and plot the average smoothed loss across folds\n",
    "avg_loss = np.mean(all_losses, axis=0)\n",
    "smoothed_avg_loss = smooth_loss(avg_loss, window_size=100)\n",
    "plt.plot(smoothed_avg_loss, label='Average Loss', linewidth=2, color='black')\n",
    "\n",
    "plt.xlabel('Mini-Batch Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Mini-Batches (Smoothed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc939cd5-1ad5-43f9-97d3-d06382dedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Mini-Batches per Epoch: {len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_labels_encoded.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356024a-ac67-4f9a-9d85-05fc7bbd6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units1: int, hidden_units2: int, hidden_units3: int, hidden_units4: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units2),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units3),\n",
    "        )\n",
    "\n",
    "        # **New Block Added Here**\n",
    "        self.linear_relu_stack3 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units3, out_features=hidden_units4),\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack4 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=hidden_units4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features=hidden_units4, out_features=output_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        x = self.linear_relu_stack3(x)  # Pass through new block\n",
    "        logits = self.linear_relu_stack4(x)  # Final output layer\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2e731-3fbc-4bab-b4bc-31d9643160e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = NeuralNetwork(40,128,64,32,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bf04-0ab7-468f-b80a-bba0e13ff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model_test, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model_test, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b16805-13a2-47e6-8c8d-cc677b5aa227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa08a28-cb43-4ecb-924e-34c39a6b8702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686dbe6-e354-46d6-98e8-d9eb3f4e5949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d2a7e-e9a4-43a1-8731-3345a578a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd505fb2-28cc-44a8-beb3-4baee288e02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f79f78-753b-4d86-89bc-6f85eec05b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
