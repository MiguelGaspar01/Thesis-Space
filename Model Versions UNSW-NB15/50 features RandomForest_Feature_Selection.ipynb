{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb2e4ac-d59e-4e27-9f18-e9f0aa674362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y_binary = train_labels_encoded[\"label\"]\n",
    "test_data_y_binary = test_labels_encoded[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b95a23d-e0ba-48a9-8e8f-a76411be9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = test_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded[\"attack_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "6    56000\n",
       "5    40000\n",
       "3    33393\n",
       "4    18184\n",
       "2    12264\n",
       "7    10491\n",
       "0     2000\n",
       "1     1746\n",
       "8     1133\n",
       "9      130\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c14778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175341, 56)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['remainder__trans_depth', 'encoder__proto_tcp', 'encoder__service_http', 'encoder__proto_udp', 'encoder__proto_arp', 'remainder__ct_src_dport_ltm', 'encoder__state_CON', 'remainder__ct_dst_sport_ltm', 'remainder__is_sm_ips_ports', 'remainder__ct_dst_ltm', 'remainder__stcpb', 'remainder__swin', 'remainder__sloss', 'remainder__dtcpb', 'remainder__ct_src_ltm', 'remainder__spkts', 'remainder__djit', 'remainder__dloss', 'remainder__dpkts', 'remainder__sjit', 'remainder__ct_srv_src', 'encoder__state_INT', 'remainder__synack', 'remainder__dbytes', 'remainder__sinpkt', 'remainder__smean', 'remainder__ct_dst_src_ltm', 'remainder__tcprtt', 'remainder__ct_srv_dst', 'remainder__ackdat', 'remainder__sbytes', 'remainder__dur', 'remainder__dttl', 'remainder__dinpkt', 'remainder__dmean', 'remainder__rate', 'remainder__sload', 'remainder__dload', 'remainder__ct_state_ttl', 'remainder__sttl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "top_40_features = np.argsort(feature_importances)[-45:]\n",
    "\n",
    "# Print selected features\n",
    "selected_features = train_data_X.columns[top_40_features]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_data_y.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([40, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_over_train = []\n",
    "loss_over_test = []\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    mean_loss = epoch_loss / len(dataloader) \n",
    "    loss_over_train.append(mean_loss)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    mean_loss = test_loss / num_batches  # Divide by the number of batches\n",
    "    loss_over_test.append(mean_loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48213e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "epochs = 200  # Number of training epochs per fold\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define loss function and learning parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Combine datasets (Only Train Data)\n",
    "dataset = train_dataset  # Use only training dataset\n",
    "\n",
    "# Store results across folds\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_losses = []\n",
    "\n",
    "# K-Fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\\n\" + \"-\" * 30)\n",
    "    print(\"initializing model...\")\n",
    "    model = KAN([45, 20,10, 5,10],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best\n",
    "    # Create data subsets for the current fold\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Re-initialize model and optimizer for each fold\n",
    "     # Replace with your model class\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training function\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        return correct / total * 100, epoch_loss / len(dataloader)\n",
    "\n",
    "    # Validation function\n",
    "    def val_loop(dataloader, model, loss_fn):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "                all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracy = correct / len(dataloader.dataset) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "        mean_loss = val_loss / len(dataloader)\n",
    "\n",
    "        return accuracy, f1, f1_macro, mean_loss\n",
    "\n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_acc, f1, f1_macro, val_loss = val_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%, F1={f1:.2f}%, Macro F1={f1_macro:.2f}%\")\n",
    "\n",
    "    # Store fold results\n",
    "    fold_accuracies.append(val_acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_f1_macro_scores.append(f1_macro)\n",
    "    fold_losses.append(val_loss)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Avg Validation Accuracy: {sum(fold_accuracies) / k_folds:.2f}%\")\n",
    "print(f\"Avg Weighted F1-score: {sum(fold_f1_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Macro F1-score: {sum(fold_f1_macro_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Validation Loss: {sum(fold_losses) / k_folds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.243774  [    0/175341]\n",
      "loss: 2.130168  [ 1600/175341]\n",
      "loss: 2.086567  [ 3200/175341]\n",
      "loss: 1.703839  [ 4800/175341]\n",
      "loss: 2.107843  [ 6400/175341]\n",
      "loss: 1.692125  [ 8000/175341]\n",
      "loss: 1.970738  [ 9600/175341]\n",
      "loss: 1.403720  [11200/175341]\n",
      "loss: 1.732954  [12800/175341]\n",
      "loss: 1.149064  [14400/175341]\n",
      "loss: 1.270265  [16000/175341]\n",
      "loss: 1.101073  [17600/175341]\n",
      "loss: 1.107978  [19200/175341]\n",
      "loss: 1.348850  [20800/175341]\n",
      "loss: 1.293973  [22400/175341]\n",
      "loss: 0.775786  [24000/175341]\n",
      "loss: 0.896277  [25600/175341]\n",
      "loss: 0.842198  [27200/175341]\n",
      "loss: 0.669076  [28800/175341]\n",
      "loss: 1.208609  [30400/175341]\n",
      "loss: 1.046609  [32000/175341]\n",
      "loss: 1.079362  [33600/175341]\n",
      "loss: 0.841125  [35200/175341]\n",
      "loss: 0.899353  [36800/175341]\n",
      "loss: 0.829140  [38400/175341]\n",
      "loss: 0.672231  [40000/175341]\n",
      "loss: 0.810935  [41600/175341]\n",
      "loss: 0.802702  [43200/175341]\n",
      "loss: 1.015134  [44800/175341]\n",
      "loss: 0.969203  [46400/175341]\n",
      "loss: 0.531884  [48000/175341]\n",
      "loss: 0.854345  [49600/175341]\n",
      "loss: 1.093905  [51200/175341]\n",
      "loss: 0.860635  [52800/175341]\n",
      "loss: 0.936440  [54400/175341]\n",
      "loss: 0.825689  [56000/175341]\n",
      "loss: 0.626013  [57600/175341]\n",
      "loss: 0.646161  [59200/175341]\n",
      "loss: 0.700059  [60800/175341]\n",
      "loss: 1.011318  [62400/175341]\n",
      "loss: 1.113217  [64000/175341]\n",
      "loss: 1.118526  [65600/175341]\n",
      "loss: 0.619668  [67200/175341]\n",
      "loss: 0.741191  [68800/175341]\n",
      "loss: 1.217558  [70400/175341]\n",
      "loss: 0.435781  [72000/175341]\n",
      "loss: 0.738435  [73600/175341]\n",
      "loss: 0.904948  [75200/175341]\n",
      "loss: 0.630657  [76800/175341]\n",
      "loss: 0.814984  [78400/175341]\n",
      "loss: 0.983252  [80000/175341]\n",
      "loss: 0.614009  [81600/175341]\n",
      "loss: 0.646895  [83200/175341]\n",
      "loss: 0.881836  [84800/175341]\n",
      "loss: 0.417706  [86400/175341]\n",
      "loss: 0.861618  [88000/175341]\n",
      "loss: 1.064120  [89600/175341]\n",
      "loss: 0.681708  [91200/175341]\n",
      "loss: 0.817645  [92800/175341]\n",
      "loss: 0.546154  [94400/175341]\n",
      "loss: 0.980282  [96000/175341]\n",
      "loss: 0.702879  [97600/175341]\n",
      "loss: 0.493292  [99200/175341]\n",
      "loss: 0.997519  [100800/175341]\n",
      "loss: 0.754826  [102400/175341]\n",
      "loss: 0.723224  [104000/175341]\n",
      "loss: 1.005219  [105600/175341]\n",
      "loss: 0.766178  [107200/175341]\n",
      "loss: 0.694514  [108800/175341]\n",
      "loss: 0.656994  [110400/175341]\n",
      "loss: 1.020224  [112000/175341]\n",
      "loss: 0.521556  [113600/175341]\n",
      "loss: 0.799895  [115200/175341]\n",
      "loss: 0.787866  [116800/175341]\n",
      "loss: 0.606306  [118400/175341]\n",
      "loss: 0.433348  [120000/175341]\n",
      "loss: 0.989133  [121600/175341]\n",
      "loss: 1.093983  [123200/175341]\n",
      "loss: 0.239592  [124800/175341]\n",
      "loss: 0.558508  [126400/175341]\n",
      "loss: 0.833585  [128000/175341]\n",
      "loss: 0.643732  [129600/175341]\n",
      "loss: 1.040625  [131200/175341]\n",
      "loss: 0.398649  [132800/175341]\n",
      "loss: 0.541690  [134400/175341]\n",
      "loss: 0.800349  [136000/175341]\n",
      "loss: 1.019175  [137600/175341]\n",
      "loss: 0.930920  [139200/175341]\n",
      "loss: 0.495507  [140800/175341]\n",
      "loss: 0.445555  [142400/175341]\n",
      "loss: 0.703951  [144000/175341]\n",
      "loss: 0.427169  [145600/175341]\n",
      "loss: 0.515976  [147200/175341]\n",
      "loss: 0.404712  [148800/175341]\n",
      "loss: 0.585734  [150400/175341]\n",
      "loss: 0.670149  [152000/175341]\n",
      "loss: 0.405123  [153600/175341]\n",
      "loss: 0.759237  [155200/175341]\n",
      "loss: 0.561973  [156800/175341]\n",
      "loss: 0.362132  [158400/175341]\n",
      "loss: 1.112348  [160000/175341]\n",
      "loss: 0.419546  [161600/175341]\n",
      "loss: 0.792369  [163200/175341]\n",
      "loss: 0.631571  [164800/175341]\n",
      "loss: 0.832044  [166400/175341]\n",
      "loss: 0.646146  [168000/175341]\n",
      "loss: 1.020057  [169600/175341]\n",
      "loss: 0.544170  [171200/175341]\n",
      "loss: 0.829443  [172800/175341]\n",
      "loss: 0.734028  [174400/175341]\n",
      "Train Accuracy: 71.0689%\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.739150, F1-score: 70.74%, Macro_F1-Score:  32.90%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.706758  [    0/175341]\n",
      "loss: 0.713634  [ 1600/175341]\n",
      "loss: 0.342910  [ 3200/175341]\n",
      "loss: 0.271770  [ 4800/175341]\n",
      "loss: 0.830877  [ 6400/175341]\n",
      "loss: 0.327251  [ 8000/175341]\n",
      "loss: 0.508694  [ 9600/175341]\n",
      "loss: 0.922641  [11200/175341]\n",
      "loss: 0.510228  [12800/175341]\n",
      "loss: 0.542905  [14400/175341]\n",
      "loss: 1.630792  [16000/175341]\n",
      "loss: 1.029930  [17600/175341]\n",
      "loss: 0.463666  [19200/175341]\n",
      "loss: 0.543329  [20800/175341]\n",
      "loss: 0.660184  [22400/175341]\n",
      "loss: 0.741659  [24000/175341]\n",
      "loss: 0.463451  [25600/175341]\n",
      "loss: 0.567815  [27200/175341]\n",
      "loss: 1.185103  [28800/175341]\n",
      "loss: 0.824079  [30400/175341]\n",
      "loss: 0.534741  [32000/175341]\n",
      "loss: 0.985500  [33600/175341]\n",
      "loss: 0.691940  [35200/175341]\n",
      "loss: 0.483899  [36800/175341]\n",
      "loss: 0.716556  [38400/175341]\n",
      "loss: 0.365002  [40000/175341]\n",
      "loss: 0.229257  [41600/175341]\n",
      "loss: 0.481201  [43200/175341]\n",
      "loss: 1.038312  [44800/175341]\n",
      "loss: 0.385288  [46400/175341]\n",
      "loss: 0.880774  [48000/175341]\n",
      "loss: 0.479829  [49600/175341]\n",
      "loss: 0.428650  [51200/175341]\n",
      "loss: 1.125618  [52800/175341]\n",
      "loss: 1.560729  [54400/175341]\n",
      "loss: 0.842151  [56000/175341]\n",
      "loss: 0.639078  [57600/175341]\n",
      "loss: 0.440898  [59200/175341]\n",
      "loss: 0.902966  [60800/175341]\n",
      "loss: 0.862891  [62400/175341]\n",
      "loss: 0.425135  [64000/175341]\n",
      "loss: 0.825877  [65600/175341]\n",
      "loss: 0.325963  [67200/175341]\n",
      "loss: 0.443894  [68800/175341]\n",
      "loss: 0.417040  [70400/175341]\n",
      "loss: 1.074236  [72000/175341]\n",
      "loss: 0.537247  [73600/175341]\n",
      "loss: 0.840727  [75200/175341]\n",
      "loss: 0.680180  [76800/175341]\n",
      "loss: 0.531863  [78400/175341]\n",
      "loss: 0.508010  [80000/175341]\n",
      "loss: 0.243746  [81600/175341]\n",
      "loss: 0.743522  [83200/175341]\n",
      "loss: 0.500598  [84800/175341]\n",
      "loss: 0.790160  [86400/175341]\n",
      "loss: 0.618418  [88000/175341]\n",
      "loss: 0.913397  [89600/175341]\n",
      "loss: 0.547064  [91200/175341]\n",
      "loss: 0.779491  [92800/175341]\n",
      "loss: 0.835109  [94400/175341]\n",
      "loss: 0.914443  [96000/175341]\n",
      "loss: 0.568826  [97600/175341]\n",
      "loss: 0.573022  [99200/175341]\n",
      "loss: 0.318715  [100800/175341]\n",
      "loss: 0.734998  [102400/175341]\n",
      "loss: 0.503466  [104000/175341]\n",
      "loss: 0.870644  [105600/175341]\n",
      "loss: 0.937042  [107200/175341]\n",
      "loss: 0.786398  [108800/175341]\n",
      "loss: 0.596987  [110400/175341]\n",
      "loss: 0.446100  [112000/175341]\n",
      "loss: 0.810727  [113600/175341]\n",
      "loss: 0.870489  [115200/175341]\n",
      "loss: 0.333941  [116800/175341]\n",
      "loss: 1.045475  [118400/175341]\n",
      "loss: 0.684038  [120000/175341]\n",
      "loss: 0.997396  [121600/175341]\n",
      "loss: 0.497096  [123200/175341]\n",
      "loss: 0.229092  [124800/175341]\n",
      "loss: 0.841880  [126400/175341]\n",
      "loss: 0.764681  [128000/175341]\n",
      "loss: 0.228629  [129600/175341]\n",
      "loss: 0.727711  [131200/175341]\n",
      "loss: 0.670990  [132800/175341]\n",
      "loss: 0.997363  [134400/175341]\n",
      "loss: 1.110125  [136000/175341]\n",
      "loss: 0.943155  [137600/175341]\n",
      "loss: 0.796224  [139200/175341]\n",
      "loss: 0.542519  [140800/175341]\n",
      "loss: 0.576091  [142400/175341]\n",
      "loss: 0.243374  [144000/175341]\n",
      "loss: 0.396016  [145600/175341]\n",
      "loss: 0.328755  [147200/175341]\n",
      "loss: 0.921308  [148800/175341]\n",
      "loss: 0.314598  [150400/175341]\n",
      "loss: 0.660907  [152000/175341]\n",
      "loss: 0.543465  [153600/175341]\n",
      "loss: 1.010170  [155200/175341]\n",
      "loss: 0.322151  [156800/175341]\n",
      "loss: 1.412029  [158400/175341]\n",
      "loss: 0.496778  [160000/175341]\n",
      "loss: 0.752014  [161600/175341]\n",
      "loss: 0.774446  [163200/175341]\n",
      "loss: 0.593232  [164800/175341]\n",
      "loss: 0.270407  [166400/175341]\n",
      "loss: 0.554701  [168000/175341]\n",
      "loss: 0.773563  [169600/175341]\n",
      "loss: 0.318886  [171200/175341]\n",
      "loss: 0.320031  [172800/175341]\n",
      "loss: 0.571773  [174400/175341]\n",
      "Train Accuracy: 77.6470%\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.716448, F1-score: 70.34%, Macro_F1-Score:  32.59%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.661879  [    0/175341]\n",
      "loss: 0.736935  [ 1600/175341]\n",
      "loss: 0.471679  [ 3200/175341]\n",
      "loss: 0.880388  [ 4800/175341]\n",
      "loss: 0.236634  [ 6400/175341]\n",
      "loss: 0.615389  [ 8000/175341]\n",
      "loss: 1.018357  [ 9600/175341]\n",
      "loss: 0.656142  [11200/175341]\n",
      "loss: 0.647877  [12800/175341]\n",
      "loss: 0.497599  [14400/175341]\n",
      "loss: 0.358929  [16000/175341]\n",
      "loss: 0.428950  [17600/175341]\n",
      "loss: 0.464432  [19200/175341]\n",
      "loss: 0.539568  [20800/175341]\n",
      "loss: 0.524368  [22400/175341]\n",
      "loss: 0.650340  [24000/175341]\n",
      "loss: 0.619257  [25600/175341]\n",
      "loss: 0.705426  [27200/175341]\n",
      "loss: 0.584570  [28800/175341]\n",
      "loss: 0.696204  [30400/175341]\n",
      "loss: 0.920246  [32000/175341]\n",
      "loss: 0.480473  [33600/175341]\n",
      "loss: 1.237178  [35200/175341]\n",
      "loss: 0.285104  [36800/175341]\n",
      "loss: 0.189533  [38400/175341]\n",
      "loss: 0.515812  [40000/175341]\n",
      "loss: 0.930577  [41600/175341]\n",
      "loss: 0.624406  [43200/175341]\n",
      "loss: 0.774641  [44800/175341]\n",
      "loss: 0.707213  [46400/175341]\n",
      "loss: 0.521568  [48000/175341]\n",
      "loss: 0.214884  [49600/175341]\n",
      "loss: 0.361127  [51200/175341]\n",
      "loss: 0.517072  [52800/175341]\n",
      "loss: 0.821815  [54400/175341]\n",
      "loss: 0.373634  [56000/175341]\n",
      "loss: 0.679278  [57600/175341]\n",
      "loss: 0.372083  [59200/175341]\n",
      "loss: 0.278856  [60800/175341]\n",
      "loss: 0.583413  [62400/175341]\n",
      "loss: 0.428697  [64000/175341]\n",
      "loss: 0.276878  [65600/175341]\n",
      "loss: 0.617218  [67200/175341]\n",
      "loss: 0.198617  [68800/175341]\n",
      "loss: 0.887632  [70400/175341]\n",
      "loss: 0.684239  [72000/175341]\n",
      "loss: 0.414284  [73600/175341]\n",
      "loss: 0.288510  [75200/175341]\n",
      "loss: 0.412525  [76800/175341]\n",
      "loss: 0.424811  [78400/175341]\n",
      "loss: 0.751743  [80000/175341]\n",
      "loss: 0.729042  [81600/175341]\n",
      "loss: 0.461438  [83200/175341]\n",
      "loss: 0.893007  [84800/175341]\n",
      "loss: 0.562032  [86400/175341]\n",
      "loss: 0.186694  [88000/175341]\n",
      "loss: 0.435911  [89600/175341]\n",
      "loss: 0.882830  [91200/175341]\n",
      "loss: 0.715776  [92800/175341]\n",
      "loss: 0.642038  [94400/175341]\n",
      "loss: 0.552526  [96000/175341]\n",
      "loss: 0.685045  [97600/175341]\n",
      "loss: 0.908261  [99200/175341]\n",
      "loss: 0.535479  [100800/175341]\n",
      "loss: 0.934754  [102400/175341]\n",
      "loss: 0.832932  [104000/175341]\n",
      "loss: 0.831192  [105600/175341]\n",
      "loss: 0.774600  [107200/175341]\n",
      "loss: 0.723317  [108800/175341]\n",
      "loss: 0.537325  [110400/175341]\n",
      "loss: 0.884432  [112000/175341]\n",
      "loss: 0.899944  [113600/175341]\n",
      "loss: 0.824058  [115200/175341]\n",
      "loss: 0.413765  [116800/175341]\n",
      "loss: 0.587174  [118400/175341]\n",
      "loss: 0.354467  [120000/175341]\n",
      "loss: 0.563856  [121600/175341]\n",
      "loss: 0.470539  [123200/175341]\n",
      "loss: 0.322596  [124800/175341]\n",
      "loss: 0.475657  [126400/175341]\n",
      "loss: 0.544687  [128000/175341]\n",
      "loss: 0.533804  [129600/175341]\n",
      "loss: 0.809580  [131200/175341]\n",
      "loss: 0.843803  [132800/175341]\n",
      "loss: 0.368702  [134400/175341]\n",
      "loss: 0.545779  [136000/175341]\n",
      "loss: 0.452985  [137600/175341]\n",
      "loss: 0.176374  [139200/175341]\n",
      "loss: 0.599590  [140800/175341]\n",
      "loss: 0.500559  [142400/175341]\n",
      "loss: 0.613416  [144000/175341]\n",
      "loss: 0.725506  [145600/175341]\n",
      "loss: 0.387711  [147200/175341]\n",
      "loss: 0.624050  [148800/175341]\n",
      "loss: 0.949543  [150400/175341]\n",
      "loss: 0.651901  [152000/175341]\n",
      "loss: 0.292426  [153600/175341]\n",
      "loss: 0.810626  [155200/175341]\n",
      "loss: 0.361246  [156800/175341]\n",
      "loss: 0.460839  [158400/175341]\n",
      "loss: 0.409490  [160000/175341]\n",
      "loss: 0.384682  [161600/175341]\n",
      "loss: 0.677485  [163200/175341]\n",
      "loss: 0.109048  [164800/175341]\n",
      "loss: 0.465505  [166400/175341]\n",
      "loss: 0.463036  [168000/175341]\n",
      "loss: 0.468374  [169600/175341]\n",
      "loss: 0.364514  [171200/175341]\n",
      "loss: 0.547866  [172800/175341]\n",
      "loss: 0.590273  [174400/175341]\n",
      "Train Accuracy: 78.3741%\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.683788, F1-score: 72.79%, Macro_F1-Score:  33.88%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.408989  [    0/175341]\n",
      "loss: 0.450654  [ 1600/175341]\n",
      "loss: 0.295972  [ 3200/175341]\n",
      "loss: 0.565643  [ 4800/175341]\n",
      "loss: 0.528983  [ 6400/175341]\n",
      "loss: 0.638211  [ 8000/175341]\n",
      "loss: 0.273555  [ 9600/175341]\n",
      "loss: 0.853384  [11200/175341]\n",
      "loss: 0.547753  [12800/175341]\n",
      "loss: 0.540886  [14400/175341]\n",
      "loss: 0.761847  [16000/175341]\n",
      "loss: 0.986445  [17600/175341]\n",
      "loss: 0.824010  [19200/175341]\n",
      "loss: 0.509529  [20800/175341]\n",
      "loss: 0.336380  [22400/175341]\n",
      "loss: 0.650134  [24000/175341]\n",
      "loss: 0.538787  [25600/175341]\n",
      "loss: 0.571114  [27200/175341]\n",
      "loss: 0.712911  [28800/175341]\n",
      "loss: 0.558756  [30400/175341]\n",
      "loss: 0.448870  [32000/175341]\n",
      "loss: 0.522030  [33600/175341]\n",
      "loss: 0.505486  [35200/175341]\n",
      "loss: 0.738879  [36800/175341]\n",
      "loss: 0.445625  [38400/175341]\n",
      "loss: 0.561551  [40000/175341]\n",
      "loss: 0.589968  [41600/175341]\n",
      "loss: 0.618987  [43200/175341]\n",
      "loss: 0.375480  [44800/175341]\n",
      "loss: 0.412555  [46400/175341]\n",
      "loss: 0.393754  [48000/175341]\n",
      "loss: 0.217835  [49600/175341]\n",
      "loss: 0.438763  [51200/175341]\n",
      "loss: 0.567356  [52800/175341]\n",
      "loss: 0.854421  [54400/175341]\n",
      "loss: 0.458816  [56000/175341]\n",
      "loss: 0.970821  [57600/175341]\n",
      "loss: 0.715305  [59200/175341]\n",
      "loss: 0.427575  [60800/175341]\n",
      "loss: 0.363096  [62400/175341]\n",
      "loss: 0.766712  [64000/175341]\n",
      "loss: 0.419513  [65600/175341]\n",
      "loss: 0.402798  [67200/175341]\n",
      "loss: 0.418081  [68800/175341]\n",
      "loss: 0.299284  [70400/175341]\n",
      "loss: 0.949335  [72000/175341]\n",
      "loss: 0.285369  [73600/175341]\n",
      "loss: 1.176992  [75200/175341]\n",
      "loss: 0.503496  [76800/175341]\n",
      "loss: 1.182965  [78400/175341]\n",
      "loss: 0.815850  [80000/175341]\n",
      "loss: 0.570356  [81600/175341]\n",
      "loss: 1.029968  [83200/175341]\n",
      "loss: 0.443110  [84800/175341]\n",
      "loss: 0.708279  [86400/175341]\n",
      "loss: 0.851562  [88000/175341]\n",
      "loss: 1.181170  [89600/175341]\n",
      "loss: 0.712845  [91200/175341]\n",
      "loss: 0.496082  [92800/175341]\n",
      "loss: 0.725307  [94400/175341]\n",
      "loss: 0.717278  [96000/175341]\n",
      "loss: 0.783682  [97600/175341]\n",
      "loss: 0.148166  [99200/175341]\n",
      "loss: 1.187182  [100800/175341]\n",
      "loss: 0.111230  [102400/175341]\n",
      "loss: 0.449564  [104000/175341]\n",
      "loss: 0.240097  [105600/175341]\n",
      "loss: 1.263176  [107200/175341]\n",
      "loss: 0.480104  [108800/175341]\n",
      "loss: 0.360135  [110400/175341]\n",
      "loss: 0.683501  [112000/175341]\n",
      "loss: 0.359364  [113600/175341]\n",
      "loss: 0.437510  [115200/175341]\n",
      "loss: 0.349023  [116800/175341]\n",
      "loss: 0.788320  [118400/175341]\n",
      "loss: 0.281625  [120000/175341]\n",
      "loss: 0.860138  [121600/175341]\n",
      "loss: 0.768805  [123200/175341]\n",
      "loss: 0.540437  [124800/175341]\n",
      "loss: 0.892127  [126400/175341]\n",
      "loss: 0.603901  [128000/175341]\n",
      "loss: 0.708889  [129600/175341]\n",
      "loss: 0.452123  [131200/175341]\n",
      "loss: 0.878998  [132800/175341]\n",
      "loss: 0.549824  [134400/175341]\n",
      "loss: 0.506638  [136000/175341]\n",
      "loss: 0.423821  [137600/175341]\n",
      "loss: 0.967872  [139200/175341]\n",
      "loss: 0.363354  [140800/175341]\n",
      "loss: 1.003764  [142400/175341]\n",
      "loss: 0.707024  [144000/175341]\n",
      "loss: 0.876104  [145600/175341]\n",
      "loss: 0.957442  [147200/175341]\n",
      "loss: 0.704190  [148800/175341]\n",
      "loss: 0.160152  [150400/175341]\n",
      "loss: 0.468064  [152000/175341]\n",
      "loss: 0.586615  [153600/175341]\n",
      "loss: 0.709246  [155200/175341]\n",
      "loss: 0.308839  [156800/175341]\n",
      "loss: 0.489458  [158400/175341]\n",
      "loss: 0.421200  [160000/175341]\n",
      "loss: 0.273567  [161600/175341]\n",
      "loss: 0.686880  [163200/175341]\n",
      "loss: 0.724374  [164800/175341]\n",
      "loss: 0.377843  [166400/175341]\n",
      "loss: 0.386162  [168000/175341]\n",
      "loss: 0.723989  [169600/175341]\n",
      "loss: 0.634223  [171200/175341]\n",
      "loss: 0.386334  [172800/175341]\n",
      "loss: 0.334412  [174400/175341]\n",
      "Train Accuracy: 78.9000%\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.667753, F1-score: 72.06%, Macro_F1-Score:  34.76%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.766771  [    0/175341]\n",
      "loss: 0.434033  [ 1600/175341]\n",
      "loss: 0.469304  [ 3200/175341]\n",
      "loss: 0.785939  [ 4800/175341]\n",
      "loss: 0.531764  [ 6400/175341]\n",
      "loss: 0.382988  [ 8000/175341]\n",
      "loss: 0.628384  [ 9600/175341]\n",
      "loss: 0.594569  [11200/175341]\n",
      "loss: 0.819405  [12800/175341]\n",
      "loss: 0.324779  [14400/175341]\n",
      "loss: 0.331365  [16000/175341]\n",
      "loss: 0.893216  [17600/175341]\n",
      "loss: 0.626792  [19200/175341]\n",
      "loss: 0.646720  [20800/175341]\n",
      "loss: 0.707875  [22400/175341]\n",
      "loss: 0.528456  [24000/175341]\n",
      "loss: 0.451917  [25600/175341]\n",
      "loss: 0.567782  [27200/175341]\n",
      "loss: 0.246324  [28800/175341]\n",
      "loss: 0.453977  [30400/175341]\n",
      "loss: 0.326589  [32000/175341]\n",
      "loss: 0.538364  [33600/175341]\n",
      "loss: 0.557646  [35200/175341]\n",
      "loss: 0.572893  [36800/175341]\n",
      "loss: 0.040649  [38400/175341]\n",
      "loss: 0.746326  [40000/175341]\n",
      "loss: 0.380093  [41600/175341]\n",
      "loss: 0.420471  [43200/175341]\n",
      "loss: 0.728750  [44800/175341]\n",
      "loss: 0.923691  [46400/175341]\n",
      "loss: 0.771310  [48000/175341]\n",
      "loss: 0.769046  [49600/175341]\n",
      "loss: 0.412069  [51200/175341]\n",
      "loss: 0.514067  [52800/175341]\n",
      "loss: 0.439703  [54400/175341]\n",
      "loss: 0.348355  [56000/175341]\n",
      "loss: 0.601806  [57600/175341]\n",
      "loss: 0.237478  [59200/175341]\n",
      "loss: 0.433888  [60800/175341]\n",
      "loss: 0.582774  [62400/175341]\n",
      "loss: 0.344486  [64000/175341]\n",
      "loss: 0.342510  [65600/175341]\n",
      "loss: 1.182720  [67200/175341]\n",
      "loss: 0.562808  [68800/175341]\n",
      "loss: 0.232858  [70400/175341]\n",
      "loss: 0.552499  [72000/175341]\n",
      "loss: 1.098427  [73600/175341]\n",
      "loss: 0.482558  [75200/175341]\n",
      "loss: 0.663194  [76800/175341]\n",
      "loss: 0.583776  [78400/175341]\n",
      "loss: 0.362966  [80000/175341]\n",
      "loss: 0.472342  [81600/175341]\n",
      "loss: 0.388007  [83200/175341]\n",
      "loss: 0.732058  [84800/175341]\n",
      "loss: 0.740132  [86400/175341]\n",
      "loss: 0.268967  [88000/175341]\n",
      "loss: 0.326135  [89600/175341]\n",
      "loss: 0.626863  [91200/175341]\n",
      "loss: 0.699383  [92800/175341]\n",
      "loss: 0.760934  [94400/175341]\n",
      "loss: 0.427187  [96000/175341]\n",
      "loss: 0.603185  [97600/175341]\n",
      "loss: 0.223275  [99200/175341]\n",
      "loss: 0.772476  [100800/175341]\n",
      "loss: 0.669022  [102400/175341]\n",
      "loss: 0.464248  [104000/175341]\n",
      "loss: 0.570254  [105600/175341]\n",
      "loss: 0.700673  [107200/175341]\n",
      "loss: 0.588997  [108800/175341]\n",
      "loss: 0.306449  [110400/175341]\n",
      "loss: 0.490641  [112000/175341]\n",
      "loss: 0.532975  [113600/175341]\n",
      "loss: 0.347241  [115200/175341]\n",
      "loss: 0.588227  [116800/175341]\n",
      "loss: 0.601498  [118400/175341]\n",
      "loss: 0.581195  [120000/175341]\n",
      "loss: 0.648960  [121600/175341]\n",
      "loss: 0.376176  [123200/175341]\n",
      "loss: 0.413723  [124800/175341]\n",
      "loss: 0.438910  [126400/175341]\n",
      "loss: 0.636198  [128000/175341]\n",
      "loss: 0.627336  [129600/175341]\n",
      "loss: 0.515989  [131200/175341]\n",
      "loss: 0.319889  [132800/175341]\n",
      "loss: 0.502098  [134400/175341]\n",
      "loss: 0.576580  [136000/175341]\n",
      "loss: 0.701748  [137600/175341]\n",
      "loss: 0.406318  [139200/175341]\n",
      "loss: 0.271282  [140800/175341]\n",
      "loss: 0.546785  [142400/175341]\n",
      "loss: 0.609611  [144000/175341]\n",
      "loss: 0.626766  [145600/175341]\n",
      "loss: 0.478311  [147200/175341]\n",
      "loss: 1.065819  [148800/175341]\n",
      "loss: 0.396100  [150400/175341]\n",
      "loss: 0.466676  [152000/175341]\n",
      "loss: 0.636331  [153600/175341]\n",
      "loss: 0.420395  [155200/175341]\n",
      "loss: 0.178304  [156800/175341]\n",
      "loss: 0.331742  [158400/175341]\n",
      "loss: 0.567644  [160000/175341]\n",
      "loss: 0.304655  [161600/175341]\n",
      "loss: 0.396644  [163200/175341]\n",
      "loss: 0.538196  [164800/175341]\n",
      "loss: 0.385111  [166400/175341]\n",
      "loss: 0.501011  [168000/175341]\n",
      "loss: 0.514033  [169600/175341]\n",
      "loss: 1.001930  [171200/175341]\n",
      "loss: 0.273352  [172800/175341]\n",
      "loss: 0.275588  [174400/175341]\n",
      "Train Accuracy: 79.3015%\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.633625, F1-score: 74.04%, Macro_F1-Score:  37.83%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.326684  [    0/175341]\n",
      "loss: 1.176037  [ 1600/175341]\n",
      "loss: 0.495426  [ 3200/175341]\n",
      "loss: 0.221235  [ 4800/175341]\n",
      "loss: 0.420835  [ 6400/175341]\n",
      "loss: 0.137860  [ 8000/175341]\n",
      "loss: 0.719569  [ 9600/175341]\n",
      "loss: 1.013865  [11200/175341]\n",
      "loss: 0.443345  [12800/175341]\n",
      "loss: 1.129790  [14400/175341]\n",
      "loss: 0.472276  [16000/175341]\n",
      "loss: 0.555102  [17600/175341]\n",
      "loss: 0.473784  [19200/175341]\n",
      "loss: 0.565178  [20800/175341]\n",
      "loss: 0.476516  [22400/175341]\n",
      "loss: 0.504670  [24000/175341]\n",
      "loss: 0.548792  [25600/175341]\n",
      "loss: 0.526507  [27200/175341]\n",
      "loss: 0.698459  [28800/175341]\n",
      "loss: 0.275265  [30400/175341]\n",
      "loss: 0.943862  [32000/175341]\n",
      "loss: 0.807456  [33600/175341]\n",
      "loss: 0.716340  [35200/175341]\n",
      "loss: 0.226427  [36800/175341]\n",
      "loss: 0.402537  [38400/175341]\n",
      "loss: 0.376614  [40000/175341]\n",
      "loss: 0.444903  [41600/175341]\n",
      "loss: 0.726727  [43200/175341]\n",
      "loss: 0.886579  [44800/175341]\n",
      "loss: 0.170914  [46400/175341]\n",
      "loss: 0.413739  [48000/175341]\n",
      "loss: 0.277039  [49600/175341]\n",
      "loss: 0.068224  [51200/175341]\n",
      "loss: 0.170836  [52800/175341]\n",
      "loss: 0.526165  [54400/175341]\n",
      "loss: 0.998047  [56000/175341]\n",
      "loss: 0.305944  [57600/175341]\n",
      "loss: 0.680101  [59200/175341]\n",
      "loss: 0.258315  [60800/175341]\n",
      "loss: 0.318475  [62400/175341]\n",
      "loss: 0.767210  [64000/175341]\n",
      "loss: 0.452516  [65600/175341]\n",
      "loss: 0.363824  [67200/175341]\n",
      "loss: 0.393932  [68800/175341]\n",
      "loss: 0.576338  [70400/175341]\n",
      "loss: 0.354254  [72000/175341]\n",
      "loss: 0.354454  [73600/175341]\n",
      "loss: 0.626355  [75200/175341]\n",
      "loss: 0.247364  [76800/175341]\n",
      "loss: 0.523427  [78400/175341]\n",
      "loss: 0.350518  [80000/175341]\n",
      "loss: 0.769166  [81600/175341]\n",
      "loss: 0.448878  [83200/175341]\n",
      "loss: 0.785132  [84800/175341]\n",
      "loss: 0.481426  [86400/175341]\n",
      "loss: 0.351817  [88000/175341]\n",
      "loss: 0.104873  [89600/175341]\n",
      "loss: 0.796217  [91200/175341]\n",
      "loss: 0.625130  [92800/175341]\n",
      "loss: 0.410324  [94400/175341]\n",
      "loss: 1.323393  [96000/175341]\n",
      "loss: 0.609919  [97600/175341]\n",
      "loss: 0.200040  [99200/175341]\n",
      "loss: 0.439271  [100800/175341]\n",
      "loss: 0.190128  [102400/175341]\n",
      "loss: 0.264274  [104000/175341]\n",
      "loss: 0.742723  [105600/175341]\n",
      "loss: 0.450541  [107200/175341]\n",
      "loss: 0.600772  [108800/175341]\n",
      "loss: 0.512271  [110400/175341]\n",
      "loss: 0.648603  [112000/175341]\n",
      "loss: 0.496238  [113600/175341]\n",
      "loss: 0.657897  [115200/175341]\n",
      "loss: 0.190509  [116800/175341]\n",
      "loss: 0.190581  [118400/175341]\n",
      "loss: 0.422459  [120000/175341]\n",
      "loss: 0.317259  [121600/175341]\n",
      "loss: 0.682402  [123200/175341]\n",
      "loss: 0.442462  [124800/175341]\n",
      "loss: 0.701483  [126400/175341]\n",
      "loss: 0.288630  [128000/175341]\n",
      "loss: 0.548279  [129600/175341]\n",
      "loss: 0.372192  [131200/175341]\n",
      "loss: 0.262198  [132800/175341]\n",
      "loss: 0.324305  [134400/175341]\n",
      "loss: 0.370198  [136000/175341]\n",
      "loss: 0.193055  [137600/175341]\n",
      "loss: 0.928870  [139200/175341]\n",
      "loss: 0.943888  [140800/175341]\n",
      "loss: 1.276895  [142400/175341]\n",
      "loss: 0.998371  [144000/175341]\n",
      "loss: 0.231306  [145600/175341]\n",
      "loss: 0.313986  [147200/175341]\n",
      "loss: 0.321385  [148800/175341]\n",
      "loss: 0.603399  [150400/175341]\n",
      "loss: 0.570024  [152000/175341]\n",
      "loss: 0.582756  [153600/175341]\n",
      "loss: 0.492600  [155200/175341]\n",
      "loss: 0.500968  [156800/175341]\n",
      "loss: 0.672755  [158400/175341]\n",
      "loss: 0.622661  [160000/175341]\n",
      "loss: 0.680347  [161600/175341]\n",
      "loss: 0.908944  [163200/175341]\n",
      "loss: 0.510832  [164800/175341]\n",
      "loss: 0.918884  [166400/175341]\n",
      "loss: 0.303850  [168000/175341]\n",
      "loss: 0.537907  [169600/175341]\n",
      "loss: 0.429875  [171200/175341]\n",
      "loss: 0.495031  [172800/175341]\n",
      "loss: 0.365458  [174400/175341]\n",
      "Train Accuracy: 79.6317%\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.638278, F1-score: 73.56%, Macro_F1-Score:  37.90%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.219007  [    0/175341]\n",
      "loss: 0.722426  [ 1600/175341]\n",
      "loss: 0.650735  [ 3200/175341]\n",
      "loss: 0.967178  [ 4800/175341]\n",
      "loss: 0.592496  [ 6400/175341]\n",
      "loss: 0.363941  [ 8000/175341]\n",
      "loss: 0.571299  [ 9600/175341]\n",
      "loss: 0.536353  [11200/175341]\n",
      "loss: 0.329549  [12800/175341]\n",
      "loss: 0.440888  [14400/175341]\n",
      "loss: 0.619478  [16000/175341]\n",
      "loss: 0.229959  [17600/175341]\n",
      "loss: 0.391703  [19200/175341]\n",
      "loss: 0.848797  [20800/175341]\n",
      "loss: 0.307853  [22400/175341]\n",
      "loss: 0.351790  [24000/175341]\n",
      "loss: 0.666264  [25600/175341]\n",
      "loss: 1.099827  [27200/175341]\n",
      "loss: 0.332341  [28800/175341]\n",
      "loss: 0.596005  [30400/175341]\n",
      "loss: 0.411025  [32000/175341]\n",
      "loss: 0.693240  [33600/175341]\n",
      "loss: 1.050694  [35200/175341]\n",
      "loss: 0.222586  [36800/175341]\n",
      "loss: 0.211825  [38400/175341]\n",
      "loss: 1.166605  [40000/175341]\n",
      "loss: 0.601771  [41600/175341]\n",
      "loss: 0.495810  [43200/175341]\n",
      "loss: 0.566886  [44800/175341]\n",
      "loss: 0.488681  [46400/175341]\n",
      "loss: 0.465787  [48000/175341]\n",
      "loss: 0.413002  [49600/175341]\n",
      "loss: 0.752938  [51200/175341]\n",
      "loss: 0.545123  [52800/175341]\n",
      "loss: 0.436638  [54400/175341]\n",
      "loss: 0.601598  [56000/175341]\n",
      "loss: 0.382994  [57600/175341]\n",
      "loss: 0.441503  [59200/175341]\n",
      "loss: 0.408269  [60800/175341]\n",
      "loss: 0.607390  [62400/175341]\n",
      "loss: 0.749640  [64000/175341]\n",
      "loss: 0.531604  [65600/175341]\n",
      "loss: 0.661871  [67200/175341]\n",
      "loss: 0.305685  [68800/175341]\n",
      "loss: 0.336209  [70400/175341]\n",
      "loss: 0.634822  [72000/175341]\n",
      "loss: 0.653879  [73600/175341]\n",
      "loss: 0.482871  [75200/175341]\n",
      "loss: 0.691660  [76800/175341]\n",
      "loss: 0.273202  [78400/175341]\n",
      "loss: 0.641496  [80000/175341]\n",
      "loss: 0.908760  [81600/175341]\n",
      "loss: 0.356376  [83200/175341]\n",
      "loss: 0.266387  [84800/175341]\n",
      "loss: 0.351586  [86400/175341]\n",
      "loss: 0.539082  [88000/175341]\n",
      "loss: 0.467347  [89600/175341]\n",
      "loss: 0.490226  [91200/175341]\n",
      "loss: 0.358318  [92800/175341]\n",
      "loss: 0.412326  [94400/175341]\n",
      "loss: 0.272997  [96000/175341]\n",
      "loss: 0.727697  [97600/175341]\n",
      "loss: 0.327898  [99200/175341]\n",
      "loss: 0.619893  [100800/175341]\n",
      "loss: 0.894392  [102400/175341]\n",
      "loss: 0.397964  [104000/175341]\n",
      "loss: 0.746830  [105600/175341]\n",
      "loss: 0.759151  [107200/175341]\n",
      "loss: 0.341256  [108800/175341]\n",
      "loss: 0.239183  [110400/175341]\n",
      "loss: 0.138917  [112000/175341]\n",
      "loss: 0.430226  [113600/175341]\n",
      "loss: 0.343155  [115200/175341]\n",
      "loss: 0.265929  [116800/175341]\n",
      "loss: 0.461846  [118400/175341]\n",
      "loss: 0.453860  [120000/175341]\n",
      "loss: 0.738855  [121600/175341]\n",
      "loss: 1.070929  [123200/175341]\n",
      "loss: 0.550974  [124800/175341]\n",
      "loss: 1.188431  [126400/175341]\n",
      "loss: 0.510906  [128000/175341]\n",
      "loss: 0.407494  [129600/175341]\n",
      "loss: 0.564989  [131200/175341]\n",
      "loss: 0.453607  [132800/175341]\n",
      "loss: 0.427370  [134400/175341]\n",
      "loss: 0.303143  [136000/175341]\n",
      "loss: 0.478924  [137600/175341]\n",
      "loss: 0.344189  [139200/175341]\n",
      "loss: 0.175296  [140800/175341]\n",
      "loss: 0.743124  [142400/175341]\n",
      "loss: 0.443598  [144000/175341]\n",
      "loss: 0.458730  [145600/175341]\n",
      "loss: 0.240550  [147200/175341]\n",
      "loss: 0.386610  [148800/175341]\n",
      "loss: 0.437493  [150400/175341]\n",
      "loss: 0.728205  [152000/175341]\n",
      "loss: 0.633458  [153600/175341]\n",
      "loss: 0.598751  [155200/175341]\n",
      "loss: 0.373178  [156800/175341]\n",
      "loss: 0.759751  [158400/175341]\n",
      "loss: 0.639917  [160000/175341]\n",
      "loss: 0.148516  [161600/175341]\n",
      "loss: 0.803074  [163200/175341]\n",
      "loss: 0.506112  [164800/175341]\n",
      "loss: 0.597099  [166400/175341]\n",
      "loss: 0.563828  [168000/175341]\n",
      "loss: 0.908484  [169600/175341]\n",
      "loss: 0.133601  [171200/175341]\n",
      "loss: 0.523206  [172800/175341]\n",
      "loss: 0.369427  [174400/175341]\n",
      "Train Accuracy: 79.7760%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.609669, F1-score: 74.83%, Macro_F1-Score:  38.93%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.309858  [    0/175341]\n",
      "loss: 0.493378  [ 1600/175341]\n",
      "loss: 0.577074  [ 3200/175341]\n",
      "loss: 0.248818  [ 4800/175341]\n",
      "loss: 0.526905  [ 6400/175341]\n",
      "loss: 0.544709  [ 8000/175341]\n",
      "loss: 0.427216  [ 9600/175341]\n",
      "loss: 0.322375  [11200/175341]\n",
      "loss: 0.507650  [12800/175341]\n",
      "loss: 0.405193  [14400/175341]\n",
      "loss: 0.301526  [16000/175341]\n",
      "loss: 0.509334  [17600/175341]\n",
      "loss: 0.661149  [19200/175341]\n",
      "loss: 0.791176  [20800/175341]\n",
      "loss: 0.792848  [22400/175341]\n",
      "loss: 0.912688  [24000/175341]\n",
      "loss: 0.883868  [25600/175341]\n",
      "loss: 0.623532  [27200/175341]\n",
      "loss: 0.667169  [28800/175341]\n",
      "loss: 0.257568  [30400/175341]\n",
      "loss: 0.143567  [32000/175341]\n",
      "loss: 0.415480  [33600/175341]\n",
      "loss: 0.556955  [35200/175341]\n",
      "loss: 0.264527  [36800/175341]\n",
      "loss: 0.331809  [38400/175341]\n",
      "loss: 0.390199  [40000/175341]\n",
      "loss: 0.458327  [41600/175341]\n",
      "loss: 0.588832  [43200/175341]\n",
      "loss: 0.599438  [44800/175341]\n",
      "loss: 0.288775  [46400/175341]\n",
      "loss: 0.383530  [48000/175341]\n",
      "loss: 0.621387  [49600/175341]\n",
      "loss: 0.419945  [51200/175341]\n",
      "loss: 0.794992  [52800/175341]\n",
      "loss: 0.858212  [54400/175341]\n",
      "loss: 0.770894  [56000/175341]\n",
      "loss: 0.737650  [57600/175341]\n",
      "loss: 0.215294  [59200/175341]\n",
      "loss: 0.414642  [60800/175341]\n",
      "loss: 0.287252  [62400/175341]\n",
      "loss: 0.694259  [64000/175341]\n",
      "loss: 0.403228  [65600/175341]\n",
      "loss: 0.251108  [67200/175341]\n",
      "loss: 0.659895  [68800/175341]\n",
      "loss: 0.333702  [70400/175341]\n",
      "loss: 0.471985  [72000/175341]\n",
      "loss: 0.471867  [73600/175341]\n",
      "loss: 0.181429  [75200/175341]\n",
      "loss: 0.530818  [76800/175341]\n",
      "loss: 0.601339  [78400/175341]\n",
      "loss: 0.403302  [80000/175341]\n",
      "loss: 0.563917  [81600/175341]\n",
      "loss: 0.665729  [83200/175341]\n",
      "loss: 0.629696  [84800/175341]\n",
      "loss: 0.562321  [86400/175341]\n",
      "loss: 0.525048  [88000/175341]\n",
      "loss: 0.475156  [89600/175341]\n",
      "loss: 0.612319  [91200/175341]\n",
      "loss: 0.241303  [92800/175341]\n",
      "loss: 0.412890  [94400/175341]\n",
      "loss: 0.421182  [96000/175341]\n",
      "loss: 0.478061  [97600/175341]\n",
      "loss: 0.342520  [99200/175341]\n",
      "loss: 0.504932  [100800/175341]\n",
      "loss: 0.599891  [102400/175341]\n",
      "loss: 0.443314  [104000/175341]\n",
      "loss: 0.674890  [105600/175341]\n",
      "loss: 0.240795  [107200/175341]\n",
      "loss: 0.363286  [108800/175341]\n",
      "loss: 0.416975  [110400/175341]\n",
      "loss: 0.374134  [112000/175341]\n",
      "loss: 0.613601  [113600/175341]\n",
      "loss: 0.410751  [115200/175341]\n",
      "loss: 0.482018  [116800/175341]\n",
      "loss: 0.499794  [118400/175341]\n",
      "loss: 0.584494  [120000/175341]\n",
      "loss: 0.559174  [121600/175341]\n",
      "loss: 0.717673  [123200/175341]\n",
      "loss: 0.334268  [124800/175341]\n",
      "loss: 0.770267  [126400/175341]\n",
      "loss: 0.270633  [128000/175341]\n",
      "loss: 0.394835  [129600/175341]\n",
      "loss: 0.116283  [131200/175341]\n",
      "loss: 0.705725  [132800/175341]\n",
      "loss: 0.311047  [134400/175341]\n",
      "loss: 0.454473  [136000/175341]\n",
      "loss: 0.531563  [137600/175341]\n",
      "loss: 0.381063  [139200/175341]\n",
      "loss: 0.631041  [140800/175341]\n",
      "loss: 0.328841  [142400/175341]\n",
      "loss: 0.693257  [144000/175341]\n",
      "loss: 0.765488  [145600/175341]\n",
      "loss: 0.580424  [147200/175341]\n",
      "loss: 1.091581  [148800/175341]\n",
      "loss: 0.611631  [150400/175341]\n",
      "loss: 0.780961  [152000/175341]\n",
      "loss: 0.181371  [153600/175341]\n",
      "loss: 0.377376  [155200/175341]\n",
      "loss: 0.722828  [156800/175341]\n",
      "loss: 0.172145  [158400/175341]\n",
      "loss: 0.364982  [160000/175341]\n",
      "loss: 0.374740  [161600/175341]\n",
      "loss: 0.460250  [163200/175341]\n",
      "loss: 0.352057  [164800/175341]\n",
      "loss: 0.212758  [166400/175341]\n",
      "loss: 0.414482  [168000/175341]\n",
      "loss: 0.698883  [169600/175341]\n",
      "loss: 0.453919  [171200/175341]\n",
      "loss: 0.342259  [172800/175341]\n",
      "loss: 0.219425  [174400/175341]\n",
      "Train Accuracy: 79.8957%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.601371, F1-score: 74.83%, Macro_F1-Score:  39.14%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.325976  [    0/175341]\n",
      "loss: 0.312175  [ 1600/175341]\n",
      "loss: 0.715163  [ 3200/175341]\n",
      "loss: 0.491354  [ 4800/175341]\n",
      "loss: 0.996607  [ 6400/175341]\n",
      "loss: 0.143694  [ 8000/175341]\n",
      "loss: 0.452381  [ 9600/175341]\n",
      "loss: 0.361216  [11200/175341]\n",
      "loss: 0.262568  [12800/175341]\n",
      "loss: 0.670379  [14400/175341]\n",
      "loss: 0.256693  [16000/175341]\n",
      "loss: 0.515446  [17600/175341]\n",
      "loss: 0.148449  [19200/175341]\n",
      "loss: 0.940858  [20800/175341]\n",
      "loss: 0.437976  [22400/175341]\n",
      "loss: 1.070207  [24000/175341]\n",
      "loss: 0.507880  [25600/175341]\n",
      "loss: 0.512446  [27200/175341]\n",
      "loss: 0.434267  [28800/175341]\n",
      "loss: 0.982370  [30400/175341]\n",
      "loss: 0.376419  [32000/175341]\n",
      "loss: 0.262727  [33600/175341]\n",
      "loss: 0.587421  [35200/175341]\n",
      "loss: 0.297912  [36800/175341]\n",
      "loss: 0.432593  [38400/175341]\n",
      "loss: 0.253376  [40000/175341]\n",
      "loss: 0.499652  [41600/175341]\n",
      "loss: 0.295030  [43200/175341]\n",
      "loss: 0.495698  [44800/175341]\n",
      "loss: 0.428885  [46400/175341]\n",
      "loss: 0.492440  [48000/175341]\n",
      "loss: 0.456407  [49600/175341]\n",
      "loss: 0.241020  [51200/175341]\n",
      "loss: 0.419155  [52800/175341]\n",
      "loss: 0.687014  [54400/175341]\n",
      "loss: 0.473693  [56000/175341]\n",
      "loss: 0.366143  [57600/175341]\n",
      "loss: 0.571130  [59200/175341]\n",
      "loss: 0.342195  [60800/175341]\n",
      "loss: 0.462914  [62400/175341]\n",
      "loss: 0.664063  [64000/175341]\n",
      "loss: 0.266006  [65600/175341]\n",
      "loss: 0.981532  [67200/175341]\n",
      "loss: 0.497071  [68800/175341]\n",
      "loss: 0.380547  [70400/175341]\n",
      "loss: 0.546842  [72000/175341]\n",
      "loss: 0.285699  [73600/175341]\n",
      "loss: 0.355792  [75200/175341]\n",
      "loss: 0.637147  [76800/175341]\n",
      "loss: 0.695161  [78400/175341]\n",
      "loss: 0.368059  [80000/175341]\n",
      "loss: 0.270906  [81600/175341]\n",
      "loss: 0.677727  [83200/175341]\n",
      "loss: 0.485022  [84800/175341]\n",
      "loss: 0.445311  [86400/175341]\n",
      "loss: 0.549596  [88000/175341]\n",
      "loss: 0.286399  [89600/175341]\n",
      "loss: 0.734583  [91200/175341]\n",
      "loss: 0.582062  [92800/175341]\n",
      "loss: 0.314663  [94400/175341]\n",
      "loss: 0.460459  [96000/175341]\n",
      "loss: 0.654274  [97600/175341]\n",
      "loss: 0.525631  [99200/175341]\n",
      "loss: 0.593940  [100800/175341]\n",
      "loss: 0.278754  [102400/175341]\n",
      "loss: 0.406301  [104000/175341]\n",
      "loss: 0.419899  [105600/175341]\n",
      "loss: 0.301008  [107200/175341]\n",
      "loss: 0.421941  [108800/175341]\n",
      "loss: 0.420851  [110400/175341]\n",
      "loss: 0.798638  [112000/175341]\n",
      "loss: 0.620107  [113600/175341]\n",
      "loss: 0.329316  [115200/175341]\n",
      "loss: 0.217896  [116800/175341]\n",
      "loss: 0.377860  [118400/175341]\n",
      "loss: 0.242425  [120000/175341]\n",
      "loss: 0.375873  [121600/175341]\n",
      "loss: 0.321193  [123200/175341]\n",
      "loss: 0.585606  [124800/175341]\n",
      "loss: 0.277578  [126400/175341]\n",
      "loss: 0.529962  [128000/175341]\n",
      "loss: 0.216682  [129600/175341]\n",
      "loss: 0.262803  [131200/175341]\n",
      "loss: 0.198821  [132800/175341]\n",
      "loss: 0.408847  [134400/175341]\n",
      "loss: 0.483465  [136000/175341]\n",
      "loss: 0.422782  [137600/175341]\n",
      "loss: 0.652964  [139200/175341]\n",
      "loss: 0.322186  [140800/175341]\n",
      "loss: 0.395191  [142400/175341]\n",
      "loss: 1.475063  [144000/175341]\n",
      "loss: 0.257002  [145600/175341]\n",
      "loss: 0.180944  [147200/175341]\n",
      "loss: 1.257978  [148800/175341]\n",
      "loss: 0.516126  [150400/175341]\n",
      "loss: 0.219390  [152000/175341]\n",
      "loss: 0.669271  [153600/175341]\n",
      "loss: 0.344414  [155200/175341]\n",
      "loss: 0.424779  [156800/175341]\n",
      "loss: 0.924442  [158400/175341]\n",
      "loss: 0.340833  [160000/175341]\n",
      "loss: 0.712065  [161600/175341]\n",
      "loss: 0.906715  [163200/175341]\n",
      "loss: 0.605255  [164800/175341]\n",
      "loss: 0.511181  [166400/175341]\n",
      "loss: 0.223898  [168000/175341]\n",
      "loss: 0.311677  [169600/175341]\n",
      "loss: 0.357489  [171200/175341]\n",
      "loss: 0.221296  [172800/175341]\n",
      "loss: 0.621186  [174400/175341]\n",
      "Train Accuracy: 80.0600%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.615389, F1-score: 73.79%, Macro_F1-Score:  39.00%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.678396  [    0/175341]\n",
      "loss: 0.908650  [ 1600/175341]\n",
      "loss: 0.624990  [ 3200/175341]\n",
      "loss: 0.217833  [ 4800/175341]\n",
      "loss: 0.234064  [ 6400/175341]\n",
      "loss: 0.628412  [ 8000/175341]\n",
      "loss: 0.302826  [ 9600/175341]\n",
      "loss: 0.802137  [11200/175341]\n",
      "loss: 0.907220  [12800/175341]\n",
      "loss: 0.466770  [14400/175341]\n",
      "loss: 0.280771  [16000/175341]\n",
      "loss: 0.602702  [17600/175341]\n",
      "loss: 0.404913  [19200/175341]\n",
      "loss: 0.673205  [20800/175341]\n",
      "loss: 0.499932  [22400/175341]\n",
      "loss: 0.360776  [24000/175341]\n",
      "loss: 0.170000  [25600/175341]\n",
      "loss: 0.389202  [27200/175341]\n",
      "loss: 0.234748  [28800/175341]\n",
      "loss: 0.797565  [30400/175341]\n",
      "loss: 0.248746  [32000/175341]\n",
      "loss: 0.367632  [33600/175341]\n",
      "loss: 0.439236  [35200/175341]\n",
      "loss: 0.586685  [36800/175341]\n",
      "loss: 0.715413  [38400/175341]\n",
      "loss: 0.571080  [40000/175341]\n",
      "loss: 0.948406  [41600/175341]\n",
      "loss: 0.903874  [43200/175341]\n",
      "loss: 0.205814  [44800/175341]\n",
      "loss: 0.739108  [46400/175341]\n",
      "loss: 1.052157  [48000/175341]\n",
      "loss: 0.620486  [49600/175341]\n",
      "loss: 0.739202  [51200/175341]\n",
      "loss: 0.331793  [52800/175341]\n",
      "loss: 0.361206  [54400/175341]\n",
      "loss: 0.635905  [56000/175341]\n",
      "loss: 0.687644  [57600/175341]\n",
      "loss: 0.482819  [59200/175341]\n",
      "loss: 0.354176  [60800/175341]\n",
      "loss: 0.557510  [62400/175341]\n",
      "loss: 0.303103  [64000/175341]\n",
      "loss: 0.586733  [65600/175341]\n",
      "loss: 0.558899  [67200/175341]\n",
      "loss: 0.847517  [68800/175341]\n",
      "loss: 0.116189  [70400/175341]\n",
      "loss: 0.683890  [72000/175341]\n",
      "loss: 0.638263  [73600/175341]\n",
      "loss: 0.492633  [75200/175341]\n",
      "loss: 0.372030  [76800/175341]\n",
      "loss: 0.282317  [78400/175341]\n",
      "loss: 0.322080  [80000/175341]\n",
      "loss: 0.487661  [81600/175341]\n",
      "loss: 0.440111  [83200/175341]\n",
      "loss: 0.514840  [84800/175341]\n",
      "loss: 0.385591  [86400/175341]\n",
      "loss: 0.621542  [88000/175341]\n",
      "loss: 0.299840  [89600/175341]\n",
      "loss: 0.358772  [91200/175341]\n",
      "loss: 0.584404  [92800/175341]\n",
      "loss: 0.217329  [94400/175341]\n",
      "loss: 0.520884  [96000/175341]\n",
      "loss: 1.414604  [97600/175341]\n",
      "loss: 0.215321  [99200/175341]\n",
      "loss: 0.517234  [100800/175341]\n",
      "loss: 0.355018  [102400/175341]\n",
      "loss: 0.432923  [104000/175341]\n",
      "loss: 0.548615  [105600/175341]\n",
      "loss: 0.466497  [107200/175341]\n",
      "loss: 0.524230  [108800/175341]\n",
      "loss: 0.531870  [110400/175341]\n",
      "loss: 0.537031  [112000/175341]\n",
      "loss: 0.486392  [113600/175341]\n",
      "loss: 0.687114  [115200/175341]\n",
      "loss: 0.460101  [116800/175341]\n",
      "loss: 0.544274  [118400/175341]\n",
      "loss: 0.340863  [120000/175341]\n",
      "loss: 0.891445  [121600/175341]\n",
      "loss: 0.521569  [123200/175341]\n",
      "loss: 0.516029  [124800/175341]\n",
      "loss: 0.375887  [126400/175341]\n",
      "loss: 0.313885  [128000/175341]\n",
      "loss: 0.589454  [129600/175341]\n",
      "loss: 0.287242  [131200/175341]\n",
      "loss: 0.334596  [132800/175341]\n",
      "loss: 0.386089  [134400/175341]\n",
      "loss: 0.344915  [136000/175341]\n",
      "loss: 0.176133  [137600/175341]\n",
      "loss: 0.467510  [139200/175341]\n",
      "loss: 0.643878  [140800/175341]\n",
      "loss: 0.136220  [142400/175341]\n",
      "loss: 0.794449  [144000/175341]\n",
      "loss: 0.823247  [145600/175341]\n",
      "loss: 0.327496  [147200/175341]\n",
      "loss: 0.396228  [148800/175341]\n",
      "loss: 0.261986  [150400/175341]\n",
      "loss: 0.422424  [152000/175341]\n",
      "loss: 0.298373  [153600/175341]\n",
      "loss: 0.414961  [155200/175341]\n",
      "loss: 0.626139  [156800/175341]\n",
      "loss: 0.640903  [158400/175341]\n",
      "loss: 0.571782  [160000/175341]\n",
      "loss: 0.752006  [161600/175341]\n",
      "loss: 0.477941  [163200/175341]\n",
      "loss: 0.715384  [164800/175341]\n",
      "loss: 0.398042  [166400/175341]\n",
      "loss: 1.297948  [168000/175341]\n",
      "loss: 0.464527  [169600/175341]\n",
      "loss: 0.282584  [171200/175341]\n",
      "loss: 0.662195  [172800/175341]\n",
      "loss: 0.270322  [174400/175341]\n",
      "Train Accuracy: 80.1341%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.601615, F1-score: 75.17%, Macro_F1-Score:  39.69%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.526330  [    0/175341]\n",
      "loss: 0.241493  [ 1600/175341]\n",
      "loss: 0.109810  [ 3200/175341]\n",
      "loss: 0.462123  [ 4800/175341]\n",
      "loss: 0.691385  [ 6400/175341]\n",
      "loss: 0.453850  [ 8000/175341]\n",
      "loss: 0.546372  [ 9600/175341]\n",
      "loss: 1.044866  [11200/175341]\n",
      "loss: 0.597673  [12800/175341]\n",
      "loss: 0.198042  [14400/175341]\n",
      "loss: 0.358045  [16000/175341]\n",
      "loss: 0.723429  [17600/175341]\n",
      "loss: 0.400179  [19200/175341]\n",
      "loss: 0.551713  [20800/175341]\n",
      "loss: 0.526209  [22400/175341]\n",
      "loss: 0.419753  [24000/175341]\n",
      "loss: 0.239254  [25600/175341]\n",
      "loss: 0.759852  [27200/175341]\n",
      "loss: 0.260184  [28800/175341]\n",
      "loss: 0.272878  [30400/175341]\n",
      "loss: 0.835471  [32000/175341]\n",
      "loss: 0.593803  [33600/175341]\n",
      "loss: 0.351160  [35200/175341]\n",
      "loss: 0.419718  [36800/175341]\n",
      "loss: 0.763889  [38400/175341]\n",
      "loss: 0.658865  [40000/175341]\n",
      "loss: 0.680140  [41600/175341]\n",
      "loss: 0.359412  [43200/175341]\n",
      "loss: 0.219143  [44800/175341]\n",
      "loss: 0.534405  [46400/175341]\n",
      "loss: 0.602016  [48000/175341]\n",
      "loss: 0.304393  [49600/175341]\n",
      "loss: 0.322224  [51200/175341]\n",
      "loss: 0.319625  [52800/175341]\n",
      "loss: 0.421528  [54400/175341]\n",
      "loss: 0.376824  [56000/175341]\n",
      "loss: 0.838697  [57600/175341]\n",
      "loss: 0.218267  [59200/175341]\n",
      "loss: 0.910279  [60800/175341]\n",
      "loss: 0.348155  [62400/175341]\n",
      "loss: 0.699979  [64000/175341]\n",
      "loss: 0.285916  [65600/175341]\n",
      "loss: 0.500841  [67200/175341]\n",
      "loss: 0.591756  [68800/175341]\n",
      "loss: 0.180889  [70400/175341]\n",
      "loss: 0.832535  [72000/175341]\n",
      "loss: 0.462012  [73600/175341]\n",
      "loss: 0.610604  [75200/175341]\n",
      "loss: 0.290184  [76800/175341]\n",
      "loss: 0.521492  [78400/175341]\n",
      "loss: 0.311868  [80000/175341]\n",
      "loss: 0.402601  [81600/175341]\n",
      "loss: 0.691210  [83200/175341]\n",
      "loss: 0.338672  [84800/175341]\n",
      "loss: 0.321905  [86400/175341]\n",
      "loss: 0.264869  [88000/175341]\n",
      "loss: 0.391592  [89600/175341]\n",
      "loss: 0.537506  [91200/175341]\n",
      "loss: 0.249871  [92800/175341]\n",
      "loss: 0.604156  [94400/175341]\n",
      "loss: 0.373853  [96000/175341]\n",
      "loss: 0.849230  [97600/175341]\n",
      "loss: 0.547916  [99200/175341]\n",
      "loss: 0.768579  [100800/175341]\n",
      "loss: 0.698360  [102400/175341]\n",
      "loss: 0.689110  [104000/175341]\n",
      "loss: 0.405718  [105600/175341]\n",
      "loss: 0.340356  [107200/175341]\n",
      "loss: 0.196303  [108800/175341]\n",
      "loss: 0.686812  [110400/175341]\n",
      "loss: 0.595059  [112000/175341]\n",
      "loss: 0.400772  [113600/175341]\n",
      "loss: 0.773595  [115200/175341]\n",
      "loss: 0.642970  [116800/175341]\n",
      "loss: 0.238274  [118400/175341]\n",
      "loss: 0.214927  [120000/175341]\n",
      "loss: 0.607875  [121600/175341]\n",
      "loss: 0.354929  [123200/175341]\n",
      "loss: 0.443576  [124800/175341]\n",
      "loss: 0.418350  [126400/175341]\n",
      "loss: 0.521462  [128000/175341]\n",
      "loss: 0.406057  [129600/175341]\n",
      "loss: 0.348711  [131200/175341]\n",
      "loss: 0.826507  [132800/175341]\n",
      "loss: 0.621274  [134400/175341]\n",
      "loss: 0.549874  [136000/175341]\n",
      "loss: 0.550899  [137600/175341]\n",
      "loss: 0.516305  [139200/175341]\n",
      "loss: 0.466369  [140800/175341]\n",
      "loss: 0.462766  [142400/175341]\n",
      "loss: 0.207261  [144000/175341]\n",
      "loss: 0.537078  [145600/175341]\n",
      "loss: 0.142334  [147200/175341]\n",
      "loss: 0.511497  [148800/175341]\n",
      "loss: 0.851549  [150400/175341]\n",
      "loss: 0.562122  [152000/175341]\n",
      "loss: 0.427932  [153600/175341]\n",
      "loss: 0.480472  [155200/175341]\n",
      "loss: 0.266993  [156800/175341]\n",
      "loss: 0.434743  [158400/175341]\n",
      "loss: 0.561925  [160000/175341]\n",
      "loss: 0.297415  [161600/175341]\n",
      "loss: 0.728122  [163200/175341]\n",
      "loss: 0.525391  [164800/175341]\n",
      "loss: 0.858648  [166400/175341]\n",
      "loss: 0.426272  [168000/175341]\n",
      "loss: 0.453173  [169600/175341]\n",
      "loss: 0.691438  [171200/175341]\n",
      "loss: 0.588999  [172800/175341]\n",
      "loss: 0.944562  [174400/175341]\n",
      "Train Accuracy: 80.2168%\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.626163, F1-score: 73.14%, Macro_F1-Score:  38.31%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.983738  [    0/175341]\n",
      "loss: 0.706157  [ 1600/175341]\n",
      "loss: 0.178377  [ 3200/175341]\n",
      "loss: 0.112721  [ 4800/175341]\n",
      "loss: 0.296287  [ 6400/175341]\n",
      "loss: 0.504528  [ 8000/175341]\n",
      "loss: 0.504778  [ 9600/175341]\n",
      "loss: 0.252302  [11200/175341]\n",
      "loss: 0.765979  [12800/175341]\n",
      "loss: 0.472612  [14400/175341]\n",
      "loss: 0.473535  [16000/175341]\n",
      "loss: 0.522170  [17600/175341]\n",
      "loss: 0.560410  [19200/175341]\n",
      "loss: 0.879526  [20800/175341]\n",
      "loss: 0.686575  [22400/175341]\n",
      "loss: 0.592146  [24000/175341]\n",
      "loss: 0.257682  [25600/175341]\n",
      "loss: 0.667251  [27200/175341]\n",
      "loss: 0.485144  [28800/175341]\n",
      "loss: 0.135613  [30400/175341]\n",
      "loss: 0.312818  [32000/175341]\n",
      "loss: 0.532326  [33600/175341]\n",
      "loss: 0.569339  [35200/175341]\n",
      "loss: 0.556861  [36800/175341]\n",
      "loss: 0.469090  [38400/175341]\n",
      "loss: 0.947294  [40000/175341]\n",
      "loss: 0.721424  [41600/175341]\n",
      "loss: 0.605691  [43200/175341]\n",
      "loss: 0.423399  [44800/175341]\n",
      "loss: 0.828690  [46400/175341]\n",
      "loss: 0.267651  [48000/175341]\n",
      "loss: 0.561505  [49600/175341]\n",
      "loss: 0.578453  [51200/175341]\n",
      "loss: 0.126030  [52800/175341]\n",
      "loss: 0.214143  [54400/175341]\n",
      "loss: 0.694887  [56000/175341]\n",
      "loss: 0.805786  [57600/175341]\n",
      "loss: 0.569522  [59200/175341]\n",
      "loss: 0.444342  [60800/175341]\n",
      "loss: 0.542989  [62400/175341]\n",
      "loss: 0.211994  [64000/175341]\n",
      "loss: 0.837989  [65600/175341]\n",
      "loss: 0.116268  [67200/175341]\n",
      "loss: 0.223301  [68800/175341]\n",
      "loss: 0.503795  [70400/175341]\n",
      "loss: 0.448462  [72000/175341]\n",
      "loss: 0.627314  [73600/175341]\n",
      "loss: 1.344003  [75200/175341]\n",
      "loss: 0.509546  [76800/175341]\n",
      "loss: 0.253592  [78400/175341]\n",
      "loss: 0.659490  [80000/175341]\n",
      "loss: 0.654258  [81600/175341]\n",
      "loss: 0.471491  [83200/175341]\n",
      "loss: 0.334306  [84800/175341]\n",
      "loss: 0.210238  [86400/175341]\n",
      "loss: 0.814232  [88000/175341]\n",
      "loss: 0.477031  [89600/175341]\n",
      "loss: 0.516223  [91200/175341]\n",
      "loss: 0.285915  [92800/175341]\n",
      "loss: 0.777969  [94400/175341]\n",
      "loss: 0.707929  [96000/175341]\n",
      "loss: 0.324797  [97600/175341]\n",
      "loss: 0.677169  [99200/175341]\n",
      "loss: 0.568521  [100800/175341]\n",
      "loss: 0.547053  [102400/175341]\n",
      "loss: 0.405453  [104000/175341]\n",
      "loss: 0.640769  [105600/175341]\n",
      "loss: 0.452829  [107200/175341]\n",
      "loss: 0.344654  [108800/175341]\n",
      "loss: 0.804798  [110400/175341]\n",
      "loss: 0.342143  [112000/175341]\n",
      "loss: 0.496559  [113600/175341]\n",
      "loss: 0.964639  [115200/175341]\n",
      "loss: 0.421318  [116800/175341]\n",
      "loss: 0.339541  [118400/175341]\n",
      "loss: 0.435356  [120000/175341]\n",
      "loss: 0.106567  [121600/175341]\n",
      "loss: 0.495055  [123200/175341]\n",
      "loss: 0.736955  [124800/175341]\n",
      "loss: 0.336597  [126400/175341]\n",
      "loss: 0.875134  [128000/175341]\n",
      "loss: 1.038232  [129600/175341]\n",
      "loss: 0.128391  [131200/175341]\n",
      "loss: 0.495527  [132800/175341]\n",
      "loss: 0.165485  [134400/175341]\n",
      "loss: 0.438274  [136000/175341]\n",
      "loss: 0.498010  [137600/175341]\n",
      "loss: 0.552391  [139200/175341]\n",
      "loss: 0.378431  [140800/175341]\n",
      "loss: 0.513940  [142400/175341]\n",
      "loss: 0.387198  [144000/175341]\n",
      "loss: 0.499073  [145600/175341]\n",
      "loss: 0.908496  [147200/175341]\n",
      "loss: 0.630918  [148800/175341]\n",
      "loss: 0.359176  [150400/175341]\n",
      "loss: 0.185588  [152000/175341]\n",
      "loss: 0.204114  [153600/175341]\n",
      "loss: 1.311716  [155200/175341]\n",
      "loss: 0.349175  [156800/175341]\n",
      "loss: 0.457691  [158400/175341]\n",
      "loss: 0.472626  [160000/175341]\n",
      "loss: 0.613958  [161600/175341]\n",
      "loss: 0.785387  [163200/175341]\n",
      "loss: 0.205338  [164800/175341]\n",
      "loss: 0.486659  [166400/175341]\n",
      "loss: 0.412472  [168000/175341]\n",
      "loss: 0.361983  [169600/175341]\n",
      "loss: 0.390599  [171200/175341]\n",
      "loss: 0.191545  [172800/175341]\n",
      "loss: 0.566456  [174400/175341]\n",
      "Train Accuracy: 80.2066%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.588351, F1-score: 75.04%, Macro_F1-Score:  40.03%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.560924  [    0/175341]\n",
      "loss: 0.458214  [ 1600/175341]\n",
      "loss: 0.730892  [ 3200/175341]\n",
      "loss: 0.332032  [ 4800/175341]\n",
      "loss: 0.170981  [ 6400/175341]\n",
      "loss: 0.571823  [ 8000/175341]\n",
      "loss: 0.281030  [ 9600/175341]\n",
      "loss: 0.729062  [11200/175341]\n",
      "loss: 0.231493  [12800/175341]\n",
      "loss: 0.793730  [14400/175341]\n",
      "loss: 0.872057  [16000/175341]\n",
      "loss: 0.357306  [17600/175341]\n",
      "loss: 0.703986  [19200/175341]\n",
      "loss: 0.486992  [20800/175341]\n",
      "loss: 0.156125  [22400/175341]\n",
      "loss: 0.393294  [24000/175341]\n",
      "loss: 0.634550  [25600/175341]\n",
      "loss: 0.667110  [27200/175341]\n",
      "loss: 0.359556  [28800/175341]\n",
      "loss: 0.536091  [30400/175341]\n",
      "loss: 0.414207  [32000/175341]\n",
      "loss: 0.917182  [33600/175341]\n",
      "loss: 0.276435  [35200/175341]\n",
      "loss: 0.520443  [36800/175341]\n",
      "loss: 0.383061  [38400/175341]\n",
      "loss: 0.445075  [40000/175341]\n",
      "loss: 0.817705  [41600/175341]\n",
      "loss: 0.570720  [43200/175341]\n",
      "loss: 0.381398  [44800/175341]\n",
      "loss: 0.832097  [46400/175341]\n",
      "loss: 0.357736  [48000/175341]\n",
      "loss: 0.261787  [49600/175341]\n",
      "loss: 0.587234  [51200/175341]\n",
      "loss: 0.547961  [52800/175341]\n",
      "loss: 0.275389  [54400/175341]\n",
      "loss: 0.399936  [56000/175341]\n",
      "loss: 0.258821  [57600/175341]\n",
      "loss: 0.467784  [59200/175341]\n",
      "loss: 0.297941  [60800/175341]\n",
      "loss: 0.204456  [62400/175341]\n",
      "loss: 0.195955  [64000/175341]\n",
      "loss: 0.432731  [65600/175341]\n",
      "loss: 0.674061  [67200/175341]\n",
      "loss: 0.524595  [68800/175341]\n",
      "loss: 0.368940  [70400/175341]\n",
      "loss: 0.420096  [72000/175341]\n",
      "loss: 0.305270  [73600/175341]\n",
      "loss: 0.368790  [75200/175341]\n",
      "loss: 0.801703  [76800/175341]\n",
      "loss: 0.366464  [78400/175341]\n",
      "loss: 1.056623  [80000/175341]\n",
      "loss: 0.827067  [81600/175341]\n",
      "loss: 0.456309  [83200/175341]\n",
      "loss: 0.506392  [84800/175341]\n",
      "loss: 0.199165  [86400/175341]\n",
      "loss: 0.227661  [88000/175341]\n",
      "loss: 0.531376  [89600/175341]\n",
      "loss: 0.469745  [91200/175341]\n",
      "loss: 0.359551  [92800/175341]\n",
      "loss: 0.655719  [94400/175341]\n",
      "loss: 0.403241  [96000/175341]\n",
      "loss: 0.760801  [97600/175341]\n",
      "loss: 0.981421  [99200/175341]\n",
      "loss: 0.469083  [100800/175341]\n",
      "loss: 0.485728  [102400/175341]\n",
      "loss: 0.496342  [104000/175341]\n",
      "loss: 0.271366  [105600/175341]\n",
      "loss: 0.317419  [107200/175341]\n",
      "loss: 0.912105  [108800/175341]\n",
      "loss: 0.917053  [110400/175341]\n",
      "loss: 0.295139  [112000/175341]\n",
      "loss: 0.279821  [113600/175341]\n",
      "loss: 0.572950  [115200/175341]\n",
      "loss: 0.646573  [116800/175341]\n",
      "loss: 0.413198  [118400/175341]\n",
      "loss: 0.803794  [120000/175341]\n",
      "loss: 0.469719  [121600/175341]\n",
      "loss: 0.643433  [123200/175341]\n",
      "loss: 0.206732  [124800/175341]\n",
      "loss: 0.907142  [126400/175341]\n",
      "loss: 0.532406  [128000/175341]\n",
      "loss: 0.252894  [129600/175341]\n",
      "loss: 0.425914  [131200/175341]\n",
      "loss: 0.370808  [132800/175341]\n",
      "loss: 0.376842  [134400/175341]\n",
      "loss: 0.406779  [136000/175341]\n",
      "loss: 0.337515  [137600/175341]\n",
      "loss: 0.592037  [139200/175341]\n",
      "loss: 0.555084  [140800/175341]\n",
      "loss: 0.625990  [142400/175341]\n",
      "loss: 0.312686  [144000/175341]\n",
      "loss: 0.238729  [145600/175341]\n",
      "loss: 0.616878  [147200/175341]\n",
      "loss: 0.389556  [148800/175341]\n",
      "loss: 0.086680  [150400/175341]\n",
      "loss: 0.620632  [152000/175341]\n",
      "loss: 0.279137  [153600/175341]\n",
      "loss: 0.338675  [155200/175341]\n",
      "loss: 0.394511  [156800/175341]\n",
      "loss: 0.873797  [158400/175341]\n",
      "loss: 0.448815  [160000/175341]\n",
      "loss: 0.370706  [161600/175341]\n",
      "loss: 0.442051  [163200/175341]\n",
      "loss: 0.646325  [164800/175341]\n",
      "loss: 0.296200  [166400/175341]\n",
      "loss: 0.361727  [168000/175341]\n",
      "loss: 0.950467  [169600/175341]\n",
      "loss: 0.382539  [171200/175341]\n",
      "loss: 0.425233  [172800/175341]\n",
      "loss: 0.284772  [174400/175341]\n",
      "Train Accuracy: 80.3041%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.593543, F1-score: 75.19%, Macro_F1-Score:  39.33%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.834802  [    0/175341]\n",
      "loss: 0.712421  [ 1600/175341]\n",
      "loss: 0.231405  [ 3200/175341]\n",
      "loss: 0.655364  [ 4800/175341]\n",
      "loss: 0.849402  [ 6400/175341]\n",
      "loss: 0.413723  [ 8000/175341]\n",
      "loss: 0.557821  [ 9600/175341]\n",
      "loss: 0.971634  [11200/175341]\n",
      "loss: 0.386289  [12800/175341]\n",
      "loss: 0.415781  [14400/175341]\n",
      "loss: 0.939667  [16000/175341]\n",
      "loss: 0.406138  [17600/175341]\n",
      "loss: 0.720891  [19200/175341]\n",
      "loss: 0.430573  [20800/175341]\n",
      "loss: 0.801022  [22400/175341]\n",
      "loss: 0.593874  [24000/175341]\n",
      "loss: 0.560021  [25600/175341]\n",
      "loss: 0.425556  [27200/175341]\n",
      "loss: 0.483966  [28800/175341]\n",
      "loss: 0.385477  [30400/175341]\n",
      "loss: 0.631526  [32000/175341]\n",
      "loss: 0.223041  [33600/175341]\n",
      "loss: 0.336666  [35200/175341]\n",
      "loss: 0.449918  [36800/175341]\n",
      "loss: 0.755355  [38400/175341]\n",
      "loss: 0.670704  [40000/175341]\n",
      "loss: 0.687517  [41600/175341]\n",
      "loss: 0.488090  [43200/175341]\n",
      "loss: 0.592252  [44800/175341]\n",
      "loss: 0.466613  [46400/175341]\n",
      "loss: 0.369665  [48000/175341]\n",
      "loss: 0.250771  [49600/175341]\n",
      "loss: 0.584142  [51200/175341]\n",
      "loss: 0.410537  [52800/175341]\n",
      "loss: 0.695714  [54400/175341]\n",
      "loss: 0.299891  [56000/175341]\n",
      "loss: 0.387334  [57600/175341]\n",
      "loss: 0.230366  [59200/175341]\n",
      "loss: 0.210467  [60800/175341]\n",
      "loss: 0.500165  [62400/175341]\n",
      "loss: 0.401875  [64000/175341]\n",
      "loss: 0.240586  [65600/175341]\n",
      "loss: 0.313961  [67200/175341]\n",
      "loss: 0.335986  [68800/175341]\n",
      "loss: 0.248588  [70400/175341]\n",
      "loss: 0.460386  [72000/175341]\n",
      "loss: 0.751656  [73600/175341]\n",
      "loss: 0.053379  [75200/175341]\n",
      "loss: 0.401902  [76800/175341]\n",
      "loss: 0.333715  [78400/175341]\n",
      "loss: 0.442843  [80000/175341]\n",
      "loss: 0.323601  [81600/175341]\n",
      "loss: 0.198429  [83200/175341]\n",
      "loss: 0.871819  [84800/175341]\n",
      "loss: 0.476851  [86400/175341]\n",
      "loss: 0.604566  [88000/175341]\n",
      "loss: 0.590501  [89600/175341]\n",
      "loss: 0.753630  [91200/175341]\n",
      "loss: 0.413902  [92800/175341]\n",
      "loss: 0.349499  [94400/175341]\n",
      "loss: 0.497871  [96000/175341]\n",
      "loss: 0.391535  [97600/175341]\n",
      "loss: 0.559264  [99200/175341]\n",
      "loss: 0.442619  [100800/175341]\n",
      "loss: 0.497572  [102400/175341]\n",
      "loss: 0.279870  [104000/175341]\n",
      "loss: 0.510780  [105600/175341]\n",
      "loss: 0.605227  [107200/175341]\n",
      "loss: 0.415829  [108800/175341]\n",
      "loss: 0.320427  [110400/175341]\n",
      "loss: 0.399212  [112000/175341]\n",
      "loss: 0.235350  [113600/175341]\n",
      "loss: 0.671819  [115200/175341]\n",
      "loss: 0.367864  [116800/175341]\n",
      "loss: 0.090451  [118400/175341]\n",
      "loss: 0.425618  [120000/175341]\n",
      "loss: 0.510825  [121600/175341]\n",
      "loss: 0.338161  [123200/175341]\n",
      "loss: 0.881625  [124800/175341]\n",
      "loss: 0.338547  [126400/175341]\n",
      "loss: 0.589465  [128000/175341]\n",
      "loss: 0.330084  [129600/175341]\n",
      "loss: 0.399954  [131200/175341]\n",
      "loss: 0.631224  [132800/175341]\n",
      "loss: 1.427740  [134400/175341]\n",
      "loss: 0.611815  [136000/175341]\n",
      "loss: 0.364177  [137600/175341]\n",
      "loss: 0.180562  [139200/175341]\n",
      "loss: 0.320820  [140800/175341]\n",
      "loss: 0.485346  [142400/175341]\n",
      "loss: 0.352347  [144000/175341]\n",
      "loss: 0.230528  [145600/175341]\n",
      "loss: 0.493346  [147200/175341]\n",
      "loss: 0.628395  [148800/175341]\n",
      "loss: 0.569352  [150400/175341]\n",
      "loss: 0.193765  [152000/175341]\n",
      "loss: 0.332701  [153600/175341]\n",
      "loss: 0.226114  [155200/175341]\n",
      "loss: 0.655974  [156800/175341]\n",
      "loss: 0.284029  [158400/175341]\n",
      "loss: 0.903080  [160000/175341]\n",
      "loss: 0.674714  [161600/175341]\n",
      "loss: 0.548850  [163200/175341]\n",
      "loss: 0.458310  [164800/175341]\n",
      "loss: 0.488969  [166400/175341]\n",
      "loss: 0.203436  [168000/175341]\n",
      "loss: 0.449783  [169600/175341]\n",
      "loss: 0.429535  [171200/175341]\n",
      "loss: 1.044987  [172800/175341]\n",
      "loss: 0.543278  [174400/175341]\n",
      "Train Accuracy: 80.3549%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.588716, F1-score: 75.18%, Macro_F1-Score:  39.81%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.556625  [    0/175341]\n",
      "loss: 0.243228  [ 1600/175341]\n",
      "loss: 0.619959  [ 3200/175341]\n",
      "loss: 0.442082  [ 4800/175341]\n",
      "loss: 0.434601  [ 6400/175341]\n",
      "loss: 0.320627  [ 8000/175341]\n",
      "loss: 1.103903  [ 9600/175341]\n",
      "loss: 0.719542  [11200/175341]\n",
      "loss: 0.726070  [12800/175341]\n",
      "loss: 0.584131  [14400/175341]\n",
      "loss: 0.914198  [16000/175341]\n",
      "loss: 0.634799  [17600/175341]\n",
      "loss: 0.687391  [19200/175341]\n",
      "loss: 0.262247  [20800/175341]\n",
      "loss: 0.485946  [22400/175341]\n",
      "loss: 0.820587  [24000/175341]\n",
      "loss: 0.285947  [25600/175341]\n",
      "loss: 0.160629  [27200/175341]\n",
      "loss: 0.205547  [28800/175341]\n",
      "loss: 0.694541  [30400/175341]\n",
      "loss: 0.578623  [32000/175341]\n",
      "loss: 0.512018  [33600/175341]\n",
      "loss: 0.486159  [35200/175341]\n",
      "loss: 0.456061  [36800/175341]\n",
      "loss: 0.480698  [38400/175341]\n",
      "loss: 0.612763  [40000/175341]\n",
      "loss: 0.711939  [41600/175341]\n",
      "loss: 0.428806  [43200/175341]\n",
      "loss: 0.590699  [44800/175341]\n",
      "loss: 0.710665  [46400/175341]\n",
      "loss: 0.632226  [48000/175341]\n",
      "loss: 0.716986  [49600/175341]\n",
      "loss: 0.525560  [51200/175341]\n",
      "loss: 0.214647  [52800/175341]\n",
      "loss: 0.619552  [54400/175341]\n",
      "loss: 0.346473  [56000/175341]\n",
      "loss: 0.181825  [57600/175341]\n",
      "loss: 0.653035  [59200/175341]\n",
      "loss: 1.053024  [60800/175341]\n",
      "loss: 0.458461  [62400/175341]\n",
      "loss: 0.263619  [64000/175341]\n",
      "loss: 0.381919  [65600/175341]\n",
      "loss: 0.480792  [67200/175341]\n",
      "loss: 0.458315  [68800/175341]\n",
      "loss: 0.436191  [70400/175341]\n",
      "loss: 0.264345  [72000/175341]\n",
      "loss: 0.668712  [73600/175341]\n",
      "loss: 0.463054  [75200/175341]\n",
      "loss: 0.254459  [76800/175341]\n",
      "loss: 0.469237  [78400/175341]\n",
      "loss: 0.527677  [80000/175341]\n",
      "loss: 0.801339  [81600/175341]\n",
      "loss: 0.612943  [83200/175341]\n",
      "loss: 0.452772  [84800/175341]\n",
      "loss: 0.644855  [86400/175341]\n",
      "loss: 0.477076  [88000/175341]\n",
      "loss: 0.379678  [89600/175341]\n",
      "loss: 0.812292  [91200/175341]\n",
      "loss: 0.304269  [92800/175341]\n",
      "loss: 0.469947  [94400/175341]\n",
      "loss: 0.512655  [96000/175341]\n",
      "loss: 0.558830  [97600/175341]\n",
      "loss: 0.476483  [99200/175341]\n",
      "loss: 0.447383  [100800/175341]\n",
      "loss: 0.357265  [102400/175341]\n",
      "loss: 0.599155  [104000/175341]\n",
      "loss: 0.533297  [105600/175341]\n",
      "loss: 0.548552  [107200/175341]\n",
      "loss: 1.050636  [108800/175341]\n",
      "loss: 0.564178  [110400/175341]\n",
      "loss: 0.389757  [112000/175341]\n",
      "loss: 0.201910  [113600/175341]\n",
      "loss: 0.230244  [115200/175341]\n",
      "loss: 0.444919  [116800/175341]\n",
      "loss: 0.764386  [118400/175341]\n",
      "loss: 0.567943  [120000/175341]\n",
      "loss: 0.476641  [121600/175341]\n",
      "loss: 0.263292  [123200/175341]\n",
      "loss: 0.671906  [124800/175341]\n",
      "loss: 0.371739  [126400/175341]\n",
      "loss: 0.234792  [128000/175341]\n",
      "loss: 0.312494  [129600/175341]\n",
      "loss: 0.637848  [131200/175341]\n",
      "loss: 0.756426  [132800/175341]\n",
      "loss: 0.821819  [134400/175341]\n",
      "loss: 0.454560  [136000/175341]\n",
      "loss: 0.206020  [137600/175341]\n",
      "loss: 0.388509  [139200/175341]\n",
      "loss: 0.583571  [140800/175341]\n",
      "loss: 0.222602  [142400/175341]\n",
      "loss: 0.209740  [144000/175341]\n",
      "loss: 0.131613  [145600/175341]\n",
      "loss: 0.388499  [147200/175341]\n",
      "loss: 0.507943  [148800/175341]\n",
      "loss: 0.784343  [150400/175341]\n",
      "loss: 0.755886  [152000/175341]\n",
      "loss: 0.296142  [153600/175341]\n",
      "loss: 0.352051  [155200/175341]\n",
      "loss: 0.201925  [156800/175341]\n",
      "loss: 0.222076  [158400/175341]\n",
      "loss: 0.290718  [160000/175341]\n",
      "loss: 0.903343  [161600/175341]\n",
      "loss: 0.671057  [163200/175341]\n",
      "loss: 0.639568  [164800/175341]\n",
      "loss: 0.505182  [166400/175341]\n",
      "loss: 0.766280  [168000/175341]\n",
      "loss: 0.769145  [169600/175341]\n",
      "loss: 0.548580  [171200/175341]\n",
      "loss: 0.425080  [172800/175341]\n",
      "loss: 0.252224  [174400/175341]\n",
      "Train Accuracy: 80.3737%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.589064, F1-score: 74.62%, Macro_F1-Score:  39.59%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.771657  [    0/175341]\n",
      "loss: 0.168205  [ 1600/175341]\n",
      "loss: 0.281464  [ 3200/175341]\n",
      "loss: 0.534405  [ 4800/175341]\n",
      "loss: 0.229749  [ 6400/175341]\n",
      "loss: 0.307495  [ 8000/175341]\n",
      "loss: 0.499635  [ 9600/175341]\n",
      "loss: 0.586375  [11200/175341]\n",
      "loss: 0.337579  [12800/175341]\n",
      "loss: 0.274571  [14400/175341]\n",
      "loss: 0.463134  [16000/175341]\n",
      "loss: 0.313017  [17600/175341]\n",
      "loss: 0.293282  [19200/175341]\n",
      "loss: 0.562691  [20800/175341]\n",
      "loss: 0.272164  [22400/175341]\n",
      "loss: 0.340214  [24000/175341]\n",
      "loss: 0.485009  [25600/175341]\n",
      "loss: 0.377735  [27200/175341]\n",
      "loss: 0.489330  [28800/175341]\n",
      "loss: 0.440367  [30400/175341]\n",
      "loss: 0.383549  [32000/175341]\n",
      "loss: 0.098716  [33600/175341]\n",
      "loss: 0.499571  [35200/175341]\n",
      "loss: 0.805969  [36800/175341]\n",
      "loss: 0.580841  [38400/175341]\n",
      "loss: 0.523250  [40000/175341]\n",
      "loss: 0.451974  [41600/175341]\n",
      "loss: 0.538645  [43200/175341]\n",
      "loss: 0.697060  [44800/175341]\n",
      "loss: 0.691626  [46400/175341]\n",
      "loss: 0.877562  [48000/175341]\n",
      "loss: 0.419229  [49600/175341]\n",
      "loss: 0.418844  [51200/175341]\n",
      "loss: 0.456044  [52800/175341]\n",
      "loss: 0.705094  [54400/175341]\n",
      "loss: 0.505134  [56000/175341]\n",
      "loss: 0.375715  [57600/175341]\n",
      "loss: 0.512104  [59200/175341]\n",
      "loss: 0.844972  [60800/175341]\n",
      "loss: 0.487793  [62400/175341]\n",
      "loss: 0.223453  [64000/175341]\n",
      "loss: 0.605297  [65600/175341]\n",
      "loss: 0.582272  [67200/175341]\n",
      "loss: 0.547352  [68800/175341]\n",
      "loss: 0.514087  [70400/175341]\n",
      "loss: 0.250909  [72000/175341]\n",
      "loss: 0.376289  [73600/175341]\n",
      "loss: 0.475113  [75200/175341]\n",
      "loss: 0.556471  [76800/175341]\n",
      "loss: 0.615794  [78400/175341]\n",
      "loss: 0.357496  [80000/175341]\n",
      "loss: 0.369294  [81600/175341]\n",
      "loss: 0.261320  [83200/175341]\n",
      "loss: 0.529173  [84800/175341]\n",
      "loss: 0.489766  [86400/175341]\n",
      "loss: 0.198324  [88000/175341]\n",
      "loss: 0.151593  [89600/175341]\n",
      "loss: 0.767201  [91200/175341]\n",
      "loss: 0.984580  [92800/175341]\n",
      "loss: 0.424748  [94400/175341]\n",
      "loss: 0.680806  [96000/175341]\n",
      "loss: 0.433083  [97600/175341]\n",
      "loss: 0.768432  [99200/175341]\n",
      "loss: 0.387194  [100800/175341]\n",
      "loss: 0.659457  [102400/175341]\n",
      "loss: 0.570747  [104000/175341]\n",
      "loss: 0.704333  [105600/175341]\n",
      "loss: 0.219664  [107200/175341]\n",
      "loss: 0.836364  [108800/175341]\n",
      "loss: 0.705771  [110400/175341]\n",
      "loss: 0.853646  [112000/175341]\n",
      "loss: 0.511090  [113600/175341]\n",
      "loss: 0.469794  [115200/175341]\n",
      "loss: 0.535899  [116800/175341]\n",
      "loss: 0.430584  [118400/175341]\n",
      "loss: 0.839459  [120000/175341]\n",
      "loss: 1.098465  [121600/175341]\n",
      "loss: 0.539323  [123200/175341]\n",
      "loss: 0.267224  [124800/175341]\n",
      "loss: 0.931924  [126400/175341]\n",
      "loss: 0.416160  [128000/175341]\n",
      "loss: 0.179338  [129600/175341]\n",
      "loss: 0.407339  [131200/175341]\n",
      "loss: 0.645383  [132800/175341]\n",
      "loss: 0.245893  [134400/175341]\n",
      "loss: 0.611356  [136000/175341]\n",
      "loss: 0.279332  [137600/175341]\n",
      "loss: 0.761611  [139200/175341]\n",
      "loss: 0.382634  [140800/175341]\n",
      "loss: 0.305274  [142400/175341]\n",
      "loss: 0.524033  [144000/175341]\n",
      "loss: 0.650217  [145600/175341]\n",
      "loss: 0.264612  [147200/175341]\n",
      "loss: 0.729949  [148800/175341]\n",
      "loss: 0.691598  [150400/175341]\n",
      "loss: 0.451206  [152000/175341]\n",
      "loss: 0.327706  [153600/175341]\n",
      "loss: 0.535168  [155200/175341]\n",
      "loss: 0.816259  [156800/175341]\n",
      "loss: 0.472466  [158400/175341]\n",
      "loss: 0.280830  [160000/175341]\n",
      "loss: 0.957276  [161600/175341]\n",
      "loss: 0.117769  [163200/175341]\n",
      "loss: 0.259329  [164800/175341]\n",
      "loss: 0.251236  [166400/175341]\n",
      "loss: 0.340479  [168000/175341]\n",
      "loss: 0.446200  [169600/175341]\n",
      "loss: 0.426121  [171200/175341]\n",
      "loss: 0.245398  [172800/175341]\n",
      "loss: 0.426204  [174400/175341]\n",
      "Train Accuracy: 80.4518%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.587001, F1-score: 74.70%, Macro_F1-Score:  39.86%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.250811  [    0/175341]\n",
      "loss: 0.466697  [ 1600/175341]\n",
      "loss: 0.148555  [ 3200/175341]\n",
      "loss: 0.619872  [ 4800/175341]\n",
      "loss: 0.367536  [ 6400/175341]\n",
      "loss: 0.470296  [ 8000/175341]\n",
      "loss: 0.573704  [ 9600/175341]\n",
      "loss: 0.744553  [11200/175341]\n",
      "loss: 0.432017  [12800/175341]\n",
      "loss: 0.438274  [14400/175341]\n",
      "loss: 0.373802  [16000/175341]\n",
      "loss: 0.637406  [17600/175341]\n",
      "loss: 0.258373  [19200/175341]\n",
      "loss: 0.466208  [20800/175341]\n",
      "loss: 0.403333  [22400/175341]\n",
      "loss: 0.540150  [24000/175341]\n",
      "loss: 0.380501  [25600/175341]\n",
      "loss: 0.774382  [27200/175341]\n",
      "loss: 0.285800  [28800/175341]\n",
      "loss: 0.553019  [30400/175341]\n",
      "loss: 0.275850  [32000/175341]\n",
      "loss: 0.331276  [33600/175341]\n",
      "loss: 0.192400  [35200/175341]\n",
      "loss: 0.385264  [36800/175341]\n",
      "loss: 0.474399  [38400/175341]\n",
      "loss: 0.765441  [40000/175341]\n",
      "loss: 0.320304  [41600/175341]\n",
      "loss: 0.416482  [43200/175341]\n",
      "loss: 0.449683  [44800/175341]\n",
      "loss: 0.652231  [46400/175341]\n",
      "loss: 0.100401  [48000/175341]\n",
      "loss: 0.192688  [49600/175341]\n",
      "loss: 0.590068  [51200/175341]\n",
      "loss: 0.413927  [52800/175341]\n",
      "loss: 1.051482  [54400/175341]\n",
      "loss: 0.686268  [56000/175341]\n",
      "loss: 0.504458  [57600/175341]\n",
      "loss: 0.576459  [59200/175341]\n",
      "loss: 0.180749  [60800/175341]\n",
      "loss: 0.653695  [62400/175341]\n",
      "loss: 0.608265  [64000/175341]\n",
      "loss: 0.339425  [65600/175341]\n",
      "loss: 0.275727  [67200/175341]\n",
      "loss: 0.699262  [68800/175341]\n",
      "loss: 0.237730  [70400/175341]\n",
      "loss: 0.401658  [72000/175341]\n",
      "loss: 0.426085  [73600/175341]\n",
      "loss: 0.608000  [75200/175341]\n",
      "loss: 0.558785  [76800/175341]\n",
      "loss: 0.701688  [78400/175341]\n",
      "loss: 0.740907  [80000/175341]\n",
      "loss: 0.406055  [81600/175341]\n",
      "loss: 0.175736  [83200/175341]\n",
      "loss: 0.622983  [84800/175341]\n",
      "loss: 0.473650  [86400/175341]\n",
      "loss: 0.664785  [88000/175341]\n",
      "loss: 0.701359  [89600/175341]\n",
      "loss: 0.304763  [91200/175341]\n",
      "loss: 0.544791  [92800/175341]\n",
      "loss: 0.339623  [94400/175341]\n",
      "loss: 0.430904  [96000/175341]\n",
      "loss: 0.464875  [97600/175341]\n",
      "loss: 0.445913  [99200/175341]\n",
      "loss: 0.382214  [100800/175341]\n",
      "loss: 0.876286  [102400/175341]\n",
      "loss: 0.289764  [104000/175341]\n",
      "loss: 0.168395  [105600/175341]\n",
      "loss: 0.757559  [107200/175341]\n",
      "loss: 0.733549  [108800/175341]\n",
      "loss: 0.435873  [110400/175341]\n",
      "loss: 0.434943  [112000/175341]\n",
      "loss: 0.429666  [113600/175341]\n",
      "loss: 0.557093  [115200/175341]\n",
      "loss: 0.776178  [116800/175341]\n",
      "loss: 0.261208  [118400/175341]\n",
      "loss: 0.214374  [120000/175341]\n",
      "loss: 0.384452  [121600/175341]\n",
      "loss: 0.396563  [123200/175341]\n",
      "loss: 0.370259  [124800/175341]\n",
      "loss: 0.421867  [126400/175341]\n",
      "loss: 0.248831  [128000/175341]\n",
      "loss: 0.537556  [129600/175341]\n",
      "loss: 0.370545  [131200/175341]\n",
      "loss: 0.294820  [132800/175341]\n",
      "loss: 0.513727  [134400/175341]\n",
      "loss: 0.996741  [136000/175341]\n",
      "loss: 0.758489  [137600/175341]\n",
      "loss: 0.433676  [139200/175341]\n",
      "loss: 0.555663  [140800/175341]\n",
      "loss: 0.408866  [142400/175341]\n",
      "loss: 0.517562  [144000/175341]\n",
      "loss: 0.682239  [145600/175341]\n",
      "loss: 0.392682  [147200/175341]\n",
      "loss: 0.564222  [148800/175341]\n",
      "loss: 0.409058  [150400/175341]\n",
      "loss: 0.423562  [152000/175341]\n",
      "loss: 0.454311  [153600/175341]\n",
      "loss: 0.679286  [155200/175341]\n",
      "loss: 0.537797  [156800/175341]\n",
      "loss: 0.321122  [158400/175341]\n",
      "loss: 0.238577  [160000/175341]\n",
      "loss: 0.593050  [161600/175341]\n",
      "loss: 0.464059  [163200/175341]\n",
      "loss: 0.178732  [164800/175341]\n",
      "loss: 0.608151  [166400/175341]\n",
      "loss: 0.397205  [168000/175341]\n",
      "loss: 0.416017  [169600/175341]\n",
      "loss: 0.316406  [171200/175341]\n",
      "loss: 0.364653  [172800/175341]\n",
      "loss: 0.528974  [174400/175341]\n",
      "Train Accuracy: 80.4860%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.596911, F1-score: 74.91%, Macro_F1-Score:  39.15%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.488222  [    0/175341]\n",
      "loss: 0.812876  [ 1600/175341]\n",
      "loss: 0.323645  [ 3200/175341]\n",
      "loss: 0.430153  [ 4800/175341]\n",
      "loss: 0.098934  [ 6400/175341]\n",
      "loss: 0.547227  [ 8000/175341]\n",
      "loss: 0.934178  [ 9600/175341]\n",
      "loss: 0.625195  [11200/175341]\n",
      "loss: 0.546345  [12800/175341]\n",
      "loss: 0.355339  [14400/175341]\n",
      "loss: 0.570999  [16000/175341]\n",
      "loss: 0.644096  [17600/175341]\n",
      "loss: 1.063812  [19200/175341]\n",
      "loss: 0.515722  [20800/175341]\n",
      "loss: 0.236562  [22400/175341]\n",
      "loss: 0.598402  [24000/175341]\n",
      "loss: 0.538275  [25600/175341]\n",
      "loss: 0.343229  [27200/175341]\n",
      "loss: 0.817282  [28800/175341]\n",
      "loss: 0.333384  [30400/175341]\n",
      "loss: 0.247037  [32000/175341]\n",
      "loss: 0.518397  [33600/175341]\n",
      "loss: 0.864827  [35200/175341]\n",
      "loss: 0.096597  [36800/175341]\n",
      "loss: 0.723175  [38400/175341]\n",
      "loss: 0.495343  [40000/175341]\n",
      "loss: 0.378396  [41600/175341]\n",
      "loss: 0.359710  [43200/175341]\n",
      "loss: 0.415735  [44800/175341]\n",
      "loss: 0.291370  [46400/175341]\n",
      "loss: 0.736696  [48000/175341]\n",
      "loss: 0.482063  [49600/175341]\n",
      "loss: 0.247831  [51200/175341]\n",
      "loss: 0.498574  [52800/175341]\n",
      "loss: 0.371888  [54400/175341]\n",
      "loss: 0.395927  [56000/175341]\n",
      "loss: 0.243027  [57600/175341]\n",
      "loss: 0.399729  [59200/175341]\n",
      "loss: 0.402262  [60800/175341]\n",
      "loss: 0.253918  [62400/175341]\n",
      "loss: 0.458694  [64000/175341]\n",
      "loss: 0.501382  [65600/175341]\n",
      "loss: 0.397829  [67200/175341]\n",
      "loss: 0.209674  [68800/175341]\n",
      "loss: 0.306082  [70400/175341]\n",
      "loss: 0.963661  [72000/175341]\n",
      "loss: 0.357077  [73600/175341]\n",
      "loss: 0.433237  [75200/175341]\n",
      "loss: 0.466592  [76800/175341]\n",
      "loss: 0.584452  [78400/175341]\n",
      "loss: 0.659964  [80000/175341]\n",
      "loss: 0.478411  [81600/175341]\n",
      "loss: 0.341155  [83200/175341]\n",
      "loss: 0.480030  [84800/175341]\n",
      "loss: 0.417098  [86400/175341]\n",
      "loss: 0.568605  [88000/175341]\n",
      "loss: 0.580661  [89600/175341]\n",
      "loss: 0.177413  [91200/175341]\n",
      "loss: 0.338345  [92800/175341]\n",
      "loss: 0.390292  [94400/175341]\n",
      "loss: 0.781984  [96000/175341]\n",
      "loss: 0.512773  [97600/175341]\n",
      "loss: 0.774769  [99200/175341]\n",
      "loss: 0.129267  [100800/175341]\n",
      "loss: 0.595644  [102400/175341]\n",
      "loss: 0.188301  [104000/175341]\n",
      "loss: 0.362341  [105600/175341]\n",
      "loss: 0.264934  [107200/175341]\n",
      "loss: 0.722573  [108800/175341]\n",
      "loss: 0.659867  [110400/175341]\n",
      "loss: 0.462150  [112000/175341]\n",
      "loss: 0.337169  [113600/175341]\n",
      "loss: 0.335525  [115200/175341]\n",
      "loss: 0.345151  [116800/175341]\n",
      "loss: 0.640209  [118400/175341]\n",
      "loss: 0.346440  [120000/175341]\n",
      "loss: 0.698303  [121600/175341]\n",
      "loss: 0.890519  [123200/175341]\n",
      "loss: 0.437017  [124800/175341]\n",
      "loss: 0.660432  [126400/175341]\n",
      "loss: 0.240010  [128000/175341]\n",
      "loss: 0.359120  [129600/175341]\n",
      "loss: 0.640783  [131200/175341]\n",
      "loss: 0.726878  [132800/175341]\n",
      "loss: 0.175174  [134400/175341]\n",
      "loss: 1.142080  [136000/175341]\n",
      "loss: 0.119139  [137600/175341]\n",
      "loss: 0.710346  [139200/175341]\n",
      "loss: 0.432538  [140800/175341]\n",
      "loss: 0.431584  [142400/175341]\n",
      "loss: 0.347101  [144000/175341]\n",
      "loss: 0.581334  [145600/175341]\n",
      "loss: 0.738914  [147200/175341]\n",
      "loss: 0.344678  [148800/175341]\n",
      "loss: 0.356061  [150400/175341]\n",
      "loss: 0.329114  [152000/175341]\n",
      "loss: 0.347305  [153600/175341]\n",
      "loss: 0.194118  [155200/175341]\n",
      "loss: 0.481737  [156800/175341]\n",
      "loss: 0.268062  [158400/175341]\n",
      "loss: 0.163631  [160000/175341]\n",
      "loss: 0.871270  [161600/175341]\n",
      "loss: 0.490573  [163200/175341]\n",
      "loss: 0.480997  [164800/175341]\n",
      "loss: 0.299907  [166400/175341]\n",
      "loss: 0.756448  [168000/175341]\n",
      "loss: 0.323983  [169600/175341]\n",
      "loss: 0.886614  [171200/175341]\n",
      "loss: 0.494139  [172800/175341]\n",
      "loss: 0.462774  [174400/175341]\n",
      "Train Accuracy: 80.5083%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.574133, F1-score: 75.37%, Macro_F1-Score:  39.82%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.512113  [    0/175341]\n",
      "loss: 0.406191  [ 1600/175341]\n",
      "loss: 0.762367  [ 3200/175341]\n",
      "loss: 0.927645  [ 4800/175341]\n",
      "loss: 0.482649  [ 6400/175341]\n",
      "loss: 0.390649  [ 8000/175341]\n",
      "loss: 0.810075  [ 9600/175341]\n",
      "loss: 0.461154  [11200/175341]\n",
      "loss: 0.345715  [12800/175341]\n",
      "loss: 0.263818  [14400/175341]\n",
      "loss: 0.435789  [16000/175341]\n",
      "loss: 0.855836  [17600/175341]\n",
      "loss: 0.619173  [19200/175341]\n",
      "loss: 0.816350  [20800/175341]\n",
      "loss: 0.267978  [22400/175341]\n",
      "loss: 0.654465  [24000/175341]\n",
      "loss: 0.480182  [25600/175341]\n",
      "loss: 0.582357  [27200/175341]\n",
      "loss: 1.037773  [28800/175341]\n",
      "loss: 0.158521  [30400/175341]\n",
      "loss: 0.333233  [32000/175341]\n",
      "loss: 0.809996  [33600/175341]\n",
      "loss: 0.334079  [35200/175341]\n",
      "loss: 0.516299  [36800/175341]\n",
      "loss: 0.291446  [38400/175341]\n",
      "loss: 0.277240  [40000/175341]\n",
      "loss: 0.325250  [41600/175341]\n",
      "loss: 0.397601  [43200/175341]\n",
      "loss: 0.196749  [44800/175341]\n",
      "loss: 0.442048  [46400/175341]\n",
      "loss: 0.579467  [48000/175341]\n",
      "loss: 0.425527  [49600/175341]\n",
      "loss: 0.451789  [51200/175341]\n",
      "loss: 0.393072  [52800/175341]\n",
      "loss: 0.248611  [54400/175341]\n",
      "loss: 0.690532  [56000/175341]\n",
      "loss: 0.194897  [57600/175341]\n",
      "loss: 0.822226  [59200/175341]\n",
      "loss: 0.882091  [60800/175341]\n",
      "loss: 0.688329  [62400/175341]\n",
      "loss: 0.269998  [64000/175341]\n",
      "loss: 0.586727  [65600/175341]\n",
      "loss: 0.586405  [67200/175341]\n",
      "loss: 0.569818  [68800/175341]\n",
      "loss: 0.555360  [70400/175341]\n",
      "loss: 0.665072  [72000/175341]\n",
      "loss: 0.501898  [73600/175341]\n",
      "loss: 0.143767  [75200/175341]\n",
      "loss: 0.714402  [76800/175341]\n",
      "loss: 0.235324  [78400/175341]\n",
      "loss: 0.661501  [80000/175341]\n",
      "loss: 0.405343  [81600/175341]\n",
      "loss: 0.893289  [83200/175341]\n",
      "loss: 0.613621  [84800/175341]\n",
      "loss: 0.652034  [86400/175341]\n",
      "loss: 0.632569  [88000/175341]\n",
      "loss: 0.598170  [89600/175341]\n",
      "loss: 0.461605  [91200/175341]\n",
      "loss: 0.285094  [92800/175341]\n",
      "loss: 0.392063  [94400/175341]\n",
      "loss: 0.481766  [96000/175341]\n",
      "loss: 0.744959  [97600/175341]\n",
      "loss: 0.319685  [99200/175341]\n",
      "loss: 0.339624  [100800/175341]\n",
      "loss: 0.389747  [102400/175341]\n",
      "loss: 0.419905  [104000/175341]\n",
      "loss: 0.274298  [105600/175341]\n",
      "loss: 0.717443  [107200/175341]\n",
      "loss: 0.512944  [108800/175341]\n",
      "loss: 0.480158  [110400/175341]\n",
      "loss: 0.201762  [112000/175341]\n",
      "loss: 0.968690  [113600/175341]\n",
      "loss: 0.305659  [115200/175341]\n",
      "loss: 0.444128  [116800/175341]\n",
      "loss: 0.776060  [118400/175341]\n",
      "loss: 0.492367  [120000/175341]\n",
      "loss: 0.439360  [121600/175341]\n",
      "loss: 0.481529  [123200/175341]\n",
      "loss: 0.465330  [124800/175341]\n",
      "loss: 0.304977  [126400/175341]\n",
      "loss: 0.420990  [128000/175341]\n",
      "loss: 0.306443  [129600/175341]\n",
      "loss: 0.445987  [131200/175341]\n",
      "loss: 0.470056  [132800/175341]\n",
      "loss: 0.514961  [134400/175341]\n",
      "loss: 0.525309  [136000/175341]\n",
      "loss: 0.160141  [137600/175341]\n",
      "loss: 0.231007  [139200/175341]\n",
      "loss: 0.647922  [140800/175341]\n",
      "loss: 0.881201  [142400/175341]\n",
      "loss: 0.451605  [144000/175341]\n",
      "loss: 0.272835  [145600/175341]\n",
      "loss: 0.468978  [147200/175341]\n",
      "loss: 0.373759  [148800/175341]\n",
      "loss: 0.218497  [150400/175341]\n",
      "loss: 0.777222  [152000/175341]\n",
      "loss: 0.221829  [153600/175341]\n",
      "loss: 0.502442  [155200/175341]\n",
      "loss: 0.637325  [156800/175341]\n",
      "loss: 0.550595  [158400/175341]\n",
      "loss: 0.421932  [160000/175341]\n",
      "loss: 0.675556  [161600/175341]\n",
      "loss: 0.692594  [163200/175341]\n",
      "loss: 0.764661  [164800/175341]\n",
      "loss: 0.576336  [166400/175341]\n",
      "loss: 0.502554  [168000/175341]\n",
      "loss: 0.525023  [169600/175341]\n",
      "loss: 0.675683  [171200/175341]\n",
      "loss: 0.804685  [172800/175341]\n",
      "loss: 0.146651  [174400/175341]\n",
      "Train Accuracy: 80.5647%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.566782, F1-score: 75.57%, Macro_F1-Score:  40.52%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.939047  [    0/175341]\n",
      "loss: 0.600206  [ 1600/175341]\n",
      "loss: 0.600286  [ 3200/175341]\n",
      "loss: 0.773122  [ 4800/175341]\n",
      "loss: 0.342099  [ 6400/175341]\n",
      "loss: 0.729933  [ 8000/175341]\n",
      "loss: 0.642473  [ 9600/175341]\n",
      "loss: 0.294057  [11200/175341]\n",
      "loss: 0.440892  [12800/175341]\n",
      "loss: 0.337756  [14400/175341]\n",
      "loss: 0.212676  [16000/175341]\n",
      "loss: 0.235744  [17600/175341]\n",
      "loss: 0.077401  [19200/175341]\n",
      "loss: 0.341210  [20800/175341]\n",
      "loss: 0.342967  [22400/175341]\n",
      "loss: 0.891742  [24000/175341]\n",
      "loss: 0.512906  [25600/175341]\n",
      "loss: 0.555637  [27200/175341]\n",
      "loss: 0.542556  [28800/175341]\n",
      "loss: 0.632968  [30400/175341]\n",
      "loss: 0.246087  [32000/175341]\n",
      "loss: 0.266735  [33600/175341]\n",
      "loss: 0.617301  [35200/175341]\n",
      "loss: 0.672811  [36800/175341]\n",
      "loss: 0.262929  [38400/175341]\n",
      "loss: 0.633347  [40000/175341]\n",
      "loss: 0.214451  [41600/175341]\n",
      "loss: 0.247591  [43200/175341]\n",
      "loss: 0.467026  [44800/175341]\n",
      "loss: 0.775865  [46400/175341]\n",
      "loss: 0.954157  [48000/175341]\n",
      "loss: 0.369131  [49600/175341]\n",
      "loss: 0.239864  [51200/175341]\n",
      "loss: 0.438580  [52800/175341]\n",
      "loss: 0.517979  [54400/175341]\n",
      "loss: 0.276452  [56000/175341]\n",
      "loss: 0.248527  [57600/175341]\n",
      "loss: 0.280113  [59200/175341]\n",
      "loss: 0.779316  [60800/175341]\n",
      "loss: 0.451148  [62400/175341]\n",
      "loss: 0.252618  [64000/175341]\n",
      "loss: 0.672284  [65600/175341]\n",
      "loss: 0.560274  [67200/175341]\n",
      "loss: 0.782867  [68800/175341]\n",
      "loss: 0.858278  [70400/175341]\n",
      "loss: 0.432524  [72000/175341]\n",
      "loss: 0.197838  [73600/175341]\n",
      "loss: 0.146452  [75200/175341]\n",
      "loss: 0.781331  [76800/175341]\n",
      "loss: 0.287846  [78400/175341]\n",
      "loss: 0.333056  [80000/175341]\n",
      "loss: 0.601850  [81600/175341]\n",
      "loss: 0.635957  [83200/175341]\n",
      "loss: 0.450454  [84800/175341]\n",
      "loss: 0.467201  [86400/175341]\n",
      "loss: 0.473409  [88000/175341]\n",
      "loss: 0.238290  [89600/175341]\n",
      "loss: 0.383520  [91200/175341]\n",
      "loss: 0.824967  [92800/175341]\n",
      "loss: 0.247558  [94400/175341]\n",
      "loss: 0.899049  [96000/175341]\n",
      "loss: 0.360404  [97600/175341]\n",
      "loss: 0.896390  [99200/175341]\n",
      "loss: 0.387433  [100800/175341]\n",
      "loss: 0.095797  [102400/175341]\n",
      "loss: 0.451965  [104000/175341]\n",
      "loss: 0.817796  [105600/175341]\n",
      "loss: 0.512624  [107200/175341]\n",
      "loss: 0.569972  [108800/175341]\n",
      "loss: 0.517781  [110400/175341]\n",
      "loss: 0.390910  [112000/175341]\n",
      "loss: 0.743238  [113600/175341]\n",
      "loss: 0.529100  [115200/175341]\n",
      "loss: 0.599814  [116800/175341]\n",
      "loss: 0.459287  [118400/175341]\n",
      "loss: 0.617903  [120000/175341]\n",
      "loss: 0.597455  [121600/175341]\n",
      "loss: 0.481144  [123200/175341]\n",
      "loss: 0.379993  [124800/175341]\n",
      "loss: 0.508151  [126400/175341]\n",
      "loss: 0.420949  [128000/175341]\n",
      "loss: 0.441689  [129600/175341]\n",
      "loss: 0.242677  [131200/175341]\n",
      "loss: 0.410813  [132800/175341]\n",
      "loss: 0.603365  [134400/175341]\n",
      "loss: 0.785325  [136000/175341]\n",
      "loss: 0.465153  [137600/175341]\n",
      "loss: 0.606664  [139200/175341]\n",
      "loss: 0.293234  [140800/175341]\n",
      "loss: 0.242767  [142400/175341]\n",
      "loss: 0.366903  [144000/175341]\n",
      "loss: 0.547148  [145600/175341]\n",
      "loss: 0.685870  [147200/175341]\n",
      "loss: 0.847450  [148800/175341]\n",
      "loss: 0.700529  [150400/175341]\n",
      "loss: 0.284278  [152000/175341]\n",
      "loss: 0.389002  [153600/175341]\n",
      "loss: 0.631989  [155200/175341]\n",
      "loss: 0.334921  [156800/175341]\n",
      "loss: 0.106009  [158400/175341]\n",
      "loss: 0.367270  [160000/175341]\n",
      "loss: 0.166141  [161600/175341]\n",
      "loss: 0.176934  [163200/175341]\n",
      "loss: 0.428888  [164800/175341]\n",
      "loss: 0.443648  [166400/175341]\n",
      "loss: 0.857610  [168000/175341]\n",
      "loss: 0.444283  [169600/175341]\n",
      "loss: 0.315965  [171200/175341]\n",
      "loss: 0.372917  [172800/175341]\n",
      "loss: 0.822266  [174400/175341]\n",
      "Train Accuracy: 80.6115%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.600438, F1-score: 74.15%, Macro_F1-Score:  39.02%  \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.631229  [    0/175341]\n",
      "loss: 0.441399  [ 1600/175341]\n",
      "loss: 0.360566  [ 3200/175341]\n",
      "loss: 0.275430  [ 4800/175341]\n",
      "loss: 0.352198  [ 6400/175341]\n",
      "loss: 0.194416  [ 8000/175341]\n",
      "loss: 0.333766  [ 9600/175341]\n",
      "loss: 1.254878  [11200/175341]\n",
      "loss: 1.030543  [12800/175341]\n",
      "loss: 0.855835  [14400/175341]\n",
      "loss: 0.841503  [16000/175341]\n",
      "loss: 0.700888  [17600/175341]\n",
      "loss: 0.414373  [19200/175341]\n",
      "loss: 0.267262  [20800/175341]\n",
      "loss: 0.378083  [22400/175341]\n",
      "loss: 0.543447  [24000/175341]\n",
      "loss: 0.371579  [25600/175341]\n",
      "loss: 0.733768  [27200/175341]\n",
      "loss: 0.530861  [28800/175341]\n",
      "loss: 0.489823  [30400/175341]\n",
      "loss: 0.408667  [32000/175341]\n",
      "loss: 0.270183  [33600/175341]\n",
      "loss: 0.411486  [35200/175341]\n",
      "loss: 0.380058  [36800/175341]\n",
      "loss: 0.762395  [38400/175341]\n",
      "loss: 0.505741  [40000/175341]\n",
      "loss: 0.523421  [41600/175341]\n",
      "loss: 0.744079  [43200/175341]\n",
      "loss: 0.260983  [44800/175341]\n",
      "loss: 0.285704  [46400/175341]\n",
      "loss: 0.445498  [48000/175341]\n",
      "loss: 0.653333  [49600/175341]\n",
      "loss: 0.212220  [51200/175341]\n",
      "loss: 0.342339  [52800/175341]\n",
      "loss: 0.409312  [54400/175341]\n",
      "loss: 0.796817  [56000/175341]\n",
      "loss: 0.612800  [57600/175341]\n",
      "loss: 0.668343  [59200/175341]\n",
      "loss: 0.405589  [60800/175341]\n",
      "loss: 0.752016  [62400/175341]\n",
      "loss: 0.421469  [64000/175341]\n",
      "loss: 0.226230  [65600/175341]\n",
      "loss: 0.497360  [67200/175341]\n",
      "loss: 0.733367  [68800/175341]\n",
      "loss: 0.407858  [70400/175341]\n",
      "loss: 0.328296  [72000/175341]\n",
      "loss: 0.276968  [73600/175341]\n",
      "loss: 0.857144  [75200/175341]\n",
      "loss: 0.178182  [76800/175341]\n",
      "loss: 0.641027  [78400/175341]\n",
      "loss: 0.631721  [80000/175341]\n",
      "loss: 0.508206  [81600/175341]\n",
      "loss: 0.230932  [83200/175341]\n",
      "loss: 0.909416  [84800/175341]\n",
      "loss: 0.663728  [86400/175341]\n",
      "loss: 0.631646  [88000/175341]\n",
      "loss: 0.441856  [89600/175341]\n",
      "loss: 0.333444  [91200/175341]\n",
      "loss: 0.388066  [92800/175341]\n",
      "loss: 0.371089  [94400/175341]\n",
      "loss: 0.204507  [96000/175341]\n",
      "loss: 0.620777  [97600/175341]\n",
      "loss: 0.542616  [99200/175341]\n",
      "loss: 0.493953  [100800/175341]\n",
      "loss: 0.512878  [102400/175341]\n",
      "loss: 0.799772  [104000/175341]\n",
      "loss: 0.214462  [105600/175341]\n",
      "loss: 0.488579  [107200/175341]\n",
      "loss: 0.723146  [108800/175341]\n",
      "loss: 0.387809  [110400/175341]\n",
      "loss: 0.667217  [112000/175341]\n",
      "loss: 0.419081  [113600/175341]\n",
      "loss: 0.488019  [115200/175341]\n",
      "loss: 0.782222  [116800/175341]\n",
      "loss: 0.283511  [118400/175341]\n",
      "loss: 1.086913  [120000/175341]\n",
      "loss: 0.106020  [121600/175341]\n",
      "loss: 0.324474  [123200/175341]\n",
      "loss: 0.581727  [124800/175341]\n",
      "loss: 0.499124  [126400/175341]\n",
      "loss: 0.313902  [128000/175341]\n",
      "loss: 0.487770  [129600/175341]\n",
      "loss: 0.800992  [131200/175341]\n",
      "loss: 0.651669  [132800/175341]\n",
      "loss: 0.391498  [134400/175341]\n",
      "loss: 0.419089  [136000/175341]\n",
      "loss: 0.627860  [137600/175341]\n",
      "loss: 0.297458  [139200/175341]\n",
      "loss: 0.250173  [140800/175341]\n",
      "loss: 0.161100  [142400/175341]\n",
      "loss: 0.645218  [144000/175341]\n",
      "loss: 0.853112  [145600/175341]\n",
      "loss: 0.442357  [147200/175341]\n",
      "loss: 0.854907  [148800/175341]\n",
      "loss: 0.337753  [150400/175341]\n",
      "loss: 0.725083  [152000/175341]\n",
      "loss: 0.383047  [153600/175341]\n",
      "loss: 0.830364  [155200/175341]\n",
      "loss: 0.323711  [156800/175341]\n",
      "loss: 0.276103  [158400/175341]\n",
      "loss: 0.708308  [160000/175341]\n",
      "loss: 0.769051  [161600/175341]\n",
      "loss: 0.692771  [163200/175341]\n",
      "loss: 0.595778  [164800/175341]\n",
      "loss: 0.652232  [166400/175341]\n",
      "loss: 0.410162  [168000/175341]\n",
      "loss: 0.481091  [169600/175341]\n",
      "loss: 0.243159  [171200/175341]\n",
      "loss: 0.748644  [172800/175341]\n",
      "loss: 0.178750  [174400/175341]\n",
      "Train Accuracy: 80.6372%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.568303, F1-score: 75.58%, Macro_F1-Score:  40.38%  \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.845619  [    0/175341]\n",
      "loss: 0.750810  [ 1600/175341]\n",
      "loss: 0.208930  [ 3200/175341]\n",
      "loss: 0.779293  [ 4800/175341]\n",
      "loss: 0.424158  [ 6400/175341]\n",
      "loss: 0.444868  [ 8000/175341]\n",
      "loss: 0.487053  [ 9600/175341]\n",
      "loss: 0.976457  [11200/175341]\n",
      "loss: 0.310477  [12800/175341]\n",
      "loss: 0.203275  [14400/175341]\n",
      "loss: 0.641664  [16000/175341]\n",
      "loss: 0.389643  [17600/175341]\n",
      "loss: 0.503280  [19200/175341]\n",
      "loss: 0.580347  [20800/175341]\n",
      "loss: 0.602621  [22400/175341]\n",
      "loss: 0.742065  [24000/175341]\n",
      "loss: 0.779547  [25600/175341]\n",
      "loss: 0.345073  [27200/175341]\n",
      "loss: 0.547858  [28800/175341]\n",
      "loss: 0.695893  [30400/175341]\n",
      "loss: 0.554991  [32000/175341]\n",
      "loss: 0.797106  [33600/175341]\n",
      "loss: 0.762815  [35200/175341]\n",
      "loss: 0.592929  [36800/175341]\n",
      "loss: 0.628976  [38400/175341]\n",
      "loss: 0.581963  [40000/175341]\n",
      "loss: 0.385354  [41600/175341]\n",
      "loss: 0.938636  [43200/175341]\n",
      "loss: 0.684335  [44800/175341]\n",
      "loss: 0.706114  [46400/175341]\n",
      "loss: 0.309649  [48000/175341]\n",
      "loss: 0.544775  [49600/175341]\n",
      "loss: 0.713216  [51200/175341]\n",
      "loss: 0.389769  [52800/175341]\n",
      "loss: 0.275446  [54400/175341]\n",
      "loss: 0.783564  [56000/175341]\n",
      "loss: 0.823712  [57600/175341]\n",
      "loss: 0.689436  [59200/175341]\n",
      "loss: 0.458563  [60800/175341]\n",
      "loss: 0.780449  [62400/175341]\n",
      "loss: 0.775749  [64000/175341]\n",
      "loss: 0.565157  [65600/175341]\n",
      "loss: 0.417214  [67200/175341]\n",
      "loss: 0.386020  [68800/175341]\n",
      "loss: 0.731322  [70400/175341]\n",
      "loss: 0.249721  [72000/175341]\n",
      "loss: 0.326136  [73600/175341]\n",
      "loss: 0.323468  [75200/175341]\n",
      "loss: 0.610552  [76800/175341]\n",
      "loss: 0.868653  [78400/175341]\n",
      "loss: 0.520167  [80000/175341]\n",
      "loss: 0.495627  [81600/175341]\n",
      "loss: 0.102481  [83200/175341]\n",
      "loss: 0.294023  [84800/175341]\n",
      "loss: 0.673777  [86400/175341]\n",
      "loss: 0.261524  [88000/175341]\n",
      "loss: 0.483606  [89600/175341]\n",
      "loss: 0.633057  [91200/175341]\n",
      "loss: 0.373374  [92800/175341]\n",
      "loss: 0.380449  [94400/175341]\n",
      "loss: 0.503972  [96000/175341]\n",
      "loss: 0.548802  [97600/175341]\n",
      "loss: 0.610545  [99200/175341]\n",
      "loss: 0.388478  [100800/175341]\n",
      "loss: 0.617904  [102400/175341]\n",
      "loss: 0.535329  [104000/175341]\n",
      "loss: 0.612795  [105600/175341]\n",
      "loss: 0.532104  [107200/175341]\n",
      "loss: 0.115435  [108800/175341]\n",
      "loss: 0.576803  [110400/175341]\n",
      "loss: 0.110416  [112000/175341]\n",
      "loss: 0.568944  [113600/175341]\n",
      "loss: 0.121105  [115200/175341]\n",
      "loss: 0.398345  [116800/175341]\n",
      "loss: 0.344681  [118400/175341]\n",
      "loss: 0.758456  [120000/175341]\n",
      "loss: 0.558071  [121600/175341]\n",
      "loss: 0.596925  [123200/175341]\n",
      "loss: 0.644117  [124800/175341]\n",
      "loss: 0.632129  [126400/175341]\n",
      "loss: 0.643075  [128000/175341]\n",
      "loss: 0.781105  [129600/175341]\n",
      "loss: 0.228774  [131200/175341]\n",
      "loss: 0.733476  [132800/175341]\n",
      "loss: 1.062280  [134400/175341]\n",
      "loss: 0.361770  [136000/175341]\n",
      "loss: 0.324889  [137600/175341]\n",
      "loss: 0.235967  [139200/175341]\n",
      "loss: 0.623409  [140800/175341]\n",
      "loss: 0.262770  [142400/175341]\n",
      "loss: 0.376783  [144000/175341]\n",
      "loss: 0.337026  [145600/175341]\n",
      "loss: 0.311549  [147200/175341]\n",
      "loss: 0.797937  [148800/175341]\n",
      "loss: 0.550980  [150400/175341]\n",
      "loss: 0.662429  [152000/175341]\n",
      "loss: 0.429489  [153600/175341]\n",
      "loss: 0.163023  [155200/175341]\n",
      "loss: 0.523329  [156800/175341]\n",
      "loss: 0.283625  [158400/175341]\n",
      "loss: 0.203143  [160000/175341]\n",
      "loss: 0.362383  [161600/175341]\n",
      "loss: 0.528360  [163200/175341]\n",
      "loss: 0.565218  [164800/175341]\n",
      "loss: 0.621286  [166400/175341]\n",
      "loss: 0.629390  [168000/175341]\n",
      "loss: 0.276018  [169600/175341]\n",
      "loss: 0.480853  [171200/175341]\n",
      "loss: 0.256387  [172800/175341]\n",
      "loss: 0.729431  [174400/175341]\n",
      "Train Accuracy: 80.6474%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.575844, F1-score: 75.32%, Macro_F1-Score:  39.94%  \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.438700  [    0/175341]\n",
      "loss: 0.141874  [ 1600/175341]\n",
      "loss: 0.260178  [ 3200/175341]\n",
      "loss: 0.513125  [ 4800/175341]\n",
      "loss: 0.504626  [ 6400/175341]\n",
      "loss: 0.343437  [ 8000/175341]\n",
      "loss: 0.345790  [ 9600/175341]\n",
      "loss: 0.591199  [11200/175341]\n",
      "loss: 0.507953  [12800/175341]\n",
      "loss: 0.547977  [14400/175341]\n",
      "loss: 0.592973  [16000/175341]\n",
      "loss: 0.808743  [17600/175341]\n",
      "loss: 0.360437  [19200/175341]\n",
      "loss: 0.240688  [20800/175341]\n",
      "loss: 0.238469  [22400/175341]\n",
      "loss: 0.345712  [24000/175341]\n",
      "loss: 0.570583  [25600/175341]\n",
      "loss: 0.560084  [27200/175341]\n",
      "loss: 0.346525  [28800/175341]\n",
      "loss: 0.308437  [30400/175341]\n",
      "loss: 0.765106  [32000/175341]\n",
      "loss: 0.402220  [33600/175341]\n",
      "loss: 0.676458  [35200/175341]\n",
      "loss: 0.522584  [36800/175341]\n",
      "loss: 0.447941  [38400/175341]\n",
      "loss: 0.355273  [40000/175341]\n",
      "loss: 0.629023  [41600/175341]\n",
      "loss: 0.561828  [43200/175341]\n",
      "loss: 0.609861  [44800/175341]\n",
      "loss: 0.398162  [46400/175341]\n",
      "loss: 0.854246  [48000/175341]\n",
      "loss: 0.634129  [49600/175341]\n",
      "loss: 0.430970  [51200/175341]\n",
      "loss: 0.554405  [52800/175341]\n",
      "loss: 0.575163  [54400/175341]\n",
      "loss: 0.506805  [56000/175341]\n",
      "loss: 0.520845  [57600/175341]\n",
      "loss: 0.787017  [59200/175341]\n",
      "loss: 0.569517  [60800/175341]\n",
      "loss: 0.204589  [62400/175341]\n",
      "loss: 0.733222  [64000/175341]\n",
      "loss: 0.728813  [65600/175341]\n",
      "loss: 0.433557  [67200/175341]\n",
      "loss: 0.602732  [68800/175341]\n",
      "loss: 0.460544  [70400/175341]\n",
      "loss: 0.318729  [72000/175341]\n",
      "loss: 0.450145  [73600/175341]\n",
      "loss: 0.624911  [75200/175341]\n",
      "loss: 0.512826  [76800/175341]\n",
      "loss: 0.299318  [78400/175341]\n",
      "loss: 0.558509  [80000/175341]\n",
      "loss: 0.445156  [81600/175341]\n",
      "loss: 0.353900  [83200/175341]\n",
      "loss: 0.227565  [84800/175341]\n",
      "loss: 0.471469  [86400/175341]\n",
      "loss: 0.466275  [88000/175341]\n",
      "loss: 0.230347  [89600/175341]\n",
      "loss: 0.566998  [91200/175341]\n",
      "loss: 0.894289  [92800/175341]\n",
      "loss: 0.593354  [94400/175341]\n",
      "loss: 0.185361  [96000/175341]\n",
      "loss: 0.705968  [97600/175341]\n",
      "loss: 1.053595  [99200/175341]\n",
      "loss: 0.234982  [100800/175341]\n",
      "loss: 0.174610  [102400/175341]\n",
      "loss: 0.239920  [104000/175341]\n",
      "loss: 0.422463  [105600/175341]\n",
      "loss: 0.791846  [107200/175341]\n",
      "loss: 0.347682  [108800/175341]\n",
      "loss: 0.279983  [110400/175341]\n",
      "loss: 0.333250  [112000/175341]\n",
      "loss: 0.570131  [113600/175341]\n",
      "loss: 0.343819  [115200/175341]\n",
      "loss: 0.157074  [116800/175341]\n",
      "loss: 0.364519  [118400/175341]\n",
      "loss: 0.605044  [120000/175341]\n",
      "loss: 0.293414  [121600/175341]\n",
      "loss: 0.674713  [123200/175341]\n",
      "loss: 0.324702  [124800/175341]\n",
      "loss: 0.374364  [126400/175341]\n",
      "loss: 0.636375  [128000/175341]\n",
      "loss: 0.702757  [129600/175341]\n",
      "loss: 0.439009  [131200/175341]\n",
      "loss: 0.416119  [132800/175341]\n",
      "loss: 0.662889  [134400/175341]\n",
      "loss: 0.390955  [136000/175341]\n",
      "loss: 0.367392  [137600/175341]\n",
      "loss: 0.175013  [139200/175341]\n",
      "loss: 0.361963  [140800/175341]\n",
      "loss: 0.433062  [142400/175341]\n",
      "loss: 0.475249  [144000/175341]\n",
      "loss: 0.143673  [145600/175341]\n",
      "loss: 0.231801  [147200/175341]\n",
      "loss: 0.471881  [148800/175341]\n",
      "loss: 0.244188  [150400/175341]\n",
      "loss: 0.441025  [152000/175341]\n",
      "loss: 0.370399  [153600/175341]\n",
      "loss: 0.426298  [155200/175341]\n",
      "loss: 0.359723  [156800/175341]\n",
      "loss: 0.584129  [158400/175341]\n",
      "loss: 0.301437  [160000/175341]\n",
      "loss: 0.755876  [161600/175341]\n",
      "loss: 0.391406  [163200/175341]\n",
      "loss: 0.188220  [164800/175341]\n",
      "loss: 0.465487  [166400/175341]\n",
      "loss: 0.201208  [168000/175341]\n",
      "loss: 0.631307  [169600/175341]\n",
      "loss: 0.265043  [171200/175341]\n",
      "loss: 0.287986  [172800/175341]\n",
      "loss: 0.330184  [174400/175341]\n",
      "Train Accuracy: 80.6856%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.576784, F1-score: 75.36%, Macro_F1-Score:  39.98%  \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.399484  [    0/175341]\n",
      "loss: 0.497934  [ 1600/175341]\n",
      "loss: 0.111452  [ 3200/175341]\n",
      "loss: 0.312393  [ 4800/175341]\n",
      "loss: 0.717336  [ 6400/175341]\n",
      "loss: 0.725994  [ 8000/175341]\n",
      "loss: 0.448569  [ 9600/175341]\n",
      "loss: 0.289818  [11200/175341]\n",
      "loss: 0.600404  [12800/175341]\n",
      "loss: 0.566646  [14400/175341]\n",
      "loss: 0.637673  [16000/175341]\n",
      "loss: 0.460403  [17600/175341]\n",
      "loss: 0.539335  [19200/175341]\n",
      "loss: 0.718027  [20800/175341]\n",
      "loss: 0.368236  [22400/175341]\n",
      "loss: 0.426753  [24000/175341]\n",
      "loss: 0.436935  [25600/175341]\n",
      "loss: 0.710437  [27200/175341]\n",
      "loss: 0.497905  [28800/175341]\n",
      "loss: 0.537656  [30400/175341]\n",
      "loss: 0.156819  [32000/175341]\n",
      "loss: 0.141415  [33600/175341]\n",
      "loss: 0.567953  [35200/175341]\n",
      "loss: 0.209539  [36800/175341]\n",
      "loss: 1.119611  [38400/175341]\n",
      "loss: 0.319227  [40000/175341]\n",
      "loss: 0.357573  [41600/175341]\n",
      "loss: 0.368733  [43200/175341]\n",
      "loss: 0.232775  [44800/175341]\n",
      "loss: 0.411957  [46400/175341]\n",
      "loss: 0.474834  [48000/175341]\n",
      "loss: 0.478142  [49600/175341]\n",
      "loss: 0.357798  [51200/175341]\n",
      "loss: 0.268530  [52800/175341]\n",
      "loss: 0.456912  [54400/175341]\n",
      "loss: 0.937621  [56000/175341]\n",
      "loss: 0.459883  [57600/175341]\n",
      "loss: 0.496429  [59200/175341]\n",
      "loss: 0.309949  [60800/175341]\n",
      "loss: 0.683766  [62400/175341]\n",
      "loss: 0.363881  [64000/175341]\n",
      "loss: 0.346745  [65600/175341]\n",
      "loss: 0.310387  [67200/175341]\n",
      "loss: 0.864601  [68800/175341]\n",
      "loss: 0.258493  [70400/175341]\n",
      "loss: 0.257557  [72000/175341]\n",
      "loss: 0.739409  [73600/175341]\n",
      "loss: 0.293958  [75200/175341]\n",
      "loss: 0.478544  [76800/175341]\n",
      "loss: 0.442170  [78400/175341]\n",
      "loss: 0.469012  [80000/175341]\n",
      "loss: 0.237870  [81600/175341]\n",
      "loss: 0.204736  [83200/175341]\n",
      "loss: 0.513236  [84800/175341]\n",
      "loss: 0.363615  [86400/175341]\n",
      "loss: 0.961497  [88000/175341]\n",
      "loss: 0.587462  [89600/175341]\n",
      "loss: 0.407270  [91200/175341]\n",
      "loss: 0.546071  [92800/175341]\n",
      "loss: 0.645061  [94400/175341]\n",
      "loss: 0.588863  [96000/175341]\n",
      "loss: 0.213216  [97600/175341]\n",
      "loss: 0.908379  [99200/175341]\n",
      "loss: 0.460113  [100800/175341]\n",
      "loss: 0.481652  [102400/175341]\n",
      "loss: 0.525396  [104000/175341]\n",
      "loss: 0.622716  [105600/175341]\n",
      "loss: 0.600334  [107200/175341]\n",
      "loss: 0.674562  [108800/175341]\n",
      "loss: 0.329974  [110400/175341]\n",
      "loss: 0.724604  [112000/175341]\n",
      "loss: 0.844161  [113600/175341]\n",
      "loss: 0.520041  [115200/175341]\n",
      "loss: 0.354358  [116800/175341]\n",
      "loss: 0.304081  [118400/175341]\n",
      "loss: 0.522641  [120000/175341]\n",
      "loss: 0.459181  [121600/175341]\n",
      "loss: 0.938022  [123200/175341]\n",
      "loss: 0.677916  [124800/175341]\n",
      "loss: 0.572519  [126400/175341]\n",
      "loss: 0.672888  [128000/175341]\n",
      "loss: 0.664112  [129600/175341]\n",
      "loss: 0.535162  [131200/175341]\n",
      "loss: 0.289611  [132800/175341]\n",
      "loss: 0.665780  [134400/175341]\n",
      "loss: 0.260191  [136000/175341]\n",
      "loss: 0.648630  [137600/175341]\n",
      "loss: 0.665255  [139200/175341]\n",
      "loss: 0.164015  [140800/175341]\n",
      "loss: 0.365147  [142400/175341]\n",
      "loss: 0.175059  [144000/175341]\n",
      "loss: 0.251781  [145600/175341]\n",
      "loss: 0.273172  [147200/175341]\n",
      "loss: 0.410910  [148800/175341]\n",
      "loss: 0.428780  [150400/175341]\n",
      "loss: 0.405038  [152000/175341]\n",
      "loss: 0.214198  [153600/175341]\n",
      "loss: 0.679949  [155200/175341]\n",
      "loss: 0.669972  [156800/175341]\n",
      "loss: 0.498494  [158400/175341]\n",
      "loss: 0.609484  [160000/175341]\n",
      "loss: 0.469777  [161600/175341]\n",
      "loss: 0.675754  [163200/175341]\n",
      "loss: 0.340393  [164800/175341]\n",
      "loss: 0.853746  [166400/175341]\n",
      "loss: 0.549096  [168000/175341]\n",
      "loss: 0.456017  [169600/175341]\n",
      "loss: 0.389627  [171200/175341]\n",
      "loss: 0.601761  [172800/175341]\n",
      "loss: 0.471517  [174400/175341]\n",
      "Train Accuracy: 80.7102%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.573256, F1-score: 74.89%, Macro_F1-Score:  40.27%  \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.592554  [    0/175341]\n",
      "loss: 0.553231  [ 1600/175341]\n",
      "loss: 0.336453  [ 3200/175341]\n",
      "loss: 0.547987  [ 4800/175341]\n",
      "loss: 1.027165  [ 6400/175341]\n",
      "loss: 0.388335  [ 8000/175341]\n",
      "loss: 0.454965  [ 9600/175341]\n",
      "loss: 0.195205  [11200/175341]\n",
      "loss: 0.341779  [12800/175341]\n",
      "loss: 0.665472  [14400/175341]\n",
      "loss: 0.618130  [16000/175341]\n",
      "loss: 0.682265  [17600/175341]\n",
      "loss: 0.297333  [19200/175341]\n",
      "loss: 0.562397  [20800/175341]\n",
      "loss: 0.497721  [22400/175341]\n",
      "loss: 0.771681  [24000/175341]\n",
      "loss: 0.691405  [25600/175341]\n",
      "loss: 0.752922  [27200/175341]\n",
      "loss: 0.300502  [28800/175341]\n",
      "loss: 0.433516  [30400/175341]\n",
      "loss: 0.368227  [32000/175341]\n",
      "loss: 0.396562  [33600/175341]\n",
      "loss: 0.803507  [35200/175341]\n",
      "loss: 0.221822  [36800/175341]\n",
      "loss: 0.449269  [38400/175341]\n",
      "loss: 0.615416  [40000/175341]\n",
      "loss: 0.433305  [41600/175341]\n",
      "loss: 0.307872  [43200/175341]\n",
      "loss: 0.304725  [44800/175341]\n",
      "loss: 0.511147  [46400/175341]\n",
      "loss: 0.399799  [48000/175341]\n",
      "loss: 0.773475  [49600/175341]\n",
      "loss: 0.163236  [51200/175341]\n",
      "loss: 0.618418  [52800/175341]\n",
      "loss: 0.395852  [54400/175341]\n",
      "loss: 0.373632  [56000/175341]\n",
      "loss: 0.452689  [57600/175341]\n",
      "loss: 0.587277  [59200/175341]\n",
      "loss: 0.746635  [60800/175341]\n",
      "loss: 0.536670  [62400/175341]\n",
      "loss: 0.526185  [64000/175341]\n",
      "loss: 0.258616  [65600/175341]\n",
      "loss: 0.382616  [67200/175341]\n",
      "loss: 0.174042  [68800/175341]\n",
      "loss: 0.472939  [70400/175341]\n",
      "loss: 0.599700  [72000/175341]\n",
      "loss: 0.505446  [73600/175341]\n",
      "loss: 0.424676  [75200/175341]\n",
      "loss: 0.363262  [76800/175341]\n",
      "loss: 0.277986  [78400/175341]\n",
      "loss: 0.134764  [80000/175341]\n",
      "loss: 0.658443  [81600/175341]\n",
      "loss: 0.797797  [83200/175341]\n",
      "loss: 0.352394  [84800/175341]\n",
      "loss: 0.225681  [86400/175341]\n",
      "loss: 0.536948  [88000/175341]\n",
      "loss: 0.428395  [89600/175341]\n",
      "loss: 0.499876  [91200/175341]\n",
      "loss: 0.258132  [92800/175341]\n",
      "loss: 0.635413  [94400/175341]\n",
      "loss: 0.457261  [96000/175341]\n",
      "loss: 0.467750  [97600/175341]\n",
      "loss: 0.198061  [99200/175341]\n",
      "loss: 0.345440  [100800/175341]\n",
      "loss: 0.225307  [102400/175341]\n",
      "loss: 0.478759  [104000/175341]\n",
      "loss: 0.439791  [105600/175341]\n",
      "loss: 0.993656  [107200/175341]\n",
      "loss: 0.788492  [108800/175341]\n",
      "loss: 0.563994  [110400/175341]\n",
      "loss: 0.436231  [112000/175341]\n",
      "loss: 0.685578  [113600/175341]\n",
      "loss: 0.993075  [115200/175341]\n",
      "loss: 0.475851  [116800/175341]\n",
      "loss: 0.647536  [118400/175341]\n",
      "loss: 0.394111  [120000/175341]\n",
      "loss: 0.426222  [121600/175341]\n",
      "loss: 0.329608  [123200/175341]\n",
      "loss: 0.288621  [124800/175341]\n",
      "loss: 0.546130  [126400/175341]\n",
      "loss: 0.598406  [128000/175341]\n",
      "loss: 0.195507  [129600/175341]\n",
      "loss: 0.304881  [131200/175341]\n",
      "loss: 0.653930  [132800/175341]\n",
      "loss: 0.529913  [134400/175341]\n",
      "loss: 0.288885  [136000/175341]\n",
      "loss: 0.470478  [137600/175341]\n",
      "loss: 0.217505  [139200/175341]\n",
      "loss: 0.417108  [140800/175341]\n",
      "loss: 0.519035  [142400/175341]\n",
      "loss: 0.300902  [144000/175341]\n",
      "loss: 0.919442  [145600/175341]\n",
      "loss: 0.390783  [147200/175341]\n",
      "loss: 0.433938  [148800/175341]\n",
      "loss: 0.670215  [150400/175341]\n",
      "loss: 0.543057  [152000/175341]\n",
      "loss: 0.757084  [153600/175341]\n",
      "loss: 0.899806  [155200/175341]\n",
      "loss: 0.629934  [156800/175341]\n",
      "loss: 0.383756  [158400/175341]\n",
      "loss: 0.777174  [160000/175341]\n",
      "loss: 0.577030  [161600/175341]\n",
      "loss: 0.333887  [163200/175341]\n",
      "loss: 0.297878  [164800/175341]\n",
      "loss: 0.377421  [166400/175341]\n",
      "loss: 0.211661  [168000/175341]\n",
      "loss: 0.144569  [169600/175341]\n",
      "loss: 0.792986  [171200/175341]\n",
      "loss: 0.383385  [172800/175341]\n",
      "loss: 0.504124  [174400/175341]\n",
      "Train Accuracy: 80.7398%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.549646, F1-score: 76.76%, Macro_F1-Score:  40.84%  \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.979785  [    0/175341]\n",
      "loss: 0.552486  [ 1600/175341]\n",
      "loss: 0.276526  [ 3200/175341]\n",
      "loss: 0.468289  [ 4800/175341]\n",
      "loss: 0.286486  [ 6400/175341]\n",
      "loss: 0.510172  [ 8000/175341]\n",
      "loss: 0.289457  [ 9600/175341]\n",
      "loss: 0.509858  [11200/175341]\n",
      "loss: 0.586722  [12800/175341]\n",
      "loss: 0.682204  [14400/175341]\n",
      "loss: 0.202535  [16000/175341]\n",
      "loss: 0.294660  [17600/175341]\n",
      "loss: 0.377240  [19200/175341]\n",
      "loss: 0.578698  [20800/175341]\n",
      "loss: 0.307713  [22400/175341]\n",
      "loss: 0.225262  [24000/175341]\n",
      "loss: 0.610758  [25600/175341]\n",
      "loss: 0.429923  [27200/175341]\n",
      "loss: 0.469873  [28800/175341]\n",
      "loss: 0.321250  [30400/175341]\n",
      "loss: 0.271513  [32000/175341]\n",
      "loss: 0.387745  [33600/175341]\n",
      "loss: 0.319208  [35200/175341]\n",
      "loss: 0.188236  [36800/175341]\n",
      "loss: 0.209169  [38400/175341]\n",
      "loss: 0.415061  [40000/175341]\n",
      "loss: 0.348414  [41600/175341]\n",
      "loss: 0.496089  [43200/175341]\n",
      "loss: 0.391472  [44800/175341]\n",
      "loss: 0.556051  [46400/175341]\n",
      "loss: 0.393665  [48000/175341]\n",
      "loss: 0.613708  [49600/175341]\n",
      "loss: 0.353913  [51200/175341]\n",
      "loss: 0.454604  [52800/175341]\n",
      "loss: 0.762730  [54400/175341]\n",
      "loss: 0.740425  [56000/175341]\n",
      "loss: 0.618569  [57600/175341]\n",
      "loss: 0.503396  [59200/175341]\n",
      "loss: 0.789631  [60800/175341]\n",
      "loss: 0.314972  [62400/175341]\n",
      "loss: 0.242705  [64000/175341]\n",
      "loss: 0.836382  [65600/175341]\n",
      "loss: 0.200142  [67200/175341]\n",
      "loss: 0.468372  [68800/175341]\n",
      "loss: 0.273963  [70400/175341]\n",
      "loss: 0.514811  [72000/175341]\n",
      "loss: 0.488383  [73600/175341]\n",
      "loss: 0.485952  [75200/175341]\n",
      "loss: 0.641450  [76800/175341]\n",
      "loss: 1.017710  [78400/175341]\n",
      "loss: 0.453844  [80000/175341]\n",
      "loss: 0.250359  [81600/175341]\n",
      "loss: 0.148725  [83200/175341]\n",
      "loss: 0.536066  [84800/175341]\n",
      "loss: 0.454975  [86400/175341]\n",
      "loss: 0.310406  [88000/175341]\n",
      "loss: 0.474337  [89600/175341]\n",
      "loss: 0.277099  [91200/175341]\n",
      "loss: 0.536056  [92800/175341]\n",
      "loss: 0.588993  [94400/175341]\n",
      "loss: 0.639121  [96000/175341]\n",
      "loss: 0.622088  [97600/175341]\n",
      "loss: 0.359483  [99200/175341]\n",
      "loss: 0.616251  [100800/175341]\n",
      "loss: 0.580452  [102400/175341]\n",
      "loss: 0.297405  [104000/175341]\n",
      "loss: 0.553041  [105600/175341]\n",
      "loss: 0.134657  [107200/175341]\n",
      "loss: 0.382932  [108800/175341]\n",
      "loss: 0.650031  [110400/175341]\n",
      "loss: 0.288952  [112000/175341]\n",
      "loss: 0.279970  [113600/175341]\n",
      "loss: 0.296230  [115200/175341]\n",
      "loss: 1.023375  [116800/175341]\n",
      "loss: 0.693784  [118400/175341]\n",
      "loss: 0.246893  [120000/175341]\n",
      "loss: 0.499940  [121600/175341]\n",
      "loss: 0.645526  [123200/175341]\n",
      "loss: 0.770553  [124800/175341]\n",
      "loss: 0.283491  [126400/175341]\n",
      "loss: 0.328831  [128000/175341]\n",
      "loss: 0.307641  [129600/175341]\n",
      "loss: 0.820346  [131200/175341]\n",
      "loss: 0.421156  [132800/175341]\n",
      "loss: 0.549301  [134400/175341]\n",
      "loss: 0.498505  [136000/175341]\n",
      "loss: 0.400383  [137600/175341]\n",
      "loss: 0.753634  [139200/175341]\n",
      "loss: 0.490716  [140800/175341]\n",
      "loss: 0.720401  [142400/175341]\n",
      "loss: 0.551073  [144000/175341]\n",
      "loss: 0.363697  [145600/175341]\n",
      "loss: 0.575041  [147200/175341]\n",
      "loss: 0.275304  [148800/175341]\n",
      "loss: 0.440646  [150400/175341]\n",
      "loss: 0.395165  [152000/175341]\n",
      "loss: 0.408095  [153600/175341]\n",
      "loss: 0.389775  [155200/175341]\n",
      "loss: 0.352493  [156800/175341]\n",
      "loss: 0.424115  [158400/175341]\n",
      "loss: 0.872905  [160000/175341]\n",
      "loss: 0.252980  [161600/175341]\n",
      "loss: 0.729684  [163200/175341]\n",
      "loss: 0.396497  [164800/175341]\n",
      "loss: 0.511582  [166400/175341]\n",
      "loss: 0.205310  [168000/175341]\n",
      "loss: 0.856838  [169600/175341]\n",
      "loss: 0.421370  [171200/175341]\n",
      "loss: 0.469026  [172800/175341]\n",
      "loss: 0.398409  [174400/175341]\n",
      "Train Accuracy: 80.7461%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.555375, F1-score: 76.26%, Macro_F1-Score:  40.50%  \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.161213  [    0/175341]\n",
      "loss: 0.529369  [ 1600/175341]\n",
      "loss: 0.400175  [ 3200/175341]\n",
      "loss: 0.526645  [ 4800/175341]\n",
      "loss: 0.263462  [ 6400/175341]\n",
      "loss: 0.627502  [ 8000/175341]\n",
      "loss: 0.892308  [ 9600/175341]\n",
      "loss: 1.015723  [11200/175341]\n",
      "loss: 0.447628  [12800/175341]\n",
      "loss: 0.796089  [14400/175341]\n",
      "loss: 0.655942  [16000/175341]\n",
      "loss: 0.573831  [17600/175341]\n",
      "loss: 0.213616  [19200/175341]\n",
      "loss: 1.000918  [20800/175341]\n",
      "loss: 0.737239  [22400/175341]\n",
      "loss: 0.520686  [24000/175341]\n",
      "loss: 0.744431  [25600/175341]\n",
      "loss: 0.503998  [27200/175341]\n",
      "loss: 0.375287  [28800/175341]\n",
      "loss: 0.557576  [30400/175341]\n",
      "loss: 0.619084  [32000/175341]\n",
      "loss: 0.807024  [33600/175341]\n",
      "loss: 0.141431  [35200/175341]\n",
      "loss: 0.378570  [36800/175341]\n",
      "loss: 0.430067  [38400/175341]\n",
      "loss: 0.359414  [40000/175341]\n",
      "loss: 0.417328  [41600/175341]\n",
      "loss: 0.290735  [43200/175341]\n",
      "loss: 0.532425  [44800/175341]\n",
      "loss: 0.472479  [46400/175341]\n",
      "loss: 0.752388  [48000/175341]\n",
      "loss: 0.828403  [49600/175341]\n",
      "loss: 0.273814  [51200/175341]\n",
      "loss: 0.235715  [52800/175341]\n",
      "loss: 0.462406  [54400/175341]\n",
      "loss: 0.330656  [56000/175341]\n",
      "loss: 0.355208  [57600/175341]\n",
      "loss: 0.682599  [59200/175341]\n",
      "loss: 0.209840  [60800/175341]\n",
      "loss: 0.589645  [62400/175341]\n",
      "loss: 0.713761  [64000/175341]\n",
      "loss: 0.475013  [65600/175341]\n",
      "loss: 0.266071  [67200/175341]\n",
      "loss: 0.127968  [68800/175341]\n",
      "loss: 0.307900  [70400/175341]\n",
      "loss: 0.325689  [72000/175341]\n",
      "loss: 0.243407  [73600/175341]\n",
      "loss: 0.806817  [75200/175341]\n",
      "loss: 0.459091  [76800/175341]\n",
      "loss: 0.502590  [78400/175341]\n",
      "loss: 0.660905  [80000/175341]\n",
      "loss: 0.626607  [81600/175341]\n",
      "loss: 0.648022  [83200/175341]\n",
      "loss: 0.140601  [84800/175341]\n",
      "loss: 0.447754  [86400/175341]\n",
      "loss: 0.344602  [88000/175341]\n",
      "loss: 0.519143  [89600/175341]\n",
      "loss: 0.317459  [91200/175341]\n",
      "loss: 0.223823  [92800/175341]\n",
      "loss: 0.367072  [94400/175341]\n",
      "loss: 0.302173  [96000/175341]\n",
      "loss: 0.157143  [97600/175341]\n",
      "loss: 0.549676  [99200/175341]\n",
      "loss: 0.729009  [100800/175341]\n",
      "loss: 0.491026  [102400/175341]\n",
      "loss: 0.479227  [104000/175341]\n",
      "loss: 0.663648  [105600/175341]\n",
      "loss: 0.268630  [107200/175341]\n",
      "loss: 0.495199  [108800/175341]\n",
      "loss: 0.226181  [110400/175341]\n",
      "loss: 0.083361  [112000/175341]\n",
      "loss: 0.857147  [113600/175341]\n",
      "loss: 0.323879  [115200/175341]\n",
      "loss: 0.854713  [116800/175341]\n",
      "loss: 0.892456  [118400/175341]\n",
      "loss: 0.177287  [120000/175341]\n",
      "loss: 0.504826  [121600/175341]\n",
      "loss: 0.929913  [123200/175341]\n",
      "loss: 0.511015  [124800/175341]\n",
      "loss: 0.460745  [126400/175341]\n",
      "loss: 0.665067  [128000/175341]\n",
      "loss: 0.459558  [129600/175341]\n",
      "loss: 0.446465  [131200/175341]\n",
      "loss: 0.229417  [132800/175341]\n",
      "loss: 0.472498  [134400/175341]\n",
      "loss: 0.388147  [136000/175341]\n",
      "loss: 0.248378  [137600/175341]\n",
      "loss: 0.508846  [139200/175341]\n",
      "loss: 0.433060  [140800/175341]\n",
      "loss: 0.614852  [142400/175341]\n",
      "loss: 0.875244  [144000/175341]\n",
      "loss: 0.532867  [145600/175341]\n",
      "loss: 0.374423  [147200/175341]\n",
      "loss: 0.727616  [148800/175341]\n",
      "loss: 0.441250  [150400/175341]\n",
      "loss: 0.841093  [152000/175341]\n",
      "loss: 0.438648  [153600/175341]\n",
      "loss: 0.391495  [155200/175341]\n",
      "loss: 0.484652  [156800/175341]\n",
      "loss: 0.400825  [158400/175341]\n",
      "loss: 0.686584  [160000/175341]\n",
      "loss: 0.546872  [161600/175341]\n",
      "loss: 0.489631  [163200/175341]\n",
      "loss: 0.401964  [164800/175341]\n",
      "loss: 0.721655  [166400/175341]\n",
      "loss: 0.378905  [168000/175341]\n",
      "loss: 0.165742  [169600/175341]\n",
      "loss: 0.453343  [171200/175341]\n",
      "loss: 0.674839  [172800/175341]\n",
      "loss: 0.261056  [174400/175341]\n",
      "Train Accuracy: 80.8225%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.581744, F1-score: 75.06%, Macro_F1-Score:  40.04%  \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.325732  [    0/175341]\n",
      "loss: 0.363420  [ 1600/175341]\n",
      "loss: 0.960737  [ 3200/175341]\n",
      "loss: 0.440251  [ 4800/175341]\n",
      "loss: 0.930172  [ 6400/175341]\n",
      "loss: 0.555940  [ 8000/175341]\n",
      "loss: 0.542051  [ 9600/175341]\n",
      "loss: 0.583415  [11200/175341]\n",
      "loss: 0.373700  [12800/175341]\n",
      "loss: 1.017807  [14400/175341]\n",
      "loss: 0.645088  [16000/175341]\n",
      "loss: 0.393579  [17600/175341]\n",
      "loss: 0.613717  [19200/175341]\n",
      "loss: 0.844900  [20800/175341]\n",
      "loss: 0.656366  [22400/175341]\n",
      "loss: 0.265188  [24000/175341]\n",
      "loss: 0.526549  [25600/175341]\n",
      "loss: 0.266884  [27200/175341]\n",
      "loss: 0.404265  [28800/175341]\n",
      "loss: 0.243506  [30400/175341]\n",
      "loss: 0.395491  [32000/175341]\n",
      "loss: 0.664325  [33600/175341]\n",
      "loss: 0.601200  [35200/175341]\n",
      "loss: 0.734650  [36800/175341]\n",
      "loss: 0.709685  [38400/175341]\n",
      "loss: 0.163434  [40000/175341]\n",
      "loss: 0.505187  [41600/175341]\n",
      "loss: 1.162809  [43200/175341]\n",
      "loss: 0.197747  [44800/175341]\n",
      "loss: 0.520923  [46400/175341]\n",
      "loss: 0.779648  [48000/175341]\n",
      "loss: 1.110993  [49600/175341]\n",
      "loss: 0.925463  [51200/175341]\n",
      "loss: 0.419948  [52800/175341]\n",
      "loss: 0.416114  [54400/175341]\n",
      "loss: 0.589925  [56000/175341]\n",
      "loss: 0.471389  [57600/175341]\n",
      "loss: 0.511295  [59200/175341]\n",
      "loss: 0.957720  [60800/175341]\n",
      "loss: 0.539016  [62400/175341]\n",
      "loss: 0.393127  [64000/175341]\n",
      "loss: 0.556349  [65600/175341]\n",
      "loss: 0.379166  [67200/175341]\n",
      "loss: 0.157093  [68800/175341]\n",
      "loss: 0.347065  [70400/175341]\n",
      "loss: 0.718142  [72000/175341]\n",
      "loss: 0.304150  [73600/175341]\n",
      "loss: 1.018594  [75200/175341]\n",
      "loss: 0.948959  [76800/175341]\n",
      "loss: 0.576912  [78400/175341]\n",
      "loss: 0.505724  [80000/175341]\n",
      "loss: 0.645773  [81600/175341]\n",
      "loss: 0.392233  [83200/175341]\n",
      "loss: 0.248961  [84800/175341]\n",
      "loss: 0.145181  [86400/175341]\n",
      "loss: 0.312362  [88000/175341]\n",
      "loss: 0.631347  [89600/175341]\n",
      "loss: 0.393531  [91200/175341]\n",
      "loss: 0.509157  [92800/175341]\n",
      "loss: 0.568600  [94400/175341]\n",
      "loss: 0.716176  [96000/175341]\n",
      "loss: 0.526440  [97600/175341]\n",
      "loss: 0.721405  [99200/175341]\n",
      "loss: 0.386174  [100800/175341]\n",
      "loss: 0.415011  [102400/175341]\n",
      "loss: 0.310336  [104000/175341]\n",
      "loss: 0.608362  [105600/175341]\n",
      "loss: 0.325985  [107200/175341]\n",
      "loss: 0.603899  [108800/175341]\n",
      "loss: 0.494466  [110400/175341]\n",
      "loss: 0.493530  [112000/175341]\n",
      "loss: 0.704087  [113600/175341]\n",
      "loss: 0.405660  [115200/175341]\n",
      "loss: 0.835661  [116800/175341]\n",
      "loss: 0.592417  [118400/175341]\n",
      "loss: 0.828978  [120000/175341]\n",
      "loss: 0.356744  [121600/175341]\n",
      "loss: 0.422805  [123200/175341]\n",
      "loss: 0.501090  [124800/175341]\n",
      "loss: 0.465626  [126400/175341]\n",
      "loss: 0.072371  [128000/175341]\n",
      "loss: 0.710766  [129600/175341]\n",
      "loss: 0.307839  [131200/175341]\n",
      "loss: 0.531439  [132800/175341]\n",
      "loss: 0.574883  [134400/175341]\n",
      "loss: 0.215752  [136000/175341]\n",
      "loss: 0.188500  [137600/175341]\n",
      "loss: 0.418643  [139200/175341]\n",
      "loss: 0.229700  [140800/175341]\n",
      "loss: 0.240510  [142400/175341]\n",
      "loss: 0.564660  [144000/175341]\n",
      "loss: 0.214605  [145600/175341]\n",
      "loss: 0.527787  [147200/175341]\n",
      "loss: 0.764389  [148800/175341]\n",
      "loss: 0.501534  [150400/175341]\n",
      "loss: 0.581800  [152000/175341]\n",
      "loss: 0.756115  [153600/175341]\n",
      "loss: 1.187062  [155200/175341]\n",
      "loss: 0.588727  [156800/175341]\n",
      "loss: 0.221198  [158400/175341]\n",
      "loss: 0.644317  [160000/175341]\n",
      "loss: 0.606944  [161600/175341]\n",
      "loss: 0.747827  [163200/175341]\n",
      "loss: 0.712349  [164800/175341]\n",
      "loss: 0.458789  [166400/175341]\n",
      "loss: 0.520506  [168000/175341]\n",
      "loss: 0.319590  [169600/175341]\n",
      "loss: 0.503509  [171200/175341]\n",
      "loss: 0.417940  [172800/175341]\n",
      "loss: 0.431405  [174400/175341]\n",
      "Train Accuracy: 80.8311%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.568229, F1-score: 74.77%, Macro_F1-Score:  40.22%  \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.415345  [    0/175341]\n",
      "loss: 0.586712  [ 1600/175341]\n",
      "loss: 0.651135  [ 3200/175341]\n",
      "loss: 0.438013  [ 4800/175341]\n",
      "loss: 0.240887  [ 6400/175341]\n",
      "loss: 0.355234  [ 8000/175341]\n",
      "loss: 0.551380  [ 9600/175341]\n",
      "loss: 0.522821  [11200/175341]\n",
      "loss: 0.660882  [12800/175341]\n",
      "loss: 0.619193  [14400/175341]\n",
      "loss: 0.298342  [16000/175341]\n",
      "loss: 0.475064  [17600/175341]\n",
      "loss: 0.396998  [19200/175341]\n",
      "loss: 0.216628  [20800/175341]\n",
      "loss: 0.771177  [22400/175341]\n",
      "loss: 0.297647  [24000/175341]\n",
      "loss: 0.395176  [25600/175341]\n",
      "loss: 0.495554  [27200/175341]\n",
      "loss: 0.248626  [28800/175341]\n",
      "loss: 0.567180  [30400/175341]\n",
      "loss: 0.245783  [32000/175341]\n",
      "loss: 0.528621  [33600/175341]\n",
      "loss: 0.762730  [35200/175341]\n",
      "loss: 0.442743  [36800/175341]\n",
      "loss: 0.377711  [38400/175341]\n",
      "loss: 0.131219  [40000/175341]\n",
      "loss: 0.450887  [41600/175341]\n",
      "loss: 0.826024  [43200/175341]\n",
      "loss: 0.892752  [44800/175341]\n",
      "loss: 0.625949  [46400/175341]\n",
      "loss: 0.519372  [48000/175341]\n",
      "loss: 0.292701  [49600/175341]\n",
      "loss: 0.488049  [51200/175341]\n",
      "loss: 0.451222  [52800/175341]\n",
      "loss: 0.368992  [54400/175341]\n",
      "loss: 0.361036  [56000/175341]\n",
      "loss: 0.364745  [57600/175341]\n",
      "loss: 0.429452  [59200/175341]\n",
      "loss: 0.463521  [60800/175341]\n",
      "loss: 0.312679  [62400/175341]\n",
      "loss: 0.310101  [64000/175341]\n",
      "loss: 0.395081  [65600/175341]\n",
      "loss: 0.417027  [67200/175341]\n",
      "loss: 0.722834  [68800/175341]\n",
      "loss: 0.327875  [70400/175341]\n",
      "loss: 0.345574  [72000/175341]\n",
      "loss: 0.567417  [73600/175341]\n",
      "loss: 0.342696  [75200/175341]\n",
      "loss: 0.437795  [76800/175341]\n",
      "loss: 0.345719  [78400/175341]\n",
      "loss: 0.263598  [80000/175341]\n",
      "loss: 0.285733  [81600/175341]\n",
      "loss: 0.681604  [83200/175341]\n",
      "loss: 0.511062  [84800/175341]\n",
      "loss: 0.416744  [86400/175341]\n",
      "loss: 0.222582  [88000/175341]\n",
      "loss: 0.508752  [89600/175341]\n",
      "loss: 0.194893  [91200/175341]\n",
      "loss: 0.511840  [92800/175341]\n",
      "loss: 0.313169  [94400/175341]\n",
      "loss: 0.718663  [96000/175341]\n",
      "loss: 0.570404  [97600/175341]\n",
      "loss: 0.604778  [99200/175341]\n",
      "loss: 0.556594  [100800/175341]\n",
      "loss: 0.443719  [102400/175341]\n",
      "loss: 0.623062  [104000/175341]\n",
      "loss: 0.444553  [105600/175341]\n",
      "loss: 0.959724  [107200/175341]\n",
      "loss: 0.716084  [108800/175341]\n",
      "loss: 0.456337  [110400/175341]\n",
      "loss: 0.451335  [112000/175341]\n",
      "loss: 0.396351  [113600/175341]\n",
      "loss: 0.546013  [115200/175341]\n",
      "loss: 0.278717  [116800/175341]\n",
      "loss: 0.614667  [118400/175341]\n",
      "loss: 0.496700  [120000/175341]\n",
      "loss: 0.309902  [121600/175341]\n",
      "loss: 0.249580  [123200/175341]\n",
      "loss: 0.471849  [124800/175341]\n",
      "loss: 0.444960  [126400/175341]\n",
      "loss: 0.864380  [128000/175341]\n",
      "loss: 0.381640  [129600/175341]\n",
      "loss: 0.250060  [131200/175341]\n",
      "loss: 0.281486  [132800/175341]\n",
      "loss: 0.364663  [134400/175341]\n",
      "loss: 0.405344  [136000/175341]\n",
      "loss: 0.342099  [137600/175341]\n",
      "loss: 0.405354  [139200/175341]\n",
      "loss: 0.338102  [140800/175341]\n",
      "loss: 0.230779  [142400/175341]\n",
      "loss: 0.787281  [144000/175341]\n",
      "loss: 0.444980  [145600/175341]\n",
      "loss: 0.107398  [147200/175341]\n",
      "loss: 0.156429  [148800/175341]\n",
      "loss: 0.599760  [150400/175341]\n",
      "loss: 0.083068  [152000/175341]\n",
      "loss: 0.452067  [153600/175341]\n",
      "loss: 0.260103  [155200/175341]\n",
      "loss: 0.172375  [156800/175341]\n",
      "loss: 0.424713  [158400/175341]\n",
      "loss: 0.560876  [160000/175341]\n",
      "loss: 0.151577  [161600/175341]\n",
      "loss: 0.431843  [163200/175341]\n",
      "loss: 0.607260  [164800/175341]\n",
      "loss: 0.419374  [166400/175341]\n",
      "loss: 0.784046  [168000/175341]\n",
      "loss: 0.470111  [169600/175341]\n",
      "loss: 0.394714  [171200/175341]\n",
      "loss: 0.396544  [172800/175341]\n",
      "loss: 0.225164  [174400/175341]\n",
      "Train Accuracy: 80.8294%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.560113, F1-score: 75.97%, Macro_F1-Score:  41.11%  \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.333721  [    0/175341]\n",
      "loss: 0.257694  [ 1600/175341]\n",
      "loss: 0.121509  [ 3200/175341]\n",
      "loss: 0.327733  [ 4800/175341]\n",
      "loss: 0.447835  [ 6400/175341]\n",
      "loss: 0.295740  [ 8000/175341]\n",
      "loss: 1.129904  [ 9600/175341]\n",
      "loss: 0.313136  [11200/175341]\n",
      "loss: 0.458657  [12800/175341]\n",
      "loss: 0.640694  [14400/175341]\n",
      "loss: 0.264753  [16000/175341]\n",
      "loss: 0.234054  [17600/175341]\n",
      "loss: 0.246068  [19200/175341]\n",
      "loss: 0.528466  [20800/175341]\n",
      "loss: 0.779520  [22400/175341]\n",
      "loss: 0.456062  [24000/175341]\n",
      "loss: 0.403124  [25600/175341]\n",
      "loss: 0.232703  [27200/175341]\n",
      "loss: 0.516313  [28800/175341]\n",
      "loss: 0.481959  [30400/175341]\n",
      "loss: 0.315647  [32000/175341]\n",
      "loss: 0.475740  [33600/175341]\n",
      "loss: 0.583498  [35200/175341]\n",
      "loss: 0.599319  [36800/175341]\n",
      "loss: 0.381966  [38400/175341]\n",
      "loss: 0.322891  [40000/175341]\n",
      "loss: 0.487070  [41600/175341]\n",
      "loss: 0.238476  [43200/175341]\n",
      "loss: 0.455794  [44800/175341]\n",
      "loss: 0.783890  [46400/175341]\n",
      "loss: 0.911709  [48000/175341]\n",
      "loss: 0.755445  [49600/175341]\n",
      "loss: 0.471040  [51200/175341]\n",
      "loss: 0.251383  [52800/175341]\n",
      "loss: 0.659765  [54400/175341]\n",
      "loss: 0.272822  [56000/175341]\n",
      "loss: 0.542237  [57600/175341]\n",
      "loss: 0.242628  [59200/175341]\n",
      "loss: 0.147589  [60800/175341]\n",
      "loss: 0.818350  [62400/175341]\n",
      "loss: 0.488361  [64000/175341]\n",
      "loss: 0.278430  [65600/175341]\n",
      "loss: 0.295548  [67200/175341]\n",
      "loss: 0.352842  [68800/175341]\n",
      "loss: 0.212017  [70400/175341]\n",
      "loss: 0.555692  [72000/175341]\n",
      "loss: 0.687077  [73600/175341]\n",
      "loss: 0.513903  [75200/175341]\n",
      "loss: 0.739832  [76800/175341]\n",
      "loss: 0.705269  [78400/175341]\n",
      "loss: 0.688983  [80000/175341]\n",
      "loss: 0.227264  [81600/175341]\n",
      "loss: 0.634385  [83200/175341]\n",
      "loss: 0.372432  [84800/175341]\n",
      "loss: 0.612480  [86400/175341]\n",
      "loss: 0.572000  [88000/175341]\n",
      "loss: 0.386495  [89600/175341]\n",
      "loss: 0.353016  [91200/175341]\n",
      "loss: 0.530210  [92800/175341]\n",
      "loss: 0.957126  [94400/175341]\n",
      "loss: 0.485235  [96000/175341]\n",
      "loss: 0.671148  [97600/175341]\n",
      "loss: 0.379315  [99200/175341]\n",
      "loss: 0.629399  [100800/175341]\n",
      "loss: 0.442210  [102400/175341]\n",
      "loss: 0.487781  [104000/175341]\n",
      "loss: 0.270949  [105600/175341]\n",
      "loss: 0.583123  [107200/175341]\n",
      "loss: 0.451005  [108800/175341]\n",
      "loss: 0.254614  [110400/175341]\n",
      "loss: 0.329233  [112000/175341]\n",
      "loss: 0.902615  [113600/175341]\n",
      "loss: 0.442232  [115200/175341]\n",
      "loss: 0.356162  [116800/175341]\n",
      "loss: 0.747024  [118400/175341]\n",
      "loss: 0.316727  [120000/175341]\n",
      "loss: 0.332498  [121600/175341]\n",
      "loss: 0.385965  [123200/175341]\n",
      "loss: 0.615330  [124800/175341]\n",
      "loss: 0.790379  [126400/175341]\n",
      "loss: 0.544600  [128000/175341]\n",
      "loss: 0.282704  [129600/175341]\n",
      "loss: 0.394225  [131200/175341]\n",
      "loss: 0.323252  [132800/175341]\n",
      "loss: 1.200381  [134400/175341]\n",
      "loss: 0.361026  [136000/175341]\n",
      "loss: 0.346522  [137600/175341]\n",
      "loss: 0.569174  [139200/175341]\n",
      "loss: 0.367919  [140800/175341]\n",
      "loss: 0.394549  [142400/175341]\n",
      "loss: 0.686384  [144000/175341]\n",
      "loss: 0.324953  [145600/175341]\n",
      "loss: 0.681999  [147200/175341]\n",
      "loss: 0.096387  [148800/175341]\n",
      "loss: 0.465965  [150400/175341]\n",
      "loss: 0.508680  [152000/175341]\n",
      "loss: 0.647725  [153600/175341]\n",
      "loss: 0.320280  [155200/175341]\n",
      "loss: 0.491929  [156800/175341]\n",
      "loss: 0.253538  [158400/175341]\n",
      "loss: 0.257977  [160000/175341]\n",
      "loss: 0.388951  [161600/175341]\n",
      "loss: 0.272464  [163200/175341]\n",
      "loss: 0.411604  [164800/175341]\n",
      "loss: 0.634752  [166400/175341]\n",
      "loss: 0.396871  [168000/175341]\n",
      "loss: 0.419163  [169600/175341]\n",
      "loss: 0.431641  [171200/175341]\n",
      "loss: 0.295479  [172800/175341]\n",
      "loss: 0.493164  [174400/175341]\n",
      "Train Accuracy: 80.9109%\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.574947, F1-score: 74.52%, Macro_F1-Score:  39.72%  \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.506874  [    0/175341]\n",
      "loss: 0.610280  [ 1600/175341]\n",
      "loss: 0.610315  [ 3200/175341]\n",
      "loss: 0.286541  [ 4800/175341]\n",
      "loss: 0.566086  [ 6400/175341]\n",
      "loss: 0.201875  [ 8000/175341]\n",
      "loss: 0.559655  [ 9600/175341]\n",
      "loss: 0.504512  [11200/175341]\n",
      "loss: 0.387386  [12800/175341]\n",
      "loss: 0.574331  [14400/175341]\n",
      "loss: 0.254911  [16000/175341]\n",
      "loss: 0.457444  [17600/175341]\n",
      "loss: 0.640376  [19200/175341]\n",
      "loss: 0.486655  [20800/175341]\n",
      "loss: 0.206288  [22400/175341]\n",
      "loss: 0.560001  [24000/175341]\n",
      "loss: 0.789192  [25600/175341]\n",
      "loss: 0.868730  [27200/175341]\n",
      "loss: 0.678546  [28800/175341]\n",
      "loss: 0.458106  [30400/175341]\n",
      "loss: 0.540324  [32000/175341]\n",
      "loss: 0.561302  [33600/175341]\n",
      "loss: 0.382905  [35200/175341]\n",
      "loss: 0.602165  [36800/175341]\n",
      "loss: 0.250417  [38400/175341]\n",
      "loss: 0.171488  [40000/175341]\n",
      "loss: 1.124737  [41600/175341]\n",
      "loss: 0.289095  [43200/175341]\n",
      "loss: 0.256728  [44800/175341]\n",
      "loss: 0.978435  [46400/175341]\n",
      "loss: 0.515171  [48000/175341]\n",
      "loss: 0.187049  [49600/175341]\n",
      "loss: 0.404762  [51200/175341]\n",
      "loss: 0.394101  [52800/175341]\n",
      "loss: 0.215835  [54400/175341]\n",
      "loss: 0.539978  [56000/175341]\n",
      "loss: 0.879409  [57600/175341]\n",
      "loss: 0.207884  [59200/175341]\n",
      "loss: 0.197019  [60800/175341]\n",
      "loss: 0.455987  [62400/175341]\n",
      "loss: 0.531515  [64000/175341]\n",
      "loss: 0.483537  [65600/175341]\n",
      "loss: 0.461371  [67200/175341]\n",
      "loss: 0.720011  [68800/175341]\n",
      "loss: 0.299139  [70400/175341]\n",
      "loss: 0.675508  [72000/175341]\n",
      "loss: 0.348936  [73600/175341]\n",
      "loss: 1.057008  [75200/175341]\n",
      "loss: 0.083809  [76800/175341]\n",
      "loss: 0.918346  [78400/175341]\n",
      "loss: 0.425820  [80000/175341]\n",
      "loss: 0.404221  [81600/175341]\n",
      "loss: 0.548420  [83200/175341]\n",
      "loss: 0.256919  [84800/175341]\n",
      "loss: 0.820930  [86400/175341]\n",
      "loss: 0.372805  [88000/175341]\n",
      "loss: 0.534459  [89600/175341]\n",
      "loss: 0.083051  [91200/175341]\n",
      "loss: 0.446228  [92800/175341]\n",
      "loss: 0.826636  [94400/175341]\n",
      "loss: 0.354070  [96000/175341]\n",
      "loss: 0.741431  [97600/175341]\n",
      "loss: 0.618503  [99200/175341]\n",
      "loss: 0.331337  [100800/175341]\n",
      "loss: 0.305637  [102400/175341]\n",
      "loss: 0.476693  [104000/175341]\n",
      "loss: 0.897682  [105600/175341]\n",
      "loss: 0.511617  [107200/175341]\n",
      "loss: 0.473658  [108800/175341]\n",
      "loss: 0.184495  [110400/175341]\n",
      "loss: 0.367696  [112000/175341]\n",
      "loss: 0.353700  [113600/175341]\n",
      "loss: 0.501063  [115200/175341]\n",
      "loss: 0.868538  [116800/175341]\n",
      "loss: 0.305955  [118400/175341]\n",
      "loss: 0.717327  [120000/175341]\n",
      "loss: 0.451056  [121600/175341]\n",
      "loss: 0.543022  [123200/175341]\n",
      "loss: 0.668074  [124800/175341]\n",
      "loss: 0.417966  [126400/175341]\n",
      "loss: 1.000230  [128000/175341]\n",
      "loss: 0.489190  [129600/175341]\n",
      "loss: 0.150807  [131200/175341]\n",
      "loss: 0.461709  [132800/175341]\n",
      "loss: 0.230476  [134400/175341]\n",
      "loss: 0.392034  [136000/175341]\n",
      "loss: 0.547596  [137600/175341]\n",
      "loss: 0.654454  [139200/175341]\n",
      "loss: 0.178145  [140800/175341]\n",
      "loss: 0.482511  [142400/175341]\n",
      "loss: 0.494302  [144000/175341]\n",
      "loss: 0.305262  [145600/175341]\n",
      "loss: 0.493188  [147200/175341]\n",
      "loss: 0.549951  [148800/175341]\n",
      "loss: 0.681894  [150400/175341]\n",
      "loss: 0.401196  [152000/175341]\n",
      "loss: 1.008074  [153600/175341]\n",
      "loss: 0.725830  [155200/175341]\n",
      "loss: 0.414978  [156800/175341]\n",
      "loss: 0.556393  [158400/175341]\n",
      "loss: 0.836869  [160000/175341]\n",
      "loss: 0.817316  [161600/175341]\n",
      "loss: 0.491751  [163200/175341]\n",
      "loss: 0.713761  [164800/175341]\n",
      "loss: 0.689612  [166400/175341]\n",
      "loss: 0.268741  [168000/175341]\n",
      "loss: 0.349199  [169600/175341]\n",
      "loss: 0.278338  [171200/175341]\n",
      "loss: 0.493624  [172800/175341]\n",
      "loss: 0.348171  [174400/175341]\n",
      "Train Accuracy: 80.8716%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.556548, F1-score: 76.27%, Macro_F1-Score:  40.72%  \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.454291  [    0/175341]\n",
      "loss: 0.369423  [ 1600/175341]\n",
      "loss: 0.190583  [ 3200/175341]\n",
      "loss: 0.377862  [ 4800/175341]\n",
      "loss: 0.321032  [ 6400/175341]\n",
      "loss: 0.543102  [ 8000/175341]\n",
      "loss: 0.428623  [ 9600/175341]\n",
      "loss: 0.570888  [11200/175341]\n",
      "loss: 0.503947  [12800/175341]\n",
      "loss: 0.901732  [14400/175341]\n",
      "loss: 0.554342  [16000/175341]\n",
      "loss: 0.095539  [17600/175341]\n",
      "loss: 0.436007  [19200/175341]\n",
      "loss: 0.328495  [20800/175341]\n",
      "loss: 0.303637  [22400/175341]\n",
      "loss: 0.672660  [24000/175341]\n",
      "loss: 0.258395  [25600/175341]\n",
      "loss: 0.877471  [27200/175341]\n",
      "loss: 0.256710  [28800/175341]\n",
      "loss: 0.301549  [30400/175341]\n",
      "loss: 0.781964  [32000/175341]\n",
      "loss: 0.713208  [33600/175341]\n",
      "loss: 0.355645  [35200/175341]\n",
      "loss: 0.551239  [36800/175341]\n",
      "loss: 0.312468  [38400/175341]\n",
      "loss: 0.512401  [40000/175341]\n",
      "loss: 0.429722  [41600/175341]\n",
      "loss: 0.419310  [43200/175341]\n",
      "loss: 0.189884  [44800/175341]\n",
      "loss: 0.222344  [46400/175341]\n",
      "loss: 0.565020  [48000/175341]\n",
      "loss: 0.181377  [49600/175341]\n",
      "loss: 0.613062  [51200/175341]\n",
      "loss: 0.753978  [52800/175341]\n",
      "loss: 0.220786  [54400/175341]\n",
      "loss: 0.890626  [56000/175341]\n",
      "loss: 0.347634  [57600/175341]\n",
      "loss: 0.469779  [59200/175341]\n",
      "loss: 0.723952  [60800/175341]\n",
      "loss: 0.406268  [62400/175341]\n",
      "loss: 0.722353  [64000/175341]\n",
      "loss: 0.477179  [65600/175341]\n",
      "loss: 0.339096  [67200/175341]\n",
      "loss: 0.345729  [68800/175341]\n",
      "loss: 0.178351  [70400/175341]\n",
      "loss: 0.610507  [72000/175341]\n",
      "loss: 0.574087  [73600/175341]\n",
      "loss: 0.586662  [75200/175341]\n",
      "loss: 0.798053  [76800/175341]\n",
      "loss: 0.178382  [78400/175341]\n",
      "loss: 0.312660  [80000/175341]\n",
      "loss: 0.242953  [81600/175341]\n",
      "loss: 0.237806  [83200/175341]\n",
      "loss: 0.513695  [84800/175341]\n",
      "loss: 0.331409  [86400/175341]\n",
      "loss: 0.292331  [88000/175341]\n",
      "loss: 0.514512  [89600/175341]\n",
      "loss: 0.488793  [91200/175341]\n",
      "loss: 0.551038  [92800/175341]\n",
      "loss: 0.294686  [94400/175341]\n",
      "loss: 0.826502  [96000/175341]\n",
      "loss: 0.207683  [97600/175341]\n",
      "loss: 0.138192  [99200/175341]\n",
      "loss: 0.108346  [100800/175341]\n",
      "loss: 0.129911  [102400/175341]\n",
      "loss: 0.342164  [104000/175341]\n",
      "loss: 0.818391  [105600/175341]\n",
      "loss: 0.287806  [107200/175341]\n",
      "loss: 1.034487  [108800/175341]\n",
      "loss: 0.348417  [110400/175341]\n",
      "loss: 0.467144  [112000/175341]\n",
      "loss: 0.418032  [113600/175341]\n",
      "loss: 0.552199  [115200/175341]\n",
      "loss: 0.261195  [116800/175341]\n",
      "loss: 0.329463  [118400/175341]\n",
      "loss: 0.664721  [120000/175341]\n",
      "loss: 0.635018  [121600/175341]\n",
      "loss: 0.330678  [123200/175341]\n",
      "loss: 0.494047  [124800/175341]\n",
      "loss: 0.587874  [126400/175341]\n",
      "loss: 0.643412  [128000/175341]\n",
      "loss: 0.564059  [129600/175341]\n",
      "loss: 0.480970  [131200/175341]\n",
      "loss: 0.579906  [132800/175341]\n",
      "loss: 0.389283  [134400/175341]\n",
      "loss: 0.680196  [136000/175341]\n",
      "loss: 0.371875  [137600/175341]\n",
      "loss: 0.541781  [139200/175341]\n",
      "loss: 0.217587  [140800/175341]\n",
      "loss: 0.318367  [142400/175341]\n",
      "loss: 0.731235  [144000/175341]\n",
      "loss: 0.242617  [145600/175341]\n",
      "loss: 0.716028  [147200/175341]\n",
      "loss: 0.461145  [148800/175341]\n",
      "loss: 0.498649  [150400/175341]\n",
      "loss: 0.388799  [152000/175341]\n",
      "loss: 0.200139  [153600/175341]\n",
      "loss: 0.851602  [155200/175341]\n",
      "loss: 0.276744  [156800/175341]\n",
      "loss: 0.481970  [158400/175341]\n",
      "loss: 0.409257  [160000/175341]\n",
      "loss: 0.365488  [161600/175341]\n",
      "loss: 0.192572  [163200/175341]\n",
      "loss: 0.160087  [164800/175341]\n",
      "loss: 0.585500  [166400/175341]\n",
      "loss: 0.478886  [168000/175341]\n",
      "loss: 0.843581  [169600/175341]\n",
      "loss: 0.741008  [171200/175341]\n",
      "loss: 0.490582  [172800/175341]\n",
      "loss: 0.610877  [174400/175341]\n",
      "Train Accuracy: 80.8721%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.557119, F1-score: 76.68%, Macro_F1-Score:  40.55%  \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.246219  [    0/175341]\n",
      "loss: 0.202451  [ 1600/175341]\n",
      "loss: 0.708859  [ 3200/175341]\n",
      "loss: 0.556737  [ 4800/175341]\n",
      "loss: 0.438775  [ 6400/175341]\n",
      "loss: 0.623272  [ 8000/175341]\n",
      "loss: 0.401589  [ 9600/175341]\n",
      "loss: 0.366130  [11200/175341]\n",
      "loss: 0.475028  [12800/175341]\n",
      "loss: 0.408988  [14400/175341]\n",
      "loss: 0.147528  [16000/175341]\n",
      "loss: 0.339480  [17600/175341]\n",
      "loss: 0.446069  [19200/175341]\n",
      "loss: 0.669759  [20800/175341]\n",
      "loss: 0.444064  [22400/175341]\n",
      "loss: 0.226487  [24000/175341]\n",
      "loss: 0.246340  [25600/175341]\n",
      "loss: 0.362722  [27200/175341]\n",
      "loss: 0.625866  [28800/175341]\n",
      "loss: 0.404855  [30400/175341]\n",
      "loss: 0.581157  [32000/175341]\n",
      "loss: 0.777177  [33600/175341]\n",
      "loss: 0.531403  [35200/175341]\n",
      "loss: 0.791791  [36800/175341]\n",
      "loss: 0.411970  [38400/175341]\n",
      "loss: 0.389236  [40000/175341]\n",
      "loss: 0.298040  [41600/175341]\n",
      "loss: 0.619423  [43200/175341]\n",
      "loss: 0.353664  [44800/175341]\n",
      "loss: 0.579118  [46400/175341]\n",
      "loss: 0.447659  [48000/175341]\n",
      "loss: 0.171998  [49600/175341]\n",
      "loss: 0.481571  [51200/175341]\n",
      "loss: 0.695627  [52800/175341]\n",
      "loss: 0.536772  [54400/175341]\n",
      "loss: 0.384304  [56000/175341]\n",
      "loss: 0.363198  [57600/175341]\n",
      "loss: 0.602994  [59200/175341]\n",
      "loss: 0.242323  [60800/175341]\n",
      "loss: 0.543061  [62400/175341]\n",
      "loss: 0.615598  [64000/175341]\n",
      "loss: 0.229226  [65600/175341]\n",
      "loss: 0.193839  [67200/175341]\n",
      "loss: 0.612436  [68800/175341]\n",
      "loss: 0.219580  [70400/175341]\n",
      "loss: 0.182586  [72000/175341]\n",
      "loss: 0.494995  [73600/175341]\n",
      "loss: 0.231571  [75200/175341]\n",
      "loss: 0.335944  [76800/175341]\n",
      "loss: 0.405280  [78400/175341]\n",
      "loss: 0.647250  [80000/175341]\n",
      "loss: 0.554704  [81600/175341]\n",
      "loss: 0.323076  [83200/175341]\n",
      "loss: 0.263207  [84800/175341]\n",
      "loss: 0.697186  [86400/175341]\n",
      "loss: 0.631752  [88000/175341]\n",
      "loss: 0.418288  [89600/175341]\n",
      "loss: 0.592966  [91200/175341]\n",
      "loss: 0.344911  [92800/175341]\n",
      "loss: 0.283623  [94400/175341]\n",
      "loss: 0.316878  [96000/175341]\n",
      "loss: 0.334050  [97600/175341]\n",
      "loss: 0.142514  [99200/175341]\n",
      "loss: 0.933915  [100800/175341]\n",
      "loss: 0.450682  [102400/175341]\n",
      "loss: 0.199938  [104000/175341]\n",
      "loss: 0.202442  [105600/175341]\n",
      "loss: 0.344997  [107200/175341]\n",
      "loss: 0.360051  [108800/175341]\n",
      "loss: 0.234767  [110400/175341]\n",
      "loss: 0.508978  [112000/175341]\n",
      "loss: 0.411375  [113600/175341]\n",
      "loss: 0.872739  [115200/175341]\n",
      "loss: 0.722042  [116800/175341]\n",
      "loss: 0.376272  [118400/175341]\n",
      "loss: 0.345108  [120000/175341]\n",
      "loss: 0.297275  [121600/175341]\n",
      "loss: 0.629823  [123200/175341]\n",
      "loss: 0.470486  [124800/175341]\n",
      "loss: 0.174351  [126400/175341]\n",
      "loss: 0.213370  [128000/175341]\n",
      "loss: 0.400559  [129600/175341]\n",
      "loss: 0.646095  [131200/175341]\n",
      "loss: 0.252499  [132800/175341]\n",
      "loss: 0.271958  [134400/175341]\n",
      "loss: 0.469518  [136000/175341]\n",
      "loss: 0.238318  [137600/175341]\n",
      "loss: 0.429056  [139200/175341]\n",
      "loss: 0.375862  [140800/175341]\n",
      "loss: 0.178295  [142400/175341]\n",
      "loss: 0.425174  [144000/175341]\n",
      "loss: 0.509889  [145600/175341]\n",
      "loss: 0.534322  [147200/175341]\n",
      "loss: 0.808692  [148800/175341]\n",
      "loss: 1.045706  [150400/175341]\n",
      "loss: 0.256501  [152000/175341]\n",
      "loss: 0.682844  [153600/175341]\n",
      "loss: 0.604195  [155200/175341]\n",
      "loss: 0.169031  [156800/175341]\n",
      "loss: 0.655824  [158400/175341]\n",
      "loss: 0.498021  [160000/175341]\n",
      "loss: 0.296126  [161600/175341]\n",
      "loss: 0.686396  [163200/175341]\n",
      "loss: 0.410430  [164800/175341]\n",
      "loss: 0.483344  [166400/175341]\n",
      "loss: 0.325682  [168000/175341]\n",
      "loss: 0.529921  [169600/175341]\n",
      "loss: 0.595796  [171200/175341]\n",
      "loss: 0.425008  [172800/175341]\n",
      "loss: 0.628769  [174400/175341]\n",
      "Train Accuracy: 80.9503%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.566067, F1-score: 75.23%, Macro_F1-Score:  39.95%  \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.252318  [    0/175341]\n",
      "loss: 0.460837  [ 1600/175341]\n",
      "loss: 0.566284  [ 3200/175341]\n",
      "loss: 0.682203  [ 4800/175341]\n",
      "loss: 0.330549  [ 6400/175341]\n",
      "loss: 0.879847  [ 8000/175341]\n",
      "loss: 0.654282  [ 9600/175341]\n",
      "loss: 0.629784  [11200/175341]\n",
      "loss: 0.274837  [12800/175341]\n",
      "loss: 0.449353  [14400/175341]\n",
      "loss: 0.372662  [16000/175341]\n",
      "loss: 0.225359  [17600/175341]\n",
      "loss: 0.123504  [19200/175341]\n",
      "loss: 1.009029  [20800/175341]\n",
      "loss: 0.594373  [22400/175341]\n",
      "loss: 0.822061  [24000/175341]\n",
      "loss: 0.393750  [25600/175341]\n",
      "loss: 0.763697  [27200/175341]\n",
      "loss: 0.334230  [28800/175341]\n",
      "loss: 0.905409  [30400/175341]\n",
      "loss: 0.370826  [32000/175341]\n",
      "loss: 0.319671  [33600/175341]\n",
      "loss: 0.630870  [35200/175341]\n",
      "loss: 0.282477  [36800/175341]\n",
      "loss: 0.209321  [38400/175341]\n",
      "loss: 0.350666  [40000/175341]\n",
      "loss: 0.652396  [41600/175341]\n",
      "loss: 0.090234  [43200/175341]\n",
      "loss: 0.553433  [44800/175341]\n",
      "loss: 0.593551  [46400/175341]\n",
      "loss: 0.241690  [48000/175341]\n",
      "loss: 0.692603  [49600/175341]\n",
      "loss: 0.110075  [51200/175341]\n",
      "loss: 0.395243  [52800/175341]\n",
      "loss: 0.456736  [54400/175341]\n",
      "loss: 0.348547  [56000/175341]\n",
      "loss: 0.304336  [57600/175341]\n",
      "loss: 0.476292  [59200/175341]\n",
      "loss: 0.953178  [60800/175341]\n",
      "loss: 0.416568  [62400/175341]\n",
      "loss: 0.410391  [64000/175341]\n",
      "loss: 0.386315  [65600/175341]\n",
      "loss: 0.880996  [67200/175341]\n",
      "loss: 0.296557  [68800/175341]\n",
      "loss: 0.533922  [70400/175341]\n",
      "loss: 0.446639  [72000/175341]\n",
      "loss: 0.929238  [73600/175341]\n",
      "loss: 0.282968  [75200/175341]\n",
      "loss: 0.684694  [76800/175341]\n",
      "loss: 0.204030  [78400/175341]\n",
      "loss: 0.330245  [80000/175341]\n",
      "loss: 0.333714  [81600/175341]\n",
      "loss: 0.368405  [83200/175341]\n",
      "loss: 0.362353  [84800/175341]\n",
      "loss: 0.182023  [86400/175341]\n",
      "loss: 0.668214  [88000/175341]\n",
      "loss: 0.831583  [89600/175341]\n",
      "loss: 0.171295  [91200/175341]\n",
      "loss: 0.750758  [92800/175341]\n",
      "loss: 0.756269  [94400/175341]\n",
      "loss: 0.647610  [96000/175341]\n",
      "loss: 0.180017  [97600/175341]\n",
      "loss: 0.723610  [99200/175341]\n",
      "loss: 0.851062  [100800/175341]\n",
      "loss: 0.243674  [102400/175341]\n",
      "loss: 0.402003  [104000/175341]\n",
      "loss: 0.276598  [105600/175341]\n",
      "loss: 0.458861  [107200/175341]\n",
      "loss: 0.523126  [108800/175341]\n",
      "loss: 0.536785  [110400/175341]\n",
      "loss: 0.269762  [112000/175341]\n",
      "loss: 0.802845  [113600/175341]\n",
      "loss: 0.633837  [115200/175341]\n",
      "loss: 0.561782  [116800/175341]\n",
      "loss: 0.529258  [118400/175341]\n",
      "loss: 0.434000  [120000/175341]\n",
      "loss: 0.309444  [121600/175341]\n",
      "loss: 0.331726  [123200/175341]\n",
      "loss: 0.576311  [124800/175341]\n",
      "loss: 0.771840  [126400/175341]\n",
      "loss: 0.147431  [128000/175341]\n",
      "loss: 0.191577  [129600/175341]\n",
      "loss: 0.231284  [131200/175341]\n",
      "loss: 0.489277  [132800/175341]\n",
      "loss: 0.432088  [134400/175341]\n",
      "loss: 0.785223  [136000/175341]\n",
      "loss: 0.334899  [137600/175341]\n",
      "loss: 0.613539  [139200/175341]\n",
      "loss: 0.517154  [140800/175341]\n",
      "loss: 0.232729  [142400/175341]\n",
      "loss: 0.248672  [144000/175341]\n",
      "loss: 0.464881  [145600/175341]\n",
      "loss: 0.501691  [147200/175341]\n",
      "loss: 0.353386  [148800/175341]\n",
      "loss: 0.615791  [150400/175341]\n",
      "loss: 0.902839  [152000/175341]\n",
      "loss: 0.381309  [153600/175341]\n",
      "loss: 0.715339  [155200/175341]\n",
      "loss: 0.560307  [156800/175341]\n",
      "loss: 0.331318  [158400/175341]\n",
      "loss: 0.808906  [160000/175341]\n",
      "loss: 0.155045  [161600/175341]\n",
      "loss: 0.444247  [163200/175341]\n",
      "loss: 0.295582  [164800/175341]\n",
      "loss: 0.662499  [166400/175341]\n",
      "loss: 0.741584  [168000/175341]\n",
      "loss: 0.829172  [169600/175341]\n",
      "loss: 0.473688  [171200/175341]\n",
      "loss: 0.282308  [172800/175341]\n",
      "loss: 0.394453  [174400/175341]\n",
      "Train Accuracy: 80.8944%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.563786, F1-score: 75.64%, Macro_F1-Score:  40.45%  \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.385021  [    0/175341]\n",
      "loss: 0.226186  [ 1600/175341]\n",
      "loss: 0.448782  [ 3200/175341]\n",
      "loss: 0.422970  [ 4800/175341]\n",
      "loss: 0.178490  [ 6400/175341]\n",
      "loss: 0.294022  [ 8000/175341]\n",
      "loss: 0.586067  [ 9600/175341]\n",
      "loss: 0.267422  [11200/175341]\n",
      "loss: 0.362866  [12800/175341]\n",
      "loss: 0.417993  [14400/175341]\n",
      "loss: 0.442543  [16000/175341]\n",
      "loss: 0.260758  [17600/175341]\n",
      "loss: 0.505739  [19200/175341]\n",
      "loss: 0.668766  [20800/175341]\n",
      "loss: 0.707609  [22400/175341]\n",
      "loss: 0.303707  [24000/175341]\n",
      "loss: 0.712559  [25600/175341]\n",
      "loss: 0.435939  [27200/175341]\n",
      "loss: 0.315685  [28800/175341]\n",
      "loss: 0.773166  [30400/175341]\n",
      "loss: 0.641758  [32000/175341]\n",
      "loss: 0.384754  [33600/175341]\n",
      "loss: 0.523264  [35200/175341]\n",
      "loss: 0.422019  [36800/175341]\n",
      "loss: 0.501057  [38400/175341]\n",
      "loss: 0.452087  [40000/175341]\n",
      "loss: 0.472091  [41600/175341]\n",
      "loss: 0.439439  [43200/175341]\n",
      "loss: 0.391259  [44800/175341]\n",
      "loss: 0.568936  [46400/175341]\n",
      "loss: 0.791908  [48000/175341]\n",
      "loss: 0.610118  [49600/175341]\n",
      "loss: 0.384466  [51200/175341]\n",
      "loss: 0.678116  [52800/175341]\n",
      "loss: 0.267290  [54400/175341]\n",
      "loss: 0.486551  [56000/175341]\n",
      "loss: 0.386518  [57600/175341]\n",
      "loss: 0.365165  [59200/175341]\n",
      "loss: 0.196765  [60800/175341]\n",
      "loss: 0.446932  [62400/175341]\n",
      "loss: 0.172258  [64000/175341]\n",
      "loss: 0.382090  [65600/175341]\n",
      "loss: 0.426386  [67200/175341]\n",
      "loss: 0.225313  [68800/175341]\n",
      "loss: 0.568455  [70400/175341]\n",
      "loss: 0.916585  [72000/175341]\n",
      "loss: 0.514461  [73600/175341]\n",
      "loss: 0.308541  [75200/175341]\n",
      "loss: 0.500269  [76800/175341]\n",
      "loss: 0.428588  [78400/175341]\n",
      "loss: 0.214866  [80000/175341]\n",
      "loss: 0.268160  [81600/175341]\n",
      "loss: 0.811965  [83200/175341]\n",
      "loss: 0.613414  [84800/175341]\n",
      "loss: 0.865177  [86400/175341]\n",
      "loss: 0.896705  [88000/175341]\n",
      "loss: 0.509128  [89600/175341]\n",
      "loss: 0.816074  [91200/175341]\n",
      "loss: 0.763899  [92800/175341]\n",
      "loss: 0.819062  [94400/175341]\n",
      "loss: 0.698143  [96000/175341]\n",
      "loss: 0.367018  [97600/175341]\n",
      "loss: 0.506205  [99200/175341]\n",
      "loss: 0.645686  [100800/175341]\n",
      "loss: 0.431374  [102400/175341]\n",
      "loss: 0.389450  [104000/175341]\n",
      "loss: 0.293917  [105600/175341]\n",
      "loss: 0.606161  [107200/175341]\n",
      "loss: 0.525483  [108800/175341]\n",
      "loss: 0.458080  [110400/175341]\n",
      "loss: 0.334208  [112000/175341]\n",
      "loss: 0.337215  [113600/175341]\n",
      "loss: 0.316141  [115200/175341]\n",
      "loss: 0.414200  [116800/175341]\n",
      "loss: 0.312106  [118400/175341]\n",
      "loss: 0.663476  [120000/175341]\n",
      "loss: 0.519056  [121600/175341]\n",
      "loss: 0.145351  [123200/175341]\n",
      "loss: 0.204653  [124800/175341]\n",
      "loss: 0.397145  [126400/175341]\n",
      "loss: 0.329233  [128000/175341]\n",
      "loss: 0.295070  [129600/175341]\n",
      "loss: 0.456931  [131200/175341]\n",
      "loss: 0.663808  [132800/175341]\n",
      "loss: 0.617168  [134400/175341]\n",
      "loss: 0.413941  [136000/175341]\n",
      "loss: 0.413430  [137600/175341]\n",
      "loss: 0.436951  [139200/175341]\n",
      "loss: 0.278081  [140800/175341]\n",
      "loss: 0.148782  [142400/175341]\n",
      "loss: 0.476601  [144000/175341]\n",
      "loss: 0.389086  [145600/175341]\n",
      "loss: 0.780022  [147200/175341]\n",
      "loss: 0.596559  [148800/175341]\n",
      "loss: 0.725734  [150400/175341]\n",
      "loss: 0.335266  [152000/175341]\n",
      "loss: 0.355591  [153600/175341]\n",
      "loss: 0.403533  [155200/175341]\n",
      "loss: 0.278282  [156800/175341]\n",
      "loss: 0.777751  [158400/175341]\n",
      "loss: 0.420085  [160000/175341]\n",
      "loss: 0.561665  [161600/175341]\n",
      "loss: 0.543659  [163200/175341]\n",
      "loss: 0.213475  [164800/175341]\n",
      "loss: 0.164235  [166400/175341]\n",
      "loss: 0.448250  [168000/175341]\n",
      "loss: 0.696978  [169600/175341]\n",
      "loss: 0.456731  [171200/175341]\n",
      "loss: 0.746561  [172800/175341]\n",
      "loss: 0.508648  [174400/175341]\n",
      "Train Accuracy: 80.9702%\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.593856, F1-score: 73.13%, Macro_F1-Score:  39.02%  \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.629317  [    0/175341]\n",
      "loss: 0.449639  [ 1600/175341]\n",
      "loss: 0.895819  [ 3200/175341]\n",
      "loss: 0.461526  [ 4800/175341]\n",
      "loss: 0.394586  [ 6400/175341]\n",
      "loss: 0.142623  [ 8000/175341]\n",
      "loss: 0.465915  [ 9600/175341]\n",
      "loss: 0.475192  [11200/175341]\n",
      "loss: 0.481830  [12800/175341]\n",
      "loss: 0.158516  [14400/175341]\n",
      "loss: 0.584730  [16000/175341]\n",
      "loss: 0.471517  [17600/175341]\n",
      "loss: 0.217795  [19200/175341]\n",
      "loss: 0.410092  [20800/175341]\n",
      "loss: 0.541221  [22400/175341]\n",
      "loss: 0.598643  [24000/175341]\n",
      "loss: 0.313071  [25600/175341]\n",
      "loss: 0.486296  [27200/175341]\n",
      "loss: 1.054379  [28800/175341]\n",
      "loss: 0.841766  [30400/175341]\n",
      "loss: 0.326670  [32000/175341]\n",
      "loss: 0.281941  [33600/175341]\n",
      "loss: 0.125150  [35200/175341]\n",
      "loss: 0.200682  [36800/175341]\n",
      "loss: 0.914711  [38400/175341]\n",
      "loss: 0.381822  [40000/175341]\n",
      "loss: 0.366847  [41600/175341]\n",
      "loss: 0.234645  [43200/175341]\n",
      "loss: 0.265299  [44800/175341]\n",
      "loss: 0.358912  [46400/175341]\n",
      "loss: 0.502588  [48000/175341]\n",
      "loss: 0.628444  [49600/175341]\n",
      "loss: 0.568985  [51200/175341]\n",
      "loss: 0.986541  [52800/175341]\n",
      "loss: 0.390284  [54400/175341]\n",
      "loss: 0.347045  [56000/175341]\n",
      "loss: 0.186059  [57600/175341]\n",
      "loss: 0.651209  [59200/175341]\n",
      "loss: 0.204337  [60800/175341]\n",
      "loss: 0.744563  [62400/175341]\n",
      "loss: 0.338410  [64000/175341]\n",
      "loss: 0.214626  [65600/175341]\n",
      "loss: 0.709492  [67200/175341]\n",
      "loss: 0.234606  [68800/175341]\n",
      "loss: 0.502768  [70400/175341]\n",
      "loss: 0.656331  [72000/175341]\n",
      "loss: 0.326392  [73600/175341]\n",
      "loss: 0.728427  [75200/175341]\n",
      "loss: 0.607431  [76800/175341]\n",
      "loss: 0.800667  [78400/175341]\n",
      "loss: 0.507253  [80000/175341]\n",
      "loss: 0.561558  [81600/175341]\n",
      "loss: 0.654866  [83200/175341]\n",
      "loss: 0.430190  [84800/175341]\n",
      "loss: 0.269449  [86400/175341]\n",
      "loss: 0.409613  [88000/175341]\n",
      "loss: 0.528224  [89600/175341]\n",
      "loss: 1.061577  [91200/175341]\n",
      "loss: 0.484768  [92800/175341]\n",
      "loss: 0.181181  [94400/175341]\n",
      "loss: 0.321973  [96000/175341]\n",
      "loss: 0.626886  [97600/175341]\n",
      "loss: 0.334964  [99200/175341]\n",
      "loss: 0.623108  [100800/175341]\n",
      "loss: 0.341654  [102400/175341]\n",
      "loss: 0.200396  [104000/175341]\n",
      "loss: 0.256854  [105600/175341]\n",
      "loss: 0.276165  [107200/175341]\n",
      "loss: 0.331996  [108800/175341]\n",
      "loss: 0.260355  [110400/175341]\n",
      "loss: 0.493433  [112000/175341]\n",
      "loss: 0.548667  [113600/175341]\n",
      "loss: 0.296439  [115200/175341]\n",
      "loss: 0.296228  [116800/175341]\n",
      "loss: 0.357103  [118400/175341]\n",
      "loss: 0.410805  [120000/175341]\n",
      "loss: 0.419164  [121600/175341]\n",
      "loss: 0.405816  [123200/175341]\n",
      "loss: 0.295421  [124800/175341]\n",
      "loss: 0.567733  [126400/175341]\n",
      "loss: 0.104674  [128000/175341]\n",
      "loss: 0.697459  [129600/175341]\n",
      "loss: 0.403613  [131200/175341]\n",
      "loss: 0.650566  [132800/175341]\n",
      "loss: 0.409490  [134400/175341]\n",
      "loss: 0.746796  [136000/175341]\n",
      "loss: 0.249223  [137600/175341]\n",
      "loss: 0.453221  [139200/175341]\n",
      "loss: 0.322708  [140800/175341]\n",
      "loss: 0.776313  [142400/175341]\n",
      "loss: 0.750658  [144000/175341]\n",
      "loss: 0.214929  [145600/175341]\n",
      "loss: 0.795548  [147200/175341]\n",
      "loss: 0.702499  [148800/175341]\n",
      "loss: 0.772335  [150400/175341]\n",
      "loss: 0.640924  [152000/175341]\n",
      "loss: 0.945005  [153600/175341]\n",
      "loss: 0.414242  [155200/175341]\n",
      "loss: 0.511586  [156800/175341]\n",
      "loss: 0.211385  [158400/175341]\n",
      "loss: 1.269332  [160000/175341]\n",
      "loss: 0.294552  [161600/175341]\n",
      "loss: 0.464532  [163200/175341]\n",
      "loss: 0.269589  [164800/175341]\n",
      "loss: 0.563929  [166400/175341]\n",
      "loss: 0.469613  [168000/175341]\n",
      "loss: 0.511600  [169600/175341]\n",
      "loss: 0.299916  [171200/175341]\n",
      "loss: 0.796252  [172800/175341]\n",
      "loss: 0.382734  [174400/175341]\n",
      "Train Accuracy: 81.0381%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.579699, F1-score: 74.26%, Macro_F1-Score:  39.57%  \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.347815  [    0/175341]\n",
      "loss: 0.175324  [ 1600/175341]\n",
      "loss: 0.284647  [ 3200/175341]\n",
      "loss: 0.054972  [ 4800/175341]\n",
      "loss: 0.319601  [ 6400/175341]\n",
      "loss: 0.145214  [ 8000/175341]\n",
      "loss: 0.443677  [ 9600/175341]\n",
      "loss: 0.461878  [11200/175341]\n",
      "loss: 0.462035  [12800/175341]\n",
      "loss: 0.218555  [14400/175341]\n",
      "loss: 0.274078  [16000/175341]\n",
      "loss: 0.547732  [17600/175341]\n",
      "loss: 0.812140  [19200/175341]\n",
      "loss: 0.566923  [20800/175341]\n",
      "loss: 0.243003  [22400/175341]\n",
      "loss: 0.400201  [24000/175341]\n",
      "loss: 0.354333  [25600/175341]\n",
      "loss: 0.302159  [27200/175341]\n",
      "loss: 0.532124  [28800/175341]\n",
      "loss: 0.565073  [30400/175341]\n",
      "loss: 0.621685  [32000/175341]\n",
      "loss: 0.500169  [33600/175341]\n",
      "loss: 0.338341  [35200/175341]\n",
      "loss: 0.271058  [36800/175341]\n",
      "loss: 0.461842  [38400/175341]\n",
      "loss: 0.666705  [40000/175341]\n",
      "loss: 0.697790  [41600/175341]\n",
      "loss: 0.704971  [43200/175341]\n",
      "loss: 0.691010  [44800/175341]\n",
      "loss: 0.239064  [46400/175341]\n",
      "loss: 0.281409  [48000/175341]\n",
      "loss: 0.131419  [49600/175341]\n",
      "loss: 0.492052  [51200/175341]\n",
      "loss: 0.190858  [52800/175341]\n",
      "loss: 0.456745  [54400/175341]\n",
      "loss: 0.340830  [56000/175341]\n",
      "loss: 0.537842  [57600/175341]\n",
      "loss: 0.419453  [59200/175341]\n",
      "loss: 0.413565  [60800/175341]\n",
      "loss: 0.420123  [62400/175341]\n",
      "loss: 0.156105  [64000/175341]\n",
      "loss: 0.518643  [65600/175341]\n",
      "loss: 0.339138  [67200/175341]\n",
      "loss: 0.466581  [68800/175341]\n",
      "loss: 0.307714  [70400/175341]\n",
      "loss: 1.000081  [72000/175341]\n",
      "loss: 0.440401  [73600/175341]\n",
      "loss: 0.615816  [75200/175341]\n",
      "loss: 0.446110  [76800/175341]\n",
      "loss: 0.384784  [78400/175341]\n",
      "loss: 0.230505  [80000/175341]\n",
      "loss: 0.256344  [81600/175341]\n",
      "loss: 0.478726  [83200/175341]\n",
      "loss: 0.247805  [84800/175341]\n",
      "loss: 0.331870  [86400/175341]\n",
      "loss: 1.352139  [88000/175341]\n",
      "loss: 0.472424  [89600/175341]\n",
      "loss: 0.453580  [91200/175341]\n",
      "loss: 0.426300  [92800/175341]\n",
      "loss: 0.406481  [94400/175341]\n",
      "loss: 0.405639  [96000/175341]\n",
      "loss: 0.581034  [97600/175341]\n",
      "loss: 0.286919  [99200/175341]\n",
      "loss: 0.673720  [100800/175341]\n",
      "loss: 0.101057  [102400/175341]\n",
      "loss: 0.378954  [104000/175341]\n",
      "loss: 0.612374  [105600/175341]\n",
      "loss: 0.822186  [107200/175341]\n",
      "loss: 0.630592  [108800/175341]\n",
      "loss: 0.645624  [110400/175341]\n",
      "loss: 0.292091  [112000/175341]\n",
      "loss: 0.479080  [113600/175341]\n",
      "loss: 0.522059  [115200/175341]\n",
      "loss: 0.791026  [116800/175341]\n",
      "loss: 0.700185  [118400/175341]\n",
      "loss: 0.322868  [120000/175341]\n",
      "loss: 0.863244  [121600/175341]\n",
      "loss: 0.218220  [123200/175341]\n",
      "loss: 0.430338  [124800/175341]\n",
      "loss: 0.529463  [126400/175341]\n",
      "loss: 0.599056  [128000/175341]\n",
      "loss: 0.339745  [129600/175341]\n",
      "loss: 0.736946  [131200/175341]\n",
      "loss: 0.499409  [132800/175341]\n",
      "loss: 0.622869  [134400/175341]\n",
      "loss: 0.518693  [136000/175341]\n",
      "loss: 0.493906  [137600/175341]\n",
      "loss: 0.802803  [139200/175341]\n",
      "loss: 0.495591  [140800/175341]\n",
      "loss: 0.427505  [142400/175341]\n",
      "loss: 0.508116  [144000/175341]\n",
      "loss: 0.589697  [145600/175341]\n",
      "loss: 0.332213  [147200/175341]\n",
      "loss: 0.394899  [148800/175341]\n",
      "loss: 0.755448  [150400/175341]\n",
      "loss: 0.847716  [152000/175341]\n",
      "loss: 0.395715  [153600/175341]\n",
      "loss: 0.970237  [155200/175341]\n",
      "loss: 0.719447  [156800/175341]\n",
      "loss: 0.266916  [158400/175341]\n",
      "loss: 0.155141  [160000/175341]\n",
      "loss: 0.345774  [161600/175341]\n",
      "loss: 0.633949  [163200/175341]\n",
      "loss: 0.492437  [164800/175341]\n",
      "loss: 0.669406  [166400/175341]\n",
      "loss: 0.530793  [168000/175341]\n",
      "loss: 0.520977  [169600/175341]\n",
      "loss: 0.226407  [171200/175341]\n",
      "loss: 0.416833  [172800/175341]\n",
      "loss: 0.415289  [174400/175341]\n",
      "Train Accuracy: 81.0643%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.575638, F1-score: 74.31%, Macro_F1-Score:  39.81%  \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.122009  [    0/175341]\n",
      "loss: 0.301442  [ 1600/175341]\n",
      "loss: 0.338304  [ 3200/175341]\n",
      "loss: 0.508411  [ 4800/175341]\n",
      "loss: 0.364607  [ 6400/175341]\n",
      "loss: 0.408037  [ 8000/175341]\n",
      "loss: 0.099044  [ 9600/175341]\n",
      "loss: 0.353966  [11200/175341]\n",
      "loss: 0.355800  [12800/175341]\n",
      "loss: 0.789881  [14400/175341]\n",
      "loss: 0.354419  [16000/175341]\n",
      "loss: 0.381429  [17600/175341]\n",
      "loss: 0.672349  [19200/175341]\n",
      "loss: 0.780134  [20800/175341]\n",
      "loss: 0.272832  [22400/175341]\n",
      "loss: 0.357307  [24000/175341]\n",
      "loss: 0.620539  [25600/175341]\n",
      "loss: 0.596073  [27200/175341]\n",
      "loss: 0.482355  [28800/175341]\n",
      "loss: 0.757768  [30400/175341]\n",
      "loss: 0.174748  [32000/175341]\n",
      "loss: 0.312660  [33600/175341]\n",
      "loss: 0.403602  [35200/175341]\n",
      "loss: 0.412704  [36800/175341]\n",
      "loss: 0.359463  [38400/175341]\n",
      "loss: 0.348942  [40000/175341]\n",
      "loss: 0.212707  [41600/175341]\n",
      "loss: 0.389848  [43200/175341]\n",
      "loss: 0.725759  [44800/175341]\n",
      "loss: 0.288950  [46400/175341]\n",
      "loss: 0.269796  [48000/175341]\n",
      "loss: 0.399455  [49600/175341]\n",
      "loss: 0.303125  [51200/175341]\n",
      "loss: 0.314180  [52800/175341]\n",
      "loss: 0.404067  [54400/175341]\n",
      "loss: 0.349817  [56000/175341]\n",
      "loss: 0.381401  [57600/175341]\n",
      "loss: 0.313178  [59200/175341]\n",
      "loss: 0.223672  [60800/175341]\n",
      "loss: 0.620544  [62400/175341]\n",
      "loss: 0.328879  [64000/175341]\n",
      "loss: 0.458872  [65600/175341]\n",
      "loss: 0.287548  [67200/175341]\n",
      "loss: 0.297179  [68800/175341]\n",
      "loss: 1.025512  [70400/175341]\n",
      "loss: 0.096404  [72000/175341]\n",
      "loss: 0.415800  [73600/175341]\n",
      "loss: 0.326723  [75200/175341]\n",
      "loss: 0.393614  [76800/175341]\n",
      "loss: 0.528493  [78400/175341]\n",
      "loss: 0.426656  [80000/175341]\n",
      "loss: 0.428854  [81600/175341]\n",
      "loss: 0.390643  [83200/175341]\n",
      "loss: 0.820063  [84800/175341]\n",
      "loss: 0.349699  [86400/175341]\n",
      "loss: 0.424198  [88000/175341]\n",
      "loss: 0.382673  [89600/175341]\n",
      "loss: 0.902354  [91200/175341]\n",
      "loss: 0.677911  [92800/175341]\n",
      "loss: 0.189019  [94400/175341]\n",
      "loss: 0.205543  [96000/175341]\n",
      "loss: 0.347526  [97600/175341]\n",
      "loss: 0.440510  [99200/175341]\n",
      "loss: 0.213440  [100800/175341]\n",
      "loss: 0.404260  [102400/175341]\n",
      "loss: 0.714218  [104000/175341]\n",
      "loss: 0.815271  [105600/175341]\n",
      "loss: 0.754947  [107200/175341]\n",
      "loss: 0.352928  [108800/175341]\n",
      "loss: 0.721714  [110400/175341]\n",
      "loss: 1.139736  [112000/175341]\n",
      "loss: 0.420888  [113600/175341]\n",
      "loss: 0.324211  [115200/175341]\n",
      "loss: 0.659237  [116800/175341]\n",
      "loss: 0.411099  [118400/175341]\n",
      "loss: 0.706233  [120000/175341]\n",
      "loss: 0.022337  [121600/175341]\n",
      "loss: 0.243906  [123200/175341]\n",
      "loss: 0.393523  [124800/175341]\n",
      "loss: 0.754509  [126400/175341]\n",
      "loss: 0.591389  [128000/175341]\n",
      "loss: 0.767504  [129600/175341]\n",
      "loss: 0.517672  [131200/175341]\n",
      "loss: 0.456746  [132800/175341]\n",
      "loss: 0.506718  [134400/175341]\n",
      "loss: 0.363877  [136000/175341]\n",
      "loss: 0.418788  [137600/175341]\n",
      "loss: 0.520201  [139200/175341]\n",
      "loss: 0.668968  [140800/175341]\n",
      "loss: 0.594299  [142400/175341]\n",
      "loss: 0.431549  [144000/175341]\n",
      "loss: 0.519827  [145600/175341]\n",
      "loss: 0.521073  [147200/175341]\n",
      "loss: 0.524861  [148800/175341]\n",
      "loss: 0.603118  [150400/175341]\n",
      "loss: 0.409283  [152000/175341]\n",
      "loss: 0.681522  [153600/175341]\n",
      "loss: 0.209973  [155200/175341]\n",
      "loss: 0.774006  [156800/175341]\n",
      "loss: 0.365626  [158400/175341]\n",
      "loss: 0.464433  [160000/175341]\n",
      "loss: 0.201618  [161600/175341]\n",
      "loss: 0.962919  [163200/175341]\n",
      "loss: 0.209740  [164800/175341]\n",
      "loss: 0.371113  [166400/175341]\n",
      "loss: 0.523788  [168000/175341]\n",
      "loss: 0.149446  [169600/175341]\n",
      "loss: 0.451447  [171200/175341]\n",
      "loss: 0.342898  [172800/175341]\n",
      "loss: 0.607223  [174400/175341]\n",
      "Train Accuracy: 81.0404%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.567314, F1-score: 75.18%, Macro_F1-Score:  40.35%  \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.428724  [    0/175341]\n",
      "loss: 0.221648  [ 1600/175341]\n",
      "loss: 0.725808  [ 3200/175341]\n",
      "loss: 0.214147  [ 4800/175341]\n",
      "loss: 0.723463  [ 6400/175341]\n",
      "loss: 0.180464  [ 8000/175341]\n",
      "loss: 0.639591  [ 9600/175341]\n",
      "loss: 0.413724  [11200/175341]\n",
      "loss: 0.277330  [12800/175341]\n",
      "loss: 0.349951  [14400/175341]\n",
      "loss: 0.332098  [16000/175341]\n",
      "loss: 0.804740  [17600/175341]\n",
      "loss: 0.710048  [19200/175341]\n",
      "loss: 0.400406  [20800/175341]\n",
      "loss: 0.340788  [22400/175341]\n",
      "loss: 0.849490  [24000/175341]\n",
      "loss: 0.258904  [25600/175341]\n",
      "loss: 0.256454  [27200/175341]\n",
      "loss: 0.337862  [28800/175341]\n",
      "loss: 0.619316  [30400/175341]\n",
      "loss: 0.406546  [32000/175341]\n",
      "loss: 0.402309  [33600/175341]\n",
      "loss: 0.618887  [35200/175341]\n",
      "loss: 0.646056  [36800/175341]\n",
      "loss: 0.940003  [38400/175341]\n",
      "loss: 0.250129  [40000/175341]\n",
      "loss: 0.561464  [41600/175341]\n",
      "loss: 0.223370  [43200/175341]\n",
      "loss: 0.471304  [44800/175341]\n",
      "loss: 0.394999  [46400/175341]\n",
      "loss: 0.691856  [48000/175341]\n",
      "loss: 0.381750  [49600/175341]\n",
      "loss: 0.643089  [51200/175341]\n",
      "loss: 0.635386  [52800/175341]\n",
      "loss: 0.667205  [54400/175341]\n",
      "loss: 0.344867  [56000/175341]\n",
      "loss: 0.278995  [57600/175341]\n",
      "loss: 0.150915  [59200/175341]\n",
      "loss: 0.729783  [60800/175341]\n",
      "loss: 0.516574  [62400/175341]\n",
      "loss: 0.450351  [64000/175341]\n",
      "loss: 0.418852  [65600/175341]\n",
      "loss: 0.350099  [67200/175341]\n",
      "loss: 0.329339  [68800/175341]\n",
      "loss: 0.373319  [70400/175341]\n",
      "loss: 0.508244  [72000/175341]\n",
      "loss: 0.517961  [73600/175341]\n",
      "loss: 0.773311  [75200/175341]\n",
      "loss: 0.354121  [76800/175341]\n",
      "loss: 0.486740  [78400/175341]\n",
      "loss: 0.439821  [80000/175341]\n",
      "loss: 0.619773  [81600/175341]\n",
      "loss: 0.511848  [83200/175341]\n",
      "loss: 0.771250  [84800/175341]\n",
      "loss: 0.159812  [86400/175341]\n",
      "loss: 0.332932  [88000/175341]\n",
      "loss: 0.393749  [89600/175341]\n",
      "loss: 0.450499  [91200/175341]\n",
      "loss: 0.269164  [92800/175341]\n",
      "loss: 0.265069  [94400/175341]\n",
      "loss: 0.410905  [96000/175341]\n",
      "loss: 0.263344  [97600/175341]\n",
      "loss: 0.518167  [99200/175341]\n",
      "loss: 0.664184  [100800/175341]\n",
      "loss: 0.269550  [102400/175341]\n",
      "loss: 0.325189  [104000/175341]\n",
      "loss: 0.127519  [105600/175341]\n",
      "loss: 0.609104  [107200/175341]\n",
      "loss: 0.537883  [108800/175341]\n",
      "loss: 0.464136  [110400/175341]\n",
      "loss: 0.337296  [112000/175341]\n",
      "loss: 0.130796  [113600/175341]\n",
      "loss: 0.458553  [115200/175341]\n",
      "loss: 0.750052  [116800/175341]\n",
      "loss: 0.277465  [118400/175341]\n",
      "loss: 0.715991  [120000/175341]\n",
      "loss: 0.331879  [121600/175341]\n",
      "loss: 0.334332  [123200/175341]\n",
      "loss: 0.484033  [124800/175341]\n",
      "loss: 0.470667  [126400/175341]\n",
      "loss: 0.462131  [128000/175341]\n",
      "loss: 1.084589  [129600/175341]\n",
      "loss: 0.485815  [131200/175341]\n",
      "loss: 0.371523  [132800/175341]\n",
      "loss: 0.846474  [134400/175341]\n",
      "loss: 0.620166  [136000/175341]\n",
      "loss: 0.304970  [137600/175341]\n",
      "loss: 0.563666  [139200/175341]\n",
      "loss: 0.420549  [140800/175341]\n",
      "loss: 0.536051  [142400/175341]\n",
      "loss: 0.158208  [144000/175341]\n",
      "loss: 0.457063  [145600/175341]\n",
      "loss: 0.559263  [147200/175341]\n",
      "loss: 0.335366  [148800/175341]\n",
      "loss: 0.604736  [150400/175341]\n",
      "loss: 0.915520  [152000/175341]\n",
      "loss: 0.384574  [153600/175341]\n",
      "loss: 0.467886  [155200/175341]\n",
      "loss: 0.578291  [156800/175341]\n",
      "loss: 0.782523  [158400/175341]\n",
      "loss: 0.712661  [160000/175341]\n",
      "loss: 0.362894  [161600/175341]\n",
      "loss: 0.457404  [163200/175341]\n",
      "loss: 0.901281  [164800/175341]\n",
      "loss: 0.732550  [166400/175341]\n",
      "loss: 0.335152  [168000/175341]\n",
      "loss: 0.390993  [169600/175341]\n",
      "loss: 0.276430  [171200/175341]\n",
      "loss: 0.563399  [172800/175341]\n",
      "loss: 0.399084  [174400/175341]\n",
      "Train Accuracy: 81.0284%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.573756, F1-score: 74.86%, Macro_F1-Score:  40.33%  \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.917745  [    0/175341]\n",
      "loss: 0.323176  [ 1600/175341]\n",
      "loss: 0.253482  [ 3200/175341]\n",
      "loss: 0.202209  [ 4800/175341]\n",
      "loss: 0.544383  [ 6400/175341]\n",
      "loss: 0.599037  [ 8000/175341]\n",
      "loss: 0.281520  [ 9600/175341]\n",
      "loss: 0.210211  [11200/175341]\n",
      "loss: 0.265735  [12800/175341]\n",
      "loss: 0.333683  [14400/175341]\n",
      "loss: 0.124441  [16000/175341]\n",
      "loss: 0.705335  [17600/175341]\n",
      "loss: 0.518228  [19200/175341]\n",
      "loss: 0.507461  [20800/175341]\n",
      "loss: 0.489022  [22400/175341]\n",
      "loss: 0.422013  [24000/175341]\n",
      "loss: 0.602887  [25600/175341]\n",
      "loss: 0.412900  [27200/175341]\n",
      "loss: 0.378899  [28800/175341]\n",
      "loss: 0.600160  [30400/175341]\n",
      "loss: 0.123203  [32000/175341]\n",
      "loss: 0.696532  [33600/175341]\n",
      "loss: 0.220992  [35200/175341]\n",
      "loss: 0.274782  [36800/175341]\n",
      "loss: 0.295116  [38400/175341]\n",
      "loss: 0.341096  [40000/175341]\n",
      "loss: 0.304384  [41600/175341]\n",
      "loss: 0.599353  [43200/175341]\n",
      "loss: 0.279092  [44800/175341]\n",
      "loss: 0.530058  [46400/175341]\n",
      "loss: 0.232779  [48000/175341]\n",
      "loss: 0.621169  [49600/175341]\n",
      "loss: 0.492009  [51200/175341]\n",
      "loss: 0.265468  [52800/175341]\n",
      "loss: 0.378066  [54400/175341]\n",
      "loss: 0.297458  [56000/175341]\n",
      "loss: 0.418880  [57600/175341]\n",
      "loss: 0.736132  [59200/175341]\n",
      "loss: 0.300145  [60800/175341]\n",
      "loss: 0.421961  [62400/175341]\n",
      "loss: 0.618229  [64000/175341]\n",
      "loss: 0.391495  [65600/175341]\n",
      "loss: 0.233325  [67200/175341]\n",
      "loss: 0.336995  [68800/175341]\n",
      "loss: 0.317925  [70400/175341]\n",
      "loss: 0.288318  [72000/175341]\n",
      "loss: 0.597768  [73600/175341]\n",
      "loss: 0.639230  [75200/175341]\n",
      "loss: 0.147053  [76800/175341]\n",
      "loss: 0.737903  [78400/175341]\n",
      "loss: 0.346970  [80000/175341]\n",
      "loss: 0.328882  [81600/175341]\n",
      "loss: 0.237555  [83200/175341]\n",
      "loss: 0.684236  [84800/175341]\n",
      "loss: 0.904388  [86400/175341]\n",
      "loss: 0.172621  [88000/175341]\n",
      "loss: 0.365692  [89600/175341]\n",
      "loss: 0.343175  [91200/175341]\n",
      "loss: 0.511879  [92800/175341]\n",
      "loss: 0.328034  [94400/175341]\n",
      "loss: 0.406592  [96000/175341]\n",
      "loss: 0.463044  [97600/175341]\n",
      "loss: 0.283673  [99200/175341]\n",
      "loss: 0.663125  [100800/175341]\n",
      "loss: 0.414669  [102400/175341]\n",
      "loss: 0.404728  [104000/175341]\n",
      "loss: 0.340574  [105600/175341]\n",
      "loss: 0.177045  [107200/175341]\n",
      "loss: 0.244447  [108800/175341]\n",
      "loss: 0.444766  [110400/175341]\n",
      "loss: 0.261637  [112000/175341]\n",
      "loss: 0.514016  [113600/175341]\n",
      "loss: 0.808027  [115200/175341]\n",
      "loss: 0.386319  [116800/175341]\n",
      "loss: 0.839844  [118400/175341]\n",
      "loss: 0.486752  [120000/175341]\n",
      "loss: 0.355490  [121600/175341]\n",
      "loss: 0.540516  [123200/175341]\n",
      "loss: 0.144412  [124800/175341]\n",
      "loss: 0.503471  [126400/175341]\n",
      "loss: 0.940847  [128000/175341]\n",
      "loss: 0.412116  [129600/175341]\n",
      "loss: 0.131174  [131200/175341]\n",
      "loss: 0.253084  [132800/175341]\n",
      "loss: 0.687940  [134400/175341]\n",
      "loss: 0.742559  [136000/175341]\n",
      "loss: 0.343936  [137600/175341]\n",
      "loss: 0.166145  [139200/175341]\n",
      "loss: 0.909166  [140800/175341]\n",
      "loss: 0.194351  [142400/175341]\n",
      "loss: 0.303713  [144000/175341]\n",
      "loss: 0.557023  [145600/175341]\n",
      "loss: 0.207483  [147200/175341]\n",
      "loss: 0.435447  [148800/175341]\n",
      "loss: 0.500640  [150400/175341]\n",
      "loss: 0.308085  [152000/175341]\n",
      "loss: 0.508651  [153600/175341]\n",
      "loss: 0.470095  [155200/175341]\n",
      "loss: 0.336096  [156800/175341]\n",
      "loss: 0.339017  [158400/175341]\n",
      "loss: 0.112240  [160000/175341]\n",
      "loss: 0.304693  [161600/175341]\n",
      "loss: 0.614463  [163200/175341]\n",
      "loss: 0.812308  [164800/175341]\n",
      "loss: 0.375943  [166400/175341]\n",
      "loss: 0.529214  [168000/175341]\n",
      "loss: 0.355571  [169600/175341]\n",
      "loss: 0.236888  [171200/175341]\n",
      "loss: 0.466236  [172800/175341]\n",
      "loss: 0.746899  [174400/175341]\n",
      "Train Accuracy: 81.0695%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.563580, F1-score: 75.10%, Macro_F1-Score:  40.03%  \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.313228  [    0/175341]\n",
      "loss: 0.551771  [ 1600/175341]\n",
      "loss: 0.442717  [ 3200/175341]\n",
      "loss: 0.225455  [ 4800/175341]\n",
      "loss: 0.294071  [ 6400/175341]\n",
      "loss: 0.603204  [ 8000/175341]\n",
      "loss: 0.117111  [ 9600/175341]\n",
      "loss: 0.631470  [11200/175341]\n",
      "loss: 0.518625  [12800/175341]\n",
      "loss: 0.337380  [14400/175341]\n",
      "loss: 0.502755  [16000/175341]\n",
      "loss: 0.318998  [17600/175341]\n",
      "loss: 0.504647  [19200/175341]\n",
      "loss: 0.412658  [20800/175341]\n",
      "loss: 0.234675  [22400/175341]\n",
      "loss: 0.326468  [24000/175341]\n",
      "loss: 0.945486  [25600/175341]\n",
      "loss: 0.384354  [27200/175341]\n",
      "loss: 0.340358  [28800/175341]\n",
      "loss: 0.391760  [30400/175341]\n",
      "loss: 0.424279  [32000/175341]\n",
      "loss: 0.479659  [33600/175341]\n",
      "loss: 0.772417  [35200/175341]\n",
      "loss: 0.467878  [36800/175341]\n",
      "loss: 0.272419  [38400/175341]\n",
      "loss: 0.357811  [40000/175341]\n",
      "loss: 0.582400  [41600/175341]\n",
      "loss: 0.171048  [43200/175341]\n",
      "loss: 0.241023  [44800/175341]\n",
      "loss: 0.436355  [46400/175341]\n",
      "loss: 0.842167  [48000/175341]\n",
      "loss: 0.278086  [49600/175341]\n",
      "loss: 0.579611  [51200/175341]\n",
      "loss: 0.611965  [52800/175341]\n",
      "loss: 0.367218  [54400/175341]\n",
      "loss: 0.219633  [56000/175341]\n",
      "loss: 0.703215  [57600/175341]\n",
      "loss: 0.382396  [59200/175341]\n",
      "loss: 0.332910  [60800/175341]\n",
      "loss: 0.438931  [62400/175341]\n",
      "loss: 0.425062  [64000/175341]\n",
      "loss: 0.557314  [65600/175341]\n",
      "loss: 0.414621  [67200/175341]\n",
      "loss: 0.203336  [68800/175341]\n",
      "loss: 0.709566  [70400/175341]\n",
      "loss: 0.497841  [72000/175341]\n",
      "loss: 0.369308  [73600/175341]\n",
      "loss: 0.670726  [75200/175341]\n",
      "loss: 0.466179  [76800/175341]\n",
      "loss: 0.322993  [78400/175341]\n",
      "loss: 0.427284  [80000/175341]\n",
      "loss: 0.113155  [81600/175341]\n",
      "loss: 0.454794  [83200/175341]\n",
      "loss: 0.553510  [84800/175341]\n",
      "loss: 0.728173  [86400/175341]\n",
      "loss: 0.679380  [88000/175341]\n",
      "loss: 0.543303  [89600/175341]\n",
      "loss: 0.415073  [91200/175341]\n",
      "loss: 0.489199  [92800/175341]\n",
      "loss: 0.503303  [94400/175341]\n",
      "loss: 0.470344  [96000/175341]\n",
      "loss: 0.444071  [97600/175341]\n",
      "loss: 0.800781  [99200/175341]\n",
      "loss: 0.379074  [100800/175341]\n",
      "loss: 0.351554  [102400/175341]\n",
      "loss: 0.365442  [104000/175341]\n",
      "loss: 0.559151  [105600/175341]\n",
      "loss: 0.731487  [107200/175341]\n",
      "loss: 0.265430  [108800/175341]\n",
      "loss: 0.464397  [110400/175341]\n",
      "loss: 0.781612  [112000/175341]\n",
      "loss: 0.257299  [113600/175341]\n",
      "loss: 0.628138  [115200/175341]\n",
      "loss: 0.681763  [116800/175341]\n",
      "loss: 0.423640  [118400/175341]\n",
      "loss: 0.190007  [120000/175341]\n",
      "loss: 0.582592  [121600/175341]\n",
      "loss: 0.368254  [123200/175341]\n",
      "loss: 0.606595  [124800/175341]\n",
      "loss: 0.444132  [126400/175341]\n",
      "loss: 0.695032  [128000/175341]\n",
      "loss: 0.570461  [129600/175341]\n",
      "loss: 0.387317  [131200/175341]\n",
      "loss: 0.288885  [132800/175341]\n",
      "loss: 0.433428  [134400/175341]\n",
      "loss: 0.698835  [136000/175341]\n",
      "loss: 0.621915  [137600/175341]\n",
      "loss: 0.723722  [139200/175341]\n",
      "loss: 0.384473  [140800/175341]\n",
      "loss: 0.546902  [142400/175341]\n",
      "loss: 0.180866  [144000/175341]\n",
      "loss: 0.552013  [145600/175341]\n",
      "loss: 0.310426  [147200/175341]\n",
      "loss: 0.382619  [148800/175341]\n",
      "loss: 0.484777  [150400/175341]\n",
      "loss: 0.144287  [152000/175341]\n",
      "loss: 0.543707  [153600/175341]\n",
      "loss: 0.476823  [155200/175341]\n",
      "loss: 0.316647  [156800/175341]\n",
      "loss: 0.469652  [158400/175341]\n",
      "loss: 0.529229  [160000/175341]\n",
      "loss: 0.419825  [161600/175341]\n",
      "loss: 0.194206  [163200/175341]\n",
      "loss: 0.457299  [164800/175341]\n",
      "loss: 0.354320  [166400/175341]\n",
      "loss: 0.423831  [168000/175341]\n",
      "loss: 0.405854  [169600/175341]\n",
      "loss: 0.547099  [171200/175341]\n",
      "loss: 0.193821  [172800/175341]\n",
      "loss: 0.574574  [174400/175341]\n",
      "Train Accuracy: 81.1407%\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.543166, F1-score: 77.25%, Macro_F1-Score:  40.89%  \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.693966  [    0/175341]\n",
      "loss: 0.199926  [ 1600/175341]\n",
      "loss: 0.721371  [ 3200/175341]\n",
      "loss: 0.357969  [ 4800/175341]\n",
      "loss: 0.112494  [ 6400/175341]\n",
      "loss: 0.578741  [ 8000/175341]\n",
      "loss: 0.644721  [ 9600/175341]\n",
      "loss: 0.608159  [11200/175341]\n",
      "loss: 0.237426  [12800/175341]\n",
      "loss: 0.209297  [14400/175341]\n",
      "loss: 0.721335  [16000/175341]\n",
      "loss: 0.472334  [17600/175341]\n",
      "loss: 0.462598  [19200/175341]\n",
      "loss: 0.740846  [20800/175341]\n",
      "loss: 0.608891  [22400/175341]\n",
      "loss: 0.222674  [24000/175341]\n",
      "loss: 0.357586  [25600/175341]\n",
      "loss: 0.273572  [27200/175341]\n",
      "loss: 0.926808  [28800/175341]\n",
      "loss: 0.560066  [30400/175341]\n",
      "loss: 0.515826  [32000/175341]\n",
      "loss: 0.504497  [33600/175341]\n",
      "loss: 0.566256  [35200/175341]\n",
      "loss: 0.740885  [36800/175341]\n",
      "loss: 0.253722  [38400/175341]\n",
      "loss: 0.308327  [40000/175341]\n",
      "loss: 0.199145  [41600/175341]\n",
      "loss: 0.824525  [43200/175341]\n",
      "loss: 0.137018  [44800/175341]\n",
      "loss: 0.181575  [46400/175341]\n",
      "loss: 0.507568  [48000/175341]\n",
      "loss: 0.360825  [49600/175341]\n",
      "loss: 0.445170  [51200/175341]\n",
      "loss: 0.623029  [52800/175341]\n",
      "loss: 0.669624  [54400/175341]\n",
      "loss: 0.828722  [56000/175341]\n",
      "loss: 0.432674  [57600/175341]\n",
      "loss: 0.223433  [59200/175341]\n",
      "loss: 0.479830  [60800/175341]\n",
      "loss: 0.380010  [62400/175341]\n",
      "loss: 0.656350  [64000/175341]\n",
      "loss: 0.431430  [65600/175341]\n",
      "loss: 0.231275  [67200/175341]\n",
      "loss: 0.345203  [68800/175341]\n",
      "loss: 0.164533  [70400/175341]\n",
      "loss: 0.366744  [72000/175341]\n",
      "loss: 0.281348  [73600/175341]\n",
      "loss: 0.364849  [75200/175341]\n",
      "loss: 0.232735  [76800/175341]\n",
      "loss: 0.874314  [78400/175341]\n",
      "loss: 0.907699  [80000/175341]\n",
      "loss: 0.380123  [81600/175341]\n",
      "loss: 0.223638  [83200/175341]\n",
      "loss: 0.914531  [84800/175341]\n",
      "loss: 0.398334  [86400/175341]\n",
      "loss: 0.136229  [88000/175341]\n",
      "loss: 0.326669  [89600/175341]\n",
      "loss: 0.795451  [91200/175341]\n",
      "loss: 0.540421  [92800/175341]\n",
      "loss: 0.287648  [94400/175341]\n",
      "loss: 0.532835  [96000/175341]\n",
      "loss: 0.307930  [97600/175341]\n",
      "loss: 0.561033  [99200/175341]\n",
      "loss: 0.400814  [100800/175341]\n",
      "loss: 0.512049  [102400/175341]\n",
      "loss: 0.645795  [104000/175341]\n",
      "loss: 0.213628  [105600/175341]\n",
      "loss: 0.354256  [107200/175341]\n",
      "loss: 0.374498  [108800/175341]\n",
      "loss: 0.593624  [110400/175341]\n",
      "loss: 0.526715  [112000/175341]\n",
      "loss: 0.604704  [113600/175341]\n",
      "loss: 0.103843  [115200/175341]\n",
      "loss: 0.306532  [116800/175341]\n",
      "loss: 0.265466  [118400/175341]\n",
      "loss: 0.362084  [120000/175341]\n",
      "loss: 0.530047  [121600/175341]\n",
      "loss: 0.383400  [123200/175341]\n",
      "loss: 0.364044  [124800/175341]\n",
      "loss: 0.335227  [126400/175341]\n",
      "loss: 0.608476  [128000/175341]\n",
      "loss: 0.373329  [129600/175341]\n",
      "loss: 0.990287  [131200/175341]\n",
      "loss: 0.974522  [132800/175341]\n",
      "loss: 0.383819  [134400/175341]\n",
      "loss: 0.486094  [136000/175341]\n",
      "loss: 0.219203  [137600/175341]\n",
      "loss: 0.670887  [139200/175341]\n",
      "loss: 0.651666  [140800/175341]\n",
      "loss: 0.596698  [142400/175341]\n",
      "loss: 0.914114  [144000/175341]\n",
      "loss: 0.138310  [145600/175341]\n",
      "loss: 0.581790  [147200/175341]\n",
      "loss: 0.490949  [148800/175341]\n",
      "loss: 0.829670  [150400/175341]\n",
      "loss: 0.340382  [152000/175341]\n",
      "loss: 0.741387  [153600/175341]\n",
      "loss: 0.700604  [155200/175341]\n",
      "loss: 0.249193  [156800/175341]\n",
      "loss: 0.364812  [158400/175341]\n",
      "loss: 0.491354  [160000/175341]\n",
      "loss: 0.281886  [161600/175341]\n",
      "loss: 0.698927  [163200/175341]\n",
      "loss: 0.386624  [164800/175341]\n",
      "loss: 0.349642  [166400/175341]\n",
      "loss: 0.100290  [168000/175341]\n",
      "loss: 0.223772  [169600/175341]\n",
      "loss: 0.555834  [171200/175341]\n",
      "loss: 0.681278  [172800/175341]\n",
      "loss: 0.798053  [174400/175341]\n",
      "Train Accuracy: 81.1311%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.550818, F1-score: 76.67%, Macro_F1-Score:  41.05%  \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.700059  [    0/175341]\n",
      "loss: 0.281779  [ 1600/175341]\n",
      "loss: 0.499669  [ 3200/175341]\n",
      "loss: 0.415466  [ 4800/175341]\n",
      "loss: 0.383563  [ 6400/175341]\n",
      "loss: 0.644655  [ 8000/175341]\n",
      "loss: 0.444951  [ 9600/175341]\n",
      "loss: 0.846960  [11200/175341]\n",
      "loss: 0.453554  [12800/175341]\n",
      "loss: 0.455171  [14400/175341]\n",
      "loss: 0.317927  [16000/175341]\n",
      "loss: 0.342889  [17600/175341]\n",
      "loss: 0.221622  [19200/175341]\n",
      "loss: 0.265349  [20800/175341]\n",
      "loss: 0.470841  [22400/175341]\n",
      "loss: 0.548724  [24000/175341]\n",
      "loss: 0.649607  [25600/175341]\n",
      "loss: 0.830339  [27200/175341]\n",
      "loss: 0.877664  [28800/175341]\n",
      "loss: 0.402018  [30400/175341]\n",
      "loss: 0.560371  [32000/175341]\n",
      "loss: 0.142718  [33600/175341]\n",
      "loss: 0.451412  [35200/175341]\n",
      "loss: 0.303533  [36800/175341]\n",
      "loss: 0.786331  [38400/175341]\n",
      "loss: 0.551377  [40000/175341]\n",
      "loss: 0.417140  [41600/175341]\n",
      "loss: 0.777146  [43200/175341]\n",
      "loss: 0.738853  [44800/175341]\n",
      "loss: 0.362709  [46400/175341]\n",
      "loss: 0.383210  [48000/175341]\n",
      "loss: 0.128602  [49600/175341]\n",
      "loss: 0.606079  [51200/175341]\n",
      "loss: 0.548930  [52800/175341]\n",
      "loss: 0.378008  [54400/175341]\n",
      "loss: 0.408803  [56000/175341]\n",
      "loss: 0.427676  [57600/175341]\n",
      "loss: 0.184582  [59200/175341]\n",
      "loss: 0.743798  [60800/175341]\n",
      "loss: 1.017322  [62400/175341]\n",
      "loss: 0.397970  [64000/175341]\n",
      "loss: 0.493355  [65600/175341]\n",
      "loss: 0.360952  [67200/175341]\n",
      "loss: 0.328045  [68800/175341]\n",
      "loss: 0.515210  [70400/175341]\n",
      "loss: 1.122238  [72000/175341]\n",
      "loss: 1.098376  [73600/175341]\n",
      "loss: 0.684057  [75200/175341]\n",
      "loss: 0.396800  [76800/175341]\n",
      "loss: 0.433296  [78400/175341]\n",
      "loss: 0.284032  [80000/175341]\n",
      "loss: 0.359835  [81600/175341]\n",
      "loss: 0.363727  [83200/175341]\n",
      "loss: 0.359613  [84800/175341]\n",
      "loss: 0.421090  [86400/175341]\n",
      "loss: 0.481041  [88000/175341]\n",
      "loss: 0.758438  [89600/175341]\n",
      "loss: 0.349999  [91200/175341]\n",
      "loss: 0.245809  [92800/175341]\n",
      "loss: 0.477298  [94400/175341]\n",
      "loss: 0.538133  [96000/175341]\n",
      "loss: 0.571520  [97600/175341]\n",
      "loss: 0.782287  [99200/175341]\n",
      "loss: 0.466933  [100800/175341]\n",
      "loss: 0.202481  [102400/175341]\n",
      "loss: 0.504086  [104000/175341]\n",
      "loss: 0.363452  [105600/175341]\n",
      "loss: 0.232435  [107200/175341]\n",
      "loss: 0.415707  [108800/175341]\n",
      "loss: 0.353268  [110400/175341]\n",
      "loss: 0.477887  [112000/175341]\n",
      "loss: 0.336845  [113600/175341]\n",
      "loss: 0.492009  [115200/175341]\n",
      "loss: 0.431626  [116800/175341]\n",
      "loss: 0.722201  [118400/175341]\n",
      "loss: 0.165497  [120000/175341]\n",
      "loss: 0.524307  [121600/175341]\n",
      "loss: 0.438524  [123200/175341]\n",
      "loss: 0.803301  [124800/175341]\n",
      "loss: 0.309803  [126400/175341]\n",
      "loss: 0.328505  [128000/175341]\n",
      "loss: 0.066208  [129600/175341]\n",
      "loss: 0.601490  [131200/175341]\n",
      "loss: 0.783794  [132800/175341]\n",
      "loss: 0.609211  [134400/175341]\n",
      "loss: 0.475121  [136000/175341]\n",
      "loss: 0.486165  [137600/175341]\n",
      "loss: 0.389217  [139200/175341]\n",
      "loss: 0.521067  [140800/175341]\n",
      "loss: 0.214542  [142400/175341]\n",
      "loss: 0.267451  [144000/175341]\n",
      "loss: 0.629587  [145600/175341]\n",
      "loss: 0.668828  [147200/175341]\n",
      "loss: 0.473834  [148800/175341]\n",
      "loss: 0.437358  [150400/175341]\n",
      "loss: 0.741287  [152000/175341]\n",
      "loss: 0.447199  [153600/175341]\n",
      "loss: 0.687413  [155200/175341]\n",
      "loss: 0.620895  [156800/175341]\n",
      "loss: 0.541665  [158400/175341]\n",
      "loss: 0.586707  [160000/175341]\n",
      "loss: 0.192389  [161600/175341]\n",
      "loss: 0.401085  [163200/175341]\n",
      "loss: 0.152278  [164800/175341]\n",
      "loss: 0.345021  [166400/175341]\n",
      "loss: 0.282892  [168000/175341]\n",
      "loss: 0.324100  [169600/175341]\n",
      "loss: 0.560439  [171200/175341]\n",
      "loss: 0.227411  [172800/175341]\n",
      "loss: 0.398345  [174400/175341]\n",
      "Train Accuracy: 81.1060%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.551338, F1-score: 76.32%, Macro_F1-Score:  40.44%  \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.379214  [    0/175341]\n",
      "loss: 0.823059  [ 1600/175341]\n",
      "loss: 0.578200  [ 3200/175341]\n",
      "loss: 0.417857  [ 4800/175341]\n",
      "loss: 0.232791  [ 6400/175341]\n",
      "loss: 0.362482  [ 8000/175341]\n",
      "loss: 0.516529  [ 9600/175341]\n",
      "loss: 0.221238  [11200/175341]\n",
      "loss: 0.354105  [12800/175341]\n",
      "loss: 0.241123  [14400/175341]\n",
      "loss: 0.429190  [16000/175341]\n",
      "loss: 0.613489  [17600/175341]\n",
      "loss: 0.141637  [19200/175341]\n",
      "loss: 0.506350  [20800/175341]\n",
      "loss: 0.576487  [22400/175341]\n",
      "loss: 0.592049  [24000/175341]\n",
      "loss: 0.560308  [25600/175341]\n",
      "loss: 0.699575  [27200/175341]\n",
      "loss: 0.165461  [28800/175341]\n",
      "loss: 0.325452  [30400/175341]\n",
      "loss: 0.332162  [32000/175341]\n",
      "loss: 0.475284  [33600/175341]\n",
      "loss: 0.495087  [35200/175341]\n",
      "loss: 0.513663  [36800/175341]\n",
      "loss: 0.327228  [38400/175341]\n",
      "loss: 0.157632  [40000/175341]\n",
      "loss: 0.612659  [41600/175341]\n",
      "loss: 0.697870  [43200/175341]\n",
      "loss: 0.330249  [44800/175341]\n",
      "loss: 0.602750  [46400/175341]\n",
      "loss: 0.205929  [48000/175341]\n",
      "loss: 0.611233  [49600/175341]\n",
      "loss: 0.526532  [51200/175341]\n",
      "loss: 0.553614  [52800/175341]\n",
      "loss: 1.063972  [54400/175341]\n",
      "loss: 0.406965  [56000/175341]\n",
      "loss: 0.186097  [57600/175341]\n",
      "loss: 0.390134  [59200/175341]\n",
      "loss: 0.885884  [60800/175341]\n",
      "loss: 0.466461  [62400/175341]\n",
      "loss: 0.631906  [64000/175341]\n",
      "loss: 0.436908  [65600/175341]\n",
      "loss: 0.378167  [67200/175341]\n",
      "loss: 0.247972  [68800/175341]\n",
      "loss: 0.639954  [70400/175341]\n",
      "loss: 0.818775  [72000/175341]\n",
      "loss: 0.385392  [73600/175341]\n",
      "loss: 0.342715  [75200/175341]\n",
      "loss: 0.341016  [76800/175341]\n",
      "loss: 0.183505  [78400/175341]\n",
      "loss: 0.673206  [80000/175341]\n",
      "loss: 0.152264  [81600/175341]\n",
      "loss: 0.492630  [83200/175341]\n",
      "loss: 0.349142  [84800/175341]\n",
      "loss: 0.305273  [86400/175341]\n",
      "loss: 0.722547  [88000/175341]\n",
      "loss: 1.004184  [89600/175341]\n",
      "loss: 0.327717  [91200/175341]\n",
      "loss: 0.396893  [92800/175341]\n",
      "loss: 0.531277  [94400/175341]\n",
      "loss: 0.422801  [96000/175341]\n",
      "loss: 0.873859  [97600/175341]\n",
      "loss: 0.352608  [99200/175341]\n",
      "loss: 0.767536  [100800/175341]\n",
      "loss: 0.572562  [102400/175341]\n",
      "loss: 0.544632  [104000/175341]\n",
      "loss: 0.381841  [105600/175341]\n",
      "loss: 0.528292  [107200/175341]\n",
      "loss: 0.216923  [108800/175341]\n",
      "loss: 0.341979  [110400/175341]\n",
      "loss: 0.402565  [112000/175341]\n",
      "loss: 0.964806  [113600/175341]\n",
      "loss: 0.455177  [115200/175341]\n",
      "loss: 0.316092  [116800/175341]\n",
      "loss: 0.572668  [118400/175341]\n",
      "loss: 0.490111  [120000/175341]\n",
      "loss: 0.356590  [121600/175341]\n",
      "loss: 0.416370  [123200/175341]\n",
      "loss: 0.860541  [124800/175341]\n",
      "loss: 0.731227  [126400/175341]\n",
      "loss: 0.497515  [128000/175341]\n",
      "loss: 0.468358  [129600/175341]\n",
      "loss: 0.218623  [131200/175341]\n",
      "loss: 0.201853  [132800/175341]\n",
      "loss: 0.411996  [134400/175341]\n",
      "loss: 0.926759  [136000/175341]\n",
      "loss: 0.665855  [137600/175341]\n",
      "loss: 0.358669  [139200/175341]\n",
      "loss: 0.430639  [140800/175341]\n",
      "loss: 0.744683  [142400/175341]\n",
      "loss: 0.543175  [144000/175341]\n",
      "loss: 0.147152  [145600/175341]\n",
      "loss: 0.528707  [147200/175341]\n",
      "loss: 0.304330  [148800/175341]\n",
      "loss: 0.294272  [150400/175341]\n",
      "loss: 0.398404  [152000/175341]\n",
      "loss: 0.065080  [153600/175341]\n",
      "loss: 0.567105  [155200/175341]\n",
      "loss: 0.231049  [156800/175341]\n",
      "loss: 0.820628  [158400/175341]\n",
      "loss: 0.871432  [160000/175341]\n",
      "loss: 0.673730  [161600/175341]\n",
      "loss: 0.296838  [163200/175341]\n",
      "loss: 0.562943  [164800/175341]\n",
      "loss: 0.237939  [166400/175341]\n",
      "loss: 0.297466  [168000/175341]\n",
      "loss: 0.552176  [169600/175341]\n",
      "loss: 0.362865  [171200/175341]\n",
      "loss: 0.523432  [172800/175341]\n",
      "loss: 0.688480  [174400/175341]\n",
      "Train Accuracy: 81.1202%\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.543935, F1-score: 77.24%, Macro_F1-Score:  41.03%  \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.693896  [    0/175341]\n",
      "loss: 0.681852  [ 1600/175341]\n",
      "loss: 0.575830  [ 3200/175341]\n",
      "loss: 0.655843  [ 4800/175341]\n",
      "loss: 0.707375  [ 6400/175341]\n",
      "loss: 0.444162  [ 8000/175341]\n",
      "loss: 0.770074  [ 9600/175341]\n",
      "loss: 0.534316  [11200/175341]\n",
      "loss: 0.162218  [12800/175341]\n",
      "loss: 0.352581  [14400/175341]\n",
      "loss: 0.324937  [16000/175341]\n",
      "loss: 0.467108  [17600/175341]\n",
      "loss: 0.307476  [19200/175341]\n",
      "loss: 0.509842  [20800/175341]\n",
      "loss: 0.153459  [22400/175341]\n",
      "loss: 0.525682  [24000/175341]\n",
      "loss: 0.197319  [25600/175341]\n",
      "loss: 0.556138  [27200/175341]\n",
      "loss: 0.548032  [28800/175341]\n",
      "loss: 0.747230  [30400/175341]\n",
      "loss: 0.627396  [32000/175341]\n",
      "loss: 0.399529  [33600/175341]\n",
      "loss: 0.457208  [35200/175341]\n",
      "loss: 0.461906  [36800/175341]\n",
      "loss: 0.746328  [38400/175341]\n",
      "loss: 0.264205  [40000/175341]\n",
      "loss: 0.407083  [41600/175341]\n",
      "loss: 0.764738  [43200/175341]\n",
      "loss: 0.457517  [44800/175341]\n",
      "loss: 0.617674  [46400/175341]\n",
      "loss: 0.749761  [48000/175341]\n",
      "loss: 0.564688  [49600/175341]\n",
      "loss: 0.632808  [51200/175341]\n",
      "loss: 0.477948  [52800/175341]\n",
      "loss: 0.600176  [54400/175341]\n",
      "loss: 0.602845  [56000/175341]\n",
      "loss: 0.393618  [57600/175341]\n",
      "loss: 0.472712  [59200/175341]\n",
      "loss: 0.418420  [60800/175341]\n",
      "loss: 0.241550  [62400/175341]\n",
      "loss: 0.277843  [64000/175341]\n",
      "loss: 0.906841  [65600/175341]\n",
      "loss: 0.542597  [67200/175341]\n",
      "loss: 0.268982  [68800/175341]\n",
      "loss: 0.648962  [70400/175341]\n",
      "loss: 0.463508  [72000/175341]\n",
      "loss: 0.774341  [73600/175341]\n",
      "loss: 0.223621  [75200/175341]\n",
      "loss: 0.423973  [76800/175341]\n",
      "loss: 0.354115  [78400/175341]\n",
      "loss: 0.549505  [80000/175341]\n",
      "loss: 0.474607  [81600/175341]\n",
      "loss: 0.117877  [83200/175341]\n",
      "loss: 0.708719  [84800/175341]\n",
      "loss: 0.355291  [86400/175341]\n",
      "loss: 0.660609  [88000/175341]\n",
      "loss: 0.238773  [89600/175341]\n",
      "loss: 0.187846  [91200/175341]\n",
      "loss: 0.639979  [92800/175341]\n",
      "loss: 0.635668  [94400/175341]\n",
      "loss: 0.497825  [96000/175341]\n",
      "loss: 0.272465  [97600/175341]\n",
      "loss: 0.799769  [99200/175341]\n",
      "loss: 0.336236  [100800/175341]\n",
      "loss: 0.539826  [102400/175341]\n",
      "loss: 0.557241  [104000/175341]\n",
      "loss: 0.755124  [105600/175341]\n",
      "loss: 0.681091  [107200/175341]\n",
      "loss: 0.370594  [108800/175341]\n",
      "loss: 0.297108  [110400/175341]\n",
      "loss: 0.615669  [112000/175341]\n",
      "loss: 0.261840  [113600/175341]\n",
      "loss: 0.628242  [115200/175341]\n",
      "loss: 0.176073  [116800/175341]\n",
      "loss: 0.395998  [118400/175341]\n",
      "loss: 0.493656  [120000/175341]\n",
      "loss: 0.552736  [121600/175341]\n",
      "loss: 0.714872  [123200/175341]\n",
      "loss: 0.330542  [124800/175341]\n",
      "loss: 0.298174  [126400/175341]\n",
      "loss: 0.686740  [128000/175341]\n",
      "loss: 0.153398  [129600/175341]\n",
      "loss: 1.171138  [131200/175341]\n",
      "loss: 0.551161  [132800/175341]\n",
      "loss: 0.491623  [134400/175341]\n",
      "loss: 0.396535  [136000/175341]\n",
      "loss: 0.467523  [137600/175341]\n",
      "loss: 0.412986  [139200/175341]\n",
      "loss: 0.487501  [140800/175341]\n",
      "loss: 0.198721  [142400/175341]\n",
      "loss: 0.531521  [144000/175341]\n",
      "loss: 0.303149  [145600/175341]\n",
      "loss: 0.483986  [147200/175341]\n",
      "loss: 0.260265  [148800/175341]\n",
      "loss: 0.726617  [150400/175341]\n",
      "loss: 0.479855  [152000/175341]\n",
      "loss: 0.261078  [153600/175341]\n",
      "loss: 0.260173  [155200/175341]\n",
      "loss: 0.597986  [156800/175341]\n",
      "loss: 0.674719  [158400/175341]\n",
      "loss: 0.339005  [160000/175341]\n",
      "loss: 1.196401  [161600/175341]\n",
      "loss: 0.301042  [163200/175341]\n",
      "loss: 0.490271  [164800/175341]\n",
      "loss: 0.681822  [166400/175341]\n",
      "loss: 0.612046  [168000/175341]\n",
      "loss: 0.566705  [169600/175341]\n",
      "loss: 0.236400  [171200/175341]\n",
      "loss: 0.212126  [172800/175341]\n",
      "loss: 0.516503  [174400/175341]\n",
      "Train Accuracy: 81.0974%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.547271, F1-score: 76.44%, Macro_F1-Score:  40.51%  \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.447911  [    0/175341]\n",
      "loss: 0.549722  [ 1600/175341]\n",
      "loss: 0.403072  [ 3200/175341]\n",
      "loss: 0.499986  [ 4800/175341]\n",
      "loss: 0.384629  [ 6400/175341]\n",
      "loss: 0.319997  [ 8000/175341]\n",
      "loss: 0.763187  [ 9600/175341]\n",
      "loss: 0.482642  [11200/175341]\n",
      "loss: 0.335584  [12800/175341]\n",
      "loss: 0.413724  [14400/175341]\n",
      "loss: 0.152564  [16000/175341]\n",
      "loss: 0.112065  [17600/175341]\n",
      "loss: 0.327398  [19200/175341]\n",
      "loss: 0.477245  [20800/175341]\n",
      "loss: 0.525104  [22400/175341]\n",
      "loss: 0.323025  [24000/175341]\n",
      "loss: 0.577983  [25600/175341]\n",
      "loss: 0.192320  [27200/175341]\n",
      "loss: 0.229206  [28800/175341]\n",
      "loss: 0.238040  [30400/175341]\n",
      "loss: 0.624806  [32000/175341]\n",
      "loss: 0.436611  [33600/175341]\n",
      "loss: 0.226893  [35200/175341]\n",
      "loss: 0.274741  [36800/175341]\n",
      "loss: 0.639636  [38400/175341]\n",
      "loss: 0.077441  [40000/175341]\n",
      "loss: 0.299651  [41600/175341]\n",
      "loss: 0.529505  [43200/175341]\n",
      "loss: 0.402662  [44800/175341]\n",
      "loss: 0.324793  [46400/175341]\n",
      "loss: 0.572256  [48000/175341]\n",
      "loss: 0.241185  [49600/175341]\n",
      "loss: 0.476900  [51200/175341]\n",
      "loss: 0.364923  [52800/175341]\n",
      "loss: 0.462839  [54400/175341]\n",
      "loss: 0.501113  [56000/175341]\n",
      "loss: 0.592049  [57600/175341]\n",
      "loss: 0.575387  [59200/175341]\n",
      "loss: 0.318007  [60800/175341]\n",
      "loss: 0.210714  [62400/175341]\n",
      "loss: 0.391938  [64000/175341]\n",
      "loss: 0.184437  [65600/175341]\n",
      "loss: 0.843900  [67200/175341]\n",
      "loss: 0.367759  [68800/175341]\n",
      "loss: 1.022489  [70400/175341]\n",
      "loss: 0.360327  [72000/175341]\n",
      "loss: 0.301078  [73600/175341]\n",
      "loss: 0.215732  [75200/175341]\n",
      "loss: 0.647737  [76800/175341]\n",
      "loss: 0.148646  [78400/175341]\n",
      "loss: 0.391838  [80000/175341]\n",
      "loss: 0.183753  [81600/175341]\n",
      "loss: 0.512034  [83200/175341]\n",
      "loss: 0.651705  [84800/175341]\n",
      "loss: 0.646037  [86400/175341]\n",
      "loss: 0.305597  [88000/175341]\n",
      "loss: 0.543148  [89600/175341]\n",
      "loss: 0.499006  [91200/175341]\n",
      "loss: 0.175317  [92800/175341]\n",
      "loss: 0.239460  [94400/175341]\n",
      "loss: 0.545815  [96000/175341]\n",
      "loss: 0.278670  [97600/175341]\n",
      "loss: 0.400768  [99200/175341]\n",
      "loss: 0.382170  [100800/175341]\n",
      "loss: 0.549443  [102400/175341]\n",
      "loss: 0.573818  [104000/175341]\n",
      "loss: 0.266346  [105600/175341]\n",
      "loss: 0.605185  [107200/175341]\n",
      "loss: 0.211331  [108800/175341]\n",
      "loss: 0.828006  [110400/175341]\n",
      "loss: 0.509582  [112000/175341]\n",
      "loss: 0.578651  [113600/175341]\n",
      "loss: 0.637250  [115200/175341]\n",
      "loss: 0.612576  [116800/175341]\n",
      "loss: 0.311260  [118400/175341]\n",
      "loss: 0.422967  [120000/175341]\n",
      "loss: 0.708848  [121600/175341]\n",
      "loss: 0.769330  [123200/175341]\n",
      "loss: 0.494886  [124800/175341]\n",
      "loss: 0.325297  [126400/175341]\n",
      "loss: 0.534906  [128000/175341]\n",
      "loss: 0.466513  [129600/175341]\n",
      "loss: 0.762274  [131200/175341]\n",
      "loss: 0.499917  [132800/175341]\n",
      "loss: 0.575112  [134400/175341]\n",
      "loss: 0.212476  [136000/175341]\n",
      "loss: 0.232961  [137600/175341]\n",
      "loss: 0.314218  [139200/175341]\n",
      "loss: 0.863216  [140800/175341]\n",
      "loss: 0.554704  [142400/175341]\n",
      "loss: 0.654017  [144000/175341]\n",
      "loss: 0.320807  [145600/175341]\n",
      "loss: 0.231927  [147200/175341]\n",
      "loss: 0.675713  [148800/175341]\n",
      "loss: 1.109604  [150400/175341]\n",
      "loss: 0.247968  [152000/175341]\n",
      "loss: 0.540248  [153600/175341]\n",
      "loss: 0.780596  [155200/175341]\n",
      "loss: 0.618007  [156800/175341]\n",
      "loss: 0.350887  [158400/175341]\n",
      "loss: 0.304404  [160000/175341]\n",
      "loss: 0.452228  [161600/175341]\n",
      "loss: 0.163638  [163200/175341]\n",
      "loss: 0.345649  [164800/175341]\n",
      "loss: 0.487604  [166400/175341]\n",
      "loss: 0.408756  [168000/175341]\n",
      "loss: 0.342562  [169600/175341]\n",
      "loss: 0.498577  [171200/175341]\n",
      "loss: 0.415760  [172800/175341]\n",
      "loss: 0.704349  [174400/175341]\n",
      "Train Accuracy: 81.1710%\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.555409, F1-score: 75.56%, Macro_F1-Score:  40.43%  \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.417329  [    0/175341]\n",
      "loss: 0.557644  [ 1600/175341]\n",
      "loss: 0.262366  [ 3200/175341]\n",
      "loss: 0.366450  [ 4800/175341]\n",
      "loss: 0.381915  [ 6400/175341]\n",
      "loss: 0.226391  [ 8000/175341]\n",
      "loss: 0.516537  [ 9600/175341]\n",
      "loss: 0.542832  [11200/175341]\n",
      "loss: 0.327686  [12800/175341]\n",
      "loss: 0.677993  [14400/175341]\n",
      "loss: 0.291337  [16000/175341]\n",
      "loss: 0.498402  [17600/175341]\n",
      "loss: 0.559684  [19200/175341]\n",
      "loss: 0.381028  [20800/175341]\n",
      "loss: 0.133743  [22400/175341]\n",
      "loss: 0.333618  [24000/175341]\n",
      "loss: 0.635702  [25600/175341]\n",
      "loss: 0.444970  [27200/175341]\n",
      "loss: 0.458961  [28800/175341]\n",
      "loss: 0.545926  [30400/175341]\n",
      "loss: 0.308562  [32000/175341]\n",
      "loss: 0.316164  [33600/175341]\n",
      "loss: 0.311429  [35200/175341]\n",
      "loss: 0.303096  [36800/175341]\n",
      "loss: 0.520078  [38400/175341]\n",
      "loss: 0.804560  [40000/175341]\n",
      "loss: 0.456861  [41600/175341]\n",
      "loss: 0.492084  [43200/175341]\n",
      "loss: 0.875672  [44800/175341]\n",
      "loss: 0.855480  [46400/175341]\n",
      "loss: 0.340330  [48000/175341]\n",
      "loss: 0.348788  [49600/175341]\n",
      "loss: 0.481854  [51200/175341]\n",
      "loss: 0.282989  [52800/175341]\n",
      "loss: 0.344430  [54400/175341]\n",
      "loss: 0.447144  [56000/175341]\n",
      "loss: 0.138299  [57600/175341]\n",
      "loss: 0.588462  [59200/175341]\n",
      "loss: 0.449818  [60800/175341]\n",
      "loss: 0.499874  [62400/175341]\n",
      "loss: 0.360131  [64000/175341]\n",
      "loss: 0.221815  [65600/175341]\n",
      "loss: 0.566026  [67200/175341]\n",
      "loss: 0.454387  [68800/175341]\n",
      "loss: 0.317389  [70400/175341]\n",
      "loss: 0.252586  [72000/175341]\n",
      "loss: 0.631732  [73600/175341]\n",
      "loss: 0.086887  [75200/175341]\n",
      "loss: 0.602366  [76800/175341]\n",
      "loss: 0.378727  [78400/175341]\n",
      "loss: 0.452675  [80000/175341]\n",
      "loss: 0.228595  [81600/175341]\n",
      "loss: 0.347053  [83200/175341]\n",
      "loss: 0.319849  [84800/175341]\n",
      "loss: 0.470715  [86400/175341]\n",
      "loss: 0.583457  [88000/175341]\n",
      "loss: 0.436276  [89600/175341]\n",
      "loss: 0.276957  [91200/175341]\n",
      "loss: 0.367743  [92800/175341]\n",
      "loss: 0.330602  [94400/175341]\n",
      "loss: 0.407116  [96000/175341]\n",
      "loss: 0.325805  [97600/175341]\n",
      "loss: 0.698668  [99200/175341]\n",
      "loss: 0.283356  [100800/175341]\n",
      "loss: 0.652099  [102400/175341]\n",
      "loss: 0.500515  [104000/175341]\n",
      "loss: 0.526211  [105600/175341]\n",
      "loss: 0.557384  [107200/175341]\n",
      "loss: 0.778190  [108800/175341]\n",
      "loss: 0.226634  [110400/175341]\n",
      "loss: 0.521907  [112000/175341]\n",
      "loss: 0.620720  [113600/175341]\n",
      "loss: 1.082200  [115200/175341]\n",
      "loss: 0.720172  [116800/175341]\n",
      "loss: 0.357061  [118400/175341]\n",
      "loss: 0.294918  [120000/175341]\n",
      "loss: 0.533063  [121600/175341]\n",
      "loss: 0.557561  [123200/175341]\n",
      "loss: 0.567709  [124800/175341]\n",
      "loss: 1.093354  [126400/175341]\n",
      "loss: 0.448379  [128000/175341]\n",
      "loss: 0.544556  [129600/175341]\n",
      "loss: 0.392706  [131200/175341]\n",
      "loss: 0.491251  [132800/175341]\n",
      "loss: 0.812512  [134400/175341]\n",
      "loss: 0.319996  [136000/175341]\n",
      "loss: 0.603791  [137600/175341]\n",
      "loss: 0.519712  [139200/175341]\n",
      "loss: 0.156881  [140800/175341]\n",
      "loss: 0.915662  [142400/175341]\n",
      "loss: 0.523489  [144000/175341]\n",
      "loss: 0.613825  [145600/175341]\n",
      "loss: 0.657478  [147200/175341]\n",
      "loss: 0.684459  [148800/175341]\n",
      "loss: 0.408213  [150400/175341]\n",
      "loss: 0.814980  [152000/175341]\n",
      "loss: 0.410348  [153600/175341]\n",
      "loss: 0.362370  [155200/175341]\n",
      "loss: 0.340595  [156800/175341]\n",
      "loss: 0.749692  [158400/175341]\n",
      "loss: 0.295608  [160000/175341]\n",
      "loss: 0.476385  [161600/175341]\n",
      "loss: 0.221247  [163200/175341]\n",
      "loss: 0.668790  [164800/175341]\n",
      "loss: 0.536179  [166400/175341]\n",
      "loss: 0.209140  [168000/175341]\n",
      "loss: 0.539925  [169600/175341]\n",
      "loss: 0.222296  [171200/175341]\n",
      "loss: 0.443323  [172800/175341]\n",
      "loss: 0.328704  [174400/175341]\n",
      "Train Accuracy: 81.1875%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.568789, F1-score: 75.60%, Macro_F1-Score:  40.26%  \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.488509  [    0/175341]\n",
      "loss: 0.271315  [ 1600/175341]\n",
      "loss: 0.770088  [ 3200/175341]\n",
      "loss: 0.379892  [ 4800/175341]\n",
      "loss: 0.510427  [ 6400/175341]\n",
      "loss: 0.328113  [ 8000/175341]\n",
      "loss: 0.267394  [ 9600/175341]\n",
      "loss: 0.481139  [11200/175341]\n",
      "loss: 0.176370  [12800/175341]\n",
      "loss: 1.107023  [14400/175341]\n",
      "loss: 1.013983  [16000/175341]\n",
      "loss: 0.596159  [17600/175341]\n",
      "loss: 0.465584  [19200/175341]\n",
      "loss: 0.391937  [20800/175341]\n",
      "loss: 0.520580  [22400/175341]\n",
      "loss: 0.382466  [24000/175341]\n",
      "loss: 0.879204  [25600/175341]\n",
      "loss: 0.501527  [27200/175341]\n",
      "loss: 0.403685  [28800/175341]\n",
      "loss: 0.336236  [30400/175341]\n",
      "loss: 0.388607  [32000/175341]\n",
      "loss: 0.133412  [33600/175341]\n",
      "loss: 0.235492  [35200/175341]\n",
      "loss: 0.500779  [36800/175341]\n",
      "loss: 0.296996  [38400/175341]\n",
      "loss: 0.635207  [40000/175341]\n",
      "loss: 0.769211  [41600/175341]\n",
      "loss: 0.275574  [43200/175341]\n",
      "loss: 0.665901  [44800/175341]\n",
      "loss: 0.465025  [46400/175341]\n",
      "loss: 0.470770  [48000/175341]\n",
      "loss: 0.834780  [49600/175341]\n",
      "loss: 0.464577  [51200/175341]\n",
      "loss: 0.742191  [52800/175341]\n",
      "loss: 0.383604  [54400/175341]\n",
      "loss: 0.420087  [56000/175341]\n",
      "loss: 0.537811  [57600/175341]\n",
      "loss: 0.523431  [59200/175341]\n",
      "loss: 0.449542  [60800/175341]\n",
      "loss: 0.642016  [62400/175341]\n",
      "loss: 0.808739  [64000/175341]\n",
      "loss: 0.447722  [65600/175341]\n",
      "loss: 0.427177  [67200/175341]\n",
      "loss: 0.442384  [68800/175341]\n",
      "loss: 0.350362  [70400/175341]\n",
      "loss: 0.490172  [72000/175341]\n",
      "loss: 0.607680  [73600/175341]\n",
      "loss: 0.043728  [75200/175341]\n",
      "loss: 0.190762  [76800/175341]\n",
      "loss: 0.459332  [78400/175341]\n",
      "loss: 0.468945  [80000/175341]\n",
      "loss: 0.633239  [81600/175341]\n",
      "loss: 0.687867  [83200/175341]\n",
      "loss: 0.284380  [84800/175341]\n",
      "loss: 0.225166  [86400/175341]\n",
      "loss: 0.530731  [88000/175341]\n",
      "loss: 0.430069  [89600/175341]\n",
      "loss: 0.383656  [91200/175341]\n",
      "loss: 0.573638  [92800/175341]\n",
      "loss: 0.532180  [94400/175341]\n",
      "loss: 0.451923  [96000/175341]\n",
      "loss: 0.412168  [97600/175341]\n",
      "loss: 0.357577  [99200/175341]\n",
      "loss: 0.392294  [100800/175341]\n",
      "loss: 0.457021  [102400/175341]\n",
      "loss: 0.255593  [104000/175341]\n",
      "loss: 0.412847  [105600/175341]\n",
      "loss: 1.360693  [107200/175341]\n",
      "loss: 0.104731  [108800/175341]\n",
      "loss: 0.632531  [110400/175341]\n",
      "loss: 0.545293  [112000/175341]\n",
      "loss: 0.310121  [113600/175341]\n",
      "loss: 0.553706  [115200/175341]\n",
      "loss: 0.881328  [116800/175341]\n",
      "loss: 0.544066  [118400/175341]\n",
      "loss: 0.328527  [120000/175341]\n",
      "loss: 0.420758  [121600/175341]\n",
      "loss: 0.464971  [123200/175341]\n",
      "loss: 0.508936  [124800/175341]\n",
      "loss: 0.901425  [126400/175341]\n",
      "loss: 0.196106  [128000/175341]\n",
      "loss: 0.811980  [129600/175341]\n",
      "loss: 0.096875  [131200/175341]\n",
      "loss: 0.466263  [132800/175341]\n",
      "loss: 0.570337  [134400/175341]\n",
      "loss: 0.396918  [136000/175341]\n",
      "loss: 0.741942  [137600/175341]\n",
      "loss: 0.766695  [139200/175341]\n",
      "loss: 0.620504  [140800/175341]\n",
      "loss: 0.751753  [142400/175341]\n",
      "loss: 0.631332  [144000/175341]\n",
      "loss: 0.371258  [145600/175341]\n",
      "loss: 0.344808  [147200/175341]\n",
      "loss: 0.457649  [148800/175341]\n",
      "loss: 0.545222  [150400/175341]\n",
      "loss: 0.386424  [152000/175341]\n",
      "loss: 0.561224  [153600/175341]\n",
      "loss: 0.441886  [155200/175341]\n",
      "loss: 0.641018  [156800/175341]\n",
      "loss: 0.440424  [158400/175341]\n",
      "loss: 0.390692  [160000/175341]\n",
      "loss: 0.187279  [161600/175341]\n",
      "loss: 0.595684  [163200/175341]\n",
      "loss: 0.874357  [164800/175341]\n",
      "loss: 0.914595  [166400/175341]\n",
      "loss: 0.502417  [168000/175341]\n",
      "loss: 0.502261  [169600/175341]\n",
      "loss: 0.952678  [171200/175341]\n",
      "loss: 1.156337  [172800/175341]\n",
      "loss: 0.338800  [174400/175341]\n",
      "Train Accuracy: 81.2098%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.560996, F1-score: 75.58%, Macro_F1-Score:  40.08%  \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.713098  [    0/175341]\n",
      "loss: 0.285572  [ 1600/175341]\n",
      "loss: 0.314972  [ 3200/175341]\n",
      "loss: 0.209381  [ 4800/175341]\n",
      "loss: 0.566239  [ 6400/175341]\n",
      "loss: 0.572850  [ 8000/175341]\n",
      "loss: 0.471440  [ 9600/175341]\n",
      "loss: 0.279461  [11200/175341]\n",
      "loss: 0.566920  [12800/175341]\n",
      "loss: 0.557103  [14400/175341]\n",
      "loss: 0.731404  [16000/175341]\n",
      "loss: 0.120641  [17600/175341]\n",
      "loss: 0.139253  [19200/175341]\n",
      "loss: 0.336125  [20800/175341]\n",
      "loss: 0.789505  [22400/175341]\n",
      "loss: 0.727198  [24000/175341]\n",
      "loss: 0.658478  [25600/175341]\n",
      "loss: 0.320114  [27200/175341]\n",
      "loss: 0.233855  [28800/175341]\n",
      "loss: 0.295530  [30400/175341]\n",
      "loss: 0.495565  [32000/175341]\n",
      "loss: 0.899797  [33600/175341]\n",
      "loss: 0.229768  [35200/175341]\n",
      "loss: 0.720233  [36800/175341]\n",
      "loss: 0.350730  [38400/175341]\n",
      "loss: 0.760718  [40000/175341]\n",
      "loss: 0.500629  [41600/175341]\n",
      "loss: 0.190241  [43200/175341]\n",
      "loss: 0.234067  [44800/175341]\n",
      "loss: 0.456839  [46400/175341]\n",
      "loss: 0.577066  [48000/175341]\n",
      "loss: 0.352945  [49600/175341]\n",
      "loss: 0.503368  [51200/175341]\n",
      "loss: 0.400565  [52800/175341]\n",
      "loss: 0.323994  [54400/175341]\n",
      "loss: 0.335366  [56000/175341]\n",
      "loss: 0.300535  [57600/175341]\n",
      "loss: 0.293709  [59200/175341]\n",
      "loss: 0.303858  [60800/175341]\n",
      "loss: 0.451683  [62400/175341]\n",
      "loss: 0.296190  [64000/175341]\n",
      "loss: 0.643331  [65600/175341]\n",
      "loss: 0.732927  [67200/175341]\n",
      "loss: 0.302977  [68800/175341]\n",
      "loss: 0.349577  [70400/175341]\n",
      "loss: 0.816629  [72000/175341]\n",
      "loss: 0.610443  [73600/175341]\n",
      "loss: 0.162652  [75200/175341]\n",
      "loss: 0.742186  [76800/175341]\n",
      "loss: 1.077731  [78400/175341]\n",
      "loss: 0.266275  [80000/175341]\n",
      "loss: 0.548011  [81600/175341]\n",
      "loss: 0.280881  [83200/175341]\n",
      "loss: 0.585931  [84800/175341]\n",
      "loss: 0.943963  [86400/175341]\n",
      "loss: 0.172931  [88000/175341]\n",
      "loss: 0.581157  [89600/175341]\n",
      "loss: 0.519924  [91200/175341]\n",
      "loss: 0.613383  [92800/175341]\n",
      "loss: 0.248525  [94400/175341]\n",
      "loss: 0.460696  [96000/175341]\n",
      "loss: 0.480217  [97600/175341]\n",
      "loss: 0.668386  [99200/175341]\n",
      "loss: 0.498783  [100800/175341]\n",
      "loss: 0.357308  [102400/175341]\n",
      "loss: 0.543997  [104000/175341]\n",
      "loss: 0.829682  [105600/175341]\n",
      "loss: 0.481849  [107200/175341]\n",
      "loss: 0.419634  [108800/175341]\n",
      "loss: 0.748573  [110400/175341]\n",
      "loss: 0.605505  [112000/175341]\n",
      "loss: 0.230569  [113600/175341]\n",
      "loss: 0.755663  [115200/175341]\n",
      "loss: 0.308212  [116800/175341]\n",
      "loss: 0.267780  [118400/175341]\n",
      "loss: 0.677074  [120000/175341]\n",
      "loss: 0.197053  [121600/175341]\n",
      "loss: 0.167926  [123200/175341]\n",
      "loss: 0.484834  [124800/175341]\n",
      "loss: 0.343152  [126400/175341]\n",
      "loss: 0.710244  [128000/175341]\n",
      "loss: 0.293950  [129600/175341]\n",
      "loss: 0.507889  [131200/175341]\n",
      "loss: 0.449343  [132800/175341]\n",
      "loss: 0.261240  [134400/175341]\n",
      "loss: 0.901843  [136000/175341]\n",
      "loss: 0.449438  [137600/175341]\n",
      "loss: 0.492540  [139200/175341]\n",
      "loss: 0.307035  [140800/175341]\n",
      "loss: 0.350610  [142400/175341]\n",
      "loss: 0.284290  [144000/175341]\n",
      "loss: 0.254329  [145600/175341]\n",
      "loss: 0.167338  [147200/175341]\n",
      "loss: 0.548056  [148800/175341]\n",
      "loss: 0.719544  [150400/175341]\n",
      "loss: 0.223659  [152000/175341]\n",
      "loss: 0.447407  [153600/175341]\n",
      "loss: 0.333619  [155200/175341]\n",
      "loss: 0.555789  [156800/175341]\n",
      "loss: 0.685118  [158400/175341]\n",
      "loss: 0.143171  [160000/175341]\n",
      "loss: 0.541394  [161600/175341]\n",
      "loss: 0.413975  [163200/175341]\n",
      "loss: 0.580174  [164800/175341]\n",
      "loss: 0.173615  [166400/175341]\n",
      "loss: 0.239758  [168000/175341]\n",
      "loss: 0.550100  [169600/175341]\n",
      "loss: 0.499289  [171200/175341]\n",
      "loss: 0.781551  [172800/175341]\n",
      "loss: 0.463405  [174400/175341]\n",
      "Train Accuracy: 81.1915%\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.572023, F1-score: 74.56%, Macro_F1-Score:  39.89%  \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.885081  [    0/175341]\n",
      "loss: 0.282963  [ 1600/175341]\n",
      "loss: 0.568431  [ 3200/175341]\n",
      "loss: 0.455146  [ 4800/175341]\n",
      "loss: 0.347728  [ 6400/175341]\n",
      "loss: 0.499223  [ 8000/175341]\n",
      "loss: 0.838833  [ 9600/175341]\n",
      "loss: 0.187839  [11200/175341]\n",
      "loss: 0.806010  [12800/175341]\n",
      "loss: 0.731619  [14400/175341]\n",
      "loss: 0.603575  [16000/175341]\n",
      "loss: 0.455180  [17600/175341]\n",
      "loss: 0.491177  [19200/175341]\n",
      "loss: 0.488529  [20800/175341]\n",
      "loss: 0.616963  [22400/175341]\n",
      "loss: 0.507332  [24000/175341]\n",
      "loss: 0.401535  [25600/175341]\n",
      "loss: 0.647023  [27200/175341]\n",
      "loss: 0.169411  [28800/175341]\n",
      "loss: 0.287918  [30400/175341]\n",
      "loss: 0.379709  [32000/175341]\n",
      "loss: 0.648261  [33600/175341]\n",
      "loss: 0.680227  [35200/175341]\n",
      "loss: 0.440152  [36800/175341]\n",
      "loss: 0.664010  [38400/175341]\n",
      "loss: 0.219494  [40000/175341]\n",
      "loss: 0.356229  [41600/175341]\n",
      "loss: 0.324843  [43200/175341]\n",
      "loss: 0.318354  [44800/175341]\n",
      "loss: 0.512361  [46400/175341]\n",
      "loss: 0.318474  [48000/175341]\n",
      "loss: 0.297026  [49600/175341]\n",
      "loss: 0.307333  [51200/175341]\n",
      "loss: 0.629548  [52800/175341]\n",
      "loss: 0.553175  [54400/175341]\n",
      "loss: 0.417254  [56000/175341]\n",
      "loss: 0.478739  [57600/175341]\n",
      "loss: 0.406630  [59200/175341]\n",
      "loss: 0.568948  [60800/175341]\n",
      "loss: 0.436477  [62400/175341]\n",
      "loss: 0.419345  [64000/175341]\n",
      "loss: 0.556107  [65600/175341]\n",
      "loss: 0.571077  [67200/175341]\n",
      "loss: 0.421766  [68800/175341]\n",
      "loss: 0.250046  [70400/175341]\n",
      "loss: 0.298883  [72000/175341]\n",
      "loss: 0.424652  [73600/175341]\n",
      "loss: 0.714413  [75200/175341]\n",
      "loss: 0.429470  [76800/175341]\n",
      "loss: 0.278545  [78400/175341]\n",
      "loss: 0.212067  [80000/175341]\n",
      "loss: 0.400817  [81600/175341]\n",
      "loss: 0.365799  [83200/175341]\n",
      "loss: 0.434564  [84800/175341]\n",
      "loss: 0.838692  [86400/175341]\n",
      "loss: 0.399809  [88000/175341]\n",
      "loss: 0.759542  [89600/175341]\n",
      "loss: 0.312377  [91200/175341]\n",
      "loss: 0.572477  [92800/175341]\n",
      "loss: 0.366503  [94400/175341]\n",
      "loss: 0.483461  [96000/175341]\n",
      "loss: 0.543377  [97600/175341]\n",
      "loss: 0.568167  [99200/175341]\n",
      "loss: 0.289027  [100800/175341]\n",
      "loss: 0.207471  [102400/175341]\n",
      "loss: 0.361076  [104000/175341]\n",
      "loss: 0.408110  [105600/175341]\n",
      "loss: 0.360615  [107200/175341]\n",
      "loss: 0.289824  [108800/175341]\n",
      "loss: 0.333782  [110400/175341]\n",
      "loss: 0.336722  [112000/175341]\n",
      "loss: 0.482832  [113600/175341]\n",
      "loss: 0.376444  [115200/175341]\n",
      "loss: 0.512383  [116800/175341]\n",
      "loss: 0.337807  [118400/175341]\n",
      "loss: 0.335933  [120000/175341]\n",
      "loss: 0.755662  [121600/175341]\n",
      "loss: 1.177991  [123200/175341]\n",
      "loss: 0.503419  [124800/175341]\n",
      "loss: 0.786803  [126400/175341]\n",
      "loss: 0.531012  [128000/175341]\n",
      "loss: 0.604025  [129600/175341]\n",
      "loss: 0.774144  [131200/175341]\n",
      "loss: 0.394418  [132800/175341]\n",
      "loss: 0.361931  [134400/175341]\n",
      "loss: 0.591707  [136000/175341]\n",
      "loss: 0.516546  [137600/175341]\n",
      "loss: 0.453240  [139200/175341]\n",
      "loss: 0.549142  [140800/175341]\n",
      "loss: 0.586998  [142400/175341]\n",
      "loss: 0.272146  [144000/175341]\n",
      "loss: 0.198295  [145600/175341]\n",
      "loss: 0.453742  [147200/175341]\n",
      "loss: 0.538645  [148800/175341]\n",
      "loss: 0.613844  [150400/175341]\n",
      "loss: 0.525591  [152000/175341]\n",
      "loss: 0.326715  [153600/175341]\n",
      "loss: 0.397046  [155200/175341]\n",
      "loss: 0.424886  [156800/175341]\n",
      "loss: 0.643418  [158400/175341]\n",
      "loss: 0.585313  [160000/175341]\n",
      "loss: 0.391933  [161600/175341]\n",
      "loss: 0.381731  [163200/175341]\n",
      "loss: 0.439674  [164800/175341]\n",
      "loss: 0.578825  [166400/175341]\n",
      "loss: 0.323180  [168000/175341]\n",
      "loss: 0.469829  [169600/175341]\n",
      "loss: 0.505958  [171200/175341]\n",
      "loss: 0.609625  [172800/175341]\n",
      "loss: 0.146417  [174400/175341]\n",
      "Train Accuracy: 81.2092%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.562478, F1-score: 75.60%, Macro_F1-Score:  40.29%  \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.460602  [    0/175341]\n",
      "loss: 0.160373  [ 1600/175341]\n",
      "loss: 0.536726  [ 3200/175341]\n",
      "loss: 0.301192  [ 4800/175341]\n",
      "loss: 0.510305  [ 6400/175341]\n",
      "loss: 0.511465  [ 8000/175341]\n",
      "loss: 0.386849  [ 9600/175341]\n",
      "loss: 0.629158  [11200/175341]\n",
      "loss: 0.399467  [12800/175341]\n",
      "loss: 0.166879  [14400/175341]\n",
      "loss: 0.286672  [16000/175341]\n",
      "loss: 0.572307  [17600/175341]\n",
      "loss: 0.546490  [19200/175341]\n",
      "loss: 0.445704  [20800/175341]\n",
      "loss: 0.374352  [22400/175341]\n",
      "loss: 0.648477  [24000/175341]\n",
      "loss: 0.392141  [25600/175341]\n",
      "loss: 0.370089  [27200/175341]\n",
      "loss: 0.077534  [28800/175341]\n",
      "loss: 0.643634  [30400/175341]\n",
      "loss: 0.474740  [32000/175341]\n",
      "loss: 0.279599  [33600/175341]\n",
      "loss: 0.647840  [35200/175341]\n",
      "loss: 0.799956  [36800/175341]\n",
      "loss: 0.425612  [38400/175341]\n",
      "loss: 0.352315  [40000/175341]\n",
      "loss: 0.672281  [41600/175341]\n",
      "loss: 0.943255  [43200/175341]\n",
      "loss: 0.365357  [44800/175341]\n",
      "loss: 0.608940  [46400/175341]\n",
      "loss: 0.541331  [48000/175341]\n",
      "loss: 0.634300  [49600/175341]\n",
      "loss: 0.427231  [51200/175341]\n",
      "loss: 0.813160  [52800/175341]\n",
      "loss: 0.330739  [54400/175341]\n",
      "loss: 0.759394  [56000/175341]\n",
      "loss: 0.570196  [57600/175341]\n",
      "loss: 0.506318  [59200/175341]\n",
      "loss: 0.553205  [60800/175341]\n",
      "loss: 0.518496  [62400/175341]\n",
      "loss: 0.719069  [64000/175341]\n",
      "loss: 0.381737  [65600/175341]\n",
      "loss: 0.415956  [67200/175341]\n",
      "loss: 0.703565  [68800/175341]\n",
      "loss: 0.399869  [70400/175341]\n",
      "loss: 0.818833  [72000/175341]\n",
      "loss: 0.439055  [73600/175341]\n",
      "loss: 0.426102  [75200/175341]\n",
      "loss: 0.369992  [76800/175341]\n",
      "loss: 0.619520  [78400/175341]\n",
      "loss: 0.860189  [80000/175341]\n",
      "loss: 0.180677  [81600/175341]\n",
      "loss: 0.414357  [83200/175341]\n",
      "loss: 0.364682  [84800/175341]\n",
      "loss: 0.360786  [86400/175341]\n",
      "loss: 0.569659  [88000/175341]\n",
      "loss: 0.561430  [89600/175341]\n",
      "loss: 0.349837  [91200/175341]\n",
      "loss: 0.284878  [92800/175341]\n",
      "loss: 0.210069  [94400/175341]\n",
      "loss: 0.392999  [96000/175341]\n",
      "loss: 0.479625  [97600/175341]\n",
      "loss: 0.498633  [99200/175341]\n",
      "loss: 0.543551  [100800/175341]\n",
      "loss: 0.554837  [102400/175341]\n",
      "loss: 0.406132  [104000/175341]\n",
      "loss: 0.143115  [105600/175341]\n",
      "loss: 0.056645  [107200/175341]\n",
      "loss: 0.516362  [108800/175341]\n",
      "loss: 0.342208  [110400/175341]\n",
      "loss: 0.388882  [112000/175341]\n",
      "loss: 0.678236  [113600/175341]\n",
      "loss: 0.307038  [115200/175341]\n",
      "loss: 0.419196  [116800/175341]\n",
      "loss: 1.137089  [118400/175341]\n",
      "loss: 0.832552  [120000/175341]\n",
      "loss: 0.494484  [121600/175341]\n",
      "loss: 0.856532  [123200/175341]\n",
      "loss: 0.795735  [124800/175341]\n",
      "loss: 0.618004  [126400/175341]\n",
      "loss: 0.328185  [128000/175341]\n",
      "loss: 0.350456  [129600/175341]\n",
      "loss: 0.576210  [131200/175341]\n",
      "loss: 0.788036  [132800/175341]\n",
      "loss: 0.295405  [134400/175341]\n",
      "loss: 0.484822  [136000/175341]\n",
      "loss: 0.456413  [137600/175341]\n",
      "loss: 0.641431  [139200/175341]\n",
      "loss: 0.589071  [140800/175341]\n",
      "loss: 0.263895  [142400/175341]\n",
      "loss: 0.571432  [144000/175341]\n",
      "loss: 0.471823  [145600/175341]\n",
      "loss: 0.600132  [147200/175341]\n",
      "loss: 0.389208  [148800/175341]\n",
      "loss: 0.218375  [150400/175341]\n",
      "loss: 0.471533  [152000/175341]\n",
      "loss: 0.336221  [153600/175341]\n",
      "loss: 0.164325  [155200/175341]\n",
      "loss: 0.316379  [156800/175341]\n",
      "loss: 0.237132  [158400/175341]\n",
      "loss: 0.198913  [160000/175341]\n",
      "loss: 0.653621  [161600/175341]\n",
      "loss: 0.401377  [163200/175341]\n",
      "loss: 0.321655  [164800/175341]\n",
      "loss: 0.408267  [166400/175341]\n",
      "loss: 0.661012  [168000/175341]\n",
      "loss: 0.720584  [169600/175341]\n",
      "loss: 0.928130  [171200/175341]\n",
      "loss: 0.182944  [172800/175341]\n",
      "loss: 0.441842  [174400/175341]\n",
      "Train Accuracy: 81.2274%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.555221, F1-score: 76.16%, Macro_F1-Score:  40.16%  \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.383796  [    0/175341]\n",
      "loss: 0.238888  [ 1600/175341]\n",
      "loss: 0.411364  [ 3200/175341]\n",
      "loss: 0.106992  [ 4800/175341]\n",
      "loss: 0.178978  [ 6400/175341]\n",
      "loss: 0.902577  [ 8000/175341]\n",
      "loss: 0.784601  [ 9600/175341]\n",
      "loss: 0.980909  [11200/175341]\n",
      "loss: 0.195853  [12800/175341]\n",
      "loss: 0.377879  [14400/175341]\n",
      "loss: 0.716753  [16000/175341]\n",
      "loss: 0.575266  [17600/175341]\n",
      "loss: 1.099875  [19200/175341]\n",
      "loss: 0.902808  [20800/175341]\n",
      "loss: 0.653749  [22400/175341]\n",
      "loss: 0.339646  [24000/175341]\n",
      "loss: 0.488882  [25600/175341]\n",
      "loss: 0.111299  [27200/175341]\n",
      "loss: 0.527885  [28800/175341]\n",
      "loss: 0.368862  [30400/175341]\n",
      "loss: 0.164612  [32000/175341]\n",
      "loss: 0.206912  [33600/175341]\n",
      "loss: 0.471142  [35200/175341]\n",
      "loss: 0.868225  [36800/175341]\n",
      "loss: 0.512943  [38400/175341]\n",
      "loss: 0.455669  [40000/175341]\n",
      "loss: 0.621374  [41600/175341]\n",
      "loss: 0.210116  [43200/175341]\n",
      "loss: 0.829310  [44800/175341]\n",
      "loss: 0.155214  [46400/175341]\n",
      "loss: 0.608336  [48000/175341]\n",
      "loss: 0.656361  [49600/175341]\n",
      "loss: 0.936646  [51200/175341]\n",
      "loss: 0.332948  [52800/175341]\n",
      "loss: 0.437112  [54400/175341]\n",
      "loss: 0.248904  [56000/175341]\n",
      "loss: 0.371807  [57600/175341]\n",
      "loss: 0.349995  [59200/175341]\n",
      "loss: 0.492636  [60800/175341]\n",
      "loss: 0.535700  [62400/175341]\n",
      "loss: 0.308761  [64000/175341]\n",
      "loss: 0.301130  [65600/175341]\n",
      "loss: 0.639104  [67200/175341]\n",
      "loss: 0.527304  [68800/175341]\n",
      "loss: 0.319214  [70400/175341]\n",
      "loss: 1.040759  [72000/175341]\n",
      "loss: 0.556277  [73600/175341]\n",
      "loss: 0.554306  [75200/175341]\n",
      "loss: 0.301374  [76800/175341]\n",
      "loss: 0.142120  [78400/175341]\n",
      "loss: 0.350933  [80000/175341]\n",
      "loss: 0.279791  [81600/175341]\n",
      "loss: 0.137741  [83200/175341]\n",
      "loss: 0.655520  [84800/175341]\n",
      "loss: 0.384516  [86400/175341]\n",
      "loss: 0.648141  [88000/175341]\n",
      "loss: 0.095063  [89600/175341]\n",
      "loss: 0.627510  [91200/175341]\n",
      "loss: 0.716039  [92800/175341]\n",
      "loss: 0.273798  [94400/175341]\n",
      "loss: 0.522628  [96000/175341]\n",
      "loss: 0.323339  [97600/175341]\n",
      "loss: 0.398658  [99200/175341]\n",
      "loss: 0.326074  [100800/175341]\n",
      "loss: 0.414889  [102400/175341]\n",
      "loss: 0.407024  [104000/175341]\n",
      "loss: 0.254642  [105600/175341]\n",
      "loss: 0.544533  [107200/175341]\n",
      "loss: 0.235599  [108800/175341]\n",
      "loss: 0.856952  [110400/175341]\n",
      "loss: 0.583524  [112000/175341]\n",
      "loss: 0.361122  [113600/175341]\n",
      "loss: 0.188304  [115200/175341]\n",
      "loss: 0.666216  [116800/175341]\n",
      "loss: 0.291045  [118400/175341]\n",
      "loss: 0.207879  [120000/175341]\n",
      "loss: 0.411540  [121600/175341]\n",
      "loss: 0.400016  [123200/175341]\n",
      "loss: 0.345828  [124800/175341]\n",
      "loss: 0.293454  [126400/175341]\n",
      "loss: 0.375796  [128000/175341]\n",
      "loss: 0.561315  [129600/175341]\n",
      "loss: 0.270239  [131200/175341]\n",
      "loss: 0.635805  [132800/175341]\n",
      "loss: 0.265863  [134400/175341]\n",
      "loss: 0.432102  [136000/175341]\n",
      "loss: 0.413339  [137600/175341]\n",
      "loss: 0.528077  [139200/175341]\n",
      "loss: 0.498361  [140800/175341]\n",
      "loss: 0.406537  [142400/175341]\n",
      "loss: 0.636815  [144000/175341]\n",
      "loss: 0.650704  [145600/175341]\n",
      "loss: 0.207049  [147200/175341]\n",
      "loss: 0.391454  [148800/175341]\n",
      "loss: 0.433802  [150400/175341]\n",
      "loss: 0.605375  [152000/175341]\n",
      "loss: 0.487924  [153600/175341]\n",
      "loss: 0.594001  [155200/175341]\n",
      "loss: 0.676522  [156800/175341]\n",
      "loss: 0.537428  [158400/175341]\n",
      "loss: 0.685269  [160000/175341]\n",
      "loss: 0.594662  [161600/175341]\n",
      "loss: 0.892313  [163200/175341]\n",
      "loss: 0.397228  [164800/175341]\n",
      "loss: 0.741692  [166400/175341]\n",
      "loss: 0.406642  [168000/175341]\n",
      "loss: 0.431806  [169600/175341]\n",
      "loss: 0.333047  [171200/175341]\n",
      "loss: 0.310586  [172800/175341]\n",
      "loss: 0.118920  [174400/175341]\n",
      "Train Accuracy: 81.2656%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.549672, F1-score: 76.59%, Macro_F1-Score:  40.90%  \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.663646  [    0/175341]\n",
      "loss: 0.395602  [ 1600/175341]\n",
      "loss: 0.350782  [ 3200/175341]\n",
      "loss: 0.407439  [ 4800/175341]\n",
      "loss: 0.573495  [ 6400/175341]\n",
      "loss: 0.574822  [ 8000/175341]\n",
      "loss: 0.356992  [ 9600/175341]\n",
      "loss: 0.549094  [11200/175341]\n",
      "loss: 0.579022  [12800/175341]\n",
      "loss: 0.732965  [14400/175341]\n",
      "loss: 0.285651  [16000/175341]\n",
      "loss: 0.265727  [17600/175341]\n",
      "loss: 0.446356  [19200/175341]\n",
      "loss: 0.258291  [20800/175341]\n",
      "loss: 0.690806  [22400/175341]\n",
      "loss: 0.459932  [24000/175341]\n",
      "loss: 0.543575  [25600/175341]\n",
      "loss: 0.398611  [27200/175341]\n",
      "loss: 0.451550  [28800/175341]\n",
      "loss: 0.305554  [30400/175341]\n",
      "loss: 0.241286  [32000/175341]\n",
      "loss: 0.361061  [33600/175341]\n",
      "loss: 0.396127  [35200/175341]\n",
      "loss: 0.220596  [36800/175341]\n",
      "loss: 0.324496  [38400/175341]\n",
      "loss: 0.564514  [40000/175341]\n",
      "loss: 0.741169  [41600/175341]\n",
      "loss: 0.316147  [43200/175341]\n",
      "loss: 0.477796  [44800/175341]\n",
      "loss: 0.416289  [46400/175341]\n",
      "loss: 0.343721  [48000/175341]\n",
      "loss: 0.272652  [49600/175341]\n",
      "loss: 0.129931  [51200/175341]\n",
      "loss: 0.605756  [52800/175341]\n",
      "loss: 0.192817  [54400/175341]\n",
      "loss: 0.239066  [56000/175341]\n",
      "loss: 0.662628  [57600/175341]\n",
      "loss: 0.502406  [59200/175341]\n",
      "loss: 0.229724  [60800/175341]\n",
      "loss: 0.247858  [62400/175341]\n",
      "loss: 0.597824  [64000/175341]\n",
      "loss: 0.361990  [65600/175341]\n",
      "loss: 0.886038  [67200/175341]\n",
      "loss: 0.220656  [68800/175341]\n",
      "loss: 0.518544  [70400/175341]\n",
      "loss: 0.605181  [72000/175341]\n",
      "loss: 0.162657  [73600/175341]\n",
      "loss: 0.691560  [75200/175341]\n",
      "loss: 0.296571  [76800/175341]\n",
      "loss: 0.517401  [78400/175341]\n",
      "loss: 0.466353  [80000/175341]\n",
      "loss: 0.782318  [81600/175341]\n",
      "loss: 0.725046  [83200/175341]\n",
      "loss: 0.595032  [84800/175341]\n",
      "loss: 0.750877  [86400/175341]\n",
      "loss: 0.766791  [88000/175341]\n",
      "loss: 0.459459  [89600/175341]\n",
      "loss: 0.262065  [91200/175341]\n",
      "loss: 0.506243  [92800/175341]\n",
      "loss: 0.589942  [94400/175341]\n",
      "loss: 0.544365  [96000/175341]\n",
      "loss: 0.658168  [97600/175341]\n",
      "loss: 0.317384  [99200/175341]\n",
      "loss: 0.625380  [100800/175341]\n",
      "loss: 0.647667  [102400/175341]\n",
      "loss: 0.166322  [104000/175341]\n",
      "loss: 0.430318  [105600/175341]\n",
      "loss: 0.253210  [107200/175341]\n",
      "loss: 0.310948  [108800/175341]\n",
      "loss: 0.699204  [110400/175341]\n",
      "loss: 0.346575  [112000/175341]\n",
      "loss: 0.581586  [113600/175341]\n",
      "loss: 0.216700  [115200/175341]\n",
      "loss: 0.255915  [116800/175341]\n",
      "loss: 0.480058  [118400/175341]\n",
      "loss: 0.532571  [120000/175341]\n",
      "loss: 0.618854  [121600/175341]\n",
      "loss: 0.816709  [123200/175341]\n",
      "loss: 0.328875  [124800/175341]\n",
      "loss: 0.934739  [126400/175341]\n",
      "loss: 0.770064  [128000/175341]\n",
      "loss: 0.389200  [129600/175341]\n",
      "loss: 0.288014  [131200/175341]\n",
      "loss: 0.591857  [132800/175341]\n",
      "loss: 0.332381  [134400/175341]\n",
      "loss: 0.212261  [136000/175341]\n",
      "loss: 0.597238  [137600/175341]\n",
      "loss: 0.618043  [139200/175341]\n",
      "loss: 0.344358  [140800/175341]\n",
      "loss: 0.623415  [142400/175341]\n",
      "loss: 0.218782  [144000/175341]\n",
      "loss: 0.242141  [145600/175341]\n",
      "loss: 0.496714  [147200/175341]\n",
      "loss: 0.621469  [148800/175341]\n",
      "loss: 0.870032  [150400/175341]\n",
      "loss: 0.308442  [152000/175341]\n",
      "loss: 0.533945  [153600/175341]\n",
      "loss: 0.415427  [155200/175341]\n",
      "loss: 0.873142  [156800/175341]\n",
      "loss: 0.489295  [158400/175341]\n",
      "loss: 0.318927  [160000/175341]\n",
      "loss: 0.366225  [161600/175341]\n",
      "loss: 0.736531  [163200/175341]\n",
      "loss: 0.566947  [164800/175341]\n",
      "loss: 0.670607  [166400/175341]\n",
      "loss: 0.341923  [168000/175341]\n",
      "loss: 0.256805  [169600/175341]\n",
      "loss: 0.422959  [171200/175341]\n",
      "loss: 0.663762  [172800/175341]\n",
      "loss: 0.467839  [174400/175341]\n",
      "Train Accuracy: 81.2702%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.577594, F1-score: 74.39%, Macro_F1-Score:  39.80%  \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.323310  [    0/175341]\n",
      "loss: 0.381739  [ 1600/175341]\n",
      "loss: 0.439359  [ 3200/175341]\n",
      "loss: 0.347614  [ 4800/175341]\n",
      "loss: 0.205719  [ 6400/175341]\n",
      "loss: 0.458810  [ 8000/175341]\n",
      "loss: 0.297285  [ 9600/175341]\n",
      "loss: 0.547905  [11200/175341]\n",
      "loss: 0.534689  [12800/175341]\n",
      "loss: 0.802756  [14400/175341]\n",
      "loss: 0.319445  [16000/175341]\n",
      "loss: 0.487722  [17600/175341]\n",
      "loss: 0.496487  [19200/175341]\n",
      "loss: 0.551508  [20800/175341]\n",
      "loss: 0.266078  [22400/175341]\n",
      "loss: 0.276504  [24000/175341]\n",
      "loss: 0.220556  [25600/175341]\n",
      "loss: 0.402822  [27200/175341]\n",
      "loss: 0.448338  [28800/175341]\n",
      "loss: 0.279990  [30400/175341]\n",
      "loss: 0.350996  [32000/175341]\n",
      "loss: 0.598897  [33600/175341]\n",
      "loss: 0.670060  [35200/175341]\n",
      "loss: 0.426095  [36800/175341]\n",
      "loss: 0.608340  [38400/175341]\n",
      "loss: 0.261085  [40000/175341]\n",
      "loss: 0.354571  [41600/175341]\n",
      "loss: 0.320955  [43200/175341]\n",
      "loss: 0.744053  [44800/175341]\n",
      "loss: 0.217467  [46400/175341]\n",
      "loss: 0.428294  [48000/175341]\n",
      "loss: 0.520806  [49600/175341]\n",
      "loss: 0.670677  [51200/175341]\n",
      "loss: 0.409678  [52800/175341]\n",
      "loss: 0.520894  [54400/175341]\n",
      "loss: 0.288204  [56000/175341]\n",
      "loss: 0.460279  [57600/175341]\n",
      "loss: 0.182993  [59200/175341]\n",
      "loss: 0.486876  [60800/175341]\n",
      "loss: 0.626560  [62400/175341]\n",
      "loss: 0.421647  [64000/175341]\n",
      "loss: 0.278965  [65600/175341]\n",
      "loss: 0.480855  [67200/175341]\n",
      "loss: 0.479943  [68800/175341]\n",
      "loss: 0.185594  [70400/175341]\n",
      "loss: 0.541628  [72000/175341]\n",
      "loss: 0.332256  [73600/175341]\n",
      "loss: 0.814857  [75200/175341]\n",
      "loss: 0.384459  [76800/175341]\n",
      "loss: 0.298479  [78400/175341]\n",
      "loss: 0.206012  [80000/175341]\n",
      "loss: 0.462477  [81600/175341]\n",
      "loss: 0.333260  [83200/175341]\n",
      "loss: 0.463492  [84800/175341]\n",
      "loss: 0.428305  [86400/175341]\n",
      "loss: 0.274889  [88000/175341]\n",
      "loss: 0.503563  [89600/175341]\n",
      "loss: 0.282396  [91200/175341]\n",
      "loss: 0.533840  [92800/175341]\n",
      "loss: 0.276990  [94400/175341]\n",
      "loss: 0.306544  [96000/175341]\n",
      "loss: 0.733314  [97600/175341]\n",
      "loss: 0.381018  [99200/175341]\n",
      "loss: 0.600809  [100800/175341]\n",
      "loss: 1.050135  [102400/175341]\n",
      "loss: 0.355601  [104000/175341]\n",
      "loss: 0.561511  [105600/175341]\n",
      "loss: 0.558596  [107200/175341]\n",
      "loss: 0.614666  [108800/175341]\n",
      "loss: 0.564582  [110400/175341]\n",
      "loss: 0.328006  [112000/175341]\n",
      "loss: 0.165103  [113600/175341]\n",
      "loss: 0.546322  [115200/175341]\n",
      "loss: 0.778674  [116800/175341]\n",
      "loss: 0.341255  [118400/175341]\n",
      "loss: 0.325646  [120000/175341]\n",
      "loss: 0.585066  [121600/175341]\n",
      "loss: 0.645773  [123200/175341]\n",
      "loss: 0.656119  [124800/175341]\n",
      "loss: 0.373630  [126400/175341]\n",
      "loss: 0.386494  [128000/175341]\n",
      "loss: 0.566618  [129600/175341]\n",
      "loss: 0.287709  [131200/175341]\n",
      "loss: 0.101697  [132800/175341]\n",
      "loss: 0.695025  [134400/175341]\n",
      "loss: 0.282964  [136000/175341]\n",
      "loss: 0.234478  [137600/175341]\n",
      "loss: 0.384881  [139200/175341]\n",
      "loss: 0.194175  [140800/175341]\n",
      "loss: 0.827311  [142400/175341]\n",
      "loss: 0.637699  [144000/175341]\n",
      "loss: 0.448418  [145600/175341]\n",
      "loss: 0.674636  [147200/175341]\n",
      "loss: 0.438736  [148800/175341]\n",
      "loss: 0.174302  [150400/175341]\n",
      "loss: 0.783078  [152000/175341]\n",
      "loss: 0.441184  [153600/175341]\n",
      "loss: 0.460541  [155200/175341]\n",
      "loss: 0.119255  [156800/175341]\n",
      "loss: 0.351729  [158400/175341]\n",
      "loss: 0.858246  [160000/175341]\n",
      "loss: 0.712283  [161600/175341]\n",
      "loss: 0.346462  [163200/175341]\n",
      "loss: 0.405596  [164800/175341]\n",
      "loss: 0.690186  [166400/175341]\n",
      "loss: 0.574405  [168000/175341]\n",
      "loss: 0.676231  [169600/175341]\n",
      "loss: 0.766778  [171200/175341]\n",
      "loss: 0.559569  [172800/175341]\n",
      "loss: 0.378514  [174400/175341]\n",
      "Train Accuracy: 81.2645%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.570265, F1-score: 75.23%, Macro_F1-Score:  40.21%  \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.344252  [    0/175341]\n",
      "loss: 0.426789  [ 1600/175341]\n",
      "loss: 0.359866  [ 3200/175341]\n",
      "loss: 0.406879  [ 4800/175341]\n",
      "loss: 0.908950  [ 6400/175341]\n",
      "loss: 0.395310  [ 8000/175341]\n",
      "loss: 0.369657  [ 9600/175341]\n",
      "loss: 0.594541  [11200/175341]\n",
      "loss: 0.561320  [12800/175341]\n",
      "loss: 0.425332  [14400/175341]\n",
      "loss: 0.385636  [16000/175341]\n",
      "loss: 0.500370  [17600/175341]\n",
      "loss: 0.292900  [19200/175341]\n",
      "loss: 0.104848  [20800/175341]\n",
      "loss: 0.384005  [22400/175341]\n",
      "loss: 0.427675  [24000/175341]\n",
      "loss: 0.929289  [25600/175341]\n",
      "loss: 0.467849  [27200/175341]\n",
      "loss: 0.270269  [28800/175341]\n",
      "loss: 0.516887  [30400/175341]\n",
      "loss: 0.331724  [32000/175341]\n",
      "loss: 0.263796  [33600/175341]\n",
      "loss: 0.700106  [35200/175341]\n",
      "loss: 0.604967  [36800/175341]\n",
      "loss: 0.612832  [38400/175341]\n",
      "loss: 0.631231  [40000/175341]\n",
      "loss: 0.416877  [41600/175341]\n",
      "loss: 0.587030  [43200/175341]\n",
      "loss: 0.924776  [44800/175341]\n",
      "loss: 0.489424  [46400/175341]\n",
      "loss: 0.391748  [48000/175341]\n",
      "loss: 0.615271  [49600/175341]\n",
      "loss: 0.195479  [51200/175341]\n",
      "loss: 0.288123  [52800/175341]\n",
      "loss: 0.861621  [54400/175341]\n",
      "loss: 0.396702  [56000/175341]\n",
      "loss: 0.532625  [57600/175341]\n",
      "loss: 0.375448  [59200/175341]\n",
      "loss: 0.412802  [60800/175341]\n",
      "loss: 0.451282  [62400/175341]\n",
      "loss: 0.630596  [64000/175341]\n",
      "loss: 0.765649  [65600/175341]\n",
      "loss: 0.325780  [67200/175341]\n",
      "loss: 0.254603  [68800/175341]\n",
      "loss: 0.499157  [70400/175341]\n",
      "loss: 0.527957  [72000/175341]\n",
      "loss: 0.362252  [73600/175341]\n",
      "loss: 0.236039  [75200/175341]\n",
      "loss: 0.620772  [76800/175341]\n",
      "loss: 0.265484  [78400/175341]\n",
      "loss: 0.752416  [80000/175341]\n",
      "loss: 0.121352  [81600/175341]\n",
      "loss: 0.507013  [83200/175341]\n",
      "loss: 0.435446  [84800/175341]\n",
      "loss: 0.852669  [86400/175341]\n",
      "loss: 0.449145  [88000/175341]\n",
      "loss: 0.408639  [89600/175341]\n",
      "loss: 0.583052  [91200/175341]\n",
      "loss: 0.249530  [92800/175341]\n",
      "loss: 0.596022  [94400/175341]\n",
      "loss: 0.452221  [96000/175341]\n",
      "loss: 0.476173  [97600/175341]\n",
      "loss: 0.341560  [99200/175341]\n",
      "loss: 0.616635  [100800/175341]\n",
      "loss: 0.377147  [102400/175341]\n",
      "loss: 0.551857  [104000/175341]\n",
      "loss: 0.278957  [105600/175341]\n",
      "loss: 0.810793  [107200/175341]\n",
      "loss: 0.866416  [108800/175341]\n",
      "loss: 0.466957  [110400/175341]\n",
      "loss: 0.312119  [112000/175341]\n",
      "loss: 0.307403  [113600/175341]\n",
      "loss: 0.495788  [115200/175341]\n",
      "loss: 0.262757  [116800/175341]\n",
      "loss: 0.376465  [118400/175341]\n",
      "loss: 0.494423  [120000/175341]\n",
      "loss: 0.142524  [121600/175341]\n",
      "loss: 0.419598  [123200/175341]\n",
      "loss: 0.420197  [124800/175341]\n",
      "loss: 0.509970  [126400/175341]\n",
      "loss: 0.089418  [128000/175341]\n",
      "loss: 0.833397  [129600/175341]\n",
      "loss: 0.639656  [131200/175341]\n",
      "loss: 0.513030  [132800/175341]\n",
      "loss: 0.494639  [134400/175341]\n",
      "loss: 0.230959  [136000/175341]\n",
      "loss: 0.184769  [137600/175341]\n",
      "loss: 0.449409  [139200/175341]\n",
      "loss: 1.102497  [140800/175341]\n",
      "loss: 0.325004  [142400/175341]\n",
      "loss: 0.131673  [144000/175341]\n",
      "loss: 0.366420  [145600/175341]\n",
      "loss: 0.530026  [147200/175341]\n",
      "loss: 0.583974  [148800/175341]\n",
      "loss: 0.327115  [150400/175341]\n",
      "loss: 0.465971  [152000/175341]\n",
      "loss: 0.665032  [153600/175341]\n",
      "loss: 0.893854  [155200/175341]\n",
      "loss: 0.519687  [156800/175341]\n",
      "loss: 0.400049  [158400/175341]\n",
      "loss: 0.576652  [160000/175341]\n",
      "loss: 0.275511  [161600/175341]\n",
      "loss: 0.500973  [163200/175341]\n",
      "loss: 0.371493  [164800/175341]\n",
      "loss: 0.393565  [166400/175341]\n",
      "loss: 0.266832  [168000/175341]\n",
      "loss: 0.422996  [169600/175341]\n",
      "loss: 0.150054  [171200/175341]\n",
      "loss: 0.901542  [172800/175341]\n",
      "loss: 0.726515  [174400/175341]\n",
      "Train Accuracy: 81.2845%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.565816, F1-score: 75.47%, Macro_F1-Score:  39.99%  \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.706493  [    0/175341]\n",
      "loss: 0.825962  [ 1600/175341]\n",
      "loss: 0.340086  [ 3200/175341]\n",
      "loss: 0.181779  [ 4800/175341]\n",
      "loss: 0.597773  [ 6400/175341]\n",
      "loss: 0.517113  [ 8000/175341]\n",
      "loss: 0.418783  [ 9600/175341]\n",
      "loss: 0.307098  [11200/175341]\n",
      "loss: 0.489396  [12800/175341]\n",
      "loss: 0.601123  [14400/175341]\n",
      "loss: 0.559372  [16000/175341]\n",
      "loss: 0.433692  [17600/175341]\n",
      "loss: 0.441089  [19200/175341]\n",
      "loss: 0.854873  [20800/175341]\n",
      "loss: 0.334574  [22400/175341]\n",
      "loss: 0.417170  [24000/175341]\n",
      "loss: 0.285591  [25600/175341]\n",
      "loss: 0.192319  [27200/175341]\n",
      "loss: 0.489074  [28800/175341]\n",
      "loss: 0.292186  [30400/175341]\n",
      "loss: 0.600551  [32000/175341]\n",
      "loss: 0.479397  [33600/175341]\n",
      "loss: 0.733933  [35200/175341]\n",
      "loss: 0.553581  [36800/175341]\n",
      "loss: 0.692207  [38400/175341]\n",
      "loss: 0.197808  [40000/175341]\n",
      "loss: 0.336535  [41600/175341]\n",
      "loss: 0.410626  [43200/175341]\n",
      "loss: 0.373690  [44800/175341]\n",
      "loss: 0.321583  [46400/175341]\n",
      "loss: 0.312791  [48000/175341]\n",
      "loss: 0.528463  [49600/175341]\n",
      "loss: 0.593954  [51200/175341]\n",
      "loss: 0.361035  [52800/175341]\n",
      "loss: 0.888704  [54400/175341]\n",
      "loss: 0.126964  [56000/175341]\n",
      "loss: 0.791274  [57600/175341]\n",
      "loss: 0.194240  [59200/175341]\n",
      "loss: 0.485331  [60800/175341]\n",
      "loss: 0.271125  [62400/175341]\n",
      "loss: 0.155681  [64000/175341]\n",
      "loss: 0.392604  [65600/175341]\n",
      "loss: 0.402556  [67200/175341]\n",
      "loss: 0.446466  [68800/175341]\n",
      "loss: 0.567490  [70400/175341]\n",
      "loss: 0.496492  [72000/175341]\n",
      "loss: 0.235093  [73600/175341]\n",
      "loss: 0.334147  [75200/175341]\n",
      "loss: 0.405292  [76800/175341]\n",
      "loss: 0.198616  [78400/175341]\n",
      "loss: 0.288948  [80000/175341]\n",
      "loss: 0.178392  [81600/175341]\n",
      "loss: 0.289535  [83200/175341]\n",
      "loss: 0.371021  [84800/175341]\n",
      "loss: 0.220622  [86400/175341]\n",
      "loss: 0.421054  [88000/175341]\n",
      "loss: 0.240800  [89600/175341]\n",
      "loss: 0.471798  [91200/175341]\n",
      "loss: 0.669271  [92800/175341]\n",
      "loss: 0.102176  [94400/175341]\n",
      "loss: 0.731542  [96000/175341]\n",
      "loss: 0.715486  [97600/175341]\n",
      "loss: 0.689188  [99200/175341]\n",
      "loss: 0.856082  [100800/175341]\n",
      "loss: 0.236208  [102400/175341]\n",
      "loss: 0.248097  [104000/175341]\n",
      "loss: 0.834828  [105600/175341]\n",
      "loss: 0.595028  [107200/175341]\n",
      "loss: 0.433839  [108800/175341]\n",
      "loss: 0.428566  [110400/175341]\n",
      "loss: 0.293290  [112000/175341]\n",
      "loss: 0.270700  [113600/175341]\n",
      "loss: 0.515566  [115200/175341]\n",
      "loss: 0.247590  [116800/175341]\n",
      "loss: 0.536172  [118400/175341]\n",
      "loss: 0.457004  [120000/175341]\n",
      "loss: 0.313128  [121600/175341]\n",
      "loss: 0.526912  [123200/175341]\n",
      "loss: 0.729729  [124800/175341]\n",
      "loss: 0.540389  [126400/175341]\n",
      "loss: 0.773279  [128000/175341]\n",
      "loss: 0.533022  [129600/175341]\n",
      "loss: 0.791569  [131200/175341]\n",
      "loss: 0.296153  [132800/175341]\n",
      "loss: 0.457250  [134400/175341]\n",
      "loss: 0.544613  [136000/175341]\n",
      "loss: 1.149274  [137600/175341]\n",
      "loss: 0.361914  [139200/175341]\n",
      "loss: 0.690357  [140800/175341]\n",
      "loss: 0.210382  [142400/175341]\n",
      "loss: 0.444517  [144000/175341]\n",
      "loss: 0.447890  [145600/175341]\n",
      "loss: 0.750331  [147200/175341]\n",
      "loss: 0.338650  [148800/175341]\n",
      "loss: 0.462596  [150400/175341]\n",
      "loss: 0.423905  [152000/175341]\n",
      "loss: 0.236080  [153600/175341]\n",
      "loss: 0.212604  [155200/175341]\n",
      "loss: 0.533008  [156800/175341]\n",
      "loss: 0.183075  [158400/175341]\n",
      "loss: 0.303732  [160000/175341]\n",
      "loss: 0.263950  [161600/175341]\n",
      "loss: 0.418200  [163200/175341]\n",
      "loss: 0.380466  [164800/175341]\n",
      "loss: 0.394417  [166400/175341]\n",
      "loss: 0.632957  [168000/175341]\n",
      "loss: 0.246382  [169600/175341]\n",
      "loss: 0.223771  [171200/175341]\n",
      "loss: 0.769663  [172800/175341]\n",
      "loss: 0.259104  [174400/175341]\n",
      "Train Accuracy: 81.2708%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.568334, F1-score: 74.94%, Macro_F1-Score:  40.26%  \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.341126  [    0/175341]\n",
      "loss: 0.654395  [ 1600/175341]\n",
      "loss: 0.459965  [ 3200/175341]\n",
      "loss: 0.611449  [ 4800/175341]\n",
      "loss: 0.572405  [ 6400/175341]\n",
      "loss: 0.447548  [ 8000/175341]\n",
      "loss: 0.245096  [ 9600/175341]\n",
      "loss: 0.598330  [11200/175341]\n",
      "loss: 0.588353  [12800/175341]\n",
      "loss: 0.497737  [14400/175341]\n",
      "loss: 0.546821  [16000/175341]\n",
      "loss: 0.988460  [17600/175341]\n",
      "loss: 0.528319  [19200/175341]\n",
      "loss: 0.261772  [20800/175341]\n",
      "loss: 0.500444  [22400/175341]\n",
      "loss: 0.679122  [24000/175341]\n",
      "loss: 0.070806  [25600/175341]\n",
      "loss: 0.527835  [27200/175341]\n",
      "loss: 0.295863  [28800/175341]\n",
      "loss: 0.123638  [30400/175341]\n",
      "loss: 0.442499  [32000/175341]\n",
      "loss: 0.239460  [33600/175341]\n",
      "loss: 0.528489  [35200/175341]\n",
      "loss: 1.425460  [36800/175341]\n",
      "loss: 0.082180  [38400/175341]\n",
      "loss: 0.446502  [40000/175341]\n",
      "loss: 0.420320  [41600/175341]\n",
      "loss: 0.724120  [43200/175341]\n",
      "loss: 0.358604  [44800/175341]\n",
      "loss: 0.316843  [46400/175341]\n",
      "loss: 0.631551  [48000/175341]\n",
      "loss: 0.608260  [49600/175341]\n",
      "loss: 0.317555  [51200/175341]\n",
      "loss: 0.301368  [52800/175341]\n",
      "loss: 0.412723  [54400/175341]\n",
      "loss: 0.131120  [56000/175341]\n",
      "loss: 0.108602  [57600/175341]\n",
      "loss: 0.251473  [59200/175341]\n",
      "loss: 0.441508  [60800/175341]\n",
      "loss: 0.494741  [62400/175341]\n",
      "loss: 0.293621  [64000/175341]\n",
      "loss: 0.459828  [65600/175341]\n",
      "loss: 0.125282  [67200/175341]\n",
      "loss: 0.281411  [68800/175341]\n",
      "loss: 0.892201  [70400/175341]\n",
      "loss: 0.506609  [72000/175341]\n",
      "loss: 0.425731  [73600/175341]\n",
      "loss: 0.727407  [75200/175341]\n",
      "loss: 0.251371  [76800/175341]\n",
      "loss: 0.173461  [78400/175341]\n",
      "loss: 0.458670  [80000/175341]\n",
      "loss: 0.231604  [81600/175341]\n",
      "loss: 0.465582  [83200/175341]\n",
      "loss: 0.451731  [84800/175341]\n",
      "loss: 0.372364  [86400/175341]\n",
      "loss: 0.816470  [88000/175341]\n",
      "loss: 0.494389  [89600/175341]\n",
      "loss: 0.348824  [91200/175341]\n",
      "loss: 0.111214  [92800/175341]\n",
      "loss: 0.454714  [94400/175341]\n",
      "loss: 0.744360  [96000/175341]\n",
      "loss: 0.579003  [97600/175341]\n",
      "loss: 0.500912  [99200/175341]\n",
      "loss: 0.310627  [100800/175341]\n",
      "loss: 0.211427  [102400/175341]\n",
      "loss: 0.377306  [104000/175341]\n",
      "loss: 0.466025  [105600/175341]\n",
      "loss: 0.464623  [107200/175341]\n",
      "loss: 0.276408  [108800/175341]\n",
      "loss: 0.502563  [110400/175341]\n",
      "loss: 0.531113  [112000/175341]\n",
      "loss: 0.843550  [113600/175341]\n",
      "loss: 0.944781  [115200/175341]\n",
      "loss: 0.877954  [116800/175341]\n",
      "loss: 0.580064  [118400/175341]\n",
      "loss: 0.454371  [120000/175341]\n",
      "loss: 0.341152  [121600/175341]\n",
      "loss: 0.612365  [123200/175341]\n",
      "loss: 0.344828  [124800/175341]\n",
      "loss: 0.676940  [126400/175341]\n",
      "loss: 1.034575  [128000/175341]\n",
      "loss: 0.600769  [129600/175341]\n",
      "loss: 0.411765  [131200/175341]\n",
      "loss: 0.206730  [132800/175341]\n",
      "loss: 0.484588  [134400/175341]\n",
      "loss: 0.108124  [136000/175341]\n",
      "loss: 0.484375  [137600/175341]\n",
      "loss: 0.064738  [139200/175341]\n",
      "loss: 0.362796  [140800/175341]\n",
      "loss: 0.487418  [142400/175341]\n",
      "loss: 0.189659  [144000/175341]\n",
      "loss: 0.660262  [145600/175341]\n",
      "loss: 0.349364  [147200/175341]\n",
      "loss: 0.115386  [148800/175341]\n",
      "loss: 0.260529  [150400/175341]\n",
      "loss: 0.375880  [152000/175341]\n",
      "loss: 0.300737  [153600/175341]\n",
      "loss: 0.463700  [155200/175341]\n",
      "loss: 0.441585  [156800/175341]\n",
      "loss: 0.272369  [158400/175341]\n",
      "loss: 0.266082  [160000/175341]\n",
      "loss: 0.791286  [161600/175341]\n",
      "loss: 0.170296  [163200/175341]\n",
      "loss: 0.305003  [164800/175341]\n",
      "loss: 0.536046  [166400/175341]\n",
      "loss: 0.620028  [168000/175341]\n",
      "loss: 0.391326  [169600/175341]\n",
      "loss: 0.597080  [171200/175341]\n",
      "loss: 0.541016  [172800/175341]\n",
      "loss: 0.301427  [174400/175341]\n",
      "Train Accuracy: 81.3318%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.558570, F1-score: 76.04%, Macro_F1-Score:  40.19%  \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.326406  [    0/175341]\n",
      "loss: 0.629479  [ 1600/175341]\n",
      "loss: 0.538570  [ 3200/175341]\n",
      "loss: 0.357149  [ 4800/175341]\n",
      "loss: 0.697423  [ 6400/175341]\n",
      "loss: 0.277910  [ 8000/175341]\n",
      "loss: 0.523185  [ 9600/175341]\n",
      "loss: 0.191304  [11200/175341]\n",
      "loss: 0.307076  [12800/175341]\n",
      "loss: 0.456212  [14400/175341]\n",
      "loss: 0.444235  [16000/175341]\n",
      "loss: 0.269287  [17600/175341]\n",
      "loss: 0.630252  [19200/175341]\n",
      "loss: 0.726004  [20800/175341]\n",
      "loss: 0.615924  [22400/175341]\n",
      "loss: 0.952424  [24000/175341]\n",
      "loss: 0.378699  [25600/175341]\n",
      "loss: 0.419035  [27200/175341]\n",
      "loss: 0.850473  [28800/175341]\n",
      "loss: 0.126138  [30400/175341]\n",
      "loss: 0.169487  [32000/175341]\n",
      "loss: 0.577243  [33600/175341]\n",
      "loss: 0.583458  [35200/175341]\n",
      "loss: 0.615315  [36800/175341]\n",
      "loss: 0.898325  [38400/175341]\n",
      "loss: 0.276218  [40000/175341]\n",
      "loss: 0.338150  [41600/175341]\n",
      "loss: 0.506629  [43200/175341]\n",
      "loss: 0.498127  [44800/175341]\n",
      "loss: 0.416041  [46400/175341]\n",
      "loss: 0.505768  [48000/175341]\n",
      "loss: 0.362760  [49600/175341]\n",
      "loss: 0.446811  [51200/175341]\n",
      "loss: 0.607836  [52800/175341]\n",
      "loss: 0.512956  [54400/175341]\n",
      "loss: 0.634244  [56000/175341]\n",
      "loss: 0.662497  [57600/175341]\n",
      "loss: 0.172899  [59200/175341]\n",
      "loss: 0.502522  [60800/175341]\n",
      "loss: 0.407912  [62400/175341]\n",
      "loss: 0.437861  [64000/175341]\n",
      "loss: 0.234472  [65600/175341]\n",
      "loss: 0.538373  [67200/175341]\n",
      "loss: 0.413941  [68800/175341]\n",
      "loss: 0.392480  [70400/175341]\n",
      "loss: 0.331164  [72000/175341]\n",
      "loss: 0.488284  [73600/175341]\n",
      "loss: 0.974922  [75200/175341]\n",
      "loss: 0.522983  [76800/175341]\n",
      "loss: 0.692845  [78400/175341]\n",
      "loss: 0.325256  [80000/175341]\n",
      "loss: 0.093792  [81600/175341]\n",
      "loss: 0.164109  [83200/175341]\n",
      "loss: 0.398259  [84800/175341]\n",
      "loss: 0.200168  [86400/175341]\n",
      "loss: 0.305634  [88000/175341]\n",
      "loss: 0.414067  [89600/175341]\n",
      "loss: 0.782358  [91200/175341]\n",
      "loss: 0.223453  [92800/175341]\n",
      "loss: 0.470043  [94400/175341]\n",
      "loss: 0.423824  [96000/175341]\n",
      "loss: 0.385630  [97600/175341]\n",
      "loss: 0.161669  [99200/175341]\n",
      "loss: 0.456459  [100800/175341]\n",
      "loss: 0.538866  [102400/175341]\n",
      "loss: 0.713488  [104000/175341]\n",
      "loss: 0.240668  [105600/175341]\n",
      "loss: 0.460117  [107200/175341]\n",
      "loss: 0.378848  [108800/175341]\n",
      "loss: 0.231687  [110400/175341]\n",
      "loss: 0.340878  [112000/175341]\n",
      "loss: 0.399162  [113600/175341]\n",
      "loss: 0.761272  [115200/175341]\n",
      "loss: 0.308762  [116800/175341]\n",
      "loss: 0.206405  [118400/175341]\n",
      "loss: 0.197832  [120000/175341]\n",
      "loss: 0.608031  [121600/175341]\n",
      "loss: 0.762525  [123200/175341]\n",
      "loss: 0.239382  [124800/175341]\n",
      "loss: 0.851646  [126400/175341]\n",
      "loss: 0.295070  [128000/175341]\n",
      "loss: 0.334912  [129600/175341]\n",
      "loss: 0.276643  [131200/175341]\n",
      "loss: 0.579842  [132800/175341]\n",
      "loss: 0.407960  [134400/175341]\n",
      "loss: 0.537944  [136000/175341]\n",
      "loss: 0.299715  [137600/175341]\n",
      "loss: 0.333244  [139200/175341]\n",
      "loss: 0.404965  [140800/175341]\n",
      "loss: 0.465161  [142400/175341]\n",
      "loss: 0.415663  [144000/175341]\n",
      "loss: 0.373710  [145600/175341]\n",
      "loss: 0.277851  [147200/175341]\n",
      "loss: 0.151386  [148800/175341]\n",
      "loss: 0.505618  [150400/175341]\n",
      "loss: 0.231293  [152000/175341]\n",
      "loss: 0.427550  [153600/175341]\n",
      "loss: 0.442086  [155200/175341]\n",
      "loss: 0.711661  [156800/175341]\n",
      "loss: 0.246059  [158400/175341]\n",
      "loss: 0.376556  [160000/175341]\n",
      "loss: 0.542829  [161600/175341]\n",
      "loss: 0.092113  [163200/175341]\n",
      "loss: 0.855002  [164800/175341]\n",
      "loss: 0.674431  [166400/175341]\n",
      "loss: 1.013807  [168000/175341]\n",
      "loss: 0.187056  [169600/175341]\n",
      "loss: 0.211979  [171200/175341]\n",
      "loss: 0.769635  [172800/175341]\n",
      "loss: 0.405619  [174400/175341]\n",
      "Train Accuracy: 81.3386%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.561614, F1-score: 76.09%, Macro_F1-Score:  41.19%  \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.716679  [    0/175341]\n",
      "loss: 0.256425  [ 1600/175341]\n",
      "loss: 0.040916  [ 3200/175341]\n",
      "loss: 0.134024  [ 4800/175341]\n",
      "loss: 0.603294  [ 6400/175341]\n",
      "loss: 0.939946  [ 8000/175341]\n",
      "loss: 0.430709  [ 9600/175341]\n",
      "loss: 0.833840  [11200/175341]\n",
      "loss: 0.282910  [12800/175341]\n",
      "loss: 0.314308  [14400/175341]\n",
      "loss: 0.140813  [16000/175341]\n",
      "loss: 0.726150  [17600/175341]\n",
      "loss: 0.614570  [19200/175341]\n",
      "loss: 0.359811  [20800/175341]\n",
      "loss: 0.663124  [22400/175341]\n",
      "loss: 0.501114  [24000/175341]\n",
      "loss: 0.223444  [25600/175341]\n",
      "loss: 0.390392  [27200/175341]\n",
      "loss: 0.307180  [28800/175341]\n",
      "loss: 0.379348  [30400/175341]\n",
      "loss: 0.536684  [32000/175341]\n",
      "loss: 0.781679  [33600/175341]\n",
      "loss: 0.378763  [35200/175341]\n",
      "loss: 0.373675  [36800/175341]\n",
      "loss: 0.956573  [38400/175341]\n",
      "loss: 0.470960  [40000/175341]\n",
      "loss: 0.325578  [41600/175341]\n",
      "loss: 0.246805  [43200/175341]\n",
      "loss: 0.351658  [44800/175341]\n",
      "loss: 0.823628  [46400/175341]\n",
      "loss: 1.023742  [48000/175341]\n",
      "loss: 0.347136  [49600/175341]\n",
      "loss: 0.447575  [51200/175341]\n",
      "loss: 0.311590  [52800/175341]\n",
      "loss: 0.734514  [54400/175341]\n",
      "loss: 0.894207  [56000/175341]\n",
      "loss: 0.329994  [57600/175341]\n",
      "loss: 0.332373  [59200/175341]\n",
      "loss: 0.301475  [60800/175341]\n",
      "loss: 0.275668  [62400/175341]\n",
      "loss: 0.338140  [64000/175341]\n",
      "loss: 0.242070  [65600/175341]\n",
      "loss: 0.596208  [67200/175341]\n",
      "loss: 0.783414  [68800/175341]\n",
      "loss: 0.231683  [70400/175341]\n",
      "loss: 0.334154  [72000/175341]\n",
      "loss: 0.621762  [73600/175341]\n",
      "loss: 0.766910  [75200/175341]\n",
      "loss: 0.486899  [76800/175341]\n",
      "loss: 0.258969  [78400/175341]\n",
      "loss: 0.848248  [80000/175341]\n",
      "loss: 0.295303  [81600/175341]\n",
      "loss: 0.249218  [83200/175341]\n",
      "loss: 0.385773  [84800/175341]\n",
      "loss: 0.527142  [86400/175341]\n",
      "loss: 0.549866  [88000/175341]\n",
      "loss: 0.141592  [89600/175341]\n",
      "loss: 0.581411  [91200/175341]\n",
      "loss: 0.176978  [92800/175341]\n",
      "loss: 1.292687  [94400/175341]\n",
      "loss: 0.534817  [96000/175341]\n",
      "loss: 0.663841  [97600/175341]\n",
      "loss: 0.308845  [99200/175341]\n",
      "loss: 0.364628  [100800/175341]\n",
      "loss: 0.339366  [102400/175341]\n",
      "loss: 0.341529  [104000/175341]\n",
      "loss: 0.288048  [105600/175341]\n",
      "loss: 0.553511  [107200/175341]\n",
      "loss: 0.188750  [108800/175341]\n",
      "loss: 0.722347  [110400/175341]\n",
      "loss: 0.341751  [112000/175341]\n",
      "loss: 0.455983  [113600/175341]\n",
      "loss: 0.184154  [115200/175341]\n",
      "loss: 0.431624  [116800/175341]\n",
      "loss: 0.312344  [118400/175341]\n",
      "loss: 0.442191  [120000/175341]\n",
      "loss: 0.267044  [121600/175341]\n",
      "loss: 0.222331  [123200/175341]\n",
      "loss: 0.665873  [124800/175341]\n",
      "loss: 0.431838  [126400/175341]\n",
      "loss: 0.132417  [128000/175341]\n",
      "loss: 0.552451  [129600/175341]\n",
      "loss: 0.508251  [131200/175341]\n",
      "loss: 0.319314  [132800/175341]\n",
      "loss: 0.751227  [134400/175341]\n",
      "loss: 0.611116  [136000/175341]\n",
      "loss: 0.332597  [137600/175341]\n",
      "loss: 0.630589  [139200/175341]\n",
      "loss: 0.262954  [140800/175341]\n",
      "loss: 0.163089  [142400/175341]\n",
      "loss: 0.377233  [144000/175341]\n",
      "loss: 0.190228  [145600/175341]\n",
      "loss: 0.321829  [147200/175341]\n",
      "loss: 0.399961  [148800/175341]\n",
      "loss: 0.187914  [150400/175341]\n",
      "loss: 0.449886  [152000/175341]\n",
      "loss: 0.714635  [153600/175341]\n",
      "loss: 0.511220  [155200/175341]\n",
      "loss: 0.459804  [156800/175341]\n",
      "loss: 0.382831  [158400/175341]\n",
      "loss: 1.230520  [160000/175341]\n",
      "loss: 0.677197  [161600/175341]\n",
      "loss: 0.271522  [163200/175341]\n",
      "loss: 0.468793  [164800/175341]\n",
      "loss: 0.544495  [166400/175341]\n",
      "loss: 0.490927  [168000/175341]\n",
      "loss: 0.496214  [169600/175341]\n",
      "loss: 0.227940  [171200/175341]\n",
      "loss: 0.580517  [172800/175341]\n",
      "loss: 0.686644  [174400/175341]\n",
      "Train Accuracy: 81.3694%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.540364, F1-score: 77.30%, Macro_F1-Score:  41.43%  \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.432371  [    0/175341]\n",
      "loss: 0.320857  [ 1600/175341]\n",
      "loss: 0.520829  [ 3200/175341]\n",
      "loss: 0.413797  [ 4800/175341]\n",
      "loss: 0.437242  [ 6400/175341]\n",
      "loss: 0.485290  [ 8000/175341]\n",
      "loss: 0.450805  [ 9600/175341]\n",
      "loss: 0.588017  [11200/175341]\n",
      "loss: 0.535629  [12800/175341]\n",
      "loss: 0.475640  [14400/175341]\n",
      "loss: 0.146984  [16000/175341]\n",
      "loss: 0.392948  [17600/175341]\n",
      "loss: 0.283332  [19200/175341]\n",
      "loss: 0.229489  [20800/175341]\n",
      "loss: 0.437135  [22400/175341]\n",
      "loss: 0.276978  [24000/175341]\n",
      "loss: 0.359763  [25600/175341]\n",
      "loss: 0.418853  [27200/175341]\n",
      "loss: 0.521474  [28800/175341]\n",
      "loss: 0.216020  [30400/175341]\n",
      "loss: 0.556677  [32000/175341]\n",
      "loss: 0.448115  [33600/175341]\n",
      "loss: 0.493227  [35200/175341]\n",
      "loss: 0.488212  [36800/175341]\n",
      "loss: 0.252436  [38400/175341]\n",
      "loss: 0.092752  [40000/175341]\n",
      "loss: 0.282418  [41600/175341]\n",
      "loss: 0.338704  [43200/175341]\n",
      "loss: 0.156164  [44800/175341]\n",
      "loss: 0.445705  [46400/175341]\n",
      "loss: 0.577363  [48000/175341]\n",
      "loss: 0.563999  [49600/175341]\n",
      "loss: 0.431675  [51200/175341]\n",
      "loss: 0.607226  [52800/175341]\n",
      "loss: 0.588417  [54400/175341]\n",
      "loss: 0.331319  [56000/175341]\n",
      "loss: 0.341915  [57600/175341]\n",
      "loss: 0.422969  [59200/175341]\n",
      "loss: 0.147078  [60800/175341]\n",
      "loss: 0.081827  [62400/175341]\n",
      "loss: 0.248209  [64000/175341]\n",
      "loss: 0.304097  [65600/175341]\n",
      "loss: 0.648874  [67200/175341]\n",
      "loss: 0.536986  [68800/175341]\n",
      "loss: 0.658950  [70400/175341]\n",
      "loss: 0.176357  [72000/175341]\n",
      "loss: 0.502760  [73600/175341]\n",
      "loss: 0.848899  [75200/175341]\n",
      "loss: 0.713192  [76800/175341]\n",
      "loss: 0.310122  [78400/175341]\n",
      "loss: 0.383585  [80000/175341]\n",
      "loss: 0.355795  [81600/175341]\n",
      "loss: 0.479584  [83200/175341]\n",
      "loss: 0.156956  [84800/175341]\n",
      "loss: 0.641058  [86400/175341]\n",
      "loss: 0.358343  [88000/175341]\n",
      "loss: 0.360954  [89600/175341]\n",
      "loss: 0.354515  [91200/175341]\n",
      "loss: 0.727540  [92800/175341]\n",
      "loss: 0.092398  [94400/175341]\n",
      "loss: 0.501251  [96000/175341]\n",
      "loss: 0.580906  [97600/175341]\n",
      "loss: 0.375382  [99200/175341]\n",
      "loss: 0.353270  [100800/175341]\n",
      "loss: 0.510967  [102400/175341]\n",
      "loss: 0.448869  [104000/175341]\n",
      "loss: 0.295413  [105600/175341]\n",
      "loss: 0.423730  [107200/175341]\n",
      "loss: 0.483857  [108800/175341]\n",
      "loss: 0.418111  [110400/175341]\n",
      "loss: 0.765268  [112000/175341]\n",
      "loss: 0.534156  [113600/175341]\n",
      "loss: 0.382061  [115200/175341]\n",
      "loss: 0.276242  [116800/175341]\n",
      "loss: 0.850656  [118400/175341]\n",
      "loss: 0.772725  [120000/175341]\n",
      "loss: 0.266451  [121600/175341]\n",
      "loss: 0.311108  [123200/175341]\n",
      "loss: 0.933694  [124800/175341]\n",
      "loss: 0.459693  [126400/175341]\n",
      "loss: 0.592597  [128000/175341]\n",
      "loss: 0.196007  [129600/175341]\n",
      "loss: 1.048833  [131200/175341]\n",
      "loss: 0.239064  [132800/175341]\n",
      "loss: 0.627382  [134400/175341]\n",
      "loss: 0.421426  [136000/175341]\n",
      "loss: 0.111078  [137600/175341]\n",
      "loss: 0.614441  [139200/175341]\n",
      "loss: 0.474909  [140800/175341]\n",
      "loss: 0.727445  [142400/175341]\n",
      "loss: 0.587429  [144000/175341]\n",
      "loss: 0.239957  [145600/175341]\n",
      "loss: 0.333198  [147200/175341]\n",
      "loss: 0.310190  [148800/175341]\n",
      "loss: 0.277432  [150400/175341]\n",
      "loss: 0.326781  [152000/175341]\n",
      "loss: 0.207673  [153600/175341]\n",
      "loss: 0.149329  [155200/175341]\n",
      "loss: 0.253059  [156800/175341]\n",
      "loss: 0.674298  [158400/175341]\n",
      "loss: 0.722602  [160000/175341]\n",
      "loss: 0.271944  [161600/175341]\n",
      "loss: 0.290136  [163200/175341]\n",
      "loss: 0.559206  [164800/175341]\n",
      "loss: 0.415368  [166400/175341]\n",
      "loss: 0.925283  [168000/175341]\n",
      "loss: 0.206826  [169600/175341]\n",
      "loss: 0.421025  [171200/175341]\n",
      "loss: 0.307287  [172800/175341]\n",
      "loss: 0.538100  [174400/175341]\n",
      "Train Accuracy: 81.3426%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.555764, F1-score: 75.51%, Macro_F1-Score:  40.43%  \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.461572  [    0/175341]\n",
      "loss: 0.286610  [ 1600/175341]\n",
      "loss: 0.366195  [ 3200/175341]\n",
      "loss: 0.306022  [ 4800/175341]\n",
      "loss: 0.287003  [ 6400/175341]\n",
      "loss: 0.518035  [ 8000/175341]\n",
      "loss: 0.451716  [ 9600/175341]\n",
      "loss: 0.349581  [11200/175341]\n",
      "loss: 0.167703  [12800/175341]\n",
      "loss: 0.748888  [14400/175341]\n",
      "loss: 0.405742  [16000/175341]\n",
      "loss: 0.476579  [17600/175341]\n",
      "loss: 0.487492  [19200/175341]\n",
      "loss: 0.488056  [20800/175341]\n",
      "loss: 0.465187  [22400/175341]\n",
      "loss: 0.330278  [24000/175341]\n",
      "loss: 0.092116  [25600/175341]\n",
      "loss: 0.260548  [27200/175341]\n",
      "loss: 0.206014  [28800/175341]\n",
      "loss: 0.277036  [30400/175341]\n",
      "loss: 0.303491  [32000/175341]\n",
      "loss: 0.279440  [33600/175341]\n",
      "loss: 0.297159  [35200/175341]\n",
      "loss: 0.284054  [36800/175341]\n",
      "loss: 0.304075  [38400/175341]\n",
      "loss: 0.369438  [40000/175341]\n",
      "loss: 0.295611  [41600/175341]\n",
      "loss: 0.343410  [43200/175341]\n",
      "loss: 0.651418  [44800/175341]\n",
      "loss: 0.202695  [46400/175341]\n",
      "loss: 0.452657  [48000/175341]\n",
      "loss: 0.543563  [49600/175341]\n",
      "loss: 0.311528  [51200/175341]\n",
      "loss: 0.290932  [52800/175341]\n",
      "loss: 0.428778  [54400/175341]\n",
      "loss: 0.683030  [56000/175341]\n",
      "loss: 0.674682  [57600/175341]\n",
      "loss: 0.526807  [59200/175341]\n",
      "loss: 0.259998  [60800/175341]\n",
      "loss: 0.291391  [62400/175341]\n",
      "loss: 0.486175  [64000/175341]\n",
      "loss: 0.088589  [65600/175341]\n",
      "loss: 0.368702  [67200/175341]\n",
      "loss: 0.664557  [68800/175341]\n",
      "loss: 0.299020  [70400/175341]\n",
      "loss: 0.639173  [72000/175341]\n",
      "loss: 0.230024  [73600/175341]\n",
      "loss: 0.359053  [75200/175341]\n",
      "loss: 0.321832  [76800/175341]\n",
      "loss: 0.267362  [78400/175341]\n",
      "loss: 0.299796  [80000/175341]\n",
      "loss: 0.997338  [81600/175341]\n",
      "loss: 0.225798  [83200/175341]\n",
      "loss: 0.470937  [84800/175341]\n",
      "loss: 0.345875  [86400/175341]\n",
      "loss: 0.313918  [88000/175341]\n",
      "loss: 1.094540  [89600/175341]\n",
      "loss: 0.381115  [91200/175341]\n",
      "loss: 0.179082  [92800/175341]\n",
      "loss: 0.410739  [94400/175341]\n",
      "loss: 0.755802  [96000/175341]\n",
      "loss: 0.657458  [97600/175341]\n",
      "loss: 0.358049  [99200/175341]\n",
      "loss: 0.291543  [100800/175341]\n",
      "loss: 0.359210  [102400/175341]\n",
      "loss: 0.355007  [104000/175341]\n",
      "loss: 0.365844  [105600/175341]\n",
      "loss: 0.342307  [107200/175341]\n",
      "loss: 0.467227  [108800/175341]\n",
      "loss: 0.265806  [110400/175341]\n",
      "loss: 0.338813  [112000/175341]\n",
      "loss: 0.698246  [113600/175341]\n",
      "loss: 0.306326  [115200/175341]\n",
      "loss: 0.469658  [116800/175341]\n",
      "loss: 0.459307  [118400/175341]\n",
      "loss: 0.849315  [120000/175341]\n",
      "loss: 0.239916  [121600/175341]\n",
      "loss: 0.517282  [123200/175341]\n",
      "loss: 0.192898  [124800/175341]\n",
      "loss: 0.529666  [126400/175341]\n",
      "loss: 0.886313  [128000/175341]\n",
      "loss: 0.416321  [129600/175341]\n",
      "loss: 0.381794  [131200/175341]\n",
      "loss: 0.220587  [132800/175341]\n",
      "loss: 0.346412  [134400/175341]\n",
      "loss: 0.460285  [136000/175341]\n",
      "loss: 0.616255  [137600/175341]\n",
      "loss: 0.512307  [139200/175341]\n",
      "loss: 0.075437  [140800/175341]\n",
      "loss: 0.649017  [142400/175341]\n",
      "loss: 0.470088  [144000/175341]\n",
      "loss: 0.422593  [145600/175341]\n",
      "loss: 0.123128  [147200/175341]\n",
      "loss: 0.182860  [148800/175341]\n",
      "loss: 0.644360  [150400/175341]\n",
      "loss: 0.261090  [152000/175341]\n",
      "loss: 0.388668  [153600/175341]\n",
      "loss: 0.252307  [155200/175341]\n",
      "loss: 0.249780  [156800/175341]\n",
      "loss: 0.371715  [158400/175341]\n",
      "loss: 0.718554  [160000/175341]\n",
      "loss: 0.459396  [161600/175341]\n",
      "loss: 0.398675  [163200/175341]\n",
      "loss: 0.443211  [164800/175341]\n",
      "loss: 0.313196  [166400/175341]\n",
      "loss: 0.457484  [168000/175341]\n",
      "loss: 0.549262  [169600/175341]\n",
      "loss: 0.418750  [171200/175341]\n",
      "loss: 0.742344  [172800/175341]\n",
      "loss: 0.484180  [174400/175341]\n",
      "Train Accuracy: 81.3917%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.555054, F1-score: 76.47%, Macro_F1-Score:  40.27%  \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.634809  [    0/175341]\n",
      "loss: 0.634302  [ 1600/175341]\n",
      "loss: 0.302498  [ 3200/175341]\n",
      "loss: 0.208395  [ 4800/175341]\n",
      "loss: 0.429098  [ 6400/175341]\n",
      "loss: 0.399085  [ 8000/175341]\n",
      "loss: 0.711168  [ 9600/175341]\n",
      "loss: 0.316849  [11200/175341]\n",
      "loss: 0.545415  [12800/175341]\n",
      "loss: 0.951790  [14400/175341]\n",
      "loss: 1.005089  [16000/175341]\n",
      "loss: 0.542267  [17600/175341]\n",
      "loss: 0.342614  [19200/175341]\n",
      "loss: 0.464133  [20800/175341]\n",
      "loss: 0.555536  [22400/175341]\n",
      "loss: 0.485530  [24000/175341]\n",
      "loss: 0.348041  [25600/175341]\n",
      "loss: 0.383689  [27200/175341]\n",
      "loss: 0.410597  [28800/175341]\n",
      "loss: 0.317306  [30400/175341]\n",
      "loss: 0.647874  [32000/175341]\n",
      "loss: 0.290110  [33600/175341]\n",
      "loss: 0.414678  [35200/175341]\n",
      "loss: 0.727268  [36800/175341]\n",
      "loss: 0.064460  [38400/175341]\n",
      "loss: 0.658091  [40000/175341]\n",
      "loss: 0.247737  [41600/175341]\n",
      "loss: 0.534727  [43200/175341]\n",
      "loss: 0.275147  [44800/175341]\n",
      "loss: 0.354061  [46400/175341]\n",
      "loss: 0.521067  [48000/175341]\n",
      "loss: 0.521426  [49600/175341]\n",
      "loss: 0.442924  [51200/175341]\n",
      "loss: 0.436129  [52800/175341]\n",
      "loss: 0.584263  [54400/175341]\n",
      "loss: 0.540844  [56000/175341]\n",
      "loss: 0.319576  [57600/175341]\n",
      "loss: 0.794365  [59200/175341]\n",
      "loss: 0.389442  [60800/175341]\n",
      "loss: 0.442894  [62400/175341]\n",
      "loss: 0.519909  [64000/175341]\n",
      "loss: 0.736899  [65600/175341]\n",
      "loss: 0.418745  [67200/175341]\n",
      "loss: 0.547566  [68800/175341]\n",
      "loss: 0.530610  [70400/175341]\n",
      "loss: 0.390689  [72000/175341]\n",
      "loss: 0.684101  [73600/175341]\n",
      "loss: 0.282981  [75200/175341]\n",
      "loss: 0.760501  [76800/175341]\n",
      "loss: 0.645551  [78400/175341]\n",
      "loss: 0.441253  [80000/175341]\n",
      "loss: 0.991045  [81600/175341]\n",
      "loss: 0.731003  [83200/175341]\n",
      "loss: 0.340127  [84800/175341]\n",
      "loss: 0.686825  [86400/175341]\n",
      "loss: 0.835218  [88000/175341]\n",
      "loss: 0.703108  [89600/175341]\n",
      "loss: 0.846706  [91200/175341]\n",
      "loss: 0.505214  [92800/175341]\n",
      "loss: 0.174631  [94400/175341]\n",
      "loss: 0.563757  [96000/175341]\n",
      "loss: 0.387989  [97600/175341]\n",
      "loss: 0.203687  [99200/175341]\n",
      "loss: 0.993076  [100800/175341]\n",
      "loss: 0.831753  [102400/175341]\n",
      "loss: 0.527711  [104000/175341]\n",
      "loss: 0.546281  [105600/175341]\n",
      "loss: 0.242378  [107200/175341]\n",
      "loss: 0.610456  [108800/175341]\n",
      "loss: 0.607268  [110400/175341]\n",
      "loss: 0.396553  [112000/175341]\n",
      "loss: 0.417127  [113600/175341]\n",
      "loss: 0.541456  [115200/175341]\n",
      "loss: 0.650461  [116800/175341]\n",
      "loss: 0.144237  [118400/175341]\n",
      "loss: 0.528749  [120000/175341]\n",
      "loss: 0.388379  [121600/175341]\n",
      "loss: 0.846122  [123200/175341]\n",
      "loss: 0.492772  [124800/175341]\n",
      "loss: 0.247359  [126400/175341]\n",
      "loss: 0.397995  [128000/175341]\n",
      "loss: 0.510594  [129600/175341]\n",
      "loss: 0.765439  [131200/175341]\n",
      "loss: 0.249088  [132800/175341]\n",
      "loss: 0.309931  [134400/175341]\n",
      "loss: 0.260541  [136000/175341]\n",
      "loss: 0.665373  [137600/175341]\n",
      "loss: 0.430960  [139200/175341]\n",
      "loss: 0.289652  [140800/175341]\n",
      "loss: 0.292991  [142400/175341]\n",
      "loss: 0.519906  [144000/175341]\n",
      "loss: 0.427537  [145600/175341]\n",
      "loss: 0.561101  [147200/175341]\n",
      "loss: 0.265577  [148800/175341]\n",
      "loss: 0.732630  [150400/175341]\n",
      "loss: 0.393788  [152000/175341]\n",
      "loss: 0.285276  [153600/175341]\n",
      "loss: 0.300418  [155200/175341]\n",
      "loss: 0.776985  [156800/175341]\n",
      "loss: 0.447729  [158400/175341]\n",
      "loss: 0.488154  [160000/175341]\n",
      "loss: 0.712671  [161600/175341]\n",
      "loss: 0.289292  [163200/175341]\n",
      "loss: 0.525712  [164800/175341]\n",
      "loss: 0.409373  [166400/175341]\n",
      "loss: 0.602619  [168000/175341]\n",
      "loss: 0.641372  [169600/175341]\n",
      "loss: 0.813106  [171200/175341]\n",
      "loss: 0.200658  [172800/175341]\n",
      "loss: 0.669968  [174400/175341]\n",
      "Train Accuracy: 81.4339%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.553169, F1-score: 75.77%, Macro_F1-Score:  40.56%  \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.713482  [    0/175341]\n",
      "loss: 0.395950  [ 1600/175341]\n",
      "loss: 0.446912  [ 3200/175341]\n",
      "loss: 0.540146  [ 4800/175341]\n",
      "loss: 0.517560  [ 6400/175341]\n",
      "loss: 0.263528  [ 8000/175341]\n",
      "loss: 0.435662  [ 9600/175341]\n",
      "loss: 0.723858  [11200/175341]\n",
      "loss: 0.520949  [12800/175341]\n",
      "loss: 0.663670  [14400/175341]\n",
      "loss: 0.355998  [16000/175341]\n",
      "loss: 0.235380  [17600/175341]\n",
      "loss: 0.249747  [19200/175341]\n",
      "loss: 0.765151  [20800/175341]\n",
      "loss: 0.594071  [22400/175341]\n",
      "loss: 0.134762  [24000/175341]\n",
      "loss: 0.336714  [25600/175341]\n",
      "loss: 0.203101  [27200/175341]\n",
      "loss: 0.139748  [28800/175341]\n",
      "loss: 0.336424  [30400/175341]\n",
      "loss: 0.496648  [32000/175341]\n",
      "loss: 0.665711  [33600/175341]\n",
      "loss: 0.702604  [35200/175341]\n",
      "loss: 0.924819  [36800/175341]\n",
      "loss: 0.369494  [38400/175341]\n",
      "loss: 0.307903  [40000/175341]\n",
      "loss: 0.217002  [41600/175341]\n",
      "loss: 0.491114  [43200/175341]\n",
      "loss: 0.182123  [44800/175341]\n",
      "loss: 0.854010  [46400/175341]\n",
      "loss: 0.475830  [48000/175341]\n",
      "loss: 0.928008  [49600/175341]\n",
      "loss: 0.648605  [51200/175341]\n",
      "loss: 0.472634  [52800/175341]\n",
      "loss: 0.246165  [54400/175341]\n",
      "loss: 0.104574  [56000/175341]\n",
      "loss: 0.357957  [57600/175341]\n",
      "loss: 0.457603  [59200/175341]\n",
      "loss: 0.353953  [60800/175341]\n",
      "loss: 0.303543  [62400/175341]\n",
      "loss: 0.370673  [64000/175341]\n",
      "loss: 0.616141  [65600/175341]\n",
      "loss: 0.300310  [67200/175341]\n",
      "loss: 0.474972  [68800/175341]\n",
      "loss: 0.287538  [70400/175341]\n",
      "loss: 0.606419  [72000/175341]\n",
      "loss: 0.277831  [73600/175341]\n",
      "loss: 0.202847  [75200/175341]\n",
      "loss: 0.202429  [76800/175341]\n",
      "loss: 0.208883  [78400/175341]\n",
      "loss: 0.747513  [80000/175341]\n",
      "loss: 0.331406  [81600/175341]\n",
      "loss: 0.281237  [83200/175341]\n",
      "loss: 0.635986  [84800/175341]\n",
      "loss: 0.616486  [86400/175341]\n",
      "loss: 0.232450  [88000/175341]\n",
      "loss: 0.341087  [89600/175341]\n",
      "loss: 0.273606  [91200/175341]\n",
      "loss: 0.395639  [92800/175341]\n",
      "loss: 0.347122  [94400/175341]\n",
      "loss: 0.780965  [96000/175341]\n",
      "loss: 0.760271  [97600/175341]\n",
      "loss: 0.350632  [99200/175341]\n",
      "loss: 0.351237  [100800/175341]\n",
      "loss: 0.969188  [102400/175341]\n",
      "loss: 0.869545  [104000/175341]\n",
      "loss: 0.476417  [105600/175341]\n",
      "loss: 0.551587  [107200/175341]\n",
      "loss: 0.346507  [108800/175341]\n",
      "loss: 0.366711  [110400/175341]\n",
      "loss: 0.272518  [112000/175341]\n",
      "loss: 0.447174  [113600/175341]\n",
      "loss: 0.651984  [115200/175341]\n",
      "loss: 0.187853  [116800/175341]\n",
      "loss: 0.295824  [118400/175341]\n",
      "loss: 0.581496  [120000/175341]\n",
      "loss: 0.385301  [121600/175341]\n",
      "loss: 0.791457  [123200/175341]\n",
      "loss: 0.812286  [124800/175341]\n",
      "loss: 0.949929  [126400/175341]\n",
      "loss: 0.594787  [128000/175341]\n",
      "loss: 0.752721  [129600/175341]\n",
      "loss: 0.555784  [131200/175341]\n",
      "loss: 0.232436  [132800/175341]\n",
      "loss: 0.460601  [134400/175341]\n",
      "loss: 0.418039  [136000/175341]\n",
      "loss: 0.450803  [137600/175341]\n",
      "loss: 0.093325  [139200/175341]\n",
      "loss: 0.305796  [140800/175341]\n",
      "loss: 0.481908  [142400/175341]\n",
      "loss: 0.468067  [144000/175341]\n",
      "loss: 0.268151  [145600/175341]\n",
      "loss: 0.371671  [147200/175341]\n",
      "loss: 0.305397  [148800/175341]\n",
      "loss: 0.472626  [150400/175341]\n",
      "loss: 0.220734  [152000/175341]\n",
      "loss: 0.522708  [153600/175341]\n",
      "loss: 0.581452  [155200/175341]\n",
      "loss: 0.451840  [156800/175341]\n",
      "loss: 0.630812  [158400/175341]\n",
      "loss: 0.536878  [160000/175341]\n",
      "loss: 0.556676  [161600/175341]\n",
      "loss: 0.649465  [163200/175341]\n",
      "loss: 0.725163  [164800/175341]\n",
      "loss: 0.300164  [166400/175341]\n",
      "loss: 0.366822  [168000/175341]\n",
      "loss: 0.902946  [169600/175341]\n",
      "loss: 0.362130  [171200/175341]\n",
      "loss: 0.435558  [172800/175341]\n",
      "loss: 0.483973  [174400/175341]\n",
      "Train Accuracy: 81.3883%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.543418, F1-score: 76.90%, Macro_F1-Score:  40.56%  \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.544693  [    0/175341]\n",
      "loss: 0.617064  [ 1600/175341]\n",
      "loss: 0.272107  [ 3200/175341]\n",
      "loss: 0.460603  [ 4800/175341]\n",
      "loss: 0.399279  [ 6400/175341]\n",
      "loss: 0.508529  [ 8000/175341]\n",
      "loss: 0.827978  [ 9600/175341]\n",
      "loss: 0.230803  [11200/175341]\n",
      "loss: 0.542171  [12800/175341]\n",
      "loss: 0.502238  [14400/175341]\n",
      "loss: 0.302752  [16000/175341]\n",
      "loss: 0.319146  [17600/175341]\n",
      "loss: 0.742719  [19200/175341]\n",
      "loss: 0.495093  [20800/175341]\n",
      "loss: 0.484762  [22400/175341]\n",
      "loss: 0.576551  [24000/175341]\n",
      "loss: 0.332166  [25600/175341]\n",
      "loss: 0.331320  [27200/175341]\n",
      "loss: 0.555620  [28800/175341]\n",
      "loss: 0.369849  [30400/175341]\n",
      "loss: 0.429030  [32000/175341]\n",
      "loss: 0.287161  [33600/175341]\n",
      "loss: 0.196836  [35200/175341]\n",
      "loss: 0.319046  [36800/175341]\n",
      "loss: 0.133705  [38400/175341]\n",
      "loss: 0.502413  [40000/175341]\n",
      "loss: 0.630387  [41600/175341]\n",
      "loss: 0.619625  [43200/175341]\n",
      "loss: 0.378446  [44800/175341]\n",
      "loss: 0.354041  [46400/175341]\n",
      "loss: 0.253460  [48000/175341]\n",
      "loss: 0.328302  [49600/175341]\n",
      "loss: 0.756418  [51200/175341]\n",
      "loss: 0.717044  [52800/175341]\n",
      "loss: 0.602140  [54400/175341]\n",
      "loss: 0.388392  [56000/175341]\n",
      "loss: 0.456341  [57600/175341]\n",
      "loss: 0.487130  [59200/175341]\n",
      "loss: 0.203556  [60800/175341]\n",
      "loss: 0.380766  [62400/175341]\n",
      "loss: 0.416487  [64000/175341]\n",
      "loss: 0.487889  [65600/175341]\n",
      "loss: 0.621156  [67200/175341]\n",
      "loss: 0.116015  [68800/175341]\n",
      "loss: 0.215180  [70400/175341]\n",
      "loss: 0.692277  [72000/175341]\n",
      "loss: 0.693515  [73600/175341]\n",
      "loss: 0.167653  [75200/175341]\n",
      "loss: 0.504398  [76800/175341]\n",
      "loss: 0.572966  [78400/175341]\n",
      "loss: 0.900864  [80000/175341]\n",
      "loss: 0.252772  [81600/175341]\n",
      "loss: 0.272341  [83200/175341]\n",
      "loss: 0.448502  [84800/175341]\n",
      "loss: 0.543924  [86400/175341]\n",
      "loss: 0.526944  [88000/175341]\n",
      "loss: 0.406707  [89600/175341]\n",
      "loss: 0.483838  [91200/175341]\n",
      "loss: 0.188716  [92800/175341]\n",
      "loss: 0.219612  [94400/175341]\n",
      "loss: 0.899390  [96000/175341]\n",
      "loss: 0.433611  [97600/175341]\n",
      "loss: 0.771027  [99200/175341]\n",
      "loss: 0.605606  [100800/175341]\n",
      "loss: 0.179057  [102400/175341]\n",
      "loss: 0.586179  [104000/175341]\n",
      "loss: 0.097668  [105600/175341]\n",
      "loss: 0.488546  [107200/175341]\n",
      "loss: 0.809663  [108800/175341]\n",
      "loss: 0.577754  [110400/175341]\n",
      "loss: 0.355509  [112000/175341]\n",
      "loss: 0.474866  [113600/175341]\n",
      "loss: 0.452124  [115200/175341]\n",
      "loss: 0.436323  [116800/175341]\n",
      "loss: 0.114878  [118400/175341]\n",
      "loss: 0.456117  [120000/175341]\n",
      "loss: 0.631143  [121600/175341]\n",
      "loss: 0.446188  [123200/175341]\n",
      "loss: 0.621825  [124800/175341]\n",
      "loss: 0.524832  [126400/175341]\n",
      "loss: 0.486284  [128000/175341]\n",
      "loss: 0.585595  [129600/175341]\n",
      "loss: 0.503393  [131200/175341]\n",
      "loss: 0.568123  [132800/175341]\n",
      "loss: 0.478245  [134400/175341]\n",
      "loss: 0.517487  [136000/175341]\n",
      "loss: 0.637068  [137600/175341]\n",
      "loss: 0.455114  [139200/175341]\n",
      "loss: 0.183518  [140800/175341]\n",
      "loss: 0.051233  [142400/175341]\n",
      "loss: 0.218926  [144000/175341]\n",
      "loss: 0.352528  [145600/175341]\n",
      "loss: 0.275477  [147200/175341]\n",
      "loss: 0.343367  [148800/175341]\n",
      "loss: 0.404925  [150400/175341]\n",
      "loss: 0.513529  [152000/175341]\n",
      "loss: 0.345500  [153600/175341]\n",
      "loss: 0.628876  [155200/175341]\n",
      "loss: 0.209109  [156800/175341]\n",
      "loss: 0.447871  [158400/175341]\n",
      "loss: 0.533705  [160000/175341]\n",
      "loss: 0.372519  [161600/175341]\n",
      "loss: 0.355033  [163200/175341]\n",
      "loss: 0.665059  [164800/175341]\n",
      "loss: 0.387611  [166400/175341]\n",
      "loss: 0.384414  [168000/175341]\n",
      "loss: 0.417862  [169600/175341]\n",
      "loss: 0.551745  [171200/175341]\n",
      "loss: 0.827890  [172800/175341]\n",
      "loss: 0.449517  [174400/175341]\n",
      "Train Accuracy: 81.4185%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.564260, F1-score: 75.02%, Macro_F1-Score:  40.14%  \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.443837  [    0/175341]\n",
      "loss: 0.763907  [ 1600/175341]\n",
      "loss: 0.031686  [ 3200/175341]\n",
      "loss: 0.386420  [ 4800/175341]\n",
      "loss: 0.344496  [ 6400/175341]\n",
      "loss: 0.528020  [ 8000/175341]\n",
      "loss: 0.184360  [ 9600/175341]\n",
      "loss: 0.839186  [11200/175341]\n",
      "loss: 0.590768  [12800/175341]\n",
      "loss: 0.382625  [14400/175341]\n",
      "loss: 0.774782  [16000/175341]\n",
      "loss: 0.467575  [17600/175341]\n",
      "loss: 0.530842  [19200/175341]\n",
      "loss: 0.623744  [20800/175341]\n",
      "loss: 0.352839  [22400/175341]\n",
      "loss: 0.121936  [24000/175341]\n",
      "loss: 0.390528  [25600/175341]\n",
      "loss: 0.451512  [27200/175341]\n",
      "loss: 0.295132  [28800/175341]\n",
      "loss: 0.778572  [30400/175341]\n",
      "loss: 0.629145  [32000/175341]\n",
      "loss: 0.569219  [33600/175341]\n",
      "loss: 0.284132  [35200/175341]\n",
      "loss: 0.737330  [36800/175341]\n",
      "loss: 0.462204  [38400/175341]\n",
      "loss: 0.439491  [40000/175341]\n",
      "loss: 0.169605  [41600/175341]\n",
      "loss: 0.348001  [43200/175341]\n",
      "loss: 0.297103  [44800/175341]\n",
      "loss: 0.310786  [46400/175341]\n",
      "loss: 0.562486  [48000/175341]\n",
      "loss: 0.171615  [49600/175341]\n",
      "loss: 0.279935  [51200/175341]\n",
      "loss: 0.764706  [52800/175341]\n",
      "loss: 0.226633  [54400/175341]\n",
      "loss: 0.424558  [56000/175341]\n",
      "loss: 0.564577  [57600/175341]\n",
      "loss: 0.504457  [59200/175341]\n",
      "loss: 0.593716  [60800/175341]\n",
      "loss: 0.240255  [62400/175341]\n",
      "loss: 0.841844  [64000/175341]\n",
      "loss: 0.392007  [65600/175341]\n",
      "loss: 0.421310  [67200/175341]\n",
      "loss: 0.311893  [68800/175341]\n",
      "loss: 0.271266  [70400/175341]\n",
      "loss: 0.486356  [72000/175341]\n",
      "loss: 0.373131  [73600/175341]\n",
      "loss: 0.413746  [75200/175341]\n",
      "loss: 0.514367  [76800/175341]\n",
      "loss: 0.563183  [78400/175341]\n",
      "loss: 1.111906  [80000/175341]\n",
      "loss: 0.522520  [81600/175341]\n",
      "loss: 0.630463  [83200/175341]\n",
      "loss: 0.505154  [84800/175341]\n",
      "loss: 0.479809  [86400/175341]\n",
      "loss: 0.697722  [88000/175341]\n",
      "loss: 0.439736  [89600/175341]\n",
      "loss: 0.605799  [91200/175341]\n",
      "loss: 0.419966  [92800/175341]\n",
      "loss: 0.476705  [94400/175341]\n",
      "loss: 0.618887  [96000/175341]\n",
      "loss: 0.553235  [97600/175341]\n",
      "loss: 0.352156  [99200/175341]\n",
      "loss: 0.506427  [100800/175341]\n",
      "loss: 0.602189  [102400/175341]\n",
      "loss: 0.404007  [104000/175341]\n",
      "loss: 0.476860  [105600/175341]\n",
      "loss: 0.188520  [107200/175341]\n",
      "loss: 0.046249  [108800/175341]\n",
      "loss: 0.401002  [110400/175341]\n",
      "loss: 0.727132  [112000/175341]\n",
      "loss: 0.493104  [113600/175341]\n",
      "loss: 0.630202  [115200/175341]\n",
      "loss: 0.312564  [116800/175341]\n",
      "loss: 0.348262  [118400/175341]\n",
      "loss: 0.422350  [120000/175341]\n",
      "loss: 0.292124  [121600/175341]\n",
      "loss: 1.052357  [123200/175341]\n",
      "loss: 0.719467  [124800/175341]\n",
      "loss: 0.327592  [126400/175341]\n",
      "loss: 0.539122  [128000/175341]\n",
      "loss: 0.367041  [129600/175341]\n",
      "loss: 0.790793  [131200/175341]\n",
      "loss: 0.087488  [132800/175341]\n",
      "loss: 0.519279  [134400/175341]\n",
      "loss: 0.645644  [136000/175341]\n",
      "loss: 0.591784  [137600/175341]\n",
      "loss: 0.341073  [139200/175341]\n",
      "loss: 0.652303  [140800/175341]\n",
      "loss: 0.385149  [142400/175341]\n",
      "loss: 0.471050  [144000/175341]\n",
      "loss: 0.131958  [145600/175341]\n",
      "loss: 0.200358  [147200/175341]\n",
      "loss: 0.343597  [148800/175341]\n",
      "loss: 0.431591  [150400/175341]\n",
      "loss: 0.605607  [152000/175341]\n",
      "loss: 0.705849  [153600/175341]\n",
      "loss: 0.294877  [155200/175341]\n",
      "loss: 0.648928  [156800/175341]\n",
      "loss: 0.392903  [158400/175341]\n",
      "loss: 0.383892  [160000/175341]\n",
      "loss: 0.356995  [161600/175341]\n",
      "loss: 0.747561  [163200/175341]\n",
      "loss: 0.742909  [164800/175341]\n",
      "loss: 0.320215  [166400/175341]\n",
      "loss: 0.186486  [168000/175341]\n",
      "loss: 0.756524  [169600/175341]\n",
      "loss: 0.170093  [171200/175341]\n",
      "loss: 0.128323  [172800/175341]\n",
      "loss: 0.550812  [174400/175341]\n",
      "Train Accuracy: 81.4407%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.552119, F1-score: 75.79%, Macro_F1-Score:  40.12%  \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.353526  [    0/175341]\n",
      "loss: 0.683852  [ 1600/175341]\n",
      "loss: 0.145080  [ 3200/175341]\n",
      "loss: 0.525397  [ 4800/175341]\n",
      "loss: 0.412142  [ 6400/175341]\n",
      "loss: 0.682499  [ 8000/175341]\n",
      "loss: 0.867059  [ 9600/175341]\n",
      "loss: 0.382402  [11200/175341]\n",
      "loss: 0.512278  [12800/175341]\n",
      "loss: 0.396307  [14400/175341]\n",
      "loss: 0.396627  [16000/175341]\n",
      "loss: 0.580288  [17600/175341]\n",
      "loss: 0.685859  [19200/175341]\n",
      "loss: 0.624809  [20800/175341]\n",
      "loss: 0.675310  [22400/175341]\n",
      "loss: 0.160374  [24000/175341]\n",
      "loss: 0.288376  [25600/175341]\n",
      "loss: 0.768443  [27200/175341]\n",
      "loss: 0.482160  [28800/175341]\n",
      "loss: 0.808685  [30400/175341]\n",
      "loss: 0.432492  [32000/175341]\n",
      "loss: 0.432538  [33600/175341]\n",
      "loss: 0.185339  [35200/175341]\n",
      "loss: 0.327709  [36800/175341]\n",
      "loss: 0.537333  [38400/175341]\n",
      "loss: 0.516361  [40000/175341]\n",
      "loss: 0.657843  [41600/175341]\n",
      "loss: 0.636574  [43200/175341]\n",
      "loss: 0.550090  [44800/175341]\n",
      "loss: 0.470864  [46400/175341]\n",
      "loss: 0.454747  [48000/175341]\n",
      "loss: 0.208650  [49600/175341]\n",
      "loss: 0.390124  [51200/175341]\n",
      "loss: 0.588141  [52800/175341]\n",
      "loss: 0.313036  [54400/175341]\n",
      "loss: 0.698948  [56000/175341]\n",
      "loss: 0.259480  [57600/175341]\n",
      "loss: 0.361055  [59200/175341]\n",
      "loss: 0.408444  [60800/175341]\n",
      "loss: 0.328224  [62400/175341]\n",
      "loss: 0.255232  [64000/175341]\n",
      "loss: 0.571794  [65600/175341]\n",
      "loss: 0.317100  [67200/175341]\n",
      "loss: 0.380534  [68800/175341]\n",
      "loss: 0.583498  [70400/175341]\n",
      "loss: 0.261871  [72000/175341]\n",
      "loss: 0.446226  [73600/175341]\n",
      "loss: 0.421690  [75200/175341]\n",
      "loss: 0.789887  [76800/175341]\n",
      "loss: 0.327165  [78400/175341]\n",
      "loss: 0.271179  [80000/175341]\n",
      "loss: 0.227081  [81600/175341]\n",
      "loss: 0.303426  [83200/175341]\n",
      "loss: 0.170836  [84800/175341]\n",
      "loss: 0.670051  [86400/175341]\n",
      "loss: 0.616894  [88000/175341]\n",
      "loss: 0.644349  [89600/175341]\n",
      "loss: 0.082824  [91200/175341]\n",
      "loss: 0.554892  [92800/175341]\n",
      "loss: 0.703716  [94400/175341]\n",
      "loss: 0.419093  [96000/175341]\n",
      "loss: 0.223346  [97600/175341]\n",
      "loss: 0.697024  [99200/175341]\n",
      "loss: 0.579836  [100800/175341]\n",
      "loss: 0.922530  [102400/175341]\n",
      "loss: 0.587126  [104000/175341]\n",
      "loss: 0.302465  [105600/175341]\n",
      "loss: 0.228998  [107200/175341]\n",
      "loss: 0.847496  [108800/175341]\n",
      "loss: 0.822957  [110400/175341]\n",
      "loss: 0.208372  [112000/175341]\n",
      "loss: 0.744880  [113600/175341]\n",
      "loss: 0.370162  [115200/175341]\n",
      "loss: 0.301656  [116800/175341]\n",
      "loss: 0.294614  [118400/175341]\n",
      "loss: 0.642880  [120000/175341]\n",
      "loss: 0.367513  [121600/175341]\n",
      "loss: 0.501098  [123200/175341]\n",
      "loss: 0.529535  [124800/175341]\n",
      "loss: 0.268047  [126400/175341]\n",
      "loss: 0.263776  [128000/175341]\n",
      "loss: 0.445207  [129600/175341]\n",
      "loss: 0.170072  [131200/175341]\n",
      "loss: 0.693205  [132800/175341]\n",
      "loss: 0.857978  [134400/175341]\n",
      "loss: 0.451422  [136000/175341]\n",
      "loss: 0.207166  [137600/175341]\n",
      "loss: 0.435555  [139200/175341]\n",
      "loss: 0.318350  [140800/175341]\n",
      "loss: 0.453218  [142400/175341]\n",
      "loss: 0.831670  [144000/175341]\n",
      "loss: 0.425470  [145600/175341]\n",
      "loss: 0.344534  [147200/175341]\n",
      "loss: 0.756399  [148800/175341]\n",
      "loss: 0.271156  [150400/175341]\n",
      "loss: 0.280774  [152000/175341]\n",
      "loss: 0.289090  [153600/175341]\n",
      "loss: 0.364120  [155200/175341]\n",
      "loss: 0.143294  [156800/175341]\n",
      "loss: 0.303706  [158400/175341]\n",
      "loss: 0.510143  [160000/175341]\n",
      "loss: 0.556326  [161600/175341]\n",
      "loss: 0.282125  [163200/175341]\n",
      "loss: 0.302643  [164800/175341]\n",
      "loss: 0.591049  [166400/175341]\n",
      "loss: 0.254644  [168000/175341]\n",
      "loss: 0.533996  [169600/175341]\n",
      "loss: 0.228693  [171200/175341]\n",
      "loss: 0.440634  [172800/175341]\n",
      "loss: 0.443419  [174400/175341]\n",
      "Train Accuracy: 81.4533%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.548408, F1-score: 76.43%, Macro_F1-Score:  40.50%  \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.257060  [    0/175341]\n",
      "loss: 0.624880  [ 1600/175341]\n",
      "loss: 0.541994  [ 3200/175341]\n",
      "loss: 0.382864  [ 4800/175341]\n",
      "loss: 0.289675  [ 6400/175341]\n",
      "loss: 0.404666  [ 8000/175341]\n",
      "loss: 0.743500  [ 9600/175341]\n",
      "loss: 0.112166  [11200/175341]\n",
      "loss: 0.334130  [12800/175341]\n",
      "loss: 0.326352  [14400/175341]\n",
      "loss: 0.223756  [16000/175341]\n",
      "loss: 1.246048  [17600/175341]\n",
      "loss: 0.294407  [19200/175341]\n",
      "loss: 0.533814  [20800/175341]\n",
      "loss: 0.324297  [22400/175341]\n",
      "loss: 0.155983  [24000/175341]\n",
      "loss: 0.284706  [25600/175341]\n",
      "loss: 0.200263  [27200/175341]\n",
      "loss: 0.632700  [28800/175341]\n",
      "loss: 0.540490  [30400/175341]\n",
      "loss: 0.395516  [32000/175341]\n",
      "loss: 0.391786  [33600/175341]\n",
      "loss: 0.439661  [35200/175341]\n",
      "loss: 0.556954  [36800/175341]\n",
      "loss: 0.657786  [38400/175341]\n",
      "loss: 0.279836  [40000/175341]\n",
      "loss: 0.436812  [41600/175341]\n",
      "loss: 0.555233  [43200/175341]\n",
      "loss: 0.483477  [44800/175341]\n",
      "loss: 0.288109  [46400/175341]\n",
      "loss: 0.575744  [48000/175341]\n",
      "loss: 0.180858  [49600/175341]\n",
      "loss: 0.242853  [51200/175341]\n",
      "loss: 0.300845  [52800/175341]\n",
      "loss: 0.444840  [54400/175341]\n",
      "loss: 0.715842  [56000/175341]\n",
      "loss: 0.690879  [57600/175341]\n",
      "loss: 0.489693  [59200/175341]\n",
      "loss: 0.945113  [60800/175341]\n",
      "loss: 0.762017  [62400/175341]\n",
      "loss: 0.410319  [64000/175341]\n",
      "loss: 0.448314  [65600/175341]\n",
      "loss: 0.332552  [67200/175341]\n",
      "loss: 0.527918  [68800/175341]\n",
      "loss: 0.540997  [70400/175341]\n",
      "loss: 0.574718  [72000/175341]\n",
      "loss: 0.116247  [73600/175341]\n",
      "loss: 0.762418  [75200/175341]\n",
      "loss: 0.785670  [76800/175341]\n",
      "loss: 0.292527  [78400/175341]\n",
      "loss: 0.103373  [80000/175341]\n",
      "loss: 0.298817  [81600/175341]\n",
      "loss: 0.470256  [83200/175341]\n",
      "loss: 0.587069  [84800/175341]\n",
      "loss: 0.568681  [86400/175341]\n",
      "loss: 0.297878  [88000/175341]\n",
      "loss: 0.162770  [89600/175341]\n",
      "loss: 0.355438  [91200/175341]\n",
      "loss: 0.749121  [92800/175341]\n",
      "loss: 0.570976  [94400/175341]\n",
      "loss: 0.318060  [96000/175341]\n",
      "loss: 0.468952  [97600/175341]\n",
      "loss: 0.511423  [99200/175341]\n",
      "loss: 0.279732  [100800/175341]\n",
      "loss: 0.448523  [102400/175341]\n",
      "loss: 0.290946  [104000/175341]\n",
      "loss: 0.230031  [105600/175341]\n",
      "loss: 0.260997  [107200/175341]\n",
      "loss: 0.431179  [108800/175341]\n",
      "loss: 0.596999  [110400/175341]\n",
      "loss: 0.833055  [112000/175341]\n",
      "loss: 0.164080  [113600/175341]\n",
      "loss: 0.512190  [115200/175341]\n",
      "loss: 0.559718  [116800/175341]\n",
      "loss: 0.235906  [118400/175341]\n",
      "loss: 0.456986  [120000/175341]\n",
      "loss: 1.080505  [121600/175341]\n",
      "loss: 0.278486  [123200/175341]\n",
      "loss: 0.474352  [124800/175341]\n",
      "loss: 0.424899  [126400/175341]\n",
      "loss: 0.598998  [128000/175341]\n",
      "loss: 0.960439  [129600/175341]\n",
      "loss: 0.355047  [131200/175341]\n",
      "loss: 0.275283  [132800/175341]\n",
      "loss: 0.339249  [134400/175341]\n",
      "loss: 0.309765  [136000/175341]\n",
      "loss: 0.482271  [137600/175341]\n",
      "loss: 0.309529  [139200/175341]\n",
      "loss: 0.345787  [140800/175341]\n",
      "loss: 0.386615  [142400/175341]\n",
      "loss: 0.438908  [144000/175341]\n",
      "loss: 0.259326  [145600/175341]\n",
      "loss: 0.316126  [147200/175341]\n",
      "loss: 0.641556  [148800/175341]\n",
      "loss: 0.425457  [150400/175341]\n",
      "loss: 0.447842  [152000/175341]\n",
      "loss: 0.335599  [153600/175341]\n",
      "loss: 0.377063  [155200/175341]\n",
      "loss: 0.308604  [156800/175341]\n",
      "loss: 0.484271  [158400/175341]\n",
      "loss: 0.491093  [160000/175341]\n",
      "loss: 0.480920  [161600/175341]\n",
      "loss: 0.260538  [163200/175341]\n",
      "loss: 0.390917  [164800/175341]\n",
      "loss: 0.480319  [166400/175341]\n",
      "loss: 0.448899  [168000/175341]\n",
      "loss: 0.334389  [169600/175341]\n",
      "loss: 0.657659  [171200/175341]\n",
      "loss: 0.228310  [172800/175341]\n",
      "loss: 0.364157  [174400/175341]\n",
      "Train Accuracy: 81.4493%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.549783, F1-score: 75.89%, Macro_F1-Score:  41.37%  \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.494362  [    0/175341]\n",
      "loss: 0.371046  [ 1600/175341]\n",
      "loss: 0.407770  [ 3200/175341]\n",
      "loss: 0.230932  [ 4800/175341]\n",
      "loss: 0.578632  [ 6400/175341]\n",
      "loss: 0.560237  [ 8000/175341]\n",
      "loss: 0.632191  [ 9600/175341]\n",
      "loss: 0.736699  [11200/175341]\n",
      "loss: 0.343191  [12800/175341]\n",
      "loss: 0.483431  [14400/175341]\n",
      "loss: 0.784746  [16000/175341]\n",
      "loss: 0.348768  [17600/175341]\n",
      "loss: 0.659794  [19200/175341]\n",
      "loss: 0.253206  [20800/175341]\n",
      "loss: 0.717677  [22400/175341]\n",
      "loss: 0.317825  [24000/175341]\n",
      "loss: 0.059582  [25600/175341]\n",
      "loss: 0.947302  [27200/175341]\n",
      "loss: 0.383878  [28800/175341]\n",
      "loss: 0.573756  [30400/175341]\n",
      "loss: 0.770421  [32000/175341]\n",
      "loss: 0.984516  [33600/175341]\n",
      "loss: 0.330719  [35200/175341]\n",
      "loss: 0.470127  [36800/175341]\n",
      "loss: 0.419596  [38400/175341]\n",
      "loss: 0.526419  [40000/175341]\n",
      "loss: 0.609910  [41600/175341]\n",
      "loss: 0.639881  [43200/175341]\n",
      "loss: 0.734487  [44800/175341]\n",
      "loss: 0.443667  [46400/175341]\n",
      "loss: 0.591514  [48000/175341]\n",
      "loss: 0.504269  [49600/175341]\n",
      "loss: 0.454094  [51200/175341]\n",
      "loss: 0.243677  [52800/175341]\n",
      "loss: 0.530678  [54400/175341]\n",
      "loss: 0.615642  [56000/175341]\n",
      "loss: 0.595433  [57600/175341]\n",
      "loss: 0.333567  [59200/175341]\n",
      "loss: 0.224816  [60800/175341]\n",
      "loss: 0.237774  [62400/175341]\n",
      "loss: 0.830895  [64000/175341]\n",
      "loss: 0.741892  [65600/175341]\n",
      "loss: 0.593443  [67200/175341]\n",
      "loss: 0.343400  [68800/175341]\n",
      "loss: 0.345618  [70400/175341]\n",
      "loss: 0.421024  [72000/175341]\n",
      "loss: 0.610915  [73600/175341]\n",
      "loss: 0.242399  [75200/175341]\n",
      "loss: 0.590100  [76800/175341]\n",
      "loss: 0.428539  [78400/175341]\n",
      "loss: 0.707832  [80000/175341]\n",
      "loss: 0.490979  [81600/175341]\n",
      "loss: 0.269400  [83200/175341]\n",
      "loss: 0.669432  [84800/175341]\n",
      "loss: 0.373314  [86400/175341]\n",
      "loss: 0.175057  [88000/175341]\n",
      "loss: 0.357547  [89600/175341]\n",
      "loss: 0.369839  [91200/175341]\n",
      "loss: 0.685046  [92800/175341]\n",
      "loss: 0.505253  [94400/175341]\n",
      "loss: 0.084603  [96000/175341]\n",
      "loss: 0.315973  [97600/175341]\n",
      "loss: 0.698661  [99200/175341]\n",
      "loss: 0.286875  [100800/175341]\n",
      "loss: 0.447394  [102400/175341]\n",
      "loss: 0.489134  [104000/175341]\n",
      "loss: 0.133056  [105600/175341]\n",
      "loss: 0.228360  [107200/175341]\n",
      "loss: 0.392805  [108800/175341]\n",
      "loss: 0.662773  [110400/175341]\n",
      "loss: 0.135537  [112000/175341]\n",
      "loss: 0.705580  [113600/175341]\n",
      "loss: 0.393966  [115200/175341]\n",
      "loss: 0.501425  [116800/175341]\n",
      "loss: 0.546029  [118400/175341]\n",
      "loss: 0.280718  [120000/175341]\n",
      "loss: 0.413577  [121600/175341]\n",
      "loss: 0.441655  [123200/175341]\n",
      "loss: 0.513069  [124800/175341]\n",
      "loss: 0.441137  [126400/175341]\n",
      "loss: 0.334934  [128000/175341]\n",
      "loss: 0.834443  [129600/175341]\n",
      "loss: 1.356561  [131200/175341]\n",
      "loss: 0.461550  [132800/175341]\n",
      "loss: 1.429924  [134400/175341]\n",
      "loss: 0.250886  [136000/175341]\n",
      "loss: 0.384976  [137600/175341]\n",
      "loss: 0.503939  [139200/175341]\n",
      "loss: 0.532175  [140800/175341]\n",
      "loss: 0.501932  [142400/175341]\n",
      "loss: 0.836309  [144000/175341]\n",
      "loss: 0.669371  [145600/175341]\n",
      "loss: 0.374248  [147200/175341]\n",
      "loss: 0.223556  [148800/175341]\n",
      "loss: 0.853030  [150400/175341]\n",
      "loss: 0.561120  [152000/175341]\n",
      "loss: 0.408647  [153600/175341]\n",
      "loss: 0.786457  [155200/175341]\n",
      "loss: 0.567293  [156800/175341]\n",
      "loss: 0.206243  [158400/175341]\n",
      "loss: 0.320696  [160000/175341]\n",
      "loss: 0.229507  [161600/175341]\n",
      "loss: 0.562653  [163200/175341]\n",
      "loss: 0.310116  [164800/175341]\n",
      "loss: 0.222745  [166400/175341]\n",
      "loss: 0.359171  [168000/175341]\n",
      "loss: 0.576102  [169600/175341]\n",
      "loss: 0.231040  [171200/175341]\n",
      "loss: 0.452312  [172800/175341]\n",
      "loss: 0.829039  [174400/175341]\n",
      "Train Accuracy: 81.4658%\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.577001, F1-score: 74.23%, Macro_F1-Score:  39.72%  \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.074834  [    0/175341]\n",
      "loss: 0.521185  [ 1600/175341]\n",
      "loss: 0.415220  [ 3200/175341]\n",
      "loss: 0.347277  [ 4800/175341]\n",
      "loss: 0.516725  [ 6400/175341]\n",
      "loss: 0.607192  [ 8000/175341]\n",
      "loss: 0.587968  [ 9600/175341]\n",
      "loss: 0.360188  [11200/175341]\n",
      "loss: 0.385831  [12800/175341]\n",
      "loss: 0.524403  [14400/175341]\n",
      "loss: 0.114370  [16000/175341]\n",
      "loss: 0.237463  [17600/175341]\n",
      "loss: 0.661297  [19200/175341]\n",
      "loss: 0.669552  [20800/175341]\n",
      "loss: 0.296898  [22400/175341]\n",
      "loss: 0.684755  [24000/175341]\n",
      "loss: 0.436145  [25600/175341]\n",
      "loss: 0.252928  [27200/175341]\n",
      "loss: 0.665455  [28800/175341]\n",
      "loss: 0.400215  [30400/175341]\n",
      "loss: 0.630624  [32000/175341]\n",
      "loss: 0.574865  [33600/175341]\n",
      "loss: 0.338339  [35200/175341]\n",
      "loss: 0.598878  [36800/175341]\n",
      "loss: 1.042449  [38400/175341]\n",
      "loss: 0.180497  [40000/175341]\n",
      "loss: 0.264854  [41600/175341]\n",
      "loss: 0.511307  [43200/175341]\n",
      "loss: 0.470005  [44800/175341]\n",
      "loss: 0.228739  [46400/175341]\n",
      "loss: 0.640693  [48000/175341]\n",
      "loss: 0.547128  [49600/175341]\n",
      "loss: 0.475379  [51200/175341]\n",
      "loss: 0.616304  [52800/175341]\n",
      "loss: 0.373115  [54400/175341]\n",
      "loss: 0.450590  [56000/175341]\n",
      "loss: 0.537821  [57600/175341]\n",
      "loss: 0.146589  [59200/175341]\n",
      "loss: 0.499653  [60800/175341]\n",
      "loss: 0.385941  [62400/175341]\n",
      "loss: 0.248945  [64000/175341]\n",
      "loss: 0.052244  [65600/175341]\n",
      "loss: 0.773331  [67200/175341]\n",
      "loss: 0.606194  [68800/175341]\n",
      "loss: 0.468117  [70400/175341]\n",
      "loss: 0.338052  [72000/175341]\n",
      "loss: 0.275993  [73600/175341]\n",
      "loss: 0.555803  [75200/175341]\n",
      "loss: 0.535467  [76800/175341]\n",
      "loss: 0.328166  [78400/175341]\n",
      "loss: 0.347527  [80000/175341]\n",
      "loss: 0.496532  [81600/175341]\n",
      "loss: 0.455611  [83200/175341]\n",
      "loss: 0.200124  [84800/175341]\n",
      "loss: 0.160165  [86400/175341]\n",
      "loss: 0.821546  [88000/175341]\n",
      "loss: 0.663989  [89600/175341]\n",
      "loss: 0.267092  [91200/175341]\n",
      "loss: 0.573354  [92800/175341]\n",
      "loss: 0.269444  [94400/175341]\n",
      "loss: 0.364148  [96000/175341]\n",
      "loss: 0.406656  [97600/175341]\n",
      "loss: 0.703200  [99200/175341]\n",
      "loss: 0.813121  [100800/175341]\n",
      "loss: 0.335878  [102400/175341]\n",
      "loss: 0.329840  [104000/175341]\n",
      "loss: 0.598694  [105600/175341]\n",
      "loss: 0.585054  [107200/175341]\n",
      "loss: 0.439093  [108800/175341]\n",
      "loss: 0.648182  [110400/175341]\n",
      "loss: 0.471684  [112000/175341]\n",
      "loss: 0.384029  [113600/175341]\n",
      "loss: 0.442631  [115200/175341]\n",
      "loss: 0.516089  [116800/175341]\n",
      "loss: 0.331866  [118400/175341]\n",
      "loss: 0.556681  [120000/175341]\n",
      "loss: 0.604812  [121600/175341]\n",
      "loss: 0.223082  [123200/175341]\n",
      "loss: 0.407954  [124800/175341]\n",
      "loss: 0.700420  [126400/175341]\n",
      "loss: 0.341618  [128000/175341]\n",
      "loss: 0.556120  [129600/175341]\n",
      "loss: 0.401678  [131200/175341]\n",
      "loss: 0.354045  [132800/175341]\n",
      "loss: 0.486306  [134400/175341]\n",
      "loss: 0.288433  [136000/175341]\n",
      "loss: 0.339903  [137600/175341]\n",
      "loss: 0.403570  [139200/175341]\n",
      "loss: 0.593668  [140800/175341]\n",
      "loss: 0.707166  [142400/175341]\n",
      "loss: 0.627407  [144000/175341]\n",
      "loss: 0.698447  [145600/175341]\n",
      "loss: 0.578744  [147200/175341]\n",
      "loss: 0.594614  [148800/175341]\n",
      "loss: 0.277003  [150400/175341]\n",
      "loss: 0.885678  [152000/175341]\n",
      "loss: 0.558872  [153600/175341]\n",
      "loss: 0.643196  [155200/175341]\n",
      "loss: 0.689822  [156800/175341]\n",
      "loss: 0.266973  [158400/175341]\n",
      "loss: 0.649840  [160000/175341]\n",
      "loss: 0.341399  [161600/175341]\n",
      "loss: 0.596707  [163200/175341]\n",
      "loss: 0.788679  [164800/175341]\n",
      "loss: 0.449969  [166400/175341]\n",
      "loss: 0.330585  [168000/175341]\n",
      "loss: 0.327577  [169600/175341]\n",
      "loss: 0.362549  [171200/175341]\n",
      "loss: 0.281677  [172800/175341]\n",
      "loss: 0.456491  [174400/175341]\n",
      "Train Accuracy: 81.4573%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.543172, F1-score: 76.29%, Macro_F1-Score:  40.93%  \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.497522  [    0/175341]\n",
      "loss: 0.681911  [ 1600/175341]\n",
      "loss: 0.673055  [ 3200/175341]\n",
      "loss: 0.299193  [ 4800/175341]\n",
      "loss: 0.192746  [ 6400/175341]\n",
      "loss: 0.333137  [ 8000/175341]\n",
      "loss: 0.745444  [ 9600/175341]\n",
      "loss: 0.381224  [11200/175341]\n",
      "loss: 0.304293  [12800/175341]\n",
      "loss: 0.308959  [14400/175341]\n",
      "loss: 0.413389  [16000/175341]\n",
      "loss: 0.159875  [17600/175341]\n",
      "loss: 0.664135  [19200/175341]\n",
      "loss: 0.518545  [20800/175341]\n",
      "loss: 0.695974  [22400/175341]\n",
      "loss: 0.404865  [24000/175341]\n",
      "loss: 0.577597  [25600/175341]\n",
      "loss: 0.390594  [27200/175341]\n",
      "loss: 0.856594  [28800/175341]\n",
      "loss: 0.280215  [30400/175341]\n",
      "loss: 0.538613  [32000/175341]\n",
      "loss: 0.153360  [33600/175341]\n",
      "loss: 0.725796  [35200/175341]\n",
      "loss: 0.311391  [36800/175341]\n",
      "loss: 0.414409  [38400/175341]\n",
      "loss: 0.311599  [40000/175341]\n",
      "loss: 0.353568  [41600/175341]\n",
      "loss: 0.368662  [43200/175341]\n",
      "loss: 0.296219  [44800/175341]\n",
      "loss: 0.406969  [46400/175341]\n",
      "loss: 0.532984  [48000/175341]\n",
      "loss: 0.307405  [49600/175341]\n",
      "loss: 0.231256  [51200/175341]\n",
      "loss: 0.395977  [52800/175341]\n",
      "loss: 0.318951  [54400/175341]\n",
      "loss: 0.749750  [56000/175341]\n",
      "loss: 0.535865  [57600/175341]\n",
      "loss: 0.305702  [59200/175341]\n",
      "loss: 0.339289  [60800/175341]\n",
      "loss: 0.333817  [62400/175341]\n",
      "loss: 0.384211  [64000/175341]\n",
      "loss: 0.172107  [65600/175341]\n",
      "loss: 0.406202  [67200/175341]\n",
      "loss: 0.187115  [68800/175341]\n",
      "loss: 0.478974  [70400/175341]\n",
      "loss: 0.231023  [72000/175341]\n",
      "loss: 0.302605  [73600/175341]\n",
      "loss: 0.840120  [75200/175341]\n",
      "loss: 0.310834  [76800/175341]\n",
      "loss: 0.224775  [78400/175341]\n",
      "loss: 0.318965  [80000/175341]\n",
      "loss: 0.583778  [81600/175341]\n",
      "loss: 0.493092  [83200/175341]\n",
      "loss: 0.212800  [84800/175341]\n",
      "loss: 0.517185  [86400/175341]\n",
      "loss: 0.815662  [88000/175341]\n",
      "loss: 0.493340  [89600/175341]\n",
      "loss: 0.630833  [91200/175341]\n",
      "loss: 0.836953  [92800/175341]\n",
      "loss: 0.163743  [94400/175341]\n",
      "loss: 0.389392  [96000/175341]\n",
      "loss: 0.478962  [97600/175341]\n",
      "loss: 0.793665  [99200/175341]\n",
      "loss: 0.401093  [100800/175341]\n",
      "loss: 0.547309  [102400/175341]\n",
      "loss: 0.401692  [104000/175341]\n",
      "loss: 0.362873  [105600/175341]\n",
      "loss: 0.369010  [107200/175341]\n",
      "loss: 0.295196  [108800/175341]\n",
      "loss: 0.605569  [110400/175341]\n",
      "loss: 0.938121  [112000/175341]\n",
      "loss: 0.184724  [113600/175341]\n",
      "loss: 0.506544  [115200/175341]\n",
      "loss: 0.461088  [116800/175341]\n",
      "loss: 0.448266  [118400/175341]\n",
      "loss: 0.558755  [120000/175341]\n",
      "loss: 0.586921  [121600/175341]\n",
      "loss: 0.442114  [123200/175341]\n",
      "loss: 0.254150  [124800/175341]\n",
      "loss: 0.854475  [126400/175341]\n",
      "loss: 0.602919  [128000/175341]\n",
      "loss: 1.119450  [129600/175341]\n",
      "loss: 0.545324  [131200/175341]\n",
      "loss: 0.681191  [132800/175341]\n",
      "loss: 0.387925  [134400/175341]\n",
      "loss: 0.820019  [136000/175341]\n",
      "loss: 0.263773  [137600/175341]\n",
      "loss: 0.385149  [139200/175341]\n",
      "loss: 0.497867  [140800/175341]\n",
      "loss: 0.413765  [142400/175341]\n",
      "loss: 0.468352  [144000/175341]\n",
      "loss: 0.320658  [145600/175341]\n",
      "loss: 0.464878  [147200/175341]\n",
      "loss: 0.457527  [148800/175341]\n",
      "loss: 0.256684  [150400/175341]\n",
      "loss: 0.204741  [152000/175341]\n",
      "loss: 0.579478  [153600/175341]\n",
      "loss: 0.556258  [155200/175341]\n",
      "loss: 0.430660  [156800/175341]\n",
      "loss: 0.596678  [158400/175341]\n",
      "loss: 0.118530  [160000/175341]\n",
      "loss: 0.534280  [161600/175341]\n",
      "loss: 0.432237  [163200/175341]\n",
      "loss: 0.453709  [164800/175341]\n",
      "loss: 0.472902  [166400/175341]\n",
      "loss: 0.665336  [168000/175341]\n",
      "loss: 0.624046  [169600/175341]\n",
      "loss: 0.275067  [171200/175341]\n",
      "loss: 0.197444  [172800/175341]\n",
      "loss: 0.433201  [174400/175341]\n",
      "Train Accuracy: 81.5012%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.561122, F1-score: 75.23%, Macro_F1-Score:  40.48%  \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.330767  [    0/175341]\n",
      "loss: 0.778127  [ 1600/175341]\n",
      "loss: 0.406945  [ 3200/175341]\n",
      "loss: 0.816180  [ 4800/175341]\n",
      "loss: 0.350448  [ 6400/175341]\n",
      "loss: 0.641516  [ 8000/175341]\n",
      "loss: 0.525849  [ 9600/175341]\n",
      "loss: 0.460691  [11200/175341]\n",
      "loss: 0.369817  [12800/175341]\n",
      "loss: 0.269159  [14400/175341]\n",
      "loss: 0.257597  [16000/175341]\n",
      "loss: 0.418600  [17600/175341]\n",
      "loss: 0.879117  [19200/175341]\n",
      "loss: 0.250741  [20800/175341]\n",
      "loss: 1.172745  [22400/175341]\n",
      "loss: 0.798149  [24000/175341]\n",
      "loss: 0.349915  [25600/175341]\n",
      "loss: 0.823415  [27200/175341]\n",
      "loss: 0.411797  [28800/175341]\n",
      "loss: 0.360672  [30400/175341]\n",
      "loss: 0.338731  [32000/175341]\n",
      "loss: 0.890140  [33600/175341]\n",
      "loss: 0.360020  [35200/175341]\n",
      "loss: 0.580693  [36800/175341]\n",
      "loss: 0.822927  [38400/175341]\n",
      "loss: 0.343447  [40000/175341]\n",
      "loss: 0.093245  [41600/175341]\n",
      "loss: 0.603760  [43200/175341]\n",
      "loss: 0.348351  [44800/175341]\n",
      "loss: 0.407284  [46400/175341]\n",
      "loss: 0.527468  [48000/175341]\n",
      "loss: 0.727924  [49600/175341]\n",
      "loss: 0.275781  [51200/175341]\n",
      "loss: 0.335745  [52800/175341]\n",
      "loss: 0.954526  [54400/175341]\n",
      "loss: 0.449962  [56000/175341]\n",
      "loss: 0.453832  [57600/175341]\n",
      "loss: 0.250847  [59200/175341]\n",
      "loss: 0.474093  [60800/175341]\n",
      "loss: 0.287584  [62400/175341]\n",
      "loss: 0.310952  [64000/175341]\n",
      "loss: 0.343384  [65600/175341]\n",
      "loss: 0.412303  [67200/175341]\n",
      "loss: 0.435199  [68800/175341]\n",
      "loss: 0.371427  [70400/175341]\n",
      "loss: 0.406068  [72000/175341]\n",
      "loss: 0.773361  [73600/175341]\n",
      "loss: 0.540194  [75200/175341]\n",
      "loss: 0.375762  [76800/175341]\n",
      "loss: 0.323831  [78400/175341]\n",
      "loss: 0.547575  [80000/175341]\n",
      "loss: 0.199729  [81600/175341]\n",
      "loss: 0.104741  [83200/175341]\n",
      "loss: 0.663742  [84800/175341]\n",
      "loss: 0.272438  [86400/175341]\n",
      "loss: 0.282640  [88000/175341]\n",
      "loss: 0.385391  [89600/175341]\n",
      "loss: 0.426036  [91200/175341]\n",
      "loss: 0.279122  [92800/175341]\n",
      "loss: 0.513265  [94400/175341]\n",
      "loss: 0.759452  [96000/175341]\n",
      "loss: 0.269755  [97600/175341]\n",
      "loss: 0.287277  [99200/175341]\n",
      "loss: 0.303105  [100800/175341]\n",
      "loss: 0.251877  [102400/175341]\n",
      "loss: 0.322771  [104000/175341]\n",
      "loss: 0.373281  [105600/175341]\n",
      "loss: 0.559371  [107200/175341]\n",
      "loss: 0.610040  [108800/175341]\n",
      "loss: 0.670695  [110400/175341]\n",
      "loss: 0.354977  [112000/175341]\n",
      "loss: 0.351567  [113600/175341]\n",
      "loss: 0.239481  [115200/175341]\n",
      "loss: 0.753778  [116800/175341]\n",
      "loss: 0.350296  [118400/175341]\n",
      "loss: 0.673377  [120000/175341]\n",
      "loss: 0.788777  [121600/175341]\n",
      "loss: 0.452917  [123200/175341]\n",
      "loss: 0.249292  [124800/175341]\n",
      "loss: 1.097653  [126400/175341]\n",
      "loss: 0.410953  [128000/175341]\n",
      "loss: 0.283179  [129600/175341]\n",
      "loss: 0.276128  [131200/175341]\n",
      "loss: 0.436203  [132800/175341]\n",
      "loss: 0.430772  [134400/175341]\n",
      "loss: 0.457888  [136000/175341]\n",
      "loss: 0.573678  [137600/175341]\n",
      "loss: 0.527187  [139200/175341]\n",
      "loss: 0.454330  [140800/175341]\n",
      "loss: 0.868467  [142400/175341]\n",
      "loss: 0.400414  [144000/175341]\n",
      "loss: 0.618478  [145600/175341]\n",
      "loss: 0.484409  [147200/175341]\n",
      "loss: 0.692108  [148800/175341]\n",
      "loss: 0.228824  [150400/175341]\n",
      "loss: 0.642200  [152000/175341]\n",
      "loss: 0.547250  [153600/175341]\n",
      "loss: 0.600372  [155200/175341]\n",
      "loss: 0.909375  [156800/175341]\n",
      "loss: 0.420118  [158400/175341]\n",
      "loss: 0.536802  [160000/175341]\n",
      "loss: 0.414142  [161600/175341]\n",
      "loss: 0.647526  [163200/175341]\n",
      "loss: 0.346526  [164800/175341]\n",
      "loss: 0.521875  [166400/175341]\n",
      "loss: 0.384277  [168000/175341]\n",
      "loss: 0.584412  [169600/175341]\n",
      "loss: 0.435506  [171200/175341]\n",
      "loss: 0.486471  [172800/175341]\n",
      "loss: 0.415444  [174400/175341]\n",
      "Train Accuracy: 81.4932%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.556416, F1-score: 76.46%, Macro_F1-Score:  40.64%  \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.539329  [    0/175341]\n",
      "loss: 0.413965  [ 1600/175341]\n",
      "loss: 0.514125  [ 3200/175341]\n",
      "loss: 0.253097  [ 4800/175341]\n",
      "loss: 0.305701  [ 6400/175341]\n",
      "loss: 0.142576  [ 8000/175341]\n",
      "loss: 0.429137  [ 9600/175341]\n",
      "loss: 0.411451  [11200/175341]\n",
      "loss: 0.305903  [12800/175341]\n",
      "loss: 0.634473  [14400/175341]\n",
      "loss: 0.274232  [16000/175341]\n",
      "loss: 0.547527  [17600/175341]\n",
      "loss: 0.697364  [19200/175341]\n",
      "loss: 0.560711  [20800/175341]\n",
      "loss: 0.533541  [22400/175341]\n",
      "loss: 0.370665  [24000/175341]\n",
      "loss: 0.346202  [25600/175341]\n",
      "loss: 0.378309  [27200/175341]\n",
      "loss: 0.248925  [28800/175341]\n",
      "loss: 0.193607  [30400/175341]\n",
      "loss: 0.273106  [32000/175341]\n",
      "loss: 0.662527  [33600/175341]\n",
      "loss: 0.108766  [35200/175341]\n",
      "loss: 0.421274  [36800/175341]\n",
      "loss: 0.568652  [38400/175341]\n",
      "loss: 0.650134  [40000/175341]\n",
      "loss: 0.418958  [41600/175341]\n",
      "loss: 0.267786  [43200/175341]\n",
      "loss: 0.629680  [44800/175341]\n",
      "loss: 0.627311  [46400/175341]\n",
      "loss: 0.368709  [48000/175341]\n",
      "loss: 0.849299  [49600/175341]\n",
      "loss: 0.207158  [51200/175341]\n",
      "loss: 0.172607  [52800/175341]\n",
      "loss: 0.569460  [54400/175341]\n",
      "loss: 0.938535  [56000/175341]\n",
      "loss: 0.278811  [57600/175341]\n",
      "loss: 0.257162  [59200/175341]\n",
      "loss: 0.310405  [60800/175341]\n",
      "loss: 0.528240  [62400/175341]\n",
      "loss: 0.268122  [64000/175341]\n",
      "loss: 0.252631  [65600/175341]\n",
      "loss: 0.370690  [67200/175341]\n",
      "loss: 0.249248  [68800/175341]\n",
      "loss: 0.283525  [70400/175341]\n",
      "loss: 0.458841  [72000/175341]\n",
      "loss: 0.598775  [73600/175341]\n",
      "loss: 0.673160  [75200/175341]\n",
      "loss: 0.545922  [76800/175341]\n",
      "loss: 0.442042  [78400/175341]\n",
      "loss: 0.652241  [80000/175341]\n",
      "loss: 0.494138  [81600/175341]\n",
      "loss: 0.241197  [83200/175341]\n",
      "loss: 0.603228  [84800/175341]\n",
      "loss: 0.262569  [86400/175341]\n",
      "loss: 0.724208  [88000/175341]\n",
      "loss: 0.718355  [89600/175341]\n",
      "loss: 0.230049  [91200/175341]\n",
      "loss: 0.153273  [92800/175341]\n",
      "loss: 0.453937  [94400/175341]\n",
      "loss: 0.273034  [96000/175341]\n",
      "loss: 0.593768  [97600/175341]\n",
      "loss: 0.543174  [99200/175341]\n",
      "loss: 0.278234  [100800/175341]\n",
      "loss: 0.505888  [102400/175341]\n",
      "loss: 0.314313  [104000/175341]\n",
      "loss: 0.390803  [105600/175341]\n",
      "loss: 0.161048  [107200/175341]\n",
      "loss: 0.223937  [108800/175341]\n",
      "loss: 0.261076  [110400/175341]\n",
      "loss: 0.653290  [112000/175341]\n",
      "loss: 0.250895  [113600/175341]\n",
      "loss: 0.463389  [115200/175341]\n",
      "loss: 0.459027  [116800/175341]\n",
      "loss: 0.561812  [118400/175341]\n",
      "loss: 0.520504  [120000/175341]\n",
      "loss: 0.286509  [121600/175341]\n",
      "loss: 0.525290  [123200/175341]\n",
      "loss: 0.192621  [124800/175341]\n",
      "loss: 0.379952  [126400/175341]\n",
      "loss: 1.065140  [128000/175341]\n",
      "loss: 0.104783  [129600/175341]\n",
      "loss: 0.752165  [131200/175341]\n",
      "loss: 0.359263  [132800/175341]\n",
      "loss: 0.223561  [134400/175341]\n",
      "loss: 0.207926  [136000/175341]\n",
      "loss: 0.487170  [137600/175341]\n",
      "loss: 0.108993  [139200/175341]\n",
      "loss: 0.526006  [140800/175341]\n",
      "loss: 0.429243  [142400/175341]\n",
      "loss: 0.443289  [144000/175341]\n",
      "loss: 0.716710  [145600/175341]\n",
      "loss: 0.404526  [147200/175341]\n",
      "loss: 0.320642  [148800/175341]\n",
      "loss: 0.273374  [150400/175341]\n",
      "loss: 0.381476  [152000/175341]\n",
      "loss: 0.341114  [153600/175341]\n",
      "loss: 0.188386  [155200/175341]\n",
      "loss: 0.225443  [156800/175341]\n",
      "loss: 0.630144  [158400/175341]\n",
      "loss: 0.320893  [160000/175341]\n",
      "loss: 0.445843  [161600/175341]\n",
      "loss: 0.696841  [163200/175341]\n",
      "loss: 0.751012  [164800/175341]\n",
      "loss: 0.575302  [166400/175341]\n",
      "loss: 0.785545  [168000/175341]\n",
      "loss: 0.326848  [169600/175341]\n",
      "loss: 0.694160  [171200/175341]\n",
      "loss: 0.284035  [172800/175341]\n",
      "loss: 0.385212  [174400/175341]\n",
      "Train Accuracy: 81.5040%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.533261, F1-score: 78.07%, Macro_F1-Score:  42.80%  \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.502734  [    0/175341]\n",
      "loss: 0.567679  [ 1600/175341]\n",
      "loss: 0.726409  [ 3200/175341]\n",
      "loss: 0.615136  [ 4800/175341]\n",
      "loss: 0.576801  [ 6400/175341]\n",
      "loss: 0.575762  [ 8000/175341]\n",
      "loss: 0.658543  [ 9600/175341]\n",
      "loss: 0.507217  [11200/175341]\n",
      "loss: 0.287255  [12800/175341]\n",
      "loss: 0.307553  [14400/175341]\n",
      "loss: 0.445464  [16000/175341]\n",
      "loss: 0.373402  [17600/175341]\n",
      "loss: 0.589579  [19200/175341]\n",
      "loss: 0.173195  [20800/175341]\n",
      "loss: 0.329372  [22400/175341]\n",
      "loss: 0.546857  [24000/175341]\n",
      "loss: 0.206109  [25600/175341]\n",
      "loss: 0.424055  [27200/175341]\n",
      "loss: 0.246714  [28800/175341]\n",
      "loss: 0.451725  [30400/175341]\n",
      "loss: 0.280450  [32000/175341]\n",
      "loss: 0.391792  [33600/175341]\n",
      "loss: 0.220659  [35200/175341]\n",
      "loss: 0.685159  [36800/175341]\n",
      "loss: 0.299400  [38400/175341]\n",
      "loss: 0.393385  [40000/175341]\n",
      "loss: 0.590248  [41600/175341]\n",
      "loss: 0.246420  [43200/175341]\n",
      "loss: 0.265449  [44800/175341]\n",
      "loss: 0.440090  [46400/175341]\n",
      "loss: 0.533860  [48000/175341]\n",
      "loss: 0.649706  [49600/175341]\n",
      "loss: 0.769164  [51200/175341]\n",
      "loss: 0.171685  [52800/175341]\n",
      "loss: 0.405873  [54400/175341]\n",
      "loss: 0.372240  [56000/175341]\n",
      "loss: 0.304765  [57600/175341]\n",
      "loss: 0.516542  [59200/175341]\n",
      "loss: 0.732312  [60800/175341]\n",
      "loss: 0.376683  [62400/175341]\n",
      "loss: 0.390139  [64000/175341]\n",
      "loss: 0.214118  [65600/175341]\n",
      "loss: 0.743986  [67200/175341]\n",
      "loss: 0.501608  [68800/175341]\n",
      "loss: 0.908092  [70400/175341]\n",
      "loss: 0.295249  [72000/175341]\n",
      "loss: 0.268114  [73600/175341]\n",
      "loss: 0.490069  [75200/175341]\n",
      "loss: 0.501668  [76800/175341]\n",
      "loss: 0.525223  [78400/175341]\n",
      "loss: 0.530394  [80000/175341]\n",
      "loss: 0.566859  [81600/175341]\n",
      "loss: 0.730030  [83200/175341]\n",
      "loss: 0.340851  [84800/175341]\n",
      "loss: 0.229193  [86400/175341]\n",
      "loss: 0.336441  [88000/175341]\n",
      "loss: 0.826164  [89600/175341]\n",
      "loss: 0.461777  [91200/175341]\n",
      "loss: 0.246987  [92800/175341]\n",
      "loss: 0.497360  [94400/175341]\n",
      "loss: 0.658131  [96000/175341]\n",
      "loss: 0.483545  [97600/175341]\n",
      "loss: 0.379131  [99200/175341]\n",
      "loss: 0.660260  [100800/175341]\n",
      "loss: 0.573408  [102400/175341]\n",
      "loss: 0.171980  [104000/175341]\n",
      "loss: 0.920169  [105600/175341]\n",
      "loss: 0.827466  [107200/175341]\n",
      "loss: 0.575373  [108800/175341]\n",
      "loss: 0.671960  [110400/175341]\n",
      "loss: 0.230623  [112000/175341]\n",
      "loss: 0.323862  [113600/175341]\n",
      "loss: 0.451970  [115200/175341]\n",
      "loss: 0.476976  [116800/175341]\n",
      "loss: 0.412310  [118400/175341]\n",
      "loss: 0.260568  [120000/175341]\n",
      "loss: 0.649166  [121600/175341]\n",
      "loss: 0.569359  [123200/175341]\n",
      "loss: 0.370194  [124800/175341]\n",
      "loss: 0.667757  [126400/175341]\n",
      "loss: 0.566819  [128000/175341]\n",
      "loss: 0.274852  [129600/175341]\n",
      "loss: 0.741745  [131200/175341]\n",
      "loss: 0.616163  [132800/175341]\n",
      "loss: 0.245754  [134400/175341]\n",
      "loss: 0.194060  [136000/175341]\n",
      "loss: 0.121416  [137600/175341]\n",
      "loss: 0.635299  [139200/175341]\n",
      "loss: 0.513364  [140800/175341]\n",
      "loss: 0.287287  [142400/175341]\n",
      "loss: 0.476939  [144000/175341]\n",
      "loss: 0.250662  [145600/175341]\n",
      "loss: 0.517057  [147200/175341]\n",
      "loss: 0.121693  [148800/175341]\n",
      "loss: 0.879069  [150400/175341]\n",
      "loss: 0.447268  [152000/175341]\n",
      "loss: 0.421360  [153600/175341]\n",
      "loss: 0.454810  [155200/175341]\n",
      "loss: 0.471671  [156800/175341]\n",
      "loss: 0.624040  [158400/175341]\n",
      "loss: 0.528169  [160000/175341]\n",
      "loss: 0.309040  [161600/175341]\n",
      "loss: 0.416092  [163200/175341]\n",
      "loss: 0.557691  [164800/175341]\n",
      "loss: 0.458049  [166400/175341]\n",
      "loss: 0.197992  [168000/175341]\n",
      "loss: 0.546235  [169600/175341]\n",
      "loss: 0.224981  [171200/175341]\n",
      "loss: 0.662800  [172800/175341]\n",
      "loss: 0.495788  [174400/175341]\n",
      "Train Accuracy: 81.5143%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.552488, F1-score: 75.63%, Macro_F1-Score:  40.15%  \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.275168  [    0/175341]\n",
      "loss: 0.215083  [ 1600/175341]\n",
      "loss: 0.440278  [ 3200/175341]\n",
      "loss: 0.290986  [ 4800/175341]\n",
      "loss: 0.569964  [ 6400/175341]\n",
      "loss: 0.435583  [ 8000/175341]\n",
      "loss: 0.472876  [ 9600/175341]\n",
      "loss: 0.343155  [11200/175341]\n",
      "loss: 0.211574  [12800/175341]\n",
      "loss: 0.656154  [14400/175341]\n",
      "loss: 0.637329  [16000/175341]\n",
      "loss: 0.671665  [17600/175341]\n",
      "loss: 0.677053  [19200/175341]\n",
      "loss: 1.007039  [20800/175341]\n",
      "loss: 0.480186  [22400/175341]\n",
      "loss: 0.629527  [24000/175341]\n",
      "loss: 0.340972  [25600/175341]\n",
      "loss: 0.340717  [27200/175341]\n",
      "loss: 0.415804  [28800/175341]\n",
      "loss: 0.546629  [30400/175341]\n",
      "loss: 0.259868  [32000/175341]\n",
      "loss: 0.615372  [33600/175341]\n",
      "loss: 0.819531  [35200/175341]\n",
      "loss: 0.545449  [36800/175341]\n",
      "loss: 0.453211  [38400/175341]\n",
      "loss: 0.492141  [40000/175341]\n",
      "loss: 0.201224  [41600/175341]\n",
      "loss: 0.494473  [43200/175341]\n",
      "loss: 0.368330  [44800/175341]\n",
      "loss: 0.453460  [46400/175341]\n",
      "loss: 0.293282  [48000/175341]\n",
      "loss: 0.510277  [49600/175341]\n",
      "loss: 0.438481  [51200/175341]\n",
      "loss: 0.697643  [52800/175341]\n",
      "loss: 0.265619  [54400/175341]\n",
      "loss: 0.433229  [56000/175341]\n",
      "loss: 0.638789  [57600/175341]\n",
      "loss: 0.501573  [59200/175341]\n",
      "loss: 0.617557  [60800/175341]\n",
      "loss: 0.416047  [62400/175341]\n",
      "loss: 0.384954  [64000/175341]\n",
      "loss: 0.646539  [65600/175341]\n",
      "loss: 0.599004  [67200/175341]\n",
      "loss: 0.369397  [68800/175341]\n",
      "loss: 0.489330  [70400/175341]\n",
      "loss: 0.379745  [72000/175341]\n",
      "loss: 0.507079  [73600/175341]\n",
      "loss: 0.386275  [75200/175341]\n",
      "loss: 0.377737  [76800/175341]\n",
      "loss: 0.681588  [78400/175341]\n",
      "loss: 0.226680  [80000/175341]\n",
      "loss: 0.722109  [81600/175341]\n",
      "loss: 1.132573  [83200/175341]\n",
      "loss: 0.688405  [84800/175341]\n",
      "loss: 0.564406  [86400/175341]\n",
      "loss: 0.684303  [88000/175341]\n",
      "loss: 0.615385  [89600/175341]\n",
      "loss: 0.476752  [91200/175341]\n",
      "loss: 0.745505  [92800/175341]\n",
      "loss: 0.380528  [94400/175341]\n",
      "loss: 0.255735  [96000/175341]\n",
      "loss: 0.476240  [97600/175341]\n",
      "loss: 0.690110  [99200/175341]\n",
      "loss: 0.823349  [100800/175341]\n",
      "loss: 0.496590  [102400/175341]\n",
      "loss: 0.545732  [104000/175341]\n",
      "loss: 0.444133  [105600/175341]\n",
      "loss: 0.485190  [107200/175341]\n",
      "loss: 0.436853  [108800/175341]\n",
      "loss: 0.451899  [110400/175341]\n",
      "loss: 0.196668  [112000/175341]\n",
      "loss: 0.621689  [113600/175341]\n",
      "loss: 0.819750  [115200/175341]\n",
      "loss: 0.324598  [116800/175341]\n",
      "loss: 0.690609  [118400/175341]\n",
      "loss: 0.892561  [120000/175341]\n",
      "loss: 0.579566  [121600/175341]\n",
      "loss: 0.301790  [123200/175341]\n",
      "loss: 0.600994  [124800/175341]\n",
      "loss: 0.354253  [126400/175341]\n",
      "loss: 0.588476  [128000/175341]\n",
      "loss: 0.224526  [129600/175341]\n",
      "loss: 0.398006  [131200/175341]\n",
      "loss: 0.352812  [132800/175341]\n",
      "loss: 0.488141  [134400/175341]\n",
      "loss: 0.278239  [136000/175341]\n",
      "loss: 0.227611  [137600/175341]\n",
      "loss: 0.767391  [139200/175341]\n",
      "loss: 0.343119  [140800/175341]\n",
      "loss: 0.258484  [142400/175341]\n",
      "loss: 0.650218  [144000/175341]\n",
      "loss: 0.753293  [145600/175341]\n",
      "loss: 0.521291  [147200/175341]\n",
      "loss: 0.234065  [148800/175341]\n",
      "loss: 0.262153  [150400/175341]\n",
      "loss: 0.330006  [152000/175341]\n",
      "loss: 0.578496  [153600/175341]\n",
      "loss: 0.283335  [155200/175341]\n",
      "loss: 0.214241  [156800/175341]\n",
      "loss: 1.103431  [158400/175341]\n",
      "loss: 0.371613  [160000/175341]\n",
      "loss: 0.510102  [161600/175341]\n",
      "loss: 0.462813  [163200/175341]\n",
      "loss: 0.315226  [164800/175341]\n",
      "loss: 0.322645  [166400/175341]\n",
      "loss: 0.310349  [168000/175341]\n",
      "loss: 0.280874  [169600/175341]\n",
      "loss: 0.879055  [171200/175341]\n",
      "loss: 0.759742  [172800/175341]\n",
      "loss: 0.517573  [174400/175341]\n",
      "Train Accuracy: 81.5012%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.570744, F1-score: 75.25%, Macro_F1-Score:  39.86%  \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.279721  [    0/175341]\n",
      "loss: 0.406233  [ 1600/175341]\n",
      "loss: 0.403851  [ 3200/175341]\n",
      "loss: 0.508278  [ 4800/175341]\n",
      "loss: 0.133486  [ 6400/175341]\n",
      "loss: 0.295371  [ 8000/175341]\n",
      "loss: 0.312689  [ 9600/175341]\n",
      "loss: 0.140577  [11200/175341]\n",
      "loss: 0.451115  [12800/175341]\n",
      "loss: 0.284025  [14400/175341]\n",
      "loss: 0.450432  [16000/175341]\n",
      "loss: 0.906984  [17600/175341]\n",
      "loss: 0.296822  [19200/175341]\n",
      "loss: 0.404822  [20800/175341]\n",
      "loss: 0.802766  [22400/175341]\n",
      "loss: 0.278374  [24000/175341]\n",
      "loss: 0.667108  [25600/175341]\n",
      "loss: 0.552477  [27200/175341]\n",
      "loss: 0.367367  [28800/175341]\n",
      "loss: 0.416239  [30400/175341]\n",
      "loss: 0.612238  [32000/175341]\n",
      "loss: 0.928562  [33600/175341]\n",
      "loss: 0.378789  [35200/175341]\n",
      "loss: 0.203177  [36800/175341]\n",
      "loss: 0.232934  [38400/175341]\n",
      "loss: 0.823566  [40000/175341]\n",
      "loss: 0.418579  [41600/175341]\n",
      "loss: 0.375896  [43200/175341]\n",
      "loss: 0.676040  [44800/175341]\n",
      "loss: 0.341145  [46400/175341]\n",
      "loss: 0.706724  [48000/175341]\n",
      "loss: 0.408899  [49600/175341]\n",
      "loss: 0.771900  [51200/175341]\n",
      "loss: 0.307247  [52800/175341]\n",
      "loss: 0.386531  [54400/175341]\n",
      "loss: 0.405965  [56000/175341]\n",
      "loss: 0.196958  [57600/175341]\n",
      "loss: 0.207998  [59200/175341]\n",
      "loss: 0.605447  [60800/175341]\n",
      "loss: 0.307566  [62400/175341]\n",
      "loss: 0.304529  [64000/175341]\n",
      "loss: 0.714022  [65600/175341]\n",
      "loss: 0.641496  [67200/175341]\n",
      "loss: 0.173821  [68800/175341]\n",
      "loss: 0.167588  [70400/175341]\n",
      "loss: 0.584327  [72000/175341]\n",
      "loss: 0.341192  [73600/175341]\n",
      "loss: 0.248347  [75200/175341]\n",
      "loss: 0.639435  [76800/175341]\n",
      "loss: 0.623571  [78400/175341]\n",
      "loss: 0.470865  [80000/175341]\n",
      "loss: 0.433958  [81600/175341]\n",
      "loss: 0.498839  [83200/175341]\n",
      "loss: 0.275908  [84800/175341]\n",
      "loss: 0.382757  [86400/175341]\n",
      "loss: 0.639980  [88000/175341]\n",
      "loss: 0.868614  [89600/175341]\n",
      "loss: 0.352290  [91200/175341]\n",
      "loss: 0.375706  [92800/175341]\n",
      "loss: 0.156419  [94400/175341]\n",
      "loss: 0.407375  [96000/175341]\n",
      "loss: 0.255556  [97600/175341]\n",
      "loss: 0.573839  [99200/175341]\n",
      "loss: 0.597943  [100800/175341]\n",
      "loss: 0.268748  [102400/175341]\n",
      "loss: 0.355090  [104000/175341]\n",
      "loss: 0.323534  [105600/175341]\n",
      "loss: 0.580237  [107200/175341]\n",
      "loss: 0.945579  [108800/175341]\n",
      "loss: 0.367881  [110400/175341]\n",
      "loss: 0.432329  [112000/175341]\n",
      "loss: 0.481704  [113600/175341]\n",
      "loss: 0.273118  [115200/175341]\n",
      "loss: 0.519817  [116800/175341]\n",
      "loss: 0.322589  [118400/175341]\n",
      "loss: 0.772559  [120000/175341]\n",
      "loss: 0.880948  [121600/175341]\n",
      "loss: 0.735962  [123200/175341]\n",
      "loss: 0.849473  [124800/175341]\n",
      "loss: 0.234054  [126400/175341]\n",
      "loss: 0.888252  [128000/175341]\n",
      "loss: 0.218906  [129600/175341]\n",
      "loss: 0.666738  [131200/175341]\n",
      "loss: 0.296006  [132800/175341]\n",
      "loss: 0.372986  [134400/175341]\n",
      "loss: 0.510890  [136000/175341]\n",
      "loss: 0.535682  [137600/175341]\n",
      "loss: 0.391220  [139200/175341]\n",
      "loss: 0.530927  [140800/175341]\n",
      "loss: 0.369631  [142400/175341]\n",
      "loss: 0.150193  [144000/175341]\n",
      "loss: 0.164737  [145600/175341]\n",
      "loss: 0.298304  [147200/175341]\n",
      "loss: 0.641383  [148800/175341]\n",
      "loss: 0.192004  [150400/175341]\n",
      "loss: 0.159389  [152000/175341]\n",
      "loss: 0.328515  [153600/175341]\n",
      "loss: 0.240796  [155200/175341]\n",
      "loss: 0.461266  [156800/175341]\n",
      "loss: 0.348537  [158400/175341]\n",
      "loss: 0.235275  [160000/175341]\n",
      "loss: 0.514742  [161600/175341]\n",
      "loss: 0.413141  [163200/175341]\n",
      "loss: 0.723292  [164800/175341]\n",
      "loss: 0.443061  [166400/175341]\n",
      "loss: 0.678874  [168000/175341]\n",
      "loss: 0.637721  [169600/175341]\n",
      "loss: 0.240021  [171200/175341]\n",
      "loss: 0.407011  [172800/175341]\n",
      "loss: 0.408646  [174400/175341]\n",
      "Train Accuracy: 81.5172%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.562505, F1-score: 75.66%, Macro_F1-Score:  40.29%  \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.522174  [    0/175341]\n",
      "loss: 0.473716  [ 1600/175341]\n",
      "loss: 0.980371  [ 3200/175341]\n",
      "loss: 0.515286  [ 4800/175341]\n",
      "loss: 0.609769  [ 6400/175341]\n",
      "loss: 0.647540  [ 8000/175341]\n",
      "loss: 0.363244  [ 9600/175341]\n",
      "loss: 0.720946  [11200/175341]\n",
      "loss: 0.361390  [12800/175341]\n",
      "loss: 0.575549  [14400/175341]\n",
      "loss: 0.339186  [16000/175341]\n",
      "loss: 0.546806  [17600/175341]\n",
      "loss: 0.351403  [19200/175341]\n",
      "loss: 0.198179  [20800/175341]\n",
      "loss: 0.282468  [22400/175341]\n",
      "loss: 0.693693  [24000/175341]\n",
      "loss: 0.480459  [25600/175341]\n",
      "loss: 0.500957  [27200/175341]\n",
      "loss: 0.378122  [28800/175341]\n",
      "loss: 0.265481  [30400/175341]\n",
      "loss: 0.385964  [32000/175341]\n",
      "loss: 0.409344  [33600/175341]\n",
      "loss: 0.376138  [35200/175341]\n",
      "loss: 0.424106  [36800/175341]\n",
      "loss: 0.275387  [38400/175341]\n",
      "loss: 0.279380  [40000/175341]\n",
      "loss: 0.397895  [41600/175341]\n",
      "loss: 0.380077  [43200/175341]\n",
      "loss: 0.640880  [44800/175341]\n",
      "loss: 0.441931  [46400/175341]\n",
      "loss: 0.524755  [48000/175341]\n",
      "loss: 0.432553  [49600/175341]\n",
      "loss: 1.175833  [51200/175341]\n",
      "loss: 0.452241  [52800/175341]\n",
      "loss: 0.311546  [54400/175341]\n",
      "loss: 0.268809  [56000/175341]\n",
      "loss: 0.425965  [57600/175341]\n",
      "loss: 0.602534  [59200/175341]\n",
      "loss: 0.557449  [60800/175341]\n",
      "loss: 0.339659  [62400/175341]\n",
      "loss: 0.668705  [64000/175341]\n",
      "loss: 0.121385  [65600/175341]\n",
      "loss: 0.399523  [67200/175341]\n",
      "loss: 0.338135  [68800/175341]\n",
      "loss: 0.380687  [70400/175341]\n",
      "loss: 0.628347  [72000/175341]\n",
      "loss: 0.551829  [73600/175341]\n",
      "loss: 0.451134  [75200/175341]\n",
      "loss: 0.106818  [76800/175341]\n",
      "loss: 0.383173  [78400/175341]\n",
      "loss: 0.195587  [80000/175341]\n",
      "loss: 0.473157  [81600/175341]\n",
      "loss: 0.489720  [83200/175341]\n",
      "loss: 0.538092  [84800/175341]\n",
      "loss: 0.480903  [86400/175341]\n",
      "loss: 0.445887  [88000/175341]\n",
      "loss: 0.710145  [89600/175341]\n",
      "loss: 0.386859  [91200/175341]\n",
      "loss: 0.295667  [92800/175341]\n",
      "loss: 0.463987  [94400/175341]\n",
      "loss: 0.499335  [96000/175341]\n",
      "loss: 0.724108  [97600/175341]\n",
      "loss: 0.502617  [99200/175341]\n",
      "loss: 0.300546  [100800/175341]\n",
      "loss: 0.396148  [102400/175341]\n",
      "loss: 0.737268  [104000/175341]\n",
      "loss: 0.347527  [105600/175341]\n",
      "loss: 0.549541  [107200/175341]\n",
      "loss: 0.426455  [108800/175341]\n",
      "loss: 0.475180  [110400/175341]\n",
      "loss: 0.164592  [112000/175341]\n",
      "loss: 0.293804  [113600/175341]\n",
      "loss: 0.150664  [115200/175341]\n",
      "loss: 0.592282  [116800/175341]\n",
      "loss: 0.815371  [118400/175341]\n",
      "loss: 0.279014  [120000/175341]\n",
      "loss: 0.408246  [121600/175341]\n",
      "loss: 0.503825  [123200/175341]\n",
      "loss: 0.177791  [124800/175341]\n",
      "loss: 0.860005  [126400/175341]\n",
      "loss: 0.453671  [128000/175341]\n",
      "loss: 0.325763  [129600/175341]\n",
      "loss: 0.473063  [131200/175341]\n",
      "loss: 0.373624  [132800/175341]\n",
      "loss: 0.122884  [134400/175341]\n",
      "loss: 0.287457  [136000/175341]\n",
      "loss: 0.485428  [137600/175341]\n",
      "loss: 0.763480  [139200/175341]\n",
      "loss: 0.134521  [140800/175341]\n",
      "loss: 0.492109  [142400/175341]\n",
      "loss: 0.230999  [144000/175341]\n",
      "loss: 0.294280  [145600/175341]\n",
      "loss: 0.600516  [147200/175341]\n",
      "loss: 0.501795  [148800/175341]\n",
      "loss: 0.333356  [150400/175341]\n",
      "loss: 0.470237  [152000/175341]\n",
      "loss: 0.555022  [153600/175341]\n",
      "loss: 0.418541  [155200/175341]\n",
      "loss: 0.196250  [156800/175341]\n",
      "loss: 0.267799  [158400/175341]\n",
      "loss: 0.414655  [160000/175341]\n",
      "loss: 0.241197  [161600/175341]\n",
      "loss: 0.689818  [163200/175341]\n",
      "loss: 0.250845  [164800/175341]\n",
      "loss: 0.618522  [166400/175341]\n",
      "loss: 0.325884  [168000/175341]\n",
      "loss: 0.396825  [169600/175341]\n",
      "loss: 0.268327  [171200/175341]\n",
      "loss: 0.090426  [172800/175341]\n",
      "loss: 0.407131  [174400/175341]\n",
      "Train Accuracy: 81.5149%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.564624, F1-score: 75.28%, Macro_F1-Score:  40.42%  \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.324206  [    0/175341]\n",
      "loss: 0.535523  [ 1600/175341]\n",
      "loss: 0.155243  [ 3200/175341]\n",
      "loss: 0.677737  [ 4800/175341]\n",
      "loss: 0.520487  [ 6400/175341]\n",
      "loss: 0.273586  [ 8000/175341]\n",
      "loss: 0.793917  [ 9600/175341]\n",
      "loss: 0.696772  [11200/175341]\n",
      "loss: 0.701704  [12800/175341]\n",
      "loss: 0.129628  [14400/175341]\n",
      "loss: 0.409179  [16000/175341]\n",
      "loss: 0.558041  [17600/175341]\n",
      "loss: 0.738303  [19200/175341]\n",
      "loss: 0.371417  [20800/175341]\n",
      "loss: 0.366745  [22400/175341]\n",
      "loss: 0.961231  [24000/175341]\n",
      "loss: 0.521538  [25600/175341]\n",
      "loss: 0.184559  [27200/175341]\n",
      "loss: 0.213895  [28800/175341]\n",
      "loss: 0.139336  [30400/175341]\n",
      "loss: 0.566105  [32000/175341]\n",
      "loss: 0.434249  [33600/175341]\n",
      "loss: 0.327078  [35200/175341]\n",
      "loss: 0.407500  [36800/175341]\n",
      "loss: 0.509111  [38400/175341]\n",
      "loss: 0.826407  [40000/175341]\n",
      "loss: 0.540046  [41600/175341]\n",
      "loss: 0.137945  [43200/175341]\n",
      "loss: 0.473846  [44800/175341]\n",
      "loss: 0.360081  [46400/175341]\n",
      "loss: 0.249207  [48000/175341]\n",
      "loss: 0.713830  [49600/175341]\n",
      "loss: 0.304754  [51200/175341]\n",
      "loss: 0.229316  [52800/175341]\n",
      "loss: 0.423397  [54400/175341]\n",
      "loss: 0.421697  [56000/175341]\n",
      "loss: 0.498510  [57600/175341]\n",
      "loss: 0.128431  [59200/175341]\n",
      "loss: 0.337613  [60800/175341]\n",
      "loss: 0.594165  [62400/175341]\n",
      "loss: 0.232053  [64000/175341]\n",
      "loss: 0.295854  [65600/175341]\n",
      "loss: 0.888813  [67200/175341]\n",
      "loss: 0.274782  [68800/175341]\n",
      "loss: 0.215916  [70400/175341]\n",
      "loss: 0.105814  [72000/175341]\n",
      "loss: 0.356387  [73600/175341]\n",
      "loss: 0.299566  [75200/175341]\n",
      "loss: 0.613332  [76800/175341]\n",
      "loss: 0.282526  [78400/175341]\n",
      "loss: 0.277202  [80000/175341]\n",
      "loss: 0.730611  [81600/175341]\n",
      "loss: 0.548707  [83200/175341]\n",
      "loss: 0.460503  [84800/175341]\n",
      "loss: 0.192761  [86400/175341]\n",
      "loss: 0.308905  [88000/175341]\n",
      "loss: 0.249964  [89600/175341]\n",
      "loss: 0.354128  [91200/175341]\n",
      "loss: 0.361965  [92800/175341]\n",
      "loss: 0.686585  [94400/175341]\n",
      "loss: 0.197897  [96000/175341]\n",
      "loss: 0.568715  [97600/175341]\n",
      "loss: 0.554585  [99200/175341]\n",
      "loss: 0.313737  [100800/175341]\n",
      "loss: 0.278031  [102400/175341]\n",
      "loss: 0.564571  [104000/175341]\n",
      "loss: 0.615199  [105600/175341]\n",
      "loss: 0.475552  [107200/175341]\n",
      "loss: 0.420987  [108800/175341]\n",
      "loss: 0.429224  [110400/175341]\n",
      "loss: 0.556148  [112000/175341]\n",
      "loss: 0.532973  [113600/175341]\n",
      "loss: 0.546149  [115200/175341]\n",
      "loss: 0.467059  [116800/175341]\n",
      "loss: 0.422208  [118400/175341]\n",
      "loss: 0.535502  [120000/175341]\n",
      "loss: 0.351707  [121600/175341]\n",
      "loss: 0.363345  [123200/175341]\n",
      "loss: 0.344828  [124800/175341]\n",
      "loss: 0.261595  [126400/175341]\n",
      "loss: 0.386657  [128000/175341]\n",
      "loss: 0.447553  [129600/175341]\n",
      "loss: 0.410106  [131200/175341]\n",
      "loss: 0.599425  [132800/175341]\n",
      "loss: 0.289077  [134400/175341]\n",
      "loss: 0.438404  [136000/175341]\n",
      "loss: 0.595630  [137600/175341]\n",
      "loss: 0.161015  [139200/175341]\n",
      "loss: 0.420700  [140800/175341]\n",
      "loss: 0.682836  [142400/175341]\n",
      "loss: 0.795958  [144000/175341]\n",
      "loss: 1.088222  [145600/175341]\n",
      "loss: 0.524030  [147200/175341]\n",
      "loss: 0.444478  [148800/175341]\n",
      "loss: 0.234557  [150400/175341]\n",
      "loss: 0.232670  [152000/175341]\n",
      "loss: 0.572581  [153600/175341]\n",
      "loss: 0.463909  [155200/175341]\n",
      "loss: 0.373289  [156800/175341]\n",
      "loss: 0.282499  [158400/175341]\n",
      "loss: 0.763852  [160000/175341]\n",
      "loss: 0.455624  [161600/175341]\n",
      "loss: 0.365181  [163200/175341]\n",
      "loss: 0.661165  [164800/175341]\n",
      "loss: 0.808594  [166400/175341]\n",
      "loss: 0.293903  [168000/175341]\n",
      "loss: 0.418950  [169600/175341]\n",
      "loss: 0.295085  [171200/175341]\n",
      "loss: 0.580332  [172800/175341]\n",
      "loss: 0.250383  [174400/175341]\n",
      "Train Accuracy: 81.5582%\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.534285, F1-score: 77.43%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.211518  [    0/175341]\n",
      "loss: 0.395473  [ 1600/175341]\n",
      "loss: 0.669678  [ 3200/175341]\n",
      "loss: 0.447682  [ 4800/175341]\n",
      "loss: 0.436317  [ 6400/175341]\n",
      "loss: 0.227774  [ 8000/175341]\n",
      "loss: 0.258811  [ 9600/175341]\n",
      "loss: 0.679475  [11200/175341]\n",
      "loss: 0.396255  [12800/175341]\n",
      "loss: 0.378486  [14400/175341]\n",
      "loss: 0.376184  [16000/175341]\n",
      "loss: 0.564776  [17600/175341]\n",
      "loss: 0.441587  [19200/175341]\n",
      "loss: 0.471525  [20800/175341]\n",
      "loss: 0.537141  [22400/175341]\n",
      "loss: 0.676466  [24000/175341]\n",
      "loss: 0.070957  [25600/175341]\n",
      "loss: 0.134134  [27200/175341]\n",
      "loss: 0.767018  [28800/175341]\n",
      "loss: 0.280960  [30400/175341]\n",
      "loss: 0.551409  [32000/175341]\n",
      "loss: 0.416449  [33600/175341]\n",
      "loss: 0.241063  [35200/175341]\n",
      "loss: 0.390125  [36800/175341]\n",
      "loss: 0.062408  [38400/175341]\n",
      "loss: 0.431346  [40000/175341]\n",
      "loss: 0.404829  [41600/175341]\n",
      "loss: 0.435761  [43200/175341]\n",
      "loss: 0.485993  [44800/175341]\n",
      "loss: 0.249586  [46400/175341]\n",
      "loss: 0.739728  [48000/175341]\n",
      "loss: 0.500097  [49600/175341]\n",
      "loss: 0.137880  [51200/175341]\n",
      "loss: 0.543196  [52800/175341]\n",
      "loss: 0.564854  [54400/175341]\n",
      "loss: 0.485261  [56000/175341]\n",
      "loss: 0.489881  [57600/175341]\n",
      "loss: 0.611081  [59200/175341]\n",
      "loss: 0.794665  [60800/175341]\n",
      "loss: 0.766888  [62400/175341]\n",
      "loss: 0.789570  [64000/175341]\n",
      "loss: 0.467095  [65600/175341]\n",
      "loss: 0.162331  [67200/175341]\n",
      "loss: 0.565059  [68800/175341]\n",
      "loss: 0.498016  [70400/175341]\n",
      "loss: 0.596052  [72000/175341]\n",
      "loss: 0.565066  [73600/175341]\n",
      "loss: 0.756372  [75200/175341]\n",
      "loss: 0.291826  [76800/175341]\n",
      "loss: 0.958256  [78400/175341]\n",
      "loss: 0.213125  [80000/175341]\n",
      "loss: 0.257034  [81600/175341]\n",
      "loss: 0.237975  [83200/175341]\n",
      "loss: 0.557021  [84800/175341]\n",
      "loss: 0.283554  [86400/175341]\n",
      "loss: 0.480320  [88000/175341]\n",
      "loss: 0.609723  [89600/175341]\n",
      "loss: 0.417143  [91200/175341]\n",
      "loss: 0.385459  [92800/175341]\n",
      "loss: 0.508072  [94400/175341]\n",
      "loss: 0.194411  [96000/175341]\n",
      "loss: 0.421881  [97600/175341]\n",
      "loss: 0.502886  [99200/175341]\n",
      "loss: 0.349356  [100800/175341]\n",
      "loss: 0.453772  [102400/175341]\n",
      "loss: 0.494194  [104000/175341]\n",
      "loss: 0.643888  [105600/175341]\n",
      "loss: 0.516780  [107200/175341]\n",
      "loss: 0.650369  [108800/175341]\n",
      "loss: 0.443647  [110400/175341]\n",
      "loss: 0.489112  [112000/175341]\n",
      "loss: 1.004866  [113600/175341]\n",
      "loss: 0.416853  [115200/175341]\n",
      "loss: 0.370990  [116800/175341]\n",
      "loss: 0.730875  [118400/175341]\n",
      "loss: 0.591913  [120000/175341]\n",
      "loss: 0.463314  [121600/175341]\n",
      "loss: 0.611708  [123200/175341]\n",
      "loss: 0.659246  [124800/175341]\n",
      "loss: 1.158675  [126400/175341]\n",
      "loss: 0.329243  [128000/175341]\n",
      "loss: 0.300820  [129600/175341]\n",
      "loss: 0.931814  [131200/175341]\n",
      "loss: 0.741331  [132800/175341]\n",
      "loss: 0.281147  [134400/175341]\n",
      "loss: 0.237560  [136000/175341]\n",
      "loss: 0.475452  [137600/175341]\n",
      "loss: 0.536377  [139200/175341]\n",
      "loss: 0.384226  [140800/175341]\n",
      "loss: 0.635140  [142400/175341]\n",
      "loss: 0.808720  [144000/175341]\n",
      "loss: 0.377837  [145600/175341]\n",
      "loss: 0.723085  [147200/175341]\n",
      "loss: 0.362031  [148800/175341]\n",
      "loss: 0.228962  [150400/175341]\n",
      "loss: 0.270477  [152000/175341]\n",
      "loss: 0.461536  [153600/175341]\n",
      "loss: 0.503556  [155200/175341]\n",
      "loss: 0.458105  [156800/175341]\n",
      "loss: 0.634945  [158400/175341]\n",
      "loss: 0.482030  [160000/175341]\n",
      "loss: 0.422743  [161600/175341]\n",
      "loss: 0.346100  [163200/175341]\n",
      "loss: 0.812585  [164800/175341]\n",
      "loss: 0.641667  [166400/175341]\n",
      "loss: 0.535209  [168000/175341]\n",
      "loss: 0.586557  [169600/175341]\n",
      "loss: 1.282781  [171200/175341]\n",
      "loss: 0.453093  [172800/175341]\n",
      "loss: 1.036497  [174400/175341]\n",
      "Train Accuracy: 81.5223%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.544734, F1-score: 76.15%, Macro_F1-Score:  40.95%  \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.620741  [    0/175341]\n",
      "loss: 0.336459  [ 1600/175341]\n",
      "loss: 0.308792  [ 3200/175341]\n",
      "loss: 0.717417  [ 4800/175341]\n",
      "loss: 0.749260  [ 6400/175341]\n",
      "loss: 0.436254  [ 8000/175341]\n",
      "loss: 0.899552  [ 9600/175341]\n",
      "loss: 0.235768  [11200/175341]\n",
      "loss: 0.276261  [12800/175341]\n",
      "loss: 0.378002  [14400/175341]\n",
      "loss: 0.271318  [16000/175341]\n",
      "loss: 0.733229  [17600/175341]\n",
      "loss: 0.284759  [19200/175341]\n",
      "loss: 0.517574  [20800/175341]\n",
      "loss: 0.137357  [22400/175341]\n",
      "loss: 0.234747  [24000/175341]\n",
      "loss: 0.293782  [25600/175341]\n",
      "loss: 0.706507  [27200/175341]\n",
      "loss: 0.401000  [28800/175341]\n",
      "loss: 0.209802  [30400/175341]\n",
      "loss: 1.069704  [32000/175341]\n",
      "loss: 0.167442  [33600/175341]\n",
      "loss: 0.234229  [35200/175341]\n",
      "loss: 0.408372  [36800/175341]\n",
      "loss: 0.615403  [38400/175341]\n",
      "loss: 0.461242  [40000/175341]\n",
      "loss: 0.320100  [41600/175341]\n",
      "loss: 0.313357  [43200/175341]\n",
      "loss: 0.883589  [44800/175341]\n",
      "loss: 0.526101  [46400/175341]\n",
      "loss: 0.445816  [48000/175341]\n",
      "loss: 0.442948  [49600/175341]\n",
      "loss: 0.525444  [51200/175341]\n",
      "loss: 0.813264  [52800/175341]\n",
      "loss: 0.432841  [54400/175341]\n",
      "loss: 0.232405  [56000/175341]\n",
      "loss: 0.266166  [57600/175341]\n",
      "loss: 0.293935  [59200/175341]\n",
      "loss: 0.848817  [60800/175341]\n",
      "loss: 0.403228  [62400/175341]\n",
      "loss: 0.606887  [64000/175341]\n",
      "loss: 0.151882  [65600/175341]\n",
      "loss: 0.565987  [67200/175341]\n",
      "loss: 0.397136  [68800/175341]\n",
      "loss: 0.683347  [70400/175341]\n",
      "loss: 0.172715  [72000/175341]\n",
      "loss: 0.377510  [73600/175341]\n",
      "loss: 0.675423  [75200/175341]\n",
      "loss: 0.717397  [76800/175341]\n",
      "loss: 0.316967  [78400/175341]\n",
      "loss: 0.350412  [80000/175341]\n",
      "loss: 0.572068  [81600/175341]\n",
      "loss: 0.485226  [83200/175341]\n",
      "loss: 0.416663  [84800/175341]\n",
      "loss: 0.876217  [86400/175341]\n",
      "loss: 0.776913  [88000/175341]\n",
      "loss: 0.357277  [89600/175341]\n",
      "loss: 0.413614  [91200/175341]\n",
      "loss: 0.386094  [92800/175341]\n",
      "loss: 0.273222  [94400/175341]\n",
      "loss: 0.695004  [96000/175341]\n",
      "loss: 0.714753  [97600/175341]\n",
      "loss: 0.583522  [99200/175341]\n",
      "loss: 0.400054  [100800/175341]\n",
      "loss: 0.639748  [102400/175341]\n",
      "loss: 0.352640  [104000/175341]\n",
      "loss: 0.463573  [105600/175341]\n",
      "loss: 0.572534  [107200/175341]\n",
      "loss: 0.316474  [108800/175341]\n",
      "loss: 1.013363  [110400/175341]\n",
      "loss: 0.380784  [112000/175341]\n",
      "loss: 0.472432  [113600/175341]\n",
      "loss: 0.460695  [115200/175341]\n",
      "loss: 0.697641  [116800/175341]\n",
      "loss: 0.434455  [118400/175341]\n",
      "loss: 1.274098  [120000/175341]\n",
      "loss: 0.549910  [121600/175341]\n",
      "loss: 0.670824  [123200/175341]\n",
      "loss: 0.423879  [124800/175341]\n",
      "loss: 0.210131  [126400/175341]\n",
      "loss: 0.546917  [128000/175341]\n",
      "loss: 0.246085  [129600/175341]\n",
      "loss: 0.783921  [131200/175341]\n",
      "loss: 0.565062  [132800/175341]\n",
      "loss: 0.565261  [134400/175341]\n",
      "loss: 0.452792  [136000/175341]\n",
      "loss: 0.302610  [137600/175341]\n",
      "loss: 0.454327  [139200/175341]\n",
      "loss: 0.205802  [140800/175341]\n",
      "loss: 0.543059  [142400/175341]\n",
      "loss: 0.665493  [144000/175341]\n",
      "loss: 0.191538  [145600/175341]\n",
      "loss: 0.772905  [147200/175341]\n",
      "loss: 0.233828  [148800/175341]\n",
      "loss: 0.481345  [150400/175341]\n",
      "loss: 0.570000  [152000/175341]\n",
      "loss: 0.330892  [153600/175341]\n",
      "loss: 0.621437  [155200/175341]\n",
      "loss: 0.336357  [156800/175341]\n",
      "loss: 0.480312  [158400/175341]\n",
      "loss: 0.233542  [160000/175341]\n",
      "loss: 0.141449  [161600/175341]\n",
      "loss: 0.452136  [163200/175341]\n",
      "loss: 0.751314  [164800/175341]\n",
      "loss: 0.196032  [166400/175341]\n",
      "loss: 0.327267  [168000/175341]\n",
      "loss: 0.721708  [169600/175341]\n",
      "loss: 0.267532  [171200/175341]\n",
      "loss: 0.645863  [172800/175341]\n",
      "loss: 0.494098  [174400/175341]\n",
      "Train Accuracy: 81.5651%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.559780, F1-score: 75.73%, Macro_F1-Score:  40.77%  \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.773058  [    0/175341]\n",
      "loss: 0.408542  [ 1600/175341]\n",
      "loss: 0.779143  [ 3200/175341]\n",
      "loss: 0.453438  [ 4800/175341]\n",
      "loss: 0.102363  [ 6400/175341]\n",
      "loss: 0.416072  [ 8000/175341]\n",
      "loss: 0.265951  [ 9600/175341]\n",
      "loss: 0.747995  [11200/175341]\n",
      "loss: 0.478634  [12800/175341]\n",
      "loss: 0.392959  [14400/175341]\n",
      "loss: 0.765156  [16000/175341]\n",
      "loss: 0.590152  [17600/175341]\n",
      "loss: 0.751804  [19200/175341]\n",
      "loss: 0.357359  [20800/175341]\n",
      "loss: 0.510343  [22400/175341]\n",
      "loss: 0.625633  [24000/175341]\n",
      "loss: 0.596263  [25600/175341]\n",
      "loss: 0.547565  [27200/175341]\n",
      "loss: 0.408116  [28800/175341]\n",
      "loss: 0.595133  [30400/175341]\n",
      "loss: 0.606693  [32000/175341]\n",
      "loss: 0.598402  [33600/175341]\n",
      "loss: 0.177236  [35200/175341]\n",
      "loss: 0.692367  [36800/175341]\n",
      "loss: 0.875487  [38400/175341]\n",
      "loss: 0.234809  [40000/175341]\n",
      "loss: 0.236624  [41600/175341]\n",
      "loss: 0.410858  [43200/175341]\n",
      "loss: 0.436424  [44800/175341]\n",
      "loss: 0.297815  [46400/175341]\n",
      "loss: 0.226251  [48000/175341]\n",
      "loss: 0.415144  [49600/175341]\n",
      "loss: 0.222056  [51200/175341]\n",
      "loss: 0.326050  [52800/175341]\n",
      "loss: 0.331793  [54400/175341]\n",
      "loss: 0.647962  [56000/175341]\n",
      "loss: 0.551603  [57600/175341]\n",
      "loss: 0.428922  [59200/175341]\n",
      "loss: 0.544334  [60800/175341]\n",
      "loss: 0.231546  [62400/175341]\n",
      "loss: 0.271423  [64000/175341]\n",
      "loss: 0.768629  [65600/175341]\n",
      "loss: 0.217535  [67200/175341]\n",
      "loss: 0.796090  [68800/175341]\n",
      "loss: 0.338555  [70400/175341]\n",
      "loss: 0.417205  [72000/175341]\n",
      "loss: 0.437391  [73600/175341]\n",
      "loss: 0.289687  [75200/175341]\n",
      "loss: 0.214296  [76800/175341]\n",
      "loss: 0.471460  [78400/175341]\n",
      "loss: 0.584251  [80000/175341]\n",
      "loss: 0.319340  [81600/175341]\n",
      "loss: 0.358230  [83200/175341]\n",
      "loss: 0.652070  [84800/175341]\n",
      "loss: 0.615548  [86400/175341]\n",
      "loss: 0.302744  [88000/175341]\n",
      "loss: 0.660751  [89600/175341]\n",
      "loss: 0.420348  [91200/175341]\n",
      "loss: 0.583696  [92800/175341]\n",
      "loss: 0.332436  [94400/175341]\n",
      "loss: 0.512112  [96000/175341]\n",
      "loss: 0.526372  [97600/175341]\n",
      "loss: 0.540623  [99200/175341]\n",
      "loss: 0.441749  [100800/175341]\n",
      "loss: 0.646594  [102400/175341]\n",
      "loss: 0.640312  [104000/175341]\n",
      "loss: 0.426894  [105600/175341]\n",
      "loss: 0.247512  [107200/175341]\n",
      "loss: 0.506047  [108800/175341]\n",
      "loss: 0.225712  [110400/175341]\n",
      "loss: 0.471851  [112000/175341]\n",
      "loss: 0.671319  [113600/175341]\n",
      "loss: 0.797022  [115200/175341]\n",
      "loss: 0.986477  [116800/175341]\n",
      "loss: 0.607561  [118400/175341]\n",
      "loss: 0.536582  [120000/175341]\n",
      "loss: 0.290584  [121600/175341]\n",
      "loss: 0.574665  [123200/175341]\n",
      "loss: 0.174986  [124800/175341]\n",
      "loss: 0.331581  [126400/175341]\n",
      "loss: 0.566806  [128000/175341]\n",
      "loss: 0.621766  [129600/175341]\n",
      "loss: 0.114925  [131200/175341]\n",
      "loss: 0.767442  [132800/175341]\n",
      "loss: 0.647304  [134400/175341]\n",
      "loss: 0.410276  [136000/175341]\n",
      "loss: 0.486675  [137600/175341]\n",
      "loss: 0.825331  [139200/175341]\n",
      "loss: 0.584116  [140800/175341]\n",
      "loss: 0.446752  [142400/175341]\n",
      "loss: 0.275768  [144000/175341]\n",
      "loss: 0.375443  [145600/175341]\n",
      "loss: 0.567524  [147200/175341]\n",
      "loss: 0.682000  [148800/175341]\n",
      "loss: 0.317808  [150400/175341]\n",
      "loss: 0.575621  [152000/175341]\n",
      "loss: 0.506100  [153600/175341]\n",
      "loss: 0.232977  [155200/175341]\n",
      "loss: 0.445271  [156800/175341]\n",
      "loss: 0.481797  [158400/175341]\n",
      "loss: 0.290785  [160000/175341]\n",
      "loss: 0.949700  [161600/175341]\n",
      "loss: 0.233410  [163200/175341]\n",
      "loss: 0.428370  [164800/175341]\n",
      "loss: 0.241595  [166400/175341]\n",
      "loss: 0.496100  [168000/175341]\n",
      "loss: 0.349508  [169600/175341]\n",
      "loss: 0.364784  [171200/175341]\n",
      "loss: 0.524300  [172800/175341]\n",
      "loss: 0.536194  [174400/175341]\n",
      "Train Accuracy: 81.5394%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.567560, F1-score: 75.15%, Macro_F1-Score:  39.98%  \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.373576  [    0/175341]\n",
      "loss: 0.625624  [ 1600/175341]\n",
      "loss: 0.662983  [ 3200/175341]\n",
      "loss: 0.331522  [ 4800/175341]\n",
      "loss: 0.434719  [ 6400/175341]\n",
      "loss: 0.695696  [ 8000/175341]\n",
      "loss: 0.494274  [ 9600/175341]\n",
      "loss: 0.299494  [11200/175341]\n",
      "loss: 0.509053  [12800/175341]\n",
      "loss: 0.638984  [14400/175341]\n",
      "loss: 0.476113  [16000/175341]\n",
      "loss: 0.432085  [17600/175341]\n",
      "loss: 0.154885  [19200/175341]\n",
      "loss: 0.662488  [20800/175341]\n",
      "loss: 0.387790  [22400/175341]\n",
      "loss: 0.661526  [24000/175341]\n",
      "loss: 0.524504  [25600/175341]\n",
      "loss: 0.470378  [27200/175341]\n",
      "loss: 1.379926  [28800/175341]\n",
      "loss: 0.430995  [30400/175341]\n",
      "loss: 0.423724  [32000/175341]\n",
      "loss: 0.738161  [33600/175341]\n",
      "loss: 0.854666  [35200/175341]\n",
      "loss: 0.750668  [36800/175341]\n",
      "loss: 0.359472  [38400/175341]\n",
      "loss: 0.718161  [40000/175341]\n",
      "loss: 0.327841  [41600/175341]\n",
      "loss: 0.533711  [43200/175341]\n",
      "loss: 0.381577  [44800/175341]\n",
      "loss: 0.377465  [46400/175341]\n",
      "loss: 0.248948  [48000/175341]\n",
      "loss: 0.304905  [49600/175341]\n",
      "loss: 0.158976  [51200/175341]\n",
      "loss: 0.162729  [52800/175341]\n",
      "loss: 0.541652  [54400/175341]\n",
      "loss: 0.187609  [56000/175341]\n",
      "loss: 0.316437  [57600/175341]\n",
      "loss: 0.516209  [59200/175341]\n",
      "loss: 0.413590  [60800/175341]\n",
      "loss: 0.998810  [62400/175341]\n",
      "loss: 0.667503  [64000/175341]\n",
      "loss: 0.722103  [65600/175341]\n",
      "loss: 0.362385  [67200/175341]\n",
      "loss: 0.437376  [68800/175341]\n",
      "loss: 0.309539  [70400/175341]\n",
      "loss: 0.112662  [72000/175341]\n",
      "loss: 0.532729  [73600/175341]\n",
      "loss: 0.422751  [75200/175341]\n",
      "loss: 0.359443  [76800/175341]\n",
      "loss: 0.351126  [78400/175341]\n",
      "loss: 0.433003  [80000/175341]\n",
      "loss: 0.363542  [81600/175341]\n",
      "loss: 0.791196  [83200/175341]\n",
      "loss: 0.448008  [84800/175341]\n",
      "loss: 0.677634  [86400/175341]\n",
      "loss: 0.157779  [88000/175341]\n",
      "loss: 0.483376  [89600/175341]\n",
      "loss: 0.213606  [91200/175341]\n",
      "loss: 0.361434  [92800/175341]\n",
      "loss: 0.388963  [94400/175341]\n",
      "loss: 0.328997  [96000/175341]\n",
      "loss: 0.489525  [97600/175341]\n",
      "loss: 0.749373  [99200/175341]\n",
      "loss: 0.661017  [100800/175341]\n",
      "loss: 0.314273  [102400/175341]\n",
      "loss: 0.403795  [104000/175341]\n",
      "loss: 0.381244  [105600/175341]\n",
      "loss: 0.819946  [107200/175341]\n",
      "loss: 0.367512  [108800/175341]\n",
      "loss: 0.373502  [110400/175341]\n",
      "loss: 0.494473  [112000/175341]\n",
      "loss: 0.309621  [113600/175341]\n",
      "loss: 0.484194  [115200/175341]\n",
      "loss: 0.724831  [116800/175341]\n",
      "loss: 0.296566  [118400/175341]\n",
      "loss: 0.485942  [120000/175341]\n",
      "loss: 0.443841  [121600/175341]\n",
      "loss: 0.409528  [123200/175341]\n",
      "loss: 0.517706  [124800/175341]\n",
      "loss: 0.238457  [126400/175341]\n",
      "loss: 0.190877  [128000/175341]\n",
      "loss: 0.183052  [129600/175341]\n",
      "loss: 0.288243  [131200/175341]\n",
      "loss: 0.262891  [132800/175341]\n",
      "loss: 0.519721  [134400/175341]\n",
      "loss: 0.448246  [136000/175341]\n",
      "loss: 0.233283  [137600/175341]\n",
      "loss: 0.347151  [139200/175341]\n",
      "loss: 0.308457  [140800/175341]\n",
      "loss: 0.272810  [142400/175341]\n",
      "loss: 0.259768  [144000/175341]\n",
      "loss: 0.364259  [145600/175341]\n",
      "loss: 0.742936  [147200/175341]\n",
      "loss: 0.881163  [148800/175341]\n",
      "loss: 0.442602  [150400/175341]\n",
      "loss: 0.540535  [152000/175341]\n",
      "loss: 0.363907  [153600/175341]\n",
      "loss: 0.277135  [155200/175341]\n",
      "loss: 0.814841  [156800/175341]\n",
      "loss: 0.410604  [158400/175341]\n",
      "loss: 0.724298  [160000/175341]\n",
      "loss: 0.341068  [161600/175341]\n",
      "loss: 0.653957  [163200/175341]\n",
      "loss: 0.201135  [164800/175341]\n",
      "loss: 0.556828  [166400/175341]\n",
      "loss: 0.290216  [168000/175341]\n",
      "loss: 0.244491  [169600/175341]\n",
      "loss: 0.533535  [171200/175341]\n",
      "loss: 0.263746  [172800/175341]\n",
      "loss: 0.354789  [174400/175341]\n",
      "Train Accuracy: 81.5668%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.551439, F1-score: 75.89%, Macro_F1-Score:  40.51%  \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.333931  [    0/175341]\n",
      "loss: 0.403072  [ 1600/175341]\n",
      "loss: 0.488577  [ 3200/175341]\n",
      "loss: 0.298330  [ 4800/175341]\n",
      "loss: 0.985143  [ 6400/175341]\n",
      "loss: 0.276229  [ 8000/175341]\n",
      "loss: 0.175502  [ 9600/175341]\n",
      "loss: 0.649057  [11200/175341]\n",
      "loss: 0.552978  [12800/175341]\n",
      "loss: 0.222550  [14400/175341]\n",
      "loss: 0.421073  [16000/175341]\n",
      "loss: 0.365543  [17600/175341]\n",
      "loss: 0.561241  [19200/175341]\n",
      "loss: 0.646851  [20800/175341]\n",
      "loss: 0.366003  [22400/175341]\n",
      "loss: 0.444537  [24000/175341]\n",
      "loss: 0.208790  [25600/175341]\n",
      "loss: 0.552142  [27200/175341]\n",
      "loss: 0.562125  [28800/175341]\n",
      "loss: 0.256923  [30400/175341]\n",
      "loss: 0.292171  [32000/175341]\n",
      "loss: 0.309956  [33600/175341]\n",
      "loss: 0.385982  [35200/175341]\n",
      "loss: 0.491901  [36800/175341]\n",
      "loss: 0.194177  [38400/175341]\n",
      "loss: 0.517593  [40000/175341]\n",
      "loss: 0.257755  [41600/175341]\n",
      "loss: 0.269788  [43200/175341]\n",
      "loss: 0.311910  [44800/175341]\n",
      "loss: 0.892202  [46400/175341]\n",
      "loss: 0.730064  [48000/175341]\n",
      "loss: 0.406505  [49600/175341]\n",
      "loss: 0.463396  [51200/175341]\n",
      "loss: 0.704466  [52800/175341]\n",
      "loss: 0.359886  [54400/175341]\n",
      "loss: 0.739295  [56000/175341]\n",
      "loss: 0.443594  [57600/175341]\n",
      "loss: 0.168320  [59200/175341]\n",
      "loss: 0.898989  [60800/175341]\n",
      "loss: 0.227360  [62400/175341]\n",
      "loss: 0.594079  [64000/175341]\n",
      "loss: 0.579710  [65600/175341]\n",
      "loss: 0.556688  [67200/175341]\n",
      "loss: 0.669562  [68800/175341]\n",
      "loss: 0.447234  [70400/175341]\n",
      "loss: 0.357915  [72000/175341]\n",
      "loss: 0.458841  [73600/175341]\n",
      "loss: 0.637400  [75200/175341]\n",
      "loss: 0.288163  [76800/175341]\n",
      "loss: 0.588823  [78400/175341]\n",
      "loss: 0.594166  [80000/175341]\n",
      "loss: 0.330907  [81600/175341]\n",
      "loss: 0.226003  [83200/175341]\n",
      "loss: 0.158323  [84800/175341]\n",
      "loss: 0.229759  [86400/175341]\n",
      "loss: 0.759975  [88000/175341]\n",
      "loss: 0.508505  [89600/175341]\n",
      "loss: 0.860251  [91200/175341]\n",
      "loss: 0.606154  [92800/175341]\n",
      "loss: 0.561614  [94400/175341]\n",
      "loss: 0.360855  [96000/175341]\n",
      "loss: 0.692845  [97600/175341]\n",
      "loss: 0.438493  [99200/175341]\n",
      "loss: 0.646349  [100800/175341]\n",
      "loss: 0.615439  [102400/175341]\n",
      "loss: 0.573132  [104000/175341]\n",
      "loss: 0.876311  [105600/175341]\n",
      "loss: 0.589254  [107200/175341]\n",
      "loss: 0.418477  [108800/175341]\n",
      "loss: 0.390998  [110400/175341]\n",
      "loss: 0.357008  [112000/175341]\n",
      "loss: 0.373528  [113600/175341]\n",
      "loss: 0.351011  [115200/175341]\n",
      "loss: 0.569094  [116800/175341]\n",
      "loss: 0.527735  [118400/175341]\n",
      "loss: 0.540065  [120000/175341]\n",
      "loss: 0.514109  [121600/175341]\n",
      "loss: 0.331645  [123200/175341]\n",
      "loss: 0.378485  [124800/175341]\n",
      "loss: 0.662637  [126400/175341]\n",
      "loss: 0.655420  [128000/175341]\n",
      "loss: 0.601258  [129600/175341]\n",
      "loss: 0.490050  [131200/175341]\n",
      "loss: 0.399039  [132800/175341]\n",
      "loss: 0.146510  [134400/175341]\n",
      "loss: 0.443977  [136000/175341]\n",
      "loss: 0.862058  [137600/175341]\n",
      "loss: 0.503667  [139200/175341]\n",
      "loss: 0.549290  [140800/175341]\n",
      "loss: 0.519307  [142400/175341]\n",
      "loss: 0.190503  [144000/175341]\n",
      "loss: 0.412749  [145600/175341]\n",
      "loss: 0.767258  [147200/175341]\n",
      "loss: 0.706664  [148800/175341]\n",
      "loss: 0.339481  [150400/175341]\n",
      "loss: 0.647418  [152000/175341]\n",
      "loss: 0.587055  [153600/175341]\n",
      "loss: 1.025697  [155200/175341]\n",
      "loss: 0.416754  [156800/175341]\n",
      "loss: 0.576828  [158400/175341]\n",
      "loss: 0.723519  [160000/175341]\n",
      "loss: 0.491075  [161600/175341]\n",
      "loss: 0.721892  [163200/175341]\n",
      "loss: 0.687573  [164800/175341]\n",
      "loss: 0.302212  [166400/175341]\n",
      "loss: 0.550875  [168000/175341]\n",
      "loss: 0.334190  [169600/175341]\n",
      "loss: 0.498885  [171200/175341]\n",
      "loss: 0.844755  [172800/175341]\n",
      "loss: 0.445017  [174400/175341]\n",
      "Train Accuracy: 81.5326%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.551598, F1-score: 76.22%, Macro_F1-Score:  40.62%  \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.280280  [    0/175341]\n",
      "loss: 0.237497  [ 1600/175341]\n",
      "loss: 0.520798  [ 3200/175341]\n",
      "loss: 0.124254  [ 4800/175341]\n",
      "loss: 0.249112  [ 6400/175341]\n",
      "loss: 0.551350  [ 8000/175341]\n",
      "loss: 0.288690  [ 9600/175341]\n",
      "loss: 0.131912  [11200/175341]\n",
      "loss: 0.500943  [12800/175341]\n",
      "loss: 0.240926  [14400/175341]\n",
      "loss: 0.106711  [16000/175341]\n",
      "loss: 0.237594  [17600/175341]\n",
      "loss: 0.148756  [19200/175341]\n",
      "loss: 0.508882  [20800/175341]\n",
      "loss: 0.657986  [22400/175341]\n",
      "loss: 0.236234  [24000/175341]\n",
      "loss: 0.331149  [25600/175341]\n",
      "loss: 0.604098  [27200/175341]\n",
      "loss: 0.987205  [28800/175341]\n",
      "loss: 0.765623  [30400/175341]\n",
      "loss: 0.394321  [32000/175341]\n",
      "loss: 0.505045  [33600/175341]\n",
      "loss: 0.352681  [35200/175341]\n",
      "loss: 0.251869  [36800/175341]\n",
      "loss: 0.759214  [38400/175341]\n",
      "loss: 0.453797  [40000/175341]\n",
      "loss: 0.421360  [41600/175341]\n",
      "loss: 0.444641  [43200/175341]\n",
      "loss: 0.250799  [44800/175341]\n",
      "loss: 0.721455  [46400/175341]\n",
      "loss: 0.271152  [48000/175341]\n",
      "loss: 0.460669  [49600/175341]\n",
      "loss: 0.557198  [51200/175341]\n",
      "loss: 0.340851  [52800/175341]\n",
      "loss: 0.425976  [54400/175341]\n",
      "loss: 0.453297  [56000/175341]\n",
      "loss: 0.286072  [57600/175341]\n",
      "loss: 0.300988  [59200/175341]\n",
      "loss: 0.414140  [60800/175341]\n",
      "loss: 0.500403  [62400/175341]\n",
      "loss: 0.671413  [64000/175341]\n",
      "loss: 0.668849  [65600/175341]\n",
      "loss: 0.651420  [67200/175341]\n",
      "loss: 0.536998  [68800/175341]\n",
      "loss: 0.353586  [70400/175341]\n",
      "loss: 0.260151  [72000/175341]\n",
      "loss: 0.644158  [73600/175341]\n",
      "loss: 0.574262  [75200/175341]\n",
      "loss: 0.770033  [76800/175341]\n",
      "loss: 0.421268  [78400/175341]\n",
      "loss: 0.383971  [80000/175341]\n",
      "loss: 0.237717  [81600/175341]\n",
      "loss: 0.426909  [83200/175341]\n",
      "loss: 0.434372  [84800/175341]\n",
      "loss: 0.455692  [86400/175341]\n",
      "loss: 0.456915  [88000/175341]\n",
      "loss: 0.250431  [89600/175341]\n",
      "loss: 0.479818  [91200/175341]\n",
      "loss: 0.664515  [92800/175341]\n",
      "loss: 1.140580  [94400/175341]\n",
      "loss: 0.383500  [96000/175341]\n",
      "loss: 0.265038  [97600/175341]\n",
      "loss: 0.241922  [99200/175341]\n",
      "loss: 0.798821  [100800/175341]\n",
      "loss: 0.455079  [102400/175341]\n",
      "loss: 0.741322  [104000/175341]\n",
      "loss: 0.318687  [105600/175341]\n",
      "loss: 0.164539  [107200/175341]\n",
      "loss: 0.307079  [108800/175341]\n",
      "loss: 0.311727  [110400/175341]\n",
      "loss: 0.501129  [112000/175341]\n",
      "loss: 0.453309  [113600/175341]\n",
      "loss: 0.569300  [115200/175341]\n",
      "loss: 0.489883  [116800/175341]\n",
      "loss: 0.455243  [118400/175341]\n",
      "loss: 0.589146  [120000/175341]\n",
      "loss: 0.307059  [121600/175341]\n",
      "loss: 0.359601  [123200/175341]\n",
      "loss: 0.281686  [124800/175341]\n",
      "loss: 0.732819  [126400/175341]\n",
      "loss: 0.450912  [128000/175341]\n",
      "loss: 0.072324  [129600/175341]\n",
      "loss: 0.222011  [131200/175341]\n",
      "loss: 0.522177  [132800/175341]\n",
      "loss: 0.275610  [134400/175341]\n",
      "loss: 0.325569  [136000/175341]\n",
      "loss: 0.418635  [137600/175341]\n",
      "loss: 0.336964  [139200/175341]\n",
      "loss: 0.435824  [140800/175341]\n",
      "loss: 0.402346  [142400/175341]\n",
      "loss: 0.445230  [144000/175341]\n",
      "loss: 0.748149  [145600/175341]\n",
      "loss: 0.468559  [147200/175341]\n",
      "loss: 0.752040  [148800/175341]\n",
      "loss: 0.843350  [150400/175341]\n",
      "loss: 0.446414  [152000/175341]\n",
      "loss: 0.545178  [153600/175341]\n",
      "loss: 0.497852  [155200/175341]\n",
      "loss: 0.442699  [156800/175341]\n",
      "loss: 0.533005  [158400/175341]\n",
      "loss: 0.334198  [160000/175341]\n",
      "loss: 0.597001  [161600/175341]\n",
      "loss: 0.193723  [163200/175341]\n",
      "loss: 0.604054  [164800/175341]\n",
      "loss: 0.310110  [166400/175341]\n",
      "loss: 0.600208  [168000/175341]\n",
      "loss: 0.952241  [169600/175341]\n",
      "loss: 0.382739  [171200/175341]\n",
      "loss: 0.464130  [172800/175341]\n",
      "loss: 0.574686  [174400/175341]\n",
      "Train Accuracy: 81.5725%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.558079, F1-score: 75.84%, Macro_F1-Score:  40.93%  \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.930112  [    0/175341]\n",
      "loss: 0.448668  [ 1600/175341]\n",
      "loss: 0.423445  [ 3200/175341]\n",
      "loss: 0.322327  [ 4800/175341]\n",
      "loss: 0.337974  [ 6400/175341]\n",
      "loss: 0.346646  [ 8000/175341]\n",
      "loss: 0.470565  [ 9600/175341]\n",
      "loss: 0.428094  [11200/175341]\n",
      "loss: 0.613213  [12800/175341]\n",
      "loss: 0.243655  [14400/175341]\n",
      "loss: 0.338531  [16000/175341]\n",
      "loss: 0.444512  [17600/175341]\n",
      "loss: 0.690507  [19200/175341]\n",
      "loss: 0.441993  [20800/175341]\n",
      "loss: 0.555755  [22400/175341]\n",
      "loss: 0.680085  [24000/175341]\n",
      "loss: 0.462809  [25600/175341]\n",
      "loss: 0.221205  [27200/175341]\n",
      "loss: 0.202446  [28800/175341]\n",
      "loss: 0.559133  [30400/175341]\n",
      "loss: 0.200764  [32000/175341]\n",
      "loss: 0.209342  [33600/175341]\n",
      "loss: 0.313984  [35200/175341]\n",
      "loss: 0.575610  [36800/175341]\n",
      "loss: 0.676599  [38400/175341]\n",
      "loss: 0.300949  [40000/175341]\n",
      "loss: 0.474842  [41600/175341]\n",
      "loss: 0.301847  [43200/175341]\n",
      "loss: 0.115441  [44800/175341]\n",
      "loss: 0.955527  [46400/175341]\n",
      "loss: 0.283528  [48000/175341]\n",
      "loss: 0.731316  [49600/175341]\n",
      "loss: 0.348234  [51200/175341]\n",
      "loss: 0.459359  [52800/175341]\n",
      "loss: 0.504909  [54400/175341]\n",
      "loss: 0.497234  [56000/175341]\n",
      "loss: 0.419007  [57600/175341]\n",
      "loss: 0.899162  [59200/175341]\n",
      "loss: 0.404959  [60800/175341]\n",
      "loss: 0.298608  [62400/175341]\n",
      "loss: 0.338660  [64000/175341]\n",
      "loss: 0.655856  [65600/175341]\n",
      "loss: 0.445766  [67200/175341]\n",
      "loss: 0.971027  [68800/175341]\n",
      "loss: 0.354403  [70400/175341]\n",
      "loss: 0.378930  [72000/175341]\n",
      "loss: 0.615924  [73600/175341]\n",
      "loss: 0.588621  [75200/175341]\n",
      "loss: 0.402849  [76800/175341]\n",
      "loss: 0.700982  [78400/175341]\n",
      "loss: 0.235220  [80000/175341]\n",
      "loss: 0.817796  [81600/175341]\n",
      "loss: 0.604015  [83200/175341]\n",
      "loss: 0.267221  [84800/175341]\n",
      "loss: 0.616199  [86400/175341]\n",
      "loss: 0.537968  [88000/175341]\n",
      "loss: 0.246028  [89600/175341]\n",
      "loss: 0.219744  [91200/175341]\n",
      "loss: 0.441013  [92800/175341]\n",
      "loss: 0.742016  [94400/175341]\n",
      "loss: 0.751123  [96000/175341]\n",
      "loss: 0.355041  [97600/175341]\n",
      "loss: 0.662977  [99200/175341]\n",
      "loss: 0.519814  [100800/175341]\n",
      "loss: 0.554916  [102400/175341]\n",
      "loss: 0.601567  [104000/175341]\n",
      "loss: 0.518849  [105600/175341]\n",
      "loss: 0.169704  [107200/175341]\n",
      "loss: 0.350539  [108800/175341]\n",
      "loss: 0.588550  [110400/175341]\n",
      "loss: 0.379145  [112000/175341]\n",
      "loss: 0.254753  [113600/175341]\n",
      "loss: 0.524638  [115200/175341]\n",
      "loss: 0.651276  [116800/175341]\n",
      "loss: 0.509004  [118400/175341]\n",
      "loss: 0.441423  [120000/175341]\n",
      "loss: 0.539184  [121600/175341]\n",
      "loss: 0.525598  [123200/175341]\n",
      "loss: 0.273331  [124800/175341]\n",
      "loss: 0.410841  [126400/175341]\n",
      "loss: 0.366886  [128000/175341]\n",
      "loss: 0.371236  [129600/175341]\n",
      "loss: 0.366147  [131200/175341]\n",
      "loss: 0.309142  [132800/175341]\n",
      "loss: 0.755753  [134400/175341]\n",
      "loss: 0.502218  [136000/175341]\n",
      "loss: 0.584071  [137600/175341]\n",
      "loss: 0.329492  [139200/175341]\n",
      "loss: 0.463440  [140800/175341]\n",
      "loss: 0.587055  [142400/175341]\n",
      "loss: 0.513682  [144000/175341]\n",
      "loss: 0.464782  [145600/175341]\n",
      "loss: 0.425075  [147200/175341]\n",
      "loss: 0.381003  [148800/175341]\n",
      "loss: 0.347117  [150400/175341]\n",
      "loss: 0.580757  [152000/175341]\n",
      "loss: 0.629012  [153600/175341]\n",
      "loss: 0.418107  [155200/175341]\n",
      "loss: 0.538108  [156800/175341]\n",
      "loss: 0.545437  [158400/175341]\n",
      "loss: 0.388665  [160000/175341]\n",
      "loss: 0.414171  [161600/175341]\n",
      "loss: 0.103761  [163200/175341]\n",
      "loss: 0.369851  [164800/175341]\n",
      "loss: 0.639797  [166400/175341]\n",
      "loss: 0.816175  [168000/175341]\n",
      "loss: 0.349330  [169600/175341]\n",
      "loss: 0.518959  [171200/175341]\n",
      "loss: 0.346349  [172800/175341]\n",
      "loss: 0.278204  [174400/175341]\n",
      "Train Accuracy: 81.5297%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.569593, F1-score: 75.41%, Macro_F1-Score:  41.31%  \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.417155  [    0/175341]\n",
      "loss: 0.521889  [ 1600/175341]\n",
      "loss: 0.649093  [ 3200/175341]\n",
      "loss: 0.717332  [ 4800/175341]\n",
      "loss: 0.394004  [ 6400/175341]\n",
      "loss: 0.572119  [ 8000/175341]\n",
      "loss: 0.391216  [ 9600/175341]\n",
      "loss: 0.140758  [11200/175341]\n",
      "loss: 0.295333  [12800/175341]\n",
      "loss: 0.464492  [14400/175341]\n",
      "loss: 0.560895  [16000/175341]\n",
      "loss: 0.471618  [17600/175341]\n",
      "loss: 0.384812  [19200/175341]\n",
      "loss: 0.453418  [20800/175341]\n",
      "loss: 0.304010  [22400/175341]\n",
      "loss: 0.365948  [24000/175341]\n",
      "loss: 0.354913  [25600/175341]\n",
      "loss: 0.293937  [27200/175341]\n",
      "loss: 0.408269  [28800/175341]\n",
      "loss: 0.128231  [30400/175341]\n",
      "loss: 0.629149  [32000/175341]\n",
      "loss: 0.408183  [33600/175341]\n",
      "loss: 0.285060  [35200/175341]\n",
      "loss: 0.298995  [36800/175341]\n",
      "loss: 0.336305  [38400/175341]\n",
      "loss: 0.561207  [40000/175341]\n",
      "loss: 0.683215  [41600/175341]\n",
      "loss: 0.482705  [43200/175341]\n",
      "loss: 0.107001  [44800/175341]\n",
      "loss: 0.406787  [46400/175341]\n",
      "loss: 0.555024  [48000/175341]\n",
      "loss: 0.562589  [49600/175341]\n",
      "loss: 0.604051  [51200/175341]\n",
      "loss: 0.535096  [52800/175341]\n",
      "loss: 0.368426  [54400/175341]\n",
      "loss: 0.344286  [56000/175341]\n",
      "loss: 0.678901  [57600/175341]\n",
      "loss: 0.296415  [59200/175341]\n",
      "loss: 0.454213  [60800/175341]\n",
      "loss: 0.098947  [62400/175341]\n",
      "loss: 0.532730  [64000/175341]\n",
      "loss: 0.449928  [65600/175341]\n",
      "loss: 0.637113  [67200/175341]\n",
      "loss: 0.244184  [68800/175341]\n",
      "loss: 0.510217  [70400/175341]\n",
      "loss: 0.499720  [72000/175341]\n",
      "loss: 0.348624  [73600/175341]\n",
      "loss: 0.619991  [75200/175341]\n",
      "loss: 1.083453  [76800/175341]\n",
      "loss: 0.454187  [78400/175341]\n",
      "loss: 0.528582  [80000/175341]\n",
      "loss: 0.526360  [81600/175341]\n",
      "loss: 0.405307  [83200/175341]\n",
      "loss: 0.471668  [84800/175341]\n",
      "loss: 0.250589  [86400/175341]\n",
      "loss: 0.699281  [88000/175341]\n",
      "loss: 0.265105  [89600/175341]\n",
      "loss: 0.605556  [91200/175341]\n",
      "loss: 0.985228  [92800/175341]\n",
      "loss: 0.303859  [94400/175341]\n",
      "loss: 0.400245  [96000/175341]\n",
      "loss: 0.496311  [97600/175341]\n",
      "loss: 0.419281  [99200/175341]\n",
      "loss: 0.334342  [100800/175341]\n",
      "loss: 0.491143  [102400/175341]\n",
      "loss: 0.621952  [104000/175341]\n",
      "loss: 0.518948  [105600/175341]\n",
      "loss: 0.638335  [107200/175341]\n",
      "loss: 0.372609  [108800/175341]\n",
      "loss: 0.674539  [110400/175341]\n",
      "loss: 0.307819  [112000/175341]\n",
      "loss: 0.632679  [113600/175341]\n",
      "loss: 0.500055  [115200/175341]\n",
      "loss: 0.172325  [116800/175341]\n",
      "loss: 0.582669  [118400/175341]\n",
      "loss: 0.445838  [120000/175341]\n",
      "loss: 0.448100  [121600/175341]\n",
      "loss: 0.324589  [123200/175341]\n",
      "loss: 0.913951  [124800/175341]\n",
      "loss: 0.380608  [126400/175341]\n",
      "loss: 0.214281  [128000/175341]\n",
      "loss: 0.521009  [129600/175341]\n",
      "loss: 0.160903  [131200/175341]\n",
      "loss: 0.330056  [132800/175341]\n",
      "loss: 0.444448  [134400/175341]\n",
      "loss: 0.475130  [136000/175341]\n",
      "loss: 0.567880  [137600/175341]\n",
      "loss: 0.407749  [139200/175341]\n",
      "loss: 0.354876  [140800/175341]\n",
      "loss: 0.814511  [142400/175341]\n",
      "loss: 0.192443  [144000/175341]\n",
      "loss: 0.277598  [145600/175341]\n",
      "loss: 0.501125  [147200/175341]\n",
      "loss: 0.346516  [148800/175341]\n",
      "loss: 0.602308  [150400/175341]\n",
      "loss: 0.333643  [152000/175341]\n",
      "loss: 0.756599  [153600/175341]\n",
      "loss: 0.659882  [155200/175341]\n",
      "loss: 0.450030  [156800/175341]\n",
      "loss: 0.557818  [158400/175341]\n",
      "loss: 0.371172  [160000/175341]\n",
      "loss: 0.293257  [161600/175341]\n",
      "loss: 0.348991  [163200/175341]\n",
      "loss: 0.268982  [164800/175341]\n",
      "loss: 0.566410  [166400/175341]\n",
      "loss: 0.325285  [168000/175341]\n",
      "loss: 0.575121  [169600/175341]\n",
      "loss: 0.618083  [171200/175341]\n",
      "loss: 0.072946  [172800/175341]\n",
      "loss: 0.471883  [174400/175341]\n",
      "Train Accuracy: 81.5673%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.544250, F1-score: 76.32%, Macro_F1-Score:  40.71%  \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.427161  [    0/175341]\n",
      "loss: 0.785338  [ 1600/175341]\n",
      "loss: 0.776719  [ 3200/175341]\n",
      "loss: 0.422680  [ 4800/175341]\n",
      "loss: 0.405291  [ 6400/175341]\n",
      "loss: 0.210556  [ 8000/175341]\n",
      "loss: 0.608355  [ 9600/175341]\n",
      "loss: 0.477430  [11200/175341]\n",
      "loss: 1.027041  [12800/175341]\n",
      "loss: 0.684445  [14400/175341]\n",
      "loss: 0.441015  [16000/175341]\n",
      "loss: 0.410966  [17600/175341]\n",
      "loss: 0.295146  [19200/175341]\n",
      "loss: 0.476248  [20800/175341]\n",
      "loss: 0.366061  [22400/175341]\n",
      "loss: 0.744785  [24000/175341]\n",
      "loss: 0.424909  [25600/175341]\n",
      "loss: 0.276919  [27200/175341]\n",
      "loss: 0.375158  [28800/175341]\n",
      "loss: 0.351948  [30400/175341]\n",
      "loss: 0.338055  [32000/175341]\n",
      "loss: 0.530674  [33600/175341]\n",
      "loss: 0.143259  [35200/175341]\n",
      "loss: 0.111942  [36800/175341]\n",
      "loss: 0.143715  [38400/175341]\n",
      "loss: 0.386413  [40000/175341]\n",
      "loss: 0.342534  [41600/175341]\n",
      "loss: 0.450063  [43200/175341]\n",
      "loss: 0.676898  [44800/175341]\n",
      "loss: 0.518782  [46400/175341]\n",
      "loss: 0.335079  [48000/175341]\n",
      "loss: 0.309123  [49600/175341]\n",
      "loss: 0.353410  [51200/175341]\n",
      "loss: 0.623000  [52800/175341]\n",
      "loss: 0.240188  [54400/175341]\n",
      "loss: 0.739441  [56000/175341]\n",
      "loss: 0.811242  [57600/175341]\n",
      "loss: 0.355728  [59200/175341]\n",
      "loss: 0.471020  [60800/175341]\n",
      "loss: 0.208844  [62400/175341]\n",
      "loss: 0.732915  [64000/175341]\n",
      "loss: 0.371240  [65600/175341]\n",
      "loss: 0.957902  [67200/175341]\n",
      "loss: 0.667399  [68800/175341]\n",
      "loss: 0.400418  [70400/175341]\n",
      "loss: 0.384608  [72000/175341]\n",
      "loss: 0.412169  [73600/175341]\n",
      "loss: 0.524409  [75200/175341]\n",
      "loss: 0.175077  [76800/175341]\n",
      "loss: 0.597291  [78400/175341]\n",
      "loss: 0.440318  [80000/175341]\n",
      "loss: 0.363578  [81600/175341]\n",
      "loss: 0.198985  [83200/175341]\n",
      "loss: 0.481568  [84800/175341]\n",
      "loss: 0.491898  [86400/175341]\n",
      "loss: 0.715046  [88000/175341]\n",
      "loss: 0.494093  [89600/175341]\n",
      "loss: 0.352057  [91200/175341]\n",
      "loss: 0.500023  [92800/175341]\n",
      "loss: 0.724277  [94400/175341]\n",
      "loss: 0.390627  [96000/175341]\n",
      "loss: 0.251269  [97600/175341]\n",
      "loss: 0.331439  [99200/175341]\n",
      "loss: 0.348175  [100800/175341]\n",
      "loss: 0.307944  [102400/175341]\n",
      "loss: 0.785164  [104000/175341]\n",
      "loss: 0.763036  [105600/175341]\n",
      "loss: 0.847346  [107200/175341]\n",
      "loss: 0.329122  [108800/175341]\n",
      "loss: 0.300248  [110400/175341]\n",
      "loss: 0.206673  [112000/175341]\n",
      "loss: 0.463152  [113600/175341]\n",
      "loss: 0.376378  [115200/175341]\n",
      "loss: 0.928944  [116800/175341]\n",
      "loss: 0.145121  [118400/175341]\n",
      "loss: 0.225951  [120000/175341]\n",
      "loss: 0.461183  [121600/175341]\n",
      "loss: 0.374212  [123200/175341]\n",
      "loss: 0.212995  [124800/175341]\n",
      "loss: 0.637673  [126400/175341]\n",
      "loss: 0.484484  [128000/175341]\n",
      "loss: 0.870830  [129600/175341]\n",
      "loss: 0.240490  [131200/175341]\n",
      "loss: 0.292888  [132800/175341]\n",
      "loss: 0.451398  [134400/175341]\n",
      "loss: 0.360218  [136000/175341]\n",
      "loss: 0.725283  [137600/175341]\n",
      "loss: 0.415092  [139200/175341]\n",
      "loss: 0.545617  [140800/175341]\n",
      "loss: 0.275608  [142400/175341]\n",
      "loss: 0.458560  [144000/175341]\n",
      "loss: 0.239724  [145600/175341]\n",
      "loss: 0.690715  [147200/175341]\n",
      "loss: 0.479383  [148800/175341]\n",
      "loss: 0.255425  [150400/175341]\n",
      "loss: 0.329756  [152000/175341]\n",
      "loss: 0.482770  [153600/175341]\n",
      "loss: 0.541873  [155200/175341]\n",
      "loss: 0.521204  [156800/175341]\n",
      "loss: 0.222373  [158400/175341]\n",
      "loss: 0.345768  [160000/175341]\n",
      "loss: 0.370275  [161600/175341]\n",
      "loss: 0.513691  [163200/175341]\n",
      "loss: 0.261290  [164800/175341]\n",
      "loss: 0.258609  [166400/175341]\n",
      "loss: 0.312551  [168000/175341]\n",
      "loss: 0.257794  [169600/175341]\n",
      "loss: 0.496735  [171200/175341]\n",
      "loss: 0.389318  [172800/175341]\n",
      "loss: 0.389861  [174400/175341]\n",
      "Train Accuracy: 81.5708%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.540028, F1-score: 76.29%, Macro_F1-Score:  41.00%  \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.470515  [    0/175341]\n",
      "loss: 0.400952  [ 1600/175341]\n",
      "loss: 0.528095  [ 3200/175341]\n",
      "loss: 0.241923  [ 4800/175341]\n",
      "loss: 0.640744  [ 6400/175341]\n",
      "loss: 0.186019  [ 8000/175341]\n",
      "loss: 0.124066  [ 9600/175341]\n",
      "loss: 0.569221  [11200/175341]\n",
      "loss: 0.469286  [12800/175341]\n",
      "loss: 0.511684  [14400/175341]\n",
      "loss: 0.524930  [16000/175341]\n",
      "loss: 0.683769  [17600/175341]\n",
      "loss: 0.240614  [19200/175341]\n",
      "loss: 0.110708  [20800/175341]\n",
      "loss: 0.194826  [22400/175341]\n",
      "loss: 0.106822  [24000/175341]\n",
      "loss: 0.537782  [25600/175341]\n",
      "loss: 0.803193  [27200/175341]\n",
      "loss: 0.491133  [28800/175341]\n",
      "loss: 0.722319  [30400/175341]\n",
      "loss: 0.198337  [32000/175341]\n",
      "loss: 0.613700  [33600/175341]\n",
      "loss: 0.419174  [35200/175341]\n",
      "loss: 0.400609  [36800/175341]\n",
      "loss: 0.488908  [38400/175341]\n",
      "loss: 0.428880  [40000/175341]\n",
      "loss: 1.201382  [41600/175341]\n",
      "loss: 0.306702  [43200/175341]\n",
      "loss: 0.212258  [44800/175341]\n",
      "loss: 0.584891  [46400/175341]\n",
      "loss: 0.546327  [48000/175341]\n",
      "loss: 0.451157  [49600/175341]\n",
      "loss: 0.276358  [51200/175341]\n",
      "loss: 0.839252  [52800/175341]\n",
      "loss: 0.563436  [54400/175341]\n",
      "loss: 0.589856  [56000/175341]\n",
      "loss: 0.427481  [57600/175341]\n",
      "loss: 0.145429  [59200/175341]\n",
      "loss: 0.367844  [60800/175341]\n",
      "loss: 0.287910  [62400/175341]\n",
      "loss: 0.543952  [64000/175341]\n",
      "loss: 0.285915  [65600/175341]\n",
      "loss: 0.204525  [67200/175341]\n",
      "loss: 0.365560  [68800/175341]\n",
      "loss: 0.553124  [70400/175341]\n",
      "loss: 0.478177  [72000/175341]\n",
      "loss: 0.487425  [73600/175341]\n",
      "loss: 0.252111  [75200/175341]\n",
      "loss: 0.477261  [76800/175341]\n",
      "loss: 0.249317  [78400/175341]\n",
      "loss: 0.379242  [80000/175341]\n",
      "loss: 0.442850  [81600/175341]\n",
      "loss: 0.513810  [83200/175341]\n",
      "loss: 0.707333  [84800/175341]\n",
      "loss: 0.253402  [86400/175341]\n",
      "loss: 0.561557  [88000/175341]\n",
      "loss: 0.757516  [89600/175341]\n",
      "loss: 0.569708  [91200/175341]\n",
      "loss: 0.505115  [92800/175341]\n",
      "loss: 0.720606  [94400/175341]\n",
      "loss: 0.493544  [96000/175341]\n",
      "loss: 0.298376  [97600/175341]\n",
      "loss: 0.186158  [99200/175341]\n",
      "loss: 0.254678  [100800/175341]\n",
      "loss: 0.581988  [102400/175341]\n",
      "loss: 0.449122  [104000/175341]\n",
      "loss: 0.864134  [105600/175341]\n",
      "loss: 0.255134  [107200/175341]\n",
      "loss: 0.409506  [108800/175341]\n",
      "loss: 0.387452  [110400/175341]\n",
      "loss: 0.311831  [112000/175341]\n",
      "loss: 0.791108  [113600/175341]\n",
      "loss: 0.602549  [115200/175341]\n",
      "loss: 0.350671  [116800/175341]\n",
      "loss: 0.714437  [118400/175341]\n",
      "loss: 0.501700  [120000/175341]\n",
      "loss: 0.613546  [121600/175341]\n",
      "loss: 0.566450  [123200/175341]\n",
      "loss: 0.396759  [124800/175341]\n",
      "loss: 0.344111  [126400/175341]\n",
      "loss: 0.461343  [128000/175341]\n",
      "loss: 0.439076  [129600/175341]\n",
      "loss: 0.648043  [131200/175341]\n",
      "loss: 0.965625  [132800/175341]\n",
      "loss: 0.401865  [134400/175341]\n",
      "loss: 0.646859  [136000/175341]\n",
      "loss: 0.615770  [137600/175341]\n",
      "loss: 0.285461  [139200/175341]\n",
      "loss: 0.648945  [140800/175341]\n",
      "loss: 0.359705  [142400/175341]\n",
      "loss: 0.299123  [144000/175341]\n",
      "loss: 0.228556  [145600/175341]\n",
      "loss: 0.527706  [147200/175341]\n",
      "loss: 0.642008  [148800/175341]\n",
      "loss: 0.355635  [150400/175341]\n",
      "loss: 1.107088  [152000/175341]\n",
      "loss: 0.740029  [153600/175341]\n",
      "loss: 0.305900  [155200/175341]\n",
      "loss: 0.499954  [156800/175341]\n",
      "loss: 0.124672  [158400/175341]\n",
      "loss: 0.720412  [160000/175341]\n",
      "loss: 0.290250  [161600/175341]\n",
      "loss: 0.471086  [163200/175341]\n",
      "loss: 0.618544  [164800/175341]\n",
      "loss: 0.343473  [166400/175341]\n",
      "loss: 0.489205  [168000/175341]\n",
      "loss: 0.365264  [169600/175341]\n",
      "loss: 0.206176  [171200/175341]\n",
      "loss: 0.617582  [172800/175341]\n",
      "loss: 0.871705  [174400/175341]\n",
      "Train Accuracy: 81.6187%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.555754, F1-score: 75.95%, Macro_F1-Score:  41.07%  \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.498073  [    0/175341]\n",
      "loss: 0.342058  [ 1600/175341]\n",
      "loss: 0.543047  [ 3200/175341]\n",
      "loss: 0.717351  [ 4800/175341]\n",
      "loss: 0.603296  [ 6400/175341]\n",
      "loss: 0.628296  [ 8000/175341]\n",
      "loss: 0.587745  [ 9600/175341]\n",
      "loss: 0.480446  [11200/175341]\n",
      "loss: 0.526737  [12800/175341]\n",
      "loss: 0.614547  [14400/175341]\n",
      "loss: 0.361386  [16000/175341]\n",
      "loss: 0.512127  [17600/175341]\n",
      "loss: 0.255136  [19200/175341]\n",
      "loss: 0.406441  [20800/175341]\n",
      "loss: 0.148980  [22400/175341]\n",
      "loss: 0.195987  [24000/175341]\n",
      "loss: 0.469985  [25600/175341]\n",
      "loss: 0.786461  [27200/175341]\n",
      "loss: 0.285277  [28800/175341]\n",
      "loss: 0.596122  [30400/175341]\n",
      "loss: 0.578027  [32000/175341]\n",
      "loss: 0.318458  [33600/175341]\n",
      "loss: 0.621749  [35200/175341]\n",
      "loss: 0.073480  [36800/175341]\n",
      "loss: 0.535784  [38400/175341]\n",
      "loss: 0.439150  [40000/175341]\n",
      "loss: 0.693221  [41600/175341]\n",
      "loss: 0.593933  [43200/175341]\n",
      "loss: 0.296455  [44800/175341]\n",
      "loss: 0.250493  [46400/175341]\n",
      "loss: 0.855547  [48000/175341]\n",
      "loss: 0.379660  [49600/175341]\n",
      "loss: 0.369639  [51200/175341]\n",
      "loss: 0.483805  [52800/175341]\n",
      "loss: 0.378550  [54400/175341]\n",
      "loss: 0.689479  [56000/175341]\n",
      "loss: 0.581366  [57600/175341]\n",
      "loss: 0.356639  [59200/175341]\n",
      "loss: 0.470291  [60800/175341]\n",
      "loss: 0.479526  [62400/175341]\n",
      "loss: 0.473720  [64000/175341]\n",
      "loss: 0.424209  [65600/175341]\n",
      "loss: 0.709520  [67200/175341]\n",
      "loss: 0.288819  [68800/175341]\n",
      "loss: 0.734366  [70400/175341]\n",
      "loss: 0.525855  [72000/175341]\n",
      "loss: 0.820607  [73600/175341]\n",
      "loss: 0.259355  [75200/175341]\n",
      "loss: 0.247014  [76800/175341]\n",
      "loss: 0.523810  [78400/175341]\n",
      "loss: 0.377150  [80000/175341]\n",
      "loss: 0.550051  [81600/175341]\n",
      "loss: 0.269187  [83200/175341]\n",
      "loss: 0.201467  [84800/175341]\n",
      "loss: 0.608834  [86400/175341]\n",
      "loss: 0.635817  [88000/175341]\n",
      "loss: 0.156027  [89600/175341]\n",
      "loss: 0.265099  [91200/175341]\n",
      "loss: 0.380979  [92800/175341]\n",
      "loss: 0.373807  [94400/175341]\n",
      "loss: 0.071638  [96000/175341]\n",
      "loss: 0.247956  [97600/175341]\n",
      "loss: 0.718075  [99200/175341]\n",
      "loss: 0.370426  [100800/175341]\n",
      "loss: 0.316547  [102400/175341]\n",
      "loss: 0.465823  [104000/175341]\n",
      "loss: 0.569081  [105600/175341]\n",
      "loss: 0.621671  [107200/175341]\n",
      "loss: 0.293038  [108800/175341]\n",
      "loss: 0.525344  [110400/175341]\n",
      "loss: 0.357196  [112000/175341]\n",
      "loss: 0.191492  [113600/175341]\n",
      "loss: 0.417582  [115200/175341]\n",
      "loss: 0.370027  [116800/175341]\n",
      "loss: 0.289426  [118400/175341]\n",
      "loss: 0.805446  [120000/175341]\n",
      "loss: 0.613423  [121600/175341]\n",
      "loss: 0.599570  [123200/175341]\n",
      "loss: 0.790267  [124800/175341]\n",
      "loss: 0.321020  [126400/175341]\n",
      "loss: 0.578399  [128000/175341]\n",
      "loss: 0.577226  [129600/175341]\n",
      "loss: 0.408247  [131200/175341]\n",
      "loss: 0.427703  [132800/175341]\n",
      "loss: 0.706794  [134400/175341]\n",
      "loss: 0.346758  [136000/175341]\n",
      "loss: 0.278692  [137600/175341]\n",
      "loss: 0.253667  [139200/175341]\n",
      "loss: 0.544497  [140800/175341]\n",
      "loss: 0.534561  [142400/175341]\n",
      "loss: 0.714922  [144000/175341]\n",
      "loss: 0.387021  [145600/175341]\n",
      "loss: 0.551637  [147200/175341]\n",
      "loss: 0.271972  [148800/175341]\n",
      "loss: 0.582621  [150400/175341]\n",
      "loss: 0.520015  [152000/175341]\n",
      "loss: 0.334751  [153600/175341]\n",
      "loss: 0.434304  [155200/175341]\n",
      "loss: 0.403915  [156800/175341]\n",
      "loss: 0.283092  [158400/175341]\n",
      "loss: 0.445572  [160000/175341]\n",
      "loss: 0.465560  [161600/175341]\n",
      "loss: 0.367668  [163200/175341]\n",
      "loss: 0.574986  [164800/175341]\n",
      "loss: 0.310475  [166400/175341]\n",
      "loss: 0.083856  [168000/175341]\n",
      "loss: 0.421784  [169600/175341]\n",
      "loss: 0.360885  [171200/175341]\n",
      "loss: 0.310576  [172800/175341]\n",
      "loss: 0.946125  [174400/175341]\n",
      "Train Accuracy: 81.6164%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.549824, F1-score: 76.15%, Macro_F1-Score:  41.06%  \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.927072  [    0/175341]\n",
      "loss: 0.575019  [ 1600/175341]\n",
      "loss: 0.226446  [ 3200/175341]\n",
      "loss: 0.677599  [ 4800/175341]\n",
      "loss: 0.253672  [ 6400/175341]\n",
      "loss: 0.416585  [ 8000/175341]\n",
      "loss: 0.477651  [ 9600/175341]\n",
      "loss: 0.559088  [11200/175341]\n",
      "loss: 0.674636  [12800/175341]\n",
      "loss: 0.625867  [14400/175341]\n",
      "loss: 0.486939  [16000/175341]\n",
      "loss: 0.389850  [17600/175341]\n",
      "loss: 0.418307  [19200/175341]\n",
      "loss: 0.480848  [20800/175341]\n",
      "loss: 0.368604  [22400/175341]\n",
      "loss: 0.174237  [24000/175341]\n",
      "loss: 0.355871  [25600/175341]\n",
      "loss: 0.370989  [27200/175341]\n",
      "loss: 0.519761  [28800/175341]\n",
      "loss: 0.442892  [30400/175341]\n",
      "loss: 0.257533  [32000/175341]\n",
      "loss: 0.247950  [33600/175341]\n",
      "loss: 0.557561  [35200/175341]\n",
      "loss: 0.929192  [36800/175341]\n",
      "loss: 0.517212  [38400/175341]\n",
      "loss: 0.873704  [40000/175341]\n",
      "loss: 0.476721  [41600/175341]\n",
      "loss: 0.754351  [43200/175341]\n",
      "loss: 0.500220  [44800/175341]\n",
      "loss: 0.281527  [46400/175341]\n",
      "loss: 0.230252  [48000/175341]\n",
      "loss: 0.549625  [49600/175341]\n",
      "loss: 0.399760  [51200/175341]\n",
      "loss: 0.854864  [52800/175341]\n",
      "loss: 0.725170  [54400/175341]\n",
      "loss: 0.340861  [56000/175341]\n",
      "loss: 0.454869  [57600/175341]\n",
      "loss: 0.523484  [59200/175341]\n",
      "loss: 0.473025  [60800/175341]\n",
      "loss: 0.351649  [62400/175341]\n",
      "loss: 0.835008  [64000/175341]\n",
      "loss: 0.259138  [65600/175341]\n",
      "loss: 0.613269  [67200/175341]\n",
      "loss: 0.766111  [68800/175341]\n",
      "loss: 0.568088  [70400/175341]\n",
      "loss: 0.315419  [72000/175341]\n",
      "loss: 0.594889  [73600/175341]\n",
      "loss: 0.371549  [75200/175341]\n",
      "loss: 0.602839  [76800/175341]\n",
      "loss: 0.672067  [78400/175341]\n",
      "loss: 0.218045  [80000/175341]\n",
      "loss: 0.761026  [81600/175341]\n",
      "loss: 0.452260  [83200/175341]\n",
      "loss: 0.473240  [84800/175341]\n",
      "loss: 0.629994  [86400/175341]\n",
      "loss: 0.391196  [88000/175341]\n",
      "loss: 0.316729  [89600/175341]\n",
      "loss: 0.630238  [91200/175341]\n",
      "loss: 0.207670  [92800/175341]\n",
      "loss: 0.270536  [94400/175341]\n",
      "loss: 0.271831  [96000/175341]\n",
      "loss: 0.567736  [97600/175341]\n",
      "loss: 0.255251  [99200/175341]\n",
      "loss: 0.519416  [100800/175341]\n",
      "loss: 0.383973  [102400/175341]\n",
      "loss: 0.525307  [104000/175341]\n",
      "loss: 0.366909  [105600/175341]\n",
      "loss: 0.189643  [107200/175341]\n",
      "loss: 0.275824  [108800/175341]\n",
      "loss: 0.462248  [110400/175341]\n",
      "loss: 0.372608  [112000/175341]\n",
      "loss: 0.640049  [113600/175341]\n",
      "loss: 0.266021  [115200/175341]\n",
      "loss: 0.356058  [116800/175341]\n",
      "loss: 0.594198  [118400/175341]\n",
      "loss: 0.688656  [120000/175341]\n",
      "loss: 0.305336  [121600/175341]\n",
      "loss: 0.251318  [123200/175341]\n",
      "loss: 0.679070  [124800/175341]\n",
      "loss: 0.418740  [126400/175341]\n",
      "loss: 0.358995  [128000/175341]\n",
      "loss: 0.500784  [129600/175341]\n",
      "loss: 0.499725  [131200/175341]\n",
      "loss: 0.542920  [132800/175341]\n",
      "loss: 0.220673  [134400/175341]\n",
      "loss: 0.475766  [136000/175341]\n",
      "loss: 0.392270  [137600/175341]\n",
      "loss: 0.239376  [139200/175341]\n",
      "loss: 0.481903  [140800/175341]\n",
      "loss: 0.461755  [142400/175341]\n",
      "loss: 0.349899  [144000/175341]\n",
      "loss: 0.739654  [145600/175341]\n",
      "loss: 0.218406  [147200/175341]\n",
      "loss: 0.280907  [148800/175341]\n",
      "loss: 0.420894  [150400/175341]\n",
      "loss: 0.440397  [152000/175341]\n",
      "loss: 0.392632  [153600/175341]\n",
      "loss: 0.162233  [155200/175341]\n",
      "loss: 0.937228  [156800/175341]\n",
      "loss: 0.342399  [158400/175341]\n",
      "loss: 0.662141  [160000/175341]\n",
      "loss: 0.561698  [161600/175341]\n",
      "loss: 0.556128  [163200/175341]\n",
      "loss: 0.398601  [164800/175341]\n",
      "loss: 0.141912  [166400/175341]\n",
      "loss: 0.333240  [168000/175341]\n",
      "loss: 0.468185  [169600/175341]\n",
      "loss: 0.594188  [171200/175341]\n",
      "loss: 0.260628  [172800/175341]\n",
      "loss: 0.568918  [174400/175341]\n",
      "Train Accuracy: 81.6295%\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.535770, F1-score: 77.26%, Macro_F1-Score:  41.38%  \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.151061  [    0/175341]\n",
      "loss: 0.340940  [ 1600/175341]\n",
      "loss: 0.389212  [ 3200/175341]\n",
      "loss: 0.337097  [ 4800/175341]\n",
      "loss: 0.749693  [ 6400/175341]\n",
      "loss: 0.606124  [ 8000/175341]\n",
      "loss: 0.657511  [ 9600/175341]\n",
      "loss: 0.461422  [11200/175341]\n",
      "loss: 0.207347  [12800/175341]\n",
      "loss: 0.536024  [14400/175341]\n",
      "loss: 0.473052  [16000/175341]\n",
      "loss: 0.684034  [17600/175341]\n",
      "loss: 0.114877  [19200/175341]\n",
      "loss: 0.232310  [20800/175341]\n",
      "loss: 0.409325  [22400/175341]\n",
      "loss: 0.574463  [24000/175341]\n",
      "loss: 0.435489  [25600/175341]\n",
      "loss: 0.786764  [27200/175341]\n",
      "loss: 0.118318  [28800/175341]\n",
      "loss: 0.593413  [30400/175341]\n",
      "loss: 0.179869  [32000/175341]\n",
      "loss: 0.193791  [33600/175341]\n",
      "loss: 0.430978  [35200/175341]\n",
      "loss: 0.733397  [36800/175341]\n",
      "loss: 0.659180  [38400/175341]\n",
      "loss: 0.280499  [40000/175341]\n",
      "loss: 0.276542  [41600/175341]\n",
      "loss: 0.256312  [43200/175341]\n",
      "loss: 0.364425  [44800/175341]\n",
      "loss: 0.154047  [46400/175341]\n",
      "loss: 0.699942  [48000/175341]\n",
      "loss: 0.841586  [49600/175341]\n",
      "loss: 0.518326  [51200/175341]\n",
      "loss: 0.427764  [52800/175341]\n",
      "loss: 0.446431  [54400/175341]\n",
      "loss: 0.838257  [56000/175341]\n",
      "loss: 0.262596  [57600/175341]\n",
      "loss: 0.325581  [59200/175341]\n",
      "loss: 0.537620  [60800/175341]\n",
      "loss: 0.330673  [62400/175341]\n",
      "loss: 0.689920  [64000/175341]\n",
      "loss: 0.622031  [65600/175341]\n",
      "loss: 0.488995  [67200/175341]\n",
      "loss: 0.278241  [68800/175341]\n",
      "loss: 0.302288  [70400/175341]\n",
      "loss: 0.382242  [72000/175341]\n",
      "loss: 0.269338  [73600/175341]\n",
      "loss: 0.615806  [75200/175341]\n",
      "loss: 0.411217  [76800/175341]\n",
      "loss: 0.436487  [78400/175341]\n",
      "loss: 0.499471  [80000/175341]\n",
      "loss: 0.239672  [81600/175341]\n",
      "loss: 0.540181  [83200/175341]\n",
      "loss: 0.502742  [84800/175341]\n",
      "loss: 0.533183  [86400/175341]\n",
      "loss: 0.345895  [88000/175341]\n",
      "loss: 1.210256  [89600/175341]\n",
      "loss: 0.636307  [91200/175341]\n",
      "loss: 0.402429  [92800/175341]\n",
      "loss: 0.478051  [94400/175341]\n",
      "loss: 0.618211  [96000/175341]\n",
      "loss: 0.538240  [97600/175341]\n",
      "loss: 0.051914  [99200/175341]\n",
      "loss: 0.578775  [100800/175341]\n",
      "loss: 0.553928  [102400/175341]\n",
      "loss: 0.416131  [104000/175341]\n",
      "loss: 0.795611  [105600/175341]\n",
      "loss: 0.435035  [107200/175341]\n",
      "loss: 1.045044  [108800/175341]\n",
      "loss: 0.566581  [110400/175341]\n",
      "loss: 0.343982  [112000/175341]\n",
      "loss: 0.315037  [113600/175341]\n",
      "loss: 0.602030  [115200/175341]\n",
      "loss: 0.546174  [116800/175341]\n",
      "loss: 0.320962  [118400/175341]\n",
      "loss: 0.192488  [120000/175341]\n",
      "loss: 0.255294  [121600/175341]\n",
      "loss: 0.742437  [123200/175341]\n",
      "loss: 0.726732  [124800/175341]\n",
      "loss: 0.885588  [126400/175341]\n",
      "loss: 0.241006  [128000/175341]\n",
      "loss: 0.302348  [129600/175341]\n",
      "loss: 0.672278  [131200/175341]\n",
      "loss: 0.517086  [132800/175341]\n",
      "loss: 0.352564  [134400/175341]\n",
      "loss: 0.663374  [136000/175341]\n",
      "loss: 0.721243  [137600/175341]\n",
      "loss: 0.641891  [139200/175341]\n",
      "loss: 0.379575  [140800/175341]\n",
      "loss: 0.406601  [142400/175341]\n",
      "loss: 0.184109  [144000/175341]\n",
      "loss: 0.485995  [145600/175341]\n",
      "loss: 0.478495  [147200/175341]\n",
      "loss: 0.212934  [148800/175341]\n",
      "loss: 0.489897  [150400/175341]\n",
      "loss: 0.354280  [152000/175341]\n",
      "loss: 1.098716  [153600/175341]\n",
      "loss: 0.267745  [155200/175341]\n",
      "loss: 0.584433  [156800/175341]\n",
      "loss: 0.364327  [158400/175341]\n",
      "loss: 0.284565  [160000/175341]\n",
      "loss: 0.375961  [161600/175341]\n",
      "loss: 0.544201  [163200/175341]\n",
      "loss: 0.202049  [164800/175341]\n",
      "loss: 0.426831  [166400/175341]\n",
      "loss: 0.333237  [168000/175341]\n",
      "loss: 0.820089  [169600/175341]\n",
      "loss: 0.323548  [171200/175341]\n",
      "loss: 0.257646  [172800/175341]\n",
      "loss: 0.210558  [174400/175341]\n",
      "Train Accuracy: 81.5788%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.557644, F1-score: 75.83%, Macro_F1-Score:  41.02%  \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.268817  [    0/175341]\n",
      "loss: 0.822649  [ 1600/175341]\n",
      "loss: 0.265059  [ 3200/175341]\n",
      "loss: 0.393867  [ 4800/175341]\n",
      "loss: 0.691167  [ 6400/175341]\n",
      "loss: 0.396066  [ 8000/175341]\n",
      "loss: 0.486884  [ 9600/175341]\n",
      "loss: 0.545639  [11200/175341]\n",
      "loss: 0.469156  [12800/175341]\n",
      "loss: 0.423425  [14400/175341]\n",
      "loss: 0.569604  [16000/175341]\n",
      "loss: 0.616284  [17600/175341]\n",
      "loss: 0.684370  [19200/175341]\n",
      "loss: 0.325288  [20800/175341]\n",
      "loss: 0.214764  [22400/175341]\n",
      "loss: 0.570661  [24000/175341]\n",
      "loss: 0.299795  [25600/175341]\n",
      "loss: 0.819672  [27200/175341]\n",
      "loss: 0.531243  [28800/175341]\n",
      "loss: 0.813209  [30400/175341]\n",
      "loss: 0.307321  [32000/175341]\n",
      "loss: 0.272080  [33600/175341]\n",
      "loss: 0.539668  [35200/175341]\n",
      "loss: 0.275255  [36800/175341]\n",
      "loss: 0.220263  [38400/175341]\n",
      "loss: 0.071924  [40000/175341]\n",
      "loss: 0.604232  [41600/175341]\n",
      "loss: 0.674576  [43200/175341]\n",
      "loss: 0.721363  [44800/175341]\n",
      "loss: 0.102337  [46400/175341]\n",
      "loss: 0.729122  [48000/175341]\n",
      "loss: 0.595851  [49600/175341]\n",
      "loss: 0.267287  [51200/175341]\n",
      "loss: 0.469512  [52800/175341]\n",
      "loss: 1.076245  [54400/175341]\n",
      "loss: 0.361145  [56000/175341]\n",
      "loss: 0.658019  [57600/175341]\n",
      "loss: 0.482794  [59200/175341]\n",
      "loss: 0.732711  [60800/175341]\n",
      "loss: 0.266165  [62400/175341]\n",
      "loss: 0.518739  [64000/175341]\n",
      "loss: 0.229833  [65600/175341]\n",
      "loss: 0.439707  [67200/175341]\n",
      "loss: 0.500471  [68800/175341]\n",
      "loss: 0.478177  [70400/175341]\n",
      "loss: 0.614600  [72000/175341]\n",
      "loss: 0.219828  [73600/175341]\n",
      "loss: 0.304176  [75200/175341]\n",
      "loss: 0.821305  [76800/175341]\n",
      "loss: 0.669298  [78400/175341]\n",
      "loss: 0.353074  [80000/175341]\n",
      "loss: 0.715096  [81600/175341]\n",
      "loss: 0.392592  [83200/175341]\n",
      "loss: 0.293058  [84800/175341]\n",
      "loss: 0.368433  [86400/175341]\n",
      "loss: 0.662804  [88000/175341]\n",
      "loss: 0.702752  [89600/175341]\n",
      "loss: 0.136957  [91200/175341]\n",
      "loss: 0.492168  [92800/175341]\n",
      "loss: 0.418370  [94400/175341]\n",
      "loss: 0.395418  [96000/175341]\n",
      "loss: 0.542709  [97600/175341]\n",
      "loss: 0.177532  [99200/175341]\n",
      "loss: 0.555567  [100800/175341]\n",
      "loss: 0.260033  [102400/175341]\n",
      "loss: 0.901655  [104000/175341]\n",
      "loss: 0.468930  [105600/175341]\n",
      "loss: 0.743145  [107200/175341]\n",
      "loss: 0.373436  [108800/175341]\n",
      "loss: 0.342374  [110400/175341]\n",
      "loss: 0.745582  [112000/175341]\n",
      "loss: 0.433283  [113600/175341]\n",
      "loss: 0.481821  [115200/175341]\n",
      "loss: 0.410042  [116800/175341]\n",
      "loss: 0.862102  [118400/175341]\n",
      "loss: 0.810944  [120000/175341]\n",
      "loss: 0.401689  [121600/175341]\n",
      "loss: 0.347376  [123200/175341]\n",
      "loss: 0.518320  [124800/175341]\n",
      "loss: 0.307418  [126400/175341]\n",
      "loss: 0.395718  [128000/175341]\n",
      "loss: 0.398258  [129600/175341]\n",
      "loss: 0.354468  [131200/175341]\n",
      "loss: 0.706507  [132800/175341]\n",
      "loss: 0.356385  [134400/175341]\n",
      "loss: 0.652526  [136000/175341]\n",
      "loss: 0.285075  [137600/175341]\n",
      "loss: 0.258976  [139200/175341]\n",
      "loss: 0.646455  [140800/175341]\n",
      "loss: 0.810867  [142400/175341]\n",
      "loss: 0.359139  [144000/175341]\n",
      "loss: 0.366482  [145600/175341]\n",
      "loss: 0.409929  [147200/175341]\n",
      "loss: 0.347034  [148800/175341]\n",
      "loss: 0.254726  [150400/175341]\n",
      "loss: 0.281548  [152000/175341]\n",
      "loss: 0.226985  [153600/175341]\n",
      "loss: 0.366199  [155200/175341]\n",
      "loss: 0.517749  [156800/175341]\n",
      "loss: 0.129580  [158400/175341]\n",
      "loss: 0.170399  [160000/175341]\n",
      "loss: 0.460665  [161600/175341]\n",
      "loss: 0.420968  [163200/175341]\n",
      "loss: 0.614538  [164800/175341]\n",
      "loss: 0.585777  [166400/175341]\n",
      "loss: 0.268315  [168000/175341]\n",
      "loss: 0.359849  [169600/175341]\n",
      "loss: 0.770508  [171200/175341]\n",
      "loss: 0.427560  [172800/175341]\n",
      "loss: 1.033633  [174400/175341]\n",
      "Train Accuracy: 81.5725%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.543477, F1-score: 76.29%, Macro_F1-Score:  41.11%  \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.503908  [    0/175341]\n",
      "loss: 0.283720  [ 1600/175341]\n",
      "loss: 0.336463  [ 3200/175341]\n",
      "loss: 0.414725  [ 4800/175341]\n",
      "loss: 0.431210  [ 6400/175341]\n",
      "loss: 0.669070  [ 8000/175341]\n",
      "loss: 0.230118  [ 9600/175341]\n",
      "loss: 0.312138  [11200/175341]\n",
      "loss: 0.457867  [12800/175341]\n",
      "loss: 0.367037  [14400/175341]\n",
      "loss: 0.264497  [16000/175341]\n",
      "loss: 0.634377  [17600/175341]\n",
      "loss: 0.291494  [19200/175341]\n",
      "loss: 0.574702  [20800/175341]\n",
      "loss: 0.436863  [22400/175341]\n",
      "loss: 0.265150  [24000/175341]\n",
      "loss: 0.464137  [25600/175341]\n",
      "loss: 0.232989  [27200/175341]\n",
      "loss: 0.223664  [28800/175341]\n",
      "loss: 0.452401  [30400/175341]\n",
      "loss: 0.296279  [32000/175341]\n",
      "loss: 0.796876  [33600/175341]\n",
      "loss: 0.307077  [35200/175341]\n",
      "loss: 0.308426  [36800/175341]\n",
      "loss: 0.252543  [38400/175341]\n",
      "loss: 0.562092  [40000/175341]\n",
      "loss: 1.058225  [41600/175341]\n",
      "loss: 0.469305  [43200/175341]\n",
      "loss: 0.980291  [44800/175341]\n",
      "loss: 0.366129  [46400/175341]\n",
      "loss: 0.469064  [48000/175341]\n",
      "loss: 0.642909  [49600/175341]\n",
      "loss: 0.428073  [51200/175341]\n",
      "loss: 0.460045  [52800/175341]\n",
      "loss: 0.449247  [54400/175341]\n",
      "loss: 0.537959  [56000/175341]\n",
      "loss: 0.118700  [57600/175341]\n",
      "loss: 0.512480  [59200/175341]\n",
      "loss: 0.519389  [60800/175341]\n",
      "loss: 0.377547  [62400/175341]\n",
      "loss: 0.542843  [64000/175341]\n",
      "loss: 0.669673  [65600/175341]\n",
      "loss: 0.344906  [67200/175341]\n",
      "loss: 0.823476  [68800/175341]\n",
      "loss: 0.351453  [70400/175341]\n",
      "loss: 0.639787  [72000/175341]\n",
      "loss: 0.328784  [73600/175341]\n",
      "loss: 0.355860  [75200/175341]\n",
      "loss: 0.593032  [76800/175341]\n",
      "loss: 0.661390  [78400/175341]\n",
      "loss: 0.648422  [80000/175341]\n",
      "loss: 0.204593  [81600/175341]\n",
      "loss: 0.525353  [83200/175341]\n",
      "loss: 0.330694  [84800/175341]\n",
      "loss: 0.666175  [86400/175341]\n",
      "loss: 0.248093  [88000/175341]\n",
      "loss: 0.716212  [89600/175341]\n",
      "loss: 0.807433  [91200/175341]\n",
      "loss: 0.583239  [92800/175341]\n",
      "loss: 0.357163  [94400/175341]\n",
      "loss: 0.312739  [96000/175341]\n",
      "loss: 0.299213  [97600/175341]\n",
      "loss: 0.405147  [99200/175341]\n",
      "loss: 0.417493  [100800/175341]\n",
      "loss: 0.789464  [102400/175341]\n",
      "loss: 0.767861  [104000/175341]\n",
      "loss: 0.460710  [105600/175341]\n",
      "loss: 0.702721  [107200/175341]\n",
      "loss: 0.742709  [108800/175341]\n",
      "loss: 0.596578  [110400/175341]\n",
      "loss: 0.348495  [112000/175341]\n",
      "loss: 0.312288  [113600/175341]\n",
      "loss: 0.296638  [115200/175341]\n",
      "loss: 0.316510  [116800/175341]\n",
      "loss: 0.831346  [118400/175341]\n",
      "loss: 0.565091  [120000/175341]\n",
      "loss: 0.491005  [121600/175341]\n",
      "loss: 0.364305  [123200/175341]\n",
      "loss: 0.805324  [124800/175341]\n",
      "loss: 0.438840  [126400/175341]\n",
      "loss: 0.117722  [128000/175341]\n",
      "loss: 0.533830  [129600/175341]\n",
      "loss: 0.204791  [131200/175341]\n",
      "loss: 0.452769  [132800/175341]\n",
      "loss: 0.593201  [134400/175341]\n",
      "loss: 0.404569  [136000/175341]\n",
      "loss: 0.330427  [137600/175341]\n",
      "loss: 0.248387  [139200/175341]\n",
      "loss: 0.608748  [140800/175341]\n",
      "loss: 0.598133  [142400/175341]\n",
      "loss: 0.309815  [144000/175341]\n",
      "loss: 0.766993  [145600/175341]\n",
      "loss: 0.322826  [147200/175341]\n",
      "loss: 0.260451  [148800/175341]\n",
      "loss: 0.210554  [150400/175341]\n",
      "loss: 0.283949  [152000/175341]\n",
      "loss: 0.640273  [153600/175341]\n",
      "loss: 0.415464  [155200/175341]\n",
      "loss: 0.519113  [156800/175341]\n",
      "loss: 0.598938  [158400/175341]\n",
      "loss: 0.425948  [160000/175341]\n",
      "loss: 0.327459  [161600/175341]\n",
      "loss: 0.230208  [163200/175341]\n",
      "loss: 0.860572  [164800/175341]\n",
      "loss: 0.645613  [166400/175341]\n",
      "loss: 0.436446  [168000/175341]\n",
      "loss: 0.279851  [169600/175341]\n",
      "loss: 0.391883  [171200/175341]\n",
      "loss: 0.480554  [172800/175341]\n",
      "loss: 0.793596  [174400/175341]\n",
      "Train Accuracy: 81.6586%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.538549, F1-score: 76.79%, Macro_F1-Score:  41.47%  \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.252590  [    0/175341]\n",
      "loss: 0.582144  [ 1600/175341]\n",
      "loss: 0.875749  [ 3200/175341]\n",
      "loss: 0.404431  [ 4800/175341]\n",
      "loss: 0.298921  [ 6400/175341]\n",
      "loss: 0.250487  [ 8000/175341]\n",
      "loss: 0.399622  [ 9600/175341]\n",
      "loss: 0.510689  [11200/175341]\n",
      "loss: 0.384229  [12800/175341]\n",
      "loss: 0.942075  [14400/175341]\n",
      "loss: 0.600563  [16000/175341]\n",
      "loss: 0.302597  [17600/175341]\n",
      "loss: 0.304971  [19200/175341]\n",
      "loss: 0.520973  [20800/175341]\n",
      "loss: 0.137802  [22400/175341]\n",
      "loss: 0.418974  [24000/175341]\n",
      "loss: 0.448197  [25600/175341]\n",
      "loss: 0.381320  [27200/175341]\n",
      "loss: 0.531762  [28800/175341]\n",
      "loss: 0.885017  [30400/175341]\n",
      "loss: 0.186197  [32000/175341]\n",
      "loss: 0.371410  [33600/175341]\n",
      "loss: 0.214810  [35200/175341]\n",
      "loss: 0.569702  [36800/175341]\n",
      "loss: 0.365387  [38400/175341]\n",
      "loss: 0.226731  [40000/175341]\n",
      "loss: 0.397011  [41600/175341]\n",
      "loss: 0.419983  [43200/175341]\n",
      "loss: 0.303536  [44800/175341]\n",
      "loss: 0.829686  [46400/175341]\n",
      "loss: 0.292405  [48000/175341]\n",
      "loss: 0.421861  [49600/175341]\n",
      "loss: 0.333343  [51200/175341]\n",
      "loss: 0.257141  [52800/175341]\n",
      "loss: 0.429480  [54400/175341]\n",
      "loss: 0.608268  [56000/175341]\n",
      "loss: 0.634679  [57600/175341]\n",
      "loss: 0.375962  [59200/175341]\n",
      "loss: 0.333379  [60800/175341]\n",
      "loss: 0.440749  [62400/175341]\n",
      "loss: 0.535154  [64000/175341]\n",
      "loss: 0.237052  [65600/175341]\n",
      "loss: 0.203490  [67200/175341]\n",
      "loss: 0.177649  [68800/175341]\n",
      "loss: 0.770091  [70400/175341]\n",
      "loss: 0.215730  [72000/175341]\n",
      "loss: 0.304460  [73600/175341]\n",
      "loss: 0.641767  [75200/175341]\n",
      "loss: 0.504265  [76800/175341]\n",
      "loss: 0.502429  [78400/175341]\n",
      "loss: 0.372145  [80000/175341]\n",
      "loss: 0.098204  [81600/175341]\n",
      "loss: 0.400755  [83200/175341]\n",
      "loss: 0.232381  [84800/175341]\n",
      "loss: 0.441636  [86400/175341]\n",
      "loss: 0.451414  [88000/175341]\n",
      "loss: 0.592655  [89600/175341]\n",
      "loss: 1.028148  [91200/175341]\n",
      "loss: 0.221752  [92800/175341]\n",
      "loss: 0.538225  [94400/175341]\n",
      "loss: 0.462810  [96000/175341]\n",
      "loss: 0.524802  [97600/175341]\n",
      "loss: 0.384522  [99200/175341]\n",
      "loss: 0.507495  [100800/175341]\n",
      "loss: 0.719152  [102400/175341]\n",
      "loss: 0.279253  [104000/175341]\n",
      "loss: 0.446714  [105600/175341]\n",
      "loss: 0.474883  [107200/175341]\n",
      "loss: 0.234551  [108800/175341]\n",
      "loss: 0.299877  [110400/175341]\n",
      "loss: 0.389562  [112000/175341]\n",
      "loss: 0.463415  [113600/175341]\n",
      "loss: 0.688469  [115200/175341]\n",
      "loss: 0.433525  [116800/175341]\n",
      "loss: 0.823736  [118400/175341]\n",
      "loss: 0.571426  [120000/175341]\n",
      "loss: 0.283843  [121600/175341]\n",
      "loss: 0.666501  [123200/175341]\n",
      "loss: 0.450144  [124800/175341]\n",
      "loss: 0.234953  [126400/175341]\n",
      "loss: 0.741224  [128000/175341]\n",
      "loss: 0.995507  [129600/175341]\n",
      "loss: 0.225325  [131200/175341]\n",
      "loss: 0.358068  [132800/175341]\n",
      "loss: 0.444920  [134400/175341]\n",
      "loss: 0.290044  [136000/175341]\n",
      "loss: 0.283926  [137600/175341]\n",
      "loss: 0.416665  [139200/175341]\n",
      "loss: 0.368555  [140800/175341]\n",
      "loss: 0.536826  [142400/175341]\n",
      "loss: 0.247634  [144000/175341]\n",
      "loss: 0.381412  [145600/175341]\n",
      "loss: 0.589469  [147200/175341]\n",
      "loss: 0.333260  [148800/175341]\n",
      "loss: 0.290724  [150400/175341]\n",
      "loss: 0.351329  [152000/175341]\n",
      "loss: 0.198161  [153600/175341]\n",
      "loss: 0.465790  [155200/175341]\n",
      "loss: 0.319341  [156800/175341]\n",
      "loss: 0.380156  [158400/175341]\n",
      "loss: 0.519730  [160000/175341]\n",
      "loss: 0.256533  [161600/175341]\n",
      "loss: 0.445271  [163200/175341]\n",
      "loss: 0.285353  [164800/175341]\n",
      "loss: 0.697685  [166400/175341]\n",
      "loss: 0.387213  [168000/175341]\n",
      "loss: 0.582162  [169600/175341]\n",
      "loss: 0.648388  [171200/175341]\n",
      "loss: 0.328433  [172800/175341]\n",
      "loss: 0.753570  [174400/175341]\n",
      "Train Accuracy: 81.5845%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.538124, F1-score: 77.00%, Macro_F1-Score:  41.79%  \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.611978  [    0/175341]\n",
      "loss: 0.663938  [ 1600/175341]\n",
      "loss: 0.649785  [ 3200/175341]\n",
      "loss: 0.219203  [ 4800/175341]\n",
      "loss: 0.365615  [ 6400/175341]\n",
      "loss: 0.508861  [ 8000/175341]\n",
      "loss: 0.240763  [ 9600/175341]\n",
      "loss: 0.423545  [11200/175341]\n",
      "loss: 0.771607  [12800/175341]\n",
      "loss: 0.426116  [14400/175341]\n",
      "loss: 0.835053  [16000/175341]\n",
      "loss: 0.339093  [17600/175341]\n",
      "loss: 0.158097  [19200/175341]\n",
      "loss: 0.340853  [20800/175341]\n",
      "loss: 0.121007  [22400/175341]\n",
      "loss: 0.197688  [24000/175341]\n",
      "loss: 0.657941  [25600/175341]\n",
      "loss: 0.228671  [27200/175341]\n",
      "loss: 0.367445  [28800/175341]\n",
      "loss: 0.573301  [30400/175341]\n",
      "loss: 0.330884  [32000/175341]\n",
      "loss: 0.380597  [33600/175341]\n",
      "loss: 0.364549  [35200/175341]\n",
      "loss: 0.801797  [36800/175341]\n",
      "loss: 0.303347  [38400/175341]\n",
      "loss: 0.282390  [40000/175341]\n",
      "loss: 0.378174  [41600/175341]\n",
      "loss: 0.608979  [43200/175341]\n",
      "loss: 1.053976  [44800/175341]\n",
      "loss: 1.066329  [46400/175341]\n",
      "loss: 0.660731  [48000/175341]\n",
      "loss: 0.513580  [49600/175341]\n",
      "loss: 0.468414  [51200/175341]\n",
      "loss: 0.410961  [52800/175341]\n",
      "loss: 0.536803  [54400/175341]\n",
      "loss: 0.215172  [56000/175341]\n",
      "loss: 0.117687  [57600/175341]\n",
      "loss: 0.539752  [59200/175341]\n",
      "loss: 0.595949  [60800/175341]\n",
      "loss: 0.472368  [62400/175341]\n",
      "loss: 0.209383  [64000/175341]\n",
      "loss: 0.143233  [65600/175341]\n",
      "loss: 0.739096  [67200/175341]\n",
      "loss: 0.919784  [68800/175341]\n",
      "loss: 0.592469  [70400/175341]\n",
      "loss: 0.302022  [72000/175341]\n",
      "loss: 0.591333  [73600/175341]\n",
      "loss: 0.430524  [75200/175341]\n",
      "loss: 0.452754  [76800/175341]\n",
      "loss: 0.705214  [78400/175341]\n",
      "loss: 0.516898  [80000/175341]\n",
      "loss: 0.548731  [81600/175341]\n",
      "loss: 0.645796  [83200/175341]\n",
      "loss: 0.209150  [84800/175341]\n",
      "loss: 0.288404  [86400/175341]\n",
      "loss: 0.428264  [88000/175341]\n",
      "loss: 0.667551  [89600/175341]\n",
      "loss: 0.215322  [91200/175341]\n",
      "loss: 0.854560  [92800/175341]\n",
      "loss: 0.251883  [94400/175341]\n",
      "loss: 0.253719  [96000/175341]\n",
      "loss: 0.382995  [97600/175341]\n",
      "loss: 0.332198  [99200/175341]\n",
      "loss: 0.797561  [100800/175341]\n",
      "loss: 0.244535  [102400/175341]\n",
      "loss: 0.513728  [104000/175341]\n",
      "loss: 0.419792  [105600/175341]\n",
      "loss: 0.335288  [107200/175341]\n",
      "loss: 0.438814  [108800/175341]\n",
      "loss: 0.286238  [110400/175341]\n",
      "loss: 0.127844  [112000/175341]\n",
      "loss: 0.362162  [113600/175341]\n",
      "loss: 0.823648  [115200/175341]\n",
      "loss: 0.136476  [116800/175341]\n",
      "loss: 0.488431  [118400/175341]\n",
      "loss: 0.423196  [120000/175341]\n",
      "loss: 0.497077  [121600/175341]\n",
      "loss: 0.310083  [123200/175341]\n",
      "loss: 1.153540  [124800/175341]\n",
      "loss: 0.246602  [126400/175341]\n",
      "loss: 0.589426  [128000/175341]\n",
      "loss: 0.508950  [129600/175341]\n",
      "loss: 0.749414  [131200/175341]\n",
      "loss: 0.171023  [132800/175341]\n",
      "loss: 0.600718  [134400/175341]\n",
      "loss: 0.286789  [136000/175341]\n",
      "loss: 0.370460  [137600/175341]\n",
      "loss: 0.261950  [139200/175341]\n",
      "loss: 0.477913  [140800/175341]\n",
      "loss: 0.736788  [142400/175341]\n",
      "loss: 0.604635  [144000/175341]\n",
      "loss: 0.694801  [145600/175341]\n",
      "loss: 0.489098  [147200/175341]\n",
      "loss: 0.431563  [148800/175341]\n",
      "loss: 0.226015  [150400/175341]\n",
      "loss: 0.433726  [152000/175341]\n",
      "loss: 0.365773  [153600/175341]\n",
      "loss: 0.412479  [155200/175341]\n",
      "loss: 0.614828  [156800/175341]\n",
      "loss: 0.585539  [158400/175341]\n",
      "loss: 0.851021  [160000/175341]\n",
      "loss: 0.519263  [161600/175341]\n",
      "loss: 0.160104  [163200/175341]\n",
      "loss: 0.454938  [164800/175341]\n",
      "loss: 0.709727  [166400/175341]\n",
      "loss: 0.699811  [168000/175341]\n",
      "loss: 0.395636  [169600/175341]\n",
      "loss: 0.458523  [171200/175341]\n",
      "loss: 0.603017  [172800/175341]\n",
      "loss: 0.473338  [174400/175341]\n",
      "Train Accuracy: 81.6603%\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.536774, F1-score: 77.29%, Macro_F1-Score:  41.81%  \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.195239  [    0/175341]\n",
      "loss: 0.350058  [ 1600/175341]\n",
      "loss: 0.374263  [ 3200/175341]\n",
      "loss: 0.157775  [ 4800/175341]\n",
      "loss: 0.467254  [ 6400/175341]\n",
      "loss: 0.204277  [ 8000/175341]\n",
      "loss: 0.459015  [ 9600/175341]\n",
      "loss: 0.363823  [11200/175341]\n",
      "loss: 0.433694  [12800/175341]\n",
      "loss: 0.441622  [14400/175341]\n",
      "loss: 0.400792  [16000/175341]\n",
      "loss: 0.804570  [17600/175341]\n",
      "loss: 0.529566  [19200/175341]\n",
      "loss: 0.362469  [20800/175341]\n",
      "loss: 0.424836  [22400/175341]\n",
      "loss: 0.464896  [24000/175341]\n",
      "loss: 0.531095  [25600/175341]\n",
      "loss: 0.452074  [27200/175341]\n",
      "loss: 0.623197  [28800/175341]\n",
      "loss: 0.324624  [30400/175341]\n",
      "loss: 0.768390  [32000/175341]\n",
      "loss: 0.285445  [33600/175341]\n",
      "loss: 0.386362  [35200/175341]\n",
      "loss: 0.292674  [36800/175341]\n",
      "loss: 0.409901  [38400/175341]\n",
      "loss: 0.225725  [40000/175341]\n",
      "loss: 0.430241  [41600/175341]\n",
      "loss: 0.410167  [43200/175341]\n",
      "loss: 0.263181  [44800/175341]\n",
      "loss: 0.436289  [46400/175341]\n",
      "loss: 0.312777  [48000/175341]\n",
      "loss: 0.515652  [49600/175341]\n",
      "loss: 0.259042  [51200/175341]\n",
      "loss: 0.446161  [52800/175341]\n",
      "loss: 0.516803  [54400/175341]\n",
      "loss: 0.324859  [56000/175341]\n",
      "loss: 0.560341  [57600/175341]\n",
      "loss: 0.502312  [59200/175341]\n",
      "loss: 0.483469  [60800/175341]\n",
      "loss: 0.307231  [62400/175341]\n",
      "loss: 0.415751  [64000/175341]\n",
      "loss: 0.343260  [65600/175341]\n",
      "loss: 0.362178  [67200/175341]\n",
      "loss: 0.611950  [68800/175341]\n",
      "loss: 0.420841  [70400/175341]\n",
      "loss: 0.478179  [72000/175341]\n",
      "loss: 0.316444  [73600/175341]\n",
      "loss: 0.163359  [75200/175341]\n",
      "loss: 0.678537  [76800/175341]\n",
      "loss: 0.547560  [78400/175341]\n",
      "loss: 0.823613  [80000/175341]\n",
      "loss: 0.230736  [81600/175341]\n",
      "loss: 0.539978  [83200/175341]\n",
      "loss: 0.510634  [84800/175341]\n",
      "loss: 0.480230  [86400/175341]\n",
      "loss: 0.312825  [88000/175341]\n",
      "loss: 0.360047  [89600/175341]\n",
      "loss: 0.426098  [91200/175341]\n",
      "loss: 0.659145  [92800/175341]\n",
      "loss: 0.296785  [94400/175341]\n",
      "loss: 0.308385  [96000/175341]\n",
      "loss: 0.155493  [97600/175341]\n",
      "loss: 0.550107  [99200/175341]\n",
      "loss: 0.551886  [100800/175341]\n",
      "loss: 0.797808  [102400/175341]\n",
      "loss: 0.285118  [104000/175341]\n",
      "loss: 0.334756  [105600/175341]\n",
      "loss: 0.169728  [107200/175341]\n",
      "loss: 0.382125  [108800/175341]\n",
      "loss: 0.418777  [110400/175341]\n",
      "loss: 0.357600  [112000/175341]\n",
      "loss: 0.645884  [113600/175341]\n",
      "loss: 0.184982  [115200/175341]\n",
      "loss: 0.527525  [116800/175341]\n",
      "loss: 0.762215  [118400/175341]\n",
      "loss: 0.371532  [120000/175341]\n",
      "loss: 0.381911  [121600/175341]\n",
      "loss: 0.351044  [123200/175341]\n",
      "loss: 0.526529  [124800/175341]\n",
      "loss: 0.361253  [126400/175341]\n",
      "loss: 0.555655  [128000/175341]\n",
      "loss: 1.065268  [129600/175341]\n",
      "loss: 0.116738  [131200/175341]\n",
      "loss: 0.490529  [132800/175341]\n",
      "loss: 0.127127  [134400/175341]\n",
      "loss: 0.520425  [136000/175341]\n",
      "loss: 0.569258  [137600/175341]\n",
      "loss: 0.378920  [139200/175341]\n",
      "loss: 0.834337  [140800/175341]\n",
      "loss: 0.493304  [142400/175341]\n",
      "loss: 0.332103  [144000/175341]\n",
      "loss: 0.564607  [145600/175341]\n",
      "loss: 0.494023  [147200/175341]\n",
      "loss: 0.341231  [148800/175341]\n",
      "loss: 0.371914  [150400/175341]\n",
      "loss: 0.303429  [152000/175341]\n",
      "loss: 0.365014  [153600/175341]\n",
      "loss: 0.407419  [155200/175341]\n",
      "loss: 0.431590  [156800/175341]\n",
      "loss: 0.525089  [158400/175341]\n",
      "loss: 0.374860  [160000/175341]\n",
      "loss: 0.338158  [161600/175341]\n",
      "loss: 0.294693  [163200/175341]\n",
      "loss: 0.417391  [164800/175341]\n",
      "loss: 0.452917  [166400/175341]\n",
      "loss: 0.421359  [168000/175341]\n",
      "loss: 0.531566  [169600/175341]\n",
      "loss: 0.368046  [171200/175341]\n",
      "loss: 0.289817  [172800/175341]\n",
      "loss: 0.275672  [174400/175341]\n",
      "Train Accuracy: 81.6346%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.543092, F1-score: 77.62%, Macro_F1-Score:  42.51%  \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.419616  [    0/175341]\n",
      "loss: 0.693560  [ 1600/175341]\n",
      "loss: 0.756257  [ 3200/175341]\n",
      "loss: 0.655330  [ 4800/175341]\n",
      "loss: 0.568063  [ 6400/175341]\n",
      "loss: 1.397464  [ 8000/175341]\n",
      "loss: 0.592692  [ 9600/175341]\n",
      "loss: 0.456847  [11200/175341]\n",
      "loss: 0.332549  [12800/175341]\n",
      "loss: 0.654590  [14400/175341]\n",
      "loss: 0.402919  [16000/175341]\n",
      "loss: 0.234020  [17600/175341]\n",
      "loss: 0.519117  [19200/175341]\n",
      "loss: 0.915223  [20800/175341]\n",
      "loss: 0.150408  [22400/175341]\n",
      "loss: 0.755670  [24000/175341]\n",
      "loss: 0.307807  [25600/175341]\n",
      "loss: 0.438093  [27200/175341]\n",
      "loss: 0.387258  [28800/175341]\n",
      "loss: 0.322495  [30400/175341]\n",
      "loss: 0.365516  [32000/175341]\n",
      "loss: 0.278131  [33600/175341]\n",
      "loss: 0.362700  [35200/175341]\n",
      "loss: 0.118991  [36800/175341]\n",
      "loss: 0.316811  [38400/175341]\n",
      "loss: 0.213983  [40000/175341]\n",
      "loss: 0.906625  [41600/175341]\n",
      "loss: 0.348424  [43200/175341]\n",
      "loss: 0.706770  [44800/175341]\n",
      "loss: 0.341575  [46400/175341]\n",
      "loss: 0.299740  [48000/175341]\n",
      "loss: 0.399782  [49600/175341]\n",
      "loss: 0.421036  [51200/175341]\n",
      "loss: 0.513280  [52800/175341]\n",
      "loss: 0.622739  [54400/175341]\n",
      "loss: 0.393938  [56000/175341]\n",
      "loss: 0.389867  [57600/175341]\n",
      "loss: 0.486113  [59200/175341]\n",
      "loss: 0.213285  [60800/175341]\n",
      "loss: 0.400493  [62400/175341]\n",
      "loss: 0.604841  [64000/175341]\n",
      "loss: 0.402346  [65600/175341]\n",
      "loss: 0.875698  [67200/175341]\n",
      "loss: 0.402207  [68800/175341]\n",
      "loss: 0.786482  [70400/175341]\n",
      "loss: 0.418199  [72000/175341]\n",
      "loss: 0.531523  [73600/175341]\n",
      "loss: 0.447216  [75200/175341]\n",
      "loss: 0.452933  [76800/175341]\n",
      "loss: 0.611781  [78400/175341]\n",
      "loss: 0.280579  [80000/175341]\n",
      "loss: 0.473061  [81600/175341]\n",
      "loss: 0.393246  [83200/175341]\n",
      "loss: 0.306369  [84800/175341]\n",
      "loss: 0.657620  [86400/175341]\n",
      "loss: 0.250210  [88000/175341]\n",
      "loss: 0.247477  [89600/175341]\n",
      "loss: 0.710109  [91200/175341]\n",
      "loss: 0.294610  [92800/175341]\n",
      "loss: 0.241587  [94400/175341]\n",
      "loss: 0.119311  [96000/175341]\n",
      "loss: 0.490753  [97600/175341]\n",
      "loss: 0.243434  [99200/175341]\n",
      "loss: 0.297914  [100800/175341]\n",
      "loss: 0.287386  [102400/175341]\n",
      "loss: 0.182053  [104000/175341]\n",
      "loss: 0.347405  [105600/175341]\n",
      "loss: 0.289659  [107200/175341]\n",
      "loss: 0.173313  [108800/175341]\n",
      "loss: 0.515368  [110400/175341]\n",
      "loss: 0.208979  [112000/175341]\n",
      "loss: 0.563802  [113600/175341]\n",
      "loss: 0.846051  [115200/175341]\n",
      "loss: 0.358891  [116800/175341]\n",
      "loss: 0.247883  [118400/175341]\n",
      "loss: 0.772133  [120000/175341]\n",
      "loss: 0.866829  [121600/175341]\n",
      "loss: 0.507950  [123200/175341]\n",
      "loss: 0.430914  [124800/175341]\n",
      "loss: 0.516459  [126400/175341]\n",
      "loss: 0.306735  [128000/175341]\n",
      "loss: 0.557788  [129600/175341]\n",
      "loss: 0.165811  [131200/175341]\n",
      "loss: 0.314009  [132800/175341]\n",
      "loss: 0.476581  [134400/175341]\n",
      "loss: 0.467659  [136000/175341]\n",
      "loss: 0.883729  [137600/175341]\n",
      "loss: 0.378216  [139200/175341]\n",
      "loss: 0.379270  [140800/175341]\n",
      "loss: 0.543024  [142400/175341]\n",
      "loss: 0.362973  [144000/175341]\n",
      "loss: 0.827492  [145600/175341]\n",
      "loss: 0.279437  [147200/175341]\n",
      "loss: 0.174746  [148800/175341]\n",
      "loss: 0.353480  [150400/175341]\n",
      "loss: 0.633359  [152000/175341]\n",
      "loss: 0.468131  [153600/175341]\n",
      "loss: 0.404441  [155200/175341]\n",
      "loss: 0.088048  [156800/175341]\n",
      "loss: 0.373625  [158400/175341]\n",
      "loss: 0.307528  [160000/175341]\n",
      "loss: 0.661196  [161600/175341]\n",
      "loss: 0.491404  [163200/175341]\n",
      "loss: 1.057048  [164800/175341]\n",
      "loss: 0.278555  [166400/175341]\n",
      "loss: 0.317366  [168000/175341]\n",
      "loss: 0.315393  [169600/175341]\n",
      "loss: 0.728225  [171200/175341]\n",
      "loss: 0.564187  [172800/175341]\n",
      "loss: 0.766972  [174400/175341]\n",
      "Train Accuracy: 81.6717%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.547735, F1-score: 76.53%, Macro_F1-Score:  41.54%  \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.521886  [    0/175341]\n",
      "loss: 0.135543  [ 1600/175341]\n",
      "loss: 0.235088  [ 3200/175341]\n",
      "loss: 0.733055  [ 4800/175341]\n",
      "loss: 0.447120  [ 6400/175341]\n",
      "loss: 0.598518  [ 8000/175341]\n",
      "loss: 0.380596  [ 9600/175341]\n",
      "loss: 0.083605  [11200/175341]\n",
      "loss: 0.555094  [12800/175341]\n",
      "loss: 0.391311  [14400/175341]\n",
      "loss: 0.831087  [16000/175341]\n",
      "loss: 0.616702  [17600/175341]\n",
      "loss: 0.351632  [19200/175341]\n",
      "loss: 0.247411  [20800/175341]\n",
      "loss: 0.505373  [22400/175341]\n",
      "loss: 0.425747  [24000/175341]\n",
      "loss: 0.431464  [25600/175341]\n",
      "loss: 1.097421  [27200/175341]\n",
      "loss: 0.475572  [28800/175341]\n",
      "loss: 0.734388  [30400/175341]\n",
      "loss: 0.335566  [32000/175341]\n",
      "loss: 0.595201  [33600/175341]\n",
      "loss: 0.227899  [35200/175341]\n",
      "loss: 0.492383  [36800/175341]\n",
      "loss: 0.419142  [38400/175341]\n",
      "loss: 0.234866  [40000/175341]\n",
      "loss: 0.632928  [41600/175341]\n",
      "loss: 0.771977  [43200/175341]\n",
      "loss: 0.351547  [44800/175341]\n",
      "loss: 0.704440  [46400/175341]\n",
      "loss: 0.153176  [48000/175341]\n",
      "loss: 0.187641  [49600/175341]\n",
      "loss: 0.419813  [51200/175341]\n",
      "loss: 0.325878  [52800/175341]\n",
      "loss: 0.292751  [54400/175341]\n",
      "loss: 0.365868  [56000/175341]\n",
      "loss: 0.370562  [57600/175341]\n",
      "loss: 0.365476  [59200/175341]\n",
      "loss: 0.541747  [60800/175341]\n",
      "loss: 0.278853  [62400/175341]\n",
      "loss: 0.463801  [64000/175341]\n",
      "loss: 0.206283  [65600/175341]\n",
      "loss: 0.312389  [67200/175341]\n",
      "loss: 0.635958  [68800/175341]\n",
      "loss: 0.127621  [70400/175341]\n",
      "loss: 0.486604  [72000/175341]\n",
      "loss: 0.416170  [73600/175341]\n",
      "loss: 0.109418  [75200/175341]\n",
      "loss: 0.471992  [76800/175341]\n",
      "loss: 0.348426  [78400/175341]\n",
      "loss: 0.554973  [80000/175341]\n",
      "loss: 0.642273  [81600/175341]\n",
      "loss: 0.442147  [83200/175341]\n",
      "loss: 0.525376  [84800/175341]\n",
      "loss: 0.723978  [86400/175341]\n",
      "loss: 0.632154  [88000/175341]\n",
      "loss: 0.320724  [89600/175341]\n",
      "loss: 0.519427  [91200/175341]\n",
      "loss: 0.627804  [92800/175341]\n",
      "loss: 0.587687  [94400/175341]\n",
      "loss: 0.645122  [96000/175341]\n",
      "loss: 0.176776  [97600/175341]\n",
      "loss: 0.552975  [99200/175341]\n",
      "loss: 0.211334  [100800/175341]\n",
      "loss: 0.433881  [102400/175341]\n",
      "loss: 0.363347  [104000/175341]\n",
      "loss: 0.402221  [105600/175341]\n",
      "loss: 0.163749  [107200/175341]\n",
      "loss: 0.470830  [108800/175341]\n",
      "loss: 0.762755  [110400/175341]\n",
      "loss: 0.444835  [112000/175341]\n",
      "loss: 0.348142  [113600/175341]\n",
      "loss: 0.784007  [115200/175341]\n",
      "loss: 0.632886  [116800/175341]\n",
      "loss: 0.419311  [118400/175341]\n",
      "loss: 0.374497  [120000/175341]\n",
      "loss: 0.603520  [121600/175341]\n",
      "loss: 0.398493  [123200/175341]\n",
      "loss: 0.493225  [124800/175341]\n",
      "loss: 0.272438  [126400/175341]\n",
      "loss: 0.839606  [128000/175341]\n",
      "loss: 0.333806  [129600/175341]\n",
      "loss: 0.438925  [131200/175341]\n",
      "loss: 0.545573  [132800/175341]\n",
      "loss: 0.355448  [134400/175341]\n",
      "loss: 0.058554  [136000/175341]\n",
      "loss: 0.321892  [137600/175341]\n",
      "loss: 0.259527  [139200/175341]\n",
      "loss: 0.336335  [140800/175341]\n",
      "loss: 0.627900  [142400/175341]\n",
      "loss: 0.466459  [144000/175341]\n",
      "loss: 0.515403  [145600/175341]\n",
      "loss: 0.338358  [147200/175341]\n",
      "loss: 0.381247  [148800/175341]\n",
      "loss: 0.634079  [150400/175341]\n",
      "loss: 0.306017  [152000/175341]\n",
      "loss: 0.298994  [153600/175341]\n",
      "loss: 0.338095  [155200/175341]\n",
      "loss: 0.288454  [156800/175341]\n",
      "loss: 0.659430  [158400/175341]\n",
      "loss: 0.344229  [160000/175341]\n",
      "loss: 0.643296  [161600/175341]\n",
      "loss: 0.409723  [163200/175341]\n",
      "loss: 0.711678  [164800/175341]\n",
      "loss: 0.372164  [166400/175341]\n",
      "loss: 0.431296  [168000/175341]\n",
      "loss: 0.422112  [169600/175341]\n",
      "loss: 0.295941  [171200/175341]\n",
      "loss: 0.360774  [172800/175341]\n",
      "loss: 0.305578  [174400/175341]\n",
      "Train Accuracy: 81.6244%\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.551577, F1-score: 76.19%, Macro_F1-Score:  41.40%  \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.198821  [    0/175341]\n",
      "loss: 0.536847  [ 1600/175341]\n",
      "loss: 0.107369  [ 3200/175341]\n",
      "loss: 0.406791  [ 4800/175341]\n",
      "loss: 0.180509  [ 6400/175341]\n",
      "loss: 0.237213  [ 8000/175341]\n",
      "loss: 0.464576  [ 9600/175341]\n",
      "loss: 0.544142  [11200/175341]\n",
      "loss: 0.335522  [12800/175341]\n",
      "loss: 0.410563  [14400/175341]\n",
      "loss: 0.440481  [16000/175341]\n",
      "loss: 0.355014  [17600/175341]\n",
      "loss: 0.527668  [19200/175341]\n",
      "loss: 0.266178  [20800/175341]\n",
      "loss: 0.412309  [22400/175341]\n",
      "loss: 0.230646  [24000/175341]\n",
      "loss: 0.392808  [25600/175341]\n",
      "loss: 0.305305  [27200/175341]\n",
      "loss: 0.675267  [28800/175341]\n",
      "loss: 0.268843  [30400/175341]\n",
      "loss: 0.575614  [32000/175341]\n",
      "loss: 0.428473  [33600/175341]\n",
      "loss: 0.467063  [35200/175341]\n",
      "loss: 0.997726  [36800/175341]\n",
      "loss: 0.508304  [38400/175341]\n",
      "loss: 0.491977  [40000/175341]\n",
      "loss: 0.352003  [41600/175341]\n",
      "loss: 0.279024  [43200/175341]\n",
      "loss: 0.106937  [44800/175341]\n",
      "loss: 0.429143  [46400/175341]\n",
      "loss: 0.397055  [48000/175341]\n",
      "loss: 0.616831  [49600/175341]\n",
      "loss: 0.414595  [51200/175341]\n",
      "loss: 0.479215  [52800/175341]\n",
      "loss: 0.484410  [54400/175341]\n",
      "loss: 0.600210  [56000/175341]\n",
      "loss: 0.407177  [57600/175341]\n",
      "loss: 0.339323  [59200/175341]\n",
      "loss: 0.800809  [60800/175341]\n",
      "loss: 0.289345  [62400/175341]\n",
      "loss: 0.283196  [64000/175341]\n",
      "loss: 0.411698  [65600/175341]\n",
      "loss: 0.344790  [67200/175341]\n",
      "loss: 0.468962  [68800/175341]\n",
      "loss: 0.453000  [70400/175341]\n",
      "loss: 0.453938  [72000/175341]\n",
      "loss: 0.420264  [73600/175341]\n",
      "loss: 0.888222  [75200/175341]\n",
      "loss: 0.591049  [76800/175341]\n",
      "loss: 0.543754  [78400/175341]\n",
      "loss: 0.350757  [80000/175341]\n",
      "loss: 0.367560  [81600/175341]\n",
      "loss: 0.669896  [83200/175341]\n",
      "loss: 0.774553  [84800/175341]\n",
      "loss: 0.421773  [86400/175341]\n",
      "loss: 0.544638  [88000/175341]\n",
      "loss: 0.418898  [89600/175341]\n",
      "loss: 0.314136  [91200/175341]\n",
      "loss: 0.351474  [92800/175341]\n",
      "loss: 0.250743  [94400/175341]\n",
      "loss: 0.252090  [96000/175341]\n",
      "loss: 0.955461  [97600/175341]\n",
      "loss: 0.572671  [99200/175341]\n",
      "loss: 0.290398  [100800/175341]\n",
      "loss: 0.545458  [102400/175341]\n",
      "loss: 0.743212  [104000/175341]\n",
      "loss: 0.263030  [105600/175341]\n",
      "loss: 0.674466  [107200/175341]\n",
      "loss: 0.219359  [108800/175341]\n",
      "loss: 0.492053  [110400/175341]\n",
      "loss: 0.526846  [112000/175341]\n",
      "loss: 0.337257  [113600/175341]\n",
      "loss: 0.425920  [115200/175341]\n",
      "loss: 0.867715  [116800/175341]\n",
      "loss: 0.377888  [118400/175341]\n",
      "loss: 0.324851  [120000/175341]\n",
      "loss: 0.323520  [121600/175341]\n",
      "loss: 0.438975  [123200/175341]\n",
      "loss: 0.430320  [124800/175341]\n",
      "loss: 0.806819  [126400/175341]\n",
      "loss: 0.278920  [128000/175341]\n",
      "loss: 0.319198  [129600/175341]\n",
      "loss: 0.375626  [131200/175341]\n",
      "loss: 0.390942  [132800/175341]\n",
      "loss: 0.386398  [134400/175341]\n",
      "loss: 0.557158  [136000/175341]\n",
      "loss: 0.289945  [137600/175341]\n",
      "loss: 0.476970  [139200/175341]\n",
      "loss: 0.506233  [140800/175341]\n",
      "loss: 0.599165  [142400/175341]\n",
      "loss: 0.387835  [144000/175341]\n",
      "loss: 0.202952  [145600/175341]\n",
      "loss: 0.442885  [147200/175341]\n",
      "loss: 0.278352  [148800/175341]\n",
      "loss: 0.231141  [150400/175341]\n",
      "loss: 0.387832  [152000/175341]\n",
      "loss: 0.474169  [153600/175341]\n",
      "loss: 0.447648  [155200/175341]\n",
      "loss: 0.408534  [156800/175341]\n",
      "loss: 0.678839  [158400/175341]\n",
      "loss: 0.161000  [160000/175341]\n",
      "loss: 0.355279  [161600/175341]\n",
      "loss: 0.344389  [163200/175341]\n",
      "loss: 0.364144  [164800/175341]\n",
      "loss: 0.340390  [166400/175341]\n",
      "loss: 0.644583  [168000/175341]\n",
      "loss: 0.494928  [169600/175341]\n",
      "loss: 0.630043  [171200/175341]\n",
      "loss: 0.257105  [172800/175341]\n",
      "loss: 0.415009  [174400/175341]\n",
      "Train Accuracy: 81.6637%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.550501, F1-score: 76.38%, Macro_F1-Score:  41.23%  \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.361252  [    0/175341]\n",
      "loss: 0.258563  [ 1600/175341]\n",
      "loss: 0.428559  [ 3200/175341]\n",
      "loss: 0.158275  [ 4800/175341]\n",
      "loss: 0.305723  [ 6400/175341]\n",
      "loss: 0.389929  [ 8000/175341]\n",
      "loss: 0.271811  [ 9600/175341]\n",
      "loss: 0.428601  [11200/175341]\n",
      "loss: 0.417276  [12800/175341]\n",
      "loss: 0.289598  [14400/175341]\n",
      "loss: 0.328016  [16000/175341]\n",
      "loss: 0.469271  [17600/175341]\n",
      "loss: 0.218451  [19200/175341]\n",
      "loss: 0.422441  [20800/175341]\n",
      "loss: 0.123203  [22400/175341]\n",
      "loss: 0.536209  [24000/175341]\n",
      "loss: 0.228333  [25600/175341]\n",
      "loss: 0.173330  [27200/175341]\n",
      "loss: 0.488492  [28800/175341]\n",
      "loss: 0.204543  [30400/175341]\n",
      "loss: 0.610451  [32000/175341]\n",
      "loss: 0.352595  [33600/175341]\n",
      "loss: 0.485623  [35200/175341]\n",
      "loss: 0.352326  [36800/175341]\n",
      "loss: 0.418120  [38400/175341]\n",
      "loss: 0.328578  [40000/175341]\n",
      "loss: 0.587170  [41600/175341]\n",
      "loss: 0.223623  [43200/175341]\n",
      "loss: 0.575635  [44800/175341]\n",
      "loss: 0.200410  [46400/175341]\n",
      "loss: 0.598107  [48000/175341]\n",
      "loss: 0.415216  [49600/175341]\n",
      "loss: 0.430530  [51200/175341]\n",
      "loss: 0.270812  [52800/175341]\n",
      "loss: 0.429512  [54400/175341]\n",
      "loss: 0.613003  [56000/175341]\n",
      "loss: 0.408826  [57600/175341]\n",
      "loss: 0.579212  [59200/175341]\n",
      "loss: 0.362472  [60800/175341]\n",
      "loss: 0.335920  [62400/175341]\n",
      "loss: 0.371927  [64000/175341]\n",
      "loss: 0.346252  [65600/175341]\n",
      "loss: 0.589942  [67200/175341]\n",
      "loss: 0.348319  [68800/175341]\n",
      "loss: 0.654823  [70400/175341]\n",
      "loss: 0.509870  [72000/175341]\n",
      "loss: 0.584775  [73600/175341]\n",
      "loss: 0.581042  [75200/175341]\n",
      "loss: 0.312295  [76800/175341]\n",
      "loss: 0.304325  [78400/175341]\n",
      "loss: 0.314860  [80000/175341]\n",
      "loss: 0.331085  [81600/175341]\n",
      "loss: 0.307599  [83200/175341]\n",
      "loss: 0.269719  [84800/175341]\n",
      "loss: 0.574962  [86400/175341]\n",
      "loss: 0.217224  [88000/175341]\n",
      "loss: 0.320361  [89600/175341]\n",
      "loss: 0.429701  [91200/175341]\n",
      "loss: 0.954023  [92800/175341]\n",
      "loss: 0.721439  [94400/175341]\n",
      "loss: 0.370313  [96000/175341]\n",
      "loss: 0.635976  [97600/175341]\n",
      "loss: 0.564309  [99200/175341]\n",
      "loss: 0.338720  [100800/175341]\n",
      "loss: 0.395424  [102400/175341]\n",
      "loss: 0.453085  [104000/175341]\n",
      "loss: 0.094977  [105600/175341]\n",
      "loss: 0.584908  [107200/175341]\n",
      "loss: 0.913136  [108800/175341]\n",
      "loss: 0.581002  [110400/175341]\n",
      "loss: 0.241158  [112000/175341]\n",
      "loss: 0.318812  [113600/175341]\n",
      "loss: 0.537367  [115200/175341]\n",
      "loss: 0.691950  [116800/175341]\n",
      "loss: 0.588666  [118400/175341]\n",
      "loss: 0.299536  [120000/175341]\n",
      "loss: 0.296456  [121600/175341]\n",
      "loss: 0.361204  [123200/175341]\n",
      "loss: 0.504861  [124800/175341]\n",
      "loss: 0.639635  [126400/175341]\n",
      "loss: 0.617958  [128000/175341]\n",
      "loss: 0.421809  [129600/175341]\n",
      "loss: 0.383149  [131200/175341]\n",
      "loss: 0.367585  [132800/175341]\n",
      "loss: 0.976822  [134400/175341]\n",
      "loss: 0.880491  [136000/175341]\n",
      "loss: 0.106053  [137600/175341]\n",
      "loss: 0.847762  [139200/175341]\n",
      "loss: 0.221394  [140800/175341]\n",
      "loss: 0.638676  [142400/175341]\n",
      "loss: 0.316826  [144000/175341]\n",
      "loss: 0.555644  [145600/175341]\n",
      "loss: 0.672685  [147200/175341]\n",
      "loss: 0.186549  [148800/175341]\n",
      "loss: 0.414610  [150400/175341]\n",
      "loss: 0.426830  [152000/175341]\n",
      "loss: 0.766057  [153600/175341]\n",
      "loss: 0.563751  [155200/175341]\n",
      "loss: 0.467094  [156800/175341]\n",
      "loss: 0.519602  [158400/175341]\n",
      "loss: 0.726248  [160000/175341]\n",
      "loss: 0.411934  [161600/175341]\n",
      "loss: 0.759833  [163200/175341]\n",
      "loss: 0.312384  [164800/175341]\n",
      "loss: 0.368479  [166400/175341]\n",
      "loss: 0.264571  [168000/175341]\n",
      "loss: 0.468466  [169600/175341]\n",
      "loss: 0.488441  [171200/175341]\n",
      "loss: 0.465687  [172800/175341]\n",
      "loss: 0.390613  [174400/175341]\n",
      "Train Accuracy: 81.6985%\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.564247, F1-score: 75.74%, Macro_F1-Score:  41.29%  \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.282295  [    0/175341]\n",
      "loss: 0.178432  [ 1600/175341]\n",
      "loss: 0.271260  [ 3200/175341]\n",
      "loss: 0.361533  [ 4800/175341]\n",
      "loss: 0.292305  [ 6400/175341]\n",
      "loss: 0.356180  [ 8000/175341]\n",
      "loss: 0.564597  [ 9600/175341]\n",
      "loss: 0.273407  [11200/175341]\n",
      "loss: 0.355184  [12800/175341]\n",
      "loss: 0.326985  [14400/175341]\n",
      "loss: 0.223117  [16000/175341]\n",
      "loss: 0.550889  [17600/175341]\n",
      "loss: 0.459206  [19200/175341]\n",
      "loss: 0.792942  [20800/175341]\n",
      "loss: 1.163754  [22400/175341]\n",
      "loss: 0.426511  [24000/175341]\n",
      "loss: 0.324020  [25600/175341]\n",
      "loss: 0.629738  [27200/175341]\n",
      "loss: 0.554776  [28800/175341]\n",
      "loss: 0.775449  [30400/175341]\n",
      "loss: 0.395788  [32000/175341]\n",
      "loss: 0.303403  [33600/175341]\n",
      "loss: 0.441611  [35200/175341]\n",
      "loss: 0.836348  [36800/175341]\n",
      "loss: 0.308116  [38400/175341]\n",
      "loss: 0.156636  [40000/175341]\n",
      "loss: 0.482481  [41600/175341]\n",
      "loss: 0.489370  [43200/175341]\n",
      "loss: 0.277144  [44800/175341]\n",
      "loss: 0.338758  [46400/175341]\n",
      "loss: 0.527276  [48000/175341]\n",
      "loss: 0.203543  [49600/175341]\n",
      "loss: 1.155269  [51200/175341]\n",
      "loss: 0.652728  [52800/175341]\n",
      "loss: 0.644158  [54400/175341]\n",
      "loss: 0.667017  [56000/175341]\n",
      "loss: 0.591716  [57600/175341]\n",
      "loss: 0.253493  [59200/175341]\n",
      "loss: 0.563198  [60800/175341]\n",
      "loss: 0.445933  [62400/175341]\n",
      "loss: 0.332737  [64000/175341]\n",
      "loss: 0.707358  [65600/175341]\n",
      "loss: 0.428813  [67200/175341]\n",
      "loss: 0.601467  [68800/175341]\n",
      "loss: 0.425214  [70400/175341]\n",
      "loss: 0.520629  [72000/175341]\n",
      "loss: 0.468411  [73600/175341]\n",
      "loss: 0.200552  [75200/175341]\n",
      "loss: 0.691882  [76800/175341]\n",
      "loss: 0.396730  [78400/175341]\n",
      "loss: 0.535657  [80000/175341]\n",
      "loss: 0.453703  [81600/175341]\n",
      "loss: 0.699403  [83200/175341]\n",
      "loss: 0.235675  [84800/175341]\n",
      "loss: 0.071850  [86400/175341]\n",
      "loss: 0.511193  [88000/175341]\n",
      "loss: 0.526879  [89600/175341]\n",
      "loss: 0.181175  [91200/175341]\n",
      "loss: 0.429883  [92800/175341]\n",
      "loss: 0.429789  [94400/175341]\n",
      "loss: 0.473654  [96000/175341]\n",
      "loss: 0.224979  [97600/175341]\n",
      "loss: 0.368041  [99200/175341]\n",
      "loss: 0.762942  [100800/175341]\n",
      "loss: 0.635514  [102400/175341]\n",
      "loss: 0.572753  [104000/175341]\n",
      "loss: 0.267475  [105600/175341]\n",
      "loss: 0.428581  [107200/175341]\n",
      "loss: 0.563161  [108800/175341]\n",
      "loss: 0.416844  [110400/175341]\n",
      "loss: 0.313558  [112000/175341]\n",
      "loss: 0.422863  [113600/175341]\n",
      "loss: 0.347482  [115200/175341]\n",
      "loss: 0.114459  [116800/175341]\n",
      "loss: 0.402556  [118400/175341]\n",
      "loss: 0.327563  [120000/175341]\n",
      "loss: 0.658013  [121600/175341]\n",
      "loss: 0.232006  [123200/175341]\n",
      "loss: 0.500745  [124800/175341]\n",
      "loss: 0.225350  [126400/175341]\n",
      "loss: 0.648310  [128000/175341]\n",
      "loss: 0.612733  [129600/175341]\n",
      "loss: 0.465913  [131200/175341]\n",
      "loss: 0.340480  [132800/175341]\n",
      "loss: 0.707231  [134400/175341]\n",
      "loss: 0.866014  [136000/175341]\n",
      "loss: 0.453160  [137600/175341]\n",
      "loss: 0.870144  [139200/175341]\n",
      "loss: 0.537977  [140800/175341]\n",
      "loss: 0.766424  [142400/175341]\n",
      "loss: 0.482568  [144000/175341]\n",
      "loss: 0.253125  [145600/175341]\n",
      "loss: 0.307929  [147200/175341]\n",
      "loss: 0.550309  [148800/175341]\n",
      "loss: 0.522626  [150400/175341]\n",
      "loss: 0.301895  [152000/175341]\n",
      "loss: 0.675004  [153600/175341]\n",
      "loss: 0.477717  [155200/175341]\n",
      "loss: 0.654018  [156800/175341]\n",
      "loss: 0.286400  [158400/175341]\n",
      "loss: 0.518032  [160000/175341]\n",
      "loss: 0.238863  [161600/175341]\n",
      "loss: 0.624543  [163200/175341]\n",
      "loss: 0.171557  [164800/175341]\n",
      "loss: 0.556114  [166400/175341]\n",
      "loss: 0.420013  [168000/175341]\n",
      "loss: 0.193101  [169600/175341]\n",
      "loss: 0.400085  [171200/175341]\n",
      "loss: 0.511255  [172800/175341]\n",
      "loss: 0.317312  [174400/175341]\n",
      "Train Accuracy: 81.6700%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.543468, F1-score: 76.64%, Macro_F1-Score:  41.20%  \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.676940  [    0/175341]\n",
      "loss: 0.530424  [ 1600/175341]\n",
      "loss: 0.543712  [ 3200/175341]\n",
      "loss: 0.498385  [ 4800/175341]\n",
      "loss: 0.504408  [ 6400/175341]\n",
      "loss: 0.250444  [ 8000/175341]\n",
      "loss: 0.610871  [ 9600/175341]\n",
      "loss: 0.441165  [11200/175341]\n",
      "loss: 0.438826  [12800/175341]\n",
      "loss: 0.653177  [14400/175341]\n",
      "loss: 0.408121  [16000/175341]\n",
      "loss: 0.480559  [17600/175341]\n",
      "loss: 0.444666  [19200/175341]\n",
      "loss: 0.498775  [20800/175341]\n",
      "loss: 0.581443  [22400/175341]\n",
      "loss: 0.438763  [24000/175341]\n",
      "loss: 0.242563  [25600/175341]\n",
      "loss: 0.384337  [27200/175341]\n",
      "loss: 0.215950  [28800/175341]\n",
      "loss: 0.646011  [30400/175341]\n",
      "loss: 0.318543  [32000/175341]\n",
      "loss: 0.636920  [33600/175341]\n",
      "loss: 0.553667  [35200/175341]\n",
      "loss: 0.318199  [36800/175341]\n",
      "loss: 0.346620  [38400/175341]\n",
      "loss: 0.387690  [40000/175341]\n",
      "loss: 0.243670  [41600/175341]\n",
      "loss: 0.653014  [43200/175341]\n",
      "loss: 0.721681  [44800/175341]\n",
      "loss: 0.327348  [46400/175341]\n",
      "loss: 0.599650  [48000/175341]\n",
      "loss: 0.375653  [49600/175341]\n",
      "loss: 0.242139  [51200/175341]\n",
      "loss: 0.184546  [52800/175341]\n",
      "loss: 0.433215  [54400/175341]\n",
      "loss: 0.445412  [56000/175341]\n",
      "loss: 0.211936  [57600/175341]\n",
      "loss: 0.566305  [59200/175341]\n",
      "loss: 0.184017  [60800/175341]\n",
      "loss: 0.334072  [62400/175341]\n",
      "loss: 0.360166  [64000/175341]\n",
      "loss: 0.307603  [65600/175341]\n",
      "loss: 0.581418  [67200/175341]\n",
      "loss: 0.959706  [68800/175341]\n",
      "loss: 0.168903  [70400/175341]\n",
      "loss: 0.331100  [72000/175341]\n",
      "loss: 0.308864  [73600/175341]\n",
      "loss: 0.449742  [75200/175341]\n",
      "loss: 0.539826  [76800/175341]\n",
      "loss: 0.586894  [78400/175341]\n",
      "loss: 0.521491  [80000/175341]\n",
      "loss: 0.993533  [81600/175341]\n",
      "loss: 0.296137  [83200/175341]\n",
      "loss: 0.306258  [84800/175341]\n",
      "loss: 0.263652  [86400/175341]\n",
      "loss: 0.286550  [88000/175341]\n",
      "loss: 0.658031  [89600/175341]\n",
      "loss: 0.245948  [91200/175341]\n",
      "loss: 0.446710  [92800/175341]\n",
      "loss: 0.132001  [94400/175341]\n",
      "loss: 0.315884  [96000/175341]\n",
      "loss: 0.548779  [97600/175341]\n",
      "loss: 0.768711  [99200/175341]\n",
      "loss: 0.161683  [100800/175341]\n",
      "loss: 0.945036  [102400/175341]\n",
      "loss: 0.551239  [104000/175341]\n",
      "loss: 0.293964  [105600/175341]\n",
      "loss: 0.523819  [107200/175341]\n",
      "loss: 0.266677  [108800/175341]\n",
      "loss: 0.699223  [110400/175341]\n",
      "loss: 0.510014  [112000/175341]\n",
      "loss: 0.220266  [113600/175341]\n",
      "loss: 0.508748  [115200/175341]\n",
      "loss: 0.345227  [116800/175341]\n",
      "loss: 0.507909  [118400/175341]\n",
      "loss: 1.012444  [120000/175341]\n",
      "loss: 0.433470  [121600/175341]\n",
      "loss: 0.497542  [123200/175341]\n",
      "loss: 0.707772  [124800/175341]\n",
      "loss: 0.549529  [126400/175341]\n",
      "loss: 0.611724  [128000/175341]\n",
      "loss: 0.369143  [129600/175341]\n",
      "loss: 0.275913  [131200/175341]\n",
      "loss: 0.272967  [132800/175341]\n",
      "loss: 0.384575  [134400/175341]\n",
      "loss: 0.376987  [136000/175341]\n",
      "loss: 0.358153  [137600/175341]\n",
      "loss: 0.257459  [139200/175341]\n",
      "loss: 0.359065  [140800/175341]\n",
      "loss: 0.633459  [142400/175341]\n",
      "loss: 0.288828  [144000/175341]\n",
      "loss: 0.562269  [145600/175341]\n",
      "loss: 0.397284  [147200/175341]\n",
      "loss: 0.341379  [148800/175341]\n",
      "loss: 0.320005  [150400/175341]\n",
      "loss: 0.325338  [152000/175341]\n",
      "loss: 0.530541  [153600/175341]\n",
      "loss: 0.468367  [155200/175341]\n",
      "loss: 0.371353  [156800/175341]\n",
      "loss: 0.542175  [158400/175341]\n",
      "loss: 0.621477  [160000/175341]\n",
      "loss: 0.263149  [161600/175341]\n",
      "loss: 0.938365  [163200/175341]\n",
      "loss: 0.577329  [164800/175341]\n",
      "loss: 1.057022  [166400/175341]\n",
      "loss: 0.297714  [168000/175341]\n",
      "loss: 0.490660  [169600/175341]\n",
      "loss: 0.404837  [171200/175341]\n",
      "loss: 0.328686  [172800/175341]\n",
      "loss: 0.306383  [174400/175341]\n",
      "Train Accuracy: 81.6466%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.560778, F1-score: 76.21%, Macro_F1-Score:  40.47%  \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.651789  [    0/175341]\n",
      "loss: 0.250277  [ 1600/175341]\n",
      "loss: 0.450114  [ 3200/175341]\n",
      "loss: 0.364764  [ 4800/175341]\n",
      "loss: 0.426605  [ 6400/175341]\n",
      "loss: 0.326880  [ 8000/175341]\n",
      "loss: 0.831021  [ 9600/175341]\n",
      "loss: 0.527852  [11200/175341]\n",
      "loss: 0.414717  [12800/175341]\n",
      "loss: 0.276539  [14400/175341]\n",
      "loss: 0.398455  [16000/175341]\n",
      "loss: 0.339553  [17600/175341]\n",
      "loss: 0.496283  [19200/175341]\n",
      "loss: 0.522685  [20800/175341]\n",
      "loss: 0.349303  [22400/175341]\n",
      "loss: 0.306724  [24000/175341]\n",
      "loss: 0.869435  [25600/175341]\n",
      "loss: 0.358952  [27200/175341]\n",
      "loss: 0.277410  [28800/175341]\n",
      "loss: 0.644487  [30400/175341]\n",
      "loss: 0.253008  [32000/175341]\n",
      "loss: 0.646069  [33600/175341]\n",
      "loss: 0.489600  [35200/175341]\n",
      "loss: 0.333206  [36800/175341]\n",
      "loss: 0.715075  [38400/175341]\n",
      "loss: 0.418373  [40000/175341]\n",
      "loss: 0.216074  [41600/175341]\n",
      "loss: 0.530594  [43200/175341]\n",
      "loss: 0.247780  [44800/175341]\n",
      "loss: 0.455008  [46400/175341]\n",
      "loss: 0.212769  [48000/175341]\n",
      "loss: 0.180793  [49600/175341]\n",
      "loss: 0.436125  [51200/175341]\n",
      "loss: 0.370026  [52800/175341]\n",
      "loss: 0.352677  [54400/175341]\n",
      "loss: 0.527038  [56000/175341]\n",
      "loss: 0.595071  [57600/175341]\n",
      "loss: 0.506858  [59200/175341]\n",
      "loss: 0.050554  [60800/175341]\n",
      "loss: 0.719728  [62400/175341]\n",
      "loss: 0.288798  [64000/175341]\n",
      "loss: 0.528127  [65600/175341]\n",
      "loss: 0.983973  [67200/175341]\n",
      "loss: 0.534370  [68800/175341]\n",
      "loss: 0.384965  [70400/175341]\n",
      "loss: 0.188677  [72000/175341]\n",
      "loss: 0.770052  [73600/175341]\n",
      "loss: 0.804518  [75200/175341]\n",
      "loss: 0.509502  [76800/175341]\n",
      "loss: 0.367684  [78400/175341]\n",
      "loss: 0.636273  [80000/175341]\n",
      "loss: 0.225741  [81600/175341]\n",
      "loss: 0.591905  [83200/175341]\n",
      "loss: 0.507565  [84800/175341]\n",
      "loss: 0.441840  [86400/175341]\n",
      "loss: 0.274013  [88000/175341]\n",
      "loss: 0.289505  [89600/175341]\n",
      "loss: 0.608379  [91200/175341]\n",
      "loss: 0.097382  [92800/175341]\n",
      "loss: 0.601712  [94400/175341]\n",
      "loss: 0.539581  [96000/175341]\n",
      "loss: 0.316953  [97600/175341]\n",
      "loss: 0.440102  [99200/175341]\n",
      "loss: 0.451509  [100800/175341]\n",
      "loss: 0.463368  [102400/175341]\n",
      "loss: 0.215937  [104000/175341]\n",
      "loss: 0.401693  [105600/175341]\n",
      "loss: 0.600075  [107200/175341]\n",
      "loss: 0.066132  [108800/175341]\n",
      "loss: 0.430267  [110400/175341]\n",
      "loss: 0.694440  [112000/175341]\n",
      "loss: 0.434297  [113600/175341]\n",
      "loss: 0.682485  [115200/175341]\n",
      "loss: 0.166020  [116800/175341]\n",
      "loss: 0.330335  [118400/175341]\n",
      "loss: 0.444228  [120000/175341]\n",
      "loss: 0.553297  [121600/175341]\n",
      "loss: 0.426066  [123200/175341]\n",
      "loss: 0.441725  [124800/175341]\n",
      "loss: 0.320636  [126400/175341]\n",
      "loss: 0.512050  [128000/175341]\n",
      "loss: 0.457793  [129600/175341]\n",
      "loss: 0.655529  [131200/175341]\n",
      "loss: 0.339604  [132800/175341]\n",
      "loss: 0.205060  [134400/175341]\n",
      "loss: 0.224149  [136000/175341]\n",
      "loss: 0.580208  [137600/175341]\n",
      "loss: 0.275795  [139200/175341]\n",
      "loss: 0.627408  [140800/175341]\n",
      "loss: 0.619959  [142400/175341]\n",
      "loss: 0.653746  [144000/175341]\n",
      "loss: 0.411817  [145600/175341]\n",
      "loss: 0.571958  [147200/175341]\n",
      "loss: 0.000822  [148800/175341]\n",
      "loss: 0.625636  [150400/175341]\n",
      "loss: 0.581304  [152000/175341]\n",
      "loss: 0.256920  [153600/175341]\n",
      "loss: 0.107764  [155200/175341]\n",
      "loss: 0.544711  [156800/175341]\n",
      "loss: 0.396061  [158400/175341]\n",
      "loss: 0.569006  [160000/175341]\n",
      "loss: 0.096819  [161600/175341]\n",
      "loss: 0.669385  [163200/175341]\n",
      "loss: 0.633059  [164800/175341]\n",
      "loss: 0.443719  [166400/175341]\n",
      "loss: 0.253133  [168000/175341]\n",
      "loss: 0.389608  [169600/175341]\n",
      "loss: 0.190321  [171200/175341]\n",
      "loss: 0.287234  [172800/175341]\n",
      "loss: 0.417712  [174400/175341]\n",
      "Train Accuracy: 81.6900%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.548403, F1-score: 76.60%, Macro_F1-Score:  42.73%  \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.492227  [    0/175341]\n",
      "loss: 0.570167  [ 1600/175341]\n",
      "loss: 0.396284  [ 3200/175341]\n",
      "loss: 0.655408  [ 4800/175341]\n",
      "loss: 0.431544  [ 6400/175341]\n",
      "loss: 0.470074  [ 8000/175341]\n",
      "loss: 0.395671  [ 9600/175341]\n",
      "loss: 0.772836  [11200/175341]\n",
      "loss: 0.391017  [12800/175341]\n",
      "loss: 0.408435  [14400/175341]\n",
      "loss: 0.375351  [16000/175341]\n",
      "loss: 0.415466  [17600/175341]\n",
      "loss: 0.565959  [19200/175341]\n",
      "loss: 0.467649  [20800/175341]\n",
      "loss: 0.251480  [22400/175341]\n",
      "loss: 0.448958  [24000/175341]\n",
      "loss: 0.480445  [25600/175341]\n",
      "loss: 0.183994  [27200/175341]\n",
      "loss: 0.451637  [28800/175341]\n",
      "loss: 1.098302  [30400/175341]\n",
      "loss: 0.551743  [32000/175341]\n",
      "loss: 0.344910  [33600/175341]\n",
      "loss: 0.325843  [35200/175341]\n",
      "loss: 0.284966  [36800/175341]\n",
      "loss: 0.339716  [38400/175341]\n",
      "loss: 0.221131  [40000/175341]\n",
      "loss: 0.182170  [41600/175341]\n",
      "loss: 0.500260  [43200/175341]\n",
      "loss: 0.578926  [44800/175341]\n",
      "loss: 0.659458  [46400/175341]\n",
      "loss: 0.502646  [48000/175341]\n",
      "loss: 0.156690  [49600/175341]\n",
      "loss: 0.296646  [51200/175341]\n",
      "loss: 0.623701  [52800/175341]\n",
      "loss: 0.544649  [54400/175341]\n",
      "loss: 0.436103  [56000/175341]\n",
      "loss: 0.501390  [57600/175341]\n",
      "loss: 0.362304  [59200/175341]\n",
      "loss: 0.670674  [60800/175341]\n",
      "loss: 0.306372  [62400/175341]\n",
      "loss: 0.828925  [64000/175341]\n",
      "loss: 0.316386  [65600/175341]\n",
      "loss: 0.537588  [67200/175341]\n",
      "loss: 0.386642  [68800/175341]\n",
      "loss: 0.542851  [70400/175341]\n",
      "loss: 0.761567  [72000/175341]\n",
      "loss: 0.433803  [73600/175341]\n",
      "loss: 0.180266  [75200/175341]\n",
      "loss: 0.306693  [76800/175341]\n",
      "loss: 0.582328  [78400/175341]\n",
      "loss: 0.350195  [80000/175341]\n",
      "loss: 0.509544  [81600/175341]\n",
      "loss: 0.348371  [83200/175341]\n",
      "loss: 0.321422  [84800/175341]\n",
      "loss: 0.471873  [86400/175341]\n",
      "loss: 0.720132  [88000/175341]\n",
      "loss: 0.237010  [89600/175341]\n",
      "loss: 0.962937  [91200/175341]\n",
      "loss: 0.690333  [92800/175341]\n",
      "loss: 0.672112  [94400/175341]\n",
      "loss: 0.299373  [96000/175341]\n",
      "loss: 0.713469  [97600/175341]\n",
      "loss: 0.815585  [99200/175341]\n",
      "loss: 0.445240  [100800/175341]\n",
      "loss: 0.430705  [102400/175341]\n",
      "loss: 0.550741  [104000/175341]\n",
      "loss: 0.695394  [105600/175341]\n",
      "loss: 0.076015  [107200/175341]\n",
      "loss: 0.264043  [108800/175341]\n",
      "loss: 0.382094  [110400/175341]\n",
      "loss: 1.107459  [112000/175341]\n",
      "loss: 0.479899  [113600/175341]\n",
      "loss: 0.610276  [115200/175341]\n",
      "loss: 0.253727  [116800/175341]\n",
      "loss: 0.295329  [118400/175341]\n",
      "loss: 0.369595  [120000/175341]\n",
      "loss: 0.746624  [121600/175341]\n",
      "loss: 0.564610  [123200/175341]\n",
      "loss: 0.444564  [124800/175341]\n",
      "loss: 0.530226  [126400/175341]\n",
      "loss: 0.298438  [128000/175341]\n",
      "loss: 0.326798  [129600/175341]\n",
      "loss: 0.186631  [131200/175341]\n",
      "loss: 0.352220  [132800/175341]\n",
      "loss: 0.584703  [134400/175341]\n",
      "loss: 0.259832  [136000/175341]\n",
      "loss: 0.511554  [137600/175341]\n",
      "loss: 0.109923  [139200/175341]\n",
      "loss: 0.485263  [140800/175341]\n",
      "loss: 0.451207  [142400/175341]\n",
      "loss: 0.854609  [144000/175341]\n",
      "loss: 0.510778  [145600/175341]\n",
      "loss: 0.566827  [147200/175341]\n",
      "loss: 0.601873  [148800/175341]\n",
      "loss: 0.426790  [150400/175341]\n",
      "loss: 0.340976  [152000/175341]\n",
      "loss: 0.702970  [153600/175341]\n",
      "loss: 0.417684  [155200/175341]\n",
      "loss: 0.567715  [156800/175341]\n",
      "loss: 0.315813  [158400/175341]\n",
      "loss: 0.483526  [160000/175341]\n",
      "loss: 0.803570  [161600/175341]\n",
      "loss: 0.389002  [163200/175341]\n",
      "loss: 0.289798  [164800/175341]\n",
      "loss: 0.481279  [166400/175341]\n",
      "loss: 0.095152  [168000/175341]\n",
      "loss: 0.582231  [169600/175341]\n",
      "loss: 0.845855  [171200/175341]\n",
      "loss: 0.228310  [172800/175341]\n",
      "loss: 0.166097  [174400/175341]\n",
      "Train Accuracy: 81.7344%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.550323, F1-score: 76.92%, Macro_F1-Score:  42.82%  \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.554196  [    0/175341]\n",
      "loss: 0.183549  [ 1600/175341]\n",
      "loss: 0.357745  [ 3200/175341]\n",
      "loss: 0.516043  [ 4800/175341]\n",
      "loss: 0.802709  [ 6400/175341]\n",
      "loss: 0.914735  [ 8000/175341]\n",
      "loss: 0.704767  [ 9600/175341]\n",
      "loss: 0.680854  [11200/175341]\n",
      "loss: 0.562559  [12800/175341]\n",
      "loss: 0.325978  [14400/175341]\n",
      "loss: 0.437075  [16000/175341]\n",
      "loss: 0.183554  [17600/175341]\n",
      "loss: 0.738292  [19200/175341]\n",
      "loss: 0.548633  [20800/175341]\n",
      "loss: 0.039966  [22400/175341]\n",
      "loss: 0.476307  [24000/175341]\n",
      "loss: 0.229340  [25600/175341]\n",
      "loss: 0.440746  [27200/175341]\n",
      "loss: 0.616580  [28800/175341]\n",
      "loss: 0.268201  [30400/175341]\n",
      "loss: 0.689998  [32000/175341]\n",
      "loss: 0.468761  [33600/175341]\n",
      "loss: 0.580943  [35200/175341]\n",
      "loss: 0.398179  [36800/175341]\n",
      "loss: 0.498389  [38400/175341]\n",
      "loss: 0.369964  [40000/175341]\n",
      "loss: 0.355736  [41600/175341]\n",
      "loss: 0.219008  [43200/175341]\n",
      "loss: 0.493399  [44800/175341]\n",
      "loss: 0.338186  [46400/175341]\n",
      "loss: 0.436247  [48000/175341]\n",
      "loss: 0.636308  [49600/175341]\n",
      "loss: 0.300417  [51200/175341]\n",
      "loss: 0.320354  [52800/175341]\n",
      "loss: 0.370833  [54400/175341]\n",
      "loss: 0.387299  [56000/175341]\n",
      "loss: 0.463440  [57600/175341]\n",
      "loss: 0.403192  [59200/175341]\n",
      "loss: 0.166942  [60800/175341]\n",
      "loss: 0.118415  [62400/175341]\n",
      "loss: 0.330738  [64000/175341]\n",
      "loss: 0.665440  [65600/175341]\n",
      "loss: 0.657406  [67200/175341]\n",
      "loss: 0.286973  [68800/175341]\n",
      "loss: 0.512814  [70400/175341]\n",
      "loss: 0.655699  [72000/175341]\n",
      "loss: 0.816645  [73600/175341]\n",
      "loss: 0.359184  [75200/175341]\n",
      "loss: 0.450672  [76800/175341]\n",
      "loss: 0.350509  [78400/175341]\n",
      "loss: 0.493566  [80000/175341]\n",
      "loss: 0.355172  [81600/175341]\n",
      "loss: 0.215038  [83200/175341]\n",
      "loss: 0.225426  [84800/175341]\n",
      "loss: 0.075114  [86400/175341]\n",
      "loss: 0.444556  [88000/175341]\n",
      "loss: 0.235997  [89600/175341]\n",
      "loss: 0.097096  [91200/175341]\n",
      "loss: 0.429611  [92800/175341]\n",
      "loss: 0.544092  [94400/175341]\n",
      "loss: 0.578429  [96000/175341]\n",
      "loss: 0.303248  [97600/175341]\n",
      "loss: 0.577354  [99200/175341]\n",
      "loss: 0.118488  [100800/175341]\n",
      "loss: 0.244201  [102400/175341]\n",
      "loss: 0.607270  [104000/175341]\n",
      "loss: 0.640262  [105600/175341]\n",
      "loss: 0.568557  [107200/175341]\n",
      "loss: 0.324927  [108800/175341]\n",
      "loss: 0.406882  [110400/175341]\n",
      "loss: 0.659074  [112000/175341]\n",
      "loss: 0.358310  [113600/175341]\n",
      "loss: 0.284916  [115200/175341]\n",
      "loss: 0.299648  [116800/175341]\n",
      "loss: 0.490277  [118400/175341]\n",
      "loss: 0.147008  [120000/175341]\n",
      "loss: 0.744498  [121600/175341]\n",
      "loss: 0.475782  [123200/175341]\n",
      "loss: 0.517396  [124800/175341]\n",
      "loss: 0.432601  [126400/175341]\n",
      "loss: 0.377444  [128000/175341]\n",
      "loss: 0.397259  [129600/175341]\n",
      "loss: 0.260800  [131200/175341]\n",
      "loss: 0.524909  [132800/175341]\n",
      "loss: 0.608602  [134400/175341]\n",
      "loss: 0.393281  [136000/175341]\n",
      "loss: 0.347882  [137600/175341]\n",
      "loss: 0.422321  [139200/175341]\n",
      "loss: 0.249794  [140800/175341]\n",
      "loss: 0.294598  [142400/175341]\n",
      "loss: 0.872005  [144000/175341]\n",
      "loss: 0.663053  [145600/175341]\n",
      "loss: 0.909524  [147200/175341]\n",
      "loss: 0.474706  [148800/175341]\n",
      "loss: 0.720458  [150400/175341]\n",
      "loss: 0.336968  [152000/175341]\n",
      "loss: 0.664403  [153600/175341]\n",
      "loss: 0.739255  [155200/175341]\n",
      "loss: 0.472164  [156800/175341]\n",
      "loss: 0.537250  [158400/175341]\n",
      "loss: 0.412002  [160000/175341]\n",
      "loss: 0.653613  [161600/175341]\n",
      "loss: 0.261548  [163200/175341]\n",
      "loss: 0.494064  [164800/175341]\n",
      "loss: 0.158816  [166400/175341]\n",
      "loss: 0.428641  [168000/175341]\n",
      "loss: 0.548981  [169600/175341]\n",
      "loss: 0.369166  [171200/175341]\n",
      "loss: 0.445913  [172800/175341]\n",
      "loss: 0.754880  [174400/175341]\n",
      "Train Accuracy: 81.7071%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.545199, F1-score: 76.50%, Macro_F1-Score:  41.80%  \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.464626  [    0/175341]\n",
      "loss: 0.354424  [ 1600/175341]\n",
      "loss: 1.000460  [ 3200/175341]\n",
      "loss: 0.376030  [ 4800/175341]\n",
      "loss: 0.422155  [ 6400/175341]\n",
      "loss: 0.457035  [ 8000/175341]\n",
      "loss: 0.227947  [ 9600/175341]\n",
      "loss: 0.931298  [11200/175341]\n",
      "loss: 0.294904  [12800/175341]\n",
      "loss: 0.242995  [14400/175341]\n",
      "loss: 0.798140  [16000/175341]\n",
      "loss: 0.518849  [17600/175341]\n",
      "loss: 0.390144  [19200/175341]\n",
      "loss: 0.647883  [20800/175341]\n",
      "loss: 0.825644  [22400/175341]\n",
      "loss: 0.623370  [24000/175341]\n",
      "loss: 0.524114  [25600/175341]\n",
      "loss: 0.200521  [27200/175341]\n",
      "loss: 0.529874  [28800/175341]\n",
      "loss: 0.368537  [30400/175341]\n",
      "loss: 0.733442  [32000/175341]\n",
      "loss: 0.714085  [33600/175341]\n",
      "loss: 0.328316  [35200/175341]\n",
      "loss: 0.291230  [36800/175341]\n",
      "loss: 0.382972  [38400/175341]\n",
      "loss: 0.151665  [40000/175341]\n",
      "loss: 0.357381  [41600/175341]\n",
      "loss: 0.222089  [43200/175341]\n",
      "loss: 0.209168  [44800/175341]\n",
      "loss: 0.381129  [46400/175341]\n",
      "loss: 0.478133  [48000/175341]\n",
      "loss: 0.804276  [49600/175341]\n",
      "loss: 0.260684  [51200/175341]\n",
      "loss: 0.315551  [52800/175341]\n",
      "loss: 0.242148  [54400/175341]\n",
      "loss: 0.381341  [56000/175341]\n",
      "loss: 0.639564  [57600/175341]\n",
      "loss: 0.155610  [59200/175341]\n",
      "loss: 0.483238  [60800/175341]\n",
      "loss: 0.553810  [62400/175341]\n",
      "loss: 1.014476  [64000/175341]\n",
      "loss: 0.850099  [65600/175341]\n",
      "loss: 0.559348  [67200/175341]\n",
      "loss: 0.597742  [68800/175341]\n",
      "loss: 0.347347  [70400/175341]\n",
      "loss: 0.176508  [72000/175341]\n",
      "loss: 0.460833  [73600/175341]\n",
      "loss: 0.442807  [75200/175341]\n",
      "loss: 0.402881  [76800/175341]\n",
      "loss: 0.184199  [78400/175341]\n",
      "loss: 0.273918  [80000/175341]\n",
      "loss: 0.390199  [81600/175341]\n",
      "loss: 0.694240  [83200/175341]\n",
      "loss: 0.704535  [84800/175341]\n",
      "loss: 0.403182  [86400/175341]\n",
      "loss: 0.637871  [88000/175341]\n",
      "loss: 0.523963  [89600/175341]\n",
      "loss: 0.390741  [91200/175341]\n",
      "loss: 0.663090  [92800/175341]\n",
      "loss: 0.076040  [94400/175341]\n",
      "loss: 0.252244  [96000/175341]\n",
      "loss: 0.266938  [97600/175341]\n",
      "loss: 0.458479  [99200/175341]\n",
      "loss: 0.208651  [100800/175341]\n",
      "loss: 0.461467  [102400/175341]\n",
      "loss: 0.900445  [104000/175341]\n",
      "loss: 0.320487  [105600/175341]\n",
      "loss: 0.489674  [107200/175341]\n",
      "loss: 0.409800  [108800/175341]\n",
      "loss: 0.222800  [110400/175341]\n",
      "loss: 0.197868  [112000/175341]\n",
      "loss: 0.353209  [113600/175341]\n",
      "loss: 0.510974  [115200/175341]\n",
      "loss: 0.483361  [116800/175341]\n",
      "loss: 0.673535  [118400/175341]\n",
      "loss: 0.542239  [120000/175341]\n",
      "loss: 0.173623  [121600/175341]\n",
      "loss: 0.692799  [123200/175341]\n",
      "loss: 0.451270  [124800/175341]\n",
      "loss: 0.344962  [126400/175341]\n",
      "loss: 0.181475  [128000/175341]\n",
      "loss: 0.601819  [129600/175341]\n",
      "loss: 0.470952  [131200/175341]\n",
      "loss: 0.368769  [132800/175341]\n",
      "loss: 0.292149  [134400/175341]\n",
      "loss: 0.982047  [136000/175341]\n",
      "loss: 0.484979  [137600/175341]\n",
      "loss: 0.338524  [139200/175341]\n",
      "loss: 0.281796  [140800/175341]\n",
      "loss: 0.753101  [142400/175341]\n",
      "loss: 0.468033  [144000/175341]\n",
      "loss: 0.202723  [145600/175341]\n",
      "loss: 0.515648  [147200/175341]\n",
      "loss: 0.333774  [148800/175341]\n",
      "loss: 0.380667  [150400/175341]\n",
      "loss: 0.354532  [152000/175341]\n",
      "loss: 0.388106  [153600/175341]\n",
      "loss: 0.478198  [155200/175341]\n",
      "loss: 0.689985  [156800/175341]\n",
      "loss: 0.585143  [158400/175341]\n",
      "loss: 0.675763  [160000/175341]\n",
      "loss: 0.429506  [161600/175341]\n",
      "loss: 0.540981  [163200/175341]\n",
      "loss: 0.627680  [164800/175341]\n",
      "loss: 0.221176  [166400/175341]\n",
      "loss: 0.603038  [168000/175341]\n",
      "loss: 0.250204  [169600/175341]\n",
      "loss: 0.609244  [171200/175341]\n",
      "loss: 0.461592  [172800/175341]\n",
      "loss: 0.918259  [174400/175341]\n",
      "Train Accuracy: 81.7470%\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.559998, F1-score: 75.51%, Macro_F1-Score:  40.97%  \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.716598  [    0/175341]\n",
      "loss: 0.394464  [ 1600/175341]\n",
      "loss: 0.342951  [ 3200/175341]\n",
      "loss: 0.393672  [ 4800/175341]\n",
      "loss: 0.470889  [ 6400/175341]\n",
      "loss: 0.756517  [ 8000/175341]\n",
      "loss: 0.520524  [ 9600/175341]\n",
      "loss: 0.751873  [11200/175341]\n",
      "loss: 0.523727  [12800/175341]\n",
      "loss: 0.098479  [14400/175341]\n",
      "loss: 0.681801  [16000/175341]\n",
      "loss: 0.348891  [17600/175341]\n",
      "loss: 0.513802  [19200/175341]\n",
      "loss: 0.402205  [20800/175341]\n",
      "loss: 0.892432  [22400/175341]\n",
      "loss: 0.945954  [24000/175341]\n",
      "loss: 0.719088  [25600/175341]\n",
      "loss: 0.347783  [27200/175341]\n",
      "loss: 0.431338  [28800/175341]\n",
      "loss: 0.513351  [30400/175341]\n",
      "loss: 0.536685  [32000/175341]\n",
      "loss: 0.855894  [33600/175341]\n",
      "loss: 0.775088  [35200/175341]\n",
      "loss: 0.177498  [36800/175341]\n",
      "loss: 0.602978  [38400/175341]\n",
      "loss: 0.187789  [40000/175341]\n",
      "loss: 0.544123  [41600/175341]\n",
      "loss: 0.508749  [43200/175341]\n",
      "loss: 0.838548  [44800/175341]\n",
      "loss: 0.425831  [46400/175341]\n",
      "loss: 0.860729  [48000/175341]\n",
      "loss: 0.695740  [49600/175341]\n",
      "loss: 0.367343  [51200/175341]\n",
      "loss: 0.515807  [52800/175341]\n",
      "loss: 0.616237  [54400/175341]\n",
      "loss: 0.338291  [56000/175341]\n",
      "loss: 0.400202  [57600/175341]\n",
      "loss: 0.462007  [59200/175341]\n",
      "loss: 0.338446  [60800/175341]\n",
      "loss: 0.285599  [62400/175341]\n",
      "loss: 0.431024  [64000/175341]\n",
      "loss: 0.431780  [65600/175341]\n",
      "loss: 0.497189  [67200/175341]\n",
      "loss: 0.461625  [68800/175341]\n",
      "loss: 0.248803  [70400/175341]\n",
      "loss: 0.537739  [72000/175341]\n",
      "loss: 0.646252  [73600/175341]\n",
      "loss: 0.215487  [75200/175341]\n",
      "loss: 0.832269  [76800/175341]\n",
      "loss: 0.692614  [78400/175341]\n",
      "loss: 0.701656  [80000/175341]\n",
      "loss: 0.458966  [81600/175341]\n",
      "loss: 0.488733  [83200/175341]\n",
      "loss: 0.739091  [84800/175341]\n",
      "loss: 0.485987  [86400/175341]\n",
      "loss: 0.978839  [88000/175341]\n",
      "loss: 0.776757  [89600/175341]\n",
      "loss: 0.600454  [91200/175341]\n",
      "loss: 0.332063  [92800/175341]\n",
      "loss: 0.322525  [94400/175341]\n",
      "loss: 0.736142  [96000/175341]\n",
      "loss: 0.458769  [97600/175341]\n",
      "loss: 0.743836  [99200/175341]\n",
      "loss: 0.318369  [100800/175341]\n",
      "loss: 0.737517  [102400/175341]\n",
      "loss: 0.508608  [104000/175341]\n",
      "loss: 0.410005  [105600/175341]\n",
      "loss: 0.465844  [107200/175341]\n",
      "loss: 0.630874  [108800/175341]\n",
      "loss: 0.819145  [110400/175341]\n",
      "loss: 0.280226  [112000/175341]\n",
      "loss: 0.339063  [113600/175341]\n",
      "loss: 0.386721  [115200/175341]\n",
      "loss: 0.800470  [116800/175341]\n",
      "loss: 0.589862  [118400/175341]\n",
      "loss: 0.366997  [120000/175341]\n",
      "loss: 0.586519  [121600/175341]\n",
      "loss: 0.405075  [123200/175341]\n",
      "loss: 0.682453  [124800/175341]\n",
      "loss: 0.439380  [126400/175341]\n",
      "loss: 0.765801  [128000/175341]\n",
      "loss: 0.229837  [129600/175341]\n",
      "loss: 0.925246  [131200/175341]\n",
      "loss: 0.450531  [132800/175341]\n",
      "loss: 0.255607  [134400/175341]\n",
      "loss: 0.707057  [136000/175341]\n",
      "loss: 0.545394  [137600/175341]\n",
      "loss: 0.391676  [139200/175341]\n",
      "loss: 0.401827  [140800/175341]\n",
      "loss: 0.731355  [142400/175341]\n",
      "loss: 0.210046  [144000/175341]\n",
      "loss: 0.332264  [145600/175341]\n",
      "loss: 0.388642  [147200/175341]\n",
      "loss: 0.751571  [148800/175341]\n",
      "loss: 0.361803  [150400/175341]\n",
      "loss: 0.447119  [152000/175341]\n",
      "loss: 0.391488  [153600/175341]\n",
      "loss: 0.600960  [155200/175341]\n",
      "loss: 0.260897  [156800/175341]\n",
      "loss: 0.405142  [158400/175341]\n",
      "loss: 0.505374  [160000/175341]\n",
      "loss: 0.639278  [161600/175341]\n",
      "loss: 0.357305  [163200/175341]\n",
      "loss: 0.464082  [164800/175341]\n",
      "loss: 0.448176  [166400/175341]\n",
      "loss: 0.591505  [168000/175341]\n",
      "loss: 0.282509  [169600/175341]\n",
      "loss: 0.540753  [171200/175341]\n",
      "loss: 0.560550  [172800/175341]\n",
      "loss: 0.445827  [174400/175341]\n",
      "Train Accuracy: 81.7082%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.551957, F1-score: 76.16%, Macro_F1-Score:  41.94%  \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.199874  [    0/175341]\n",
      "loss: 1.218086  [ 1600/175341]\n",
      "loss: 0.338671  [ 3200/175341]\n",
      "loss: 0.642279  [ 4800/175341]\n",
      "loss: 0.266924  [ 6400/175341]\n",
      "loss: 0.467257  [ 8000/175341]\n",
      "loss: 0.329984  [ 9600/175341]\n",
      "loss: 0.692694  [11200/175341]\n",
      "loss: 0.685227  [12800/175341]\n",
      "loss: 0.495382  [14400/175341]\n",
      "loss: 0.342560  [16000/175341]\n",
      "loss: 0.347735  [17600/175341]\n",
      "loss: 0.334047  [19200/175341]\n",
      "loss: 0.455090  [20800/175341]\n",
      "loss: 0.159820  [22400/175341]\n",
      "loss: 0.467010  [24000/175341]\n",
      "loss: 0.345551  [25600/175341]\n",
      "loss: 0.651980  [27200/175341]\n",
      "loss: 0.435587  [28800/175341]\n",
      "loss: 0.682247  [30400/175341]\n",
      "loss: 0.693121  [32000/175341]\n",
      "loss: 0.528750  [33600/175341]\n",
      "loss: 0.552490  [35200/175341]\n",
      "loss: 0.316961  [36800/175341]\n",
      "loss: 0.530098  [38400/175341]\n",
      "loss: 0.414599  [40000/175341]\n",
      "loss: 0.737661  [41600/175341]\n",
      "loss: 0.386480  [43200/175341]\n",
      "loss: 0.375764  [44800/175341]\n",
      "loss: 0.360564  [46400/175341]\n",
      "loss: 0.815680  [48000/175341]\n",
      "loss: 0.215125  [49600/175341]\n",
      "loss: 0.616106  [51200/175341]\n",
      "loss: 0.315491  [52800/175341]\n",
      "loss: 0.920475  [54400/175341]\n",
      "loss: 0.649043  [56000/175341]\n",
      "loss: 0.593512  [57600/175341]\n",
      "loss: 0.145702  [59200/175341]\n",
      "loss: 0.333211  [60800/175341]\n",
      "loss: 0.846340  [62400/175341]\n",
      "loss: 0.699534  [64000/175341]\n",
      "loss: 0.196023  [65600/175341]\n",
      "loss: 0.649455  [67200/175341]\n",
      "loss: 0.483669  [68800/175341]\n",
      "loss: 0.512631  [70400/175341]\n",
      "loss: 0.265903  [72000/175341]\n",
      "loss: 0.278096  [73600/175341]\n",
      "loss: 0.435493  [75200/175341]\n",
      "loss: 0.948956  [76800/175341]\n",
      "loss: 0.333936  [78400/175341]\n",
      "loss: 0.365129  [80000/175341]\n",
      "loss: 0.288644  [81600/175341]\n",
      "loss: 0.432674  [83200/175341]\n",
      "loss: 0.512825  [84800/175341]\n",
      "loss: 0.251691  [86400/175341]\n",
      "loss: 0.455613  [88000/175341]\n",
      "loss: 0.614545  [89600/175341]\n",
      "loss: 0.463302  [91200/175341]\n",
      "loss: 0.225045  [92800/175341]\n",
      "loss: 0.336706  [94400/175341]\n",
      "loss: 0.360180  [96000/175341]\n",
      "loss: 0.466840  [97600/175341]\n",
      "loss: 0.502177  [99200/175341]\n",
      "loss: 0.539129  [100800/175341]\n",
      "loss: 0.306629  [102400/175341]\n",
      "loss: 0.262050  [104000/175341]\n",
      "loss: 0.521422  [105600/175341]\n",
      "loss: 0.308387  [107200/175341]\n",
      "loss: 0.350970  [108800/175341]\n",
      "loss: 0.680926  [110400/175341]\n",
      "loss: 0.830773  [112000/175341]\n",
      "loss: 0.313967  [113600/175341]\n",
      "loss: 0.468647  [115200/175341]\n",
      "loss: 0.701279  [116800/175341]\n",
      "loss: 0.250214  [118400/175341]\n",
      "loss: 0.596705  [120000/175341]\n",
      "loss: 0.216440  [121600/175341]\n",
      "loss: 0.312437  [123200/175341]\n",
      "loss: 0.674441  [124800/175341]\n",
      "loss: 0.310946  [126400/175341]\n",
      "loss: 0.586917  [128000/175341]\n",
      "loss: 0.268836  [129600/175341]\n",
      "loss: 0.267603  [131200/175341]\n",
      "loss: 0.431055  [132800/175341]\n",
      "loss: 0.381821  [134400/175341]\n",
      "loss: 0.567740  [136000/175341]\n",
      "loss: 0.441441  [137600/175341]\n",
      "loss: 0.704380  [139200/175341]\n",
      "loss: 0.730040  [140800/175341]\n",
      "loss: 0.220953  [142400/175341]\n",
      "loss: 0.658976  [144000/175341]\n",
      "loss: 0.198248  [145600/175341]\n",
      "loss: 0.754707  [147200/175341]\n",
      "loss: 0.279787  [148800/175341]\n",
      "loss: 0.540972  [150400/175341]\n",
      "loss: 0.332628  [152000/175341]\n",
      "loss: 0.432268  [153600/175341]\n",
      "loss: 0.278734  [155200/175341]\n",
      "loss: 0.609251  [156800/175341]\n",
      "loss: 0.910352  [158400/175341]\n",
      "loss: 0.588806  [160000/175341]\n",
      "loss: 0.367407  [161600/175341]\n",
      "loss: 0.337526  [163200/175341]\n",
      "loss: 0.334585  [164800/175341]\n",
      "loss: 0.549799  [166400/175341]\n",
      "loss: 0.221202  [168000/175341]\n",
      "loss: 0.297853  [169600/175341]\n",
      "loss: 0.885711  [171200/175341]\n",
      "loss: 0.168564  [172800/175341]\n",
      "loss: 0.437536  [174400/175341]\n",
      "Train Accuracy: 81.6951%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.547403, F1-score: 76.28%, Macro_F1-Score:  41.49%  \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.677939  [    0/175341]\n",
      "loss: 0.620396  [ 1600/175341]\n",
      "loss: 0.410266  [ 3200/175341]\n",
      "loss: 0.544028  [ 4800/175341]\n",
      "loss: 0.780528  [ 6400/175341]\n",
      "loss: 0.321485  [ 8000/175341]\n",
      "loss: 0.298387  [ 9600/175341]\n",
      "loss: 0.343990  [11200/175341]\n",
      "loss: 0.325856  [12800/175341]\n",
      "loss: 0.481794  [14400/175341]\n",
      "loss: 0.444367  [16000/175341]\n",
      "loss: 0.892752  [17600/175341]\n",
      "loss: 0.223088  [19200/175341]\n",
      "loss: 0.922932  [20800/175341]\n",
      "loss: 0.164978  [22400/175341]\n",
      "loss: 0.576742  [24000/175341]\n",
      "loss: 0.776893  [25600/175341]\n",
      "loss: 0.412881  [27200/175341]\n",
      "loss: 0.430260  [28800/175341]\n",
      "loss: 1.295025  [30400/175341]\n",
      "loss: 0.443510  [32000/175341]\n",
      "loss: 0.733064  [33600/175341]\n",
      "loss: 0.340344  [35200/175341]\n",
      "loss: 0.143230  [36800/175341]\n",
      "loss: 0.401640  [38400/175341]\n",
      "loss: 0.526599  [40000/175341]\n",
      "loss: 1.191636  [41600/175341]\n",
      "loss: 0.299918  [43200/175341]\n",
      "loss: 0.435704  [44800/175341]\n",
      "loss: 0.371642  [46400/175341]\n",
      "loss: 0.912461  [48000/175341]\n",
      "loss: 0.395899  [49600/175341]\n",
      "loss: 0.338966  [51200/175341]\n",
      "loss: 0.525026  [52800/175341]\n",
      "loss: 1.037923  [54400/175341]\n",
      "loss: 0.515900  [56000/175341]\n",
      "loss: 0.507866  [57600/175341]\n",
      "loss: 0.396745  [59200/175341]\n",
      "loss: 0.748020  [60800/175341]\n",
      "loss: 0.391401  [62400/175341]\n",
      "loss: 0.495995  [64000/175341]\n",
      "loss: 0.483386  [65600/175341]\n",
      "loss: 0.162207  [67200/175341]\n",
      "loss: 0.213389  [68800/175341]\n",
      "loss: 0.397195  [70400/175341]\n",
      "loss: 0.546326  [72000/175341]\n",
      "loss: 0.337504  [73600/175341]\n",
      "loss: 0.254317  [75200/175341]\n",
      "loss: 0.381788  [76800/175341]\n",
      "loss: 0.272654  [78400/175341]\n",
      "loss: 0.510980  [80000/175341]\n",
      "loss: 0.288194  [81600/175341]\n",
      "loss: 0.211404  [83200/175341]\n",
      "loss: 0.397037  [84800/175341]\n",
      "loss: 0.216022  [86400/175341]\n",
      "loss: 0.370341  [88000/175341]\n",
      "loss: 0.462792  [89600/175341]\n",
      "loss: 0.503410  [91200/175341]\n",
      "loss: 0.451589  [92800/175341]\n",
      "loss: 0.387719  [94400/175341]\n",
      "loss: 0.642735  [96000/175341]\n",
      "loss: 0.400780  [97600/175341]\n",
      "loss: 0.656354  [99200/175341]\n",
      "loss: 0.328031  [100800/175341]\n",
      "loss: 0.320189  [102400/175341]\n",
      "loss: 0.460458  [104000/175341]\n",
      "loss: 0.113936  [105600/175341]\n",
      "loss: 0.241859  [107200/175341]\n",
      "loss: 0.123093  [108800/175341]\n",
      "loss: 0.334212  [110400/175341]\n",
      "loss: 0.944056  [112000/175341]\n",
      "loss: 0.352925  [113600/175341]\n",
      "loss: 0.380271  [115200/175341]\n",
      "loss: 0.594378  [116800/175341]\n",
      "loss: 0.681886  [118400/175341]\n",
      "loss: 0.147802  [120000/175341]\n",
      "loss: 0.404616  [121600/175341]\n",
      "loss: 0.399042  [123200/175341]\n",
      "loss: 1.053966  [124800/175341]\n",
      "loss: 0.712627  [126400/175341]\n",
      "loss: 0.766562  [128000/175341]\n",
      "loss: 0.338798  [129600/175341]\n",
      "loss: 0.302905  [131200/175341]\n",
      "loss: 0.547553  [132800/175341]\n",
      "loss: 0.253151  [134400/175341]\n",
      "loss: 0.332731  [136000/175341]\n",
      "loss: 0.475530  [137600/175341]\n",
      "loss: 0.229970  [139200/175341]\n",
      "loss: 0.369121  [140800/175341]\n",
      "loss: 0.837173  [142400/175341]\n",
      "loss: 0.364252  [144000/175341]\n",
      "loss: 0.391405  [145600/175341]\n",
      "loss: 0.299902  [147200/175341]\n",
      "loss: 0.288875  [148800/175341]\n",
      "loss: 0.439973  [150400/175341]\n",
      "loss: 0.651751  [152000/175341]\n",
      "loss: 0.435116  [153600/175341]\n",
      "loss: 0.512759  [155200/175341]\n",
      "loss: 0.768766  [156800/175341]\n",
      "loss: 0.427490  [158400/175341]\n",
      "loss: 0.375238  [160000/175341]\n",
      "loss: 0.459770  [161600/175341]\n",
      "loss: 0.511380  [163200/175341]\n",
      "loss: 0.265332  [164800/175341]\n",
      "loss: 0.669107  [166400/175341]\n",
      "loss: 0.142627  [168000/175341]\n",
      "loss: 0.398975  [169600/175341]\n",
      "loss: 0.267841  [171200/175341]\n",
      "loss: 0.432739  [172800/175341]\n",
      "loss: 0.474421  [174400/175341]\n",
      "Train Accuracy: 81.7561%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.544536, F1-score: 76.90%, Macro_F1-Score:  41.77%  \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.432985  [    0/175341]\n",
      "loss: 0.396483  [ 1600/175341]\n",
      "loss: 0.551772  [ 3200/175341]\n",
      "loss: 0.539320  [ 4800/175341]\n",
      "loss: 0.306161  [ 6400/175341]\n",
      "loss: 0.330326  [ 8000/175341]\n",
      "loss: 0.102748  [ 9600/175341]\n",
      "loss: 0.559258  [11200/175341]\n",
      "loss: 0.105576  [12800/175341]\n",
      "loss: 0.327949  [14400/175341]\n",
      "loss: 0.330898  [16000/175341]\n",
      "loss: 0.226713  [17600/175341]\n",
      "loss: 0.584466  [19200/175341]\n",
      "loss: 0.261649  [20800/175341]\n",
      "loss: 0.393026  [22400/175341]\n",
      "loss: 0.640649  [24000/175341]\n",
      "loss: 0.478103  [25600/175341]\n",
      "loss: 0.269738  [27200/175341]\n",
      "loss: 0.685499  [28800/175341]\n",
      "loss: 0.177745  [30400/175341]\n",
      "loss: 0.277280  [32000/175341]\n",
      "loss: 0.673679  [33600/175341]\n",
      "loss: 0.349502  [35200/175341]\n",
      "loss: 0.507002  [36800/175341]\n",
      "loss: 0.872845  [38400/175341]\n",
      "loss: 0.382749  [40000/175341]\n",
      "loss: 0.551887  [41600/175341]\n",
      "loss: 0.604563  [43200/175341]\n",
      "loss: 0.190599  [44800/175341]\n",
      "loss: 0.317648  [46400/175341]\n",
      "loss: 0.784645  [48000/175341]\n",
      "loss: 0.168517  [49600/175341]\n",
      "loss: 0.270056  [51200/175341]\n",
      "loss: 0.821469  [52800/175341]\n",
      "loss: 0.156499  [54400/175341]\n",
      "loss: 0.505444  [56000/175341]\n",
      "loss: 0.295170  [57600/175341]\n",
      "loss: 0.461507  [59200/175341]\n",
      "loss: 0.549204  [60800/175341]\n",
      "loss: 0.195101  [62400/175341]\n",
      "loss: 0.452209  [64000/175341]\n",
      "loss: 0.604638  [65600/175341]\n",
      "loss: 0.455495  [67200/175341]\n",
      "loss: 0.487854  [68800/175341]\n",
      "loss: 0.480122  [70400/175341]\n",
      "loss: 0.295835  [72000/175341]\n",
      "loss: 0.480750  [73600/175341]\n",
      "loss: 0.464164  [75200/175341]\n",
      "loss: 0.355577  [76800/175341]\n",
      "loss: 0.839234  [78400/175341]\n",
      "loss: 0.392055  [80000/175341]\n",
      "loss: 0.602729  [81600/175341]\n",
      "loss: 0.389744  [83200/175341]\n",
      "loss: 0.559345  [84800/175341]\n",
      "loss: 0.551615  [86400/175341]\n",
      "loss: 0.521394  [88000/175341]\n",
      "loss: 0.323176  [89600/175341]\n",
      "loss: 0.658944  [91200/175341]\n",
      "loss: 0.305539  [92800/175341]\n",
      "loss: 0.516592  [94400/175341]\n",
      "loss: 0.312479  [96000/175341]\n",
      "loss: 0.535405  [97600/175341]\n",
      "loss: 0.298483  [99200/175341]\n",
      "loss: 0.261834  [100800/175341]\n",
      "loss: 0.583340  [102400/175341]\n",
      "loss: 0.122896  [104000/175341]\n",
      "loss: 0.708512  [105600/175341]\n",
      "loss: 0.357471  [107200/175341]\n",
      "loss: 0.658051  [108800/175341]\n",
      "loss: 0.722660  [110400/175341]\n",
      "loss: 0.895218  [112000/175341]\n",
      "loss: 0.411465  [113600/175341]\n",
      "loss: 0.429370  [115200/175341]\n",
      "loss: 0.219411  [116800/175341]\n",
      "loss: 0.127877  [118400/175341]\n",
      "loss: 0.628799  [120000/175341]\n",
      "loss: 0.687669  [121600/175341]\n",
      "loss: 0.522616  [123200/175341]\n",
      "loss: 0.419381  [124800/175341]\n",
      "loss: 0.323741  [126400/175341]\n",
      "loss: 0.627829  [128000/175341]\n",
      "loss: 0.672521  [129600/175341]\n",
      "loss: 0.314323  [131200/175341]\n",
      "loss: 0.383299  [132800/175341]\n",
      "loss: 0.294749  [134400/175341]\n",
      "loss: 0.521805  [136000/175341]\n",
      "loss: 0.484417  [137600/175341]\n",
      "loss: 0.397217  [139200/175341]\n",
      "loss: 0.428084  [140800/175341]\n",
      "loss: 0.187043  [142400/175341]\n",
      "loss: 0.741778  [144000/175341]\n",
      "loss: 0.493521  [145600/175341]\n",
      "loss: 0.772433  [147200/175341]\n",
      "loss: 0.537823  [148800/175341]\n",
      "loss: 0.172113  [150400/175341]\n",
      "loss: 0.465130  [152000/175341]\n",
      "loss: 0.419466  [153600/175341]\n",
      "loss: 0.051976  [155200/175341]\n",
      "loss: 0.234631  [156800/175341]\n",
      "loss: 0.368623  [158400/175341]\n",
      "loss: 0.079103  [160000/175341]\n",
      "loss: 0.267273  [161600/175341]\n",
      "loss: 0.489467  [163200/175341]\n",
      "loss: 0.569890  [164800/175341]\n",
      "loss: 0.623102  [166400/175341]\n",
      "loss: 0.236161  [168000/175341]\n",
      "loss: 0.641870  [169600/175341]\n",
      "loss: 0.280748  [171200/175341]\n",
      "loss: 0.686122  [172800/175341]\n",
      "loss: 0.603843  [174400/175341]\n",
      "Train Accuracy: 81.7504%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.566888, F1-score: 75.02%, Macro_F1-Score:  40.90%  \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.693130  [    0/175341]\n",
      "loss: 0.479781  [ 1600/175341]\n",
      "loss: 0.612175  [ 3200/175341]\n",
      "loss: 0.337992  [ 4800/175341]\n",
      "loss: 0.592082  [ 6400/175341]\n",
      "loss: 0.681065  [ 8000/175341]\n",
      "loss: 0.544655  [ 9600/175341]\n",
      "loss: 0.643484  [11200/175341]\n",
      "loss: 0.349329  [12800/175341]\n",
      "loss: 0.701471  [14400/175341]\n",
      "loss: 0.352023  [16000/175341]\n",
      "loss: 0.079313  [17600/175341]\n",
      "loss: 0.371949  [19200/175341]\n",
      "loss: 0.405284  [20800/175341]\n",
      "loss: 0.424113  [22400/175341]\n",
      "loss: 0.078876  [24000/175341]\n",
      "loss: 0.576280  [25600/175341]\n",
      "loss: 0.687163  [27200/175341]\n",
      "loss: 0.683239  [28800/175341]\n",
      "loss: 0.745556  [30400/175341]\n",
      "loss: 0.517692  [32000/175341]\n",
      "loss: 0.376510  [33600/175341]\n",
      "loss: 0.403806  [35200/175341]\n",
      "loss: 0.215733  [36800/175341]\n",
      "loss: 0.484834  [38400/175341]\n",
      "loss: 0.699129  [40000/175341]\n",
      "loss: 0.331544  [41600/175341]\n",
      "loss: 0.717096  [43200/175341]\n",
      "loss: 0.339876  [44800/175341]\n",
      "loss: 0.361412  [46400/175341]\n",
      "loss: 0.466464  [48000/175341]\n",
      "loss: 0.547359  [49600/175341]\n",
      "loss: 0.330748  [51200/175341]\n",
      "loss: 0.542623  [52800/175341]\n",
      "loss: 0.486137  [54400/175341]\n",
      "loss: 0.320009  [56000/175341]\n",
      "loss: 0.464758  [57600/175341]\n",
      "loss: 0.855545  [59200/175341]\n",
      "loss: 0.595147  [60800/175341]\n",
      "loss: 0.437533  [62400/175341]\n",
      "loss: 0.342092  [64000/175341]\n",
      "loss: 0.397699  [65600/175341]\n",
      "loss: 0.711874  [67200/175341]\n",
      "loss: 0.430596  [68800/175341]\n",
      "loss: 0.836964  [70400/175341]\n",
      "loss: 0.503911  [72000/175341]\n",
      "loss: 0.399118  [73600/175341]\n",
      "loss: 0.303967  [75200/175341]\n",
      "loss: 0.691433  [76800/175341]\n",
      "loss: 0.401915  [78400/175341]\n",
      "loss: 0.358726  [80000/175341]\n",
      "loss: 0.357183  [81600/175341]\n",
      "loss: 0.366857  [83200/175341]\n",
      "loss: 0.558940  [84800/175341]\n",
      "loss: 0.550614  [86400/175341]\n",
      "loss: 0.866956  [88000/175341]\n",
      "loss: 0.554673  [89600/175341]\n",
      "loss: 0.295604  [91200/175341]\n",
      "loss: 0.847196  [92800/175341]\n",
      "loss: 0.487779  [94400/175341]\n",
      "loss: 0.414958  [96000/175341]\n",
      "loss: 0.519049  [97600/175341]\n",
      "loss: 0.698389  [99200/175341]\n",
      "loss: 0.308612  [100800/175341]\n",
      "loss: 0.681225  [102400/175341]\n",
      "loss: 0.521474  [104000/175341]\n",
      "loss: 1.200749  [105600/175341]\n",
      "loss: 0.215631  [107200/175341]\n",
      "loss: 0.423085  [108800/175341]\n",
      "loss: 0.376532  [110400/175341]\n",
      "loss: 0.269649  [112000/175341]\n",
      "loss: 0.627051  [113600/175341]\n",
      "loss: 0.557578  [115200/175341]\n",
      "loss: 0.586117  [116800/175341]\n",
      "loss: 0.474693  [118400/175341]\n",
      "loss: 0.508406  [120000/175341]\n",
      "loss: 0.336877  [121600/175341]\n",
      "loss: 0.178280  [123200/175341]\n",
      "loss: 0.410194  [124800/175341]\n",
      "loss: 0.182291  [126400/175341]\n",
      "loss: 0.412360  [128000/175341]\n",
      "loss: 0.209417  [129600/175341]\n",
      "loss: 0.432411  [131200/175341]\n",
      "loss: 0.166124  [132800/175341]\n",
      "loss: 0.445015  [134400/175341]\n",
      "loss: 0.272983  [136000/175341]\n",
      "loss: 0.133319  [137600/175341]\n",
      "loss: 0.402447  [139200/175341]\n",
      "loss: 0.706121  [140800/175341]\n",
      "loss: 0.274198  [142400/175341]\n",
      "loss: 0.569027  [144000/175341]\n",
      "loss: 0.481623  [145600/175341]\n",
      "loss: 0.305321  [147200/175341]\n",
      "loss: 1.071723  [148800/175341]\n",
      "loss: 0.985880  [150400/175341]\n",
      "loss: 0.384042  [152000/175341]\n",
      "loss: 0.377610  [153600/175341]\n",
      "loss: 0.278104  [155200/175341]\n",
      "loss: 0.418780  [156800/175341]\n",
      "loss: 0.539139  [158400/175341]\n",
      "loss: 0.564703  [160000/175341]\n",
      "loss: 0.314198  [161600/175341]\n",
      "loss: 0.558268  [163200/175341]\n",
      "loss: 0.395393  [164800/175341]\n",
      "loss: 0.392412  [166400/175341]\n",
      "loss: 0.479248  [168000/175341]\n",
      "loss: 0.359622  [169600/175341]\n",
      "loss: 0.167792  [171200/175341]\n",
      "loss: 0.153433  [172800/175341]\n",
      "loss: 0.361395  [174400/175341]\n",
      "Train Accuracy: 81.7014%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.548414, F1-score: 76.58%, Macro_F1-Score:  41.53%  \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.359228  [    0/175341]\n",
      "loss: 0.375334  [ 1600/175341]\n",
      "loss: 0.734114  [ 3200/175341]\n",
      "loss: 0.568385  [ 4800/175341]\n",
      "loss: 0.330117  [ 6400/175341]\n",
      "loss: 0.599386  [ 8000/175341]\n",
      "loss: 0.377552  [ 9600/175341]\n",
      "loss: 0.539995  [11200/175341]\n",
      "loss: 0.250192  [12800/175341]\n",
      "loss: 0.509483  [14400/175341]\n",
      "loss: 0.732386  [16000/175341]\n",
      "loss: 0.361416  [17600/175341]\n",
      "loss: 0.466719  [19200/175341]\n",
      "loss: 0.418471  [20800/175341]\n",
      "loss: 0.534130  [22400/175341]\n",
      "loss: 0.401233  [24000/175341]\n",
      "loss: 0.985018  [25600/175341]\n",
      "loss: 0.754515  [27200/175341]\n",
      "loss: 0.464240  [28800/175341]\n",
      "loss: 0.611009  [30400/175341]\n",
      "loss: 0.797536  [32000/175341]\n",
      "loss: 0.305905  [33600/175341]\n",
      "loss: 0.518918  [35200/175341]\n",
      "loss: 0.205090  [36800/175341]\n",
      "loss: 0.182277  [38400/175341]\n",
      "loss: 0.248354  [40000/175341]\n",
      "loss: 0.161941  [41600/175341]\n",
      "loss: 0.544233  [43200/175341]\n",
      "loss: 0.284993  [44800/175341]\n",
      "loss: 0.511445  [46400/175341]\n",
      "loss: 0.367867  [48000/175341]\n",
      "loss: 0.705754  [49600/175341]\n",
      "loss: 0.447656  [51200/175341]\n",
      "loss: 0.388297  [52800/175341]\n",
      "loss: 0.322967  [54400/175341]\n",
      "loss: 0.678412  [56000/175341]\n",
      "loss: 0.442599  [57600/175341]\n",
      "loss: 0.310128  [59200/175341]\n",
      "loss: 0.823169  [60800/175341]\n",
      "loss: 0.321346  [62400/175341]\n",
      "loss: 0.226504  [64000/175341]\n",
      "loss: 0.191340  [65600/175341]\n",
      "loss: 0.109025  [67200/175341]\n",
      "loss: 0.484412  [68800/175341]\n",
      "loss: 0.752786  [70400/175341]\n",
      "loss: 0.337072  [72000/175341]\n",
      "loss: 0.399885  [73600/175341]\n",
      "loss: 0.347111  [75200/175341]\n",
      "loss: 0.330187  [76800/175341]\n",
      "loss: 0.743603  [78400/175341]\n",
      "loss: 0.286404  [80000/175341]\n",
      "loss: 0.626438  [81600/175341]\n",
      "loss: 0.262889  [83200/175341]\n",
      "loss: 0.630400  [84800/175341]\n",
      "loss: 0.323315  [86400/175341]\n",
      "loss: 0.223605  [88000/175341]\n",
      "loss: 0.246832  [89600/175341]\n",
      "loss: 0.272485  [91200/175341]\n",
      "loss: 0.203054  [92800/175341]\n",
      "loss: 0.342834  [94400/175341]\n",
      "loss: 0.154714  [96000/175341]\n",
      "loss: 0.127608  [97600/175341]\n",
      "loss: 0.586069  [99200/175341]\n",
      "loss: 0.441054  [100800/175341]\n",
      "loss: 0.505487  [102400/175341]\n",
      "loss: 0.133178  [104000/175341]\n",
      "loss: 0.503904  [105600/175341]\n",
      "loss: 0.538616  [107200/175341]\n",
      "loss: 0.144201  [108800/175341]\n",
      "loss: 0.685365  [110400/175341]\n",
      "loss: 0.153618  [112000/175341]\n",
      "loss: 0.296367  [113600/175341]\n",
      "loss: 0.464188  [115200/175341]\n",
      "loss: 0.247274  [116800/175341]\n",
      "loss: 0.114458  [118400/175341]\n",
      "loss: 0.192151  [120000/175341]\n",
      "loss: 0.464033  [121600/175341]\n",
      "loss: 0.290596  [123200/175341]\n",
      "loss: 0.218196  [124800/175341]\n",
      "loss: 0.383178  [126400/175341]\n",
      "loss: 0.422142  [128000/175341]\n",
      "loss: 0.137584  [129600/175341]\n",
      "loss: 0.637639  [131200/175341]\n",
      "loss: 0.293486  [132800/175341]\n",
      "loss: 0.640827  [134400/175341]\n",
      "loss: 0.471787  [136000/175341]\n",
      "loss: 0.091518  [137600/175341]\n",
      "loss: 0.510257  [139200/175341]\n",
      "loss: 0.922479  [140800/175341]\n",
      "loss: 0.575209  [142400/175341]\n",
      "loss: 0.252895  [144000/175341]\n",
      "loss: 0.525428  [145600/175341]\n",
      "loss: 0.585802  [147200/175341]\n",
      "loss: 0.779359  [148800/175341]\n",
      "loss: 0.643157  [150400/175341]\n",
      "loss: 0.471067  [152000/175341]\n",
      "loss: 0.735027  [153600/175341]\n",
      "loss: 0.565413  [155200/175341]\n",
      "loss: 0.207349  [156800/175341]\n",
      "loss: 0.923201  [158400/175341]\n",
      "loss: 0.201761  [160000/175341]\n",
      "loss: 0.450039  [161600/175341]\n",
      "loss: 0.278891  [163200/175341]\n",
      "loss: 0.286240  [164800/175341]\n",
      "loss: 0.473016  [166400/175341]\n",
      "loss: 0.498019  [168000/175341]\n",
      "loss: 0.705009  [169600/175341]\n",
      "loss: 0.425758  [171200/175341]\n",
      "loss: 0.606597  [172800/175341]\n",
      "loss: 0.497888  [174400/175341]\n",
      "Train Accuracy: 81.7652%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.547041, F1-score: 76.46%, Macro_F1-Score:  41.85%  \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.471396  [    0/175341]\n",
      "loss: 0.755338  [ 1600/175341]\n",
      "loss: 0.365440  [ 3200/175341]\n",
      "loss: 0.178853  [ 4800/175341]\n",
      "loss: 0.230143  [ 6400/175341]\n",
      "loss: 0.545051  [ 8000/175341]\n",
      "loss: 0.537167  [ 9600/175341]\n",
      "loss: 0.109921  [11200/175341]\n",
      "loss: 0.305749  [12800/175341]\n",
      "loss: 0.531461  [14400/175341]\n",
      "loss: 0.505780  [16000/175341]\n",
      "loss: 0.442579  [17600/175341]\n",
      "loss: 0.274550  [19200/175341]\n",
      "loss: 0.547667  [20800/175341]\n",
      "loss: 0.422011  [22400/175341]\n",
      "loss: 0.744864  [24000/175341]\n",
      "loss: 0.880346  [25600/175341]\n",
      "loss: 0.602360  [27200/175341]\n",
      "loss: 0.275994  [28800/175341]\n",
      "loss: 0.562842  [30400/175341]\n",
      "loss: 0.789565  [32000/175341]\n",
      "loss: 0.388852  [33600/175341]\n",
      "loss: 0.227561  [35200/175341]\n",
      "loss: 0.611726  [36800/175341]\n",
      "loss: 0.805409  [38400/175341]\n",
      "loss: 0.656118  [40000/175341]\n",
      "loss: 0.467661  [41600/175341]\n",
      "loss: 0.259218  [43200/175341]\n",
      "loss: 0.335706  [44800/175341]\n",
      "loss: 0.457983  [46400/175341]\n",
      "loss: 0.330763  [48000/175341]\n",
      "loss: 0.162178  [49600/175341]\n",
      "loss: 0.466678  [51200/175341]\n",
      "loss: 0.319436  [52800/175341]\n",
      "loss: 0.569254  [54400/175341]\n",
      "loss: 0.311398  [56000/175341]\n",
      "loss: 0.629513  [57600/175341]\n",
      "loss: 0.313571  [59200/175341]\n",
      "loss: 0.255395  [60800/175341]\n",
      "loss: 0.275121  [62400/175341]\n",
      "loss: 0.403970  [64000/175341]\n",
      "loss: 0.464161  [65600/175341]\n",
      "loss: 0.171194  [67200/175341]\n",
      "loss: 0.430648  [68800/175341]\n",
      "loss: 0.399000  [70400/175341]\n",
      "loss: 0.395444  [72000/175341]\n",
      "loss: 0.531071  [73600/175341]\n",
      "loss: 0.355620  [75200/175341]\n",
      "loss: 0.596052  [76800/175341]\n",
      "loss: 0.309744  [78400/175341]\n",
      "loss: 0.499915  [80000/175341]\n",
      "loss: 0.789448  [81600/175341]\n",
      "loss: 0.330201  [83200/175341]\n",
      "loss: 0.450450  [84800/175341]\n",
      "loss: 0.630058  [86400/175341]\n",
      "loss: 0.354859  [88000/175341]\n",
      "loss: 0.317121  [89600/175341]\n",
      "loss: 0.513402  [91200/175341]\n",
      "loss: 0.458700  [92800/175341]\n",
      "loss: 0.260146  [94400/175341]\n",
      "loss: 0.255484  [96000/175341]\n",
      "loss: 0.481094  [97600/175341]\n",
      "loss: 0.633772  [99200/175341]\n",
      "loss: 0.228459  [100800/175341]\n",
      "loss: 0.433232  [102400/175341]\n",
      "loss: 0.447997  [104000/175341]\n",
      "loss: 0.333309  [105600/175341]\n",
      "loss: 0.781700  [107200/175341]\n",
      "loss: 0.393768  [108800/175341]\n",
      "loss: 0.575350  [110400/175341]\n",
      "loss: 0.540941  [112000/175341]\n",
      "loss: 0.504010  [113600/175341]\n",
      "loss: 0.365316  [115200/175341]\n",
      "loss: 0.380579  [116800/175341]\n",
      "loss: 0.421461  [118400/175341]\n",
      "loss: 0.297401  [120000/175341]\n",
      "loss: 0.458065  [121600/175341]\n",
      "loss: 0.528354  [123200/175341]\n",
      "loss: 0.341175  [124800/175341]\n",
      "loss: 0.232560  [126400/175341]\n",
      "loss: 0.608950  [128000/175341]\n",
      "loss: 0.259841  [129600/175341]\n",
      "loss: 0.234548  [131200/175341]\n",
      "loss: 0.576123  [132800/175341]\n",
      "loss: 0.245055  [134400/175341]\n",
      "loss: 0.371305  [136000/175341]\n",
      "loss: 0.355809  [137600/175341]\n",
      "loss: 0.413139  [139200/175341]\n",
      "loss: 0.106720  [140800/175341]\n",
      "loss: 0.299913  [142400/175341]\n",
      "loss: 0.701847  [144000/175341]\n",
      "loss: 0.473301  [145600/175341]\n",
      "loss: 0.134288  [147200/175341]\n",
      "loss: 0.294072  [148800/175341]\n",
      "loss: 1.095055  [150400/175341]\n",
      "loss: 0.543407  [152000/175341]\n",
      "loss: 0.656963  [153600/175341]\n",
      "loss: 0.279021  [155200/175341]\n",
      "loss: 0.508062  [156800/175341]\n",
      "loss: 0.274607  [158400/175341]\n",
      "loss: 0.554110  [160000/175341]\n",
      "loss: 0.297365  [161600/175341]\n",
      "loss: 0.284379  [163200/175341]\n",
      "loss: 0.639980  [164800/175341]\n",
      "loss: 0.205674  [166400/175341]\n",
      "loss: 0.598594  [168000/175341]\n",
      "loss: 0.338794  [169600/175341]\n",
      "loss: 0.180438  [171200/175341]\n",
      "loss: 0.145869  [172800/175341]\n",
      "loss: 0.651891  [174400/175341]\n",
      "Train Accuracy: 81.7379%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.543778, F1-score: 76.80%, Macro_F1-Score:  41.36%  \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.751784  [    0/175341]\n",
      "loss: 0.581285  [ 1600/175341]\n",
      "loss: 0.302887  [ 3200/175341]\n",
      "loss: 0.321463  [ 4800/175341]\n",
      "loss: 0.266641  [ 6400/175341]\n",
      "loss: 0.400572  [ 8000/175341]\n",
      "loss: 0.498439  [ 9600/175341]\n",
      "loss: 0.347179  [11200/175341]\n",
      "loss: 0.381040  [12800/175341]\n",
      "loss: 0.474845  [14400/175341]\n",
      "loss: 0.338643  [16000/175341]\n",
      "loss: 0.657647  [17600/175341]\n",
      "loss: 0.187071  [19200/175341]\n",
      "loss: 0.542399  [20800/175341]\n",
      "loss: 0.636160  [22400/175341]\n",
      "loss: 0.059601  [24000/175341]\n",
      "loss: 0.390121  [25600/175341]\n",
      "loss: 0.463702  [27200/175341]\n",
      "loss: 0.488315  [28800/175341]\n",
      "loss: 0.478008  [30400/175341]\n",
      "loss: 0.776183  [32000/175341]\n",
      "loss: 0.190000  [33600/175341]\n",
      "loss: 0.384838  [35200/175341]\n",
      "loss: 0.475515  [36800/175341]\n",
      "loss: 0.334334  [38400/175341]\n",
      "loss: 0.554223  [40000/175341]\n",
      "loss: 0.248993  [41600/175341]\n",
      "loss: 0.332365  [43200/175341]\n",
      "loss: 0.514959  [44800/175341]\n",
      "loss: 0.442148  [46400/175341]\n",
      "loss: 0.587121  [48000/175341]\n",
      "loss: 0.590066  [49600/175341]\n",
      "loss: 0.265366  [51200/175341]\n",
      "loss: 0.605138  [52800/175341]\n",
      "loss: 0.414784  [54400/175341]\n",
      "loss: 0.326881  [56000/175341]\n",
      "loss: 0.290961  [57600/175341]\n",
      "loss: 0.137248  [59200/175341]\n",
      "loss: 0.583939  [60800/175341]\n",
      "loss: 0.485901  [62400/175341]\n",
      "loss: 0.514844  [64000/175341]\n",
      "loss: 0.750827  [65600/175341]\n",
      "loss: 0.600513  [67200/175341]\n",
      "loss: 0.328036  [68800/175341]\n",
      "loss: 0.842191  [70400/175341]\n",
      "loss: 0.305559  [72000/175341]\n",
      "loss: 0.314611  [73600/175341]\n",
      "loss: 0.166131  [75200/175341]\n",
      "loss: 0.121420  [76800/175341]\n",
      "loss: 0.476813  [78400/175341]\n",
      "loss: 0.433233  [80000/175341]\n",
      "loss: 0.292862  [81600/175341]\n",
      "loss: 0.123579  [83200/175341]\n",
      "loss: 0.147317  [84800/175341]\n",
      "loss: 0.185656  [86400/175341]\n",
      "loss: 0.479587  [88000/175341]\n",
      "loss: 0.549635  [89600/175341]\n",
      "loss: 0.189064  [91200/175341]\n",
      "loss: 0.190272  [92800/175341]\n",
      "loss: 0.357086  [94400/175341]\n",
      "loss: 0.145624  [96000/175341]\n",
      "loss: 0.315934  [97600/175341]\n",
      "loss: 0.551008  [99200/175341]\n",
      "loss: 0.423878  [100800/175341]\n",
      "loss: 0.412940  [102400/175341]\n",
      "loss: 0.359261  [104000/175341]\n",
      "loss: 0.522776  [105600/175341]\n",
      "loss: 0.521168  [107200/175341]\n",
      "loss: 0.203050  [108800/175341]\n",
      "loss: 0.282165  [110400/175341]\n",
      "loss: 0.294342  [112000/175341]\n",
      "loss: 0.743562  [113600/175341]\n",
      "loss: 0.185503  [115200/175341]\n",
      "loss: 0.536579  [116800/175341]\n",
      "loss: 0.773034  [118400/175341]\n",
      "loss: 0.367474  [120000/175341]\n",
      "loss: 0.682326  [121600/175341]\n",
      "loss: 0.632398  [123200/175341]\n",
      "loss: 0.228300  [124800/175341]\n",
      "loss: 0.363951  [126400/175341]\n",
      "loss: 0.321364  [128000/175341]\n",
      "loss: 0.622022  [129600/175341]\n",
      "loss: 0.430726  [131200/175341]\n",
      "loss: 0.176737  [132800/175341]\n",
      "loss: 0.468380  [134400/175341]\n",
      "loss: 0.237579  [136000/175341]\n",
      "loss: 0.160322  [137600/175341]\n",
      "loss: 0.448100  [139200/175341]\n",
      "loss: 0.525908  [140800/175341]\n",
      "loss: 0.311428  [142400/175341]\n",
      "loss: 0.501779  [144000/175341]\n",
      "loss: 0.462236  [145600/175341]\n",
      "loss: 0.989595  [147200/175341]\n",
      "loss: 0.540139  [148800/175341]\n",
      "loss: 0.178729  [150400/175341]\n",
      "loss: 0.674512  [152000/175341]\n",
      "loss: 0.368754  [153600/175341]\n",
      "loss: 0.247023  [155200/175341]\n",
      "loss: 0.393712  [156800/175341]\n",
      "loss: 0.558241  [158400/175341]\n",
      "loss: 0.573468  [160000/175341]\n",
      "loss: 0.464380  [161600/175341]\n",
      "loss: 0.384540  [163200/175341]\n",
      "loss: 0.252659  [164800/175341]\n",
      "loss: 0.367891  [166400/175341]\n",
      "loss: 0.196809  [168000/175341]\n",
      "loss: 0.493066  [169600/175341]\n",
      "loss: 0.342770  [171200/175341]\n",
      "loss: 0.385895  [172800/175341]\n",
      "loss: 0.702178  [174400/175341]\n",
      "Train Accuracy: 81.7305%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.545095, F1-score: 76.79%, Macro_F1-Score:  41.57%  \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.417446  [    0/175341]\n",
      "loss: 0.335513  [ 1600/175341]\n",
      "loss: 0.173378  [ 3200/175341]\n",
      "loss: 0.212817  [ 4800/175341]\n",
      "loss: 0.716875  [ 6400/175341]\n",
      "loss: 0.299934  [ 8000/175341]\n",
      "loss: 0.319927  [ 9600/175341]\n",
      "loss: 0.442528  [11200/175341]\n",
      "loss: 0.284351  [12800/175341]\n",
      "loss: 0.117121  [14400/175341]\n",
      "loss: 0.591287  [16000/175341]\n",
      "loss: 0.446777  [17600/175341]\n",
      "loss: 0.504832  [19200/175341]\n",
      "loss: 0.325075  [20800/175341]\n",
      "loss: 0.384251  [22400/175341]\n",
      "loss: 0.185470  [24000/175341]\n",
      "loss: 0.135291  [25600/175341]\n",
      "loss: 0.252414  [27200/175341]\n",
      "loss: 0.535625  [28800/175341]\n",
      "loss: 0.429858  [30400/175341]\n",
      "loss: 0.414370  [32000/175341]\n",
      "loss: 0.364317  [33600/175341]\n",
      "loss: 0.333524  [35200/175341]\n",
      "loss: 0.701802  [36800/175341]\n",
      "loss: 0.225359  [38400/175341]\n",
      "loss: 0.420690  [40000/175341]\n",
      "loss: 1.075460  [41600/175341]\n",
      "loss: 0.468925  [43200/175341]\n",
      "loss: 0.620143  [44800/175341]\n",
      "loss: 1.129053  [46400/175341]\n",
      "loss: 0.394041  [48000/175341]\n",
      "loss: 0.462505  [49600/175341]\n",
      "loss: 0.174001  [51200/175341]\n",
      "loss: 0.535892  [52800/175341]\n",
      "loss: 0.222400  [54400/175341]\n",
      "loss: 0.504845  [56000/175341]\n",
      "loss: 0.395000  [57600/175341]\n",
      "loss: 0.517770  [59200/175341]\n",
      "loss: 0.758864  [60800/175341]\n",
      "loss: 0.466116  [62400/175341]\n",
      "loss: 0.543485  [64000/175341]\n",
      "loss: 0.626214  [65600/175341]\n",
      "loss: 0.401479  [67200/175341]\n",
      "loss: 0.633787  [68800/175341]\n",
      "loss: 0.361322  [70400/175341]\n",
      "loss: 0.436591  [72000/175341]\n",
      "loss: 0.295077  [73600/175341]\n",
      "loss: 0.715160  [75200/175341]\n",
      "loss: 0.507354  [76800/175341]\n",
      "loss: 0.356166  [78400/175341]\n",
      "loss: 0.587367  [80000/175341]\n",
      "loss: 0.387967  [81600/175341]\n",
      "loss: 0.154094  [83200/175341]\n",
      "loss: 0.463341  [84800/175341]\n",
      "loss: 0.600429  [86400/175341]\n",
      "loss: 0.294845  [88000/175341]\n",
      "loss: 1.227423  [89600/175341]\n",
      "loss: 0.410191  [91200/175341]\n",
      "loss: 0.560230  [92800/175341]\n",
      "loss: 0.533763  [94400/175341]\n",
      "loss: 0.607367  [96000/175341]\n",
      "loss: 0.336106  [97600/175341]\n",
      "loss: 0.155862  [99200/175341]\n",
      "loss: 0.263150  [100800/175341]\n",
      "loss: 0.173355  [102400/175341]\n",
      "loss: 0.583621  [104000/175341]\n",
      "loss: 0.155839  [105600/175341]\n",
      "loss: 0.816860  [107200/175341]\n",
      "loss: 0.632053  [108800/175341]\n",
      "loss: 0.466075  [110400/175341]\n",
      "loss: 0.187503  [112000/175341]\n",
      "loss: 0.678201  [113600/175341]\n",
      "loss: 0.406021  [115200/175341]\n",
      "loss: 0.416494  [116800/175341]\n",
      "loss: 0.636199  [118400/175341]\n",
      "loss: 0.235765  [120000/175341]\n",
      "loss: 0.252433  [121600/175341]\n",
      "loss: 0.393850  [123200/175341]\n",
      "loss: 0.395006  [124800/175341]\n",
      "loss: 0.560900  [126400/175341]\n",
      "loss: 0.589463  [128000/175341]\n",
      "loss: 0.296036  [129600/175341]\n",
      "loss: 0.522920  [131200/175341]\n",
      "loss: 0.414358  [132800/175341]\n",
      "loss: 0.644681  [134400/175341]\n",
      "loss: 0.703974  [136000/175341]\n",
      "loss: 0.202676  [137600/175341]\n",
      "loss: 0.611931  [139200/175341]\n",
      "loss: 0.592563  [140800/175341]\n",
      "loss: 0.306096  [142400/175341]\n",
      "loss: 0.228648  [144000/175341]\n",
      "loss: 0.111337  [145600/175341]\n",
      "loss: 0.759503  [147200/175341]\n",
      "loss: 0.153188  [148800/175341]\n",
      "loss: 0.606096  [150400/175341]\n",
      "loss: 0.241607  [152000/175341]\n",
      "loss: 0.272471  [153600/175341]\n",
      "loss: 0.822895  [155200/175341]\n",
      "loss: 0.370834  [156800/175341]\n",
      "loss: 0.611655  [158400/175341]\n",
      "loss: 0.123897  [160000/175341]\n",
      "loss: 0.399952  [161600/175341]\n",
      "loss: 0.227807  [163200/175341]\n",
      "loss: 0.158447  [164800/175341]\n",
      "loss: 0.376933  [166400/175341]\n",
      "loss: 0.310305  [168000/175341]\n",
      "loss: 0.357223  [169600/175341]\n",
      "loss: 0.561147  [171200/175341]\n",
      "loss: 0.546292  [172800/175341]\n",
      "loss: 0.441926  [174400/175341]\n",
      "Train Accuracy: 81.7749%\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.554627, F1-score: 75.91%, Macro_F1-Score:  41.63%  \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.516001  [    0/175341]\n",
      "loss: 0.274538  [ 1600/175341]\n",
      "loss: 0.442865  [ 3200/175341]\n",
      "loss: 0.444494  [ 4800/175341]\n",
      "loss: 0.522388  [ 6400/175341]\n",
      "loss: 0.487793  [ 8000/175341]\n",
      "loss: 0.227432  [ 9600/175341]\n",
      "loss: 0.649332  [11200/175341]\n",
      "loss: 0.204906  [12800/175341]\n",
      "loss: 0.178258  [14400/175341]\n",
      "loss: 0.637196  [16000/175341]\n",
      "loss: 0.207320  [17600/175341]\n",
      "loss: 0.534903  [19200/175341]\n",
      "loss: 0.262906  [20800/175341]\n",
      "loss: 0.195195  [22400/175341]\n",
      "loss: 0.646880  [24000/175341]\n",
      "loss: 0.512521  [25600/175341]\n",
      "loss: 0.385359  [27200/175341]\n",
      "loss: 0.394903  [28800/175341]\n",
      "loss: 0.450362  [30400/175341]\n",
      "loss: 0.405003  [32000/175341]\n",
      "loss: 0.760971  [33600/175341]\n",
      "loss: 0.606280  [35200/175341]\n",
      "loss: 0.260042  [36800/175341]\n",
      "loss: 0.464516  [38400/175341]\n",
      "loss: 0.305528  [40000/175341]\n",
      "loss: 0.358375  [41600/175341]\n",
      "loss: 0.406465  [43200/175341]\n",
      "loss: 0.216086  [44800/175341]\n",
      "loss: 0.316194  [46400/175341]\n",
      "loss: 0.142531  [48000/175341]\n",
      "loss: 0.445066  [49600/175341]\n",
      "loss: 0.336185  [51200/175341]\n",
      "loss: 0.679864  [52800/175341]\n",
      "loss: 0.370152  [54400/175341]\n",
      "loss: 0.539998  [56000/175341]\n",
      "loss: 0.328638  [57600/175341]\n",
      "loss: 0.465176  [59200/175341]\n",
      "loss: 0.419785  [60800/175341]\n",
      "loss: 0.314397  [62400/175341]\n",
      "loss: 0.590401  [64000/175341]\n",
      "loss: 0.309059  [65600/175341]\n",
      "loss: 0.593496  [67200/175341]\n",
      "loss: 0.380020  [68800/175341]\n",
      "loss: 0.470579  [70400/175341]\n",
      "loss: 0.628842  [72000/175341]\n",
      "loss: 0.142572  [73600/175341]\n",
      "loss: 0.373577  [75200/175341]\n",
      "loss: 0.488231  [76800/175341]\n",
      "loss: 0.355604  [78400/175341]\n",
      "loss: 0.366933  [80000/175341]\n",
      "loss: 0.599037  [81600/175341]\n",
      "loss: 0.508536  [83200/175341]\n",
      "loss: 0.613429  [84800/175341]\n",
      "loss: 0.698810  [86400/175341]\n",
      "loss: 0.118239  [88000/175341]\n",
      "loss: 0.169559  [89600/175341]\n",
      "loss: 0.372393  [91200/175341]\n",
      "loss: 1.083465  [92800/175341]\n",
      "loss: 0.406141  [94400/175341]\n",
      "loss: 0.418460  [96000/175341]\n",
      "loss: 0.639502  [97600/175341]\n",
      "loss: 0.486627  [99200/175341]\n",
      "loss: 0.352843  [100800/175341]\n",
      "loss: 0.362659  [102400/175341]\n",
      "loss: 0.444070  [104000/175341]\n",
      "loss: 0.427660  [105600/175341]\n",
      "loss: 0.559245  [107200/175341]\n",
      "loss: 0.527247  [108800/175341]\n",
      "loss: 0.700829  [110400/175341]\n",
      "loss: 0.425720  [112000/175341]\n",
      "loss: 0.430401  [113600/175341]\n",
      "loss: 0.541867  [115200/175341]\n",
      "loss: 0.567264  [116800/175341]\n",
      "loss: 0.173368  [118400/175341]\n",
      "loss: 0.210504  [120000/175341]\n",
      "loss: 0.680638  [121600/175341]\n",
      "loss: 0.504783  [123200/175341]\n",
      "loss: 0.242850  [124800/175341]\n",
      "loss: 0.779424  [126400/175341]\n",
      "loss: 0.152398  [128000/175341]\n",
      "loss: 0.416006  [129600/175341]\n",
      "loss: 1.019518  [131200/175341]\n",
      "loss: 0.436841  [132800/175341]\n",
      "loss: 0.258819  [134400/175341]\n",
      "loss: 0.465373  [136000/175341]\n",
      "loss: 0.247989  [137600/175341]\n",
      "loss: 0.282166  [139200/175341]\n",
      "loss: 0.352907  [140800/175341]\n",
      "loss: 0.449542  [142400/175341]\n",
      "loss: 0.303911  [144000/175341]\n",
      "loss: 0.378246  [145600/175341]\n",
      "loss: 0.354291  [147200/175341]\n",
      "loss: 0.343531  [148800/175341]\n",
      "loss: 0.387332  [150400/175341]\n",
      "loss: 0.409331  [152000/175341]\n",
      "loss: 0.290705  [153600/175341]\n",
      "loss: 0.207795  [155200/175341]\n",
      "loss: 0.246474  [156800/175341]\n",
      "loss: 0.413894  [158400/175341]\n",
      "loss: 0.398275  [160000/175341]\n",
      "loss: 0.641861  [161600/175341]\n",
      "loss: 0.318619  [163200/175341]\n",
      "loss: 0.290025  [164800/175341]\n",
      "loss: 0.661750  [166400/175341]\n",
      "loss: 0.127005  [168000/175341]\n",
      "loss: 0.425666  [169600/175341]\n",
      "loss: 0.415082  [171200/175341]\n",
      "loss: 0.324436  [172800/175341]\n",
      "loss: 0.622978  [174400/175341]\n",
      "Train Accuracy: 81.7059%\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.580901, F1-score: 74.48%, Macro_F1-Score:  40.58%  \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.530473  [    0/175341]\n",
      "loss: 0.373284  [ 1600/175341]\n",
      "loss: 0.458225  [ 3200/175341]\n",
      "loss: 0.775858  [ 4800/175341]\n",
      "loss: 0.207993  [ 6400/175341]\n",
      "loss: 0.473158  [ 8000/175341]\n",
      "loss: 0.535651  [ 9600/175341]\n",
      "loss: 0.420941  [11200/175341]\n",
      "loss: 0.564250  [12800/175341]\n",
      "loss: 0.597777  [14400/175341]\n",
      "loss: 0.263806  [16000/175341]\n",
      "loss: 0.465504  [17600/175341]\n",
      "loss: 0.438907  [19200/175341]\n",
      "loss: 0.404055  [20800/175341]\n",
      "loss: 0.274235  [22400/175341]\n",
      "loss: 0.367362  [24000/175341]\n",
      "loss: 0.232664  [25600/175341]\n",
      "loss: 0.277333  [27200/175341]\n",
      "loss: 1.121887  [28800/175341]\n",
      "loss: 0.670049  [30400/175341]\n",
      "loss: 0.466181  [32000/175341]\n",
      "loss: 0.509920  [33600/175341]\n",
      "loss: 0.536409  [35200/175341]\n",
      "loss: 0.862553  [36800/175341]\n",
      "loss: 0.364718  [38400/175341]\n",
      "loss: 0.363135  [40000/175341]\n",
      "loss: 0.065541  [41600/175341]\n",
      "loss: 0.188007  [43200/175341]\n",
      "loss: 0.444708  [44800/175341]\n",
      "loss: 0.181575  [46400/175341]\n",
      "loss: 0.366192  [48000/175341]\n",
      "loss: 0.707743  [49600/175341]\n",
      "loss: 0.801451  [51200/175341]\n",
      "loss: 0.648597  [52800/175341]\n",
      "loss: 0.583598  [54400/175341]\n",
      "loss: 0.166388  [56000/175341]\n",
      "loss: 0.325721  [57600/175341]\n",
      "loss: 0.632704  [59200/175341]\n",
      "loss: 0.701225  [60800/175341]\n",
      "loss: 0.463192  [62400/175341]\n",
      "loss: 0.584091  [64000/175341]\n",
      "loss: 0.110303  [65600/175341]\n",
      "loss: 0.515248  [67200/175341]\n",
      "loss: 0.407774  [68800/175341]\n",
      "loss: 0.361052  [70400/175341]\n",
      "loss: 0.114867  [72000/175341]\n",
      "loss: 0.188601  [73600/175341]\n",
      "loss: 0.155946  [75200/175341]\n",
      "loss: 0.479049  [76800/175341]\n",
      "loss: 0.444635  [78400/175341]\n",
      "loss: 0.660240  [80000/175341]\n",
      "loss: 0.593275  [81600/175341]\n",
      "loss: 0.642338  [83200/175341]\n",
      "loss: 0.938686  [84800/175341]\n",
      "loss: 0.081185  [86400/175341]\n",
      "loss: 0.546821  [88000/175341]\n",
      "loss: 0.399641  [89600/175341]\n",
      "loss: 0.253077  [91200/175341]\n",
      "loss: 0.356708  [92800/175341]\n",
      "loss: 0.541958  [94400/175341]\n",
      "loss: 0.654807  [96000/175341]\n",
      "loss: 0.423540  [97600/175341]\n",
      "loss: 0.346354  [99200/175341]\n",
      "loss: 0.374122  [100800/175341]\n",
      "loss: 0.359691  [102400/175341]\n",
      "loss: 0.538881  [104000/175341]\n",
      "loss: 0.299768  [105600/175341]\n",
      "loss: 0.364701  [107200/175341]\n",
      "loss: 0.377644  [108800/175341]\n",
      "loss: 0.353689  [110400/175341]\n",
      "loss: 0.618653  [112000/175341]\n",
      "loss: 0.538050  [113600/175341]\n",
      "loss: 0.279017  [115200/175341]\n",
      "loss: 0.309558  [116800/175341]\n",
      "loss: 0.814586  [118400/175341]\n",
      "loss: 0.686729  [120000/175341]\n",
      "loss: 0.461384  [121600/175341]\n",
      "loss: 0.638842  [123200/175341]\n",
      "loss: 0.319415  [124800/175341]\n",
      "loss: 0.976496  [126400/175341]\n",
      "loss: 0.433845  [128000/175341]\n",
      "loss: 0.597670  [129600/175341]\n",
      "loss: 0.296526  [131200/175341]\n",
      "loss: 0.553468  [132800/175341]\n",
      "loss: 0.576678  [134400/175341]\n",
      "loss: 0.430287  [136000/175341]\n",
      "loss: 0.266197  [137600/175341]\n",
      "loss: 0.509216  [139200/175341]\n",
      "loss: 0.363051  [140800/175341]\n",
      "loss: 0.365241  [142400/175341]\n",
      "loss: 0.289576  [144000/175341]\n",
      "loss: 0.829456  [145600/175341]\n",
      "loss: 0.624566  [147200/175341]\n",
      "loss: 0.419615  [148800/175341]\n",
      "loss: 0.445669  [150400/175341]\n",
      "loss: 0.757745  [152000/175341]\n",
      "loss: 0.337951  [153600/175341]\n",
      "loss: 0.680937  [155200/175341]\n",
      "loss: 0.763632  [156800/175341]\n",
      "loss: 0.352983  [158400/175341]\n",
      "loss: 0.312226  [160000/175341]\n",
      "loss: 0.378435  [161600/175341]\n",
      "loss: 0.319062  [163200/175341]\n",
      "loss: 0.207689  [164800/175341]\n",
      "loss: 0.307954  [166400/175341]\n",
      "loss: 0.330157  [168000/175341]\n",
      "loss: 0.174278  [169600/175341]\n",
      "loss: 0.139473  [171200/175341]\n",
      "loss: 0.353243  [172800/175341]\n",
      "loss: 0.295796  [174400/175341]\n",
      "Train Accuracy: 81.7270%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.544282, F1-score: 76.96%, Macro_F1-Score:  41.90%  \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.621522  [    0/175341]\n",
      "loss: 0.505035  [ 1600/175341]\n",
      "loss: 0.433241  [ 3200/175341]\n",
      "loss: 0.288050  [ 4800/175341]\n",
      "loss: 0.762432  [ 6400/175341]\n",
      "loss: 0.760823  [ 8000/175341]\n",
      "loss: 0.445141  [ 9600/175341]\n",
      "loss: 0.502129  [11200/175341]\n",
      "loss: 0.132652  [12800/175341]\n",
      "loss: 0.623231  [14400/175341]\n",
      "loss: 0.170166  [16000/175341]\n",
      "loss: 0.585591  [17600/175341]\n",
      "loss: 0.535316  [19200/175341]\n",
      "loss: 0.658067  [20800/175341]\n",
      "loss: 0.566888  [22400/175341]\n",
      "loss: 0.631527  [24000/175341]\n",
      "loss: 0.392534  [25600/175341]\n",
      "loss: 0.119526  [27200/175341]\n",
      "loss: 0.367095  [28800/175341]\n",
      "loss: 0.460046  [30400/175341]\n",
      "loss: 0.727357  [32000/175341]\n",
      "loss: 0.146707  [33600/175341]\n",
      "loss: 0.153044  [35200/175341]\n",
      "loss: 0.450405  [36800/175341]\n",
      "loss: 0.304428  [38400/175341]\n",
      "loss: 0.265618  [40000/175341]\n",
      "loss: 0.206145  [41600/175341]\n",
      "loss: 0.067451  [43200/175341]\n",
      "loss: 0.471655  [44800/175341]\n",
      "loss: 0.263703  [46400/175341]\n",
      "loss: 0.578014  [48000/175341]\n",
      "loss: 0.434607  [49600/175341]\n",
      "loss: 0.440399  [51200/175341]\n",
      "loss: 0.328645  [52800/175341]\n",
      "loss: 0.677611  [54400/175341]\n",
      "loss: 0.259797  [56000/175341]\n",
      "loss: 0.495592  [57600/175341]\n",
      "loss: 0.331012  [59200/175341]\n",
      "loss: 0.352661  [60800/175341]\n",
      "loss: 0.658851  [62400/175341]\n",
      "loss: 0.537547  [64000/175341]\n",
      "loss: 0.522004  [65600/175341]\n",
      "loss: 0.610145  [67200/175341]\n",
      "loss: 0.184724  [68800/175341]\n",
      "loss: 0.992129  [70400/175341]\n",
      "loss: 0.256678  [72000/175341]\n",
      "loss: 0.552692  [73600/175341]\n",
      "loss: 0.675601  [75200/175341]\n",
      "loss: 0.614953  [76800/175341]\n",
      "loss: 0.214704  [78400/175341]\n",
      "loss: 0.670277  [80000/175341]\n",
      "loss: 0.238883  [81600/175341]\n",
      "loss: 0.226684  [83200/175341]\n",
      "loss: 0.267310  [84800/175341]\n",
      "loss: 0.323791  [86400/175341]\n",
      "loss: 0.457438  [88000/175341]\n",
      "loss: 0.304298  [89600/175341]\n",
      "loss: 0.549660  [91200/175341]\n",
      "loss: 0.573273  [92800/175341]\n",
      "loss: 0.229879  [94400/175341]\n",
      "loss: 0.098550  [96000/175341]\n",
      "loss: 0.471255  [97600/175341]\n",
      "loss: 0.286338  [99200/175341]\n",
      "loss: 0.482545  [100800/175341]\n",
      "loss: 0.480347  [102400/175341]\n",
      "loss: 0.369387  [104000/175341]\n",
      "loss: 0.778675  [105600/175341]\n",
      "loss: 0.257803  [107200/175341]\n",
      "loss: 0.315044  [108800/175341]\n",
      "loss: 0.330215  [110400/175341]\n",
      "loss: 0.335823  [112000/175341]\n",
      "loss: 0.985073  [113600/175341]\n",
      "loss: 0.485020  [115200/175341]\n",
      "loss: 0.542546  [116800/175341]\n",
      "loss: 0.106783  [118400/175341]\n",
      "loss: 0.357183  [120000/175341]\n",
      "loss: 0.504221  [121600/175341]\n",
      "loss: 0.527957  [123200/175341]\n",
      "loss: 0.320779  [124800/175341]\n",
      "loss: 0.587596  [126400/175341]\n",
      "loss: 0.852289  [128000/175341]\n",
      "loss: 0.934579  [129600/175341]\n",
      "loss: 0.310191  [131200/175341]\n",
      "loss: 0.334319  [132800/175341]\n",
      "loss: 0.459078  [134400/175341]\n",
      "loss: 0.427446  [136000/175341]\n",
      "loss: 0.164140  [137600/175341]\n",
      "loss: 0.965616  [139200/175341]\n",
      "loss: 0.262733  [140800/175341]\n",
      "loss: 0.190175  [142400/175341]\n",
      "loss: 0.527666  [144000/175341]\n",
      "loss: 0.386821  [145600/175341]\n",
      "loss: 0.169685  [147200/175341]\n",
      "loss: 0.656018  [148800/175341]\n",
      "loss: 0.539015  [150400/175341]\n",
      "loss: 0.199858  [152000/175341]\n",
      "loss: 0.614421  [153600/175341]\n",
      "loss: 1.004267  [155200/175341]\n",
      "loss: 0.226888  [156800/175341]\n",
      "loss: 0.458858  [158400/175341]\n",
      "loss: 0.316985  [160000/175341]\n",
      "loss: 0.414084  [161600/175341]\n",
      "loss: 0.255473  [163200/175341]\n",
      "loss: 0.352101  [164800/175341]\n",
      "loss: 0.425250  [166400/175341]\n",
      "loss: 0.432394  [168000/175341]\n",
      "loss: 0.291240  [169600/175341]\n",
      "loss: 0.303183  [171200/175341]\n",
      "loss: 0.690040  [172800/175341]\n",
      "loss: 0.202510  [174400/175341]\n",
      "Train Accuracy: 81.7590%\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.564089, F1-score: 75.33%, Macro_F1-Score:  41.53%  \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.368619  [    0/175341]\n",
      "loss: 0.235527  [ 1600/175341]\n",
      "loss: 0.262036  [ 3200/175341]\n",
      "loss: 0.477627  [ 4800/175341]\n",
      "loss: 0.361730  [ 6400/175341]\n",
      "loss: 0.294329  [ 8000/175341]\n",
      "loss: 0.465094  [ 9600/175341]\n",
      "loss: 0.332721  [11200/175341]\n",
      "loss: 0.287112  [12800/175341]\n",
      "loss: 0.542063  [14400/175341]\n",
      "loss: 0.623437  [16000/175341]\n",
      "loss: 0.381953  [17600/175341]\n",
      "loss: 0.428559  [19200/175341]\n",
      "loss: 0.819513  [20800/175341]\n",
      "loss: 0.245950  [22400/175341]\n",
      "loss: 0.117538  [24000/175341]\n",
      "loss: 0.142184  [25600/175341]\n",
      "loss: 0.477139  [27200/175341]\n",
      "loss: 0.596144  [28800/175341]\n",
      "loss: 0.566050  [30400/175341]\n",
      "loss: 0.651513  [32000/175341]\n",
      "loss: 0.500669  [33600/175341]\n",
      "loss: 0.168068  [35200/175341]\n",
      "loss: 0.349903  [36800/175341]\n",
      "loss: 0.455011  [38400/175341]\n",
      "loss: 0.643492  [40000/175341]\n",
      "loss: 0.284332  [41600/175341]\n",
      "loss: 0.587571  [43200/175341]\n",
      "loss: 0.844084  [44800/175341]\n",
      "loss: 0.870600  [46400/175341]\n",
      "loss: 0.507864  [48000/175341]\n",
      "loss: 0.553107  [49600/175341]\n",
      "loss: 0.555558  [51200/175341]\n",
      "loss: 0.322527  [52800/175341]\n",
      "loss: 0.335559  [54400/175341]\n",
      "loss: 0.327880  [56000/175341]\n",
      "loss: 0.602609  [57600/175341]\n",
      "loss: 0.617073  [59200/175341]\n",
      "loss: 0.605384  [60800/175341]\n",
      "loss: 0.495421  [62400/175341]\n",
      "loss: 0.473387  [64000/175341]\n",
      "loss: 0.495252  [65600/175341]\n",
      "loss: 0.526840  [67200/175341]\n",
      "loss: 0.479094  [68800/175341]\n",
      "loss: 0.206079  [70400/175341]\n",
      "loss: 0.269953  [72000/175341]\n",
      "loss: 0.953087  [73600/175341]\n",
      "loss: 0.648434  [75200/175341]\n",
      "loss: 0.198439  [76800/175341]\n",
      "loss: 0.608197  [78400/175341]\n",
      "loss: 0.425587  [80000/175341]\n",
      "loss: 0.575325  [81600/175341]\n",
      "loss: 0.390590  [83200/175341]\n",
      "loss: 0.774037  [84800/175341]\n",
      "loss: 0.386494  [86400/175341]\n",
      "loss: 0.779142  [88000/175341]\n",
      "loss: 0.443383  [89600/175341]\n",
      "loss: 0.187529  [91200/175341]\n",
      "loss: 0.209135  [92800/175341]\n",
      "loss: 0.576738  [94400/175341]\n",
      "loss: 0.316226  [96000/175341]\n",
      "loss: 0.146129  [97600/175341]\n",
      "loss: 0.474925  [99200/175341]\n",
      "loss: 0.280532  [100800/175341]\n",
      "loss: 0.396303  [102400/175341]\n",
      "loss: 0.166143  [104000/175341]\n",
      "loss: 0.653122  [105600/175341]\n",
      "loss: 0.857055  [107200/175341]\n",
      "loss: 0.295451  [108800/175341]\n",
      "loss: 0.289944  [110400/175341]\n",
      "loss: 0.486022  [112000/175341]\n",
      "loss: 0.380829  [113600/175341]\n",
      "loss: 0.749176  [115200/175341]\n",
      "loss: 0.301575  [116800/175341]\n",
      "loss: 0.486482  [118400/175341]\n",
      "loss: 0.430215  [120000/175341]\n",
      "loss: 0.258058  [121600/175341]\n",
      "loss: 0.612542  [123200/175341]\n",
      "loss: 0.559967  [124800/175341]\n",
      "loss: 0.408283  [126400/175341]\n",
      "loss: 0.454850  [128000/175341]\n",
      "loss: 0.382172  [129600/175341]\n",
      "loss: 0.316663  [131200/175341]\n",
      "loss: 0.365416  [132800/175341]\n",
      "loss: 0.483211  [134400/175341]\n",
      "loss: 0.205191  [136000/175341]\n",
      "loss: 0.275756  [137600/175341]\n",
      "loss: 0.682222  [139200/175341]\n",
      "loss: 0.627187  [140800/175341]\n",
      "loss: 0.567604  [142400/175341]\n",
      "loss: 0.494133  [144000/175341]\n",
      "loss: 0.299304  [145600/175341]\n",
      "loss: 1.055687  [147200/175341]\n",
      "loss: 0.448130  [148800/175341]\n",
      "loss: 0.661267  [150400/175341]\n",
      "loss: 0.665933  [152000/175341]\n",
      "loss: 0.779230  [153600/175341]\n",
      "loss: 0.438572  [155200/175341]\n",
      "loss: 1.053301  [156800/175341]\n",
      "loss: 0.371491  [158400/175341]\n",
      "loss: 0.382182  [160000/175341]\n",
      "loss: 0.399194  [161600/175341]\n",
      "loss: 0.386549  [163200/175341]\n",
      "loss: 0.238160  [164800/175341]\n",
      "loss: 0.395196  [166400/175341]\n",
      "loss: 0.567507  [168000/175341]\n",
      "loss: 0.556010  [169600/175341]\n",
      "loss: 1.018794  [171200/175341]\n",
      "loss: 0.485934  [172800/175341]\n",
      "loss: 0.328065  [174400/175341]\n",
      "Train Accuracy: 81.7784%\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.541644, F1-score: 76.78%, Macro_F1-Score:  42.16%  \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.613350  [    0/175341]\n",
      "loss: 0.269945  [ 1600/175341]\n",
      "loss: 0.490841  [ 3200/175341]\n",
      "loss: 0.748024  [ 4800/175341]\n",
      "loss: 0.387321  [ 6400/175341]\n",
      "loss: 0.515849  [ 8000/175341]\n",
      "loss: 0.688633  [ 9600/175341]\n",
      "loss: 0.242812  [11200/175341]\n",
      "loss: 0.860800  [12800/175341]\n",
      "loss: 0.704372  [14400/175341]\n",
      "loss: 0.407662  [16000/175341]\n",
      "loss: 0.271146  [17600/175341]\n",
      "loss: 0.236641  [19200/175341]\n",
      "loss: 0.412500  [20800/175341]\n",
      "loss: 0.443272  [22400/175341]\n",
      "loss: 0.290520  [24000/175341]\n",
      "loss: 0.761973  [25600/175341]\n",
      "loss: 0.238155  [27200/175341]\n",
      "loss: 0.356046  [28800/175341]\n",
      "loss: 0.216979  [30400/175341]\n",
      "loss: 0.695802  [32000/175341]\n",
      "loss: 0.642773  [33600/175341]\n",
      "loss: 0.509681  [35200/175341]\n",
      "loss: 0.225226  [36800/175341]\n",
      "loss: 0.364976  [38400/175341]\n",
      "loss: 0.330049  [40000/175341]\n",
      "loss: 0.345055  [41600/175341]\n",
      "loss: 0.492029  [43200/175341]\n",
      "loss: 0.381718  [44800/175341]\n",
      "loss: 0.607906  [46400/175341]\n",
      "loss: 0.553402  [48000/175341]\n",
      "loss: 0.286405  [49600/175341]\n",
      "loss: 0.555234  [51200/175341]\n",
      "loss: 0.634236  [52800/175341]\n",
      "loss: 0.130571  [54400/175341]\n",
      "loss: 0.167767  [56000/175341]\n",
      "loss: 0.506744  [57600/175341]\n",
      "loss: 0.626328  [59200/175341]\n",
      "loss: 0.226295  [60800/175341]\n",
      "loss: 0.533484  [62400/175341]\n",
      "loss: 0.189701  [64000/175341]\n",
      "loss: 0.470173  [65600/175341]\n",
      "loss: 0.408856  [67200/175341]\n",
      "loss: 0.369813  [68800/175341]\n",
      "loss: 0.389607  [70400/175341]\n",
      "loss: 0.635449  [72000/175341]\n",
      "loss: 0.316171  [73600/175341]\n",
      "loss: 0.338824  [75200/175341]\n",
      "loss: 0.244000  [76800/175341]\n",
      "loss: 0.378447  [78400/175341]\n",
      "loss: 0.594118  [80000/175341]\n",
      "loss: 0.470709  [81600/175341]\n",
      "loss: 0.697596  [83200/175341]\n",
      "loss: 0.132201  [84800/175341]\n",
      "loss: 0.397938  [86400/175341]\n",
      "loss: 0.344576  [88000/175341]\n",
      "loss: 0.539591  [89600/175341]\n",
      "loss: 0.333418  [91200/175341]\n",
      "loss: 0.353874  [92800/175341]\n",
      "loss: 0.653901  [94400/175341]\n",
      "loss: 0.384852  [96000/175341]\n",
      "loss: 0.146457  [97600/175341]\n",
      "loss: 0.215542  [99200/175341]\n",
      "loss: 0.252377  [100800/175341]\n",
      "loss: 0.670260  [102400/175341]\n",
      "loss: 0.405968  [104000/175341]\n",
      "loss: 0.476876  [105600/175341]\n",
      "loss: 0.283069  [107200/175341]\n",
      "loss: 0.619773  [108800/175341]\n",
      "loss: 0.612677  [110400/175341]\n",
      "loss: 0.586482  [112000/175341]\n",
      "loss: 0.571289  [113600/175341]\n",
      "loss: 0.437986  [115200/175341]\n",
      "loss: 0.367846  [116800/175341]\n",
      "loss: 0.516101  [118400/175341]\n",
      "loss: 0.865947  [120000/175341]\n",
      "loss: 0.191416  [121600/175341]\n",
      "loss: 0.366028  [123200/175341]\n",
      "loss: 0.142215  [124800/175341]\n",
      "loss: 0.329532  [126400/175341]\n",
      "loss: 0.316482  [128000/175341]\n",
      "loss: 0.720251  [129600/175341]\n",
      "loss: 0.267246  [131200/175341]\n",
      "loss: 0.855237  [132800/175341]\n",
      "loss: 0.598230  [134400/175341]\n",
      "loss: 0.502002  [136000/175341]\n",
      "loss: 0.598994  [137600/175341]\n",
      "loss: 0.652212  [139200/175341]\n",
      "loss: 0.510369  [140800/175341]\n",
      "loss: 0.486718  [142400/175341]\n",
      "loss: 0.483620  [144000/175341]\n",
      "loss: 0.381509  [145600/175341]\n",
      "loss: 0.226493  [147200/175341]\n",
      "loss: 0.146600  [148800/175341]\n",
      "loss: 0.284285  [150400/175341]\n",
      "loss: 0.406200  [152000/175341]\n",
      "loss: 0.539351  [153600/175341]\n",
      "loss: 0.234080  [155200/175341]\n",
      "loss: 0.482787  [156800/175341]\n",
      "loss: 0.307224  [158400/175341]\n",
      "loss: 0.566073  [160000/175341]\n",
      "loss: 0.562197  [161600/175341]\n",
      "loss: 0.163935  [163200/175341]\n",
      "loss: 0.302185  [164800/175341]\n",
      "loss: 0.548028  [166400/175341]\n",
      "loss: 0.539818  [168000/175341]\n",
      "loss: 0.388784  [169600/175341]\n",
      "loss: 0.212701  [171200/175341]\n",
      "loss: 0.615344  [172800/175341]\n",
      "loss: 0.473810  [174400/175341]\n",
      "Train Accuracy: 81.7943%\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.568887, F1-score: 74.64%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.558233  [    0/175341]\n",
      "loss: 0.337107  [ 1600/175341]\n",
      "loss: 0.739303  [ 3200/175341]\n",
      "loss: 0.545592  [ 4800/175341]\n",
      "loss: 0.728294  [ 6400/175341]\n",
      "loss: 0.641358  [ 8000/175341]\n",
      "loss: 0.764824  [ 9600/175341]\n",
      "loss: 0.542132  [11200/175341]\n",
      "loss: 0.567861  [12800/175341]\n",
      "loss: 0.385561  [14400/175341]\n",
      "loss: 0.372302  [16000/175341]\n",
      "loss: 0.255716  [17600/175341]\n",
      "loss: 0.411984  [19200/175341]\n",
      "loss: 0.682625  [20800/175341]\n",
      "loss: 0.593600  [22400/175341]\n",
      "loss: 0.184707  [24000/175341]\n",
      "loss: 0.317788  [25600/175341]\n",
      "loss: 0.463632  [27200/175341]\n",
      "loss: 0.551608  [28800/175341]\n",
      "loss: 0.395913  [30400/175341]\n",
      "loss: 0.422362  [32000/175341]\n",
      "loss: 0.507581  [33600/175341]\n",
      "loss: 0.414167  [35200/175341]\n",
      "loss: 0.695110  [36800/175341]\n",
      "loss: 0.186026  [38400/175341]\n",
      "loss: 0.619674  [40000/175341]\n",
      "loss: 0.484677  [41600/175341]\n",
      "loss: 0.485032  [43200/175341]\n",
      "loss: 0.438195  [44800/175341]\n",
      "loss: 0.256026  [46400/175341]\n",
      "loss: 0.299053  [48000/175341]\n",
      "loss: 0.365564  [49600/175341]\n",
      "loss: 0.716316  [51200/175341]\n",
      "loss: 0.312241  [52800/175341]\n",
      "loss: 0.396250  [54400/175341]\n",
      "loss: 0.670733  [56000/175341]\n",
      "loss: 0.491215  [57600/175341]\n",
      "loss: 0.578333  [59200/175341]\n",
      "loss: 0.305414  [60800/175341]\n",
      "loss: 0.418171  [62400/175341]\n",
      "loss: 0.420199  [64000/175341]\n",
      "loss: 0.309630  [65600/175341]\n",
      "loss: 0.152695  [67200/175341]\n",
      "loss: 0.576352  [68800/175341]\n",
      "loss: 0.621386  [70400/175341]\n",
      "loss: 0.356380  [72000/175341]\n",
      "loss: 0.707749  [73600/175341]\n",
      "loss: 0.351031  [75200/175341]\n",
      "loss: 0.280885  [76800/175341]\n",
      "loss: 0.426217  [78400/175341]\n",
      "loss: 0.248004  [80000/175341]\n",
      "loss: 0.255728  [81600/175341]\n",
      "loss: 0.202884  [83200/175341]\n",
      "loss: 0.670515  [84800/175341]\n",
      "loss: 0.126658  [86400/175341]\n",
      "loss: 0.816112  [88000/175341]\n",
      "loss: 0.351817  [89600/175341]\n",
      "loss: 0.516271  [91200/175341]\n",
      "loss: 0.192273  [92800/175341]\n",
      "loss: 0.495602  [94400/175341]\n",
      "loss: 0.465835  [96000/175341]\n",
      "loss: 0.333718  [97600/175341]\n",
      "loss: 0.643632  [99200/175341]\n",
      "loss: 0.148243  [100800/175341]\n",
      "loss: 0.461645  [102400/175341]\n",
      "loss: 0.762773  [104000/175341]\n",
      "loss: 0.729372  [105600/175341]\n",
      "loss: 0.500399  [107200/175341]\n",
      "loss: 0.311796  [108800/175341]\n",
      "loss: 0.305212  [110400/175341]\n",
      "loss: 0.798593  [112000/175341]\n",
      "loss: 0.194664  [113600/175341]\n",
      "loss: 0.229199  [115200/175341]\n",
      "loss: 1.023959  [116800/175341]\n",
      "loss: 0.998625  [118400/175341]\n",
      "loss: 0.327363  [120000/175341]\n",
      "loss: 0.338715  [121600/175341]\n",
      "loss: 0.476103  [123200/175341]\n",
      "loss: 0.604475  [124800/175341]\n",
      "loss: 0.316721  [126400/175341]\n",
      "loss: 0.480088  [128000/175341]\n",
      "loss: 0.596254  [129600/175341]\n",
      "loss: 0.359410  [131200/175341]\n",
      "loss: 0.415448  [132800/175341]\n",
      "loss: 0.495369  [134400/175341]\n",
      "loss: 0.312523  [136000/175341]\n",
      "loss: 0.311381  [137600/175341]\n",
      "loss: 0.542802  [139200/175341]\n",
      "loss: 0.419779  [140800/175341]\n",
      "loss: 0.291084  [142400/175341]\n",
      "loss: 0.311355  [144000/175341]\n",
      "loss: 0.457655  [145600/175341]\n",
      "loss: 0.190573  [147200/175341]\n",
      "loss: 0.529081  [148800/175341]\n",
      "loss: 0.376560  [150400/175341]\n",
      "loss: 0.706534  [152000/175341]\n",
      "loss: 0.378097  [153600/175341]\n",
      "loss: 0.665104  [155200/175341]\n",
      "loss: 0.789167  [156800/175341]\n",
      "loss: 0.519309  [158400/175341]\n",
      "loss: 0.488304  [160000/175341]\n",
      "loss: 0.679034  [161600/175341]\n",
      "loss: 0.091361  [163200/175341]\n",
      "loss: 0.444979  [164800/175341]\n",
      "loss: 0.532197  [166400/175341]\n",
      "loss: 0.240846  [168000/175341]\n",
      "loss: 0.664185  [169600/175341]\n",
      "loss: 0.351775  [171200/175341]\n",
      "loss: 0.960160  [172800/175341]\n",
      "loss: 0.778615  [174400/175341]\n",
      "Train Accuracy: 81.7852%\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.569540, F1-score: 74.93%, Macro_F1-Score:  41.21%  \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.492494  [    0/175341]\n",
      "loss: 0.374869  [ 1600/175341]\n",
      "loss: 0.334668  [ 3200/175341]\n",
      "loss: 0.423885  [ 4800/175341]\n",
      "loss: 0.426965  [ 6400/175341]\n",
      "loss: 0.310295  [ 8000/175341]\n",
      "loss: 0.374926  [ 9600/175341]\n",
      "loss: 0.524977  [11200/175341]\n",
      "loss: 0.716766  [12800/175341]\n",
      "loss: 0.315842  [14400/175341]\n",
      "loss: 0.580820  [16000/175341]\n",
      "loss: 0.332965  [17600/175341]\n",
      "loss: 0.485217  [19200/175341]\n",
      "loss: 0.634509  [20800/175341]\n",
      "loss: 0.254095  [22400/175341]\n",
      "loss: 0.412008  [24000/175341]\n",
      "loss: 0.508177  [25600/175341]\n",
      "loss: 0.426560  [27200/175341]\n",
      "loss: 0.408743  [28800/175341]\n",
      "loss: 0.438361  [30400/175341]\n",
      "loss: 0.225081  [32000/175341]\n",
      "loss: 0.261703  [33600/175341]\n",
      "loss: 0.502665  [35200/175341]\n",
      "loss: 0.376153  [36800/175341]\n",
      "loss: 0.421719  [38400/175341]\n",
      "loss: 0.425292  [40000/175341]\n",
      "loss: 0.794648  [41600/175341]\n",
      "loss: 0.476821  [43200/175341]\n",
      "loss: 0.416422  [44800/175341]\n",
      "loss: 0.489246  [46400/175341]\n",
      "loss: 0.394400  [48000/175341]\n",
      "loss: 0.325061  [49600/175341]\n",
      "loss: 0.047679  [51200/175341]\n",
      "loss: 0.228289  [52800/175341]\n",
      "loss: 0.198644  [54400/175341]\n",
      "loss: 0.165048  [56000/175341]\n",
      "loss: 0.716774  [57600/175341]\n",
      "loss: 0.412621  [59200/175341]\n",
      "loss: 0.596298  [60800/175341]\n",
      "loss: 0.273876  [62400/175341]\n",
      "loss: 0.737210  [64000/175341]\n",
      "loss: 0.311448  [65600/175341]\n",
      "loss: 0.242438  [67200/175341]\n",
      "loss: 0.312114  [68800/175341]\n",
      "loss: 0.376759  [70400/175341]\n",
      "loss: 0.450819  [72000/175341]\n",
      "loss: 0.371984  [73600/175341]\n",
      "loss: 0.573952  [75200/175341]\n",
      "loss: 0.508231  [76800/175341]\n",
      "loss: 0.525727  [78400/175341]\n",
      "loss: 0.649459  [80000/175341]\n",
      "loss: 0.287001  [81600/175341]\n",
      "loss: 0.375654  [83200/175341]\n",
      "loss: 0.675475  [84800/175341]\n",
      "loss: 0.316834  [86400/175341]\n",
      "loss: 0.115928  [88000/175341]\n",
      "loss: 0.416681  [89600/175341]\n",
      "loss: 0.510887  [91200/175341]\n",
      "loss: 0.511883  [92800/175341]\n",
      "loss: 0.684594  [94400/175341]\n",
      "loss: 0.361662  [96000/175341]\n",
      "loss: 0.364462  [97600/175341]\n",
      "loss: 0.189388  [99200/175341]\n",
      "loss: 0.620319  [100800/175341]\n",
      "loss: 0.240644  [102400/175341]\n",
      "loss: 0.242166  [104000/175341]\n",
      "loss: 0.331742  [105600/175341]\n",
      "loss: 0.542073  [107200/175341]\n",
      "loss: 0.183330  [108800/175341]\n",
      "loss: 0.336384  [110400/175341]\n",
      "loss: 0.333727  [112000/175341]\n",
      "loss: 0.649298  [113600/175341]\n",
      "loss: 0.444275  [115200/175341]\n",
      "loss: 0.187297  [116800/175341]\n",
      "loss: 0.523301  [118400/175341]\n",
      "loss: 0.429968  [120000/175341]\n",
      "loss: 0.242109  [121600/175341]\n",
      "loss: 0.455220  [123200/175341]\n",
      "loss: 0.174088  [124800/175341]\n",
      "loss: 0.696984  [126400/175341]\n",
      "loss: 0.608584  [128000/175341]\n",
      "loss: 0.354843  [129600/175341]\n",
      "loss: 0.451297  [131200/175341]\n",
      "loss: 0.371777  [132800/175341]\n",
      "loss: 0.394464  [134400/175341]\n",
      "loss: 0.339987  [136000/175341]\n",
      "loss: 0.550719  [137600/175341]\n",
      "loss: 0.357045  [139200/175341]\n",
      "loss: 0.249555  [140800/175341]\n",
      "loss: 0.421612  [142400/175341]\n",
      "loss: 0.367701  [144000/175341]\n",
      "loss: 0.287295  [145600/175341]\n",
      "loss: 0.308623  [147200/175341]\n",
      "loss: 0.454639  [148800/175341]\n",
      "loss: 0.276193  [150400/175341]\n",
      "loss: 0.405349  [152000/175341]\n",
      "loss: 0.239691  [153600/175341]\n",
      "loss: 0.151544  [155200/175341]\n",
      "loss: 0.230255  [156800/175341]\n",
      "loss: 0.573945  [158400/175341]\n",
      "loss: 0.322707  [160000/175341]\n",
      "loss: 0.928773  [161600/175341]\n",
      "loss: 0.447588  [163200/175341]\n",
      "loss: 0.403148  [164800/175341]\n",
      "loss: 0.211904  [166400/175341]\n",
      "loss: 0.856570  [168000/175341]\n",
      "loss: 0.636537  [169600/175341]\n",
      "loss: 0.634725  [171200/175341]\n",
      "loss: 0.203258  [172800/175341]\n",
      "loss: 0.478855  [174400/175341]\n",
      "Train Accuracy: 81.7915%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.561069, F1-score: 75.51%, Macro_F1-Score:  41.27%  \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.692277  [    0/175341]\n",
      "loss: 0.709512  [ 1600/175341]\n",
      "loss: 0.316626  [ 3200/175341]\n",
      "loss: 0.382040  [ 4800/175341]\n",
      "loss: 0.528673  [ 6400/175341]\n",
      "loss: 0.423059  [ 8000/175341]\n",
      "loss: 0.321725  [ 9600/175341]\n",
      "loss: 0.297166  [11200/175341]\n",
      "loss: 0.490307  [12800/175341]\n",
      "loss: 0.434877  [14400/175341]\n",
      "loss: 0.908754  [16000/175341]\n",
      "loss: 0.654639  [17600/175341]\n",
      "loss: 0.748373  [19200/175341]\n",
      "loss: 0.395458  [20800/175341]\n",
      "loss: 0.497821  [22400/175341]\n",
      "loss: 0.775467  [24000/175341]\n",
      "loss: 0.269443  [25600/175341]\n",
      "loss: 0.781720  [27200/175341]\n",
      "loss: 0.443421  [28800/175341]\n",
      "loss: 0.565147  [30400/175341]\n",
      "loss: 0.467427  [32000/175341]\n",
      "loss: 0.594138  [33600/175341]\n",
      "loss: 0.224863  [35200/175341]\n",
      "loss: 0.532536  [36800/175341]\n",
      "loss: 0.423171  [38400/175341]\n",
      "loss: 0.718429  [40000/175341]\n",
      "loss: 0.404833  [41600/175341]\n",
      "loss: 0.634267  [43200/175341]\n",
      "loss: 0.345433  [44800/175341]\n",
      "loss: 0.414988  [46400/175341]\n",
      "loss: 0.251536  [48000/175341]\n",
      "loss: 0.666839  [49600/175341]\n",
      "loss: 0.602046  [51200/175341]\n",
      "loss: 0.551503  [52800/175341]\n",
      "loss: 0.301163  [54400/175341]\n",
      "loss: 0.528715  [56000/175341]\n",
      "loss: 0.315075  [57600/175341]\n",
      "loss: 0.357921  [59200/175341]\n",
      "loss: 0.546218  [60800/175341]\n",
      "loss: 0.658524  [62400/175341]\n",
      "loss: 0.280668  [64000/175341]\n",
      "loss: 0.334113  [65600/175341]\n",
      "loss: 0.208439  [67200/175341]\n",
      "loss: 0.324452  [68800/175341]\n",
      "loss: 0.399282  [70400/175341]\n",
      "loss: 0.325736  [72000/175341]\n",
      "loss: 0.316699  [73600/175341]\n",
      "loss: 0.340790  [75200/175341]\n",
      "loss: 0.564261  [76800/175341]\n",
      "loss: 0.659338  [78400/175341]\n",
      "loss: 0.649137  [80000/175341]\n",
      "loss: 0.088254  [81600/175341]\n",
      "loss: 0.680619  [83200/175341]\n",
      "loss: 0.231812  [84800/175341]\n",
      "loss: 0.254833  [86400/175341]\n",
      "loss: 0.628341  [88000/175341]\n",
      "loss: 0.857505  [89600/175341]\n",
      "loss: 0.399368  [91200/175341]\n",
      "loss: 0.528678  [92800/175341]\n",
      "loss: 0.483997  [94400/175341]\n",
      "loss: 0.622438  [96000/175341]\n",
      "loss: 0.230632  [97600/175341]\n",
      "loss: 0.355543  [99200/175341]\n",
      "loss: 0.455193  [100800/175341]\n",
      "loss: 0.434483  [102400/175341]\n",
      "loss: 0.097163  [104000/175341]\n",
      "loss: 0.369936  [105600/175341]\n",
      "loss: 0.846692  [107200/175341]\n",
      "loss: 0.236681  [108800/175341]\n",
      "loss: 0.154778  [110400/175341]\n",
      "loss: 0.304145  [112000/175341]\n",
      "loss: 0.456400  [113600/175341]\n",
      "loss: 0.357462  [115200/175341]\n",
      "loss: 0.384479  [116800/175341]\n",
      "loss: 0.466751  [118400/175341]\n",
      "loss: 0.467203  [120000/175341]\n",
      "loss: 0.378147  [121600/175341]\n",
      "loss: 0.562998  [123200/175341]\n",
      "loss: 0.277322  [124800/175341]\n",
      "loss: 0.725834  [126400/175341]\n",
      "loss: 0.317966  [128000/175341]\n",
      "loss: 0.174128  [129600/175341]\n",
      "loss: 0.520222  [131200/175341]\n",
      "loss: 0.561073  [132800/175341]\n",
      "loss: 0.558374  [134400/175341]\n",
      "loss: 0.765135  [136000/175341]\n",
      "loss: 0.548662  [137600/175341]\n",
      "loss: 0.415495  [139200/175341]\n",
      "loss: 0.401856  [140800/175341]\n",
      "loss: 0.456818  [142400/175341]\n",
      "loss: 0.261816  [144000/175341]\n",
      "loss: 0.622411  [145600/175341]\n",
      "loss: 0.384737  [147200/175341]\n",
      "loss: 0.201460  [148800/175341]\n",
      "loss: 0.227645  [150400/175341]\n",
      "loss: 0.854782  [152000/175341]\n",
      "loss: 0.352266  [153600/175341]\n",
      "loss: 0.482996  [155200/175341]\n",
      "loss: 0.350034  [156800/175341]\n",
      "loss: 0.246969  [158400/175341]\n",
      "loss: 0.289283  [160000/175341]\n",
      "loss: 0.567258  [161600/175341]\n",
      "loss: 0.412984  [163200/175341]\n",
      "loss: 0.154104  [164800/175341]\n",
      "loss: 0.425857  [166400/175341]\n",
      "loss: 0.525711  [168000/175341]\n",
      "loss: 0.415083  [169600/175341]\n",
      "loss: 0.742614  [171200/175341]\n",
      "loss: 0.179098  [172800/175341]\n",
      "loss: 0.166366  [174400/175341]\n",
      "Train Accuracy: 81.8531%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.552765, F1-score: 76.20%, Macro_F1-Score:  42.18%  \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.259152  [    0/175341]\n",
      "loss: 0.237628  [ 1600/175341]\n",
      "loss: 0.559283  [ 3200/175341]\n",
      "loss: 0.482655  [ 4800/175341]\n",
      "loss: 0.514955  [ 6400/175341]\n",
      "loss: 0.236682  [ 8000/175341]\n",
      "loss: 0.561308  [ 9600/175341]\n",
      "loss: 0.367455  [11200/175341]\n",
      "loss: 0.233124  [12800/175341]\n",
      "loss: 0.439048  [14400/175341]\n",
      "loss: 0.170297  [16000/175341]\n",
      "loss: 0.265667  [17600/175341]\n",
      "loss: 0.228982  [19200/175341]\n",
      "loss: 0.539290  [20800/175341]\n",
      "loss: 0.308738  [22400/175341]\n",
      "loss: 0.647124  [24000/175341]\n",
      "loss: 0.330612  [25600/175341]\n",
      "loss: 0.632509  [27200/175341]\n",
      "loss: 0.474979  [28800/175341]\n",
      "loss: 0.331395  [30400/175341]\n",
      "loss: 0.744084  [32000/175341]\n",
      "loss: 0.223473  [33600/175341]\n",
      "loss: 0.453378  [35200/175341]\n",
      "loss: 0.286714  [36800/175341]\n",
      "loss: 0.385235  [38400/175341]\n",
      "loss: 0.407364  [40000/175341]\n",
      "loss: 0.547486  [41600/175341]\n",
      "loss: 0.352952  [43200/175341]\n",
      "loss: 0.473353  [44800/175341]\n",
      "loss: 0.400002  [46400/175341]\n",
      "loss: 0.357429  [48000/175341]\n",
      "loss: 0.885142  [49600/175341]\n",
      "loss: 0.235174  [51200/175341]\n",
      "loss: 0.411925  [52800/175341]\n",
      "loss: 0.363092  [54400/175341]\n",
      "loss: 0.563123  [56000/175341]\n",
      "loss: 0.340012  [57600/175341]\n",
      "loss: 0.300318  [59200/175341]\n",
      "loss: 0.203649  [60800/175341]\n",
      "loss: 0.541986  [62400/175341]\n",
      "loss: 0.333734  [64000/175341]\n",
      "loss: 0.261474  [65600/175341]\n",
      "loss: 0.332663  [67200/175341]\n",
      "loss: 0.527702  [68800/175341]\n",
      "loss: 0.358151  [70400/175341]\n",
      "loss: 0.607255  [72000/175341]\n",
      "loss: 0.563466  [73600/175341]\n",
      "loss: 0.343042  [75200/175341]\n",
      "loss: 0.243505  [76800/175341]\n",
      "loss: 0.646309  [78400/175341]\n",
      "loss: 0.294493  [80000/175341]\n",
      "loss: 0.581275  [81600/175341]\n",
      "loss: 0.764168  [83200/175341]\n",
      "loss: 0.250039  [84800/175341]\n",
      "loss: 0.539352  [86400/175341]\n",
      "loss: 0.440075  [88000/175341]\n",
      "loss: 0.393298  [89600/175341]\n",
      "loss: 0.376539  [91200/175341]\n",
      "loss: 0.335672  [92800/175341]\n",
      "loss: 0.448840  [94400/175341]\n",
      "loss: 0.244968  [96000/175341]\n",
      "loss: 0.513638  [97600/175341]\n",
      "loss: 0.509041  [99200/175341]\n",
      "loss: 0.200575  [100800/175341]\n",
      "loss: 0.242834  [102400/175341]\n",
      "loss: 0.313573  [104000/175341]\n",
      "loss: 0.511975  [105600/175341]\n",
      "loss: 0.557131  [107200/175341]\n",
      "loss: 0.361369  [108800/175341]\n",
      "loss: 0.440859  [110400/175341]\n",
      "loss: 0.325336  [112000/175341]\n",
      "loss: 0.395719  [113600/175341]\n",
      "loss: 0.345012  [115200/175341]\n",
      "loss: 0.335601  [116800/175341]\n",
      "loss: 0.244050  [118400/175341]\n",
      "loss: 0.374644  [120000/175341]\n",
      "loss: 0.255971  [121600/175341]\n",
      "loss: 0.277488  [123200/175341]\n",
      "loss: 0.203371  [124800/175341]\n",
      "loss: 0.436303  [126400/175341]\n",
      "loss: 0.270438  [128000/175341]\n",
      "loss: 0.381715  [129600/175341]\n",
      "loss: 0.749766  [131200/175341]\n",
      "loss: 0.086663  [132800/175341]\n",
      "loss: 0.326283  [134400/175341]\n",
      "loss: 0.640183  [136000/175341]\n",
      "loss: 0.932216  [137600/175341]\n",
      "loss: 0.093108  [139200/175341]\n",
      "loss: 0.689118  [140800/175341]\n",
      "loss: 0.459657  [142400/175341]\n",
      "loss: 0.354065  [144000/175341]\n",
      "loss: 0.166893  [145600/175341]\n",
      "loss: 0.424856  [147200/175341]\n",
      "loss: 0.355562  [148800/175341]\n",
      "loss: 0.382542  [150400/175341]\n",
      "loss: 0.699561  [152000/175341]\n",
      "loss: 0.488739  [153600/175341]\n",
      "loss: 0.504337  [155200/175341]\n",
      "loss: 0.603436  [156800/175341]\n",
      "loss: 0.424692  [158400/175341]\n",
      "loss: 0.512582  [160000/175341]\n",
      "loss: 0.325251  [161600/175341]\n",
      "loss: 0.648836  [163200/175341]\n",
      "loss: 0.341474  [164800/175341]\n",
      "loss: 0.739488  [166400/175341]\n",
      "loss: 0.249947  [168000/175341]\n",
      "loss: 0.185614  [169600/175341]\n",
      "loss: 0.193190  [171200/175341]\n",
      "loss: 0.383418  [172800/175341]\n",
      "loss: 0.270683  [174400/175341]\n",
      "Train Accuracy: 81.8166%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.546018, F1-score: 76.47%, Macro_F1-Score:  42.05%  \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.273311  [    0/175341]\n",
      "loss: 0.415944  [ 1600/175341]\n",
      "loss: 0.207921  [ 3200/175341]\n",
      "loss: 0.149988  [ 4800/175341]\n",
      "loss: 0.817293  [ 6400/175341]\n",
      "loss: 0.791739  [ 8000/175341]\n",
      "loss: 0.201040  [ 9600/175341]\n",
      "loss: 0.287493  [11200/175341]\n",
      "loss: 0.205727  [12800/175341]\n",
      "loss: 0.376096  [14400/175341]\n",
      "loss: 0.374737  [16000/175341]\n",
      "loss: 0.402769  [17600/175341]\n",
      "loss: 0.420128  [19200/175341]\n",
      "loss: 0.450981  [20800/175341]\n",
      "loss: 0.833960  [22400/175341]\n",
      "loss: 0.400582  [24000/175341]\n",
      "loss: 0.707116  [25600/175341]\n",
      "loss: 0.221966  [27200/175341]\n",
      "loss: 0.378930  [28800/175341]\n",
      "loss: 0.709358  [30400/175341]\n",
      "loss: 0.496879  [32000/175341]\n",
      "loss: 0.531065  [33600/175341]\n",
      "loss: 0.331135  [35200/175341]\n",
      "loss: 0.707887  [36800/175341]\n",
      "loss: 0.378855  [38400/175341]\n",
      "loss: 0.637252  [40000/175341]\n",
      "loss: 0.393435  [41600/175341]\n",
      "loss: 0.672412  [43200/175341]\n",
      "loss: 0.645597  [44800/175341]\n",
      "loss: 0.587231  [46400/175341]\n",
      "loss: 0.475398  [48000/175341]\n",
      "loss: 0.433378  [49600/175341]\n",
      "loss: 0.393475  [51200/175341]\n",
      "loss: 0.306738  [52800/175341]\n",
      "loss: 0.354453  [54400/175341]\n",
      "loss: 0.767042  [56000/175341]\n",
      "loss: 0.649745  [57600/175341]\n",
      "loss: 0.552759  [59200/175341]\n",
      "loss: 0.656418  [60800/175341]\n",
      "loss: 0.571227  [62400/175341]\n",
      "loss: 0.274189  [64000/175341]\n",
      "loss: 0.365726  [65600/175341]\n",
      "loss: 0.386438  [67200/175341]\n",
      "loss: 0.563881  [68800/175341]\n",
      "loss: 0.219867  [70400/175341]\n",
      "loss: 0.452819  [72000/175341]\n",
      "loss: 0.482612  [73600/175341]\n",
      "loss: 0.395227  [75200/175341]\n",
      "loss: 0.236206  [76800/175341]\n",
      "loss: 0.260447  [78400/175341]\n",
      "loss: 0.630796  [80000/175341]\n",
      "loss: 0.267826  [81600/175341]\n",
      "loss: 0.427404  [83200/175341]\n",
      "loss: 0.377838  [84800/175341]\n",
      "loss: 0.361292  [86400/175341]\n",
      "loss: 0.518166  [88000/175341]\n",
      "loss: 0.355379  [89600/175341]\n",
      "loss: 0.274191  [91200/175341]\n",
      "loss: 0.932310  [92800/175341]\n",
      "loss: 0.424175  [94400/175341]\n",
      "loss: 0.898998  [96000/175341]\n",
      "loss: 0.582327  [97600/175341]\n",
      "loss: 0.558493  [99200/175341]\n",
      "loss: 0.486978  [100800/175341]\n",
      "loss: 0.521493  [102400/175341]\n",
      "loss: 0.536543  [104000/175341]\n",
      "loss: 0.309321  [105600/175341]\n",
      "loss: 0.729459  [107200/175341]\n",
      "loss: 0.584859  [108800/175341]\n",
      "loss: 0.444350  [110400/175341]\n",
      "loss: 0.698128  [112000/175341]\n",
      "loss: 0.613949  [113600/175341]\n",
      "loss: 0.327543  [115200/175341]\n",
      "loss: 0.483184  [116800/175341]\n",
      "loss: 0.510428  [118400/175341]\n",
      "loss: 0.474787  [120000/175341]\n",
      "loss: 0.367211  [121600/175341]\n",
      "loss: 0.238299  [123200/175341]\n",
      "loss: 0.526834  [124800/175341]\n",
      "loss: 0.444509  [126400/175341]\n",
      "loss: 0.439286  [128000/175341]\n",
      "loss: 0.357479  [129600/175341]\n",
      "loss: 0.261273  [131200/175341]\n",
      "loss: 0.489557  [132800/175341]\n",
      "loss: 0.441667  [134400/175341]\n",
      "loss: 0.326911  [136000/175341]\n",
      "loss: 0.400014  [137600/175341]\n",
      "loss: 0.513351  [139200/175341]\n",
      "loss: 0.305661  [140800/175341]\n",
      "loss: 0.763465  [142400/175341]\n",
      "loss: 0.266318  [144000/175341]\n",
      "loss: 0.401787  [145600/175341]\n",
      "loss: 0.296533  [147200/175341]\n",
      "loss: 0.440434  [148800/175341]\n",
      "loss: 0.472941  [150400/175341]\n",
      "loss: 0.531664  [152000/175341]\n",
      "loss: 0.517900  [153600/175341]\n",
      "loss: 0.492181  [155200/175341]\n",
      "loss: 0.160518  [156800/175341]\n",
      "loss: 0.521323  [158400/175341]\n",
      "loss: 0.146676  [160000/175341]\n",
      "loss: 0.314868  [161600/175341]\n",
      "loss: 0.454046  [163200/175341]\n",
      "loss: 0.526552  [164800/175341]\n",
      "loss: 0.502857  [166400/175341]\n",
      "loss: 0.289307  [168000/175341]\n",
      "loss: 0.403859  [169600/175341]\n",
      "loss: 0.657199  [171200/175341]\n",
      "loss: 0.635978  [172800/175341]\n",
      "loss: 0.595524  [174400/175341]\n",
      "Train Accuracy: 81.7687%\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.559981, F1-score: 75.87%, Macro_F1-Score:  42.54%  \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.119026  [    0/175341]\n",
      "loss: 0.305062  [ 1600/175341]\n",
      "loss: 0.423721  [ 3200/175341]\n",
      "loss: 0.772577  [ 4800/175341]\n",
      "loss: 0.415303  [ 6400/175341]\n",
      "loss: 0.354579  [ 8000/175341]\n",
      "loss: 0.367508  [ 9600/175341]\n",
      "loss: 0.350591  [11200/175341]\n",
      "loss: 0.407944  [12800/175341]\n",
      "loss: 0.245440  [14400/175341]\n",
      "loss: 0.355696  [16000/175341]\n",
      "loss: 0.439981  [17600/175341]\n",
      "loss: 0.160087  [19200/175341]\n",
      "loss: 0.384944  [20800/175341]\n",
      "loss: 0.425575  [22400/175341]\n",
      "loss: 0.529851  [24000/175341]\n",
      "loss: 0.317605  [25600/175341]\n",
      "loss: 0.188193  [27200/175341]\n",
      "loss: 0.590032  [28800/175341]\n",
      "loss: 0.401429  [30400/175341]\n",
      "loss: 0.347539  [32000/175341]\n",
      "loss: 0.329050  [33600/175341]\n",
      "loss: 0.450087  [35200/175341]\n",
      "loss: 0.585008  [36800/175341]\n",
      "loss: 0.283263  [38400/175341]\n",
      "loss: 0.372315  [40000/175341]\n",
      "loss: 0.799493  [41600/175341]\n",
      "loss: 0.350990  [43200/175341]\n",
      "loss: 0.600182  [44800/175341]\n",
      "loss: 0.517072  [46400/175341]\n",
      "loss: 0.547427  [48000/175341]\n",
      "loss: 0.936302  [49600/175341]\n",
      "loss: 0.654654  [51200/175341]\n",
      "loss: 0.384041  [52800/175341]\n",
      "loss: 0.714507  [54400/175341]\n",
      "loss: 0.185021  [56000/175341]\n",
      "loss: 1.004746  [57600/175341]\n",
      "loss: 0.423259  [59200/175341]\n",
      "loss: 0.208160  [60800/175341]\n",
      "loss: 0.793970  [62400/175341]\n",
      "loss: 0.690409  [64000/175341]\n",
      "loss: 0.263609  [65600/175341]\n",
      "loss: 0.189424  [67200/175341]\n",
      "loss: 0.701011  [68800/175341]\n",
      "loss: 0.598893  [70400/175341]\n",
      "loss: 0.315406  [72000/175341]\n",
      "loss: 0.473132  [73600/175341]\n",
      "loss: 0.647548  [75200/175341]\n",
      "loss: 0.244822  [76800/175341]\n",
      "loss: 0.537836  [78400/175341]\n",
      "loss: 0.212140  [80000/175341]\n",
      "loss: 0.301964  [81600/175341]\n",
      "loss: 0.146271  [83200/175341]\n",
      "loss: 0.579373  [84800/175341]\n",
      "loss: 0.371223  [86400/175341]\n",
      "loss: 0.531560  [88000/175341]\n",
      "loss: 0.610218  [89600/175341]\n",
      "loss: 0.757928  [91200/175341]\n",
      "loss: 0.523600  [92800/175341]\n",
      "loss: 0.280187  [94400/175341]\n",
      "loss: 0.462537  [96000/175341]\n",
      "loss: 0.516638  [97600/175341]\n",
      "loss: 0.518610  [99200/175341]\n",
      "loss: 0.362674  [100800/175341]\n",
      "loss: 0.456841  [102400/175341]\n",
      "loss: 0.194783  [104000/175341]\n",
      "loss: 0.585672  [105600/175341]\n",
      "loss: 0.267892  [107200/175341]\n",
      "loss: 0.307377  [108800/175341]\n",
      "loss: 0.577760  [110400/175341]\n",
      "loss: 0.304337  [112000/175341]\n",
      "loss: 0.243069  [113600/175341]\n",
      "loss: 0.165531  [115200/175341]\n",
      "loss: 1.079417  [116800/175341]\n",
      "loss: 0.393960  [118400/175341]\n",
      "loss: 0.380081  [120000/175341]\n",
      "loss: 0.253785  [121600/175341]\n",
      "loss: 0.228530  [123200/175341]\n",
      "loss: 0.749461  [124800/175341]\n",
      "loss: 0.400607  [126400/175341]\n",
      "loss: 0.481952  [128000/175341]\n",
      "loss: 0.199378  [129600/175341]\n",
      "loss: 0.645706  [131200/175341]\n",
      "loss: 0.490214  [132800/175341]\n",
      "loss: 0.242607  [134400/175341]\n",
      "loss: 0.438627  [136000/175341]\n",
      "loss: 0.352902  [137600/175341]\n",
      "loss: 0.464199  [139200/175341]\n",
      "loss: 0.971725  [140800/175341]\n",
      "loss: 0.465534  [142400/175341]\n",
      "loss: 0.926745  [144000/175341]\n",
      "loss: 0.358622  [145600/175341]\n",
      "loss: 0.324452  [147200/175341]\n",
      "loss: 0.195942  [148800/175341]\n",
      "loss: 0.358252  [150400/175341]\n",
      "loss: 0.512639  [152000/175341]\n",
      "loss: 0.906298  [153600/175341]\n",
      "loss: 0.403602  [155200/175341]\n",
      "loss: 0.586399  [156800/175341]\n",
      "loss: 0.509295  [158400/175341]\n",
      "loss: 0.289082  [160000/175341]\n",
      "loss: 0.204491  [161600/175341]\n",
      "loss: 0.250752  [163200/175341]\n",
      "loss: 0.435676  [164800/175341]\n",
      "loss: 0.279639  [166400/175341]\n",
      "loss: 0.571929  [168000/175341]\n",
      "loss: 0.486885  [169600/175341]\n",
      "loss: 0.356796  [171200/175341]\n",
      "loss: 0.346949  [172800/175341]\n",
      "loss: 0.263500  [174400/175341]\n",
      "Train Accuracy: 81.8012%\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.531118, F1-score: 77.74%, Macro_F1-Score:  42.94%  \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.671646  [    0/175341]\n",
      "loss: 0.482376  [ 1600/175341]\n",
      "loss: 0.378144  [ 3200/175341]\n",
      "loss: 0.522318  [ 4800/175341]\n",
      "loss: 0.472924  [ 6400/175341]\n",
      "loss: 0.551125  [ 8000/175341]\n",
      "loss: 0.384828  [ 9600/175341]\n",
      "loss: 0.207869  [11200/175341]\n",
      "loss: 0.538212  [12800/175341]\n",
      "loss: 0.494122  [14400/175341]\n",
      "loss: 0.308279  [16000/175341]\n",
      "loss: 0.484890  [17600/175341]\n",
      "loss: 0.840582  [19200/175341]\n",
      "loss: 0.257190  [20800/175341]\n",
      "loss: 0.424862  [22400/175341]\n",
      "loss: 0.292374  [24000/175341]\n",
      "loss: 0.373885  [25600/175341]\n",
      "loss: 0.899563  [27200/175341]\n",
      "loss: 0.329915  [28800/175341]\n",
      "loss: 0.327410  [30400/175341]\n",
      "loss: 0.527617  [32000/175341]\n",
      "loss: 0.571288  [33600/175341]\n",
      "loss: 0.480766  [35200/175341]\n",
      "loss: 0.465203  [36800/175341]\n",
      "loss: 0.275890  [38400/175341]\n",
      "loss: 0.258519  [40000/175341]\n",
      "loss: 0.341330  [41600/175341]\n",
      "loss: 0.650014  [43200/175341]\n",
      "loss: 0.755574  [44800/175341]\n",
      "loss: 0.454480  [46400/175341]\n",
      "loss: 0.465897  [48000/175341]\n",
      "loss: 0.181177  [49600/175341]\n",
      "loss: 0.465359  [51200/175341]\n",
      "loss: 0.383338  [52800/175341]\n",
      "loss: 0.506087  [54400/175341]\n",
      "loss: 0.583636  [56000/175341]\n",
      "loss: 0.902815  [57600/175341]\n",
      "loss: 0.674519  [59200/175341]\n",
      "loss: 0.837746  [60800/175341]\n",
      "loss: 0.201656  [62400/175341]\n",
      "loss: 0.251113  [64000/175341]\n",
      "loss: 0.319128  [65600/175341]\n",
      "loss: 0.305669  [67200/175341]\n",
      "loss: 0.176662  [68800/175341]\n",
      "loss: 0.398340  [70400/175341]\n",
      "loss: 0.526134  [72000/175341]\n",
      "loss: 0.263757  [73600/175341]\n",
      "loss: 0.532313  [75200/175341]\n",
      "loss: 0.457873  [76800/175341]\n",
      "loss: 0.545803  [78400/175341]\n",
      "loss: 0.223482  [80000/175341]\n",
      "loss: 0.356502  [81600/175341]\n",
      "loss: 0.315913  [83200/175341]\n",
      "loss: 0.438560  [84800/175341]\n",
      "loss: 0.520074  [86400/175341]\n",
      "loss: 0.672182  [88000/175341]\n",
      "loss: 0.402612  [89600/175341]\n",
      "loss: 0.772011  [91200/175341]\n",
      "loss: 0.866769  [92800/175341]\n",
      "loss: 0.317204  [94400/175341]\n",
      "loss: 0.624253  [96000/175341]\n",
      "loss: 0.404061  [97600/175341]\n",
      "loss: 0.144320  [99200/175341]\n",
      "loss: 0.451317  [100800/175341]\n",
      "loss: 0.589104  [102400/175341]\n",
      "loss: 0.672751  [104000/175341]\n",
      "loss: 0.706442  [105600/175341]\n",
      "loss: 0.803393  [107200/175341]\n",
      "loss: 0.535093  [108800/175341]\n",
      "loss: 0.180990  [110400/175341]\n",
      "loss: 0.650556  [112000/175341]\n",
      "loss: 0.222134  [113600/175341]\n",
      "loss: 0.775619  [115200/175341]\n",
      "loss: 0.837735  [116800/175341]\n",
      "loss: 0.488219  [118400/175341]\n",
      "loss: 0.829856  [120000/175341]\n",
      "loss: 0.681232  [121600/175341]\n",
      "loss: 0.841954  [123200/175341]\n",
      "loss: 0.311800  [124800/175341]\n",
      "loss: 0.420048  [126400/175341]\n",
      "loss: 1.057238  [128000/175341]\n",
      "loss: 0.489460  [129600/175341]\n",
      "loss: 0.332091  [131200/175341]\n",
      "loss: 0.224351  [132800/175341]\n",
      "loss: 0.208882  [134400/175341]\n",
      "loss: 0.812761  [136000/175341]\n",
      "loss: 0.490084  [137600/175341]\n",
      "loss: 0.487089  [139200/175341]\n",
      "loss: 0.239662  [140800/175341]\n",
      "loss: 0.215535  [142400/175341]\n",
      "loss: 0.253609  [144000/175341]\n",
      "loss: 0.435706  [145600/175341]\n",
      "loss: 1.106063  [147200/175341]\n",
      "loss: 0.198161  [148800/175341]\n",
      "loss: 0.440880  [150400/175341]\n",
      "loss: 0.671458  [152000/175341]\n",
      "loss: 0.506216  [153600/175341]\n",
      "loss: 0.468585  [155200/175341]\n",
      "loss: 0.525087  [156800/175341]\n",
      "loss: 0.538693  [158400/175341]\n",
      "loss: 0.188686  [160000/175341]\n",
      "loss: 0.706263  [161600/175341]\n",
      "loss: 0.061828  [163200/175341]\n",
      "loss: 0.440169  [164800/175341]\n",
      "loss: 0.741422  [166400/175341]\n",
      "loss: 0.492740  [168000/175341]\n",
      "loss: 0.197642  [169600/175341]\n",
      "loss: 0.608011  [171200/175341]\n",
      "loss: 0.247892  [172800/175341]\n",
      "loss: 0.095759  [174400/175341]\n",
      "Train Accuracy: 81.8582%\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.574138, F1-score: 74.92%, Macro_F1-Score:  41.33%  \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.540497  [    0/175341]\n",
      "loss: 0.302482  [ 1600/175341]\n",
      "loss: 0.357424  [ 3200/175341]\n",
      "loss: 0.466660  [ 4800/175341]\n",
      "loss: 0.606848  [ 6400/175341]\n",
      "loss: 0.674831  [ 8000/175341]\n",
      "loss: 0.356326  [ 9600/175341]\n",
      "loss: 0.270518  [11200/175341]\n",
      "loss: 0.481666  [12800/175341]\n",
      "loss: 0.430552  [14400/175341]\n",
      "loss: 0.394102  [16000/175341]\n",
      "loss: 0.304664  [17600/175341]\n",
      "loss: 0.209119  [19200/175341]\n",
      "loss: 0.939183  [20800/175341]\n",
      "loss: 0.330097  [22400/175341]\n",
      "loss: 0.520891  [24000/175341]\n",
      "loss: 0.685449  [25600/175341]\n",
      "loss: 0.314857  [27200/175341]\n",
      "loss: 0.181553  [28800/175341]\n",
      "loss: 0.412368  [30400/175341]\n",
      "loss: 0.370467  [32000/175341]\n",
      "loss: 0.592723  [33600/175341]\n",
      "loss: 0.235254  [35200/175341]\n",
      "loss: 0.312877  [36800/175341]\n",
      "loss: 0.249266  [38400/175341]\n",
      "loss: 0.764402  [40000/175341]\n",
      "loss: 0.439132  [41600/175341]\n",
      "loss: 0.617954  [43200/175341]\n",
      "loss: 0.435880  [44800/175341]\n",
      "loss: 0.486729  [46400/175341]\n",
      "loss: 0.551199  [48000/175341]\n",
      "loss: 0.475300  [49600/175341]\n",
      "loss: 0.542125  [51200/175341]\n",
      "loss: 0.842463  [52800/175341]\n",
      "loss: 0.386465  [54400/175341]\n",
      "loss: 0.432223  [56000/175341]\n",
      "loss: 0.262662  [57600/175341]\n",
      "loss: 0.209846  [59200/175341]\n",
      "loss: 0.264591  [60800/175341]\n",
      "loss: 0.354777  [62400/175341]\n",
      "loss: 0.627026  [64000/175341]\n",
      "loss: 0.412001  [65600/175341]\n",
      "loss: 0.582791  [67200/175341]\n",
      "loss: 0.454459  [68800/175341]\n",
      "loss: 0.469218  [70400/175341]\n",
      "loss: 0.200780  [72000/175341]\n",
      "loss: 0.930774  [73600/175341]\n",
      "loss: 0.391091  [75200/175341]\n",
      "loss: 0.374452  [76800/175341]\n",
      "loss: 0.765476  [78400/175341]\n",
      "loss: 0.098600  [80000/175341]\n",
      "loss: 0.301043  [81600/175341]\n",
      "loss: 0.240537  [83200/175341]\n",
      "loss: 0.286988  [84800/175341]\n",
      "loss: 0.366114  [86400/175341]\n",
      "loss: 0.431312  [88000/175341]\n",
      "loss: 0.633880  [89600/175341]\n",
      "loss: 0.344574  [91200/175341]\n",
      "loss: 0.407720  [92800/175341]\n",
      "loss: 0.377563  [94400/175341]\n",
      "loss: 0.354932  [96000/175341]\n",
      "loss: 0.739542  [97600/175341]\n",
      "loss: 0.378410  [99200/175341]\n",
      "loss: 0.883943  [100800/175341]\n",
      "loss: 0.477746  [102400/175341]\n",
      "loss: 0.304401  [104000/175341]\n",
      "loss: 0.456870  [105600/175341]\n",
      "loss: 0.451927  [107200/175341]\n",
      "loss: 0.408574  [108800/175341]\n",
      "loss: 0.949553  [110400/175341]\n",
      "loss: 0.532350  [112000/175341]\n",
      "loss: 0.477400  [113600/175341]\n",
      "loss: 0.506469  [115200/175341]\n",
      "loss: 0.239296  [116800/175341]\n",
      "loss: 0.496426  [118400/175341]\n",
      "loss: 0.356499  [120000/175341]\n",
      "loss: 0.444100  [121600/175341]\n",
      "loss: 0.793534  [123200/175341]\n",
      "loss: 0.546296  [124800/175341]\n",
      "loss: 0.402125  [126400/175341]\n",
      "loss: 0.321902  [128000/175341]\n",
      "loss: 0.549973  [129600/175341]\n",
      "loss: 0.165285  [131200/175341]\n",
      "loss: 0.224770  [132800/175341]\n",
      "loss: 0.381950  [134400/175341]\n",
      "loss: 0.286064  [136000/175341]\n",
      "loss: 0.751945  [137600/175341]\n",
      "loss: 0.336772  [139200/175341]\n",
      "loss: 0.788727  [140800/175341]\n",
      "loss: 0.730525  [142400/175341]\n",
      "loss: 0.369411  [144000/175341]\n",
      "loss: 0.505997  [145600/175341]\n",
      "loss: 0.537704  [147200/175341]\n",
      "loss: 0.351180  [148800/175341]\n",
      "loss: 0.799360  [150400/175341]\n",
      "loss: 0.360422  [152000/175341]\n",
      "loss: 0.573221  [153600/175341]\n",
      "loss: 0.534549  [155200/175341]\n",
      "loss: 0.428788  [156800/175341]\n",
      "loss: 0.463987  [158400/175341]\n",
      "loss: 0.580586  [160000/175341]\n",
      "loss: 0.337334  [161600/175341]\n",
      "loss: 0.443606  [163200/175341]\n",
      "loss: 0.549686  [164800/175341]\n",
      "loss: 0.217529  [166400/175341]\n",
      "loss: 0.319362  [168000/175341]\n",
      "loss: 0.178419  [169600/175341]\n",
      "loss: 0.372585  [171200/175341]\n",
      "loss: 0.325100  [172800/175341]\n",
      "loss: 0.515828  [174400/175341]\n",
      "Train Accuracy: 81.7738%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.532559, F1-score: 77.01%, Macro_F1-Score:  42.55%  \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.090620  [    0/175341]\n",
      "loss: 0.341220  [ 1600/175341]\n",
      "loss: 0.298613  [ 3200/175341]\n",
      "loss: 0.462336  [ 4800/175341]\n",
      "loss: 0.476955  [ 6400/175341]\n",
      "loss: 0.610955  [ 8000/175341]\n",
      "loss: 0.595825  [ 9600/175341]\n",
      "loss: 0.173817  [11200/175341]\n",
      "loss: 0.680593  [12800/175341]\n",
      "loss: 0.447175  [14400/175341]\n",
      "loss: 0.647135  [16000/175341]\n",
      "loss: 0.329217  [17600/175341]\n",
      "loss: 0.350331  [19200/175341]\n",
      "loss: 0.651394  [20800/175341]\n",
      "loss: 0.125657  [22400/175341]\n",
      "loss: 0.575923  [24000/175341]\n",
      "loss: 0.224298  [25600/175341]\n",
      "loss: 0.432597  [27200/175341]\n",
      "loss: 0.329658  [28800/175341]\n",
      "loss: 0.561127  [30400/175341]\n",
      "loss: 0.518826  [32000/175341]\n",
      "loss: 0.594938  [33600/175341]\n",
      "loss: 0.636148  [35200/175341]\n",
      "loss: 0.275043  [36800/175341]\n",
      "loss: 0.538048  [38400/175341]\n",
      "loss: 0.960316  [40000/175341]\n",
      "loss: 0.511702  [41600/175341]\n",
      "loss: 0.362176  [43200/175341]\n",
      "loss: 0.386347  [44800/175341]\n",
      "loss: 0.981226  [46400/175341]\n",
      "loss: 0.417792  [48000/175341]\n",
      "loss: 0.750735  [49600/175341]\n",
      "loss: 0.815203  [51200/175341]\n",
      "loss: 0.354140  [52800/175341]\n",
      "loss: 0.481426  [54400/175341]\n",
      "loss: 0.453455  [56000/175341]\n",
      "loss: 0.527413  [57600/175341]\n",
      "loss: 0.258348  [59200/175341]\n",
      "loss: 0.413650  [60800/175341]\n",
      "loss: 0.416495  [62400/175341]\n",
      "loss: 0.447705  [64000/175341]\n",
      "loss: 0.392217  [65600/175341]\n",
      "loss: 1.050980  [67200/175341]\n",
      "loss: 0.691726  [68800/175341]\n",
      "loss: 0.296324  [70400/175341]\n",
      "loss: 0.338151  [72000/175341]\n",
      "loss: 0.264401  [73600/175341]\n",
      "loss: 0.594709  [75200/175341]\n",
      "loss: 0.503290  [76800/175341]\n",
      "loss: 0.238579  [78400/175341]\n",
      "loss: 0.426697  [80000/175341]\n",
      "loss: 0.463972  [81600/175341]\n",
      "loss: 0.354099  [83200/175341]\n",
      "loss: 0.832834  [84800/175341]\n",
      "loss: 0.308934  [86400/175341]\n",
      "loss: 0.244519  [88000/175341]\n",
      "loss: 0.513110  [89600/175341]\n",
      "loss: 0.294671  [91200/175341]\n",
      "loss: 0.149468  [92800/175341]\n",
      "loss: 0.061734  [94400/175341]\n",
      "loss: 0.489173  [96000/175341]\n",
      "loss: 0.508214  [97600/175341]\n",
      "loss: 0.176670  [99200/175341]\n",
      "loss: 0.718768  [100800/175341]\n",
      "loss: 0.393576  [102400/175341]\n",
      "loss: 0.364479  [104000/175341]\n",
      "loss: 0.189498  [105600/175341]\n",
      "loss: 0.465616  [107200/175341]\n",
      "loss: 0.320339  [108800/175341]\n",
      "loss: 0.407201  [110400/175341]\n",
      "loss: 0.704942  [112000/175341]\n",
      "loss: 0.331240  [113600/175341]\n",
      "loss: 0.579943  [115200/175341]\n",
      "loss: 0.367776  [116800/175341]\n",
      "loss: 0.207314  [118400/175341]\n",
      "loss: 0.685260  [120000/175341]\n",
      "loss: 0.283651  [121600/175341]\n",
      "loss: 0.436366  [123200/175341]\n",
      "loss: 0.273654  [124800/175341]\n",
      "loss: 0.605822  [126400/175341]\n",
      "loss: 0.175958  [128000/175341]\n",
      "loss: 0.257572  [129600/175341]\n",
      "loss: 0.182963  [131200/175341]\n",
      "loss: 0.259955  [132800/175341]\n",
      "loss: 0.661432  [134400/175341]\n",
      "loss: 0.169783  [136000/175341]\n",
      "loss: 0.700081  [137600/175341]\n",
      "loss: 1.251371  [139200/175341]\n",
      "loss: 0.522921  [140800/175341]\n",
      "loss: 0.409017  [142400/175341]\n",
      "loss: 0.487639  [144000/175341]\n",
      "loss: 0.337682  [145600/175341]\n",
      "loss: 0.503199  [147200/175341]\n",
      "loss: 0.325781  [148800/175341]\n",
      "loss: 0.605247  [150400/175341]\n",
      "loss: 0.432872  [152000/175341]\n",
      "loss: 0.237831  [153600/175341]\n",
      "loss: 1.017307  [155200/175341]\n",
      "loss: 0.457490  [156800/175341]\n",
      "loss: 0.274458  [158400/175341]\n",
      "loss: 0.809524  [160000/175341]\n",
      "loss: 0.491517  [161600/175341]\n",
      "loss: 0.204678  [163200/175341]\n",
      "loss: 0.363576  [164800/175341]\n",
      "loss: 0.479097  [166400/175341]\n",
      "loss: 0.345415  [168000/175341]\n",
      "loss: 0.597589  [169600/175341]\n",
      "loss: 0.452565  [171200/175341]\n",
      "loss: 0.152558  [172800/175341]\n",
      "loss: 0.275207  [174400/175341]\n",
      "Train Accuracy: 81.8502%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.549076, F1-score: 76.16%, Macro_F1-Score:  41.34%  \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.588218  [    0/175341]\n",
      "loss: 0.398154  [ 1600/175341]\n",
      "loss: 0.496243  [ 3200/175341]\n",
      "loss: 0.600920  [ 4800/175341]\n",
      "loss: 0.234668  [ 6400/175341]\n",
      "loss: 0.466664  [ 8000/175341]\n",
      "loss: 0.184338  [ 9600/175341]\n",
      "loss: 0.334047  [11200/175341]\n",
      "loss: 0.492964  [12800/175341]\n",
      "loss: 0.286653  [14400/175341]\n",
      "loss: 0.784551  [16000/175341]\n",
      "loss: 0.562669  [17600/175341]\n",
      "loss: 0.473697  [19200/175341]\n",
      "loss: 0.313656  [20800/175341]\n",
      "loss: 0.341660  [22400/175341]\n",
      "loss: 0.176912  [24000/175341]\n",
      "loss: 0.342146  [25600/175341]\n",
      "loss: 0.671234  [27200/175341]\n",
      "loss: 0.852832  [28800/175341]\n",
      "loss: 0.324116  [30400/175341]\n",
      "loss: 0.325720  [32000/175341]\n",
      "loss: 0.125705  [33600/175341]\n",
      "loss: 0.522284  [35200/175341]\n",
      "loss: 0.563574  [36800/175341]\n",
      "loss: 0.806302  [38400/175341]\n",
      "loss: 0.242901  [40000/175341]\n",
      "loss: 0.378575  [41600/175341]\n",
      "loss: 0.428592  [43200/175341]\n",
      "loss: 0.469311  [44800/175341]\n",
      "loss: 0.477542  [46400/175341]\n",
      "loss: 0.609246  [48000/175341]\n",
      "loss: 0.654167  [49600/175341]\n",
      "loss: 0.375541  [51200/175341]\n",
      "loss: 0.837588  [52800/175341]\n",
      "loss: 0.593611  [54400/175341]\n",
      "loss: 0.449262  [56000/175341]\n",
      "loss: 0.368744  [57600/175341]\n",
      "loss: 0.105863  [59200/175341]\n",
      "loss: 0.383145  [60800/175341]\n",
      "loss: 0.189975  [62400/175341]\n",
      "loss: 0.220196  [64000/175341]\n",
      "loss: 0.995728  [65600/175341]\n",
      "loss: 0.528013  [67200/175341]\n",
      "loss: 0.223310  [68800/175341]\n",
      "loss: 0.189463  [70400/175341]\n",
      "loss: 0.360255  [72000/175341]\n",
      "loss: 0.634554  [73600/175341]\n",
      "loss: 0.792919  [75200/175341]\n",
      "loss: 0.398379  [76800/175341]\n",
      "loss: 0.530595  [78400/175341]\n",
      "loss: 0.419667  [80000/175341]\n",
      "loss: 0.405154  [81600/175341]\n",
      "loss: 0.619352  [83200/175341]\n",
      "loss: 0.536704  [84800/175341]\n",
      "loss: 0.506874  [86400/175341]\n",
      "loss: 0.706175  [88000/175341]\n",
      "loss: 0.605717  [89600/175341]\n",
      "loss: 0.568714  [91200/175341]\n",
      "loss: 0.742443  [92800/175341]\n",
      "loss: 0.513238  [94400/175341]\n",
      "loss: 0.636495  [96000/175341]\n",
      "loss: 0.394854  [97600/175341]\n",
      "loss: 0.302795  [99200/175341]\n",
      "loss: 0.847285  [100800/175341]\n",
      "loss: 0.523984  [102400/175341]\n",
      "loss: 0.250983  [104000/175341]\n",
      "loss: 0.835592  [105600/175341]\n",
      "loss: 0.580206  [107200/175341]\n",
      "loss: 0.513331  [108800/175341]\n",
      "loss: 0.673513  [110400/175341]\n",
      "loss: 0.522077  [112000/175341]\n",
      "loss: 0.260264  [113600/175341]\n",
      "loss: 0.190234  [115200/175341]\n",
      "loss: 0.492356  [116800/175341]\n",
      "loss: 0.619761  [118400/175341]\n",
      "loss: 0.418194  [120000/175341]\n",
      "loss: 0.599607  [121600/175341]\n",
      "loss: 0.553689  [123200/175341]\n",
      "loss: 0.591404  [124800/175341]\n",
      "loss: 0.368419  [126400/175341]\n",
      "loss: 0.356578  [128000/175341]\n",
      "loss: 0.094262  [129600/175341]\n",
      "loss: 0.217825  [131200/175341]\n",
      "loss: 0.372010  [132800/175341]\n",
      "loss: 0.483933  [134400/175341]\n",
      "loss: 0.724087  [136000/175341]\n",
      "loss: 0.361221  [137600/175341]\n",
      "loss: 0.731625  [139200/175341]\n",
      "loss: 0.159901  [140800/175341]\n",
      "loss: 0.469219  [142400/175341]\n",
      "loss: 0.469116  [144000/175341]\n",
      "loss: 0.313945  [145600/175341]\n",
      "loss: 0.180558  [147200/175341]\n",
      "loss: 0.131649  [148800/175341]\n",
      "loss: 0.453113  [150400/175341]\n",
      "loss: 0.699286  [152000/175341]\n",
      "loss: 0.450289  [153600/175341]\n",
      "loss: 0.641325  [155200/175341]\n",
      "loss: 0.461653  [156800/175341]\n",
      "loss: 0.384667  [158400/175341]\n",
      "loss: 0.169777  [160000/175341]\n",
      "loss: 0.337847  [161600/175341]\n",
      "loss: 0.423154  [163200/175341]\n",
      "loss: 1.095695  [164800/175341]\n",
      "loss: 0.456470  [166400/175341]\n",
      "loss: 0.644514  [168000/175341]\n",
      "loss: 0.298274  [169600/175341]\n",
      "loss: 0.442252  [171200/175341]\n",
      "loss: 0.443087  [172800/175341]\n",
      "loss: 0.390119  [174400/175341]\n",
      "Train Accuracy: 81.8211%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.536528, F1-score: 77.75%, Macro_F1-Score:  43.77%  \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.525341  [    0/175341]\n",
      "loss: 0.839192  [ 1600/175341]\n",
      "loss: 0.324739  [ 3200/175341]\n",
      "loss: 0.643762  [ 4800/175341]\n",
      "loss: 0.836707  [ 6400/175341]\n",
      "loss: 0.454486  [ 8000/175341]\n",
      "loss: 0.469106  [ 9600/175341]\n",
      "loss: 0.572674  [11200/175341]\n",
      "loss: 0.583017  [12800/175341]\n",
      "loss: 0.445766  [14400/175341]\n",
      "loss: 0.185828  [16000/175341]\n",
      "loss: 0.430040  [17600/175341]\n",
      "loss: 0.473010  [19200/175341]\n",
      "loss: 0.405761  [20800/175341]\n",
      "loss: 0.869296  [22400/175341]\n",
      "loss: 0.321087  [24000/175341]\n",
      "loss: 0.373121  [25600/175341]\n",
      "loss: 0.958999  [27200/175341]\n",
      "loss: 0.673930  [28800/175341]\n",
      "loss: 1.055789  [30400/175341]\n",
      "loss: 0.257735  [32000/175341]\n",
      "loss: 0.506762  [33600/175341]\n",
      "loss: 0.380292  [35200/175341]\n",
      "loss: 0.160440  [36800/175341]\n",
      "loss: 0.514576  [38400/175341]\n",
      "loss: 0.501425  [40000/175341]\n",
      "loss: 0.357139  [41600/175341]\n",
      "loss: 0.466257  [43200/175341]\n",
      "loss: 0.312417  [44800/175341]\n",
      "loss: 0.292023  [46400/175341]\n",
      "loss: 0.562120  [48000/175341]\n",
      "loss: 0.165172  [49600/175341]\n",
      "loss: 0.515511  [51200/175341]\n",
      "loss: 0.914001  [52800/175341]\n",
      "loss: 0.685734  [54400/175341]\n",
      "loss: 0.660109  [56000/175341]\n",
      "loss: 0.421293  [57600/175341]\n",
      "loss: 0.519542  [59200/175341]\n",
      "loss: 0.382885  [60800/175341]\n",
      "loss: 0.308483  [62400/175341]\n",
      "loss: 0.288903  [64000/175341]\n",
      "loss: 0.408137  [65600/175341]\n",
      "loss: 0.292435  [67200/175341]\n",
      "loss: 0.485151  [68800/175341]\n",
      "loss: 0.249060  [70400/175341]\n",
      "loss: 0.453381  [72000/175341]\n",
      "loss: 0.459367  [73600/175341]\n",
      "loss: 0.647614  [75200/175341]\n",
      "loss: 0.335050  [76800/175341]\n",
      "loss: 0.446354  [78400/175341]\n",
      "loss: 0.265769  [80000/175341]\n",
      "loss: 0.339941  [81600/175341]\n",
      "loss: 0.437737  [83200/175341]\n",
      "loss: 0.559096  [84800/175341]\n",
      "loss: 0.427076  [86400/175341]\n",
      "loss: 0.493557  [88000/175341]\n",
      "loss: 0.529680  [89600/175341]\n",
      "loss: 0.314001  [91200/175341]\n",
      "loss: 0.147666  [92800/175341]\n",
      "loss: 0.783991  [94400/175341]\n",
      "loss: 0.541563  [96000/175341]\n",
      "loss: 0.520670  [97600/175341]\n",
      "loss: 0.351383  [99200/175341]\n",
      "loss: 0.693413  [100800/175341]\n",
      "loss: 0.429857  [102400/175341]\n",
      "loss: 0.546711  [104000/175341]\n",
      "loss: 0.358941  [105600/175341]\n",
      "loss: 0.717813  [107200/175341]\n",
      "loss: 0.541526  [108800/175341]\n",
      "loss: 0.489431  [110400/175341]\n",
      "loss: 0.147172  [112000/175341]\n",
      "loss: 0.773495  [113600/175341]\n",
      "loss: 0.231978  [115200/175341]\n",
      "loss: 0.132511  [116800/175341]\n",
      "loss: 0.636072  [118400/175341]\n",
      "loss: 0.394448  [120000/175341]\n",
      "loss: 0.604501  [121600/175341]\n",
      "loss: 0.434790  [123200/175341]\n",
      "loss: 0.657483  [124800/175341]\n",
      "loss: 0.498803  [126400/175341]\n",
      "loss: 0.453000  [128000/175341]\n",
      "loss: 0.598702  [129600/175341]\n",
      "loss: 0.409531  [131200/175341]\n",
      "loss: 0.455451  [132800/175341]\n",
      "loss: 0.464288  [134400/175341]\n",
      "loss: 0.453891  [136000/175341]\n",
      "loss: 0.646777  [137600/175341]\n",
      "loss: 0.293231  [139200/175341]\n",
      "loss: 0.470063  [140800/175341]\n",
      "loss: 0.502125  [142400/175341]\n",
      "loss: 0.583119  [144000/175341]\n",
      "loss: 0.575337  [145600/175341]\n",
      "loss: 0.379476  [147200/175341]\n",
      "loss: 0.478789  [148800/175341]\n",
      "loss: 0.390913  [150400/175341]\n",
      "loss: 0.223306  [152000/175341]\n",
      "loss: 0.239607  [153600/175341]\n",
      "loss: 0.497608  [155200/175341]\n",
      "loss: 0.477945  [156800/175341]\n",
      "loss: 0.287451  [158400/175341]\n",
      "loss: 0.836834  [160000/175341]\n",
      "loss: 0.860370  [161600/175341]\n",
      "loss: 0.382755  [163200/175341]\n",
      "loss: 0.431876  [164800/175341]\n",
      "loss: 0.184691  [166400/175341]\n",
      "loss: 0.566174  [168000/175341]\n",
      "loss: 0.580538  [169600/175341]\n",
      "loss: 0.208068  [171200/175341]\n",
      "loss: 0.338098  [172800/175341]\n",
      "loss: 0.374471  [174400/175341]\n",
      "Train Accuracy: 81.8286%\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.527532, F1-score: 78.14%, Macro_F1-Score:  43.06%  \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.340651  [    0/175341]\n",
      "loss: 0.400600  [ 1600/175341]\n",
      "loss: 0.422741  [ 3200/175341]\n",
      "loss: 0.511147  [ 4800/175341]\n",
      "loss: 0.330323  [ 6400/175341]\n",
      "loss: 0.417181  [ 8000/175341]\n",
      "loss: 0.339145  [ 9600/175341]\n",
      "loss: 0.225982  [11200/175341]\n",
      "loss: 0.794291  [12800/175341]\n",
      "loss: 0.234663  [14400/175341]\n",
      "loss: 0.825186  [16000/175341]\n",
      "loss: 0.441501  [17600/175341]\n",
      "loss: 0.446415  [19200/175341]\n",
      "loss: 0.673501  [20800/175341]\n",
      "loss: 0.326487  [22400/175341]\n",
      "loss: 0.439125  [24000/175341]\n",
      "loss: 0.393882  [25600/175341]\n",
      "loss: 0.271013  [27200/175341]\n",
      "loss: 0.335306  [28800/175341]\n",
      "loss: 0.431499  [30400/175341]\n",
      "loss: 0.518573  [32000/175341]\n",
      "loss: 0.393493  [33600/175341]\n",
      "loss: 0.288092  [35200/175341]\n",
      "loss: 0.343621  [36800/175341]\n",
      "loss: 0.481821  [38400/175341]\n",
      "loss: 0.801622  [40000/175341]\n",
      "loss: 0.443004  [41600/175341]\n",
      "loss: 0.320939  [43200/175341]\n",
      "loss: 0.440704  [44800/175341]\n",
      "loss: 0.480510  [46400/175341]\n",
      "loss: 0.509225  [48000/175341]\n",
      "loss: 0.538829  [49600/175341]\n",
      "loss: 0.144747  [51200/175341]\n",
      "loss: 0.568631  [52800/175341]\n",
      "loss: 0.753432  [54400/175341]\n",
      "loss: 0.415606  [56000/175341]\n",
      "loss: 0.281528  [57600/175341]\n",
      "loss: 0.577632  [59200/175341]\n",
      "loss: 0.336444  [60800/175341]\n",
      "loss: 0.472516  [62400/175341]\n",
      "loss: 0.450308  [64000/175341]\n",
      "loss: 0.199397  [65600/175341]\n",
      "loss: 0.591353  [67200/175341]\n",
      "loss: 0.412131  [68800/175341]\n",
      "loss: 0.397876  [70400/175341]\n",
      "loss: 0.760892  [72000/175341]\n",
      "loss: 0.571693  [73600/175341]\n",
      "loss: 0.503248  [75200/175341]\n",
      "loss: 0.545551  [76800/175341]\n",
      "loss: 0.116816  [78400/175341]\n",
      "loss: 0.108683  [80000/175341]\n",
      "loss: 0.345069  [81600/175341]\n",
      "loss: 0.485358  [83200/175341]\n",
      "loss: 0.779055  [84800/175341]\n",
      "loss: 0.951368  [86400/175341]\n",
      "loss: 0.358266  [88000/175341]\n",
      "loss: 0.459479  [89600/175341]\n",
      "loss: 0.122183  [91200/175341]\n",
      "loss: 0.355668  [92800/175341]\n",
      "loss: 0.576016  [94400/175341]\n",
      "loss: 0.489093  [96000/175341]\n",
      "loss: 0.526628  [97600/175341]\n",
      "loss: 0.133419  [99200/175341]\n",
      "loss: 0.360135  [100800/175341]\n",
      "loss: 0.212964  [102400/175341]\n",
      "loss: 0.413223  [104000/175341]\n",
      "loss: 0.439637  [105600/175341]\n",
      "loss: 0.278298  [107200/175341]\n",
      "loss: 0.060898  [108800/175341]\n",
      "loss: 0.444688  [110400/175341]\n",
      "loss: 0.364343  [112000/175341]\n",
      "loss: 0.279840  [113600/175341]\n",
      "loss: 0.263590  [115200/175341]\n",
      "loss: 0.286793  [116800/175341]\n",
      "loss: 0.458874  [118400/175341]\n",
      "loss: 0.456807  [120000/175341]\n",
      "loss: 0.528584  [121600/175341]\n",
      "loss: 0.540478  [123200/175341]\n",
      "loss: 0.251330  [124800/175341]\n",
      "loss: 0.287808  [126400/175341]\n",
      "loss: 0.436423  [128000/175341]\n",
      "loss: 0.404431  [129600/175341]\n",
      "loss: 0.272148  [131200/175341]\n",
      "loss: 0.551132  [132800/175341]\n",
      "loss: 0.614068  [134400/175341]\n",
      "loss: 0.239860  [136000/175341]\n",
      "loss: 0.511638  [137600/175341]\n",
      "loss: 0.445767  [139200/175341]\n",
      "loss: 0.931689  [140800/175341]\n",
      "loss: 0.378724  [142400/175341]\n",
      "loss: 0.375348  [144000/175341]\n",
      "loss: 0.738307  [145600/175341]\n",
      "loss: 0.615425  [147200/175341]\n",
      "loss: 0.153936  [148800/175341]\n",
      "loss: 0.357178  [150400/175341]\n",
      "loss: 0.814710  [152000/175341]\n",
      "loss: 0.607230  [153600/175341]\n",
      "loss: 0.467806  [155200/175341]\n",
      "loss: 0.819060  [156800/175341]\n",
      "loss: 0.622855  [158400/175341]\n",
      "loss: 0.380914  [160000/175341]\n",
      "loss: 0.496801  [161600/175341]\n",
      "loss: 0.374378  [163200/175341]\n",
      "loss: 0.875217  [164800/175341]\n",
      "loss: 0.285108  [166400/175341]\n",
      "loss: 0.752999  [168000/175341]\n",
      "loss: 0.143139  [169600/175341]\n",
      "loss: 0.640801  [171200/175341]\n",
      "loss: 0.231200  [172800/175341]\n",
      "loss: 0.534254  [174400/175341]\n",
      "Train Accuracy: 81.8109%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.542175, F1-score: 76.29%, Macro_F1-Score:  42.18%  \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.282489  [    0/175341]\n",
      "loss: 0.757270  [ 1600/175341]\n",
      "loss: 0.474954  [ 3200/175341]\n",
      "loss: 0.185006  [ 4800/175341]\n",
      "loss: 0.548172  [ 6400/175341]\n",
      "loss: 0.470769  [ 8000/175341]\n",
      "loss: 0.352526  [ 9600/175341]\n",
      "loss: 0.421796  [11200/175341]\n",
      "loss: 0.241913  [12800/175341]\n",
      "loss: 0.172267  [14400/175341]\n",
      "loss: 0.604598  [16000/175341]\n",
      "loss: 0.616070  [17600/175341]\n",
      "loss: 0.957794  [19200/175341]\n",
      "loss: 0.474346  [20800/175341]\n",
      "loss: 0.502526  [22400/175341]\n",
      "loss: 0.355994  [24000/175341]\n",
      "loss: 0.409165  [25600/175341]\n",
      "loss: 0.673392  [27200/175341]\n",
      "loss: 0.849123  [28800/175341]\n",
      "loss: 0.371014  [30400/175341]\n",
      "loss: 0.188132  [32000/175341]\n",
      "loss: 0.412648  [33600/175341]\n",
      "loss: 0.661789  [35200/175341]\n",
      "loss: 0.402664  [36800/175341]\n",
      "loss: 0.170635  [38400/175341]\n",
      "loss: 0.399355  [40000/175341]\n",
      "loss: 0.369114  [41600/175341]\n",
      "loss: 0.615740  [43200/175341]\n",
      "loss: 0.441918  [44800/175341]\n",
      "loss: 0.496972  [46400/175341]\n",
      "loss: 0.260720  [48000/175341]\n",
      "loss: 0.835774  [49600/175341]\n",
      "loss: 0.204489  [51200/175341]\n",
      "loss: 0.191206  [52800/175341]\n",
      "loss: 0.513336  [54400/175341]\n",
      "loss: 1.033445  [56000/175341]\n",
      "loss: 0.465253  [57600/175341]\n",
      "loss: 0.285856  [59200/175341]\n",
      "loss: 0.366340  [60800/175341]\n",
      "loss: 0.405877  [62400/175341]\n",
      "loss: 0.401799  [64000/175341]\n",
      "loss: 0.243677  [65600/175341]\n",
      "loss: 0.246932  [67200/175341]\n",
      "loss: 0.472036  [68800/175341]\n",
      "loss: 0.742234  [70400/175341]\n",
      "loss: 0.420347  [72000/175341]\n",
      "loss: 0.861400  [73600/175341]\n",
      "loss: 0.232440  [75200/175341]\n",
      "loss: 0.651702  [76800/175341]\n",
      "loss: 0.188382  [78400/175341]\n",
      "loss: 0.469433  [80000/175341]\n",
      "loss: 0.392722  [81600/175341]\n",
      "loss: 0.634223  [83200/175341]\n",
      "loss: 0.167865  [84800/175341]\n",
      "loss: 0.430014  [86400/175341]\n",
      "loss: 0.352704  [88000/175341]\n",
      "loss: 0.663518  [89600/175341]\n",
      "loss: 0.503820  [91200/175341]\n",
      "loss: 0.459415  [92800/175341]\n",
      "loss: 0.221710  [94400/175341]\n",
      "loss: 0.519259  [96000/175341]\n",
      "loss: 0.338973  [97600/175341]\n",
      "loss: 0.478272  [99200/175341]\n",
      "loss: 0.579465  [100800/175341]\n",
      "loss: 0.380201  [102400/175341]\n",
      "loss: 0.882509  [104000/175341]\n",
      "loss: 0.541653  [105600/175341]\n",
      "loss: 0.671615  [107200/175341]\n",
      "loss: 0.642022  [108800/175341]\n",
      "loss: 0.695698  [110400/175341]\n",
      "loss: 0.671908  [112000/175341]\n",
      "loss: 1.601362  [113600/175341]\n",
      "loss: 0.312941  [115200/175341]\n",
      "loss: 0.338349  [116800/175341]\n",
      "loss: 0.282444  [118400/175341]\n",
      "loss: 0.400275  [120000/175341]\n",
      "loss: 0.142686  [121600/175341]\n",
      "loss: 0.481677  [123200/175341]\n",
      "loss: 0.238260  [124800/175341]\n",
      "loss: 0.543681  [126400/175341]\n",
      "loss: 0.557166  [128000/175341]\n",
      "loss: 0.547183  [129600/175341]\n",
      "loss: 0.372917  [131200/175341]\n",
      "loss: 0.849745  [132800/175341]\n",
      "loss: 0.335275  [134400/175341]\n",
      "loss: 0.123860  [136000/175341]\n",
      "loss: 0.894102  [137600/175341]\n",
      "loss: 0.163999  [139200/175341]\n",
      "loss: 0.346173  [140800/175341]\n",
      "loss: 0.352151  [142400/175341]\n",
      "loss: 0.518948  [144000/175341]\n",
      "loss: 0.667168  [145600/175341]\n",
      "loss: 0.436980  [147200/175341]\n",
      "loss: 0.337226  [148800/175341]\n",
      "loss: 0.325812  [150400/175341]\n",
      "loss: 0.456695  [152000/175341]\n",
      "loss: 0.844497  [153600/175341]\n",
      "loss: 0.757536  [155200/175341]\n",
      "loss: 0.242578  [156800/175341]\n",
      "loss: 1.069170  [158400/175341]\n",
      "loss: 0.206573  [160000/175341]\n",
      "loss: 0.305718  [161600/175341]\n",
      "loss: 0.209894  [163200/175341]\n",
      "loss: 0.513587  [164800/175341]\n",
      "loss: 0.401083  [166400/175341]\n",
      "loss: 0.504250  [168000/175341]\n",
      "loss: 0.123128  [169600/175341]\n",
      "loss: 0.361346  [171200/175341]\n",
      "loss: 0.135290  [172800/175341]\n",
      "loss: 0.544488  [174400/175341]\n",
      "Train Accuracy: 81.8571%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.537412, F1-score: 76.63%, Macro_F1-Score:  41.97%  \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.184576  [    0/175341]\n",
      "loss: 0.470580  [ 1600/175341]\n",
      "loss: 0.228505  [ 3200/175341]\n",
      "loss: 0.824024  [ 4800/175341]\n",
      "loss: 0.478554  [ 6400/175341]\n",
      "loss: 0.542619  [ 8000/175341]\n",
      "loss: 0.303537  [ 9600/175341]\n",
      "loss: 0.085369  [11200/175341]\n",
      "loss: 0.187499  [12800/175341]\n",
      "loss: 0.329861  [14400/175341]\n",
      "loss: 0.318208  [16000/175341]\n",
      "loss: 0.436780  [17600/175341]\n",
      "loss: 0.346043  [19200/175341]\n",
      "loss: 0.369913  [20800/175341]\n",
      "loss: 0.561098  [22400/175341]\n",
      "loss: 0.444529  [24000/175341]\n",
      "loss: 0.171394  [25600/175341]\n",
      "loss: 0.330084  [27200/175341]\n",
      "loss: 0.293461  [28800/175341]\n",
      "loss: 0.693413  [30400/175341]\n",
      "loss: 0.534134  [32000/175341]\n",
      "loss: 0.291561  [33600/175341]\n",
      "loss: 0.413051  [35200/175341]\n",
      "loss: 0.514435  [36800/175341]\n",
      "loss: 0.352429  [38400/175341]\n",
      "loss: 0.377806  [40000/175341]\n",
      "loss: 0.592227  [41600/175341]\n",
      "loss: 0.547595  [43200/175341]\n",
      "loss: 0.201729  [44800/175341]\n",
      "loss: 0.200483  [46400/175341]\n",
      "loss: 0.533399  [48000/175341]\n",
      "loss: 0.215920  [49600/175341]\n",
      "loss: 0.353551  [51200/175341]\n",
      "loss: 0.435690  [52800/175341]\n",
      "loss: 0.625038  [54400/175341]\n",
      "loss: 0.362936  [56000/175341]\n",
      "loss: 0.721874  [57600/175341]\n",
      "loss: 0.443042  [59200/175341]\n",
      "loss: 0.338617  [60800/175341]\n",
      "loss: 0.211828  [62400/175341]\n",
      "loss: 0.755582  [64000/175341]\n",
      "loss: 0.540667  [65600/175341]\n",
      "loss: 0.606010  [67200/175341]\n",
      "loss: 0.578451  [68800/175341]\n",
      "loss: 0.602741  [70400/175341]\n",
      "loss: 0.401178  [72000/175341]\n",
      "loss: 0.983458  [73600/175341]\n",
      "loss: 0.301807  [75200/175341]\n",
      "loss: 0.077392  [76800/175341]\n",
      "loss: 0.177886  [78400/175341]\n",
      "loss: 0.276594  [80000/175341]\n",
      "loss: 0.337323  [81600/175341]\n",
      "loss: 0.514163  [83200/175341]\n",
      "loss: 0.533696  [84800/175341]\n",
      "loss: 0.563031  [86400/175341]\n",
      "loss: 0.116822  [88000/175341]\n",
      "loss: 0.393538  [89600/175341]\n",
      "loss: 0.433351  [91200/175341]\n",
      "loss: 0.469155  [92800/175341]\n",
      "loss: 0.475248  [94400/175341]\n",
      "loss: 0.673384  [96000/175341]\n",
      "loss: 0.422295  [97600/175341]\n",
      "loss: 0.586955  [99200/175341]\n",
      "loss: 0.361389  [100800/175341]\n",
      "loss: 0.446557  [102400/175341]\n",
      "loss: 0.409130  [104000/175341]\n",
      "loss: 0.287387  [105600/175341]\n",
      "loss: 0.554860  [107200/175341]\n",
      "loss: 0.573339  [108800/175341]\n",
      "loss: 0.347955  [110400/175341]\n",
      "loss: 0.399536  [112000/175341]\n",
      "loss: 0.517245  [113600/175341]\n",
      "loss: 0.430384  [115200/175341]\n",
      "loss: 0.573025  [116800/175341]\n",
      "loss: 0.285079  [118400/175341]\n",
      "loss: 0.528660  [120000/175341]\n",
      "loss: 0.530294  [121600/175341]\n",
      "loss: 0.541903  [123200/175341]\n",
      "loss: 0.124462  [124800/175341]\n",
      "loss: 0.221252  [126400/175341]\n",
      "loss: 0.337763  [128000/175341]\n",
      "loss: 0.349430  [129600/175341]\n",
      "loss: 0.644644  [131200/175341]\n",
      "loss: 0.327321  [132800/175341]\n",
      "loss: 0.453566  [134400/175341]\n",
      "loss: 0.328416  [136000/175341]\n",
      "loss: 0.398504  [137600/175341]\n",
      "loss: 0.379453  [139200/175341]\n",
      "loss: 0.420178  [140800/175341]\n",
      "loss: 0.710093  [142400/175341]\n",
      "loss: 0.443718  [144000/175341]\n",
      "loss: 0.450430  [145600/175341]\n",
      "loss: 0.314336  [147200/175341]\n",
      "loss: 0.734629  [148800/175341]\n",
      "loss: 0.186447  [150400/175341]\n",
      "loss: 0.591505  [152000/175341]\n",
      "loss: 0.645418  [153600/175341]\n",
      "loss: 0.456782  [155200/175341]\n",
      "loss: 0.538493  [156800/175341]\n",
      "loss: 0.366259  [158400/175341]\n",
      "loss: 0.889016  [160000/175341]\n",
      "loss: 0.533669  [161600/175341]\n",
      "loss: 0.407723  [163200/175341]\n",
      "loss: 0.219377  [164800/175341]\n",
      "loss: 0.893669  [166400/175341]\n",
      "loss: 0.532713  [168000/175341]\n",
      "loss: 0.530120  [169600/175341]\n",
      "loss: 0.389959  [171200/175341]\n",
      "loss: 0.404826  [172800/175341]\n",
      "loss: 0.255056  [174400/175341]\n",
      "Train Accuracy: 81.8057%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.540979, F1-score: 76.84%, Macro_F1-Score:  41.83%  \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.548730  [    0/175341]\n",
      "loss: 0.679705  [ 1600/175341]\n",
      "loss: 0.474014  [ 3200/175341]\n",
      "loss: 0.302501  [ 4800/175341]\n",
      "loss: 0.279021  [ 6400/175341]\n",
      "loss: 0.279044  [ 8000/175341]\n",
      "loss: 0.132639  [ 9600/175341]\n",
      "loss: 0.687404  [11200/175341]\n",
      "loss: 0.714641  [12800/175341]\n",
      "loss: 0.380250  [14400/175341]\n",
      "loss: 0.469249  [16000/175341]\n",
      "loss: 0.334058  [17600/175341]\n",
      "loss: 0.598966  [19200/175341]\n",
      "loss: 0.576434  [20800/175341]\n",
      "loss: 0.413667  [22400/175341]\n",
      "loss: 0.404872  [24000/175341]\n",
      "loss: 0.515822  [25600/175341]\n",
      "loss: 0.351015  [27200/175341]\n",
      "loss: 0.665354  [28800/175341]\n",
      "loss: 0.416408  [30400/175341]\n",
      "loss: 0.342419  [32000/175341]\n",
      "loss: 0.121432  [33600/175341]\n",
      "loss: 0.326384  [35200/175341]\n",
      "loss: 0.320253  [36800/175341]\n",
      "loss: 0.589594  [38400/175341]\n",
      "loss: 0.510495  [40000/175341]\n",
      "loss: 0.422394  [41600/175341]\n",
      "loss: 0.215351  [43200/175341]\n",
      "loss: 0.651906  [44800/175341]\n",
      "loss: 0.448403  [46400/175341]\n",
      "loss: 0.042289  [48000/175341]\n",
      "loss: 0.521049  [49600/175341]\n",
      "loss: 0.435350  [51200/175341]\n",
      "loss: 0.450601  [52800/175341]\n",
      "loss: 0.612643  [54400/175341]\n",
      "loss: 0.359440  [56000/175341]\n",
      "loss: 0.348247  [57600/175341]\n",
      "loss: 0.698816  [59200/175341]\n",
      "loss: 0.301536  [60800/175341]\n",
      "loss: 0.271398  [62400/175341]\n",
      "loss: 0.625692  [64000/175341]\n",
      "loss: 0.285670  [65600/175341]\n",
      "loss: 0.868438  [67200/175341]\n",
      "loss: 0.287057  [68800/175341]\n",
      "loss: 0.777717  [70400/175341]\n",
      "loss: 0.451276  [72000/175341]\n",
      "loss: 0.439643  [73600/175341]\n",
      "loss: 0.541162  [75200/175341]\n",
      "loss: 0.466481  [76800/175341]\n",
      "loss: 0.466079  [78400/175341]\n",
      "loss: 0.318626  [80000/175341]\n",
      "loss: 0.460835  [81600/175341]\n",
      "loss: 0.753527  [83200/175341]\n",
      "loss: 0.055921  [84800/175341]\n",
      "loss: 0.901759  [86400/175341]\n",
      "loss: 0.383161  [88000/175341]\n",
      "loss: 0.468610  [89600/175341]\n",
      "loss: 0.795667  [91200/175341]\n",
      "loss: 0.778840  [92800/175341]\n",
      "loss: 0.348974  [94400/175341]\n",
      "loss: 1.017713  [96000/175341]\n",
      "loss: 0.523427  [97600/175341]\n",
      "loss: 0.147880  [99200/175341]\n",
      "loss: 0.187622  [100800/175341]\n",
      "loss: 0.490288  [102400/175341]\n",
      "loss: 0.342444  [104000/175341]\n",
      "loss: 0.724694  [105600/175341]\n",
      "loss: 0.451794  [107200/175341]\n",
      "loss: 0.927450  [108800/175341]\n",
      "loss: 0.571687  [110400/175341]\n",
      "loss: 0.441936  [112000/175341]\n",
      "loss: 0.286940  [113600/175341]\n",
      "loss: 0.237122  [115200/175341]\n",
      "loss: 0.490146  [116800/175341]\n",
      "loss: 0.224485  [118400/175341]\n",
      "loss: 0.375921  [120000/175341]\n",
      "loss: 0.369726  [121600/175341]\n",
      "loss: 0.718471  [123200/175341]\n",
      "loss: 0.680312  [124800/175341]\n",
      "loss: 0.594606  [126400/175341]\n",
      "loss: 0.774324  [128000/175341]\n",
      "loss: 0.176220  [129600/175341]\n",
      "loss: 0.439962  [131200/175341]\n",
      "loss: 0.293631  [132800/175341]\n",
      "loss: 0.446283  [134400/175341]\n",
      "loss: 0.423810  [136000/175341]\n",
      "loss: 0.402757  [137600/175341]\n",
      "loss: 0.461661  [139200/175341]\n",
      "loss: 0.672413  [140800/175341]\n",
      "loss: 0.620632  [142400/175341]\n",
      "loss: 0.337772  [144000/175341]\n",
      "loss: 0.625406  [145600/175341]\n",
      "loss: 0.565509  [147200/175341]\n",
      "loss: 0.372526  [148800/175341]\n",
      "loss: 0.243376  [150400/175341]\n",
      "loss: 0.193372  [152000/175341]\n",
      "loss: 0.473496  [153600/175341]\n",
      "loss: 0.375176  [155200/175341]\n",
      "loss: 0.596187  [156800/175341]\n",
      "loss: 0.423984  [158400/175341]\n",
      "loss: 0.248982  [160000/175341]\n",
      "loss: 0.115908  [161600/175341]\n",
      "loss: 0.846480  [163200/175341]\n",
      "loss: 0.349682  [164800/175341]\n",
      "loss: 0.359551  [166400/175341]\n",
      "loss: 0.474777  [168000/175341]\n",
      "loss: 0.306735  [169600/175341]\n",
      "loss: 0.430987  [171200/175341]\n",
      "loss: 0.323893  [172800/175341]\n",
      "loss: 0.489748  [174400/175341]\n",
      "Train Accuracy: 81.8622%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.533773, F1-score: 77.13%, Macro_F1-Score:  42.55%  \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.452503  [    0/175341]\n",
      "loss: 0.386223  [ 1600/175341]\n",
      "loss: 0.702496  [ 3200/175341]\n",
      "loss: 0.356220  [ 4800/175341]\n",
      "loss: 0.454003  [ 6400/175341]\n",
      "loss: 0.266439  [ 8000/175341]\n",
      "loss: 0.633954  [ 9600/175341]\n",
      "loss: 0.275226  [11200/175341]\n",
      "loss: 0.586060  [12800/175341]\n",
      "loss: 0.549561  [14400/175341]\n",
      "loss: 0.181454  [16000/175341]\n",
      "loss: 0.506109  [17600/175341]\n",
      "loss: 0.182260  [19200/175341]\n",
      "loss: 0.590476  [20800/175341]\n",
      "loss: 0.390032  [22400/175341]\n",
      "loss: 0.106960  [24000/175341]\n",
      "loss: 0.639611  [25600/175341]\n",
      "loss: 0.341046  [27200/175341]\n",
      "loss: 0.651831  [28800/175341]\n",
      "loss: 0.241868  [30400/175341]\n",
      "loss: 0.442491  [32000/175341]\n",
      "loss: 0.356595  [33600/175341]\n",
      "loss: 0.325211  [35200/175341]\n",
      "loss: 0.710417  [36800/175341]\n",
      "loss: 0.388494  [38400/175341]\n",
      "loss: 0.120119  [40000/175341]\n",
      "loss: 0.565179  [41600/175341]\n",
      "loss: 0.558703  [43200/175341]\n",
      "loss: 0.246889  [44800/175341]\n",
      "loss: 0.655658  [46400/175341]\n",
      "loss: 0.630988  [48000/175341]\n",
      "loss: 0.353538  [49600/175341]\n",
      "loss: 0.120282  [51200/175341]\n",
      "loss: 1.085902  [52800/175341]\n",
      "loss: 0.346902  [54400/175341]\n",
      "loss: 0.503402  [56000/175341]\n",
      "loss: 0.391465  [57600/175341]\n",
      "loss: 0.552911  [59200/175341]\n",
      "loss: 1.029595  [60800/175341]\n",
      "loss: 0.367105  [62400/175341]\n",
      "loss: 0.369772  [64000/175341]\n",
      "loss: 0.496600  [65600/175341]\n",
      "loss: 0.390180  [67200/175341]\n",
      "loss: 0.381730  [68800/175341]\n",
      "loss: 0.169594  [70400/175341]\n",
      "loss: 0.320348  [72000/175341]\n",
      "loss: 0.392749  [73600/175341]\n",
      "loss: 0.538522  [75200/175341]\n",
      "loss: 0.536849  [76800/175341]\n",
      "loss: 0.374190  [78400/175341]\n",
      "loss: 0.598551  [80000/175341]\n",
      "loss: 0.302456  [81600/175341]\n",
      "loss: 0.250388  [83200/175341]\n",
      "loss: 0.709226  [84800/175341]\n",
      "loss: 0.557931  [86400/175341]\n",
      "loss: 0.189238  [88000/175341]\n",
      "loss: 0.533933  [89600/175341]\n",
      "loss: 0.491382  [91200/175341]\n",
      "loss: 0.444159  [92800/175341]\n",
      "loss: 0.744082  [94400/175341]\n",
      "loss: 0.440266  [96000/175341]\n",
      "loss: 0.366443  [97600/175341]\n",
      "loss: 0.286573  [99200/175341]\n",
      "loss: 0.529180  [100800/175341]\n",
      "loss: 0.860334  [102400/175341]\n",
      "loss: 0.252811  [104000/175341]\n",
      "loss: 0.653938  [105600/175341]\n",
      "loss: 0.899887  [107200/175341]\n",
      "loss: 0.438245  [108800/175341]\n",
      "loss: 0.599251  [110400/175341]\n",
      "loss: 0.714054  [112000/175341]\n",
      "loss: 0.414566  [113600/175341]\n",
      "loss: 0.606509  [115200/175341]\n",
      "loss: 0.560127  [116800/175341]\n",
      "loss: 0.374125  [118400/175341]\n",
      "loss: 0.336662  [120000/175341]\n",
      "loss: 0.480446  [121600/175341]\n",
      "loss: 0.289994  [123200/175341]\n",
      "loss: 0.330826  [124800/175341]\n",
      "loss: 0.606661  [126400/175341]\n",
      "loss: 0.415453  [128000/175341]\n",
      "loss: 0.521337  [129600/175341]\n",
      "loss: 0.669188  [131200/175341]\n",
      "loss: 0.660273  [132800/175341]\n",
      "loss: 0.487239  [134400/175341]\n",
      "loss: 0.199071  [136000/175341]\n",
      "loss: 0.333215  [137600/175341]\n",
      "loss: 0.459091  [139200/175341]\n",
      "loss: 0.643738  [140800/175341]\n",
      "loss: 0.402322  [142400/175341]\n",
      "loss: 0.597307  [144000/175341]\n",
      "loss: 0.304773  [145600/175341]\n",
      "loss: 0.739565  [147200/175341]\n",
      "loss: 0.618372  [148800/175341]\n",
      "loss: 0.581911  [150400/175341]\n",
      "loss: 0.201129  [152000/175341]\n",
      "loss: 1.013372  [153600/175341]\n",
      "loss: 0.817017  [155200/175341]\n",
      "loss: 0.152160  [156800/175341]\n",
      "loss: 0.225789  [158400/175341]\n",
      "loss: 0.316637  [160000/175341]\n",
      "loss: 0.213986  [161600/175341]\n",
      "loss: 0.366457  [163200/175341]\n",
      "loss: 0.723759  [164800/175341]\n",
      "loss: 0.322612  [166400/175341]\n",
      "loss: 0.546646  [168000/175341]\n",
      "loss: 0.333844  [169600/175341]\n",
      "loss: 0.504856  [171200/175341]\n",
      "loss: 0.417696  [172800/175341]\n",
      "loss: 0.352101  [174400/175341]\n",
      "Train Accuracy: 81.8291%\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.535574, F1-score: 77.39%, Macro_F1-Score:  42.06%  \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.359088  [    0/175341]\n",
      "loss: 0.044353  [ 1600/175341]\n",
      "loss: 0.361123  [ 3200/175341]\n",
      "loss: 0.144668  [ 4800/175341]\n",
      "loss: 0.370066  [ 6400/175341]\n",
      "loss: 0.581942  [ 8000/175341]\n",
      "loss: 0.318826  [ 9600/175341]\n",
      "loss: 0.345307  [11200/175341]\n",
      "loss: 0.714391  [12800/175341]\n",
      "loss: 0.148134  [14400/175341]\n",
      "loss: 0.643720  [16000/175341]\n",
      "loss: 0.309871  [17600/175341]\n",
      "loss: 0.391608  [19200/175341]\n",
      "loss: 0.236972  [20800/175341]\n",
      "loss: 0.625542  [22400/175341]\n",
      "loss: 0.763218  [24000/175341]\n",
      "loss: 0.487716  [25600/175341]\n",
      "loss: 0.226560  [27200/175341]\n",
      "loss: 0.235178  [28800/175341]\n",
      "loss: 0.667104  [30400/175341]\n",
      "loss: 0.546154  [32000/175341]\n",
      "loss: 0.539524  [33600/175341]\n",
      "loss: 0.653048  [35200/175341]\n",
      "loss: 0.592782  [36800/175341]\n",
      "loss: 0.438281  [38400/175341]\n",
      "loss: 0.060187  [40000/175341]\n",
      "loss: 0.631265  [41600/175341]\n",
      "loss: 0.176426  [43200/175341]\n",
      "loss: 0.278631  [44800/175341]\n",
      "loss: 0.373815  [46400/175341]\n",
      "loss: 0.261290  [48000/175341]\n",
      "loss: 0.556300  [49600/175341]\n",
      "loss: 0.140712  [51200/175341]\n",
      "loss: 0.177656  [52800/175341]\n",
      "loss: 0.361160  [54400/175341]\n",
      "loss: 0.356944  [56000/175341]\n",
      "loss: 0.075084  [57600/175341]\n",
      "loss: 0.865567  [59200/175341]\n",
      "loss: 0.653665  [60800/175341]\n",
      "loss: 0.789567  [62400/175341]\n",
      "loss: 0.318879  [64000/175341]\n",
      "loss: 0.479044  [65600/175341]\n",
      "loss: 0.785574  [67200/175341]\n",
      "loss: 0.310553  [68800/175341]\n",
      "loss: 0.451568  [70400/175341]\n",
      "loss: 0.414429  [72000/175341]\n",
      "loss: 0.449161  [73600/175341]\n",
      "loss: 0.766906  [75200/175341]\n",
      "loss: 0.248441  [76800/175341]\n",
      "loss: 0.579682  [78400/175341]\n",
      "loss: 0.705374  [80000/175341]\n",
      "loss: 0.332740  [81600/175341]\n",
      "loss: 0.300237  [83200/175341]\n",
      "loss: 0.445000  [84800/175341]\n",
      "loss: 0.546311  [86400/175341]\n",
      "loss: 0.333164  [88000/175341]\n",
      "loss: 0.205906  [89600/175341]\n",
      "loss: 0.154833  [91200/175341]\n",
      "loss: 0.440181  [92800/175341]\n",
      "loss: 0.503930  [94400/175341]\n",
      "loss: 0.192480  [96000/175341]\n",
      "loss: 0.278650  [97600/175341]\n",
      "loss: 0.489011  [99200/175341]\n",
      "loss: 0.450317  [100800/175341]\n",
      "loss: 0.527431  [102400/175341]\n",
      "loss: 0.196158  [104000/175341]\n",
      "loss: 0.100871  [105600/175341]\n",
      "loss: 0.579464  [107200/175341]\n",
      "loss: 0.610517  [108800/175341]\n",
      "loss: 0.555487  [110400/175341]\n",
      "loss: 0.301934  [112000/175341]\n",
      "loss: 0.571900  [113600/175341]\n",
      "loss: 0.680242  [115200/175341]\n",
      "loss: 0.991481  [116800/175341]\n",
      "loss: 0.489228  [118400/175341]\n",
      "loss: 0.320782  [120000/175341]\n",
      "loss: 0.053525  [121600/175341]\n",
      "loss: 0.295041  [123200/175341]\n",
      "loss: 0.384354  [124800/175341]\n",
      "loss: 0.556798  [126400/175341]\n",
      "loss: 0.436551  [128000/175341]\n",
      "loss: 0.350957  [129600/175341]\n",
      "loss: 0.521885  [131200/175341]\n",
      "loss: 0.519871  [132800/175341]\n",
      "loss: 0.633609  [134400/175341]\n",
      "loss: 0.588436  [136000/175341]\n",
      "loss: 0.562530  [137600/175341]\n",
      "loss: 0.874440  [139200/175341]\n",
      "loss: 0.552096  [140800/175341]\n",
      "loss: 0.863765  [142400/175341]\n",
      "loss: 0.578623  [144000/175341]\n",
      "loss: 0.262450  [145600/175341]\n",
      "loss: 0.875680  [147200/175341]\n",
      "loss: 0.423719  [148800/175341]\n",
      "loss: 0.429322  [150400/175341]\n",
      "loss: 0.578068  [152000/175341]\n",
      "loss: 0.565337  [153600/175341]\n",
      "loss: 0.828861  [155200/175341]\n",
      "loss: 0.320727  [156800/175341]\n",
      "loss: 0.652891  [158400/175341]\n",
      "loss: 0.532505  [160000/175341]\n",
      "loss: 0.432290  [161600/175341]\n",
      "loss: 0.295335  [163200/175341]\n",
      "loss: 0.305773  [164800/175341]\n",
      "loss: 0.429676  [166400/175341]\n",
      "loss: 0.374582  [168000/175341]\n",
      "loss: 0.122752  [169600/175341]\n",
      "loss: 0.674277  [171200/175341]\n",
      "loss: 0.300387  [172800/175341]\n",
      "loss: 0.370921  [174400/175341]\n",
      "Train Accuracy: 81.8890%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.543490, F1-score: 76.61%, Macro_F1-Score:  41.95%  \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.404658  [    0/175341]\n",
      "loss: 0.370858  [ 1600/175341]\n",
      "loss: 0.696293  [ 3200/175341]\n",
      "loss: 0.445265  [ 4800/175341]\n",
      "loss: 0.406449  [ 6400/175341]\n",
      "loss: 0.363364  [ 8000/175341]\n",
      "loss: 0.328513  [ 9600/175341]\n",
      "loss: 0.070291  [11200/175341]\n",
      "loss: 0.458488  [12800/175341]\n",
      "loss: 0.590541  [14400/175341]\n",
      "loss: 0.316799  [16000/175341]\n",
      "loss: 0.368106  [17600/175341]\n",
      "loss: 0.490473  [19200/175341]\n",
      "loss: 0.367108  [20800/175341]\n",
      "loss: 0.515722  [22400/175341]\n",
      "loss: 0.406653  [24000/175341]\n",
      "loss: 0.110336  [25600/175341]\n",
      "loss: 0.238986  [27200/175341]\n",
      "loss: 0.278417  [28800/175341]\n",
      "loss: 0.391613  [30400/175341]\n",
      "loss: 0.197593  [32000/175341]\n",
      "loss: 0.338192  [33600/175341]\n",
      "loss: 0.213049  [35200/175341]\n",
      "loss: 0.539007  [36800/175341]\n",
      "loss: 0.528250  [38400/175341]\n",
      "loss: 0.273945  [40000/175341]\n",
      "loss: 0.407083  [41600/175341]\n",
      "loss: 0.468999  [43200/175341]\n",
      "loss: 0.418798  [44800/175341]\n",
      "loss: 1.048646  [46400/175341]\n",
      "loss: 0.528645  [48000/175341]\n",
      "loss: 0.868631  [49600/175341]\n",
      "loss: 0.727800  [51200/175341]\n",
      "loss: 0.875020  [52800/175341]\n",
      "loss: 0.226998  [54400/175341]\n",
      "loss: 0.279650  [56000/175341]\n",
      "loss: 0.697754  [57600/175341]\n",
      "loss: 0.421837  [59200/175341]\n",
      "loss: 0.304089  [60800/175341]\n",
      "loss: 0.673980  [62400/175341]\n",
      "loss: 0.072061  [64000/175341]\n",
      "loss: 0.423222  [65600/175341]\n",
      "loss: 0.704168  [67200/175341]\n",
      "loss: 0.790224  [68800/175341]\n",
      "loss: 0.319035  [70400/175341]\n",
      "loss: 0.443512  [72000/175341]\n",
      "loss: 0.370740  [73600/175341]\n",
      "loss: 0.493132  [75200/175341]\n",
      "loss: 0.734640  [76800/175341]\n",
      "loss: 0.494963  [78400/175341]\n",
      "loss: 0.634664  [80000/175341]\n",
      "loss: 0.343596  [81600/175341]\n",
      "loss: 0.552452  [83200/175341]\n",
      "loss: 0.307708  [84800/175341]\n",
      "loss: 0.498968  [86400/175341]\n",
      "loss: 0.965131  [88000/175341]\n",
      "loss: 0.266390  [89600/175341]\n",
      "loss: 0.444053  [91200/175341]\n",
      "loss: 0.168648  [92800/175341]\n",
      "loss: 0.590101  [94400/175341]\n",
      "loss: 0.454810  [96000/175341]\n",
      "loss: 0.360628  [97600/175341]\n",
      "loss: 0.578809  [99200/175341]\n",
      "loss: 0.500043  [100800/175341]\n",
      "loss: 0.393854  [102400/175341]\n",
      "loss: 0.527040  [104000/175341]\n",
      "loss: 0.316039  [105600/175341]\n",
      "loss: 0.297918  [107200/175341]\n",
      "loss: 0.489421  [108800/175341]\n",
      "loss: 0.424144  [110400/175341]\n",
      "loss: 0.651430  [112000/175341]\n",
      "loss: 0.403758  [113600/175341]\n",
      "loss: 0.422874  [115200/175341]\n",
      "loss: 0.304750  [116800/175341]\n",
      "loss: 0.710329  [118400/175341]\n",
      "loss: 0.583636  [120000/175341]\n",
      "loss: 0.559421  [121600/175341]\n",
      "loss: 0.478405  [123200/175341]\n",
      "loss: 0.250923  [124800/175341]\n",
      "loss: 0.234681  [126400/175341]\n",
      "loss: 0.137251  [128000/175341]\n",
      "loss: 0.387828  [129600/175341]\n",
      "loss: 0.443345  [131200/175341]\n",
      "loss: 0.662903  [132800/175341]\n",
      "loss: 0.384606  [134400/175341]\n",
      "loss: 0.408822  [136000/175341]\n",
      "loss: 0.335468  [137600/175341]\n",
      "loss: 0.507419  [139200/175341]\n",
      "loss: 0.099594  [140800/175341]\n",
      "loss: 0.399484  [142400/175341]\n",
      "loss: 0.551729  [144000/175341]\n",
      "loss: 0.393009  [145600/175341]\n",
      "loss: 0.428685  [147200/175341]\n",
      "loss: 0.638373  [148800/175341]\n",
      "loss: 0.146171  [150400/175341]\n",
      "loss: 0.792983  [152000/175341]\n",
      "loss: 0.267013  [153600/175341]\n",
      "loss: 0.325629  [155200/175341]\n",
      "loss: 0.660357  [156800/175341]\n",
      "loss: 0.717283  [158400/175341]\n",
      "loss: 0.338884  [160000/175341]\n",
      "loss: 0.465104  [161600/175341]\n",
      "loss: 0.396595  [163200/175341]\n",
      "loss: 0.543219  [164800/175341]\n",
      "loss: 0.200587  [166400/175341]\n",
      "loss: 0.769081  [168000/175341]\n",
      "loss: 0.262131  [169600/175341]\n",
      "loss: 0.179708  [171200/175341]\n",
      "loss: 0.652963  [172800/175341]\n",
      "loss: 0.304222  [174400/175341]\n",
      "Train Accuracy: 81.8097%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.549760, F1-score: 76.27%, Macro_F1-Score:  41.45%  \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.427751  [    0/175341]\n",
      "loss: 0.344119  [ 1600/175341]\n",
      "loss: 0.338578  [ 3200/175341]\n",
      "loss: 0.346377  [ 4800/175341]\n",
      "loss: 0.380573  [ 6400/175341]\n",
      "loss: 0.077222  [ 8000/175341]\n",
      "loss: 0.538297  [ 9600/175341]\n",
      "loss: 0.650052  [11200/175341]\n",
      "loss: 0.582754  [12800/175341]\n",
      "loss: 0.527550  [14400/175341]\n",
      "loss: 0.386643  [16000/175341]\n",
      "loss: 0.481977  [17600/175341]\n",
      "loss: 0.187711  [19200/175341]\n",
      "loss: 0.561663  [20800/175341]\n",
      "loss: 0.617759  [22400/175341]\n",
      "loss: 0.322678  [24000/175341]\n",
      "loss: 0.665510  [25600/175341]\n",
      "loss: 0.294835  [27200/175341]\n",
      "loss: 0.571013  [28800/175341]\n",
      "loss: 0.692772  [30400/175341]\n",
      "loss: 0.584733  [32000/175341]\n",
      "loss: 0.443990  [33600/175341]\n",
      "loss: 0.583048  [35200/175341]\n",
      "loss: 0.296879  [36800/175341]\n",
      "loss: 0.211295  [38400/175341]\n",
      "loss: 0.328640  [40000/175341]\n",
      "loss: 0.101943  [41600/175341]\n",
      "loss: 0.337335  [43200/175341]\n",
      "loss: 0.231531  [44800/175341]\n",
      "loss: 0.781260  [46400/175341]\n",
      "loss: 0.655093  [48000/175341]\n",
      "loss: 0.287811  [49600/175341]\n",
      "loss: 0.490536  [51200/175341]\n",
      "loss: 0.708264  [52800/175341]\n",
      "loss: 0.569496  [54400/175341]\n",
      "loss: 0.790213  [56000/175341]\n",
      "loss: 0.565787  [57600/175341]\n",
      "loss: 0.100458  [59200/175341]\n",
      "loss: 0.341193  [60800/175341]\n",
      "loss: 0.289902  [62400/175341]\n",
      "loss: 0.408042  [64000/175341]\n",
      "loss: 0.464741  [65600/175341]\n",
      "loss: 0.159632  [67200/175341]\n",
      "loss: 0.311381  [68800/175341]\n",
      "loss: 0.268376  [70400/175341]\n",
      "loss: 0.466969  [72000/175341]\n",
      "loss: 1.171672  [73600/175341]\n",
      "loss: 0.613621  [75200/175341]\n",
      "loss: 0.613278  [76800/175341]\n",
      "loss: 0.238752  [78400/175341]\n",
      "loss: 0.503417  [80000/175341]\n",
      "loss: 0.287961  [81600/175341]\n",
      "loss: 0.511854  [83200/175341]\n",
      "loss: 0.328618  [84800/175341]\n",
      "loss: 0.263824  [86400/175341]\n",
      "loss: 0.664072  [88000/175341]\n",
      "loss: 0.614676  [89600/175341]\n",
      "loss: 0.220465  [91200/175341]\n",
      "loss: 0.165552  [92800/175341]\n",
      "loss: 0.491447  [94400/175341]\n",
      "loss: 0.457121  [96000/175341]\n",
      "loss: 0.616199  [97600/175341]\n",
      "loss: 0.888178  [99200/175341]\n",
      "loss: 0.160502  [100800/175341]\n",
      "loss: 0.501130  [102400/175341]\n",
      "loss: 0.152956  [104000/175341]\n",
      "loss: 0.541455  [105600/175341]\n",
      "loss: 0.663704  [107200/175341]\n",
      "loss: 0.389203  [108800/175341]\n",
      "loss: 0.438414  [110400/175341]\n",
      "loss: 0.750409  [112000/175341]\n",
      "loss: 0.419655  [113600/175341]\n",
      "loss: 0.501725  [115200/175341]\n",
      "loss: 0.603735  [116800/175341]\n",
      "loss: 0.873613  [118400/175341]\n",
      "loss: 0.662083  [120000/175341]\n",
      "loss: 0.617662  [121600/175341]\n",
      "loss: 0.218717  [123200/175341]\n",
      "loss: 0.258899  [124800/175341]\n",
      "loss: 0.694981  [126400/175341]\n",
      "loss: 0.317952  [128000/175341]\n",
      "loss: 0.706609  [129600/175341]\n",
      "loss: 0.456923  [131200/175341]\n",
      "loss: 0.379258  [132800/175341]\n",
      "loss: 0.482425  [134400/175341]\n",
      "loss: 1.150828  [136000/175341]\n",
      "loss: 0.296716  [137600/175341]\n",
      "loss: 0.332732  [139200/175341]\n",
      "loss: 0.766908  [140800/175341]\n",
      "loss: 0.206461  [142400/175341]\n",
      "loss: 0.113824  [144000/175341]\n",
      "loss: 0.116744  [145600/175341]\n",
      "loss: 0.577948  [147200/175341]\n",
      "loss: 0.370879  [148800/175341]\n",
      "loss: 0.433641  [150400/175341]\n",
      "loss: 0.173073  [152000/175341]\n",
      "loss: 0.593737  [153600/175341]\n",
      "loss: 0.608002  [155200/175341]\n",
      "loss: 0.553451  [156800/175341]\n",
      "loss: 0.506343  [158400/175341]\n",
      "loss: 0.357075  [160000/175341]\n",
      "loss: 0.787843  [161600/175341]\n",
      "loss: 0.768064  [163200/175341]\n",
      "loss: 0.504934  [164800/175341]\n",
      "loss: 0.400676  [166400/175341]\n",
      "loss: 0.120276  [168000/175341]\n",
      "loss: 0.714065  [169600/175341]\n",
      "loss: 0.718328  [171200/175341]\n",
      "loss: 0.444103  [172800/175341]\n",
      "loss: 0.800766  [174400/175341]\n",
      "Train Accuracy: 81.8548%\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.550510, F1-score: 76.62%, Macro_F1-Score:  42.25%  \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.947987  [    0/175341]\n",
      "loss: 0.489394  [ 1600/175341]\n",
      "loss: 0.105264  [ 3200/175341]\n",
      "loss: 0.338163  [ 4800/175341]\n",
      "loss: 0.338868  [ 6400/175341]\n",
      "loss: 0.055438  [ 8000/175341]\n",
      "loss: 1.009599  [ 9600/175341]\n",
      "loss: 0.390342  [11200/175341]\n",
      "loss: 0.470175  [12800/175341]\n",
      "loss: 0.508027  [14400/175341]\n",
      "loss: 0.317658  [16000/175341]\n",
      "loss: 0.504096  [17600/175341]\n",
      "loss: 0.180213  [19200/175341]\n",
      "loss: 0.354953  [20800/175341]\n",
      "loss: 0.619356  [22400/175341]\n",
      "loss: 0.304869  [24000/175341]\n",
      "loss: 0.350877  [25600/175341]\n",
      "loss: 0.306583  [27200/175341]\n",
      "loss: 0.464291  [28800/175341]\n",
      "loss: 0.497589  [30400/175341]\n",
      "loss: 0.849599  [32000/175341]\n",
      "loss: 0.146193  [33600/175341]\n",
      "loss: 0.678391  [35200/175341]\n",
      "loss: 0.146752  [36800/175341]\n",
      "loss: 0.108280  [38400/175341]\n",
      "loss: 0.379428  [40000/175341]\n",
      "loss: 0.480217  [41600/175341]\n",
      "loss: 0.583910  [43200/175341]\n",
      "loss: 0.936238  [44800/175341]\n",
      "loss: 0.567610  [46400/175341]\n",
      "loss: 0.623117  [48000/175341]\n",
      "loss: 0.375929  [49600/175341]\n",
      "loss: 0.244376  [51200/175341]\n",
      "loss: 0.532381  [52800/175341]\n",
      "loss: 0.276620  [54400/175341]\n",
      "loss: 0.378371  [56000/175341]\n",
      "loss: 0.426753  [57600/175341]\n",
      "loss: 0.328799  [59200/175341]\n",
      "loss: 0.721196  [60800/175341]\n",
      "loss: 0.406768  [62400/175341]\n",
      "loss: 0.374486  [64000/175341]\n",
      "loss: 0.459382  [65600/175341]\n",
      "loss: 0.350507  [67200/175341]\n",
      "loss: 0.670472  [68800/175341]\n",
      "loss: 0.452498  [70400/175341]\n",
      "loss: 0.627325  [72000/175341]\n",
      "loss: 0.382518  [73600/175341]\n",
      "loss: 0.333877  [75200/175341]\n",
      "loss: 0.788971  [76800/175341]\n",
      "loss: 0.579579  [78400/175341]\n",
      "loss: 0.632555  [80000/175341]\n",
      "loss: 0.352137  [81600/175341]\n",
      "loss: 0.665486  [83200/175341]\n",
      "loss: 0.371015  [84800/175341]\n",
      "loss: 0.482127  [86400/175341]\n",
      "loss: 0.585766  [88000/175341]\n",
      "loss: 0.584424  [89600/175341]\n",
      "loss: 0.365795  [91200/175341]\n",
      "loss: 0.622640  [92800/175341]\n",
      "loss: 0.460944  [94400/175341]\n",
      "loss: 0.704686  [96000/175341]\n",
      "loss: 0.442630  [97600/175341]\n",
      "loss: 0.333384  [99200/175341]\n",
      "loss: 0.405184  [100800/175341]\n",
      "loss: 0.373119  [102400/175341]\n",
      "loss: 0.360816  [104000/175341]\n",
      "loss: 0.274979  [105600/175341]\n",
      "loss: 0.654293  [107200/175341]\n",
      "loss: 0.229631  [108800/175341]\n",
      "loss: 0.439027  [110400/175341]\n",
      "loss: 0.331829  [112000/175341]\n",
      "loss: 0.564587  [113600/175341]\n",
      "loss: 0.834626  [115200/175341]\n",
      "loss: 0.535190  [116800/175341]\n",
      "loss: 0.495677  [118400/175341]\n",
      "loss: 0.504496  [120000/175341]\n",
      "loss: 0.648351  [121600/175341]\n",
      "loss: 0.782046  [123200/175341]\n",
      "loss: 0.230436  [124800/175341]\n",
      "loss: 0.358707  [126400/175341]\n",
      "loss: 0.697842  [128000/175341]\n",
      "loss: 0.323460  [129600/175341]\n",
      "loss: 0.501165  [131200/175341]\n",
      "loss: 0.350989  [132800/175341]\n",
      "loss: 0.217152  [134400/175341]\n",
      "loss: 0.436456  [136000/175341]\n",
      "loss: 0.325736  [137600/175341]\n",
      "loss: 0.596700  [139200/175341]\n",
      "loss: 0.299294  [140800/175341]\n",
      "loss: 0.236725  [142400/175341]\n",
      "loss: 0.101912  [144000/175341]\n",
      "loss: 0.540533  [145600/175341]\n",
      "loss: 0.500588  [147200/175341]\n",
      "loss: 0.815087  [148800/175341]\n",
      "loss: 0.470682  [150400/175341]\n",
      "loss: 0.507038  [152000/175341]\n",
      "loss: 0.736902  [153600/175341]\n",
      "loss: 0.825870  [155200/175341]\n",
      "loss: 0.187453  [156800/175341]\n",
      "loss: 0.417861  [158400/175341]\n",
      "loss: 0.477823  [160000/175341]\n",
      "loss: 0.557197  [161600/175341]\n",
      "loss: 0.696578  [163200/175341]\n",
      "loss: 0.315384  [164800/175341]\n",
      "loss: 0.560211  [166400/175341]\n",
      "loss: 0.404203  [168000/175341]\n",
      "loss: 0.680875  [169600/175341]\n",
      "loss: 0.278158  [171200/175341]\n",
      "loss: 0.753707  [172800/175341]\n",
      "loss: 0.528002  [174400/175341]\n",
      "Train Accuracy: 81.8782%\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.569294, F1-score: 75.38%, Macro_F1-Score:  41.46%  \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.394672  [    0/175341]\n",
      "loss: 0.043413  [ 1600/175341]\n",
      "loss: 0.186091  [ 3200/175341]\n",
      "loss: 0.316934  [ 4800/175341]\n",
      "loss: 0.482012  [ 6400/175341]\n",
      "loss: 0.368857  [ 8000/175341]\n",
      "loss: 0.456077  [ 9600/175341]\n",
      "loss: 0.616347  [11200/175341]\n",
      "loss: 0.272645  [12800/175341]\n",
      "loss: 0.582981  [14400/175341]\n",
      "loss: 0.355427  [16000/175341]\n",
      "loss: 0.537994  [17600/175341]\n",
      "loss: 0.391364  [19200/175341]\n",
      "loss: 0.303819  [20800/175341]\n",
      "loss: 0.269318  [22400/175341]\n",
      "loss: 0.130659  [24000/175341]\n",
      "loss: 0.229861  [25600/175341]\n",
      "loss: 0.662591  [27200/175341]\n",
      "loss: 0.404558  [28800/175341]\n",
      "loss: 0.734277  [30400/175341]\n",
      "loss: 0.429989  [32000/175341]\n",
      "loss: 0.248207  [33600/175341]\n",
      "loss: 0.505507  [35200/175341]\n",
      "loss: 0.232766  [36800/175341]\n",
      "loss: 0.558470  [38400/175341]\n",
      "loss: 0.594106  [40000/175341]\n",
      "loss: 0.499905  [41600/175341]\n",
      "loss: 0.397928  [43200/175341]\n",
      "loss: 0.725684  [44800/175341]\n",
      "loss: 0.244468  [46400/175341]\n",
      "loss: 0.176063  [48000/175341]\n",
      "loss: 0.437586  [49600/175341]\n",
      "loss: 0.305934  [51200/175341]\n",
      "loss: 0.385730  [52800/175341]\n",
      "loss: 0.344389  [54400/175341]\n",
      "loss: 0.708111  [56000/175341]\n",
      "loss: 0.260309  [57600/175341]\n",
      "loss: 0.466071  [59200/175341]\n",
      "loss: 0.769739  [60800/175341]\n",
      "loss: 0.884644  [62400/175341]\n",
      "loss: 0.448084  [64000/175341]\n",
      "loss: 0.772580  [65600/175341]\n",
      "loss: 0.703119  [67200/175341]\n",
      "loss: 0.490333  [68800/175341]\n",
      "loss: 0.557175  [70400/175341]\n",
      "loss: 0.514366  [72000/175341]\n",
      "loss: 0.335454  [73600/175341]\n",
      "loss: 0.345754  [75200/175341]\n",
      "loss: 0.346300  [76800/175341]\n",
      "loss: 0.280680  [78400/175341]\n",
      "loss: 0.658266  [80000/175341]\n",
      "loss: 0.568124  [81600/175341]\n",
      "loss: 0.539373  [83200/175341]\n",
      "loss: 0.657352  [84800/175341]\n",
      "loss: 0.565557  [86400/175341]\n",
      "loss: 0.651013  [88000/175341]\n",
      "loss: 0.429760  [89600/175341]\n",
      "loss: 0.461346  [91200/175341]\n",
      "loss: 0.465105  [92800/175341]\n",
      "loss: 0.675504  [94400/175341]\n",
      "loss: 0.445877  [96000/175341]\n",
      "loss: 0.442760  [97600/175341]\n",
      "loss: 0.375158  [99200/175341]\n",
      "loss: 0.680976  [100800/175341]\n",
      "loss: 0.227004  [102400/175341]\n",
      "loss: 0.222078  [104000/175341]\n",
      "loss: 0.258823  [105600/175341]\n",
      "loss: 0.222540  [107200/175341]\n",
      "loss: 0.503388  [108800/175341]\n",
      "loss: 0.248515  [110400/175341]\n",
      "loss: 0.347025  [112000/175341]\n",
      "loss: 0.383304  [113600/175341]\n",
      "loss: 0.674027  [115200/175341]\n",
      "loss: 0.523854  [116800/175341]\n",
      "loss: 0.190126  [118400/175341]\n",
      "loss: 0.595079  [120000/175341]\n",
      "loss: 0.278245  [121600/175341]\n",
      "loss: 0.259688  [123200/175341]\n",
      "loss: 0.786163  [124800/175341]\n",
      "loss: 0.235487  [126400/175341]\n",
      "loss: 0.264402  [128000/175341]\n",
      "loss: 0.707197  [129600/175341]\n",
      "loss: 0.661720  [131200/175341]\n",
      "loss: 0.781982  [132800/175341]\n",
      "loss: 0.812841  [134400/175341]\n",
      "loss: 0.514307  [136000/175341]\n",
      "loss: 0.503192  [137600/175341]\n",
      "loss: 0.306811  [139200/175341]\n",
      "loss: 0.236493  [140800/175341]\n",
      "loss: 0.393422  [142400/175341]\n",
      "loss: 0.441772  [144000/175341]\n",
      "loss: 0.243721  [145600/175341]\n",
      "loss: 0.529862  [147200/175341]\n",
      "loss: 0.295091  [148800/175341]\n",
      "loss: 0.181419  [150400/175341]\n",
      "loss: 0.615685  [152000/175341]\n",
      "loss: 0.722626  [153600/175341]\n",
      "loss: 0.530438  [155200/175341]\n",
      "loss: 0.704767  [156800/175341]\n",
      "loss: 0.449851  [158400/175341]\n",
      "loss: 0.292202  [160000/175341]\n",
      "loss: 0.712633  [161600/175341]\n",
      "loss: 0.953989  [163200/175341]\n",
      "loss: 0.383111  [164800/175341]\n",
      "loss: 0.188699  [166400/175341]\n",
      "loss: 0.198705  [168000/175341]\n",
      "loss: 0.580450  [169600/175341]\n",
      "loss: 0.129000  [171200/175341]\n",
      "loss: 0.504523  [172800/175341]\n",
      "loss: 0.479438  [174400/175341]\n",
      "Train Accuracy: 81.8822%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.532046, F1-score: 77.27%, Macro_F1-Score:  42.31%  \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.371160  [    0/175341]\n",
      "loss: 0.227873  [ 1600/175341]\n",
      "loss: 0.557321  [ 3200/175341]\n",
      "loss: 0.819920  [ 4800/175341]\n",
      "loss: 0.732850  [ 6400/175341]\n",
      "loss: 0.533322  [ 8000/175341]\n",
      "loss: 0.713578  [ 9600/175341]\n",
      "loss: 0.333111  [11200/175341]\n",
      "loss: 0.621852  [12800/175341]\n",
      "loss: 0.253446  [14400/175341]\n",
      "loss: 0.404429  [16000/175341]\n",
      "loss: 0.856671  [17600/175341]\n",
      "loss: 0.229300  [19200/175341]\n",
      "loss: 0.273301  [20800/175341]\n",
      "loss: 0.544731  [22400/175341]\n",
      "loss: 0.492214  [24000/175341]\n",
      "loss: 0.503600  [25600/175341]\n",
      "loss: 0.671645  [27200/175341]\n",
      "loss: 0.658989  [28800/175341]\n",
      "loss: 0.497241  [30400/175341]\n",
      "loss: 0.403864  [32000/175341]\n",
      "loss: 0.991383  [33600/175341]\n",
      "loss: 0.406169  [35200/175341]\n",
      "loss: 0.838300  [36800/175341]\n",
      "loss: 0.282435  [38400/175341]\n",
      "loss: 0.245541  [40000/175341]\n",
      "loss: 0.497267  [41600/175341]\n",
      "loss: 0.250927  [43200/175341]\n",
      "loss: 0.600010  [44800/175341]\n",
      "loss: 0.752561  [46400/175341]\n",
      "loss: 0.222707  [48000/175341]\n",
      "loss: 0.442495  [49600/175341]\n",
      "loss: 0.437543  [51200/175341]\n",
      "loss: 0.544756  [52800/175341]\n",
      "loss: 0.612768  [54400/175341]\n",
      "loss: 0.752067  [56000/175341]\n",
      "loss: 0.592049  [57600/175341]\n",
      "loss: 0.341758  [59200/175341]\n",
      "loss: 0.500596  [60800/175341]\n",
      "loss: 0.290320  [62400/175341]\n",
      "loss: 0.571291  [64000/175341]\n",
      "loss: 0.332535  [65600/175341]\n",
      "loss: 1.342828  [67200/175341]\n",
      "loss: 0.415051  [68800/175341]\n",
      "loss: 0.532318  [70400/175341]\n",
      "loss: 0.556937  [72000/175341]\n",
      "loss: 0.233137  [73600/175341]\n",
      "loss: 0.661231  [75200/175341]\n",
      "loss: 0.183133  [76800/175341]\n",
      "loss: 0.823977  [78400/175341]\n",
      "loss: 0.451179  [80000/175341]\n",
      "loss: 0.685078  [81600/175341]\n",
      "loss: 0.629387  [83200/175341]\n",
      "loss: 0.207595  [84800/175341]\n",
      "loss: 0.205274  [86400/175341]\n",
      "loss: 0.306551  [88000/175341]\n",
      "loss: 0.585405  [89600/175341]\n",
      "loss: 0.241783  [91200/175341]\n",
      "loss: 0.299939  [92800/175341]\n",
      "loss: 0.247308  [94400/175341]\n",
      "loss: 0.367077  [96000/175341]\n",
      "loss: 0.445448  [97600/175341]\n",
      "loss: 0.292831  [99200/175341]\n",
      "loss: 0.229048  [100800/175341]\n",
      "loss: 0.592252  [102400/175341]\n",
      "loss: 0.198739  [104000/175341]\n",
      "loss: 0.182485  [105600/175341]\n",
      "loss: 0.708181  [107200/175341]\n",
      "loss: 0.649029  [108800/175341]\n",
      "loss: 0.576914  [110400/175341]\n",
      "loss: 0.419331  [112000/175341]\n",
      "loss: 0.498642  [113600/175341]\n",
      "loss: 0.759124  [115200/175341]\n",
      "loss: 0.358026  [116800/175341]\n",
      "loss: 0.467659  [118400/175341]\n",
      "loss: 0.927941  [120000/175341]\n",
      "loss: 0.252850  [121600/175341]\n",
      "loss: 0.355936  [123200/175341]\n",
      "loss: 0.563204  [124800/175341]\n",
      "loss: 0.572093  [126400/175341]\n",
      "loss: 0.724316  [128000/175341]\n",
      "loss: 0.341341  [129600/175341]\n",
      "loss: 0.288054  [131200/175341]\n",
      "loss: 1.292618  [132800/175341]\n",
      "loss: 0.600628  [134400/175341]\n",
      "loss: 0.743216  [136000/175341]\n",
      "loss: 0.350837  [137600/175341]\n",
      "loss: 0.455883  [139200/175341]\n",
      "loss: 0.476086  [140800/175341]\n",
      "loss: 0.440046  [142400/175341]\n",
      "loss: 0.147164  [144000/175341]\n",
      "loss: 0.319419  [145600/175341]\n",
      "loss: 0.493713  [147200/175341]\n",
      "loss: 0.787324  [148800/175341]\n",
      "loss: 0.458035  [150400/175341]\n",
      "loss: 0.622075  [152000/175341]\n",
      "loss: 0.304824  [153600/175341]\n",
      "loss: 0.561353  [155200/175341]\n",
      "loss: 0.601149  [156800/175341]\n",
      "loss: 0.518808  [158400/175341]\n",
      "loss: 0.552418  [160000/175341]\n",
      "loss: 0.377651  [161600/175341]\n",
      "loss: 0.227242  [163200/175341]\n",
      "loss: 0.886869  [164800/175341]\n",
      "loss: 0.275149  [166400/175341]\n",
      "loss: 0.579217  [168000/175341]\n",
      "loss: 0.484759  [169600/175341]\n",
      "loss: 0.319674  [171200/175341]\n",
      "loss: 0.337752  [172800/175341]\n",
      "loss: 0.172827  [174400/175341]\n",
      "Train Accuracy: 81.8759%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.552088, F1-score: 76.31%, Macro_F1-Score:  41.40%  \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.609836  [    0/175341]\n",
      "loss: 0.155741  [ 1600/175341]\n",
      "loss: 0.456810  [ 3200/175341]\n",
      "loss: 0.843157  [ 4800/175341]\n",
      "loss: 0.498007  [ 6400/175341]\n",
      "loss: 0.808965  [ 8000/175341]\n",
      "loss: 0.841417  [ 9600/175341]\n",
      "loss: 0.353712  [11200/175341]\n",
      "loss: 0.385710  [12800/175341]\n",
      "loss: 0.165139  [14400/175341]\n",
      "loss: 0.590143  [16000/175341]\n",
      "loss: 0.219417  [17600/175341]\n",
      "loss: 0.283514  [19200/175341]\n",
      "loss: 0.772095  [20800/175341]\n",
      "loss: 0.397754  [22400/175341]\n",
      "loss: 0.598624  [24000/175341]\n",
      "loss: 0.400861  [25600/175341]\n",
      "loss: 0.408578  [27200/175341]\n",
      "loss: 0.852763  [28800/175341]\n",
      "loss: 0.279920  [30400/175341]\n",
      "loss: 0.663281  [32000/175341]\n",
      "loss: 0.274767  [33600/175341]\n",
      "loss: 0.222071  [35200/175341]\n",
      "loss: 0.037610  [36800/175341]\n",
      "loss: 0.478725  [38400/175341]\n",
      "loss: 0.503820  [40000/175341]\n",
      "loss: 0.974375  [41600/175341]\n",
      "loss: 0.318723  [43200/175341]\n",
      "loss: 0.820623  [44800/175341]\n",
      "loss: 0.498992  [46400/175341]\n",
      "loss: 0.250483  [48000/175341]\n",
      "loss: 0.433930  [49600/175341]\n",
      "loss: 0.581927  [51200/175341]\n",
      "loss: 0.580019  [52800/175341]\n",
      "loss: 0.570775  [54400/175341]\n",
      "loss: 0.466661  [56000/175341]\n",
      "loss: 0.247270  [57600/175341]\n",
      "loss: 0.435463  [59200/175341]\n",
      "loss: 0.639885  [60800/175341]\n",
      "loss: 0.593209  [62400/175341]\n",
      "loss: 0.607129  [64000/175341]\n",
      "loss: 0.240854  [65600/175341]\n",
      "loss: 0.351668  [67200/175341]\n",
      "loss: 0.213030  [68800/175341]\n",
      "loss: 0.194148  [70400/175341]\n",
      "loss: 0.465543  [72000/175341]\n",
      "loss: 0.582487  [73600/175341]\n",
      "loss: 0.452274  [75200/175341]\n",
      "loss: 0.409563  [76800/175341]\n",
      "loss: 0.330730  [78400/175341]\n",
      "loss: 0.696867  [80000/175341]\n",
      "loss: 0.560222  [81600/175341]\n",
      "loss: 0.330378  [83200/175341]\n",
      "loss: 0.427244  [84800/175341]\n",
      "loss: 0.397713  [86400/175341]\n",
      "loss: 0.188275  [88000/175341]\n",
      "loss: 0.304246  [89600/175341]\n",
      "loss: 0.500408  [91200/175341]\n",
      "loss: 0.681272  [92800/175341]\n",
      "loss: 0.269250  [94400/175341]\n",
      "loss: 0.310406  [96000/175341]\n",
      "loss: 0.191233  [97600/175341]\n",
      "loss: 0.460167  [99200/175341]\n",
      "loss: 0.529884  [100800/175341]\n",
      "loss: 0.173358  [102400/175341]\n",
      "loss: 0.464035  [104000/175341]\n",
      "loss: 0.484243  [105600/175341]\n",
      "loss: 0.236852  [107200/175341]\n",
      "loss: 0.322472  [108800/175341]\n",
      "loss: 0.780851  [110400/175341]\n",
      "loss: 0.258685  [112000/175341]\n",
      "loss: 0.586618  [113600/175341]\n",
      "loss: 0.396874  [115200/175341]\n",
      "loss: 0.473544  [116800/175341]\n",
      "loss: 0.447957  [118400/175341]\n",
      "loss: 0.529325  [120000/175341]\n",
      "loss: 0.664819  [121600/175341]\n",
      "loss: 0.450442  [123200/175341]\n",
      "loss: 0.143082  [124800/175341]\n",
      "loss: 0.207436  [126400/175341]\n",
      "loss: 0.880638  [128000/175341]\n",
      "loss: 0.509581  [129600/175341]\n",
      "loss: 0.483278  [131200/175341]\n",
      "loss: 0.483046  [132800/175341]\n",
      "loss: 0.521267  [134400/175341]\n",
      "loss: 0.476552  [136000/175341]\n",
      "loss: 0.662694  [137600/175341]\n",
      "loss: 0.578130  [139200/175341]\n",
      "loss: 0.097780  [140800/175341]\n",
      "loss: 0.638796  [142400/175341]\n",
      "loss: 0.234344  [144000/175341]\n",
      "loss: 0.570958  [145600/175341]\n",
      "loss: 0.715509  [147200/175341]\n",
      "loss: 0.281770  [148800/175341]\n",
      "loss: 0.620230  [150400/175341]\n",
      "loss: 0.446045  [152000/175341]\n",
      "loss: 0.388821  [153600/175341]\n",
      "loss: 0.187993  [155200/175341]\n",
      "loss: 0.557348  [156800/175341]\n",
      "loss: 0.324394  [158400/175341]\n",
      "loss: 0.230106  [160000/175341]\n",
      "loss: 0.346012  [161600/175341]\n",
      "loss: 0.278181  [163200/175341]\n",
      "loss: 0.788324  [164800/175341]\n",
      "loss: 0.310233  [166400/175341]\n",
      "loss: 0.438713  [168000/175341]\n",
      "loss: 0.649989  [169600/175341]\n",
      "loss: 0.251641  [171200/175341]\n",
      "loss: 0.703478  [172800/175341]\n",
      "loss: 0.366661  [174400/175341]\n",
      "Train Accuracy: 81.7989%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.551880, F1-score: 76.30%, Macro_F1-Score:  41.36%  \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.394648  [    0/175341]\n",
      "loss: 0.538915  [ 1600/175341]\n",
      "loss: 0.608408  [ 3200/175341]\n",
      "loss: 0.355178  [ 4800/175341]\n",
      "loss: 0.387003  [ 6400/175341]\n",
      "loss: 0.479921  [ 8000/175341]\n",
      "loss: 0.398562  [ 9600/175341]\n",
      "loss: 0.506550  [11200/175341]\n",
      "loss: 0.620399  [12800/175341]\n",
      "loss: 0.615050  [14400/175341]\n",
      "loss: 0.851141  [16000/175341]\n",
      "loss: 0.358178  [17600/175341]\n",
      "loss: 0.472572  [19200/175341]\n",
      "loss: 0.841265  [20800/175341]\n",
      "loss: 0.357491  [22400/175341]\n",
      "loss: 0.718511  [24000/175341]\n",
      "loss: 0.505708  [25600/175341]\n",
      "loss: 0.445635  [27200/175341]\n",
      "loss: 0.527855  [28800/175341]\n",
      "loss: 0.235205  [30400/175341]\n",
      "loss: 0.979814  [32000/175341]\n",
      "loss: 0.137908  [33600/175341]\n",
      "loss: 0.278259  [35200/175341]\n",
      "loss: 0.475690  [36800/175341]\n",
      "loss: 0.165726  [38400/175341]\n",
      "loss: 0.518968  [40000/175341]\n",
      "loss: 0.689743  [41600/175341]\n",
      "loss: 0.774498  [43200/175341]\n",
      "loss: 0.177176  [44800/175341]\n",
      "loss: 0.475928  [46400/175341]\n",
      "loss: 0.479188  [48000/175341]\n",
      "loss: 0.418571  [49600/175341]\n",
      "loss: 0.231919  [51200/175341]\n",
      "loss: 0.624085  [52800/175341]\n",
      "loss: 0.333383  [54400/175341]\n",
      "loss: 0.329239  [56000/175341]\n",
      "loss: 0.722355  [57600/175341]\n",
      "loss: 0.205645  [59200/175341]\n",
      "loss: 0.858319  [60800/175341]\n",
      "loss: 0.266553  [62400/175341]\n",
      "loss: 0.658964  [64000/175341]\n",
      "loss: 0.383182  [65600/175341]\n",
      "loss: 0.373085  [67200/175341]\n",
      "loss: 0.557859  [68800/175341]\n",
      "loss: 0.366097  [70400/175341]\n",
      "loss: 0.728818  [72000/175341]\n",
      "loss: 0.150370  [73600/175341]\n",
      "loss: 0.429578  [75200/175341]\n",
      "loss: 0.369777  [76800/175341]\n",
      "loss: 0.926641  [78400/175341]\n",
      "loss: 0.365921  [80000/175341]\n",
      "loss: 0.724819  [81600/175341]\n",
      "loss: 0.628760  [83200/175341]\n",
      "loss: 0.412747  [84800/175341]\n",
      "loss: 0.487560  [86400/175341]\n",
      "loss: 0.311083  [88000/175341]\n",
      "loss: 0.654000  [89600/175341]\n",
      "loss: 0.220886  [91200/175341]\n",
      "loss: 0.292052  [92800/175341]\n",
      "loss: 0.408006  [94400/175341]\n",
      "loss: 0.495962  [96000/175341]\n",
      "loss: 0.243837  [97600/175341]\n",
      "loss: 0.609666  [99200/175341]\n",
      "loss: 0.110876  [100800/175341]\n",
      "loss: 0.613663  [102400/175341]\n",
      "loss: 0.528267  [104000/175341]\n",
      "loss: 0.278145  [105600/175341]\n",
      "loss: 0.627214  [107200/175341]\n",
      "loss: 0.186592  [108800/175341]\n",
      "loss: 0.222896  [110400/175341]\n",
      "loss: 0.605940  [112000/175341]\n",
      "loss: 0.052847  [113600/175341]\n",
      "loss: 0.858443  [115200/175341]\n",
      "loss: 0.399067  [116800/175341]\n",
      "loss: 0.223745  [118400/175341]\n",
      "loss: 0.413762  [120000/175341]\n",
      "loss: 0.594520  [121600/175341]\n",
      "loss: 0.514806  [123200/175341]\n",
      "loss: 0.692543  [124800/175341]\n",
      "loss: 0.626119  [126400/175341]\n",
      "loss: 0.726624  [128000/175341]\n",
      "loss: 0.173905  [129600/175341]\n",
      "loss: 0.138021  [131200/175341]\n",
      "loss: 0.516871  [132800/175341]\n",
      "loss: 0.134784  [134400/175341]\n",
      "loss: 0.613991  [136000/175341]\n",
      "loss: 0.209857  [137600/175341]\n",
      "loss: 0.417782  [139200/175341]\n",
      "loss: 0.466855  [140800/175341]\n",
      "loss: 0.480237  [142400/175341]\n",
      "loss: 0.305311  [144000/175341]\n",
      "loss: 0.476615  [145600/175341]\n",
      "loss: 0.393855  [147200/175341]\n",
      "loss: 0.266511  [148800/175341]\n",
      "loss: 0.560574  [150400/175341]\n",
      "loss: 0.370775  [152000/175341]\n",
      "loss: 0.674955  [153600/175341]\n",
      "loss: 0.689472  [155200/175341]\n",
      "loss: 1.036651  [156800/175341]\n",
      "loss: 0.697121  [158400/175341]\n",
      "loss: 0.434102  [160000/175341]\n",
      "loss: 0.608339  [161600/175341]\n",
      "loss: 0.486938  [163200/175341]\n",
      "loss: 0.920322  [164800/175341]\n",
      "loss: 0.425122  [166400/175341]\n",
      "loss: 0.138552  [168000/175341]\n",
      "loss: 0.471300  [169600/175341]\n",
      "loss: 0.849930  [171200/175341]\n",
      "loss: 0.570410  [172800/175341]\n",
      "loss: 0.887554  [174400/175341]\n",
      "Train Accuracy: 81.8913%\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.543332, F1-score: 77.30%, Macro_F1-Score:  42.27%  \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.440227  [    0/175341]\n",
      "loss: 0.676601  [ 1600/175341]\n",
      "loss: 0.220271  [ 3200/175341]\n",
      "loss: 0.215861  [ 4800/175341]\n",
      "loss: 0.336156  [ 6400/175341]\n",
      "loss: 0.249600  [ 8000/175341]\n",
      "loss: 0.215775  [ 9600/175341]\n",
      "loss: 0.249117  [11200/175341]\n",
      "loss: 0.174860  [12800/175341]\n",
      "loss: 0.675072  [14400/175341]\n",
      "loss: 0.667756  [16000/175341]\n",
      "loss: 0.698861  [17600/175341]\n",
      "loss: 0.402125  [19200/175341]\n",
      "loss: 0.804638  [20800/175341]\n",
      "loss: 0.331456  [22400/175341]\n",
      "loss: 0.514575  [24000/175341]\n",
      "loss: 0.262903  [25600/175341]\n",
      "loss: 0.588204  [27200/175341]\n",
      "loss: 0.392841  [28800/175341]\n",
      "loss: 0.271498  [30400/175341]\n",
      "loss: 0.231179  [32000/175341]\n",
      "loss: 0.228164  [33600/175341]\n",
      "loss: 0.274433  [35200/175341]\n",
      "loss: 0.749446  [36800/175341]\n",
      "loss: 0.362300  [38400/175341]\n",
      "loss: 0.387186  [40000/175341]\n",
      "loss: 0.268592  [41600/175341]\n",
      "loss: 0.273705  [43200/175341]\n",
      "loss: 0.378617  [44800/175341]\n",
      "loss: 0.292236  [46400/175341]\n",
      "loss: 0.642459  [48000/175341]\n",
      "loss: 0.836717  [49600/175341]\n",
      "loss: 0.546142  [51200/175341]\n",
      "loss: 0.186597  [52800/175341]\n",
      "loss: 0.498260  [54400/175341]\n",
      "loss: 0.699471  [56000/175341]\n",
      "loss: 0.236662  [57600/175341]\n",
      "loss: 0.304446  [59200/175341]\n",
      "loss: 0.729232  [60800/175341]\n",
      "loss: 0.486252  [62400/175341]\n",
      "loss: 0.505923  [64000/175341]\n",
      "loss: 0.695767  [65600/175341]\n",
      "loss: 0.181229  [67200/175341]\n",
      "loss: 0.456057  [68800/175341]\n",
      "loss: 0.361696  [70400/175341]\n",
      "loss: 0.411462  [72000/175341]\n",
      "loss: 0.500092  [73600/175341]\n",
      "loss: 0.551216  [75200/175341]\n",
      "loss: 0.578627  [76800/175341]\n",
      "loss: 0.095323  [78400/175341]\n",
      "loss: 0.164463  [80000/175341]\n",
      "loss: 0.384863  [81600/175341]\n",
      "loss: 0.709975  [83200/175341]\n",
      "loss: 0.269938  [84800/175341]\n",
      "loss: 0.164762  [86400/175341]\n",
      "loss: 0.164927  [88000/175341]\n",
      "loss: 0.517040  [89600/175341]\n",
      "loss: 0.649504  [91200/175341]\n",
      "loss: 0.387650  [92800/175341]\n",
      "loss: 0.176304  [94400/175341]\n",
      "loss: 0.318065  [96000/175341]\n",
      "loss: 0.279718  [97600/175341]\n",
      "loss: 0.393626  [99200/175341]\n",
      "loss: 1.079981  [100800/175341]\n",
      "loss: 0.325320  [102400/175341]\n",
      "loss: 0.411412  [104000/175341]\n",
      "loss: 0.422642  [105600/175341]\n",
      "loss: 0.798492  [107200/175341]\n",
      "loss: 0.463685  [108800/175341]\n",
      "loss: 0.229860  [110400/175341]\n",
      "loss: 0.679869  [112000/175341]\n",
      "loss: 0.722500  [113600/175341]\n",
      "loss: 0.635756  [115200/175341]\n",
      "loss: 0.221427  [116800/175341]\n",
      "loss: 0.267439  [118400/175341]\n",
      "loss: 0.566426  [120000/175341]\n",
      "loss: 0.435142  [121600/175341]\n",
      "loss: 0.260126  [123200/175341]\n",
      "loss: 0.283062  [124800/175341]\n",
      "loss: 0.583853  [126400/175341]\n",
      "loss: 0.560215  [128000/175341]\n",
      "loss: 0.726502  [129600/175341]\n",
      "loss: 0.268153  [131200/175341]\n",
      "loss: 0.158207  [132800/175341]\n",
      "loss: 0.357016  [134400/175341]\n",
      "loss: 0.867203  [136000/175341]\n",
      "loss: 0.632554  [137600/175341]\n",
      "loss: 0.308743  [139200/175341]\n",
      "loss: 0.366961  [140800/175341]\n",
      "loss: 0.580988  [142400/175341]\n",
      "loss: 0.531757  [144000/175341]\n",
      "loss: 0.745774  [145600/175341]\n",
      "loss: 0.393521  [147200/175341]\n",
      "loss: 0.247060  [148800/175341]\n",
      "loss: 0.828941  [150400/175341]\n",
      "loss: 0.471928  [152000/175341]\n",
      "loss: 0.303670  [153600/175341]\n",
      "loss: 0.654784  [155200/175341]\n",
      "loss: 0.417831  [156800/175341]\n",
      "loss: 0.360274  [158400/175341]\n",
      "loss: 0.504699  [160000/175341]\n",
      "loss: 0.305375  [161600/175341]\n",
      "loss: 0.331577  [163200/175341]\n",
      "loss: 0.520299  [164800/175341]\n",
      "loss: 0.559095  [166400/175341]\n",
      "loss: 0.760484  [168000/175341]\n",
      "loss: 0.243255  [169600/175341]\n",
      "loss: 0.251602  [171200/175341]\n",
      "loss: 0.334862  [172800/175341]\n",
      "loss: 0.556827  [174400/175341]\n",
      "Train Accuracy: 81.8970%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.571245, F1-score: 75.41%, Macro_F1-Score:  41.22%  \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.634268  [    0/175341]\n",
      "loss: 0.420474  [ 1600/175341]\n",
      "loss: 0.526589  [ 3200/175341]\n",
      "loss: 0.982582  [ 4800/175341]\n",
      "loss: 0.364975  [ 6400/175341]\n",
      "loss: 0.376477  [ 8000/175341]\n",
      "loss: 0.427503  [ 9600/175341]\n",
      "loss: 0.784378  [11200/175341]\n",
      "loss: 0.644482  [12800/175341]\n",
      "loss: 0.303622  [14400/175341]\n",
      "loss: 0.452337  [16000/175341]\n",
      "loss: 0.600962  [17600/175341]\n",
      "loss: 0.910641  [19200/175341]\n",
      "loss: 0.598701  [20800/175341]\n",
      "loss: 0.276595  [22400/175341]\n",
      "loss: 0.334887  [24000/175341]\n",
      "loss: 0.186984  [25600/175341]\n",
      "loss: 0.396027  [27200/175341]\n",
      "loss: 0.490517  [28800/175341]\n",
      "loss: 0.669965  [30400/175341]\n",
      "loss: 0.379462  [32000/175341]\n",
      "loss: 0.366500  [33600/175341]\n",
      "loss: 0.383770  [35200/175341]\n",
      "loss: 0.739960  [36800/175341]\n",
      "loss: 0.232466  [38400/175341]\n",
      "loss: 0.443652  [40000/175341]\n",
      "loss: 0.365960  [41600/175341]\n",
      "loss: 0.307893  [43200/175341]\n",
      "loss: 0.612987  [44800/175341]\n",
      "loss: 0.888096  [46400/175341]\n",
      "loss: 0.098354  [48000/175341]\n",
      "loss: 0.633728  [49600/175341]\n",
      "loss: 0.300177  [51200/175341]\n",
      "loss: 0.714496  [52800/175341]\n",
      "loss: 0.328012  [54400/175341]\n",
      "loss: 0.216495  [56000/175341]\n",
      "loss: 0.482056  [57600/175341]\n",
      "loss: 0.376915  [59200/175341]\n",
      "loss: 0.960188  [60800/175341]\n",
      "loss: 0.213108  [62400/175341]\n",
      "loss: 0.690768  [64000/175341]\n",
      "loss: 0.520706  [65600/175341]\n",
      "loss: 0.446967  [67200/175341]\n",
      "loss: 0.306121  [68800/175341]\n",
      "loss: 0.162689  [70400/175341]\n",
      "loss: 0.533842  [72000/175341]\n",
      "loss: 0.552228  [73600/175341]\n",
      "loss: 0.428624  [75200/175341]\n",
      "loss: 0.389937  [76800/175341]\n",
      "loss: 0.431897  [78400/175341]\n",
      "loss: 0.229324  [80000/175341]\n",
      "loss: 0.498582  [81600/175341]\n",
      "loss: 0.641680  [83200/175341]\n",
      "loss: 0.332644  [84800/175341]\n",
      "loss: 0.743035  [86400/175341]\n",
      "loss: 0.577072  [88000/175341]\n",
      "loss: 0.390218  [89600/175341]\n",
      "loss: 0.482393  [91200/175341]\n",
      "loss: 0.746480  [92800/175341]\n",
      "loss: 0.591616  [94400/175341]\n",
      "loss: 0.666072  [96000/175341]\n",
      "loss: 0.643111  [97600/175341]\n",
      "loss: 0.280088  [99200/175341]\n",
      "loss: 0.448979  [100800/175341]\n",
      "loss: 0.330261  [102400/175341]\n",
      "loss: 0.466085  [104000/175341]\n",
      "loss: 0.371858  [105600/175341]\n",
      "loss: 0.373772  [107200/175341]\n",
      "loss: 0.529884  [108800/175341]\n",
      "loss: 0.204030  [110400/175341]\n",
      "loss: 0.570540  [112000/175341]\n",
      "loss: 0.834583  [113600/175341]\n",
      "loss: 0.504250  [115200/175341]\n",
      "loss: 0.188509  [116800/175341]\n",
      "loss: 0.246841  [118400/175341]\n",
      "loss: 0.330858  [120000/175341]\n",
      "loss: 0.547883  [121600/175341]\n",
      "loss: 0.565421  [123200/175341]\n",
      "loss: 0.273818  [124800/175341]\n",
      "loss: 0.934933  [126400/175341]\n",
      "loss: 0.719041  [128000/175341]\n",
      "loss: 0.133124  [129600/175341]\n",
      "loss: 0.453342  [131200/175341]\n",
      "loss: 0.202420  [132800/175341]\n",
      "loss: 0.625285  [134400/175341]\n",
      "loss: 0.533539  [136000/175341]\n",
      "loss: 0.588399  [137600/175341]\n",
      "loss: 0.445459  [139200/175341]\n",
      "loss: 0.745138  [140800/175341]\n",
      "loss: 0.141375  [142400/175341]\n",
      "loss: 0.125150  [144000/175341]\n",
      "loss: 0.562934  [145600/175341]\n",
      "loss: 0.189360  [147200/175341]\n",
      "loss: 0.385419  [148800/175341]\n",
      "loss: 0.186770  [150400/175341]\n",
      "loss: 0.405463  [152000/175341]\n",
      "loss: 0.198341  [153600/175341]\n",
      "loss: 0.475364  [155200/175341]\n",
      "loss: 0.548713  [156800/175341]\n",
      "loss: 0.451796  [158400/175341]\n",
      "loss: 0.447823  [160000/175341]\n",
      "loss: 0.392521  [161600/175341]\n",
      "loss: 0.952679  [163200/175341]\n",
      "loss: 0.583463  [164800/175341]\n",
      "loss: 0.429195  [166400/175341]\n",
      "loss: 0.465621  [168000/175341]\n",
      "loss: 0.579008  [169600/175341]\n",
      "loss: 0.355792  [171200/175341]\n",
      "loss: 0.356012  [172800/175341]\n",
      "loss: 0.245054  [174400/175341]\n",
      "Train Accuracy: 81.8457%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.550451, F1-score: 76.70%, Macro_F1-Score:  42.40%  \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.540370  [    0/175341]\n",
      "loss: 0.410767  [ 1600/175341]\n",
      "loss: 0.209585  [ 3200/175341]\n",
      "loss: 0.750575  [ 4800/175341]\n",
      "loss: 0.459814  [ 6400/175341]\n",
      "loss: 0.200431  [ 8000/175341]\n",
      "loss: 0.257385  [ 9600/175341]\n",
      "loss: 0.164151  [11200/175341]\n",
      "loss: 0.178543  [12800/175341]\n",
      "loss: 0.487874  [14400/175341]\n",
      "loss: 0.302174  [16000/175341]\n",
      "loss: 0.354958  [17600/175341]\n",
      "loss: 0.742248  [19200/175341]\n",
      "loss: 0.600740  [20800/175341]\n",
      "loss: 0.511893  [22400/175341]\n",
      "loss: 0.125667  [24000/175341]\n",
      "loss: 0.265425  [25600/175341]\n",
      "loss: 0.390601  [27200/175341]\n",
      "loss: 0.582914  [28800/175341]\n",
      "loss: 0.591995  [30400/175341]\n",
      "loss: 0.533609  [32000/175341]\n",
      "loss: 0.624124  [33600/175341]\n",
      "loss: 0.452178  [35200/175341]\n",
      "loss: 0.019161  [36800/175341]\n",
      "loss: 0.347411  [38400/175341]\n",
      "loss: 0.595222  [40000/175341]\n",
      "loss: 0.429594  [41600/175341]\n",
      "loss: 0.264638  [43200/175341]\n",
      "loss: 0.401424  [44800/175341]\n",
      "loss: 0.268471  [46400/175341]\n",
      "loss: 0.283649  [48000/175341]\n",
      "loss: 0.365766  [49600/175341]\n",
      "loss: 0.731698  [51200/175341]\n",
      "loss: 0.155941  [52800/175341]\n",
      "loss: 0.350739  [54400/175341]\n",
      "loss: 0.534620  [56000/175341]\n",
      "loss: 0.518093  [57600/175341]\n",
      "loss: 0.487808  [59200/175341]\n",
      "loss: 0.352873  [60800/175341]\n",
      "loss: 0.379661  [62400/175341]\n",
      "loss: 0.215172  [64000/175341]\n",
      "loss: 0.896258  [65600/175341]\n",
      "loss: 0.460527  [67200/175341]\n",
      "loss: 0.596069  [68800/175341]\n",
      "loss: 0.357423  [70400/175341]\n",
      "loss: 0.393144  [72000/175341]\n",
      "loss: 0.301015  [73600/175341]\n",
      "loss: 0.156984  [75200/175341]\n",
      "loss: 0.424445  [76800/175341]\n",
      "loss: 0.595875  [78400/175341]\n",
      "loss: 0.248128  [80000/175341]\n",
      "loss: 0.166493  [81600/175341]\n",
      "loss: 0.213334  [83200/175341]\n",
      "loss: 0.569366  [84800/175341]\n",
      "loss: 0.394498  [86400/175341]\n",
      "loss: 0.513741  [88000/175341]\n",
      "loss: 0.361340  [89600/175341]\n",
      "loss: 0.484352  [91200/175341]\n",
      "loss: 0.648960  [92800/175341]\n",
      "loss: 0.153422  [94400/175341]\n",
      "loss: 0.255332  [96000/175341]\n",
      "loss: 0.513819  [97600/175341]\n",
      "loss: 0.163411  [99200/175341]\n",
      "loss: 0.386230  [100800/175341]\n",
      "loss: 0.439318  [102400/175341]\n",
      "loss: 0.782315  [104000/175341]\n",
      "loss: 0.264701  [105600/175341]\n",
      "loss: 0.457374  [107200/175341]\n",
      "loss: 1.362881  [108800/175341]\n",
      "loss: 0.211032  [110400/175341]\n",
      "loss: 0.493600  [112000/175341]\n",
      "loss: 0.245453  [113600/175341]\n",
      "loss: 0.208174  [115200/175341]\n",
      "loss: 0.692236  [116800/175341]\n",
      "loss: 0.781304  [118400/175341]\n",
      "loss: 0.367194  [120000/175341]\n",
      "loss: 0.614800  [121600/175341]\n",
      "loss: 0.173545  [123200/175341]\n",
      "loss: 0.543304  [124800/175341]\n",
      "loss: 0.407271  [126400/175341]\n",
      "loss: 0.349055  [128000/175341]\n",
      "loss: 0.741191  [129600/175341]\n",
      "loss: 0.176014  [131200/175341]\n",
      "loss: 0.339879  [132800/175341]\n",
      "loss: 0.291082  [134400/175341]\n",
      "loss: 0.499169  [136000/175341]\n",
      "loss: 0.745254  [137600/175341]\n",
      "loss: 0.764276  [139200/175341]\n",
      "loss: 0.315427  [140800/175341]\n",
      "loss: 0.472067  [142400/175341]\n",
      "loss: 0.576041  [144000/175341]\n",
      "loss: 0.783863  [145600/175341]\n",
      "loss: 0.372917  [147200/175341]\n",
      "loss: 0.227497  [148800/175341]\n",
      "loss: 0.638811  [150400/175341]\n",
      "loss: 0.178126  [152000/175341]\n",
      "loss: 0.208586  [153600/175341]\n",
      "loss: 0.654775  [155200/175341]\n",
      "loss: 0.257553  [156800/175341]\n",
      "loss: 0.429231  [158400/175341]\n",
      "loss: 0.457252  [160000/175341]\n",
      "loss: 0.568543  [161600/175341]\n",
      "loss: 0.436865  [163200/175341]\n",
      "loss: 0.277033  [164800/175341]\n",
      "loss: 0.408889  [166400/175341]\n",
      "loss: 0.566151  [168000/175341]\n",
      "loss: 0.198427  [169600/175341]\n",
      "loss: 0.338301  [171200/175341]\n",
      "loss: 0.260754  [172800/175341]\n",
      "loss: 0.478481  [174400/175341]\n",
      "Train Accuracy: 81.8816%\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.542178, F1-score: 77.26%, Macro_F1-Score:  42.76%  \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.295633  [    0/175341]\n",
      "loss: 0.459386  [ 1600/175341]\n",
      "loss: 0.297748  [ 3200/175341]\n",
      "loss: 0.562768  [ 4800/175341]\n",
      "loss: 0.358074  [ 6400/175341]\n",
      "loss: 0.357370  [ 8000/175341]\n",
      "loss: 0.464963  [ 9600/175341]\n",
      "loss: 0.405414  [11200/175341]\n",
      "loss: 0.498171  [12800/175341]\n",
      "loss: 0.251145  [14400/175341]\n",
      "loss: 0.261801  [16000/175341]\n",
      "loss: 0.204353  [17600/175341]\n",
      "loss: 0.375158  [19200/175341]\n",
      "loss: 0.551425  [20800/175341]\n",
      "loss: 0.407841  [22400/175341]\n",
      "loss: 0.758486  [24000/175341]\n",
      "loss: 0.324929  [25600/175341]\n",
      "loss: 0.170945  [27200/175341]\n",
      "loss: 0.418438  [28800/175341]\n",
      "loss: 0.437238  [30400/175341]\n",
      "loss: 0.264201  [32000/175341]\n",
      "loss: 0.270186  [33600/175341]\n",
      "loss: 0.512070  [35200/175341]\n",
      "loss: 0.212325  [36800/175341]\n",
      "loss: 0.527225  [38400/175341]\n",
      "loss: 0.525633  [40000/175341]\n",
      "loss: 1.009356  [41600/175341]\n",
      "loss: 0.317331  [43200/175341]\n",
      "loss: 0.624910  [44800/175341]\n",
      "loss: 0.593363  [46400/175341]\n",
      "loss: 0.568061  [48000/175341]\n",
      "loss: 0.355032  [49600/175341]\n",
      "loss: 0.589378  [51200/175341]\n",
      "loss: 0.417230  [52800/175341]\n",
      "loss: 0.290044  [54400/175341]\n",
      "loss: 0.237289  [56000/175341]\n",
      "loss: 0.545183  [57600/175341]\n",
      "loss: 0.341215  [59200/175341]\n",
      "loss: 0.270220  [60800/175341]\n",
      "loss: 0.265704  [62400/175341]\n",
      "loss: 0.803462  [64000/175341]\n",
      "loss: 0.394154  [65600/175341]\n",
      "loss: 0.518448  [67200/175341]\n",
      "loss: 0.333151  [68800/175341]\n",
      "loss: 0.232809  [70400/175341]\n",
      "loss: 0.300433  [72000/175341]\n",
      "loss: 0.146871  [73600/175341]\n",
      "loss: 0.276384  [75200/175341]\n",
      "loss: 0.716470  [76800/175341]\n",
      "loss: 0.513382  [78400/175341]\n",
      "loss: 0.760523  [80000/175341]\n",
      "loss: 0.309151  [81600/175341]\n",
      "loss: 0.419860  [83200/175341]\n",
      "loss: 0.331912  [84800/175341]\n",
      "loss: 0.585673  [86400/175341]\n",
      "loss: 0.245688  [88000/175341]\n",
      "loss: 0.229745  [89600/175341]\n",
      "loss: 0.964631  [91200/175341]\n",
      "loss: 0.297256  [92800/175341]\n",
      "loss: 0.366280  [94400/175341]\n",
      "loss: 0.670389  [96000/175341]\n",
      "loss: 0.429033  [97600/175341]\n",
      "loss: 0.186450  [99200/175341]\n",
      "loss: 0.216399  [100800/175341]\n",
      "loss: 0.367709  [102400/175341]\n",
      "loss: 0.222401  [104000/175341]\n",
      "loss: 0.494880  [105600/175341]\n",
      "loss: 0.387867  [107200/175341]\n",
      "loss: 0.166169  [108800/175341]\n",
      "loss: 0.443920  [110400/175341]\n",
      "loss: 0.611211  [112000/175341]\n",
      "loss: 0.361884  [113600/175341]\n",
      "loss: 0.238724  [115200/175341]\n",
      "loss: 0.394571  [116800/175341]\n",
      "loss: 0.452047  [118400/175341]\n",
      "loss: 0.357387  [120000/175341]\n",
      "loss: 0.100425  [121600/175341]\n",
      "loss: 0.545063  [123200/175341]\n",
      "loss: 0.695823  [124800/175341]\n",
      "loss: 0.752634  [126400/175341]\n",
      "loss: 0.391019  [128000/175341]\n",
      "loss: 0.160752  [129600/175341]\n",
      "loss: 0.520850  [131200/175341]\n",
      "loss: 0.256560  [132800/175341]\n",
      "loss: 0.181795  [134400/175341]\n",
      "loss: 0.418525  [136000/175341]\n",
      "loss: 0.320802  [137600/175341]\n",
      "loss: 0.208518  [139200/175341]\n",
      "loss: 0.764490  [140800/175341]\n",
      "loss: 0.342355  [142400/175341]\n",
      "loss: 0.462217  [144000/175341]\n",
      "loss: 0.509995  [145600/175341]\n",
      "loss: 0.723249  [147200/175341]\n",
      "loss: 0.223964  [148800/175341]\n",
      "loss: 0.460246  [150400/175341]\n",
      "loss: 0.639112  [152000/175341]\n",
      "loss: 0.239046  [153600/175341]\n",
      "loss: 0.324814  [155200/175341]\n",
      "loss: 0.261193  [156800/175341]\n",
      "loss: 0.540675  [158400/175341]\n",
      "loss: 0.281126  [160000/175341]\n",
      "loss: 0.293829  [161600/175341]\n",
      "loss: 0.691798  [163200/175341]\n",
      "loss: 0.482888  [164800/175341]\n",
      "loss: 1.307867  [166400/175341]\n",
      "loss: 0.317968  [168000/175341]\n",
      "loss: 0.327478  [169600/175341]\n",
      "loss: 0.311467  [171200/175341]\n",
      "loss: 0.550795  [172800/175341]\n",
      "loss: 0.570522  [174400/175341]\n",
      "Train Accuracy: 81.9489%\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.548316, F1-score: 76.19%, Macro_F1-Score:  41.99%  \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.152983  [    0/175341]\n",
      "loss: 0.537089  [ 1600/175341]\n",
      "loss: 0.286904  [ 3200/175341]\n",
      "loss: 0.358941  [ 4800/175341]\n",
      "loss: 0.545055  [ 6400/175341]\n",
      "loss: 0.414559  [ 8000/175341]\n",
      "loss: 0.665679  [ 9600/175341]\n",
      "loss: 0.460867  [11200/175341]\n",
      "loss: 0.484829  [12800/175341]\n",
      "loss: 0.581499  [14400/175341]\n",
      "loss: 0.334413  [16000/175341]\n",
      "loss: 0.628939  [17600/175341]\n",
      "loss: 0.931784  [19200/175341]\n",
      "loss: 0.453065  [20800/175341]\n",
      "loss: 0.273258  [22400/175341]\n",
      "loss: 0.208746  [24000/175341]\n",
      "loss: 0.145461  [25600/175341]\n",
      "loss: 0.266066  [27200/175341]\n",
      "loss: 0.218079  [28800/175341]\n",
      "loss: 0.408144  [30400/175341]\n",
      "loss: 0.715808  [32000/175341]\n",
      "loss: 0.721725  [33600/175341]\n",
      "loss: 1.052417  [35200/175341]\n",
      "loss: 0.330953  [36800/175341]\n",
      "loss: 0.452549  [38400/175341]\n",
      "loss: 0.493352  [40000/175341]\n",
      "loss: 0.447578  [41600/175341]\n",
      "loss: 0.239875  [43200/175341]\n",
      "loss: 0.764313  [44800/175341]\n",
      "loss: 1.019938  [46400/175341]\n",
      "loss: 0.195893  [48000/175341]\n",
      "loss: 0.256461  [49600/175341]\n",
      "loss: 0.351025  [51200/175341]\n",
      "loss: 0.587865  [52800/175341]\n",
      "loss: 0.269840  [54400/175341]\n",
      "loss: 0.289259  [56000/175341]\n",
      "loss: 1.064897  [57600/175341]\n",
      "loss: 0.281609  [59200/175341]\n",
      "loss: 0.711131  [60800/175341]\n",
      "loss: 0.635308  [62400/175341]\n",
      "loss: 0.338678  [64000/175341]\n",
      "loss: 0.513692  [65600/175341]\n",
      "loss: 0.534840  [67200/175341]\n",
      "loss: 0.394642  [68800/175341]\n",
      "loss: 0.682219  [70400/175341]\n",
      "loss: 0.606593  [72000/175341]\n",
      "loss: 0.369395  [73600/175341]\n",
      "loss: 0.496024  [75200/175341]\n",
      "loss: 0.261972  [76800/175341]\n",
      "loss: 0.507541  [78400/175341]\n",
      "loss: 0.130565  [80000/175341]\n",
      "loss: 0.156045  [81600/175341]\n",
      "loss: 0.555492  [83200/175341]\n",
      "loss: 0.580144  [84800/175341]\n",
      "loss: 0.364505  [86400/175341]\n",
      "loss: 0.676351  [88000/175341]\n",
      "loss: 0.638716  [89600/175341]\n",
      "loss: 1.087905  [91200/175341]\n",
      "loss: 0.412542  [92800/175341]\n",
      "loss: 0.118162  [94400/175341]\n",
      "loss: 0.586616  [96000/175341]\n",
      "loss: 0.457178  [97600/175341]\n",
      "loss: 0.650447  [99200/175341]\n",
      "loss: 0.191262  [100800/175341]\n",
      "loss: 0.386579  [102400/175341]\n",
      "loss: 0.076872  [104000/175341]\n",
      "loss: 0.625424  [105600/175341]\n",
      "loss: 0.334898  [107200/175341]\n",
      "loss: 0.444601  [108800/175341]\n",
      "loss: 0.418681  [110400/175341]\n",
      "loss: 0.528418  [112000/175341]\n",
      "loss: 0.446449  [113600/175341]\n",
      "loss: 0.301506  [115200/175341]\n",
      "loss: 0.533032  [116800/175341]\n",
      "loss: 0.098103  [118400/175341]\n",
      "loss: 0.221948  [120000/175341]\n",
      "loss: 0.476078  [121600/175341]\n",
      "loss: 0.716311  [123200/175341]\n",
      "loss: 0.541888  [124800/175341]\n",
      "loss: 0.453068  [126400/175341]\n",
      "loss: 0.681364  [128000/175341]\n",
      "loss: 0.304039  [129600/175341]\n",
      "loss: 0.451843  [131200/175341]\n",
      "loss: 0.318693  [132800/175341]\n",
      "loss: 0.557734  [134400/175341]\n",
      "loss: 0.408974  [136000/175341]\n",
      "loss: 0.484529  [137600/175341]\n",
      "loss: 0.121796  [139200/175341]\n",
      "loss: 0.450179  [140800/175341]\n",
      "loss: 0.623608  [142400/175341]\n",
      "loss: 0.373692  [144000/175341]\n",
      "loss: 0.250465  [145600/175341]\n",
      "loss: 0.437220  [147200/175341]\n",
      "loss: 0.441030  [148800/175341]\n",
      "loss: 0.511603  [150400/175341]\n",
      "loss: 0.592250  [152000/175341]\n",
      "loss: 1.055716  [153600/175341]\n",
      "loss: 0.471739  [155200/175341]\n",
      "loss: 0.133275  [156800/175341]\n",
      "loss: 0.783418  [158400/175341]\n",
      "loss: 0.510319  [160000/175341]\n",
      "loss: 0.243239  [161600/175341]\n",
      "loss: 0.351044  [163200/175341]\n",
      "loss: 0.367302  [164800/175341]\n",
      "loss: 0.481183  [166400/175341]\n",
      "loss: 0.210894  [168000/175341]\n",
      "loss: 0.484309  [169600/175341]\n",
      "loss: 0.392560  [171200/175341]\n",
      "loss: 0.290519  [172800/175341]\n",
      "loss: 0.258291  [174400/175341]\n",
      "Train Accuracy: 81.9329%\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.536765, F1-score: 77.72%, Macro_F1-Score:  43.21%  \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.175747  [    0/175341]\n",
      "loss: 0.705736  [ 1600/175341]\n",
      "loss: 0.294802  [ 3200/175341]\n",
      "loss: 0.544267  [ 4800/175341]\n",
      "loss: 0.404395  [ 6400/175341]\n",
      "loss: 0.679822  [ 8000/175341]\n",
      "loss: 0.153279  [ 9600/175341]\n",
      "loss: 0.389151  [11200/175341]\n",
      "loss: 0.337465  [12800/175341]\n",
      "loss: 0.645901  [14400/175341]\n",
      "loss: 0.179904  [16000/175341]\n",
      "loss: 0.705101  [17600/175341]\n",
      "loss: 0.569544  [19200/175341]\n",
      "loss: 0.541669  [20800/175341]\n",
      "loss: 0.305595  [22400/175341]\n",
      "loss: 0.210371  [24000/175341]\n",
      "loss: 0.384746  [25600/175341]\n",
      "loss: 0.604975  [27200/175341]\n",
      "loss: 0.150443  [28800/175341]\n",
      "loss: 0.438055  [30400/175341]\n",
      "loss: 0.446232  [32000/175341]\n",
      "loss: 0.308016  [33600/175341]\n",
      "loss: 0.148484  [35200/175341]\n",
      "loss: 0.346458  [36800/175341]\n",
      "loss: 0.475093  [38400/175341]\n",
      "loss: 0.276887  [40000/175341]\n",
      "loss: 0.210035  [41600/175341]\n",
      "loss: 0.509853  [43200/175341]\n",
      "loss: 0.405476  [44800/175341]\n",
      "loss: 0.412437  [46400/175341]\n",
      "loss: 0.747629  [48000/175341]\n",
      "loss: 0.488868  [49600/175341]\n",
      "loss: 0.168552  [51200/175341]\n",
      "loss: 0.242558  [52800/175341]\n",
      "loss: 0.369182  [54400/175341]\n",
      "loss: 0.245059  [56000/175341]\n",
      "loss: 0.341653  [57600/175341]\n",
      "loss: 0.672129  [59200/175341]\n",
      "loss: 0.498948  [60800/175341]\n",
      "loss: 0.493876  [62400/175341]\n",
      "loss: 0.344997  [64000/175341]\n",
      "loss: 0.325318  [65600/175341]\n",
      "loss: 0.271963  [67200/175341]\n",
      "loss: 0.261780  [68800/175341]\n",
      "loss: 0.609916  [70400/175341]\n",
      "loss: 0.674664  [72000/175341]\n",
      "loss: 0.134579  [73600/175341]\n",
      "loss: 0.274946  [75200/175341]\n",
      "loss: 0.819508  [76800/175341]\n",
      "loss: 0.403098  [78400/175341]\n",
      "loss: 0.627787  [80000/175341]\n",
      "loss: 0.398465  [81600/175341]\n",
      "loss: 0.323910  [83200/175341]\n",
      "loss: 0.236190  [84800/175341]\n",
      "loss: 0.480721  [86400/175341]\n",
      "loss: 0.247014  [88000/175341]\n",
      "loss: 0.469481  [89600/175341]\n",
      "loss: 0.500354  [91200/175341]\n",
      "loss: 0.237789  [92800/175341]\n",
      "loss: 0.966624  [94400/175341]\n",
      "loss: 0.470827  [96000/175341]\n",
      "loss: 0.541216  [97600/175341]\n",
      "loss: 0.245177  [99200/175341]\n",
      "loss: 0.160530  [100800/175341]\n",
      "loss: 0.606583  [102400/175341]\n",
      "loss: 0.415299  [104000/175341]\n",
      "loss: 0.351999  [105600/175341]\n",
      "loss: 0.178838  [107200/175341]\n",
      "loss: 0.370326  [108800/175341]\n",
      "loss: 0.608812  [110400/175341]\n",
      "loss: 0.493665  [112000/175341]\n",
      "loss: 0.600359  [113600/175341]\n",
      "loss: 0.584399  [115200/175341]\n",
      "loss: 0.440214  [116800/175341]\n",
      "loss: 0.152752  [118400/175341]\n",
      "loss: 0.560101  [120000/175341]\n",
      "loss: 0.467296  [121600/175341]\n",
      "loss: 0.626253  [123200/175341]\n",
      "loss: 0.547500  [124800/175341]\n",
      "loss: 0.140421  [126400/175341]\n",
      "loss: 0.316481  [128000/175341]\n",
      "loss: 0.915593  [129600/175341]\n",
      "loss: 0.180292  [131200/175341]\n",
      "loss: 0.139185  [132800/175341]\n",
      "loss: 0.298635  [134400/175341]\n",
      "loss: 0.448124  [136000/175341]\n",
      "loss: 0.443028  [137600/175341]\n",
      "loss: 0.635253  [139200/175341]\n",
      "loss: 0.554001  [140800/175341]\n",
      "loss: 0.341465  [142400/175341]\n",
      "loss: 0.483377  [144000/175341]\n",
      "loss: 0.254075  [145600/175341]\n",
      "loss: 0.928576  [147200/175341]\n",
      "loss: 0.305154  [148800/175341]\n",
      "loss: 0.509038  [150400/175341]\n",
      "loss: 0.193413  [152000/175341]\n",
      "loss: 0.436299  [153600/175341]\n",
      "loss: 0.286603  [155200/175341]\n",
      "loss: 0.407870  [156800/175341]\n",
      "loss: 0.131387  [158400/175341]\n",
      "loss: 0.169113  [160000/175341]\n",
      "loss: 0.681623  [161600/175341]\n",
      "loss: 0.198106  [163200/175341]\n",
      "loss: 0.532488  [164800/175341]\n",
      "loss: 0.465761  [166400/175341]\n",
      "loss: 0.599875  [168000/175341]\n",
      "loss: 0.223704  [169600/175341]\n",
      "loss: 0.227885  [171200/175341]\n",
      "loss: 0.570022  [172800/175341]\n",
      "loss: 0.523283  [174400/175341]\n",
      "Train Accuracy: 81.9181%\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.528720, F1-score: 77.75%, Macro_F1-Score:  42.28%  \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.449396  [    0/175341]\n",
      "loss: 0.252801  [ 1600/175341]\n",
      "loss: 0.396511  [ 3200/175341]\n",
      "loss: 0.448374  [ 4800/175341]\n",
      "loss: 0.230676  [ 6400/175341]\n",
      "loss: 0.604962  [ 8000/175341]\n",
      "loss: 0.289927  [ 9600/175341]\n",
      "loss: 0.782752  [11200/175341]\n",
      "loss: 0.112847  [12800/175341]\n",
      "loss: 0.532928  [14400/175341]\n",
      "loss: 0.088028  [16000/175341]\n",
      "loss: 0.567631  [17600/175341]\n",
      "loss: 0.738811  [19200/175341]\n",
      "loss: 0.400342  [20800/175341]\n",
      "loss: 0.301678  [22400/175341]\n",
      "loss: 0.376429  [24000/175341]\n",
      "loss: 0.665784  [25600/175341]\n",
      "loss: 0.291243  [27200/175341]\n",
      "loss: 0.463439  [28800/175341]\n",
      "loss: 0.449140  [30400/175341]\n",
      "loss: 0.537013  [32000/175341]\n",
      "loss: 0.436198  [33600/175341]\n",
      "loss: 0.440863  [35200/175341]\n",
      "loss: 0.747601  [36800/175341]\n",
      "loss: 0.689296  [38400/175341]\n",
      "loss: 0.398463  [40000/175341]\n",
      "loss: 0.533993  [41600/175341]\n",
      "loss: 0.604443  [43200/175341]\n",
      "loss: 0.385654  [44800/175341]\n",
      "loss: 0.729865  [46400/175341]\n",
      "loss: 1.104943  [48000/175341]\n",
      "loss: 0.466081  [49600/175341]\n",
      "loss: 0.722970  [51200/175341]\n",
      "loss: 0.516156  [52800/175341]\n",
      "loss: 0.659042  [54400/175341]\n",
      "loss: 0.375921  [56000/175341]\n",
      "loss: 0.675594  [57600/175341]\n",
      "loss: 1.000884  [59200/175341]\n",
      "loss: 0.562337  [60800/175341]\n",
      "loss: 0.563885  [62400/175341]\n",
      "loss: 0.457179  [64000/175341]\n",
      "loss: 0.238658  [65600/175341]\n",
      "loss: 0.499722  [67200/175341]\n",
      "loss: 0.444222  [68800/175341]\n",
      "loss: 0.219572  [70400/175341]\n",
      "loss: 0.524235  [72000/175341]\n",
      "loss: 0.209130  [73600/175341]\n",
      "loss: 0.454309  [75200/175341]\n",
      "loss: 0.419780  [76800/175341]\n",
      "loss: 0.303507  [78400/175341]\n",
      "loss: 0.445559  [80000/175341]\n",
      "loss: 0.594344  [81600/175341]\n",
      "loss: 0.497779  [83200/175341]\n",
      "loss: 0.776294  [84800/175341]\n",
      "loss: 0.227313  [86400/175341]\n",
      "loss: 0.314694  [88000/175341]\n",
      "loss: 0.404532  [89600/175341]\n",
      "loss: 0.561228  [91200/175341]\n",
      "loss: 0.420103  [92800/175341]\n",
      "loss: 1.030005  [94400/175341]\n",
      "loss: 0.211157  [96000/175341]\n",
      "loss: 0.291698  [97600/175341]\n",
      "loss: 0.387458  [99200/175341]\n",
      "loss: 0.405901  [100800/175341]\n",
      "loss: 0.406449  [102400/175341]\n",
      "loss: 1.002282  [104000/175341]\n",
      "loss: 0.530919  [105600/175341]\n",
      "loss: 0.448876  [107200/175341]\n",
      "loss: 0.496884  [108800/175341]\n",
      "loss: 0.551453  [110400/175341]\n",
      "loss: 0.460642  [112000/175341]\n",
      "loss: 0.521480  [113600/175341]\n",
      "loss: 0.281543  [115200/175341]\n",
      "loss: 0.690662  [116800/175341]\n",
      "loss: 0.498221  [118400/175341]\n",
      "loss: 0.504698  [120000/175341]\n",
      "loss: 0.343838  [121600/175341]\n",
      "loss: 0.651001  [123200/175341]\n",
      "loss: 0.370983  [124800/175341]\n",
      "loss: 0.862377  [126400/175341]\n",
      "loss: 0.331358  [128000/175341]\n",
      "loss: 0.266205  [129600/175341]\n",
      "loss: 0.626545  [131200/175341]\n",
      "loss: 0.212669  [132800/175341]\n",
      "loss: 0.535711  [134400/175341]\n",
      "loss: 0.689025  [136000/175341]\n",
      "loss: 0.446168  [137600/175341]\n",
      "loss: 0.498093  [139200/175341]\n",
      "loss: 0.317254  [140800/175341]\n",
      "loss: 0.280229  [142400/175341]\n",
      "loss: 0.671087  [144000/175341]\n",
      "loss: 0.143195  [145600/175341]\n",
      "loss: 0.485993  [147200/175341]\n",
      "loss: 0.135513  [148800/175341]\n",
      "loss: 0.750948  [150400/175341]\n",
      "loss: 0.904523  [152000/175341]\n",
      "loss: 0.496199  [153600/175341]\n",
      "loss: 0.601442  [155200/175341]\n",
      "loss: 0.211603  [156800/175341]\n",
      "loss: 0.667202  [158400/175341]\n",
      "loss: 0.895763  [160000/175341]\n",
      "loss: 0.610123  [161600/175341]\n",
      "loss: 0.326285  [163200/175341]\n",
      "loss: 0.806183  [164800/175341]\n",
      "loss: 0.250223  [166400/175341]\n",
      "loss: 0.218822  [168000/175341]\n",
      "loss: 0.430195  [169600/175341]\n",
      "loss: 0.350425  [171200/175341]\n",
      "loss: 0.603947  [172800/175341]\n",
      "loss: 0.330718  [174400/175341]\n",
      "Train Accuracy: 81.8531%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.544530, F1-score: 76.59%, Macro_F1-Score:  41.79%  \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.193814  [    0/175341]\n",
      "loss: 0.872310  [ 1600/175341]\n",
      "loss: 0.108869  [ 3200/175341]\n",
      "loss: 0.319653  [ 4800/175341]\n",
      "loss: 0.675918  [ 6400/175341]\n",
      "loss: 1.053818  [ 8000/175341]\n",
      "loss: 0.144990  [ 9600/175341]\n",
      "loss: 0.536135  [11200/175341]\n",
      "loss: 0.364232  [12800/175341]\n",
      "loss: 0.288882  [14400/175341]\n",
      "loss: 0.753512  [16000/175341]\n",
      "loss: 0.707064  [17600/175341]\n",
      "loss: 0.160683  [19200/175341]\n",
      "loss: 0.233022  [20800/175341]\n",
      "loss: 0.493858  [22400/175341]\n",
      "loss: 0.255322  [24000/175341]\n",
      "loss: 0.403787  [25600/175341]\n",
      "loss: 0.265624  [27200/175341]\n",
      "loss: 0.303311  [28800/175341]\n",
      "loss: 0.766059  [30400/175341]\n",
      "loss: 0.427213  [32000/175341]\n",
      "loss: 0.453472  [33600/175341]\n",
      "loss: 0.536584  [35200/175341]\n",
      "loss: 0.381909  [36800/175341]\n",
      "loss: 0.351338  [38400/175341]\n",
      "loss: 0.425985  [40000/175341]\n",
      "loss: 0.684333  [41600/175341]\n",
      "loss: 0.297426  [43200/175341]\n",
      "loss: 0.427006  [44800/175341]\n",
      "loss: 0.370408  [46400/175341]\n",
      "loss: 0.225609  [48000/175341]\n",
      "loss: 1.000237  [49600/175341]\n",
      "loss: 0.556101  [51200/175341]\n",
      "loss: 0.195222  [52800/175341]\n",
      "loss: 0.179606  [54400/175341]\n",
      "loss: 0.460858  [56000/175341]\n",
      "loss: 0.554154  [57600/175341]\n",
      "loss: 0.185693  [59200/175341]\n",
      "loss: 0.399767  [60800/175341]\n",
      "loss: 0.287457  [62400/175341]\n",
      "loss: 0.500598  [64000/175341]\n",
      "loss: 0.700067  [65600/175341]\n",
      "loss: 0.605551  [67200/175341]\n",
      "loss: 0.561962  [68800/175341]\n",
      "loss: 0.533898  [70400/175341]\n",
      "loss: 0.175352  [72000/175341]\n",
      "loss: 0.484630  [73600/175341]\n",
      "loss: 0.676086  [75200/175341]\n",
      "loss: 0.674837  [76800/175341]\n",
      "loss: 0.272492  [78400/175341]\n",
      "loss: 0.202832  [80000/175341]\n",
      "loss: 0.237075  [81600/175341]\n",
      "loss: 0.308532  [83200/175341]\n",
      "loss: 0.302299  [84800/175341]\n",
      "loss: 0.370655  [86400/175341]\n",
      "loss: 0.472933  [88000/175341]\n",
      "loss: 0.279977  [89600/175341]\n",
      "loss: 0.217016  [91200/175341]\n",
      "loss: 0.127577  [92800/175341]\n",
      "loss: 0.506969  [94400/175341]\n",
      "loss: 0.324000  [96000/175341]\n",
      "loss: 0.396688  [97600/175341]\n",
      "loss: 0.158543  [99200/175341]\n",
      "loss: 0.895265  [100800/175341]\n",
      "loss: 0.692501  [102400/175341]\n",
      "loss: 0.374442  [104000/175341]\n",
      "loss: 0.254476  [105600/175341]\n",
      "loss: 0.344368  [107200/175341]\n",
      "loss: 0.542368  [108800/175341]\n",
      "loss: 0.390981  [110400/175341]\n",
      "loss: 0.362196  [112000/175341]\n",
      "loss: 0.567488  [113600/175341]\n",
      "loss: 0.147921  [115200/175341]\n",
      "loss: 0.432023  [116800/175341]\n",
      "loss: 0.567815  [118400/175341]\n",
      "loss: 0.539201  [120000/175341]\n",
      "loss: 0.656126  [121600/175341]\n",
      "loss: 0.543810  [123200/175341]\n",
      "loss: 0.171258  [124800/175341]\n",
      "loss: 0.516727  [126400/175341]\n",
      "loss: 0.683542  [128000/175341]\n",
      "loss: 0.234412  [129600/175341]\n",
      "loss: 0.437107  [131200/175341]\n",
      "loss: 0.499483  [132800/175341]\n",
      "loss: 0.400507  [134400/175341]\n",
      "loss: 0.816025  [136000/175341]\n",
      "loss: 0.302882  [137600/175341]\n",
      "loss: 0.395765  [139200/175341]\n",
      "loss: 0.622515  [140800/175341]\n",
      "loss: 0.177515  [142400/175341]\n",
      "loss: 0.404041  [144000/175341]\n",
      "loss: 0.212796  [145600/175341]\n",
      "loss: 0.475627  [147200/175341]\n",
      "loss: 0.718442  [148800/175341]\n",
      "loss: 0.550139  [150400/175341]\n",
      "loss: 0.577974  [152000/175341]\n",
      "loss: 0.310232  [153600/175341]\n",
      "loss: 0.357374  [155200/175341]\n",
      "loss: 0.666877  [156800/175341]\n",
      "loss: 0.370945  [158400/175341]\n",
      "loss: 0.506147  [160000/175341]\n",
      "loss: 0.537735  [161600/175341]\n",
      "loss: 0.639142  [163200/175341]\n",
      "loss: 0.286115  [164800/175341]\n",
      "loss: 0.763684  [166400/175341]\n",
      "loss: 0.556379  [168000/175341]\n",
      "loss: 0.632330  [169600/175341]\n",
      "loss: 0.492098  [171200/175341]\n",
      "loss: 0.267149  [172800/175341]\n",
      "loss: 0.385538  [174400/175341]\n",
      "Train Accuracy: 81.8747%\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.543701, F1-score: 77.12%, Macro_F1-Score:  42.07%  \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.317653  [    0/175341]\n",
      "loss: 0.527830  [ 1600/175341]\n",
      "loss: 0.201635  [ 3200/175341]\n",
      "loss: 0.305163  [ 4800/175341]\n",
      "loss: 0.411353  [ 6400/175341]\n",
      "loss: 0.250161  [ 8000/175341]\n",
      "loss: 0.451998  [ 9600/175341]\n",
      "loss: 0.281503  [11200/175341]\n",
      "loss: 0.470128  [12800/175341]\n",
      "loss: 0.202651  [14400/175341]\n",
      "loss: 0.284046  [16000/175341]\n",
      "loss: 0.660207  [17600/175341]\n",
      "loss: 0.296258  [19200/175341]\n",
      "loss: 0.294279  [20800/175341]\n",
      "loss: 0.407960  [22400/175341]\n",
      "loss: 0.488665  [24000/175341]\n",
      "loss: 0.135741  [25600/175341]\n",
      "loss: 0.316605  [27200/175341]\n",
      "loss: 0.371819  [28800/175341]\n",
      "loss: 0.482310  [30400/175341]\n",
      "loss: 0.293680  [32000/175341]\n",
      "loss: 0.386933  [33600/175341]\n",
      "loss: 0.165693  [35200/175341]\n",
      "loss: 0.197136  [36800/175341]\n",
      "loss: 0.537600  [38400/175341]\n",
      "loss: 0.259805  [40000/175341]\n",
      "loss: 0.361918  [41600/175341]\n",
      "loss: 0.193521  [43200/175341]\n",
      "loss: 0.298429  [44800/175341]\n",
      "loss: 0.441878  [46400/175341]\n",
      "loss: 0.477431  [48000/175341]\n",
      "loss: 0.534802  [49600/175341]\n",
      "loss: 0.737823  [51200/175341]\n",
      "loss: 0.273803  [52800/175341]\n",
      "loss: 0.328002  [54400/175341]\n",
      "loss: 0.497079  [56000/175341]\n",
      "loss: 0.503264  [57600/175341]\n",
      "loss: 0.334319  [59200/175341]\n",
      "loss: 1.176679  [60800/175341]\n",
      "loss: 0.515140  [62400/175341]\n",
      "loss: 0.252171  [64000/175341]\n",
      "loss: 0.433084  [65600/175341]\n",
      "loss: 0.240605  [67200/175341]\n",
      "loss: 0.516223  [68800/175341]\n",
      "loss: 0.248518  [70400/175341]\n",
      "loss: 0.571118  [72000/175341]\n",
      "loss: 0.435313  [73600/175341]\n",
      "loss: 0.720609  [75200/175341]\n",
      "loss: 0.566299  [76800/175341]\n",
      "loss: 0.545255  [78400/175341]\n",
      "loss: 0.594198  [80000/175341]\n",
      "loss: 0.643942  [81600/175341]\n",
      "loss: 0.603874  [83200/175341]\n",
      "loss: 0.289882  [84800/175341]\n",
      "loss: 0.203428  [86400/175341]\n",
      "loss: 0.562236  [88000/175341]\n",
      "loss: 0.848374  [89600/175341]\n",
      "loss: 0.521686  [91200/175341]\n",
      "loss: 0.770634  [92800/175341]\n",
      "loss: 0.689129  [94400/175341]\n",
      "loss: 0.443418  [96000/175341]\n",
      "loss: 0.373548  [97600/175341]\n",
      "loss: 0.509821  [99200/175341]\n",
      "loss: 0.828201  [100800/175341]\n",
      "loss: 0.485826  [102400/175341]\n",
      "loss: 0.574531  [104000/175341]\n",
      "loss: 0.375135  [105600/175341]\n",
      "loss: 0.130568  [107200/175341]\n",
      "loss: 0.270852  [108800/175341]\n",
      "loss: 0.549219  [110400/175341]\n",
      "loss: 0.472969  [112000/175341]\n",
      "loss: 0.255872  [113600/175341]\n",
      "loss: 0.329720  [115200/175341]\n",
      "loss: 0.246927  [116800/175341]\n",
      "loss: 0.437716  [118400/175341]\n",
      "loss: 0.379716  [120000/175341]\n",
      "loss: 0.093611  [121600/175341]\n",
      "loss: 0.655443  [123200/175341]\n",
      "loss: 0.385871  [124800/175341]\n",
      "loss: 0.371426  [126400/175341]\n",
      "loss: 0.591584  [128000/175341]\n",
      "loss: 0.573119  [129600/175341]\n",
      "loss: 0.441670  [131200/175341]\n",
      "loss: 0.440064  [132800/175341]\n",
      "loss: 0.490308  [134400/175341]\n",
      "loss: 0.801533  [136000/175341]\n",
      "loss: 0.455367  [137600/175341]\n",
      "loss: 0.585233  [139200/175341]\n",
      "loss: 0.370922  [140800/175341]\n",
      "loss: 0.241805  [142400/175341]\n",
      "loss: 0.675315  [144000/175341]\n",
      "loss: 0.209784  [145600/175341]\n",
      "loss: 0.927266  [147200/175341]\n",
      "loss: 0.515077  [148800/175341]\n",
      "loss: 0.287477  [150400/175341]\n",
      "loss: 0.770271  [152000/175341]\n",
      "loss: 0.460129  [153600/175341]\n",
      "loss: 0.635229  [155200/175341]\n",
      "loss: 0.244870  [156800/175341]\n",
      "loss: 0.480726  [158400/175341]\n",
      "loss: 1.067433  [160000/175341]\n",
      "loss: 0.375263  [161600/175341]\n",
      "loss: 0.131745  [163200/175341]\n",
      "loss: 0.677143  [164800/175341]\n",
      "loss: 0.467305  [166400/175341]\n",
      "loss: 0.401912  [168000/175341]\n",
      "loss: 0.510659  [169600/175341]\n",
      "loss: 0.170034  [171200/175341]\n",
      "loss: 0.941226  [172800/175341]\n",
      "loss: 0.601630  [174400/175341]\n",
      "Train Accuracy: 81.9312%\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.547754, F1-score: 76.27%, Macro_F1-Score:  41.74%  \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.162874  [    0/175341]\n",
      "loss: 0.203564  [ 1600/175341]\n",
      "loss: 0.948631  [ 3200/175341]\n",
      "loss: 0.470765  [ 4800/175341]\n",
      "loss: 0.363796  [ 6400/175341]\n",
      "loss: 0.284014  [ 8000/175341]\n",
      "loss: 0.808219  [ 9600/175341]\n",
      "loss: 0.929448  [11200/175341]\n",
      "loss: 0.212906  [12800/175341]\n",
      "loss: 0.142396  [14400/175341]\n",
      "loss: 0.271981  [16000/175341]\n",
      "loss: 0.367038  [17600/175341]\n",
      "loss: 0.572993  [19200/175341]\n",
      "loss: 0.935480  [20800/175341]\n",
      "loss: 0.237230  [22400/175341]\n",
      "loss: 0.810671  [24000/175341]\n",
      "loss: 0.427784  [25600/175341]\n",
      "loss: 0.230983  [27200/175341]\n",
      "loss: 0.201336  [28800/175341]\n",
      "loss: 0.618003  [30400/175341]\n",
      "loss: 0.575890  [32000/175341]\n",
      "loss: 0.235591  [33600/175341]\n",
      "loss: 0.144123  [35200/175341]\n",
      "loss: 0.915231  [36800/175341]\n",
      "loss: 0.655357  [38400/175341]\n",
      "loss: 0.304958  [40000/175341]\n",
      "loss: 0.296110  [41600/175341]\n",
      "loss: 0.559616  [43200/175341]\n",
      "loss: 0.357206  [44800/175341]\n",
      "loss: 0.298704  [46400/175341]\n",
      "loss: 0.341941  [48000/175341]\n",
      "loss: 0.608263  [49600/175341]\n",
      "loss: 0.332395  [51200/175341]\n",
      "loss: 0.404967  [52800/175341]\n",
      "loss: 0.766152  [54400/175341]\n",
      "loss: 0.548971  [56000/175341]\n",
      "loss: 0.472986  [57600/175341]\n",
      "loss: 0.434079  [59200/175341]\n",
      "loss: 0.134413  [60800/175341]\n",
      "loss: 0.546224  [62400/175341]\n",
      "loss: 0.824206  [64000/175341]\n",
      "loss: 0.696930  [65600/175341]\n",
      "loss: 0.579059  [67200/175341]\n",
      "loss: 0.314640  [68800/175341]\n",
      "loss: 0.661202  [70400/175341]\n",
      "loss: 0.610657  [72000/175341]\n",
      "loss: 0.369052  [73600/175341]\n",
      "loss: 0.358957  [75200/175341]\n",
      "loss: 0.216089  [76800/175341]\n",
      "loss: 0.337597  [78400/175341]\n",
      "loss: 0.426984  [80000/175341]\n",
      "loss: 0.374881  [81600/175341]\n",
      "loss: 0.225351  [83200/175341]\n",
      "loss: 0.524168  [84800/175341]\n",
      "loss: 0.349723  [86400/175341]\n",
      "loss: 0.453953  [88000/175341]\n",
      "loss: 0.472253  [89600/175341]\n",
      "loss: 0.545805  [91200/175341]\n",
      "loss: 0.657570  [92800/175341]\n",
      "loss: 0.723351  [94400/175341]\n",
      "loss: 0.502711  [96000/175341]\n",
      "loss: 0.732822  [97600/175341]\n",
      "loss: 0.667215  [99200/175341]\n",
      "loss: 0.377169  [100800/175341]\n",
      "loss: 0.623569  [102400/175341]\n",
      "loss: 0.380550  [104000/175341]\n",
      "loss: 0.520331  [105600/175341]\n",
      "loss: 0.640848  [107200/175341]\n",
      "loss: 0.262589  [108800/175341]\n",
      "loss: 0.389229  [110400/175341]\n",
      "loss: 0.228167  [112000/175341]\n",
      "loss: 0.707179  [113600/175341]\n",
      "loss: 0.301576  [115200/175341]\n",
      "loss: 0.698219  [116800/175341]\n",
      "loss: 0.198887  [118400/175341]\n",
      "loss: 0.523525  [120000/175341]\n",
      "loss: 0.541738  [121600/175341]\n",
      "loss: 0.359357  [123200/175341]\n",
      "loss: 0.047297  [124800/175341]\n",
      "loss: 0.544886  [126400/175341]\n",
      "loss: 0.419391  [128000/175341]\n",
      "loss: 0.364458  [129600/175341]\n",
      "loss: 0.828542  [131200/175341]\n",
      "loss: 0.414352  [132800/175341]\n",
      "loss: 0.230390  [134400/175341]\n",
      "loss: 0.554348  [136000/175341]\n",
      "loss: 0.367231  [137600/175341]\n",
      "loss: 0.342305  [139200/175341]\n",
      "loss: 0.559717  [140800/175341]\n",
      "loss: 0.749782  [142400/175341]\n",
      "loss: 0.619731  [144000/175341]\n",
      "loss: 0.540167  [145600/175341]\n",
      "loss: 0.259679  [147200/175341]\n",
      "loss: 0.158037  [148800/175341]\n",
      "loss: 0.673761  [150400/175341]\n",
      "loss: 0.057390  [152000/175341]\n",
      "loss: 0.323558  [153600/175341]\n",
      "loss: 0.540803  [155200/175341]\n",
      "loss: 0.705753  [156800/175341]\n",
      "loss: 0.723011  [158400/175341]\n",
      "loss: 0.526864  [160000/175341]\n",
      "loss: 0.577149  [161600/175341]\n",
      "loss: 0.558200  [163200/175341]\n",
      "loss: 0.332498  [164800/175341]\n",
      "loss: 0.609364  [166400/175341]\n",
      "loss: 0.429609  [168000/175341]\n",
      "loss: 0.579515  [169600/175341]\n",
      "loss: 0.265861  [171200/175341]\n",
      "loss: 0.257937  [172800/175341]\n",
      "loss: 0.614065  [174400/175341]\n",
      "Train Accuracy: 81.9050%\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.538318, F1-score: 77.02%, Macro_F1-Score:  42.11%  \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.402103  [    0/175341]\n",
      "loss: 0.475578  [ 1600/175341]\n",
      "loss: 0.200623  [ 3200/175341]\n",
      "loss: 0.983476  [ 4800/175341]\n",
      "loss: 0.437945  [ 6400/175341]\n",
      "loss: 0.809588  [ 8000/175341]\n",
      "loss: 0.609856  [ 9600/175341]\n",
      "loss: 0.540177  [11200/175341]\n",
      "loss: 0.212075  [12800/175341]\n",
      "loss: 0.457548  [14400/175341]\n",
      "loss: 0.162664  [16000/175341]\n",
      "loss: 0.305862  [17600/175341]\n",
      "loss: 0.188880  [19200/175341]\n",
      "loss: 0.230316  [20800/175341]\n",
      "loss: 0.424333  [22400/175341]\n",
      "loss: 0.292049  [24000/175341]\n",
      "loss: 0.485127  [25600/175341]\n",
      "loss: 0.717244  [27200/175341]\n",
      "loss: 0.387404  [28800/175341]\n",
      "loss: 0.204848  [30400/175341]\n",
      "loss: 0.142817  [32000/175341]\n",
      "loss: 0.212932  [33600/175341]\n",
      "loss: 0.213591  [35200/175341]\n",
      "loss: 0.284523  [36800/175341]\n",
      "loss: 0.559772  [38400/175341]\n",
      "loss: 0.961585  [40000/175341]\n",
      "loss: 0.443758  [41600/175341]\n",
      "loss: 0.346282  [43200/175341]\n",
      "loss: 0.362355  [44800/175341]\n",
      "loss: 0.318954  [46400/175341]\n",
      "loss: 0.716184  [48000/175341]\n",
      "loss: 0.306524  [49600/175341]\n",
      "loss: 0.167702  [51200/175341]\n",
      "loss: 0.501850  [52800/175341]\n",
      "loss: 0.660789  [54400/175341]\n",
      "loss: 0.709104  [56000/175341]\n",
      "loss: 0.445396  [57600/175341]\n",
      "loss: 0.492441  [59200/175341]\n",
      "loss: 0.243074  [60800/175341]\n",
      "loss: 0.473548  [62400/175341]\n",
      "loss: 0.441317  [64000/175341]\n",
      "loss: 0.277600  [65600/175341]\n",
      "loss: 0.295068  [67200/175341]\n",
      "loss: 0.479311  [68800/175341]\n",
      "loss: 0.158351  [70400/175341]\n",
      "loss: 0.414417  [72000/175341]\n",
      "loss: 0.367152  [73600/175341]\n",
      "loss: 0.492188  [75200/175341]\n",
      "loss: 0.261523  [76800/175341]\n",
      "loss: 0.161030  [78400/175341]\n",
      "loss: 0.783069  [80000/175341]\n",
      "loss: 0.347224  [81600/175341]\n",
      "loss: 0.187771  [83200/175341]\n",
      "loss: 0.355316  [84800/175341]\n",
      "loss: 0.267976  [86400/175341]\n",
      "loss: 0.350438  [88000/175341]\n",
      "loss: 0.474163  [89600/175341]\n",
      "loss: 0.276935  [91200/175341]\n",
      "loss: 0.466887  [92800/175341]\n",
      "loss: 0.387404  [94400/175341]\n",
      "loss: 0.242000  [96000/175341]\n",
      "loss: 0.308332  [97600/175341]\n",
      "loss: 0.289696  [99200/175341]\n",
      "loss: 0.700497  [100800/175341]\n",
      "loss: 0.198242  [102400/175341]\n",
      "loss: 0.577531  [104000/175341]\n",
      "loss: 0.700834  [105600/175341]\n",
      "loss: 0.367266  [107200/175341]\n",
      "loss: 0.316821  [108800/175341]\n",
      "loss: 0.244622  [110400/175341]\n",
      "loss: 0.587367  [112000/175341]\n",
      "loss: 0.330369  [113600/175341]\n",
      "loss: 0.890718  [115200/175341]\n",
      "loss: 0.524305  [116800/175341]\n",
      "loss: 0.816455  [118400/175341]\n",
      "loss: 0.602706  [120000/175341]\n",
      "loss: 0.157604  [121600/175341]\n",
      "loss: 0.302342  [123200/175341]\n",
      "loss: 0.423859  [124800/175341]\n",
      "loss: 0.768888  [126400/175341]\n",
      "loss: 0.209465  [128000/175341]\n",
      "loss: 0.494382  [129600/175341]\n",
      "loss: 0.508977  [131200/175341]\n",
      "loss: 0.288691  [132800/175341]\n",
      "loss: 0.513559  [134400/175341]\n",
      "loss: 0.404552  [136000/175341]\n",
      "loss: 0.522307  [137600/175341]\n",
      "loss: 0.348672  [139200/175341]\n",
      "loss: 0.495136  [140800/175341]\n",
      "loss: 0.407798  [142400/175341]\n",
      "loss: 0.258958  [144000/175341]\n",
      "loss: 0.519388  [145600/175341]\n",
      "loss: 0.365927  [147200/175341]\n",
      "loss: 0.412986  [148800/175341]\n",
      "loss: 0.250695  [150400/175341]\n",
      "loss: 0.574376  [152000/175341]\n",
      "loss: 0.820318  [153600/175341]\n",
      "loss: 0.483594  [155200/175341]\n",
      "loss: 0.539087  [156800/175341]\n",
      "loss: 0.717123  [158400/175341]\n",
      "loss: 0.533356  [160000/175341]\n",
      "loss: 0.438271  [161600/175341]\n",
      "loss: 0.807941  [163200/175341]\n",
      "loss: 0.573788  [164800/175341]\n",
      "loss: 1.044329  [166400/175341]\n",
      "loss: 0.043334  [168000/175341]\n",
      "loss: 0.742057  [169600/175341]\n",
      "loss: 0.207223  [171200/175341]\n",
      "loss: 0.784008  [172800/175341]\n",
      "loss: 0.362210  [174400/175341]\n",
      "Train Accuracy: 81.9266%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.541116, F1-score: 76.81%, Macro_F1-Score:  42.29%  \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.606244  [    0/175341]\n",
      "loss: 0.669657  [ 1600/175341]\n",
      "loss: 0.299232  [ 3200/175341]\n",
      "loss: 0.503524  [ 4800/175341]\n",
      "loss: 0.684886  [ 6400/175341]\n",
      "loss: 0.631096  [ 8000/175341]\n",
      "loss: 0.254857  [ 9600/175341]\n",
      "loss: 0.489519  [11200/175341]\n",
      "loss: 0.318777  [12800/175341]\n",
      "loss: 0.759417  [14400/175341]\n",
      "loss: 0.508500  [16000/175341]\n",
      "loss: 0.690908  [17600/175341]\n",
      "loss: 0.844261  [19200/175341]\n",
      "loss: 0.420274  [20800/175341]\n",
      "loss: 0.699289  [22400/175341]\n",
      "loss: 0.265821  [24000/175341]\n",
      "loss: 0.673665  [25600/175341]\n",
      "loss: 0.355442  [27200/175341]\n",
      "loss: 0.316845  [28800/175341]\n",
      "loss: 0.405046  [30400/175341]\n",
      "loss: 0.748273  [32000/175341]\n",
      "loss: 0.381015  [33600/175341]\n",
      "loss: 0.345294  [35200/175341]\n",
      "loss: 0.428043  [36800/175341]\n",
      "loss: 0.553389  [38400/175341]\n",
      "loss: 0.161829  [40000/175341]\n",
      "loss: 1.010473  [41600/175341]\n",
      "loss: 0.606997  [43200/175341]\n",
      "loss: 0.310044  [44800/175341]\n",
      "loss: 0.781860  [46400/175341]\n",
      "loss: 0.881041  [48000/175341]\n",
      "loss: 0.391368  [49600/175341]\n",
      "loss: 0.367547  [51200/175341]\n",
      "loss: 0.650993  [52800/175341]\n",
      "loss: 0.322704  [54400/175341]\n",
      "loss: 0.729674  [56000/175341]\n",
      "loss: 0.223755  [57600/175341]\n",
      "loss: 0.479872  [59200/175341]\n",
      "loss: 0.412104  [60800/175341]\n",
      "loss: 0.611088  [62400/175341]\n",
      "loss: 0.793645  [64000/175341]\n",
      "loss: 0.406596  [65600/175341]\n",
      "loss: 0.366072  [67200/175341]\n",
      "loss: 0.531742  [68800/175341]\n",
      "loss: 0.339609  [70400/175341]\n",
      "loss: 0.326943  [72000/175341]\n",
      "loss: 0.208680  [73600/175341]\n",
      "loss: 0.424810  [75200/175341]\n",
      "loss: 1.107091  [76800/175341]\n",
      "loss: 0.319619  [78400/175341]\n",
      "loss: 0.198097  [80000/175341]\n",
      "loss: 0.561855  [81600/175341]\n",
      "loss: 1.024439  [83200/175341]\n",
      "loss: 0.224488  [84800/175341]\n",
      "loss: 0.573163  [86400/175341]\n",
      "loss: 0.470183  [88000/175341]\n",
      "loss: 0.157885  [89600/175341]\n",
      "loss: 0.484302  [91200/175341]\n",
      "loss: 0.333772  [92800/175341]\n",
      "loss: 0.798800  [94400/175341]\n",
      "loss: 0.625768  [96000/175341]\n",
      "loss: 0.428595  [97600/175341]\n",
      "loss: 0.265990  [99200/175341]\n",
      "loss: 0.789671  [100800/175341]\n",
      "loss: 0.370285  [102400/175341]\n",
      "loss: 0.294383  [104000/175341]\n",
      "loss: 0.566049  [105600/175341]\n",
      "loss: 0.643212  [107200/175341]\n",
      "loss: 0.805627  [108800/175341]\n",
      "loss: 0.457283  [110400/175341]\n",
      "loss: 0.827383  [112000/175341]\n",
      "loss: 0.309338  [113600/175341]\n",
      "loss: 0.432710  [115200/175341]\n",
      "loss: 0.617946  [116800/175341]\n",
      "loss: 0.372341  [118400/175341]\n",
      "loss: 0.778519  [120000/175341]\n",
      "loss: 0.656567  [121600/175341]\n",
      "loss: 0.278764  [123200/175341]\n",
      "loss: 0.455174  [124800/175341]\n",
      "loss: 0.909657  [126400/175341]\n",
      "loss: 0.558393  [128000/175341]\n",
      "loss: 0.626545  [129600/175341]\n",
      "loss: 0.481512  [131200/175341]\n",
      "loss: 0.580073  [132800/175341]\n",
      "loss: 0.813059  [134400/175341]\n",
      "loss: 0.852396  [136000/175341]\n",
      "loss: 0.205898  [137600/175341]\n",
      "loss: 0.274564  [139200/175341]\n",
      "loss: 0.310377  [140800/175341]\n",
      "loss: 0.407025  [142400/175341]\n",
      "loss: 0.219521  [144000/175341]\n",
      "loss: 0.313639  [145600/175341]\n",
      "loss: 0.927668  [147200/175341]\n",
      "loss: 0.521599  [148800/175341]\n",
      "loss: 0.233332  [150400/175341]\n",
      "loss: 0.219816  [152000/175341]\n",
      "loss: 0.526857  [153600/175341]\n",
      "loss: 0.451348  [155200/175341]\n",
      "loss: 0.558237  [156800/175341]\n",
      "loss: 0.355351  [158400/175341]\n",
      "loss: 0.293362  [160000/175341]\n",
      "loss: 0.581816  [161600/175341]\n",
      "loss: 0.224643  [163200/175341]\n",
      "loss: 0.452830  [164800/175341]\n",
      "loss: 0.505036  [166400/175341]\n",
      "loss: 0.336374  [168000/175341]\n",
      "loss: 1.104381  [169600/175341]\n",
      "loss: 0.295537  [171200/175341]\n",
      "loss: 0.528329  [172800/175341]\n",
      "loss: 0.455803  [174400/175341]\n",
      "Train Accuracy: 81.8976%\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.554590, F1-score: 76.20%, Macro_F1-Score:  41.60%  \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.520841  [    0/175341]\n",
      "loss: 0.287436  [ 1600/175341]\n",
      "loss: 0.412191  [ 3200/175341]\n",
      "loss: 0.652351  [ 4800/175341]\n",
      "loss: 1.231112  [ 6400/175341]\n",
      "loss: 0.451116  [ 8000/175341]\n",
      "loss: 0.197860  [ 9600/175341]\n",
      "loss: 0.379629  [11200/175341]\n",
      "loss: 0.598691  [12800/175341]\n",
      "loss: 0.766920  [14400/175341]\n",
      "loss: 0.348534  [16000/175341]\n",
      "loss: 0.667344  [17600/175341]\n",
      "loss: 0.472322  [19200/175341]\n",
      "loss: 0.685150  [20800/175341]\n",
      "loss: 0.379341  [22400/175341]\n",
      "loss: 0.609889  [24000/175341]\n",
      "loss: 0.712622  [25600/175341]\n",
      "loss: 0.183359  [27200/175341]\n",
      "loss: 0.747139  [28800/175341]\n",
      "loss: 0.442532  [30400/175341]\n",
      "loss: 0.784570  [32000/175341]\n",
      "loss: 0.452710  [33600/175341]\n",
      "loss: 0.856676  [35200/175341]\n",
      "loss: 0.670135  [36800/175341]\n",
      "loss: 0.508960  [38400/175341]\n",
      "loss: 0.489117  [40000/175341]\n",
      "loss: 0.282788  [41600/175341]\n",
      "loss: 0.637266  [43200/175341]\n",
      "loss: 0.200462  [44800/175341]\n",
      "loss: 0.229031  [46400/175341]\n",
      "loss: 0.706568  [48000/175341]\n",
      "loss: 1.093459  [49600/175341]\n",
      "loss: 0.570046  [51200/175341]\n",
      "loss: 0.170637  [52800/175341]\n",
      "loss: 0.531731  [54400/175341]\n",
      "loss: 0.295406  [56000/175341]\n",
      "loss: 0.162765  [57600/175341]\n",
      "loss: 0.340729  [59200/175341]\n",
      "loss: 0.226930  [60800/175341]\n",
      "loss: 0.539730  [62400/175341]\n",
      "loss: 0.449178  [64000/175341]\n",
      "loss: 0.513710  [65600/175341]\n",
      "loss: 0.682724  [67200/175341]\n",
      "loss: 0.146964  [68800/175341]\n",
      "loss: 0.563839  [70400/175341]\n",
      "loss: 0.539531  [72000/175341]\n",
      "loss: 0.473542  [73600/175341]\n",
      "loss: 0.761555  [75200/175341]\n",
      "loss: 0.509562  [76800/175341]\n",
      "loss: 0.650935  [78400/175341]\n",
      "loss: 0.518900  [80000/175341]\n",
      "loss: 0.419067  [81600/175341]\n",
      "loss: 0.404402  [83200/175341]\n",
      "loss: 0.562195  [84800/175341]\n",
      "loss: 0.443672  [86400/175341]\n",
      "loss: 0.342424  [88000/175341]\n",
      "loss: 0.367512  [89600/175341]\n",
      "loss: 0.678876  [91200/175341]\n",
      "loss: 0.270634  [92800/175341]\n",
      "loss: 0.272678  [94400/175341]\n",
      "loss: 0.296114  [96000/175341]\n",
      "loss: 0.625252  [97600/175341]\n",
      "loss: 1.010373  [99200/175341]\n",
      "loss: 0.642396  [100800/175341]\n",
      "loss: 1.374122  [102400/175341]\n",
      "loss: 0.255851  [104000/175341]\n",
      "loss: 0.594964  [105600/175341]\n",
      "loss: 0.576822  [107200/175341]\n",
      "loss: 0.684643  [108800/175341]\n",
      "loss: 0.339538  [110400/175341]\n",
      "loss: 0.491107  [112000/175341]\n",
      "loss: 0.373477  [113600/175341]\n",
      "loss: 0.206872  [115200/175341]\n",
      "loss: 0.151202  [116800/175341]\n",
      "loss: 0.469727  [118400/175341]\n",
      "loss: 0.468754  [120000/175341]\n",
      "loss: 0.919133  [121600/175341]\n",
      "loss: 1.145155  [123200/175341]\n",
      "loss: 0.595285  [124800/175341]\n",
      "loss: 0.544017  [126400/175341]\n",
      "loss: 0.316756  [128000/175341]\n",
      "loss: 0.483300  [129600/175341]\n",
      "loss: 0.447763  [131200/175341]\n",
      "loss: 0.438500  [132800/175341]\n",
      "loss: 0.283107  [134400/175341]\n",
      "loss: 0.732760  [136000/175341]\n",
      "loss: 0.298328  [137600/175341]\n",
      "loss: 0.541579  [139200/175341]\n",
      "loss: 0.522558  [140800/175341]\n",
      "loss: 0.282804  [142400/175341]\n",
      "loss: 0.378879  [144000/175341]\n",
      "loss: 0.355467  [145600/175341]\n",
      "loss: 0.277573  [147200/175341]\n",
      "loss: 0.732242  [148800/175341]\n",
      "loss: 0.464247  [150400/175341]\n",
      "loss: 0.226813  [152000/175341]\n",
      "loss: 0.506787  [153600/175341]\n",
      "loss: 0.722758  [155200/175341]\n",
      "loss: 0.352602  [156800/175341]\n",
      "loss: 0.590516  [158400/175341]\n",
      "loss: 0.794911  [160000/175341]\n",
      "loss: 0.609432  [161600/175341]\n",
      "loss: 0.446108  [163200/175341]\n",
      "loss: 0.280227  [164800/175341]\n",
      "loss: 0.664928  [166400/175341]\n",
      "loss: 0.565977  [168000/175341]\n",
      "loss: 0.546438  [169600/175341]\n",
      "loss: 0.649087  [171200/175341]\n",
      "loss: 0.325949  [172800/175341]\n",
      "loss: 0.565177  [174400/175341]\n",
      "Train Accuracy: 81.9449%\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.547770, F1-score: 76.45%, Macro_F1-Score:  41.75%  \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.373976  [    0/175341]\n",
      "loss: 0.114594  [ 1600/175341]\n",
      "loss: 0.205962  [ 3200/175341]\n",
      "loss: 0.193823  [ 4800/175341]\n",
      "loss: 0.343175  [ 6400/175341]\n",
      "loss: 0.443815  [ 8000/175341]\n",
      "loss: 0.628093  [ 9600/175341]\n",
      "loss: 0.476679  [11200/175341]\n",
      "loss: 0.523293  [12800/175341]\n",
      "loss: 0.346146  [14400/175341]\n",
      "loss: 0.380523  [16000/175341]\n",
      "loss: 0.639823  [17600/175341]\n",
      "loss: 0.393500  [19200/175341]\n",
      "loss: 0.393738  [20800/175341]\n",
      "loss: 0.671967  [22400/175341]\n",
      "loss: 0.585635  [24000/175341]\n",
      "loss: 0.783189  [25600/175341]\n",
      "loss: 0.428366  [27200/175341]\n",
      "loss: 0.325943  [28800/175341]\n",
      "loss: 0.503393  [30400/175341]\n",
      "loss: 0.235896  [32000/175341]\n",
      "loss: 0.399002  [33600/175341]\n",
      "loss: 0.410154  [35200/175341]\n",
      "loss: 0.271174  [36800/175341]\n",
      "loss: 0.196953  [38400/175341]\n",
      "loss: 0.400417  [40000/175341]\n",
      "loss: 0.534904  [41600/175341]\n",
      "loss: 0.557891  [43200/175341]\n",
      "loss: 0.604661  [44800/175341]\n",
      "loss: 0.721672  [46400/175341]\n",
      "loss: 0.388925  [48000/175341]\n",
      "loss: 0.340758  [49600/175341]\n",
      "loss: 0.604194  [51200/175341]\n",
      "loss: 0.196372  [52800/175341]\n",
      "loss: 0.714782  [54400/175341]\n",
      "loss: 0.181034  [56000/175341]\n",
      "loss: 0.283835  [57600/175341]\n",
      "loss: 0.483325  [59200/175341]\n",
      "loss: 0.499580  [60800/175341]\n",
      "loss: 1.046036  [62400/175341]\n",
      "loss: 0.693507  [64000/175341]\n",
      "loss: 0.579142  [65600/175341]\n",
      "loss: 0.356463  [67200/175341]\n",
      "loss: 0.469375  [68800/175341]\n",
      "loss: 0.179490  [70400/175341]\n",
      "loss: 0.604194  [72000/175341]\n",
      "loss: 0.238122  [73600/175341]\n",
      "loss: 0.746894  [75200/175341]\n",
      "loss: 0.732441  [76800/175341]\n",
      "loss: 0.435990  [78400/175341]\n",
      "loss: 0.464795  [80000/175341]\n",
      "loss: 0.165863  [81600/175341]\n",
      "loss: 1.312232  [83200/175341]\n",
      "loss: 0.319090  [84800/175341]\n",
      "loss: 0.457921  [86400/175341]\n",
      "loss: 0.400878  [88000/175341]\n",
      "loss: 0.709759  [89600/175341]\n",
      "loss: 0.258015  [91200/175341]\n",
      "loss: 0.252372  [92800/175341]\n",
      "loss: 1.010607  [94400/175341]\n",
      "loss: 0.393040  [96000/175341]\n",
      "loss: 0.195864  [97600/175341]\n",
      "loss: 0.495735  [99200/175341]\n",
      "loss: 0.381974  [100800/175341]\n",
      "loss: 0.364808  [102400/175341]\n",
      "loss: 0.401865  [104000/175341]\n",
      "loss: 0.572437  [105600/175341]\n",
      "loss: 0.267690  [107200/175341]\n",
      "loss: 0.558413  [108800/175341]\n",
      "loss: 0.644651  [110400/175341]\n",
      "loss: 0.387383  [112000/175341]\n",
      "loss: 0.675976  [113600/175341]\n",
      "loss: 0.182253  [115200/175341]\n",
      "loss: 0.304428  [116800/175341]\n",
      "loss: 0.515621  [118400/175341]\n",
      "loss: 0.468381  [120000/175341]\n",
      "loss: 0.278407  [121600/175341]\n",
      "loss: 0.509172  [123200/175341]\n",
      "loss: 0.380116  [124800/175341]\n",
      "loss: 0.383017  [126400/175341]\n",
      "loss: 0.586220  [128000/175341]\n",
      "loss: 0.575332  [129600/175341]\n",
      "loss: 0.160157  [131200/175341]\n",
      "loss: 0.654621  [132800/175341]\n",
      "loss: 0.428484  [134400/175341]\n",
      "loss: 1.002670  [136000/175341]\n",
      "loss: 0.659890  [137600/175341]\n",
      "loss: 0.205401  [139200/175341]\n",
      "loss: 0.469547  [140800/175341]\n",
      "loss: 0.339857  [142400/175341]\n",
      "loss: 0.464902  [144000/175341]\n",
      "loss: 0.744174  [145600/175341]\n",
      "loss: 0.417369  [147200/175341]\n",
      "loss: 0.522343  [148800/175341]\n",
      "loss: 0.191130  [150400/175341]\n",
      "loss: 0.403190  [152000/175341]\n",
      "loss: 0.622005  [153600/175341]\n",
      "loss: 0.441660  [155200/175341]\n",
      "loss: 0.532342  [156800/175341]\n",
      "loss: 0.840733  [158400/175341]\n",
      "loss: 0.367836  [160000/175341]\n",
      "loss: 0.597768  [161600/175341]\n",
      "loss: 0.465683  [163200/175341]\n",
      "loss: 0.457965  [164800/175341]\n",
      "loss: 0.332658  [166400/175341]\n",
      "loss: 0.272989  [168000/175341]\n",
      "loss: 0.299139  [169600/175341]\n",
      "loss: 0.396553  [171200/175341]\n",
      "loss: 0.403730  [172800/175341]\n",
      "loss: 0.789112  [174400/175341]\n",
      "Train Accuracy: 81.9637%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.537902, F1-score: 76.70%, Macro_F1-Score:  41.86%  \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.305797  [    0/175341]\n",
      "loss: 0.518171  [ 1600/175341]\n",
      "loss: 1.060016  [ 3200/175341]\n",
      "loss: 0.400631  [ 4800/175341]\n",
      "loss: 0.347011  [ 6400/175341]\n",
      "loss: 0.236042  [ 8000/175341]\n",
      "loss: 0.579790  [ 9600/175341]\n",
      "loss: 0.220104  [11200/175341]\n",
      "loss: 0.485082  [12800/175341]\n",
      "loss: 0.248687  [14400/175341]\n",
      "loss: 0.074863  [16000/175341]\n",
      "loss: 0.358994  [17600/175341]\n",
      "loss: 0.780854  [19200/175341]\n",
      "loss: 0.365665  [20800/175341]\n",
      "loss: 0.844016  [22400/175341]\n",
      "loss: 0.145398  [24000/175341]\n",
      "loss: 0.485013  [25600/175341]\n",
      "loss: 0.408314  [27200/175341]\n",
      "loss: 0.335271  [28800/175341]\n",
      "loss: 0.323186  [30400/175341]\n",
      "loss: 0.238520  [32000/175341]\n",
      "loss: 0.339002  [33600/175341]\n",
      "loss: 0.742564  [35200/175341]\n",
      "loss: 0.554356  [36800/175341]\n",
      "loss: 0.241176  [38400/175341]\n",
      "loss: 0.775392  [40000/175341]\n",
      "loss: 0.700151  [41600/175341]\n",
      "loss: 0.302786  [43200/175341]\n",
      "loss: 0.691230  [44800/175341]\n",
      "loss: 0.563289  [46400/175341]\n",
      "loss: 0.298866  [48000/175341]\n",
      "loss: 0.483369  [49600/175341]\n",
      "loss: 0.428129  [51200/175341]\n",
      "loss: 0.256593  [52800/175341]\n",
      "loss: 0.087439  [54400/175341]\n",
      "loss: 0.525199  [56000/175341]\n",
      "loss: 0.465973  [57600/175341]\n",
      "loss: 0.478129  [59200/175341]\n",
      "loss: 0.430218  [60800/175341]\n",
      "loss: 0.310056  [62400/175341]\n",
      "loss: 0.455414  [64000/175341]\n",
      "loss: 0.482569  [65600/175341]\n",
      "loss: 0.219036  [67200/175341]\n",
      "loss: 0.509406  [68800/175341]\n",
      "loss: 0.345867  [70400/175341]\n",
      "loss: 0.152395  [72000/175341]\n",
      "loss: 0.300739  [73600/175341]\n",
      "loss: 0.590078  [75200/175341]\n",
      "loss: 0.287853  [76800/175341]\n",
      "loss: 0.148547  [78400/175341]\n",
      "loss: 0.338178  [80000/175341]\n",
      "loss: 0.341502  [81600/175341]\n",
      "loss: 0.365050  [83200/175341]\n",
      "loss: 0.228197  [84800/175341]\n",
      "loss: 0.152497  [86400/175341]\n",
      "loss: 0.635264  [88000/175341]\n",
      "loss: 0.365449  [89600/175341]\n",
      "loss: 0.653201  [91200/175341]\n",
      "loss: 0.930223  [92800/175341]\n",
      "loss: 0.553605  [94400/175341]\n",
      "loss: 0.326066  [96000/175341]\n",
      "loss: 0.163836  [97600/175341]\n",
      "loss: 0.198720  [99200/175341]\n",
      "loss: 0.165538  [100800/175341]\n",
      "loss: 0.245097  [102400/175341]\n",
      "loss: 0.401658  [104000/175341]\n",
      "loss: 0.190342  [105600/175341]\n",
      "loss: 0.603891  [107200/175341]\n",
      "loss: 0.330112  [108800/175341]\n",
      "loss: 0.530770  [110400/175341]\n",
      "loss: 0.569249  [112000/175341]\n",
      "loss: 0.777376  [113600/175341]\n",
      "loss: 0.549803  [115200/175341]\n",
      "loss: 0.133443  [116800/175341]\n",
      "loss: 0.609239  [118400/175341]\n",
      "loss: 0.355653  [120000/175341]\n",
      "loss: 0.830640  [121600/175341]\n",
      "loss: 0.198291  [123200/175341]\n",
      "loss: 0.145919  [124800/175341]\n",
      "loss: 0.594182  [126400/175341]\n",
      "loss: 0.281197  [128000/175341]\n",
      "loss: 0.509256  [129600/175341]\n",
      "loss: 0.510427  [131200/175341]\n",
      "loss: 0.379503  [132800/175341]\n",
      "loss: 0.447767  [134400/175341]\n",
      "loss: 0.456402  [136000/175341]\n",
      "loss: 0.226550  [137600/175341]\n",
      "loss: 0.494703  [139200/175341]\n",
      "loss: 0.414104  [140800/175341]\n",
      "loss: 0.227454  [142400/175341]\n",
      "loss: 0.406002  [144000/175341]\n",
      "loss: 0.325268  [145600/175341]\n",
      "loss: 0.292864  [147200/175341]\n",
      "loss: 0.706856  [148800/175341]\n",
      "loss: 0.395789  [150400/175341]\n",
      "loss: 0.399716  [152000/175341]\n",
      "loss: 0.751141  [153600/175341]\n",
      "loss: 0.264472  [155200/175341]\n",
      "loss: 0.344674  [156800/175341]\n",
      "loss: 0.343717  [158400/175341]\n",
      "loss: 0.599978  [160000/175341]\n",
      "loss: 0.696076  [161600/175341]\n",
      "loss: 0.190345  [163200/175341]\n",
      "loss: 0.407467  [164800/175341]\n",
      "loss: 0.131025  [166400/175341]\n",
      "loss: 0.500851  [168000/175341]\n",
      "loss: 0.358603  [169600/175341]\n",
      "loss: 0.362910  [171200/175341]\n",
      "loss: 0.231835  [172800/175341]\n",
      "loss: 0.219095  [174400/175341]\n",
      "Train Accuracy: 81.9198%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.551337, F1-score: 76.23%, Macro_F1-Score:  42.50%  \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.282321  [    0/175341]\n",
      "loss: 0.533108  [ 1600/175341]\n",
      "loss: 0.046712  [ 3200/175341]\n",
      "loss: 0.700581  [ 4800/175341]\n",
      "loss: 0.180637  [ 6400/175341]\n",
      "loss: 0.233436  [ 8000/175341]\n",
      "loss: 0.284031  [ 9600/175341]\n",
      "loss: 0.445656  [11200/175341]\n",
      "loss: 0.486124  [12800/175341]\n",
      "loss: 0.307780  [14400/175341]\n",
      "loss: 0.382191  [16000/175341]\n",
      "loss: 0.452012  [17600/175341]\n",
      "loss: 0.280823  [19200/175341]\n",
      "loss: 0.193991  [20800/175341]\n",
      "loss: 0.408139  [22400/175341]\n",
      "loss: 0.481032  [24000/175341]\n",
      "loss: 0.175150  [25600/175341]\n",
      "loss: 0.217759  [27200/175341]\n",
      "loss: 0.463736  [28800/175341]\n",
      "loss: 0.214805  [30400/175341]\n",
      "loss: 0.897362  [32000/175341]\n",
      "loss: 0.227006  [33600/175341]\n",
      "loss: 0.318290  [35200/175341]\n",
      "loss: 0.232452  [36800/175341]\n",
      "loss: 0.121180  [38400/175341]\n",
      "loss: 0.289062  [40000/175341]\n",
      "loss: 0.259189  [41600/175341]\n",
      "loss: 0.674506  [43200/175341]\n",
      "loss: 0.333773  [44800/175341]\n",
      "loss: 0.846779  [46400/175341]\n",
      "loss: 0.527460  [48000/175341]\n",
      "loss: 0.257893  [49600/175341]\n",
      "loss: 0.227858  [51200/175341]\n",
      "loss: 0.523236  [52800/175341]\n",
      "loss: 0.327977  [54400/175341]\n",
      "loss: 0.284716  [56000/175341]\n",
      "loss: 0.273571  [57600/175341]\n",
      "loss: 0.095428  [59200/175341]\n",
      "loss: 0.525714  [60800/175341]\n",
      "loss: 0.340813  [62400/175341]\n",
      "loss: 0.403328  [64000/175341]\n",
      "loss: 0.527188  [65600/175341]\n",
      "loss: 0.411067  [67200/175341]\n",
      "loss: 0.595623  [68800/175341]\n",
      "loss: 0.606129  [70400/175341]\n",
      "loss: 0.515918  [72000/175341]\n",
      "loss: 0.552735  [73600/175341]\n",
      "loss: 0.465442  [75200/175341]\n",
      "loss: 0.413716  [76800/175341]\n",
      "loss: 0.260754  [78400/175341]\n",
      "loss: 1.020173  [80000/175341]\n",
      "loss: 0.134324  [81600/175341]\n",
      "loss: 0.205766  [83200/175341]\n",
      "loss: 0.167605  [84800/175341]\n",
      "loss: 0.398946  [86400/175341]\n",
      "loss: 0.383534  [88000/175341]\n",
      "loss: 0.415333  [89600/175341]\n",
      "loss: 0.479067  [91200/175341]\n",
      "loss: 0.580216  [92800/175341]\n",
      "loss: 0.360074  [94400/175341]\n",
      "loss: 0.500580  [96000/175341]\n",
      "loss: 0.422913  [97600/175341]\n",
      "loss: 0.321048  [99200/175341]\n",
      "loss: 0.313734  [100800/175341]\n",
      "loss: 0.303251  [102400/175341]\n",
      "loss: 0.684137  [104000/175341]\n",
      "loss: 0.464698  [105600/175341]\n",
      "loss: 0.306860  [107200/175341]\n",
      "loss: 0.832102  [108800/175341]\n",
      "loss: 0.449973  [110400/175341]\n",
      "loss: 0.202095  [112000/175341]\n",
      "loss: 0.551341  [113600/175341]\n",
      "loss: 0.352935  [115200/175341]\n",
      "loss: 0.403884  [116800/175341]\n",
      "loss: 0.063822  [118400/175341]\n",
      "loss: 0.222989  [120000/175341]\n",
      "loss: 0.416232  [121600/175341]\n",
      "loss: 0.265577  [123200/175341]\n",
      "loss: 0.667584  [124800/175341]\n",
      "loss: 0.520651  [126400/175341]\n",
      "loss: 0.956301  [128000/175341]\n",
      "loss: 0.674714  [129600/175341]\n",
      "loss: 0.706123  [131200/175341]\n",
      "loss: 0.713999  [132800/175341]\n",
      "loss: 0.375915  [134400/175341]\n",
      "loss: 0.485627  [136000/175341]\n",
      "loss: 0.275098  [137600/175341]\n",
      "loss: 0.293630  [139200/175341]\n",
      "loss: 0.307820  [140800/175341]\n",
      "loss: 0.736824  [142400/175341]\n",
      "loss: 0.415524  [144000/175341]\n",
      "loss: 0.324420  [145600/175341]\n",
      "loss: 0.299870  [147200/175341]\n",
      "loss: 0.529343  [148800/175341]\n",
      "loss: 0.477892  [150400/175341]\n",
      "loss: 0.257159  [152000/175341]\n",
      "loss: 0.405653  [153600/175341]\n",
      "loss: 0.445483  [155200/175341]\n",
      "loss: 0.167738  [156800/175341]\n",
      "loss: 0.345060  [158400/175341]\n",
      "loss: 0.669881  [160000/175341]\n",
      "loss: 0.237984  [161600/175341]\n",
      "loss: 0.159938  [163200/175341]\n",
      "loss: 0.136180  [164800/175341]\n",
      "loss: 0.468672  [166400/175341]\n",
      "loss: 0.273026  [168000/175341]\n",
      "loss: 0.540793  [169600/175341]\n",
      "loss: 0.192474  [171200/175341]\n",
      "loss: 0.434691  [172800/175341]\n",
      "loss: 0.218326  [174400/175341]\n",
      "Train Accuracy: 81.9181%\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.556716, F1-score: 75.55%, Macro_F1-Score:  41.44%  \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.770820  [    0/175341]\n",
      "loss: 0.630753  [ 1600/175341]\n",
      "loss: 0.304310  [ 3200/175341]\n",
      "loss: 0.723723  [ 4800/175341]\n",
      "loss: 0.445835  [ 6400/175341]\n",
      "loss: 0.758466  [ 8000/175341]\n",
      "loss: 0.368726  [ 9600/175341]\n",
      "loss: 0.532955  [11200/175341]\n",
      "loss: 0.380633  [12800/175341]\n",
      "loss: 0.165488  [14400/175341]\n",
      "loss: 0.270204  [16000/175341]\n",
      "loss: 0.165927  [17600/175341]\n",
      "loss: 0.387832  [19200/175341]\n",
      "loss: 0.240350  [20800/175341]\n",
      "loss: 0.554240  [22400/175341]\n",
      "loss: 0.115536  [24000/175341]\n",
      "loss: 0.699731  [25600/175341]\n",
      "loss: 0.235547  [27200/175341]\n",
      "loss: 0.308077  [28800/175341]\n",
      "loss: 0.281717  [30400/175341]\n",
      "loss: 0.100049  [32000/175341]\n",
      "loss: 0.204444  [33600/175341]\n",
      "loss: 0.196342  [35200/175341]\n",
      "loss: 0.363945  [36800/175341]\n",
      "loss: 0.427836  [38400/175341]\n",
      "loss: 0.415208  [40000/175341]\n",
      "loss: 0.238206  [41600/175341]\n",
      "loss: 0.546061  [43200/175341]\n",
      "loss: 0.349348  [44800/175341]\n",
      "loss: 0.475172  [46400/175341]\n",
      "loss: 0.301401  [48000/175341]\n",
      "loss: 0.520188  [49600/175341]\n",
      "loss: 0.314658  [51200/175341]\n",
      "loss: 0.445466  [52800/175341]\n",
      "loss: 0.486749  [54400/175341]\n",
      "loss: 0.535706  [56000/175341]\n",
      "loss: 0.344860  [57600/175341]\n",
      "loss: 0.356592  [59200/175341]\n",
      "loss: 0.365714  [60800/175341]\n",
      "loss: 0.434872  [62400/175341]\n",
      "loss: 0.546307  [64000/175341]\n",
      "loss: 0.156873  [65600/175341]\n",
      "loss: 0.274541  [67200/175341]\n",
      "loss: 0.327699  [68800/175341]\n",
      "loss: 0.633509  [70400/175341]\n",
      "loss: 0.217175  [72000/175341]\n",
      "loss: 0.473106  [73600/175341]\n",
      "loss: 0.860130  [75200/175341]\n",
      "loss: 0.625339  [76800/175341]\n",
      "loss: 0.547437  [78400/175341]\n",
      "loss: 0.598396  [80000/175341]\n",
      "loss: 0.266866  [81600/175341]\n",
      "loss: 0.457230  [83200/175341]\n",
      "loss: 0.745595  [84800/175341]\n",
      "loss: 0.173763  [86400/175341]\n",
      "loss: 0.348105  [88000/175341]\n",
      "loss: 0.408673  [89600/175341]\n",
      "loss: 0.270673  [91200/175341]\n",
      "loss: 0.465154  [92800/175341]\n",
      "loss: 1.060163  [94400/175341]\n",
      "loss: 0.718477  [96000/175341]\n",
      "loss: 0.785245  [97600/175341]\n",
      "loss: 0.289626  [99200/175341]\n",
      "loss: 0.424458  [100800/175341]\n",
      "loss: 0.659621  [102400/175341]\n",
      "loss: 0.417055  [104000/175341]\n",
      "loss: 0.207692  [105600/175341]\n",
      "loss: 0.401423  [107200/175341]\n",
      "loss: 0.725253  [108800/175341]\n",
      "loss: 0.439018  [110400/175341]\n",
      "loss: 0.633926  [112000/175341]\n",
      "loss: 0.679290  [113600/175341]\n",
      "loss: 0.205364  [115200/175341]\n",
      "loss: 0.510169  [116800/175341]\n",
      "loss: 0.594600  [118400/175341]\n",
      "loss: 0.927626  [120000/175341]\n",
      "loss: 0.459035  [121600/175341]\n",
      "loss: 0.424667  [123200/175341]\n",
      "loss: 0.303573  [124800/175341]\n",
      "loss: 0.895808  [126400/175341]\n",
      "loss: 0.517728  [128000/175341]\n",
      "loss: 0.600349  [129600/175341]\n",
      "loss: 0.486941  [131200/175341]\n",
      "loss: 0.194145  [132800/175341]\n",
      "loss: 0.589245  [134400/175341]\n",
      "loss: 0.365822  [136000/175341]\n",
      "loss: 1.134300  [137600/175341]\n",
      "loss: 0.984274  [139200/175341]\n",
      "loss: 0.606905  [140800/175341]\n",
      "loss: 0.284909  [142400/175341]\n",
      "loss: 0.306765  [144000/175341]\n",
      "loss: 0.405834  [145600/175341]\n",
      "loss: 0.414262  [147200/175341]\n",
      "loss: 0.984759  [148800/175341]\n",
      "loss: 0.584178  [150400/175341]\n",
      "loss: 0.360254  [152000/175341]\n",
      "loss: 0.542671  [153600/175341]\n",
      "loss: 0.527131  [155200/175341]\n",
      "loss: 0.154281  [156800/175341]\n",
      "loss: 0.390821  [158400/175341]\n",
      "loss: 0.609579  [160000/175341]\n",
      "loss: 0.295354  [161600/175341]\n",
      "loss: 0.479033  [163200/175341]\n",
      "loss: 0.417428  [164800/175341]\n",
      "loss: 0.313274  [166400/175341]\n",
      "loss: 0.693385  [168000/175341]\n",
      "loss: 0.778465  [169600/175341]\n",
      "loss: 0.456814  [171200/175341]\n",
      "loss: 0.910980  [172800/175341]\n",
      "loss: 0.287353  [174400/175341]\n",
      "Train Accuracy: 81.9158%\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.544334, F1-score: 76.86%, Macro_F1-Score:  41.88%  \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.134632  [    0/175341]\n",
      "loss: 0.301234  [ 1600/175341]\n",
      "loss: 0.250436  [ 3200/175341]\n",
      "loss: 0.390332  [ 4800/175341]\n",
      "loss: 0.459907  [ 6400/175341]\n",
      "loss: 0.296987  [ 8000/175341]\n",
      "loss: 0.680883  [ 9600/175341]\n",
      "loss: 0.725595  [11200/175341]\n",
      "loss: 0.599961  [12800/175341]\n",
      "loss: 0.309226  [14400/175341]\n",
      "loss: 0.365600  [16000/175341]\n",
      "loss: 0.525840  [17600/175341]\n",
      "loss: 0.374674  [19200/175341]\n",
      "loss: 0.374692  [20800/175341]\n",
      "loss: 0.675696  [22400/175341]\n",
      "loss: 0.670591  [24000/175341]\n",
      "loss: 0.355677  [25600/175341]\n",
      "loss: 0.142313  [27200/175341]\n",
      "loss: 0.482507  [28800/175341]\n",
      "loss: 0.339527  [30400/175341]\n",
      "loss: 0.522471  [32000/175341]\n",
      "loss: 0.133117  [33600/175341]\n",
      "loss: 0.427116  [35200/175341]\n",
      "loss: 0.591469  [36800/175341]\n",
      "loss: 0.587249  [38400/175341]\n",
      "loss: 0.450295  [40000/175341]\n",
      "loss: 0.146602  [41600/175341]\n",
      "loss: 0.523361  [43200/175341]\n",
      "loss: 0.763992  [44800/175341]\n",
      "loss: 0.807540  [46400/175341]\n",
      "loss: 0.476679  [48000/175341]\n",
      "loss: 0.180230  [49600/175341]\n",
      "loss: 0.469853  [51200/175341]\n",
      "loss: 0.593353  [52800/175341]\n",
      "loss: 0.483432  [54400/175341]\n",
      "loss: 0.150145  [56000/175341]\n",
      "loss: 0.090258  [57600/175341]\n",
      "loss: 0.270731  [59200/175341]\n",
      "loss: 0.306119  [60800/175341]\n",
      "loss: 0.625245  [62400/175341]\n",
      "loss: 0.345005  [64000/175341]\n",
      "loss: 0.549130  [65600/175341]\n",
      "loss: 0.081158  [67200/175341]\n",
      "loss: 0.618060  [68800/175341]\n",
      "loss: 0.382098  [70400/175341]\n",
      "loss: 0.411235  [72000/175341]\n",
      "loss: 0.654333  [73600/175341]\n",
      "loss: 0.316277  [75200/175341]\n",
      "loss: 0.340030  [76800/175341]\n",
      "loss: 0.396308  [78400/175341]\n",
      "loss: 0.454310  [80000/175341]\n",
      "loss: 0.451442  [81600/175341]\n",
      "loss: 0.264713  [83200/175341]\n",
      "loss: 0.754232  [84800/175341]\n",
      "loss: 0.436710  [86400/175341]\n",
      "loss: 0.355196  [88000/175341]\n",
      "loss: 0.275768  [89600/175341]\n",
      "loss: 0.357430  [91200/175341]\n",
      "loss: 1.071615  [92800/175341]\n",
      "loss: 0.404651  [94400/175341]\n",
      "loss: 0.531767  [96000/175341]\n",
      "loss: 0.368243  [97600/175341]\n",
      "loss: 0.415653  [99200/175341]\n",
      "loss: 0.514110  [100800/175341]\n",
      "loss: 0.607702  [102400/175341]\n",
      "loss: 0.332070  [104000/175341]\n",
      "loss: 0.325358  [105600/175341]\n",
      "loss: 0.307508  [107200/175341]\n",
      "loss: 0.449254  [108800/175341]\n",
      "loss: 0.982176  [110400/175341]\n",
      "loss: 0.209601  [112000/175341]\n",
      "loss: 0.235414  [113600/175341]\n",
      "loss: 0.447514  [115200/175341]\n",
      "loss: 0.560094  [116800/175341]\n",
      "loss: 0.608553  [118400/175341]\n",
      "loss: 0.634061  [120000/175341]\n",
      "loss: 0.548499  [121600/175341]\n",
      "loss: 0.153330  [123200/175341]\n",
      "loss: 0.388385  [124800/175341]\n",
      "loss: 0.781727  [126400/175341]\n",
      "loss: 0.428377  [128000/175341]\n",
      "loss: 0.591711  [129600/175341]\n",
      "loss: 0.597782  [131200/175341]\n",
      "loss: 0.468238  [132800/175341]\n",
      "loss: 0.657260  [134400/175341]\n",
      "loss: 0.526982  [136000/175341]\n",
      "loss: 0.508223  [137600/175341]\n",
      "loss: 0.279124  [139200/175341]\n",
      "loss: 0.377818  [140800/175341]\n",
      "loss: 0.216537  [142400/175341]\n",
      "loss: 0.798781  [144000/175341]\n",
      "loss: 0.478673  [145600/175341]\n",
      "loss: 0.476727  [147200/175341]\n",
      "loss: 0.417242  [148800/175341]\n",
      "loss: 0.444478  [150400/175341]\n",
      "loss: 0.183804  [152000/175341]\n",
      "loss: 0.593130  [153600/175341]\n",
      "loss: 0.431667  [155200/175341]\n",
      "loss: 0.464406  [156800/175341]\n",
      "loss: 0.407991  [158400/175341]\n",
      "loss: 0.580047  [160000/175341]\n",
      "loss: 0.648479  [161600/175341]\n",
      "loss: 0.447801  [163200/175341]\n",
      "loss: 0.631283  [164800/175341]\n",
      "loss: 0.839254  [166400/175341]\n",
      "loss: 0.690827  [168000/175341]\n",
      "loss: 0.394377  [169600/175341]\n",
      "loss: 0.534989  [171200/175341]\n",
      "loss: 0.681327  [172800/175341]\n",
      "loss: 0.348102  [174400/175341]\n",
      "Train Accuracy: 81.9295%\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.567213, F1-score: 75.22%, Macro_F1-Score:  41.56%  \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.660194  [    0/175341]\n",
      "loss: 0.522935  [ 1600/175341]\n",
      "loss: 0.237090  [ 3200/175341]\n",
      "loss: 0.238108  [ 4800/175341]\n",
      "loss: 0.263618  [ 6400/175341]\n",
      "loss: 0.502487  [ 8000/175341]\n",
      "loss: 0.372571  [ 9600/175341]\n",
      "loss: 0.524321  [11200/175341]\n",
      "loss: 0.538235  [12800/175341]\n",
      "loss: 0.401318  [14400/175341]\n",
      "loss: 0.429623  [16000/175341]\n",
      "loss: 0.479736  [17600/175341]\n",
      "loss: 0.741983  [19200/175341]\n",
      "loss: 0.463125  [20800/175341]\n",
      "loss: 0.337355  [22400/175341]\n",
      "loss: 0.278817  [24000/175341]\n",
      "loss: 0.254662  [25600/175341]\n",
      "loss: 0.620937  [27200/175341]\n",
      "loss: 0.314656  [28800/175341]\n",
      "loss: 0.330962  [30400/175341]\n",
      "loss: 0.654234  [32000/175341]\n",
      "loss: 0.554644  [33600/175341]\n",
      "loss: 0.134917  [35200/175341]\n",
      "loss: 0.383305  [36800/175341]\n",
      "loss: 0.719677  [38400/175341]\n",
      "loss: 0.412980  [40000/175341]\n",
      "loss: 0.010878  [41600/175341]\n",
      "loss: 0.829589  [43200/175341]\n",
      "loss: 0.676641  [44800/175341]\n",
      "loss: 0.650027  [46400/175341]\n",
      "loss: 0.477849  [48000/175341]\n",
      "loss: 0.547202  [49600/175341]\n",
      "loss: 0.932795  [51200/175341]\n",
      "loss: 0.079190  [52800/175341]\n",
      "loss: 0.346144  [54400/175341]\n",
      "loss: 0.638751  [56000/175341]\n",
      "loss: 0.301991  [57600/175341]\n",
      "loss: 0.594469  [59200/175341]\n",
      "loss: 0.873196  [60800/175341]\n",
      "loss: 0.787343  [62400/175341]\n",
      "loss: 0.444359  [64000/175341]\n",
      "loss: 0.274899  [65600/175341]\n",
      "loss: 0.524680  [67200/175341]\n",
      "loss: 0.368805  [68800/175341]\n",
      "loss: 0.267245  [70400/175341]\n",
      "loss: 0.518597  [72000/175341]\n",
      "loss: 0.485045  [73600/175341]\n",
      "loss: 0.441402  [75200/175341]\n",
      "loss: 0.339153  [76800/175341]\n",
      "loss: 0.411456  [78400/175341]\n",
      "loss: 0.318645  [80000/175341]\n",
      "loss: 0.299269  [81600/175341]\n",
      "loss: 0.514050  [83200/175341]\n",
      "loss: 0.805911  [84800/175341]\n",
      "loss: 0.568155  [86400/175341]\n",
      "loss: 0.539976  [88000/175341]\n",
      "loss: 0.597910  [89600/175341]\n",
      "loss: 0.341863  [91200/175341]\n",
      "loss: 0.387298  [92800/175341]\n",
      "loss: 0.425336  [94400/175341]\n",
      "loss: 0.309368  [96000/175341]\n",
      "loss: 0.566811  [97600/175341]\n",
      "loss: 0.470346  [99200/175341]\n",
      "loss: 0.165968  [100800/175341]\n",
      "loss: 0.222253  [102400/175341]\n",
      "loss: 0.479535  [104000/175341]\n",
      "loss: 0.461577  [105600/175341]\n",
      "loss: 0.551190  [107200/175341]\n",
      "loss: 0.518614  [108800/175341]\n",
      "loss: 0.777864  [110400/175341]\n",
      "loss: 0.395902  [112000/175341]\n",
      "loss: 0.584253  [113600/175341]\n",
      "loss: 0.705495  [115200/175341]\n",
      "loss: 0.570382  [116800/175341]\n",
      "loss: 0.631514  [118400/175341]\n",
      "loss: 0.220371  [120000/175341]\n",
      "loss: 0.532975  [121600/175341]\n",
      "loss: 1.226653  [123200/175341]\n",
      "loss: 0.383758  [124800/175341]\n",
      "loss: 0.232875  [126400/175341]\n",
      "loss: 0.164425  [128000/175341]\n",
      "loss: 0.359583  [129600/175341]\n",
      "loss: 0.618175  [131200/175341]\n",
      "loss: 0.668810  [132800/175341]\n",
      "loss: 0.244060  [134400/175341]\n",
      "loss: 0.810553  [136000/175341]\n",
      "loss: 0.344379  [137600/175341]\n",
      "loss: 0.447551  [139200/175341]\n",
      "loss: 0.456365  [140800/175341]\n",
      "loss: 0.168672  [142400/175341]\n",
      "loss: 0.336539  [144000/175341]\n",
      "loss: 0.334481  [145600/175341]\n",
      "loss: 0.235138  [147200/175341]\n",
      "loss: 0.502103  [148800/175341]\n",
      "loss: 0.516269  [150400/175341]\n",
      "loss: 0.923309  [152000/175341]\n",
      "loss: 0.545860  [153600/175341]\n",
      "loss: 0.818314  [155200/175341]\n",
      "loss: 0.434003  [156800/175341]\n",
      "loss: 1.006602  [158400/175341]\n",
      "loss: 0.625742  [160000/175341]\n",
      "loss: 0.423549  [161600/175341]\n",
      "loss: 0.612282  [163200/175341]\n",
      "loss: 0.300991  [164800/175341]\n",
      "loss: 0.089046  [166400/175341]\n",
      "loss: 0.866381  [168000/175341]\n",
      "loss: 0.374013  [169600/175341]\n",
      "loss: 0.281650  [171200/175341]\n",
      "loss: 0.320958  [172800/175341]\n",
      "loss: 0.527348  [174400/175341]\n",
      "Train Accuracy: 81.9386%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.556349, F1-score: 76.05%, Macro_F1-Score:  41.41%  \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.691382  [    0/175341]\n",
      "loss: 0.677346  [ 1600/175341]\n",
      "loss: 0.176889  [ 3200/175341]\n",
      "loss: 0.380582  [ 4800/175341]\n",
      "loss: 0.553928  [ 6400/175341]\n",
      "loss: 0.576205  [ 8000/175341]\n",
      "loss: 0.297815  [ 9600/175341]\n",
      "loss: 0.346252  [11200/175341]\n",
      "loss: 0.490882  [12800/175341]\n",
      "loss: 0.376904  [14400/175341]\n",
      "loss: 0.669145  [16000/175341]\n",
      "loss: 0.305392  [17600/175341]\n",
      "loss: 0.195754  [19200/175341]\n",
      "loss: 0.446986  [20800/175341]\n",
      "loss: 0.144319  [22400/175341]\n",
      "loss: 0.327741  [24000/175341]\n",
      "loss: 0.352251  [25600/175341]\n",
      "loss: 0.217789  [27200/175341]\n",
      "loss: 0.270513  [28800/175341]\n",
      "loss: 0.220508  [30400/175341]\n",
      "loss: 0.338330  [32000/175341]\n",
      "loss: 0.086311  [33600/175341]\n",
      "loss: 0.465038  [35200/175341]\n",
      "loss: 0.191943  [36800/175341]\n",
      "loss: 0.473034  [38400/175341]\n",
      "loss: 0.653118  [40000/175341]\n",
      "loss: 0.354651  [41600/175341]\n",
      "loss: 0.278199  [43200/175341]\n",
      "loss: 0.256670  [44800/175341]\n",
      "loss: 0.354661  [46400/175341]\n",
      "loss: 0.304127  [48000/175341]\n",
      "loss: 0.230319  [49600/175341]\n",
      "loss: 0.645486  [51200/175341]\n",
      "loss: 0.588991  [52800/175341]\n",
      "loss: 0.181341  [54400/175341]\n",
      "loss: 0.663736  [56000/175341]\n",
      "loss: 0.376943  [57600/175341]\n",
      "loss: 0.571192  [59200/175341]\n",
      "loss: 0.138527  [60800/175341]\n",
      "loss: 0.687349  [62400/175341]\n",
      "loss: 0.422897  [64000/175341]\n",
      "loss: 0.759608  [65600/175341]\n",
      "loss: 0.330423  [67200/175341]\n",
      "loss: 0.140919  [68800/175341]\n",
      "loss: 1.025456  [70400/175341]\n",
      "loss: 0.412601  [72000/175341]\n",
      "loss: 0.414932  [73600/175341]\n",
      "loss: 0.486483  [75200/175341]\n",
      "loss: 0.565286  [76800/175341]\n",
      "loss: 0.520968  [78400/175341]\n",
      "loss: 0.629601  [80000/175341]\n",
      "loss: 0.188747  [81600/175341]\n",
      "loss: 0.366271  [83200/175341]\n",
      "loss: 0.210692  [84800/175341]\n",
      "loss: 0.373753  [86400/175341]\n",
      "loss: 0.565496  [88000/175341]\n",
      "loss: 0.371443  [89600/175341]\n",
      "loss: 0.444429  [91200/175341]\n",
      "loss: 0.211013  [92800/175341]\n",
      "loss: 0.503255  [94400/175341]\n",
      "loss: 0.297058  [96000/175341]\n",
      "loss: 0.220032  [97600/175341]\n",
      "loss: 0.281779  [99200/175341]\n",
      "loss: 0.661077  [100800/175341]\n",
      "loss: 0.771335  [102400/175341]\n",
      "loss: 0.355926  [104000/175341]\n",
      "loss: 0.359862  [105600/175341]\n",
      "loss: 0.149617  [107200/175341]\n",
      "loss: 0.545936  [108800/175341]\n",
      "loss: 0.127441  [110400/175341]\n",
      "loss: 0.649442  [112000/175341]\n",
      "loss: 0.483024  [113600/175341]\n",
      "loss: 0.376388  [115200/175341]\n",
      "loss: 0.326168  [116800/175341]\n",
      "loss: 0.446040  [118400/175341]\n",
      "loss: 0.183674  [120000/175341]\n",
      "loss: 0.486911  [121600/175341]\n",
      "loss: 0.506234  [123200/175341]\n",
      "loss: 0.395221  [124800/175341]\n",
      "loss: 0.594952  [126400/175341]\n",
      "loss: 0.215917  [128000/175341]\n",
      "loss: 0.089686  [129600/175341]\n",
      "loss: 0.193743  [131200/175341]\n",
      "loss: 0.291166  [132800/175341]\n",
      "loss: 0.570179  [134400/175341]\n",
      "loss: 0.463016  [136000/175341]\n",
      "loss: 0.603179  [137600/175341]\n",
      "loss: 0.488539  [139200/175341]\n",
      "loss: 1.149703  [140800/175341]\n",
      "loss: 0.505137  [142400/175341]\n",
      "loss: 0.622988  [144000/175341]\n",
      "loss: 0.329407  [145600/175341]\n",
      "loss: 0.256178  [147200/175341]\n",
      "loss: 0.434124  [148800/175341]\n",
      "loss: 0.558599  [150400/175341]\n",
      "loss: 0.239455  [152000/175341]\n",
      "loss: 0.778794  [153600/175341]\n",
      "loss: 0.311357  [155200/175341]\n",
      "loss: 0.736291  [156800/175341]\n",
      "loss: 0.556767  [158400/175341]\n",
      "loss: 0.242181  [160000/175341]\n",
      "loss: 0.411325  [161600/175341]\n",
      "loss: 0.143276  [163200/175341]\n",
      "loss: 0.470811  [164800/175341]\n",
      "loss: 0.781077  [166400/175341]\n",
      "loss: 0.447455  [168000/175341]\n",
      "loss: 0.296142  [169600/175341]\n",
      "loss: 0.462458  [171200/175341]\n",
      "loss: 0.780335  [172800/175341]\n",
      "loss: 0.494164  [174400/175341]\n",
      "Train Accuracy: 81.9358%\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.534065, F1-score: 77.19%, Macro_F1-Score:  41.94%  \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.471666  [    0/175341]\n",
      "loss: 0.538980  [ 1600/175341]\n",
      "loss: 0.238875  [ 3200/175341]\n",
      "loss: 0.150106  [ 4800/175341]\n",
      "loss: 0.268607  [ 6400/175341]\n",
      "loss: 0.129262  [ 8000/175341]\n",
      "loss: 0.572683  [ 9600/175341]\n",
      "loss: 0.158285  [11200/175341]\n",
      "loss: 0.350733  [12800/175341]\n",
      "loss: 0.452550  [14400/175341]\n",
      "loss: 0.537960  [16000/175341]\n",
      "loss: 0.637753  [17600/175341]\n",
      "loss: 0.739417  [19200/175341]\n",
      "loss: 0.167625  [20800/175341]\n",
      "loss: 0.601086  [22400/175341]\n",
      "loss: 0.769078  [24000/175341]\n",
      "loss: 0.494238  [25600/175341]\n",
      "loss: 0.566185  [27200/175341]\n",
      "loss: 0.363861  [28800/175341]\n",
      "loss: 0.339767  [30400/175341]\n",
      "loss: 0.778550  [32000/175341]\n",
      "loss: 0.314974  [33600/175341]\n",
      "loss: 0.495301  [35200/175341]\n",
      "loss: 0.481593  [36800/175341]\n",
      "loss: 0.165892  [38400/175341]\n",
      "loss: 0.324215  [40000/175341]\n",
      "loss: 0.497940  [41600/175341]\n",
      "loss: 0.379875  [43200/175341]\n",
      "loss: 0.585441  [44800/175341]\n",
      "loss: 0.725459  [46400/175341]\n",
      "loss: 0.322566  [48000/175341]\n",
      "loss: 0.412462  [49600/175341]\n",
      "loss: 0.146002  [51200/175341]\n",
      "loss: 0.426787  [52800/175341]\n",
      "loss: 0.488213  [54400/175341]\n",
      "loss: 0.580738  [56000/175341]\n",
      "loss: 0.433833  [57600/175341]\n",
      "loss: 0.412746  [59200/175341]\n",
      "loss: 0.694047  [60800/175341]\n",
      "loss: 0.516192  [62400/175341]\n",
      "loss: 0.298321  [64000/175341]\n",
      "loss: 0.606286  [65600/175341]\n",
      "loss: 0.798032  [67200/175341]\n",
      "loss: 0.467139  [68800/175341]\n",
      "loss: 0.591482  [70400/175341]\n",
      "loss: 0.336540  [72000/175341]\n",
      "loss: 0.430142  [73600/175341]\n",
      "loss: 0.121934  [75200/175341]\n",
      "loss: 0.351248  [76800/175341]\n",
      "loss: 0.301150  [78400/175341]\n",
      "loss: 0.395393  [80000/175341]\n",
      "loss: 0.436206  [81600/175341]\n",
      "loss: 0.644665  [83200/175341]\n",
      "loss: 0.748171  [84800/175341]\n",
      "loss: 0.221094  [86400/175341]\n",
      "loss: 0.209920  [88000/175341]\n",
      "loss: 0.587733  [89600/175341]\n",
      "loss: 0.661958  [91200/175341]\n",
      "loss: 0.423718  [92800/175341]\n",
      "loss: 0.364374  [94400/175341]\n",
      "loss: 0.193179  [96000/175341]\n",
      "loss: 0.761337  [97600/175341]\n",
      "loss: 0.381769  [99200/175341]\n",
      "loss: 0.373080  [100800/175341]\n",
      "loss: 0.244978  [102400/175341]\n",
      "loss: 0.332255  [104000/175341]\n",
      "loss: 0.488307  [105600/175341]\n",
      "loss: 0.387867  [107200/175341]\n",
      "loss: 0.336223  [108800/175341]\n",
      "loss: 0.349143  [110400/175341]\n",
      "loss: 0.424303  [112000/175341]\n",
      "loss: 0.500159  [113600/175341]\n",
      "loss: 0.687578  [115200/175341]\n",
      "loss: 1.008716  [116800/175341]\n",
      "loss: 0.376198  [118400/175341]\n",
      "loss: 0.136584  [120000/175341]\n",
      "loss: 0.658886  [121600/175341]\n",
      "loss: 0.534579  [123200/175341]\n",
      "loss: 0.608332  [124800/175341]\n",
      "loss: 0.511433  [126400/175341]\n",
      "loss: 0.293224  [128000/175341]\n",
      "loss: 0.634303  [129600/175341]\n",
      "loss: 0.737765  [131200/175341]\n",
      "loss: 0.280910  [132800/175341]\n",
      "loss: 0.500622  [134400/175341]\n",
      "loss: 0.311594  [136000/175341]\n",
      "loss: 0.691059  [137600/175341]\n",
      "loss: 0.263355  [139200/175341]\n",
      "loss: 0.484030  [140800/175341]\n",
      "loss: 0.322547  [142400/175341]\n",
      "loss: 0.520770  [144000/175341]\n",
      "loss: 0.377865  [145600/175341]\n",
      "loss: 0.513664  [147200/175341]\n",
      "loss: 0.375450  [148800/175341]\n",
      "loss: 0.554579  [150400/175341]\n",
      "loss: 0.352579  [152000/175341]\n",
      "loss: 0.321412  [153600/175341]\n",
      "loss: 0.267481  [155200/175341]\n",
      "loss: 0.464137  [156800/175341]\n",
      "loss: 0.242293  [158400/175341]\n",
      "loss: 0.318846  [160000/175341]\n",
      "loss: 0.295775  [161600/175341]\n",
      "loss: 0.592548  [163200/175341]\n",
      "loss: 0.360804  [164800/175341]\n",
      "loss: 0.555832  [166400/175341]\n",
      "loss: 0.370665  [168000/175341]\n",
      "loss: 0.328999  [169600/175341]\n",
      "loss: 0.760569  [171200/175341]\n",
      "loss: 0.364887  [172800/175341]\n",
      "loss: 0.198928  [174400/175341]\n",
      "Train Accuracy: 81.8890%\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.550553, F1-score: 76.68%, Macro_F1-Score:  42.23%  \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.560341  [    0/175341]\n",
      "loss: 0.127467  [ 1600/175341]\n",
      "loss: 0.643417  [ 3200/175341]\n",
      "loss: 0.507213  [ 4800/175341]\n",
      "loss: 0.172007  [ 6400/175341]\n",
      "loss: 0.337158  [ 8000/175341]\n",
      "loss: 0.618877  [ 9600/175341]\n",
      "loss: 0.478232  [11200/175341]\n",
      "loss: 0.245460  [12800/175341]\n",
      "loss: 0.565987  [14400/175341]\n",
      "loss: 0.363036  [16000/175341]\n",
      "loss: 0.388401  [17600/175341]\n",
      "loss: 0.396664  [19200/175341]\n",
      "loss: 0.383705  [20800/175341]\n",
      "loss: 0.364225  [22400/175341]\n",
      "loss: 0.155763  [24000/175341]\n",
      "loss: 0.179186  [25600/175341]\n",
      "loss: 0.308945  [27200/175341]\n",
      "loss: 0.578214  [28800/175341]\n",
      "loss: 0.369646  [30400/175341]\n",
      "loss: 0.920112  [32000/175341]\n",
      "loss: 0.172747  [33600/175341]\n",
      "loss: 0.201416  [35200/175341]\n",
      "loss: 0.404303  [36800/175341]\n",
      "loss: 0.247865  [38400/175341]\n",
      "loss: 0.407697  [40000/175341]\n",
      "loss: 0.398949  [41600/175341]\n",
      "loss: 0.568336  [43200/175341]\n",
      "loss: 0.801264  [44800/175341]\n",
      "loss: 0.708252  [46400/175341]\n",
      "loss: 0.208538  [48000/175341]\n",
      "loss: 0.260547  [49600/175341]\n",
      "loss: 0.168333  [51200/175341]\n",
      "loss: 0.534760  [52800/175341]\n",
      "loss: 0.273082  [54400/175341]\n",
      "loss: 0.503696  [56000/175341]\n",
      "loss: 0.973673  [57600/175341]\n",
      "loss: 0.475732  [59200/175341]\n",
      "loss: 0.320611  [60800/175341]\n",
      "loss: 0.690080  [62400/175341]\n",
      "loss: 0.170644  [64000/175341]\n",
      "loss: 0.327396  [65600/175341]\n",
      "loss: 0.551308  [67200/175341]\n",
      "loss: 0.270852  [68800/175341]\n",
      "loss: 0.376146  [70400/175341]\n",
      "loss: 0.460584  [72000/175341]\n",
      "loss: 0.508894  [73600/175341]\n",
      "loss: 0.487978  [75200/175341]\n",
      "loss: 0.309462  [76800/175341]\n",
      "loss: 0.198666  [78400/175341]\n",
      "loss: 0.304414  [80000/175341]\n",
      "loss: 0.357494  [81600/175341]\n",
      "loss: 0.340331  [83200/175341]\n",
      "loss: 0.847813  [84800/175341]\n",
      "loss: 0.514256  [86400/175341]\n",
      "loss: 0.937925  [88000/175341]\n",
      "loss: 0.330394  [89600/175341]\n",
      "loss: 0.340028  [91200/175341]\n",
      "loss: 0.615114  [92800/175341]\n",
      "loss: 0.196999  [94400/175341]\n",
      "loss: 0.644426  [96000/175341]\n",
      "loss: 0.965864  [97600/175341]\n",
      "loss: 0.502837  [99200/175341]\n",
      "loss: 0.466885  [100800/175341]\n",
      "loss: 0.576939  [102400/175341]\n",
      "loss: 0.342357  [104000/175341]\n",
      "loss: 0.434152  [105600/175341]\n",
      "loss: 1.095325  [107200/175341]\n",
      "loss: 0.662539  [108800/175341]\n",
      "loss: 0.244255  [110400/175341]\n",
      "loss: 0.731986  [112000/175341]\n",
      "loss: 0.319735  [113600/175341]\n",
      "loss: 0.499276  [115200/175341]\n",
      "loss: 0.562644  [116800/175341]\n",
      "loss: 0.207571  [118400/175341]\n",
      "loss: 0.314130  [120000/175341]\n",
      "loss: 0.889326  [121600/175341]\n",
      "loss: 0.315596  [123200/175341]\n",
      "loss: 0.307433  [124800/175341]\n",
      "loss: 0.315464  [126400/175341]\n",
      "loss: 0.365394  [128000/175341]\n",
      "loss: 0.216901  [129600/175341]\n",
      "loss: 0.354382  [131200/175341]\n",
      "loss: 0.392481  [132800/175341]\n",
      "loss: 0.374886  [134400/175341]\n",
      "loss: 0.432055  [136000/175341]\n",
      "loss: 0.614517  [137600/175341]\n",
      "loss: 0.263385  [139200/175341]\n",
      "loss: 0.243967  [140800/175341]\n",
      "loss: 0.578757  [142400/175341]\n",
      "loss: 0.405237  [144000/175341]\n",
      "loss: 0.475475  [145600/175341]\n",
      "loss: 0.859069  [147200/175341]\n",
      "loss: 0.413235  [148800/175341]\n",
      "loss: 0.359685  [150400/175341]\n",
      "loss: 0.616100  [152000/175341]\n",
      "loss: 0.524168  [153600/175341]\n",
      "loss: 0.394112  [155200/175341]\n",
      "loss: 0.590621  [156800/175341]\n",
      "loss: 0.926532  [158400/175341]\n",
      "loss: 0.245740  [160000/175341]\n",
      "loss: 0.676614  [161600/175341]\n",
      "loss: 0.661524  [163200/175341]\n",
      "loss: 0.218690  [164800/175341]\n",
      "loss: 0.411394  [166400/175341]\n",
      "loss: 0.372528  [168000/175341]\n",
      "loss: 0.226503  [169600/175341]\n",
      "loss: 0.509772  [171200/175341]\n",
      "loss: 0.418531  [172800/175341]\n",
      "loss: 0.265474  [174400/175341]\n",
      "Train Accuracy: 81.9546%\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.568548, F1-score: 75.42%, Macro_F1-Score:  41.39%  \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.624126  [    0/175341]\n",
      "loss: 0.256121  [ 1600/175341]\n",
      "loss: 0.347868  [ 3200/175341]\n",
      "loss: 0.767422  [ 4800/175341]\n",
      "loss: 0.390191  [ 6400/175341]\n",
      "loss: 0.333233  [ 8000/175341]\n",
      "loss: 0.717975  [ 9600/175341]\n",
      "loss: 0.854312  [11200/175341]\n",
      "loss: 0.315921  [12800/175341]\n",
      "loss: 0.758985  [14400/175341]\n",
      "loss: 0.153152  [16000/175341]\n",
      "loss: 0.067714  [17600/175341]\n",
      "loss: 1.094421  [19200/175341]\n",
      "loss: 0.439774  [20800/175341]\n",
      "loss: 0.307705  [22400/175341]\n",
      "loss: 0.231866  [24000/175341]\n",
      "loss: 0.140305  [25600/175341]\n",
      "loss: 0.520928  [27200/175341]\n",
      "loss: 0.238182  [28800/175341]\n",
      "loss: 0.327442  [30400/175341]\n",
      "loss: 0.962746  [32000/175341]\n",
      "loss: 0.743955  [33600/175341]\n",
      "loss: 0.735421  [35200/175341]\n",
      "loss: 0.509351  [36800/175341]\n",
      "loss: 0.357349  [38400/175341]\n",
      "loss: 0.465425  [40000/175341]\n",
      "loss: 0.338138  [41600/175341]\n",
      "loss: 0.913736  [43200/175341]\n",
      "loss: 0.165361  [44800/175341]\n",
      "loss: 0.392211  [46400/175341]\n",
      "loss: 0.620391  [48000/175341]\n",
      "loss: 0.284837  [49600/175341]\n",
      "loss: 0.651124  [51200/175341]\n",
      "loss: 0.353099  [52800/175341]\n",
      "loss: 0.885635  [54400/175341]\n",
      "loss: 0.307299  [56000/175341]\n",
      "loss: 0.307332  [57600/175341]\n",
      "loss: 0.362974  [59200/175341]\n",
      "loss: 0.180877  [60800/175341]\n",
      "loss: 0.476333  [62400/175341]\n",
      "loss: 0.216729  [64000/175341]\n",
      "loss: 0.337982  [65600/175341]\n",
      "loss: 0.687328  [67200/175341]\n",
      "loss: 0.474020  [68800/175341]\n",
      "loss: 0.402670  [70400/175341]\n",
      "loss: 0.306897  [72000/175341]\n",
      "loss: 0.731605  [73600/175341]\n",
      "loss: 0.419233  [75200/175341]\n",
      "loss: 0.294632  [76800/175341]\n",
      "loss: 0.627201  [78400/175341]\n",
      "loss: 0.402352  [80000/175341]\n",
      "loss: 0.294373  [81600/175341]\n",
      "loss: 0.105787  [83200/175341]\n",
      "loss: 0.716616  [84800/175341]\n",
      "loss: 0.544971  [86400/175341]\n",
      "loss: 0.495835  [88000/175341]\n",
      "loss: 0.460115  [89600/175341]\n",
      "loss: 0.321026  [91200/175341]\n",
      "loss: 0.779612  [92800/175341]\n",
      "loss: 0.247649  [94400/175341]\n",
      "loss: 0.519861  [96000/175341]\n",
      "loss: 0.405650  [97600/175341]\n",
      "loss: 0.515245  [99200/175341]\n",
      "loss: 0.306490  [100800/175341]\n",
      "loss: 0.616902  [102400/175341]\n",
      "loss: 0.181309  [104000/175341]\n",
      "loss: 0.245456  [105600/175341]\n",
      "loss: 0.510027  [107200/175341]\n",
      "loss: 0.376258  [108800/175341]\n",
      "loss: 0.267252  [110400/175341]\n",
      "loss: 0.271301  [112000/175341]\n",
      "loss: 0.391685  [113600/175341]\n",
      "loss: 0.664764  [115200/175341]\n",
      "loss: 0.169579  [116800/175341]\n",
      "loss: 0.322557  [118400/175341]\n",
      "loss: 0.434669  [120000/175341]\n",
      "loss: 0.464523  [121600/175341]\n",
      "loss: 0.546578  [123200/175341]\n",
      "loss: 0.306144  [124800/175341]\n",
      "loss: 0.501397  [126400/175341]\n",
      "loss: 0.399422  [128000/175341]\n",
      "loss: 0.347513  [129600/175341]\n",
      "loss: 0.551255  [131200/175341]\n",
      "loss: 1.138863  [132800/175341]\n",
      "loss: 0.407505  [134400/175341]\n",
      "loss: 0.561533  [136000/175341]\n",
      "loss: 0.548741  [137600/175341]\n",
      "loss: 0.358728  [139200/175341]\n",
      "loss: 0.746020  [140800/175341]\n",
      "loss: 0.417396  [142400/175341]\n",
      "loss: 0.559045  [144000/175341]\n",
      "loss: 0.734447  [145600/175341]\n",
      "loss: 0.190220  [147200/175341]\n",
      "loss: 0.326404  [148800/175341]\n",
      "loss: 0.287037  [150400/175341]\n",
      "loss: 0.252737  [152000/175341]\n",
      "loss: 0.271405  [153600/175341]\n",
      "loss: 0.214683  [155200/175341]\n",
      "loss: 0.353223  [156800/175341]\n",
      "loss: 0.441429  [158400/175341]\n",
      "loss: 0.521887  [160000/175341]\n",
      "loss: 0.449626  [161600/175341]\n",
      "loss: 0.551000  [163200/175341]\n",
      "loss: 0.312032  [164800/175341]\n",
      "loss: 0.795713  [166400/175341]\n",
      "loss: 0.597078  [168000/175341]\n",
      "loss: 0.540363  [169600/175341]\n",
      "loss: 0.750795  [171200/175341]\n",
      "loss: 0.666499  [172800/175341]\n",
      "loss: 0.876795  [174400/175341]\n",
      "Train Accuracy: 81.9814%\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.542798, F1-score: 76.78%, Macro_F1-Score:  42.19%  \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.213983  [    0/175341]\n",
      "loss: 0.473556  [ 1600/175341]\n",
      "loss: 0.323790  [ 3200/175341]\n",
      "loss: 0.560744  [ 4800/175341]\n",
      "loss: 0.352308  [ 6400/175341]\n",
      "loss: 0.351212  [ 8000/175341]\n",
      "loss: 0.260344  [ 9600/175341]\n",
      "loss: 0.655904  [11200/175341]\n",
      "loss: 0.275100  [12800/175341]\n",
      "loss: 0.310564  [14400/175341]\n",
      "loss: 0.262448  [16000/175341]\n",
      "loss: 0.315495  [17600/175341]\n",
      "loss: 0.697457  [19200/175341]\n",
      "loss: 0.320582  [20800/175341]\n",
      "loss: 0.396937  [22400/175341]\n",
      "loss: 0.336900  [24000/175341]\n",
      "loss: 0.265121  [25600/175341]\n",
      "loss: 0.459731  [27200/175341]\n",
      "loss: 0.345559  [28800/175341]\n",
      "loss: 0.585217  [30400/175341]\n",
      "loss: 0.337312  [32000/175341]\n",
      "loss: 0.369979  [33600/175341]\n",
      "loss: 0.449071  [35200/175341]\n",
      "loss: 0.264728  [36800/175341]\n",
      "loss: 0.465236  [38400/175341]\n",
      "loss: 0.233939  [40000/175341]\n",
      "loss: 0.592848  [41600/175341]\n",
      "loss: 0.344363  [43200/175341]\n",
      "loss: 0.619560  [44800/175341]\n",
      "loss: 0.297130  [46400/175341]\n",
      "loss: 0.225558  [48000/175341]\n",
      "loss: 0.294714  [49600/175341]\n",
      "loss: 0.819858  [51200/175341]\n",
      "loss: 0.223728  [52800/175341]\n",
      "loss: 0.502537  [54400/175341]\n",
      "loss: 0.503725  [56000/175341]\n",
      "loss: 0.328432  [57600/175341]\n",
      "loss: 0.144433  [59200/175341]\n",
      "loss: 0.769931  [60800/175341]\n",
      "loss: 0.465651  [62400/175341]\n",
      "loss: 0.436750  [64000/175341]\n",
      "loss: 0.267516  [65600/175341]\n",
      "loss: 0.398944  [67200/175341]\n",
      "loss: 0.296908  [68800/175341]\n",
      "loss: 0.469413  [70400/175341]\n",
      "loss: 0.445000  [72000/175341]\n",
      "loss: 0.526416  [73600/175341]\n",
      "loss: 0.236778  [75200/175341]\n",
      "loss: 0.555806  [76800/175341]\n",
      "loss: 0.327390  [78400/175341]\n",
      "loss: 0.713474  [80000/175341]\n",
      "loss: 0.412993  [81600/175341]\n",
      "loss: 0.437548  [83200/175341]\n",
      "loss: 0.313189  [84800/175341]\n",
      "loss: 0.196053  [86400/175341]\n",
      "loss: 0.541433  [88000/175341]\n",
      "loss: 0.641675  [89600/175341]\n",
      "loss: 0.710952  [91200/175341]\n",
      "loss: 0.298172  [92800/175341]\n",
      "loss: 0.459423  [94400/175341]\n",
      "loss: 0.509638  [96000/175341]\n",
      "loss: 0.173252  [97600/175341]\n",
      "loss: 0.787319  [99200/175341]\n",
      "loss: 0.425750  [100800/175341]\n",
      "loss: 0.684092  [102400/175341]\n",
      "loss: 0.614478  [104000/175341]\n",
      "loss: 0.221804  [105600/175341]\n",
      "loss: 0.190034  [107200/175341]\n",
      "loss: 0.698320  [108800/175341]\n",
      "loss: 0.458010  [110400/175341]\n",
      "loss: 0.686356  [112000/175341]\n",
      "loss: 0.468646  [113600/175341]\n",
      "loss: 0.684651  [115200/175341]\n",
      "loss: 0.937022  [116800/175341]\n",
      "loss: 0.580465  [118400/175341]\n",
      "loss: 0.439130  [120000/175341]\n",
      "loss: 0.488971  [121600/175341]\n",
      "loss: 0.350418  [123200/175341]\n",
      "loss: 0.283409  [124800/175341]\n",
      "loss: 0.559955  [126400/175341]\n",
      "loss: 0.582853  [128000/175341]\n",
      "loss: 0.528802  [129600/175341]\n",
      "loss: 0.486820  [131200/175341]\n",
      "loss: 0.177007  [132800/175341]\n",
      "loss: 0.307142  [134400/175341]\n",
      "loss: 0.702202  [136000/175341]\n",
      "loss: 0.274785  [137600/175341]\n",
      "loss: 0.401870  [139200/175341]\n",
      "loss: 0.560180  [140800/175341]\n",
      "loss: 0.196572  [142400/175341]\n",
      "loss: 0.510137  [144000/175341]\n",
      "loss: 0.351645  [145600/175341]\n",
      "loss: 0.334810  [147200/175341]\n",
      "loss: 0.529417  [148800/175341]\n",
      "loss: 0.232448  [150400/175341]\n",
      "loss: 0.223906  [152000/175341]\n",
      "loss: 0.704362  [153600/175341]\n",
      "loss: 0.424390  [155200/175341]\n",
      "loss: 0.621655  [156800/175341]\n",
      "loss: 0.400174  [158400/175341]\n",
      "loss: 0.397976  [160000/175341]\n",
      "loss: 0.315799  [161600/175341]\n",
      "loss: 0.257271  [163200/175341]\n",
      "loss: 0.364814  [164800/175341]\n",
      "loss: 0.497766  [166400/175341]\n",
      "loss: 0.183091  [168000/175341]\n",
      "loss: 0.381719  [169600/175341]\n",
      "loss: 0.338172  [171200/175341]\n",
      "loss: 0.350228  [172800/175341]\n",
      "loss: 0.689580  [174400/175341]\n",
      "Train Accuracy: 81.9443%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.546926, F1-score: 76.60%, Macro_F1-Score:  41.93%  \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.606381  [    0/175341]\n",
      "loss: 0.350227  [ 1600/175341]\n",
      "loss: 0.331425  [ 3200/175341]\n",
      "loss: 0.641946  [ 4800/175341]\n",
      "loss: 0.450468  [ 6400/175341]\n",
      "loss: 0.433156  [ 8000/175341]\n",
      "loss: 0.564329  [ 9600/175341]\n",
      "loss: 0.327925  [11200/175341]\n",
      "loss: 0.327530  [12800/175341]\n",
      "loss: 0.264523  [14400/175341]\n",
      "loss: 0.685916  [16000/175341]\n",
      "loss: 0.490472  [17600/175341]\n",
      "loss: 0.279644  [19200/175341]\n",
      "loss: 0.205810  [20800/175341]\n",
      "loss: 0.151717  [22400/175341]\n",
      "loss: 0.727993  [24000/175341]\n",
      "loss: 0.570552  [25600/175341]\n",
      "loss: 0.531673  [27200/175341]\n",
      "loss: 0.319782  [28800/175341]\n",
      "loss: 0.471629  [30400/175341]\n",
      "loss: 0.572037  [32000/175341]\n",
      "loss: 0.431648  [33600/175341]\n",
      "loss: 0.285500  [35200/175341]\n",
      "loss: 0.454577  [36800/175341]\n",
      "loss: 0.349556  [38400/175341]\n",
      "loss: 0.272617  [40000/175341]\n",
      "loss: 0.694701  [41600/175341]\n",
      "loss: 0.231908  [43200/175341]\n",
      "loss: 0.190820  [44800/175341]\n",
      "loss: 0.457799  [46400/175341]\n",
      "loss: 0.507577  [48000/175341]\n",
      "loss: 0.254115  [49600/175341]\n",
      "loss: 0.384597  [51200/175341]\n",
      "loss: 0.307456  [52800/175341]\n",
      "loss: 0.447272  [54400/175341]\n",
      "loss: 0.520592  [56000/175341]\n",
      "loss: 0.488361  [57600/175341]\n",
      "loss: 0.792873  [59200/175341]\n",
      "loss: 0.737367  [60800/175341]\n",
      "loss: 0.565981  [62400/175341]\n",
      "loss: 0.361125  [64000/175341]\n",
      "loss: 0.711924  [65600/175341]\n",
      "loss: 0.602433  [67200/175341]\n",
      "loss: 0.352286  [68800/175341]\n",
      "loss: 0.751136  [70400/175341]\n",
      "loss: 0.426775  [72000/175341]\n",
      "loss: 0.557256  [73600/175341]\n",
      "loss: 0.483692  [75200/175341]\n",
      "loss: 0.191816  [76800/175341]\n",
      "loss: 0.467106  [78400/175341]\n",
      "loss: 0.462678  [80000/175341]\n",
      "loss: 0.313484  [81600/175341]\n",
      "loss: 0.729591  [83200/175341]\n",
      "loss: 0.231187  [84800/175341]\n",
      "loss: 0.634554  [86400/175341]\n",
      "loss: 0.456353  [88000/175341]\n",
      "loss: 0.572418  [89600/175341]\n",
      "loss: 0.364413  [91200/175341]\n",
      "loss: 0.311477  [92800/175341]\n",
      "loss: 0.851740  [94400/175341]\n",
      "loss: 0.466196  [96000/175341]\n",
      "loss: 0.940610  [97600/175341]\n",
      "loss: 0.616646  [99200/175341]\n",
      "loss: 0.423623  [100800/175341]\n",
      "loss: 0.474652  [102400/175341]\n",
      "loss: 0.379781  [104000/175341]\n",
      "loss: 0.591996  [105600/175341]\n",
      "loss: 0.613090  [107200/175341]\n",
      "loss: 0.588652  [108800/175341]\n",
      "loss: 0.571600  [110400/175341]\n",
      "loss: 0.383976  [112000/175341]\n",
      "loss: 0.743937  [113600/175341]\n",
      "loss: 0.175922  [115200/175341]\n",
      "loss: 0.592577  [116800/175341]\n",
      "loss: 0.176524  [118400/175341]\n",
      "loss: 0.464153  [120000/175341]\n",
      "loss: 0.414398  [121600/175341]\n",
      "loss: 0.465409  [123200/175341]\n",
      "loss: 0.539636  [124800/175341]\n",
      "loss: 0.652201  [126400/175341]\n",
      "loss: 0.665890  [128000/175341]\n",
      "loss: 0.744032  [129600/175341]\n",
      "loss: 0.548362  [131200/175341]\n",
      "loss: 0.237583  [132800/175341]\n",
      "loss: 0.888059  [134400/175341]\n",
      "loss: 0.518432  [136000/175341]\n",
      "loss: 0.288194  [137600/175341]\n",
      "loss: 0.308565  [139200/175341]\n",
      "loss: 0.851275  [140800/175341]\n",
      "loss: 0.246061  [142400/175341]\n",
      "loss: 0.376395  [144000/175341]\n",
      "loss: 0.299771  [145600/175341]\n",
      "loss: 0.450919  [147200/175341]\n",
      "loss: 0.570689  [148800/175341]\n",
      "loss: 0.311812  [150400/175341]\n",
      "loss: 0.434137  [152000/175341]\n",
      "loss: 0.750519  [153600/175341]\n",
      "loss: 0.887502  [155200/175341]\n",
      "loss: 0.420627  [156800/175341]\n",
      "loss: 0.287087  [158400/175341]\n",
      "loss: 0.781829  [160000/175341]\n",
      "loss: 0.315937  [161600/175341]\n",
      "loss: 0.328475  [163200/175341]\n",
      "loss: 0.452917  [164800/175341]\n",
      "loss: 0.401475  [166400/175341]\n",
      "loss: 0.291405  [168000/175341]\n",
      "loss: 0.669357  [169600/175341]\n",
      "loss: 0.106745  [171200/175341]\n",
      "loss: 0.692884  [172800/175341]\n",
      "loss: 0.192018  [174400/175341]\n",
      "Train Accuracy: 81.9255%\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.551339, F1-score: 75.90%, Macro_F1-Score:  42.08%  \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.527202  [    0/175341]\n",
      "loss: 0.312769  [ 1600/175341]\n",
      "loss: 0.300394  [ 3200/175341]\n",
      "loss: 0.089053  [ 4800/175341]\n",
      "loss: 0.657193  [ 6400/175341]\n",
      "loss: 0.568263  [ 8000/175341]\n",
      "loss: 0.276658  [ 9600/175341]\n",
      "loss: 0.317016  [11200/175341]\n",
      "loss: 0.538733  [12800/175341]\n",
      "loss: 0.501483  [14400/175341]\n",
      "loss: 0.878718  [16000/175341]\n",
      "loss: 0.522619  [17600/175341]\n",
      "loss: 0.477514  [19200/175341]\n",
      "loss: 0.533571  [20800/175341]\n",
      "loss: 0.213838  [22400/175341]\n",
      "loss: 0.133029  [24000/175341]\n",
      "loss: 0.387953  [25600/175341]\n",
      "loss: 0.189735  [27200/175341]\n",
      "loss: 0.404873  [28800/175341]\n",
      "loss: 0.487845  [30400/175341]\n",
      "loss: 0.312939  [32000/175341]\n",
      "loss: 0.252904  [33600/175341]\n",
      "loss: 0.232078  [35200/175341]\n",
      "loss: 0.267076  [36800/175341]\n",
      "loss: 0.599225  [38400/175341]\n",
      "loss: 0.485022  [40000/175341]\n",
      "loss: 0.256896  [41600/175341]\n",
      "loss: 0.227725  [43200/175341]\n",
      "loss: 0.488291  [44800/175341]\n",
      "loss: 0.628494  [46400/175341]\n",
      "loss: 0.427206  [48000/175341]\n",
      "loss: 0.162971  [49600/175341]\n",
      "loss: 0.252891  [51200/175341]\n",
      "loss: 0.320541  [52800/175341]\n",
      "loss: 0.261979  [54400/175341]\n",
      "loss: 0.643488  [56000/175341]\n",
      "loss: 0.585717  [57600/175341]\n",
      "loss: 0.805192  [59200/175341]\n",
      "loss: 0.332380  [60800/175341]\n",
      "loss: 0.368816  [62400/175341]\n",
      "loss: 0.430315  [64000/175341]\n",
      "loss: 0.511173  [65600/175341]\n",
      "loss: 0.161117  [67200/175341]\n",
      "loss: 0.580333  [68800/175341]\n",
      "loss: 0.200230  [70400/175341]\n",
      "loss: 0.361062  [72000/175341]\n",
      "loss: 0.448396  [73600/175341]\n",
      "loss: 0.800974  [75200/175341]\n",
      "loss: 0.170710  [76800/175341]\n",
      "loss: 0.259958  [78400/175341]\n",
      "loss: 0.175498  [80000/175341]\n",
      "loss: 1.316008  [81600/175341]\n",
      "loss: 0.374357  [83200/175341]\n",
      "loss: 0.309842  [84800/175341]\n",
      "loss: 0.280089  [86400/175341]\n",
      "loss: 0.408259  [88000/175341]\n",
      "loss: 0.250651  [89600/175341]\n",
      "loss: 0.489449  [91200/175341]\n",
      "loss: 0.412298  [92800/175341]\n",
      "loss: 0.341359  [94400/175341]\n",
      "loss: 0.755046  [96000/175341]\n",
      "loss: 0.374092  [97600/175341]\n",
      "loss: 0.501857  [99200/175341]\n",
      "loss: 0.203958  [100800/175341]\n",
      "loss: 0.152680  [102400/175341]\n",
      "loss: 0.497748  [104000/175341]\n",
      "loss: 0.593239  [105600/175341]\n",
      "loss: 0.392026  [107200/175341]\n",
      "loss: 0.257450  [108800/175341]\n",
      "loss: 0.053162  [110400/175341]\n",
      "loss: 0.657483  [112000/175341]\n",
      "loss: 0.573676  [113600/175341]\n",
      "loss: 0.860320  [115200/175341]\n",
      "loss: 0.320564  [116800/175341]\n",
      "loss: 0.264136  [118400/175341]\n",
      "loss: 0.087501  [120000/175341]\n",
      "loss: 0.293467  [121600/175341]\n",
      "loss: 0.336848  [123200/175341]\n",
      "loss: 0.130494  [124800/175341]\n",
      "loss: 0.274634  [126400/175341]\n",
      "loss: 0.130851  [128000/175341]\n",
      "loss: 0.439263  [129600/175341]\n",
      "loss: 0.369517  [131200/175341]\n",
      "loss: 0.685115  [132800/175341]\n",
      "loss: 0.730356  [134400/175341]\n",
      "loss: 0.450545  [136000/175341]\n",
      "loss: 0.427004  [137600/175341]\n",
      "loss: 0.204386  [139200/175341]\n",
      "loss: 0.291127  [140800/175341]\n",
      "loss: 0.181654  [142400/175341]\n",
      "loss: 0.392381  [144000/175341]\n",
      "loss: 0.385896  [145600/175341]\n",
      "loss: 0.199040  [147200/175341]\n",
      "loss: 0.425649  [148800/175341]\n",
      "loss: 0.291189  [150400/175341]\n",
      "loss: 0.306371  [152000/175341]\n",
      "loss: 0.360192  [153600/175341]\n",
      "loss: 0.390320  [155200/175341]\n",
      "loss: 0.488240  [156800/175341]\n",
      "loss: 0.393347  [158400/175341]\n",
      "loss: 0.641736  [160000/175341]\n",
      "loss: 0.460339  [161600/175341]\n",
      "loss: 0.477812  [163200/175341]\n",
      "loss: 0.441996  [164800/175341]\n",
      "loss: 0.592400  [166400/175341]\n",
      "loss: 0.138695  [168000/175341]\n",
      "loss: 0.624853  [169600/175341]\n",
      "loss: 0.359652  [171200/175341]\n",
      "loss: 0.309455  [172800/175341]\n",
      "loss: 0.441330  [174400/175341]\n",
      "Train Accuracy: 81.9432%\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.533576, F1-score: 77.08%, Macro_F1-Score:  42.68%  \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.498510  [    0/175341]\n",
      "loss: 0.325444  [ 1600/175341]\n",
      "loss: 0.705999  [ 3200/175341]\n",
      "loss: 0.402396  [ 4800/175341]\n",
      "loss: 0.373833  [ 6400/175341]\n",
      "loss: 0.395619  [ 8000/175341]\n",
      "loss: 0.607355  [ 9600/175341]\n",
      "loss: 0.832388  [11200/175341]\n",
      "loss: 0.261076  [12800/175341]\n",
      "loss: 0.329288  [14400/175341]\n",
      "loss: 0.599907  [16000/175341]\n",
      "loss: 0.466712  [17600/175341]\n",
      "loss: 0.256689  [19200/175341]\n",
      "loss: 0.239955  [20800/175341]\n",
      "loss: 0.088568  [22400/175341]\n",
      "loss: 0.735660  [24000/175341]\n",
      "loss: 0.390733  [25600/175341]\n",
      "loss: 0.235267  [27200/175341]\n",
      "loss: 0.301472  [28800/175341]\n",
      "loss: 0.659293  [30400/175341]\n",
      "loss: 0.183088  [32000/175341]\n",
      "loss: 0.373167  [33600/175341]\n",
      "loss: 0.675758  [35200/175341]\n",
      "loss: 0.362169  [36800/175341]\n",
      "loss: 0.698115  [38400/175341]\n",
      "loss: 0.568049  [40000/175341]\n",
      "loss: 0.468086  [41600/175341]\n",
      "loss: 0.210111  [43200/175341]\n",
      "loss: 0.522479  [44800/175341]\n",
      "loss: 0.835860  [46400/175341]\n",
      "loss: 0.645681  [48000/175341]\n",
      "loss: 0.227912  [49600/175341]\n",
      "loss: 0.277406  [51200/175341]\n",
      "loss: 0.445041  [52800/175341]\n",
      "loss: 0.645141  [54400/175341]\n",
      "loss: 0.479208  [56000/175341]\n",
      "loss: 0.378912  [57600/175341]\n",
      "loss: 0.689779  [59200/175341]\n",
      "loss: 0.324114  [60800/175341]\n",
      "loss: 0.285107  [62400/175341]\n",
      "loss: 0.989434  [64000/175341]\n",
      "loss: 0.511648  [65600/175341]\n",
      "loss: 0.365718  [67200/175341]\n",
      "loss: 0.379765  [68800/175341]\n",
      "loss: 0.657879  [70400/175341]\n",
      "loss: 0.312401  [72000/175341]\n",
      "loss: 0.480337  [73600/175341]\n",
      "loss: 0.346029  [75200/175341]\n",
      "loss: 0.377193  [76800/175341]\n",
      "loss: 0.284411  [78400/175341]\n",
      "loss: 0.443114  [80000/175341]\n",
      "loss: 0.485982  [81600/175341]\n",
      "loss: 0.462468  [83200/175341]\n",
      "loss: 0.990475  [84800/175341]\n",
      "loss: 0.326077  [86400/175341]\n",
      "loss: 0.473578  [88000/175341]\n",
      "loss: 0.209950  [89600/175341]\n",
      "loss: 0.872973  [91200/175341]\n",
      "loss: 0.291733  [92800/175341]\n",
      "loss: 0.590321  [94400/175341]\n",
      "loss: 0.125472  [96000/175341]\n",
      "loss: 0.288738  [97600/175341]\n",
      "loss: 0.443009  [99200/175341]\n",
      "loss: 0.235958  [100800/175341]\n",
      "loss: 0.480178  [102400/175341]\n",
      "loss: 0.206520  [104000/175341]\n",
      "loss: 0.459724  [105600/175341]\n",
      "loss: 0.210824  [107200/175341]\n",
      "loss: 0.384314  [108800/175341]\n",
      "loss: 0.439133  [110400/175341]\n",
      "loss: 0.572279  [112000/175341]\n",
      "loss: 0.474780  [113600/175341]\n",
      "loss: 0.493307  [115200/175341]\n",
      "loss: 0.230062  [116800/175341]\n",
      "loss: 0.313278  [118400/175341]\n",
      "loss: 0.670922  [120000/175341]\n",
      "loss: 0.075026  [121600/175341]\n",
      "loss: 0.684504  [123200/175341]\n",
      "loss: 0.317636  [124800/175341]\n",
      "loss: 0.625244  [126400/175341]\n",
      "loss: 0.506868  [128000/175341]\n",
      "loss: 0.304192  [129600/175341]\n",
      "loss: 0.170413  [131200/175341]\n",
      "loss: 0.570813  [132800/175341]\n",
      "loss: 0.197892  [134400/175341]\n",
      "loss: 0.345152  [136000/175341]\n",
      "loss: 0.648037  [137600/175341]\n",
      "loss: 0.478072  [139200/175341]\n",
      "loss: 0.347029  [140800/175341]\n",
      "loss: 0.446268  [142400/175341]\n",
      "loss: 0.455182  [144000/175341]\n",
      "loss: 0.445693  [145600/175341]\n",
      "loss: 0.353819  [147200/175341]\n",
      "loss: 0.366036  [148800/175341]\n",
      "loss: 0.195026  [150400/175341]\n",
      "loss: 0.380682  [152000/175341]\n",
      "loss: 0.784101  [153600/175341]\n",
      "loss: 0.572747  [155200/175341]\n",
      "loss: 1.041604  [156800/175341]\n",
      "loss: 0.183243  [158400/175341]\n",
      "loss: 0.560137  [160000/175341]\n",
      "loss: 0.369016  [161600/175341]\n",
      "loss: 0.199888  [163200/175341]\n",
      "loss: 0.554445  [164800/175341]\n",
      "loss: 0.676460  [166400/175341]\n",
      "loss: 0.526537  [168000/175341]\n",
      "loss: 0.498983  [169600/175341]\n",
      "loss: 0.473080  [171200/175341]\n",
      "loss: 0.638821  [172800/175341]\n",
      "loss: 0.259999  [174400/175341]\n",
      "Train Accuracy: 81.9603%\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.560677, F1-score: 75.61%, Macro_F1-Score:  41.45%  \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.420959  [    0/175341]\n",
      "loss: 0.494376  [ 1600/175341]\n",
      "loss: 0.219928  [ 3200/175341]\n",
      "loss: 0.914172  [ 4800/175341]\n",
      "loss: 0.103452  [ 6400/175341]\n",
      "loss: 1.007026  [ 8000/175341]\n",
      "loss: 0.349010  [ 9600/175341]\n",
      "loss: 0.365281  [11200/175341]\n",
      "loss: 0.387811  [12800/175341]\n",
      "loss: 0.756473  [14400/175341]\n",
      "loss: 0.177510  [16000/175341]\n",
      "loss: 0.141232  [17600/175341]\n",
      "loss: 0.495023  [19200/175341]\n",
      "loss: 0.294564  [20800/175341]\n",
      "loss: 0.654764  [22400/175341]\n",
      "loss: 0.468303  [24000/175341]\n",
      "loss: 0.413000  [25600/175341]\n",
      "loss: 0.241761  [27200/175341]\n",
      "loss: 0.453180  [28800/175341]\n",
      "loss: 0.304386  [30400/175341]\n",
      "loss: 0.545110  [32000/175341]\n",
      "loss: 0.351769  [33600/175341]\n",
      "loss: 0.261190  [35200/175341]\n",
      "loss: 0.285428  [36800/175341]\n",
      "loss: 0.512195  [38400/175341]\n",
      "loss: 0.468363  [40000/175341]\n",
      "loss: 0.207459  [41600/175341]\n",
      "loss: 0.859289  [43200/175341]\n",
      "loss: 0.250833  [44800/175341]\n",
      "loss: 0.327879  [46400/175341]\n",
      "loss: 0.302811  [48000/175341]\n",
      "loss: 0.409137  [49600/175341]\n",
      "loss: 0.626277  [51200/175341]\n",
      "loss: 0.357683  [52800/175341]\n",
      "loss: 0.229036  [54400/175341]\n",
      "loss: 0.376548  [56000/175341]\n",
      "loss: 0.175042  [57600/175341]\n",
      "loss: 0.455712  [59200/175341]\n",
      "loss: 0.399549  [60800/175341]\n",
      "loss: 0.665149  [62400/175341]\n",
      "loss: 0.559941  [64000/175341]\n",
      "loss: 0.758101  [65600/175341]\n",
      "loss: 0.469823  [67200/175341]\n",
      "loss: 0.528825  [68800/175341]\n",
      "loss: 0.392625  [70400/175341]\n",
      "loss: 0.352197  [72000/175341]\n",
      "loss: 0.592831  [73600/175341]\n",
      "loss: 0.499582  [75200/175341]\n",
      "loss: 0.343486  [76800/175341]\n",
      "loss: 0.581055  [78400/175341]\n",
      "loss: 0.293622  [80000/175341]\n",
      "loss: 0.583112  [81600/175341]\n",
      "loss: 0.476715  [83200/175341]\n",
      "loss: 0.643541  [84800/175341]\n",
      "loss: 0.500368  [86400/175341]\n",
      "loss: 0.345783  [88000/175341]\n",
      "loss: 0.827460  [89600/175341]\n",
      "loss: 0.278231  [91200/175341]\n",
      "loss: 0.251337  [92800/175341]\n",
      "loss: 0.710716  [94400/175341]\n",
      "loss: 0.190377  [96000/175341]\n",
      "loss: 0.321488  [97600/175341]\n",
      "loss: 0.402288  [99200/175341]\n",
      "loss: 0.392058  [100800/175341]\n",
      "loss: 0.854482  [102400/175341]\n",
      "loss: 0.600694  [104000/175341]\n",
      "loss: 0.297939  [105600/175341]\n",
      "loss: 0.377832  [107200/175341]\n",
      "loss: 0.788761  [108800/175341]\n",
      "loss: 0.235526  [110400/175341]\n",
      "loss: 0.511517  [112000/175341]\n",
      "loss: 0.391417  [113600/175341]\n",
      "loss: 0.324348  [115200/175341]\n",
      "loss: 0.335347  [116800/175341]\n",
      "loss: 0.314954  [118400/175341]\n",
      "loss: 0.361321  [120000/175341]\n",
      "loss: 0.606647  [121600/175341]\n",
      "loss: 0.531165  [123200/175341]\n",
      "loss: 0.325944  [124800/175341]\n",
      "loss: 1.097021  [126400/175341]\n",
      "loss: 0.440096  [128000/175341]\n",
      "loss: 0.139944  [129600/175341]\n",
      "loss: 0.266772  [131200/175341]\n",
      "loss: 0.358509  [132800/175341]\n",
      "loss: 0.918151  [134400/175341]\n",
      "loss: 0.713131  [136000/175341]\n",
      "loss: 0.199766  [137600/175341]\n",
      "loss: 0.091686  [139200/175341]\n",
      "loss: 0.478543  [140800/175341]\n",
      "loss: 0.553741  [142400/175341]\n",
      "loss: 0.412454  [144000/175341]\n",
      "loss: 0.746542  [145600/175341]\n",
      "loss: 0.293360  [147200/175341]\n",
      "loss: 0.609894  [148800/175341]\n",
      "loss: 0.489789  [150400/175341]\n",
      "loss: 0.520013  [152000/175341]\n",
      "loss: 0.657016  [153600/175341]\n",
      "loss: 0.507212  [155200/175341]\n",
      "loss: 0.568692  [156800/175341]\n",
      "loss: 0.746727  [158400/175341]\n",
      "loss: 0.398743  [160000/175341]\n",
      "loss: 0.825230  [161600/175341]\n",
      "loss: 0.377773  [163200/175341]\n",
      "loss: 0.226466  [164800/175341]\n",
      "loss: 0.474934  [166400/175341]\n",
      "loss: 0.511868  [168000/175341]\n",
      "loss: 0.213911  [169600/175341]\n",
      "loss: 0.479449  [171200/175341]\n",
      "loss: 0.880736  [172800/175341]\n",
      "loss: 0.449693  [174400/175341]\n",
      "Train Accuracy: 81.9592%\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.539700, F1-score: 76.70%, Macro_F1-Score:  42.33%  \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.542091  [    0/175341]\n",
      "loss: 0.202014  [ 1600/175341]\n",
      "loss: 0.401777  [ 3200/175341]\n",
      "loss: 0.835064  [ 4800/175341]\n",
      "loss: 0.347635  [ 6400/175341]\n",
      "loss: 0.480950  [ 8000/175341]\n",
      "loss: 0.608303  [ 9600/175341]\n",
      "loss: 0.569539  [11200/175341]\n",
      "loss: 0.635803  [12800/175341]\n",
      "loss: 0.360756  [14400/175341]\n",
      "loss: 0.677174  [16000/175341]\n",
      "loss: 0.665287  [17600/175341]\n",
      "loss: 0.148047  [19200/175341]\n",
      "loss: 0.310102  [20800/175341]\n",
      "loss: 0.438497  [22400/175341]\n",
      "loss: 0.574710  [24000/175341]\n",
      "loss: 0.453844  [25600/175341]\n",
      "loss: 0.509281  [27200/175341]\n",
      "loss: 0.424849  [28800/175341]\n",
      "loss: 0.563464  [30400/175341]\n",
      "loss: 0.278100  [32000/175341]\n",
      "loss: 0.400528  [33600/175341]\n",
      "loss: 0.115631  [35200/175341]\n",
      "loss: 0.628967  [36800/175341]\n",
      "loss: 0.223595  [38400/175341]\n",
      "loss: 0.489968  [40000/175341]\n",
      "loss: 0.278123  [41600/175341]\n",
      "loss: 0.236036  [43200/175341]\n",
      "loss: 0.624377  [44800/175341]\n",
      "loss: 0.440387  [46400/175341]\n",
      "loss: 0.274611  [48000/175341]\n",
      "loss: 0.560429  [49600/175341]\n",
      "loss: 0.303735  [51200/175341]\n",
      "loss: 0.579326  [52800/175341]\n",
      "loss: 0.472325  [54400/175341]\n",
      "loss: 0.729878  [56000/175341]\n",
      "loss: 0.306089  [57600/175341]\n",
      "loss: 0.275444  [59200/175341]\n",
      "loss: 0.372306  [60800/175341]\n",
      "loss: 0.411206  [62400/175341]\n",
      "loss: 0.444477  [64000/175341]\n",
      "loss: 0.391435  [65600/175341]\n",
      "loss: 0.580056  [67200/175341]\n",
      "loss: 0.331638  [68800/175341]\n",
      "loss: 0.316338  [70400/175341]\n",
      "loss: 0.294781  [72000/175341]\n",
      "loss: 0.383361  [73600/175341]\n",
      "loss: 0.219081  [75200/175341]\n",
      "loss: 0.429038  [76800/175341]\n",
      "loss: 0.403251  [78400/175341]\n",
      "loss: 0.614667  [80000/175341]\n",
      "loss: 0.521407  [81600/175341]\n",
      "loss: 0.622307  [83200/175341]\n",
      "loss: 0.378796  [84800/175341]\n",
      "loss: 0.244451  [86400/175341]\n",
      "loss: 0.465000  [88000/175341]\n",
      "loss: 1.102604  [89600/175341]\n",
      "loss: 0.785173  [91200/175341]\n",
      "loss: 0.173006  [92800/175341]\n",
      "loss: 0.169966  [94400/175341]\n",
      "loss: 0.073677  [96000/175341]\n",
      "loss: 0.298931  [97600/175341]\n",
      "loss: 0.205532  [99200/175341]\n",
      "loss: 0.432908  [100800/175341]\n",
      "loss: 0.626157  [102400/175341]\n",
      "loss: 0.549754  [104000/175341]\n",
      "loss: 0.286085  [105600/175341]\n",
      "loss: 0.309916  [107200/175341]\n",
      "loss: 0.467934  [108800/175341]\n",
      "loss: 0.582272  [110400/175341]\n",
      "loss: 0.940181  [112000/175341]\n",
      "loss: 0.776217  [113600/175341]\n",
      "loss: 0.707517  [115200/175341]\n",
      "loss: 0.314945  [116800/175341]\n",
      "loss: 0.348384  [118400/175341]\n",
      "loss: 0.537082  [120000/175341]\n",
      "loss: 0.313569  [121600/175341]\n",
      "loss: 0.382129  [123200/175341]\n",
      "loss: 0.251560  [124800/175341]\n",
      "loss: 0.461432  [126400/175341]\n",
      "loss: 0.194389  [128000/175341]\n",
      "loss: 0.120011  [129600/175341]\n",
      "loss: 0.293015  [131200/175341]\n",
      "loss: 0.472200  [132800/175341]\n",
      "loss: 0.598822  [134400/175341]\n",
      "loss: 0.321303  [136000/175341]\n",
      "loss: 0.384640  [137600/175341]\n",
      "loss: 0.253429  [139200/175341]\n",
      "loss: 0.734409  [140800/175341]\n",
      "loss: 0.685210  [142400/175341]\n",
      "loss: 0.657577  [144000/175341]\n",
      "loss: 0.324983  [145600/175341]\n",
      "loss: 0.285713  [147200/175341]\n",
      "loss: 0.188399  [148800/175341]\n",
      "loss: 0.395412  [150400/175341]\n",
      "loss: 0.209833  [152000/175341]\n",
      "loss: 0.905516  [153600/175341]\n",
      "loss: 0.101461  [155200/175341]\n",
      "loss: 0.281395  [156800/175341]\n",
      "loss: 0.254388  [158400/175341]\n",
      "loss: 1.029451  [160000/175341]\n",
      "loss: 0.385054  [161600/175341]\n",
      "loss: 0.269312  [163200/175341]\n",
      "loss: 0.107108  [164800/175341]\n",
      "loss: 0.682940  [166400/175341]\n",
      "loss: 0.609703  [168000/175341]\n",
      "loss: 0.114899  [169600/175341]\n",
      "loss: 0.428949  [171200/175341]\n",
      "loss: 1.009551  [172800/175341]\n",
      "loss: 0.718496  [174400/175341]\n",
      "Train Accuracy: 81.9985%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.549155, F1-score: 76.24%, Macro_F1-Score:  42.34%  \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.242677  [    0/175341]\n",
      "loss: 0.417595  [ 1600/175341]\n",
      "loss: 0.382227  [ 3200/175341]\n",
      "loss: 0.586610  [ 4800/175341]\n",
      "loss: 0.284840  [ 6400/175341]\n",
      "loss: 0.615043  [ 8000/175341]\n",
      "loss: 1.018471  [ 9600/175341]\n",
      "loss: 0.931244  [11200/175341]\n",
      "loss: 0.099695  [12800/175341]\n",
      "loss: 0.455778  [14400/175341]\n",
      "loss: 0.248922  [16000/175341]\n",
      "loss: 0.365263  [17600/175341]\n",
      "loss: 0.867127  [19200/175341]\n",
      "loss: 0.326257  [20800/175341]\n",
      "loss: 0.442588  [22400/175341]\n",
      "loss: 0.206199  [24000/175341]\n",
      "loss: 0.397074  [25600/175341]\n",
      "loss: 0.681923  [27200/175341]\n",
      "loss: 0.245622  [28800/175341]\n",
      "loss: 0.245951  [30400/175341]\n",
      "loss: 0.265998  [32000/175341]\n",
      "loss: 0.391937  [33600/175341]\n",
      "loss: 0.249852  [35200/175341]\n",
      "loss: 0.310274  [36800/175341]\n",
      "loss: 0.334187  [38400/175341]\n",
      "loss: 1.080495  [40000/175341]\n",
      "loss: 0.659150  [41600/175341]\n",
      "loss: 0.415558  [43200/175341]\n",
      "loss: 0.438672  [44800/175341]\n",
      "loss: 0.470052  [46400/175341]\n",
      "loss: 0.432367  [48000/175341]\n",
      "loss: 0.640625  [49600/175341]\n",
      "loss: 0.611407  [51200/175341]\n",
      "loss: 0.321804  [52800/175341]\n",
      "loss: 0.350348  [54400/175341]\n",
      "loss: 0.732094  [56000/175341]\n",
      "loss: 0.413352  [57600/175341]\n",
      "loss: 0.478189  [59200/175341]\n",
      "loss: 0.194939  [60800/175341]\n",
      "loss: 0.592525  [62400/175341]\n",
      "loss: 0.461858  [64000/175341]\n",
      "loss: 0.307286  [65600/175341]\n",
      "loss: 0.327397  [67200/175341]\n",
      "loss: 0.099808  [68800/175341]\n",
      "loss: 0.147965  [70400/175341]\n",
      "loss: 0.306452  [72000/175341]\n",
      "loss: 0.322999  [73600/175341]\n",
      "loss: 0.327622  [75200/175341]\n",
      "loss: 0.351704  [76800/175341]\n",
      "loss: 0.592571  [78400/175341]\n",
      "loss: 0.556148  [80000/175341]\n",
      "loss: 0.508148  [81600/175341]\n",
      "loss: 0.447342  [83200/175341]\n",
      "loss: 0.307086  [84800/175341]\n",
      "loss: 0.669979  [86400/175341]\n",
      "loss: 0.264298  [88000/175341]\n",
      "loss: 0.251014  [89600/175341]\n",
      "loss: 0.220448  [91200/175341]\n",
      "loss: 0.679909  [92800/175341]\n",
      "loss: 0.372296  [94400/175341]\n",
      "loss: 0.473949  [96000/175341]\n",
      "loss: 0.379412  [97600/175341]\n",
      "loss: 0.350315  [99200/175341]\n",
      "loss: 0.391393  [100800/175341]\n",
      "loss: 0.366173  [102400/175341]\n",
      "loss: 0.408491  [104000/175341]\n",
      "loss: 0.245430  [105600/175341]\n",
      "loss: 0.355067  [107200/175341]\n",
      "loss: 0.128021  [108800/175341]\n",
      "loss: 0.158050  [110400/175341]\n",
      "loss: 0.559245  [112000/175341]\n",
      "loss: 0.343060  [113600/175341]\n",
      "loss: 0.749570  [115200/175341]\n",
      "loss: 0.793037  [116800/175341]\n",
      "loss: 0.706804  [118400/175341]\n",
      "loss: 0.584693  [120000/175341]\n",
      "loss: 0.140506  [121600/175341]\n",
      "loss: 0.242045  [123200/175341]\n",
      "loss: 0.215571  [124800/175341]\n",
      "loss: 0.622413  [126400/175341]\n",
      "loss: 0.237442  [128000/175341]\n",
      "loss: 0.450170  [129600/175341]\n",
      "loss: 0.260688  [131200/175341]\n",
      "loss: 0.336659  [132800/175341]\n",
      "loss: 0.334839  [134400/175341]\n",
      "loss: 1.086982  [136000/175341]\n",
      "loss: 0.559387  [137600/175341]\n",
      "loss: 0.444185  [139200/175341]\n",
      "loss: 0.094814  [140800/175341]\n",
      "loss: 0.647380  [142400/175341]\n",
      "loss: 0.534720  [144000/175341]\n",
      "loss: 0.337294  [145600/175341]\n",
      "loss: 0.358183  [147200/175341]\n",
      "loss: 0.182144  [148800/175341]\n",
      "loss: 0.487787  [150400/175341]\n",
      "loss: 0.577511  [152000/175341]\n",
      "loss: 0.378523  [153600/175341]\n",
      "loss: 0.244195  [155200/175341]\n",
      "loss: 0.647247  [156800/175341]\n",
      "loss: 0.908864  [158400/175341]\n",
      "loss: 0.587265  [160000/175341]\n",
      "loss: 0.228778  [161600/175341]\n",
      "loss: 0.485085  [163200/175341]\n",
      "loss: 0.175903  [164800/175341]\n",
      "loss: 0.398120  [166400/175341]\n",
      "loss: 0.810177  [168000/175341]\n",
      "loss: 0.174013  [169600/175341]\n",
      "loss: 0.726125  [171200/175341]\n",
      "loss: 0.475328  [172800/175341]\n",
      "loss: 0.412260  [174400/175341]\n",
      "Train Accuracy: 81.9352%\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4) #using L2 regularization\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add252-99f2-4d32-bb41-6e558870ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noise scale is low (0.2)\n",
    "plt.plot(loss_over_train, label=\"Training Loss\")\n",
    "plt.plot(loss_over_test, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_data_y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_X_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get class predictions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert tensors to NumPy arrays for sklearn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\efficient_kan\\kan.py:278\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x, update_grid)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_grid:\n\u001b[0;32m    277\u001b[0m         layer\u001b[38;5;241m.\u001b[39mupdate_grid(x)\n\u001b[1;32m--> 278\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\efficient_kan\\kan.py:154\u001b[0m, in \u001b[0;36mKANLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\n\u001b[0;32m    155\u001b[0m     original_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    156\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c224d",
   "metadata": {},
   "source": [
    "## Feature Selection using Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
