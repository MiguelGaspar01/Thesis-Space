{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed6d0c0-22ce-4494-967a-a09a640ae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def set_global_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility across NumPy, PyTorch, and OS operations.\"\"\"\n",
    "    \n",
    "    # Set Python random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch random seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "    \n",
    "    # Ensure deterministic behavior in PyTorch (optional, can slow training)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for other libraries\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Set global seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_data_X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTESTER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAmbiente de Trabalho\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThesis Folder\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData After Preprocess CICIDS2017\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed_test_data_X_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_data_X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTESTER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAmbiente de Trabalho\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThesis Folder\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData After Preprocess CICIDS2017\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed_train_data_X_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_labels_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTESTER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAmbiente de Trabalho\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mThesis Folder\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData After Preprocess CICIDS2017\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed_test_data_y_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess CICIDS2017\\processed_test_data_X_data.csv\")\n",
    "train_data_X = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess CICIDS2017\\processed_train_data_X_data.csv\")\n",
    "test_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess CICIDS2017\\processed_test_data_y_data.csv\")\n",
    "train_labels_encoded = pd.read_csv(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess CICIDS2017\\processed_train_data_y_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77878a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X.drop(columns=[\"Destination Port\", \"Source Port\"], inplace=True)\n",
    "test_data_X.drop(columns=[\"Destination Port\", \"Source Port\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95a23d-e0ba-48a9-8e8f-a76411be9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y = test_labels_encoded[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a731f3a-ba64-42b5-96f6-c1dcb5183fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_y = train_labels_encoded[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy = \"all\", random_state=42, n_neighbors= 5)\n",
    "train_data_X,train_data_y = adasyn.fit_resample(train_data_X, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c173ce-09da-44d1-9c68-8d351f7e5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "4    20986\n",
       "2    20790\n",
       "5    20202\n",
       "1     8201\n",
       "6     1659\n",
       "0     1499\n",
       "3       25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c438adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fec4c1-e780-4ade-b035-e8e6469310af",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c14778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73362, 77)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Length of Fwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Bytess', 'Flow Packetss', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd Header Length', 'Fwd Packetss', 'Bwd Packetss', 'Min Packet Length', 'FIN Flag Count', 'SYN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'Down Up Ratio', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Max', 'Idle Min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [31 33 55 56 57 58 59 60] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the top 40 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=50)\n",
    "X_selected = selector.fit_transform(train_data_X, train_labels_encoded[\"Label\"])\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = train_data_X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035c2ae-2be5-4249-bb01-77bc7d29585d",
   "metadata": {},
   "source": [
    "## Modelling EfficentKan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccf7cd-d4f0-49f5-9b06-ed0b2d444d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_tensor = torch.tensor(train_data_X[features].values, dtype=torch.float32).squeeze()\n",
    "#test_X_tensor = torch.tensor(test_data_X[features].values, dtype=torch.float32).squeeze() #changed to have all features\n",
    "\n",
    "train_X_tensor = torch.tensor(train_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "test_X_tensor = torch.tensor(test_data_X[selected_features].values, dtype=torch.float32).squeeze()\n",
    "\n",
    "train_Y_tensor = torch.tensor(train_data_y.values, dtype=torch.long).squeeze()\n",
    "\n",
    "test_Y_tensor = torch.tensor(test_data_y.values, dtype=torch.long).squeeze()\n",
    "#a dictionary with the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357325e-6c3f-4696-ac8c-9c4ab4c8a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c01a6-69d2-466f-88eb-3bfa57eee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_tensor, test_Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638f1d3-b977-41bb-b150-6698ce803af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ea5f-6950-458e-aaae-c3d4aae9ecf0",
   "metadata": {},
   "source": [
    "## Modelling with efficient Kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90c1ca-27d1-4494-9b5b-d6ec3020f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficient_kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4eaf5f-fbfd-4db6-b316-e5b93ee0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define model\n",
    "model = KAN([40, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8606303-5979-499a-9480-020063f0530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid_size, model.spline_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e210a4-fe86-4b81-813b-4ddfc926ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ae572-eb21-41d2-b309-d7526d5413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN([30, 20,10, 5,15],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53a788-40a9-46d1-b42e-dc9948a173d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_over_train = []\n",
    "loss_over_test = []\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0  # For accuracy tracking\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    mean_loss = epoch_loss / len(dataloader) \n",
    "    loss_over_train.append(mean_loss)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Train Accuracy: {accuracy:.4f}%\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            # Collect all predictions and true labels for F1-score calculation\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    mean_loss = test_loss / num_batches  # Divide by the number of batches\n",
    "    loss_over_test.append(mean_loss)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted') * 100  # Weighted F1-score to handle class imbalance\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100  # Macro F1-score to handle class imbalance\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, F1-score: {f1:.2f}%, Macro_F1-Score: {f1_macro: .2f}%  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9449d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "------------------------------\n",
      "initializing model...\n",
      "Epoch 1: Train Acc=67.03%, Val Acc=79.38%, F1=73.59%, Macro F1=37.21%\n",
      "Epoch 2: Train Acc=81.80%, Val Acc=89.09%, F1=87.05%, Macro F1=51.81%\n",
      "Epoch 3: Train Acc=90.97%, Val Acc=91.29%, F1=90.36%, Macro F1=64.32%\n",
      "Epoch 4: Train Acc=91.65%, Val Acc=91.96%, F1=91.03%, Macro F1=64.78%\n",
      "Epoch 5: Train Acc=92.70%, Val Acc=92.83%, F1=92.45%, Macro F1=71.53%\n",
      "Epoch 6: Train Acc=93.28%, Val Acc=93.14%, F1=92.77%, Macro F1=71.82%\n",
      "Epoch 7: Train Acc=94.09%, Val Acc=94.25%, F1=94.15%, Macro F1=77.15%\n",
      "Epoch 8: Train Acc=94.73%, Val Acc=94.73%, F1=94.65%, Macro F1=77.90%\n",
      "Epoch 9: Train Acc=95.00%, Val Acc=95.08%, F1=95.01%, Macro F1=78.19%\n",
      "Epoch 10: Train Acc=95.26%, Val Acc=95.07%, F1=95.01%, Macro F1=78.06%\n",
      "Epoch 11: Train Acc=95.36%, Val Acc=95.17%, F1=95.11%, Macro F1=78.28%\n",
      "Epoch 12: Train Acc=95.54%, Val Acc=95.22%, F1=95.17%, Macro F1=78.45%\n",
      "Epoch 13: Train Acc=95.67%, Val Acc=95.37%, F1=95.34%, Macro F1=78.66%\n",
      "Epoch 14: Train Acc=95.74%, Val Acc=95.62%, F1=95.55%, Macro F1=78.91%\n",
      "Epoch 15: Train Acc=95.80%, Val Acc=95.71%, F1=95.65%, Macro F1=79.27%\n",
      "Epoch 16: Train Acc=95.86%, Val Acc=95.36%, F1=95.31%, Macro F1=78.55%\n",
      "Epoch 17: Train Acc=95.91%, Val Acc=95.67%, F1=95.61%, Macro F1=79.05%\n",
      "Epoch 18: Train Acc=95.96%, Val Acc=95.82%, F1=95.75%, Macro F1=78.98%\n",
      "Epoch 19: Train Acc=96.08%, Val Acc=95.85%, F1=95.80%, Macro F1=79.40%\n",
      "Epoch 20: Train Acc=96.15%, Val Acc=95.74%, F1=95.70%, Macro F1=78.91%\n",
      "Epoch 21: Train Acc=96.25%, Val Acc=95.83%, F1=95.79%, Macro F1=79.39%\n",
      "Epoch 22: Train Acc=96.33%, Val Acc=96.04%, F1=96.00%, Macro F1=79.94%\n",
      "Epoch 23: Train Acc=96.35%, Val Acc=96.32%, F1=96.28%, Macro F1=80.01%\n",
      "Epoch 24: Train Acc=96.44%, Val Acc=96.28%, F1=96.25%, Macro F1=80.02%\n",
      "Epoch 25: Train Acc=96.48%, Val Acc=96.32%, F1=96.28%, Macro F1=80.40%\n",
      "Epoch 26: Train Acc=96.59%, Val Acc=96.37%, F1=96.34%, Macro F1=80.35%\n",
      "Epoch 27: Train Acc=96.61%, Val Acc=96.42%, F1=96.37%, Macro F1=80.38%\n",
      "Epoch 28: Train Acc=96.71%, Val Acc=96.42%, F1=96.39%, Macro F1=80.72%\n",
      "Epoch 29: Train Acc=96.70%, Val Acc=96.39%, F1=96.36%, Macro F1=80.52%\n",
      "Epoch 30: Train Acc=96.81%, Val Acc=96.46%, F1=96.44%, Macro F1=80.54%\n",
      "Epoch 31: Train Acc=96.84%, Val Acc=96.36%, F1=96.33%, Macro F1=80.55%\n",
      "Epoch 32: Train Acc=96.89%, Val Acc=96.54%, F1=96.50%, Macro F1=80.91%\n",
      "Epoch 33: Train Acc=96.94%, Val Acc=96.65%, F1=96.62%, Macro F1=80.94%\n",
      "Epoch 34: Train Acc=97.00%, Val Acc=96.49%, F1=96.46%, Macro F1=80.68%\n",
      "Epoch 35: Train Acc=97.06%, Val Acc=96.82%, F1=96.79%, Macro F1=81.42%\n",
      "Epoch 36: Train Acc=97.06%, Val Acc=96.84%, F1=96.81%, Macro F1=81.14%\n",
      "Epoch 37: Train Acc=97.14%, Val Acc=96.82%, F1=96.79%, Macro F1=81.55%\n",
      "Epoch 38: Train Acc=97.13%, Val Acc=96.93%, F1=96.90%, Macro F1=81.52%\n",
      "Epoch 39: Train Acc=97.16%, Val Acc=96.91%, F1=96.88%, Macro F1=81.33%\n",
      "Epoch 40: Train Acc=97.23%, Val Acc=96.89%, F1=96.87%, Macro F1=81.11%\n",
      "Epoch 41: Train Acc=97.26%, Val Acc=96.92%, F1=96.89%, Macro F1=81.48%\n",
      "Epoch 42: Train Acc=97.35%, Val Acc=97.02%, F1=97.00%, Macro F1=81.64%\n",
      "Epoch 43: Train Acc=97.33%, Val Acc=96.98%, F1=96.96%, Macro F1=81.67%\n",
      "Epoch 44: Train Acc=97.39%, Val Acc=97.12%, F1=97.10%, Macro F1=81.71%\n",
      "Epoch 45: Train Acc=97.39%, Val Acc=96.96%, F1=96.94%, Macro F1=81.36%\n",
      "Epoch 46: Train Acc=97.43%, Val Acc=97.04%, F1=97.01%, Macro F1=82.02%\n",
      "Epoch 47: Train Acc=97.48%, Val Acc=97.16%, F1=97.14%, Macro F1=82.11%\n",
      "Epoch 48: Train Acc=97.50%, Val Acc=97.14%, F1=97.12%, Macro F1=81.73%\n",
      "Epoch 49: Train Acc=97.53%, Val Acc=97.21%, F1=97.19%, Macro F1=82.29%\n",
      "Epoch 50: Train Acc=97.62%, Val Acc=97.19%, F1=97.17%, Macro F1=82.18%\n",
      "Epoch 51: Train Acc=97.59%, Val Acc=97.19%, F1=97.16%, Macro F1=81.99%\n",
      "Epoch 52: Train Acc=97.60%, Val Acc=97.30%, F1=97.28%, Macro F1=82.28%\n",
      "Epoch 53: Train Acc=97.63%, Val Acc=97.22%, F1=97.19%, Macro F1=82.01%\n",
      "Epoch 54: Train Acc=97.64%, Val Acc=97.23%, F1=97.20%, Macro F1=82.34%\n",
      "Epoch 55: Train Acc=97.70%, Val Acc=97.27%, F1=97.24%, Macro F1=82.40%\n",
      "Epoch 56: Train Acc=97.69%, Val Acc=97.17%, F1=97.15%, Macro F1=81.87%\n",
      "Epoch 57: Train Acc=97.72%, Val Acc=97.39%, F1=97.37%, Macro F1=82.58%\n",
      "Epoch 58: Train Acc=97.77%, Val Acc=97.31%, F1=97.28%, Macro F1=82.53%\n",
      "Epoch 59: Train Acc=97.75%, Val Acc=97.40%, F1=97.37%, Macro F1=82.62%\n",
      "Epoch 60: Train Acc=97.75%, Val Acc=97.29%, F1=97.26%, Macro F1=82.42%\n",
      "Epoch 61: Train Acc=97.82%, Val Acc=97.40%, F1=97.38%, Macro F1=82.56%\n",
      "Epoch 62: Train Acc=97.82%, Val Acc=97.36%, F1=97.33%, Macro F1=82.57%\n",
      "Epoch 63: Train Acc=97.81%, Val Acc=97.36%, F1=97.34%, Macro F1=82.43%\n",
      "Epoch 64: Train Acc=97.85%, Val Acc=97.55%, F1=97.52%, Macro F1=82.77%\n",
      "Epoch 65: Train Acc=97.82%, Val Acc=96.97%, F1=96.95%, Macro F1=82.56%\n",
      "Epoch 66: Train Acc=97.87%, Val Acc=97.57%, F1=97.55%, Macro F1=82.91%\n",
      "Epoch 67: Train Acc=97.89%, Val Acc=97.59%, F1=97.57%, Macro F1=83.01%\n",
      "Epoch 68: Train Acc=97.88%, Val Acc=97.48%, F1=97.46%, Macro F1=82.75%\n",
      "Epoch 69: Train Acc=97.91%, Val Acc=97.61%, F1=97.60%, Macro F1=82.99%\n",
      "Epoch 70: Train Acc=97.89%, Val Acc=97.48%, F1=97.45%, Macro F1=82.73%\n",
      "Epoch 71: Train Acc=97.91%, Val Acc=97.49%, F1=97.47%, Macro F1=82.75%\n",
      "Epoch 72: Train Acc=97.92%, Val Acc=97.53%, F1=97.51%, Macro F1=82.90%\n",
      "Epoch 73: Train Acc=97.93%, Val Acc=97.49%, F1=97.47%, Macro F1=83.03%\n",
      "Epoch 74: Train Acc=97.94%, Val Acc=97.63%, F1=97.61%, Macro F1=83.13%\n",
      "Epoch 75: Train Acc=97.95%, Val Acc=97.44%, F1=97.42%, Macro F1=82.78%\n",
      "Epoch 76: Train Acc=97.95%, Val Acc=97.29%, F1=97.26%, Macro F1=82.43%\n",
      "Epoch 77: Train Acc=97.97%, Val Acc=97.54%, F1=97.52%, Macro F1=82.95%\n",
      "Epoch 78: Train Acc=97.94%, Val Acc=97.73%, F1=97.71%, Macro F1=83.25%\n",
      "Epoch 79: Train Acc=97.99%, Val Acc=97.48%, F1=97.46%, Macro F1=82.55%\n",
      "Epoch 80: Train Acc=97.99%, Val Acc=97.73%, F1=97.71%, Macro F1=83.23%\n",
      "Epoch 81: Train Acc=97.99%, Val Acc=97.66%, F1=97.64%, Macro F1=83.00%\n",
      "Epoch 82: Train Acc=97.97%, Val Acc=97.41%, F1=97.39%, Macro F1=82.55%\n",
      "Epoch 83: Train Acc=98.01%, Val Acc=97.71%, F1=97.69%, Macro F1=83.32%\n",
      "Epoch 84: Train Acc=98.01%, Val Acc=97.53%, F1=97.51%, Macro F1=82.73%\n",
      "Epoch 85: Train Acc=97.99%, Val Acc=97.49%, F1=97.47%, Macro F1=82.84%\n",
      "Epoch 86: Train Acc=98.04%, Val Acc=97.72%, F1=97.71%, Macro F1=83.25%\n",
      "Epoch 87: Train Acc=98.05%, Val Acc=97.61%, F1=97.60%, Macro F1=82.97%\n",
      "Epoch 88: Train Acc=98.02%, Val Acc=97.63%, F1=97.61%, Macro F1=83.26%\n",
      "Epoch 89: Train Acc=98.05%, Val Acc=97.61%, F1=97.59%, Macro F1=83.10%\n",
      "Epoch 90: Train Acc=98.07%, Val Acc=97.65%, F1=97.63%, Macro F1=82.91%\n",
      "Epoch 91: Train Acc=98.07%, Val Acc=97.66%, F1=97.64%, Macro F1=83.30%\n",
      "Epoch 92: Train Acc=98.06%, Val Acc=97.79%, F1=97.78%, Macro F1=83.38%\n",
      "Epoch 93: Train Acc=98.06%, Val Acc=97.73%, F1=97.71%, Macro F1=83.28%\n",
      "Epoch 94: Train Acc=98.12%, Val Acc=97.76%, F1=97.75%, Macro F1=83.38%\n",
      "Epoch 95: Train Acc=98.07%, Val Acc=97.73%, F1=97.71%, Macro F1=83.30%\n",
      "Epoch 96: Train Acc=98.10%, Val Acc=97.81%, F1=97.79%, Macro F1=83.30%\n",
      "Epoch 97: Train Acc=98.09%, Val Acc=97.66%, F1=97.64%, Macro F1=83.25%\n",
      "Epoch 98: Train Acc=98.11%, Val Acc=97.79%, F1=97.77%, Macro F1=83.22%\n",
      "Epoch 99: Train Acc=98.08%, Val Acc=97.65%, F1=97.63%, Macro F1=83.29%\n",
      "Epoch 100: Train Acc=98.11%, Val Acc=97.70%, F1=97.68%, Macro F1=83.33%\n",
      "Epoch 101: Train Acc=98.12%, Val Acc=97.83%, F1=97.81%, Macro F1=83.53%\n",
      "Epoch 102: Train Acc=98.14%, Val Acc=97.95%, F1=97.93%, Macro F1=83.56%\n",
      "Epoch 103: Train Acc=98.16%, Val Acc=97.84%, F1=97.82%, Macro F1=83.58%\n",
      "Epoch 104: Train Acc=98.09%, Val Acc=97.91%, F1=97.89%, Macro F1=83.46%\n",
      "Epoch 105: Train Acc=98.11%, Val Acc=97.87%, F1=97.86%, Macro F1=83.46%\n",
      "Epoch 106: Train Acc=98.17%, Val Acc=97.87%, F1=97.86%, Macro F1=90.42%\n",
      "Epoch 107: Train Acc=98.16%, Val Acc=97.93%, F1=97.92%, Macro F1=90.67%\n",
      "Epoch 108: Train Acc=98.19%, Val Acc=97.83%, F1=97.81%, Macro F1=83.08%\n",
      "Epoch 109: Train Acc=98.16%, Val Acc=97.73%, F1=97.71%, Macro F1=83.33%\n",
      "Epoch 110: Train Acc=98.18%, Val Acc=97.95%, F1=97.93%, Macro F1=83.70%\n",
      "Epoch 111: Train Acc=98.14%, Val Acc=97.81%, F1=97.80%, Macro F1=83.48%\n",
      "Epoch 112: Train Acc=98.20%, Val Acc=97.84%, F1=97.82%, Macro F1=83.33%\n",
      "Epoch 113: Train Acc=98.20%, Val Acc=97.88%, F1=97.87%, Macro F1=90.61%\n",
      "Epoch 114: Train Acc=98.20%, Val Acc=97.83%, F1=97.82%, Macro F1=90.60%\n",
      "Epoch 115: Train Acc=98.20%, Val Acc=97.63%, F1=97.61%, Macro F1=82.93%\n",
      "Epoch 116: Train Acc=98.19%, Val Acc=97.98%, F1=97.96%, Macro F1=83.64%\n",
      "Epoch 117: Train Acc=98.19%, Val Acc=97.96%, F1=97.95%, Macro F1=90.68%\n",
      "Epoch 118: Train Acc=98.16%, Val Acc=97.79%, F1=97.76%, Macro F1=83.24%\n",
      "Epoch 119: Train Acc=98.20%, Val Acc=97.94%, F1=97.93%, Macro F1=83.52%\n",
      "Epoch 120: Train Acc=98.20%, Val Acc=97.34%, F1=97.34%, Macro F1=90.36%\n",
      "Epoch 121: Train Acc=98.22%, Val Acc=98.06%, F1=98.05%, Macro F1=90.98%\n",
      "Epoch 122: Train Acc=98.26%, Val Acc=98.00%, F1=97.99%, Macro F1=83.94%\n",
      "Epoch 123: Train Acc=98.25%, Val Acc=97.84%, F1=97.83%, Macro F1=90.55%\n",
      "Epoch 124: Train Acc=98.23%, Val Acc=98.02%, F1=98.02%, Macro F1=90.99%\n",
      "Epoch 125: Train Acc=98.26%, Val Acc=97.98%, F1=97.97%, Macro F1=90.76%\n",
      "Epoch 126: Train Acc=98.29%, Val Acc=97.97%, F1=97.96%, Macro F1=90.82%\n",
      "Epoch 127: Train Acc=98.25%, Val Acc=97.85%, F1=97.85%, Macro F1=89.07%\n",
      "Epoch 128: Train Acc=98.25%, Val Acc=97.85%, F1=97.84%, Macro F1=89.16%\n",
      "Epoch 129: Train Acc=98.27%, Val Acc=97.71%, F1=97.70%, Macro F1=90.46%\n",
      "Epoch 130: Train Acc=98.28%, Val Acc=97.88%, F1=97.87%, Macro F1=89.07%\n",
      "Epoch 131: Train Acc=98.26%, Val Acc=98.01%, F1=98.00%, Macro F1=90.89%\n",
      "Epoch 132: Train Acc=98.26%, Val Acc=98.01%, F1=98.00%, Macro F1=90.88%\n",
      "Epoch 133: Train Acc=98.23%, Val Acc=98.02%, F1=98.01%, Macro F1=90.93%\n",
      "Epoch 134: Train Acc=98.25%, Val Acc=97.87%, F1=97.87%, Macro F1=89.18%\n",
      "Epoch 135: Train Acc=98.32%, Val Acc=98.02%, F1=98.01%, Macro F1=90.82%\n",
      "Epoch 136: Train Acc=98.30%, Val Acc=97.86%, F1=97.85%, Macro F1=89.46%\n",
      "Epoch 137: Train Acc=98.28%, Val Acc=97.96%, F1=97.95%, Macro F1=89.31%\n",
      "Epoch 138: Train Acc=98.31%, Val Acc=98.02%, F1=98.01%, Macro F1=90.85%\n",
      "Epoch 139: Train Acc=98.34%, Val Acc=98.01%, F1=98.00%, Macro F1=89.45%\n",
      "Epoch 140: Train Acc=98.31%, Val Acc=97.99%, F1=97.98%, Macro F1=89.46%\n",
      "Epoch 141: Train Acc=98.35%, Val Acc=98.16%, F1=98.15%, Macro F1=89.72%\n",
      "Epoch 142: Train Acc=98.35%, Val Acc=97.88%, F1=97.87%, Macro F1=90.65%\n",
      "Epoch 143: Train Acc=98.32%, Val Acc=97.83%, F1=97.82%, Macro F1=90.12%\n",
      "Epoch 144: Train Acc=98.31%, Val Acc=98.13%, F1=98.13%, Macro F1=91.13%\n",
      "Epoch 145: Train Acc=98.37%, Val Acc=98.02%, F1=98.01%, Macro F1=90.86%\n",
      "Epoch 146: Train Acc=98.32%, Val Acc=97.87%, F1=97.86%, Macro F1=89.29%\n",
      "Epoch 147: Train Acc=98.32%, Val Acc=98.04%, F1=98.03%, Macro F1=91.07%\n",
      "Epoch 148: Train Acc=98.35%, Val Acc=97.62%, F1=97.61%, Macro F1=88.48%\n",
      "Epoch 149: Train Acc=98.35%, Val Acc=98.07%, F1=98.07%, Macro F1=89.64%\n",
      "Epoch 150: Train Acc=98.32%, Val Acc=98.15%, F1=98.15%, Macro F1=89.66%\n",
      "Epoch 151: Train Acc=98.33%, Val Acc=98.11%, F1=98.10%, Macro F1=89.48%\n",
      "Epoch 152: Train Acc=98.38%, Val Acc=97.98%, F1=97.97%, Macro F1=89.49%\n",
      "Epoch 153: Train Acc=98.37%, Val Acc=98.19%, F1=98.18%, Macro F1=89.68%\n",
      "Epoch 154: Train Acc=98.36%, Val Acc=98.08%, F1=98.07%, Macro F1=91.12%\n",
      "Epoch 155: Train Acc=98.36%, Val Acc=98.12%, F1=98.11%, Macro F1=89.56%\n",
      "Epoch 156: Train Acc=98.38%, Val Acc=98.09%, F1=98.08%, Macro F1=89.56%\n",
      "Epoch 157: Train Acc=98.38%, Val Acc=97.96%, F1=97.96%, Macro F1=89.38%\n",
      "Epoch 158: Train Acc=98.35%, Val Acc=98.11%, F1=98.10%, Macro F1=90.86%\n",
      "Epoch 159: Train Acc=98.38%, Val Acc=98.08%, F1=98.08%, Macro F1=91.00%\n",
      "Epoch 160: Train Acc=98.37%, Val Acc=98.09%, F1=98.08%, Macro F1=89.43%\n",
      "Epoch 161: Train Acc=98.40%, Val Acc=98.16%, F1=98.15%, Macro F1=91.25%\n",
      "Epoch 162: Train Acc=98.35%, Val Acc=98.16%, F1=98.15%, Macro F1=89.73%\n",
      "Epoch 163: Train Acc=98.41%, Val Acc=98.13%, F1=98.12%, Macro F1=91.04%\n",
      "Epoch 164: Train Acc=98.40%, Val Acc=98.15%, F1=98.14%, Macro F1=90.98%\n",
      "Epoch 165: Train Acc=98.42%, Val Acc=98.06%, F1=98.05%, Macro F1=91.09%\n",
      "Epoch 166: Train Acc=98.42%, Val Acc=98.18%, F1=98.17%, Macro F1=91.16%\n",
      "Epoch 167: Train Acc=98.39%, Val Acc=98.16%, F1=98.15%, Macro F1=89.68%\n",
      "Epoch 168: Train Acc=98.40%, Val Acc=98.04%, F1=98.03%, Macro F1=90.84%\n",
      "Epoch 169: Train Acc=98.42%, Val Acc=98.11%, F1=98.10%, Macro F1=91.08%\n",
      "Epoch 170: Train Acc=98.40%, Val Acc=98.10%, F1=98.09%, Macro F1=89.54%\n",
      "Epoch 171: Train Acc=98.42%, Val Acc=98.10%, F1=98.09%, Macro F1=89.68%\n",
      "Epoch 172: Train Acc=98.41%, Val Acc=98.17%, F1=98.17%, Macro F1=91.06%\n",
      "Epoch 173: Train Acc=98.41%, Val Acc=98.12%, F1=98.11%, Macro F1=91.01%\n",
      "Epoch 174: Train Acc=98.43%, Val Acc=98.19%, F1=98.19%, Macro F1=91.06%\n",
      "Epoch 175: Train Acc=98.41%, Val Acc=98.14%, F1=98.13%, Macro F1=89.59%\n",
      "Epoch 176: Train Acc=98.44%, Val Acc=98.13%, F1=98.13%, Macro F1=89.79%\n",
      "Epoch 177: Train Acc=98.44%, Val Acc=98.15%, F1=98.14%, Macro F1=91.23%\n",
      "Epoch 178: Train Acc=98.43%, Val Acc=98.19%, F1=98.18%, Macro F1=91.22%\n",
      "Epoch 179: Train Acc=98.41%, Val Acc=98.11%, F1=98.11%, Macro F1=91.11%\n",
      "Epoch 180: Train Acc=98.39%, Val Acc=98.09%, F1=98.08%, Macro F1=90.80%\n",
      "Epoch 181: Train Acc=98.45%, Val Acc=98.13%, F1=98.12%, Macro F1=90.92%\n",
      "Epoch 182: Train Acc=98.44%, Val Acc=98.19%, F1=98.18%, Macro F1=89.60%\n",
      "Epoch 183: Train Acc=98.43%, Val Acc=98.08%, F1=98.07%, Macro F1=90.89%\n",
      "Epoch 184: Train Acc=98.48%, Val Acc=98.16%, F1=98.15%, Macro F1=91.11%\n",
      "Epoch 185: Train Acc=98.44%, Val Acc=98.15%, F1=98.15%, Macro F1=91.08%\n",
      "Epoch 186: Train Acc=98.46%, Val Acc=98.11%, F1=98.10%, Macro F1=90.92%\n",
      "Epoch 187: Train Acc=98.45%, Val Acc=98.03%, F1=98.02%, Macro F1=91.11%\n",
      "Epoch 188: Train Acc=98.48%, Val Acc=98.23%, F1=98.23%, Macro F1=91.14%\n",
      "Epoch 189: Train Acc=98.46%, Val Acc=98.16%, F1=98.15%, Macro F1=91.17%\n",
      "Epoch 190: Train Acc=98.46%, Val Acc=98.30%, F1=98.29%, Macro F1=91.28%\n",
      "Epoch 191: Train Acc=97.40%, Val Acc=97.87%, F1=97.85%, Macro F1=90.36%\n",
      "Epoch 192: Train Acc=98.16%, Val Acc=97.81%, F1=97.79%, Macro F1=90.44%\n",
      "Epoch 193: Train Acc=98.24%, Val Acc=97.96%, F1=97.95%, Macro F1=90.71%\n",
      "Epoch 194: Train Acc=98.30%, Val Acc=97.96%, F1=97.95%, Macro F1=90.66%\n",
      "Epoch 195: Train Acc=98.29%, Val Acc=98.04%, F1=98.03%, Macro F1=90.99%\n",
      "Epoch 196: Train Acc=98.36%, Val Acc=98.15%, F1=98.14%, Macro F1=91.01%\n",
      "Epoch 197: Train Acc=98.46%, Val Acc=98.10%, F1=98.09%, Macro F1=90.96%\n",
      "Epoch 198: Train Acc=98.41%, Val Acc=97.81%, F1=97.79%, Macro F1=90.62%\n",
      "Epoch 199: Train Acc=98.42%, Val Acc=98.08%, F1=98.08%, Macro F1=90.94%\n",
      "Epoch 200: Train Acc=98.44%, Val Acc=98.05%, F1=98.04%, Macro F1=91.05%\n",
      "\n",
      "Fold 2/5\n",
      "------------------------------\n",
      "initializing model...\n",
      "Epoch 1: Train Acc=64.08%, Val Acc=76.61%, F1=71.88%, Macro F1=36.64%\n",
      "Epoch 2: Train Acc=84.35%, Val Acc=88.64%, F1=86.70%, Macro F1=51.14%\n",
      "Epoch 3: Train Acc=90.45%, Val Acc=91.26%, F1=90.21%, Macro F1=64.78%\n",
      "Epoch 4: Train Acc=91.78%, Val Acc=92.37%, F1=91.90%, Macro F1=70.64%\n",
      "Epoch 5: Train Acc=92.90%, Val Acc=93.16%, F1=92.91%, Macro F1=73.34%\n",
      "Epoch 6: Train Acc=93.66%, Val Acc=93.90%, F1=93.68%, Macro F1=74.55%\n",
      "Epoch 7: Train Acc=94.06%, Val Acc=93.98%, F1=93.78%, Macro F1=74.69%\n",
      "Epoch 8: Train Acc=94.31%, Val Acc=94.08%, F1=93.94%, Macro F1=75.18%\n",
      "Epoch 9: Train Acc=94.45%, Val Acc=94.38%, F1=94.24%, Macro F1=75.86%\n",
      "Epoch 10: Train Acc=94.77%, Val Acc=95.36%, F1=95.27%, Macro F1=77.36%\n",
      "Epoch 11: Train Acc=95.22%, Val Acc=95.30%, F1=95.25%, Macro F1=78.73%\n",
      "Epoch 12: Train Acc=95.53%, Val Acc=95.43%, F1=95.40%, Macro F1=78.92%\n",
      "Epoch 13: Train Acc=95.58%, Val Acc=95.43%, F1=95.39%, Macro F1=78.93%\n",
      "Epoch 14: Train Acc=95.70%, Val Acc=95.78%, F1=95.74%, Macro F1=79.45%\n",
      "Epoch 15: Train Acc=95.75%, Val Acc=95.66%, F1=95.62%, Macro F1=79.27%\n",
      "Epoch 16: Train Acc=95.85%, Val Acc=95.89%, F1=95.85%, Macro F1=79.61%\n",
      "Epoch 17: Train Acc=95.94%, Val Acc=95.92%, F1=95.89%, Macro F1=79.79%\n",
      "Epoch 18: Train Acc=95.95%, Val Acc=95.87%, F1=95.84%, Macro F1=79.73%\n",
      "Epoch 19: Train Acc=96.03%, Val Acc=95.89%, F1=95.85%, Macro F1=79.87%\n",
      "Epoch 20: Train Acc=96.08%, Val Acc=96.15%, F1=96.11%, Macro F1=80.17%\n",
      "Epoch 21: Train Acc=96.15%, Val Acc=96.10%, F1=96.06%, Macro F1=80.02%\n",
      "Epoch 22: Train Acc=96.18%, Val Acc=96.19%, F1=96.13%, Macro F1=80.23%\n",
      "Epoch 23: Train Acc=96.23%, Val Acc=96.14%, F1=96.09%, Macro F1=80.12%\n",
      "Epoch 24: Train Acc=96.30%, Val Acc=96.48%, F1=96.42%, Macro F1=80.59%\n",
      "Epoch 25: Train Acc=96.35%, Val Acc=96.46%, F1=96.43%, Macro F1=80.65%\n",
      "Epoch 26: Train Acc=96.45%, Val Acc=96.34%, F1=96.31%, Macro F1=80.50%\n",
      "Epoch 27: Train Acc=96.53%, Val Acc=96.48%, F1=96.44%, Macro F1=80.51%\n",
      "Epoch 28: Train Acc=96.48%, Val Acc=96.57%, F1=96.53%, Macro F1=80.78%\n",
      "Epoch 29: Train Acc=96.54%, Val Acc=96.32%, F1=96.28%, Macro F1=80.47%\n",
      "Epoch 30: Train Acc=96.56%, Val Acc=96.33%, F1=96.28%, Macro F1=80.42%\n",
      "Epoch 31: Train Acc=96.59%, Val Acc=96.30%, F1=96.26%, Macro F1=80.58%\n",
      "Epoch 32: Train Acc=96.66%, Val Acc=96.62%, F1=96.57%, Macro F1=80.81%\n",
      "Epoch 33: Train Acc=96.64%, Val Acc=96.64%, F1=96.61%, Macro F1=81.06%\n",
      "Epoch 34: Train Acc=96.65%, Val Acc=96.52%, F1=96.48%, Macro F1=80.95%\n",
      "Epoch 35: Train Acc=96.68%, Val Acc=96.59%, F1=96.56%, Macro F1=80.66%\n",
      "Epoch 36: Train Acc=96.77%, Val Acc=96.48%, F1=96.45%, Macro F1=80.59%\n",
      "Epoch 37: Train Acc=96.74%, Val Acc=96.67%, F1=96.64%, Macro F1=80.89%\n",
      "Epoch 38: Train Acc=96.70%, Val Acc=96.68%, F1=96.66%, Macro F1=81.03%\n",
      "Epoch 39: Train Acc=96.77%, Val Acc=96.59%, F1=96.55%, Macro F1=80.79%\n",
      "Epoch 40: Train Acc=96.75%, Val Acc=96.84%, F1=96.82%, Macro F1=81.41%\n",
      "Epoch 41: Train Acc=96.77%, Val Acc=96.11%, F1=96.07%, Macro F1=80.30%\n",
      "Epoch 42: Train Acc=96.76%, Val Acc=96.85%, F1=96.83%, Macro F1=81.26%\n",
      "Epoch 43: Train Acc=96.79%, Val Acc=96.74%, F1=96.71%, Macro F1=81.20%\n",
      "Epoch 44: Train Acc=96.86%, Val Acc=96.83%, F1=96.80%, Macro F1=81.34%\n",
      "Epoch 45: Train Acc=96.84%, Val Acc=96.69%, F1=96.67%, Macro F1=81.12%\n",
      "Epoch 46: Train Acc=96.88%, Val Acc=97.00%, F1=96.98%, Macro F1=81.51%\n",
      "Epoch 47: Train Acc=96.88%, Val Acc=96.76%, F1=96.73%, Macro F1=81.30%\n",
      "Epoch 48: Train Acc=96.90%, Val Acc=96.51%, F1=96.48%, Macro F1=80.97%\n",
      "Epoch 49: Train Acc=96.91%, Val Acc=96.86%, F1=96.83%, Macro F1=81.39%\n",
      "Epoch 50: Train Acc=96.89%, Val Acc=96.46%, F1=96.41%, Macro F1=80.89%\n",
      "Epoch 51: Train Acc=96.94%, Val Acc=97.03%, F1=96.99%, Macro F1=81.60%\n",
      "Epoch 52: Train Acc=96.96%, Val Acc=96.88%, F1=96.84%, Macro F1=81.58%\n",
      "Epoch 53: Train Acc=96.94%, Val Acc=96.91%, F1=96.85%, Macro F1=81.10%\n",
      "Epoch 54: Train Acc=97.00%, Val Acc=97.02%, F1=96.99%, Macro F1=81.56%\n",
      "Epoch 55: Train Acc=97.01%, Val Acc=97.09%, F1=97.07%, Macro F1=81.61%\n",
      "Epoch 56: Train Acc=96.97%, Val Acc=96.65%, F1=96.62%, Macro F1=81.05%\n",
      "Epoch 57: Train Acc=97.04%, Val Acc=96.84%, F1=96.79%, Macro F1=81.11%\n",
      "Epoch 58: Train Acc=97.01%, Val Acc=97.01%, F1=96.98%, Macro F1=81.67%\n",
      "Epoch 59: Train Acc=96.99%, Val Acc=96.49%, F1=96.45%, Macro F1=81.02%\n",
      "Epoch 60: Train Acc=97.11%, Val Acc=97.15%, F1=97.12%, Macro F1=81.68%\n",
      "Epoch 61: Train Acc=97.09%, Val Acc=96.20%, F1=96.16%, Macro F1=81.01%\n",
      "Epoch 62: Train Acc=96.14%, Val Acc=96.83%, F1=96.80%, Macro F1=81.40%\n",
      "Epoch 63: Train Acc=96.98%, Val Acc=97.10%, F1=97.06%, Macro F1=81.52%\n",
      "Epoch 64: Train Acc=97.00%, Val Acc=97.03%, F1=97.00%, Macro F1=81.89%\n",
      "Epoch 65: Train Acc=97.06%, Val Acc=96.93%, F1=96.88%, Macro F1=81.69%\n",
      "Epoch 66: Train Acc=97.12%, Val Acc=97.30%, F1=97.27%, Macro F1=82.22%\n",
      "Epoch 67: Train Acc=97.13%, Val Acc=97.02%, F1=96.99%, Macro F1=81.55%\n",
      "Epoch 68: Train Acc=97.18%, Val Acc=97.05%, F1=97.02%, Macro F1=81.52%\n",
      "Epoch 69: Train Acc=97.20%, Val Acc=96.99%, F1=96.96%, Macro F1=81.38%\n",
      "Epoch 70: Train Acc=97.20%, Val Acc=97.33%, F1=97.29%, Macro F1=82.17%\n",
      "Epoch 71: Train Acc=97.27%, Val Acc=97.25%, F1=97.21%, Macro F1=81.94%\n",
      "Epoch 72: Train Acc=97.29%, Val Acc=97.28%, F1=97.25%, Macro F1=82.04%\n",
      "Epoch 73: Train Acc=97.36%, Val Acc=97.19%, F1=97.15%, Macro F1=81.97%\n",
      "Epoch 74: Train Acc=97.37%, Val Acc=97.24%, F1=97.20%, Macro F1=81.89%\n",
      "Epoch 75: Train Acc=97.36%, Val Acc=97.26%, F1=97.23%, Macro F1=82.06%\n",
      "Epoch 76: Train Acc=97.38%, Val Acc=97.31%, F1=97.27%, Macro F1=82.34%\n",
      "Epoch 77: Train Acc=97.45%, Val Acc=97.43%, F1=97.40%, Macro F1=82.46%\n",
      "Epoch 78: Train Acc=97.43%, Val Acc=97.51%, F1=97.47%, Macro F1=82.48%\n",
      "Epoch 79: Train Acc=97.40%, Val Acc=97.53%, F1=97.49%, Macro F1=82.50%\n",
      "Epoch 80: Train Acc=97.46%, Val Acc=97.37%, F1=97.34%, Macro F1=82.32%\n",
      "Epoch 81: Train Acc=97.52%, Val Acc=97.52%, F1=97.48%, Macro F1=82.64%\n",
      "Epoch 82: Train Acc=97.52%, Val Acc=97.54%, F1=97.51%, Macro F1=82.60%\n",
      "Epoch 83: Train Acc=97.57%, Val Acc=97.49%, F1=97.45%, Macro F1=82.65%\n",
      "Epoch 84: Train Acc=97.59%, Val Acc=97.53%, F1=97.49%, Macro F1=82.40%\n",
      "Epoch 85: Train Acc=97.60%, Val Acc=97.58%, F1=97.55%, Macro F1=82.79%\n",
      "Epoch 86: Train Acc=97.67%, Val Acc=97.59%, F1=97.56%, Macro F1=82.85%\n",
      "Epoch 87: Train Acc=97.62%, Val Acc=97.65%, F1=97.62%, Macro F1=82.85%\n",
      "Epoch 88: Train Acc=97.67%, Val Acc=97.93%, F1=97.90%, Macro F1=83.53%\n",
      "Epoch 89: Train Acc=97.74%, Val Acc=97.70%, F1=97.67%, Macro F1=83.19%\n",
      "Epoch 90: Train Acc=97.71%, Val Acc=97.85%, F1=97.82%, Macro F1=83.32%\n",
      "Epoch 91: Train Acc=97.80%, Val Acc=97.61%, F1=97.59%, Macro F1=82.60%\n",
      "Epoch 92: Train Acc=97.72%, Val Acc=97.79%, F1=97.76%, Macro F1=83.48%\n",
      "Epoch 93: Train Acc=97.80%, Val Acc=97.75%, F1=97.73%, Macro F1=83.56%\n",
      "Epoch 94: Train Acc=97.82%, Val Acc=97.99%, F1=97.96%, Macro F1=83.71%\n",
      "Epoch 95: Train Acc=97.85%, Val Acc=97.74%, F1=97.70%, Macro F1=82.62%\n",
      "Epoch 96: Train Acc=97.85%, Val Acc=97.81%, F1=97.78%, Macro F1=83.45%\n",
      "Epoch 97: Train Acc=97.88%, Val Acc=98.08%, F1=98.06%, Macro F1=83.75%\n",
      "Epoch 98: Train Acc=97.92%, Val Acc=97.98%, F1=97.95%, Macro F1=83.86%\n",
      "Epoch 99: Train Acc=97.91%, Val Acc=97.98%, F1=97.96%, Macro F1=83.78%\n",
      "Epoch 100: Train Acc=97.93%, Val Acc=98.00%, F1=97.98%, Macro F1=83.62%\n",
      "Epoch 101: Train Acc=97.95%, Val Acc=97.59%, F1=97.57%, Macro F1=83.00%\n",
      "Epoch 102: Train Acc=97.95%, Val Acc=98.06%, F1=98.04%, Macro F1=83.91%\n",
      "Epoch 103: Train Acc=97.99%, Val Acc=98.13%, F1=98.10%, Macro F1=84.03%\n",
      "Epoch 104: Train Acc=97.99%, Val Acc=97.91%, F1=97.89%, Macro F1=83.60%\n",
      "Epoch 105: Train Acc=98.02%, Val Acc=98.15%, F1=98.13%, Macro F1=84.15%\n",
      "Epoch 106: Train Acc=98.02%, Val Acc=98.13%, F1=98.10%, Macro F1=84.08%\n",
      "Epoch 107: Train Acc=98.01%, Val Acc=98.15%, F1=98.12%, Macro F1=84.08%\n",
      "Epoch 108: Train Acc=98.06%, Val Acc=98.07%, F1=98.05%, Macro F1=83.78%\n",
      "Epoch 109: Train Acc=98.07%, Val Acc=98.23%, F1=98.20%, Macro F1=84.13%\n",
      "Epoch 110: Train Acc=98.09%, Val Acc=98.13%, F1=98.10%, Macro F1=84.06%\n",
      "Epoch 111: Train Acc=98.08%, Val Acc=98.04%, F1=98.01%, Macro F1=84.01%\n",
      "Epoch 112: Train Acc=98.10%, Val Acc=98.23%, F1=98.21%, Macro F1=84.19%\n",
      "Epoch 113: Train Acc=98.10%, Val Acc=98.21%, F1=98.19%, Macro F1=84.19%\n",
      "Epoch 114: Train Acc=98.12%, Val Acc=98.00%, F1=97.97%, Macro F1=83.66%\n",
      "Epoch 115: Train Acc=98.13%, Val Acc=98.23%, F1=98.21%, Macro F1=84.29%\n",
      "Epoch 116: Train Acc=98.15%, Val Acc=98.29%, F1=98.27%, Macro F1=84.28%\n",
      "Epoch 117: Train Acc=98.15%, Val Acc=98.26%, F1=98.23%, Macro F1=84.23%\n",
      "Epoch 118: Train Acc=98.19%, Val Acc=98.06%, F1=98.03%, Macro F1=83.62%\n",
      "Epoch 119: Train Acc=98.15%, Val Acc=98.15%, F1=98.12%, Macro F1=83.92%\n",
      "Epoch 120: Train Acc=98.18%, Val Acc=98.26%, F1=98.24%, Macro F1=84.10%\n",
      "Epoch 121: Train Acc=98.22%, Val Acc=98.17%, F1=98.14%, Macro F1=84.16%\n",
      "Epoch 122: Train Acc=98.20%, Val Acc=98.21%, F1=98.19%, Macro F1=84.18%\n",
      "Epoch 123: Train Acc=98.20%, Val Acc=97.93%, F1=97.91%, Macro F1=84.00%\n",
      "Epoch 124: Train Acc=98.18%, Val Acc=98.20%, F1=98.18%, Macro F1=84.04%\n",
      "Epoch 125: Train Acc=98.17%, Val Acc=98.28%, F1=98.26%, Macro F1=84.22%\n",
      "Epoch 126: Train Acc=98.19%, Val Acc=98.15%, F1=98.12%, Macro F1=83.92%\n",
      "Epoch 127: Train Acc=98.22%, Val Acc=98.20%, F1=98.18%, Macro F1=84.22%\n",
      "Epoch 128: Train Acc=98.15%, Val Acc=98.30%, F1=98.27%, Macro F1=84.28%\n",
      "Epoch 129: Train Acc=98.24%, Val Acc=98.20%, F1=98.18%, Macro F1=83.84%\n",
      "Epoch 130: Train Acc=98.23%, Val Acc=98.27%, F1=98.25%, Macro F1=84.17%\n",
      "Epoch 131: Train Acc=98.19%, Val Acc=98.26%, F1=98.23%, Macro F1=84.18%\n",
      "Epoch 132: Train Acc=98.21%, Val Acc=98.21%, F1=98.19%, Macro F1=84.27%\n",
      "Epoch 133: Train Acc=98.23%, Val Acc=98.32%, F1=98.29%, Macro F1=84.18%\n",
      "Epoch 134: Train Acc=98.21%, Val Acc=98.20%, F1=98.18%, Macro F1=84.02%\n",
      "Epoch 135: Train Acc=98.25%, Val Acc=98.35%, F1=98.33%, Macro F1=84.31%\n",
      "Epoch 136: Train Acc=98.28%, Val Acc=98.17%, F1=98.14%, Macro F1=84.22%\n",
      "Epoch 137: Train Acc=98.23%, Val Acc=98.20%, F1=98.18%, Macro F1=83.83%\n",
      "Epoch 138: Train Acc=98.22%, Val Acc=98.28%, F1=98.25%, Macro F1=84.11%\n",
      "Epoch 139: Train Acc=98.30%, Val Acc=98.22%, F1=98.20%, Macro F1=84.08%\n",
      "Epoch 140: Train Acc=98.26%, Val Acc=98.33%, F1=98.31%, Macro F1=84.25%\n",
      "Epoch 141: Train Acc=98.25%, Val Acc=97.87%, F1=97.85%, Macro F1=83.86%\n",
      "Epoch 142: Train Acc=98.27%, Val Acc=98.29%, F1=98.27%, Macro F1=84.23%\n",
      "Epoch 143: Train Acc=98.27%, Val Acc=98.18%, F1=98.16%, Macro F1=84.14%\n",
      "Epoch 144: Train Acc=98.27%, Val Acc=98.22%, F1=98.20%, Macro F1=84.14%\n",
      "Epoch 145: Train Acc=98.28%, Val Acc=98.34%, F1=98.31%, Macro F1=84.33%\n",
      "Epoch 146: Train Acc=98.29%, Val Acc=98.31%, F1=98.29%, Macro F1=84.19%\n",
      "Epoch 147: Train Acc=98.28%, Val Acc=98.34%, F1=98.32%, Macro F1=84.32%\n",
      "Epoch 148: Train Acc=98.27%, Val Acc=98.36%, F1=98.34%, Macro F1=84.32%\n",
      "Epoch 149: Train Acc=98.32%, Val Acc=98.27%, F1=98.25%, Macro F1=84.15%\n",
      "Epoch 150: Train Acc=98.35%, Val Acc=98.26%, F1=98.24%, Macro F1=84.09%\n",
      "Epoch 151: Train Acc=98.27%, Val Acc=98.32%, F1=98.30%, Macro F1=84.25%\n",
      "Epoch 152: Train Acc=98.30%, Val Acc=98.34%, F1=98.32%, Macro F1=84.30%\n",
      "Epoch 153: Train Acc=98.26%, Val Acc=98.07%, F1=98.05%, Macro F1=84.07%\n",
      "Epoch 154: Train Acc=98.28%, Val Acc=98.21%, F1=98.19%, Macro F1=83.71%\n",
      "Epoch 155: Train Acc=98.31%, Val Acc=98.30%, F1=98.27%, Macro F1=84.07%\n",
      "Epoch 156: Train Acc=98.30%, Val Acc=98.32%, F1=98.30%, Macro F1=84.07%\n",
      "Epoch 157: Train Acc=98.32%, Val Acc=98.39%, F1=98.37%, Macro F1=84.38%\n",
      "Epoch 158: Train Acc=98.28%, Val Acc=98.40%, F1=98.38%, Macro F1=84.23%\n",
      "Epoch 159: Train Acc=98.30%, Val Acc=98.25%, F1=98.23%, Macro F1=84.31%\n",
      "Epoch 160: Train Acc=98.32%, Val Acc=98.31%, F1=98.29%, Macro F1=84.23%\n",
      "Epoch 161: Train Acc=98.31%, Val Acc=98.30%, F1=98.28%, Macro F1=84.04%\n",
      "Epoch 162: Train Acc=98.34%, Val Acc=98.33%, F1=98.31%, Macro F1=84.28%\n",
      "Epoch 163: Train Acc=98.32%, Val Acc=98.31%, F1=98.29%, Macro F1=84.16%\n",
      "Epoch 164: Train Acc=98.30%, Val Acc=98.27%, F1=98.24%, Macro F1=83.91%\n",
      "Epoch 165: Train Acc=98.33%, Val Acc=98.36%, F1=98.33%, Macro F1=84.19%\n",
      "Epoch 166: Train Acc=98.34%, Val Acc=98.34%, F1=98.31%, Macro F1=84.14%\n",
      "Epoch 167: Train Acc=98.35%, Val Acc=97.89%, F1=97.88%, Macro F1=83.11%\n",
      "Epoch 168: Train Acc=98.34%, Val Acc=98.30%, F1=98.28%, Macro F1=84.16%\n",
      "Epoch 169: Train Acc=98.35%, Val Acc=98.28%, F1=98.26%, Macro F1=84.21%\n",
      "Epoch 170: Train Acc=98.34%, Val Acc=98.39%, F1=98.37%, Macro F1=84.13%\n",
      "Epoch 171: Train Acc=98.34%, Val Acc=98.28%, F1=98.25%, Macro F1=84.08%\n",
      "Epoch 172: Train Acc=98.35%, Val Acc=98.32%, F1=98.30%, Macro F1=84.06%\n",
      "Epoch 173: Train Acc=98.34%, Val Acc=98.43%, F1=98.41%, Macro F1=84.32%\n",
      "Epoch 174: Train Acc=98.37%, Val Acc=98.34%, F1=98.32%, Macro F1=84.21%\n",
      "Epoch 175: Train Acc=98.38%, Val Acc=98.23%, F1=98.21%, Macro F1=84.13%\n",
      "Epoch 176: Train Acc=98.36%, Val Acc=98.30%, F1=98.27%, Macro F1=84.24%\n",
      "Epoch 177: Train Acc=98.36%, Val Acc=98.42%, F1=98.40%, Macro F1=84.29%\n",
      "Epoch 178: Train Acc=98.34%, Val Acc=98.26%, F1=98.24%, Macro F1=84.21%\n",
      "Epoch 179: Train Acc=98.39%, Val Acc=98.29%, F1=98.27%, Macro F1=84.06%\n",
      "Epoch 180: Train Acc=98.33%, Val Acc=98.35%, F1=98.33%, Macro F1=84.19%\n",
      "Epoch 181: Train Acc=98.38%, Val Acc=98.30%, F1=98.28%, Macro F1=84.23%\n",
      "Epoch 182: Train Acc=98.33%, Val Acc=98.38%, F1=98.36%, Macro F1=84.20%\n",
      "Epoch 183: Train Acc=98.37%, Val Acc=98.36%, F1=98.34%, Macro F1=84.14%\n",
      "Epoch 184: Train Acc=98.37%, Val Acc=98.24%, F1=98.22%, Macro F1=84.04%\n",
      "Epoch 185: Train Acc=98.35%, Val Acc=98.38%, F1=98.36%, Macro F1=84.28%\n",
      "Epoch 186: Train Acc=98.36%, Val Acc=98.28%, F1=98.25%, Macro F1=84.16%\n",
      "Epoch 187: Train Acc=98.39%, Val Acc=98.30%, F1=98.28%, Macro F1=84.11%\n",
      "Epoch 188: Train Acc=98.36%, Val Acc=98.38%, F1=98.35%, Macro F1=84.27%\n",
      "Epoch 189: Train Acc=98.37%, Val Acc=97.93%, F1=97.91%, Macro F1=83.98%\n",
      "Epoch 190: Train Acc=98.41%, Val Acc=98.41%, F1=98.39%, Macro F1=84.31%\n",
      "Epoch 191: Train Acc=98.36%, Val Acc=98.30%, F1=98.28%, Macro F1=84.01%\n",
      "Epoch 192: Train Acc=98.37%, Val Acc=98.37%, F1=98.35%, Macro F1=84.20%\n",
      "Epoch 193: Train Acc=98.37%, Val Acc=98.38%, F1=98.35%, Macro F1=84.18%\n",
      "Epoch 194: Train Acc=98.40%, Val Acc=98.41%, F1=98.38%, Macro F1=84.21%\n",
      "Epoch 195: Train Acc=98.37%, Val Acc=98.40%, F1=98.38%, Macro F1=84.30%\n",
      "Epoch 196: Train Acc=98.37%, Val Acc=98.32%, F1=98.30%, Macro F1=84.20%\n",
      "Epoch 197: Train Acc=98.37%, Val Acc=98.41%, F1=98.38%, Macro F1=84.16%\n",
      "Epoch 198: Train Acc=98.36%, Val Acc=98.38%, F1=98.36%, Macro F1=84.15%\n",
      "Epoch 199: Train Acc=98.39%, Val Acc=98.32%, F1=98.29%, Macro F1=84.05%\n",
      "Epoch 200: Train Acc=98.40%, Val Acc=98.25%, F1=98.23%, Macro F1=84.09%\n",
      "\n",
      "Fold 3/5\n",
      "------------------------------\n",
      "initializing model...\n",
      "Epoch 1: Train Acc=67.08%, Val Acc=85.62%, F1=83.82%, Macro F1=50.03%\n",
      "Epoch 2: Train Acc=87.42%, Val Acc=90.06%, F1=89.07%, Macro F1=64.21%\n",
      "Epoch 3: Train Acc=91.03%, Val Acc=91.17%, F1=90.14%, Macro F1=64.72%\n",
      "Epoch 4: Train Acc=91.73%, Val Acc=91.20%, F1=90.24%, Macro F1=64.42%\n",
      "Epoch 5: Train Acc=92.63%, Val Acc=93.07%, F1=92.55%, Macro F1=70.81%\n",
      "Epoch 6: Train Acc=93.28%, Val Acc=94.15%, F1=93.99%, Macro F1=76.30%\n",
      "Epoch 7: Train Acc=94.19%, Val Acc=94.61%, F1=94.48%, Macro F1=77.42%\n",
      "Epoch 8: Train Acc=94.66%, Val Acc=94.77%, F1=94.71%, Macro F1=78.96%\n",
      "Epoch 9: Train Acc=94.91%, Val Acc=95.50%, F1=95.45%, Macro F1=79.61%\n",
      "Epoch 10: Train Acc=95.17%, Val Acc=95.88%, F1=95.84%, Macro F1=79.81%\n",
      "Epoch 11: Train Acc=95.32%, Val Acc=96.01%, F1=95.97%, Macro F1=80.36%\n",
      "Epoch 12: Train Acc=95.43%, Val Acc=96.18%, F1=96.15%, Macro F1=80.70%\n",
      "Epoch 13: Train Acc=95.63%, Val Acc=96.21%, F1=96.17%, Macro F1=80.83%\n",
      "Epoch 14: Train Acc=95.68%, Val Acc=96.25%, F1=96.21%, Macro F1=80.79%\n",
      "Epoch 15: Train Acc=95.77%, Val Acc=95.97%, F1=95.92%, Macro F1=80.43%\n",
      "Epoch 16: Train Acc=95.92%, Val Acc=96.18%, F1=96.14%, Macro F1=80.90%\n",
      "Epoch 17: Train Acc=96.02%, Val Acc=96.56%, F1=96.53%, Macro F1=81.22%\n",
      "Epoch 18: Train Acc=96.13%, Val Acc=96.29%, F1=96.24%, Macro F1=80.82%\n",
      "Epoch 19: Train Acc=96.21%, Val Acc=96.40%, F1=96.36%, Macro F1=81.02%\n",
      "Epoch 20: Train Acc=96.23%, Val Acc=96.61%, F1=96.56%, Macro F1=81.24%\n",
      "Epoch 21: Train Acc=96.31%, Val Acc=96.67%, F1=96.64%, Macro F1=81.31%\n",
      "Epoch 22: Train Acc=96.34%, Val Acc=96.59%, F1=96.55%, Macro F1=81.10%\n",
      "Epoch 23: Train Acc=96.42%, Val Acc=96.72%, F1=96.68%, Macro F1=81.22%\n",
      "Epoch 24: Train Acc=96.44%, Val Acc=96.89%, F1=96.86%, Macro F1=81.49%\n",
      "Epoch 25: Train Acc=96.51%, Val Acc=96.90%, F1=96.86%, Macro F1=81.27%\n",
      "Epoch 26: Train Acc=96.55%, Val Acc=96.94%, F1=96.91%, Macro F1=81.55%\n",
      "Epoch 27: Train Acc=96.64%, Val Acc=96.65%, F1=96.59%, Macro F1=80.87%\n",
      "Epoch 28: Train Acc=96.65%, Val Acc=96.90%, F1=96.86%, Macro F1=81.53%\n",
      "Epoch 29: Train Acc=96.68%, Val Acc=97.08%, F1=97.05%, Macro F1=81.71%\n",
      "Epoch 30: Train Acc=96.78%, Val Acc=97.06%, F1=97.03%, Macro F1=81.75%\n",
      "Epoch 31: Train Acc=96.73%, Val Acc=97.11%, F1=97.08%, Macro F1=81.90%\n",
      "Epoch 32: Train Acc=96.87%, Val Acc=96.89%, F1=96.83%, Macro F1=81.06%\n",
      "Epoch 33: Train Acc=96.83%, Val Acc=96.99%, F1=96.95%, Macro F1=81.50%\n",
      "Epoch 34: Train Acc=96.92%, Val Acc=97.25%, F1=97.22%, Macro F1=82.09%\n",
      "Epoch 35: Train Acc=97.04%, Val Acc=97.34%, F1=97.31%, Macro F1=82.32%\n",
      "Epoch 36: Train Acc=97.10%, Val Acc=97.29%, F1=97.27%, Macro F1=82.30%\n",
      "Epoch 37: Train Acc=97.14%, Val Acc=97.31%, F1=97.29%, Macro F1=82.34%\n",
      "Epoch 38: Train Acc=97.25%, Val Acc=97.45%, F1=97.42%, Macro F1=82.30%\n",
      "Epoch 39: Train Acc=97.27%, Val Acc=97.41%, F1=97.39%, Macro F1=82.46%\n",
      "Epoch 40: Train Acc=97.33%, Val Acc=97.21%, F1=97.17%, Macro F1=81.88%\n",
      "Epoch 41: Train Acc=97.41%, Val Acc=97.52%, F1=97.50%, Macro F1=82.73%\n",
      "Epoch 42: Train Acc=97.44%, Val Acc=97.58%, F1=97.55%, Macro F1=82.73%\n",
      "Epoch 43: Train Acc=97.47%, Val Acc=97.61%, F1=97.58%, Macro F1=83.07%\n",
      "Epoch 44: Train Acc=97.49%, Val Acc=97.66%, F1=97.64%, Macro F1=83.24%\n",
      "Epoch 45: Train Acc=97.50%, Val Acc=97.55%, F1=97.52%, Macro F1=82.87%\n",
      "Epoch 46: Train Acc=97.54%, Val Acc=97.62%, F1=97.60%, Macro F1=83.02%\n",
      "Epoch 47: Train Acc=97.56%, Val Acc=97.48%, F1=97.45%, Macro F1=82.20%\n",
      "Epoch 48: Train Acc=97.58%, Val Acc=97.70%, F1=97.68%, Macro F1=83.23%\n",
      "Epoch 49: Train Acc=97.62%, Val Acc=97.66%, F1=97.63%, Macro F1=83.03%\n",
      "Epoch 50: Train Acc=97.61%, Val Acc=97.63%, F1=97.61%, Macro F1=83.00%\n",
      "Epoch 51: Train Acc=97.61%, Val Acc=97.70%, F1=97.68%, Macro F1=83.17%\n",
      "Epoch 52: Train Acc=97.60%, Val Acc=97.47%, F1=97.45%, Macro F1=82.67%\n",
      "Epoch 53: Train Acc=97.73%, Val Acc=97.74%, F1=97.72%, Macro F1=83.30%\n",
      "Epoch 54: Train Acc=97.67%, Val Acc=97.72%, F1=97.69%, Macro F1=83.43%\n",
      "Epoch 55: Train Acc=97.72%, Val Acc=97.69%, F1=97.67%, Macro F1=83.27%\n",
      "Epoch 56: Train Acc=97.76%, Val Acc=96.03%, F1=96.01%, Macro F1=81.40%\n",
      "Epoch 57: Train Acc=97.74%, Val Acc=97.83%, F1=97.81%, Macro F1=83.45%\n",
      "Epoch 58: Train Acc=97.76%, Val Acc=97.78%, F1=97.77%, Macro F1=83.34%\n",
      "Epoch 59: Train Acc=97.78%, Val Acc=97.85%, F1=97.83%, Macro F1=83.53%\n",
      "Epoch 60: Train Acc=97.80%, Val Acc=97.74%, F1=97.73%, Macro F1=83.31%\n",
      "Epoch 61: Train Acc=97.82%, Val Acc=97.87%, F1=97.85%, Macro F1=83.54%\n",
      "Epoch 62: Train Acc=97.86%, Val Acc=97.85%, F1=97.83%, Macro F1=83.58%\n",
      "Epoch 63: Train Acc=97.89%, Val Acc=97.85%, F1=97.83%, Macro F1=83.48%\n",
      "Epoch 64: Train Acc=97.88%, Val Acc=97.89%, F1=97.88%, Macro F1=83.57%\n",
      "Epoch 65: Train Acc=97.87%, Val Acc=97.89%, F1=97.87%, Macro F1=83.57%\n",
      "Epoch 66: Train Acc=97.93%, Val Acc=97.85%, F1=97.83%, Macro F1=83.31%\n",
      "Epoch 67: Train Acc=97.91%, Val Acc=97.71%, F1=97.69%, Macro F1=83.37%\n",
      "Epoch 68: Train Acc=97.94%, Val Acc=97.92%, F1=97.90%, Macro F1=83.58%\n",
      "Epoch 69: Train Acc=97.94%, Val Acc=97.87%, F1=97.85%, Macro F1=83.53%\n",
      "Epoch 70: Train Acc=97.95%, Val Acc=97.96%, F1=97.94%, Macro F1=83.73%\n",
      "Epoch 71: Train Acc=97.96%, Val Acc=97.90%, F1=97.88%, Macro F1=83.58%\n",
      "Epoch 72: Train Acc=97.96%, Val Acc=97.83%, F1=97.81%, Macro F1=83.53%\n",
      "Epoch 73: Train Acc=97.99%, Val Acc=97.96%, F1=97.94%, Macro F1=83.65%\n",
      "Epoch 74: Train Acc=98.01%, Val Acc=97.98%, F1=97.96%, Macro F1=83.66%\n",
      "Epoch 75: Train Acc=98.00%, Val Acc=97.97%, F1=97.95%, Macro F1=83.56%\n",
      "Epoch 76: Train Acc=98.03%, Val Acc=98.00%, F1=97.98%, Macro F1=83.80%\n",
      "Epoch 77: Train Acc=98.03%, Val Acc=98.02%, F1=98.01%, Macro F1=83.75%\n",
      "Epoch 78: Train Acc=98.04%, Val Acc=98.03%, F1=98.01%, Macro F1=83.66%\n",
      "Epoch 79: Train Acc=98.08%, Val Acc=98.06%, F1=98.05%, Macro F1=83.71%\n",
      "Epoch 80: Train Acc=98.06%, Val Acc=97.91%, F1=97.90%, Macro F1=83.71%\n",
      "Epoch 81: Train Acc=98.06%, Val Acc=98.02%, F1=98.00%, Macro F1=83.87%\n",
      "Epoch 82: Train Acc=98.10%, Val Acc=97.85%, F1=97.84%, Macro F1=83.40%\n",
      "Epoch 83: Train Acc=98.11%, Val Acc=98.02%, F1=98.01%, Macro F1=83.70%\n",
      "Epoch 84: Train Acc=98.09%, Val Acc=98.02%, F1=98.00%, Macro F1=83.78%\n",
      "Epoch 85: Train Acc=98.11%, Val Acc=98.09%, F1=98.07%, Macro F1=83.85%\n",
      "Epoch 86: Train Acc=98.13%, Val Acc=97.92%, F1=97.90%, Macro F1=83.54%\n",
      "Epoch 87: Train Acc=98.11%, Val Acc=98.04%, F1=98.03%, Macro F1=83.77%\n",
      "Epoch 88: Train Acc=98.13%, Val Acc=98.17%, F1=98.16%, Macro F1=83.86%\n",
      "Epoch 89: Train Acc=98.16%, Val Acc=98.10%, F1=98.08%, Macro F1=83.77%\n",
      "Epoch 90: Train Acc=98.14%, Val Acc=97.98%, F1=97.96%, Macro F1=83.70%\n",
      "Epoch 91: Train Acc=98.13%, Val Acc=98.05%, F1=98.03%, Macro F1=83.86%\n",
      "Epoch 92: Train Acc=98.16%, Val Acc=98.09%, F1=98.07%, Macro F1=83.84%\n",
      "Epoch 93: Train Acc=98.19%, Val Acc=98.19%, F1=98.18%, Macro F1=83.75%\n",
      "Epoch 94: Train Acc=98.21%, Val Acc=98.04%, F1=98.02%, Macro F1=83.84%\n",
      "Epoch 95: Train Acc=98.20%, Val Acc=98.26%, F1=98.25%, Macro F1=83.95%\n",
      "Epoch 96: Train Acc=98.22%, Val Acc=98.21%, F1=98.20%, Macro F1=83.84%\n",
      "Epoch 97: Train Acc=98.26%, Val Acc=98.15%, F1=98.14%, Macro F1=83.82%\n",
      "Epoch 98: Train Acc=98.22%, Val Acc=98.17%, F1=98.16%, Macro F1=83.97%\n",
      "Epoch 99: Train Acc=98.26%, Val Acc=98.17%, F1=98.16%, Macro F1=83.96%\n",
      "Epoch 100: Train Acc=98.28%, Val Acc=98.16%, F1=98.14%, Macro F1=83.85%\n",
      "Epoch 101: Train Acc=98.25%, Val Acc=98.28%, F1=98.26%, Macro F1=83.93%\n",
      "Epoch 102: Train Acc=98.29%, Val Acc=98.22%, F1=98.20%, Macro F1=83.97%\n",
      "Epoch 103: Train Acc=98.25%, Val Acc=98.13%, F1=98.11%, Macro F1=83.83%\n",
      "Epoch 104: Train Acc=98.28%, Val Acc=98.22%, F1=98.20%, Macro F1=83.81%\n",
      "Epoch 105: Train Acc=98.30%, Val Acc=98.24%, F1=98.22%, Macro F1=83.85%\n",
      "Epoch 106: Train Acc=98.30%, Val Acc=98.06%, F1=98.04%, Macro F1=83.58%\n",
      "Epoch 107: Train Acc=98.30%, Val Acc=98.23%, F1=98.22%, Macro F1=84.04%\n",
      "Epoch 108: Train Acc=98.31%, Val Acc=97.44%, F1=97.42%, Macro F1=82.97%\n",
      "Epoch 109: Train Acc=98.33%, Val Acc=98.35%, F1=98.33%, Macro F1=84.09%\n",
      "Epoch 110: Train Acc=98.31%, Val Acc=98.30%, F1=98.28%, Macro F1=84.10%\n",
      "Epoch 111: Train Acc=98.34%, Val Acc=98.34%, F1=98.33%, Macro F1=83.99%\n",
      "Epoch 112: Train Acc=98.33%, Val Acc=98.28%, F1=98.26%, Macro F1=83.95%\n",
      "Epoch 113: Train Acc=98.32%, Val Acc=98.34%, F1=98.33%, Macro F1=84.07%\n",
      "Epoch 114: Train Acc=98.33%, Val Acc=98.24%, F1=98.23%, Macro F1=89.64%\n",
      "Epoch 115: Train Acc=98.33%, Val Acc=98.31%, F1=98.29%, Macro F1=83.97%\n",
      "Epoch 116: Train Acc=98.34%, Val Acc=98.15%, F1=98.14%, Macro F1=83.73%\n",
      "Epoch 117: Train Acc=98.34%, Val Acc=98.34%, F1=98.32%, Macro F1=84.07%\n",
      "Epoch 118: Train Acc=98.34%, Val Acc=98.18%, F1=98.16%, Macro F1=84.04%\n",
      "Epoch 119: Train Acc=98.36%, Val Acc=98.28%, F1=98.27%, Macro F1=89.75%\n",
      "Epoch 120: Train Acc=98.36%, Val Acc=98.36%, F1=98.35%, Macro F1=84.02%\n",
      "Epoch 121: Train Acc=98.36%, Val Acc=98.21%, F1=98.19%, Macro F1=83.60%\n",
      "Epoch 122: Train Acc=98.37%, Val Acc=98.26%, F1=98.25%, Macro F1=83.92%\n",
      "Epoch 123: Train Acc=98.38%, Val Acc=98.36%, F1=98.35%, Macro F1=84.16%\n",
      "Epoch 124: Train Acc=98.39%, Val Acc=98.26%, F1=98.24%, Macro F1=83.75%\n",
      "Epoch 125: Train Acc=98.35%, Val Acc=98.25%, F1=98.23%, Macro F1=83.87%\n",
      "Epoch 126: Train Acc=98.36%, Val Acc=98.34%, F1=98.33%, Macro F1=83.99%\n",
      "Epoch 127: Train Acc=98.38%, Val Acc=98.27%, F1=98.25%, Macro F1=84.10%\n",
      "Epoch 128: Train Acc=98.38%, Val Acc=98.23%, F1=98.22%, Macro F1=83.97%\n",
      "Epoch 129: Train Acc=98.36%, Val Acc=98.36%, F1=98.35%, Macro F1=89.73%\n",
      "Epoch 130: Train Acc=98.36%, Val Acc=98.36%, F1=98.36%, Macro F1=89.89%\n",
      "Epoch 131: Train Acc=98.38%, Val Acc=98.26%, F1=98.25%, Macro F1=83.96%\n",
      "Epoch 132: Train Acc=98.40%, Val Acc=98.34%, F1=98.33%, Macro F1=83.94%\n",
      "Epoch 133: Train Acc=98.41%, Val Acc=98.37%, F1=98.35%, Macro F1=84.08%\n",
      "Epoch 134: Train Acc=98.39%, Val Acc=98.40%, F1=98.38%, Macro F1=84.09%\n",
      "Epoch 135: Train Acc=98.41%, Val Acc=98.38%, F1=98.37%, Macro F1=84.10%\n",
      "Epoch 136: Train Acc=98.39%, Val Acc=98.40%, F1=98.38%, Macro F1=84.09%\n",
      "Epoch 137: Train Acc=98.41%, Val Acc=98.34%, F1=98.32%, Macro F1=83.98%\n",
      "Epoch 138: Train Acc=98.42%, Val Acc=98.21%, F1=98.19%, Macro F1=83.77%\n",
      "Epoch 139: Train Acc=98.39%, Val Acc=98.35%, F1=98.33%, Macro F1=84.09%\n",
      "Epoch 140: Train Acc=98.41%, Val Acc=98.39%, F1=98.37%, Macro F1=84.10%\n",
      "Epoch 141: Train Acc=98.40%, Val Acc=98.38%, F1=98.36%, Macro F1=84.01%\n",
      "Epoch 142: Train Acc=98.40%, Val Acc=98.34%, F1=98.32%, Macro F1=84.03%\n",
      "Epoch 143: Train Acc=98.43%, Val Acc=98.39%, F1=98.38%, Macro F1=84.14%\n",
      "Epoch 144: Train Acc=98.40%, Val Acc=98.37%, F1=98.35%, Macro F1=84.07%\n",
      "Epoch 145: Train Acc=98.42%, Val Acc=98.38%, F1=98.36%, Macro F1=84.16%\n",
      "Epoch 146: Train Acc=98.41%, Val Acc=98.32%, F1=98.31%, Macro F1=84.02%\n",
      "Epoch 147: Train Acc=98.43%, Val Acc=98.32%, F1=98.30%, Macro F1=83.87%\n",
      "Epoch 148: Train Acc=98.42%, Val Acc=98.38%, F1=98.37%, Macro F1=84.04%\n",
      "Epoch 149: Train Acc=98.40%, Val Acc=98.36%, F1=98.35%, Macro F1=83.89%\n",
      "Epoch 150: Train Acc=98.42%, Val Acc=98.38%, F1=98.37%, Macro F1=84.09%\n",
      "Epoch 151: Train Acc=98.42%, Val Acc=98.37%, F1=98.35%, Macro F1=83.90%\n",
      "Epoch 152: Train Acc=98.41%, Val Acc=98.36%, F1=98.35%, Macro F1=84.16%\n",
      "Epoch 153: Train Acc=98.41%, Val Acc=98.34%, F1=98.33%, Macro F1=83.95%\n",
      "Epoch 154: Train Acc=98.41%, Val Acc=98.39%, F1=98.38%, Macro F1=84.13%\n",
      "Epoch 155: Train Acc=98.42%, Val Acc=98.39%, F1=98.38%, Macro F1=84.11%\n",
      "Epoch 156: Train Acc=98.44%, Val Acc=98.34%, F1=98.32%, Macro F1=84.07%\n",
      "Epoch 157: Train Acc=98.43%, Val Acc=98.41%, F1=98.39%, Macro F1=84.07%\n",
      "Epoch 158: Train Acc=98.44%, Val Acc=98.39%, F1=98.38%, Macro F1=84.15%\n",
      "Epoch 159: Train Acc=98.42%, Val Acc=98.34%, F1=98.33%, Macro F1=83.99%\n",
      "Epoch 160: Train Acc=98.44%, Val Acc=98.38%, F1=98.36%, Macro F1=84.10%\n",
      "Epoch 161: Train Acc=98.44%, Val Acc=98.41%, F1=98.40%, Macro F1=84.16%\n",
      "Epoch 162: Train Acc=98.43%, Val Acc=98.40%, F1=98.38%, Macro F1=84.20%\n",
      "Epoch 163: Train Acc=98.46%, Val Acc=98.46%, F1=98.45%, Macro F1=89.94%\n",
      "Epoch 164: Train Acc=98.44%, Val Acc=98.18%, F1=98.17%, Macro F1=84.07%\n",
      "Epoch 165: Train Acc=98.43%, Val Acc=98.38%, F1=98.37%, Macro F1=84.06%\n",
      "Epoch 166: Train Acc=98.44%, Val Acc=98.34%, F1=98.33%, Macro F1=84.03%\n",
      "Epoch 167: Train Acc=98.46%, Val Acc=98.43%, F1=98.42%, Macro F1=89.93%\n",
      "Epoch 168: Train Acc=98.43%, Val Acc=98.41%, F1=98.40%, Macro F1=84.11%\n",
      "Epoch 169: Train Acc=98.43%, Val Acc=98.30%, F1=98.28%, Macro F1=84.06%\n",
      "Epoch 170: Train Acc=98.44%, Val Acc=98.04%, F1=98.02%, Macro F1=83.66%\n",
      "Epoch 171: Train Acc=98.43%, Val Acc=98.40%, F1=98.38%, Macro F1=84.20%\n",
      "Epoch 172: Train Acc=98.46%, Val Acc=98.39%, F1=98.38%, Macro F1=89.93%\n",
      "Epoch 173: Train Acc=98.40%, Val Acc=98.34%, F1=98.32%, Macro F1=84.19%\n",
      "Epoch 174: Train Acc=98.43%, Val Acc=98.44%, F1=98.42%, Macro F1=84.28%\n",
      "Epoch 175: Train Acc=98.46%, Val Acc=98.40%, F1=98.39%, Macro F1=89.93%\n",
      "Epoch 176: Train Acc=98.48%, Val Acc=98.38%, F1=98.37%, Macro F1=84.12%\n",
      "Epoch 177: Train Acc=98.44%, Val Acc=98.36%, F1=98.35%, Macro F1=89.86%\n",
      "Epoch 178: Train Acc=98.43%, Val Acc=98.41%, F1=98.40%, Macro F1=84.19%\n",
      "Epoch 179: Train Acc=98.49%, Val Acc=98.32%, F1=98.31%, Macro F1=89.79%\n",
      "Epoch 180: Train Acc=98.46%, Val Acc=98.27%, F1=98.26%, Macro F1=89.55%\n",
      "Epoch 181: Train Acc=98.45%, Val Acc=98.41%, F1=98.39%, Macro F1=84.17%\n",
      "Epoch 182: Train Acc=98.46%, Val Acc=98.38%, F1=98.37%, Macro F1=84.19%\n",
      "Epoch 183: Train Acc=98.44%, Val Acc=98.45%, F1=98.44%, Macro F1=89.96%\n",
      "Epoch 184: Train Acc=98.47%, Val Acc=98.43%, F1=98.42%, Macro F1=84.22%\n",
      "Epoch 185: Train Acc=98.47%, Val Acc=98.41%, F1=98.40%, Macro F1=89.88%\n",
      "Epoch 186: Train Acc=98.45%, Val Acc=98.34%, F1=98.33%, Macro F1=84.05%\n",
      "Epoch 187: Train Acc=98.48%, Val Acc=98.27%, F1=98.26%, Macro F1=89.82%\n",
      "Epoch 188: Train Acc=98.47%, Val Acc=98.41%, F1=98.40%, Macro F1=89.87%\n",
      "Epoch 189: Train Acc=98.47%, Val Acc=98.35%, F1=98.33%, Macro F1=83.95%\n",
      "Epoch 190: Train Acc=98.48%, Val Acc=98.36%, F1=98.34%, Macro F1=84.22%\n",
      "Epoch 191: Train Acc=98.44%, Val Acc=98.43%, F1=98.42%, Macro F1=89.95%\n",
      "Epoch 192: Train Acc=98.46%, Val Acc=98.33%, F1=98.31%, Macro F1=84.03%\n",
      "Epoch 193: Train Acc=98.44%, Val Acc=98.42%, F1=98.40%, Macro F1=84.21%\n",
      "Epoch 194: Train Acc=98.48%, Val Acc=98.43%, F1=98.41%, Macro F1=84.21%\n",
      "Epoch 195: Train Acc=98.47%, Val Acc=98.45%, F1=98.44%, Macro F1=89.90%\n",
      "Epoch 196: Train Acc=98.47%, Val Acc=98.38%, F1=98.38%, Macro F1=89.75%\n",
      "Epoch 197: Train Acc=98.47%, Val Acc=98.37%, F1=98.36%, Macro F1=89.79%\n",
      "Epoch 198: Train Acc=98.46%, Val Acc=98.34%, F1=98.33%, Macro F1=89.61%\n",
      "Epoch 199: Train Acc=98.48%, Val Acc=98.45%, F1=98.43%, Macro F1=84.24%\n",
      "Epoch 200: Train Acc=98.49%, Val Acc=98.46%, F1=98.45%, Macro F1=90.03%\n",
      "\n",
      "Fold 4/5\n",
      "------------------------------\n",
      "initializing model...\n",
      "Epoch 1: Train Acc=69.55%, Val Acc=79.23%, F1=74.77%, Macro F1=49.39%\n",
      "Epoch 2: Train Acc=83.20%, Val Acc=88.83%, F1=87.97%, Macro F1=62.91%\n",
      "Epoch 3: Train Acc=90.00%, Val Acc=90.81%, F1=89.87%, Macro F1=64.43%\n",
      "Epoch 4: Train Acc=91.28%, Val Acc=91.98%, F1=91.01%, Macro F1=65.28%\n",
      "Epoch 5: Train Acc=92.70%, Val Acc=93.33%, F1=92.96%, Macro F1=72.49%\n",
      "Epoch 6: Train Acc=94.03%, Val Acc=94.55%, F1=94.42%, Macro F1=77.19%\n",
      "Epoch 7: Train Acc=94.72%, Val Acc=95.04%, F1=94.97%, Macro F1=78.66%\n",
      "Epoch 8: Train Acc=95.04%, Val Acc=95.32%, F1=95.26%, Macro F1=79.25%\n",
      "Epoch 9: Train Acc=95.35%, Val Acc=95.42%, F1=95.36%, Macro F1=79.14%\n",
      "Epoch 10: Train Acc=95.52%, Val Acc=95.82%, F1=95.78%, Macro F1=80.08%\n",
      "Epoch 11: Train Acc=95.70%, Val Acc=95.96%, F1=95.91%, Macro F1=80.12%\n",
      "Epoch 12: Train Acc=95.86%, Val Acc=96.07%, F1=96.02%, Macro F1=80.12%\n",
      "Epoch 13: Train Acc=95.97%, Val Acc=96.13%, F1=96.08%, Macro F1=80.23%\n",
      "Epoch 14: Train Acc=96.12%, Val Acc=96.48%, F1=96.45%, Macro F1=80.72%\n",
      "Epoch 15: Train Acc=96.23%, Val Acc=96.36%, F1=96.32%, Macro F1=80.86%\n",
      "Epoch 16: Train Acc=96.34%, Val Acc=96.58%, F1=96.54%, Macro F1=80.88%\n",
      "Epoch 17: Train Acc=96.46%, Val Acc=96.59%, F1=96.56%, Macro F1=81.15%\n",
      "Epoch 18: Train Acc=96.53%, Val Acc=96.89%, F1=96.85%, Macro F1=81.54%\n",
      "Epoch 19: Train Acc=96.64%, Val Acc=96.74%, F1=96.70%, Macro F1=81.55%\n",
      "Epoch 20: Train Acc=96.78%, Val Acc=96.84%, F1=96.82%, Macro F1=81.78%\n",
      "Epoch 21: Train Acc=96.84%, Val Acc=97.02%, F1=97.00%, Macro F1=81.90%\n",
      "Epoch 22: Train Acc=96.94%, Val Acc=96.99%, F1=96.96%, Macro F1=81.93%\n",
      "Epoch 23: Train Acc=97.01%, Val Acc=96.98%, F1=96.95%, Macro F1=81.95%\n",
      "Epoch 24: Train Acc=97.11%, Val Acc=97.10%, F1=97.08%, Macro F1=82.19%\n",
      "Epoch 25: Train Acc=97.16%, Val Acc=97.34%, F1=97.31%, Macro F1=82.79%\n",
      "Epoch 26: Train Acc=97.19%, Val Acc=97.28%, F1=97.26%, Macro F1=82.68%\n",
      "Epoch 27: Train Acc=97.27%, Val Acc=97.12%, F1=97.09%, Macro F1=82.33%\n",
      "Epoch 28: Train Acc=97.29%, Val Acc=97.38%, F1=97.36%, Macro F1=82.80%\n",
      "Epoch 29: Train Acc=97.31%, Val Acc=97.40%, F1=97.38%, Macro F1=82.93%\n",
      "Epoch 30: Train Acc=97.34%, Val Acc=97.46%, F1=97.44%, Macro F1=82.81%\n",
      "Epoch 31: Train Acc=97.38%, Val Acc=97.34%, F1=97.31%, Macro F1=82.98%\n",
      "Epoch 32: Train Acc=97.39%, Val Acc=97.52%, F1=97.50%, Macro F1=82.89%\n",
      "Epoch 33: Train Acc=97.40%, Val Acc=97.50%, F1=97.48%, Macro F1=83.29%\n",
      "Epoch 34: Train Acc=97.43%, Val Acc=97.56%, F1=97.54%, Macro F1=83.37%\n",
      "Epoch 35: Train Acc=97.42%, Val Acc=97.46%, F1=97.44%, Macro F1=83.06%\n",
      "Epoch 36: Train Acc=97.45%, Val Acc=97.58%, F1=97.56%, Macro F1=83.36%\n",
      "Epoch 37: Train Acc=97.49%, Val Acc=97.61%, F1=97.59%, Macro F1=83.35%\n",
      "Epoch 38: Train Acc=97.48%, Val Acc=97.63%, F1=97.62%, Macro F1=83.34%\n",
      "Epoch 39: Train Acc=97.52%, Val Acc=97.57%, F1=97.55%, Macro F1=83.46%\n",
      "Epoch 40: Train Acc=97.55%, Val Acc=97.54%, F1=97.52%, Macro F1=83.16%\n",
      "Epoch 41: Train Acc=97.52%, Val Acc=97.63%, F1=97.62%, Macro F1=83.44%\n",
      "Epoch 42: Train Acc=97.54%, Val Acc=97.53%, F1=97.51%, Macro F1=83.11%\n",
      "Epoch 43: Train Acc=97.58%, Val Acc=97.61%, F1=97.60%, Macro F1=83.43%\n",
      "Epoch 44: Train Acc=97.57%, Val Acc=97.76%, F1=97.74%, Macro F1=83.55%\n",
      "Epoch 45: Train Acc=97.63%, Val Acc=97.68%, F1=97.66%, Macro F1=83.39%\n",
      "Epoch 46: Train Acc=97.60%, Val Acc=97.60%, F1=97.58%, Macro F1=83.36%\n",
      "Epoch 47: Train Acc=97.63%, Val Acc=97.61%, F1=97.60%, Macro F1=83.36%\n",
      "Epoch 48: Train Acc=97.65%, Val Acc=97.40%, F1=97.39%, Macro F1=82.83%\n",
      "Epoch 49: Train Acc=97.63%, Val Acc=97.67%, F1=97.65%, Macro F1=83.47%\n",
      "Epoch 50: Train Acc=97.66%, Val Acc=97.69%, F1=97.67%, Macro F1=83.48%\n",
      "Epoch 51: Train Acc=97.67%, Val Acc=97.67%, F1=97.65%, Macro F1=83.56%\n",
      "Epoch 52: Train Acc=97.69%, Val Acc=97.72%, F1=97.70%, Macro F1=83.56%\n",
      "Epoch 53: Train Acc=97.70%, Val Acc=97.79%, F1=97.78%, Macro F1=83.60%\n",
      "Epoch 54: Train Acc=97.67%, Val Acc=97.62%, F1=97.60%, Macro F1=83.33%\n",
      "Epoch 55: Train Acc=97.72%, Val Acc=97.73%, F1=97.71%, Macro F1=83.53%\n",
      "Epoch 56: Train Acc=97.73%, Val Acc=97.80%, F1=97.78%, Macro F1=83.61%\n",
      "Epoch 57: Train Acc=97.75%, Val Acc=97.78%, F1=97.77%, Macro F1=83.68%\n",
      "Epoch 58: Train Acc=97.73%, Val Acc=97.78%, F1=97.77%, Macro F1=83.59%\n",
      "Epoch 59: Train Acc=97.72%, Val Acc=97.79%, F1=97.78%, Macro F1=83.67%\n",
      "Epoch 60: Train Acc=97.76%, Val Acc=97.51%, F1=97.47%, Macro F1=82.34%\n",
      "Epoch 61: Train Acc=97.76%, Val Acc=97.59%, F1=97.57%, Macro F1=83.22%\n",
      "Epoch 62: Train Acc=97.77%, Val Acc=97.68%, F1=97.66%, Macro F1=83.54%\n",
      "Epoch 63: Train Acc=97.80%, Val Acc=97.78%, F1=97.77%, Macro F1=83.56%\n",
      "Epoch 64: Train Acc=97.81%, Val Acc=97.89%, F1=97.88%, Macro F1=83.75%\n",
      "Epoch 65: Train Acc=97.75%, Val Acc=97.81%, F1=97.79%, Macro F1=83.67%\n",
      "Epoch 66: Train Acc=97.83%, Val Acc=97.85%, F1=97.83%, Macro F1=83.64%\n",
      "Epoch 67: Train Acc=97.76%, Val Acc=97.77%, F1=97.75%, Macro F1=83.44%\n",
      "Epoch 68: Train Acc=97.81%, Val Acc=97.89%, F1=97.88%, Macro F1=83.76%\n",
      "Epoch 69: Train Acc=97.82%, Val Acc=97.77%, F1=97.75%, Macro F1=83.49%\n",
      "Epoch 70: Train Acc=97.82%, Val Acc=97.76%, F1=97.75%, Macro F1=83.53%\n",
      "Epoch 71: Train Acc=97.83%, Val Acc=97.83%, F1=97.82%, Macro F1=83.70%\n",
      "Epoch 72: Train Acc=97.85%, Val Acc=97.76%, F1=97.75%, Macro F1=83.65%\n",
      "Epoch 73: Train Acc=97.86%, Val Acc=97.82%, F1=97.80%, Macro F1=83.68%\n",
      "Epoch 74: Train Acc=97.84%, Val Acc=97.84%, F1=97.82%, Macro F1=83.68%\n",
      "Epoch 75: Train Acc=97.87%, Val Acc=97.85%, F1=97.83%, Macro F1=83.74%\n",
      "Epoch 76: Train Acc=97.89%, Val Acc=97.85%, F1=97.84%, Macro F1=83.70%\n",
      "Epoch 77: Train Acc=97.85%, Val Acc=97.83%, F1=97.81%, Macro F1=83.70%\n",
      "Epoch 78: Train Acc=97.87%, Val Acc=97.78%, F1=97.77%, Macro F1=83.47%\n",
      "Epoch 79: Train Acc=97.89%, Val Acc=97.87%, F1=97.85%, Macro F1=83.78%\n",
      "Epoch 80: Train Acc=97.90%, Val Acc=97.87%, F1=97.86%, Macro F1=83.62%\n",
      "Epoch 81: Train Acc=97.92%, Val Acc=97.84%, F1=97.82%, Macro F1=83.71%\n",
      "Epoch 82: Train Acc=97.94%, Val Acc=97.94%, F1=97.93%, Macro F1=83.85%\n",
      "Epoch 83: Train Acc=97.91%, Val Acc=97.53%, F1=97.50%, Macro F1=82.64%\n",
      "Epoch 84: Train Acc=97.89%, Val Acc=97.84%, F1=97.82%, Macro F1=83.69%\n",
      "Epoch 85: Train Acc=97.89%, Val Acc=97.90%, F1=97.88%, Macro F1=83.72%\n",
      "Epoch 86: Train Acc=97.92%, Val Acc=97.98%, F1=97.96%, Macro F1=83.76%\n",
      "Epoch 87: Train Acc=97.95%, Val Acc=98.02%, F1=98.00%, Macro F1=83.80%\n",
      "Epoch 88: Train Acc=97.80%, Val Acc=97.82%, F1=97.80%, Macro F1=83.53%\n",
      "Epoch 89: Train Acc=97.93%, Val Acc=97.98%, F1=97.96%, Macro F1=83.69%\n",
      "Epoch 90: Train Acc=97.94%, Val Acc=97.85%, F1=97.83%, Macro F1=83.64%\n",
      "Epoch 91: Train Acc=97.96%, Val Acc=97.87%, F1=97.85%, Macro F1=83.72%\n",
      "Epoch 92: Train Acc=97.97%, Val Acc=97.97%, F1=97.96%, Macro F1=83.73%\n",
      "Epoch 93: Train Acc=97.96%, Val Acc=97.99%, F1=97.98%, Macro F1=83.69%\n",
      "Epoch 94: Train Acc=97.99%, Val Acc=97.84%, F1=97.82%, Macro F1=83.69%\n",
      "Epoch 95: Train Acc=97.97%, Val Acc=97.96%, F1=97.94%, Macro F1=83.73%\n",
      "Epoch 96: Train Acc=98.01%, Val Acc=97.89%, F1=97.87%, Macro F1=83.62%\n",
      "Epoch 97: Train Acc=97.99%, Val Acc=97.84%, F1=97.82%, Macro F1=83.56%\n",
      "Epoch 98: Train Acc=98.00%, Val Acc=98.04%, F1=98.03%, Macro F1=83.67%\n",
      "Epoch 99: Train Acc=98.01%, Val Acc=98.00%, F1=97.98%, Macro F1=83.71%\n",
      "Epoch 100: Train Acc=98.03%, Val Acc=98.00%, F1=97.99%, Macro F1=83.70%\n",
      "Epoch 101: Train Acc=98.04%, Val Acc=97.96%, F1=97.94%, Macro F1=83.92%\n",
      "Epoch 102: Train Acc=98.03%, Val Acc=97.96%, F1=97.95%, Macro F1=83.68%\n",
      "Epoch 103: Train Acc=98.02%, Val Acc=98.08%, F1=98.07%, Macro F1=83.75%\n",
      "Epoch 104: Train Acc=98.06%, Val Acc=97.95%, F1=97.93%, Macro F1=83.75%\n",
      "Epoch 105: Train Acc=98.09%, Val Acc=98.02%, F1=98.01%, Macro F1=83.67%\n",
      "Epoch 106: Train Acc=98.06%, Val Acc=98.04%, F1=98.03%, Macro F1=83.78%\n",
      "Epoch 107: Train Acc=98.05%, Val Acc=98.00%, F1=97.99%, Macro F1=83.87%\n",
      "Epoch 108: Train Acc=98.04%, Val Acc=98.11%, F1=98.10%, Macro F1=83.89%\n",
      "Epoch 109: Train Acc=98.08%, Val Acc=97.95%, F1=97.93%, Macro F1=83.82%\n",
      "Epoch 110: Train Acc=98.08%, Val Acc=98.00%, F1=97.99%, Macro F1=83.82%\n",
      "Epoch 111: Train Acc=98.05%, Val Acc=98.03%, F1=98.01%, Macro F1=83.84%\n",
      "Epoch 112: Train Acc=98.09%, Val Acc=98.11%, F1=98.09%, Macro F1=83.94%\n",
      "Epoch 113: Train Acc=98.12%, Val Acc=97.94%, F1=97.93%, Macro F1=83.51%\n",
      "Epoch 114: Train Acc=98.10%, Val Acc=98.13%, F1=98.11%, Macro F1=83.96%\n",
      "Epoch 115: Train Acc=98.11%, Val Acc=98.11%, F1=98.10%, Macro F1=83.87%\n",
      "Epoch 116: Train Acc=98.15%, Val Acc=98.12%, F1=98.11%, Macro F1=83.80%\n",
      "Epoch 117: Train Acc=98.12%, Val Acc=98.14%, F1=98.13%, Macro F1=83.85%\n",
      "Epoch 118: Train Acc=98.13%, Val Acc=98.08%, F1=98.06%, Macro F1=83.88%\n",
      "Epoch 119: Train Acc=98.15%, Val Acc=98.11%, F1=98.10%, Macro F1=83.69%\n",
      "Epoch 120: Train Acc=98.17%, Val Acc=98.06%, F1=98.04%, Macro F1=83.80%\n",
      "Epoch 121: Train Acc=98.12%, Val Acc=97.81%, F1=97.79%, Macro F1=83.75%\n",
      "Epoch 122: Train Acc=98.15%, Val Acc=98.20%, F1=98.19%, Macro F1=83.97%\n",
      "Epoch 123: Train Acc=98.17%, Val Acc=98.06%, F1=98.05%, Macro F1=83.83%\n",
      "Epoch 124: Train Acc=98.20%, Val Acc=98.20%, F1=98.19%, Macro F1=84.02%\n",
      "Epoch 125: Train Acc=98.19%, Val Acc=98.20%, F1=98.19%, Macro F1=83.89%\n",
      "Epoch 126: Train Acc=98.21%, Val Acc=98.11%, F1=98.10%, Macro F1=83.94%\n",
      "Epoch 127: Train Acc=98.20%, Val Acc=97.98%, F1=97.96%, Macro F1=83.87%\n",
      "Epoch 128: Train Acc=98.19%, Val Acc=98.10%, F1=98.09%, Macro F1=84.00%\n",
      "Epoch 129: Train Acc=98.20%, Val Acc=98.12%, F1=98.11%, Macro F1=83.81%\n",
      "Epoch 130: Train Acc=98.18%, Val Acc=98.26%, F1=98.24%, Macro F1=84.10%\n",
      "Epoch 131: Train Acc=98.25%, Val Acc=98.15%, F1=98.14%, Macro F1=83.93%\n",
      "Epoch 132: Train Acc=98.25%, Val Acc=98.04%, F1=98.03%, Macro F1=83.89%\n",
      "Epoch 133: Train Acc=98.24%, Val Acc=98.23%, F1=98.22%, Macro F1=84.11%\n",
      "Epoch 134: Train Acc=98.27%, Val Acc=98.22%, F1=98.21%, Macro F1=84.01%\n",
      "Epoch 135: Train Acc=98.24%, Val Acc=98.19%, F1=98.18%, Macro F1=84.00%\n",
      "Epoch 136: Train Acc=98.23%, Val Acc=98.06%, F1=98.05%, Macro F1=83.84%\n",
      "Epoch 137: Train Acc=98.28%, Val Acc=98.29%, F1=98.28%, Macro F1=84.19%\n",
      "Epoch 138: Train Acc=98.26%, Val Acc=98.05%, F1=98.04%, Macro F1=83.92%\n",
      "Epoch 139: Train Acc=98.30%, Val Acc=98.26%, F1=98.24%, Macro F1=84.10%\n",
      "Epoch 140: Train Acc=98.31%, Val Acc=98.30%, F1=98.29%, Macro F1=84.08%\n",
      "Epoch 141: Train Acc=98.25%, Val Acc=98.29%, F1=98.28%, Macro F1=84.25%\n",
      "Epoch 142: Train Acc=98.31%, Val Acc=97.40%, F1=97.29%, Macro F1=80.52%\n",
      "Epoch 143: Train Acc=98.29%, Val Acc=98.28%, F1=98.26%, Macro F1=84.12%\n",
      "Epoch 144: Train Acc=98.29%, Val Acc=98.25%, F1=98.24%, Macro F1=84.17%\n",
      "Epoch 145: Train Acc=98.32%, Val Acc=98.19%, F1=98.17%, Macro F1=83.95%\n",
      "Epoch 146: Train Acc=98.32%, Val Acc=97.96%, F1=97.94%, Macro F1=83.92%\n",
      "Epoch 147: Train Acc=98.32%, Val Acc=97.97%, F1=97.96%, Macro F1=83.97%\n",
      "Epoch 148: Train Acc=98.31%, Val Acc=98.27%, F1=98.26%, Macro F1=84.05%\n",
      "Epoch 149: Train Acc=98.33%, Val Acc=98.31%, F1=98.30%, Macro F1=84.19%\n",
      "Epoch 150: Train Acc=98.35%, Val Acc=98.17%, F1=98.15%, Macro F1=83.79%\n",
      "Epoch 151: Train Acc=98.35%, Val Acc=98.33%, F1=98.32%, Macro F1=84.21%\n",
      "Epoch 152: Train Acc=98.38%, Val Acc=98.17%, F1=98.15%, Macro F1=83.73%\n",
      "Epoch 153: Train Acc=98.36%, Val Acc=98.28%, F1=98.27%, Macro F1=84.18%\n",
      "Epoch 154: Train Acc=98.34%, Val Acc=98.36%, F1=98.35%, Macro F1=84.22%\n",
      "Epoch 155: Train Acc=98.37%, Val Acc=98.38%, F1=98.37%, Macro F1=84.17%\n",
      "Epoch 156: Train Acc=98.36%, Val Acc=98.28%, F1=98.27%, Macro F1=84.05%\n",
      "Epoch 157: Train Acc=98.37%, Val Acc=98.18%, F1=98.17%, Macro F1=83.81%\n",
      "Epoch 158: Train Acc=98.41%, Val Acc=98.12%, F1=98.10%, Macro F1=84.17%\n",
      "Epoch 159: Train Acc=98.37%, Val Acc=98.32%, F1=98.31%, Macro F1=84.10%\n",
      "Epoch 160: Train Acc=98.39%, Val Acc=98.36%, F1=98.35%, Macro F1=84.18%\n",
      "Epoch 161: Train Acc=98.38%, Val Acc=98.33%, F1=98.32%, Macro F1=84.25%\n",
      "Epoch 162: Train Acc=98.39%, Val Acc=98.24%, F1=98.23%, Macro F1=84.22%\n",
      "Epoch 163: Train Acc=98.44%, Val Acc=98.36%, F1=98.35%, Macro F1=84.18%\n",
      "Epoch 164: Train Acc=98.39%, Val Acc=98.35%, F1=98.34%, Macro F1=84.27%\n",
      "Epoch 165: Train Acc=98.41%, Val Acc=98.35%, F1=98.34%, Macro F1=84.36%\n",
      "Epoch 166: Train Acc=98.40%, Val Acc=98.38%, F1=98.37%, Macro F1=84.37%\n",
      "Epoch 167: Train Acc=98.43%, Val Acc=98.30%, F1=98.29%, Macro F1=84.31%\n",
      "Epoch 168: Train Acc=98.42%, Val Acc=98.25%, F1=98.24%, Macro F1=84.16%\n",
      "Epoch 169: Train Acc=98.43%, Val Acc=98.37%, F1=98.36%, Macro F1=84.26%\n",
      "Epoch 170: Train Acc=98.44%, Val Acc=98.43%, F1=98.42%, Macro F1=84.42%\n",
      "Epoch 171: Train Acc=98.40%, Val Acc=98.44%, F1=98.43%, Macro F1=84.38%\n",
      "Epoch 172: Train Acc=98.40%, Val Acc=98.15%, F1=98.14%, Macro F1=83.97%\n",
      "Epoch 173: Train Acc=98.42%, Val Acc=98.41%, F1=98.40%, Macro F1=84.43%\n",
      "Epoch 174: Train Acc=98.46%, Val Acc=98.38%, F1=98.37%, Macro F1=84.38%\n",
      "Epoch 175: Train Acc=98.44%, Val Acc=98.43%, F1=98.42%, Macro F1=84.35%\n",
      "Epoch 176: Train Acc=98.43%, Val Acc=98.41%, F1=98.39%, Macro F1=84.39%\n",
      "Epoch 177: Train Acc=98.47%, Val Acc=97.48%, F1=97.37%, Macro F1=80.54%\n",
      "Epoch 178: Train Acc=98.46%, Val Acc=98.36%, F1=98.34%, Macro F1=84.29%\n",
      "Epoch 179: Train Acc=98.48%, Val Acc=98.34%, F1=98.32%, Macro F1=84.11%\n",
      "Epoch 180: Train Acc=98.44%, Val Acc=98.30%, F1=98.29%, Macro F1=84.27%\n",
      "Epoch 181: Train Acc=98.47%, Val Acc=98.39%, F1=98.38%, Macro F1=84.33%\n",
      "Epoch 182: Train Acc=98.46%, Val Acc=98.33%, F1=98.32%, Macro F1=84.22%\n",
      "Epoch 183: Train Acc=98.44%, Val Acc=98.00%, F1=97.98%, Macro F1=83.70%\n",
      "Epoch 184: Train Acc=98.43%, Val Acc=98.38%, F1=98.37%, Macro F1=84.44%\n",
      "Epoch 185: Train Acc=98.48%, Val Acc=98.30%, F1=98.28%, Macro F1=84.19%\n",
      "Epoch 186: Train Acc=98.48%, Val Acc=98.38%, F1=98.37%, Macro F1=84.32%\n",
      "Epoch 187: Train Acc=98.47%, Val Acc=98.11%, F1=98.09%, Macro F1=84.28%\n",
      "Epoch 188: Train Acc=98.46%, Val Acc=98.32%, F1=98.31%, Macro F1=84.09%\n",
      "Epoch 189: Train Acc=98.48%, Val Acc=98.30%, F1=98.28%, Macro F1=84.19%\n",
      "Epoch 190: Train Acc=98.45%, Val Acc=98.44%, F1=98.43%, Macro F1=84.40%\n",
      "Epoch 191: Train Acc=98.46%, Val Acc=98.41%, F1=98.40%, Macro F1=84.33%\n",
      "Epoch 192: Train Acc=98.44%, Val Acc=98.40%, F1=98.39%, Macro F1=84.43%\n",
      "Epoch 193: Train Acc=98.49%, Val Acc=98.41%, F1=98.39%, Macro F1=84.44%\n",
      "Epoch 194: Train Acc=98.49%, Val Acc=98.33%, F1=98.32%, Macro F1=84.35%\n",
      "Epoch 195: Train Acc=98.48%, Val Acc=98.43%, F1=98.42%, Macro F1=84.50%\n",
      "Epoch 196: Train Acc=98.48%, Val Acc=98.36%, F1=98.35%, Macro F1=84.38%\n",
      "Epoch 197: Train Acc=98.47%, Val Acc=98.33%, F1=98.32%, Macro F1=84.28%\n",
      "Epoch 198: Train Acc=98.47%, Val Acc=98.33%, F1=98.32%, Macro F1=84.35%\n",
      "Epoch 199: Train Acc=98.49%, Val Acc=98.44%, F1=98.43%, Macro F1=84.35%\n",
      "Epoch 200: Train Acc=98.51%, Val Acc=98.38%, F1=98.37%, Macro F1=84.36%\n",
      "\n",
      "Fold 5/5\n",
      "------------------------------\n",
      "initializing model...\n",
      "Epoch 1: Train Acc=73.39%, Val Acc=83.85%, F1=82.03%, Macro F1=48.89%\n",
      "Epoch 2: Train Acc=87.30%, Val Acc=90.29%, F1=89.22%, Macro F1=64.13%\n",
      "Epoch 3: Train Acc=91.10%, Val Acc=91.41%, F1=90.32%, Macro F1=64.87%\n",
      "Epoch 4: Train Acc=91.78%, Val Acc=92.19%, F1=91.65%, Macro F1=70.61%\n",
      "Epoch 5: Train Acc=92.67%, Val Acc=92.73%, F1=92.20%, Macro F1=71.03%\n",
      "Epoch 6: Train Acc=93.22%, Val Acc=92.95%, F1=92.44%, Macro F1=71.25%\n",
      "Epoch 7: Train Acc=93.74%, Val Acc=93.40%, F1=92.87%, Macro F1=71.45%\n",
      "Epoch 8: Train Acc=94.06%, Val Acc=93.89%, F1=93.37%, Macro F1=71.87%\n",
      "Epoch 9: Train Acc=94.95%, Val Acc=94.92%, F1=94.78%, Macro F1=77.46%\n",
      "Epoch 10: Train Acc=95.17%, Val Acc=95.11%, F1=95.03%, Macro F1=78.74%\n",
      "Epoch 11: Train Acc=95.32%, Val Acc=95.12%, F1=95.05%, Macro F1=78.73%\n",
      "Epoch 12: Train Acc=95.52%, Val Acc=95.28%, F1=95.21%, Macro F1=79.02%\n",
      "Epoch 13: Train Acc=95.61%, Val Acc=95.42%, F1=95.35%, Macro F1=79.15%\n",
      "Epoch 14: Train Acc=95.69%, Val Acc=95.58%, F1=95.52%, Macro F1=79.60%\n",
      "Epoch 15: Train Acc=95.75%, Val Acc=95.54%, F1=95.47%, Macro F1=79.34%\n",
      "Epoch 16: Train Acc=95.88%, Val Acc=95.82%, F1=95.72%, Macro F1=79.17%\n",
      "Epoch 17: Train Acc=95.91%, Val Acc=95.56%, F1=95.49%, Macro F1=79.67%\n",
      "Epoch 18: Train Acc=96.00%, Val Acc=95.89%, F1=95.84%, Macro F1=79.94%\n",
      "Epoch 19: Train Acc=96.10%, Val Acc=95.88%, F1=95.82%, Macro F1=80.07%\n",
      "Epoch 20: Train Acc=96.13%, Val Acc=96.05%, F1=95.99%, Macro F1=80.36%\n",
      "Epoch 21: Train Acc=96.25%, Val Acc=95.84%, F1=95.72%, Macro F1=78.91%\n",
      "Epoch 22: Train Acc=96.30%, Val Acc=96.14%, F1=96.08%, Macro F1=80.29%\n",
      "Epoch 23: Train Acc=96.42%, Val Acc=96.24%, F1=96.18%, Macro F1=80.66%\n",
      "Epoch 24: Train Acc=96.46%, Val Acc=96.49%, F1=96.44%, Macro F1=80.90%\n",
      "Epoch 25: Train Acc=96.57%, Val Acc=96.34%, F1=96.29%, Macro F1=80.80%\n",
      "Epoch 26: Train Acc=96.64%, Val Acc=96.50%, F1=96.45%, Macro F1=81.09%\n",
      "Epoch 27: Train Acc=96.69%, Val Acc=96.63%, F1=96.58%, Macro F1=81.27%\n",
      "Epoch 28: Train Acc=96.70%, Val Acc=96.71%, F1=96.65%, Macro F1=81.18%\n",
      "Epoch 29: Train Acc=96.76%, Val Acc=96.63%, F1=96.58%, Macro F1=81.17%\n",
      "Epoch 30: Train Acc=96.83%, Val Acc=96.59%, F1=96.54%, Macro F1=81.24%\n",
      "Epoch 31: Train Acc=96.86%, Val Acc=96.70%, F1=96.65%, Macro F1=81.24%\n",
      "Epoch 32: Train Acc=96.89%, Val Acc=96.35%, F1=96.30%, Macro F1=81.22%\n",
      "Epoch 33: Train Acc=96.93%, Val Acc=96.69%, F1=96.64%, Macro F1=81.56%\n",
      "Epoch 34: Train Acc=97.02%, Val Acc=96.87%, F1=96.82%, Macro F1=82.06%\n",
      "Epoch 35: Train Acc=97.03%, Val Acc=96.63%, F1=96.59%, Macro F1=81.66%\n",
      "Epoch 36: Train Acc=97.06%, Val Acc=96.99%, F1=96.94%, Macro F1=81.96%\n",
      "Epoch 37: Train Acc=97.10%, Val Acc=96.97%, F1=96.92%, Macro F1=82.10%\n",
      "Epoch 38: Train Acc=97.19%, Val Acc=96.93%, F1=96.89%, Macro F1=82.26%\n",
      "Epoch 39: Train Acc=97.22%, Val Acc=96.81%, F1=96.76%, Macro F1=81.82%\n",
      "Epoch 40: Train Acc=97.27%, Val Acc=97.06%, F1=97.01%, Macro F1=82.37%\n",
      "Epoch 41: Train Acc=97.29%, Val Acc=97.06%, F1=97.02%, Macro F1=82.19%\n",
      "Epoch 42: Train Acc=97.28%, Val Acc=97.12%, F1=97.08%, Macro F1=82.44%\n",
      "Epoch 43: Train Acc=97.32%, Val Acc=97.21%, F1=97.16%, Macro F1=82.84%\n",
      "Epoch 44: Train Acc=97.33%, Val Acc=97.23%, F1=97.18%, Macro F1=82.66%\n",
      "Epoch 45: Train Acc=97.03%, Val Acc=96.91%, F1=96.87%, Macro F1=81.93%\n",
      "Epoch 46: Train Acc=97.27%, Val Acc=97.08%, F1=97.03%, Macro F1=82.10%\n",
      "Epoch 47: Train Acc=97.29%, Val Acc=97.31%, F1=97.26%, Macro F1=82.76%\n",
      "Epoch 48: Train Acc=97.34%, Val Acc=97.11%, F1=97.07%, Macro F1=82.38%\n",
      "Epoch 49: Train Acc=97.37%, Val Acc=97.44%, F1=97.39%, Macro F1=83.00%\n",
      "Epoch 50: Train Acc=97.40%, Val Acc=97.43%, F1=97.39%, Macro F1=83.09%\n",
      "Epoch 51: Train Acc=97.48%, Val Acc=97.35%, F1=97.30%, Macro F1=82.96%\n",
      "Epoch 52: Train Acc=97.58%, Val Acc=97.63%, F1=97.60%, Macro F1=83.04%\n",
      "Epoch 53: Train Acc=97.56%, Val Acc=97.57%, F1=97.54%, Macro F1=82.96%\n",
      "Epoch 54: Train Acc=97.64%, Val Acc=97.66%, F1=97.62%, Macro F1=83.46%\n",
      "Epoch 55: Train Acc=97.71%, Val Acc=97.81%, F1=97.77%, Macro F1=83.59%\n",
      "Epoch 56: Train Acc=97.74%, Val Acc=97.66%, F1=97.62%, Macro F1=83.45%\n",
      "Epoch 57: Train Acc=97.79%, Val Acc=97.72%, F1=97.68%, Macro F1=83.31%\n",
      "Epoch 58: Train Acc=97.81%, Val Acc=97.73%, F1=97.69%, Macro F1=83.63%\n",
      "Epoch 59: Train Acc=97.90%, Val Acc=97.61%, F1=97.58%, Macro F1=83.49%\n",
      "Epoch 60: Train Acc=97.93%, Val Acc=97.95%, F1=97.91%, Macro F1=83.84%\n",
      "Epoch 61: Train Acc=97.92%, Val Acc=97.82%, F1=97.78%, Macro F1=83.46%\n",
      "Epoch 62: Train Acc=97.97%, Val Acc=97.76%, F1=97.72%, Macro F1=83.74%\n",
      "Epoch 63: Train Acc=97.97%, Val Acc=98.01%, F1=97.98%, Macro F1=84.01%\n",
      "Epoch 64: Train Acc=98.03%, Val Acc=97.89%, F1=97.85%, Macro F1=83.94%\n",
      "Epoch 65: Train Acc=98.02%, Val Acc=98.05%, F1=98.02%, Macro F1=84.04%\n",
      "Epoch 66: Train Acc=98.07%, Val Acc=97.95%, F1=97.91%, Macro F1=83.89%\n",
      "Epoch 67: Train Acc=98.06%, Val Acc=98.10%, F1=98.06%, Macro F1=84.06%\n",
      "Epoch 68: Train Acc=98.05%, Val Acc=98.06%, F1=98.03%, Macro F1=83.96%\n",
      "Epoch 69: Train Acc=98.05%, Val Acc=98.01%, F1=97.97%, Macro F1=83.59%\n",
      "Epoch 70: Train Acc=98.06%, Val Acc=98.24%, F1=98.21%, Macro F1=84.15%\n",
      "Epoch 71: Train Acc=98.09%, Val Acc=98.11%, F1=98.08%, Macro F1=84.02%\n",
      "Epoch 72: Train Acc=98.12%, Val Acc=98.16%, F1=98.13%, Macro F1=83.98%\n",
      "Epoch 73: Train Acc=98.05%, Val Acc=98.07%, F1=98.04%, Macro F1=83.96%\n",
      "Epoch 74: Train Acc=98.08%, Val Acc=98.14%, F1=98.11%, Macro F1=84.07%\n",
      "Epoch 75: Train Acc=98.12%, Val Acc=98.16%, F1=98.13%, Macro F1=84.03%\n",
      "Epoch 76: Train Acc=98.11%, Val Acc=98.02%, F1=97.98%, Macro F1=83.85%\n",
      "Epoch 77: Train Acc=98.11%, Val Acc=98.08%, F1=98.04%, Macro F1=83.86%\n",
      "Epoch 78: Train Acc=98.18%, Val Acc=98.24%, F1=98.21%, Macro F1=83.97%\n",
      "Epoch 79: Train Acc=98.18%, Val Acc=98.24%, F1=98.21%, Macro F1=84.12%\n",
      "Epoch 80: Train Acc=98.20%, Val Acc=98.15%, F1=98.12%, Macro F1=83.96%\n",
      "Epoch 81: Train Acc=98.21%, Val Acc=98.05%, F1=98.02%, Macro F1=83.91%\n",
      "Epoch 82: Train Acc=98.18%, Val Acc=98.21%, F1=98.18%, Macro F1=84.00%\n",
      "Epoch 83: Train Acc=98.20%, Val Acc=98.23%, F1=98.20%, Macro F1=83.97%\n",
      "Epoch 84: Train Acc=98.22%, Val Acc=98.30%, F1=98.27%, Macro F1=84.15%\n",
      "Epoch 85: Train Acc=98.18%, Val Acc=98.08%, F1=98.04%, Macro F1=84.03%\n",
      "Epoch 86: Train Acc=98.24%, Val Acc=98.23%, F1=98.20%, Macro F1=84.13%\n",
      "Epoch 87: Train Acc=98.20%, Val Acc=98.28%, F1=98.24%, Macro F1=84.13%\n",
      "Epoch 88: Train Acc=98.24%, Val Acc=98.06%, F1=98.03%, Macro F1=83.60%\n",
      "Epoch 89: Train Acc=98.22%, Val Acc=98.26%, F1=98.23%, Macro F1=84.08%\n",
      "Epoch 90: Train Acc=98.25%, Val Acc=98.28%, F1=98.25%, Macro F1=84.19%\n",
      "Epoch 91: Train Acc=98.27%, Val Acc=98.20%, F1=98.17%, Macro F1=83.78%\n",
      "Epoch 92: Train Acc=98.25%, Val Acc=98.11%, F1=98.08%, Macro F1=84.02%\n",
      "Epoch 93: Train Acc=98.26%, Val Acc=98.18%, F1=98.15%, Macro F1=83.96%\n",
      "Epoch 94: Train Acc=98.26%, Val Acc=98.21%, F1=98.17%, Macro F1=84.01%\n",
      "Epoch 95: Train Acc=98.27%, Val Acc=98.24%, F1=98.21%, Macro F1=84.01%\n",
      "Epoch 96: Train Acc=98.27%, Val Acc=98.30%, F1=98.27%, Macro F1=84.14%\n",
      "Epoch 97: Train Acc=98.28%, Val Acc=98.25%, F1=98.22%, Macro F1=83.91%\n",
      "Epoch 98: Train Acc=98.33%, Val Acc=98.27%, F1=98.24%, Macro F1=84.13%\n",
      "Epoch 99: Train Acc=98.29%, Val Acc=98.22%, F1=98.19%, Macro F1=83.82%\n",
      "Epoch 100: Train Acc=98.34%, Val Acc=98.34%, F1=98.31%, Macro F1=84.10%\n",
      "Epoch 101: Train Acc=98.33%, Val Acc=98.07%, F1=98.04%, Macro F1=83.84%\n",
      "Epoch 102: Train Acc=98.26%, Val Acc=98.31%, F1=98.28%, Macro F1=84.04%\n",
      "Epoch 103: Train Acc=98.34%, Val Acc=98.38%, F1=98.35%, Macro F1=84.26%\n",
      "Epoch 104: Train Acc=98.32%, Val Acc=98.32%, F1=98.29%, Macro F1=84.11%\n",
      "Epoch 105: Train Acc=98.34%, Val Acc=98.39%, F1=98.36%, Macro F1=84.31%\n",
      "Epoch 106: Train Acc=98.32%, Val Acc=98.22%, F1=98.19%, Macro F1=83.90%\n",
      "Epoch 107: Train Acc=98.30%, Val Acc=98.23%, F1=98.20%, Macro F1=83.79%\n",
      "Epoch 108: Train Acc=98.32%, Val Acc=97.96%, F1=97.92%, Macro F1=83.48%\n",
      "Epoch 109: Train Acc=98.32%, Val Acc=98.18%, F1=98.15%, Macro F1=84.11%\n",
      "Epoch 110: Train Acc=98.33%, Val Acc=98.26%, F1=98.23%, Macro F1=84.14%\n",
      "Epoch 111: Train Acc=98.32%, Val Acc=98.32%, F1=98.29%, Macro F1=84.19%\n",
      "Epoch 112: Train Acc=98.35%, Val Acc=98.35%, F1=98.32%, Macro F1=84.07%\n",
      "Epoch 113: Train Acc=98.34%, Val Acc=98.36%, F1=98.33%, Macro F1=84.15%\n",
      "Epoch 114: Train Acc=98.34%, Val Acc=97.91%, F1=97.85%, Macro F1=82.45%\n",
      "Epoch 115: Train Acc=98.36%, Val Acc=98.30%, F1=98.27%, Macro F1=84.05%\n",
      "Epoch 116: Train Acc=98.35%, Val Acc=98.26%, F1=98.23%, Macro F1=84.01%\n",
      "Epoch 117: Train Acc=98.36%, Val Acc=98.30%, F1=98.27%, Macro F1=84.04%\n",
      "Epoch 118: Train Acc=98.35%, Val Acc=98.02%, F1=97.99%, Macro F1=84.05%\n",
      "Epoch 119: Train Acc=98.36%, Val Acc=98.16%, F1=98.13%, Macro F1=84.15%\n",
      "Epoch 120: Train Acc=98.33%, Val Acc=98.34%, F1=98.30%, Macro F1=84.23%\n",
      "Epoch 121: Train Acc=98.36%, Val Acc=98.29%, F1=98.26%, Macro F1=84.08%\n",
      "Epoch 122: Train Acc=98.35%, Val Acc=97.91%, F1=97.88%, Macro F1=83.99%\n",
      "Epoch 123: Train Acc=98.34%, Val Acc=98.36%, F1=98.33%, Macro F1=84.27%\n",
      "Epoch 124: Train Acc=98.35%, Val Acc=98.34%, F1=98.30%, Macro F1=84.23%\n",
      "Epoch 125: Train Acc=98.36%, Val Acc=98.23%, F1=98.20%, Macro F1=83.99%\n",
      "Epoch 126: Train Acc=98.38%, Val Acc=98.28%, F1=98.25%, Macro F1=84.15%\n",
      "Epoch 127: Train Acc=98.35%, Val Acc=98.32%, F1=98.28%, Macro F1=83.83%\n",
      "Epoch 128: Train Acc=98.33%, Val Acc=98.23%, F1=98.20%, Macro F1=84.02%\n",
      "Epoch 129: Train Acc=98.34%, Val Acc=98.34%, F1=98.31%, Macro F1=84.09%\n",
      "Epoch 130: Train Acc=98.40%, Val Acc=98.33%, F1=98.30%, Macro F1=84.15%\n",
      "Epoch 131: Train Acc=98.35%, Val Acc=98.37%, F1=98.34%, Macro F1=84.26%\n",
      "Epoch 132: Train Acc=98.38%, Val Acc=98.38%, F1=98.35%, Macro F1=84.35%\n",
      "Epoch 133: Train Acc=98.38%, Val Acc=98.32%, F1=98.28%, Macro F1=84.13%\n",
      "Epoch 134: Train Acc=98.39%, Val Acc=98.13%, F1=98.10%, Macro F1=83.95%\n",
      "Epoch 135: Train Acc=98.39%, Val Acc=98.14%, F1=98.11%, Macro F1=84.00%\n",
      "Epoch 136: Train Acc=98.37%, Val Acc=98.30%, F1=98.26%, Macro F1=84.09%\n",
      "Epoch 137: Train Acc=98.41%, Val Acc=98.37%, F1=98.34%, Macro F1=84.25%\n",
      "Epoch 138: Train Acc=98.37%, Val Acc=98.39%, F1=98.36%, Macro F1=84.15%\n",
      "Epoch 139: Train Acc=98.37%, Val Acc=98.30%, F1=98.26%, Macro F1=84.12%\n",
      "Epoch 140: Train Acc=98.39%, Val Acc=98.29%, F1=98.26%, Macro F1=84.17%\n",
      "Epoch 141: Train Acc=98.41%, Val Acc=98.31%, F1=98.28%, Macro F1=84.02%\n",
      "Epoch 142: Train Acc=98.40%, Val Acc=98.31%, F1=98.28%, Macro F1=84.11%\n",
      "Epoch 143: Train Acc=98.40%, Val Acc=98.30%, F1=98.27%, Macro F1=84.17%\n",
      "Epoch 144: Train Acc=98.40%, Val Acc=98.34%, F1=98.30%, Macro F1=84.20%\n",
      "Epoch 145: Train Acc=98.41%, Val Acc=98.00%, F1=97.96%, Macro F1=83.39%\n",
      "Epoch 146: Train Acc=98.37%, Val Acc=98.40%, F1=98.37%, Macro F1=84.30%\n",
      "Epoch 147: Train Acc=98.41%, Val Acc=98.19%, F1=98.15%, Macro F1=84.10%\n",
      "Epoch 148: Train Acc=98.37%, Val Acc=98.42%, F1=98.39%, Macro F1=84.26%\n",
      "Epoch 149: Train Acc=98.43%, Val Acc=98.28%, F1=98.24%, Macro F1=84.09%\n",
      "Epoch 150: Train Acc=98.41%, Val Acc=98.22%, F1=98.19%, Macro F1=83.95%\n",
      "Epoch 151: Train Acc=98.35%, Val Acc=98.38%, F1=98.35%, Macro F1=84.32%\n",
      "Epoch 152: Train Acc=98.40%, Val Acc=98.35%, F1=98.32%, Macro F1=84.10%\n",
      "Epoch 153: Train Acc=98.42%, Val Acc=98.38%, F1=98.35%, Macro F1=84.20%\n",
      "Epoch 154: Train Acc=98.39%, Val Acc=98.20%, F1=98.17%, Macro F1=84.02%\n",
      "Epoch 155: Train Acc=98.37%, Val Acc=98.21%, F1=98.17%, Macro F1=84.07%\n",
      "Epoch 156: Train Acc=98.44%, Val Acc=98.38%, F1=98.35%, Macro F1=84.29%\n",
      "Epoch 157: Train Acc=98.39%, Val Acc=98.29%, F1=98.26%, Macro F1=84.01%\n",
      "Epoch 158: Train Acc=98.44%, Val Acc=98.26%, F1=98.22%, Macro F1=84.06%\n",
      "Epoch 159: Train Acc=98.41%, Val Acc=98.41%, F1=98.37%, Macro F1=84.25%\n",
      "Epoch 160: Train Acc=98.41%, Val Acc=98.14%, F1=98.10%, Macro F1=83.44%\n",
      "Epoch 161: Train Acc=98.45%, Val Acc=98.19%, F1=98.16%, Macro F1=84.04%\n",
      "Epoch 162: Train Acc=98.41%, Val Acc=98.30%, F1=98.27%, Macro F1=84.26%\n",
      "Epoch 163: Train Acc=98.42%, Val Acc=98.34%, F1=98.31%, Macro F1=84.22%\n",
      "Epoch 164: Train Acc=98.41%, Val Acc=98.34%, F1=98.31%, Macro F1=84.13%\n",
      "Epoch 165: Train Acc=98.41%, Val Acc=98.29%, F1=98.26%, Macro F1=84.04%\n",
      "Epoch 166: Train Acc=98.41%, Val Acc=98.37%, F1=98.34%, Macro F1=84.32%\n",
      "Epoch 167: Train Acc=98.38%, Val Acc=98.41%, F1=98.37%, Macro F1=84.19%\n",
      "Epoch 168: Train Acc=98.43%, Val Acc=98.29%, F1=98.26%, Macro F1=83.94%\n",
      "Epoch 169: Train Acc=98.41%, Val Acc=98.40%, F1=98.37%, Macro F1=84.24%\n",
      "Epoch 170: Train Acc=98.43%, Val Acc=98.39%, F1=98.36%, Macro F1=84.16%\n",
      "Epoch 171: Train Acc=98.42%, Val Acc=98.40%, F1=98.37%, Macro F1=84.26%\n",
      "Epoch 172: Train Acc=98.40%, Val Acc=98.41%, F1=98.37%, Macro F1=84.24%\n",
      "Epoch 173: Train Acc=98.43%, Val Acc=98.34%, F1=98.31%, Macro F1=84.21%\n",
      "Epoch 174: Train Acc=98.44%, Val Acc=98.40%, F1=98.37%, Macro F1=84.22%\n",
      "Epoch 175: Train Acc=98.44%, Val Acc=98.33%, F1=98.30%, Macro F1=84.11%\n",
      "Epoch 176: Train Acc=98.41%, Val Acc=98.28%, F1=98.24%, Macro F1=84.14%\n",
      "Epoch 177: Train Acc=98.44%, Val Acc=98.22%, F1=98.19%, Macro F1=84.08%\n",
      "Epoch 178: Train Acc=98.43%, Val Acc=98.36%, F1=98.32%, Macro F1=83.99%\n",
      "Epoch 179: Train Acc=98.44%, Val Acc=98.38%, F1=98.35%, Macro F1=84.28%\n",
      "Epoch 180: Train Acc=98.46%, Val Acc=98.39%, F1=98.36%, Macro F1=84.11%\n",
      "Epoch 181: Train Acc=98.45%, Val Acc=98.20%, F1=98.17%, Macro F1=84.06%\n",
      "Epoch 182: Train Acc=98.41%, Val Acc=98.26%, F1=98.22%, Macro F1=84.19%\n",
      "Epoch 183: Train Acc=98.44%, Val Acc=98.30%, F1=98.27%, Macro F1=84.20%\n",
      "Epoch 184: Train Acc=98.44%, Val Acc=98.33%, F1=98.30%, Macro F1=84.02%\n",
      "Epoch 185: Train Acc=98.47%, Val Acc=98.34%, F1=98.30%, Macro F1=83.96%\n",
      "Epoch 186: Train Acc=98.43%, Val Acc=98.35%, F1=98.32%, Macro F1=84.04%\n",
      "Epoch 187: Train Acc=98.47%, Val Acc=98.35%, F1=98.32%, Macro F1=84.21%\n",
      "Epoch 188: Train Acc=98.47%, Val Acc=98.23%, F1=98.19%, Macro F1=84.04%\n",
      "Epoch 189: Train Acc=98.44%, Val Acc=98.39%, F1=98.36%, Macro F1=84.22%\n",
      "Epoch 190: Train Acc=98.42%, Val Acc=98.32%, F1=98.29%, Macro F1=84.15%\n",
      "Epoch 191: Train Acc=98.46%, Val Acc=98.40%, F1=98.37%, Macro F1=84.12%\n",
      "Epoch 192: Train Acc=98.43%, Val Acc=98.30%, F1=98.27%, Macro F1=84.08%\n",
      "Epoch 193: Train Acc=98.44%, Val Acc=98.41%, F1=98.38%, Macro F1=84.23%\n",
      "Epoch 194: Train Acc=98.46%, Val Acc=98.41%, F1=98.37%, Macro F1=84.18%\n",
      "Epoch 195: Train Acc=98.44%, Val Acc=98.42%, F1=98.39%, Macro F1=84.27%\n",
      "Epoch 196: Train Acc=98.46%, Val Acc=98.47%, F1=98.43%, Macro F1=84.30%\n",
      "Epoch 197: Train Acc=98.42%, Val Acc=98.44%, F1=98.41%, Macro F1=84.22%\n",
      "Epoch 198: Train Acc=98.49%, Val Acc=98.43%, F1=98.39%, Macro F1=84.27%\n",
      "Epoch 199: Train Acc=98.47%, Val Acc=98.37%, F1=98.34%, Macro F1=84.29%\n",
      "Epoch 200: Train Acc=98.46%, Val Acc=98.38%, F1=98.35%, Macro F1=84.12%\n",
      "\n",
      "Cross-Validation Results:\n",
      "Avg Validation Accuracy: 98.30%\n",
      "Avg Weighted F1-score: 98.29%\n",
      "Avg Macro F1-score: 86.73%\n",
      "Avg Validation Loss: 0.051637\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "epochs = 200  # Number of training epochs per fold\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define loss function and learning parameters\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Combine datasets (Only Train Data)\n",
    "dataset = train_dataset  # Use only training dataset\n",
    "\n",
    "# Store results across folds\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_f1_macro_scores = []\n",
    "fold_losses = []\n",
    "\n",
    "# K-Fold cross-validation loop\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\\n\" + \"-\" * 30)\n",
    "    print(\"initializing model...\")\n",
    "    model = KAN([50, 20,10, 5,7],grid_size = 6, spline_order = 3, scale_noise=0.2, scale_base=0.2, scale_spline=0.2) #best\n",
    "    # Create data subsets for the current fold\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Re-initialize model and optimizer for each fold\n",
    "     # Replace with your model class\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training function\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        return correct / total * 100, epoch_loss / len(dataloader)\n",
    "\n",
    "    # Validation function\n",
    "    def val_loop(dataloader, model, loss_fn):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "                all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracy = correct / len(dataloader.dataset) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted') * 100\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "        mean_loss = val_loss / len(dataloader)\n",
    "\n",
    "        return accuracy, f1, f1_macro, mean_loss\n",
    "\n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_acc, f1, f1_macro, val_loss = val_loop(val_dataloader, model, loss_fn)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%, F1={f1:.2f}%, Macro F1={f1_macro:.2f}%\")\n",
    "\n",
    "    # Store fold results\n",
    "    fold_accuracies.append(val_acc)\n",
    "    fold_f1_scores.append(f1)\n",
    "    fold_f1_macro_scores.append(f1_macro)\n",
    "    fold_losses.append(val_loss)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Avg Validation Accuracy: {sum(fold_accuracies) / k_folds:.2f}%\")\n",
    "print(f\"Avg Weighted F1-score: {sum(fold_f1_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Macro F1-score: {sum(fold_f1_macro_scores) / k_folds:.2f}%\")\n",
    "print(f\"Avg Validation Loss: {sum(fold_losses) / k_folds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398cd152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Protocol  Flow Duration  Total Fwd Packets  Total Backward Packets  Total Length of Fwd Packets  Total Length of Bwd Packets  Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  Bwd Packet Length Max  Bwd Packet Length Min  Bwd Packet Length Mean  Bwd Packet Length Std  Flow Bytess  Flow Packetss  Flow IAT Mean  Flow IAT Std  Flow IAT Max  Flow IAT Min  Fwd IAT Total  Fwd IAT Mean  Fwd IAT Std  Fwd IAT Max  Fwd IAT Min  Bwd IAT Total  Bwd IAT Mean  Bwd IAT Std  Bwd IAT Max  Bwd IAT Min  Fwd PSH Flags  Bwd PSH Flags  Fwd URG Flags  Bwd URG Flags  Fwd Header Length  Bwd Header Length  Fwd Packetss  Bwd Packetss  Min Packet Length  Max Packet Length  Packet Length Mean  Packet Length Std  Packet Length Variance  FIN Flag Count  SYN Flag Count  RST Flag Count  PSH Flag Count  ACK Flag Count  URG Flag Count  CWE Flag Count  ECE Flag Count  Down Up Ratio  Average Packet Size  Avg Fwd Segment Size  Avg Bwd Segment Size  Fwd Avg Bytes Bulk  Fwd Avg Packets Bulk  Fwd Avg Bulk Rate  Bwd Avg Bytes Bulk  Bwd Avg Packets Bulk  Bwd Avg Bulk Rate  Subflow Fwd Packets  Subflow Fwd Bytes  Subflow Bwd Packets  Subflow Bwd Bytes  Init_Win_bytes_forward  Init_Win_bytes_backward  act_data_pkt_fwd  min_seg_size_forward  Active Mean  Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min\n",
       "0.485467  0.409628       0.000000           0.463685                0.465106                     0.474739                     0.466135               0.558999               0.466776                0.000000               0.476358               0.557261               0.477352                0.000000               0.592266     0.588512       0.412587       0.000000      0.410003      0.522400      0.000000       0.000000      0.000000     0.000000     0.000000     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.380032           0.451307           0.585964      0.619705      0.563132           0.456984           0.467926            0.446019           0.446019                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.54076        0.469707             0.466776              0.477352              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.000000             0.465106           0.463685             0.474739           0.516488                0.479085                 0.000000          0.466904              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000    869\n",
       "          0.417053       0.000000           0.463685                0.465106                     0.474739                     0.466135               0.558999               0.466776                0.000000               0.476358               0.557261               0.477352                0.000000               0.588145     0.581211       0.419819       0.000000      0.417402      0.528020      0.000000       0.000000      0.000000     0.000000     0.000000     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.380032           0.451307           0.578823      0.609637      0.563132           0.456984           0.467926            0.446019           0.446019                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.54076        0.469707             0.466776              0.477352              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.000000             0.465106           0.463685             0.474739           0.516488                0.479085                 0.000000          0.466904              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000    771\n",
       "          0.409628       0.000000           0.463685                0.000000                     0.474739                     0.000000               0.000000               0.000000                0.000000               0.476358               0.557261               0.477352                0.000000               0.565546     0.588512       0.412587       0.000000      0.410003      0.522400      0.000000       0.000000      0.000000     0.000000     0.000000     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.457783           0.451307           0.585964      0.619705      0.000000           0.456984           0.426784            0.485467           0.485467                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.54076        0.428067             0.000000              0.477352              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.000000             0.000000           0.463685             0.474739           0.599402                0.479085                 0.000000          0.638383              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000    757\n",
       "          0.417053       0.000000           0.463685                0.000000                     0.474739                     0.000000               0.000000               0.000000                0.000000               0.476358               0.557261               0.477352                0.000000               0.561939     0.581211       0.419819       0.000000      0.417402      0.528020      0.000000       0.000000      0.000000     0.000000     0.000000     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.457783           0.451307           0.578823      0.609637      0.000000           0.456984           0.426784            0.485467           0.485467                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.54076        0.428067             0.000000              0.477352              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.000000             0.000000           0.463685             0.474739           0.599402                0.479085                 0.000000          0.638383              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000    747\n",
       "          0.325799       0.495172           0.000000                0.000000                     0.000000                     0.000000               0.000000               0.000000                0.000000               0.000000               0.000000               0.000000                0.000000               0.000000     0.672366       0.330560       0.000000      0.327030      0.443744      0.484001       0.484857      0.000000     0.484246     0.522028     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.505190           0.000000           0.674201      0.000000      0.000000           0.000000           0.000000            0.000000           0.000000                0.0             0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.00000        0.000000             0.000000              0.000000              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.495172             0.000000           0.000000             0.000000           0.448404                0.000000                 0.000000          0.535927              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000    638\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ... \n",
       "          0.551964       0.531055           0.463685                0.000000                     0.000000                     0.000000               0.000000               0.000000                0.000000               0.000000               0.000000               0.000000                0.000000               0.000000     0.422982       0.577837       0.577049      0.571332      0.560168      0.554600       0.579125      0.580400     0.572096     0.681170     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.545160           0.518205           0.423652      0.456679      0.000000           0.000000           0.000000            0.000000           0.000000                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.00000        0.000000             0.000000              0.000000              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.531055             0.000000           0.463685             0.000000           0.599402                0.763458                 0.000000          0.535927              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000      1\n",
       "                         0.622913           0.626838                0.612936                     0.572496                     0.567230               0.000000               0.557726                0.565323               0.573899               0.000000               0.573801                0.574093               0.485518     0.449411       0.550205       0.564043      0.570010      0.375150      0.554601       0.554638      0.569208     0.570738     0.522028     0.558252       0.552739      0.558608     0.555857     0.519681     0.0            0.0            0.0            0.0            0.576483           0.572093           0.450117      0.473056      0.000000           0.571969           0.570532            0.571307           0.571307                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.54076        0.570108             0.557726              0.573801              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.622913             0.612936           0.626838             0.572496           0.544085                0.519435                 0.661000          0.399563              0.681797     0.0         0.669475    0.695963    0.575606   0.0       0.575606  0.576317      1\n",
       "          0.551967       0.622913           0.608489                0.609637                     0.577653                     0.564334               0.000000               0.556829                0.565017               0.576649               0.000000               0.579500                0.578265               0.488005     0.448303       0.551902       0.564594      0.569825      0.443744      0.554605       0.554640      0.569182     0.570606     0.522028     0.557558       0.554168      0.558427     0.553466     0.617894     0.0            0.0            0.0            0.0            0.576483           0.562833           0.450115      0.468914      0.000000           0.574350           0.575846            0.575657           0.575657                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.00000        0.575805             0.556829              0.579500              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.622913             0.609637           0.608489             0.577653           0.544085                0.520377                 0.661000          0.399563              0.682908     0.0         0.670277    0.696729    0.575573   0.0       0.575573  0.576254      1\n",
       "          0.551970       0.531055           0.463685                0.000000                     0.000000                     0.000000               0.000000               0.000000                0.000000               0.000000               0.000000               0.000000                0.000000               0.000000     0.422977       0.577842       0.577481      0.571639      0.607132      0.554610       0.579132      0.580884     0.572451     0.638383     0.000000       0.000000      0.000000     0.000000     0.000000     0.0            0.0            0.0            0.0            0.545160           0.518205           0.423647      0.456673      0.000000           0.000000           0.000000            0.000000           0.000000                0.0             0.0             0.0             1.0             0.0             0.0             0.0             0.0             0.00000        0.000000             0.000000              0.000000              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.531055             0.000000           0.463685             0.000000           0.599402                0.691849                 0.000000          0.535927              0.000000     0.0         0.000000    0.000000    0.000000   0.0       0.000000  0.000000      1\n",
       "1.000000  0.915178       0.545970           0.549932                0.562088                     0.560741                     0.551318               0.619968               0.548146                0.535148               0.552499               0.668312               0.560315                0.555708               0.410154     0.273162       0.723562       0.748554      1.000000      0.443744      1.000000       0.736668      0.809029     1.000000     0.544085     1.000000       0.726606      0.789891     1.000000     0.537484     0.0            0.0            0.0            0.0            0.530674           0.550623           0.257578      0.400555      0.620495           0.546354           0.550833            0.545648           0.545648                0.0             0.0             0.0             0.0             0.0             0.0             0.0             0.0             0.54076        0.551649             0.548146              0.560315              0.0                 0.0                   0.0                0.0                 0.0                   0.0                0.545970             0.562088           0.549932             0.560741           0.000000                0.000000                 0.590372          0.399563              0.664887     0.0         0.660883    0.678964    1.000000   0.0       1.000000  1.000000      1\n",
       "Name: count, Length: 46221, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92990a-790c-4d0d-b1fe-550199e0b99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046386, F1-score: 98.31%, Macro_F1-Score:  84.10%  \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046656, F1-score: 98.30%, Macro_F1-Score:  84.06%  \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047827, F1-score: 98.33%, Macro_F1-Score:  84.06%  \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046496, F1-score: 98.36%, Macro_F1-Score:  84.02%  \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.048037, F1-score: 98.30%, Macro_F1-Score:  83.96%  \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.048116, F1-score: 98.29%, Macro_F1-Score:  84.07%  \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045545, F1-score: 98.36%, Macro_F1-Score:  84.22%  \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045891, F1-score: 98.36%, Macro_F1-Score:  84.04%  \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.051296, F1-score: 98.08%, Macro_F1-Score:  83.87%  \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045729, F1-score: 98.33%, Macro_F1-Score:  84.19%  \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048172, F1-score: 98.19%, Macro_F1-Score:  84.09%  \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046832, F1-score: 98.38%, Macro_F1-Score:  84.26%  \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046909, F1-score: 98.34%, Macro_F1-Score:  84.20%  \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046519, F1-score: 98.35%, Macro_F1-Score:  84.15%  \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047847, F1-score: 98.32%, Macro_F1-Score:  84.06%  \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.048830, F1-score: 98.32%, Macro_F1-Score:  83.96%  \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.054064, F1-score: 98.18%, Macro_F1-Score:  83.83%  \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.050455, F1-score: 98.21%, Macro_F1-Score:  84.15%  \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.049599, F1-score: 98.24%, Macro_F1-Score:  84.17%  \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.052317, F1-score: 98.20%, Macro_F1-Score:  84.02%  \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.049678, F1-score: 98.24%, Macro_F1-Score:  84.16%  \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.049125, F1-score: 98.22%, Macro_F1-Score:  84.15%  \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048606, F1-score: 98.23%, Macro_F1-Score:  84.12%  \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.049598, F1-score: 98.21%, Macro_F1-Score:  84.01%  \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.050046, F1-score: 98.29%, Macro_F1-Score:  84.04%  \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047776, F1-score: 98.36%, Macro_F1-Score:  84.21%  \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047884, F1-score: 98.20%, Macro_F1-Score:  84.02%  \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.051371, F1-score: 98.12%, Macro_F1-Score:  83.98%  \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046836, F1-score: 98.35%, Macro_F1-Score:  84.23%  \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049126, F1-score: 98.38%, Macro_F1-Score:  90.02%  \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047020, F1-score: 98.35%, Macro_F1-Score:  84.27%  \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047874, F1-score: 98.29%, Macro_F1-Score:  84.05%  \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047652, F1-score: 98.25%, Macro_F1-Score:  84.12%  \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046987, F1-score: 98.33%, Macro_F1-Score:  84.11%  \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047154, F1-score: 98.38%, Macro_F1-Score:  84.24%  \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046823, F1-score: 98.34%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.053450, F1-score: 98.13%, Macro_F1-Score:  89.47%  \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046666, F1-score: 98.41%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048195, F1-score: 98.39%, Macro_F1-Score:  84.18%  \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.051064, F1-score: 98.33%, Macro_F1-Score:  84.05%  \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046891, F1-score: 98.33%, Macro_F1-Score:  84.05%  \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.049956, F1-score: 98.27%, Macro_F1-Score:  84.13%  \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046200, F1-score: 98.32%, Macro_F1-Score:  89.75%  \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046640, F1-score: 98.31%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047635, F1-score: 98.17%, Macro_F1-Score:  89.74%  \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046380, F1-score: 98.30%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046963, F1-score: 98.36%, Macro_F1-Score:  90.02%  \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045881, F1-score: 98.28%, Macro_F1-Score:  89.88%  \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045039, F1-score: 98.43%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046386, F1-score: 98.43%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046699, F1-score: 98.29%, Macro_F1-Score:  84.11%  \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045941, F1-score: 98.35%, Macro_F1-Score:  84.05%  \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.052345, F1-score: 98.13%, Macro_F1-Score:  84.01%  \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045116, F1-score: 98.38%, Macro_F1-Score:  84.10%  \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045839, F1-score: 98.37%, Macro_F1-Score:  84.23%  \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.048442, F1-score: 98.25%, Macro_F1-Score:  84.25%  \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045512, F1-score: 98.37%, Macro_F1-Score:  89.85%  \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047696, F1-score: 98.22%, Macro_F1-Score:  89.74%  \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046136, F1-score: 98.29%, Macro_F1-Score:  84.23%  \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.048225, F1-score: 98.28%, Macro_F1-Score:  89.78%  \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048851, F1-score: 98.35%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046949, F1-score: 98.43%, Macro_F1-Score:  90.06%  \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048383, F1-score: 98.24%, Macro_F1-Score:  89.63%  \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047292, F1-score: 98.36%, Macro_F1-Score:  89.87%  \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.047035, F1-score: 98.39%, Macro_F1-Score:  84.05%  \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047850, F1-score: 98.20%, Macro_F1-Score:  89.72%  \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047353, F1-score: 98.25%, Macro_F1-Score:  89.88%  \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046929, F1-score: 98.27%, Macro_F1-Score:  89.69%  \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.047435, F1-score: 98.18%, Macro_F1-Score:  84.00%  \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045177, F1-score: 98.43%, Macro_F1-Score:  89.99%  \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045178, F1-score: 98.42%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046794, F1-score: 98.36%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046411, F1-score: 98.31%, Macro_F1-Score:  84.13%  \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046608, F1-score: 98.41%, Macro_F1-Score:  84.18%  \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.051239, F1-score: 98.25%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044505, F1-score: 98.40%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.046638, F1-score: 98.15%, Macro_F1-Score:  84.02%  \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046971, F1-score: 98.30%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044574, F1-score: 98.43%, Macro_F1-Score:  89.99%  \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047282, F1-score: 98.27%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044092, F1-score: 98.41%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044338, F1-score: 98.41%, Macro_F1-Score:  84.15%  \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046467, F1-score: 98.38%, Macro_F1-Score:  89.80%  \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045619, F1-score: 98.42%, Macro_F1-Score:  84.15%  \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.045788, F1-score: 98.23%, Macro_F1-Score:  84.04%  \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046491, F1-score: 98.39%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045786, F1-score: 98.36%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047292, F1-score: 98.33%, Macro_F1-Score:  89.63%  \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045859, F1-score: 98.39%, Macro_F1-Score:  89.88%  \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045019, F1-score: 98.37%, Macro_F1-Score:  89.89%  \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044515, F1-score: 98.38%, Macro_F1-Score:  90.02%  \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.052531, F1-score: 98.08%, Macro_F1-Score:  83.73%  \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044203, F1-score: 98.45%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.053810, F1-score: 98.12%, Macro_F1-Score:  89.62%  \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046398, F1-score: 98.42%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044553, F1-score: 98.42%, Macro_F1-Score:  89.87%  \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045607, F1-score: 98.33%, Macro_F1-Score:  89.75%  \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044907, F1-score: 98.41%, Macro_F1-Score:  89.99%  \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044872, F1-score: 98.37%, Macro_F1-Score:  89.98%  \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.053906, F1-score: 98.08%, Macro_F1-Score:  89.67%  \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045687, F1-score: 98.40%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046493, F1-score: 98.25%, Macro_F1-Score:  89.88%  \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046313, F1-score: 98.36%, Macro_F1-Score:  89.75%  \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045491, F1-score: 98.40%, Macro_F1-Score:  84.21%  \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046343, F1-score: 98.32%, Macro_F1-Score:  84.08%  \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046063, F1-score: 98.37%, Macro_F1-Score:  89.76%  \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046160, F1-score: 98.36%, Macro_F1-Score:  89.80%  \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045262, F1-score: 98.42%, Macro_F1-Score:  89.88%  \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047163, F1-score: 98.25%, Macro_F1-Score:  89.73%  \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045194, F1-score: 98.42%, Macro_F1-Score:  90.01%  \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043503, F1-score: 98.45%, Macro_F1-Score:  90.02%  \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044892, F1-score: 98.36%, Macro_F1-Score:  84.17%  \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046208, F1-score: 98.36%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044594, F1-score: 98.37%, Macro_F1-Score:  84.11%  \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044697, F1-score: 98.41%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043611, F1-score: 98.36%, Macro_F1-Score:  84.14%  \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044512, F1-score: 98.41%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045414, F1-score: 98.41%, Macro_F1-Score:  89.89%  \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043536, F1-score: 98.44%, Macro_F1-Score:  84.23%  \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048887, F1-score: 98.24%, Macro_F1-Score:  83.88%  \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043228, F1-score: 98.46%, Macro_F1-Score:  90.01%  \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047486, F1-score: 98.31%, Macro_F1-Score:  84.26%  \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045440, F1-score: 98.39%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045142, F1-score: 98.41%, Macro_F1-Score:  89.77%  \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046787, F1-score: 98.38%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044238, F1-score: 98.45%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.051344, F1-score: 98.14%, Macro_F1-Score:  89.78%  \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044954, F1-score: 98.39%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.047054, F1-score: 98.30%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045383, F1-score: 98.34%, Macro_F1-Score:  89.70%  \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045296, F1-score: 98.35%, Macro_F1-Score:  89.83%  \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045430, F1-score: 98.34%, Macro_F1-Score:  89.65%  \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046172, F1-score: 98.34%, Macro_F1-Score:  89.79%  \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.044953, F1-score: 98.31%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042651, F1-score: 98.45%, Macro_F1-Score:  84.30%  \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046648, F1-score: 98.35%, Macro_F1-Score:  89.73%  \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043771, F1-score: 98.41%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045431, F1-score: 98.40%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.070804, F1-score: 97.47%, Macro_F1-Score:  86.51%  \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044406, F1-score: 98.36%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045570, F1-score: 98.40%, Macro_F1-Score:  84.06%  \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045000, F1-score: 98.30%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046008, F1-score: 98.35%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045079, F1-score: 98.34%, Macro_F1-Score:  84.14%  \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043061, F1-score: 98.36%, Macro_F1-Score:  89.99%  \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048208, F1-score: 98.35%, Macro_F1-Score:  89.76%  \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048670, F1-score: 98.38%, Macro_F1-Score:  89.87%  \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045740, F1-score: 98.29%, Macro_F1-Score:  89.96%  \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.043721, F1-score: 98.33%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045780, F1-score: 98.32%, Macro_F1-Score:  83.90%  \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.045113, F1-score: 98.47%, Macro_F1-Score:  89.97%  \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044824, F1-score: 98.39%, Macro_F1-Score:  89.85%  \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044180, F1-score: 98.42%, Macro_F1-Score:  84.20%  \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042713, F1-score: 98.48%, Macro_F1-Score:  90.00%  \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045305, F1-score: 98.43%, Macro_F1-Score:  89.85%  \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049016, F1-score: 98.36%, Macro_F1-Score:  89.72%  \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043781, F1-score: 98.43%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044295, F1-score: 98.45%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.046365, F1-score: 98.24%, Macro_F1-Score:  89.41%  \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044953, F1-score: 98.39%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044364, F1-score: 98.35%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044560, F1-score: 98.39%, Macro_F1-Score:  84.19%  \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043302, F1-score: 98.47%, Macro_F1-Score:  89.98%  \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.044191, F1-score: 98.20%, Macro_F1-Score:  89.76%  \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.044564, F1-score: 98.34%, Macro_F1-Score:  89.93%  \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043573, F1-score: 98.48%, Macro_F1-Score:  90.07%  \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045243, F1-score: 98.32%, Macro_F1-Score:  89.96%  \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044492, F1-score: 98.39%, Macro_F1-Score:  84.25%  \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044832, F1-score: 98.40%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.044464, F1-score: 98.30%, Macro_F1-Score:  83.99%  \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045330, F1-score: 98.40%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044302, F1-score: 98.49%, Macro_F1-Score:  90.08%  \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043628, F1-score: 98.42%, Macro_F1-Score:  89.92%  \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042857, F1-score: 98.47%, Macro_F1-Score:  90.03%  \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.045044, F1-score: 98.29%, Macro_F1-Score:  89.82%  \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046412, F1-score: 98.35%, Macro_F1-Score:  83.94%  \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046001, F1-score: 98.43%, Macro_F1-Score:  89.91%  \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044625, F1-score: 98.39%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043654, F1-score: 98.36%, Macro_F1-Score:  89.86%  \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043392, F1-score: 98.45%, Macro_F1-Score:  89.89%  \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045428, F1-score: 98.37%, Macro_F1-Score:  89.58%  \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042565, F1-score: 98.48%, Macro_F1-Score:  90.08%  \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043334, F1-score: 98.46%, Macro_F1-Score:  90.01%  \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043233, F1-score: 98.46%, Macro_F1-Score:  90.01%  \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043566, F1-score: 98.39%, Macro_F1-Score:  89.96%  \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044086, F1-score: 98.42%, Macro_F1-Score:  90.06%  \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043181, F1-score: 98.49%, Macro_F1-Score:  84.37%  \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045383, F1-score: 98.40%, Macro_F1-Score:  89.79%  \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043137, F1-score: 98.43%, Macro_F1-Score:  90.00%  \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043290, F1-score: 98.47%, Macro_F1-Score:  89.89%  \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.044817, F1-score: 98.42%, Macro_F1-Score:  90.04%  \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043339, F1-score: 98.42%, Macro_F1-Score:  89.90%  \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042744, F1-score: 98.48%, Macro_F1-Score:  90.07%  \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043466, F1-score: 98.49%, Macro_F1-Score:  90.10%  \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.043428, F1-score: 98.44%, Macro_F1-Score:  89.94%  \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.046594, F1-score: 98.35%, Macro_F1-Score:  89.61%  \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043696, F1-score: 98.51%, Macro_F1-Score:  90.08%  \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.045495, F1-score: 98.40%, Macro_F1-Score:  89.97%  \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.043826, F1-score: 98.49%, Macro_F1-Score:  90.01%  \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.042605, F1-score: 98.49%, Macro_F1-Score:  90.13%  \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = FocalLoss(alpha=0.5, gamma = 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4) #using L2 regularization\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") #wider network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add252-99f2-4d32-bb41-6e558870ac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noise scale is low (0.2)\n",
    "plt.plot(loss_over_train, label=\"Training Loss\")\n",
    "plt.plot(loss_over_test, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46642c-d070-4eed-bfb4-3c5a7d21e93c",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605965a8-0cd9-434c-aee0-478a4531365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the LabelEncoder object from the file\n",
    "with open(r\"C:\\Users\\TESTER\\OneDrive\\Ambiente de Trabalho\\Thesis Folder\\Data After Preprocess\\label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "\n",
    "# Now you can use the label_encoder object in the second notebook\n",
    "original_class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d9c-dcf8-497e-931d-9ad812150687",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels_test = label_encoder.inverse_transform(test_data_y.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517902-9ea6-4c23-8654-e295a0f3c84d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (7), usually from a call to set_ticks, does not match the number of labels (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))  \u001b[38;5;66;03m#Creates both a figure and an ax\u001b[39;00m\n\u001b[0;32m     24\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39mclass_labels)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mdisp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Rotate axis labels for better visibility\u001b[39;00m\n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py:185\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.plot\u001b[1;34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m colorbar:\n\u001b[0;32m    184\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcolorbar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_, ax\u001b[38;5;241m=\u001b[39max)\n\u001b[1;32m--> 185\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredicted label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim((n_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m    195\u001b[0m plt\u001b[38;5;241m.\u001b[39msetp(ax\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39mxticks_rotation)\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:146\u001b[0m, in \u001b[0;36mArtist.__init_subclass__.<locals>.<lambda>\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_autogenerated_signature\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mArtist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mset\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:1241\u001b[0m, in \u001b[0;36mArtist.set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:1233\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \n\u001b[0;32m   1231\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\artist.py:1209\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[1;34m(self, props, errfmt)\u001b[0m\n\u001b[0;32m   1205\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m   1206\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1207\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk),\n\u001b[0;32m   1208\u001b[0m                     name\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m-> 1209\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpchanged()\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_base.py:74\u001b[0m, in \u001b[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axis.py:2117\u001b[0m, in \u001b[0;36mAxis.set_ticklabels\u001b[1;34m(self, labels, minor, fontdict, **kwargs)\u001b[0m\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker\u001b[38;5;241m.\u001b[39mFixedLocator):\n\u001b[0;32m   2114\u001b[0m     \u001b[38;5;66;03m# Passing [] as a list of labels is often used as a way to\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 labels\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of FixedLocator locations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2119\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), usually from a call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set_ticks, does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2121\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the number of labels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2122\u001b[0m     tickd \u001b[38;5;241m=\u001b[39m {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs, labels)}\n\u001b[0;32m   2123\u001b[0m     func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_with_dict, tickd)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of FixedLocator locations (7), usually from a call to set_ticks, does not match the number of labels (10)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_X_tensor)  # Forward pass\n",
    "    y_pred = torch.argmax(y_pred, dim=1)  # Get class predictions\n",
    "\n",
    "# Convert tensors to NumPy arrays for sklearn\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_true = test_Y_tensor.cpu().numpy()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))  #Creates both a figure and an ax\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "# Rotate axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd35ae8-6d07-4480-ac58-1ed59db4db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_score = F.softmax(model(test_X_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcbd44-687c-45a2-9c00-879918c67e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099999994"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y_score.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8846-8af7-4fd7-9d88-9fefb32eb6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n\u001b[0;32m      3\u001b[0m label_binarizer \u001b[38;5;241m=\u001b[39m LabelBinarizer()\u001b[38;5;241m.\u001b[39mfit(train_data_y)\n\u001b[1;32m----> 4\u001b[0m y_onehot_test \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_binarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_onehot_test\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# (n_samples, n_classes)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:363\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_is_multilabel \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_type_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe object was not fitted with multilabel input.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlabel_binarize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TESTER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:530\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    528\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m y_type:\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultioutput target data is not supported with label binarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of target data is not known\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(train_data_y)\n",
    "y_onehot_test = label_binarizer.transform(test_labels_encoded)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20700c-6029-414e-bac8-a661f60724a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test.ravel(),\n",
    "    y_score.ravel(),\n",
    "    name=\"micro-average OvR\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True)\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b54eef",
   "metadata": {},
   "source": [
    "## Feature Selection using ANOVA F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the top 40 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=40)\n",
    "X_selected = selector.fit_transform(train_data_X, train_labels_encoded[\"label\"])\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = train_data_X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", selected_features.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
