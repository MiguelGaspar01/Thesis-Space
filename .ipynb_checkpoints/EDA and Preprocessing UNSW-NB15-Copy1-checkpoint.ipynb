{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444a49bd-270c-4174-8a95-e94d53f0f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "#pip install git+https://github.com/KindXiaoming/pykan.git\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f75fa4e-1729-49f1-a197-a9b40368377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_testing-set.csv\")\n",
    "train_data = pd.read_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\UNSW_NB15_training-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c4fc6f-5f2b-4a23-940a-c67baa5aa567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 131)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[\"proto\"].unique()), len(test_data[\"proto\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fed6f0e-91c3-495f-af48-05bd68d96ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = train_data.select_dtypes(include=\"number\").drop(columns=\"id\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47e5f81-6ae6-4c93-ad25-f2f810fedeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175341 entries, 0 to 175340\n",
      "Data columns (total 40 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   dur                175341 non-null  float64\n",
      " 1   spkts              175341 non-null  int64  \n",
      " 2   dpkts              175341 non-null  int64  \n",
      " 3   sbytes             175341 non-null  int64  \n",
      " 4   dbytes             175341 non-null  int64  \n",
      " 5   rate               175341 non-null  float64\n",
      " 6   sttl               175341 non-null  int64  \n",
      " 7   dttl               175341 non-null  int64  \n",
      " 8   sload              175341 non-null  float64\n",
      " 9   dload              175341 non-null  float64\n",
      " 10  sloss              175341 non-null  int64  \n",
      " 11  dloss              175341 non-null  int64  \n",
      " 12  sinpkt             175341 non-null  float64\n",
      " 13  dinpkt             175341 non-null  float64\n",
      " 14  sjit               175341 non-null  float64\n",
      " 15  djit               175341 non-null  float64\n",
      " 16  swin               175341 non-null  int64  \n",
      " 17  stcpb              175341 non-null  int64  \n",
      " 18  dtcpb              175341 non-null  int64  \n",
      " 19  dwin               175341 non-null  int64  \n",
      " 20  tcprtt             175341 non-null  float64\n",
      " 21  synack             175341 non-null  float64\n",
      " 22  ackdat             175341 non-null  float64\n",
      " 23  smean              175341 non-null  int64  \n",
      " 24  dmean              175341 non-null  int64  \n",
      " 25  trans_depth        175341 non-null  int64  \n",
      " 26  response_body_len  175341 non-null  int64  \n",
      " 27  ct_srv_src         175341 non-null  int64  \n",
      " 28  ct_state_ttl       175341 non-null  int64  \n",
      " 29  ct_dst_ltm         175341 non-null  int64  \n",
      " 30  ct_src_dport_ltm   175341 non-null  int64  \n",
      " 31  ct_dst_sport_ltm   175341 non-null  int64  \n",
      " 32  ct_dst_src_ltm     175341 non-null  int64  \n",
      " 33  is_ftp_login       175341 non-null  int64  \n",
      " 34  ct_ftp_cmd         175341 non-null  int64  \n",
      " 35  ct_flw_http_mthd   175341 non-null  int64  \n",
      " 36  ct_src_ltm         175341 non-null  int64  \n",
      " 37  ct_srv_dst         175341 non-null  int64  \n",
      " 38  is_sm_ips_ports    175341 non-null  int64  \n",
      " 39  label              175341 non-null  int64  \n",
      "dtypes: float64(11), int64(29)\n",
      "memory usage: 53.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',\n",
       "        'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
       "        'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean',\n",
       "        'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src',\n",
       "        'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
       "        'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
       "        'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'label'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[columns_numeric].info(),columns_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e117f765-2b1f-4a7d-be31-0fd7a920609e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbfa7a1-44a4-40cf-bef1-02c4bbd9265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_numeric = [\n",
    "    col for col in columns_numeric\n",
    "    if not len(train_data[col].value_counts()) < 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7cb8512-1c45-445b-8697-96927ba3a237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afca2383-4965-47ae-86d6-b03bfc13fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba62952-362d-4233-b35c-2052726b2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ac6c23-b9e6-4b62-94c3-e9b7259e19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>sinpkt</th>\n",
       "      <th>dinpkt</th>\n",
       "      <th>sjit</th>\n",
       "      <th>djit</th>\n",
       "      <th>swin</th>\n",
       "      <th>stcpb</th>\n",
       "      <th>dtcpb</th>\n",
       "      <th>dwin</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>ackdat</th>\n",
       "      <th>smean</th>\n",
       "      <th>dmean</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>response_body_len</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_state_ttl</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.121478</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>252</td>\n",
       "      <td>254</td>\n",
       "      <td>14158.942380</td>\n",
       "      <td>8495.365234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.2956</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>30.177547</td>\n",
       "      <td>11.830604</td>\n",
       "      <td>255</td>\n",
       "      <td>621772692</td>\n",
       "      <td>2202533631</td>\n",
       "      <td>255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.649902</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>8395.112305</td>\n",
       "      <td>503571.312500</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>49.9150</td>\n",
       "      <td>15.432865</td>\n",
       "      <td>61.426934</td>\n",
       "      <td>1387.778330</td>\n",
       "      <td>255</td>\n",
       "      <td>1417884146</td>\n",
       "      <td>3077387971</td>\n",
       "      <td>255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>1106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dur proto service state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
       "id                                                                          \n",
       "1   0.121478   tcp       -   FIN      6      4     258     172  74.087490   \n",
       "2   0.649902   tcp       -   FIN     14     38     734   42014  78.473372   \n",
       "\n",
       "    sttl  dttl         sload          dload  sloss  dloss   sinpkt     dinpkt  \\\n",
       "id                                                                              \n",
       "1    252   254  14158.942380    8495.365234      0      0  24.2956   8.375000   \n",
       "2     62   252   8395.112305  503571.312500      2     17  49.9150  15.432865   \n",
       "\n",
       "         sjit         djit  swin       stcpb       dtcpb  dwin  tcprtt  \\\n",
       "id                                                                       \n",
       "1   30.177547    11.830604   255   621772692  2202533631   255     0.0   \n",
       "2   61.426934  1387.778330   255  1417884146  3077387971   255     0.0   \n",
       "\n",
       "    synack  ackdat  smean  dmean  trans_depth  response_body_len  ct_srv_src  \\\n",
       "id                                                                             \n",
       "1      0.0     0.0     43     43            0                  0           1   \n",
       "2      0.0     0.0     52   1106            0                  0          43   \n",
       "\n",
       "    ct_state_ttl  ct_dst_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
       "id                                                                 \n",
       "1              0           1                 1                 1   \n",
       "2              1           1                 1                 1   \n",
       "\n",
       "    ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  \\\n",
       "id                                                                           \n",
       "1                1             0           0                 0           1   \n",
       "2                2             0           0                 0           1   \n",
       "\n",
       "    ct_srv_dst  is_sm_ips_ports attack_cat  label  \n",
       "id                                                 \n",
       "1            1                0     Normal      0  \n",
       "2            6                0     Normal      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 120)\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9f7415-a4c8-4ff7-a876-c7f5befb5f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sbytes\n",
       "114      39121\n",
       "200      23484\n",
       "146       4989\n",
       "168       3578\n",
       "564       3530\n",
       "         ...  \n",
       "5546         1\n",
       "4734         1\n",
       "72662        1\n",
       "31049        1\n",
       "8756         1\n",
       "Name: count, Length: 7214, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"sbytes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b39ce05-7bd4-4483-b9d6-1692d4d5ac93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ftp_login\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_ftp_login\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m test_data\u001b[38;5;241m.\u001b[39miloc[test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_ftp_login\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_ftp_login\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:2164\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2162\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 2164\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[0;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[0;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   2174\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1749\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1747\u001b[0m key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(key):\n\u001b[1;32m-> 1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "train_data.iloc[train_data[\"is_ftp_login\"] > 1, \"is_ftp_login\"] = 1\n",
    "test_data.iloc[test_data[\"is_ftp_login\"] > 1, \"is_ftp_login\"] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09aa3d-fd42-4598-bb3e-3e2a20e3dc56",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872b710-6197-484a-93e8-abd2a6587929",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb095de-80f2-4667-9666-d3dbb6d83b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742da2b9-e62c-4540-9d4a-18492a2f6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#scaler = RobustScaler().set_output(transform=\"pandas\")\n",
    "#train_data[columns_numeric] = scaler.fit_transform(train_data[columns_numeric])\n",
    "#test_data[columns_numeric] = scaler.transform(test_data[columns_numeric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cc5c7-fc2c-488a-b5e3-ab7cd603d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Winsorizer:\n",
    "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "        \"\"\"\n",
    "        Initialize the Winsorizer with predefined percentiles.\n",
    "        \"\"\"\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "        self.bounds = {}  # Store calculated bounds for each feature\n",
    "\n",
    "    def fit(self, X_train, columns_numeric):\n",
    "        \"\"\"\n",
    "        Compute the Winsorization bounds from the training data only for numeric columns.\n",
    "        \"\"\"\n",
    "        self.bounds = {}  # Reset bounds\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            lower_bound = np.percentile(X_train[col], self.lower_percentile)\n",
    "            upper_bound = np.percentile(X_train[col], self.upper_percentile)\n",
    "            self.bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        print(\"Winsorization bounds computed for numeric columns.\")\n",
    "\n",
    "    def transform(self, X, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply the stored Winsorization bounds to the numeric columns of any dataset (train or test).\n",
    "        \"\"\"\n",
    "        X_winsorized = X.copy()\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            if col in self.bounds:\n",
    "                lower_bound, upper_bound = self.bounds[col]\n",
    "                X_winsorized[col] = np.clip(X[col], lower_bound, upper_bound)\n",
    "        \n",
    "        print(\"Winsorization applied to numeric columns.\")\n",
    "        return X_winsorized\n",
    "\n",
    "    def fit_transform(self, X_train, columns_numeric):\n",
    "        \"\"\"\n",
    "        Combine fit and transform for training data.\n",
    "        \"\"\"\n",
    "        self.fit(X_train, columns_numeric)\n",
    "        return self.transform(X_train, columns_numeric)\n",
    "\n",
    "\n",
    "# Instantiate and apply Winsorization\n",
    "#winsorizer = Winsorizer(lower_percentile=1, upper_percentile=99)\n",
    "#winsorizer.fit(train_data, columns_numeric)\n",
    "#train_data = winsorizer.transform(train_data, columns_numeric)\n",
    "#test_data = winsorizer.transform(test_data, columns_numeric)  # Apply same transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc993fd2-7ecf-4df1-a9d5-48d77ca852fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimate_contamination_iqr(data, columns_numeric):\n",
    "    \"\"\"\n",
    "    Estimate a reasonable contamination value based on IQR.\n",
    "    \"\"\"\n",
    "    contamination_rates = []\n",
    "    for col in columns_numeric:\n",
    "        q1 = np.percentile(data[col], 25)\n",
    "        q3 = np.percentile(data[col], 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Count outliers\n",
    "        num_outliers = np.sum((data[col] < lower_bound) | (data[col] > upper_bound))\n",
    "        contamination_rates.append(num_outliers / len(data))\n",
    "\n",
    "    return np.mean(contamination_rates)  # Average contamination rate across all numeric columns\n",
    "\n",
    "# Example usage\n",
    "estimated_contamination = estimate_contamination_iqr(train_data, columns_numeric)\n",
    "print(f\"Estimated contamination: {estimated_contamination:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197decca-5200-4368-95fc-73728f966457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class OutlierIsolationForest:\n",
    "    def __init__(self, contamination=0.05, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Isolation Forest for outlier detection.\n",
    "\n",
    "        Parameters:\n",
    "        - contamination (float): The proportion of data points to be considered as outliers.\n",
    "        - random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.contamination = contamination\n",
    "        self.random_state = random_state\n",
    "        self.iso_forest = None\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Fit the Isolation Forest model on the numeric features to detect outliers.\n",
    "\n",
    "        Parameters:\n",
    "        - dataframe (pd.DataFrame): The dataset containing numeric features.\n",
    "        - columns_numeric (list): List of numeric columns to process.\n",
    "        \"\"\"\n",
    "        print(\"Fitting Isolation Forest for outlier detection...\")\n",
    "\n",
    "        # Ensure numeric columns are float type\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Initialize and fit Isolation Forest\n",
    "        self.iso_forest = IsolationForest(contamination=self.contamination, random_state=self.random_state)\n",
    "        self.iso_forest.fit(dataframe[columns_numeric])\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply Isolation Forest to remove detected outliers.\n",
    "\n",
    "        Parameters:\n",
    "        - dataframe (pd.DataFrame): The dataset to process.\n",
    "        - columns_numeric (list): List of numeric columns to process.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: The cleaned dataset without outliers.\n",
    "        \"\"\"\n",
    "        print(\"Applying Isolation Forest for outlier removal...\")\n",
    "\n",
    "        # Ensure numeric columns are float type\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Predict outliers (-1 indicates an outlier, 1 indicates normal data)\n",
    "        outlier_predictions = self.iso_forest.predict(dataframe[columns_numeric])\n",
    "        \n",
    "        # Keep only non-outliers\n",
    "        dataframe_cleaned = dataframe[outlier_predictions == 1].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Outliers removed: {len(dataframe) - len(dataframe_cleaned)}\")\n",
    "        return dataframe_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab1353-df57-4f09-810d-7ecec2972963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iso_forest_outlier = OutlierIsolationForest(contamination=0.091)  # Adjust contamination as needed\n",
    "#iso_forest_outlier.fit(train_data, columns_numeric)\n",
    "#train_data = iso_forest_outlier.transform(train_data, columns_numeric)\n",
    "#test_data = iso_forest_outlier.transform(test_data, columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b2824-0782-4dfc-8f18-fa011ce1babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "class OutlierKNN:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierTreatment class, which stores IQR bounds.\n",
    "        The outliers will be imputed after scaling and normalization using KNNImputer.\n",
    "        \"\"\"\n",
    "        self.iqr_bounds = {}  # Store IQR bounds for each column\n",
    "        self.scaler = None  # Store StandardScaler\n",
    "        self.minmax_scaler = None  # Store MinMaxScaler\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=n_neighbors)  # KNN Imputer\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Compute IQR bounds and fit scalers and imputer on training data only.\n",
    "        \"\"\"\n",
    "        print(\"Fitting outlier treatment...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Compute IQR bounds\n",
    "        for col in columns_numeric:\n",
    "            q1 = dataframe[col].quantile(0.25)\n",
    "            q3 = dataframe[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.iqr_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        # Replace outliers with NaN to be imputed\n",
    "        for col in columns_numeric:\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Fit StandardScaler and MinMaxScaler\n",
    "        self.scaler = StandardScaler()\n",
    "        self.minmax_scaler = MinMaxScaler()\n",
    "\n",
    "        dataframe_scaled = self.scaler.fit_transform(dataframe[columns_numeric])\n",
    "        dataframe_scaled = self.minmax_scaler.fit_transform(dataframe_scaled)\n",
    "\n",
    "        # Fit KNN Imputer on scaled data\n",
    "        self.knn_imputer.fit(dataframe_scaled)\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply outlier treatment using precomputed IQR bounds, scaling, normalization, and KNN imputation.\n",
    "        \"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        # Replace outliers with NaN to be imputed\n",
    "        for col in columns_numeric:\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Apply scaling and normalization\n",
    "        dataframe_scaled = self.scaler.transform(dataframe[columns_numeric])\n",
    "        dataframe_scaled = self.minmax_scaler.transform(dataframe_scaled)\n",
    "\n",
    "        # Apply KNN Imputation\n",
    "        dataframe_imputed = self.knn_imputer.transform(dataframe_scaled)\n",
    "\n",
    "        # Reverse scaling to restore original range\n",
    "        dataframe_restored = self.scaler.inverse_transform(self.minmax_scaler.inverse_transform(dataframe_imputed))\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        dataframe[columns_numeric] = dataframe_restored\n",
    "\n",
    "        print(\"Transformation completed.\")\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af3c68-bba4-486e-b958-d9e39c9b2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNNOutlierApplier = OutlierKNN(n_neighbors=3)\n",
    "\n",
    "#KNNOutlierApplier.fit(train_data,columns_numeric)\n",
    "\n",
    "#train_data = KNNOutlierApplier.transform(train_data,columns_numeric)\n",
    "#test_data = KNNOutlierApplier.transform(test_data, columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141f1c2-3e0e-4494-b4c5-5bff0c88c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class specific = PROBLEMATIC\n",
    "\n",
    "def treat_outliers(dataframe, columns_numeric, isTest=False):\n",
    "    \"\"\"\n",
    "    Treats outliers in the dataset for the specified numeric columns.\n",
    "\n",
    "    On the training set:\n",
    "        - Identifies outliers using the IQR method for each column grouped by \"attack_cat\".\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "        - Stores the IQR bounds for each column and class.\n",
    "\n",
    "    On the test set:\n",
    "        - Uses the stored IQR bounds from training for each column and class.\n",
    "        - Replaces outliers with the mean value of the column grouped by \"attack_cat\".\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The dataset to process.\n",
    "        columns_numeric (list): List of numeric columns to focus on for outlier treatment.\n",
    "        isTest (bool): Whether the function is applied to the test set.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset with outliers treated.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(treat_outliers, \"iqr_bounds\"):\n",
    "        treat_outliers.iqr_bounds = {}  # Store IQR bounds for training (keyed by column and class)\n",
    "\n",
    "    if isTest and not treat_outliers.iqr_bounds:\n",
    "        raise ValueError(\"IQR bounds are not set. Train the function on the training data first.\")\n",
    "\n",
    "    print(\"Treating outliers...\")\n",
    "    \n",
    "    # Ensure all numeric columns are cast to float to handle outliers consistently\n",
    "    dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "    # Process each numeric column\n",
    "    for col in columns_numeric:\n",
    "        if not isTest:\n",
    "            # Initialize dictionary for column if not already present\n",
    "            treat_outliers.iqr_bounds[col] = {}\n",
    "\n",
    "            # Group by \"attack_cat\" to calculate class-specific IQR bounds\n",
    "            for attack_cat, group in dataframe.groupby(\"attack_cat\"):\n",
    "                q1 = group[col].quantile(0.25)\n",
    "                q3 = group[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                treat_outliers.iqr_bounds[col][attack_cat] = (lower_bound, upper_bound)\n",
    "\n",
    "                # Identify outliers for this class\n",
    "                outliers = (group[col] < lower_bound) | (group[col] > upper_bound)\n",
    "\n",
    "                # Replace outliers with the class mean\n",
    "                replacement_mean = group[col].mean()\n",
    "                dataframe.loc[group[outliers].index, col] = replacement_mean\n",
    "        else:\n",
    "            # Use stored IQR bounds for test set\n",
    "            for attack_cat, bounds in treat_outliers.iqr_bounds[col].items():\n",
    "                lower_bound, upper_bound = bounds\n",
    "        \n",
    "                # Get test data rows for this attack_cat\n",
    "                class_rows = dataframe[dataframe[\"attack_cat\"] == attack_cat]\n",
    "        \n",
    "                # Identify outliers\n",
    "                outliers = (class_rows[col] < lower_bound) | (class_rows[col] > upper_bound)\n",
    "        \n",
    "                # Replace outliers with the training class mean\n",
    "                replacement_mean = dataframe[dataframe[\"attack_cat\"] == attack_cat][col].mean()\n",
    "                dataframe.loc[class_rows[outliers].index, col] = replacement_mean\n",
    "\n",
    "\n",
    "    print(\"Outlier treatment completed.\")\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8a262-a511-4715-bce5-8f2f7a8a16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class OutlierTreatment:\n",
    "    def __init__(self, strategy=\"mean\"):\n",
    "        \"\"\"\n",
    "        Initialize the OutlierTreatment class, which stores IQR bounds and global means/medians from training.\n",
    "        \"\"\"\n",
    "        self.iqr_bounds = {}  # Store IQR bounds for each column\n",
    "        self.global_imputer = None  # Store imputer for handling outliers\n",
    "\n",
    "        if strategy not in [\"mean\", \"median\"]:\n",
    "            raise ValueError(\"Invalid strategy. Use 'mean' or 'median'.\")\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Compute IQR bounds and global statistics (mean/median) from training data only.\n",
    "        \"\"\"\n",
    "        print(\"Fitting outlier treatment...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        self.global_imputer = SimpleImputer(strategy=self.strategy)\n",
    "        self.global_imputer.fit(dataframe[columns_numeric])  # Fit global imputer\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            q1 = dataframe[col].quantile(0.25)\n",
    "            q3 = dataframe[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            self.iqr_bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        print(\"Fitting completed.\")\n",
    "\n",
    "    def transform(self, dataframe, columns_numeric):\n",
    "        \"\"\"\n",
    "        Apply outlier treatment using precomputed IQR bounds and global imputation.\n",
    "        \"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "\n",
    "        dataframe[columns_numeric] = dataframe[columns_numeric].astype(float)\n",
    "\n",
    "        for col in columns_numeric:\n",
    "            if col not in self.iqr_bounds:\n",
    "                raise ValueError(f\"IQR bounds for column '{col}' are not set. Ensure you fit on training data first.\")\n",
    "\n",
    "            lower_bound, upper_bound = self.iqr_bounds[col]\n",
    "            outliers = (dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)\n",
    "\n",
    "            # Replace outliers with NaN to be imputed\n",
    "            dataframe.loc[outliers, col] = np.nan\n",
    "\n",
    "        # Apply global imputation\n",
    "        dataframe[columns_numeric] = self.global_imputer.transform(dataframe[columns_numeric])\n",
    "\n",
    "        print(\"Transformation completed.\")\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67678e7e-d83c-4d5a-91b1-63da92b1f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = treat_outliers(train_data, columns_numeric)\n",
    "#test_data = treat_outliers(test_data, columns_numeric, isTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba3182-5181-4560-a21d-983aab6abf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlierhandler = OutlierTreatment(strategy = \"median\")\n",
    "#outlierhandler.fit(train_data,columns_numeric)\n",
    "#train_data = outlierhandler.transform(train_data,columns_numeric)\n",
    "#test_data = outlierhandler.transform(test_data,columns_numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba1d4f-2e0e-4a35-aa77-2c5eea7171cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(20,2, figsize=(20,40), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for col,ax in zip(columns_numeric, axes):\n",
    "    g = sns.boxplot(data = train_data, y = col, x = train_data[\"attack_cat\"],hue = train_data[\"attack_cat\"], ax = ax)\n",
    "    g.set_title(f\"Boxplot for {col}\")\n",
    "[fig.delaxes(ax) for ax in axes.flatten() if not ax.has_data()]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab53e0-79a2-453a-9792-1b758967e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "train_cat.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23f0c7-76e5-4eac-982b-ac547a0a9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce categories in categorical columns while preserving the target column\n",
    "def reduce_categories(train_data, test_data, target_column, threshold=6, debug=False):\n",
    "    # Select categorical columns in train and test data, excluding the target column\n",
    "    train_cat = train_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    test_cat = test_data.select_dtypes(exclude=[np.number]).drop(columns=[target_column], errors='ignore')\n",
    "    \n",
    "    # Iterate through categorical columns\n",
    "    for feature in train_cat.columns:\n",
    "        if debug:\n",
    "            print(f\"Processing feature: {feature}\")\n",
    "            print(f\"Number of unique categories before reduction: {train_cat[feature].nunique()}\")\n",
    "            print('----------------------------------------------------')\n",
    "        \n",
    "        # Check if the number of unique categories exceeds the threshold\n",
    "        if train_cat[feature].nunique() > threshold:\n",
    "            # Identify the top categories in the train set\n",
    "            top_categories = train_data[feature].value_counts().head(threshold).index\n",
    "            \n",
    "            # Reduce train and test data to these top categories, replacing others with '-'\n",
    "            train_data[feature] = np.where(train_data[feature].isin(top_categories), train_data[feature], '-')\n",
    "            test_data[feature] = np.where(test_data[feature].isin(top_categories), test_data[feature], '-')\n",
    "    \n",
    "    # Return the modified train and test datasets\n",
    "    return train_data, test_data\n",
    "\n",
    "# Apply the function to your train and test datasets\n",
    "train_data, test_data = reduce_categories(train_data, test_data, target_column='attack_cat', threshold=6, debug=False)\n",
    "\n",
    "# Check the resulting categorical columns\n",
    "train_cat = train_data.select_dtypes(exclude=[np.number])\n",
    "test_cat = test_data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "print(\"Train categorical summary after reduction:\")\n",
    "print(train_cat.describe(include='all'))\n",
    "\n",
    "print(\"\\nTest categorical summary after reduction:\")\n",
    "print(test_cat.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9250b9-da54-49ea-823c-70d2c65101bc",
   "metadata": {},
   "source": [
    "## Normalizing and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb003f-055e-441c-9e29-69e0880e29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc7eb1-09f6-4301-a811-adcf13a479f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train_data.select_dtypes(include=\"number\").columns\n",
    "\n",
    "normalizer = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "#r_scaler = RobustScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "#train_data[numeric] = r_scaler.fit_transform(train_data[numeric])\n",
    "#test_data[numeric] = r_scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = scaler.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = scaler.transform(test_data[numeric])\n",
    "\n",
    "train_data[numeric] = normalizer.fit_transform(train_data[numeric])\n",
    "test_data[numeric] = normalizer.transform(test_data[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f57f7-4b70-4b50-b7cb-3af10c44bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237a713-240f-4f46-82d4-0c55bdf1d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[\"state\"].unique()), len(test_data[\"state\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f098c9-1990-49e0-9d8a-1047acdeb567",
   "metadata": {},
   "source": [
    "## Treating Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3c56b-3f26-4db7-9fcf-1421a44e91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set skewness threshold (e.g., |skewness| > 1 is considered highly skewed)\n",
    "skew_threshold = 1\n",
    "\n",
    "numeric_cols = train_data.select_dtypes(include = \"number\").columns\n",
    "\n",
    "# Select numeric columns\n",
    "# Calculate skewness for numeric columns\n",
    "skewness = train_data[numeric_cols].skew()\n",
    "\n",
    "# Identify columns with skewness above the threshold\n",
    "skewed_cols = skewness[skewness.abs() > skew_threshold].index\n",
    "print(\"Skewed columns:\", skewed_cols)\n",
    "\n",
    "# Apply log transformation to skewed columns\n",
    "for col in skewed_cols:\n",
    "    # Add 1 to avoid issues with log(0)\n",
    "    train_data[col] = np.log1p(train_data[col])\n",
    "    test_data[col] = np.log1p(test_data[col])  # Apply same transformation to test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f467fa9-3cd3-4393-8b49-165534f19504",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdc915-e753-4a4a-9f19-977ce4c395c9",
   "metadata": {},
   "source": [
    "## OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09567e29-cacc-4d54-abf8-474f88fcedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "train_data_X = train_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "train_data_y = train_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "test_data_X = test_data.drop(columns=[\"attack_cat\", \"label\"])\n",
    "test_data_y = test_data[[\"attack_cat\", \"label\"]]\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = train_data_X.select_dtypes(include=\"object\").columns\n",
    "print(categorical_cols)  # Verify the categorical columns\n",
    "\n",
    "# Use OneHotEncoder with handle_unknown='ignore' to ensure consistency\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define and fit the encoder on training data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', drop='first')  # Drop first category to avoid dummy variable trap\n",
    "ct = ColumnTransformer(transformers=[('encoder', encoder, categorical_cols)], remainder='passthrough')\n",
    "\n",
    "# Fit the encoder on train data\n",
    "train_data_X_encoded = ct.fit_transform(train_data_X)\n",
    "\n",
    "# Transform the test data using the same encoder\n",
    "test_data_X_encoded = ct.transform(test_data_X)\n",
    "\n",
    "# Convert the encoded data back to DataFrame for alignment\n",
    "import pandas as pd\n",
    "train_data_X_encoded = pd.DataFrame(train_data_X_encoded, columns=ct.get_feature_names_out(), index=train_data_X.index)\n",
    "test_data_X_encoded = pd.DataFrame(test_data_X_encoded, columns=ct.get_feature_names_out(), index=test_data_X.index)\n",
    "\n",
    "# Ensure consistent columns between train and test\n",
    "train_data_X, test_data_X = train_data_X_encoded.align(test_data_X_encoded, join=\"outer\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc749ff-5d8c-42fb-bd0f-8632dc76be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, column_name in enumerate(test_data_X.columns):\n",
    "    print(index, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Ensure you work with copies of the DataFrames\n",
    "train_data_X = train_data_X.copy()\n",
    "test_data_X = test_data_X.copy()\n",
    "\n",
    "# Filter only numeric columns for scaling\n",
    "numeric_cols_train = train_data_X.select_dtypes(include=['number']).columns\n",
    "numeric_cols_test = test_data_X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Ensure numeric columns are cast to float before scaling\n",
    "train_data_X[numeric_cols_train] = train_data_X[numeric_cols_train].astype(float)\n",
    "test_data_X[numeric_cols_test] = test_data_X[numeric_cols_test].astype(float)\n",
    "\n",
    "# Initialize scalers\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply StandardScaler to train_data\n",
    "train_data_scaled = standard_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply StandardScaler to test_data\n",
    "test_data_scaled = standard_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to train_data\n",
    "train_data_scaled = minmax_scaler.fit_transform(train_data_X[numeric_cols_train])\n",
    "train_data_X.loc[:, numeric_cols_train] = train_data_scaled\n",
    "\n",
    "# Apply MinMaxScaler to test_data\n",
    "test_data_scaled = minmax_scaler.transform(test_data_X[numeric_cols_test])\n",
    "test_data_X.loc[:, numeric_cols_test] = test_data_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51262960-8085-401f-a7da-44c9b682258d",
   "metadata": {},
   "source": [
    "### Converting data to required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8a649-8f38-4489-a686-caa97f61f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data_X = pd.DataFrame(scaler.fit_transform(train_data_X), columns=train_data_X.columns)\n",
    "test_data_X = pd.DataFrame(scaler.transform(test_data_X), columns=test_data_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eb3fd-f663-480d-b324-df1bdc58a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = pd.DataFrame(label_encoder.fit_transform(train_data_y[\"attack_cat\"]))\n",
    "test_labels_encoded = pd.DataFrame(label_encoder.transform(test_data_y[\"attack_cat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad226e-b843-4273-82ee-9809c5d04aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_X_data.csv\", index=False)\n",
    "test_data_X.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_X_data.csv\", index=False)\n",
    "train_labels_encoded.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_train_data_y_data.csv\", index=False)\n",
    "test_labels_encoded.to_csv(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\processed_test_data_y_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491aeaf5-7cdd-4e18-ae14-043b15453f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_class_labels = label_encoder.classes_\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Save the LabelEncoder object to a file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Thesis Space Desktop\\Data After Preprocess\\label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646596b7-a6c0-466d-a559-7509d1e1678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004cb90-c27d-4926-a5d7-6b11d3cb17db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142e740-6dc1-48c3-91e3-4d364d2140b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a9a89-a036-4913-b787-9add6c251cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f312a2-9d06-415e-914e-2e46866d13df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbee27-9515-46f4-96ac-8ebed45448f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236eb12-063b-44b0-b3b6-41a1bd0a1f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcaa09-f0dd-4e2b-bc95-ddd2e187c2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3dd1d-836a-41a6-9196-3b785874898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6399c-3fa9-4cc6-a063-962e37923dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b499f-b1e7-4f06-b751-13c93c96b378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47351b-e56f-4f9e-92cb-3e29e96458a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165a3d2-5dd3-47fb-99c0-b492dee8adbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7341c-73f8-4d54-9148-daf1212e8f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
